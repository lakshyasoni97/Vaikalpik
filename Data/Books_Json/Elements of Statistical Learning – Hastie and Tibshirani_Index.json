{"1": {"Title": "Introduction", "Sub Topics": {}}, "2": {"Title": "Overview of Supervised Learning", "Sub Topics": {"2.1": {"Title": "Introduction", "Sub Topics": {}}, "2.2": {"Title": "Variable Types and Terminology", "Sub Topics": {}}, "2.4": {"Title": "Statistical Decision Theory", "Sub Topics": {}}, "2.5": {"Title": "Local Methods in High Dimensions", "Sub Topics": {}}, "2.7": {"Title": "Structured Regression Models", "Sub Topics": {"2.7.1": {"Title": "Di\ufb03culty of the Problem"}}}, "2.8": {"Title": "Classes of Restricted Estimators", "Sub Topics": {"2.8.1": {"Title": "Roughness Penalty and Bayesian Methods"}, "2.8.2": {"Title": "Kernel Methods and Local Regression"}, "2.8.3": {"Title": "Basis Functions and Dictionary Methods"}}}, "2.9": {"Title": "Model Selection and the Bias\u2013Variance Tradeo\ufb00", "Sub Topics": {}}}}, "3": {"Title": "Linear Methods for Regression", "Sub Topics": {"3.1": {"Title": "Introduction", "Sub Topics": {}}, "3.2": {"Title": "Linear Regression Models and Least Squares", "Sub Topics": {"3.2.1": {"Title": "Example: Prostate Cancer"}, "3.2.2": {"Title": "The Gauss\u2013Markov Theorem"}, "3.2.4": {"Title": "Multiple Outputs"}}}, "3.3": {"Title": "Subset Selection", "Sub Topics": {"3.3.1": {"Title": "Best-Subset Selection"}, "3.3.2": {"Title": "Forward- and Backward-Stepwise Selection"}, "3.3.3": {"Title": "Forward-Stagewise Regression"}, "3.3.4": {"Title": "Prostate Cancer Data Example (Continued)"}}}, "3.4": {"Title": "Shrinkage Methods", "Sub Topics": {"3.4.1": {"Title": "Ridge Regression"}, "3.4.2": {"Title": "The Lasso"}, "3.4.4": {"Title": "Least Angle Regression"}}}, "3.5": {"Title": "Methods Using Derived Input Directions", "Sub Topics": {"3.5.1": {"Title": "Principal Components Regression"}, "3.5.2": {"Title": "Partial Least Squares"}}}, "3.7": {"Title": "Multiple Outcome Shrinkage and Selection", "Sub Topics": {}}, "3.8": {"Title": "More on the Lasso and Related Path Algorithms", "Sub Topics": {"3.8.1": {"Title": "Incremental Forward Stagewise Regression"}, "3.8.2": {"Title": "Piecewise-Linear Path Algorithms"}, "3.8.3": {"Title": "The Dantzig Selector"}, "3.8.4": {"Title": "The Grouped Lasso"}, "3.8.5": {"Title": "Further Properties of the Lasso"}, "3.8.6": {"Title": "Pathwise Coordinate Optimization"}}}, "3.9": {"Title": "Computational Considerations", "Sub Topics": {}}}}, "4": {"Title": "Linear Methods for Classi\ufb01cation", "Sub Topics": {"4.1": {"Title": "Introduction", "Sub Topics": {}}, "4.2": {"Title": "Linear Regression of an Indicator Matrix", "Sub Topics": {}}, "4.3": {"Title": "Linear Discriminant Analysis", "Sub Topics": {"4.3.1": {"Title": "Regularized Discriminant Analysis"}, "4.3.2": {"Title": "Computations for LDA"}, "4.3.3": {"Title": "Reduced-Rank Linear Discriminant Analysis"}}}, "4.4": {"Title": "Logistic Regression", "Sub Topics": {"4.4.1": {"Title": "Fitting Logistic Regression Models"}, "4.4.2": {"Title": "Example: South African Heart Disease"}, "4.4.3": {"Title": "Quadratic Approximations and Inference"}, "4.4.4": {"Title": "L1Regularized Logistic Regression"}, "4.4.5": {"Title": "Logistic Regression or LDA?"}}}, "4.5": {"Title": "Separating Hyperplanes", "Sub Topics": {"4.5.1": {"Title": "Rosenblatt\u2019s Perceptron Learning Algorithm"}, "4.5.2": {"Title": "Optimal Separating Hyperplanes"}}}}}, "5": {"Title": "Basis Expansions and Regularization", "Sub Topics": {"5.1": {"Title": "Introduction", "Sub Topics": {}}, "5.2": {"Title": "Piecewise Polynomials and Splines", "Sub Topics": {"5.2.1": {"Title": "Natural Cubic Splines"}, "5.2.3": {"Title": "Example: Phoneme Recognition"}}}, "5.3": {"Title": "Filtering and Feature Extraction", "Sub Topics": {}}, "5.4": {"Title": "Smoothing Splines", "Sub Topics": {"5.4.1": {"Title": "Degrees of Freedom and Smoother Matrices"}}}, "5.5": {"Title": "Automatic Selection of the Smoothing Parameters", "Sub Topics": {"5.5.1": {"Title": "Fixing the Degrees of Freedom"}, "5.5.2": {"Title": "The Bias\u2013Variance Tradeo\ufb00"}}}, "5.6": {"Title": "Nonparametric Logistic Regression", "Sub Topics": {}}, "5.7": {"Title": "Multidimensional Splines", "Sub Topics": {}}, "5.8": {"Title": "Regularization and Reproducing Kernel Hilbert Spaces", "Sub Topics": {"5.8.1": {"Title": "Spaces of Functions Generated by Kernels"}, "5.8.2": {"Title": "Examples of RKHS"}}}, "5.9": {"Title": "Wavelet Smoothing", "Sub Topics": {"5.9.1": {"Title": "Wavelet Bases and the Wavelet Transform"}, "5.9.2": {"Title": "Adaptive Wavelet Filtering"}}}}}, "6": {"Title": "Kernel Smoothing Methods", "Sub Topics": {"6.1": {"Title": "One-Dimensional Kernel Smoothers", "Sub Topics": {"6.1.1": {"Title": "Local Linear Regression"}, "6.1.2": {"Title": "Local Polynomial Regression"}}}, "6.2": {"Title": "Selecting the Width of the Kernel", "Sub Topics": {}}, "6.3": {"Title": "Local Regression in IRp", "Sub Topics": {}}, "6.4": {"Title": "Structured Local Regression Models in IRp", "Sub Topics": {"6.4.1": {"Title": "Structured Kernels"}, "6.4.2": {"Title": "Structured Regression Functions"}}}, "6.5": {"Title": "Local Likelihood and Other Models", "Sub Topics": {}}, "6.6": {"Title": "Kernel Density Estimation and Classi\ufb01cation", "Sub Topics": {"6.6.1": {"Title": "Kernel Density Estimation"}, "6.6.2": {"Title": "Kernel Density Classi\ufb01cation"}, "6.6.3": {"Title": "The Naive Bayes Classi\ufb01er"}}}, "6.7": {"Title": "Radial Basis Functions and Kernels", "Sub Topics": {}}, "6.8": {"Title": "Mixture Models for Density Estimation and Classi\ufb01cation", "Sub Topics": {}}, "6.9": {"Title": "Computational Considerations", "Sub Topics": {}}}}, "7": {"Title": "Model Assessment and Selection", "Sub Topics": {"7.1": {"Title": "Introduction", "Sub Topics": {}}, "7.2": {"Title": "Bias, Variance and Model Complexity", "Sub Topics": {}}, "7.3": {"Title": "The Bias\u2013Variance Decomposition", "Sub Topics": {"7.3.1": {"Title": "Example: Bias\u2013Variance Tradeo\ufb00"}}}, "7.4": {"Title": "Optimism of the Training Error Rate", "Sub Topics": {}}, "7.5": {"Title": "Estimates of In-Sample Prediction Error", "Sub Topics": {}}, "7.6": {"Title": "The E\ufb00ective Number of Parameters", "Sub Topics": {}}, "7.7": {"Title": "The Bayesian Approach and BIC", "Sub Topics": {}}, "7.8": {"Title": "Minimum Description Length", "Sub Topics": {}}, "7.9": {"Title": "Vapnik\u2013Chervonenkis Dimension", "Sub Topics": {"7.9.1": {"Title": "Example (Continued)"}}}, "7.10": {"Title": "Cross-Validation", "Sub Topics": {"7.10.1": {"Title": "K-Fold Cross-Validation"}, "7.10.3": {"Title": "Does Cross-Validation Really Work?"}}}, "7.11": {"Title": "Bootstrap Methods", "Sub Topics": {"7.11.1": {"Title": "Example (Continued)"}}}, "7.12": {"Title": "Conditional or Expected Test Error?", "Sub Topics": {}}}}, "8": {"Title": "Model Inference and Averaging", "Sub Topics": {"8.1": {"Title": "Introduction", "Sub Topics": {}}, "8.2": {"Title": "The Bootstrap and Maximum Likelihood Methods", "Sub Topics": {"8.2.1": {"Title": "A Smoothing Example"}, "8.2.2": {"Title": "Maximum Likelihood Inference"}, "8.2.3": {"Title": "Bootstrap versus Maximum Likelihood"}}}, "8.3": {"Title": "Bayesian Methods", "Sub Topics": {}}, "8.5": {"Title": "The EM Algorithm", "Sub Topics": {"8.5.1": {"Title": "Two-Component Mixture Model"}, "8.5.2": {"Title": "The EM Algorithm in General"}, "8.5.3": {"Title": "EM as a Maximization\u2013Maximization Procedure"}}}, "8.6": {"Title": "MCMC for Sampling from the Posterior", "Sub Topics": {}}, "8.7": {"Title": "Bagging", "Sub Topics": {"8.7.1": {"Title": "Example: Trees with Simulated Data"}}}, "8.8": {"Title": "Model Averaging and Stacking", "Sub Topics": {}}, "8.9": {"Title": "Stochastic Search: Bumping", "Sub Topics": {}}}}, "9": {"Title": "Additive Models, Trees, and Related Methods", "Sub Topics": {"9.1": {"Title": "Generalized Additive Models", "Sub Topics": {"9.1.1": {"Title": "Fitting Additive Models"}, "9.1.2": {"Title": "Example: Additive Logistic Regression"}, "9.1.3": {"Title": "Summary"}}}, "9.2": {"Title": "Tree-Based Methods", "Sub Topics": {"9.2.1": {"Title": "Background"}, "9.2.2": {"Title": "Regression Trees"}, "9.2.3": {"Title": "Classi\ufb01cation Trees"}, "9.2.4": {"Title": "Other Issues"}, "9.2.5": {"Title": "Spam Example (Continued)"}}}, "9.3": {"Title": "PRIM: Bump Hunting", "Sub Topics": {"9.3.1": {"Title": "Spam Example (Continued)"}}}, "9.4": {"Title": "MARS: Multivariate Adaptive Regression Splines", "Sub Topics": {"9.4.1": {"Title": "Spam Example (Continued)"}, "9.4.2": {"Title": "Example (Simulated Data)"}, "9.4.3": {"Title": "Other Issues"}}}, "9.5": {"Title": "Hierarchical Mixtures of Experts", "Sub Topics": {}}, "9.6": {"Title": "Missing Data", "Sub Topics": {}}, "9.7": {"Title": "Computational Considerations", "Sub Topics": {}}}}, "10": {"Title": "Boosting and Additive Trees", "Sub Topics": {"10.1": {"Title": "Boosting Methods", "Sub Topics": {"10.1.1": {"Title": "Outline of This Chapter"}}}, "10.2": {"Title": "Boosting Fits an Additive Model", "Sub Topics": {}}, "10.3": {"Title": "Forward Stagewise Additive Modeling", "Sub Topics": {}}, "10.4": {"Title": "Exponential Loss and AdaBoost", "Sub Topics": {}}, "10.5": {"Title": "Why Exponential Loss?", "Sub Topics": {}}, "10.6": {"Title": "Loss Functions and Robustness", "Sub Topics": {}}, "10.7": {"Title": "\u201cO\ufb00-the-Shelf\u201d Procedures for Data Mining", "Sub Topics": {}}, "10.8": {"Title": "Example: Spam Data", "Sub Topics": {}}, "10.9": {"Title": "Boosting Trees", "Sub Topics": {}}, "10.10": {"Title": "Numerical Optimization via Gradient Boosting", "Sub Topics": {"10.10.1": {"Title": "Steepest Descent"}, "10.10.2": {"Title": "Gradient Boosting"}, "10.10.3": {"Title": "Implementations of Gradient Boosting"}}}, "10.11": {"Title": "Right-Sized Trees for Boosting", "Sub Topics": {}}, "10.12": {"Title": "Regularization", "Sub Topics": {"10.12.1": {"Title": "Shrinkage"}, "10.12.2": {"Title": "Subsampling"}}}, "10.13": {"Title": "Interpretation", "Sub Topics": {"10.13.1": {"Title": "Relative Importance of Predictor Variables"}, "10.13.2": {"Title": "Partial Dependence Plots"}}}, "10.14": {"Title": "Illustrations", "Sub Topics": {"10.14.1": {"Title": "California Housing"}, "10.14.2": {"Title": "New Zealand Fish"}, "10.14.3": {"Title": "Demographics Data"}}}}}, "11": {"Title": "Neural Networks", "Sub Topics": {"11.1": {"Title": "Introduction", "Sub Topics": {}}, "11.2": {"Title": "Projection Pursuit Regression", "Sub Topics": {}}, "11.3": {"Title": "Neural Networks", "Sub Topics": {}}, "11.4": {"Title": "Fitting Neural Networks", "Sub Topics": {}}, "11.5": {"Title": "Some Issues in Training Neural Networks", "Sub Topics": {"11.5.1": {"Title": "Starting Values"}, "11.5.2": {"Title": "Over\ufb01tting"}, "11.5.3": {"Title": "Scaling of the Inputs"}, "11.5.4": {"Title": "Number of Hidden Units and Layers"}, "11.5.5": {"Title": "Multiple Minima"}}}, "11.6": {"Title": "Example: Simulated Data", "Sub Topics": {}}, "11.7": {"Title": "Example: ZIP Code Data", "Sub Topics": {}}, "11.8": {"Title": "Discussion", "Sub Topics": {}}, "11.9": {"Title": "Bayesian Neural Nets and the NIPS 2003 Challenge", "Sub Topics": {"11.9.1": {"Title": "Bayes, Boosting and Bagging"}, "11.9.2": {"Title": "Performance Comparisons"}}}, "11.10": {"Title": "Computational Considerations", "Sub Topics": {}}}}, "13": {"Title": "Prototype Methods and Nearest-Neighbors", "Sub Topics": {"13.1": {"Title": "Introduction", "Sub Topics": {}}, "13.2": {"Title": "Prototype Methods", "Sub Topics": {"13.2.1": {"Title": "K-means Clustering"}, "13.2.2": {"Title": "Learning Vector Quantization"}, "13.2.3": {"Title": "Gaussian Mixtures"}}}, "13.3": {"Title": "k-Nearest-Neighbor Classi\ufb01ers", "Sub Topics": {"13.3.1": {"Title": "Example: A Comparative Study"}, "13.3.3": {"Title": "Invariant Metrics and Tangent Distance"}}}, "13.4": {"Title": "Adaptive Nearest-Neighbor Methods", "Sub Topics": {"13.4.1": {"Title": "Example"}}}, "13.5": {"Title": "Computational Considerations", "Sub Topics": {}}}}, "14": {"Title": "Unsupervised Learning", "Sub Topics": {"14.1": {"Title": "Introduction", "Sub Topics": {}}, "14.2": {"Title": "Association Rules", "Sub Topics": {"14.2.1": {"Title": "Market Basket Analysis"}, "14.2.2": {"Title": "The Apriori Algorithm"}, "14.2.3": {"Title": "Example: Market Basket Analysis"}, "14.2.4": {"Title": "Unsupervised as Supervised Learning"}, "14.2.5": {"Title": "Generalized Association Rules"}, "14.2.6": {"Title": "Choice of Supervised Learning Method"}, "14.2.7": {"Title": "Example: Market Basket Analysis (Continued)"}}}, "14.3": {"Title": "Cluster Analysis", "Sub Topics": {"14.3.1": {"Title": "Proximity Matrices"}, "14.3.2": {"Title": "Dissimilarities Based on Attributes"}, "14.3.3": {"Title": "Object Dissimilarity"}, "14.3.4": {"Title": "Clustering Algorithms"}, "14.3.5": {"Title": "Combinatorial Algorithms"}, "14.3.6": {"Title": "K-means"}, "14.3.7": {"Title": "Gaussian Mixtures as Soft K-means Clustering"}, "14.3.8": {"Title": "Example: Human Tumor Microarray Data"}, "14.3.9": {"Title": "Vector Quantization"}, "14.3.10": {"Title": "K-medoids"}, "14.3.11": {"Title": "Practical Issues"}, "14.3.12": {"Title": "Hierarchical Clustering"}}}, "14.4": {"Title": "Self-Organizing Maps", "Sub Topics": {}}, "14.5": {"Title": "Principal Components, Curves and Surfaces", "Sub Topics": {"14.5.1": {"Title": "Principal Components"}, "14.5.2": {"Title": "Principal Curves and Surfaces"}, "14.5.3": {"Title": "Spectral Clustering"}, "14.5.4": {"Title": "Kernel Principal Components"}, "14.5.5": {"Title": "Sparse Principal Components"}}}, "14.6": {"Title": "Non-negative Matrix Factorization", "Sub Topics": {"14.6.1": {"Title": "Archetypal Analysis"}}}, "14.8": {"Title": "Multidimensional Scaling", "Sub Topics": {}}, "14.10": {"Title": "The Google PageRank Algorithm", "Sub Topics": {}}}}, "15": {"Title": "Random Forests", "Sub Topics": {"15.1": {"Title": "Introduction", "Sub Topics": {}}, "15.2": {"Title": "De\ufb01nition of Random Forests", "Sub Topics": {}}, "15.3": {"Title": "Details of Random Forests", "Sub Topics": {"15.3.1": {"Title": "Out of Bag Samples"}, "15.3.2": {"Title": "Variable Importance"}, "15.3.3": {"Title": "Proximity Plots"}, "15.3.4": {"Title": "Random Forests and Over\ufb01tting"}}}, "15.4": {"Title": "Analysis of Random Forests", "Sub Topics": {"15.4.1": {"Title": "Variance and the De-Correlation E\ufb00ect"}, "15.4.2": {"Title": "Bias"}, "15.4.3": {"Title": "Adaptive Nearest Neighbors"}}}}}, "16": {"Title": "Ensemble Learning", "Sub Topics": {"16.1": {"Title": "Introduction", "Sub Topics": {}}, "16.2": {"Title": "Boosting and Regularization Paths", "Sub Topics": {"16.2.1": {"Title": "Penalized Regression"}, "16.2.2": {"Title": "The \u201cBet on Sparsity\u201d Principle"}, "16.2.3": {"Title": "Regularization Paths, Over-\ufb01tting and Margins"}}}, "16.3": {"Title": "Learning Ensembles", "Sub Topics": {"16.3.1": {"Title": "Learning a Good Ensemble"}, "16.3.2": {"Title": "Rule Ensembles"}}}}}, "17": {"Title": "Undirected Graphical Models", "Sub Topics": {"17.1": {"Title": "Introduction", "Sub Topics": {}}, "17.2": {"Title": "Markov Graphs and Their Properties", "Sub Topics": {}}, "17.3": {"Title": "Undirected Graphical Models for Continuous Variables", "Sub Topics": {"17.3.2": {"Title": "Estimation of the Graph Structure"}}}, "17.4": {"Title": "Undirected Graphical Models for Discrete Variables", "Sub Topics": {"17.4.2": {"Title": "Hidden Nodes"}, "17.4.3": {"Title": "Estimation of the Graph Structure"}, "17.4.4": {"Title": "Restricted Boltzmann Machines"}}}}}, "18": {"Title": "High-Dimensional Problems: p\u226bN", "Sub Topics": {"18.1": {"Title": "When pis Much Bigger than N", "Sub Topics": {}}, "18.3": {"Title": "Linear Classi\ufb01ers with Quadratic Regularization", "Sub Topics": {"18.3.1": {"Title": "Regularized Discriminant Analysis"}, "18.3.3": {"Title": "The Support Vector Classi\ufb01er"}, "18.3.4": {"Title": "Feature Selection"}, "18.3.5": {"Title": "Computational Shortcuts When p\u226bN"}}}, "18.4": {"Title": "Linear Classi\ufb01ers with L1Regularization", "Sub Topics": {"18.4.2": {"Title": "The Fused Lasso for Functional Data"}}}, "18.5": {"Title": "Classi\ufb01cation When Features are Unavailable", "Sub Topics": {"18.5.3": {"Title": "Example: Abstracts Classi\ufb01cation"}}}, "18.7": {"Title": "Feature Assessment and the Multiple-Testing Problem", "Sub Topics": {"18.7.1": {"Title": "The False Discovery Rate"}, "18.7.2": {"Title": "Asymmetric Cutpoints and the SAM Procedure"}, "18.7.3": {"Title": "A Bayesian Interpretation of the FDR"}}}, "18.8": {"Title": "Bibliographic Notes", "Sub Topics": {}}}}}