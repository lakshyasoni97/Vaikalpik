{"Name": "Introduction to Probability for Data Science.pdf", "Pages": {"0": "", "1": "", "2": "Introduction to Probability\nfor\nData Science\nStanley H. Chan\nPurdue University\n", "3": "Copyright \u00a92021 Stanley H. Chan\nThis book is published by Michigan Publishing under an agreement with the author. It is\nmade available free of charge in electronic form to any student or instructor interested in\nthe subject matter.\nPublished in the United States of America by\nMichigan Publishing\nManufactured in the United States of America\nISBN 978-1-60785-746-4 (hardcover)\nISBN 978-1-60785-747-1 (electronic)\nii", "4": "To Vivian, Joanna, and Cynthia Chan\nAnd ye shall know the truth, and the truth shall make you free.\nJohn 8:32\niii", "5": "iv", "6": "Preface\nThis book is an introductory textbook in undergraduate probability. It has a mission: to spell\nout the motivation ,intuition , and implication of the probabilistic tools we use in science\nand engineering. From over half a decade of teaching the course, I have distilled what I\nbelieve to be the core of probabilistic methods. I put the book in the context of data science\nto emphasize the inseparability between data (computing) and probability (theory) in our\ntime.\nProbability is one of the most interesting subjects in electrical engineering and com-\nputer science. It bridges our favorite engineering principles to the practical reality, a world\nthat is full of uncertainty. However, because probability is such a mature subject, the under-\ngraduate textbooks alone might fill several rows of shelves in a library. When the literature\nis so rich, the challenge becomes how one can pierce through to the insight while diving into\nthe details. For example, many of you have used a normal random variable before, but have\nyou ever wondered where the \u201cbell shape\u201d comes from? Every probability class will teach\nyou about flipping a coin, but how can \u201cflipping a coin\u201d ever be useful in machine learning\ntoday? Data scientists use the Poisson random variables to model the internet traffic, but\nwhere does the gorgeous Poisson equation come from? This book is designed to fill these\ngaps with knowledge that is essential to all data science students.\nThis leads to the three goals of the book. (i) Motivation: In the ocean of mathematical\ndefinitions, theorems, and equations, why should we spend our time on this particular topic\nbut not another? (ii) Intuition: When going through the derivations, is there a geometric\ninterpretation or physics beyond those equations? (iii) Implication: After we have learned a\ntopic, what new problems can we solve?\nThe book\u2019s intended audience is undergraduate juniors/seniors and first-year gradu-\nate students majoring in electrical engineering and computer science. The prerequisites are\nstandard undergraduate linear algebra and calculus, except for the section about charac-\nteristic functions, where Fourier transforms are needed. An undergraduate course in signals\nand systems would suffice, even taken concurrently while studying this book.\nThe length of the book is suitable for a two-semester course. Instructors are encouraged\nto use the set of chapters that best fits their classes. For example, a basic probability course\ncan use Chapters 1-5 as its backbone. Chapter 6 on sample statistics is suitable for students\nwho wish to gain theoretical insights into probabilistic convergence. Chapter 7 on regression\nand Chapter 8 on estimation best suit students who want to pursue machine learning and\nsignal processing. Chapter 9 discusses confidence intervals and hypothesis testing, which are\ncritical to modern data analysis. Chapter 10 introduces random processes. My approach for\nrandom processes is more tailored to information processing and communication systems,\nwhich are usually more relevant to electrical engineering students.\nAdditional teaching resources can be found on the book\u2019s website, where you can\nv", "7": "find lecture videos and homework videos. Throughout the book you will see many \u201cpractice\nexercises\u201d, which are easy problems with worked-out solutions. They can be skipped without\nloss to the flow of the book.\nAcknowledgements: If I could thank only one person, it must be Professor Fawwaz\nUlaby of the University of Michigan. Professor Ulaby has been the source of support in\nall aspects, from the book\u2019s layout to technical content, proofreading, and marketing. The\nbook would not have been published without the help of Professor Ulaby. I am deeply\nmoved by Professor Ulaby\u2019s vision that education should be made accessible to all students.\nWith textbook prices rocketing up, the EECS free textbook initiative launched by Professor\nUlaby is the most direct response to the publishers, teachers, parents, and students. Thank\nyou, Fawwaz, for your unbounded support \u2014 technically, mentally, and financially. Thank\nyou also for recommending Richard Carnes. The meticulous details Richard offered have\nsignificantly improved the fluency of the book. Thank you, Richard.\nI thank my colleagues at Purdue who had shared many thoughts with me when I\ntaught the course (in alphabetical order): Professors Mark Bell, Mary Comer, Saul Gelfand,\nAmy Reibman, and Chih-Chun Wang. My teaching assistant I-Fan Lin was instrumental in\nthe early development of this book. To the graduate students of my lab (Yiheng Chi, Nick\nChimitt, Kent Gauen, Abhiram Gnanasambandam, Guanzhe Hong, Chengxi Li, Zhiyuan\nMao, Xiangyu Qu, and Yash Sanghvi): Thank you! It would have been impossible to finish\nthe book without your participation. A few students I taught volunteered to help edit\nthe book: Benjamin Gottfried, Harrison Hsueh, Dawoon Jung, Antonio Kincaid, Deepak\nRavikumar, Krister Ulvog, Peace Umoru, Zhijing Yao. I would like to thank my Ph.D.\nadvisor Professor Truong Nguyen for encouraging me to write the book.\nFinally, I would like to thank my wife Vivian and my daughters, Joanna and Cynthia,\nfor their love, patience, and support.\nStanley H. Chan, West Lafayette, Indiana\nMay, 2021\nCompanion website:\nhttps://probability4datascience.com/\nvi", "8": "Contents\n1 Mathematical Background 1\n1.1 Infinite Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.1.1 Geometric Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.1.2 Binomial Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.2 Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.2.1 Taylor approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.2.2 Exponential series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.2.3 Logarithmic approximation . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.3 Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.3.1 Odd and even functions . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.3.2 Fundamental Theorem of Calculus . . . . . . . . . . . . . . . . . . . . 17\n1.4 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n1.4.1 Why do we need linear algebra in data science? . . . . . . . . . . . . . 20\n1.4.2 Everything you need to know about linear algebra . . . . . . . . . . . 21\n1.4.3 Inner products and norms . . . . . . . . . . . . . . . . . . . . . . . . . 24\n1.4.4 Matrix calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.5 Basic Combinatorics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n1.5.1 Birthday paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n1.5.2 Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n1.5.3 Combination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n1.7 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n1.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n2 Probability 43\n2.1 Set Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n2.1.1 Why study set theory? . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n2.1.2 Basic concepts of a set . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n2.1.3 Subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n2.1.4 Empty set and universal set . . . . . . . . . . . . . . . . . . . . . . . . 48\n2.1.5 Union . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n2.1.6 Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n2.1.7 Complement and difference . . . . . . . . . . . . . . . . . . . . . . . . 52\n2.1.8 Disjoint and partition . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n2.1.9 Set operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.1.10 Closing remarks about set theory . . . . . . . . . . . . . . . . . . . . . 57\nvii", "9": "CONTENTS\n2.2 Probability Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n2.2.1 Sample space \u2126 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n2.2.2 Event space F. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n2.2.3 Probability law P. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n2.2.4 Measure zero sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n2.2.5 Summary of the probability space . . . . . . . . . . . . . . . . . . . . 74\n2.3 Axioms of Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n2.3.1 Why these three probability axioms? . . . . . . . . . . . . . . . . . . . 75\n2.3.2 Axioms through the lens of measure . . . . . . . . . . . . . . . . . . . 76\n2.3.3 Corollaries derived from the axioms . . . . . . . . . . . . . . . . . . . 77\n2.4 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n2.4.1 Definition of conditional probability . . . . . . . . . . . . . . . . . . . 81\n2.4.2 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n2.4.3 Bayes\u2019 theorem and the law of total probability . . . . . . . . . . . . . 89\n2.4.4 The Three Prisoners problem . . . . . . . . . . . . . . . . . . . . . . . 92\n2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n2.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n2.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3 Discrete Random Variables 103\n3.1 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n3.1.1 A motivating example . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n3.1.2 Definition of a random variable . . . . . . . . . . . . . . . . . . . . . . 105\n3.1.3 Probability measure on random variables . . . . . . . . . . . . . . . . 107\n3.2 Probability Mass Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n3.2.1 Definition of probability mass function . . . . . . . . . . . . . . . . . . 110\n3.2.2 PMF and probability measure . . . . . . . . . . . . . . . . . . . . . . . 110\n3.2.3 Normalization property . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n3.2.4 PMF versus histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n3.2.5 Estimating histograms from real data . . . . . . . . . . . . . . . . . . 117\n3.3 Cumulative Distribution Functions (Discrete) . . . . . . . . . . . . . . . . . . 121\n3.3.1 Definition of the cumulative distribution function . . . . . . . . . . . . 121\n3.3.2 Properties of the CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n3.3.3 Converting between PMF and CDF . . . . . . . . . . . . . . . . . . . 124\n3.4 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n3.4.1 Definition of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n3.4.2 Existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.4.3 Properties of expectation . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.4.4 Moments and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n3.5 Common Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . 136\n3.5.1 Bernoulli random variable . . . . . . . . . . . . . . . . . . . . . . . . . 137\n3.5.2 Binomial random variable . . . . . . . . . . . . . . . . . . . . . . . . . 143\n3.5.3 Geometric random variable . . . . . . . . . . . . . . . . . . . . . . . . 149\n3.5.4 Poisson random variable . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n3.7 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n3.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\nviii", "10": "CONTENTS\n4 Continuous Random Variables 171\n4.1 Probability Density Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n4.1.1 Some intuitions about probability density functions . . . . . . . . . . . 172\n4.1.2 More in-depth discussion about PDFs . . . . . . . . . . . . . . . . . . 174\n4.1.3 Connecting with the PMF . . . . . . . . . . . . . . . . . . . . . . . . . 178\n4.2 Expectation, Moment, and Variance . . . . . . . . . . . . . . . . . . . . . . . 180\n4.2.1 Definition and properties . . . . . . . . . . . . . . . . . . . . . . . . . 180\n4.2.2 Existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n4.2.3 Moment and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n4.3 Cumulative Distribution Function . . . . . . . . . . . . . . . . . . . . . . . . 185\n4.3.1 CDF for continuous random variables . . . . . . . . . . . . . . . . . . 186\n4.3.2 Properties of CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n4.3.3 Retrieving PDF from CDF . . . . . . . . . . . . . . . . . . . . . . . . 193\n4.3.4 CDF: Unifying discrete and continuous random variables . . . . . . . 194\n4.4 Median, Mode, and Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n4.4.1 Median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n4.4.2 Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n4.4.3 Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n4.5 Uniform and Exponential Random Variables . . . . . . . . . . . . . . . . . . . 201\n4.5.1 Uniform random variables . . . . . . . . . . . . . . . . . . . . . . . . . 202\n4.5.2 Exponential random variables . . . . . . . . . . . . . . . . . . . . . . . 205\n4.5.3 Origin of exponential random variables . . . . . . . . . . . . . . . . . . 207\n4.5.4 Applications of exponential random variables . . . . . . . . . . . . . . 209\n4.6 Gaussian Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n4.6.1 Definition of a Gaussian random variable . . . . . . . . . . . . . . . . 211\n4.6.2 Standard Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n4.6.3 Skewness and kurtosis . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n4.6.4 Origin of Gaussian random variables . . . . . . . . . . . . . . . . . . 220\n4.7 Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n4.7.1 General principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n4.7.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n4.8 Generating Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n4.8.1 General principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n4.8.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\n4.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n4.10 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236\n4.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n5 Joint Distributions 241\n5.1 Joint PMF and Joint PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n5.1.1 Probability measure in 2D . . . . . . . . . . . . . . . . . . . . . . . . . 244\n5.1.2 Discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . 245\n5.1.3 Continuous random variables . . . . . . . . . . . . . . . . . . . . . . . 247\n5.1.4 Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\n5.1.5 Marginal PMF and marginal PDF . . . . . . . . . . . . . . . . . . . . 250\n5.1.6 Independent random variables . . . . . . . . . . . . . . . . . . . . . . 251\n5.1.7 Joint CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\n5.2 Joint Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\nix", "11": "CONTENTS\n5.2.1 Definition and interpretation . . . . . . . . . . . . . . . . . . . . . . . 257\n5.2.2 Covariance and correlation coefficient . . . . . . . . . . . . . . . . . . 261\n5.2.3 Independence and correlation . . . . . . . . . . . . . . . . . . . . . . . 263\n5.2.4 Computing correlation from data . . . . . . . . . . . . . . . . . . . . . 265\n5.3 Conditional PMF and PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n5.3.1 Conditional PMF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\n5.3.2 Conditional PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n5.4 Conditional Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n5.4.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n5.4.2 The law of total expectation . . . . . . . . . . . . . . . . . . . . . . . 276\n5.5 Sum of Two Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 280\n5.5.1 Intuition through convolution . . . . . . . . . . . . . . . . . . . . . . . 280\n5.5.2 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n5.5.3 Sum of common distributions . . . . . . . . . . . . . . . . . . . . . . . 282\n5.6 Random Vectors and Covariance Matrices . . . . . . . . . . . . . . . . . . . . 286\n5.6.1 PDF of random vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 286\n5.6.2 Expectation of random vectors . . . . . . . . . . . . . . . . . . . . . . 288\n5.6.3 Covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n5.6.4 Multidimensional Gaussian . . . . . . . . . . . . . . . . . . . . . . . . 290\n5.7 Transformation of Multidimensional Gaussians . . . . . . . . . . . . . . . . . 293\n5.7.1 Linear transformation of mean and covariance . . . . . . . . . . . . . . 293\n5.7.2 Eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . 295\n5.7.3 Covariance matrices are always positive semi-definite . . . . . . . . . . 297\n5.7.4 Gaussian whitening . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\n5.8 Principal-Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 303\n5.8.1 The main idea: Eigendecomposition . . . . . . . . . . . . . . . . . . . 303\n5.8.2 The eigenface problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\n5.8.3 What cannot be analyzed by PCA? . . . . . . . . . . . . . . . . . . . 311\n5.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\n5.10 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\n5.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\n6 Sample Statistics 319\n6.1 Moment-Generating and Characteristic Functions . . . . . . . . . . . . . . . . 324\n6.1.1 Moment-generating function . . . . . . . . . . . . . . . . . . . . . . . . 324\n6.1.2 Sum of independent variables via MGF . . . . . . . . . . . . . . . . . 327\n6.1.3 Characteristic functions . . . . . . . . . . . . . . . . . . . . . . . . . . 329\n6.2 Probability Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\n6.2.1 Union bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333\n6.2.2 The Cauchy-Schwarz inequality . . . . . . . . . . . . . . . . . . . . . . 335\n6.2.3 Jensen\u2019s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n6.2.4 Markov\u2019s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n6.2.5 Chebyshev\u2019s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n6.2.6 Chernoff\u2019s bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\n6.2.7 Comparing Chernoff and Chebyshev . . . . . . . . . . . . . . . . . . . 344\n6.2.8 Hoeffding\u2019s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n6.3 Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n6.3.1 Sample average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nx", "12": "CONTENTS\n6.3.2 Weak law of large numbers (WLLN) . . . . . . . . . . . . . . . . . . . 354\n6.3.3 Convergence in probability . . . . . . . . . . . . . . . . . . . . . . . . 356\n6.3.4 Can we prove WLLN using Chernoff\u2019s bound? . . . . . . . . . . . . . 358\n6.3.5 Does the weak law of large numbers always hold? . . . . . . . . . . . . 359\n6.3.6 Strong law of large numbers . . . . . . . . . . . . . . . . . . . . . . . . 360\n6.3.7 Almost sure convergence . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n6.3.8 Proof of the strong law of large numbers . . . . . . . . . . . . . . . . . 364\n6.4 Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366\n6.4.1 Convergence in distribution . . . . . . . . . . . . . . . . . . . . . . . . 367\n6.4.2 Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n6.4.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n6.4.4 Limitation of the Central Limit Theorem . . . . . . . . . . . . . . . . 378\n6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\n6.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\n6.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n7 Regression 389\n7.1 Principles of Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394\n7.1.1 Intuition: How to fit a straight line? . . . . . . . . . . . . . . . . . . . 395\n7.1.2 Solving the linear regression problem . . . . . . . . . . . . . . . . . . . 397\n7.1.3 Extension: Beyond a straight line . . . . . . . . . . . . . . . . . . . . . 401\n7.1.4 Overdetermined and underdetermined systems . . . . . . . . . . . . . 409\n7.1.5 Robust linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . 412\n7.2 Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418\n7.2.1 Overview of overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\n7.2.2 Analysis of the linear case . . . . . . . . . . . . . . . . . . . . . . . . . 420\n7.2.3 Interpreting the linear analysis results . . . . . . . . . . . . . . . . . . 425\n7.3 Bias and Variance Trade-Off . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n7.3.1 Decomposing the testing error . . . . . . . . . . . . . . . . . . . . . . 430\n7.3.2 Analysis of the bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n7.3.3 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436\n7.3.4 Bias and variance on the learning curve . . . . . . . . . . . . . . . . . 438\n7.4 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440\n7.4.1 Ridge regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440\n7.4.2 LASSO regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 449\n7.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n7.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\n7.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459\n8 Estimation 465\n8.1 Maximum-Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 468\n8.1.1 Likelihood function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\n8.1.2 Maximum-likelihood estimate . . . . . . . . . . . . . . . . . . . . . . . 472\n8.1.3 Application 1: Social network analysis . . . . . . . . . . . . . . . . . . 478\n8.1.4 Application 2: Reconstructing images . . . . . . . . . . . . . . . . . . 481\n8.1.5 More examples of ML estimation . . . . . . . . . . . . . . . . . . . . . 484\n8.1.6 Regression versus ML estimation . . . . . . . . . . . . . . . . . . . . . 487\n8.2 Properties of ML Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\nxi", "13": "CONTENTS\n8.2.1 Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\n8.2.2 Unbiased estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\n8.2.3 Consistent estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 494\n8.2.4 Invariance principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500\n8.3 Maximum A Posteriori Estimation . . . . . . . . . . . . . . . . . . . . . . . . 502\n8.3.1 The trio of likelihood, prior, and posterior . . . . . . . . . . . . . . . . 503\n8.3.2 Understanding the priors . . . . . . . . . . . . . . . . . . . . . . . . . 504\n8.3.3 MAP formulation and solution . . . . . . . . . . . . . . . . . . . . . . 506\n8.3.4 Analyzing the MAP solution . . . . . . . . . . . . . . . . . . . . . . . 508\n8.3.5 Analysis of the posterior distribution . . . . . . . . . . . . . . . . . . . 511\n8.3.6 Conjugate prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513\n8.3.7 Linking MAP with regression . . . . . . . . . . . . . . . . . . . . . . . 517\n8.4 Minimum Mean-Square Estimation . . . . . . . . . . . . . . . . . . . . . . . . 520\n8.4.1 Positioning the minimum mean-square estimation . . . . . . . . . . . 520\n8.4.2 Mean squared error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\n8.4.3 MMSE estimate = conditional expectation . . . . . . . . . . . . . . . 523\n8.4.4 MMSE estimator for multidimensional Gaussian . . . . . . . . . . . . 529\n8.4.5 Linking MMSE and neural networks . . . . . . . . . . . . . . . . . . . 533\n8.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534\n8.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535\n8.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536\n9 Confidence and Hypothesis 541\n9.1 Confidence Interval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543\n9.1.1 The randomness of an estimator . . . . . . . . . . . . . . . . . . . . . 543\n9.1.2 Understanding confidence intervals . . . . . . . . . . . . . . . . . . . . 545\n9.1.3 Constructing a confidence interval . . . . . . . . . . . . . . . . . . . . 548\n9.1.4 Properties of the confidence interval . . . . . . . . . . . . . . . . . . . 551\n9.1.5 Student\u2019s t-distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 554\n9.1.6 Comparing Student\u2019s t-distribution and Gaussian . . . . . . . . . . . . 558\n9.2 Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559\n9.2.1 A brute force approach . . . . . . . . . . . . . . . . . . . . . . . . . . 560\n9.2.2 Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562\n9.3 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566\n9.3.1 What is a hypothesis? . . . . . . . . . . . . . . . . . . . . . . . . . . . 566\n9.3.2 Critical-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567\n9.3.3 p-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571\n9.3.4 Z-test and T-test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574\n9.4 Neyman-Pearson Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577\n9.4.1 Null and alternative distributions . . . . . . . . . . . . . . . . . . . . . 577\n9.4.2 Type 1 and type 2 errors . . . . . . . . . . . . . . . . . . . . . . . . . 579\n9.4.3 Neyman-Pearson decision . . . . . . . . . . . . . . . . . . . . . . . . . 582\n9.5 ROC and Precision-Recall Curve . . . . . . . . . . . . . . . . . . . . . . . . . 589\n9.5.1 Receiver Operating Characteristic (ROC) . . . . . . . . . . . . . . . . 589\n9.5.2 Comparing ROC curves . . . . . . . . . . . . . . . . . . . . . . . . . . 592\n9.5.3 The ROC curve in practice . . . . . . . . . . . . . . . . . . . . . . . . 598\n9.5.4 The Precision-Recall (PR) curve . . . . . . . . . . . . . . . . . . . . . 601\n9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605\nxii", "14": "CONTENTS\n9.7 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606\n9.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607\n10 Random Processes 611\n10.1 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n10.1.1 Everything you need to know about a random process . . . . . . . . . 612\n10.1.2 Statistical and temporal perspectives . . . . . . . . . . . . . . . . . . . 614\n10.2 Mean and Correlation Functions . . . . . . . . . . . . . . . . . . . . . . . . . 618\n10.2.1 Mean function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618\n10.2.2 Autocorrelation function . . . . . . . . . . . . . . . . . . . . . . . . . . 622\n10.2.3 Independent processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 629\n10.3 Wide-Sense Stationary Processes . . . . . . . . . . . . . . . . . . . . . . . . . 630\n10.3.1 Definition of a WSS process . . . . . . . . . . . . . . . . . . . . . . . . 631\n10.3.2 Properties of RX(\u03c4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632\n10.3.3 Physical interpretation of RX(\u03c4) . . . . . . . . . . . . . . . . . . . . . 633\n10.4 Power Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636\n10.4.1 Basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636\n10.4.2 Origin of the power spectral density . . . . . . . . . . . . . . . . . . . 640\n10.5 WSS Process through LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . 643\n10.5.1 Review of linear time-invariant systems . . . . . . . . . . . . . . . . . 643\n10.5.2 Mean and autocorrelation through LTI Systems . . . . . . . . . . . . . 644\n10.5.3 Power spectral density through LTI systems . . . . . . . . . . . . . . . 646\n10.5.4 Cross-correlation through LTI Systems . . . . . . . . . . . . . . . . . . 649\n10.6 Optimal Linear Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653\n10.6.1 Discrete-time random processes . . . . . . . . . . . . . . . . . . . . . . 653\n10.6.2 Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654\n10.6.3 Yule-Walker equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 656\n10.6.4 Linear prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 658\n10.6.5 Wiener filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 662\n10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669\n10.8 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 670\n10.8.1 The Mean-Square Ergodic Theorem . . . . . . . . . . . . . . . . . . . 674\n10.9 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675\n10.10Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 676\nA Appendix 681\nxiii", "15": "CONTENTS\nxiv", "16": "Chapter 1\nMathematical Background\n\u201cData science\u201d has different meanings to different people. If you ask a biologist, data science\ncould mean analyzing DNA sequences. If you ask a banker, data science could mean pre-\ndicting the stock market. If you ask a software engineer, data science could mean programs\nand data structures; if you ask a machine learning scientist, data science could mean models\nand algorithms. However, one thing that is common in all these disciplines is the concept of\nuncertainty . We choose to learn from data because we believe that the latent information\nis embedded in the data \u2014 unprocessed, contains noise, and could have missing entries. If\nthere is no randomness, all data scientists can close their business because there is simply\nno problem to solve. However, the moment we see randomness, our business comes back.\nTherefore, data science is the subject of making decisions in uncertainty.\nThe mathematics of analyzing uncertainty is probability . It is thetool to help us model,\nanalyze, and predict random events. Probability can be studied in as many ways as you can\nthink of. You can take a rigorous course in probability theory, or a \u201cprobability for dummies\u201d\non the internet, or a typical undergraduate probability course offered by your school. This\nbook is different from all these. Our goal is to tell you how things work in the context of data\nscience. For example, why do we need those three axioms of probabilities and not others?\nWhere does the \u201cbell shape\u201d Gaussian random variable come from? How many samples do\nwe need to construct a reliable histogram? These questions are at the core of data science,\nand they deserve close attention rather than sweeping them under the rug.\nTo help you get used to the pace and style of this book, in this chapter, we review some\nof the very familiar topics in undergraduate algebra and calculus. These topics are meant\nto warm up your mathematics background so that you can follow the subsequent chapters.\nSpecifically, in this chapter, we cover several topics. First, in Section 1.1 we discuss infinite\nseries, something that will be used frequently when we evaluate the expectation and variance\nof random variables in Chapter 3. In Section 1.2 we review the Taylor approximation,\nwhich will be helpful when we discuss continuous random variables. Section 1.3 discusses\nintegration and reviews several tricks we can use to make integration easy. Section 1.4\ndeals with linear algebra, aka matrices and vectors, which are fundamental to modern data\nanalysis. Finally, Section 1.5 discusses permutation and combination, two basic techniques\nto count events.\n1", "17": "CHAPTER 1. MATHEMATICAL BACKGROUND\n1.1 Infinite Series\nImagine that you have a fair coin . If you get a tail, you flip it again. You do this repeatedly\nuntil you finally get a head. What is the probability that you need to flip the coin three\ntimes to get one head?\nThis is a warm-up exercise. Since the coin is fair, the probability of obtaining a head\nis1\n2. The probability of getting a tail followed by a head is1\n2\u00d71\n2=1\n4. Similarly, the\nprobability of getting two tails and then a head is1\n2\u00d71\n2\u00d71\n2=1\n8. If you follow this logic, you\ncan write down the probabilities for all other cases. For your convenience, we have drawn the\nfirst few in Figure 1.1 . As you have probably noticed, the probabilities follow the pattern\n{1\n2,1\n4,1\n8, . . .}.\nFigure 1.1: Suppose you flip a coin until you see a head. This requires you to have N\u22121tails followed\nby a head. The probability of this sequence of events are1\n2,1\n4,1\n8, . . . , which forms an infinite sequence.\nWe can also summarize these probabilities using a familiar plot called the histogram\nas shown in Figure 1.2 . The histogram for this problem has a special pattern, that every\nvalue is one order higher than the preceding one, and the sequence is infinitely long.\n1 2 3 4 5 6 7 8 9 1000.10.20.30.40.5\nFigure 1.2: The histogram of flipping a coin until we see a head. The x-axis is the number of coin flips,\nand the y-axis is the probability.\nLet us ask something harder: On average, if you want to be 90% sure that you will\nget a head, what is the minimum number of attempts you need to try? Five attempts?\nTen attempts? Indeed, if you try ten attempts, you will very likely accomplish your goal.\nHowever, this would seem to be overkill. If you try five attempts, then it becomes unclear\nwhether you will be 90% sure.\n2", "18": "1.1. INFINITE SERIES\nThis problem can be answered by analyzing the sequence of probabilities. If we make\ntwo attempts, then the probability of getting a head is the sum of the probabilities for one\nattempt and that of two attempts:\nP[success after 1 attempt] =1\n2= 0.5\nP[success after 2 attempts] =1\n2+1\n4= 0.75\nTherefore, if you make 3 attempts or 4 attempts, you get the following probabilities:\nP[success after 3 attempts] =1\n2+1\n4+1\n8= 0.875\nP[success after 4 attempts] =1\n2+1\n4+1\n8+1\n16= 0.9375.\nSo if we try four attempts, we will have a 93.75% probability of getting a head. Thus, four\nattempts is the answer.\nThe MATLAB / Python codes we used to generate Figure 1.2 are shown below.\n% MATLAB code to generate a geometric sequence\np = 1/2;\nn = 1:10;\nX = p.^n;\nbar(n,X,\u2019FaceColor\u2019,[0.8, 0.2,0.2]);\n# Python code to generate a geometric sequence\nimport numpy as np\nimport matplotlib.pyplot as plt\np = 1/2\nn = np.arange(0,10)\nX = np.power(p,n)\nplt.bar(n,X)\nThis warm-up exercise has perhaps raised some of your interest in the subject. However,\nwe will not tell you everything now. We will come back to the probability in Chapter 3\nwhen we discuss geometric random variables. In the present section, we want to make sure\nyou have the basic mathematical tools to calculate quantities, such as a sum of fractional\nnumbers. For example, what if we want to calculate P[success after 107 attempts]? Is there\na systematic way of performing the calculation?\nRemark . You should be aware that the 93.75% only says that the probability of achieving\nthe goal is high. If you have a bad day, you may still need more than four attempts. Therefore,\nwhen we stated the question, we asked for 90% \u201con average\u201d. Sometimes you may need\nmore attempts and sometimes fewer attempts, but on average, you have a 93.75% chance\nof succeeding.\n1.1.1 Geometric Series\nA geometric series is the sum of a finite or an infinite sequence of numbers with a constant\nratio between successive terms. As we have seen in the previous example, a geometric series\n3", "19": "CHAPTER 1. MATHEMATICAL BACKGROUND\nappears naturally in the context of discrete events. In Chapter 3 of this book, we will use\ngeometric series when calculating the expectation andmoments of a random variable.\nDefinition 1.1. Let0< r < 1, afinite geometric sequence of power nis a sequence\nof numbers\u001a\n1, r, r2, . . . , rn\u001b\n.\nAninfinite geometric sequence is a sequence of numbers\n\u001a\n1, r, r2, r3, . . .\u001b\n.\nTheorem 1.1. The sum of a finite geometric series of power nis\nnX\nk=0rk= 1 + r+r2+\u00b7\u00b7\u00b7+rn=1\u2212rn+1\n1\u2212r. (1.1)\nProof . We multiply both sides by 1 \u2212r. The left hand side becomes\n nX\nk=0rk!\n(1\u2212r) =\u0000\n1 +r+r2+\u00b7\u00b7\u00b7+rn\u0001\n(1\u2212r)\n=\u0000\n1 +r+r2+\u00b7\u00b7\u00b7+rn\u0001\n\u2212\u0000\nr+r2+r3+\u00b7\u00b7\u00b7+rn+1\u0001\n(a)= 1\u2212rn+1,\nwhere ( a) holds because terms are canceled due to subtractions.\n\u25a1\nA corollary of Equation (1.1) is the sum of an infinite geometric sequence.\nCorollary 1.1. Let0< r < 1. The sum of an infinite geometric series is\n\u221eX\nk=0rk= 1 + r+r2+\u00b7\u00b7\u00b7=1\n1\u2212r. (1.2)\nProof . We take the limit in Equation (1.1). This yields\n\u221eX\nk=0rk= lim\nn\u2192\u221enX\nk=0rk= lim\nn\u2192\u221e1\u2212rn+1\n1\u2212r=1\n1\u2212r.\n\u25a1\nRemark . Note that the condition 0 < r < 1 is important. If r > 1, then the limit\nlimn\u2192\u221ern+1in Equation (1.2) will diverge. The constant rcannot equal to 1, for oth-\nerwise the fraction (1 \u2212rn+1)/(1\u2212r) is undefined. We are not interested in the case when\nr= 0, because the sum is trivially 1:P\u221e\nk=00k= 1 + 01+ 02+\u00b7\u00b7\u00b7= 1.\n4", "20": "1.1. INFINITE SERIES\nPractice Exercise 1.1 . Compute the infinite series\u221eP\nk=21\n2k.\nSolution .\n\u221eX\nk=21\n2k=1\n4+1\n8+\u00b7\u00b7\u00b7+\n=1\n4\u0012\n1 +1\n2+1\n4+\u00b7\u00b7\u00b7\u0013\n=1\n4\u00b71\n1\u22121\n2=1\n2.\nRemark . You should not be confused about a geometric series and a harmonic series . A\nharmonic series concerns with the sum of {1,1\n2,1\n3,1\n4, . . .}. It turns out that1\n\u221eX\nn=11\nn= 1 +1\n2+1\n3+1\n4+\u00b7\u00b7\u00b7=\u221e.\nOn the other hand, a squared harmonic series {1,1\n22,1\n32,1\n42, . . .}converges:\n\u221eX\nn=11\nn2= 1 +1\n22+1\n32+1\n42+\u00b7\u00b7\u00b7=\u03c02\n6.\nThe latter result is known as the Basel problem .\nWe can extend the main theorem by considering more complicated series, for example\nthe following one.\nCorollary 1.2. Let0< r < 1. It holds that\n\u221eX\nk=1krk\u22121= 1 + 2 r+ 3r2+\u00b7\u00b7\u00b7=1\n(1\u2212r)2. (1.3)\nProof . Take the derivative on both sides of Equation (1.2). The left hand side becomes\nd\ndr\u221eX\nk=0rk=d\ndr\u0000\n1 +r+r2+\u00b7\u00b7\u00b7\u0001\n= 1 + 2 r+ 3r2+\u00b7\u00b7\u00b7=\u221eX\nk=1krk\u22121\nThe right hand side becomesd\ndr\u00121\n1\u2212r\u0013\n=1\n(1\u2212r)2.\n\u25a1\nPractice Exercise 1.2 . Compute the infinite sumP\u221e\nk=1k\u00b71\n3k.\n1This result can be found in Tom Apostol, Mathematical Analysis , 2nd Edition, Theorem 8.11.\n5", "21": "CHAPTER 1. MATHEMATICAL BACKGROUND\nSolution . We can use the derivative result:\n\u221eX\nk=1k\u00b71\n3k= 1\u00b71\n3+ 2\u00b71\n9+ 3\u00b71\n27+\u00b7\u00b7\u00b7\n=1\n3\u00b7\u0012\n1 + 2\u00b71\n3+ 3\u00b71\n9+\u00b7\u00b7\u00b7\u0013\n=1\n3\u00b71\n(1\u22121\n3)2=1\n3\u00b71\n4\n9=3\n4.\n1.1.2 Binomial Series\nA geometric series is useful when handling situations such as N\u22121 failures followed by\na success. However, we can easily twist the problem by asking: What is the probability\nof getting one head out of 3 independent coin tosses? In this case, the probability can be\ndetermined by enumerating all possible cases:\nP[1 head in 3 coins] = P[H,T,T] + P[T,H,T] + P[T,T,H]\n=\u00121\n2\u00d71\n2\u00d71\n2\u0013\n+\u00121\n2\u00d71\n2\u00d71\n2\u0013\n+\u00121\n2\u00d71\n2\u00d71\n2\u0013\n=3\n8.\nFigure 1.3 illustrates the situation.\nFigure 1.3: When flipping three coins independently, the probability of getting exactly one head can\ncome from three different possibilities.\nWhat lessons have we learned in this example? Notice that you need to enumerate\nall possible combinations of one head and two tails to solve this problem. The number is\n3 in our example. In general, the number of combinations can be systematically studied\nusing combinatorics , which we will discuss later in the chapter. However, the number of\ncombinations motivates us to discuss another background technique known as the binomial\nseries. The binomial series is instrumental in algebra when handling polynomials such as\n(a+b)2or (1 + x)3. It provides a valuable formula when computing these powers.\nTheorem 1.2 (Binomial theorem ).For any real numbers aandb, the binomial series\nof power nis\n(a+b)n=nX\nk=0\u0012n\nk\u0013\nan\u2212kbk, (1.4)\nwhere\u0000n\nk\u0001\n=n!\nk!(n\u2212k)!.\nThebinomial theorem is valid for any real numbers aandb. The quantity\u0000n\nk\u0001\nreads\nas \u201cnchoose k\u201d. Its definition is\n\u0012n\nk\u0013\ndef=n!\nk!(n\u2212k)!,\n6", "22": "1.1. INFINITE SERIES\nwhere n! =n(n\u22121)(n\u22122)\u00b7\u00b7\u00b73\u00b72\u00b71. We shall discuss the physical meaning of\u0000n\nk\u0001\nin\nSection 1.5. But we can quickly plug in the \u201c nchoose k\u201d into the coin flipping example by\nletting n= 3 and k= 1:\nNumber of combinations for 1 head and 2 tails =\u00123\n1\u0013\n=3!\n1!2!= 3.\nSo you can see why we want you to spend your precious time learning about the binomial\ntheorem. In MATLAB and Python,\u0000n\nk\u0001\ncan be computed using the commands as follows.\n% MATLAB code to compute (N choose K) and K!\nn = 10;\nk = 2;\nnchoosek(n,k)\nfactorial(k)\n# Python code to compute (N choose K) and K!\nfrom scipy.special import comb, factorial\nn = 10\nk = 2\ncomb(n, k)\nfactorial(k)\nThe binomial theorem makes the most sense when we also learn about the Pascal\u2019s\nidentity .\nTheorem 1.3 (Pascal\u2019s identity ).Letnandkbe positive integers such that k\u2264n.\nThen,\u0012n\nk\u0013\n+\u0012n\nk\u22121\u0013\n=\u0012n+ 1\nk\u0013\n. (1.5)\nProof . We start by recalling the definition of\u0000n\nk\u0001\n. This gives us\n\u0012n\nk\u0013\n+\u0012n\nk\u22121\u0013\n=n!\nk!(n\u2212k)!+n!\n(k\u22121)!(n\u2212(k\u22121))!\n=n!\u00121\nk!(n\u2212k)!+1\n(k\u22121)!(n\u2212k+ 1)!\u0013\n,\nwhere we factor out n! to obtain the second equation. Next, we observe that\n1\nk!(n\u2212k)!\u00d7(n\u2212k+ 1)\n(n\u2212k+ 1)=n\u2212k+ 1\nk!(n\u2212k+ 1)!,\n1\n(k\u22121)!(n\u2212k+ 1)!\u00d7k\nk=k\nk!(n\u2212k+ 1)!.\n7", "23": "CHAPTER 1. MATHEMATICAL BACKGROUND\nSubstituting into the previous equation we obtain\n\u0012n\nk\u0013\n+\u0012n\nk\u22121\u0013\n=n!\u0012n\u2212k+ 1\nk!(n\u2212k+ 1)!+k\nk!(n\u2212k+ 1)!\u0013\n=n!\u0012n+ 1\nk!(n\u2212k+ 1)!\u0013\n=(n+ 1)!\nk!(n+ 1\u2212k)!\n=\u0012n+ 1\nk\u0013\n.\n\u25a1\nThe Pascal triangle is a visualization of the coefficients of ( a+b)nas shown in Fig-\nure 1.4 . For example, when n= 5, we know that\u00005\n3\u0001\n= 10. However, by Pascal\u2019s identity, we\nknow that\u00005\n3\u0001\n=\u00004\n2\u0001\n+\u00004\n3\u0001\n. So the number 10 is actually obtained by summing the numbers\n4 and 6 of the previous row.\nFigure 1.4: Pascal triangle for n= 0, . . . , 5. Note that a number in one row is obtained by summing\ntwo numbers directly above it.\nPractice Exercise 1.3 . Find (1 + x)3.\nSolution . Using the binomial theorem, we can show that\n(1 +x)3=nX\nk=0\u00123\nk\u0013\n13\u2212kxk\n= 1 + 3 x+ 3x2+x3.\nPractice Exercise 1.4 . Let 0 < p < 1. Find\nnX\nk=0\u0012n\nk\u0013\npn\u2212k(1\u2212p)k.\n8", "24": "1.1. INFINITE SERIES\nSolution . By using the binomial theorem, we have\nnX\nk=0\u0012n\nk\u0013\npn\u2212k(1\u2212p)k= (p+ (1\u2212p))n= 1.\nThis result will be helpful when evaluating binomial random variables in Chapter 3.\nWe now prove the binomial theorem. Please feel free to skip the proof if this is your first\ntime reading the book.\nProof of the binomial theorem . We prove by induction. When n= 1,\n(a+b)1=a+b\n=1X\nk=0a1\u2212kbk.\nTherefore, the base case is verified. Assume up to case n. We need to verify case n+ 1.\n(a+b)n+1= (a+b)(a+b)n\n= (a+b)nX\nk=0\u0012n\nk\u0013\nan\u2212kbk\n=nX\nk=0\u0012n\nk\u0013\nan\u2212k+1bk+nX\nk=0\u0012n\nk\u0013\nan\u2212kbk+1.\nWe want to apply the Pascal\u2019s identity to combine the two terms. In order to do so, we note\nthat the second term in this sum can be rewritten as\nnX\nk=0\u0012n\nk\u0013\nan\u2212kbk+1=nX\nk=0\u0012n\nk\u0013\nan+1\u2212k\u22121bk+1\n=n+1X\n\u2113=1\u0012n\n\u2113\u22121\u0013\nan+1\u2212\u2113b\u2113, where \u2113=k+ 1\n=nX\n\u2113=1\u0012n\n\u2113\u22121\u0013\nan+1\u2212\u2113b\u2113+bn+1.\nThe first term in the sum can be written as\nnX\nk=0\u0012n\nk\u0013\nan\u2212k+1bk=nX\n\u2113=1\u0012n\n\u2113\u0013\nan+1\u2212\u2113b\u2113+an+1, where \u2113=k.\nTherefore, the two terms can be combined using Pascal\u2019s identity to yield\n(a+b)n+1=nX\n\u2113=1\u0014\u0012n\n\u2113\u0013\n+\u0012n\n\u2113\u22121\u0013\u0015\nan+1\u2212\u2113b\u2113+an+1+bn+1\n=nX\n\u2113=1\u0012n+ 1\n\u2113\u0013\nan+1\u2212\u2113b\u2113+an+1+bn+1=n+1X\n\u2113=0\u0012n+ 1\n\u2113\u0013\nan+1\u2212\u2113b\u2113.\n9", "25": "CHAPTER 1. MATHEMATICAL BACKGROUND\nHence, the ( n+ 1)th case is also verified. By the principle of mathematical induction, we\nhave completed the proof.\n\u25a1\nThe end of the proof. Please join us again.\n1.2 Approximation\nConsider a function f(x) = log(1 + x), for x >0 as shown in Figure 1.5 . This is a nonlinear\nfunction, and we all know that nonlinear functions are not fun to deal with. For example,\nif you want to integrate the functionRb\naxlog(1 + x)dx, then the logarithm will force you\nto do integration by parts. However, in many practical problems, you may not need the full\nrange of x >0. Suppose that you are only interested in values x\u226a1. Then the logarithm\ncan be approximated, and thus the integral can also be approximated.\n0 1 2 3 4 500.511.52\n0 0.05 0.1 0.15 0.200.050.10.150.2\nFigure 1.5: The function f(x) = log(1 + x)and the approximation bf(x) =x.\nTo see how this is even possible, we show in Figure 1.5 the nonlinear function f(x) =\nlog(1 + x) and an approximation bf(x) =x. The approximation is carefully chosen such that\nforx\u226a1, the approximation bf(x) is close to the true function f(x). Therefore, we can\nargue that for x\u226a1,\nlog(1 + x)\u2248x, (1.6)\nthereby simplifying the calculation. For example, if you want to integrate xlog(1 + x) for\n0< x < 0.1, then the integral can be approximated byR0.1\n0xlog(1 + x)dx\u2248R0.1\n0x2dx=\nx3\n3= 3.33\u00d710\u22124. (The actual integral is 3 .21\u00d710\u22124.) In this section we will learn about\nthe basic approximation techniques. We will use them when we discuss limit theorems in\nChapter 6, as well as various distributions, such as from binomial to Poisson.\n10", "26": "1.2. APPROXIMATION\n1.2.1 Taylor approximation\nGiven a function f:R\u2192R, it is often useful to analyze its behavior by approximating f\nusing its local information. Taylor approximation (or Taylor series) is one of the tools for\nsuch a task. We will use the Taylor approximation on many occasions.\nDefinition 1.2 (Taylor Approximation ).Letf:R\u2192Rbe a continuous function with\ninfinite derivatives. Let a\u2208Rbe a fixed constant. The Taylor approximation of fat\nx=ais\nf(x) =f(a) +f\u2032(a)(x\u2212a) +f\u2032\u2032(a)\n2!(x\u2212a)2+\u00b7\u00b7\u00b7\n=\u221eX\nn=0f(n)(a)\nn!(x\u2212a)n, (1.7)\nwhere f(n)denotes the nth-order derivative of f.\nTaylor approximation is a geometry-based approximation. It approximates the function\naccording to the offset, slope, curvature, and so on. According to Definition 1.2, the Taylor\nseries has an infinite number of terms. If we use a finite number of terms, we obtain the\nnth-order Taylor approximation:\nFirst-Order : f(x) =f(a)|{z}\noffset+f\u2032(a)(x\u2212a)|{z}\nslope+O((x\u2212a)2)\nSecond-Order : f(x) =f(a)|{z}\noffset+f\u2032(a)(x\u2212a)|{z}\nslope+f\u2032\u2032(a)\n2!(x\u2212a)2\n|{z }\ncurvature+O((x\u2212a)3).\nHere, the big-O notation O(\u03b5k) means any term that has an order at least power k. For\nsmall \u03b5, i.e., \u03b5\u226a1, a high-order term O(\u03b5k)\u22480 for large k.\nExample 1.1 . Let f(x) = sin x. Then the Taylor approximation at x= 0 is\nf(x)\u2248f(0) + f\u2032(0)(x\u22120) +f\u2032\u2032(0)\n2!(x\u22120)2+f\u2032\u2032\u2032(0)\n3!(x\u22120)3\n= sin(0) + (cos 0)( x\u22120)\u2212sin(0)\n2!(x\u22120)2\u2212cos(0)\n3!(x\u22120)3\n= 0 + x\u22120\u2212x3\n6=x\u2212x3\n6.\nWe can expand further to higher orders, which yields\nf(x) =x\u2212x3\n3!+x5\n5!\u2212x7\n7!+\u00b7\u00b7\u00b7\nWe show the first few approximations in Figure 1.6 .\nOne should be reminded that Taylor approximation approximates a function f(x)\nat a particular point x=a. Therefore, the approximation of fnear x= 0 and the\n11", "27": "CHAPTER 1. MATHEMATICAL BACKGROUND\napproximation of fnearx=\u03c0/2 are different. For example, the Taylor approximation\natx=\u03c0/2 for f(x) = sin xis\nf(x) = sin\u03c0\n2+ cos\u03c0\n2\u0010\nx\u2212\u03c0\n2\u0011\n\u2212sin\u03c0\n2\n2!\u0010\nx\u2212\u03c0\n2\u00112\n\u2212cos\u03c0\n2\n3!\u0010\nx\u2212\u03c0\n2\u00113\n= 1 + 0 \u22121\n4\u0010\nx\u2212\u03c0\n2\u00112\n\u22120 = 1\u22121\n4\u0010\nx\u2212\u03c0\n2\u00112\n.\n-10 -5 0 5 10\nx-4-2024\nsin x\n3rd order\n5th order\n7th order\n-10 -5 0 5 10\nx-4-2024\nsin x\n3rd order\n5th order\n7th order\n(a) Approximate at x= 0 (b) Approximate at x=\u03c0/2\nFigure 1.6: Taylor approximation of the function f(x) = sin x.\n1.2.2 Exponential series\nAn immediate application of the Taylor approximation is to derive the exponential series .\nTheorem 1.4. Letxbe any real number. Then,\nex= 1 + x+x2\n2+x3\n3!+\u00b7\u00b7\u00b7=\u221eX\nk=0xk\nk!. (1.8)\nProof . Let f(x) =exfor any x. Then, the Taylor approximation around x= 0 is\nf(x) =f(0) + f\u2032(0)(x\u22120) +f\u2032\u2032(0)\n2!(x\u22120)2+\u00b7\u00b7\u00b7\n=e0+e0(x\u22120) +e0\n2!(x\u22120)2+\u00b7\u00b7\u00b7\n= 1 + x+x2\n2+\u00b7\u00b7\u00b7=\u221eX\nk=0xk\nk!.\n\u25a1\nPractice Exercise 1.5 . Evaluate\u221eX\nk=0\u03bbke\u2212\u03bb\nk!.\n12", "28": "1.2. APPROXIMATION\nSolution .\u221eX\nk=0\u03bbke\u2212\u03bb\nk!=e\u2212\u03bb\u221eX\nk=0\u03bbk\nk!=e\u2212\u03bbe\u03bb= 1.\nThis result will be useful for Poisson random variables in Chapter 3.\nIf we substitute x=j\u03b8where j=\u221a\u22121, then we can show that\nej\u03b8\n|{z}\n=cos \u03b8+jsin\u03b8= 1 + j\u03b8+(j\u03b8)2\n2!+\u00b7\u00b7\u00b7\n=\u0012\n1\u2212\u03b82\n2!+\u03b84\n4!+\u00b7\u00b7\u00b7\u0013\n| {z }\nreal+j\u0012\n\u03b8\u2212\u03b83\n3!+\u00b7\u00b7\u00b7\u0013\n| {z }\nimaginary\nMatching the real and the imaginary terms, we can show that\ncos\u03b8= 1\u2212\u03b82\n2!+\u03b84\n4!+\u00b7\u00b7\u00b7\nsin\u03b8=\u03b8\u2212\u03b83\n3!+\u03b85\n5!+\u00b7\u00b7\u00b7\nThis gives the infinite series representations of the two trigonometric functions.\n1.2.3 Logarithmic approximation\nTaylor approximation also allows us to find approximations to logarithmic functions. We\nstart by presenting a lemma.\nLemma 1.1. Let0< x < 1be a constant. Then,\nlog(1 + x) =x\u2212x2+O(x3). (1.9)\nProof . Let f(x) = log(1 + x). Then, the derivatives of fare\nf\u2032(x) =1\n(1 +x),and f\u2032\u2032(x) =\u22121\n(1 +x)2.\nTaylor approximation at x= 0 gives\nf(x) =f(0) + f\u2032(0)(x\u22120) +f\u2032\u2032(0)\n2(x\u22120)2+O(x3)\n= log 1 +\u00121\n(1 + 0)\u0013\nx\u2212\u00121\n(1 + 0)2\u0013\nx2+O(x3)\n=x\u2212x2+O(x3).\n\u25a1\nThe difference between this result and the result we showed in the beginning of this\nsection is the order of polynomials we used to approximate the logarithm:\n13", "29": "CHAPTER 1. MATHEMATICAL BACKGROUND\n\u0088First-order: log(1 + x) =x\n\u0088Second-order: log(1 + x) =x\u2212x2.\nWhat order of approximation is good? It depends on where you want the approximation to\nbe good, and how faryou want the approximation to go. The difference between first-order\nand second-order approximations is shown in Figure 1.7 .\n0 1 2 3 4 500.511.52\n0 1 2 3 4 500.511.52\nFirst-order approximation Second-order approximation\nFigure 1.7: The function f(x) = log(1 + x), the first-order approximation bf(x) =x, and the second-\norder approximation bf(x) =x\u2212x2.\nExample 1.2 . When we prove the Central Limit Theorem in Chapter 6, we need to\nuse the following result.\nlim\nN\u2192\u221e\u0012\n1 +s2\n2N\u0013N\n=es2/2.\nThe proof of this equation can be done using the Taylor approximation. Consider\nNlog\u0010\n1 +s2\nN\u0011\n. By the logarithmic lemma, we can obtain the second-order approxi-\nmation:\nlog\u0012\n1 +s2\n2N\u0013\n=s2\n2N\u2212s4\n4N2.\nTherefore, multiplying both sides by Nyields\nNlog\u0012\n1 +s2\n2N\u0013\n=s2\n2\u2212s4\n4N.\nPutting the limit N\u2192 \u221e we can show that\nlim\nN\u2192\u221e\u001a\nNlog\u0012\n1 +s2\n2N\u0013\u001b\n=s2\n2.\nTaking exponential on both sides yields\nexp\u001a\nlim\nN\u2192\u221eNlog\u0012\n1 +s2\n2N\u0013\u001b\n= exp\u001as2\n2\u001b\n.\nMoving the limit outside the exponential yields the result. Figure 1.8 provides a pic-\ntorial illustration.\n14", "30": "1.3. INTEGRATION\n0 0.2 0.4 0.6 0.8 111.21.41.61.8\nFigure 1.8: We plot a sequence of function fN(x) =\u0010\n1 +s2\n2N\u0011N\nand its limit f(x) =es2/2.\n1.3 Integration\nWhen you learned calculus, your teacher probably told you that there are two ways to\ncompute an integral:\n\u0088Substitution : Z\nf(ax)dx=1\naZ\nf(u)du.\n\u0088By parts : Z\nu dv =u v\u2212Z\nv du.\nBesides these two, we want to teach you two more. The first technique is even and odd\nfunctions when integrating a function symmetrically about the y-axis. If a function is even,\nyou just need to integrate half of the function. If a function is odd, you will get a zero. The\nsecond technique is to leverage the fact that a probability density function integrates to 1.\nWe will discuss the first technique here and defer the second technique to Chapter 4.\nBesides the two integration techniques, we will review the fundamental theorem of\ncalculus. We will need it when we study cumulative distribution functions in Chapter 4.\n1.3.1 Odd and even functions\nDefinition 1.3. A function f:R\u2192Riseven if for any x\u2208R,\nf(x) =f(\u2212x), (1.10)\nandfisoddif\nf(x) =\u2212f(\u2212x). (1.11)\n15", "31": "CHAPTER 1. MATHEMATICAL BACKGROUND\nEssentially, an even function flips over about the y-axis, whereas an odd function flips over\nboth the x- and y-axes.\nExample 1.3 . The function f(x) =x2\u22120.4x4is even, because\nf(\u2212x) = (\u2212x)2\u22120.4(\u2212x)4=x2\u22120.4x4=f(x).\nSeeFigure 1.9 (a) for illustration. When integrating the function, we have\nZ1\n\u22121f(x)dx= 2Z1\n0f(x)dx= 2Z1\n0x2\u22120.44dx= 2\u0014x3\n3\u22120.4\n5x5\u0015x=1\nx=0=38\n75.\nExample 1.4 . The function f(x) =xexp(\u2212x2/2) is odd, because\nf(\u2212x) = (\u2212x) exp\u001a\n\u2212(\u2212x)2\n2\u001b\n=\u2212xexp\u001a\n\u2212x2\n2\u001b\n=\u2212f(x).\nSeeFigure 1.9 (b) for illustration. When integrating the function, we can let u=\u2212x.\nThen, the integral becomes\nZ1\n\u22121f(x)dx=Z0\n\u22121f(x)dx+Z1\n0f(x)dx\n=Z1\n0f(\u2212u)du+Z1\n0f(x)dx\n=\u2212Z1\n0f(u)du+Z1\n0f(x)dx= 0.\n-1.5 -1 -0.5 0 0.5 1 1.5\nx-1-0.500.51\n-1.5 -1 -0.5 0 0.5 1 1.5\nx-1-0.500.51\n(a) Even function (b) Odd function\nFigure 1.9: An even function is symmetric about the y-axis, and so the integrationRa\n\u2212af(x)dx=\n2Ra\n0f(x)dx. An odd function is anti-symmetric about the y-axis. Thus,Ra\n\u2212af(x)dx= 0.\n16", "32": "1.3. INTEGRATION\n1.3.2 Fundamental Theorem of Calculus\nOur following result is the Fundamental Theorem of Calculus . It is a handy tool that links\nintegration and differentiation.\nTheorem 1.5 (Fundamental Theorem of Calculus ).Letf: [a, b]\u2192Rbe a continu-\nous function defined on a closed interval [a, b]. Then, for any x\u2208(a, b),\nf(x) =d\ndxZx\naf(t)dt, (1.12)\nBefore we prove the result, let us understand the theorem if you have forgotten its meaning.\nExample 1.5 . Consider a function f(t) =t2. If we integrate the function from 0 to\nx, we will obtain another function\nF(x)def=Zx\n0f(t)dt=Zx\n0t2dt=x3\n3.\nOn the other hand, we can differentiate F(x) to obtain f(x):\nf(x) =d\ndxF(x) =d\ndxx3\n3=x2.\nThe fundamental theorem of calculus basically puts the two together:\nf(x) =d\ndxZx\n0f(t)dt.\nThat\u2019s it. Nothing more and nothing less.\nHow can the fundamental theorem of calculus ever be useful when studying probabil-\nity? Very soon you will learn two concepts: probability density function andcumulative\ndistribution function . These two functions are related to each other by the fundamental\ntheorem of calculus. To give you a concrete example, we write down the probability density\nfunction of an exponential random variable. (Please do not panic about the exponential\nrandom variable. Just think of it as a \u201crapidly decaying\u201d function.)\nf(x) =e\u2212x, x\u22650.\nIt turns out that the cumulative distribution function is\nF(x) =Zx\n0f(t)dt=Zx\n0e\u2212tdt= 1\u2212e\u2212x.\nYou can also check that f(x) =d\ndxF(x). The fundamental theorem of calculus says that if\nyou tell me F(x) =Rx\n0e\u2212tdt(for whatever reason), I will be able to tell you that f(x) =e\u2212x\nmerely by visually inspecting the integrand without doing the differentiation.\nFigure 1.10 illustrates the pair of functions f(x) =e\u2212xandF(x) = 1\u2212e\u2212x. One thing\nyou should notice is that the height ofF(x) is the area under the curve of f(t) from \u2212\u221etox.\nFor example, in Figure 1.10 we show the area under the curve from 0 to 2. Correspondingly\ninF(x), the height is F(2).\n17", "33": "CHAPTER 1. MATHEMATICAL BACKGROUND\n0 1 2 3 4 500.20.40.60.81\n0 1 2 3 4 500.20.40.60.81\nf(x) F(x)\nFigure 1.10: The pair of functions f(x) =e\u2212xandF(x) = 1\u2212e\u2212x\nThe following proof of the Fundamental Theorem of Calculus can be skipped if it is your\nfirst time reading the book.\nProof . Our proof is based on Stewart (6th Edition), Section 5.3. Define the integral as a\nfunction F:\nF(x) =Zx\naf(t)dt.\nThe derivative of Fwith respect to xis\nd\ndxF(x) = lim\nh\u21920F(x+h)\u2212F(x)\nh\n= lim\nh\u219201\nh Zx+h\naf(t)dt\u2212Zx\naf(t)dt!\n= lim\nh\u219201\nhZx+h\nxf(t)dt\n(a)\n\u2264lim\nh\u219201\nhZx+h\nx\u001a\nmax\nx\u2264\u03c4\u2264x+hf(\u03c4)\u001b\ndt\n= lim\nh\u21920\u001a\nmax\nx\u2264\u03c4\u2264x+hf(\u03c4)\u001b\n.\nHere, the inequality in ( a) holds because\nf(t)\u2264max\nx\u2264\u03c4\u2264x+hf(\u03c4)\nfor all x\u2264t\u2264x+h. The maximum exists because fis continuous in a closed interval.\n18", "34": "1.3. INTEGRATION\nUsing the parallel argument, we can show that\nd\ndxF(x) = lim\nh\u21920F(x+h)\u2212F(x)\nh\n= lim\nh\u219201\nh Zx+h\naf(t)dt\u2212Zx\naf(t)dt!\n= lim\nh\u219201\nhZx+h\nxf(t)dt\n\u2265lim\nh\u219201\nhZx+h\nx\u001a\nmin\nx\u2264\u03c4\u2264x+hf(\u03c4)\u001b\ndt\n= lim\nh\u21920\u001a\nmin\nx\u2264\u03c4\u2264x+hf(\u03c4)\u001b\n.\nCombining the two results, we have that\nlim\nh\u21920\u001a\nmin\nx\u2264\u03c4\u2264x+hf(\u03c4)\u001b\n\u2264d\ndxF(x)\u2264lim\nh\u21920\u001a\nmax\nx\u2264\u03c4\u2264x+hf(\u03c4)\u001b\n.\nHowever, since the two limits are both converging to f(x) as h\u21920, we conclude that\nd\ndxF(x) =f(x).\n\u25a1\nRemark . An alternative proof is to use Mean Value Theorem in terms of Riemann-Stieltjes\nintegrals (see, e.g., Tom Apostol, Mathematical Analysis , 2nd edition, Theorem 7.34). To\nhandle more general functions such as delta functions, one can use techniques in Lebesgue\u2019s\nintegration. However, this is beyond the scope of this book.\nThis is the end of the proof. Please join us again.\nIn many practical problems, the fundamental theorem of calculus needs to be used in\nconjunction with the chain rule .\nCorollary 1.3. Letf: [a, b]\u2192Rbe a continuous function defined on a closed interval\n[a, b]. Let g:R\u2192[a, b]be a continuously differentiable function. Then, for any x\u2208\n(a, b),\nd\ndxZg(x)\naf(t)dt=g\u2032(x)\u00b7f(g(x)). (1.13)\nProof . We can prove this with the chain rule: Let y=g(x). Then we have\nd\ndxZg(x)\naf(t)dt=dy\ndx\u00b7d\ndyZy\naf(t)dt=g\u2032(x)f(y),\nwhich completes the proof.\n\u25a1\n19", "35": "CHAPTER 1. MATHEMATICAL BACKGROUND\nPractice Exercise 1.6 . Evaluate the integral\nd\ndxZx\u2212\u00b5\n01\u221a\n2\u03c0\u03c32exp\u001a\n\u2212t2\n2\u03c32\u001b\ndt.\nSolution . Let y=x\u2212\u00b5. Then by using the fundamental theorem of calculus, we can\nshow that\nd\ndxZx\u2212\u00b5\n01\u221a\n2\u03c0\u03c32exp\u001a\n\u2212t2\n2\u03c32\u001b\ndt=dy\ndx\u00b7d\ndyZy\n01\u221a\n2\u03c0\u03c32exp\u001a\n\u2212t2\n2\u03c32\u001b\ndt\n=d(x\u2212\u00b5)\ndx\u00b71\u221a\n2\u03c0\u03c32exp\u001a\n\u2212y2\n2\u03c32\u001b\n=1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5)2\n2\u03c32\u001b\n.\nThis result will be useful when we do linear transformations of a Gaussian random\nvariable in Chapter 4.\n1.4 Linear Algebra\nThe two most important subjects for data science are probability , which is the subject of the\nbook you are reading, and linear algebra , which concerns matrices and vectors. We cannot\ncover linear algebra in detail because this would require another book. However, we need to\nhighlight some ideas that are important for doing data analysis.\n1.4.1 Why do we need linear algebra in data science?\nConsider a dataset of the crime rate of several cities as shown below, downloaded from\nhttps://web.stanford.edu/ ~hastie/StatLearnSparsity/data.html .\nThe table shows that the crime rate depends on several factors such as funding for the\npolice department, the percentage of high school graduates, etc.\ncity crime rate funding hs no-hs college college4\n1 478 40 74 11 31 20\n2 494 32 72 11 43 18\n3 643 57 71 18 16 16\n4 341 31 71 11 25 19\n.....................\n50 940 66 67 26 18 16\n20", "36": "1.4. LINEAR ALGEBRA\nWhat questions can we ask about this table? We can ask: What is the most influential\ncause of the crime rate? What are the leading contributions to the crime rate? To answer\nthese questions, we need to describe these numbers. One way to do it is to put the numbers\nin matrices and vectors. For example,\nycrime =\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0478\n494\n...\n940\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,xfund=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f040\n32\n...\n66\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,xhs=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f074\n72\n...\n67\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb, . . .\nWith this vector expression of the data, the analysis questions can roughly be translated\nto finding \u03b2\u2019s in the following equation:\nycrime =\u03b2fundxfund+\u03b2hsxhs+\u00b7\u00b7\u00b7+\u03b2college4 xcollege4 .\nThis equation offers a lot of useful insights. First, it is a linear model ofycrime. We call\nit a linear model because the observable ycrime is written as a linear combination of the\nvariables xfund,xhs, etc. The linear model assumes that the variables are scaled and added\nto generate the observed phenomena. This assumption is not always realistic, but it is often\na fair assumption that greatly simplifies the problem. For example, if we can show that all\n\u03b2\u2019s are zero except \u03b2fund, then we can conclude that the crime rate is solely dependent on\nthe police funding. If two variables are correlated, e.g., high school graduate and college\ngraduate, we would expect the \u03b2\u2019s to change simultaneously.\nThe linear model can further be simplified to a matrix-vector equation:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0|\n|\nycrime\n|\n|\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0| | |\n| | |\nxfund xhs\u00b7\u00b7\u00b7xcollege4\n| | |\n| | |\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b2fund\n\u03b2hs\n...\n\u03b2college4\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\nHere, the lines \u201c |\u201d emphasize that the vectors are column vectors. If we denote the matrix\nin the middle as Aand the vector as \u03b2, then the equation is equivalent to y=A\u03b2. So we\ncan find \u03b2by appropriately inverting the matrix A. If two columns of Aare dependent, we\nwill not be able to resolve the corresponding \u03b2\u2019s uniquely.\nAs you can see from the above data analysis problem, matrices and vectors offer a way\nto describe the data. We will discuss the calculations in Chapter 7. However, to understand\nhow to interpret the results from the matrix-vector equations, we need to review some basic\nideas about matrices and vectors.\n1.4.2 Everything you need to know about linear algebra\nThroughout this book, you will see different sets of notations. For linear algebra, we also\nhave a set of notations. We denote x\u2208Rdad-dimensional vector taking real numbers as its\nentries. An M-by-Nmatrix is denoted as X\u2208RM\u00d7N. The transpose of a matrix is denoted\nasXT. A matrix Xcan be viewed according to its columns and its rows:\nX=\uf8ee\n\uf8f0| | |\nx1x2\u00b7\u00b7\u00b7xN\n| | |\uf8f9\n\uf8fb,and X=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u2014x1\u2014\n\u2014x2\u2014\n...\n\u2014xM\u2014\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\n21", "37": "CHAPTER 1. MATHEMATICAL BACKGROUND\nHere, xjdenotes the jth column of X, andxidenotes the ith row of X. The ( i, j)th element\nofXis denoted as xijor [X]ij. The identity matrix is denoted as I. The ith column of I\nis denoted as ei= [0, . . . , 1, . . . , 0]T, and is called the ithstandard basis vector . An all-zero\nvector is denoted as 0= [0, . . . , 0]T.\nWhat is the most important thing to know about linear algebra? From a data analysis\npoint of view, Figure 1.11 gives us the answer. The picture is straightforward, but it captures\nall the essence. In almost all the data analysis problems, ultimately, there are three things we\ncare about: (i) The observable vector y, (ii) the variable vectors xn, and (iii) the coefficients\n\u03b2n. The set of variable vectors {xn}N\nn=1spans a vector space in which all vectors are living.\nSome of these variable vectors are correlated, and some are not. However, for the sake of\nthis discussion, let us assume they are independent of each other. Then for any observable\nvector y, we can always project yin the directions determined by {xn}N\nn=1. The projection\nofyontoxnis the coefficient \u03b2n. A larger value of \u03b2nmeans that the variable xnhas more\ncontributions.\nFigure 1.11: Representing an observable vector yby a linear combination of variable vectors x1,x2\nandx3. The combination weights are \u03b21, \u03b22, \u03b23.\nWhy is this picture so important? Because most of the data analysis problems can be\nexpressed, or approximately expressed, by the picture:\ny=NX\nn=1\u03b2nxn.\nIf you recall the crime rate example, this equation is precisely the linear model we used to\ndescribe the crime rate. This equation can also describe many other problems.\nExample 1.6 .Polynomial fitting . Consider a dataset of pairs of numbers ( tm, ym) for\nm= 1, . . . , M , as shown in Figure 1.12 . After a visual inspection of the dataset, we\npropose to use a line to fit the data. A line is specified by the equation\nym=atm+b, m = 1, . . . , M,\nwhere a\u2208Ris the slope and b\u2208Ris the y-intercept. The goal of this problem is to\nfind one line (which is fully characterized by ( a, b)) such that it has the best fit to all\nthe data pairs ( tm, ym) for m= 1, . . . , M . This problem can be described in matrices\n22", "38": "1.4. LINEAR ALGEBRA\nand vectors by noting that\n\uf8ee\n\uf8ef\uf8f0y1\n...\nyM\uf8f9\n\uf8fa\uf8fb\n|{z}\ny=a|{z}\n\u03b21\uf8ee\n\uf8ef\uf8f0t1\n...\ntM\uf8f9\n\uf8fa\uf8fb\n|{z}\nx1+b|{z}\n\u03b22\uf8ee\n\uf8ef\uf8f01\n...\n1\uf8f9\n\uf8fa\uf8fb\n|{z}\nx2,\nor more compactly,\ny=\u03b21x1+\u03b22x2.\nHere, x1= [t1, . . . , t M]Tcontains all the variable values, and x2= [1, . . . , 1]Tcontains\na constant offset.\ntm ym\n0.1622 2.1227\n0.7943 3.3354\n......\n0.7379 3.4054\n0.2691 2.5672\n0.4228 2.3796\n0.6020 3.2942\n0 0.2 0.4 0.6 0.8 112345\ndata\nbest fit\ncandidate\nFigure 1.12: Example of fitting a set of data points. The problem can be described by y=\n\u03b21x1+\u03b22x2.\nExample 1.7 .Image compression . The JPEG compression for images is based on\nthe concept of discrete cosine transform (DCT). The DCT consists of a set of basis\nvectors , or{xn}N\nn=1using our notation. In the most standard setting, each basis vector\nxnconsists of 8 \u00d78 pixels, and there are N= 64 of these xn\u2019s. Given an image, we can\npartition the image into Msmall blocks of 8 \u00d78 pixels. Let us call one of these blocks\ny. Then, DCT represents the observation yas a linear combination of the DCT basis\nvectors:\ny=NX\nn=1\u03b2nxn.\nThe coefficients {\u03b2n}N\nn=1are called the DCT coefficients. They provide a representa-\ntionofy, because once we know {\u03b2n}N\nn=1, we can completely describe ybecause the\nbasis vectors {xn}N\nn=1are known and fixed. The situation is depicted in Figure 1.13 .\nHow can we compress images using DCT? In the 1970s, scientists found that most\nimages have strong leading DCT coefficients but weak tail DCT coefficients. In other\nwords, among the N= 64 \u03b2n\u2019s, only the first few are important. If we truncate the\nnumber of DCT coefficients, we can effectively compress the number of bits required\nto represent the image.\n23", "39": "CHAPTER 1. MATHEMATICAL BACKGROUND\nFigure 1.13: JPEG image compression is based on the concept of discrete cosine transform, which\ncan be formulated as a matrix-vector problem.\nWe hope by now you are convinced of the importance of matrices and vectors in the\ncontext of data science. They are not \u201cyet another\u201d subject but an essential tool you must\nknow how to use. So, what are the technical materials you must master? Here we go.\n1.4.3 Inner products and norms\nWe assume that you know the basic operations such as matrix-vector multiplication, taking\nthe transpose, etc. If you have forgotten these, please consult any undergraduate linear\nalgebra textbook such as Gilbert Strang\u2019s Linear Algebra and its Applications . We will\nhighlight a few of the most important operations for our purposes.\nDefinition 1.4 (Inner product ).Letx= [x1, . . . , x N]T, and y= [y1, . . . , y N]T. The\ninner product xTyis\nxTy=NX\ni=1xiyi. (1.14)\nPractice Exercise 1.7 . Letx= [1,0,\u22121]T, and y= [3,2,0]T. Find xTy.\nSolution . The inner product is xTy= (1)(3) + (0)(2) + ( \u22121)(0) = 3.\nInner products are important because they tell us how two vectors are correlated.\nFigure 1.14 depicts the geometric meaning of an inner product. If two vectors are correlated\n(i.e., nearly parallel), then the inner product will give us a large value. Conversely, if the\ntwo vectors are close to perpendicular, then the inner product will be small. Therefore, the\ninner product provides a measure of the closeness/similarity between two vectors.\nFigure 1.14: Geometric interpretation of inner product: We project one vector onto the other vector.\nThe projected distance is the inner product.\n24", "40": "1.4. LINEAR ALGEBRA\nCreating vectors and computing the inner products are straightforward in MATLAB.\nWe simply need to define the column vectors xandyby using the command []with ;to\ndenote the next row. The inner product is done using the transpose operation x\u2019and vector\nmultiplication *.\n% MATLAB code to perform an inner product\nx = [1 0 -1];\ny = [3 2 0];\nz = x\u2019*y;\nIn Python, constructing a vector is done using the command np.array . Inside this\ncommand, one needs to enter the array. For a column vector, we write [[1],[2],[3]] , with\nan outer [], and three inner []for each entry. If the vector is a row vector, the one can omit\nthe inner []\u2019s by just calling np.array([1, 2, 3]) . Given two column vectors xand y,\nthe inner product is computed via np.dot(x.T,y) , where np.dot is the command for inner\nproduct, and x.Treturns the transpose of x. One can also call np.transpose(x) , which is\nthe same as x.T.\n# Python code to perform an inner product\nimport numpy as np\nx = np.array([[1],[0],[-1]])\ny = np.array([[3],[2],[0]])\nz = np.dot(np.transpose(x),y)\nprint(z)\nIn data analytics, the inner product of two vectors can be useful. Consider the vectors\ninTable 1.1 . Just from looking at the numbers, you probably will not see anything wrong.\nHowever, let\u2019s compute the inner products. It turns out that xT\n1x2=\u22120.0031, whereas\nxT\n1x3= 2.0020. There is almost no correlation between x1andx2, but there is a substan-\ntial correlation between x1andx3. What happened? The vectors x1andx2are random\nvectors constructed independently and uncorrelated to each other. The last vector x3was\nconstructed by x3= 2x1\u2212\u03c0/1000. Since x3is completely constructed from x1, they have\nto be correlated.\nx1 x2 x3\n0.0006 \u22120.0011 \u22120.0020\n\u22120.0014 \u22120.0024 \u22120.0059\n\u22120.0034 0 .0073 \u22120.0099\n.........\n0.0001 \u22120.0066 \u22120.0030\n0.0074 0 .0046 0 .0116\n0.0007 \u22120.0061 \u22120.0017\nTable 1.1: Three example vectors.\nOne caveat for this example is that the naive inner product xT\nixjis scale-dependent.\nFor example, the vectors x3=x1andx3= 1000 x1have the same amount of correlation,\n25", "41": "CHAPTER 1. MATHEMATICAL BACKGROUND\nbut the simple inner product will give a larger value for the latter case. To solve this problem\nwe first define the norm of the vectors:\nDefinition 1.5 (Norm ).Letx= [x1, . . . , x N]Tbe a vector. The \u2113p-norm of xis\n\u2225x\u2225p= NX\ni=1xp\ni!1/p\n, (1.15)\nfor any p\u22651.\nThe norm essentially tells us the length of the vector. This is most obvious if we consider\nthe\u21132-norm:\n\u2225x\u22252= NX\ni=1x2\ni!1/2\n.\nBy taking the square on both sides, one can show that \u2225x\u22252\n2=xTx. This is called the\nsquared \u21132-norm , and is the sum of the squares.\nOn MATLAB, computing the norm is done using the command norm. Here, we can\nindicate the types of norms, e.g., norm(x,1) returns the \u21131-norm whereas norm(x,2) returns\nthe\u21132-norm (which is also the default).\n% MATLAB code to compute the norm\nx = [1 0 -1];\nx_norm = norm(x);\nOn Python, the norm command is listed in the np.linalg . To call the \u21131-norm, we use\nnp.linalg.norm(x,1) , and by default the \u21132-norm is np.linalg.norm(x) .\n# Python code to compute the norm\nimport numpy as np\nx = np.array([[1],[0],[-1]])\nx_norm = np.linalg.norm(x)\nUsing the norm, one can define an angle called the cosine angle between two vectors.\nDefinition 1.6. Thecosine angle between two vectors xandyis\ncos\u03b8=xTy\n\u2225x\u22252\u2225y\u22252. (1.16)\nThe difference between the cosine angle and the basic inner product is the normaliza-\ntion in the denominator, which is the product \u2225x\u22252\u2225y\u22252. This normalization factor scales\nthe vector xtox/\u2225x\u22252andytoy/\u2225y\u22252. The scaling makes the length of the new vector\nequal to unity, but it does not change the vector\u2019s orientation. Therefore, the cosine angle\nis not affected by a very long vector or a very short vector. Only the angle matters. See\nFigure 1.15 .\n26", "42": "1.4. LINEAR ALGEBRA\nFigure 1.15: The cosine angle is the inner product divided by the norms of the vectors.\nGoing back to the previous example, after normalization we can show that the cosine\nangle between x1andx2is cos \u03b81,2=\u22120.0031, whereas the cosine angle between x1and\nx3is cos \u03b81,3= 0.8958. There is still a strong correlation between x1andx3, but now using\nthe cosine angle the value is between \u22121 and +1.\nRemark 1 : There are other norms one can use. The \u21131-norm is useful for sparse models\nwhere we want to have the fewest possible non-zeros. The \u21131-norm of xis\n\u2225x\u22251=NX\ni=1|xi|,\nwhich is the sum of absolute values. The \u2113\u221e-norm picks the maximum of {x1, . . . , x N}:\n\u2225x\u2225\u221e= lim\np\u2192\u221e NX\ni=1xp\ni!1/p\n= max {x1, . . . , x N},\nbecause as p\u2192 \u221e , only the largest element will be amplified.\nRemark 2 : The standard \u21132-norm is a circle: Just consider x= [x1, x2]T. The norm\nis\u2225x\u22252=p\nx2\n1+x2\n2. We can convert the circle to ellipses by considering a weighted norm.\nDefinition 1.7 (Weighted \u21132-norm square ).Letx= [x1, . . . , x N]Tand let W=\ndiag(w1, . . . , w N)be a non-negative diagonal matrix. The weighted \u21132-norm square of\nxis\n\u2225x\u22252\nW=xTWx\n=\u0002x1. . . x N\u0003\uf8ee\n\uf8ef\uf8f0w1. . . 0\n.........\n0. . . w N\uf8f9\n\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8f0x1\n...\nxN\uf8f9\n\uf8fa\uf8fb=NX\ni=1wix2\ni. (1.17)\nThe geometry of the weighted \u21132-norm is determined by the matrix W. For example,\nifW=I(the identity operator), then \u2225x\u22252\nW=\u2225x\u22252\n2, which defines a circle. If Wis any\n\u201cnon-negative\u201d matrix2, then \u2225x\u22252\nWdefines an ellipse.\n2The technical term for these matrices is positive semi-definite matrices.\n27", "43": "CHAPTER 1. MATHEMATICAL BACKGROUND\nIn MATLAB, the weighted inner product is just a sequence of two matrix-vector mul-\ntiplications. This can be done using the command x\u2019*W*x as shown below.\n% MATLAB code to compute the weighted norm\nW = [1 2 3; 4 5 6; 7 8 9];\nx = [2; -1; 1];\nz = x\u2019*W*x\nIn Python, constructing the matrix Wand the column vector xis done using np.array .\nThe matrix-vector multiplication is done using two np.dot commands: one for np.dot(W,x)\nand the other one for np.dot(x.T, np.dot(W,x)) .\n# Python code to compute the weighted norm\nimport numpy as np\nW = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nx = np.array([[2],[-1],[1]])\nz = np.dot(x.T, np.dot(W,x))\nprint(z)\n1.4.4 Matrix calculus\nThe last linear algebra topic we need to review is matrix calculus. As its name indicates,\nmatrix calculus is about the differentiation of matrices and vectors. Why do we need differ-\nentiation for matrices and vectors? Because we want to find the minimum or maximum of\na scalar function with a vector input.\nLet us go back to the crime rate problem we discussed earlier. Given the data, we\nwant to find the model coefficients \u03b21, . . . , \u03b2 Nsuch that the variables can best explain the\nobservation. In other words, we want to minimize the deviation between yand the prediction\noffered by our model:\nminimize\n\u03b21,...,\u03b2 N\r\r\r\r\ry\u2212NX\nn=1\u03b2nxn\r\r\r\r\r2\n.\nThis equation is self-explanatory. The norm \u2225\u2663 \u2212 \u2661\u22252measures the deviation. If ycan\nbe perfectly explained by {xn}N\nn=1, then the norm can eventually go to zero by finding a\ngood set of {\u03b21, . . . , \u03b2 N}. The symbol minimize\n\u03b21,...,\u03b2 Nmeans to minimize the function by finding\n{\u03b21, . . . , \u03b2 N}. Note that the norm is taking a vector as the input and generating a scalar as\nthe output. It can be expressed as\n\u03b5(\u03b2)def=\r\r\r\r\ry\u2212NX\nn=1\u03b2nxn\r\r\r\r\r2\n,\nto emphasize this relationship. Here we define \u03b2= [\u03b21, . . . , \u03b2 N]Tas the collection of all\ncoefficients.\nGiven this setup, how would you determine \u03b2such that the deviation is minimized?\nOur calculus teachers told us that we could take the function\u2019s derivative and set it to zero\n28", "44": "1.4. LINEAR ALGEBRA\nfor scalar problems. It is the same story for vectors. What we do is to take the derivative of\nthe error and set it equal to zero:\nd\nd\u03b2\u03b5(\u03b2) = 0 .\nNow the question arises, how do we take the derivatives of \u03b5(\u03b2) when it takes a vector as\ninput? If we can answer this question, we will find the best \u03b2. The answer is straightforward.\nSince the function has one output and many inputs, take the derivative for each element\nindependently. This is called the scalar differentiation of vectors .\nDefinition 1.8 (Scalar differentiation of vectors ).Letf:RN\u2192Rbe a differentiable\nscalar function, and let y=f(x)for some input x\u2208RN. Then,\ndy\ndx=\uf8ee\n\uf8ef\uf8f0dy/dx 1\n...\ndy/dx N\uf8f9\n\uf8fa\uf8fb.\nAs you can see from this definition, there is nothing conceptually challenging here. The only\ndifficulty is that things can get tedious because there will be many terms. However, the good\nnews is that mathematicians have already compiled a list of identities for common matrix\ndifferentiation. So instead of deriving every equation from scratch, we can enjoy the fruit of\ntheir hard work by referring to those formulae. The best place to find these equations is the\nMatrix Cookbook by Petersen and Pedersen.3Here, we will mention two of the most useful\nresults.\nExample 1.8 . Let y=xTAxfor any matrix A\u2208RN\u00d7N. Finddy\ndx.\nSolution .d\ndx\u0000\nxTAx\u0001\n=Ax+ATx.\nNow, if Ais symmetric, i.e., A=AT, then\nd\ndx\u0000\nxTAx\u0001\n= 2Ax.\nExample 1.9 . Let \u03b5=\u2225Ax\u2212y\u22252\n2, where A\u2208RN\u00d7Nis symmetric. Findd\u03b5\ndx.\nSolution . First, we note that\n\u03b5=\u2225Ax\u2212y\u22252\n2=xTATAx\u22122yTAx+yTy.\n3https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf\n29", "45": "CHAPTER 1. MATHEMATICAL BACKGROUND\nTaking the derivative with respect to xyields\nd\u03b5\ndx= 2ATAx\u22122ATy\n= 2AT(Ax\u2212y).\nGoing back to the crime rate problem, we can now show that\n0 =d\u03b5\nd\u03b2\u2225y\u2212X\u03b2\u22252= 2XT(X\u03b2\u2212y).\nTherefore, the solution is\nb\u03b2= (XTX)\u22121Xy.\nAs you can see, if we do not have access to the matrix calculus, we will not be able to solve the\nminimization problem. (There are alternative paths that do not require matrix calculus, but\nthey require an understanding of linear subspaces and properties of the projection operators.\nSo in some sense, matrix calculus is the easiest way to solve the problem.) When we discuss\nthe linear regression methods in Chapter 7, we will cover the interpretation of the inverses\nand related topics.\nIn MATLAB and Python, matrix inversion is done using the command invin MAT-\nLAB and np.linalg.inv in Python. Below is an example in Python.\n# Python code to compute a matrix inverse\nimport numpy as np\nX = np.array([[1, 3], [-2, 7], [0, 1]])\nXtX = np.dot(X.T, X)\nXtXinv = np.linalg.inv(XtX)\nprint(XtXinv)\nSometimes, instead of computing the matrix inverse we are more interested in solving a\nlinear equation X\u03b2=y(the solution of which is b\u03b2= (XTX)\u22121Xy). In both MATLAB and\nPython, there are built-in commands to do this. In MATLAB, the command is \\(backslash).\n% MATLAB code to solve X beta = y\nX = [1 3; -2 7; 0 1];\ny = [2; 1; 0];\nbeta = X\\y;\nIn Python, the built-in command is np.linalg.lstsq .\n# Python code to solve X beta = y\nimport numpy as np\nX = np.array([[1, 3], [-2, 7], [0, 1]])\ny = np.array([[2],[1],[0]])\nbeta = np.linalg.lstsq(X, y, rcond=None)[0]\nprint(beta)\n30", "46": "1.5. BASIC COMBINATORICS\nClosing remark : In this section, we have given a brief introduction to a few of the most\nrelevant concepts in linear algebra. We will introduce further concepts in linear algebra in\nlater chapters, such as eigenvalues, principal component analysis, linear transformations,\nand regularization, as they become useful for our discussion.\n1.5 Basic Combinatorics\nThe last topic we review in this chapter is combinatorics . Combinatorics concerns the\nnumber of configurations that can be obtained from certain discrete experiments. It is useful\nbecause it provides a systematic way of enumerating cases. Combinatorics often becomes\nvery challenging as the complexity of the event grows. However, you may rest assured that\nin this book, we will not tackle the more difficult problems of combinatorics; we will confine\nour discussion to two of the most basic principles: permutation andcombination .\n1.5.1 Birthday paradox\nTo motivate the discussion of combinatorics, let us start with the following problem. Suppose\nthere are 50 people in a room. What is the probability that at least one pair of people have\nthe same birthday (month and day)? (We exclude Feb. 29 in this problem.)\nThe first thing you might be thinking is that since there are 365 days, we need at least\n366 people to ensure that one pair has the same birthday. Therefore, the chance that 2 of\n50 people have the same birthday is low. This seems reasonable, but let\u2019s do a simulated\nexperiment. In Figure 1.16 we plot the probability as a function of the number of people.\nFor a room containing 50 people, the probability is 97%. To get a 50% probability, we just\nneed 23 people! How is this possible?\n0 10 20 30 40 50 60 70 80 90 100\nNumber of people00.10.20.30.40.50.60.70.80.91Probability\nFigure 1.16: The probability for two people in a group to have the same birthday as a function of the\nnumber of people in the group.\nIf you think about this problem more deeply, you will probably realize that to solve the\nproblem, we must carefully enumerate all the possible configurations. How can we do this?\nWell, suppose you walk into the room and sequentially pick two people. The probability\n31", "47": "CHAPTER 1. MATHEMATICAL BACKGROUND\nthat they have different birthdays is\nP[The first 2 people have different birthdays] =365\n365\u00d7364\n365.\nWhen you ask the first person to tell you their birthday, he or she can occupy any of the\n365 slots. This gives us365\n365. The second person has one slot short because the first person\nhas taken it, and so the probability that he or she has a different birthday from the first\nperson is364\n365. Note that this calculation is independent of how many people you have in the\nroom because you are picking them sequentially.\nIf you now choose a third person, the probability that they have different birthdays is\nP[The first 3 people have different birthdays] =365\n365\u00d7364\n365\u00d7363\n365.\nThis process can be visualized in Figure 1.17 .\nFigure 1.17: The probability for two people to have the same birthday as a function of the number of\npeople in the group. When there is only one person, this person can land on any of the 365 days. When\nthere are two people, the first person has already taken one day (out of 365 days), so the second person\ncan only choose 364 days. When there are three people, the first two people have occupied two days,\nso there are only 363 days left. If we generalize this process, we see that the number of configurations\nis365\u00d7364\u00d7 \u00b7\u00b7\u00b7 \u00d7 (365\u2212k+ 1), where kis the number of people in the room.\nSo imagine that you keep going down the list to the 50th person. The probability that\nnone of these 50 people will have the same birthday is\nP[The first 50 people have different birthdays]\n=365\n365\u00d7364\n365\u00d7363\n365\u00d7 \u00b7\u00b7\u00b7 \u00d7316\n365\u22480.03.\nThat means that the probability for 50 people to have different birthdays, the probability is\nas little as 3%. If you take the complement, you can show that with 97% probability, there\nis at least one pair of people having the same birthday.\nThe general equation for this problem is now easy to see:\nP[The first kpeople have different birthdays] =365\u00d7364\u00d7 \u00b7\u00b7\u00b7 \u00d7 (365\u2212k+ 1)\n365\u00d7365\u00d7 \u00b7\u00b7\u00b7 \u00d7 365\n=365!\n(365\u2212k)!\u00d71\n365k.\n32", "48": "1.5. BASIC COMBINATORICS\nThe first term in our equation,365!\n(365\u2212k)!, is called the permutation of picking kdays from\n365 options. We shall discuss this operation shortly.\nWhy is the probability so high with only 50 people while it seems that we need 366\npeople to ensure two identical birthdays? The difference is the notion of probabilistic and\ndeterministic . The 366-people argument is deterministic. If you have 366 people, you are\ncertain that two people will have the same birthday. This has no conflict with the proba-\nbilistic argument because the probabilistic argument says that with 50 people, we have a\n97% chance of getting two identical birthdays. With a 97% success rate, you still have a\n3% chance of failing. It is unlikely to happen, but it can still happen. The more people you\nput into the room, the stronger guarantee you will have. However, even if you have 364\npeople and the probability is almost 100%, there is still no guarantee. So there is no conflict\nbetween the two arguments since they are answering two different questions.\nNow, let\u2019s discuss the two combinatorics questions.\n1.5.2 Permutation\nPermutation concerns the following question:\nConsider a set of ndistinct balls. Suppose we want to pick kballs from the set without\nreplacement. How many ordered configurations can we obtain?\nNote that in the above question, the word \u201cordered\u201d is crucial. For example, the set\nA={a, b, c}can lead to 6 different ordered configurations\n(a, b, c ),(a, c, b ),(b, a, c ),(b, c, a ),(c, a, b ),(c, b, a ).\nAs a simple illustration of how to compute the permutation, we can consider a set of\n5 colored balls as shown in Figure 1.18 .\nFigure 1.18: Permutation. The number of choices is reduced in every stage. Therefore, the total number\nisn\u00d7(n\u22121)\u00d7 \u00b7\u00b7\u00b7 \u00d7 (n\u2212k+ 1) if there are kstages.\nIf you start with the base, which contains five balls, you will have five choices. At one\nlevel up, since one ball has already been taken, you have only four choices. You continue\nthe process until you reached the number of balls you want to collect. The number of\nconfigurations you have generated is the permutation. Here is the formula:\n33", "49": "CHAPTER 1. MATHEMATICAL BACKGROUND\nTheorem 1.6. The number of permutations of choosing kout of nis\nn!\n(n\u2212k)!\nwhere n! =n(n\u22121)(n\u22122)\u00b7\u00b7\u00b73\u00b72\u00b71.\nProof . Let\u2019s list all possible ways:\nWhich ball to pick Number of choices Why?\nThe 1st ball n No has been picked, so we\nhave nchoices\nThe 2nd ball n\u22121 The first ball has been\npicked\nThe 3rd ball n\u22122 The first two balls have\nbeen picked\n.........\nThekth ball n\u2212k+ 1 The first k\u22121 balls have\nbeen picked\nTotal: n(n\u22121)\u00b7\u00b7\u00b7(n\u2212k+ 1)\nThe total number of ordered configurations is n(n\u22121)\u00b7\u00b7\u00b7(n\u2212k+ 1). This simplifies\nto\nn(n\u22121)(n\u22122)\u00b7\u00b7\u00b7(n\u2212k+ 1)\n=n(n\u22121)(n\u22122)\u00b7\u00b7\u00b7(n\u2212k+ 1)\u00b7(n\u2212k)(n\u2212k\u22121)\u00b7\u00b7\u00b73\u00b72\u00b71\n(n\u2212k)(n\u2212k\u22121)\u00b7\u00b7\u00b73\u00b72\u00b71\n=n!\n(n\u2212k)!.\n\u25a1\nPractice Exercise 1.8 . Consider a set of 4 balls {1,2,3,4}. We want to pick two\nballs at random without replacement. The ordering matters. How many permutations\ncan we obtain?\nSolution . The possible configurations are (1,2), (2,1), (1,3), (3,1), (1,4), (4,1), (2,3),\n(3,2), (2,4), (4,2), (3,4), (4,3). So totally there are 12 configurations. We can also\nverify this number by noting that there are 4 balls altogether and so the number\nof choices for picking the first ball is 4 and the number of choices for picking the\nsecond ball is (4 \u22121) = 3. Thus, the total is 4 \u00b73 = 12. Referring to the formula, this\nresult coincides with the theorem, which states that the number of permutations is\n4!\n(4\u22122)!=4\u00b73\u00b72\u00b71\n2\u00b71= 12.\n1.5.3 Combination\nAnother operation in combinatorics is combination. Combination concerns the following\nquestion:\n34", "50": "1.5. BASIC COMBINATORICS\nConsider a set of ndistinct balls. Suppose we want to pick kballs from the set without\nreplacement. How many unordered configurations can we obtain?\nUnlike permutation, combination treats a subset of balls with whatever ordering as\none single configuration. For example, the subset ( a, b, c ) is considered the same as ( a, c, b )\nor (b, c, a ), etc.\nLet\u2019s go back to the 5-ball exercise. Suppose you have picked orange, green, and light\nblue. This is the same combination as if you have picked {green, orange, and light blue },\nor{green, light blue, and orange }.Figure 1.19 lists all the six possible configurations for\nthese three balls. So what is combination? Combination needs to take these repeated cases\ninto account.\nFigure 1.19: Combination. In this problem, we are interested in picking 3 colored balls out of 5. This\nwill give us 5\u00d74\u00d73 = 60 permutations. However, since we are not interested in the ordering, some of\nthe permutations are repeated. For example, there are 6 combos of (green, light blue, orange), which is\ncomputed from 3\u00d72\u00d71. Dividing 60 permutations by these 6 choices of the orderings will give us 10\ndistinct combinations of the colors.\nTheorem 1.7. The number of combinations of choosing kout of nis\nn!\nk!(n\u2212k)!\nwhere n! =n(n\u22121)(n\u22122)\u00b7\u00b7\u00b73\u00b72\u00b71.\nProof . We start with the permutation result, which gives usn!\n(n\u2212k)!permutations. Note that\nevery permutation has exactly kballs. However, while these kballs can be arranged in any\norder, in combination, we treat them as one single configuration. Therefore, the task is to\ncount the number of possible orderings for these kballs.\nTo this end, we note that for a set of kballs, there are in total k! possible ways of\nordering them. The number k! comes from the following table.\n35", "51": "CHAPTER 1. MATHEMATICAL BACKGROUND\nWhich ball to pick Number of choices\nThe 1st ball k\nThe 2nd ball k\u22121\n......\nThekth ball 1\nTotal: k(k\u22121)\u00b7\u00b7\u00b73\u00b72\u00b71\nTherefore, the total number of orderings for a set of kballs is k!. Since permutation\ngives usn!\n(n\u2212k)!and every permutation has k! repetitions due to ordering, we divide the\nnumber by k!. Thus the number of combinations is\nn!\nk!(n\u2212k)!.\n\u25a1\nPractice Exercise 1.9 . Consider a set of 4 balls {1,2,3,4}. We want to pick two\nballs at random without replacement. The ordering does not matter. How many com-\nbinations can we obtain?\nSolution . The permutation result gives us 12 permutations. However, among all these\n12 permutations, there are only 6 distinct pairs of numbers. We can confirm this by\nnoting that since we picked 2 balls, there are exactly 2 possible orderings for these 2\nballs. Therefore, we have12\n2= 6 number of combinations. Using the formula of the\ntheorem, we check that the number of combinations is\n4!\n2!(4\u22122)!=4\u00b73\u00b72\u00b71\n(2\u00b71)(2\u00b71)= 6.\nExample 1.10 . (Ross, 8th edition, Section 1.6) Consider the equation\nx1+x2+\u00b7\u00b7\u00b7+xK=N,\nwhere {xk}are positive integers. How many combinations of solutions of this equation\nare there?\nSolution . We can determine the number of combinations by considering the figure\nbelow. The integer Ncan be modeled as Nballs in an urn. The number of variables K\nis equivalent to the number of colors of these balls. Since all variables are positive, the\nproblem can be translated to partitioning the Nballs into Kbuckets. This, in turn,\nis the same as inserting K\u22121 dividers among N\u22121 holes. Therefore, the number of\ncombinations is \u0012N\u22121\nK\u22121\u0013\n=(N\u22121)!\n(K\u22121)!(N\u2212K)!.\nFor example, if N= 16 and K= 4, then the number of solutions is\n\u001216\u22121\n4\u22121\u0013\n=15!\n3!12!= 455 .\n36", "52": "1.6. SUMMARY\nFigure 1.20: One possible solution for N= 16 andK= 4. In general, the problem is equivalent\nto inserting K\u22121dividers among N\u22121balls.\nClosing remark . Permutations and combinations are two ways to enumerate all the pos-\nsible cases. While the conclusions are probabilistic, as the birthday paradox shows, permu-\ntation and combination are deterministic. We do not need to worry about the distribution\nof the samples, and we are not taking averages of anything. Thus, modern data analysis\nseldom uses the concepts of permutation and combination. Accordingly, combinatorics does\nnot play a large role in this book.\nDoes it mean that combinatorics is not useful? Not quite, because it still provides us\nwith powerful tools for theoretical analysis. For example, in binomial random variables, we\nneed the concept of combination to calculate the repeated cases. The Poisson random vari-\nable can be regarded as a limiting case of the binomial random variable, and so combination\nis also used. Therefore, while we do not use the concepts of permutation per se, we use them\nto define random variables.\n1.6 Summary\nIn this chapter, we have reviewed several background mathematical concepts that will be-\ncome useful later in the book. You will find that these concepts are important for under-\nstanding the rest of this book. When studying these materials, we recommend not just\nremembering the \u201crecipes\u201d of the steps but focusing on the motivations andintuitions\nbehind the techniques.\nWe would like to highlight the significance of the birthday paradox. Many of us come\nfrom an engineering background in which we were told to ensure reliability and guarantee\nsuccess. We want to ensure that the product we deliver to our customers can survive even\nin the worst-case scenario. We tend to apply deterministic arguments such as requiring 366\npeople to ensure complete coverage of the 365 days. In modern data analysis, the worst-case\nscenario may not always be relevant because of the complexity of the problem and the cost\nof such a warranty. The probabilistic argument, or the average argument, is more reasonable\nand cost-effective, as you can see from our analysis of the birthday problem. The heart of\nthe problem is the trade-off between how much confidence you need versus how much effort\nyou need to expend. Suppose an event is unlikely to happen, but if it happens, it will be\na disaster. In that case, you might prefer to be very conservative to ensure that such a\ndisaster event has a low chance of happening. Industries related to risk management such\nas insurance and investment banking are all operating under this principle.\n37", "53": "CHAPTER 1. MATHEMATICAL BACKGROUND\n1.7 Reference\nIntroductory materials\n1-1 Erwin Kreyszig, Advanced Engineering Mathematics , Wiley, 10th Edition, 2011.\n1-2 Henry Stark and John W. Woods, Probability and Random Processes with Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2002. Appendix.\n1-3 Michael J. Evans and Jeffrey S. Rosenthal, Probability and Statistics: The Science of\nUncertainty , W. H. Freeman, 2nd Edition, 2009. Appendix.\n1-4 James Stewart, Single Variable Calculus, Early Transcendentals , Thomson Brooks/-\nCole, 6th Edition, 2008. Chapter 5.\nCombinatorics\n1-5 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Section 1.6.\n1-6 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Section 2.6.\n1-7 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 3.\nAnalysis\nIn some sections of this chapter, we use results from calculus and infinite series. Many formal\nproofs can be found in the standard undergraduate real analysis textbooks.\n1-8 Tom M. Apostol, Mathematical Analysis , Pearson, 1974.\n1-9 Walter Rudin, Principles of Mathematical Analysis , McGraw Hill, 1976.\n1.8 Problems\nExercise 1. (Video Solution)\n(a) Show that\nnX\nk=0rk=1\u2212rn+1\n1\u2212r.\nfor any 0 < r < 1. EvaluateP\u221e\nk=0rk.\n(b) Using the result of (a), evaluate\n1 + 2 r+ 3r2+\u00b7\u00b7\u00b7.\n38", "54": "1.8. PROBLEMS\n(c) Evaluate the sums\n\u221eX\nk=0k\u00121\n3\u0013k+1\n,and\u221eX\nk=2k\u00121\n4\u0013k\u22121\n.\nExercise 2. (Video Solution)\nRecall that\u221eX\nk=0\u03bbk\nk!=e\u03bb.\nEvaluate\u221eX\nk=0k\u03bbke\u2212\u03bb\nk!,and\u221eX\nk=0k2\u03bbke\u2212\u03bb\nk!.\nExercise 3. (Video Solution)\nEvaluate the integrals\n(a)Zb\na1\nb\u2212a\u0012\nx\u2212a+b\n2\u00132\ndx.\n(b)Z\u221e\n0\u03bbxe\u2212\u03bbxdx.\n(c)Z\u221e\n\u2212\u221e\u03bbx\n2e\u2212\u03bb|x|dx.\nExercise 4.\n(a) Compute the result of the following matrix vector multiplication using Numpy. Submit\nyour result and codes.\n\uf8ee\n\uf8f01 2 3\n4 5 6\n7 8 9\uf8f9\n\uf8fb\u00d7\uf8ee\n\uf8f01\n2\n3\uf8f9\n\uf8fb.\n(b) Plot a sine function on the interval [ \u2212\u03c0, \u03c0] with 1000 data points.\n(c) Generate 10,000 uniformly distributed random numbers on interval [0, 1).\nUsematplotlib.pyplot.hist to generate a histogram of all the random numbers.\n39", "55": "CHAPTER 1. MATHEMATICAL BACKGROUND\nExercise 5.\nCalculate\u221eX\nk=0k\u00122\n3\u0013k+1\n.\nExercise 6.\nLet\nx=\u0014x\ny\u0015\n,\u00b5=\u00141\n0\u0015\n,\u03a3=\u00144 1\n1 1\u0015\n.\n(a) Find \u03a3\u22121, the inverse of \u03a3.\n(b) Find |\u03a3|, the determinant of \u03a3.\n(c) Simplify the two-dimensional function\nf(x) =1\n2\u03c0|\u03a3|1/2exp\u001a\n\u22121\n2(x\u2212\u00b5)T\u03a3\u22121(x\u2212\u00b5)\u001b\n.\n(d) Use matplotlib.pyplot.contour , plot the function f(x) for the range [ \u22123,3]\u00d7\n[\u22123,3].\nExercise 7.\nOut of seven electrical engineering (EE) students and five mechanical engineering (ME)\nstudents, a committee consisting of three EEs and two MEs is to be formed. In how many\nways can this be done if\n(a) any of the EEs and any of the MEs can be included?\n(b) one particular EE must be on the committee?\n(c) two particular MEs cannot be on the committee?\nExercise 8.\nFive blue balls, three red balls, and three white balls are placed in an urn. Three balls are\ndrawn at random without regard to the order in which they are drawn. Using the counting\napproach to probability, find the probability that\n(a) one blue ball, one red ball, and one white ball are drawn.\n(b) all three balls drawn are red.\n(c) exactly two of the balls drawn are blue.\nExercise 9.\nA collection of 26 English letters, a-z, is mixed in a jar. Two letters are drawn at random,\none after the other.\n40", "56": "1.8. PROBLEMS\n(a) What is the probability of drawing a vowel (a,e,i,o,u) and a consonant in either order?\n(b) Write a MATLAB / Python program to verify your answer in part (a). Randomly\ndraw two letters without replacement and check whether one is a vowel and the other\nis a consonant. Compute the probability by repeating the experiment 10000 times.\nExercise 10.\nThere are 50 students in a classroom.\n(a) What is the probability that there is at least one pair of students having the same\nbirthday? Show your steps.\n(b) Write a MATLAB / Python program to simulate the event and verify your answer\nin (a). Hint: You probably need to repeat the simulation many times to obtain a\nprobability. Submit your code and result.\nYou may assume that a year only has 365 days. You may also assume that all days have an\nequal likelihood of being taken.\n41", "57": "CHAPTER 1. MATHEMATICAL BACKGROUND\n42", "58": "Chapter 2\nProbability\nData and probability are inseparable. Data is the computational side of the story, whereas\nprobability is the theoretical side of the story. Any data science practice must be built on\nthe foundation of probability, and probability needs to address practical problems. However,\nwhat exactly is \u201cprobability\u201d? Mathematicians have been debating this for centuries. The\nfrequentists argue that probability is the relative frequency of an outcome. For example,\nflipping a fair coin has a 1/2 probability of getting a head because if you flip the coin\ninfinitely many times, you will have half of the time getting a head. The Bayesians argue\nthat probability is a subjective belief. For example, the probability of getting an A in a\nclass is subjective because no one would want to take a class infinitely many times to obtain\nthe relative frequency. Both the frequentists and Bayesians have valid points. However, the\ndifferentiation is often non-essential because the context of your problem will force you\nto align with one or the other. For example, when you have a shortage of data, then the\nsubjectivity of the Bayesians allows you to use prior knowledge, whereas the frequentists\ntell us how to compute the confidence interval of an estimate.\nNo matter whether you prefer the frequentist\u2019s view or the Bayesian\u2019s view, there is\nsomething more fundamental thanks to Andrey Kolmogorov (1903-1987). The development\nof this fundamental definition will take some effort on our part, but if we distill the essence,\nwe can summarize it as follows:\nProbability is a measure of the size of a set.\nThis sentence is not a formal definition; instead, it summarizes what we believe to be the\nessence of probability. We need to clarify some puzzles later in this chapter, but if you can\nunderstand what this sentence means, you are halfway done with this book. To spell out the\ndetails, we will describe an elementary problem that everyone knows how to solve. As we\ndiscuss this problem, we will highlight a few key concepts that will give you some intuitive\ninsights into our definition of probability, after which we will explain the sequence of topics\nto be covered in this chapter.\nPrelude: Probability of throwing a die\nSuppose that you have a fair die. It has 6 faces: {\n,\n,\n,\n,\n,\n}. What is the probability\nthat you get a number that is \u201cless than 5\u201d and is \u201can even number\u201d? This is a straightfor-\n43", "59": "CHAPTER 2. PROBABILITY\nward problem. You probably have already found the answer, which is2\n6because \u201cless than\n5\u201d and \u201can even number\u201d means {\n,\n}. However, let\u2019s go through the thinking process\nslowly by explicitly writing down the steps.\nFirst of all, how do we know that the denominator in2\n6is 6? Well, because there are six\nfaces. These six faces form a set called the sample space . A sample space is the set containing\nall possible outcomes, which in our case is \u2126 = {\n,\n,\n,\n,\n,\n}. The denominator 6 is the\nsize of the sample space.\nHow do we know that the numerator is 2? Again, implicitly in our minds, we have\nconstructed two events :E1= \u201cless than 5\u201d = {\n,\n,\n,\n}, and E2= \u201can even number\u201d\n={\n,\n,\n}. Then we take the intersection between these two events to conclude the event\nE={\n,\n}. The numerical value \u201c2\u201d is the size of this event E.\nSo, when we say that \u201cthe probability is2\n6,\u201d we are saying that the size of the event\nErelative to the sample space \u2126 is the ratio2\n6. This process involves measuring the size\nofEand \u2126. In this particular example, the measure we use is a \u201ccounter\u201d that counts the\nnumber of elements.\nThis example shows us all the necessary components of probability: (i) There is a\nsample space , which is the set that contains all the possible outcomes. (ii) There is an event ,\nwhich is a subset inside the sample space. (iii) Two events E1andE2can be combined to\nconstruct another event Ethat is still a subset inside the sample space. (iv) Probability is\na number assigned by certain rules such that it describes the relative size of the event E\ncompared with the sample space \u2126. So, when we say that probability is a measure of the\nsize of a set , we create a mapping that takes in a set and outputs the size of that set.\nOrganization of this chapter\nAs you can see from this example, since probability is a measure of the size of a set, we need\nto understand the operations of sets to understand probability. Accordingly, in Section 2.1\nwe first define sets and discuss their operations. After learning these basic concepts, we move\non to define the sample space and event space in Section 2.2. There, we discuss sample spaces\nthat are not necessarily countable and how probabilities are assigned to events. Of course,\nassigning a probability value to an event cannot be arbitrary; otherwise, the probabilities\nmay be inconsistent. Consequently, in Section 2.3 we introduce the probability axioms and\nformalize the notion of measure. Section 2.4 consists of a trio of topics that concern the\nrelationship between events using conditioning. We discuss conditional probability in Section\n2.4.1, independence in Section 2.4.2, and Bayes\u2019 theorem in Section 2.4.3.\n2.1 Set Theory\n2.1.1 Why study set theory?\nIn mathematics, we are often interested in describing a collection of numbers, for example, a\npositive interval [ a, b] on the real line or the ordered pairs of numbers that define a circle on\na graph with two axes. These collections of numbers can be abstractly defined as sets. In a\nnutshell, a set is simply a collection of things. These things can be numbers, but they can also\nbe alphabets, objects, or anything. Set theory is a mathematical tool that defines operations\non sets. It provides the basic arithmetic for us to combine, separate, and decompose sets.\n44", "60": "2.1. SET THEORY\nWhy do we start the chapter by describing set theory? Because probability is a measure\nof the size of a set . Yes, probability is not just a number telling us the relative frequency of\nevents; it is an operator that takes a set and tells us how large the set is. Using the example\nwe showed in the prelude, the event \u201ceven number\u201d of a die is a set containing numbers\n{\n,\n,\n}. When we apply probability to this set, we obtain the number3\n6, as shown in\nFigure 2.1 . Thus sets are the foundation of the study of probability.\nFigure 2.1: Probability is a measure of the size of a set. Whenever we talk about probability, it has to\nbe the probability of a set.\n2.1.2 Basic concepts of a set\nDefinition 2.1 (Set).Asetis a collection of elements. We denote\nA={\u03be1, \u03be2, . . . , \u03be n} (2.1)\nas a set, where \u03beiis the ith element in the set.\nIn this definition, Ais called a set. It is nothing but a collection of elements \u03be1, . . . , \u03be n. What\nare these \u03bei\u2019s? They can be anything. Let\u2019s see a few examples below.\nExample 2.1(a) .A={apple ,orange ,pear}is a finite set.\nExample 2.1(b) .A={1,2,3,4,5,6}is a finite set.\nExample 2.1(c) .A={2,4,6,8, . . .}is a countable but infinite set.\nExample 2.1(d) .A={x|0< x < 1}is a uncountable set.\nTo say that an element \u03beis drawn from A, we write \u03be\u2208A. For example, the number 1\nis an element in the set {1,2,3}. We write 1 \u2208 {1,2,3}. There are a few common sets that\nwe will encounter. For example,\nExample 2.2(a) .Ris the set of all real numbers including \u00b1\u221e.\nExample 2.2(b) .R2is the set of ordered pairs of real numbers.\nExample 2.2(c) . [a, b] ={x|a\u2264x\u2264b}is a closed interval on R.\nExample 2.2(d) . (a, b) ={x|a < x < b }is an open interval on R.\nExample 2.2(e) . (a, b] ={x|a < x \u2264b}is a semi-closed interval on R.\n45", "61": "CHAPTER 2. PROBABILITY\nFigure 2.2: From left to right: a closed interval, a semi-closed (or semi-open) interval, and an open\ninterval.\nSets are not limited to numbers. A set can be used to describe a collection of functions .\nExample 2.3 .A={f:R\u2192R|f(x) =ax+b, a, b \u2208R}. This is the set of all straight\nlines in 2D. The notation f:R\u2192Rmeans that the function ftakes an argument\nfromRand sends it to another real number in R. The definition f(x) =ax+bsays\nthat fis taking the specific form of ax+b. Since the constants aandbcan be any\nreal number, the equation f(x) =ax+benumerates all possible straight lines in 2D.\nSeeFigure 2.3 (a).\nExample 2.4 .A={f:R\u2192[\u22121,1]|f(t) = cos( \u03c90t+\u03b8), \u03b8\u2208[0,2\u03c0]}. This is\nthe set of all cosine functions of a fixed carrier frequency \u03c90. The phase \u03b8, however,\nis changing. Therefore, the equation f(t) = cos( \u03c90t+\u03b8) says that the set Ais the\ncollection of all possible cosines with different phases. See Figure 2.3 (b).\n-2 -1 0 1 2\nt-1-0.500.51f(t)\n-1 -0.5 0 0.5 1\nt-2-1012f(t)\nFigure 2.3: (a) The set of straight lines A={f:R\u2192R|f(x) =ax+b, a, b \u2208R}. (b) The set of\nphase-shifted cosines A={f:R\u2192[\u22121,1]|f(t) = cos( \u03c90t+\u03b8), \u03b8\u2208[0,2\u03c0]}.\nA set can also be used to describe a collection of sets. Let AandBbe two sets. Then\nC={A, B}is a set of sets.\nExample 2.5 . Let A={1,2}andB={apple ,orange }. Then\nC={A, B}={{1,2},{apple ,orange }}\n46", "62": "2.1. SET THEORY\nis a collection of sets. Note that here we are not saying Cis the union of two sets. We\nare only saying that Cis a collection of two sets. See the next example.\nExample 2.6 . Let A={1,2}andB={3}, then C={A, B}means that\nC={{1,2},{3}}.\nTherefore Ccontains only two elements. One is the set {1,2}and the other is the set\n{3}. Note that {{1,2},{3}} \u0338={1,2,3}. The former is a set of two sets. The latter is a\nset of three elements.\n2.1.3 Subsets\nGiven a set, we often want to specify a portion of the set, which is called a subset .\nDefinition 2.2 (Subset ).Bis asubset ofAif for any \u03be\u2208B,\u03beis also in A. We\nwrite\nB\u2286A (2.2)\nto denote that Bis a subset of A.\nBis called a proper subset ofAifBis a subset of AandB\u0338=A. We denote a proper subset\nasB\u2282A. Two sets AandBare equal if and only if A\u2286BandB\u2286A.\nExample 2.7 .\n\u0088IfA={1,2,3,4,5,6}, then B={1,3,5}is a proper subset of A.\n\u0088IfA={1,2}, then B={1,2}is an improper subset of A.\n\u0088IfA={t|t\u22650}, then B={t|t >0}is a proper subset of A.\nPractice Exercise 2.1 . Let A={1,2,3}. List all the subsets of A.\nSolution . The subsets of Aare:\nA={\u2205,{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3}}.\nPractice Exercise 2.2 . Prove that two sets AandBare equal if and only if A\u2286B\nandB\u2286A.\nSolution . Suppose A\u2286BandB\u2286A. Assume by contradiction that A\u0338=B. Then\nnecessarily there must exist an xsuch that x\u2208Abutx\u0338\u2208B(or vice versa). But\nA\u2286Bmeans that x\u2208Awill necessarily be in B. So it is impossible to have x\u0338\u2208B.\nConversely, suppose that A=B. Then any x\u2208Awill necessarily be in B. Therefore,\nwe have A\u2286B. Similarly, if A=Bthen any x\u2208Bwill be in A, and so B\u2286A.\n47", "63": "CHAPTER 2. PROBABILITY\n2.1.4 Empty set and universal set\nDefinition 2.3 (Empty Set ).A set is empty if it contains no element. We denote\nan empty set as\nA=\u2205. (2.3)\nA set containing an element 0 is not an empty set. It is a set of one element, {0}. The\nnumber of elements of the empty set is 0. The empty set is a subset of any set, i.e., \u2205 \u2286A\nfor any A. We use \u2286because Acould also be an empty set.\nExample 2.8(a) . The set A={x|sinx >1}is empty because no x\u2208Rcan make\nsinx >1.\nExample 2.8(b) . The set A={x|x > 5 and x < 1}is empty because the two\nconditions x >5 and x <1 are contradictory.\nDefinition 2.4 (Universal Set ).Theuniversal set is the set containing all elements\nunder consideration. We denote a universal set as\nA= \u2126. (2.4)\nThe universal set \u2126 contains itself, i.e., \u2126 \u2286\u2126. The universal set is a relative concept.\nUsually, we first define a universal set \u2126 before referring to subsets of \u2126. For example, we\ncan define \u2126 = Rand refer to intervals in R. We can also define \u2126 = [0 ,1] and refer to\nsubintervals inside [0 ,1].\n2.1.5 Union\nWe now discuss basic set operations. By operations, we mean functions of two or more sets\nwhose output value is a set. We use these operations to combine and separate sets. Let us\nfirst consdier the union of two sets. See Figure 2.4 for a graphical depiction.\nDefinition 2.5 (Finite Union ).Theunion of two sets AandBcontains all elements\ninAorinB. That is,\nA\u222aB={\u03be|\u03be\u2208Aor\u03be\u2208B}. (2.5)\nAs the definition suggests, the union of two sets connects the sets using the logical operator\n\u201dor\u201d. Therefore, the union of two sets is always larger than or equal to the individual sets.\nExample 2.9(a) . IfA={1,2},B={1,5}, then A\u222aB={1,2,5}. The overlapping\nelement 1 is absorbed. Also, note that A\u222aB\u0338={{1,2},{1,5}}. The latter is a set of\nsets.\nExample 2.9(b) . IfA= (3,4],B= (3.5,\u221e), then A\u222aB= (3,\u221e).\nExample 2.9(c) . IfA={f:R\u2192R|f(x) =ax}andB={f:R\u2192R|f(x) =b},\nthen A\u222aB= a set of sloped lines with a slope aplus a set of constant lines with\n48", "64": "2.1. SET THEORY\nheight b. Note that A\u222aB\u0338={f:R\u2192R|f(x) =ax+b}because the latter is a set of\nsloped lines with arbitrary y-intercept.\nExample 2.9(d) . IfA={1,2}andB=\u2205, then A\u222aB={1,2}.\nExample . IfA={1,2}andB= \u2126, then A\u222aB= \u2126.\nFigure 2.4: The union of two sets contains elements that are either in AorBor both.\nThe previous example can be generalized in the following exercise. What it says is that\nifAis a subset of another set B, then the union of AandBis just B. Intuitively, this should\nbe straightforward because whatever you have in Ais already in B, so the union will just\nbeB. Below is a formal proof that illustrates how to state the arguments clearly. You may\nlike to draw a picture to convince yourself that the proof is correct.\nPractice Exercise 2.3 : Prove that if A\u2286B, then A\u222aB=B.\nSolution : We will show that A\u222aB\u2286BandB\u2286A\u222aB. Let \u03be\u2208A\u222aB. Then \u03bemust\nbe inside either AorB(or both). In any case, since we know that A\u2286B, it holds\nthat if \u03be\u2208Athen \u03bemust also be in B. Therefore, for any \u03be\u2208A\u222aBwe have \u03be\u2208B.\nThis shows A\u222aB\u2286B. Conversely, if \u03be\u2208B, then \u03bemust be inside A\u222aBbecause\nA\u222aBis a larger set than B. So if \u03be\u2208Bthen \u03be\u2208A\u222aBand hence B\u2286A\u222aB. Since\nA\u222aBis a subset of Bor equal to B, and Bis a subset of A\u222aBor equal to A\u222aB, it\nfollows that A\u222aB=B.\nWhat should we do if we want to take the union of an infinite number of sets? First,\nwe need to define the concept of an infinite union .\nDefinition 2.6 (Infinite Union ).For an infinite sequence of sets A1, A2, . . ., the in-\nfinite union is defined as\n\u221e[\nn=1An={\u03be|\u03be\u2208Anforat least one nthat is finite. }. (2.6)\nAn infinite union is a natural extension of a finite union. It is not difficult to see that\n\u03be\u2208Aor\u03be\u2208B\u21d0\u21d2 \u03beis inat least one of AandB.\n49", "65": "CHAPTER 2. PROBABILITY\nSimilarly, an infinite union means that\n\u03be\u2208A1or\u03be\u2208A2or\u03be\u2208A3. . .\u21d0\u21d2 \u03beis inat least one of A1, A2, A3, . . . .\nThe finite nrequirement says that we only evaluate the sets for a finite number of n\u2019s. This\nncan be arbitrarily large, but it is finite. Why are we able to do this? Because the concept\nof an infinite union is to determine A\u221e, which is the limit of a sequence. Like any sequence\nof real numbers, the limit of a sequence of sets has to be defined by evaluating the instances\nof all possible finite cases.\nConsider a sequence of sets An=\u0002\n\u22121,1\u22121\nn\u0003\n, for n= 1,2, . . .. For example, A1=\n[\u22121,0],A2=\u0002\n\u22121,1\n2\u0003\n,A3=\u0002\n\u22121,2\n3\u0003\n,A4=\u0002\n\u22121,3\n4\u0003\n, etc.\nFigure 2.5: The infinite union ofS\u221e\nn=1\u0002\n\u22121,1\u22121\nn\u0003\n. No matter how large ngets, the point 1is never\nincluded. So the infinite union is [\u22121,1)\nTo take the infinite union, we know that the set [ \u22121,1) is always included, because the\nright-hand limit 1 \u22121\nnapproaches 1 as napproaches \u221e. So the only question concerns the\nnumber 1. Should 1 be included? According to the definition above, we ask: Is 1 an element\nofat least one of the sets A1,A2, . . . , An? Clearly it is not: 1 \u0338\u2208A1, 1\u0338\u2208A2,. . .. In fact,\n1\u0338\u2208Anfor any finite n. Therefore 1 is not an element of the infinite union, and we conclude\nthat\u221e[\nn=1An=\u221e[\nn=1\u0014\n\u22121,1\u22121\nn\u0015\n= [\u22121,1).\nPractice Exercise 2.4 . Find the infinite union of the sequences where (a) An=\u0002\n\u22121,1\u22121\nn\u0001\n, (b) An=\u0000\n\u22121,1\u22121\nn\u0003\n.\nSolution . (a)S\u221e\nn=1An= [\u22121,1). (b)S\u221e\nn=1An= (\u22121,1).\n2.1.6 Intersection\nThe union of two sets is based on the logical operator or. If we use the logical operator and,\nthen the result is the intersection of two sets.\nDefinition 2.7 (Finite Intersection ).Theintersection of two sets AandBcontains\nall elements in AandinB. That is,\nA\u2229B={\u03be|\u03be\u2208Aand\u03be\u2208B}. (2.7)\nFigure 2.6 portrays intersection graphically. Intersection finds the common elements of the\ntwo sets. It is not difficult to show that A\u2229B\u2286AandA\u2229B\u2286B.\n50", "66": "2.1. SET THEORY\nFigure 2.6: The intersection of two sets contains elements in both AandB.\nExample 2.10(a) . IfA={1,2,3,4},B={1,5,6}, then A\u2229B={1}.\nExample 2.10(b) . IfA={1,2},B={5,6}, then A\u2229B=\u2205.\nExample 2.10(c) . IfA= (3,4],B= [3.5,\u221e), then A\u2229B= [3.5,4].\nExample 2.10(d) . IfA= (3,4],B=\u2205, then A\u2229B=\u2205.\nExample 2.10(e) . IfA= (3,4],B= \u2126, then A\u2229B= (3,4].\nExample 2.11 . IfA={f:R\u2192R|f(x) =ax}andB={f:R\u2192R|f(x) =b}, then\nA\u2229B= the intersection of a set of sloped lines with a slope aand a set of constant lines\nwith height b. The only line that can satisfy both sets is the line f(x) = 0. Therefore,\nA\u2229B={f|f(x) = 0}.\nExample 2.12 . IfA={{1},{2}}andB={{2,3},{4}}, then A\u2229B=\u2205. This is\nbecause Ais a set containing two sets, and Bis a set containing two sets. The two sets\n{2}and{2,3}are not the same. Thus, AandBhave no elements in common, and so\nA\u2229B=\u2205.\nSimilarly to the infinite union, we can define the concept of infinite intersection .\nDefinition 2.8 (Infinite Intersection ).For an infinite sequence of sets A1, A2, . . .,\ntheinfinite intersection is defined as\n\u221e\\\nn=1An={\u03be|\u03be\u2208Anfor every finite n.} (2.8)\nTo understand this definition, we note that\n\u03be\u2208Aand \u03be\u2208B\u21d0\u21d2 \u03beis inevery one ofAandB.\nAs a result, it follows that\n\u03be\u2208A1and \u03be\u2208A2and \u03be\u2208A3. . .\u21d0\u21d2 \u03beis inevery one of A1, A2, A3, . . . .\n51", "67": "CHAPTER 2. PROBABILITY\nSince the infinite intersection requires that \u03beis in every one of A1,A2,. . .,An, if there is a\nsetAithat does not contain \u03be, the infinite intersection is an empty set.\nConsider the problem of finding the infinite intersection ofT\u221e\nn=1An, where\nAn=\u0014\n0,1 +1\nn\u0013\n.\nWe note that the sequence of sets is [0 ,2], [0 ,1.5], [0 ,1.33], . . . . As n\u2192 \u221e , we note that\nthe limit is either [0 ,1) or [0 ,1]. Should the right-hand limit 1 be included in the infinite\nintersection? According to the definition above, we know that 1 \u2208A1, 1\u2208A2, . . . , 1 \u2208An\nfor any finite n. Therefore, 1 is included and so\n\u221e\\\nn=1An=\u221e\\\nn=1\u0014\n0,1 +1\nn\u0013\n= [0,1].\nFigure 2.7: The infinite intersection ofT\u221e\nn=1\u0002\n0,1 +1\nn\u0001\n. No matter how large ngets, the point 1is\nnever included. So the infinite intersection is [0,1]\nPractice Exercise 2.5 . Find the infinite intersection of the sequences where (a)\nAn=\u0002\n0,1 +1\nn\u0003\n, (b) An=\u0000\n0,1 +1\nn\u0001\n, (c)An=\u0002\n0,1\u22121\nn\u0001\n, (d) An=\u0002\n0,1\u22121\nn\u0003\n.\nSolution .\n(a)T\u221e\nn=1An= [0,1].\n(b)T\u221e\nn=1An= (\u22121,1].\n(c)T\u221e\nn=1An= [0,0) =\u2205.\n(d)T\u221e\nn=1An= [0,0] ={0}.\n2.1.7 Complement and difference\nBesides union and intersection, there is a third basic operation on sets known as the com-\nplement .\nDefinition 2.9 (Complement ).Thecomplement of a set Ais the set containing all\nelements that are in \u2126but not in A. That is,\nAc={\u03be|\u03be\u2208\u2126and\u03be\u0338\u2208A}. (2.9)\nFigure 2.8 graphically portrays the idea of a complement. The complement is a set that\ncontains everything in the universal set that is not in A. Thus the complement of a set is\nalways relative to a specified universal set.\n52", "68": "2.1. SET THEORY\nFigure 2.8: [Left] The complement of a set Acontains all elements that are not in A. [Right] The\ndifference A\\Bcontains elements that are in Abut not in B.\nExample 2.13(a) . Let A={1,2,3}and \u2126 = {1,2,3,4,5,6 }. Then Ac={4,5,6}.\nExample 2.13(b) . Let A={even integers }and \u2126 = {integers }. Then Ac={odd\nintegers }.\nExample 2.13(c) . Let A={integers }and \u2126 = R. Then Ac={any real number that\nis not an integer }.\nExample 2.13(d) . Let A= [0,5) and \u2126 = R. Then Ac= (\u2212\u221e,0)\u222a[5,\u221e).\nExample 2.13(e) . Let A=Rand \u2126 = R. Then Ac=\u2205.\nThe concept of the complement will help us understand the concept of difference .\nDefinition 2.10 (Difference ).Thedifference A\\Bis the set containing all elements\ninAbut not in B.\nA\\B={\u03be|\u03be\u2208Aand\u03be\u0338\u2208B}. (2.10)\nFigure 2.8 portrays the concept of difference graphically. Note that A\\B\u0338=B\\A. The former\nremoves the elements in Bwhereas the latter removes the elements in A.\nExample 2.14(a) . Let A={1,3,5,6}andB={2,3,4}. Then A\\B={1,5,6}and\nB\\A={2,4}.\nExample 2.14(b) . Let A= [0,1],B= [2,3], then A\\B= [0,1], and B\\A= [2,3].\nThis example shows that if the two sets do not overlap, there is nothing to subtract.\nExample 2.14(c) . Let A= [0,1],B=R, then A\\B=\u2205, and B\\A= (\u2212\u221e,0)\u222a(1,\u221e).\nThis example shows that if one of the sets is the universal set, then the difference will\neither return the empty set or the complement.\n53", "69": "CHAPTER 2. PROBABILITY\nFigure 2.9: [Left] AandBare overlapping. [Right] AandBare disjoint.\nPractice Exercise 2.6 . Show that for any two sets AandB, the differences A\\B\nandB\\Anever overlap, i.e., ( A\\B)\u2229(B\\A) =\u2205.\nSolution . Suppose, by contradiction, that the intersection is not empty so that there\nexists an \u03be\u2208(A\\B)\u2229(B\\A). Then, by the definition of intersection, \u03beis an element\nof (A\\B)and(B\\A). But if \u03beis an element of ( A\\B), it cannot be an element of B.\nThis implies that \u03becannot be an element of ( B\\A) since it is a subset of B. This is a\ncontradiction because we just assumed that the \u03becan live in both ( A\\B) and ( B\\A).\nDifference can be defined in terms of intersection and complement:\nTheorem 2.1. LetAandBbe two sets. Then\nA\\B=A\u2229Bc(2.11)\nProof . Let x\u2208A\\B. Then x\u2208Aandx\u0338\u2208B. Since x\u0338\u2208B, we have x\u2208Bc. Therefore,\nx\u2208Aandx\u2208Bc. By the definition of intersection, we have x\u2208A\u2229Bc. This shows\nthat A\\B\u2286A\u2229Bc. Conversely, let x\u2208A\u2229Bc. Then, x\u2208Aandx\u2208Bc, which implies\nthat x\u2208Aandx\u0338\u2208B. By the definition of A\\B, we have that x\u2208A\\B. This shows that\nA\u2229Bc\u2286A\\B.\n\u25a1\n2.1.8 Disjoint and partition\nIt is important to be able to quantify situations in which two sets are not overlapping. In\nthis situation, we say that the sets are disjoint .\nDefinition 2.11 (Disjoint ).Two sets AandBaredisjoint if\nA\u2229B=\u2205. (2.12)\nFor a collection of sets {A1, A2, . . . , A n}, we say that the collection is disjoint if, for\nany pair i\u0338=j,\nAi\u2229Aj=\u2205. (2.13)\nA pictorial interpretation can be found in Figure 2.9 .\n54", "70": "2.1. SET THEORY\nExample 2.15(a) . Let A={x >1}andB={x <0}. Then AandBare disjoint.\nExample 2.15(b) . Let A={1,2,3}andB=\u2205. Then AandBare disjoint.\nExample 2.15(c) . Let A= (0,1) and B= [1,2). Then AandBare disjoint.\nWith the definition of disjoint, we can now define the powerful concept of partition .\nDefinition 2.12 (Partition ).A collection of sets {A1, . . . , A n}is apartition of the\nuniversal set \u2126if it satisfies the following conditions:\n\u0088(non-overlap ){A1, . . . , A n}is disjoint:\nAi\u2229Aj=\u2205. (2.14)\n\u0088(decompose ) Union of {A1, . . . , A n}gives the universal set:\nn[\ni=1Ai= \u2126. (2.15)\nIn plain language, a partition is a collection of non-overlapping subsets whose union is\nthe universal set. Partition is important because it is a decomposition of \u2126 into a smaller\nsubset, and since these subsets do not overlap, they can be analyzed separately. Partition\nis a handy tool for studying probability because it allows us to decouple complex events by\ntreating them as isolated sub-events.\nFigure 2.10: A partition of \u2126contains disjoint subsets of which the union gives us \u2126.\nExample 2.16 . Let \u2126 = {1,2,3,4,5,6}. The following sets form a partition:\nA1={1,2,3}, A 2={4,5}, A 3={6}\nExample 2.17 . Let \u2126 = {1,2,3,4,5,6}. The collection\nA1={1,2,3}, A 2={4,5}, A 3={5,6}\ndoes not form a partition, because A2\u2229A3={5}.\n55", "71": "CHAPTER 2. PROBABILITY\nIf{A1, A2, . . . , A n}forms a partition of the universal set \u2126, then for any B\u2286\u2126, we\ncan decompose Bintondisjoint subsets: B\u2229A1,B\u2229A2, . . .B\u2229An. Two properties hold:\n\u0088B\u2229AiandB\u2229Ajare disjoint if i\u0338=j.\n\u0088The union of B\u2229A1,B\u2229A2, . . .B\u2229AnisB.\nPractice Exercise 2.7 . Prove the above two statements.\nSolution . To prove the first statement, we can pick \u03be\u2208(B\u2229Ai). This means that\n\u03be\u2208Band\u03be\u2208Ai. Since \u03be\u2208Ai, it cannot be in Ajbecause AiandAjare disjoint.\nTherefore \u03becannot live in B\u2229Aj. This completes the proof, because we just showed\nthat any \u03be\u2208B\u2229Aicannot simultaneously live in B\u2229Aj.\nTo prove the second statement, we pick \u03be\u2208Sn\ni=1(B\u2229Ai). Since \u03belives in the\nunion, it has to live in at least one of the ( B\u2229Ai) for some i. Now suppose \u03be\u2208B\u2229Ai.\nThis means that \u03beis in both BandAi, so it must live in B. Therefore,Sn\ni=1(B\u2229Ai)\u2286\nB. Now, suppose we pick \u03be\u2208B. Then since it is an element in B, it must be an element\nin all of the ( B\u2229Ai)\u2019s for any i. Therefore, \u03be\u2208Sn\ni=1(B\u2229Ai), and so we showed that\nB\u2286Sn\ni=1(B\u2229Ai). Combining the two directions, we conclude thatSn\ni=1(B\u2229Ai) =B.\nExample 2.18 . Let \u2126 = {1,2,3,4,5,6}and let a partition of \u2126 be A1={1,2,3},\nA2={4,5},A3={6}. Let B={1,3,4}. Then, by the result we just proved, Bcan\nbe decomposed into three subsets:\nB\u2229A1={1,3}, B\u2229A2={4}, B\u2229A3=\u2205.\nThus we can see that B\u2229A1,B\u2229A2andB\u2229A3are disjoint. Furthermore, the union\nof these three sets gives B.\n2.1.9 Set operations\nWhen handling multiple sets, it would be useful to have some basic set operations. There\nare four basic theorems concerning set operations that you need to know for our purposes\nin this book:\nTheorem 2.2 (Commutative ).(Order does not matter)\nA\u2229B=B\u2229A, and A\u222aB=B\u222aA. (2.16)\nTheorem 2.3 (Associative ).(How to do multiple union and intersection)\nA\u222a(B\u222aC) = (A\u222aB)\u222aC,\nA\u2229(B\u2229C) = (A\u2229B)\u2229C. (2.17)\n56", "72": "2.1. SET THEORY\nTheorem 2.4 (Distributive ).(How to mix union and intersection)\nA\u2229(B\u222aC) = (A\u2229B)\u222a(A\u2229C),\nA\u222a(B\u2229C) = (A\u222aB)\u2229(A\u222aC). (2.18)\nTheorem 2.5 (De Morgan\u2019s Law ).(How to complement over intersection and union)\n(A\u2229B)c=Ac\u222aBc,\n(A\u222aB)c=Ac\u2229Bc. (2.19)\nExample 2.19 . Consider [1 ,4]\u2229([0,2]\u222a[3,5]). By the distributive property we can\nsimplify the set as\n[1,4]\u2229([0,2]\u222a[3,5]) = ([1 ,4]\u2229[0,2])\u222a([1,4]\u2229[3,5])\n= [1,2]\u222a[3,4].\nExample 2.20 . Consider ([0 ,1]\u222a[2,3])c. By De Morgan\u2019s Law we can rewrite the set\nas\n([0,2]\u222a[1,3])c= [0,2]c\u2229[1,3]c.\n2.1.10 Closing remarks about set theory\nIt should be apparent why set theory is useful: it shows us how to combine, split, and\nremove sets. In Figure 2.11 we depict the intersection of two sets A={even number }and\nB={less than or equal to 3 }. Set theory tells us how to define the intersection so that the\nprobability can be applied to the resulting set.\nFigure 2.11: When there are two events AandB, the probability of A\u2229Bis determined by first taking\nthe intersection of the two sets and then evaluating its probability.\nUniversal sets and empty sets are useful too. Universal sets cover all the possible\noutcomes of an experiment, so we should expect P[\u2126] = 1. Empty sets contain nothing,\nand so we should expect P[\u2205] = 0. These two properties are essential to define a probability\nbecause no probability can be greater than 1, and no probability can be less than 0.\n57", "73": "CHAPTER 2. PROBABILITY\n2.2 Probability Space\nWe now formally define probability. Our discussion will be based on the slogan probability\nis a measure of the size of a set . Three elements constitute a probability space :\n\u0088Sample Space \u2126: The set of all possible outcomes from an experiment.\n\u0088Event Space F: The collection of all possible events. An event Eis a subset in \u2126 that\ndefines an outcome or a combination of outcomes.\n\u0088Probability Law P: A mapping from an event Eto a number P[E] which, ideally,\nmeasures the size of the event.\nTherefore, whenever you talk about \u201cprobability,\u201d you need to specify the triplet (\u2126 ,F,P)\nto define the probability space.\nThe necessity of the three elements is illustrated in Figure 2.12 . The sample space\nis the interface with the physical world . It is the collection of all possible states that can\nresult from an experiment. Some outcomes are more likely to happen, and some are less\nlikely, but this does not matter because the sample space contains every possible outcome.\nTheprobability law is the interface with the data analysis . It is this law that defines the\nlikelihood of each of the outcomes. However, since the probability law measures the size of\na set, the probability law itself must be a function, a function whose argument is a set and\nwhose value is a number. An outcome in the sample space is not a set. Instead, a subset in\nthe sample space is a set. Therefore, the probability should input a subset and map it to a\nnumber. The collection of all possible subsets is the event space .\nFigure 2.12: Given an experiment, we define the collection of all outcomes as the sample space. A\nsubset in the sample space is called an event. The probability law is a mapping that maps an event to\na number that denotes the size of the event.\nA perceptive reader like you may be wondering why we want to complicate things to\nthis degree when calculating probability is trivial, e.g., throwing a die gives us a probability\n1\n6per face. In a simple world where problems are that easy, you can surely ignore all these\ncomplications and proceed to the answer1\n6. However, modern data analysis is not so easy.\nIf we are given an image of size 64 \u00d764 pixels, how do we tell whether this image is of a cat\nor a dog? We need to construct a probability model that tells us the likelihood of having a\n58", "74": "2.2. PROBABILITY SPACE\nparticular 64 \u00d764 image. What should be included in this probability model? We need to\nknow all the possible cases ( the sample space ), all the possible events ( the event space ),\nand the probability of each of the events ( the probability law ). If we know all these, then our\ndecision will be theoretically optimal. Of course, for high-dimensional data like images, we\nneed approximations to such a probability model. However, we first need to understand the\ntheoretical foundation of the probability space to know what approximations would make\nsense.\n2.2.1 Sample space \u2126\nWe start by defining the sample space \u2126. Given an experiment, the sample space \u2126 is the\nset containing all possible outcomes of the experiment.\nDefinition 2.13. Asample space \u2126is the set of all possible outcomes from an ex-\nperiment. We denote \u03beas an element in \u2126.\nA sample space can contain discrete outcomes or continuous outcomes, as shown in\nthe examples below and Figure 2.13 .\nExample 2.21 : (Discrete Outcomes)\n\u0088Coin flip: \u2126 = {H, T}.\n\u0088Throw a die: \u2126 = {\n,\n,\n,\n,\n,\n}.\n\u0088Paper / scissor / stone: \u2126 = {paper ,scissor ,stone}.\n\u0088Draw an even integer: \u2126 = {2,4,6,8, . . .}.\nExample 2.22 : (Continuous Outcomes)\n\u0088Waiting time for a bus in West Lafayette: \u2126 = {t|0\u2264t\u226430 minutes }.\n\u0088Phase angle of a voltage: \u2126 = {\u03b8|0\u2264\u03b8\u22642\u03c0}.\n\u0088Frequency of a pitch: \u2126 = {f|0\u2264f\u2264fmax}.\nFigure 2.13 also shows a functional example of the sample space. In this case, the\nsample space contains functions . For example,\n\u0088Set of all straight lines in 2D:\n\u2126 ={f|f(x) =ax+b, a, b \u2208R}.\n\u0088Set of all cosine functions with a phase offset:\n\u2126 ={f|f(t) = cos(2 \u03c0\u03c90t+ \u0398),0\u2264\u0398\u22642\u03c0}.\nAs we see from the above examples, the sample space is nothing but a universal set.\nThe elements inside the sample space are the outcomes of the experiment. If you change\n59", "75": "CHAPTER 2. PROBABILITY\nFigure 2.13: The sample space can take various forms: it can contain discrete numbers, or continuous\nintervals, or even functions.\nthe experiment, the possible outcomes will be different so that the sample space will be\ndifferent. For example, flipping a coin has different possible outcomes from throwing a die.\nWhat if we want to describe a composite experiment where we flip a coin and throw a\ndie? Here is the sample space:\nExample 2.23 : If the experiment contains flipping a coin and throwing a die, then\nthe sample space is\n\u001a\n(H,\n),(H,\n),(H,\n),(H,\n),(H,\n),(H,\n),\n(T,\n),(T,\n),(T,\n),(T,\n),(T,\n),(T,\n)\u001b\n.\nIn this sample space, each element is a pair of outcomes.\nPractice Exercise 2.8 . There are 8 processors on a computer. A computer job sched-\nuler chooses one processor randomly. What is the sample space? If the computer job\nscheduler can choose two processors at once, what is the sample space then?\nSolution . The sample space of the first case is \u2126 = {1,2,3,4,5,6,7,8}. The sample\nspace of the second case is \u2126 = {(1,2),(1,3),(1,4), . . . , (7,8)}.\nPractice Exercise 2.9 . A cell phone tower has a circular average coverage area of\nradius of 10 km. We observe the source locations of calls received by the tower. What\nis the sample space of all possible source locations?\nSolution . Assume that the center of the tower is located at ( x0, y0). The sample space\nis the set\n\u2126 ={(x, y)|p\n(x\u2212x0)2+ (y\u2212y0)2\u226410}.\nNot every set can be a sample space. A sample space must be exhaustive andexclusive .\nThe term \u201cexhaustive\u201d means that the sample space has to cover allpossible outcomes. If\n60", "76": "2.2. PROBABILITY SPACE\nthere is one possible outcome that is left out, then the set is no longer a sample space. The\nterm \u201cexclusive\u201d means that the sample space contains unique elements so that there is no\nrepetition of elements.\nExample 2.24 . (Counterexamples)\nThe following two examples are NOT sample spaces.\n\u0088Throw a die: \u2126 = {1,2,3}is not a sample space because it is not exhaustive .\n\u0088Throw a die: \u2126 = {1,1,2,3,4,5,6}is not a sample space because it is not exclu-\nsive.\nTherefore, a valid sample space must contain all possible outcomes, and each element\nmust be unique.\nWe summarize the concept of a sample space as follows.\nWhat is a sample space \u2126?\n\u0088A sample space \u2126 is the collection of all possible outcomes.\n\u0088The outcomes can be numbers, alphabets, vectors, or functions. The outcomes\ncan also be images, videos, EEG signals, audio speeches, etc.\n\u0088\u2126 must be exhaustive and exclusive.\n2.2.2 Event space F\nThe sample space contains all the possible outcomes. However, in many practical situations,\nwe are not interested in each of the individual outcomes; we are interested in the com-\nbinations of the outcomes. For example, when throwing a die, we may ask \u201cWhat is the\nprobability of rolling an odd number?\u201d or \u201cWhat is the probability of rolling a number that\nis less than 3?\u201d Clearly, \u201codd number\u201d is not an outcome of the experiment because the\npossible outcomes are {\n,\n,\n,\n,\n,\n}. We call \u201codd number\u201d an event . An event must\nbe a subset in the sample space.\nDefinition 2.14. Anevent Eis a subset in the sample space \u2126. The set of all possible\nevents is denoted as F.\nWhile this definition is extremely simple, we need to keep in mind a few facts about events.\nFirst, an outcome \u03beis an element in \u2126 but an event Eis a subset contained in \u2126, i.e.,\nE\u2286\u2126. Thus, an event can contain one outcome but it can also contain many outcomes.\nThe following example shows a few cases of events:\nExample 2.25 . Throw a die. Let \u2126 = {\n,\n,\n,\n,\n,\n}. The following are two pos-\nsible events, as illustrated in Figure 2.14 .\n\u0088E1={even numbers }={\n,\n,\n}.\n61", "77": "CHAPTER 2. PROBABILITY\n\u0088E2={less than 3 }={\n,\n}.\nFigure 2.14: Two examples of events: The first event contains numbers {2,4,6}, and the second\nevent contains numbers {1,2}.\nPractice Exercise 2.10 . The \u201cping\u201d command is used to measure round-trip times\nfor Internet packets. What is the sample space of all possible round-trip times? What\nis the event that a round-trip time is between 10 ms and 20 ms?\nSolution . The sample space is \u2126 = [0 ,\u221e). The event is E= [10 ,20].\nPractice Exercise 2.11 . A cell phone tower has a circular average coverage area of\nradius 10 km. We observe the source locations of calls received by the tower. What is\nthe event when the source location of a call is between 2 km and 5 km from the tower?\nSolution . Assume that the center of the tower is located at ( x0, y0). The event is\nE={(x, y)|2\u2264p\n(x\u2212x0)2+ (y\u2212y0)2\u22645}.\nThe second point we should remember is the cardinality of \u2126 and that of F. A sample\nspace containing nelements has a cardinality n. However, the event space constructed from\n\u2126 will contain 2nevents. To see why this is so, let\u2019s consider the following example.\nExample 2.26 . Consider an experiment with 3 outcomes \u2126 = {\u2663,\u2661,\u2720}. We can list\nout all the possible events: \u2205,{\u2663},{\u2661},{\u2720},{\u2663,\u2661},{\u2663,\u2720},{\u2661,\u2663},{\u2663,\u2661,\u2720}. So\nin total there are 23= 8 possible events. Figure 2.15 depicts the situation. What is\nthe difference between \u2663and{\u2663}? The former is an element, whereas the latter is a\nset. Thus, {\u2663}is an event but \u2663is not an event. Why is \u2205an event? Because we can\nask \u201cWhat is the probability that we get an odd number and an even number?\u201d The\nprobability is obviously zero, but the reason it is zero is that the event is an empty set.\n62", "78": "2.2. PROBABILITY SPACE\nFigure 2.15: The event space contains all the possible subsets inside the sample space.\nIn general, if there are nelements in the sample space, then the number of events\nis 2n. To see why this is true, we can assign to each element a binary value: either 0 or 1.\nFor example, in Table 2.1 we consider throwing a die. For each of the six faces, we assign a\nbinary code. This will give us a binary string for each event. For example, the event {\n,\n}\nis encoded as the binary string 100010 because only\n and\n are activated. We can count\nthe total number of unique strings, which is the number of strings that can be constructed\nfrom nbits. It is easily seen that this number is 2n.\nEvent\n Binary Code\n\u2205 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 000000\n{\n,\n} \u20dd \u00d7 \u00d7 \u00d7 \u20dd \u00d7 100010\n{\n,\n,\n} \u00d7 \u00d7 \u20dd \u20dd \u20dd \u00d7 001110\n............\n{\n,\n,\n,\n,\n}\u00d7 \u20dd \u20dd \u20dd \u20dd \u20dd 011111\n{\n,\n,\n,\n,\n,\n}\u20dd \u20dd \u20dd \u20dd \u20dd \u20dd 111111\nTable 2.1: An event space contains 2nevents, where nis the number of elements in the sample space.\nTo see this, we encode each outcome with a binary code. The resulting binary string then forms a unique\nindex of the event. Counting the total number of events gives us the cardinality of the event space.\nThe box below summarizes what you need to know about event spaces.\nWhat is an event space F?\n\u0088An event space Fis the set of all possible subsets. It is a set of sets.\n\u0088We need Fbecause the probability law Pis mapping a set to a number. Pdoes\nnot take an outcome from \u2126 but a subset inside \u2126.\n63", "79": "CHAPTER 2. PROBABILITY\nEvent spaces: Some advanced topics\nThe following discussions can be skipped if it is your first time reading the book.\nWhat else do we need to take care of in order to ensure that an event is well defined? A\nfew set operations seem to be necessary. For example, if E1={\n}andE2={\n}are events,\nit is necessary that E=E1\u222aE2={\n,\n}is an event too. Another example: if E1={\n,\n}\nandE2={\n,\n}are events, then it is necessary that E=E1\u2229E2={\n}is also an event.\nThe third example: if E1={\n,\n,\n,\n}is an event, then E=Ec\n1={\n,\n}should be\nan event. As you can see, there is nothing sophisticated in these examples. They are just\nsome basic set operations. We want to ensure that the event space is closed under these\nset operations. That is, we do not want to be surprised by finding that a set constructed\nfrom two events is not an event. However, since all set operations can be constructed from\nunion, intersection and complement, ensuring that the event space is closed under these\nthree operations effectively ensures that it is closed to allset operations.\nThe formal way to guarantee these is the notion of a field. This term may seem to be\nabstract, but it is indeed quite useful:\nDefinition 2.15. For an event space Fto be valid, Fmust be a fieldF. It is a field\nif it satisfies the following conditions\n\u0088\u2205 \u2208 F and\u2126\u2208 F.\n\u0088(Closed under complement) If F\u2208 F, then also Fc\u2208 F.\n\u0088(Closed under union and intersection) If F1\u2208 F andF2\u2208 F, then F1\u2229F2\u2208 F\nandF1\u222aF2\u2208 F.\nFor a finite set, i.e., a set that contains nelements, the collection of all possible subsets\nis indeed a field. This is not difficult to see if you consider rolling a die. For example,\nifE={\n,\n,\n,\n}is inside F, then Ec={\n,\n}is also inside F. This is because F\nconsists of 2nsubsets each being encoded by a unique binary string. So if E= 001111, then\nEc= 110000, which is also in F. Similar reasoning applies to intersection and union.\nAt this point, you may ask:\n\u0088Why bother constructing a field? The answer is that probability is a measure of the\nsize of a set, so we must input a set to a probability measure Pto get a number. The\nset being input to Pmust be a subset inside the sample space; otherwise, it will be\nundefined. If we regard Pas a mapping, we need to specify the collection of all its\ninputs, which is the set of all subsets, i.e., the event space. So if we do not define the\nfield, there is no way to define the measure P.\n\u0088What if the event space is not a field? If the event space is not a field, then we can\neasily construct pathological cases where we cannot assign a probability. For example,\nif the event space is not a field, then it would be possible that the complement of\nE={\n,\n,\n,\n}(which is Ec={\n,\n}) is not an event. This just does not make\nsense.\nThe concept of a field is sufficient for finite sample spaces. However, there are two\nother types of sample spaces where the concept of a field is inadequate. The first type of\n64", "80": "2.2. PROBABILITY SPACE\nsets consists of the countably infinite sets, and the second type consists of the sets defined\non the real line . There are other types of sets, but these two have important practical\napplications. Therefore, we need to have a basic understanding of these two types.\nSigma-field\nThe difficulty of a countably infinite set is that there are infinitely many subsets in the field\nof a countably infinite set. Having a finite union and a finite intersection is insufficient to\nensure the closedness of all intersections and unions. In particular, having F1\u222aF2\u2208 Fdoes\nnot automatically give usS\u221e\nn=1Fn\u2208 F because the latter is an infinite union. Therefore,\nfor countably infinite sets, their requirements to be a field are more restrictive as we need\nto ensure infinite intersection and union. The resulting field is called the \u03c3-field.\nDefinition 2.16. A sigma-field ( \u03c3-field )Fis a field such that\n\u0088Fis a field, and\n\u0088ifF1, F2, . . .\u2208 F, then the unionS\u221e\ni=1Fiand the intersectionT\u221e\ni=1Fiare both\ninF.\nWhen do we need a \u03c3-field? When the sample space is countable and has infinitely\nmany elements. For example, if the sample space contains all integers, then the collection\nof all possible subsets is a \u03c3-field. For another, if E1={2},E2={4},E3={6}, . . . , thenS\u221e\nn=1En={2,4,6,8, . . .}={positive even numbers }. Clearly, we wantS\u221e\nn=1Ento live in\nthe sample space.\nBorel sigma-field\nWhile a sigma-field allows us to consider countable sets of events, it is still insufficient for\nconsidering events defined on the real line, e.g., time, as these events are not countable.\nSo how do we define an event on the real line? It turns out that we need a different way\nto define the smallest unit . For finite sets and countable sets, the smallest units are the\nelements themselves because we can count them. For the real line, we cannot count the\nelements because any non-empty interval is uncountably infinite.\nThe smallest unit we use to construct a field for the real line is a semi-closed interval\n(\u2212\u221e, b]def={x| \u2212 \u221e < x\u2264b}.\nTheBorel \u03c3-field is defined as the sigma-field generated by the semi-closed inter-\nvals.\nDefinition 2.17. TheBorel \u03c3-fieldBis a\u03c3-field generated from semi-closed intervals:\n(\u2212\u221e, b]def={x| \u2212 \u221e < x\u2264b}.\nThe difference between the Borel \u03c3-fieldBand a regular \u03c3-field is how we measure the\nsubsets. In a \u03c3-field, we count the elements in the subsets, whereas, in a Borel \u03c3-field, we\nuse the semi-closed intervals to measure the subsets.\n65", "81": "CHAPTER 2. PROBABILITY\nBeing a field, the Borel \u03c3-field is closed under complement, union, and intersection. In\nparticular, subsets of the following forms are also in the Borel \u03c3-fieldB:\n(a, b),[a, b],(a, b],[a, b),[a,\u221e),(a,\u221e),(\u2212\u221e, b],{b}.\nFor example, ( a,\u221e) can be constructed from ( \u2212\u221e, a]c, and ( a, b] can be constructed by\ntaking the intersection of ( \u2212\u221e, b] and ( a,\u221e).\nExample 2.27 : Waiting for a bus. Let \u2126 = {0\u2264t\u226430}. The Borel \u03c3-field contains\nall semi-closed intervals ( a, b], where 0 \u2264a\u2264b\u226430. Here are two possible events:\n\u0088F1={less than 10 minutes }={0\u2264t <10}={0} \u222a({0< t\u226410} \u2229 {10}c).\n\u0088F2={more than 20 minutes }={20< t\u226430}.\nFurther discussion of the Borel \u03c3-field can be found in Leon-Garcia (3rd Edition,)\nChapter 2.9.\nThis is the end of the discussion. Please join us again.\n2.2.3 Probability law P\nThe third component of a probability space is the probability law P. Its job is to assign a\nnumber to an event.\nDefinition 2.18. Aprobability law is a function P:F \u2192 [0,1]of an event Eto a\nreal number in [0,1].\nThe probability law is thus a function , and therefore we must specify the input and\nthe output. The input to Pis an event E, which is a subset in \u2126 and an element in F. The\noutput of Pis a number between 0 and 1, which we call the probability .\nThe definition above does not specify how an event is being mapped to a number.\nHowever, since probability is a measure of the size of a set, a meaningful Pshould be\nconsistent for all events in F. This requires some rules, known as the axioms of probability ,\nwhen we define the P. Any probability law Pmust satisfy these axioms; otherwise, we will\nsee contradictions. We will discuss the axioms in the next section. For now, let us look at\ntwo examples to make sure we understand the functional nature of P.\nExample 2.28 . Consider flipping a coin. The event space is F={\u2205,{H},{T},\u2126}.\nWe can define the probability law as\nP[\u2205] = 0,P[{H}] =1\n2,P[{T}] =1\n2,P[\u2126] = 1 ,\nas shown in Figure 2.16 . This Pis clearly consistent for all the events in F.\nIs it possible to construct an invalid P? Certainly. Consider the following proba-\n66", "82": "2.2. PROBABILITY SPACE\nbility law:\nP[\u2205] = 0,P[{H}] =1\n3,P[{T}] =1\n3,P[\u2126] = 1 .\nThis law is invalid because the individual events are P[{H}] =1\n3andP[{T}] =1\n3\nbut the union is P[\u2126] = 1. To fix this problem, one possible solution is to define the\nprobability law as\nP[\u2205] = 0,P[{H}] =1\n3,P[{T}] =2\n3,P[\u2126] = 1 .\nThen, the probabilities for all the events are well defined and consistent.\nFigure 2.16: A probability law is a mapping from an event to a number. A probability law cannot be\narbitrarily assigned; it must satisfy the axioms of probability.\nExample 2.29 . Consider a sample space containing three elements \u2126 = {\u2663,\u2661,\u2720}.\nThe event space is then F=\u001a\n\u2205,{\u2663},{\u2661},{\u2720},{\u2663,\u2661},{\u2661,\u2720},{\u2663,\u2720},{\u2663,\u2661,\u2720}\u001b\n.\nOne possible Pwe could define would be\nP[\u2205] = 0,P[{\u2663}] =P[{\u2661}] =P[{\u2720}] =1\n3,\nP[{\u2663,\u2661}] =P[{\u2663,\u2720}] =P[{\u2661,\u2720}] =2\n3,P[{\u2663,\u2661,\u2720}] = 1.\nWhat is a probability law P?\n\u0088A probability law Pis afunction .\n\u0088It takes a subset (an element in F) and maps it to a number between 0 and 1.\n\u0088Pis ameasure of the size of a set.\n\u0088ForPto be valid, it must satisfy the axioms of probability .\n67", "83": "CHAPTER 2. PROBABILITY\nFigure 2.17: Probability is a measure of the size of a set. The probability can be a counter that counts\nthe number of elements, a ruler that measures the length of an interval, or an integration that measures\nthe area of a region.\nA probability law Pis a measure\nConsider the word \u201cmeasure\u201d in our slogan: probability is a measure of the size of a set .\nDepending on the nature of the set, the measure can be a counter, ruler, scale, or even a\nstopwatch. So far, all the examples we have seen are based on sets with a finite number of\nelements. For these sets, the natural choice of the probability measure is a counter. However,\nif the sets are intervals on the real line or regions in a plane, we need a different probability\nlaw to measure their size. Let\u2019s look at the examples shown in Figure 2.17 .\nExample 2.30 (Finite Set) . Consider throwing a die, so that\n\u2126 ={\n,\n,\n,\n,\n,\n}.\nThen the probability measure is a counter that reports the number of elements. If\nthe die is fair, i.e., all the 6 faces have equal probability of happening, then an event\nE={\n,\n}will have a probability P[E] =2\n6.\nExample 2.31 (Intervals) . Suppose that the sample space is a unit interval \u2126 = [0 ,1].\nLetEbe an event such that E= [a, b] where a, bare numbers in [0 ,1]. Then the\nprobability measure is a ruler that measures the length of the intervals. If all the\nnumbers on the real line have equal probability of appearing, then P[E] =b\u2212a.\nExample 2.32 (Regions) . Suppose that the sample space is the square \u2126 = [ \u22121,1]\u00d7\n[\u22121,1]. Let Ebe a circle such that E={(x, y)|x2+y2< r2}, where r <1. Then the\nprobability measure is an area measure that returns us the area of E. If we assume\nthat all coordinates in \u2126 are equally probable, then P[E] =\u03c0r2, forr <1.\nBecause probability is a measure of the size of a set, two sets can be compared according\nto their probability measures. For example, if \u2126 = {\u2663,\u2661,\u2720}, and if E1={\u2663} andE2=\n{\u2663,\u2661}, then one possible Pis to assign P[E1] =P[{\u2663}] =1\n3andP[E2] =P[{\u2663,\u2661}] = 2/3.\n68", "84": "2.2. PROBABILITY SPACE\nIn this particular case, we see that E1\u2286E2and thus\nP[E1]\u2264P[E2].\nLet\u2019s now consider the term \u201csize.\u201d Notice that the concept of the size of a set is not\nlimited to the number of elements. A better way to think about size is to imagine that it is\nthe weight of the set. This might may seem fanciful at first, but it is quite natural. Consider\nthe following example.\nExample 2.33 . (Discrete events with different weights ) Suppose we have a sample\nspace \u2126 = {\u2663,\u2661,\u2720}. Let us assign a different probability to each outcome:\nP[{\u2663}] =2\n6,P[{\u2661}] =1\n6,P[{\u2720}] =3\n6.\nAs illustrated in Figure 2.18 , since each outcome has a different weight, when de-\ntermining the probability of a set of outcomes we can add these weights (instead of\ncounting the number of outcomes). For example, when reporting P[{\u2663}] we find its\nweight P[{\u2663}] =2\n6, whereas when reporting P[{\u2661,\u2720}] we find the sum of their weights\nP[{\u2661,\u2720}] =1\n6+3\n6=4\n6. Therefore, the notion of size does not refer to the number of\nelements but to the total weight of these elements.\nFigure 2.18: This example shows the \u201cweights\u201d of three elements in a set. The weights are numbers\nbetween 0 and 1 such that the sum is 1. When applying a probability measure to this set, we sum the\nweights for the elements in the events being considered. For example, P[\u2661,\u2720] =yellow + green, and\nP[\u2663] =purple.\nExample 2.34 . (Continuous events with different weights ) Suppose that the sample\nspace is an interval, say \u2126 = [ \u22121,1]. On this interval we define a weighting function\nf(x) where f(x0) specifies the weight for x0. Because \u2126 is an interval, events defined\non this \u2126 must also be intervals. For example, we can consider two events E1= [a, b]\nandE2= [c, d]. The probabilities of these events are P[E1] =Rb\naf(x)dxandP[E2] =Rd\ncf(x)dx, as shown in Figure 2.19 .\nViewing probability as a measure is not just a game for mathematicians; rather, it\nhas fundamental significance for several reasons. First, it eliminates any dependency on\nprobability as relative frequency from the frequentist point of view. Relative frequency is a\n69", "85": "CHAPTER 2. PROBABILITY\nFigure 2.19: If the sample space is an interval on the real line, then the probability of an event is the\narea under the curve of the weighting function.\nnarrowly defined concept that is largely limited to discrete events, e.g., flipping a coin. While\nwe can assign weights to coin-toss events to deal with those biased coins, the extension to\ncontinuous events becomes problematic. By thinking of probability as a measure, we can\ngeneralize the notion to apply to intervals, areas, volumes, and so on.\nSecond, viewing probability as a measure forces us to disentangle an event from mea-\nsures . An event is a subset in the sample space. It has nothing to do with the measure\n(e.g., a ruler) you use to measure the event. The measure, on the other hand, specifies the\nweighting function you apply to measure the event when computing the probability. For\nexample, let \u2126 = [ \u22121,1] be an interval, and let E= [a, b] be an event. We can define two\nweighting functions f(x) and g(x). Correspondingly, we will have two different probability\nmeasures FandGsuch that\nF([a, b]) =Z\nEdF=Zb\naf(x)dx,\nG([a, b]) =Z\nEdG=Zb\nag(x)dx. (2.20)\nTo make sense of these notations, consider only P[[a, b]] and not F([a, b]) and G([a, b]). As you\ncan see, the event for both measures is E= [a, b] but the measures are different. Therefore,\nthe values of the probability are different.\nExample 2.35 . (Two probability laws are different if their weighting functions are\ndifferent .) Consider two different weighting functions for throwing a die. The first one\nassigns probability as the following:\nP[{\n}] =1\n12,P[{\n}] =2\n12,P[{\n}] =3\n12,\nP[{\n}] =4\n12,P[{\n}] =1\n12,P[{\n}] =1\n12,\nwhereas the second function assigns the probability like this:\nP[{\n}] =2\n12,P[{\n}] =2\n12,P[{\n}] =2\n12,\nP[{\n}] =2\n12,P[{\n}] =2\n12,P[{\n}] =2\n12.\n70", "86": "2.2. PROBABILITY SPACE\nLet an event E={\n,\n}. LetFbe the measure using the first set of probabilities, and\nletGbe the measure of the second set of probabilities. Then,\nF(E) =F({\n,\n}) =1\n12+2\n12=3\n12,\nG(E) =G({\n,\n}) =2\n12+2\n12=4\n12.\nTherefore, although the events are the same, the two different measures will give us\ntwo different probability values.\nRemark . The notationR\nEdFin Equation (2.20) is known as the Lebesgue integral . You\nshould be aware of this notation, but the theory of Lebesgue measure is beyond the scope\nof this book.\n2.2.4 Measure zero sets\nUnderstanding the measure perspective on probability allows us to understand another\nimportant concept of probability, namely measure zero sets . To introduce this concept, we\npose the question: What is the probability of obtaining a single point, say {0.5}, when the\nsample space is \u2126 = [0 ,1]?\nThe answer to this question is rooted in the compatibility between the measure and\nthe sample space. In other words, the measure has to be meaningful for the events in the\nsample space. Using \u2126 = [0 ,1], since \u2126 is an interval, an appropriate measure would be the\nlength of this interval. You may add different weighting functions to define your measure,\nbut ultimately, the measure must be an integral. If you use a \u201ccounter\u201d as a measure, then\nthe counter and the interval are not compatible because you cannot count on the real line.\nNow, suppose that we define a measure for \u2126 = [0 ,1] using a weighting function f(x).\nThis measure is determined by an integration. Then, for E={0.5}, the measure is\nP[E] =P[{0.5}] =Z0.5\n0.5f(x)dx= 0.\nIn fact, for any weighting function the integral will be zero because the length of the set\nEis zero.1An event that gives us zero probability is known as an event with measure 0 .\nFigure 2.20 shows an example.\nFigure 2.20: The probability of obtaining a single point in a continuous interval is zero.\n1We assume that fis continuous throughout [0 ,1]. If fis discontinuous at x= 0.5, some additional\nconsiderations will apply.\n71", "87": "CHAPTER 2. PROBABILITY\nWhat are measure zero sets?\n\u0088A set E(non-empty) is called a measure zero set when P[E] = 0.\n\u0088For example, {0}is a measure zero set when we use a continuous measure F.\n\u0088But{0}can have a positive measure when we use a discrete measure G.\nExample 2.36(a) . Consider a fair die with \u2126 = {\n,\n,\n,\n,\n,\n}. Then the set {\n}\nhas a probability of1\n6. The sample space does not have a measure zero event because\nthe measure we use is a counter.\nExample 2.36(b) . Consider an interval with \u2126 = [1 ,6]. Then the set {1}has measure\n0 because it is an isolated point with respect to the sample space.\nExample 2.36(c) . For any intervals, P[[a, b]] =P[(a, b)] because the two end points\nhave measure zero: P[{a}] =P[{b}] = 0.\nFormal definitions of measure zero sets\nThe following discussion of the formal definitions of measure zero sets is optional for the\nfirst reading of this book.\nWe can formally define measure zero sets as follows:\nDefinition 2.19. Let\u2126be the sample space. A set A\u2208\u2126is said to have measure\nzero if for any given \u03f5 >0,\n\u0088There exists a countable number of subsets Ansuch that A\u2286 \u222a\u221e\nn=1An, and\n\u0088P\u221e\nn=1P[An]< \u03f5.\nYou may need to read this definition carefully. Suppose we have an event A. We construct\na set of neighbors A1, . . . , A \u221esuch that Ais included in the union \u222a\u221e\nn=1An. If the sum of\nthe all P[An] is still less than \u03f5, then the set Awill have a measure zero.\nTo understand the difference between a measure for a continuous set and a countable\nset, consider Figure 2.21 . On the left side of Figure 2.21 we show an interval \u2126 in which there\nis an isolated point x0. The measure for this \u2126 is the length of the interval (relative to what-\never weighting function you use). We define a small neighborhood A0= (x0\u2212\u03f5\n2, x0+\u03f5\n2)\nsurrounding x0. The length of this interval is not more than \u03f5. We then shrink \u03f5. How-\never, regardless of how small \u03f5is, since x0is an isolated point, it is always included in the\nneighborhood. Therefore, the definition is satisfied, and so {x0}has measure zero.\nExample 2.37 . Let \u2126 = [0 ,1]. The set {0.5} \u2282\u2126 has measure zero, i.e., P[{0.5}] = 0.\nTo see this, we draw a small interval around 0.5, say [0 .5\u2212\u03f5/3,0.5 +\u03f5/3]. Inside this\ninterval, there is really nothing to measure besides the point 0.5. Thus we have found\nan interval such that it contains 0.5, and the probability is P[[0.5\u2212\u03f5/3,0.5 +\u03f5/3]] =\n72", "88": "2.2. PROBABILITY SPACE\n2\u03f5/3< \u03f5. Therefore, by definition, the set {0.5}has measure 0.\nThe situation is very different for the right-hand side of Figure 2.21 . Here, the measure\nis not the length but a counter. So if we create a neighborhood surrounding the isolated\npoint x0, we can always make a count. As a result, if you shrink \u03f5to become a very small\nnumber (in this case less than1\n4), then P[{x0}]< \u03f5will no longer be true. Therefore, the\nset{x0}has a non-zero measure when we use the counter as the measure.\nFigure 2.21: [Left] For a continuous sample space, a single point event {x0}can always be surrounded\nby a neighborhood A0whose size P[A0]< \u03f5. [Right] If you change the sample space to discrete\nelements, then a single point event {x0}can still be surrounded by a neighborhood A0. However, the\nsizeP[A0] = 1/4is a fixed number and will not work for any\u03f5.\nWhen we make probabilistic claims without considering the measure zero sets, we say\nthat an event happens almost surely .\nDefinition 2.20. An event A\u2208Ris said to hold almost surely (a.s.) if\nP[A] = 1 (2.21)\nexcept for all measure zero sets in R.\nTherefore, if a set Acontains measure zero subsets, we can simply ignore them because they\ndo not affect the probability of events. In this book, we will omit \u201ca.s.\u201d if the context is\nclear.\nExample 2.38(a) . Let \u2126 = [0 ,1]. Then P[(0,1)] = 1 almost surely because the points\n0 and 1 have measure zero in \u2126.\nExample 2.38(b) . Let \u2126 = {x|x2\u22641}and let A={x|x2<1}. Then P[A] = 1\nalmost surely because the circumference has measure zero in \u2126.\nPractice Exercise 2.12 . Let \u2126 = {f:R\u2192[\u22121,1]|f(t) = cos( \u03c90t+\u03b8)}, where \u03c90is\na fixed constant and \u03b8is random. Construct a measure zero event and an almost sure\nevent.\nSolution . Let\nE={f:R\u2192[\u22121,1]|f(t) = cos( \u03c90t+k\u03c0/2)}\nfor any integer k. That is, Econtains all the functions with a phase of \u03c0/2, 2\u03c0/2, 3\u03c0/2,\netc. Then Ewill have measure zero because it is a countable set of isolated functions.\nThe event Ecwill have probability P[Ec] = 1 almost surely because Ehas measure\n73", "89": "CHAPTER 2. PROBABILITY\nzero.\nThis is the end of the discussion. Please join us again.\n2.2.5 Summary of the probability space\nAfter the preceding long journey through theory, let us summarize.\nFirst, it is extremely important to understand our slogan: probability is a measure of\nthe size of a set . This slogan is precise, but it needs clarification. When we say probability\nis ameasure , we are thinking of it as being the probability law P. Of course, in practice, we\nalways think of probability as the number returned by the measure. However, the difference\nis not crucial. Also, \u201csize\u201d not only means the number of elements in the set, but it also\nmeans the relative weight of the set in the sample space. For example, if we use a weight\nfunction to weigh the set elements, then size would refer to the overall weight of the set.\nWhen we put all these pieces together, we can understand why a probability space\nmust consist of the three components\n(\u2126,F,P), (2.22)\nwhere \u2126 is the sample space that defines all possible outcomes, Fis the event space generated\nfrom \u2126, and Pis the probability law that maps an event to a number in [0 ,1]. Can we drop\none or more of the three components? We cannot! If we do not specify the sample space \u2126,\nthen there is no way to define the events. If we do not have a complete event space F,\nthen some events will become undefined, and further, if the probability law is applied only\nto outcomes, we will not be able to define the probability for events. Finally, if we do not\nspecify the probability law, then we do not have a way to assign probabilities.\n2.3 Axioms of Probability\nWe now turn to a deeper examination of the properties. Our motivation is simple. While\nthe definition of probability law has achieved its goal of assigning a probability to an event,\nthere must be restrictions on how the assignment can be made. For example, if we set\nP[{H}] = 1 /3, then P[{T}] must be 2 /3; otherwise, the sum of having a head and a tail\nwill be greater than 1. The necessary restrictions on assigning a probability to an event are\ncollectively known as the axioms of probability .\nDefinition 2.21. Aprobability law is a function P:F \u2192 [0,1]that maps an event\nAto a real number in [0,1]. The function must satisfy the axioms of probability :\nI.Non-negativity :P[A]\u22650, for any A\u2286\u2126.\nII.Normalization :P[\u2126] = 1 .\n74", "90": "2.3. AXIOMS OF PROBABILITY\nIII.Additivity : For any disjoint sets {A1, A2, . . .}, it must be true that\nP\"\u221e[\ni=1Ai#\n=\u221eX\ni=1P[Ai]. (2.23)\nAn axiom is a proposition that serves as a premise or starting point in a logical system.\nAxioms are not definitions, nor are they theorems. They are believed to be true or true\nwithin a certain context. In our case, the axioms are true within the context of Bayesian\nprobability. The Kolmogorov probability relies on another set of axioms. We will not dive\ninto the details of these historical issues; in this book, we will confine our discussion to the\nthree axioms given above.\n2.3.1 Why these three probability axioms?\nWhy do we need three axioms? Why not just two axioms? Why these three particular\naxioms? The reasons are summarized in the box below.\nWhy these three axioms?\n\u0088Axiom I (Non-negativity) ensures that probability is never negative.\n\u0088Axiom II (Normalization) ensures that probability is never greater than 1.\n\u0088Axiom III (Additivity) allows us to add probabilities when two events do not\noverlap.\nAxiom I is called the non-negativity axiom. It ensures that a probability value cannot\nbe negative. Non-negativity is a must for probability. It is meaningless to say that the\nprobability of getting an event is a negative number.\nAxiom II is called the normalization axiom. It ensures that the probability of observing\nall possible outcomes is 1. This gives the upper limit of the probability. The upper limit\ndoes not have to be 1. It could be 10 or 100. As long as we are consistent about this upper\nlimit, we are good. However, for historical reasons and convenience, we choose 1 to be the\nupper limit.\nAxiom III is called the additivity axiom and is the most critical one among the three.\nThe additivity axiom defines how set operations can be translated into probability oper-\nations. In a nutshell, it says that if we have a set of disjoint events, the probabilities can\nbe added. From the measure perspective, Axiom III makes sense because if Pmeasures the\nsize of an event, then two disjoint events should have their probabilities added. If two dis-\njoint events do not allow their probabilities to be added, then there is no way to measure\na combined event. Similarly, if the probabilities can somehow be added even for overlap-\nping events, there will be inconsistencies because there is no systematic way to handle the\noverlapping regions.\nThecountable additivity stated in Axiom III can be applied to both a finite number\nor an infinite number of sets. The finite case states that for any two disjoint sets AandB,\nwe have\nP[A\u222aB] =P[A] +P[B]. (2.24)\n75", "91": "CHAPTER 2. PROBABILITY\nIn other words, if AandBare disjoint, then the probability of observing either AorBis\nthe sum of the two individual probabilities. Figure 2.22 illustrates this idea.\nExample 2.39 . Let\u2019s see why Axiom III is critical. Consider throwing a fair die with\n\u2126 ={\n,\n,\n,\n,\n,\n}. The probability of getting {\n,\n}is\nP[{\n,\n}] =P[{\n} \u222a {\n }] =P[{\n}] +P[{\n}] =1\n6+1\n6=2\n6.\nIn this equation, the second equality holds because the events {\n}and{\n}are disjoint.\nIf we do not have Axiom III, then we cannot addprobabilities.\nFigure 2.22: Axiom III says P[A\u222aB] =P[A] +P[B]ifA\u2229B=\u2205.\n2.3.2 Axioms through the lens of measure\nAxioms are \u201crules\u201d we must abide by when we construct a measure. Therefore, any valid\nmeasure must be compatible with the axioms, regardless of whether we have a weighting\nfunction or not. In the following two examples, we will see how the weighting functions are\nused in the axioms.\nExample 2.40 . Consider a sample space with \u2126 = {\u2663,\u2661,\u2720}. The probability for\neach outcome is\nP[{\u2663}] =2\n6,P[{\u2661}] =1\n6,P[{\u2720}] =3\n6.\nSuppose we construct two disjoint events E1={\u2663,\u2661}andE2={\u2720}. Then Axiom\nIII says\nP[E1\u222aE2] =P[E1] +P[E2] =\u00122\n6+1\n6\u0013\n+3\n6= 1.\nNote that in this calculation, the measure Pis still a measure P. If we endow it\nwith a nonuniform weight function, then Papplies the corresponding weights to the\ncorresponding outcomes. This process is compatible with the axioms. See Figure 2.23\nfor a pictorial illustration.\n76", "92": "2.3. AXIOMS OF PROBABILITY\nExample 2.41 . Suppose the sample space is an interval \u2126 = [0 ,1]. The two events\nareE1= [a, b] and E2= [c, d]. Assume that the measure Puses a weighting function\nf(x). Then, by Axiom III, we know that\nP[E1\u222aE2] =P[E1] +P[E2]\n=P[[a, b]] +P[[c, d]] (by Axiom 3)\n=Zb\naf(x)dx+Zd\ncf(x)dx, (apply the measure) .\nAs you can see, there is no conflict between the axioms and the measure. Figure 2.24\nillustrates this example.\nFigure 2.23: Applying weighting functions to the measures: Suppose we have three elements in the set.\nTo compute the probability P[{\u2661,\u2720} \u222a {\u2663} ], we can write it as the sum of P[{\u2661,\u2720}]andP[{\u2663}].\nFigure 2.24: The axioms are compatible with the measure, even if we use a weighting function.\n2.3.3 Corollaries derived from the axioms\nThe union of AandBis equivalent to the logical operator \u201cOR\u201d. Once the logical operation\n\u201cOR\u201d is defined, all other logical operations can be defined. The following corollaries are\nexamples.\nCorollary 2.1. LetA\u2208 F be an event. Then,\n(a)P[Ac] = 1\u2212P[A].\n(b)P[A]\u22641.\n(c)P[\u2205] = 0.\n77", "93": "CHAPTER 2. PROBABILITY\nProof . (a) Since \u2126 = A\u222aAc, by finite additivity we have P[\u2126] =P[A\u222aAc] =P[A] +P[Ac].\nBy the normalization axiom, we have P[\u2126] = 1. Therefore, P[Ac] = 1\u2212P[A].\n(b) We prove by contradiction. Assume P[A]>1. Consider the complement Acwhere\nA\u222aAc= \u2126. Since P[Ac] = 1\u2212P[A], we must have P[Ac]<0 because by hypothesis P[A]>1.\nButP[Ac]<0 violates the non-negativity axiom. So we must have P[A]\u22641.\n(c) Since \u2126 = \u2126 \u222a \u2205, by the first corollary we have P[\u2205] = 1\u2212P[\u2126] = 0.\n\u25a1\nCorollary 2.2 (Unions of Two Non-Disjoint Sets ).For any AandBinF,\nP[A\u222aB] =P[A] +P[B]\u2212P[A\u2229B]. (2.25)\nThis statement is different from Axiom III because AandBare not necessarily disjoint.\nFigure 2.25: For any AandB,P[A\u222aB] =P[A] +P[B]\u2212P[A\u2229B].\nProof . First, observe that A\u222aBcan be partitioned into three disjoint subsets as A\u222aB=\n(A\\B)\u222a(A\u2229B)\u222a(B\\A). Since A\\B=A\u2229BcandB\\A=B\u2229Ac, by finite additivity we\nhave that\nP[A\u222aB] =P[A\\B] +P[A\u2229B] +P[B\\A] =P[A\u2229Bc] +P[A\u2229B] +P[B\u2229Ac]\n(a)=P[A\u2229Bc] +P[A\u2229B] +P[B\u2229Ac] +P[A\u2229B]\u2212P[A\u2229B]\n(b)=P[A\u2229(Bc\u222aB)] +P[(Ac\u222aA)\u2229B]\u2212P[A\u2229B]\n=P[A\u2229\u2126] +P[\u2126\u2229B]\u2212P[A\u2229B] =P[A] +P[B]\u2212P[A\u2229B],\nwhere in (a) we added and subtracted a term P[A\u2229B], and in (b) we used finite additivity\nso that P[A\u2229Bc] +P[A\u2229B] =P[(A\u2229Bc)\u222a(A\u2229B)] =P[A\u2229(Bc\u222aB)].\n\u25a1\nExample 2.42 . The corollary is easy to understand if we consider the following ex-\nample. Let \u2126 = {\n,\n,\n,\n,\n,\n}be the sample space of a fair die. Let A={\n,\n,\n}\nandB={\n,\n,\n}. Then\nP[A\u222aB] =P[{\n,\n,\n,\n,\n}] =5\n6.\n78", "94": "2.3. AXIOMS OF PROBABILITY\nWe can also use the corollary to obtain the same result:\nP[A\u222aB] =P[A] +P[B]\u2212P[A\u2229B]\n=P[{\n,\n,\n}] +P[{\n,\n,\n}]\u2212P[{\n}]\n=3\n6+3\n6\u22121\n6=5\n6.\nCorollary 2.3 (Inequalities) .LetAandBbe two events in F. Then,\n(a)P[A\u222aB]\u2264P[A] +P[B]. (Union Bound)\n(b) If A\u2286B, then P[A]\u2264P[B].\nProof . (a) Since P[A\u222aB] =P[A]+P[B]\u2212P[A\u2229B] and by non-negativity axiom P[A\u2229B]\u22650,\nwe must have P[A\u222aB]\u2264P[A] +P[B]. (b) If A\u2286B, then there exists a set B\\Asuch that\nB=A\u222a(B\\A). Therefore, by finite additivity we have P[B] =P[A]+P[B\\A]\u2265P[A]. Since\nP[B\\A]\u22650, it follows that P[A] +P[B\\A]\u2265P[A]. Thus we have P[B]\u2265P[A].\n\u25a1\nUnion bound is a frequently used tool for analyzing probabilities when the intersection\nA\u2229Bis difficult to evaluate. Part (b) is useful when considering two events of different\n\u201csizes.\u201d For example, in the bus-waiting example, if we let A={t\u22645}, and B={t\u226410},\nthenP[A]\u2264P[B] because we have to wait for the first 5 minutes to go into the remaining\n5 minutes.\nPractice Exercise 2.13 . Let the events AandBhaveP[A] =x,P[B] =yand\nP[A\u222aB] =z. Find the following probabilities: P[A\u2229B],P[Ac\u222aBc], and P[A\u2229Bc].\nSolution .\n(a) Note that z=P[A\u222aB] =P[A] +P[B]\u2212P[A\u2229B]. Thus, P[A\u2229B] =x+y\u2212z.\n(b) We can take the complement to obtain the result:\nP[Ac\u222aBc] = 1\u2212P[(Ac\u222aBc)c] = 1\u2212P[A\u2229B] = 1\u2212x\u2212y+z.\n(c)P[A\u2229Bc] =P[A]\u2212P[A\u2229B] =x\u2212(x+y\u2212z) =z\u2212y.\nPractice Exercise 2.14 . Consider a sample space\n\u2126 ={f:R\u2192R|f(x) =ax,for all a\u2208R, x\u2208R}.\nThere are two events: A={f|f(x) =ax, a \u22650}, and B={f|f(x) =ax, a \u22640}.\nSo, basically, Ais the set of all straight lines with positive slope, and Bis the set of\nstraight lines with negative slope. Show that the union bound is tight.\n79", "95": "CHAPTER 2. PROBABILITY\nSolution . First of all, we note that\nP[A\u222aB] =P[A] +P[B]\u2212P[A\u2229B].\nThe intersection is\nP[A\u2229B] =P[{f|f(x) = 0}].\nSince this is a point set in the real line, it has measure zero. Thus, P[A\u2229B] = 0 and\nhence P[A\u222aB] =P[A] +P[B]. So the union bound is tight.\nClosing remark . The development of today\u2019s probability theory is generally credited to\nAndrey Kolmogorov\u2019s 1933 book Foundations of the Theory of Probability . We close this\nsection by citing one of the tables of the book. The table summarizes the correspondence\nbetween set theory and random events.\nTheory of sets Random events\nAandBare disjoint, i.e., A\u2229B=\u2205Events AandBare incompatible\nA1\u2229A2\u00b7\u00b7\u00b7 \u2229 AN=\u2205 Events A1, . . . , A Nare incompatible\nA1\u2229A2\u00b7\u00b7\u00b7 \u2229 AN=X Event Xis defined as the simultaneous occur-\nrence of events A1, . . . , A N\nA1\u222aA2\u00b7\u00b7\u00b7 \u222a AN=X Event Xis defined as the occurrence of at least\none of the events A1, . . . , A N\nAcThe opposite event Acconsisting of the non-\noccurrence of event A\nA=\u2205 Event Ais impossible\nA= \u2126 Event Amust occur\nA1, . . . , A Nform a partition of \u2126 The experiment consists of determining which\nof the events A1, . . . , A Noccurs\nB\u2282A From the occurrence of event Bfollows the\ninevitable occurrence of A\nTable 2.2: Kolmogorov\u2019s summary of set theory results and random events.\n2.4 Conditional Probability\nIn many practical data science problems, we are interested in the relationship between two\nor more events. For example, an event Amay cause Bto happen, and Bmay cause C\nto happen. A legitimate question in probability is then: If Ahas happened, what is the\nprobability that Balso happens? Of course, if AandBare correlated events, then knowing\none event can tell us something about the other event. If the two events have no relationship,\nknowing one event will not tell us anything about the other.\nIn this section, we study the concept of conditional probability . There are three sub-\ntopics in this section. We summarize the key points below.\n80", "96": "2.4. CONDITIONAL PROBABILITY\nThe three main messages of this section are:\n\u0088Section 2.4.1: Conditional probability . Conditional probability of Agiven Bis\nP[A|B] =P[A\u2229B]\nP[B].\n\u0088Section 2.4.2: Independence . Two events are independent if the occurrence of\none does not influence the occurrence of the other: P[A|B] =P[A].\n\u0088Section 2.4.3: Bayes\u2019 theorem and the law of total probability . Bayes\u2019 theorem\nallows us to switch the order of the conditioning: P[A|B] vs.P[B|A], whereas the\nlaw of total probability allows us to decompose an event into smaller events.\n2.4.1 Definition of conditional probability\nWe start by defining conditional probability .\nDefinition 2.22. Consider two events AandB. Assume P[B]\u0338= 0. The conditional\nprobability ofAgiven Bis\nP[A|B]def=P[A\u2229B]\nP[B]. (2.26)\nAccording to this definition, the conditional probability of Agiven Bis the ratio of\nP[A\u2229B] toP[B]. It is the probability that Ahappens when we know that Bhas already\nhappened. Since Bhas already happened, the event that Ahas also happened is represented\nbyA\u2229B. However, since we are only interested in the relative probability of Awith respect\ntoB, we need to normalize using B. This can be seen by comparing P[A|B] andP[A\u2229B]:\nP[A|B] =P[A\u2229B]\nP[B]andP[A\u2229B] =P[A\u2229B]\nP[\u2126]. (2.27)\nThe difference is illustrated in Figure 2.26 : The intersection P[A\u2229B] calculates the overlap-\nping area of the two events. We make no assumptions about the cause-effect relationship.\nFigure 2.26: Illustration of conditional probability and its comparison with P[A\u2229B].\nWhat justifies this ratio? Suppose that Bhas already happened. Then, anything out-\nsideBwill immediately become irrelevant as far as the relationship between AandBis\nconcerned. So when we ask: \u201cWhat is the probability that Ahappens given that Bhas\nhappened?\u201d, we are effectively asking for the probability that A\u2229Bhappens under the\n81", "97": "CHAPTER 2. PROBABILITY\ncondition that Bhas happened. Note that we need to consider A\u2229Bbecause we know\nthat Bhas already happened. If we take Aonly, then there exists a region A\\Bwhich\ndoes not contain anything about B. However, since we know that Bhas happened, A\\Bis\nimpossible. In other words, among the elements of A, only those that appear in A\u2229Bare\nmeaningful.\nExample 2.43 . Let\nA={Purdue gets Big Ten championship },\nB={Purdue wins 15 games consecutively }.\nIn this example,\nP[A] = Prob. that Purdue gets the championship ,\nP[B] = Prob. that Purdue wins 15 games consecutively ,\nP[A\u2229B] = Prob. that Purdue gets the championship and wins 15 games ,\nP[A|B] = Prob. that Purdue gets the championship given that\nPurdue won 15 games .\nIf Purdue has won 15 games consecutively, then it is unlikely that Purdue will get\nthe championship because the sample space of all possible competition results is large.\nHowever, if we have already won 15 games consecutively, then the denominator of the\nprobability becomes much smaller. In this case, the conditional probability is high.\nExample 2.44 . Consider throwing a die. Let\nA={getting a 3 }and B={getting an odd number }.\nFindP[A|B] andP[B|A].\nSolution . The following probabilities are easy to calculate:\nP[A] =P[{\n}] =1\n6, and P[B] =P[{\n,\n,\n}] =3\n6.\nAlso, the intersection is\nP[A\u2229B] =P[{\n}] =1\n6.\nGiven these values, the conditional probability of Agiven Bcan be calculated as\nP[A|B] =P[A\u2229B]\nP[B]=1\n6\n3\n6=1\n3.\nIn other words, if we know that we have an odd number, then the probability of\nobtaining a 3 has to be computed over {\n,\n,\n}, which give us a probability1\n3. If we\n82", "98": "2.4. CONDITIONAL PROBABILITY\ndo not know that we have an odd number, then the probability of obtaining a 3 has\nto be computed from the sample space {\n,\n,\n,\n,\n,\n}, which will give us1\n6.\nThe other conditional probability is\nP[B|A] =P[A\u2229B]\nP[A]= 1.\nTherefore, if we know that we have rolled a 3, then the probability for this number\nbeing an odd number is 1.\nExample 2.45 . Consider the situation shown in Figure 2.27 . There are 12 points\nwith equal probabilities of happening. Find the probabilities P[A|B] andP[B|A].\nSolution . In this example, we can first calculate the individual probabilities:\nP[A] =5\n12,andP[B] =6\n12,andP[A\u2229B] =2\n12.\nThen the conditional probabilities are\nP[A|B] =P[A\u2229B]\nP[B]=2\n12\n6\n12=1\n3,\nP[B|A] =P[A\u2229B]\nP[A]=2\n12\n5\n12=2\n5.\nFigure 2.27: Visualization of Example 2.45: [Left] All the sets. [Middle] P(A|B)is the ratio between\ndots inside the light yellow region over those in yellow, which is2\n6. [Right] P[A|B]is the ratio between\ndots inside the light pink region over those in pink, which is2\n5.\nExample 2.46 . Consider a tetrahedral (4-sided) die. Let Xbe the first roll and Y\nbe the second roll. Let Bbe the event that min( X, Y) = 2 and Mbe the event that\nmax( X, Y) = 3. Find P[M|B].\nSolution . As shown in Figure 2.28 , the event Bis highlighted in green. (Why?)\nSimilarly, the event Mis highlighted in blue. (Again, why?) Therefore, the probability\n83", "99": "CHAPTER 2. PROBABILITY\nis\nP[M|B] =P[M\u2229B]\nP[B]=2\n16\n5\n16=2\n5.\nFigure 2.28: Visualization of Example 2.46. [Left] Event B. [Middle] Event M. [Right] P(M|B)is the\nratio of the number of blue squares inside the green region to the total number of green squares, which\nis2\n5.\nRemark . Notice that if P[B]\u2264P[\u2126], then P[A|B] is always larger than or equal to P[A\u2229B],\ni.e.,\nP[A|B]\u2265P[A\u2229B].\nConditional probabilities are legitimate probabilities\nConditional probabilities are legitimate probabilities. That is, given B, the probability\nP[A|B] satisfies Axioms I, II, III.\nTheorem 2.6. LetP[B]>0. The conditional probability P[A|B]satisfies Axioms I,\nII, and III.\nProof . Let\u2019s check the axioms:\n\u0088Axiom I: We want to show\nP[A|B] =P[A\u2229B]\nP[B]\u22650.\nSinceP[B]>0 and Axiom I requires P[A\u2229B]\u22650, we therefore have P[A|B]\u22650.\n\u0088Axiom II:\nP[\u2126|B] =P[\u2126\u2229B]\nP[B]\n=P[B]\nP[B]= 1.\n84", "100": "2.4. CONDITIONAL PROBABILITY\n\u0088Axiom III: Consider two disjoint sets AandC. Then,\nP[A\u222aC|B] =P[(A\u222aC)\u2229B]\nP[B]\n=P[(A\u2229B)\u222a(C\u2229B)]\nP[B]\n(a)=P[A\u2229B]\nP[B]+P[C\u2229B]\nP[B]\n=P[A|B] +P[C|B],\nwhere ( a) holds because if AandCare disjoint then ( A\u2229B)\u2229(C\u2229B) =\u2205.\n\u25a1\nTo summarize this subsection, we highlight the essence of conditional probability.\nWhat are conditional probabilities?\n\u0088Conditional probability of Agiven Bis the ratioP[A\u2229B]\nP[B].\n\u0088It is again a measure . It measures the relative size of Ainside B.\n\u0088Because it is a measure, it must satisfy the three axioms.\n2.4.2 Independence\nConditional probability deals with situations where two events AandBare related. What\nif the two events are unrelated? In probability, we have a technical term for this situation:\nstatistical independence .\nDefinition 2.23. Two events AandBare statistically independent if\nP[A\u2229B] =P[A]P[B]. (2.28)\nWhy define independence in this way? Recall that P[A|B] =P[A\u2229B]\nP[B]. IfAandBare\nindependent, then P[A\u2229B] =P[A]P[B] and so\nP[A|B] =P[A\u2229B]\nP[B]=P[A]P[B]\nP[B]=P[A]. (2.29)\nThis suggests an interpretation of independence: If the occurrence of Bprovides no addi-\ntional information about the occurrence of A, then AandBare independent.\nTherefore, we can define independence via conditional probability:\nDefinition 2.24. LetAandBbe two events such that P[A]>0andP[B]>0. Then\n85", "101": "CHAPTER 2. PROBABILITY\nAandBareindependent if\nP[A|B] =P[A]orP[B|A] =P[B]. (2.30)\nThe two statements are equivalent as long as P[A]>0 and P[B]>0. This is because\nP[A|B] =P[A\u2229B]/P[B]. IfP[A|B] =P[A] then P[A\u2229B] =P[A]P[B], which implies that\nP[B|A] =P[A\u2229B]/P[A] =P[B].\nA pictorial illustration of independence is given in Figure 2.29 . The key message is that\nif two events AandBare independent, then P[A|B] =P[A]. The conditional probability\nP[A|B] is the ratio of P[A\u2229B] over P[B], which is the intersection over B(the blue set).\nThe probability P[A] is the yellow set over the sample space \u2126.\nFigure 2.29: Independence means that the conditional probability P[A|B]is the same as P[A]. This\nimplies that the ratio of P[A\u2229B]overP[B], and the ratio of P[A\u2229\u2126]overP[\u2126]are the same.\nDisjoint versus independent\nDisjoint \u21ceIndependent . (2.31)\nThe statement says that disjoint and independent are two completely different concepts.\nIfAandBare disjoint, then A\u2229B=\u2205. This only implies that P[A\u2229B] = 0.\nHowever, it says nothing about whether P[A\u2229B] can be factorized into P[A]P[B]. IfA\nandBare independent, then we have P[A\u2229B] =P[A]P[B]. But this does not imply that\nP[A\u2229B] = 0. The only condition under which Disjoint \u21d4Independence is when P[A] = 0 or\nP[B] = 0. Figure 2.30 depicts the situation. When two sets are independent, the conditional\nprobability (which is a ratio) remains unchanged compared to unconditioned probability.\nWhen two sets are disjoint, they simply do not overlap.\nPractice Exercise 2.15 . Throw a die twice. Are AandBindependent, where\nA={1st die is 3 }and B={2nd die is 4 }.\nSolution . We can show that\nP[A\u2229B] =P[(3,4)] =1\n36,P[A] =1\n6,andP[B] =1\n6.\nSoP[A\u2229B] =P[A]P[B]. Thus, AandBare independent.\n86", "102": "2.4. CONDITIONAL PROBABILITY\nFigure 2.30: Independent means that the conditional probability, which is a ratio, is the same as the\nunconditioned probability. Disjoint means that the two sets do not overlap.\nFigure 2.31: The two events AandBare independent because P[A] =1\n6andP[A|B] =1\n6.\nA pictorial illustration of this example is shown in Figure 2.31 . The two events are\nindependent because Ais one row in the 2D space, which yields a probability of1\n6. The\nconditional probability P[A|B] is the coordinate (3 ,4) over the event B, which is a column.\nIt happens that P[A|B] =1\n6. Thus, the two events are independent.\nPractice Exercise 2.16 . Throw a die twice. Are AandBindependent?\nA={1st die is 3 }and B={sum is 7 }.\nSolution . Note that\nP[A\u2229B] =P[(3,4)] =1\n36, P[A] =1\n6,\nP[B] =P[(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)] =1\n6.\nSoP[A\u2229B] =P[A]P[B]. Thus, AandBare independent.\nA pictorial illustration of this example is shown in Figure 2.32 . Notice that whether the\ntwo events intersect is not how we determine independence (that only determines disjoint or\n87", "103": "CHAPTER 2. PROBABILITY\nnot). The key is whether the conditional probability (which is the ratio) remains unchanged\ncompared to the unconditioned probability.\nFigure 2.32: The two events AandBare independent because P[A] =1\n6andP[A\u2229B] =1\n6.\nIf we let B={sum is 8 }, then the situation is different. The intersection A\u2229Bhas a\nprobability1\n5relative to B, and therefore P[A|B] =1\n5. Hence, the two events AandBare\ndependent. If you like a more intuitive argument, you can imagine that Bhas happened,\ni.e., the sum is 8. Then the probability for the first die to be 1 is 0 because there is no way\nto construct 8 when the first die is 1. As a result, we have eliminated one choice for the first\ndie, leaving only five options. Therefore, since Bhas influenced the probability of A, they\nare dependent.\nPractice Exercise 2.17 . Throw a die twice. Let\nA={max is 2 }and B={min is 2 }.\nAreAandBindependent?\nSolution . Let us first list out AandB:\nA={(1,2),(2,1),(2,2)},\nB={(2,2),(2,3),(2,4),(2,5),(2,6),(3,2),(4,2),(5,2),(6,2)}.\nTherefore, the probabilities are\nP[A] =3\n36,P[B] =9\n36,andP[A\u2229B] =P[(2,2)] =1\n36.\nClearly, P[A\u2229B]\u0338=P[A]P[B] and so AandBare dependent.\nWhat is independence?\n\u0088Two events are independent when the ratioP[A\u2229B]/P[B]remains unchanged\ncompared to P[A].\n\u0088Independence \u0338= disjoint.\n88", "104": "2.4. CONDITIONAL PROBABILITY\n2.4.3 Bayes\u2019 theorem and the law of total probability\nTheorem 2.7 (Bayes\u2019 theorem ).For any two events AandBsuch that P[A]>0\nandP[B]>0,\nP[A|B] =P[B|A]P[A]\nP[B].\nProof . By the definition of conditional probabilities, we have\nP[A|B] =P[A\u2229B]\nP[B]andP[B|A] =P[B\u2229A]\nP[A].\nRearranging the terms yields\nP[A|B]P[B] =P[B|A]P[A],\nwhich gives the desired result by dividing both sides by P[B].\n\u25a1\nBayes\u2019 theorem provides two views of the intersection P[A\u2229B] using two different con-\nditional probabilities. We call P[B|A] the conditional probability andP[A|B] the posterior\nprobability . The order of AandBis arbitrary. We can also call P[A|B] the conditional\nprobability and P[B|A] the posterior probability. The context of the problem will make this\nclear.\nBayes\u2019 theorem provides a way to switch P[A|B] andP[B|A]. The next theorem helps\nus decompose an event into smaller events.\nTheorem 2.8 (Law of Total Probability ).Let{A1, . . . , A n}be a partition of \u2126, i.e.,\nA1, . . . , A nare disjoint and \u2126 =A1\u222a \u00b7\u00b7\u00b7 \u222a An. Then, for any B\u2286\u2126,\nP[B] =nX\ni=1P[B|Ai]P[Ai]. (2.32)\nProof . We start from the right-hand side.\nnX\ni=1P[B|Ai]P[Ai](a)=nX\ni=1P[B\u2229Ai](b)=P\"n[\ni=1(B\u2229Ai)#\n(c)=P\"\nB\u2229 n[\ni=1Ai!#\n(d)=P[B\u2229\u2126] =P[B],\nwhere ( a) follows from the definition of conditional probability, ( b) is due to Axiom III, ( c)\nholds because of the distributive property of sets, and ( d) results from the partition property\nof{A1, A2, . . . , A n}.\n\u25a1\nInterpretation . The law of total probability can be understood as follows. If the sample\nspace \u2126 consists of disjoint subsets A1, . . . , A n, we can compute the probability P[B] by\n89", "105": "CHAPTER 2. PROBABILITY\nsumming over its portion P[B\u2229A1], . . . ,P[B\u2229An]. However, each intersection can be written\nas\nP[B\u2229Ai] =P[B|Ai]P[Ai]. (2.33)\nIn other words, we write P[B\u2229Ai] as the conditional probability P[B|Ai] times the prior\nprobability P[Ai]. When we sum all these intersections, we obtain the overall probability.\nSeeFigure 2.33 for a graphical portrayal.\nFigure 2.33: The law of total probability decomposes the probability P[B]into multiple conditional\nprobabilities P[B|Ai]. The probability of obtaining each P[B|Ai]isP[Ai].\nCorollary 2.4. Let{A1, A2, . . . , A n}be a partition of \u2126, i.e., A1, . . . , A nare disjoint\nand\u2126 =A1\u222aA2\u222a \u00b7\u00b7\u00b7 \u222a An. Then, for any B\u2286\u2126,\nP[Aj|B] =P[B|Aj]P[Aj]Pn\ni=1P[B|Ai]P[Ai]. (2.34)\nProof . The result follows directly from Bayes\u2019 theorem:\nP[Aj|B] =P[B|Aj]P[Aj]\nP[B]=P[B|Aj]P[Aj]Pn\ni=1P[B|Ai]P[Ai].\n\u25a1\nExample 2.47 . Suppose there are three types of players in a tennis tournament: A,\nB, and C. Fifty percent of the contestants in the tournament are Aplayers, 25% are\nBplayers, and 25% are Cplayers. Your chance of beating the contestants depends on\nthe class of the player, as follows:\n0.3 against an Aplayer\n0.4 against a Bplayer\n0.5 against a Cplayer\nIf you play a match in this tournament, what is the probability of your winning the\nmatch? Supposing that you have won a match, what is the probability that you played\nagainst an Aplayer?\nSolution . We first list all the known probabilities. We know from the percentage\n90", "106": "2.4. CONDITIONAL PROBABILITY\nof players that\nP[A] = 0.5,P[B] = 0.25,P[C] = 0.25.\nNow, let Wbe the event that you win the match. Then the conditional probabilities\nare defined as follows:\nP[W|A] = 0.3,P[W|B] = 0.4,P[W|C] = 0.5.\nTherefore, by the law of total probability, we can show that the probability of\nwinning the match is\nP[W] =P[W|A]P[A] +P[W|B]P[B] +P[W|C]P[C]\n= (0.3)(0.5) + (0 .4)(0.25) + (0 .5)(0.25) = 0 .375.\nGiven that you have won the match, the probability of Agiven Wis\nP[A|W] =P[W|A]P[A]\nP[W]=(0.3)(0.5)\n0.375= 0.4.\nExample 2.48 . Consider the communication channel shown below. The probability\nof sending a 1 is pand the probability of sending a 0 is 1 \u2212p. Given that 1 is sent, the\nprobability of receiving 1 is 1 \u2212\u03b7. Given that 0 is sent, the probability of receiving 0\nis 1\u2212\u03b5. Find the probability that a 1 has been correctly received.\nSolution . Define the events\nS0= \u201c0 is sent\u201d ,and R0= \u201c0 is received\u201d .\nS1= \u201c1 is sent\u201d ,and R1= \u201c1 is received\u201d .\nThen, the probability that 1 is received is P[R1]. However, P[R1]\u0338= 1\u2212\u03b7because 1 \u2212\u03b7\n91", "107": "CHAPTER 2. PROBABILITY\nis the conditional probability that 1 is received given that 1 is sent. It is possible that\nwe receive 1 as a result of an error when 0 is sent. Therefore, we need to consider the\nprobability that both S0andS1occur. Using the law of total probability we have\nP[R1] =P[R1|S1]P[S1] +P[R1|S0]P[S0]\n= (1\u2212\u03b7)p+\u03b5(1\u2212p).\nNow, suppose that we have received 1. What is the probability that 1 was origi-\nnally sent? This is asking for the posterior probability P[S1|R1], which can be found\nusing Bayes\u2019 theorem\nP[S1|R1] =P[R1|S1]P[S1]\nP[R1]=(1\u2212\u03b7)p\n(1\u2212\u03b7)p+\u03b5(1\u2212p).\nWhen do we need to use Bayes\u2019 theorem and the law of total probability?\n\u0088Bayes\u2019 theorem switches the role of the conditioning, from P[A|B] toP[B|A].\nExample:\nP[win the game |play with A] and P[play with A |win the game] .\n\u0088The law of total probability decomposes an event into smaller events.\nExample:\nP[win] = P[win|A]P[A] +P[win|B]P[B].\n2.4.4 The Three Prisoners problem\nNow that you are familiar with the concepts of conditional probabilities, we would like to\nchallenge you with the following problem, known as the Three Prisoners problem . If you\nunderstand how this problem can be resolved, you have mastered conditional probability.\nOnce upon a time, there were three prisoners A,B, and C. One day, the king decided\nto pardon two of them and sentence the last one, as in this figure:\nFigure 2.34: The Three Prisoners problem: The king says that he will pardon two prisoners and sentence\none.\nOne of the prisoners, prisoner A, heard the news and wanted to ask a friendly guard\nabout his situation. The guard was honest. He was allowed to tell prisoner Athat prisoner B\nwould be pardoned or that prisoner Cwould be pardoned, but he could not tell Awhether\nhe would be pardoned. Prisoner Athought about the problem, and he began to hesitate to\nask the guard. Based on his present state of knowledge, his probability of being pardoned\n92", "108": "2.4. CONDITIONAL PROBABILITY\nis2\n3. However, if he asks the guard, this probability will be reduced to1\n2because the guard\nwould tell him that one of the two other prisoners would be pardoned, and would tell him\nwhich one it would be. Prisoner A reasons that his chance of being pardoned would then\ndrop because there are now only two prisoners left who may be pardoned, as illustrated in\nFigure 2.35 :\nFigure 2.35: The Three Prisoners problem: If you do not ask the guard, your chance of being released\nis 2/3. If you ask the guard, the guard will tell you which one of the other prisoners will be released.\nYour chance of being released apparently drops to 1/2.\nShould prisoner Aask the guard? What has gone wrong with his reasoning? This\nproblem is tricky in the sense that the verbal argument of prisoner Aseems flawless. If\nhe asked the guard, indeed, the game would be reduced to two people. However, this does\nnot seem correct, because regardless of what the guard says, the probability for Ato be\npardoned should remain unchanged. Let\u2019s see how we can solve this puzzle.\nLetXA,XB,XCbe the events of sentencing prisoners A,B,C, respectively. Let GB\nbe the event that the guard says that the prisoner Bis released. Without doing anything,\nwe know that\nP[XA] =1\n3,P[XB] =1\n3,P[XC] =1\n3.\nConditioned on these events, we can compute the following conditional probabilities that\nthe guard says Bis pardoned:\nP[GB|XA] =1\n2,P[GB|XB] = 0,P[GB|XC] = 1.\nWhy are these conditional probabilities? P[GB|XB] = 0 quite straightforward. If the king\ndecides to sentence B, the guard has no way of saying that Bwill be pardoned. Therefore,\nP[GB|XB] must be zero. P[GB|XC] = 1 is also not difficult. If the king decides to\nsentence C, then the guard has no way to tell you that Bwill be pardoned because the\nguard cannot say anything about prisoner A. Finally, P[GB|XA] =1\n2can be understood\nas follows: If the king decides to sentence A, the guard can either tell you BorC. In other\nwords, the guard flips a coin.\nWith these conditional probabilities ready, we can determine the probability. This is the\nconditional probability P[XA|GB]. That is, supposing that the guard says Bis pardoned,\nwhat is the probability that Awill be sentenced? This is the actual scenario that Ais facing.\nSolving for this conditional probability is not difficult. By Bayes\u2019 theorem we know that\nP[XA|GB] =P[GB|XA]P[XA]\nP[GB],\n93", "109": "CHAPTER 2. PROBABILITY\nandP[GB] =P[GB|XA]P[XA] +P[GB|XB]P[XB] +P[GB|XC]P[XC] according to the law of\ntotal probability. Substituting the numbers into these equations, we have that\nP[GB] =P[GB|XA]P[XA] +P[GB|XB]P[XB] +P[GB|XC]P[XC]\n=1\n2\u00d71\n3+ 0\u00d71\n3+ 1\u00d71\n3=1\n2,\nP[XA|GB] =P[GB|XA]P[XA]\nP[GB]=1\n2\u00d71\n3\n1\n2=1\n3.\nTherefore, given that the guard says Bis pardoned, the probability that Awill be sentenced\nremains1\n3. In fact, what you can show in this example is that P[XA|GB] =1\n3=P[XA].\nTherefore, the presence or absence of the guard does not alter the probability. This is because\nwhat the guard says is independent of whether the prisoners will be pardoned. The lesson\nwe learn from this problem is not to rely on verbal arguments. We need to write down the\nconditional probabilities and spell out the steps.\nFigure 2.36: The Three Prisoners problem is resolved by noting that P[XA|GB] =P[XA]. Therefore,\nthe events XAandGBare independent.\nHow to resolve the Three Prisoners problem?\n\u0088The key is that GA,GB,GCdo not form a partition . See Figure 2.36 .\n\u0088GB\u0338=XB. When GBhappens, the remaining set is not XA\u222aXC.\n\u0088The ratio P[XA\u2229GB]/P[GB] equals P[XA]. This is independence .\n94", "110": "2.5. SUMMARY\n2.5 Summary\nBy now, we hope that you have become familiar with our slogan probability is a measure\nof the size of a set . Let us summarize:\n\u0088Probability = a probability law P. You can also view it as the value returned by P.\n\u0088Measure = a ruler, a scale, a stopwatch, or another measuring device. It is a tool that\ntells you how large or small a set is. The measure has to be compatible with the set.\nIf a set is finite, then the measure can be a counter. If a set is a continuous interval,\nthen the measure can be the length of the interval.\n\u0088Size = the relative weight of the set for the sample space. Measuring the size is done\nby using a weighting function. Think of a fair coin versus a biased coin. The former\nhas a uniform weight, whereas the latter has a nonuniform weight.\n\u0088Set= an event. An event is a subset in the sample space. A probability law Palways\nmaps a setto a number. This is different from a typical function that maps a number\nto another number.\nIf you understand what this slogan means, you will understand why probability can be\napplied to discrete events, continuous events, events in n-D spaces, etc. You will also under-\nstand the notion of measure zero and the notion of almost sure . These concepts lie at the\nfoundation of modern data science, in particular, theoretical machine learning.\nThe second half of this chapter discusses the concept of conditional probability . Con-\nditional probability is a metaconcept that can be applied to any measure you use. The\nmotivation of conditional probability is to restrict the probability to a subevent happening\nin the sample space. If Bhas happened, the probability for Atoalsohappen is P[A\u2229B]/P[B].\nIf two events are not influencing each other, then we say that AandBare independent.\nAccording to Bayes\u2019 theorem, we can also switch the order of Agiven BandBgiven A, ac-\ncording to Bayes\u2019 theorem. Finally, the law of total probability gives us a way to decompose\nevents into subevents.\nWe end this chapter by mentioning a few terms related to conditional probabilities\nthat will become useful later. Let us use the tennis tournament as an example:\n\u0088P[W|A] =conditional probability = Given that you played with player A, what is\nthe probability that you will win?\n\u0088P[A] =prior probability = Without even entering the game, what is the chance that\nyou will face player A?\n\u0088P[A|W] =posterior probability = After you have won the game, what is the proba-\nbility that you have actually played with A?\nIn many practical engineering problems, the question of interest is often the last one. That\nis, supposing that you have observed something, what is the most likely cause of that event?\nFor example, supposing we have observed this particular dataset, what is the best Gaussian\nmodel that would fit the dataset? Questions like these require some analysis of conditional\nprobability, prior probability, and posterior probability.\n95", "111": "CHAPTER 2. PROBABILITY\n2.6 References\nIntroduction to Probability\n2-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 1.\n2-2 Mark D. Ward and Ellen Gundlach, Introduction to Probability , W.H. Freeman and\nCompany, 2016. Chapter 1 \u2013 Chapter 6.\n2-3 Roy D. Yates and David J. Goodman, Probability and Stochastic Processes , 3rd Edi-\ntion, Wiley 2013, Chapter 1.\n2-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapter 2.\n2-5 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n2 and Chapter 3.\n2-6 Ani Adhikari and Jim Pitman, Probability for Data Science ,http://prob140.org/\ntextbook/content/README.html . Chapters 1 and 2.\n2-7 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 2.1 \u2013 2.7.\n2-8 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 2.\n2-9 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Chapter 1.\nMeasure-Theoretic Probability\n2-10 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 2.8 and 2.9.\n2-11 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Appendix D.\n2-12 William Feller, An Introduction to Probability Theory and Its Applications , Wiley and\nSons, 3rd Edition, 1950.\n2-13 Andrey Kolmogorov, Foundations of the Theory of Probability , 2nd English Edition,\nDover 2018. (Translated from Russian to English. Originally published in 1950 by\nChelsea Publishing Company New York.)\n2-14 Patrick Billingsley, Probability and Measure , Wiley, 3rd Edition, 1995.\nReal Analysis\n2-15 Tom M. Apostol, Mathematical Analysis , Pearson, 1974.\n2-16 Walter Rudin, Principles of Mathematical Analysis , McGraw Hill, 1976.\n96", "112": "2.7. PROBLEMS\n2.7 Problems\nExercise 1.\nA space Sand three of its subsets are given by S={1,3,5,7,9,11},A={1,3,5},B=\n{7,9,11}, and C={1,3,9,11}. Find A\u2229B\u2229C,Ac\u2229B,A\u2212C, and ( A\u2212B)\u222aB.\nExercise 2.\nLetA= (\u2212\u221e, r] and B= (\u2212\u221e, s] where r\u2264s. Find an expression for C= (r, s] in terms\nofAandB. Show that B=A\u222aC, and A\u2229C=\u2205.\nExercise 3. (Video Solution)\nSimplify the following sets.\n(a) [1 ,4]\u2229([0,2]\u222a[3,5])\n(b) ([0 ,1]\u222a[2,3])c\n(c)T\u221e\ni=1(\u22121/n,+1/n)\n(d)S\u221e\ni=1[5,8\u2212(2n)\u22121]\nExercise 4.\nWe will sometimes deal with the relationship between two sets. We say that Aimplies B\nwhen Ais a subset of B(why?). Show the following results.\n(a) Show that if Aimplies B, and Bimplies C, then Aimplies C.\n(b) Show that if Aimplies B, then Bcimplies Ac.\nExercise 5.\nShow that if A\u222aB=AandA\u2229B=A, then A=B.\nExercise 6.\nA space Sis defined as S={1,3,5,7,9,22}, and three subsets as A={1,3,5},B=\n{7,9,11},C={1,3,9,11}. Assume that each element has probability 1 /6. Find the following\nprobabilities:\n(a)P[A]\n(b)P[B]\n(c)P[C]\n(d)P[A\u222aB]\n(e)P[A\u222aC]\n(f)P[(A\\C)\u222aB]\n97", "113": "CHAPTER 2. PROBABILITY\nExercise 7. (Video Solution)\nA collection of 26 letters, a-z, is mixed in a jar. Two letters are drawn at random, one after\nthe other. What is the probability of drawing a vowel (a,e,i,o,u) and a consonant in either\norder? What is the sample space?\nExercise 8.\nConsider an experiment consisting of rolling a die twice. The outcome of this experiment is\nan ordered pair whose first element is the first value rolled and whose second element is the\nsecond value rolled.\n(a) Find the sample space.\n(b) Find the set Arepresenting the event that the value on the first roll is greater than\nor equal to the value on the second roll.\n(c) Find the set Bcorresponding to the event that the first roll is a six.\n(d) Let Ccorrespond to the event that the first valued rolled and the second value rolled\ndiffer by two. Find A\u2229C.\nNote that A,B, and Cshould be subsets of the sample space specified in Part (a).\nExercise 9.\nA pair of dice are rolled.\n(a) Find the sample space \u2126\n(b) Find the probabilities of the events: (i) the sum is even, (ii) the first roll is equal to\nthe second, (iii) the first roll is larger than the second.\nExercise 10.\nLetA,BandCbe events in an event space. Find expressions for the following:\n(a) Exactly one of the three events occurs.\n(b) Exactly two of the events occurs.\n(c) Two or more of the events occur.\n(d) None of the events occur.\nExercise 11.\nA system is composed of five components, each of which is either working or failed. Consider\nan experiment that consists of observing the status of each component, and let the outcomes\nof the experiment be given by all vectors ( x1, x2, x3, x4, x5), where xiis 1 if component iis\nworking and 0 if component iis not working.\n(a) How many outcomes are in the sample space of this experiment?\n(b) Suppose that the system will work if components 1 and 2 are both working, or if\ncomponents 3 and 4 are both working, or if components 1, 3, and 5 are all working.\nLetWbe the event that the system will work. Specify all of the outcomes in W.\n98", "114": "2.7. PROBLEMS\n(c) Let Abe the event that components 4 and 5 have both failed. How many outcomes\nare in the event A?\n(d) Write out all outcomes in the event A\u2229W.\nExercise 12. (Video Solution)\nA number xis selected at random in the interval [ \u22121,2]. Let the events A={x|x <0},\nB={x||x\u22120.5|<0.5},C={x|x >0.75}. Find (a) P[A|B], (b)P[B|C], (c)P[A|Cc],\n(d)P[B|Cc].\nExercise 13. (Video Solution)\nLet the events AandBhaveP[A] =x,P[B] =yandP[A\u222aB] =z. Find the following\nprobabilities: (a) P[A\u2229B], (b)P[Ac\u2229Bc], (c)P[Ac\u222aBc], (d)P[A\u2229Bc], (e)P[Ac\u222aB].\nExercise 14.\n(a) By using the fact that P[A\u222aB]\u2264P[A]+P[B], show that P[A\u222aB\u222aC]\u2264P[A]+P[B]+\nP[C].\n(b) By using the fact that P[Sn\nk=1Ak]\u2264Pn\nk=1P[Ak], show that\nP\"n\\\nk=1Ak#\n\u22651\u2212nX\nk=1P[Ac\nk].\nExercise 15.\nUse the distributive property of set operations to prove the following generalized distributive\nlaw:\nA\u222a n\\\ni=1Bi!\n=n\\\ni=1(A\u222aBi).\nHint: Use mathematical induction. That is, show that the above is true for n= 2 and that\nit is also true for n=k+ 1 when it is true for n=k.\nExercise 16.\nThe following result is known as the Bonferroni\u2019s Inequality.\n(a) Prove that for any two events AandB, we have\nP(A\u2229B)\u2265P(A) +P(B)\u22121.\n(b) Generalize the above to the case of nevents A1, A2, . . . , A n, by showing that\nP(A1\u2229A2\u2229 \u00b7\u00b7\u00b7 \u2229 An)\u2265P(A1) +P(A2) +\u00b7\u00b7\u00b7+P(An)\u2212(n\u22121).\nHint: You may use the generalized Union Bound P(Sn\ni=1Ai)\u2264Pn\ni=1P(Ai).\nExercise 17. (Video Solution)\nLetA,B,Cbe events with probabilities P[A] = 0.5,P[B] = 0.2,P[C] = 0.4. Find\n99", "115": "CHAPTER 2. PROBABILITY\n(a)P[A\u222aB] ifAandBare independent.\n(b)P[A\u222aB] ifAandBare disjoint.\n(c)P[A\u222aB\u222aC] ifA,BandCare independent.\n(d)P[A\u222aB\u222aC] ifA,BandCare pairwise disjoint; can this happen?\nExercise 18. (Video Solution)\nA block of information is transmitted repeated over a noisy channel until an error-free block\nis received. Let M\u22651 be the number of blocks required for a transmission. Define the\nfollowing sets.\n(i)A={Mis even }\n(ii)B={Mis a multiple of 3 }\n(iii)C={Mis less than or equal to 6 }\nAssume that the probability of requiring one additional block is half of the probability\nwithout the additional block. That is:\nP[M=k] =\u00121\n2\u0013k\n, k = 1,2, . . . .\nDetermine the following probabilities.\n(a)P[A],P[B],P[C],P[Cc]\n(b)P[A\u2229B],P[A\\B],P[A\u2229B\u2229C]\n(c)P[A|B],P[B|A]\n(d)P[A|B\u2229C],P[A\u2229B|C]\nExercise 19. (Video Solution)\nA binary communication system transmits a signal Xthat is either a +2-voltage signal or\na\u22122-voltage signal. A malicious channel reduces the magnitude of the received signal by\nthe number of heads it counts in two tosses of a coin. Let Ybe the resulting signal. Possible\nvalues of Yare listed below.\n2 Heads 1 Head No Head\nX=\u22122Y= 0 Y=\u22121Y=\u22122\nX= +2 Y= 0 Y= +1 Y= +2\nAssume that the probability of having X= +2 and X=\u22122 is equal.\n(a) Find the sample space of Y, and hence the probability of each value of Y.\n(b) What are the probabilities P[X= +2|Y= 1] and P[Y= 1|X=\u22122]?\nExercise 20. (Video Solution)\nA block of 100 bits is transmitted over a binary communication channel with a probability\nof bit error p= 10\u22122.\n100", "116": "2.7. PROBLEMS\n(a) If the block has 1 or fewer errors, then the receiver accepts the block. Find the prob-\nability that the block is accepted.\n(b) If the block has more than 1 error, then the block is retransmitted. What is the\nprobability that 4 blocks are transmitted?\nExercise 21. (Video Solution)\nA machine makes errors in a certain operation with probability p. There are two types of\nerrors. The fraction of errors that are type A is \u03b1and the fraction that are type B is 1 \u2212\u03b1.\n(a) What is the probability of kerrors in noperations?\n(b) What is the probability of k1type A errors in noperations?\n(c) What is the probability of k2type B errors in noperations?\n(d) What is the joint probability of k1type A errors and k2type B errors in noperations?\nHint: There are\u0000n\nk1\u0001\u0000n\u2212k1\nk2\u0001\npossibilities of having k1type A errors and k2type B errors\ninnoperations. (Why?)\nExercise 22. (Video Solution)\nA computer manufacturer uses chips from three sources. Chips from sources A,BandC\nare defective with probabilities 0.005, 0.001 and 0.01, respectively. The proportions of chips\nfrom A,BandCare 0.5, 0.1 and 0.4 respectively. If a randomly selected chip is found to\nbe defective, find\n(a) the probability that the chips are from A.\n(b) the probability that the chips are from B.\n(c) the probability that the chips are from C.\nExercise 23. (Video Solution)\nIn a lot of 100 items, 50 items are defective. Suppose that mitems are selected for testing.\nWe say that the manufacturing process is malfunctioning if the probability that one or more\nitems are tested to be defective. Call this failure probability p. What should be the minimum\nmsuch that p\u22650.99?\nExercise 24. (Video Solution)\nOne of two coins is selected at random and tossed three times. The first coin comes up heads\nwith probability p1= 1/3 and the second coin with probability p2= 2/3.\n(a) What is the probability that the number of heads is k= 3?\n(b) Repeat (a) for k= 0,1,2.\n(c) Find the probability that coin 1 was tossed given that kheads were observed, for\nk= 0,1,2,3.\n(d) In part (c), which coin is more probably when 2 heads have been observed?\n101", "117": "CHAPTER 2. PROBABILITY\nExercise 25. (Video Solution)\nConsider the following communication channel. A source transmits a string of binary symbols\nthrough a noisy communication channel. Each symbol is 0 or 1 with probability pand\n1\u2212p, respectively, and is received incorrectly with probability \u03b50and\u03b51. Errors in different\nsymbols transmissions are independent.\nDenote Sas the source and Ras the receiver.\n(a) What is the probability that a symbol is correctly received? Hint: Find\nP[R= 1\u2229S= 1] and P[R= 0\u2229S= 0].\n(b) Find the probability of receiving 1011 conditioned on that 1011 was sent, i.e.,\nP[R= 1011 |S= 1011] .\n(c) To improve reliability, each symbol is transmitted three times, and the received\nstring is decoded by the majority rule. In other words, a 0 (or 1) is transmitted as\n000 (or 111, respectively), and it is decoded at the receiver as a 0 (or 1) if and only if\nthe received three-symbol string contains at least two 0s (or 1s, respectively). What\nis the probability that the symbol is correctly decoded, given that we send a 0?\n(d) Suppose that the scheme of part (c) is used. What is the probability that a 0 was\nsent if the string 101 was received?\n(e) Suppose the scheme of part (c) is used and given that a 0 was sent. For what value of\n\u03b50is there an improvement in the probability of correct decoding? Assume that\n\u03b50\u0338= 0.\n102", "118": "Chapter 3\nDiscrete Random Variables\nWhen working on a data analysis problem, one of the biggest challenges is the disparity\nbetween the theoretical tools we learn in school and the actual data our boss hands to us.\nBy actual data, we mean a collection of numbers, perhaps organized or perhaps not. When\nwe are given the dataset, the first thing we do would certainly not be to define the Borel\n\u03c3-field and then define the measure. Instead, we would normally compute the mean, the\nstandard deviation, and perhaps some scores about the skewness.\nThe situation is best explained by the landscape shown in Figure 3.1 . On the one hand,\nwe have well-defined probability tools, but on the other hand, we have a set of practical\n\u201cbattle skills\u201d for processing data. Often we view them as two separate entities. As long as\nwe can pull the statistics from the dataset, why bother about the theory? Alternatively, we\nhave a set of theories, but we will never verify them using the actual datasets. How can we\nbridge the two? What are the missing steps in the probability theory we have learned so\nfar? The goal of this chapter (and the next) is to fill this gap.\nFigure 3.1: The landscape of probability and data. Often we view probability and data analysis as two\ndifferent entities. However, probability and data analysis are inseparable. The goal of this chapter is to\nlink the two.\nThree concepts to bridge the gap between theory and practice\nThe starting point of our discussion is a probability space (\u2126 ,F,P). It is an abstract concept,\nbut we hope we have convinced you in Chapter 2 of its significance. However, the probability\nspace is certainly not \u201cuser friendly\u201d because no one would write a Python program to\n103", "119": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nimplement those theories. How do we make the abstract probability space more convenient\nso that we can model practical scenarios?\nThe first step is to recognize that the sample space and the event space are all based\nonstatements , for example, \u201cgetting a head when flipping a coin\u201d or \u201cwinning the game.\u201d\nThese statements are not numbers, but we (engineers) love numbers. Therefore, we should\nask a very basic question: How do we convert a statement to a number? The answer is the\nconcept of random variables .\nKey Concept 1: What are random variables?\nRandom variables are mappings from events to numbers.\nNow, suppose that we have constructed a random variable that translates statements to\nnumbers. The next task is to endow the random variable with probabilities. More precisely,\nwe need to assign probabilities to the random variable so that we can perform computations.\nThis is done using the concept called probability mass function (PMF).\nKey Concept 2: What are probability mass functions (PMFs)?\nProbability mass functions are the ideal histograms of random variables.\nThe best way to think about a PMF is a histogram, something we are familiar with.\nA histogram has two axes: The x-axis denotes the set of states and the y-axis denotes\ntheprobability . For each of the states that the random variable possesses, the histogram\ntells us the probability of getting a particular state. The PMF is the ideal histogram of a\nrandom variable. It provides a complete characterization of the random variable. If you have\na random variable, you must specify its PMF. Vice versa, if you tell us the PMF, you have\nspecified a random variable.\nWe ask the third question about pulling information from the probability mass func-\ntion, such as the mean and standard deviation. How do we obtain these numbers from the\nPMF? We are also interested in operations on the mean and standard deviations. For ex-\nample, if a professor offers ten bonus points to the entire class, how will it affect the mean\nand standard deviation? If a store provides 20% off on all its products, what will happen to\nits mean retail price and standard deviation? However, the biggest question is perhaps the\ndifference between the mean we obtain from a PMF and the mean we obtain from a his-\ntogram. Understanding this difference will immediately help us build a bridge from theory\nto practice.\nKey Concept 3: What is expectation?\nExpectation = Mean = Average computed from a PMF.\nOrganization of this chapter\nThe plan for this chapter is as follows. We will start with the basic concepts of random\nvariables in Section 3.1. We will formally define the random variables and discuss their\nrelationship with the abstract probability space. Once this linkage is built, we can put\n104", "120": "3.1. RANDOM VARIABLES\nthe abstract probability space aside and focus on the random variables. In Section 3.2\nwe will define the probability mass function (PMF) of a random variable, which tells us\nthe probability of obtaining a state of the random variable. PMF is closely related to the\nhistogram of a dataset. We will explain the connection. In Section 3.3 we take a small detour\nto consider the cumulative distribution functions (CDF). Then, we discuss the mean and\nstandard deviation in Section 3.4. Section 3.5 details a few commonly used random variables,\nincluding Bernoulli, binomial, geometric, and Poisson variables.\n3.1 Random Variables\n3.1.1 A motivating example\nConsider an experiment with 4 outcomes \u2126 = {\u2663,\u2662,\u2661,\u2660}. We want to construct the\nprobability space (\u2126 ,F,P). The sample space \u2126 is already defined. The event space Fis the\nset of all possible subsets in \u2126, which, in our case, is a set of 24subsets. For the probability\nlawP, let us assume that the probability of obtaining each outcome is\nP[{\u2663}] =1\n6,P[{\u2662}] =2\n6,P[{\u2661}] =2\n6,P[{\u2660}] =1\n6.\nTherefore, we have constructed a probability space (\u2126 ,F,P) where everything is perfectly\ndefined. So, in principle, they can live together happily forever.\nA lazy data scientist comes, and there is a (small) problem. The data scientist does not\nwant to write the symbols \u2663,\u2662,\u2661,\u2660. There is nothing wrong with his motivation because\nall of us want efficiency. How can we help him? Well, the easiest solution is to encode each\nsymbol with a number, for example, \u2663 \u2190 1,\u2662 \u2190 2,\u2661 \u2190 3,\u2660 \u2190 4, where the arrow means\nthat we assign a number to the symbol. But we can express this more formally by defining\na function X: \u2126\u2192Rwith\nX(\u2663) = 1 , X(\u2662) = 2 , X(\u2661) = 3 , X(\u2660) = 4 .\nThere is nothing new here: we have merely converted the symbols to numbers, with the help\nof a function X. However, with Xdefined, the probabilities can be written as\nP[X= 1] =1\n6,P[X= 2] =2\n6,P[X= 3] =2\n6,P[X= 4] =1\n6.\nThis is much more convenient, and so the data scientist is happy.\n3.1.2 Definition of a random variable\nThe story above is exactly the motivation for random variables. Let us define a random\nvariable formally.\nDefinition 3.1. Arandom variable Xis a function X: \u2126\u2192Rthat maps an outcome\n\u03be\u2208\u2126to a number X(\u03be)on the real line.\n105", "121": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nThis definition may be puzzling at first glance. Why should we overcomplicate things by\ndefining a function and calling it a variable ?\nIf you recall the story above, we can map the notations of the story to the notations\nof the definition as follows.\nSymbol Meaning\n\u2126 sample space = the set containing \u2663,\u2662,\u2661,\u2660\n\u03be an element in the sample space, which is one of \u2663,\u2662,\u2661,\u2660\nX a function that maps \u2663to the number 1, \u2662to the number 2, etc\nX(\u03be) a number on the real line, e.g., X(\u2663) = 1\nThis explains our informal definition of random variables:\nKey Concept 1: What are random variables?\nRandom variables are mappings from events to numbers.\nThe random variable Xis afunction . The input to the function is an outcome of the sample\nspace, whereas the output is a number on the real line. This type of function is somewhat\ndifferent from an ordinary function that often translates a number to another number.\nNevertheless, Xis a function.\nFigure 3.2: A random variable is a mapping from the outcomes in the sample space to numbers on the\nreal line. We can think of a random variable Xas a translator that translates a statement to a number.\nWhy do we call this function Xavariable ?Xis a variable because Xhas multiple\nstates . As we illustrate in Figure 3.2 , the mapping Xtranslates every outcome \u03beto a\nnumber. There are multiple numbers, which are the states of X. Each state has a certain\nprobability for Xto land on. Because Xis not deterministic, we call it a random variable.\nExample 3.1 . Suppose we flip a fair coin so that \u2126 = {head,tail}. We can define the\nrandom variable X: \u2126\u2192Ras\nX(head) = 1 , and X(tail) = 0 .\n106", "122": "3.1. RANDOM VARIABLES\nTherefore, when we write P[X= 1] we actually mean P[{head}]. Is there any difference\nbetween P[{Head}] andP[X= 1]? No, because they are describing two identical events.\nNote that the assignment of the value is totally up to you. You can say \u201chead\u201d is equal\nto the value 102. This is allowed and legitimate, but it isn\u2019t very convenient.\nExample 3.2 . Flip a coin 2 times. The sample space \u2126 is\n\u2126 ={(head ,head) ,(head ,tail),(tail,head) ,(tail,tail)}.\nSuppose that Xis a random variable that maps an outcome to a number representing\nthe sum of \u201chead,\u201d i.e.,\nX(\u00b7) = number of heads .\nThen, for the 4 \u03be\u2019s in the sample space there are only 3 distinct numbers. More precisely,\nif we let \u03be1= (head ,head), \u03be2= (head ,tail), \u03be3= (tail ,head), \u03be4= (tail ,tail), then,\nwe have\nX(\u03be1) = 2 , X(\u03be2) = 1 , X(\u03be3) = 1 , X(\u03be4) = 0 .\nA pictorial illustration of this random variable is shown in Figure 3.3 . This example\nshows that the mapping defined by the random variable is not necessarily a one-to-one\nmapping because multiple outcomes can be mapped to the same number.\nFigure 3.3: A random variable that maps a pair of coins to a number, where the number represents the\nnumber of heads.\n3.1.3 Probability measure on random variables\nBy now, we hope that you understand Key Concept 1: A random variable is a mapping\nfrom a statement to a number . However, we are now facing another difficulty. We knew\nhow to measure the size of an event using the probability law Pbecause P(\u00b7) takes an event\nE\u2208 Fand sends it to a number between [0 ,1]. After the translation X, we cannot send the\noutput X(\u03be) toP(\u00b7) because P(\u00b7) \u201ceats\u201d a set E\u2208 Fand not a number X(\u03be)\u2208R. Therefore,\nwhen we write P[X= 1], how do we measure the size of the event X= 1?\n107", "123": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nThis question appears difficult but is actually quite easy to answer. Since the prob-\nability law P(\u00b7) is always applied to an event , we need to define an event for the random\nvariable X. If we write the sets clearly, we note that \u201c X=a\u201d is equivalent to the set\nE=\u001a\n\u03be\u2208\u2126\f\f\f\fX(\u03be) =a\u001b\n.\nThis is the set that contains all possible \u03be\u2019s such that X(\u03be) =a. Therefore, when we say\n\u201cfind the probability of X=a,\u201d we are effectively asking the size of the set E={\u03be\u2208\n\u2126|X(\u03be) =a}.\nHow then do we measure the size of E? Since Eis a subset in the sample space, Eis\nmeasurable by P. All we need to do is to determine what Eis for a given a. This, in turn,\nrequires us to find the pre-image X\u22121(a), which is defined as\nX\u22121(a)def=\u001a\n\u03be\u2208\u2126\f\f\f\fX(\u03be) =a\u001b\n.\nWait a minute, is this set just equal to E? Yes, the event Ewe are seeking is exactly the\npre-image X\u22121(a). As such, the probability measure of Eis\nP[X=a] =P[X\u22121(a)].\nFigure 3.4 illustrates a situation where two outcomes \u03be1and\u03be2are mapped to the same\nvalue aon the real line. The corresponding event is the set X\u22121(a) ={\u03be1, \u03be2}.\nFigure 3.4: When computing the probability of P[{\u03be\u2208\u2126|X(\u03be) =a}], we effectively take the inverse\nmapping X\u22121(a)and compute the probability of the event P[{\u03be\u2208X\u22121(a)}] =P[{\u03be1, \u03be2}].\nExample 3.3 . Suppose we throw a die. The sample space is\n\u2126 ={\n,\n,\n,\n,\n,\n}.\nThere is a natural mapping Xthat maps X(\n) = 1, X(\n) = 2 and so on. Thus,\n108", "124": "3.1. RANDOM VARIABLES\nP[X\u22643](a)=P[X= 1] + P[X= 2] + P[X= 3]\n(b)=P[X\u22121(1)] + P[X\u22121(2)] + P[X\u22121(3)]\n(c)=P[{\n}] +P[{\n}] +P[{\n}] =3\n6.\nIn this derivation, step (a) is based on Axiom III, where the three events are disjoint.\nStep (b) is the pre-image due to the random variable X. Step (c) is the list of ac-\ntual events in the event space. Note that there is no hand-waving argument in this\nderivation. Every step is justified by the concepts and theorems we have learned so\nfar.\nExample 3.4 . Throw a die twice. The sample space is then\n\u2126 ={(\n,\n),(\n,\n), . . . , (\n,\n)}.\nThese elements can be translated to 36 outcomes:\n\u03be1= (\n ,\n), \u03be2= (\n ,\n), . . . , \u03be 36= (\n ,\n).\nLet\nX= sum of two numbers .\nThen, if we want to find the probability of getting X= 7, we can trace back and ask:\nAmong the 36 outcomes, which of those \u03bei\u2019s will give us X(\u03be) = 7? Or, what is the set\nX\u22121(7)? To this end, we can write\nP[X= 7] = P[{(\n,\n),(\n,\n),(\n,\n),(\n,\n),(\n,\n),(\n,\n)}]\n=P[(\n,\n)] +P[(\n,\n)] +P[(\n,\n)]\n+P[(\n,\n)] +P[(\n,\n)] +P[(\n,\n)]\n=1\n36+1\n36+1\n36+1\n36+1\n36+1\n36=1\n6.\nAgain, in this example, you can see that all the steps are fully justified by the concepts\nwe have learned so far.\nClosing remark . In practice, when the problem is clearly defined, we can skip the inverse\nmapping X\u22121(a). However, this does not mean that the probability triplet (\u2126 ,F,P) is gone;\nit is still present. The triplet is now just the background of the problem.\nThe set of all possible values returned by Xis denoted as X(\u2126). Since Xis not\nnecessarily a bijection, the size of X(\u2126) is not necessarily the same as the size of \u2126. The\nelements in X(\u2126) are often denoted as aorx. We call aorxone of the states ofX. Be\ncareful not to confuse xandX. The variable Xis the random variable; it is a function.\nThe variable xis a state assigned by X. A random variable Xhas multiple states. When\nwe write P[X=x], we describe the probability of a random variable Xtaking a particular\nstate x. It is exactly the same as P[{\u03be\u2208\u2126|X(\u03be) =x}].\n109", "125": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n3.2 Probability Mass Function\nRandom variables are mappings that translate events to numbers. After the translation,\nwe have a set of numbers denoting the states of the random variables. Each state has a\ndifferent probability of occurring. The probabilities are summarized by a function known as\nthe probability mass function (PMF).\n3.2.1 Definition of probability mass function\nDefinition 3.2. Theprobability mass function (PMF) of a random variable Xis a\nfunction which specifies the probability of obtaining a number X(\u03be) =x. We denote a\nPMF as\npX(x) =P[X=x]. (3.1)\nThe set of all possible states of Xis denoted as X(\u2126).\nDo not get confused by the sample space \u2126 and the set of states X(\u2126). The sample space \u2126\ncontains all the possible outcomes of the experiments, whereas X(\u2126) is the translation by\nthe mapping X. The event X=ais the set X\u22121(a)\u2286\u2126. Therefore, when we say P[X=x]\nwe really mean P[X\u22121(x)].\nThe probability mass function is a histogram summarizing the probability of each of\nthe states Xtakes. Since it is a histogram, a PMF can be easily drawn as a bar chart.\nExample 3.5 . Flip a coin twice. The sample space is \u2126 = {HH, HT, TH, TT }. We\ncan assign a random variable X= number of heads. Therefore,\nX(\u201cHH\u201d) = 2 , X(\u201cTH\u201d) = 1 , X(\u201cHT\u201d) = 1 , X(\u201cTT\u201d) = 0 .\nSo the random variable Xtakes three states: 0, 1, 2. The PMF is therefore\npX(0) =P[X= 0] = P[{\u201cTT\u201d}] =1\n4,\npX(1) =P[X= 1] = P[{\u201cTH\u201d ,\u201cHT\u201d}] =1\n2,\npX(2) =P[X= 2] = P[{\u201cHH\u201d}] =1\n4.\n3.2.2 PMF and probability measure\nIn Chapter 2, we learned that probability is a measure of the size of a set. We introduced a\nweighting function that weights each of the elements in the set. The PMF is the weighing\nfunction for discrete random variables. Two random variables are different when their PMFs\nare different because they are constructing two different measures.\n110", "126": "3.2. PROBABILITY MASS FUNCTION\nTo illustrate the idea, suppose there are two dice. They each have probability masses\nas follows.\nP[{\n}] =1\n12,P[{\n}] =2\n12,P[{\n}] =3\n12,P[{\n}] =4\n12,P[{\n}] =1\n12,P[{\n}] =1\n12,\nP[{\n}] =2\n12,P[{\n}] =2\n12,P[{\n}] =2\n12,P[{\n}] =2\n12,P[{\n}] =2\n12,P[{\n}] =2\n12,\nLet us define two random variables, XandY, for the two dice. Then, the PMFs pXandpY\ncan be defined as\npX(1) =1\n12, pX(2) =2\n12, pX(3) =3\n12, pX(4) =4\n12, pX(5) =1\n12, pX(6) =1\n12,\npY(1) =2\n12, pY(2) =2\n12, pY(3) =2\n12, pY(4) =2\n12, pY(5) =2\n12, pY(6) =2\n12.\nThese two probability mass functions correspond to two different probability measures, let\u2019s\nsayFandG. Define the event E={between 2 and 3 }. Then, F(E) andG(E) will lead to\ntwo different results:\nF(E) =P[2\u2264X\u22643] =pX(2) + pX(3) =1\n12+2\n12=3\n12,\nG(E) =P[2\u2264Y\u22643] =pY(2) + pY(3) =2\n12+2\n12=4\n12.\nNote that even though for some particular events two final results could be the same (e.g.,\n2\u2264X\u22644 and 2 \u2264Y\u22644), the underlying measures are completely different.\nFigure 3.5 shows another example of two different measures FandGon the same\nsample space \u2126 = {\u2663,\u2662,\u2661,\u2660}. Since the PMFs of the two measures are different, even\nwhen given the same event E, the resulting probabilities will be different.\nFigure 3.5: If we want to measure the size of a set E, using two different PMFs is equivalent to using\ntwo different measures. Therefore, the probabilities will be different.\nDoes pX=pYimply X=Y?If two random variables XandYhave the same PMF,\ndoes it mean that the random variables are the same? The answer is no. Consider a random\nvariable with a symmetric PMF, e.g.,\npX(\u22121) =1\n4, p X(0) =1\n2, p X(1) =1\n4. (3.2)\nSuppose Y=\u2212X. Then, pY(\u22121) =1\n4,pY(0) =1\n2, and pY(1) =1\n4, which is the same as pX.\nHowever, XandYare two different random variables. If the sample space is {\u2663,\u2662,\u2661}, we\ncan define the mappings X(\u00b7) and Y(\u00b7) as\nX(\u2663) =\u22121, X (\u2662) = 0 , X (\u2661) = +1 ,\nY(\u2663) = +1 , Y (\u2662) = 0 , Y (\u2661) =\u22121.\n111", "127": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nTherefore, when we say pX(\u22121) =1\n4, the underlying event is \u2663. But when we say pY(\u22121) =1\n4,\nthe underlying event is \u2661. The two random variables are different, although their PMFs have\nexactly the same shape.\n3.2.3 Normalization property\nHere we must mention one important property of a probability mass function. This property\nis known as the normalization property , which is a useful tool for a sanity check.\nTheorem 3.1. A PMF should satisfy the condition that\nX\nx\u2208X(\u2126)pX(x) = 1 . (3.3)\nProof . The proof follows directly from Axiom II, which states that P[\u2126] = 1. Since xcovers\nall numerical values Xcan take, and since each xis distinct, by Axiom III we have\nX\nx\u2208X(\u2126)P[X=x] =X\nx\u2208X(\u2126)P[{\u03be\u2208\u2126|X(\u03be) =x}]\n=P\uf8ee\n\uf8f0[\n\u03be\u2208\u2126{\u03be\u2208\u2126|X(\u03be) =x}\uf8f9\n\uf8fb=P[\u2126] = 1 .\n\u25a1\nPractice Exercise 3.1 . Let pX(k) =c\u00001\n2\u0001k, where k= 1,2, . . .. Find c.\nSolution . SinceP\nk\u2208X(\u2126)pX(k) = 1, we must have\n\u221eX\nk=1\u00121\n2\u0013k\n= 1.\nEvaluating the geometric series on the right-hand side, we can show that\n\u221eX\nk=1c\u00121\n2\u0013k\n=c\n2\u221eX\nk=0\u00121\n2\u0013k\n=c\n2\u00b71\n1\u22121\n2\n=c=\u21d2 c= 1.\nPractice Exercise 3.2 . Let pX(k) =c\u00b7sin\u0000\u03c0\n2k\u0001\n, where k= 1,2, . . .. Find c.\nSolution . The reader may might be tempted to sum pX(k) over all the possible k\u2019s:\n\u221eX\nk=1sin\u0010\u03c0\n2k\u0011\n= 1 + 0 \u22121 + 0 + \u00b7\u00b7\u00b7?= 0.\n112", "128": "3.2. PROBABILITY MASS FUNCTION\nHowever, a more careful inspection reveals that pX(k) is actually negative when k=\n3,7,11, . . .. This cannot happen because a probability mass function must be non-\nnegative. Therefore, the problem is not defined, and so there is no solution.\n0.06250.1250.250.5\n1 2 3 4 5 6 7 8 9 10\n-1-0.500.51\n1 2 3 4 5 6 7 8 9 10\n(a) (b)\nFigure 3.6: (a) The PMF of pX(k) =c\u00001\n2\u0001k, for k= 1,2, . . .. (b) The PMF of pX(k) = sin\u0000\u03c0\n2k\u0001\n,\nwhere k= 1,2, . . .. Note that this is not a valid PMF because probability cannot have negative values.\n3.2.4 PMF versus histogram\nPMFs are closely related to histograms. A histogram is a plot that shows the frequency of\na state. As we see in Figure 3.6 , the x-axis is a collection of states, whereas the y-axis is\nthe frequency. So a PMF is indeed a histogram.\nViewing a PMF as a histogram can help us understand a random variable. For better\nor worse, treating a random variable as a histogram could help you differentiate a random\nvariable from a variable. An ordinary variable only has one state, but a random variable\nhas multiple states. At any particular instance, we do not know which state will show up\nbefore our observation. However, we do know the probability. For example, in the coin-flip\nexample, while we do not know whether we will get \u201cHH,\u201d we know that the chance of\ngetting \u201cHH\u201d is 1/4. Of course, having a probability of 1/4 does not mean that we will get\n\u201cHH\u201d once every four trials. It only means that if we run an infinite number of experiments,\nthen 1/4 of the experiments will give us \u201cHH.\u201d\nThe linkage between PMF and histogram can be quite practical. For example, while\nwe do not know the true underlying distribution of the 26 letters of the English alphabet, we\ncan collect a large number of words and plot the histogram. The example below illustrates\nhow we can empirically define a random variable from the data.\nExample . There are 26 English letters, but the frequencies of the letters in writing are\ndifferent. If we define a random variable Xas a letter we randomly draw from an English\ntext, we can think of Xas an object with 26 different states. The mapping associated with the\nrandom variable is straightforward: X(\u201ca\u201d) = 1, X(\u201cb\u201d) = 2, etc. The probability of landing\non a particular state approximately follows a histogram shown in Figure 3.7 . The histogram\nprovides meaningful values of the probabilities, e.g., pX(1) = 0 .0847, pX(2) = 0 .0149, etc.\nThe true probability of the states may not be exactly these values. However, when we have\nenough samples, we generally expect the histogram to approach the theoretical PMF. The\nMATLAB and Python codes used to generate this histogram are shown below.\n% MATLAB code to generate the histogram\nload(\u2018ch3_data_English\u2019);\nbar(f/100,\u2018FaceColor\u2019,[0.9,0.6,0.0]);\n113", "129": "CHAPTER 3. DISCRETE RANDOM VARIABLES\na b c d e f g h i j k l m n o p q r s t u v w x y z00.020.040.060.080.10.12\nFigure 3.7: The frequency of the 26 English letters. Data source: Wikipedia.\nxticklabels({\u2018a\u2019,\u2018b\u2019,\u2018c\u2019,\u2018d\u2019,\u2018e\u2019,\u2018f\u2019,\u2018g\u2019,\u2018h\u2019,\u2018i\u2019,\u2018j\u2019,\u2018k\u2019,\u2018l\u2019,...\n\u2018m\u2019,\u2018n\u2019,\u2018o\u2019,\u2018p\u2019,\u2018q\u2019,\u2018r\u2019,\u2018s\u2019,\u2018t\u2019,\u2018u\u2019,\u2018v\u2019,\u2018w\u2019,\u2018x\u2019,\u2018y\u2019,\u2018z\u2019});\nxticks(1:26);\nyticks(0:0.02:0.2);\naxis([1 26 0 0.13]);\n# Python code generate the histogram\nimport numpy as np\nimport matplotlib.pyplot as plt\nf = np.loadtxt(\u2018./ch3_data_english.txt\u2019)\nn = np.arange(26)\nplt.bar(n, f/100)\nntag = [\u2018a\u2019,\u2018b\u2019,\u2018c\u2019,\u2018d\u2019,\u2018e\u2019,\u2018f\u2019,\u2018g\u2019,\u2018h\u2019,\u2018i\u2019,\u2018j\u2019,\u2018k\u2019,\u2018l\u2019,\u2018m\u2019,...\n\u2018n\u2019,\u2018o\u2019,\u2018p\u2019,\u2018q\u2019,\u2018r\u2019,\u2018s\u2019,\u2018t\u2019,\u2018u\u2019,\u2018v\u2019,\u2018w\u2019,\u2018x\u2019,\u2018y\u2019,\u2018z\u2019]\nplt.xticks(n, ntag)\nPMF = ideal histograms\nIf a random variable is more or less a histogram, why is the PMF such an important concept?\nThe answer to this question has two parts. The first part is that the histogram generated\nfrom a dataset is always an empirical histogram, so-called because the dataset comes from\nobservation or experience rather than theory. Thus the histograms may vary slightly every\ntime we collect a dataset.\nAs we increase the number of data points in a dataset, the histogram will eventually\nconverge to an ideal histogram, or a distribution . For example, counting the number of\nheads in 100 coin flips will fluctuate more in percentage terms than counting the heads in 10\nmillion coin flips. The latter will almost certainly have a histogram that is closer to a 50\u201350\ndistribution. Therefore, the \u201chistogram\u201d generated by a random variable can be considered\nthe ultimate histogram or the limiting histogram of the experiment.\nTo help you visualize the difference between a PMF and a histogram, we show in\nFigure 3.8 an experiment in which a die is thrown Ntimes. Assuming that the die is fair,\nthe PMF is simply pX(k) = 1 /6 for k= 1, . . . , 6, which is a uniform distribution across\nthe 6 states. Now, we can throw the die many times. As Nincreases, we observe that the\n114", "130": "3.2. PROBABILITY MASS FUNCTION\n1 2 3 4 5 600.050.10.150.2N = 100\n1 2 3 4 5 600.050.10.150.2N = 1000\n(a)N= 100 (b) N= 1000\n1 2 3 4 5 600.050.10.150.2N = 10000\n1 2 3 4 5 600.050.10.150.2N = \n(c)N= 10000 (d) PMF\nFigure 3.8: Histogram and PMF, when throwing a fair die Ntimes. As Nincreases, the histograms are\nbecoming more similar to the PMF.\nhistogram becomes more like the PMF. You can imagine that when Ngoes to infinity, the\nhistogram will eventually become the PMF. Therefore, when given a dataset, one way to\nthink of it is to treat the data as random realizations drawn from a certain PMF. The more\ndata points you have, the closer the histogram will become to the PMF.\nThe MATLAB and Python codes used to generate Figure 3.8 are shown below. The\ntwo commands we use here are randi (in MATLAB), which generates random integer num-\nbers, and hist, which computes the heights and bin centers of a histogram. In Python,\nthe corresponding commands are np.random.randint andplt.hist . Note that because of\nthe different indexing schemes in MATLAB and Python, we offset the maximum index in\nnp.random.randint to 7 instead of 6. Also, we shift the x-axes so that the bars are centered\nat the integers.\n% MATLAB code to generate the histogram\nx = [1 2 3 4 5 6];\nq = randi(6,100,1);\nfigure;\n[num,val] = hist(q,x-0.5);\nbar(num/100,\u2018FaceColor\u2019,[0.8, 0.8,0.8]);\naxis([0 7 0 0.24]);\n# Python code generate the histogram\nimport numpy as np\nimport matplotlib.pyplot as plt\nq = np.random.randint(7,size=100)\n115", "131": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nplt.hist(q+0.5,bins=6)\nThisgenerative perspective is illustrated in Figure 3.9 . We assume that the underlying\nlatent random variable has some PMF that can be described by a few parameters, e.g., the\nmean and variance. Given the data points, if we can infer these parameters, we might retrieve\nthe entire PMF (up to the uncertainty level intrinsic to the dataset). We refer to this inverse\nprocess as statistical inference.\nFigure 3.9: When analyzing a dataset, one can treat the data points are samples drawn according to a\nlatent random variable with certain a PMF. The dataset we observe is often finite, and so the histogram\nwe obtain is empirical. A major task in data analysis is statistical inference, which tries to retrieve the\nmodel information from the available measurements.\nReturning to the question of why we need to understand the PMFs, the second part\nof the answer is the difference between synthesis andanalysis . In synthesis, we start with\na known random variable and generate samples according to the PMF underlying the ran-\ndom variable. For example, on a computer, we often start with a Gaussian random variable\nand generate random numbers according to the histogram specified by the Gaussian ran-\ndom variable. Synthesis is useful because we can predict what will happen. We can, for\nexample, create millions of training samples to train a deep neural network. We can also\nevaluate algorithms used to estimate statistical quantities such as mean, variance, moments,\netc., because the synthesis approach provides us with ground truth. In supervised learning\nscenarios, synthesis is vital to ensuring sufficient training data.\nThe other direction of synthesis is analysis. The goal is to start with a dataset and\ndeduce the statistical properties of the dataset. For example, suppose we want to know\nwhether the underlying model is indeed a Gaussian model. If we know that it is a Gaussian\n(or if we choose to use a Gaussian), we want to know the parameters that define this\nGaussian. The analysis direction addresses this model selection and parameter estimation\nproblem. Moving forward, once we know the model and the parameters, we can make a\nprediction or do recovery, both of which are ubiquitous in machine learning.\nWe summarize our discussions below, which is Key Concept 2 of this chapter.\nKey Concept 2: What are probability mass functions (PMFs)?\nPMFs are the ideal histograms of random variables.\n116", "132": "3.2. PROBABILITY MASS FUNCTION\n3.2.5 Estimating histograms from real data\nThe following discussions about histogram estimation can be skipped if it is your first\ntime reading the book.\nIf you have a dataset, how would you plot the histogram? Certainly, if you have access\nto MATLAB or Python, you can call standard functions such as hist (in MATLAB) or\nnp.histogram (in Python). However, when plotting a histogram, you need to specify the\nnumber of bins (or equivalently the width of bins). If you use larger bins, then you will have\nfewer bins with many elements in each bin. Conversely, if the bin width is too small, you\nmay not have enough samples to fill the histogram. Figure 3.10 illustrates two histograms\nin which the bins are respectively too large and too small.\n0 2 4 6 8 1002004006008001000\nK = 5\n0 2 4 6 8 1001020304050\nK = 200\n(a) 5 bins (b) 200 bins\nFigure 3.10: The width of the histogram has substantial influence on the information that can be\nextracted from the histogram.\nThe MATLAB and Python codes used to generate Figure 3.10 are shown below. Note\nthat here we are using an exponential random variable (to be discussed in Chapter 4). In\nMATLAB, calling an exponential random variable is done using exprnd , whereas in Python\nthe command is np.random.exponential . For this experiment, we can specify the number\nof bins k, which can be set to k= 200 or k= 5. To suppress the Python output of the array,\nwe can add a semicolon ;. A final note is that lambda is a reserved variable in Python. Use\nsomething else.\n% MATLAB code used to generate the plots\nlambda = 1;\nk = 1000;\nX = exprnd(1/lambda,[k,1]);\n[num,val] = hist(X,200);\nbar(val,num,\u2018FaceColor\u2019,[1, 0.5,0.5]);\n# Python code used to generate the plots\nimport numpy as np\nimport matplotlib.pyplot as plt\nlambd = 1\n117", "133": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nk = 1000\nX = np.random.exponential(1/lambd, size=k)\nplt.hist(X,bins=200);\nIn statistics, there are various rules to determine the bin width of a histogram. We\nmention a few of them here. Let Kbe the number of bins and Nthe number of samples.\n\u0088Square-root: K=\u221a\nN\n\u0088Sturges\u2019 formula: K= log2N+ 1.\n\u0088Rice Rule: K= 23\u221a\nN\n\u0088Scott\u2019s normal reference rule: K=maxX\u2212minX\nh, where h=3.5\u221a\nVar[X]\n3\u221a\nNis the bin\nwidth.\nFor the example data shown in Figure 3.10 , the histograms obtained using the above rules\nare given in Figure 3.11 . As you can see, different rules have different suggested bin widths.\nSome are more conservative, e.g., using fewer bins, whereas some are less conservative. In\nany case, the suggested bin widths do seem to provide better histograms than the original\nones in Figure 3.10 . However, no bin width is the best for all purposes.\n0 1 2 3 4 50100200300400500\nSquare-root, K =  32\n0 1 2 3 4 50100200300400500\nSturges Rule, K =  11\n0 1 2 3 4 50100200300400500\nRice Rule, K =  20\n0 1 2 3 4 50100200300400500\nScott Rule, K =  22\nFigure 3.11: Histograms of a dataset using different bin width rules.\nBeyond these predefined rules, there are also algorithmic tools to determine the bin\nwidth. One such tool is known as cross-validation . Cross-validation means defining some\nkind of cross-validation score that measures the statistical risk associated with the his-\ntogram. A histogram having a lower score has a lower risk, and thus it is a better histogram.\n118", "134": "3.2. PROBABILITY MASS FUNCTION\nNote that the word \u201cbetter\u201d is relative to the optimality criteria associated with the cross-\nvalidation score. If you do not agree with our cross-validation score, our optimal bin width is\nnot necessarily the one you want. In this case, you need to specify your optimality criteria.\nTheoretically, deriving a meaningful cross-validation score is beyond the scope of this\nbook. However, it is still possible to understand the principle. Let hbe the bin width of the\nhistogram, Kthe number of bins, and Nthe number of samples. Given a dataset, we follow\nthis procedure:\n\u0088Step 1: Choose a bin width h.\n\u0088Step 2: Construct a histogram from the data, using the bin width h. The histogram will\nhave the empirical PMF values bp1,bp2, . . . ,bpK, which are the heights of the histograms\nnormalized so that the sum is 1.\n\u0088Step 3: Compute the cross-validation score (see Wasserman, All of Statistics , Section\n20.2):\nJ(h) =2\n(N\u22121)h\u2212N+ 1\n(N\u22121)h\u0000\nbp2\n1+bp2\n2+\u00b7\u00b7\u00b7+bp2\nK\u0001\n(3.4)\n\u0088Repeat Steps 1, 2, 3, until we find an hthat minimizes J(h).\nNote that when we use a different h, the PMF values bp1,bp2, . . . ,bpKwill change, and the\nnumber of bins Kwill also change. Therefore, when changing h, we are changing not only\nthe terms in J(h) that explicitly contain hbut also terms that are implicitly influenced.\n20 40 60 80 100 120 140 160 180 200\nNumber of Bins-3.5-3.4-3.3-3.2-3.1-3-2.9Cross-validation Score10-3\n20 40 60 80 100 120 140 160 180 200\nNumber of Bins012345Cross-validation Score10-4\n(a) One dataset (b) Average of many datasets\nFigure 3.12: Cross-validation score for the histogram. (a) The score of one particular dataset. (b) The\nscores for many different datasets generated by the same model.\nFor the dataset we showed in Figure 3.10 , the cross-validation score J(h) is shown in\nFigure 3.12 . We can see that although the curve is noisy, there is indeed a reasonably clear\nminimum happening around 20 \u2264K\u226430, which is consistent with some of the rules.\nThe MATLAB and Python codes we used to generate Figure 3.12 are shown below.\nThe key step is to implement Equation (3.4) inside a for-loop, where the loop goes through\nthe range of bins we are interested in. To obtain the PMF values bp1, . . . ,bpK, we call hist\nin MATLAB and np.histogram in Python. The bin width his the number of samples n\ndivided by the number of bins m.\n119", "135": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n% MATLAB code to perform the cross validation\nlambda = 1;\nn = 1000;\nX = exprnd(1/lambda,[n,1]);\nm = 6:200;\nJ = zeros(1,195);\nfor i=1:195\n[num,binc] = hist(X,m(i));\nh = n/m(i);\nJ(i) = 2/((n-1)*h)-((n+1)/((n-1)*h))*sum( (num/n).^2 );\nend\nplot(m,J,\u2018LineWidth\u2019,4,\u2018Color\u2019,[0.9,0.2,0.0]);\n# Python code to perform the cross validation\nimport numpy as np\nimport matplotlib.pyplot as plt\nlambd = 1\nn = 1000\nX = np.random.exponential(1/lambd, size=n)\nm = np.arange(5,200)\nJ = np.zeros((195))\nfor i in range(0,195):\nhist,bins = np.histogram(X,bins=m[i])\nh = n/m[i]\nJ[i] = 2/((n-1)*h)-((n+1)/((n-1)*h))*np.sum((hist/n)**2)\nplt.plot(m,J);\nInFigure 3.12 (b), we show another set of curves from the same experiment. The\ndifference here is that we assume access to the true generative model so that we can generate\nthe many datasets of the same distribution. In this experiment we generated T= 1000\ndatasets. We compute the cross-validation score J(h) for each of the datasets, yielding T\nscore functions J(1)(h), . . . , J(T)(h). We subtract the minimum because different realizations\nhave different offsets. Then we compute the average:\nJ(h) =1\nTTX\nt=1\u001a\nJ(t)(h)\u2212min\nh\b\nJ(t)(h)\t\u001b\n. (3.5)\nThis gives us a smooth red curve as shown in Figure 3.12 (b). The minimum appears to be\natN= 25. This is the optimal N, concerning the cross-validation score, on the average of\nall datasets.\nAll rules, including cross-validation, are based on optimizing for a certain objective.\nYour objective could be different from our objective, and so our optimum is not necessarily\nyour optimum. Therefore, cross-validation may not be the best. It depends on your problem.\nEnd of the discussion.\n120", "136": "3.3. CUMULATIVE DISTRIBUTION FUNCTIONS (DISCRETE)\n3.3 Cumulative Distribution Functions (Discrete)\nWhile the probability mass function (PMF) provides a complete characterization of a dis-\ncrete random variable, the PMFs themselves are technically not \u201cfunctions\u201d because the\nimpulses in the histogram are essentially delta functions. More formally, a PMF pX(k)\nshould actually be written as\npX(x) =X\nk\u2208X(\u2126)pX(k)|{z}\nPMF values\u00b7\u03b4(x\u2212k)|{z}\ndelta function.\nThis is a train of delta functions, where the height is specified by the probability mass pX(k).\nFor example, a random variable with PMF values\npX(0) =1\n4, pX(1) =1\n2, pX(2) =1\n4\nwill be expressed as\npX(x) =1\n4\u03b4(x) +1\n2\u03b4(x\u22121) +1\n4\u03b4(x\u22122).\nSince delta functions need to be integrated to generate values, the typical things we want to\ndo, e.g., integration and differentiation, are not as straightforward in the sense of Riemann-\nStieltjes.\nThe way to handle the unfriendliness of the delta functions is to consider mild modi-\nfications of the PMF. This notation of \u201ccumulative\u201d distribution functions will allow us to\nresolve the delta function problems. We will defer the technical details to the next chap-\nter. For the time being, we will briefly introduce the idea to prepare you for the technical\ndiscussion later.\n3.3.1 Definition of the cumulative distribution function\nDefinition 3.3. LetXbe a discrete random variable with \u2126 ={x1, x2, . . .}. The\ncumulative distribution function (CDF) of Xis\nFX(xk)def=P[X\u2264xk] =kX\n\u2113=1pX(x\u2113). (3.6)\nIf\u2126 ={. . . ,\u22121,0,1,2, . . .}, then the CDF of Xis\nFX(k)def=P[X\u2264k] =kX\n\u2113=\u2212\u221epX(\u2113). (3.7)\nA CDF is essentially the cumulative sum of a PMF from \u2212\u221etox, where the variable x\u2032in\nthe sum is a dummy variable.\n121", "137": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nExample 3.6 . Consider a random variable Xwith PMF pX(0) =1\n4,pX(1) =1\n2and\npX(4) =1\n4. The CDF of Xcan be computed as\nFX(0) =P[X\u22640] =pX(0) =1\n4,\nFX(1) =P[X\u22641] =pX(0) + pX(1) =3\n4,\nFX(4) =P[X\u22644] =pX(0) + pX(1) + pX(4) = 1 .\nAs shown in Figure 3.13 , the CDF of a discrete random variable is a staircase function.\n0.250.50.751\n0 1 4\n0 1 40.250.50.751\n(a) PMF pX(k) (b) CDF FX(k)\nFigure 3.13: Illustration of a PMF and a CDF.\nThe MATLAB code and the Python code used to generate Figure 3.13 are shown\nbelow. The CDF is computed using the command cumsum in MATLAB and np.cumsum in\nPython.\n% MATLAB code to generate a PMF and a CDF\np = [0.25 0.5 0.25];\nx = [0 1 4];\nF = cumsum(p);\nfigure(1);\nstem(x,p,\u2018.\u2019,\u2018LineWidth\u2019,4,\u2018MarkerSize\u2019,50);\nfigure(2);\nstairs([-4 x 10],[0 F 1],\u2018.-\u2019,\u2018LineWidth\u2019,4,\u2018MarkerSize\u2019,50);\n% Python code to generate a PMF and a CDF\nimport numpy as np\nimport matplotlib.pyplot as plt\np = np.array([0.25, 0.5, 0.25])\nx = np.array([0, 1, 4])\nF = np.cumsum(p)\nplt.stem(x,p,use_line_collection=True); plt.show()\nplt.step(x,F); plt.show()\n122", "138": "3.3. CUMULATIVE DISTRIBUTION FUNCTIONS (DISCRETE)\nWhy is CDF a better-defined function than PMF? There are technical reasons associ-\nated with whether a function is integrable. Without going into the details of these discus-\nsions, a short answer is that delta functions are defined through integrations; they are not\nfunctions. A delta function is defined as a function such that \u03b4(x) = 0 everywhere except at\nx= 0, andR\n\u2126\u03b4(x)dx= 1. On the other hand, a staircase function is always well-defined.\nThe discontinuous points of a staircase can be well defined if we specify the gap between\ntwo consecutive steps. For example, in Figure 3.13 , as soon as we specify the gap 1 /4, 1/2,\nand 1 /4, the staircase function is completely defined.\nExample .Figure 3.14 shows the empirical histogram of the English letters and the corre-\nsponding empirical CDF. We want to differentiate PMF versus histogram and CDF versus\nempirical CDF. The empirical CDF is the CDF computed from a finite dataset.\n00.020.040.060.080.10.12\na b c d e f g h i j k l m n o p q r s t u v w x y z\na b c d e f g h i j k l m n o p q r s t u v w x y z00.10.20.30.40.50.60.70.80.91\nFigure 3.14: PMF and a CDF of the frequency of English letters.\n3.3.2 Properties of the CDF\nWe observe from the example in Figure 3.13 that a CDF has several properties. First, being\na staircase function, the CDF is non-decreasing. It can stay constant for a while, but it never\ndrops. Second, the minimum value of a CDF is 0, whereas the maximum value is 1. It is 0\nfor any value that is smaller than the first state; it is 1 for any value that is larger than the\nlast state. Third, the gap at each jump is exactly the probability mass at that state. Let us\nsummarize these observations in the following theorem.\nTheorem 3.2. IfXis a discrete random variable, then the CDF of Xhas the following\nproperties:\n(i) The CDF is a sequence of increasing unit steps.\n(ii) The maximum of the CDF is when x=\u221e:FX(+\u221e) = 1 .\n(iii) The minimum of the CDF is when x=\u2212\u221e:FX(\u2212\u221e) = 0 .\n(iv) The unit steps have jumps at positions where pX(x)>0.\nProof . Statement (i) can be seen from the summation\nFX(x) =X\nx\u2032\u2264xpX(x\u2032).\n123", "139": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nSince the probability mass function is non-negative, the value of FXis larger when the value\nof the argument is larger. That is, x\u2264yimplies FX(x)\u2264FX(y). The second statement (ii)\nis true because the summation includes all possible states. So we have\nFX(+\u221e) =\u221eX\nx\u2032=\u2212\u221epX(x\u2032) = 1 .\nSimilarly, for the third statement (iii),\nFX(\u2212\u221e) =X\nx\u2032\u2264\u2212\u221epX(x\u2032).\nThe summation is taken over an empty set, and so FX(\u2212\u221e) = 0. Statement (iv) is true\nbecause the cumulative sum changes only when there is a non-zero mass in the PMF. \u25a1\nAs we can see in the proof, the basic argument of the CDF is the cumulative sum of\nthe PMF. By definition, a cumulative sum always adds mass. This is why the CDF is always\nincreasing, has 0 at \u2212\u221e, and has 1 at + \u221e. This last statement deserves more attention. It\nimplies that the unit step always has a solid dot on the left -hand side and an empty dot\non the right -hand side, because when the CDF jumps, the final value is specified by the\n\u201c\u2264\u201d sign in Equation (3.6). The technical term for this property is right continuous .\n3.3.3 Converting between PMF and CDF\nTheorem 3.3. IfXis a discrete random variable, then the PMF of Xcan be obtained\nfrom the CDF by\npX(xk) =FX(xk)\u2212FX(xk\u22121), (3.8)\nwhere we assumed that Xhas a countable set of states {x1, x2, . . .}. If the sample space\nof the random variable Xcontains integers from \u2212\u221e to+\u221e, then the PMF can be\ndefined as\npX(k) =FX(k)\u2212FX(k\u22121). (3.9)\nExample 3.7 . Continuing with the example in Figure 3.13 , if we are given the CDF\nFX(0) =1\n4, F X(1) =3\n4, F X(4) = 1 ,\nhow do we find the PMF? We know that the PMF will have non-negative values only\natx= 0,1,4. For each of these x, we can show that\npX(0) = FX(0)\u2212FX(\u2212\u221e) =1\n4\u22120 =1\n4,\npX(1) = FX(1)\u2212FX(0) =3\n4\u22121\n4=1\n2,\npX(4) = FX(4)\u2212FX(1) = 1 \u22123\n4=1\n4.\n124", "140": "3.4. EXPECTATION\n3.4 Expectation\nWhen analyzing data, it is often useful to extract certain key parameters such as the mean\nand the standard deviation. The mean and the standard deviation can be seen from the lens\nof random variables. In this section, we will formalize the idea using expectation .\n3.4.1 Definition of expectation\nDefinition 3.4. Theexpectation of a random variable Xis\nE[X] =X\nx\u2208X(\u2126)x pX(x). (3.10)\nExpectation is the mean of the random variable X. Intuitively, we can think of pX(x) as the\npercentage of times that the random variable Xattains the value x. When this percentage\nis multiplied by x, we obtain the contribution of each x. Summing over all possible values\nofxthen yields the mean. To see this more clearly, we can write the definition as\nE[X] =X\nx\u2208X(\u2126)| {z }\nsum over all statesx|{z}\na state XtakespX(x)|{z }\nthe percentage.\nFigure 3.15 illustrates a PMF that contains five states x1, . . . , x 5. Corresponding to each\nstate are pX(x1), . . . , p X(x5). For this PMF to make sense, we must assume that pX(x1) +\n\u00b7\u00b7\u00b7+pX(x5) = 1. To simplify notation, let us define pidef=pX(xi). Then the expectation\nofXis just the sum of the products: value ( xi) times height ( pi). This gives E[X] =P5\ni=1xipX(xi).\nFigure 3.15: The expectation of a random variable is the sum of xipi.\nWe emphasize that the definition of the expectation is exactly the same as the usual\nway we calculate the average of a dataset. When we calculate the average of a dataset\nD={x(1), x(2), . . . , x(N)}, we sum up these Nsamples and divide by the number of samples.\nThis is what we called the empirical average or the sample average:\naverage =1\nNNX\nn=1x(n). (3.11)\n125", "141": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nOf course, in a typical dataset, these Nsamples often take distinct values. But suppose\nthat among these Nsamples there are only Kdifferent values. For example, if we throw a\ndie a million times, every sample we record will be one of the six numbers. This situation\nis illustrated in Figure 3.16 , where we put the samples into the correct bin storing these\nvalues. In this case, to calculate the average we are effectively doing a binning:\naverage =1\nNKX\nk=1value xk\u00d7number of samples with value xk. (3.12)\nEquation (3.12) is exactly the same as Equation (3.11), as long as the samples can be grouped\nintoKdifferent values. With a little calculation, we can rewrite Equation (3.12) as\naverage =KX\nk=1|{z}\nsum of all statesvalue xk|{z}\na state Xtakes\u00d7number of samples with value xk\nN| {z }\nthe percentage,\nwhich is the same as the definition of expectation.\nFigure 3.16: If we have a dataset Dcontaining Nsamples, and if there are only Kdistinct values, we\ncan effectively put these Nsamples into Kbins. Thus, the \u201caverage\u201d (which is the sum divided by the\nnumber N) is exactly the same as our definition of expectation.\nThe difference between E[X] and the average is that E[X] is computed from the ideal\nhistogram, whereas average is computed from the empirical histogram. When the number of\nsamples Napproaches infinity, we expect the average to approximate E[X]. However, when\nNis small, the empirical average will have random fluctuations around E[X]. Every time\nwe experiment, the empirical average may be slightly different. Therefore, we can regard\nE[X] as the true average of a certain random variable, and the empirical average as a finite-\nsample average based on the particular experiment we are working with. This summarizes\nKey Concept 3 of this chapter.\nKey Concept 3: What is expectation?\nExpectation = Mean = Average computed from a PMF.\nIf we are given a dataset on a computer, computing the mean can be done by calling\nthe command mean in MATLAB and np.mean in Python. The example below shows the\ncase of finding the mean of 10000 uniformly distributed random numbers.\n126", "142": "3.4. EXPECTATION\n% MATLAB code to compute the mean of a dataset\nX = rand(10000,1);\nmX = mean(X);\n# Python code to compute the mean of a dataset\nimport numpy as np\nX = np.random.rand(10000)\nmX = np.mean(X)\nExample 3.8 . Let Xbe a random variable with PMF pX(0) = 1 /4,pX(1) = 1 /2 and\npX(2) = 1 /4. We can show that the expectation is\nE[X] = (0)\u00121\n4\u0013\n|{z}\npX(0)+ (1)\u00121\n2\u0013\n|{z}\npX(1)+ (2)\u00121\n4\u0013\n|{z}\npX(2)= 1.\nOn MATLAB and Python, if we know the PMF then computing the expectation is\nstraight-forward. Here is the code to compute the above example.\n% MATLAB code to compute the expectation\np = [0.25 0.5 0.25];\nx = [0 1 2];\nEX = sum(p.*x);\n# Python code to compute the expectation\nimport numpy as np\np = np.array([0.25, 0.5, 0.25])\nx = np.array([0, 1, 2])\nEX = np.sum(p*x)\nExample 3.9 . Flip an unfair coin, where the probability of getting a head is3\n4. Let\nXbe a random variable such that X= 1 means getting a head. Then we can show\nthatpX(1) =3\n4andpX(0) =1\n4. The expectation of Xis therefore\nE[X] = (1) pX(1) + (0) pX(0) = (1)\u00123\n4\u0013\n+ (0)\u00121\n4\u0013\n=3\n4.\nCenter of mass . How would you interpret the result of this example? Does it mean\nthat, on average, we will get 3 /4 heads (but there is not anything called 3 /4 heads!). Recall\nthe definition of a random variable: it is a translator that translates a descriptive state\nto a number on the real line. Thus the expectation, which is an operation defined on the\nreal line, can only tell us what is happening on the real line, not in the original sample\n127", "143": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nFigure 3.17: Center of mass. If a state x2is more influential than another state x1, the center of mass\nE[X]will lean towards x2.\nspace. On the real line, the expectation can be regarded as the center of mass , which is the\npoint where the \u201cforces\u201d between the two states are \u201cbalanced\u201d. In Figure 3.17 we depict a\nrandom variable with two states x1andx2. The state x1has less influence (because pX(x1)\nis smaller) than x2. Therefore the center of mass is shifted towards x2. This result shows us\nthat the value E[X] is not necessarily in the sample space. E[X] is a deterministic number\nwith nothing to do with the sample space.\nExample 3.10 . Let Xbe a random variable with PMF pX(k) =1\n2k, fork= 1,2,3, . . ..\nThe expectation is\nE[X] =\u221eX\nk=1kpX(k) =\u221eX\nk=1k\u00b71\n2k\n=1\n2\u221eX\nk=1k\u00b71\n2k\u22121=1\n2\u00b71\n(1\u22121\n2)2= 2.\nOn MATLAB and Python, if you want to verify this answer you can use the following\ncode. Here, we approximate the infinite sum by a finite sum of k= 1, . . . , 100.\n% MATLAB code to compute the expectation\nk = 1:100;\np = 0.5.^k;\nEX = sum(p.*k);\n# Python code to compute the expectation\nimport numpy as np\nk = np.arange(100)\np = np.power(0.5,k)\nEX = np.sum(p*k)\nExample 3.11 . Roll a die twice. Let Xbe the first roll and Ybe the second roll.\nLetZ= max( X, Y). To compute the expectation E[Z], we first construct the sample\nspace. Since there are two rolls, we can construct a table listing all possible pairs of\noutcomes. This will give us {(1,1),(1,2), . . . , (6,6)}. Now, we calculate Z, which is the\nmax of the two rolls. So if we have (1 ,3), then the max will be 3, whereas if we have\n(5,2), then the max will be 5. We can complete a table as shown below.\n128", "144": "3.4. EXPECTATION\n1 2 3 4 5 6\n11 2 3 4 5 6\n22 2 3 4 5 6\n33 3 3 4 5 6\n44 4 4 4 5 6\n55 5 5 5 5 6\n66 6 6 6 6 6\nThis table tell us that Zhas 6 states. The PMF of Zcan be determined by\ncounting the number of times a state shows up in the table. Thus, we can show that\npZ(1) =1\n36, pZ(2) =3\n36, pZ(3) =5\n36,\npZ(4) =7\n36, pZ(5) =9\n36, pZ(6) =11\n36.\nThe expectation of Zis therefore\nE[Z] = (1)\u00121\n36\u0013\n+ (2)\u00123\n36\u0013\n+ (3)\u00125\n36\u0013\n+ (4)\u00127\n36\u0013\n+ (5)\u00129\n36\u0013\n+ (6)\u001211\n36\u0013\n=161\n36.\nExample 3.12 . Consider a game in which we flip a coin 3 times. The reward of the\ngame is\n\u2022 $1 if there are 2 heads\n\u2022 $8 if there are 3 heads\n\u2022 $0 if there are 0 or 1 head\nThere is a cost associated with the game. To enter the game, the player has to pay\n$1.50. We want to compute the net gain, on average.\nTo answer this question, we first note that the sample space contains 8 elements:\nHHH, HHT, HTH, THH, THT, TTH, HTT, TTT. Let Xbe the number of heads.\nThen the PMF of Xis\npX(0) =1\n8, pX(1) =3\n8, pX(2) =3\n8, pX(3) =1\n8.\nWe then let Ybe the reward. The PMF of Ycan be found by \u201cadding\u201d the probabilities\nofX. This yields\npY(0) = pX(0) + pX(1) =4\n8, pY(1) = pX(2) =3\n8, pY(8) = pX(3) =1\n8.\n129", "145": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nThe expectation of Yis\nE[X] = (0)\u00124\n8\u0013\n+ (1)\u00123\n8\u0013\n+ (8)\u00121\n8\u0013\n=11\n8.\nSince the cost of the game is12\n8, the net gain (on average) is \u22121\n8.\n3.4.2 Existence of expectation\nDoes every PMF have an expectation? No, because we can construct a PMF such that the\nexpectation is undefined.\nExample 3.13 . Consider a random variable Xwith the following PMF:\npX(k) =6\n\u03c02k2, k = 1,2, . . . .\nUsing a result from algebra, one can show thatP\u221e\nk=11\nk2=\u03c02\n6. Therefore, pX(k) is a\nlegitimate PMF becauseP\u221e\nk=1pX(k) = 1. However, the expectation diverges, because\nE[X] =\u221eX\nk=1kpX(k)\n=6\n\u03c02\u221eX\nk=11\nk\u2192 \u221e ,\nwhere the limit is due to the harmonic seriesa: 1 +1\n2+1\n3+\u00b7\u00b7\u00b7=\u221e.\nahttps://en.wikipedia.org/wiki/Harmonic_series_(mathematics)\nA PMF has an expectation when it is absolutely summable .\nDefinition 3.5. A discrete random variable Xisabsolutely summable if\nE[|X|]def=X\nx\u2208X(\u2126)|x|pX(x)<\u221e. (3.13)\nThis definition tells us that not all random variables have a finite expectation. This\nis a very important mathematical result, but its practical implication is arguably limited.\nMost of the random variables we use in practice are absolutely summable. Also, note that\nthe property of absolute summability applies to discrete random variables. For continuous\nrandom variables, we have a parallel concept called absolute integrability , which will be\ndiscussed in the next chapter.\n3.4.3 Properties of expectation\nThe expectation of a random variable has several useful properties. We list them below.\nNote that these properties apply to both discrete and continuous random variables.\n130", "146": "3.4. EXPECTATION\nTheorem 3.4. The expectation of a random variable Xhas the following properties:\n(i)Function . For any function g,\nE[g(X)] =X\nx\u2208X(\u2126)g(x)pX(x).\n(ii)Linearity . For any function gandh,\nE[g(X) +h(X)] =E[g(X)] +E[h(X)].\n(iii) Scale . For any constant c,\nE[cX] =cE[X].\n(iv)DC Shift . For any constant c,\nE[X+c] =E[X] +c.\nProof of (i) : A pictorial proof of (i) is shown in Figure 3.18 . The key idea is a change of\nvariable.\nFigure 3.18: By letting g(X) =Y, the PMFs are not changed. What changes are the states.\nWhen we have a function Y=g(X), the PMF of Ywill have impulses moved from x\n(the horizontal axis) to g(x) (the vertical axis). The PMF values (i.e., the probabilities or\nthe height of the stems), however, are not changed. If the mapping g(X) is many-to-one,\nmultiple PMF values will add to the same position. Therefore, when we compute E[g(X)],\nwe compute the expectation along the vertical axis.\nPractice Exercise 3.3 . Prove statement (iii): For any constant c,E[cX] =cE[X].\nSolution . Recall the definition of expectation:\nE[cX] =X\nx\u2208X(\u2126)(cx)pX(x) =cX\nx\u2208X(\u2126)xpX(x)\n|{z }\n=E[X]=cE[X].\nStatement (iii) is illustrated in Figure 3.19 . Here, we assume that the original PMF has 3\n131", "147": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nstates X= 0,1,2. We multiply Xby a constant c= 3. This changes XtocX= 0,3,6.\nHowever, since the probabilities are not changed, the height of the PMF values remains.\nTherefore, when computing the expectation, we just multiply E[X] bycto get cE[X].\nFigure 3.19: Pictorial representation of E[cX] =cE[X]. When we multiply Xbyc, we fix the probabil-\nities but make the spacing between states wider/narrower.\nPractice Exercise 3.4 . Prove statement (ii): For any function gandh,E[g(X) +\nh(X)] =E[g(X)] +E[h(X)].\nSolution . Recall the definition of expectation:\nE[g(X) +h(X)] =X\nx\u2208X(\u2126)[g(x) +h(x)]pX(x)\n=X\nx\u2208X(\u2126)g(x)pX(x)\n| {z }\n=E[g(X)]+X\nx\u2208X(\u2126)h(x)pX(x)\n| {z }\n=E[h(X)]\n=E[g(X)] +E[h(X)].\nPractice Exercise 3.5 . Prove statement (iv): For any constant c,E[X+c] =E[X]+c.\nSolution . Recall the definition of expectation:\nE[X+c] =X\nx\u2208X(\u2126)(x+c)pX(x)\n=X\nx\u2208X(\u2126)xpX(x)\n|{z }\n=E[X]+c\u00b7X\nx\u2208X(\u2126)pX(x)\n|{z}\n=1\n=E[X] +c.\nThis result is illustrated in Figure 3.20 . As we add a constant to the random variable,\nits PMF values remain the same but their positions are shifted. Therefore, when computing\nthe mean, the mean will be shifted accordingly.\n132", "148": "3.4. EXPECTATION\nFigure 3.20: Pictorial representation of E[X+c] =E[X]+c. When we add ctoX, we fix the probabilities\nand shift the entire PMF to the left or to the right.\nExample 3.14 . Let Xbe a random variable with four equally probable states 0 ,1,2,3.\nWe want to compute the expectation E[cos(\u03c0X/2)]. To do so, we note that\nE[cos(\u03c0X/2)] =X\nx\u2208X(\u2126)cos\u0012\u03c0X\n2\u0013\npX(x)\n= (cos 0)\u00121\n4\u0013\n+ (cos\u03c0\n2)\u00121\n4\u0013\n+ (cos2\u03c0\n2)\u00121\n4\u0013\n+ (cos3\u03c0\n2)\u00121\n4\u0013\n=1 + 0 + ( \u22121) + 0\n4= 0.\nExample 3.15 . Let Xbe a random variable with E[X] = 1 and E[X2] = 3. We want\nto find the expectation E[(aX+b)2]. To do so, we realize that\nE[(aX+b)2](a)=E[a2X2+ 2abX+b2](b)=a2E[X2] + 2abE[X] +b2= 3a2+ 2ab+b2,\nwhere ( a) is due to expansion of the square, and ( b) holds in two steps. The first step\nis to apply statement (ii) for individual functions of expectations, and the second step\nis to apply statement (iii) for scalar multiple of the expectations.\n3.4.4 Moments and variance\nBased on the concept of expectation, we can define a moment :\nDefinition 3.6. Thekth moment of a random variable Xis\nE[Xk] =X\nxxkpX(x). (3.14)\nEssentially, the kth moment is the expectation applied to Xk. The definition follows from\nstatement (i) of the expectation\u2019s properties. Using this definition, we note that E[X] is the\nfirst moment and E[X2] is the second moment. Higher-order moments can be defined, but\nin practice they are less commonly used.\n133", "149": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nExample 3.16 . Flip a coin 3 times. Let Xbe the number of heads. Then\npX(0) =1\n8, pX(1) =3\n8, pX(2) =3\n8, pX(3) =1\n8.\nThe second moment E[X2] is\nE[X2] = (0)2\u00121\n8\u0013\n+ (1)2\u00123\n8\u0013\n+ (2)2\u00123\n8\u0013\n+ (4)2\u00121\n8\u0013\n= 3.\nExample 3.17 . Consider a random variable Xwith PMF\npX(k) =1\n2k, k = 1,2, . . . .\nThe second moment E[X2] is\nE[X2] =\u221eX\nk=1k2\u00121\n2\u0013k\n=1\n22\u221eX\nk=1k(k\u22121 + 1)\u00121\n2\u0013k\u22122\n=1\n22\u221eX\nk=1k(k\u22121)\u00121\n2\u0013k\u22122\n+1\n22\u221eX\nk=1k\u00121\n2\u0013k\u22122\n=1\n22\u00122\n(1\u22121\n2)3\u0013\n+1\n2\u00121\n(1\u22121\n2)2\u0013\n= 6.\nUsing the second moment, we can define the variance of a random variable.\nDefinition 3.7. Thevariance of a random variable Xis\nVar[X] =E[(X\u2212\u00b5)2], (3.15)\nwhere \u00b5=E[X]is the expectation of X.\nWe denote \u03c32by Var[ X]. The square root of the variance, \u03c3, is called the standard deviation\nofX. Like the expectation E[X], the variance Var[ X] is computed using the ideal histogram\nPMF. It is the limiting object of the usual standard deviation we calculate from a dataset.\nOn a computer, computing the variance of a dataset is done by calling built-in com-\nmands such as varin MATLAB and np.var in Python. The standard deviation is computed\nusing stdandnp.std , respectively.\n% MATLAB code to compute the variance\nX = rand(10000,1);\nvX = var(X);\nsX = std(X);\n% Python code to compute the variance\nimport numpy as np\n134", "150": "3.4. EXPECTATION\nX = np.random.rand(10000)\nvX = np.var(X)\nsX = np.std(X)\nWhat does the variance mean? It is a measure of the deviation of the random variable\nXrelative to its mean. This deviation is quantified by the squared difference ( X\u2212\u00b5)2. The\nexpectation operator takes the average of the deviation, giving us a deterministic number\nE[(X\u2212\u00b5)2].\nTheorem 3.5. The variance of a random variable Xhas the following properties:\n(i)Moment .\nVar[X] =E[X2]\u2212E[X]2.\n(ii)Scale . For any constant c,\nVar[cX] =c2Var[X].\n(iii) DC Shift . For any constant c,\nVar[X+c] = Var[ X].\nFigure 3.21: Pictorial representations of Var[cX] =c2Var[X]andVar[X+c] = Var[ X].\nPractice Exercise 3.6 . Prove Theorem 3.5 above.\nSolution . For statement (i), we show that\nVar[X] =E[(X\u2212\u00b5)2] =E[X2\u22122X\u00b5+\u00b52] =E[X2]\u2212\u00b52.\n135", "151": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nStatement (ii) holds because E[cX] =c\u00b5and\nVar[cX] =E[(cX\u2212E[cX])2]\n=E[(cX\u2212c\u00b5)2] =c2E[(X\u2212\u00b5)2] =c2Var[X].\nStatement (iii) holds because\nVar[X+c] =E[((X+c)\u2212E[X+c])2] =E[(X\u2212E[X])2] = Var[ X].\nThe properties above are useful in various ways. The first statement provides a link connect-\ning variance and the second moment. Statement (ii) implies that when Xis scaled by c, the\nvariance should be scaled by c2because of the square in the second moment. Statement (iii)\nsays that when Xis shifted by a scalar c, the variance is unchanged. This is true because\nno matter how we shift the mean, the fluctuation of the random variable remains the same.\nPractice Exercise 3.7 . Flip a coin with probability pto get a head. Let Xbe a\nrandom variable denoting the outcome. The PMF of Xis\npX(0) = 1 \u2212p, p X(1) = p.\nFindE[X],E[X2] and Var[ X].\nSolution . The expectation of Xis\nE[X] = (0) pX(0) + (1) pX(1) = (0)(1 \u2212p) + (1)( p) =p.\nThe second moment is\nE[X2] = (0)2pX(0) + (1)2pX(1) = p.\nThe variance is\nVar[X] =E[X2]\u2212E[X]2=p\u2212p2=p(1\u2212p).\n3.5 Common Discrete Random Variables\nIn the previous sections, we have conveyed three key concepts: one about the random vari-\nable, one about the PMF, and one about the mean. The next step is to introduce a few\ncommonly used discrete random variables so that you have something concrete in your \u201ctool-\nbox.\u201d As we have mentioned before, these predefined random variables should be studied\nfrom a synthesis perspective (sometimes called generative ). The plan for this section is to\nintroduce several models, derive their theoretical properties, and discuss examples.\nNote that some extra effort will be required to understand the origins of the random\nvariables. The origins of random variables are usually overlooked, but they are more impor-\ntant than the equations. For example, we will shortly discuss the Poisson random variable\n136", "152": "3.5. COMMON DISCRETE RANDOM VARIABLES\nFigure 3.22: A Bernoulli random variable has two states with probability pand1\u2212p.\nand its PMF pX(k) =\u03bbke\u2212\u03bb\nk!. Why is the Poisson random variable defined in this way? If\nyou know how the Poisson PMF was originally derived, you will understand the assumptions\nmade during the derivation. Consequently, you will know why Poisson is a good model for\ninternet traffic, recommendation scores, and image sensors for computer vision applications.\nYou will also know under what situation the Poisson model will fail. Understanding the\nphysics behind the probability models is the focus of this section.\n3.5.1 Bernoulli random variable\nWe start discussing the simplest random variable, namely the Bernoulli random variable .\nA Bernoulli random variable is a coin-flip random variable. The random variable has two\nstates: either 1 or 0. The probability of getting 1 is p, and the probability of getting 0 is\n1\u2212p. See Figure 3.22 for an illustration. Bernoulli random variables are useful for all kinds\nof binary state events: coin flip (H or T), binary bit (1 or 0), true or false, yes or no, present\nor absent, Democrat or Republican, etc.\nTo make these notions more precise, we define a Bernoulli random variable as follows.\nDefinition 3.8. LetXbe aBernoulli random variable . Then, the PMF of Xis\npX(0) = 1 \u2212p, p X(1) = p,\nwhere 0< p < 1is called the Bernoulli parameter. We write\nX\u223cBernoulli( p)\nto say that Xis drawn from a Bernoulli distribution with a parameter p.\nIn this definition, the parameter pcontrols the probability of obtaining 1. In a coin-flip event,\npis usually1\n2, meaning that the coin is fair. However, for biased coins pis not necessarily1\n2.\nFor other situations such as binary bits (0 or 1), the probability of obtaining 1 could be very\ndifferent from the probability of obtaining 0.\nIn MATLAB and Python, generating Bernoulli random variables can be done by call-\ning the binomial random number generator np.random.binomial (Python) and binornd\n(MATLAB). When the parameter nis equal to 1, the binomial random variable is equiv-\nalent to a Bernoulli random variable. The MATLAB and Python codes to synthesize a\nBernoulli random variable are shown below.\n137", "153": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n% MATLAB code to generate 1000 Bernoulli random variables\np = 0.5;\nn = 1;\nX = binornd(n,p,[1000,1]);\n[num, ~] = hist(X, 10);\nbar(linspace(0,1,10), num,\u2018FaceColor\u2019,[0.4, 0.4, 0.8]);\n# Python code to generate 1000 Bernoulli random variables\nimport numpy as np\nimport matplotlib.pyplot as plt\np = 0.5\nn = 1\nX = np.random.binomial(n,p,size=1000)\nplt.hist(X,bins=\u2018auto\u2019)\nAn alternative method in Python is to call stats.bernoulli.rvs to generate random\nBernoulli numbers.\n# Python code to call scipy.stats library\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\np = 0.5\nX = stats.bernoulli.rvs(p,size=1000)\nplt.hist(X,bins=\u2018auto\u2019);\nProperties of Bernoulli random variables\nLet us now derive a few key statistical properties of a Bernoulli random variable.\nTheorem 3.6. IfX\u223cBernoulli( p), then\nE[X] =p, E[X2] =p, Var[X] =p(1\u2212p).\nProof . The expectation can be computed as\nE[X] = (1) pX(1) + (0) pX(0) = (1)( p) + (0)(1 \u2212p) =p.\nThe second moment is\nE[X2] = (12)(p) + (02)(1\u2212p) =p.\nTherefore, the variance is\nVar[X] =E[X2]\u2212\u00b52=p\u2212p2=p(1\u2212p).\n\u25a1\nA useful property of the Python code is that we can construct an object rv. Then we\ncan call rv\u2019s attributes to determine its mean, variance, etc.\n138", "154": "3.5. COMMON DISCRETE RANDOM VARIABLES\n# Python code to generate a Bernoulli rv object\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\np = 0.5\nrv = stats.bernoulli(p)\nmean, var = rv.stats(moments=\u2018mv\u2019)\nprint(mean, var)\nIn both MATLAB and Python, we can plot the PMF of a Bernoulli random variable,\nsuch as the one shown in Figure 3.23 . To do this in MATLAB, we call the function binopdf ,\nwith the evaluation points specified by x.\n00.20.40.60.81\n-0.2 0 0.2 0.4 0.6 0.8 1 1.2\nFigure 3.23: An example of a theoretical PMF (not the empirical histogram) plotted by MATLAB.\n% MATLAB code to plot the PMF of a Bernoulli\np = 0.3;\nx = [0,1];\nf = binopdf(x,1,p);\nstem(x, f, \u2018bo\u2019, \u2018LineWidth\u2019, 8);\nIn Python, we construct a random variable rv. With rv, we can call its PMF rv.pmf :\n# Python code to plot the PMF of a Bernoulli\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\np = 0.3\nrv = stats.bernoulli(p)\nx = np.linspace(0, 1, 2)\nf = rv.pmf(x)\nplt.plot(x, f, \u2018bo\u2019, ms=10);\nplt.vlines(x, 0, f, colors=\u2018b\u2019, lw=5, alpha=0.5);\n139", "155": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nWhen will a Bernoulli random variable have the maximum variance?\nLet us take a look at the variance of the Bernoulli random variable. For any given p, the\nvariance is p(1\u2212p). This is a quadratic equation. If we let V(p) =p(1\u2212p), we can show that\nthe maximum is attained at p= 1/2. To see this, take the derivative of V(p) with respect\ntop. This will give usd\ndpV(p) = 1 \u22122p. Equating to zero yields 1 \u22122p= 0, so p= 1/2.\nWe know that p= 1/2 is a maximum and not a minimum point because the second order\nderivative V\u2032\u2032(p) =\u22122, which is negative. Therefore V(p) is maximized at p= 1/2. Now,\nsince 0 \u2264p\u22641, we also know that V(0) = 0 and V(1) = 0. Therefore, the variance is\nminimized at p= 0 and p= 1.Figure 3.24 shows a graph of the variance.\nFigure 3.24: The variance of a Bernoulli reaches maximum at p= 1/2.\nDoes this result make sense? Why is the variance maximized at p= 1/2? If we think\nabout this problem more carefully, we realize that a Bernoulli random variable represents a\ncoin-flip experiment. If the coin is biased such that it always gives heads, on the one hand,\nit is certainly a bad coin. However, on the other hand, the variance is zero because there\nis nothing to vary; you will certainly get heads. The same situation happens if the coin is\nbiased towards tails. However, if the coin is fair, i.e., p= 1/2, then the variance is large\nbecause we only have a 50% chance of getting a head or a tail whenever we flip a coin.\nNothing is certain in this case. Therefore, the maximum variance happening at p= 1/2\nmatches our intuition.\nRademacher random variable\nA slight variation of the Bernoulli random variable is the Rademacher random variable ,\nwhich has two states: +1 and \u22121. The probability getting +1 and \u22121 is 1 /2. Therefore, the\nPMF of a Rademacher random variable is\npX(\u22121) =1\n2,and pX(+1) =1\n2.\nPractice Exercise 3.8 . Show that if Xis a Rademacher random variable then\n(X+ 1)/2\u223cBernoulli(1 /2). Also show the converse: If Y\u223cBernoulli(1 /2) then 2 Y\u22121\nis a Rademacher random variable.\nSolution . Since Xcan either be +1 or \u22121, we show that if X= +1 then ( X+1)/2 = 1\nand if X=\u22121 then ( X+ 1)/2 = 0. The probabilities of getting +1 and \u22121 are equal.\nThus, the probabilities of getting ( X+ 1)/2 = 1 and 0 are also equal. So the resulting\nrandom variable is Bernoulli(1/2). The other direction can be proved similarly.\n140", "156": "3.5. COMMON DISCRETE RANDOM VARIABLES\nBernoulli in social networks: the Erd\u02dd os-R\u00b4 enyi graph\nThe study of networks is a big branch of modern data science. It includes social networks,\ncomputer networks, traffic networks, etc. The history of network science is very long, but\none of the most basic models of a network is the Erd\u02dd os-R\u00b4 enyi graph, named after Paul\nErd\u02dd os and Alfr\u00b4 ed R\u00b4 enyi. The underlying probabilistic model of the Erd\u02dd os-R\u00b4 enyi graph is\nthe Bernoulli random variable.\nTo see how a graph can be constructed from a Bernoulli random variable, we first\nintroduce the concept of a graph . A graph contains two elements: nodes and edges. For\nnode iand node j, we denote the edge connecting iandjasAij. Therefore, if we have N\nnodes, then we can construct a matrix Aof size N\u00d7N. We call this matrix the adjacency\nmatrix . For example, the adjacency matrix\nA=\uf8ee\n\uf8ef\uf8ef\uf8f00 1 1 0\n1 0 0 0\n1 0 0 1\n0 0 1 0\uf8f9\n\uf8fa\uf8fa\uf8fb\nwill have edges for node pairs (1 ,2), (1 ,3), and (3 ,4). Note that in this example we assume\nthat the adjacency matrix is symmetric, meaning that the graph is undirected. The \u201c1\u201d in\nthe adjacency matrix indicates there is an edge, and \u201c0\u201d indicates there is no edge. So A\nrepresents a binary graph.\nThe Erd\u02dd os-R\u00b4 enyi graph model says that the probability of getting an edge is an inde-\npendent Bernoulli random variable. That is\nAij\u223cBernoulli( p),\nfori < j . If we model the graph in this way, then the parameter pwill control the density\nof the graph. High values of pmean that there is a higher chance for an edge to be present.\n-2 0 2-3-2-1012p = 0.3\n  1\n  2  3\n  4\n  5  6  7\n  8  9  10\n  11  12\n  13\n  14  15  16  17  18  19  20\n  21  22  23  24\n  25  26  27\n  28  29\n  30  31\n  32  33\n  34  35\n  36\n  37\n  38  39\n  40\n-2 0 2-4-3-2-10123p = 0.5\n  1\n  2  3  4\n  5  6  7\n  8\n  9\n  10  11\n  12  13  14  15\n  16  17\n  18   19  20\n  21\n  22\n  23  24  25\n  26  27  28\n  29  30\n  31  32  33\n  34\n  35\n  36\n  37  38\n  39  40\n-4 -2 0 2 4-4-3-2-10123p = 0.7\n  1\n  2\n  3  4\n  5  6\n  7\n  8  9  10\n  11  12\n  13  14\n  15  16  17\n  18\n  19  20  21\n  22  23\n  24\n  25  26  27  28\n  29  30  31\n  32  33\n  34  35  36\n  37  38  39\n  40\n-4 -2 0 2 4-4-2024p = 0.9\n  1\n  2  3\n  4  5\n  6  7\n  8\n  9  10\n  11  12  13\n  14  15\n  16\n  17  18\n  19  20\n  21  22\n  23  24\n  25\n  26  27  28\n  29\n  30  31  32  33  34  35\n  36\n  37  38\n  39  40\nFigure 3.25: The Erd\u02dd os-R\u00b4 enyi graph. [Top] The graphs. [Bottom] The adjacency matrices.\n141", "157": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nTo illustrate the idea of an Erd\u02dd os-R\u00b4 enyi graph, we show in Figure 3.25 a graph of\n40 nodes. The edges are randomly selected by flipping a Bernoulli random variable with\nparameter p= 0.3,0.5,0.7,0.9. As we can see in the figure, a small value of pgives a graph\nwith very sparse connectivity, whereas a large value of pgives a very densely connected\ngraph. The bottom row of Figure 3.25 shows the corresponding adjacency matrices. Here,\na white pixel denotes \u201c1\u201d in the matrix and a black pixel denotes \u201c0\u201d in the matrix.\nWhile Erd\u02dd os-R\u00b4 enyi graphs are elementary, their variations can be realistic models of\nsocial networks. The stochastic block model is one such model. In a stochastic block model,\nnodes form small communities within a large network. For example, there are many majors\nin a university. Students within the same major tend to have more interactions than with\nstudents of another major. The stochastic block model achieves this goal by partitioning\nthe nodes into communities. Within each community, the nodes can have a high degree of\nconnectivity. Across different communities, the connectivity will be much lower. Figure 3.26\nillustrates a network and the corresponding adjacency matrix. In this example, the network\nhas three communities.\n-4 -3 -2 -1 0 1 2 3 4-6-4-2024\n  1  2\n  3  4  5\n  6  7\n  8\n  9  10\n  11  12\n  13  14\n  15\n  16\n  17  18  19\n  20  21  22   23  24  25\n  26  27  28\n  29  30  31  32\n  33\n  34  35  36  37\n  38  39\n  40  41   42  43\n  44  45\n  46  47\n  48\n  49  50\n  51\n  52\n  53  54  55\n  56  57  58  59  60\n  61   62  63\n  64  65  66  67\n  68  69\n  70  71  72\n  73  74  75\n  76  77  78  79\n  80\n  81  82  83  84\n  85  86  87\n  88  89\n  90\n  91  92  93\n  94  95\n  96\n  97  98  99  100\nFigure 3.26: A stochastic block model containing three communities. [Left] The graph. [Right] The\nadjacency matrix.\nIn network analysis, one of the biggest problems is determining the community struc-\nture and recovering the underlying probabilities. The former task is about grouping the\nnodes into blocks. This is a nontrivial problem because in practice the nodes are never\narranged nicely, as shown in Figure 3.26 . For example, why should Alice be node 1 and\nBob be node 2? Since we never know the correct ordering of the nodes, partitioning the\nnodes into blocks requires various estimation techniques such as clustering or iterative esti-\nmation. Recovering the underlying probability is also not easy. Given an adjacency matrix,\nwhy can we assume that the underlying network is a stochastic block model? Even if the\nmodel is correct, there will be imperfect grouping in the previous step. As such, estimat-\ning the underlying probability in the presence of these uncertainties would pose additional\nchallenges.\nToday, network analysis remains one of the hottest areas in data science. Its importance\nderives from its broad scope and impact. It can be used to analyze social networks, opinion\npolls, marketing, or even genome analysis. Nevertheless, the starting point of these advanced\nsubjects is the Bernoulli random variable, the random variable of a coin flip!\n142", "158": "3.5. COMMON DISCRETE RANDOM VARIABLES\n3.5.2 Binomial random variable\nSuppose we flip the coin ntimes count the number of heads. Since each coin flip is a random\nvariable (Bernoulli), the sum is also a random variable. It turns out that this new random\nvariable is the binomial random variable .\nDefinition 3.9. LetXbe abinomial random variable . Then, the PMF of Xis\npX(k) =\u0012n\nk\u0013\npk(1\u2212p)n\u2212k, k = 0,1, . . . , n,\nwhere 0< p < 1is the binomial parameter, and nis the total number of states. We\nwrite\nX\u223cBinomial( n, p)\nto say that Xis drawn from a binomial distribution with a parameter pof size n.\nTo understand the meaning of a binomial random variable, consider a simple experiment\nconsisting of flipping a coin three times. We know that all possible cases are HHH, HHT,\nHTH, THH, TTH, THT, HTT and TTT. Now, suppose we define X= number of heads.\nWe want to write down the probability mass function. Effectively, we ask: What is the\nprobability of getting 0 head, one head, two heads, and three heads? We can, of course,\ncount and get the answer right away for a fair coin. However, suppose the coin is unfair,\ni.e., the probability of getting a head is pwhereas that of a tail is 1 \u2212p. The probability of\ngetting each of the 8 cases is shown in Figure 3.27 below.\nFigure 3.27: The probability of getting kheads out of n= 3coins.\nHere are the detailed calculations. Let us start with X= 3.\npX(3) =P[{HHH}]\n=P[{H} \u2229 {H} \u2229 {H}]\n(a)=P[{H}]P[{H}]P[{H}]\n(b)=p3,\nwhere ( a) holds because the three events are independent. (Recall that if AandBare\nindependent then P[A\u2229B] =P[A]P[B].) (b) holds because each P[{H}] =pby definition.\nWith exactly the same argument, we can show that pX(0) =P[{TTT}] = (1 \u2212p)3.\n143", "159": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nNow, let us look at pX(2), i.e., 2 heads. This probability can be calculated as follows:\npX(2) =P[{HHT} \u222a {HTH} \u222a {THH}]\n(c)=P[{HHT}] +P[{HTH}] +P[{THH}]\n(d)=p2(1\u2212p) +p2(1\u2212p) +p2(1\u2212p) = 3 p2(1\u2212p),\nwhere ( c) holds because the three events HHT, HTH and THH are disjoint in the sample\nspace. Note that we are not using the independence argument in ( c) but the disjoint argu-\nment. We should not confuse the two. The step in ( d) uses independence, because each coin\nflip is independent.\nThe above calculation shows an interesting phenomenon: Although the three events\nHHT, HTH, and THH are different (in fact, disjoint), the number of heads in all the cases\nis the same. This happens because when counting the number of heads, the ordering of the\nheads and tails does not matter. So the same problem can be formulated as finding the\nnumber of combinations of {2 heads and 1 tail }, which in our case is\u00003\n2\u0001\n= 3.\nTo complete the story, let us also try pX(1). This probability is\npX(1) =P[{TTH} \u222a {HTT} \u222a {THT}] = 3p(1\u2212p)2.\nAgain, we see that the combination\u00003\n1\u0001\n= 3 appears in front of the p(1\u2212p)2.\nIn general, the way to interpret the binomial random variable is to decouple the prob-\nabilities p, (1\u2212p), and the number of combinations\u0000n\nk\u0001\n:\npX(k) =\u0012n\nk\u0013\n|{z }\nnumber of combinationspk\n|{z}\nprob getting kH\u2019s(1\u2212p)n\u2212k\n| {z }\nprob getting n\u2212kT\u2019s.\nThe running index kshould go with 0 ,1, . . . , n . It starts with 0 because there could be zero\nheads in the sample space. Furthermore, we note that in this definition, two parameters are\ndriving a binomial random variable: the number of Bernoulli trials nand the underlying\nprobability for each coin flip p. As such, the notation for a binomial random variable is\nBinomial( n, p), with two arguments.\nThe histogram of a binomial random variable is shown in Figure 3.28 (a). Here, we con-\nsider the example where n= 10 and p= 0.5. To generate the histogram, we use 5000 samples.\nIn MATLAB and Python, generating binomial random variables as in Figure 3.28 (a) can\nbe done by calling binornd andnp.random.binomial .\n% MATLAB code to generate 5000 Binomial random variables\np = 0.5;\nn = 10;\nX = binornd(n,p,[5000,1]);\n[num, ~] = hist(X, 10);\nbar( num,\u2018FaceColor\u2019,[0.4, 0.4, 0.8]);\n# Python code to generate 5000 Binomial random variables\nimport numpy as np\nimport matplotlib.pyplot as plt\n144", "160": "3.5. COMMON DISCRETE RANDOM VARIABLES\n1 2 3 4 5 6 7 8 9 10020040060080010001200\n00.050.10.150.20.25\n0 2 4 6 8 10\n(a) Histogram based on 5000 samples (b) PMF\nFigure 3.28: An example of a binomial distribution with n= 10 ,p= 0.5.\np = 0.5\nn = 10\nX = np.random.binomial(n,p,size=5000)\nplt.hist(X,bins=\u2018auto\u2019);\nGenerating the ideal PMF of a binomial random variable as shown in Figure 3.28 (b)\ncan be done by calling binopdf in MATLAB. In Python, we can define a random variable\nrvthrough stats.binom , and call the PMF using rv.pmf .\n% MATLAB code to generate a binomial PMF\np = 0.5;\nn = 10;\nx = 0:10;\nf = binopdf(x,n,p);\nstem(x, f, \u2019o\u2019, \u2019LineWidth\u2019, 8, \u2019Color\u2019, [0.8, 0.4, 0.4]);\n# Python code to generate a binomial PMF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\np = 0.5\nn = 10\nrv = stats.binom(n,p)\nx = np.arange(11)\nf = rv.pmf(x)\nplt.plot(x, f, \u2019bo\u2019, ms=10);\nplt.vlines(x, 0, f, colors=\u2019b\u2019, lw=5, alpha=0.5);\nThe shape of the binomial PMF is shown in Figure 3.29 . In this set of figures, we vary\none of the two parameters nandpwhile keeping the other fixed. In Figure 3.29 (a), we fix\nn= 60 and plot three sets of p= 0.1,0.5,0.9. For small pthe PMF is skewed towards the\nleft, and for large pthe PMF is skewed toward the right. Figure 3.29 (b) shows the PMF\n145", "161": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nfor a fixed p= 0.5. As we increase n, the centroid of the PMF moves towards the right.\nThus we should expect the mean of a binomial random variable to increase with p. Another\ninteresting observation is that as nincreases, the shape of the PMF approaches the Gaussian\nfunction (the bell-shaped curve). We will explain the reason for this when we discuss the\nCentral Limit Theorem.\n00.050.10.150.2\n0 10 20 30 40 50 60p = 0.1\np = 0.5\np = 0.9\n00.10.20.30.4\n0 10 20 30 40 50 60n = 5\nn = 50\nn = 100\n(a)n= 60 (b) p= 0.5\nFigure 3.29: PMFs of a binomial random variable X\u223cBinomial (n, p). (a) We assume that n= 60 . By\nvarying the probability p, we see that the PMF shifts from the left to the right, and the shape changes.\n(b) We assume that p= 0.5. By varying the number of trials, the PMF shifts and the shape becomes\nmore \u201cbell-shaped.\u201d\nThe expectation, second moment, and variance of a binomial random variable are\nsummarized in Theorem 3.7.\nTheorem 3.7. IfX\u223cBinomial( n, p), then\nE[X] =np,\nE[X2] =np(np+ (1\u2212p)),\nVar[X] =np(1\u2212p).\nWe will prove that E[X] =npusing the first principle. For E[X2] and Var[ X], we will skip\nthe proofs here and will introduce a \u201cshortcut\u201d later.\nProof . Let us start with the definition.\nE[X] =nX\nk=0k\u00b7\u0012n\nk\u0013\npk(1\u2212p)n\u2212k\n=nX\nk=0k\u00b7n!\nk!(n\u2212k)!pk(1\u2212p)n\u2212k\n= 0\u00b7n!\n0!(n\u22120)!p0(1\u2212p)n\u22120\n| {z }\n0+nX\nk=1k\u00b7n!\nk!(n\u2212k)!pk(1\u2212p)n\u2212k\n=nX\nk=1n!\n(k\u22121)!(n\u2212k)!pk(1\u2212p)n\u2212k.\n146", "162": "3.5. COMMON DISCRETE RANDOM VARIABLES\nNote that we have shifted the index from k= 0 to k= 1. Now let us apply a trick:\nE[X] =nX\nk=1n!\n(k\u22121)!(n\u2212k)!pk(1\u2212p)n\u2212k\n=nX\nk=1n!\n(k\u22121)!(n\u2212k\u22121 + 1)!pk(1\u2212p)n\u2212k.\nUsing this trick, we can show that\nnX\nk=1n!\n(k\u22121)!(n\u2212k\u22121 + 1)!pk(1\u2212p)n\u2212k\n=nX\nk=1n!\n(k\u22121)!((n\u22121)\u2212(k\u22121))!pk(1\u2212p)n\u2212k\n=nX\nk=1n(n\u22121)!\n(k\u22121)!((n\u22121)\u2212(k\u22121))!pk(1\u2212p)n\u2212k\n=npnX\nk=1(n\u22121)!\n(k\u22121)!((n\u22121)\u2212(k\u22121))!pk\u22121(1\u2212p)n\u2212k\nWith a simple substitution of \u2113=k\u22121, the above equation can be rewritten as\nE[X] =np\u00b7n\u22121X\n\u2113=0(n\u22121)!\n\u2113!((n\u22121)\u2212\u2113)!p\u2113(1\u2212p)n\u22121\u2212\u2113\n=np\u00b7n\u22121X\n\u2113=0\u0012n\u22121\nk\u0013\np\u2113(1\u2212p)n\u22121\u2212\u2113\n| {z }\nsumming PMF of Binomial( n\u22121,p)=np.\n\u25a1\nIn MATLAB, the mean and variance of a binomial random variable can be found by\ncalling the command binostat(n,p) (MATLAB).\nIn Python, the command is rv = stats.binom(n,p) followed by calling rv.stats .\n% MATLAB code to compute the mean and var of a binomial rv\np = 0.5;\nn = 10;\n[M,V] = binostat(n, p)\n# Python code to compute the mean and var of a binomial rv\nimport scipy.stats as stats\np = 0.5\nn = 10\nrv = stats.binom(n,p)\nM, V = rv.stats(moments=\u2018mv\u2019)\nprint(M, V)\n147", "163": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nAn alternative view of the binomial random variable . As we discussed, the origin of a\nbinomial random variable is the sum of a sequence of Bernoulli random variables. Because\nof this intrinsic definition, we can derive some useful results by exploiting this fact. To do so,\nlet us define I1, . . . , I nas a sequence of Bernoulli random variables with Ij\u223cBernoulli( p)\nfor all i= 1, . . . , n . Then the resulting variable\nX=I1+I2+\u00b7\u00b7\u00b7+In\nis a binomial random variable of size nand parameter p. Using this definition, we can\ncompute the expectation as follows:\nE[X] =E[I1+I2+\u00b7\u00b7\u00b7+In]\n(a)=E[I1] +E[I2] +\u00b7\u00b7\u00b7+E[In]\n=p+p+\u00b7\u00b7\u00b7+p\n=np.\nIn this derivation, the step ( a) depends on a useful fact about expectation (which we have not\nyet proved): For any two random variables XandY, it holds that E[X+Y] =E[X]+E[Y].\nTherefore, we can show that the expectation of Xisnp. This line of argument not only\nsimplifies the proof but also provides a good intuition of the expectation. If each coin flip\nhas an expectation of E[Ii] =p, then the expectation of the sum should be simply ntimes\nofp, given np.\nHow about the variance? Again, we are going to use a very useful fact about variance:\nIf two random variables XandYare independent, then Var[ X+Y] = Var[ X] + Var[ Y].\nWith this result, we can show that\nVar[X] = Var[ I1+\u00b7\u00b7\u00b7+In]\n= Var[ I1] +\u00b7\u00b7\u00b7+ Var[ In]\n=p(1\u2212p) +\u00b7\u00b7\u00b7+p(1\u2212p)\n=np(1\u2212p).\nFinally, using the fact that Var[ X] =E[X2]\u2212\u00b52, we can show that\nE[X2] = Var[ X] +\u00b52\n=np(1\u2212p) + (np)2.\nPractice Exercise 3.9 . Show that the binomial PMF sums to 1.\nSolution . We use the binomial theorem to prove this result:\nnX\nk=0pX(k) =nX\nk=0\u0012n\nk\u0013\npk(1\u2212p)n\u2212k= (p+ (1\u2212p))n= 1.\nThe CDF of the binomial random variable is not very informative. It is basically the\ncumulative sum of the PMF:\nFX(k) =kX\n\u2113=0\u0012n\n\u2113\u0013\np\u2113(1\u2212p)n\u2212\u2113.\n148", "164": "3.5. COMMON DISCRETE RANDOM VARIABLES\n00.050.10.150.2\n0 5 10 15 20 25 30\n0 5 10 15 20 25 3000.20.40.60.81\nFigure 3.30: PMF and CDF of a binomial random variable X\u223cBinomial (n, p).\nThe shapes of the PMF and the CDF is shown in Figure 3.30 .\nIn MATLAB, plotting the CDF of a binomial can be done by calling the function\nbinocdf . You may also call f = binopdf(x,n,p) , and define F = cumsum(f) as the cumu-\nlative sum of the PMF. In Python, the corresponding command is rv = stats.binom(n,p)\nfollowed by rv.cdf .\n% MATLAB code to compute the mean and var of a binomial rv\nx = 0:10;\np = 0.5;\nn = 10;\nF = binocdf(x,n,p);\nfigure; stairs(x,F,\u2018.-\u2019,\u2018LineWidth\u2019,4,\u2018MarkerSize\u2019,30);\n# Python code to compute the mean and var of a binomial rv\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\np = 0.5\nn = 10\nrv = stats.binom(n,p)\nx = np.arange(11)\nF = rv.cdf(x)\nplt.plot(x, F, \u2019bo\u2019, ms=10);\nplt.vlines(x, 0, F, colors=\u2019b\u2019, lw=5, alpha=0.5);\n3.5.3 Geometric random variable\nIn some applications, we are interested in trying a binary experiment until we succeed. For\nexample, we may want to keep calling someone until the person picks up the call. In this\ncase, the random variable can be defined as the outcome of many failures followed by a final\nsuccess. This is called the geometric random variable .\nDefinition 3.10. LetXbe ageometric random variable . Then, the PMF of Xis\npX(k) = (1 \u2212p)k\u22121p, k = 1,2, . . . ,\n149", "165": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nwhere 0< p < 1is the geometric parameter. We write\nX\u223cGeometric( p)\nto say that Xis drawn from a geometric distribution with a parameter p.\nA geometric random variable is easy to understand. We define it as Bernoulli trials with\nk\u22121 consecutive failures followed by one success. This can be seen from the definition:\npX(k) = (1 \u2212p)k\u22121\n| {z }\nk\u22121 failuresp|{z}\nfinal success.\nNote that in geometric random variables, there is no\u0000n\nk\u0001\nbecause we must have k\u22121\nconsecutive failures before one success. There is no alternative combination of the sequence.\nThe histogram and PMF of a geometric random variable are illustrated in Figure 3.31 .\nHere, we assume that p= 0.5.\n0 1 2 3 4 5 6 7 8 9 10050010001500200025003000\n00.10.20.30.40.5\n0 2 4 6 8 10\n(a) Histogram based on 5000 samples (b) PMF\nFigure 3.31: An example of a geometric distribution with p= 0.5.\nIn MATLAB, generating geometric random variables can be done by calling the com-\nmands geornd . In Python, it is np.random.geometric .\n% MATLAB code to generate 1000 geometric random variables\np = 0.5;\nX = geornd(p,[5000,1]);\n[num, ~] = hist(X, 0:10);\nbar(0:10, num, \u2018FaceColor\u2019,[0.4, 0.4, 0.8]);\n# Python code to generate 1000 geometric random variables\nimport numpy as np\nimport matplotlib.pyplot as plt\np = 0.5\nX = np.random.geometric(p,size=1000)\nplt.hist(X,bins=\u2018auto\u2019);\nTo generate the PMF plots, in MATLAB we call geopdf and in Python we call\nrv = stats.geom followed by rv.pmf .\n150", "166": "3.5. COMMON DISCRETE RANDOM VARIABLES\n% MATLAB code to generate geometric PMF\np = 0.5; x = 0:10;\nf = geopdf(x,p);\nstem(x, f, \u2018o\u2019, \u2018LineWidth\u2019, 8, \u2018Color\u2019, [0.8, 0.4, 0.4]);\n# Python code to generate 1000 geometric random variables\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nx = np.arange(1,11)\nrv = stats.geom(p)\nf = rv.pmf(x)\nplt.plot(x, f, \u2018bo\u2019, ms=8, label=\u2018geom pmf\u2019)\nplt.vlines(x, 0, f, colors=\u2018b\u2019, lw=5, alpha=0.5)\nPractice Exercise 3.10 . Show that the geometric PMF sums to one.\nSolution . We can apply infinite series to show the result:\n\u221eX\nk=1pX(k) =\u221eX\nk=1(1\u2212p)k\u22121p\n=p\u00b7\u221eX\nk=1(1\u2212p)k\u22121, \u2113 =k\u22121\n=p\u00b7\u221eX\n\u2113=0(1\u2212p)\u2113\n=p\u00b71\n1\u2212(1\u2212p)= 1.\nIt is interesting to compare the shape of the PMFs for various values of p. InFigure 3.32\nwe show the PMFs. We vary the parameter p= 0.25,0.5,0.9. For small p, the PMF starts\nwith a low value and decays at a slow speed. The opposite happens for a large p, where the\nPMF starts with a high value and decays rapidly.\nFurthermore, we can derive the following properties of the geometric random variable.\nTheorem 3.8. IfX\u223cGeometric( p), then\nE[X] =1\np,E[X2] =2\np2\u22121\np, (3.16)\nVar[X] =1\u2212p\np2.\nProof . We will prove that the mean is 1 /pand leave the second moment and variance as\n151", "167": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n00.20.40.60.81\n0 2 4 6 8p = 0.25\n00.20.40.60.81\n0 2 4 6 8p = 0.5\n00.20.40.60.81\n0 2 4 6 8p = 0.9\nFigure 3.32: PMFs of a geometric random variable X\u223cGeometric (p).\nan exercise.\nE[X] =\u221eX\nk=1kp(1\u2212p)k\u22121=p \u221eX\nk=1k(1\u2212p)k\u22121!\n(a)=p\u00121\n(1\u2212(1\u2212p))2\u0013\n=1\np,\nwhere ( a) follows from the infinite series identity in Chapter 1.\n\u25a1\n3.5.4 Poisson random variable\nIn many physical systems, the arrivals of events are typically modeled as a Poisson ran-\ndom variable, e.g., photon arrivals, electron emissions, and telephone call arrivals. In social\nnetworks, the number of conversations per user can also be modeled as a Poisson. In e-\ncommerce, the number of transactions per paying user is again modeled using a Poisson.\nDefinition 3.11. LetXbe aPoisson random variable . Then, the PMF of Xis\npX(k) =\u03bbk\nk!e\u2212\u03bb, k = 0,1,2, . . . ,\nwhere \u03bb >0is the Poisson rate. We write\nX\u223cPoisson( \u03bb)\nto say that Xis drawn from a Poisson distribution with a parameter \u03bb.\nIn this definition, the parameter \u03bbdetermines the rate of the arrival. The histogram and\nPMF of a Poisson random variable are illustrated in Figure 3.33 . Here, we assume that\n\u03bb= 1.\nThe MATLAB code and Python code used to generate the histogram are shown below.\n% MATLAB code to generate 5000 Poisson numbers\nlambda = 1;\nX = poissrnd(lambda,[5000,1]);\n152", "168": "3.5. COMMON DISCRETE RANDOM VARIABLES\n0 1 2 3 4 5 6 7 8 9 100500100015002000\n00.10.20.30.4\n0 2 4 6 8 10\n(a) Histogram based on 5000 samples (b) PMF\nFigure 3.33: An example of a Poisson distribution with \u03bb= 1.\n[num, ~] = hist(X, 0:10);\nbar(0:10, num, \u2018FaceColor\u2019,[0.4, 0.4, 0.8]);\n# Python code to generate 5000 Poisson random variables\nimport numpy as np\nimport matplotlib.pyplot as plt\nlambd = 1\nX = np.random.poisson(lambd,size=5000)\nplt.hist(X,bins=\u2018auto\u2019);\nFor the PMF, in MATLAB we can call poisspdf , and in Python we can call rv.pmf\nwith rv = stats.poisson .\n% MATLAB code to plot the Poisson PMF\nlambda = 1;\nx = 0:10;\nf = poisspdf(x,lambda);\nstem(x, f, \u2018o\u2019, \u2018LineWidth\u2019, 8, \u2018Color\u2019, [0.8, 0.4, 0.4]);\n# Python code to plot the Poisson PMF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nx = np.arange(0,11)\nrv = stats.poisson(lambd)\nf = rv.pmf(x)\nplt.plot(x, f, \u2018bo\u2019, ms=8, label=\u2018geom pmf\u2019)\nplt.vlines(x, 0, f, colors=\u2018b\u2019, lw=5, alpha=0.5)\nThe shape of the Poisson PMF changes with \u03bb. As illustrated in Figure 3.34 ,pX(k) is\nmore concentrated at lower values for smaller \u03bband becomes spread out for larger \u03bb. Thus,\nwe should expect that the mean and variance of a Poisson random variable will change\n153", "169": "CHAPTER 3. DISCRETE RANDOM VARIABLES\ntogether as a function of \u03bb. In the same figure, we show the CDF of a Poisson random\nvariable. The CDF of a Poisson is\nFX(k) =P[X\u2264k] =kX\n\u2113=0\u03bb\u2113\n\u2113!e\u2212\u03bb. (3.17)\n00.10.20.30.4\n0 5 10 15 20 = 1\n = 4\n = 10\n0 5 10 15 2000.20.40.60.81\n = 1\n = 4\n = 10\nFigure 3.34: A Poisson random variable using different \u03bb\u2019s. [Left] Probability mass function pX(k).\n[Right] Cumulative distribution function FX(k).\nExample 3.18 . Let Xbe a Poisson random variable with parameter \u03bb. Find P[X > 4]\nandP[X\u22645].\nSolution .\nP[X > 4] = 1 \u2212P[X\u22644] = 1 \u22124X\nk=0\u03bbk\nk!e\u2212\u03bb,\nP[X\u22645] =5X\nk=0\u03bbk\nk!e\u2212\u03bb.\nPractice Exercise 3.11 . Show that the Poisson PMF sums to 1.\nSolution . We use the exponential series to prove this result:\n\u221eX\nk=0pX(k) =\u221eX\nk=0\u03bbk\nk!e\u2212\u03bb=e\u2212\u03bb\u00b7\u221eX\nk=0\u03bbk\nk!\n|{z}\n=e\u03bb= 1.\nPoisson random variables in practice\n(1) Computational photography . In computational photography, the Poisson random vari-\nable is one of the most widely used models for photon arrivals. The reason pertains to the\n154", "170": "3.5. COMMON DISCRETE RANDOM VARIABLES\norigin of the Poisson random variable, which we will discuss shortly. When photons are emit-\nted from the source, they travel through the medium as a sequence of independent events.\nDuring the integration period of the camera, the photons are accumulated to generate a\nvoltage that is then translated to digital bits.\nFigure 3.35: The Poisson random variable can be used to model photon arrivals.\nIf we assume that the photon arrival rate is \u03b1(photons per second), and suppose that\nthe total amount of integration time is t, then the average number of photons that the sensor\ncan see is \u03b1t. Let Xbe the number of photons seen during the integration time. Then if we\nfollow the Poisson model, we can write down the PMF of X:\nP[X=k] =(\u03b1t)k\nk!e\u2212\u03b1t.\nTherefore, if a pixel is bright, meaning that \u03b1is large, then Xwill have a higher likelihood\nof landing on a large number.\n(2) Traffic model . The Poisson random variable can be used in many other problems. For\nexample, we can use it to model the number of passengers on a bus or the number of spam\nphone calls. The required modification to Figure 3.35 is almost trivial: merely replace the\nphotons with your favorite cartoons, e.g., a person or a phone, as shown in Figure 3.36 . In\nthe United States, shared-ride services such as Uber and Lyft need to model the vacant cars\nand the passengers. As long as they have an arrival rate and certain degrees of independence\nbetween events, the Poisson random variable will be a good model.\nAs you can see from these examples, the Poisson random variable has broad applica-\nbility. Before we continue our discussion of its applications, let us introduce a few concepts\nrelated to the Poisson random variable.\nProperties of a Poisson random variable\nWe now derive the mean and variance of a Poisson random variable.\nTheorem 3.9. IfX\u223cPoisson( \u03bb), then\nE[X] =\u03bb, E[X2] =\u03bb+\u03bb2, (3.18)\nVar[X] =\u03bb.\n155", "171": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nFigure 3.36: The Poisson random variable can be used to model passenger arrivals and the number of\nphone calls, and can be used by Uber or Lyft to provide shared rides.\nProof . Let us first prove the mean. It can be shown that\nE[X] =\u221eX\nk=0k\u00b7\u03bbk\nk!e\u2212\u03bb=\u221eX\nk=1\u03bbk\n(k\u22121)!e\u2212\u03bb\n=\u03bbe\u2212\u03bb\u221eX\nk=1\u03bbk\u22121\n(k\u22121)!=\u03bbe\u2212\u03bb\u221eX\n\u2113=0\u03bb\u2113\n\u2113!=\u03bbe\u2212\u03bbe\u03bb=\u03bb.\nThe second moment can be computed as\nE[X2] =\u221eX\nk=0k2\u00b7\u03bbk\nk!e\u2212\u03bb\n=\u221eX\nk=0k\u00b7\u03bbk\n(k\u22121)!e\u2212\u03bb\n=\u221eX\nk=0(k\u22121 + 1) \u00b7\u03bbk\n(k\u22121)!e\u2212\u03bb\n=\u221eX\nk=1(k\u22121)\u00b7\u03bbk\n(k\u22121)!e\u2212\u03bb+\u221eX\nk=1\u03bbk\n(k\u22121)!e\u2212\u03bb\n=\u03bb2\u00b7\u221eX\nk=2\u03bbk\u22122e\u2212\u03bb\n(k\u22122)!\n|{z}\n=1+\u03bb\u00b7\u221eX\nk=1\u03bbk\u22121e\u2212\u03bb\n(k\u22121)!\n|{z}\n=1.\nThe variance can be computed using Var[ X] =E[X2]\u2212\u00b52.\n\u25a1\nTo compute the mean and variance of a Poisson random variable, we can call poisstat\nin MATLAB and rv.stats(moments=\u2018mv\u2019) in Python.\n% MATLAB code to compute Poisson statistics\nlambda = 1;\n[M,V] = poisstat(lambda);\n156", "172": "3.5. COMMON DISCRETE RANDOM VARIABLES\n# Python code to compute Poisson statistics\nimport scipy.stats as stats\nlambd = 1\nrv = stats.poisson(lambd)\nM, V = rv.stats(moments=\u2019mv\u2019)\nThe Poisson random variable is special in the sense that the mean and the variance are\nequal. That is, if the mean arrival number is higher, the variance is also higher. This is very\ndifferent from some other random variables, e.g., the normal random variable where the mean\nand variance are independent. For certain engineering applications such as photography, this\nplays an important role in defining the signal-to-noise ratio. We will come back to this point\nlater.\nOrigin of the Poisson random variable\nWe now address one of the most important questions about the Poisson random variable:\nWhere does it come from? Answering this question is useful because the derivation process\nwill reveal the underlying assumptions that lead to the Poisson PMF. When you change\nthe problem setting, you will know when the Poisson PMF will hold and when the Poisson\nPMF will fail.\nOur approach to addressing this problem is to consider the photon arrival process.\n(As we have shown, there is conceptually no difference if you replace the photons with\npedestrians, passengers, or phone calls.) Our derivation follows the argument of J. Goodman,\nStatistical Optics , Section 3.7.2.\nTo begin with, we consider a photon arrival process. The total number of photons\nobserved over an integration time tis defined as X(t). Because X(t) is a Poisson random\nvariable, its arguments must be integers. The probability of observing X(t) =kis therefore\nP[X(t) =k].Figure 3.37 illustrates the notations and concepts.\nFigure 3.37: Notations for deriving the Poisson PMF.\nWe propose three hypotheses with the photon arrival process:\n\u0088For sufficiently small \u2206 t, the probability of a small impulse occurring in the time\ninterval [ t, t+ \u2206t] is equal to the product of \u2206 tand the rate \u03bb, i.e.,\nP[X(t+ \u2206t)\u2212X(t) = 1] = \u03bb\u2206t.\nThis is a linearity assumption, which typically holds for a short duration of time.\n157", "173": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n\u0088For sufficiently small \u2206 t, the probability that more than one impulse falls in \u2206 tis\nnegligible. Thus, we have that P[X(t+ \u2206t)\u2212X(t) = 0] = 1 \u2212\u03bb\u2206t.\n\u0088The number of impulses in non-overlapping time intervals is independent.\nThe significance of these three hypotheses is that if the underlying photon arrival process\nviolates any of these assumptions, then the Poisson PMF will not hold. One example is the\npresence of scattering effects, where a photon has a certain probability of going off due to\nthe scattering medium and a certain probability of coming back. In this case, the events will\nno longer be independent.\nAssuming that these hypotheses hold, then at time t+\u2206t, the probability of observing\nX(t+ \u2206t) =kcan be computed as\nP[X(t+ \u2206t) =k]\n=P[X(t) =k]\u00b7 (1\u2212\u03bb\u2206t)|{z}\n=P[X(t+\u2206t)\u2212X(t)=0]+P[X(t) =k\u22121]\u00b7 (\u03bb\u2206t)|{z}\n=P[X(t+\u2206t)\u2212X(t)=1]\n=P[X(t) =k]\u2212P[X(t) =k]\u03bb\u2206t+P[X(t) =k\u22121]\u03bb\u2206t.\nBy rearranging the terms we show that\nP[X(t+ \u2206t) =k]\u2212P[X(t) =k]\n\u2206t=\u03bb\u0012\nP[X(t) =k\u22121]\u2212P[X(t) =k]\u0013\n.\nSetting the limit of \u2206 t\u21920, we arrive at an ordinary differential equation\nd\ndtP[X(t) =k] =\u03bb\u0012\nP[X(t) =k\u22121]\u2212P[X(t) =k]\u0013\n. (3.19)\nWe claim that the Poisson PMF, i.e.,\nP[X(t) =k] =(\u03bbt)k\nk!e\u2212\u03bbt,\nwould solve this differential equation. To see this, we substitute the PMF into the equation.\nThe left-hand side gives us\nd\ndtP[X(t) =k] =d\ndt\u0012(\u03bbt)k\nk!e\u2212\u03bbt\u0013\n=\u03bbk(\u03bbt)k\u22121\nk!e\u2212\u03bbt+ (\u2212\u03bb)(\u03bbt)k\nk!e\u2212\u03bbt\n=\u03bb(\u03bbt)k\u22121\nk!e\u2212\u03bbt\u2212\u03bb(\u03bbt)k\nk!e\u2212\u03bbt\n=\u03bb\u0012\nP[X(t) =k\u22121]\u2212P[X(t) =k]\u0013\n,\nwhich is the right-hand side of the equation. To retrieve the basic form of Poisson, we can\njust set t= 1 in the PMF so that\nP[X(1) = k] =\u03bbk\nk!e\u2212\u03bb.\n158", "174": "3.5. COMMON DISCRETE RANDOM VARIABLES\nThe origin of Poisson random variables\n\u0088We assume independent arrivals.\n\u0088Probability of seeing one event is linear with the arrival rate.\n\u0088Time interval is short enough so that you see either one event or no event.\n\u0088Poisson is derived by solving a differential equation based on these assumptions.\n\u0088Poisson becomes invalid when these assumptions are violated, e.g., in the case\nof scattering of photons due to turbid medium.\nThere is an alternative approach to deriving the Poisson PMF. The idea is to drive\nthe parameter nin the binomial random variable to infinity while pushing pto zero. In this\nlimit, the binomial PMF will converge to the Poisson PMF. We will discuss this shortly.\nHowever, we recommend the physics approach we have just described because it has a rich\nmeaning and allows us to validate our assumptions.\nPoisson approximation to binomial\nWe present one additional result about the Poisson random variable. The result shows that\nPoisson can be regarded as a limiting distribution of a binomial random variable.\nTheorem 3.10. (Poisson approximation to binomial) . For small pand large n,\n\u0012n\nk\u0013\npk(1\u2212p)n\u2212k\u2248\u03bbk\nk!e\u2212\u03bb,\nwhere \u03bbdef=np.\nBefore we prove the result, let us see how close the approximation can be. In Figure 3.38 ,\nwe show a binomial distribution and a Poisson approximation. The closeness of the approx-\nimation can easily be seen.\nIn MATLAB, the code to approximate a binomial distribution with a Poisson formula\nis shown below. Here, we draw 10,000 random binomial numbers and plot their histogram.\nOn top of the plot, we use poisspdf to compute the Poisson PMF. This gives us Figure 3.38 .\nA similar set of commands can be called in Python.\n% MATLAB code to approximate binomial using Poisson\nn = 1000; p = 0.05;\nX = binornd(n,p,[10000,1]);\nt = 0:100;\n[num,val] = hist(X,t);\nlambda = n*p;\nf_pois = poisspdf(t,lambda);\nbar(num/10000,\u2018FaceColor\u2019,[0.9 0.9 0],\u2018BarWidth\u2019,1); hold on;\nplot(f_pois, \u2018LineWidth\u2019, 4);\n159", "175": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n00.020.040.06Probability\n0 20 40 60 80 100 120\nkBinomial, n = 5000, p = 0.01\nPoisson,  = 50\nFigure 3.38: Poisson approximation of binomial distribution.\n# Python code to approximate binomial using Poisson\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nn = 1000; p = 0.05\nrv1 = stats.binom(n,p)\nX = rv1.rvs(size=10000)\nplt.figure(1); plt.hist(X,bins=np.arange(0,100));\nrv2 = stats.poisson(n*p)\nf = rv2.pmf(bin)\nplt.figure(2); plt.plot(f);\nProof . Let \u03bb=np. Then,\n\u0012n\nk\u0013\npk(1\u2212p)n\u2212k=n!\nk!(n\u2212k)!\u0012\u03bb\nn\u0013k\u0012\n1\u2212\u03bb\nn\u0013n\u2212k\n=\u03bbk\nk!n(n\u22121)\u00b7\u00b7\u00b7(n\u2212k+ 1)\nn\u00b7n\u00b7\u00b7\u00b7n\u0012\n1\u2212\u03bb\nn\u0013n\u2212k\n=\u03bbk\nk!(1)\u0012\n1\u22121\nn\u0013\n\u00b7\u00b7\u00b7\u0012\n1\u2212k\u22121\nn\u0013\n| {z }\n\u21921 asn\u2192\u221e\u0012\n1\u2212\u03bb\nn\u0013\u2212k\n|{z}\n\u21921 asn\u2192\u221e\u0012\n1\u2212\u03bb\nn\u0013n\n=\u03bbk\nk!\u0012\n1\u2212\u03bb\nn\u0013n\n.\nWe claim that\u0000\n1\u2212\u03bb\nn\u0001n\u2192e\u2212\u03bb. This can be proved by noting that\nlog(1 + x)\u2248x, x \u226a1.\nIt then follows that log\u0000\n1\u2212\u03bb\nn\u0001\n\u2248 \u2212\u03bb\nn. Hence,\u0000\n1\u2212\u03bb\nn\u0001n\u2248e\u2212\u03bb\n\u25a1\n160", "176": "3.5. COMMON DISCRETE RANDOM VARIABLES\nExample 3.19 . Consider an optical communication system. The bit arrival rate is 109\nbits/sec, and the probability of having one error bit is 10\u22129. Suppose we want to find\nthe probability of having five error bits in one second.\nLetXbe the number of error bits. In one second there are 109bits. Since we\ndo not know the location of these 5 bits, we have to enumerate all possibilities. This\nleads to a binomial distribution. Using the binomial distribution, we know that the\nprobability of having kerror bits is\nP[X=k] =\u0012n\nk\u0013\npk(1\u2212p)n\u2212k\n=\u0012109\nk\u0013\n(10\u22129)k(1\u221210\u22129)109\u2212k.\nThis quantity is difficult to calculate in floating-point arithmetic.\nUsing the Poisson to binomial approximation, we can see that the probability can\nbe approximated by\nP[X=k]\u2248\u03bbk\nk!e\u2212\u03bb,\nwhere \u03bb=np= 109(10\u22129) = 1. Setting k= 5 yields P[X= 5]\u22480.003.\nPhoton arrival statistics\nPoisson random variables are useful in computer vision, but you may skip this discussion\nif it is your first reading of the book.\nThe strong connection between Poisson statistics and physics makes the Poisson ran-\ndom variable a very good fit for many physical experiments. Here we demonstrate an appli-\ncation in modeling photon shot noise.\nAn image sensor is a photon sensitive device which is used to detect incoming photons.\nIn the simplest setting, we can model a pixel in the object plane as Xm,n, for some 2D\ncoordinate [ m, n]\u2208R2. Written as an array, an M\u00d7Nimage in the object plane can be\nvisualized as\nX= object =\uf8ee\n\uf8ef\uf8f0X1,1X1,2\u00b7\u00b7\u00b7 X1,N\n............\nXM,1XM,2\u00b7\u00b7\u00b7XM,N\uf8f9\n\uf8fa\uf8fb.\nWithout loss of generality, we assume that Xm,nis normalized so that 0 \u2264Xm,n\u22641 for\nevery coordinate [ m, n]. To model the brightness, we multiply Xm,nby a scalar \u03b1 > 0. If\na pixel \u03b1Xm,nhas a large value, then it is a bright pixel; conversely, if \u03b1Xm,nhas a small\nvalue, then it is a dark pixel. At a particular pixel location [ m, n]\u2208R2, the observed pixel\nvalue Ym,nis a random variable following the Poisson statistics. This situation is illustrated\n161", "177": "CHAPTER 3. DISCRETE RANDOM VARIABLES\ninFigure 3.39 , where we see that an object-plane pixel will generate an observed pixel\nthrough the Poisson PMF.1\nFigure 3.39: The image formation process is governed by the Poisson random variable. Given a pixel\nin the object plane Xm,n, the observed pixel Ym,nis a Poisson random variable with mean \u03b1Xm,n.\nTherefore, a brighter pixel will have a higher Poisson mean, whereas a darker pixel will have a lower\nPoisson mean.\nWritten as an array, the image is\nY= observed image = Poisson\u001a\n\u03b1X\u001b\n=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0Poisson {\u03b1X1,1}Poisson {\u03b1X1,2} \u00b7\u00b7\u00b7 Poisson {\u03b1X1,N}\nPoisson {\u03b1X2,1}Poisson {\u03b1X2,2} \u00b7\u00b7\u00b7 Poisson {\u03b1X2,N}\n............\nPoisson {\u03b1XM,1}Poisson {\u03b1XM,2} \u00b7\u00b7\u00b7 Poisson {\u03b1XM,N}\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nHere, by Poisson {\u03b1Xm,n}we mean that Ym,nis a random integer with probability mass\nP[Ym,n=k] =[\u03b1Xm,n]k\nk!e\u2212\u03b1Xm,n.\nNote that this model implies that the images seen by our cameras are more or less\nan array of Poisson random variables. (We say \u201cmore or less\u201d because of other sources of\nuncertainties such as read noise, dark current, etc.) Because the observed pixels Ym,nare\nrandom variables, they fluctuate about the mean values, and hence they are noisy. We refer\nto this type of random fluctuation as the shot noise . The impact of the shot noise can be\nseen in Figure 3.40 . Here, we vary the sensor gain level \u03b1. We see that for small \u03b1the image\nis dark and has much random fluctuation. As \u03b1increases, the image becomes brighter and\nthe fluctuation becomes smaller.\nIn MATLAB, simulating the Poisson photon arrival process for an image requires the\nimage-processing toolbox. The command to read an image is imread . Depending on the data\ntype, the input array could be unit8 integers. To convert them to floating-point numbers\nbetween 0 and 1, we use the command im2double . Drawing Poisson measurements from the\nclean image is done using poissrnd . Finally, we can use imshow to display the image.\n% MATLAB code to simulate a photon arrival process\nx0 = im2double(imread(\u2019cameraman.tif\u2019));\n1The color of an image is often handled by a color filter array , which can be thought of as a wavelength\nselector that allows a specific wavelength to pass through.\n162", "178": "3.5. COMMON DISCRETE RANDOM VARIABLES\n = 10\n  = 100\n  = 1000\nFigure 3.40: Illustration of the Poisson random variable in photographing images. Here, \u03b1denotes the\ngain level of the sensor: Larger \u03b1means that there are more photons coming to the sensor.\nX = poissrnd(10*x0);\nfigure(1); imshow(x0, []);\nfigure(2); imshow(X, []);\nSimilar commands can be found in Python with the help of the cv2library. When\nreading an image, we call cv2.imread . The option 0is used to read a gray-scale image;\notherwise, we will have a 3-channel color image. The division /255 ensures that the input\narray ranges between 0 to 1. Generating the Poisson random numbers can be done using\nnp.random.poisson , or by calling the statistics library with stats.poisson.rvs(10*x0) .\nTo display the images, we call plt.imshow , with the color map option set to cmap = \u2019gray\u2019 .\n# Python code code to simulate a photon arrival process\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nx0 = cv2.imread(\u2019./cameraman.tif\u2019, 0)/255\nplt.figure(1); plt.imshow(x0,cmap=\u2019gray\u2019);\nX = np.random.poisson(10*x0)\nplt.figure(2); plt.imshow(X, cmap=\u2019gray\u2019);\nWhy study Poisson? What is shot noise?\n\u0088The Poisson random variable is used to model photon arrivals.\n\u0088Shot noise is the random fluctuation of the photon counts at the pixels. Shot\nnoise is present even if you have an ideal sensor.\nSignal-to-noise ratio of Poisson\nNow let us answer a question we asked before. A Poisson random variable has a variance\nequal to the mean. Thus, if the scene is brighter, the variance will be larger. How come our\nsimulation in Figure 3.40 shows that the fluctuation becomes smaller as the scene becomes\nbrighter?\n163", "179": "CHAPTER 3. DISCRETE RANDOM VARIABLES\nThe answer to this question lies in the signal-to-noise ratio (SNR) of the Poisson\nrandom variable. The SNR of an image defines its quality. The higher the SNR, the better\nthe image. The mathematical definition of SNR is the ratio between the signal power and\nthe noise power. In our case, the SNR is\nSNR =signal power\nnoise powerdef=E[Y]p\nVar[Y](a)=\u03bb\u221a\n\u03bb=\u221a\n\u03bb,\nwhere Y=Ym,nis one of the observed pixels and \u03bb=\u03b1Xm,nis the the corresponding object\npixel. In this equation, the step ( a) uses the properties of the Poisson random variable Y\nwhere E[Y] = Var[ Y] =\u03bb. The result SNR =\u221a\n\u03bbis very informative. It says that if the\nunderlying mean photon flux (which is \u03bb) increases, the SNR increases at a rate of\u221a\n\u03bb.\nSo, yes, the variance becomes larger when the scene is brighter. However, the gain in signal\nE[Y] overrides the gain in noisep\nVar[Y]. As a result, the big fluctuation in bright images\nis compensated by the strong signal. Thus, to minimize the shot noise one has to use a\nlonger exposure to increase the mean photon flux. When the scene is dark and the aperture\nis small, shot noise is unavoidable.\nPoisson modeling is useful for describing the problem. However, the actual engineering\nquestion is that, given a noise observation Ym,n, how would you reconstruct the clean image\nXm,n? This is a very difficult inverse problem . The typical strategy is to exploit the spatial\ncorrelations between nearby pixels, e.g., usually smooth except along some sharp edges.\nOther information about the image, e.g., the likelihood of obtaining texture patterns, can\nalso be leveraged. Modern image-processing methods are rich, ranging from classical filtering\ntechniques to deep neural networks. Static images are easier to recover because we can often\nleverage multiple measurements of the same scene to boost the SNR. Dynamic scenes are\nsubstantially harder when we need to track the motion of any underlying objects. There are\nalso newer image sensors with better photon sensitivity. The problem of imaging in the dark\nis an important research topic in computational imaging . New solutions are developed at\nthe intersection of optics, signal processing, and machine learning.\nThe end of our discussions on photon statistics.\n3.6 Summary\nArandom variable is so called because it can take more than one state. The probability mass\nfunction specifies the probability for it to land on a particular state. Therefore, whenever\nyou think of a random variable you should immediately think of its PMF (or histogram\nif you prefer). The PMF is a unique characterization of a random variable. Two random\nvariables with the same PMF are effectively the same random variables. (They are not\nidentical because there could be measure-zero sets where the two differ.) Once you have the\nPMF, you can derive the CDF, expectation, moments, variance, and so on.\nWhen your boss hands a dataset to you, which random variable (which model) should\nyou use? This is a very practical and deep question. We highlight three steps for you to\nconsider:\n164", "180": "3.7. REFERENCES\n\u0088(i)Model selection : Which random variable is the best fit for our problem? Some-\ntimes we know by physics that, for example, photon arrivals or internet traffic follow a\nPoisson random variable. However, not all datasets can be easily described by simple\nmodels. The models we have learned in this chapter are called the parametric mod-\nels because they are characterized by one or two parameters. Some datasets require\nnonparametric models, e.g., natural images, because they are just too complex. Some\ndata scientists refer to deep neural networks as parametric models because the net-\nwork weights are essentially the parameters. Some do not because when the number\nof parameters is on the order of millions, sometimes even more than the number of\ntraining samples, it seems more reasonable to call these models nonparametric. How-\never, putting this debate aside, shortlisting a few candidate models based on prior\nknowledge is essential. Even if you use deep neural networks, selecting between con-\nvolutional structures versus long short-term memory models is still a legitimate task\nthat requires an understanding of your problem.\n\u0088(ii)Parameter estimation : Suppose that you now have a candidate model; the next\ntask is to estimate the model parameter using the available training data. For example,\nfor Poisson we need to determine \u03bb, and for binomial we need to determine ( n, p). The\nestimation problem is an inverse problem. Often we need to use the PMF to construct\ncertain optimization problems. By solving the optimization problem we will find the\nbest parameter (for that particular candidate model). Modern machine learning is\ndoing significantly better now than in the old days because optimization methods\nhave advanced greatly.\n\u0088(iii)Validation . When each candidate model has been optimized to best fit the data,\nwe still need to select the best model. This is done by running various testings. For\nexample, we can construct a validation set and check which model gives us the best\nperformance (such as classification rate or regression error). However, a model with\nthe best validation score is not necessarily the best model. Your goal should be to seek\nagood model and not the best model because determining the best requires access to\nthe testing data, which we do not have. Everything being equal, the common wisdom\nis to go with a simpler model because it is generally less susceptible to overfitting.\n3.7 References\nProbability textbooks\n3-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 2.\n3-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 3.\n3-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapters 3 and 4.\n3-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapters 2 and3.\n165", "181": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n3-5 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chap-\nter 4.\n3-6 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Chapters 2 and 4.\nAdvanced probability textbooks\n3-7 William Feller, An Introduction to Probability Theory and Its Applications , Wiley and\nSons, 3rd Edition, 1950.\n3-8 Andrey Kolmogorov, Foundations of the Theory of Probability , 2nd English Edition,\nDover 2018. (Translated from Russian to English. Originally published in 1950 by\nChelsea Publishing Company New York.)\nCross-validation\n3-9 Larry Wasserman, All of Statistics , Springer 2004. Chapter 20.\n3-10 Mats Rudemo, \u201cEmpirical Choice of Histograms and Kernel Density Estimators,\u201d\nScandinavian Journal of Statistics , Vol. 9, No. 2 (1982), pp. 65-78.\n3-11 David W. Scott, Multivariate Density Estimation: Theory, Practice, and Visualization ,\nWiley, 1992.\nPoisson statistics\n3-12 Joseph Goodman, Statistical Optics , Wiley, 2015. Chapter 3.\n3-13 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd edition, 2001. Section 1.10.\n3.8 Problems\nExercise 1. (Video Solution)\nConsider an information source that produces numbers kin the set SX={1,2,3,4}. Find\nand plot the PMF in the following cases:\n(a)pk=p1/k, fork= 1,2,3,4. Hint: Find p1.\n(b)pk+1=pk/2 for k= 1,2,3.\n(c)pk+1=pk/2kfork= 1,2,3.\n(d) Can the random variables in parts (a)-(c) be extended to take on values in the set\n{1,2, . . .}? Why or why not? Hint: You may use the fact that the series 1+1\n2+1\n3+\u00b7\u00b7\u00b7\ndiverges.\nExercise 2. (Video Solution)\nTwo dice are tossed. Let Xbe the absolute difference in the number of dots facing up.\n166", "182": "3.8. PROBLEMS\n(a) Find and plot the PMF of X.\n(b) Find the probability that X\u22642.\n(c) Find E[X] and Var[ X].\nExercise 3. (Video Solution)\nLetXbe a random variable with PMF pk=c/2kfork= 1,2, . . ..\n(a) Determine the value of c.\n(b) Find P(X > 4) and P(6\u2264X\u22648).\n(c) Find E[X] and Var[ X].\nExercise 4.\nLetXbe a random variable with PMF pk=c/2kfork=\u22121,0,1,2,3,4,5.\n(a) Determine the value of c.\n(b) Find P(1\u2264X < 3) and P(1< X\u22645).\n(c) Find P[X3<5].\n(d) Find the PMF and the CDF of X.\nExercise 5. (Video Solution)\nA modem transmits a +2 voltage signal into a channel. The channel adds to this sig-\nnal a noise term that is drawn from the set {0,\u22121,\u22122,\u22123}with respective probabilities\n{4/10,3/10,2/10,1/10}.\n(a) Find the PMF of the output Yof the channel.\n(b) What is the probability that the channel\u2019s output is equal to the input of the channel?\n(c) What is the probability that the channel\u2019s output is positive?\n(d) Find the expected value and variance of Y.\nExercise 6.\nOn a given day, your golf score takes values from numbers 1 through 10, with equal proba-\nbility of getting each one. Assume that you play golf for three days, and assume that your\nthree performances are independent. Let X1,X2, and X3be the scores that you get, and\nletXbe the minimum of these three numbers.\n(a) Show that for any discrete random variable X,pX(k) =P(X > k \u22121)\u2212P(X > k ).\n(b) What is the probability P(X1> k) for k= 1, . . . , 10?\n(c) Use (a), determine the PMF pX(k), for k= 1, . . . , 10.\n167", "183": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n(d) What is the average score improvement if you play just for one day compared with\nplaying for three days and taking the minimum?\nExercise 7. (Video Solution)\nLet\ng(X) =(\n1,ifX > 10\n0,otherwise .and h(X) =(\nX\u221210,ifX\u221210>0\n0, otherwise .\n(a) Find E[g(X)] for Xas in Problem 1(a) with SX={1, . . . , 15}.\n(b) Find E[h(X)] for Xas in Problem 1(b) with SX={1, . . . , 15}.\nExercise 8. (Video Solution)\nA voltage Xis uniformly distributed in the set {\u22123, . . . , 3,4}.\n(a) Find the mean and variance of X.\n(b) Find the mean and variance of Y=\u22122X2+ 3.\n(c) Find the mean and variance of W= cos( \u03c0X/8).\n(d) Find the mean and variance of Z= cos2(\u03c0X/8).\nExercise 9. (Video Solution)\n(a) If Xis Poisson( \u03bb), compute E[1/(X+ 1)].\n(b) If Xis Bernoulli( p) and Yis Bernoulli( q), compute E[(X+Y)3] ifXandYare\nindependent.\n(c) Let Xbe a random variable with mean \u00b5and variance \u03c32. Let \u2206( \u03b8) =E[(X\u2212\u03b8)2].\nFind \u03b8that minimizes the error \u2206( \u03b8).\n(d) Suppose that X1, . . . , X nare independent uniform random variables in {0,1, . . . , 100}.\nEvaluate P[min( X1, . . . , X n)> \u2113] for any \u2113\u2208 {0,1, . . . , 100}.\nExercise 10. (Video Solution)\n(a) Consider the binomial probability mass function pX(k) =\u0000n\nk\u0001\npk(1\u2212p)n\u2212k. Show that\nthe mean is E[X] =np.\n(b) Consider the geometric probability mass function pX(k) =p(1\u2212p)kfork= 0,1, . . ..\nShow that the mean is E[X] = (1 \u2212p)/p.\n(c) Consider the Poisson probability mass function pX(k) =\u03bbk\nk!e\u2212\u03bb. Show that the vari-\nance is Var[ X] =\u03bb.\n(d) Consider the uniform probability mass function pX(k) =1\nLfork= 1, . . . , L . Show that\nthe variance is Var[ X] =L2\u22121\n12. Hint: 1 + 2 + \u00b7\u00b7\u00b7+n=n(n+1)\n2and 12+ 22+\u00b7\u00b7\u00b7+n2=\nn3\n3+n2\n2+n\n6.\n168", "184": "3.8. PROBLEMS\nExercise 11. (Video Solution)\nAn audio player uses a low-quality hard drive. The probability that the hard drive fails after\nbeing used for one month is 1 /12. If it fails, the manufacturer offers a free-of-charge repair\nfor the customer. For the cost of each repair, however, the manufacturer has to pay $20.\nThe initial cost of building the player is $50, and the manufacturer offers a 1-year warranty.\nWithin one year, the customer can ask for a free repair up to 12 times.\n(a) Let Xbe the number of months when the player fails. What is the PMF of X? Hint:\nP[X= 1] may not be very high because if the hard drive fails it will be fixed by the\nmanufacturer. Once fixed, the drive can fail again in the remaining months. So saying\nX= 1 is equivalent to saying that there is only one failure in the entire 12-month\nperiod.\n(b) What is the average cost per player?\nExercise 12. (Video Solution)\nA binary communication channel has a probability of bit error of p= 10\u22126. Suppose that\ntransmission occurs in blocks of 10,000 bits. Let Nbe the number of errors introduced by\nthe channel in a transmission block.\n(a) What is the PMF of N?\n(b) Find P[N= 0] and P[N\u22643].\n(c) For what value of pwill the probability of 1 or more errors in a block be 99%?\nHint: Use the Poisson approximation to binomial random variables.\nExercise 13. (Video Solution)\nThe number of orders waiting to be processed is given by a Poisson random variable with\nparameter \u03b1=\u03bb/n\u00b5 , where \u03bbis the average number of orders that arrive in a day, \u00b5is the\nnumber of orders that an employee can process per day, and nis the number of employees.\nLet\u03bb= 5 and \u00b5= 1. Find the number of employees required so the probability that more\nthan four orders are waiting is less than 10%.\nHint: You need to use trial and error for a few n\u2019s.\nExercise 14.\nLetXbe the number of photons counted by a receiver in an optical communication system.\nIt is known that Xis a Poisson random variable with a rate \u03bb1when a signal is present and a\nPoisson random variable with the rate \u03bb0< \u03bb1when a signal is absent. The probability that\nthe signal is present is p. Suppose that we observe X=kphotons. We want to determine a\nthreshold Tsuch that if k\u2265Twe claim that the signal is present, and if k < T we claim\nthat the signal is absent. What is the value of T?\n169", "185": "CHAPTER 3. DISCRETE RANDOM VARIABLES\n170", "186": "Chapter 4\nContinuous Random Variables\nIf you are coming to this chapter from Chapter 3, we invite you to take a 30-second pause\nand switch your mind from discrete events to continuous events. Everything is continuous\nnow. The sample space is continuous, the event space is continuous, and the probability\nmeasure is continuous. Continuous random variables are similar in many ways to discrete\nrandom variables. They are characterized by the probability density functions (the continu-\nous version of the probability mass functions); they have cumulative distribution functions;\nthey have means, moments, and variances. The most significant difference is perhaps the use\nof integration instead of summation, but this change is conceptually straightforward, aside\nfrom the difficulties associated with integrating functions. So why do we need a separate\nchapter for continuous random variables? There are several reasons.\n\u0088First, how would you define the probability of a continuous event? Note that we cannot\ncount because a continuous event is uncountable. There is also nothing called the\nprobability mass because there are infinitely many masses. To define the probability\nof continuous events, we need to go back to our \u201cslogan\u201d: probability is a measure\nof the size of a set . Because probability is a measure, we can speak meaningfully\nabout the probability of continuous events so long as we have a well-defined measure\nfor them. Defining such a measure requires some effort. We will develop the intuitions\nand the formal definitions in Section 4.1. In Section 4.2, we will discuss the expectation\nand variance of continuous random variables.\n\u0088The second challenge is the unification between continuous and discrete random vari-\nables. Since the two types of random variables ultimately measure the size of a set, it\nis natural to ask whether we can unify them. Our approach to unifying them is based\non the cumulative distribution functions (CDFs), which are well-defined functions for\ndiscrete and continuous random variables. Based on the CDF and the fundamental\ntheorem of calculus, we can show that the probability density functions and proba-\nbility mass functions can be derived from the derivative of the CDFs. These will be\ndiscussed in Section 4.3, and in Section 4.4 we will discuss some additional results\nabout the mode and median.\n\u0088The third challenge is to understand several widely used continuous random variables.\nWe will discuss the uniform random variable and the exponential random variable\nin Section 4.5. Section 4.6 deals with the important topic of the Gaussian random\nvariable . Where does a Gaussian random variable come from? Why does it have a bell\n171", "187": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nshape? Why are Gaussian random variables so popular in data science? What are the\nuseful properties of Gaussian random variables? What are the relationships between\na Gaussian random variable and other random variables? These important questions\nwill be answered in Section 4.6.\n\u0088The final challenge is the transformation of random variables. Imagine that you have a\nrandom variable Xand a function g. What will the probability mass/density function\nofg(X) be? Addressing this problem is essential because almost all practical engineer-\ning problems involve the transformation of random variables. For example, suppose\nwe have voltage measurements and we would like to compute the power. This requires\ntaking the square of the voltage. We will discuss the transformation in Section 4.7,\nand we will also discuss an essential application in generating random numbers in\nSection 4.8.\n4.1 Probability Density Function\n4.1.1 Some intuitions about probability density functions\nLet\u2019s begin by outlining some intuitive reasoning, which is needed to define the probability\nof continuous events properly. These intuitions are based on the fact that probability is a\nmeasure . In the following discussion you will see a sequence of logical arguments for con-\nstructing such a measure for continuous events. Some arguments are discussed in Chapter 2,\nbut now we place them in the context of continuous random variables.\nSuppose we are given an event Athat is a subset in the sample space \u2126, as illustrated\ninFigure 4.1 . In order to calculate the probability of A, the measure perspective suggests\nthat we consider the relative size of the set\nP[{x\u2208A}] =\u201csize\u201d of A\n\u201csize\u201d of \u2126.\nThe right-hand side of this equation captures everything about the probability: It is a\nmeasure of the size of a set. It is relative to the sample space. It is a number between 0 and\n1. It can be applied to discrete sets, and it can be applied to continuous sets.\nFigure 4.1: [Left] An event Ain the sample space \u2126. The probability that Ahappens can be calculated\nas the \u201csize\u201d of Arelative to the \u201csize\u201d of \u2126. [Right] A specific example on the real line. Note that the\nsame definition of probability applies: The probability is the size of the interval Arelative to that of the\nsample space \u2126.\n172", "188": "4.1. PROBABILITY DENSITY FUNCTION\nHow do we measure the \u201csize\u201d of a continuous set? One possible way is by means of\nintegrating the length, area, or volume covered by the set. Consider an example: Suppose\nthat the sample space is the interval \u2126 = [0 ,5] and the event is A= [2,3]. To measure the\n\u201csize\u201d of A, we can integrate Ato determine the length. That is,\nP[{x\u2208[2,3]}] =\u201csize\u201d of A\n\u201csize\u201d of \u2126=R\nAdxR\n\u2126dx=R3\n2dx\nR5\n0dx=1\n5.\nTherefore, we have translated the \u201csize\u201d of a set to an integration. However, this definition\nis a very special case because when we calculate the \u201csize\u201d of a set, we treat all the elements\nin the set with equal importance. This is a strong assumption that will be relaxed later. But\nif you agree with this line of reasoning, we can rewrite the probability as\nP[{x\u2208A}] =R\nAdxR\n\u2126dx=R\nAdx\n|\u2126|\n=Z\nA1\n|\u2126||{z}\nequally important over \u2126dx.\nThis equation says that under our assumption (that all elements are equiprobable), the\nprobability of Ais calculated as the integration of Ausing an integrand 1 /|\u2126|(note that\n1/|\u2126|is a constant with respect to x). If we evaluate the probability of another event B, all\nwe need to do is to replace Awith Band computeR\nB1\n|\u2126|dx.\nWhat happens if we want to relax the \u201cequiprobable\u201d assumption? Perhaps we can\nadopt something similar to the probability mass function (PMF). Recall that a PMF pX\nevaluated at a point xis the probability that the state xhappens, i.e., pX(x) =P[X=x].\nSo,pX(x) is the relative frequency of x. Following the same line of thinking, we can define a\nfunction fXsuch that fX(x) tells us something related to the \u201crelative frequency\u201d. To this\nend, we can treat fXas a continuous histogram with infinitesimal bin width as shown in\nFigure 4.2 . Using this fX, we can replace the constant function 1 /|\u2126|with the new function\nfX(x). This will give us\nP[{x\u2208A}] =Z\nAfX(x)|{z}\nreplace 1 /|\u2126|dx. (4.1)\nIf we compare it with a PMF, we note that when Xis discrete,\nP[{x\u2208A}] =X\nx\u2208ApX(x).\nHence, fXcan be considered a continuous version of pX, although we do not recommend\nthis way of thinking for the following reason: pX(x) is a legitimate probability, but fX(x) is\nnot a probability. Rather, fXis the probability per unit length , meaning that we need to\nintegrate fX(times dx) in order to generate a probability value. If we only look at fXat\na point x, then this point is a measure-zero set because the length of this set is zero.\nEquation (4.1) should be familiar to you from Chapter 2. The function fX(x) is pre-\ncisely the weighting function we described in that chapter.\n173", "189": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nFigure 4.2: [Left] A probability mass function (PMF) tells us the relative frequency of a state when\ncomputing the probability. In this example, the \u201csize\u201d of AispX(x2) +pX(x3). [Right] A probability\ndensity function (PDF) is the infinitesimal version of the PMF. Thus, the \u201csize\u201d of Ais the integration\nover the PDF.\nWhat is a PDF?\n\u0088A PDF is the continuous version of a PMF.\n\u0088We integrate a PDF to compute the probability.\n\u0088We integrate instead of sum because continuous events are not countable.\nTo summarize, we have learned that when measuring the size of a continuous event,\nthe discrete technique (counting the number of elements) does not work. Generalizing to\ncontinuous space requires us to integrate the event. However, since different elements in an\nevent have different relative emphases, we use the probability density function fX(x) to tell\nus the relative frequency for a state xto happen. This PDF serves the role of the PMF.\n4.1.2 More in-depth discussion about PDFs\nA continuous random variable Xis defined by its probability density function fX. This\nfunction has to satisfy several criteria, summarized as follows.\nDefinition 4.1. A probability density function fXof a random variable Xis a map-\nping fX: \u2126\u2192R, with the properties\n\u0088Non-negativity :fX(x)\u22650for all x\u2208\u2126\n\u0088Unity :R\n\u2126fX(x)dx= 1\n\u0088Measure of a set :P[{x\u2208A}] =R\nAfX(x)dx\nIf all elements of the sample space are equiprobable, then the PDF is f(x) = 1 /|\u2126|. You can\neasily check that it satisfies all three criteria.\nLet us take a closer look at the three criteria:\n\u0088Non-negativity: The non-negativity criterion fX(x)\u22650 is reminiscent of Probability\nAxiom I. It says that no matter what xwe are looking at, the probability density\nfunction fXevaluated at xshould never give a negative value. Axiom I ensures that\nwe will not get a negative probability.\n174", "190": "4.1. PROBABILITY DENSITY FUNCTION\n\u0088Unity: The unity criterionR\n\u2126f(x)dx= 1 is reminiscent of Probability Axiom II,\nwhich says that measuring over the entire sample space will give 1.\n\u0088Measure of a set: The third criterion gives us a way to measure the size of an event A.\nIt says that since each x\u2208\u2126 has a different emphasis when calculating the size of\nA, we need to scale the elements properly. This scaling is done by the PDF fX(x),\nwhich can be regarded as a histogram with a continuous x-axis. The third criterion\nis a consequence of Probability Axiom III, because if there are two events AandB\nthat are disjoint, then P[{x\u2208A} \u222a {x\u2208B}] =R\nAfX(x)dx+R\nBfX(x)dxbecause\nfX(x)\u22650 for all x.\nIf the random variable Xtakes real numbers in 1D, then a more \u201cuser-friendly\u201d definition\nof the PDF can be given.\nDefinition 4.2. LetXbe a continuous random variable. The probability density\nfunction (PDF) ofXis a function fX: \u2126\u2192Rthat, when integrated over an interval\n[a, b], yields the probability of obtaining a\u2264X\u2264b:\nP[a\u2264X\u2264b] =Zb\nafX(x)dx. (4.2)\nThis definition is just a rewriting of the previous definition by explicitly writing out\nthe definition of Aas an interval [ a, b]. Here are a few examples.\nExample 4.1 . Let fX(x) = 3 x2with \u2126 = [0 ,1]. Let A= [0,0.5]. Then the probability\nP[{X\u2208A}] is\nP[0\u2264X\u22640.5] =Z0.5\n03x2dx=1\n8.\nExample 4.2 . Let fX(x) = 1 /|\u2126|with \u2126 = [0 ,5]. Let A= [3,5]. Then the probability\nP[{X\u2208A}] is\nP[3\u2264X\u22645] =Z5\n31\n|\u2126|dx=Z5\n31\n5dx=2\n5.\nExample 4.3 . Let fX(x) = 2 xwith \u2126 = [0 ,1]. Let A={0.5}. Then the probability\nP[{X\u2208A}] is\nP[X= 0.5] =P[0.5\u2264X\u22640.5] =Z0.5\n0.52x dx = 0.\nThis example shows that evaluating the probability at an isolated point for a contin-\nuous random variable will yield 0.\n175", "191": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nPractice Exercise 4.1 . Let Xbe the phase angle of a voltage signal. Without any\nprior knowledge about Xwe may assume that Xhas an equal probability of any value\nbetween 0 to 2 \u03c0. Find the PDF of Xand compute P[0\u2264X\u2264\u03c0/2].\nSolution . Since Xhas an equal probability for any value between 0 to 2 \u03c0, the PDF\nofXis\nfX(x) =1\n2\u03c0, for 0\u2264x\u22642\u03c0.\nTherefore, the probability P[0\u2264X\u2264\u03c0/2] can be computed as\nPh\n0\u2264X\u2264\u03c0\n2i\n=Z\u03c0/2\n01\n2\u03c0dx=1\n4.\nLooking at Equation (4.2), you may wonder: If the PDF fXis analogous to PMF\npX, why didn\u2019t we require 0 \u2264fX(x)\u22641 instead of requiring only fX(x)\u22650? This is\nan excellent question, and it points exactly to the difference between a PMF and a PDF.\nNotice that fXis a mapping from the sample space \u2126 to the real line R. It does not map\n\u2126 to [0 ,1]. On the other hand, since pX(x) is the actual probability, it maps \u2126 to [0 ,1].\nThus, fX(x) can take very large values but will not explode, because we have the unity\nconstraintR\n\u2126fX(x)dx= 1. Even if fX(x) takes a large value, it will be compensated by the\nsmall dx. If you recall, there is nothing like dxin the definition of a PMF. Whenever there\nis a probability mass, we need to sum or, putting it another way, the dxin the discrete case\nis always 1. Therefore, while the probability mass PMF must not exceed 1, a probability\ndensity PDF can exceed 1.\nIffX(x)\u22651, then what is the meaning of fX(x)? Isn\u2019t it representing the probability\nof having an element X=x? If it were a discrete random variable, then yes; pX(x) is the\nprobability of having X=x(so the probability mass cannot go beyond 1). However, for a\ncontinuous random variable, fX(x) isnotthe probability of having X=x. The probability\nof having X=x(i.e., exactly at x) is 0 because an isolated point has zero measure in the\ncontinuous space. Thus, even though fX(x) takes a value larger than 1, the probability of\nXbeing xis zero.\nAt this point you can see why we call PDF a density , or density function, because each\nvalue fX(x) is the probability per unit length . If we want to calculate the probability of\nx\u2264X\u2264x+\u03b4, for example, then according to our definition, we have\nP[x\u2264X\u2264x+\u03b4] =Zx+\u03b4\nxfX(x)dx\u2248fX(x)\u00b7\u03b4.\nTherefore, the probability of P[x\u2264X\u2264x+\u03b4] can be regarded as the \u201cper unit length\u201d\ndensity fX(x) multiplied with the \u201clength\u201d \u03b4. As\u03b4\u21920, we can see that P[X=x] = 0. See\nFigure 4.3 for an illustration.\nWhy are PDFs called a density function?\n\u0088Because fX(x) is the probability per unit length .\n\u0088You need to integrate fX(x) to obtain a probability.\n176", "192": "4.1. PROBABILITY DENSITY FUNCTION\nFigure 4.3: The probability P[x\u2264X\u2264x+\u03b4]can be approximated by the density fX(x)multiplied by\nthe length \u03b4.\nExample 4.4 . Consider a random variable Xwith PDF fX(x) =1\n2\u221axfor any\n0< x\u22641, and is 0 otherwise. We can show that fX(x)\u2192 \u221e asx\u21920. However,\nfX(x) remains a valid PDF because\nZ\u221e\n\u2212\u221efX(x)dx=Z1\n01\n2\u221axdx=\u221ax\f\f\f\f1\n0= 1.\nRemark . Since isolated points have zero measure in the continuous space, the probability\nof an open interval ( a, b) is the same as the probability of a closed interval:\nP[[a, b]] =P[(a, b)] =P[(a, b]] =P[[a, b)].\nThe exception is that when the PDF of fX(x) has a delta function at aorb. In this case,\nthe probability measure at aorbwill be non-zero. We will discuss this when we talk about\nthe CDFs.\nPractice Exercise 4.2 . Let fX(x) =c(1\u2212x2) for\u22121\u2264x\u22641, and 0 otherwise. Find\nthe constant c.\nSolution . SinceR\n\u2126fX(x)dx= 1, it follows that\nZ\n\u2126fX(x)dx=Z1\n\u22121c(1\u2212x2)dx=4c\n3\u21d2c= 3/4.\nPractice Exercise 4.3 . Let fX(x) =x2for|x| \u2264a, and 0 otherwise. Find a.\nSolution . Note that\nZ\n\u2126fX(x)dx=Za\n\u2212ax2dx=x3\n3\f\f\f\fa\n\u2212a=2a3\n3.\nSetting2a3\n3= 1 yields a=3q\n3\n2.\n177", "193": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\n4.1.3 Connecting with the PMF\nThe probability density function is more general than the probability mass function. To see\nthis, consider a discrete random variable Xwith a PMF pX(x). Because pXis defined on\na countable set \u2126, we can write it as a train of delta functions and define a corresponding\nPDF:\nfX(x) =X\nxk\u2208\u2126pX(xk)\u03b4(x\u2212xk).\nExample 4.5 . IfXis a Bernoulli random variable with PMF pX(1) = pandpX(0) =\n1\u2212p, then the corresponding PDF can be written as\nfX(x) =p \u03b4(x\u22121) + (1 \u2212p)\u03b4(x\u22120).\nExample 4.6 . IfXis a binomial random variable with PMF pX(k) =\u0000n\nk\u0001\npk(1\u2212p)n\u2212k,\nthen the corresponding PDF can be written as\nfX(x) =nX\nk=0pX(k)\u03b4(x\u2212k)\n=nX\nk=0\u0012n\nk\u0013\npk(1\u2212p)n\u2212k\u03b4(x\u2212k).\nStrictly speaking, delta functions are not really functions. They are defined through\nintegrations. They satisfy the properties that \u03b4(x\u2212xk) =\u221eifx=xk,\u03b4(x\u2212xk) = 0 if\nx\u0338=xk, andZxk+\u03f5\nxk\u2212\u03f5\u03b4(x\u2212xk)dx= 1,\nfor any \u03f5 >0. Suppose we ignore the fact that delta functions are not functions and merely\ntreat them as ordinary functions with some interesting properties. In this case, we can\nimagine that for every probability mass pX(xk), there exists an interval [ a, b] such that\nthere is one and only one state xkthat lies in [ a, b], as shown in Figure 4.4 .\nFigure 4.4: We can view a PMF as a train of impulses. When computing the probability X=xk, we\nintegrate the PMF over the interval [a, b].\n178", "194": "4.1. PROBABILITY DENSITY FUNCTION\nIf we want to calculate the probability of obtaining X=xk, we can show that\nP[X=xk](a)=P[a\u2264X\u2264b]\n=Zb\nafX(x)dx\n(b)=Zb\napX(xk)\u03b4(x\u2212xk)dx\n(c)=pX(xk)Zb\na\u03b4(x\u2212xk)dx\n| {z }\n=1=pX(xk).\nHere, step ( a) holds because within [ a, b], there is no other event besides X=xk. Step ( b)\nis just the definition of our fX(x) (inside the interval [ a, b]). Step ( c) shows that the delta\nfunction integrates to 1, thus leaving the probability mass pX(xk) as the final result. Let us\nlook at an example and then comment on this intuition.\nExample 4.7 . Let Xbe a discrete random variable with PMF\npX(k) =1\n2k, k = 1,2, . . .\nThe continuous representation of the PMF can be written as\nfX(x) =\u221eX\nk=1pX(k)\u03b4(x\u2212k) =\u221eX\nk=1\u00121\n2k\u0013\n\u03b4(x\u2212k).\nSuppose we want to compute the probability P[1\u2264X\u22642]. This can be computed as\nP[1\u2264X\u22642] =Z2\n1fX(x)dx=Z2\n1\u221eX\nk=1\u00121\n2k\u0013\n\u03b4(x\u2212k)dx\n=Z2\n1\u001a1\n2\u03b4(x\u22121) +1\n4\u03b4(x\u22122) +\u00b7\u00b7\u00b7\u001b\ndx\n=1\n2Z2\n1\u03b4(x\u22121)dx\n|{z }\n=1+1\n4Z2\n1\u03b4(x\u22122)dx\n|{z }\n=1\n+1\n8Z2\n1\u03b4(x\u22123)dx\n|{z }\n=0+\u00b7\u00b7\u00b7|{z}\n=0\n=1\n2+1\n4=3\n4.\nHowever, if we want to compute the probability P[1< X\u22642], then the integration\n179", "195": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nlimit will not include the number 1 and so the delta function will remain 0. Thus,\nP[1< X\u22642] =Z2\n1+fX(x)dx\n=1\n2Z2\n1+\u03b4(x\u22121)dx\n|{z }\n=0+1\n4Z2\n1+\u03b4(x\u22122)dx\n|{z }\n=1=1\n4.\nClosing remark . To summarize, we see that a PMF can be \u201cregarded\u201d as a PDF. We are\ncareful to put a quotation around \u201cregarded\u201d because PMF and PDF are defined for different\nevents. A PMF uses a discrete measure (i.e., a counter) for countable events, whereas a PDF\nuses a continuous measure (i.e., integration) for continuous events. The way we link the two is\nby using the delta functions. Using the delta functions is valid, but the argument we provide\nhere is intuitive rather than rigorous. It is not rigorous because the integration we use is still\nthe Riemann-Stieltjes integration, which does not handle delta functions. Therefore, while\nyou can treat a discrete PDF as a train of delta functions, it is important to remember the\nlimitations of the integrations we use.\n4.2 Expectation, Moment, and Variance\n4.2.1 Definition and properties\nAs with discrete random variables, we can define expectation for continuous random vari-\nables. The definition is analogous: Just replace the summation with integration.\nDefinition 4.3. Theexpectation of a continuous random variable Xis\nE[X] =Z\n\u2126x fX(x)dx. (4.3)\nExample 4.8 . (Uniform random variable ) Let Xbe a continuous random variable\nwith PDF fX(x) =1\nb\u2212afora\u2264x\u2264b, and 0 otherwise. The expectation is\nE[X] =Z\n\u2126xfX(x)dx=Zb\nax\u00b71\nb\u2212adx=1\nb\u2212aZb\nax dx\n|{z}\n=x2\n2\f\fb\na\n=1\nb\u2212a\u00b7b2\u2212a2\n2=a+b\n2.\n180", "196": "4.2. EXPECTATION, MOMENT, AND VARIANCE\nExample 4.9 . (Exponential random variable ) Let Xbe a continuous random variable\nwith PDF fX(x) =\u03bbe\u2212\u03bbx, forx\u22650. The expectation is\nE[X] =Z\u221e\n0x \u03bbe\u2212\u03bbxdx\n=\u2212Z\u221e\n0x de\u2212\u03bbx\n=\u2212xe\u2212\u03bbx\f\f\f\f\u221e\n0|{z}\n=0+Z\u221e\n0e\u2212\u03bbxdx\n=\u22121\n\u03bbe\u2212\u03bbx\f\f\f\f\u221e\n0|{z}\n=\u22121=1\n\u03bb,\nwhere the colored step is due to integration by parts.\nIf a function gis applied to the random variable X, the expectation can be found using\nthe following theorem.\nTheorem 4.1. Letg: \u2126\u2192Rbe a function and Xbe a continuous random variable.\nThen\nE[g(X)] =Z\n\u2126g(x)fX(x)dx. (4.4)\nExample 4.10 . (Uniform random variable ) Let Xbe a continuous random variable\nwith fX(x) =1\nb\u2212afora\u2264x\u2264b, and 0 otherwise. If g(\u00b7) = (\u00b7)2, then\nE[g(X)] =E[X2] =Z\n\u2126x2fX(x)dx\n=1\nb\u2212a\u00b7Zb\nax2dx\n|{z}\n=b3\u2212a3\n3=a2+ab+b2\n3.\nPractice Exercise 4.4 . Let \u0398 be a continuous random variable with PDF f\u0398(\u03b8) =1\n2\u03c0\nfor 0\u2264\u03b8\u22642\u03c0and is 0 otherwise. Let Y= cos( \u03c9t+ \u0398). Find E[Y].\nSolution . Referring to Equation (4.4), the function gis\ng(\u03b8) = cos( \u03c9t+\u03b8).\n181", "197": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nTherefore, the expectation E[Y] is\nE[Y] =Z2\u03c0\n0cos(\u03c9t+\u03b8)f\u0398(\u03b8)d\u03b8\n=1\n2\u03c0Z2\u03c0\n0cos(\u03c9t+\u03b8)d\u03b8= 0,\nwhere the last equality holds because the integral of a sinusoid over one period is 0.\nPractice Exercise 4.5 . Let A\u2286\u2126. Let IA(X) be an indicator function such that\nIA(X) =(\n1,ifX\u2208A,\n0,ifX\u0338\u2208A.\nFindE[IA(X)].\nSolution . The expectation is\nE[IA(X)] =Z\n\u2126IA(x)fX(x)dx=Z\nx\u2208AfX(x)dx=P[X\u2208A].\nSo the probability of {X\u2208A}can be equivalently represented in terms of expectation.\nPractice Exercise 4.6 . Is it true that E[1/X] = 1/E[X]?\nSolution . No. This is because\nE\u00141\nX\u0015\n=Z\n\u21261\nxfX(x)dx\u0338=1R\n\u2126xfX(x)dx=1\nE[X].\nAll the properties of expectation we learned in the discrete case can be translated to\nthe continuous case. Specifically, we have that\n\u0088E[aX] =aE[X]: A scalar multiple of a random variable will scale the expectation.\n\u0088E[X+a] =E[X]+a: Constant addition of a random variable will offset the expectation.\n\u0088E[aX+b] =aE[X] +b: Affine transformation of a random variable will translate to\nthe expectation.\nPractice Exercise 4.7 . Prove the above three statements.\nSolution . The third statement is just the sum of the first two statements, so we just\n182", "198": "4.2. EXPECTATION, MOMENT, AND VARIANCE\nneed to show the first two:\nE[aX] =Z\n\u2126axfX(x)dx=aZ\n\u2126xfX(x)dx=aE[X],\nE[X+a] =Z\n\u2126(x+a)fX(x)dx=Z\n\u2126xfX(x)dx+a=E[X] +a.\n4.2.2 Existence of expectation\nAs we discussed in the discrete case, not all random variables have an expectation.\nDefinition 4.4. A random variable Xhas an expectation if it is absolutely integrable ,\ni.e.,\nE[|X|] =Z\n\u2126|x|fX(x)dx <\u221e. (4.5)\nBeing absolutely integrable implies that the expectation is that E[|X|] is the upper\nbound of E[X].\nTheorem 4.2. For any random variable X,\n|E[X]| \u2264E[|X|]. (4.6)\nProof . Note that fX(x)\u22650. Therefore,\n\u2212|x|fX(x)\u2264x fX(x)\u2264 |x|, fX(x),\u2200x.\nThus, integrating all three terms yields\n\u2212Z\n\u2126|x|fX(x)dx\u2264Z\n\u2126x fX(x)dx\u2264Z\n\u2126|x|fX(x)dx,\nwhich is equivalent to \u2212E[|X|]\u2264E[X]\u2264E[|X|].\n\u25a1\nExample 4.11 . Here is a random variable whose expectation is undefined. Let Xbe\na random variable with PDF\nfX(x) =1\n\u03c0(1 +x2), x \u2208R.\nThis random variable is called the Cauchy random variable . We can show that\nE[X] =Z\u221e\n\u2212\u221ex\u00b71\n\u03c0(1 +x2)dx=1\n\u03c0Z\u221e\n0x\n(1 +x2)dx+1\n\u03c0Z0\n\u2212\u221ex\n(1 +x2)dx.\n183", "199": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nThe first integral gives\nZ\u221e\n0x\n(1 +x2)dx=1\n2log(1 + x2)\f\f\f\u221e\n0=\u221e,\nand the second integral gives \u2212\u221e. Since neither integral is finite, the expectation is\nundefined. We can also check the absolutely integrability criterion:\nE[|X|] =Z\u221e\n\u2212\u221e|x| \u00b71\n\u03c0(1 +x2)dx\n(a)= 2Z\u221e\n0x\n\u03c0(1 +x2)dx\u22652Z\u221e\n1x\n\u03c0(1 +x2)dx\n(b)\n\u22652Z\u221e\n1x\n\u03c0(x2+x2)dx=1\n\u03c0log(x)\f\f\f\u221e\n1=\u221e,\nwhere in (a) we use the fact that the function being integrated is even, and in (b) we\nlower-bound1\n1+x2\u22651\nx2+x2ifx >1.\n4.2.3 Moment and variance\nThe moment and variance of a continuous random variable can be defined analogously to\nthe moment and variance of a discrete random variable, replacing the summations with\nintegrations.\nDefinition 4.5. Thekth moment of a continuous random variable Xis\nE[Xk] =Z\n\u2126xkfX(x)dx. (4.7)\nDefinition 4.6. Thevariance of a continuous random variable Xis\nVar[X] =E[(X\u2212\u00b5)2] =Z\n\u2126(x\u2212\u00b5)2fX(x)dx, (4.8)\nwhere \u00b5def=E[X].\nIt is not difficult to show that the variance can also be expressed as\nVar[X] =E[X2]\u2212\u00b52,\nbecause\nVar[X] =E[(X\u2212\u00b5)2] =E[X2]\u22122E[X]\u00b5+\u00b52=E[X2]\u2212\u00b52.\n184", "200": "4.3. CUMULATIVE DISTRIBUTION FUNCTION\nPractice Exercise 4.8 . (Uniform random variable ) Let Xbe a continuous random\nvariable with PDF fX(x) =1\nb\u2212afora\u2264x\u2264b, and 0 otherwise. Find Var[ X].\nSolution . We have shown that E[X] =a+b\n2andE[X2] =a2+ab+b2\n3. Therefore, the\nvariance is\nVar[X] =E[X2]\u2212E[X]2\n=a2+ab+b2\n3\u2212\u0012a+b\n2\u00132\n=(b\u2212a)2\n12.\nPractice Exercise 4.9 . (Exponential random variable ) Let Xbe a continuous ran-\ndom variable with PDF fX(x) =\u03bbe\u2212\u03bbxforx\u22650, and 0 otherwise. Find Var[ X].\nSolution . We have shown that E[X] =1\n\u03bb. The second moment is\nE[X2] =Z\u221e\n0x2\u03bbe\u2212\u03bbxdx\n=\u0002\n\u2212x2e\u2212\u03bbx\u0003\u221e\n0+Z\u221e\n02xe\u2212\u03bbxdx\n=2\n\u03bbZ\u221e\n0x\u03bbe\u2212\u03bbxdx\n=2\n\u03bb\u00b71\n\u03bb=2\n\u03bb2.\nTherefore,\nVar[X] =E[X2]\u2212E[X]2\n=2\n\u03bb2\u22121\n\u03bb2=1\n\u03bb2.\n4.3 Cumulative Distribution Function\nWhen we discussed discrete random variables, we introduced the concept of cumulative\ndistribution functions (CDFs). One of the motivations was that if we view a PMF as a train\nof delta functions, they are technically not well-defined functions. However, it turns out that\nthe CDF is always a well-defined function. In this section, we will complete the story by first\ndiscussing the CDF for continuous random variables. Then, we will come back and show\nyou how the CDF can be derived for discrete random variables.\n185", "201": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\n4.3.1 CDF for continuous random variables\nDefinition 4.7. LetXbe a continuous random variable with a sample space \u2126 =R.\nThecumulative distribution function (CDF) ofXis\nFX(x)def=P[X\u2264x] =Zx\n\u2212\u221efX(x\u2032)dx\u2032. (4.9)\nThe interpretation of the CDF can be seen from Figure 4.5 . Given a PDF fX, the CDF\nFXevaluated at xis the integration of fXfrom\u2212\u221eup to a point x. The integration of fX\nfrom\u2212\u221etoxis nothing but the area under the curve of fX. Since fXis non-negative, the\nlarger value xwe use to evaluate in FX(x), the more area under the curve we are looking\nat. In the extreme when x=\u2212\u221e, we can see that FX(\u2212\u221e) = 0, and when x= +\u221ewe\nhave that FX(+\u221e) =R\u221e\n\u2212\u221efX(x)dx= 1.\nFigure 4.5: A CDF is the integral of the PDF. Thus, the height of a stem in the CDF corresponds to\nthe area under the curve of the PDF.\nPractice Exercise 4.10 . (Uniform random variable ) Let Xbe a continuous random\nvariable with PDF fX(x) =1\nb\u2212afora\u2264x\u2264b, and is 0 otherwise. Find the CDF of X.\nSolution . The CDF of Xis given by\nFX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, x \u2264a,Rx\n\u2212\u221efX(x\u2032)dx\u2032=Rx\na1\nb\u2212adx\u2032=x\u2212a\nb\u2212a, a < x \u2264b,\n1, x > b.\nAs you can see from this practice exercise, we explicitly break the CDF into three segments.\nThe first segment gives FX(x) = 0 because for any x\u2264a, there is nothing to integrate,\nsince fX(x) = 0 for any x\u2264a. Similarly, for the last segment, FX(x) = 1 for all x > b\nbecause once xgoes beyond b, the integration will cover all the non-zeros of fX.Figure 4.6\nillustrates the PDF and CDF for this example.\nIn MATLAB, we can generate the PDF and CDF using the commands pdfand cdf\nrespectively. For the particular example shown in Figure 4.6 , the following code can be used.\nA similar set of commands can be implemented in Python.\n186", "202": "4.3. CUMULATIVE DISTRIBUTION FUNCTION\nFigure 4.6: Example: fX(x) = 1 /(b\u2212a)fora\u2264x\u2264b. The CDF has three segments.\n% MATLAB code to generate the PDF and CDF\nunif = makedist(\u2019Uniform\u2019,\u2019lower\u2019,-3,\u2019upper\u2019,4);\nx = linspace(-5, 10, 1500)\u2019;\nf = pdf(unif, x);\nF = cdf(unif, x);\nfigure(1); plot(x, f, \u2019LineWidth\u2019, 6);\nfigure(2); plot(x, F, \u2019LineWidth\u2019, 6);\n# Python code to generate the PDF and CDF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nx = np.linspace(-5,10,1500)\nf = stats.uniform.pdf(x,-3,4)\nF = stats.uniform.cdf(x,-3,4)\nplt.plot(x,f); plt.show()\nplt.plot(x,F); plt.show()\nPractice Exercise 4.11 . (Exponential random variable ) Let Xbe a continuous\nrandom variable with PDF fX(x) =\u03bbe\u2212\u03bbxforx\u22650, and 0 otherwise. Find the CDF\nofX.\nSolution . Clearly, for x <0, we have FX(x) = 0. For x\u22650, we can show that\nFX(x) =Zx\n0fX(x\u2032)dx\u2032=Zx\n0\u03bbe\u2212\u03bbx\u2032dx\u2032= 1\u2212e\u2212\u03bbx.\nTherefore, the complete CDF is (see Figure 4.7 for illustration):\nFX(x) =(\n0, x < 0,\n1\u2212e\u2212\u03bbx, x \u22650.\nThe MATLAB code and Python code to generate this figure are shown below.\n187", "203": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nFigure 4.7: Example: fX(x) =\u03bbe\u2212\u03bbxforx\u22650. The CDF has two segments.\n% MATLAB code to generate the PDF and CDF\npd = makedist(\u2019exp\u2019,2);\nx = linspace(-5, 10, 1500)\u2019;\nf = pdf(pd, x);\nF = cdf(pd, x);\nfigure(1); plot(x, f, \u2019LineWidth\u2019, 6);\nfigure(2); plot(x, F, \u2019LineWidth\u2019, 6);\n# Python code to generate the PDF and CDF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nx = np.linspace(-5,10,1500)\nf = stats.expon.pdf(x,2)\nF = stats.expon.cdf(x,2)\nplt.plot(x,f); plt.show()\nplt.plot(x,F); plt.show()\n4.3.2 Properties of CDF\nLet us now describe the properties of a CDF. If we compare these with those for the discrete\ncases, we see that the continuous cases simply replace the summations by integrations.\nTherefore, we should expect to inherit most of the properties from the discrete cases.\nProposition 4.1. LetXbe a random variable (either continuous or discrete), then\nthe CDF of Xhas the following properties:\n(i) The CDF is nondecreasing .\n(ii) The maximum of the CDF is when x=\u221e:FX(+\u221e) = 1 .\n(iii) The minimum of the CDF is when x=\u2212\u221e:FX(\u2212\u221e) = 0 .\n188", "204": "4.3. CUMULATIVE DISTRIBUTION FUNCTION\nProof . For (i), we notice that FX(x) =Rx\n\u2212\u221efX(x\u2032)dx\u2032. Therefore, if s\u2264tthen\nFX(s) =Zs\n\u2212\u221efX(x\u2032)dx\u2032\u2264Zt\n\u2212\u221efX(x\u2032)dx\u2032=FX(t).\nThus it shows that FXis nondecreasing. (It does not need to be increasing because a CDF\ncan have a steady state.) For (ii) and (iii), we can show that\nFX(+\u221e) =Z+\u221e\n\u2212\u221efX(x\u2032)dx\u2032= 1,and FX(\u2212\u221e) =Z\u2212\u221e\n\u2212\u221efX(x\u2032)dx\u2032= 0.\u25a1\nExample 4.12 . We can show that the CDF we derived for the uniform random variable\nsatisfies these three properties. To see this, we note that\nFX(x) =x\u2212a\nb\u2212a, a\u2264x\u2264b.\nThe derivative of this function F\u2032\nX(x) =1\nb\u2212a>0 for a\u2264x\u2264b. Also, note that\nFX(x) = 0 for x < a andx > b , soFXis nondecreasing. The other two properties\nfollow because if x=b, then FX(b) = 1, and if x=athen FX(a) = 0. Together with\nthe nondecreasing property, we show (ii) and (iii).\nProposition 4.2. LetXbe a continuous random variable. If the CDF FXis contin-\nuous at any a\u2264x\u2264b, then\nP[a\u2264X\u2264b] =FX(b)\u2212FX(a). (4.10)\nProof . The proof follows from the definition of the CDF, which states that\nFX(b)\u2212FX(a) =Zb\n\u2212\u221efX(x\u2032)dx\u2032\u2212Za\n\u2212\u221efX(x\u2032)dx\u2032\n=Zb\nafX(x\u2032)dx\u2032=P[a\u2264X\u2264b]. \u25a1\nThis result provides a very handy tool for calculating the probability of an event\na\u2264X\u2264busing the CDF. It says that P[a\u2264X\u2264b] is the difference between FX(b) and\nFX(a). So, if we are given FX, calculating the probability of a\u2264X\u2264bjust involves\nevaluating the CDF at aandb. The result also shows that for a continuous random vari-\nableX,P[X=x0] =FX(x0)\u2212FX(x0) = 0. This is consistent with our arguments from the\nmeasure\u2019s point of view.\nExample 4.13 . (Exponential random variable ) We showed that the exponential ran-\ndom variable Xwith a PDF fX(x) =\u03bbe\u2212\u03bbxforx\u22650 (and fX(x) = 0 for x < 0)\nhas a CDF given by FX(x) = 1\u2212e\u2212\u03bbxforx\u22650. Suppose we want to calculate the\n189", "205": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nprobability P[1\u2264X\u22643]. Then the PDF approach gives us\nP[1\u2264X\u22643] =Z3\n1fX(x)dx=Z3\n1\u03bbe\u2212\u03bbxdx=\u2212e\u2212\u03bbx\f\f\f\f3\n1=e\u22123\u03bb\u2212e\u2212\u03bb.\nIf we take the CDF approach, we can show that\nP[1\u2264X\u22643] =FX(3)\u2212FX(1)\n= (1\u2212e\u2212\u03bb)\u2212(1\u2212e\u22123\u03bb) =e\u22123\u03bb\u2212e\u2212\u03bb,\nwhich yields the same as the PDF approach.\nExample 4.14 . Let Xbe a random variable with PDF fX(x) = 2 xfor 0\u2264x\u22641,\nand is 0 otherwise. We can show that the CDF is\nFX(x) =Zx\n0fX(t)dt=Zx\n02t dt=t2\f\f\f\fx\n0=x2, 0\u2264x\u22641.\nTherefore, to compute the probability P[1/3\u2264X\u22641/2], we have\nP\u00141\n3\u2264X\u22641\n2\u0015\n=FX\u00121\n2\u0013\n\u2212FX\u00121\n3\u0013\n=\u00121\n2\u00132\n\u2212\u00121\n3\u00132\n=5\n36.\n\u25a1\nA CDF can be used for both continuous and discrete random variables. However, before\nwe can do that, we need a tool to handle the discontinuities. The following definition is a\nsummary of the three types of continuity.\nDefinition 4.8. A function FX(x)is said to be\n\u0088Left-continuous atx=bifFX(b) =FX(b\u2212)def= lim h\u21920FX(b\u2212h);\n\u0088Right-continuous atx=bifFX(b) =FX(b+)def= lim h\u21920FX(b+h);\n\u0088Continuous atx=bif it is both right-continuous and left-continuous at x=b.\nIn this case, we have\nlim\nh\u21920FX(b\u2212h) = lim\nh\u21920FX(b+h) =F(b).\nIn this definition, the step size h >0 is shrinking to zero. The point b\u2212hstays at the left of\nb, and b+hstays at the right of b. Thus, if we set the limit h\u21920,b\u2212hwill approach a point\nb\u2212whereas b+hwill approach a point b+. If it happens that FX(b\u2212) =FX(b) then we say\nthatFXis left-continuous at b. IfFX(b+) =FX(b) then we say that FXis right-continuous\natb. These are summarized in Figure 4.8 .\nWhenever FXhas a discontinuous point, it can be left-continuous, right-continuous, or\nneither. (\u201cNeither\u201d happens if FX(b) take a value other than FX(b+) orFX(b\u2212). You can\n190", "206": "4.3. CUMULATIVE DISTRIBUTION FUNCTION\nFigure 4.8: The definition of left- and right-continuous at a point b.\nalways create a nasty function that satisfies this condition.) For continuous functions, it is\nnecessary that FX(b\u2212) =FX(b+). If this happens, there is no gap between the two points.\nTheorem 4.3. For any random variable X(discrete or continuous), FX(x)is always\nright-continuous . That is,\nFX(b) =FX(b+)def= lim\nh\u21920FX(b+h) (4.11)\nRight-continuous means that if FX(x) is piecewise, it must have a solid left end and an\nempty right end . Figure 4.9 shows an example of a valid CDF and an invalid CDF.\nFigure 4.9: A CDF must be right-continuous.\nThe reason why FXis always right-continuous is that the inequality X\u2264xhas a\nclosed right-hand limit. Imagine the following situation: A discrete random variable Xhas\nfour states: 1 ,2,3,4. Then,\nlim\nh\u21920FX(3 +h) = lim\nh\u21920\u201c3 + h\u201dX\nk=1pX(k) =pX(1) + pX(2) + pX(3) = FX(3).\nSimilarly, if you have a continuous random variable Xwith a PDF fX, then\nlim\nh\u21920FX(b+h) = lim\nh\u21920Zb+h\n\u2212\u221efX(t)dt=Zb\n\u2212\u221efX(t)dt=FX(b).\n191", "207": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nIn other words, the \u201c \u2264\u201d ensures that the rightmost state is included. If we defined CDF\nusing <, we would have gotten left-hand continuous, but this would be inconvenient because\nthe<requires us to deal with limits whenever we evaluate X < x .\nTheorem 4.4. For any random variable X(discrete or continuous), P[X=b]is\nP[X=b] =(\nFX(b)\u2212FX(b\u2212),ifFXis discontinuous at x=b\n0,otherwise.(4.12)\nThis proposition states that when FX(x) is discontinuous at x=b, then P[X=b] is\nthe difference between FX(b) and the limit from the left. In other words, the height of the\ngap determines the probability at the discontinuity. If FX(x) is continuous at x=b, then\nFX(b) = lim h\u21920FX(b\u2212h) and so P[X=b] = 0.\nFigure 4.10: Illustration of Equation (4.12). Since the CDF is discontinuous at a point x=b, the gap\nFX(b)\u2212FX(b\u2212)will define the probability P[X=b].\nExample 4.15 . Consider a random variable Xwith a PDF\nfX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3x,0\u2264x\u22641,\n1\n2, x= 3,\n0,otherwise .\nThe CDF FX(x) will consist of a few segments. The first segment is 0 \u2264x <1. We\ncan show that\nFX(x) =Zx\n0fX(t)dt=Zx\n0t dt=t2\n2\f\f\f\fx\n0=x2\n2,0\u2264x <1.\nThe second segment is when 1 \u2264x <3. Since there is no new fXto integrate, the\nCDF stays at FX(x) =FX(1) =1\n2for 1\u2264x <3. The third segment is x >3. Because\nthis range has covered the entire sample space, we have FX(x) = 1 for x >3. How\nabout x= 3? We can show that\nFX(3) = FX(3+) = 1 .\n192", "208": "4.3. CUMULATIVE DISTRIBUTION FUNCTION\nTherefore, to summarize, the CDF is\nFX(x) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f30, x < 0,\nx2\n2, 0\u2264x <1,\n1\n2, 1\u2264x <3,\n1, x \u22653.\nA graphical illustration is shown in Figure 4.11 .\nFigure 4.11: An example of converting a PDF to a CDF.\n4.3.3 Retrieving PDF from CDF\nThus far, we have only seen how to obtain FX(x) from fX(x). In order to go in the reverse\ndirection, we recall the fundamental theorem of calculus. This states that if a function fis\ncontinuous, then\nf(x) =d\ndxZx\naf(t)dt\nfor some constant a. Using this result for CDF and PDF, we have the following:\nTheorem 4.5. Theprobability density function (PDF) is the derivative of the cu-\nmulative distribution function (CDF):\nfX(x) =dFX(x)\ndx=d\ndxZx\n\u2212\u221efX(x\u2032)dx\u2032, (4.13)\nprovided FXis differentiable at x. IfFXis not differentiable at x=x0, then,\nfX(x0) =P[X=x0]\u03b4(x\u2212x0).\nExample 4.16 . Consider a CDF\nFX(x) =(\n0, x < 0,\n1\u22121\n4e\u22122x, x \u22650.\nWe want to find the PDF fX(x). To do so, we first show that FX(0) =3\n4. This\n193", "209": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\ncorresponds to a discontinuity at x= 0, as shown in Figure 4.12 .\nFigure 4.12: An example of converting a PDF to a CDF.\nBecause of the discontinuity, we need to consider three cases:\nfX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3dFX(x)\ndx, x < 0,\nP[X= 0]\u03b4(x\u22120), x = 0,\ndFX(x)\ndx, x > 0.\nWhen x <0,FX(x) = 0, sodFX(x)\ndx= 0. When x >0,FX(x) = 1\u22121\n4e\u22122x, so\ndFX(x)\ndx=1\n2e\u22122x.\nWhen x= 0, the probability P[X= 0] is determined by the gap between the solid dot\nand the empty dot. This yields\nP[X= 0] = FX(0)\u2212lim\nh\u21920FX(0\u2212h) =3\n4\u22120 =3\n4.\nTherefore, the overall PDF is\nfX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, x < 0,\n3\n4\u03b4(x\u22120), x = 0,\n1\n2e\u22122x, x > 0.\nFigure 4.12 illustrates this example.\n4.3.4 CDF: Unifying discrete and continuous random variables\nThe CDF is always a well-defined function. It is integrable everywhere. If the underlying\nrandom variable is continuous, the CDF is also continuous. If the underlying random variable\nis discrete, the CDF is a staircase function. We have seen enough CDFs for continuous\nrandom variables. Let us (re)visit a few discrete random variables.\nExample 4.17 . (Geometric random variable) Consider a geometric random variable\nwith PMF pX(k) = (1 \u2212p)k\u22121p, fork= 1,2, . . ..\n194", "210": "4.3. CUMULATIVE DISTRIBUTION FUNCTION\nFigure 4.13: PMF and CDF of a geometric random variable.\nWe can show that the CDF is\nFX(k) =kX\n\u2113=1pX(\u2113) =kX\n\u2113=1(1\u2212p)\u2113\u22121p=p\u00b71\u2212(1\u2212p)k\n1\u2212(1\u2212p)= 1\u2212(1\u2212p)k.\nFor a sanity check, we can try to retrieve the PMF from the CDF:\npX(k) =FX(k)\u2212FX(k\u22121)\n= (1\u2212(1\u2212p)k)\u2212(1\u2212(1\u2212p)k\u22121)\n= (1\u2212p)k\u22121p.\nA graphical portrayal of this example is shown in Figure 4.13 .\nIf we treat the PMFs as delta functions in the above example, then the continuous\ndefinition also applies. Since the CDF is a piecewise constant function, the derivative is\nexactly a delta function. For some problems, it is easier to start with CDF and then compute\nthe PMF or PDF. Here is an example.\nExample 4.18 . Let X1,X2andX3be three independent discrete random variables\nwith sample space \u2126 = {1,2, . . . , 10}. Define X= max {X1, X2, X3}. We want to\nfind the PMF of X. To tackle this problem, we first observe that the PMF for X1is\npX1(k) =1\n10. Thus, the CDF of X1is\nFX1(k) =kX\n\u2113=1pX1(\u2113) =k\n10.\nThen, we can show that the CDF of Xis\nFX(k) =P[X\u2264k] =P[max{X1, X2, X3} \u2264k]\n(a)=P[X1\u2264k\u2229X2\u2264k\u2229X3\u2264k]\n(b)=P[X1\u2264k]P[X2\u2264k]P[X3\u2264k]\n=\u0012k\n10\u00133\n,\n195", "211": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nwhere in ( a) we use the fact that max {X1, X2, X3} \u2264kif and only if all three elements\nare less than k, and in ( b) we use independence. Consequently, the PMF of Xis\npX(k) =FX(k)\u2212FX(k\u22121) =\u0012k\n10\u00133\n\u2212\u0012k\u22121\n10\u00133\n.\nWhat is a CDF?\n\u0088CDF is FX(x) =P[X\u2264x]. It is the cumulative sum of the PMF/PDF.\n\u0088CDF is either a staircase function, a smooth function, or a hybrid. Unlike a\nPDF, which is not defined for discrete random variables, the CDF is always well\ndefined.\n\u0088CDFd\ndx\u2212\u2192PDF.\n\u0088CDFR\n\u2190\u2212PDF.\n\u0088Gap of jump in CDF = height of delta in PDF.\n4.4 Median, Mode, and Mean\nThere are three statistical quantities that we are frequently interested in: mean, mode, and\nmedian. We all know how to compute these from a dataset. For example, to compute the\nmedian of a dataset, we sort the data and pick the number that sits in the 50th percentile.\nHowever, the median computed in this way is the empirical median , i.e., it is a value\ncomputed from a particular dataset. If the data is generated from a random variable (with\na given PDF), how do we compute the mean, median, and mode?\n4.4.1 Median\nImagine you have a sequence of numbers as shown below.\nn 1 2 3 4 5 6 7 8 9 \u00b7\u00b7\u00b7 100\nxn1.5 2.5 3.1 1.1 \u22120.4\u22124.1 0.5 2.2 \u22123.4\u00b7\u00b7\u00b7 \u2212 1.4\nHow do we compute the median? We first sort the sequence (either in ascending order\nor descending order), and then pick the middle one. On computer, we permute the samples\n{x1\u2032, x2\u2032, . . . , x N\u2032}= sort {x1, x2, . . . , x N},\nsuch that x1\u2032< x2\u2032< . . . < x N\u2032is ordered. The median is the one positioned at the middle.\nThere are, of course, built-in commands such as median in MATLAB and np.median in\nPython to perform the median operation.\nNow, how do we compute the median if we are given a random variable Xwith a PDF\nfX(x)? The answer is by integrating the PDF.\n196", "212": "4.4. MEDIAN, MODE, AND MEAN\nDefinition 4.9. LetXbe a continuous random variable with PDF fX. The median\nofXis a point c\u2208Rsuch that\nZc\n\u2212\u221efX(x)dx=Z\u221e\ncfX(x)dx. (4.14)\nWhy is the median defined in this way? This is becauseRc\n\u2212\u221efX(x)dxis the area under\nthe curve on the left of c, andR\u221e\ncfX(x)dxis the area under the curve on the right of c.\nThe area under the curve tells us the percentage of numbers that are less than the cutoff.\nTherefore, if the left area equals the right area, then cmust be the median.\nHow to find the median from the PDF\n\u0088Find a point cthat separates the PDF into two equal areas\nFigure 4.14: [Left] The median is computed as the point such that the two areas under the curve are\nequal. [Right] The median is computed as the point such that FXhits 0.5.\nThe median can also be evaluated from the CDF as follows.\nTheorem 4.6. Themedian of a random variable Xis the point csuch that\nFX(c) =1\n2. (4.15)\nProof . Since FX(x) =Rx\n\u2212\u221efX(x\u2032)dx\u2032, we have\nFX(c) =Zc\n\u2212\u221efX(x)dx\n=Z\u221e\ncfX(x)dx= 1\u2212FX(c).\nRearranging the terms shows that FX(c) =1\n2. \u25a1\n197", "213": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nHow to find median from CDF\n\u0088Find a point csuch that FX(c) = 0 .5.\nExample 4.19 . (Uniform random variable ) Let Xbe a continuous random variable\nwith PDF fX(x) =1\nb\u2212afora\u2264x\u2264b, and is 0 otherwise. We know that the CDF of\nXisFX(x) =x\u2212a\nb\u2212afora\u2264x\u2264b. Therefore, the median of Xis the number c\u2208R\nsuch that FX(c) =1\n2. Substituting into the CDF yieldsc\u2212a\nb\u2212a=1\n2, which gives c=a+b\n2.\nExample 4.20 . (Exponential random variable ) Let Xbe a continuous random vari-\nable with PDF fX(x) =\u03bbe\u2212\u03bbxforx\u22650. We know that the CDF of XisFX(x) =\n1\u2212e\u2212\u03bbxforx\u22650. The median of Xis the point csuch that FX(c) =1\n2. This gives\n1\u2212e\u2212\u03bbc=1\n2, which is c=log 2\n\u03bb.\n4.4.2 Mode\nThe mode is the peak of the PDF. We can see this from the definition below.\nDefinition 4.10. LetXbe a continuous random variable. The mode is the point c\nsuch that fX(x)attains the maximum:\nc=argmax\nx\u2208\u2126fX(x) =argmax\nx\u2208\u2126d\ndxFX(x). (4.16)\nThe second equality holds because fX(x) =F\u2032\nX(x) =d\ndxRx\n\u2212\u221efX(t)dt. A pictorial illustra-\ntion of mode is given in Figure 4.15 . Note that the mode of a random variable is not unique,\ne.g., a mixture of two identical Gaussians with different means has two modes.\nFigure 4.15: [Left] The mode appears at the peak of the PDF. [Right] The mode appears at the steepest\nslope of the CDF.\n198", "214": "4.4. MEDIAN, MODE, AND MEAN\nHow to find mode from PDF\n\u0088Find a point csuch that fX(c) is maximized.\nHow to find mode from CDF\n\u0088Continuous: Find a point csuch that FX(c) has the steepest slope.\n\u0088Discrete: Find a point csuch that FX(c) has the biggest gap in a jump.\nExample 4.21 . Let Xbe a continuous random variable with PDF fX(x) = 6 x(1\u2212x)\nfor 0\u2264x\u22641. The mode of Xhappens at argmax\nxfX(x). To find this maximum, we\ntake the derivative of fX. This gives\n0 =d\ndxfX(x) =d\ndx6x(1\u2212x) = 6(1 \u22122x).\nSetting this equal to zero yields x=1\n2.\nTo ensure that this point is a maximum, we take the second-order derivative:\nd2\ndx2fX(x) =d\ndx6(1\u22122x) =\u221212<0.\nTherefore, we conclude that x=1\n2is a maximum point. Hence, the mode of Xis\nx=1\n2.\n4.4.3 Mean\nWe have defined the mean as the expectation of X. Here, we show how to compute the\nexpectation from the CDF. To simplify the demonstration, let us first assume that X > 0.\nLemma 4.1. LetX > 0. Then E[X]can be computed from FXas\nE[X] =Z\u221e\n0(1\u2212FX(t))dt. (4.17)\nProof . The trick is to change the integration order:\nZ\u221e\n0(1\u2212FX(t))dt=Z\u221e\n0[1\u2212P[X\u2264t]]dt=Z\u221e\n0P[X > t ]dt\n=Z\u221e\n0Z\u221e\ntfX(x)dx dt(a)=Z\u221e\n0Zx\n0fX(x)dt dx\n=Z\u221e\n0Zx\n0dtfX(x)dx=Z\u221e\n0xfX(x)dx=E[X].\nHere, step ( a) is due to the change of integration order. See Figure 4.16 for an illustration.\n\u25a1\nWe draw a picture to illustrate the above lemma. As shown in Figure 4.17 , the mean\nof a positive random variable X > 0 is equivalent to the area above the CDF.\n199", "215": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nFigure 4.16: The double integration can be evaluated by xthent, ortthenx.\nFigure 4.17: The mean of a positive random variable X > 0can be calculated by integrating the CDF\u2019s\ncomplement.\nLemma 4.2. LetX < 0. Then E[X]can be computed from FXas\nE[X] =Z0\n\u2212\u221eFX(t)dt. (4.18)\nProof . The idea here is also to change the integration order.\nZ0\n\u2212\u221eFX(t)dt=Z0\n\u2212\u221eP[X\u2264t]dt=Z0\n\u2212\u221eZt\n\u2212\u221efX(x)dx dt\n=Z0\n\u2212\u221eZ0\nxfX(x)dt dx =Z0\n\u2212\u221exfX(x)dx=E[X].\n\u25a1\nTheorem 4.7. The mean of a random variable Xcan be computed from the CDF as\nE[X] =Z\u221e\n0(1\u2212FX(t))dt\u2212Z0\n\u2212\u221eFX(t)dt. (4.19)\n200", "216": "4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES\nProof . For any random variable X, we can partition X=X+\u2212X\u2212where X+andX\u2212are\nthe positive and negative parts, respectively. Then, the above two lemmas will give us\nE[X] =E[X+\u2212X\u2212] =E[X+]\u2212E[X\u2212]\n=Z\u221e\n0(1\u2212FX(t))dt\u2212Z0\n\u2212\u221eFX(t)dt.\n\u25a1\nAs illustrated in Figure 4.18 , this equation is equivalent to computing the areas above\nand below the CDF and taking the difference.\nFigure 4.18: The mean of a random variable Xcan be calculated by computing the area in the CDF.\nHow to find the mean from the CDF\n\u0088A formula is given by Equation (4.20):\nE[X] =Z\u221e\n0(1\u2212FX(t))dt\u2212Z0\n\u2212\u221eFX(t)dt. (4.20)\n\u0088This result is not commonly used, but the proof technique of switching the inte-\ngration order is important.\n4.5 Uniform and Exponential Random Variables\nThere are many useful continuous random variables. In this section, we discuss two of them:\nuniform random variables and exponential random variables. In the next section, we will\ndiscuss the Gaussian random variables. Similarly to the way we discussed discrete random\nvariables, we take a generative / synthesis perspective when studying continuous random\nvariables. We assume we have access to the PDF of the random variables so we can derive\nthe theoretical mean and variance. The opposite direction, namely inferring the underlying\nmodel parameters from a dataset, will be discussed later.\n201", "217": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\n4.5.1 Uniform random variables\nDefinition 4.11. LetXbe a continuous uniform random variable . The PDF of Xis\nfX(x) =(\n1\nb\u2212a, a \u2264x\u2264b,\n0, otherwise ,(4.21)\nwhere [a, b]is the interval on which Xis defined. We write\nX\u223cUniform( a, b)\nto mean that Xis drawn from a uniform distribution on an interval [a, b].\n0 0.2 0.4 0.6 0.8 100.511.522.53\n0 0.2 0.4 0.6 0.8 100.20.40.60.811.2\n(a) PDF (b) CDF\nFigure 4.19: The PDF and CDF of X\u223cUniform (0.2,0.6).\nThe shape of the PDF of a uniform random variable is shown in Figure 4.19 . In this\nfigure, we assume that the random variables X\u223cUniform(0 .2,0.6) are taken from the\nsample space \u2126 = [0 ,1]. Note that the height of the uniform distribution is greater than 1,\nsince\nfX(x) =(\n1\n0.6\u22120.2= 2.5, 0.2\u2264x\u22640.6,\n0, otherwise .\nThere is nothing wrong with this PDF, because fX(x) is the probability per unit length . If we\nintegrate fX(x) over any sub-interval between 0.2 and 0.6, we can show that the probability\nis between 0 and 1.\nThe CDF of a uniform random variable can be determined by integrating fX(x):\nFX(x) =Zx\n\u2212\u221efX(t)dt\n=Zx\na1\nb\u2212adt\n=x\u2212a\nb\u2212a, a \u2264x\u2264b.\n202", "218": "4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES\nTherefore, the complete CDF is\nFX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, x < a,\nx\u2212a\nb\u2212a, a \u2264x\u2264b,\n1, x > b.\nThe corresponding CDF for the PDF we showed in Figure 4.19 (a) is shown in Figure 4.19 (b).\nIt can be seen that although the height of the PDF exceeds 1, the CDF grows linearly and\nsaturates at 1.\nRemark . The uniform distribution can also be defined for discrete random variables. In\nthis case, the probability mass function is given by\npX(k) =1\nb\u2212a+ 1, k =a, a+ 1, . . . , b.\nThe presence of \u201c1\u201d in the denominator of the PMF is because kruns from atob, including\nthe two endpoints.\nIn MATLAB and Python, generating uniform random numbers can be done by calling\ncommands unifrnd (MATLAB), and stats.uniform.rvs (Python). For discrete uniform\nrandom variables, in MATLAB the command is unidrnd , and in Python the command is\nstats.randint .\n% MATLAB code to generate 1000 uniform random numbers\na = 0; b = 1;\nX = unifrnd(a,b,[1000,1]);\nhist(X);\n# Python code to generate 1000 uniform random numbers\nimport scipy.stats as stats\na = 0; b = 1;\nX = stats.uniform.rvs(a,b,size=1000)\nplt.hist(X);\nTo compute the empirical average and variance of the random numbers in MATLAB\nwe can call the command mean andvar. The corresponding command in Python is np.mean\nandnp.var . We can also compute the median and mode, as shown below.\n% MATLAB code to compute empirical mean, var, median, mode\nX = unifrnd(a,b,[1000,1]);\nM = mean(X);\nV = var(X);\nMed = median(X);\nMod = mode(X);\n# Python code to compute empirical mean, var, median, mode\nX = stats.uniform.rvs(a,b,size=1000)\nM = np.mean(X)\nV = np.var(X)\n203", "219": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nMed = np.median(X)\nMod = stats.mode(X)\nThe mean and variance of a uniform random variable are given by the theorem below.\nTheorem 4.8. IfX\u223cUniform( a, b), then\nE[X] =a+b\n2and Var[X] =(b\u2212a)2\n12. (4.22)\nProof . We have derived these results before. Here is a recap for completeness:\nE[X] =Z\u221e\n\u2212\u221exfX(x)dx=Zb\nax\nb\u2212adx=a+b\n2,\nE[X2] =Z\u221e\n\u2212\u221ex2fX(x)dx=Zb\nax2\nb\u2212adx=a2+ab+b2\n3,\nVar[X] =E[X2]\u2212E[X]2=(b\u2212a)2\n12.\n\u25a1\nThe result should be intuitive because it says that the mean is the midpoint of the\nPDF.\nWhen will we encounter a uniform random variable? Uniform random variables are one\nof the most elementary continuous random variables. Given a uniform random variable, we\ncan construct any random variable by using an appropriate transformation. We will discuss\nthis technique as part of our discussion about generating random numbers.\nIn MATLAB, computing the mean and variance of a uniform random variable can be\ndone using the command unifstat . The Python coommand is stats.uniform.stats .\n% MATLAB code to compute mean and variance\na = 0; b = 1;\n[M,V] = unifstat(a,b)\n# Python code to compute mean and variance\nimport scipy.stats as stats\na = 0; b = 1;\nM, V = stats.uniform.stats(a,b,moments=\u2019mv\u2019)\nTo evaluate the probability P[\u2113\u2264X\u2264u] for a uniform random variable, we can call\nunifcdf in MATLAB and\n% MATLAB code to compute the probability P(0.2 < X < 0.3)\na = 0; b = 1;\nF = unifcdf(0.3,a,b) - unifcdf(0.2,a,b)\n204", "220": "4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES\n# Python code to compute the probability P(0.2 < X < 0.3)\na = 0; b = 1;\nF = stats.uniform.cdf(0.3,a,b)-stats.uniform.cdf(0.2,a,b)\nAn alternative is to define an object rv = stats.uniform , and call the CDF attribute:\n# Python code to compute the probability P(0.2 < X < 0.3)\na = 0; b = 1;\nrv = stats.uniform(a,b)\nF = rv.cdf(0.3)-rv.cdf(0.2)\n4.5.2 Exponential random variables\nDefinition 4.12. LetXbe an exponential random variable . The PDF of Xis\nfX(x) =(\n\u03bbe\u2212\u03bbx, x \u22650,\n0, otherwise ,(4.23)\nwhere \u03bb >0is a parameter. We write\nX\u223cExponential( \u03bb)\nto mean that Xis drawn from an exponential distribution of parameter \u03bb.\nIn this definition, the parameter \u03bbof the exponential random variable determines the rate\nof decay. A large \u03bbimplies a faster decay. The PDF of an exponential random variable is\nillustrated in Figure 4.20 . We show two values of \u03bb. Note that the initial value fX(0) is\nfX(0) = \u03bbe\u2212\u03bb0=\u03bb.\nTherefore, as long as \u03bb >1,fX(0) will exceed 1.\nThe CDF of an exponential random variable can be determined by\nFX(x) =Zx\n\u2212\u221efX(t)dt\n=Zx\n0\u03bbe\u2212\u03bbtdt= 1\u2212e\u2212\u03bbx, x \u22650.\nTherefore, if we consider the entire real line, the CDF is\nFX(x) =(\n0, x < 0,\n1\u2212e\u2212\u03bbx, x \u22650.\nThe corresponding CDFs for the PDFs shown in Figure 4.20 (a) are shown in Fig-\nure 4.20 (b). For larger \u03bb, the PDF fX(x) decays faster but the CDF FX(x) increases faster.\n205", "221": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\n0 0.2 0.4 0.6 0.8 10123456\n = 2\n = 5\n0 0.2 0.4 0.6 0.8 100.20.40.60.811.2\n = 2\n = 5\n(a) PDF (b) CDF\nFigure 4.20: (a) The PDF and (c) the CDF of X\u223cExponential (\u03bb).\nIn MATLAB, the code used to generate Figure 4.20 (a) is shown below. There are\nmultiple ways of doing this. An alternative way is to call exppdf , which will return the same\nresult. In Python, the corresponding command is stats.expon.pdf . Note that in Python\nthe parameter \u03bbis specified in scale option.\n% MATLAB code to plot the exponential PDF\nlambda1 = 1/2; lambda2 = 1/5;\nx = linspace(0,1,1000);\nf1 = pdf(\u2019exp\u2019,x, lambda1);\nf2 = pdf(\u2019exp\u2019,x, lambda2);\nplot(x, f1, \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0 0.2 0.8]); hold on;\nplot(x, f2, \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0.8 0.2 0]);\n# Python code to plot the exponential PDF\nlambd1 = 1/2\nlambd2 = 1/5\nx = np.linspace(0,1,1000)\nf1 = stats.expon.pdf(x,scale=lambd1)\nf2 = stats.expon.pdf(x,scale=lambd2)\nplt.plot(x, f1)\nplt.plot(x, f2)\nTo plot the CDF, we replace pdfbycdf. Similarly, in Python we replace expon.pdf\nbyexpon.cdf .\n% MATLAB code to plot the exponential CDF\nF = cdf(\u2019exp\u2019,x, lambda1);\nplot(x, F, \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0 0.2 0.8]);\n# Python code to plot the exponential CDF\nF = stats.expon.cdf(x,scale=lambd1)\nplt.plot(x, F)\n206", "222": "4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES\nTheorem 4.9. IfX\u223cExponential( \u03bb), then\nE[X] =1\n\u03bband Var[X] =1\n\u03bb2. (4.24)\nProof . We have discussed this proof before. Here is a recap for completeness:\nE[X] =Z\u221e\n\u2212\u221exfX(x)dx=Z\u221e\n0\u03bbxe\u2212\u03bbxdx\n=\u2212Z\u221e\n0xde\u2212\u03bbx\n=\u2212xe\u2212\u03bbx\f\f\f\u221e\n0+Z\u221e\n0e\u2212\u03bbxdx=1\n\u03bb,\nE[X2] =Z\u221e\n\u2212\u221ex2fX(x)dx=Z\u221e\n0\u03bbx2e\u2212\u03bbxdx\n=\u2212Z\u221e\n0x2de\u2212\u03bbx\n=\u2212x2e\u2212\u03bbx\f\f\f\u221e\n0+Z\u221e\n02xe\u2212\u03bbxdx\n= 0 +2\n\u03bbE[X] =2\n\u03bb2.\nThus, Var[ X] =E[X2]\u2212E[X]2=1\n\u03bb2.\n\u25a1\nComputing the mean and variance of an exponential random variable in MATLAB and\nPython follows the similar procedures that we described above.\n4.5.3 Origin of exponential random variables\nExponential random variables are closely related to Poisson random variables. Recall that\nthe definition of a Poisson random variable is a random variable that describes the number\nof events that happen in a certain period, e.g., photon arrivals, number of pedestrians, phone\ncalls, etc. We summarize the origin of an exponential random variable as follows.\nWhat is the origin of exponential random variables?\n\u0088An exponential random variable is the interarrival time between two consecutive\nPoisson events.\n\u0088That is, an exponential random variable is how much time it takes to go from N\nPoisson counts to N+ 1 Poisson counts.\nAn example will clarify this concept. Imagine that you are waiting for a bus, as illus-\ntrated in Figure 4.21 . Passengers arrive at the bus stop with an arrival rate \u03bbper unit time.\nThus, for some time t, the average number of people that arrive is \u03bbt. Let Nbe a random\n207", "223": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nvariable denoting the number of people. We assume that Nis Poisson with a parameter \u03bbt.\nThat is, for any duration t, the probability of observing npeople follows the PMF\nP[N=n] =(\u03bbt)n\nn!e\u2212\u03bbt.\nFigure 4.21: For any fixed period of time t, the number of people Nis modeled as a Poisson random\nvariable with a parameter \u03bbt.\nFigure 4.22: The interarrival time Tbetween two consecutive Poisson events is an exponential random\nvariable.\nLetTbe the interarrival time between two people, by which we mean the time between\ntwo consecutive arrivals, as shown in Figure 4.22 . Note that Tis a random variable because\nTdepends on N, which is itself a random variable. To find the PDF of T, we first find the\nCDF of T. We note that\nP[T > t ](a)=P[interarrival time > t]\n(b)=P[no arrival in t](c)=P[N= 0] =(\u03bbt)0\n0!e\u2212\u03bbt=e\u2212\u03bbt.\nIn this set of arguments, (a) holds because Tis the interarrival time, and (b) holds be-\ncause interarrival time is between two consecutive arrivals. If the interarrival time is larger\nthan t, there is no arrival during the period. Equality (c) holds because Nis the number of\npassengers.\nSinceP[T > t ] = 1\u2212FT(t), where FT(t) is the CDF of T, we can show that\nFT(t) = 1\u2212e\u2212\u03bbt,\nfT(t) =d\ndtFT(t) =\u03bbe\u2212\u03bbt.\nTherefore, the interarrival time Tfollows an exponential distribution.\nSince exponential random variables are tightly connected to Poisson random variables,\nwe should expect them to be useful for modeling temporal events. We discuss two examples.\n208", "224": "4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES\n4.5.4 Applications of exponential random variables\nExample 4.22 . (Photon arrivals ) Single-photon image sensors are designed to op-\nerate in the photon-limited regime. The number-one goal of using these sensors is to\ncount the number of arriving photons precisely. However, for some applications not\nall single-photon image sensors are used to count photons. Some are used to measure\nthe time between two photon arrivals, such as time-of-flight systems. In this case, we\nare interested in measuring the time it takes for a pulse to bounce back to the sensor.\nThe more time it takes for a pulse to come back, the greater the distance between the\nobject and the sensor. Other applications utilize the time information. For example,\nhigh-dynamic-range imaging can be achieved by recording the time between two pho-\nton arrivals because brighter regions have a higher Poisson rate \u03bband darker regions\nhave a lower \u03bb.\nThe figure above illustrates an example of high-dynamic-range imaging. When the\nscene is bright, the large \u03bbwill generate more photons. Therefore, the interarrival time\nbetween the consecutive photons will be relatively short. If we plot the histogram of\nthe interarrival time, we observe that most of the interarrival time will be concentrated\nat small values. Dark regions behave in the opposite manner. The interarrival time will\ntypically be much longer. In addition, because there is more variation in the photon\narrival times, the histogram will look shorter and wider. Nevertheless, both cases are\nmodeled by the exponential random variable.\nExample 4.23 . (Energy-efficient escalator ) Many airports today have installed variable-\nspeed escalators. These escalators change their speeds according to the traffic. If there\nare no passengers for more than a certain period (say, 60 seconds), the escalator will\nswitch from the full-speed mode to the low-speed mode. For moderately busy esca-\nlators, the variable-speed configuration can save energy. The interesting data-science\nproblem is to determine, given a traffic pattern, e.g., the one shown in Figure 4.23 ,\nwhether we can predict the amount of energy savings?\nWe will not dive into the details of this problem, but we can briefly discuss the\nprinciple. Consider a fixed arrival rate \u03bb(say, the average from 07:00 to 08:00). The in-\nterarrival time, according to our discussion above, follows an exponential distribution.\n209", "225": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nSo we know that\nfT(t) =\u03bbe\u2212\u03bbt.\nSuppose that the escalator switches to low-speed mode when the interarrival time\nexceeds \u03c4. Then we can define a new variable Yto denote the amount of time that\nthe escalator will operate in the low-speed mode. This new variable is\nY=(\nT\u2212\u03c4, T > \u03c4,\n0, T \u2264\u03c4.\nIn other words, if the interarrival time Tis more than \u03c4, then the amount of time\nsaved Y takes the value T\u2212\u03c4, but if the interarrival time is less than \u03c4, then there is\nno saving.\nFigure 4.23: The variable-speed escalator problem. [Left] We model the passengers as independent\nPoisson arrivals. Thus, the interarrival time is exponential. [Right] A hypothetical passenger arrival\nrate (number of people per minute), from 06:00 to 23:00.\nFigure 4.24: The escalator problem requires modeling the cutoff threshold \u03c4such that if T > \u03c4 ,\nthe savings are Y=T\u2212\u03c4. IfT < \u03c4 , then Y= 0. The left-hand side of the figure shows how the\nPDF of Yis constructed.\nThe PDF of Ycan be computed according to Figure 4.24 . There are two parts\nto the calculation. When Y= 0, there is a probability mass such that\nfY(0) =P[Y= 0] =Z\u03c4\n0fT(t)dt=Z\u03c4\n0\u03bbe\u2212\u03bbtdt= 1\u2212e\u2212\u03bb\u03c4.\nFor other values of y, we can show that\nfY(y) =fT(y+\u03c4) =\u03bbe\u2212\u03bb(y+\u03c4).\nTherefore, to summarize, we can show that the PDF of Yis\nfY(y) =(\n(1\u2212e\u2212\u03bb\u03c4)\u03b4(y), y = 0,\n\u03bbe\u2212\u03bb(y+\u03c4), y > 0.\n210", "226": "4.6. GAUSSIAN RANDOM VARIABLES\nConsequently, we can compute E[Y] and Var[ Y] and analyze how these values change\nfor\u03bb(which itself changes with the time of day). Furthermore, we can analyze the\namount of savings in terms of dollars. We leave these problems as an exercise.\nClosing remark . The photon arrival problem and the escalator problem are two of many\nexamples we can find in which exponential random variables are useful for modeling a\nproblem. We did not go into the details of the problems because each of them requires some\nadditional modeling to address the real practical problem. We encourage you to explore these\nproblems further. Our message is simple: Many problems can be modeled by exponential\nrandom variables, most of which are associated with time.\n4.6 Gaussian Random Variables\nWe now discuss themost important continuous random variable \u2014 the Gaussian random\nvariable (also known as the normal random variable ). We call it the most important random\nvariable because it is widely used in almost all scientific disciplines. Many of us have used\nGaussian random variables before, and perhaps its bell shape is the first lesson we learn in\nstatistics. However, there are many mysteries about Gaussian random variables which you\nmay have missed, such as: Where does the Gaussian random variable come from? Why does\nit take a bell shape? What are the properties of a Gaussian random variable? The objective\nof this section is to explain everything you need to know about a Gaussian random variable.\n4.6.1 Definition of a Gaussian random variable\nDefinition 4.13. AGaussian random variable is a random variable Xsuch that its\nPDF is\nfX(x) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5)2\n2\u03c32\u001b\n, (4.25)\nwhere (\u00b5, \u03c32)are parameters of the distribution. We write\nX\u223cGaussian (\u00b5, \u03c32) or X\u223c N(\u00b5, \u03c32)\nto say that Xis drawn from a Gaussian distribution of parameter (\u00b5, \u03c32).\nGaussian random variables have two parameters ( \u00b5, \u03c32). It is noteworthy that the mean\nis\u00b5and the variance is \u03c32\u2014 these two parameters are exactly the first moment and the\nsecond central moment of the random variable. Most other random variables do not have\nthis property.\nNote that a Gaussian random variable is positive from \u2212\u221e to\u221e. Thus, fX(x) has\na non-zero value for any x, even though the value may be extremely small. A Gaussian\nrandom variable is also symmetric about \u00b5. If\u00b5= 0, then fX(x) is an even function.\nThe shape of the Gaussian is illustrated in Figure 4.25 . When we fix the variance and\nchange the mean, the PDF of the Gaussian moves left or right depending on the sign of the\nmean. When we fix the mean and change the variance, the PDF of the Gaussian changes\n211", "227": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nits width. Since any PDF should integrate to unity, a wider Gaussian means that the PDF\nis shorter. Note also that if \u03c3is very small, it is possible that fX(x)>1 although the\nintegration over \u2126 will still be 1.\n-10 -5 0 5 1000.10.20.30.40.5\n = -3\n = -0.3\n = 0\n = 1.2\n = 4\n-10 -5 0 5 1000.10.20.30.40.5\n = 0.8\n = 1\n = 2\n = 3\n = 4\n\u00b5changes, \u03c3= 1 \u00b5= 0,\u03c3changes\nFigure 4.25: A Gaussian random variable with different \u00b5and\u03c3.\nOn a computer, plotting the Gaussian PDF can be done by calling the function\npdf(\u2019norm\u2019,x) in MATLAB, and stats.norm.pdf in Python.\n% MATLAB to generate a Gaussian PDF\nx = linspace(-10,10,1000);\nmu = 0; sigma = 1;\nf = pdf(\u2019norm\u2019,x,mu,sigma);\nplot(x, f);\n# Python to generate a Gaussian PDF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nx = np.linspace(-10,10,1000)\nmu = 0; sigma = 1;\nf = stats.norm.pdf(x,mu,sigma)\nplt.plot(x,f)\nOur next result concerns the mean and variance of a Gaussian random variable. You\nmay wonder why we need this theorem when we already know that \u00b5is the mean and \u03c32is\nthe variance. The answer is that we have not proven these two facts.\nTheorem 4.10. IfX\u223cGaussian (\u00b5, \u03c32), then\nE[X] =\u00b5,and Var[X] =\u03c32. (4.26)\n212", "228": "4.6. GAUSSIAN RANDOM VARIABLES\nProof . The expectation can be derived via substitution:\nE[X] =1\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221exe\u2212(x\u2212\u00b5)2\n2\u03c32dx\n(a)=1\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221e(y+\u00b5)e\u2212y2\n2\u03c32dy\n=1\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221eye\u2212y2\n2\u03c32dy+1\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221e\u00b5e\u2212y2\n2\u03c32dy\n(b)= 0 + \u00b5\u00121\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221ee\u2212y2\n2\u03c32dy\u0013\n(c)=\u00b5,\nwhere in (a) we substitute y=x\u2212\u00b5, in (b) we use the fact that the first integrand is odd\nso that the integration is 0, and in (c) we observe that integration over the entire sample\nspace of the PDF yields 1.\nThe variance is also derived by substitution.\nVar[X] =1\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221e(x\u2212\u00b5)2e\u2212(x\u2212\u00b5)2\n2\u03c32dx\n(a)=\u03c32\n\u221a\n2\u03c0Z\u221e\n\u2212\u221ey2e\u2212y2\n2dy\n=\u03c32\n\u221a\n2\u03c0\u0012\n\u2212ye\u2212y2\n2\f\f\f\u221e\n\u2212\u221e\u0013\n+\u03c32\n\u221a\n2\u03c0Z\u221e\n\u2212\u221ee\u2212y2\n2dy\n= 0 + \u03c32\u00121\u221a\n2\u03c0Z\u221e\n\u2212\u221ee\u2212y2\n2dy\u0013\n=\u03c32,\nwhere in (a) we substitute y= (x\u2212\u00b5)/\u03c3.\n4.6.2 Standard Gaussian\nWe need to evaluate the probability P[a\u2264X\u2264b] of a Gaussian random variable Xin many\npractical situations. This involves the integration of the Gaussian PDF, i.e., determining the\nCDF. Unfortunately, there is no closed-form expression of P[a\u2264X\u2264b] in terms of ( \u00b5, \u03c32).\nThis leads to what we call the standard Gaussian.\nDefinition 4.14. Thestandard Gaussian (or standard normal) random variable X\nhas a PDF\nfX(x) =1\u221a\n2\u03c0e\u2212x2\n2. (4.27)\nThat is, X\u223c N(0,1)is a Gaussian with \u00b5= 0and\u03c32= 1.\nThe CDF of the standard Gaussian can be determined by integrating the PDF. We have a\nspecial notation for this CDF. Figure 4.26 illustrates the idea.\n213", "229": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nDefinition 4.15. TheCDF of the standard Gaussian is defined as the \u03a6(\u00b7)function\n\u03a6(x)def=FX(x) =1\u221a\n2\u03c0Zx\n\u2212\u221ee\u2212t2\n2dt. (4.28)\nFigure 4.26: Definition of the CDF of the standard Gaussian \u03a6(x).\n% MATLAB code to generate standard Gaussian PDF and CDF\nx = linspace(-5,5,1000);\nf = normpdf(x,0,1);\nF = normcdf(x,0,1);\nfigure; plot(x, f);\nfigure; plot(x, F);\n# Python code to generate standard Gaussian PDF and CDF\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nx = np.linspace(-10,10,1000)\nf = stats.norm.pdf(x)\nF = stats.norm.cdf(x)\nplt.plot(x,f); plt.show()\nplt.plot(x,F); plt.show()\nThe standard Gaussian\u2019s CDF is related to a so-called error function defined as\nerf(x) =2\u221a\u03c0Zx\n0e\u2212t2dt. (4.29)\nIt is easy to link \u03a6( x) with erf( x):\n\u03a6(x) =1\n2\u0014\n1 + erf\u0012x\u221a\n2\u0013\u0015\n, and erf( x) = 2\u03a6( x\u221a\n2)\u22121.\nWith the standard Gaussian CDF, we can define the CDF of an arbitrary Gaussian.\n214", "230": "4.6. GAUSSIAN RANDOM VARIABLES\nTheorem 4.11 (CDF of an arbitrary Gaussian ).LetX\u223c N(\u00b5, \u03c32). Then\nFX(x) = \u03a6\u0012x\u2212\u00b5\n\u03c3\u0013\n. (4.30)\nProof . We start by expressing FX(x):\nFX(x) =P[X\u2264x]\n=Zx\n\u2212\u221e1\u221a\n2\u03c0\u03c32e\u2212(t\u2212\u00b5)2\n2\u03c32dt.\nSubstituting y=t\u2212\u00b5\n\u03c3, and using the definition of standard Gaussian, we have\nZx\n\u2212\u221e1\u221a\n2\u03c0\u03c32e\u2212(t\u2212\u00b5)2\n2\u03c32dt=Zx\u2212\u00b5\n\u03c3\n\u2212\u221e1\u221a\n2\u03c0e\u2212y2\n2dy\n= \u03a6\u0012x\u2212\u00b5\n\u03c3\u0013\n.\u25a1\nIf you would like to verify this on a computer, you can try the following code.\n% MATLAB code to verify standardized Gaussian\nx = linspace(-5,5,1000);\nmu = 3; sigma = 2;\nf1 = normpdf((x-mu)/sigma,0,1); % standardized\nf2 = normpdf(x, mu, sigma); % raw\n# Python code to verify standardized Gaussian\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nx = np.linspace(-5,5,1000)\nmu = 3; sigma = 2;\nf1 = stats.norm.pdf((x-mu)/sigma,0,1) # standardized\nf2 = stats.norm.cdf(x,mu,sigma) # raw\nAn immediate consequence of this result is that\nP[a < X \u2264b] = \u03a6\u0012b\u2212\u00b5\n\u03c3\u0013\n\u2212\u03a6\u0012a\u2212\u00b5\n\u03c3\u0013\n. (4.31)\nTo see this, note that\nP[a < X \u2264b] =P[X\u2264b]\u2212P[X\u2264a]\n= \u03a6\u0012b\u2212\u00b5\n\u03c3\u0013\n\u2212\u03a6\u0012a\u2212\u00b5\n\u03c3\u0013\n.\nThe inequality signs of the two end points are not important. That is, the statement also\nholds for P[a\u2264X\u2264b] orP[a < X < b ], because Xis a continuous random variable at\nevery x. Thus, P[X=a] =P[X=b] = 0 for any aandb. Besides this, \u03a6 has several\nproperties of interest. See if you can prove these:\n215", "231": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nCorollary 4.1. LetX\u223c N(\u00b5, \u03c32). Then the following results hold:\n\u0088\u03a6(y) = 1\u2212\u03a6(\u2212y).\n\u0088P[X\u2265b] = 1\u2212\u03a6\u0010\nb\u2212\u00b5\n\u03c3\u0011\n.\n\u0088P[|X| \u2265b] = 1\u2212\u03a6\u0010\nb\u2212\u00b5\n\u03c3\u0011\n+ \u03a6\u0010\n\u2212b\u2212\u00b5\n\u03c3\u0011\n.\n4.6.3 Skewness and kurtosis\nIn modern data analysis we are sometimes interested in high-order moments. Here we con-\nsider two useful quantities: skewness andkurtosis .\nDefinition 4.16. For a random variable Xwith PDF fX(x), define the following\ncentral moments as\nmean =E[X]def=\u00b5,\nvariance =Eh\n(X\u2212\u00b5)2idef=\u03c32,\nskewness =E\"\u0012X\u2212\u00b5\n\u03c3\u00133#\ndef=\u03b3,\nkurtosis =E\"\u0012X\u2212\u00b5\n\u03c3\u00134#\ndef=\u03ba, excess kurtosisdef=\u03ba\u22123.\nAs you can see from the definitions above, skewness is the third central moment,\nwhereas kurtosis is the fourth central moment. Both skewness and kurtosis can be regarded\nas \u201cdeviations\u201d from a standard Gaussian \u2014not in terms of mean and variance but in terms\nof shape.\nSkewness measures the asymmetry of the distribution. Figure 4.27 shows three differ-\nent distributions: one with left skewness, one with right skewness, and one symmetric. The\nskewness of a curve is\n\u0088Skewed towards left: positive\n\u0088Skewed towards right: negative\n\u0088Symmetric: zero\nWhat is skewness?\n\u0088E\u0014\u0010\nX\u2212\u00b5\n\u03c3\u00113\u0015\n.\n\u0088Measures the asymmetry of the distribution.\n\u0088Gaussian has skewness 0.\n216", "232": "4.6. GAUSSIAN RANDOM VARIABLES\n0 5 10 15 2000.10.20.30.4\npositive skewness\nsymmetric\nnegative skewness\nFigure 4.27: Skewness of a distribution measures the asymmetry of the distribution. In this example\nthe skewnesses are: orange = 0.8943, black = 0, blue = -1.414.\nKurtosis measures how heavy-tailed the distribution is. There are two forms of kurtosis:\none is the standard kurtosis, which is the fourth central moment, and the other is the excess\nkurtosis, which is \u03baexcess =\u03ba\u22123. The constant 3 comes from the kurtosis of a standard\nGaussian. Excess kurtosis is more widely used in data analysis. The interpretation of kurtosis\nis the comparison to a Gaussian. If the kurtosis is positive, the distribution has a tail that\ndecays faster than a Gaussian. If the kurtosis is negative, the distribution has a tail that\ndecays more slowly than a Gaussian. Figure 4.28 illustrates the (excess) kurtosis of three\ndifferent distributions.\n-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.81\nkurtosis > 0\nkurtosis = 0\nkurtosis < 0\nFigure 4.28: Kurtosis of a distribution measures how heavy-tailed the distribution is. In this example,\nthe (excess) kurtoses are: orange = 2.8567, black = 0, blue = \u22120.1242.\nWhat is kurtosis?\n\u0088\u03ba=E\u0014\u0010\nX\u2212\u00b5\n\u03c3\u00114\u0015\n.\n\u0088Measures how heavy-tailed the distribution is. Gaussian has kurtosis 3.\n\u0088Some statisticians prefer excess kurtosis \u03ba\u22123, so that Gaussian has excess\nkurtosis 0.\n217", "233": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nRandom variable Mean Variance Skewness Excess kurtosis\n\u00b5 \u03c32\u03b3 \u03ba \u22123\nBernoulli p p(1\u2212p)1\u22122p\u221a\np(1\u2212p)1\n1\u2212p+1\np\u22126\nBinomial np np (1\u2212p)1\u22122p\u221a\nnp(1\u2212p)6p2\u22126p+1\nnp(1\u2212p)\nGeometric1\np1\u2212p\np22\u2212p\u221a1\u2212pp2\u22126p+6\n1\u2212p\nPoisson \u03bb \u03bb1\u221a\n\u03bb1\n\u03bb\nUniforma+b\n2(b\u2212a)2\n120 \u22126\n5\nExponential1\n\u03bb1\n\u03bb2 2 6\nGaussian \u00b5 \u03c320 0\nTable 4.1: The first few moments of commonly used random variables.\nOn a computer, computing the empirical skewness and kurtosis is done by built-in\ncommands. Their implementations are based on the finite-sample calculations\n\u03b3\u22481\nNNX\nn=1\u0012Xn\u2212\u00b5\n\u03c3\u00133\n,\n\u03ba\u22481\nNNX\nn=1\u0012Xn\u2212\u00b5\n\u03c3\u00134\n.\nThe MATLAB and Python built-in commands are shown below, using a gamma distribution\nas an example.\n% MATLAB code to compute skewness and kurtosis\nX = random(\u2019gamma\u2019,3,5,[10000,1]);\ns = skewness(X);\nk = kurtosis(X);\n# Python code to compute skewness and kurtosis\nimport scipy.stats as stats\nX = stats.gamma.rvs(3,5,size=10000)\ns = stats.skew(X)\nk = stats.kurtosis(X)\nExample 4.24 . To further illustrate the behavior of skewness and kurtosis, we consider\nan example using the gamma random variable X. The PDF of Xis given by the\nequation\nfX(x) =1\n\u0393(k)\u03b8kxk\u22121e\u2212x\n\u03b8, (4.32)\nwhere \u0393( \u00b7) is known as the gamma function. If kis an integer, the gamma function is\n218", "234": "4.6. GAUSSIAN RANDOM VARIABLES\njust the factorial: \u0393( k) = (k\u22121)!. A gamma random variable is parametrized by two\nparameters ( k, \u03b8). As kincreases or decreases, the shape of the PDF will change. For\nexample, when k= 1, the distribution is simplified to an exponential distribution.\nWithout going through the (tedious) integration, we can show that the skewness\nand the (excess) kurtosis of Gamma( k, \u03b8) are\nskewness =2\u221a\nk,\n(excess) kurtosis =6\nk.\nAs we can see from these results, the skewness and kurtosis diminish as kgrows. This\ncan be confirmed from the PDF of Gamma( k, \u03b8) as shown in Figure 4.29 .\n0 5 10 15 20 25 3000.10.20.30.4\nk = 2\nk = 5\nk = 10\nk = 15\nk = 20\nFigure 4.29: The PDF of a gamma distribution Gamma (k, \u03b8), where \u03b8= 1. The skewness and\nthe kurtosis are decaying to zero.\nExample 4.25 . Let us look at a real example. On April 15, 1912, RMS Titanic sank\nafter hitting an iceberg. The disaster killed 1502 out of 2224 passengers and crew. A\nhundred years later, we want to analyze the data. At https://www.kaggle.com/c/\ntitanic/ there is a dataset collecting the identities, age, gender, etc., of the passengers.\nWe partition the dataset into two: one for those who died and the other one for those\nwho survived. We plot the histograms of the ages of the two groups and compute\nseveral statistics of the dataset. Figure 4.30 shows the two datasets.\n0 20 40 60 80\nage010203040\n0 20 40 60 80\nage010203040\nGroup 1 (died) Group 2 (survived)\n219", "235": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nFigure 4.30: The Titanic dataset https://www.kaggle.com/c/titanic/ .\nStatistics Group 1 (Died) Group 2 (Survived)\nMean 30.6262 28.3437\nStandard Deviation 14.1721 14.9510\nSkewness 0.5835 0.1795\nExcess Kurtosis 0.2652 \u22120.0772\nNote that the two groups of people have very similar means and standard devia-\ntions. In other words, if we only compare the mean and standard deviation, it is nearly\nimpossible to differentiate the two groups. However, the skewness and kurtosis provide\nmore information related to the shape of the histograms. For example, Group 1 has\nmore positive skewness, whereas Group 2 is almost symmetrical. One interpretation is\nthat more young people offered lifeboats to children and older people. The kurtosis of\nGroup 1 is slightly positive, whereas that of Group 2 is slightly negative. Therefore,\nhigh-order moments can sometimes be useful for data analysis.\n4.6.4 Origin of Gaussian random variables\nThe Gaussian random variable has a long history. Here, we provide one perspective on why\nGaussian random variables are so useful. We give some intuitive arguments but leave the\nformal mathematical treatment for later when we introduce the Central Limit Theorem.\nLet\u2019s begin with a numerical experiment. Consider throwing a fair die. We know that\nthis will give us a (discrete) uniform random variable X. If we repeat the experiment many\ntimes we can plot the histogram, and it will return us a plot of 6 impulses with equal height,\nas shown in Figure 4.31 (a).\nNow, suppose we throw two dice. Call them X1andX2, and let Z=X1+X2, i.e.,\nthe sum of two dice. We want to find the distribution of Z. To do so, we first list out all\nthe possible outcomes in the sample space; this gives us {(1,1),(1,2), . . . , (6,6)}. We then\nsum the numbers, which gives us a list of states of Z:{2,3,4, . . . , 12}. The probability of\ngetting these states is shown in Figure 4.31 (b), which has a triangular shape. The triangular\nshape makes sense because to get the state \u201c2\u201d, we must have the pair (1 ,1), which is quite\nunlikely. However, if we want to get the state 7, it would be much easier to get a pair, e.g.,\n(6,1),(5,2),(4,3),(3,4),(2,5),(1,6) would all do the job.\nNow, what will happen if we throw 5 dice and consider Z=X1+X2+\u00b7\u00b7\u00b7+X5? It turns\nout that the distribution will continue to evolve and give something like Figure 4.31 (c).\nThis is starting to approximate a bell shape. Finally, if we throw 100 dice and consider\nZ=X1+X2+\u00b7\u00b7\u00b7+X100, the distribution will look like Figure 4.31 (d). The shape is\nbecoming a Gaussian! This numerical example demonstrates a fascinating phenomenon: As\nwe sum more random variables, the distribution of the sum will eventually converge to a\nGaussian.\nIf you are curious about how we plot the above figures, the following MATLAB and\nPython code can be useful.\n% MATLAB code to show the histogram of Z = X1+X2+X3\nN = 10000;\nX1 = randi(6,1,N);\nX2 = randi(6,1,N);\n220", "236": "4.6. GAUSSIAN RANDOM VARIABLES\n(a)X1 (b)X1+X2 (c)X1+\u00b7\u00b7\u00b7+X5 (d)X1+\u00b7\u00b7\u00b7+X100\nFigure 4.31: When adding uniform random variables, the overall distribution approaches a Gaussian as\nthe number of summed variables increase.\nX3 = randi(6,1,N);\nZ = X1 + X2 + X3;\nhistogram(Z, 2.5:18.5);\n# Python code to show the histogram of Z = X1+X2+X3\nimport numpy as np\nimport matplotlib.pyplot as plt\nN = 10000\nX1 = np.random.randint(1,6,size=N)\nX2 = np.random.randint(1,6,size=N)\nX3 = np.random.randint(1,6,size=N)\nZ = X1 + X2 + X3\nplt.hist(Z,bins=np.arange(2.5,18.5))\nCan we provide a more formal description of this? Yes, but we need some new mathe-\nmatical tools that we have not yet developed. So, for the time being, we will outline the flow\nof the arguments and leave the technical details to a later chapter. Suppose we have two\nindependent random variables with identical distributions, e.g., X1andX2, where both are\nuniform. This gives us PDFs fX1(x) and fX2(x) that are two identical rectangular functions.\nBy what operation can we combine these two rectangular functions and create a triangle\nfunction? The key lies in the concept of convolution . If you convolve two rectangle functions,\nyou will get a triangle function. Here we define the convolution of fXas\n(fX\u2217fX)(x) =Z\u221e\n\u2212\u221efX(\u03c4)fX(x\u2212\u03c4)d\u03c4.\nIn fact, for any pair of random variables X1andX2(not necessarily uniform random vari-\nables), the sum Z=X1+X2will have a PDF given by the convolution of the two PDFs. We\nhave not yet proven this, but if you trust what we are saying, we can effectively generalize\nthis argument to many random variables. If we have Nrandom variables, then the sum\nZ=X1+X2+\u00b7\u00b7\u00b7+XNwill have a PDF that is the result of Nconvolutions of all the\nindividual PDFs.\nWhat is the PDF of X+Y?\n\u0088Summing X+Yis equivalent to convolving the PDFs fX\u2217fY.\n221", "237": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\n\u0088If you sum many random variables, you convolve all their PDFs.\nHow do we analyze these convolutions? We need a second set of tools related to Fourier\ntransforms. The Fourier transform of a PDF is known as the characteristic function , which\nwe will discuss later, but the name is not important now. What matters is the important\nproperty of the Fourier transform, that a convolution in the original space is multiplication\nin the Fourier space. That is,\nF {(fX\u2217fX\u2217 \u00b7\u00b7\u00b7 \u2217 fX)}=F{fX} \u00b7 F{ fX} \u00b7 \u00b7\u00b7\u00b7 \u00b7 F{ fX}.\nMultiplication in the Fourier space is much easier to analyze. In particular, for independent\nand identically distributed random variables, the multiplication will easily translate to ad-\ndition in the exponent. Then, by truncating the exponent to the second order, we can show\nthat the limiting object in the Fourier space is approaching a Gaussian. Finally, since the\ninverse Fourier transform of a Gaussian remains a Gaussian, we have shown that the infinite\nconvolution will give us a Gaussian.\nHere is some numerical evidence for what we have just described. Recall that the\nFourier transform of a rectangle function is the sinc function. Therefore, if we have an\ninfinite convolution of rectangular functions, equivalently, we have an infinite product of sinc\nfunctions in the Fourier space. Multiplying sinc functions is reasonably easy. See Figure 4.32\nfor the first three sincs. It is evident that with just three sinc functions, the shape closely\napproximates a Gaussian.\n-10 -8 -6 -4 -2 0 2 4 6 8 10-0.5-0.2500.250.50.7511.25\n(sin x)/x\n(sin x)2/x2\n(sin x)3/x3\nFigure 4.32: Convolving the PDF of a uniform distribution is equivalent to multiplying their Fourier\ntransforms in the Fourier space. As the number of convolutions grows, the product is gradually becoming\nGaussian.\nHow about distributions that are not rectangular? We invite you to numerically visu-\nalize the effect when you convolve the function many times. You will see that as the number\nof convolutions grows, the resulting function will become more and more like a Gaussian.\nRegardless of what the input random variables are, as long as you add them, the sum will\nhave a distribution that looks like a Gaussian:\nX1+X2+\u00b7\u00b7\u00b7+XN\u21ddGaussian .\nWe use the notation \u21ddto emphasize that the convergence is not the usual form of conver-\ngence. We will make this precise later.\n222", "238": "4.7. FUNCTIONS OF RANDOM VARIABLES\nThe implication of this line of discussion is important. Regardless of the underlying\ntrue physical process, if we are only interested in the sum (or average), the distribution\nwill be more or less Gaussian. In most engineering problems, we are looking at the sum\nor average. For example, when generating an image using an image sensor, the sensor will\nadd a certain amount of read noise. Read noise is caused by the random fluctuation of the\nelectrons in the transistors due to thermal distortions. For high-photon-flux situations, we\nare typically interested in the average read noise rather than the electron-level read noise.\nThus Gaussian random variables become a reasonable model for that. In other applications,\nsuch as imaging through a turbulent medium, the random phase distortions (which alter\nthe phase of the wavefront) can also be modeled as a Gaussian random variable. Here is the\nsummary of the origin of a Gaussian random variable:\nWhat is the origin of Gaussian?\n\u0088When we sum many independent random variables, the resulting random vari-\nable is a Gaussian.\n\u0088This is known as the Central Limit Theorem . The theorem applies to anyran-\ndom variable.\n\u0088Summing random variables is equivalent to convolving the PDFs. Convolving\nPDFs infinitely many times yields the bell shape.\n4.7 Functions of Random Variables\nOne common question we encounter in practice is the transformation of random variables.\nThe question can be summarized as follows: Given a random variable Xwith PDF fX(x)\nand CDF FX(x), and supposing that Y=g(X) for some function g, what are fY(y) and\nFY(y)? This is a prevalent question. For example, we measure the voltage V, and we want\nto analyze the power P=V2/R. This involves taking the square of a random variable.\nAnother example: We know the distribution of the phase \u0398, but we want to analyze the\nsignal cos( \u03c9t+ \u0398). This involves a cosine transformation. How do we convert one variable\nto another? Answering this question is the goal of this section.\n4.7.1 General principle\nWe will first outline the general principle for tackling this type of problem. In the following\nsubsection, we will give a few concrete examples.\nSuppose we are given a random variable Xwith PDF fX(x) and CDF FX(x). Let Y=\ng(X) for some known and fixed function g. For simplicity, we assume that gis monotonically\n223", "239": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nincreasing. In this case, the CDF of Ycan be determined as follows.\nFY(y)(a)=P[Y\u2264y](b)=P[g(X)\u2264y]\n(c)=P[X\u2264g\u22121(y)]\n(d)=FX(g\u22121(y)).\nThis sequence of steps is not difficult to understand. Step (a) is the definition of CDF. Step\n(b) substitutes g(X) for Y. Step (c) uses the fact that since gis invertible, we can apply\nthe inverse of gto both sides of g(X)\u2264yto yield X\u2264g\u22121(y). Step (d) is the definition of\nthe CDF, but this time applied to P[X\u2264 \u2663] =FX(\u2663), for some \u2663.\nIt will be useful to visualize the situation in Figure 4.33 . Here, we consider a uniformly\ndistributed Xso that the CDF FX(x) is a straight line. According to FX, any samples\ndrawn according to FXare equally likely, as illustrated by the yellow dots on the x-axis.\nAs we transform the X\u2019s through Y=g(X), we increase/decrease the spacing between\ntwo samples. Therefore, some samples become more concentrated while some become less\nconcentrated. The distribution of these transformed samples (the yellow dots on the y-axis)\nforms a new CDF FY(y). The result FY(y) =FX(g\u22121(y)) holds when we look at Y. The\nsamples are traveling with g\u22121in order to go back to FX. Therefore, we need g\u22121in the\nformula.\nFigure 4.33: When transforming a random variable XtoY=g(X), the distributions are defined\naccording to the spacing between samples. In this figure, a uniformly distributed Xwill become squeezed\nby some parts of gand widened in other parts of g.\nWhy should we use the CDF and not the PDF in Figure 4.33 ? The advantage of the\nCDF is that it is an increasing function. Therefore, no matter what the function gis, the\ninput and the output functions will still be increasing. If we use the PDF, then the non-\nmonotonic behavior of the PDF will interact with another nonlinear function g. It becomes\nmuch harder to decouple the two.\nWe can carry out the integrations to determine FX(g\u22121(y)). It can be shown that\nFX(g\u22121(y)) =Zg\u22121(y)\n\u2212\u221efX(x\u2032)dx\u2032, (4.33)\n224", "240": "4.7. FUNCTIONS OF RANDOM VARIABLES\nand hence, by the fundamental theorem of calculus, we have\nfY(y) =d\ndyFY(y) =d\ndyFX(g\u22121(y)) =d\ndyZg\u22121(y)\n\u2212\u221efX(x\u2032)dx\u2032\n=\u0012d g\u22121(y)\ndy\u0013\n\u00b7fX(g\u22121(y)), (4.34)\nwhere the last step is due to the chain rule. Based on this line of reasoning we can summarize\na \u201crecipe\u201d for this problem.\nHow to find the PDF of Y=g(X)\n\u0088Step 1: Find the CDF FY(y), which is FY(y) =FX(g\u22121(y)).\n\u0088Step 2: Find the PDF fY(y), which is fY(y) =\u0010\nd g\u22121(y)\ndy\u0011\n\u00b7fX(g\u22121(y)).\nThis recipe works when gis a one-to-one mapping. If gis not one-to-one, e.g., g(x) =x2\nimplies g\u22121(y) =\u00b1\u221ay, then we will have some issues with the above two steps. When this\nhappens, then instead of writing X\u2264g\u22121(y) we need to determine the set {x|g(x)\u2264y}.\n4.7.2 Examples\nExample 4.26 . (Linear transform) Let Xbe a random variable with PDF fX(x) and\nCDF FX(x). Let Y= 2X+ 3. Find fY(y) and FY(y). Express the answers in terms of\nfX(x) and FX(x).\nSolution . We first note that\nFY(y) =P[Y\u2264y]\n=P[2X+ 3\u2264y]\n=P\u0014\nX\u2264y\u22123\n2\u0015\n=FX\u0012y\u22123\n2\u0013\n.\nTherefore, the PDF is\nfY(y) =d\ndyFY(y)\n=d\ndyFX\u0012y\u22123\n2\u0013\n=F\u2032\nX\u0012y\u22123\n2\u0013d\ndy\u0012y\u22123\n2\u0013\n=1\n2fX\u0012y\u22123\n2\u0013\n.\nFollow-Up . (Linear transformation of a Gaussian random variable).Suppose Xis a Gaus-\nsian random variable with zero mean and unit variance, and let Y=aX+b. Then the CDF\n225", "241": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nand PDF of Yare respectively\nFY(y) =FX\u0012y\u2212b\na\u0013\n= \u03a6\u0012y\u2212b\na\u0013\n,\nfY(y) =1\nafX\u0012y\u2212b\na\u0013\n=1\u221a\n2\u03c0ae\u2212(y\u2212b)2\n2a2.\nFollow-Up . (Linear transformation of an exponential random variable). Suppose Xis an\nexponential random variable with parameter \u03bb, and let Y=aX+b. Then the CDF and\nPDF of Yare respectively\nFY(y) =FX\u0012y\u2212b\na\u0013\n= 1\u2212e\u2212\u03bb\na(y\u2212b), y \u2265b,\nfY(y) =1\nafX\u0012y\u2212b\na\u0013\n=\u03bb\nae\u2212\u03bb\na(y\u2212b), y \u2265b.\nExample 4.27 . Let Xbe a random variable with PDF fX(x) and CDF FX(x). Sup-\nposing that Y=X2, find fY(y) and FY(y). Express the answers in terms of fX(x)\nandFX(x).\nSolution . We note that\nFY(y) =P[Y\u2264y] =P[X2\u2264y] =P[\u2212\u221ay\u2264X\u2264\u221ay]\n=FX(\u221ay)\u2212FX(\u2212\u221ay).\nTherefore, the PDF is\nfY(y) =d\ndyFY(y)\n=d\ndy(FX(\u221ay)\u2212FX(\u2212\u221ay))\n=F\u2032\nX(\u221ay)d\ndy\u221ay\u2212F\u2032\nX(\u2212\u221ay)d\ndy(\u2212\u221ay)\n=1\n2\u221ay(fX(\u221ay) +fX(\u2212\u221ay)).\n226", "242": "4.7. FUNCTIONS OF RANDOM VARIABLES\nFigure 4.34: When transforming a random variable XtoY=X2, the CDF becomes FY(y) =\u221ay\u2212a\nb\u2212aand the PDF becomes fY(y) =1\u221ay(b\u2212a).\nFollow Up . (Square of a uniform random variable) Suppose Xis a uniform random variable\nin [a, b] (assume a >0), and let Y=X2. Then the CDF and PDF of Yare respectively\nFY(y) =\u221ay\u2212a\nb\u2212a, a2\u2264y\u2264b2,\nfY(y) =1\u221ay(b\u2212a), a2\u2264y\u2264b2.\nExample 4.28 . Let X\u223cUniform(0 ,2\u03c0). Suppose Y= cos X. Find fY(y) and FY(y).\nSolution . First, we need to find the CDF of X. This can be done by noting that\nFX(x) =Zx\n\u2212\u221efX(x\u2032)dx\u2032=Zx\n01\n2\u03c0dx\u2032=x\n2\u03c0.\nThus, the CDF of Yis\nFY(y) =P[Y\u2264y] =P[cosX\u2264y]\n=P[cos\u22121y\u2264X\u22642\u03c0\u2212cos\u22121y]\n=FX(2\u03c0\u2212cos\u22121y)\u2212FX(cos\u22121y)\n= 1\u2212cos\u22121y\n\u03c0.\nThe PDF of Yis\nfY(y) =d\ndyFY(y) =d\ndy\u0012\n1\u2212cos\u22121y\n\u03c0\u0013\n=1\n\u03c0p\n1\u2212y2,\n227", "243": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nwhere we used the fact thatd\ndycos\u22121y=\u22121\u221a\n1\u2212y2.\nExample 4.29 . Let Xbe a random variable with PDF\nfX(x) =aexe\u2212aex.\nLetY=eX, and find fY(y).\nSolution . We first note that\nFY(y) =P[Y\u2264y] =P[eX\u2264y]\n=P[X\u2264logy] =Zlogy\n\u2212\u221eaexe\u2212aexdx.\nTo find the PDF, we recall the fundamental theorem of calculus. This gives us\nfY(y) =d\ndyZlogy\n\u2212\u221eaexe\u2212aexdx\n=\u0012d\ndylogy\u0013 \nd\ndlogyZlogy\n\u2212\u221eaexe\u2212aexdx!\n=1\nyaelogye\u2212aelogy=ae\u2212ay.\nClosing remark . The transformation of random variables is a fundamental technique in\ndata science. The approach we have presented is the most rudimentary yet the most intuitive.\nThe key is to visualize the transformation and how the random samples are allocated after\nthe transformation. Note that the density of the random samples is related to the slope of\nthe CDF. Therefore, if the transformation maps many samples to similar values, the slope\nof the CDF will be steep. Once you understand this picture, the transformation will be a\nlot easier to understand.\nIs it possible to replace the paper-and-pencil derivation of a transformation with a\ncomputer? If the objective is to transform random realizations, then the answer is yes\nbecause your goal is to transform numbers to numbers, which can be done on a computer.\nFor example, transforming a sample x1to\u221ax1is straightforward on a computer. However,\nif the objective is to derive the theoretical expression of the PDF, then the answer is no.\nWhy might we want to derive the theoretical PDF? We might want to analyze the mean,\nvariance, or other statistical properties. We may also want to reverse-engineer and determine\na transformation that can yield a specific PDF. This would require a paper-and-pencil\nderivation. In what follows, we will discuss a handy application of the transformations.\nWhat are the rules of thumb for transformation of random variables?\n\u0088Always find the CDF FY(y) =P[g(X)\u2264y]. Ask yourself: What are the values\nofXsuch that g(X)\u2264y? Think of the cosine example.\n228", "244": "4.8. GENERATING RANDOM NUMBERS\n\u0088Sometimes you do not need to solve for FY(y) explicitly. The fundamental the-\norem of calculus can help you find fY(y).\n\u0088Draw pictures. Ask yourself whether you need to squeeze or stretch the samples.\n4.8 Generating Random Numbers\nMost scientific computing software nowadays has built-in random number generators. For\ncommon types of random variables, e.g., Gaussian or exponential, these random number\ngenerators can easily generate numbers according to the chosen distribution. However, if we\nare given an arbitrary PDF (or PMF) that is not among the list of predefined distributions,\nhow can we generate random numbers according to the PDF or PMF we want?\n4.8.1 General principle\nGenerating random numbers according to the desired distribution can be formulated as\nan inverse problem. Suppose that we can generate uniformly random numbers according\nto Uniform(0,1). This is a fragile assumption, and this process can be done on almost all\ncomputers today. Let us call this random variable Uand its realization u. Suppose that we\nalso have a desired distribution fX(x) (and its CDF FX(x)). We can put the two random\nvariables UandXon the two axes of Figure 4.35 , yielding an input-output relationship.\nThe inverse problem is: By using what transformation g, such that X=g(U), can we make\nsure that Xis distributed according to fX(x) (or FX(x))?\nFigure 4.35: Generating random numbers according to a known CDF. The idea is to first generate a\nuniform(0,1) random variable, then do an inverse mapping F\u22121\nX.\nTheorem 4.12. The transformation gthat can turn a uniform random variable into\n229", "245": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\na random variable following a distribution FX(x)is given by\ng(u) =F\u22121\nX(u). (4.35)\nThat is, if g=F\u22121\nX, then g(U)will be distributed according to fX(orFX).\nProof . First, we know that if U\u223cUniform(0 ,1), then fU(u) = 1 for 0 \u2264u\u22641, so\nFU(u) =Zu\n\u2212\u221efU(u)du=u,\nfor 0\u2264u\u22641. Let g=F\u22121\nXand define Y=g(U). Then the CDF of Yis\nFY(y) =P[Y\u2264y] =P[g(U)\u2264y]\n=P[F\u22121\nX(U)\u2264y]\n=P[U\u2264FX(y)] =FX(y).\nTherefore, we have shown that the CDF of Yis the CDF of X. \u25a1\nThe theorem above states that if we want a distribution FX, then the transformation\nshould be g=F\u22121\nX. This suggests a two-step process for generating random numbers.\nHow do we generate random numbers from an arbitrary distribution FX?\n\u0088Step 1: Generate a random number U\u223cUniform(0 ,1).\n\u0088Step 2: Let\nY=F\u22121\nX(U). (4.36)\nThen the distribution of YisFX.\n4.8.2 Examples\nExample 4.30 . How can we generate Gaussian random numbers with mean \u00b5and\nvariance \u03c32from uniform random numbers?\nFirst, we generate U\u223cUniform(0 ,1). The CDF of the ideal distribution is\nFX(x) = \u03a6\u0012x\u2212\u00b5\n\u03c3\u0013\n.\nTherefore, the transformation gis\ng(U) =F\u22121\nX(U) =\u03c3\u03a6\u22121(U) +\u00b5.\nInFigure 4.36 , we plot the CDF of FXand the transformation g.\n230", "246": "4.8. GENERATING RANDOM NUMBERS\n-10 -8 -6 -4 -2 0 2 4 6 8 1000.10.20.30.40.50.60.70.80.91\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1-3-2-1012345678910-4\n(a)FX(\u00b7) (b) g(\u00b7)\nFigure 4.36: To generate random numbers according to Gaussian (0,1), we plot its CDF in (a)\nand the transformation gin (b).\nTo visualize the random variables before and after the transformation, we plot\nthe histograms in Figure 4.37 .\n0 0.2 0.4 0.6 0.8 10100200300400\n-5 0 5 10020040060080010001200\n(a) PDF of U (b) PDF of g(U)\nFigure 4.37: (a) PDF of the uniform random variable. (b) The PDF of the transformed random\nvariable.\nThe MATLAB and Python codes used to generate the histograms above are shown\nbelow.\n% MATLAB code to generate Gaussian from uniform\nmu = 3;\nsigma = 2;\nU = rand(10000,1);\ngU = sigma*icdf(\u2019norm\u2019,U,0,1)+mu;\nfigure; hist(U);\nfigure; hist(gU);\n# Python code to generate Gaussian from uniform\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n231", "247": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nmu = 3\nsigma = 2\nU = stats.uniform.rvs(0,1,size=10000)\ngU = sigma*stats.norm.ppf(U)+mu\nplt.hist(U); plt.show()\nplt.hist(gU); plt.show()\nExample 4.31 . How can we generate exponential random numbers with parameter \u03bb\nfrom uniform random numbers?\nFirst, we generate U\u223cUniform(0 ,1). The CDF of the ideal distribution is\nFX(x) = 1\u2212e\u2212\u03bbx.\nTherefore, the transformation gis\ng(U) =F\u22121\nX(U) =\u22121\n\u03bblog(1\u2212U).\nThe CDF of the exponential random variable and the transformation gare shown\ninFigure 4.38 .\n0 1 2 3 4 500.10.20.30.40.50.60.70.80.91\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 156701234\n(a)FX(\u00b7) (b) g(\u00b7)\nFigure 4.38: To generate random numbers according to Exponential (1), we plot its CDF in (a)\nand the transformation gin (b).\nThe PDF of the uniform random variable Uand the PDF of the transformed\nvariable g(U) are shown in Figure 4.39 .\n232", "248": "4.8. GENERATING RANDOM NUMBERS\n0 0.2 0.4 0.6 0.8 10100200300400\n0 2 4 6 8 10050010001500200025003000\n(a) PDF of U (b) PDF of g(U)\nFigure 4.39: (a) PDF of the uniform random variable. (b) The PDF of the transformed random\nvariable.\nThe MATLAB and Python codes for this transformation are shown below.\n% MATLAB code to generate exponential random variables\nlambda = 1;\nU = rand(10000,1);\ngU = -(1/lambda)*log(1-U);\n# Python code to generate exponential random variables\nimport numpy as np\nimport scipy.stats as stats\nlambd = 1;\nU = stats.uniform.rvs(0,1,size=10000)\ngU = -(1/lambd)*np.log(1-U)\nExample 4.32 . How can we generate the 4 integers 1 ,2,3,4, according to the his-\ntogram [0 .1 0.5 0.3 0.1], from uniform random numbers?\nFirst, we generate U\u223cUniform(0 ,1). The CDF of the ideal distribution is\nFX(x) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f30.1, x = 1,\n0.1 + 0 .5 = 0 .6, x = 2,\n0.1 + 0 .5 + 0 .3 = 0 .9, x = 3,\n0.1 + 0 .5 + 0 .3 + 0 .1 = 1 .0, x = 4.\nThis CDF is not invertible. However, we can still define the \u201cinverse\u201d mapping\n233", "249": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nas\ng(U) =F\u22121\nX(U)\n=\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f31, 0.0\u2264U\u22640.1,\n2, 0.1< U\u22640.6,\n3, 0.6< U\u22640.9,\n4, 0.9< U\u22641.0.\nFor example, if 0 .1< U\u22640.6, then on the black curve shown in Figure 4.40 (a), we\nare looking at the second vertical line from the left. This will go to \u201c2\u201d on the x-axis.\nTherefore, the inversely mapped value is 2 for 0 .1< U\u22640.6.\n0 1 2 3 4 50.10.60.91\n0 0.1 0.6 0.9 101234\n(a)FX(\u00b7) (b) g(\u00b7)\nFigure 4.40: To generate random numbers according to a predefined histogram, we first define\nthe CDF in (a) and the corresponding transformation in (b).\nThe PDFs of the transformed variables, before and after, are shown in Fig-\nure 4.41 .\n0 0.2 0.4 0.6 0.8 10100200300400\n0 1 2 3 4 50100020003000400050006000\n(a) PDF of U (b) PDF of g(U)\nFigure 4.41: (a) PDF of the uniform random variable. (b) The PDF of the transformed random\nvariable.\nIn MATLAB, the above PDFs can be plotted using the commands below. In Python,\nwe need to use the logical comparison np.logical_and to identify the indices. An alternative\nis to use gU[((U<=0.5)*(U>=0.0)).astype(np.bool)]=1 .\n234", "250": "4.9. SUMMARY\n% MATLAB code to generate the desired random variables\nU = rand(10000,1);\ngU = zeros(10000,1);\ngU((U>=0) & (U<=0.1)) = 1;\ngU((U>0.1) & (U<=0.6)) = 2;\ngU((U>0.6) & (U<=0.9)) = 3;\ngU((U>0.9) & (U<=1)) = 4;\n# Python code to generate the desired random variables\nimport numpy as np\nimport scipy.stats as stats\nU = stats.uniform.rvs(0,1,size=10000)\ngU = np.zeros(10000)\ngU[np.logical_and(U >= 0.0, U <= 0.1)] = 1\ngU[np.logical_and(U > 0.1, U <= 0.6)] = 2\ngU[np.logical_and(U > 0.6, U <= 0.9)] = 3\ngU[np.logical_and(U > 0.9, U <= 1)] = 4\n4.9 Summary\nLet us summarize this chapter by revisiting the four bullet points from the beginning of the\nchapter.\n\u0088Definition of a continuous random variable . Continuous random variables are mea-\nsured by lengths, areas, and volumes, which are all defined by integrations. This makes\nthem different from discrete random variables, which are measured by counts (and\nsummations). Because of the different measures being used to define random variables,\nwe consequently have different ways of defining expectation, variance, moments, etc.,\nall in terms of integrations.\n\u0088Unification of discrete and continuous random variables . The unification is done by\nthe CDF. The CDF of a discrete random variable can be written as a train of step\nfunctions. After taking the derivative, we will obtain the PDF, which is a train of\nimpulses.\n\u0088Origin of Gaussian random variables . The origin of the Gaussian random variable lies\nin the fact that many observable events in engineering are sums of independent events.\nThe summation of independent random variables is equivalent to taking convolutions\nof the PDFs. At the limit, they will converge to a bell-shaped function, which is the\nGaussian. Gaussians are everywhere because we observe sums more often than we\nobserve individual states.\n\u0088Transformation of random variables . Transformation of random variables is done\nin the CDF space. The transformation can be used to generate random numbers\n235", "251": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\naccording to a predefined distribution. Specifically, if we want to generate random\nnumbers according to FX, then the transformation is g=F\u22121\nX.\n4.10 Reference\nPDF, CDF, expectation\n4-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 3.1, 3.2.\n4-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 4.1 - 4.3.\n4-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 4.\n4-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapter 4.1, 4.2, 5.1, 5.3, 5.5.\n4-5 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n4.10, 5.1, 5.2, 5.3.\n4-6 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd edition, 2001. Chapter 2.4, 2.5, 4.1, 4.4.\nGaussian random variables\n4-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 3.3.\n4-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 4.4.\n4-9 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n5.4.\n4-10 Mark D. Ward and Ellen Gundlach, Introduction to Probability , W.H. Freeman and\nCompany, 2016. Chapter 35.\nTransformation of random variables\n4-11 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 4.1.\n4-12 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 4.5.\n4-13 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 5.\n4-14 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapter 5.4.\n236", "252": "4.11. PROBLEMS\n4-15 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n5.7.\n4-16 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd edition, 2001. Chapter 3.1, 3.2.\nAdvanced probability textbooks\n4-17 William Feller, An Introduction to Probability Theory and Its Applications , Wiley and\nSons, 3rd Edition, 1950.\n4-18 Andrey Kolmogorov, Foundations of the Theory of Probability , 2nd English Edition,\nDover 2018. (Translated from Russian to English. Originally published in 1950 by\nChelsea Publishing Company New York.)\n4.11 Problems\nExercise 1. (Video Solution)\nLetXbe a Gaussian random variable with \u00b5= 5 and \u03c32= 16.\n(a) Find P[X > 4] and P[2\u2264X\u22647].\n(b) If P[X < a ] = 0.8869, find a.\n(c) IfP[X > b ] = 0.1131, find b.\n(d) If P[13< X\u2264c] = 0.0011, find c.\nExercise 2. (Video Solution)\nCompute E[Y] andE[Y2] for the following random variables:\n(a)Y=Acos(\u03c9t+\u03b8), where A\u223c N(\u00b5, \u03c32).\n(b)Y=acos(\u03c9t+ \u0398), where \u0398 \u223cUniform(0 ,2\u03c0).\n(c)Y=acos(\u03c9T+\u03b8), where T\u223cUniform\u0000\n\u2212\u03c0\n\u03c9,\u03c0\n\u03c9\u0001\n.\nExercise 3. (Video Solution)\nConsider a CDF\nFX(x) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f30, ifx <\u22121,\n0.5, if\u22121\u2264x <0,\n(1 +x)/2,if 0\u2264x <1,\n1, otherwise .\n(a) Find P[X <\u22121],P[\u22120.5< X < 0.5] and P[X > 0.5].\n(b) Find fX(x).\n237", "253": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nExercise 4. (Video Solution)\nA random variable Xhas CDF:\nFX(x) =(\n0, ifx <0,\n1\u22121\n4e\u22122x, ifx\u22650.\n(a) Find P[X\u22642],P[X= 0],P[X < 0],P[2< X < 6] and P[X > 10].\n(b) Find fX(x).\nExercise 5. (Video Solution)\nA random variable Xhas PDF\nfX(x) =(\ncx(1\u2212x2), 0\u2264x\u22641,\n0, otherwise .\nFind c,FX(x), and E[X].\nExercise 6. (Video Solution)\nA continuous random variable Xhas a cumulative distribution\nFX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, x < 0,\n0.5 +csin2(\u03c0x/2), 0\u2264x\u22641,\n1, x > 1.\n(a) What values can cassume?\n(b) Find fX(x).\nExercise 7. (Video Solution)\nA continuous random variable Xis uniformly distributed in [ \u22122,2].\n(a) Let Y= sin( \u03c0X/8). Find fY(y).\n(b) Let Z=\u22122X2+ 3. Find fZ(z).\nHint: Compute FY(y) from FX(x), and used\ndysin\u22121y=1\u221a\n1\u2212y2.\nExercise 8.\nLetY=eX.\n(a) Find the CDF and PDF of Yin terms of the CDF and PDF of X.\n(b) Find the PDF of Ywhen Xis a Gaussian random variable. In this case, Yis said to\nbe a lognormal random variable.\nExercise 9.\nThe random variable Xhas the PDF\nfX(x) =(\n1\n2\u221ax, 0\u2264x\u22641,\n0, otherwise .\n238", "254": "4.11. PROBLEMS\nLetYbe a new random variable\nY=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, X < 0,\u221a\nX, 0\u2264X\u22641,\n1, X > 1.\nFind FY(y) and fY(y), for\u2212\u221e< y < \u221e.\nExercise 10.\nA random variable Xhas the PDF\nfX(x) =(\n2xe\u2212x2, x \u22650,\n0, x < 0.\nLet\nY=g(X) =(\n1\u2212e\u2212X2, X \u22650,\n0, X < 0.\nFind the PDF of Y.\nExercise 11.\nA random variable Xhas the PDF\nfX(x) =1\n2e\u2212|x|,\u2212\u221e< x < \u221e.\nLetY=g(X) =e\u2212X. Find the PDF of Y.\nExercise 12.\nA random variable Xhas the PDF\nfX(x) =1\u221a\n2\u03c0\u03c32e\u2212x2\n2\u03c32,\u2212\u221e< x < \u221e.\nFind the PDF of Ywhere\nY=g(X) =(\nX, |X|> K,\n\u2212X, |X|< K.\nExercise 13.\nA random variable Xhas the PDF\nfX(x) =1\nx2\u221a\n2\u03c0e\u2212x2\n2,\u2212\u221e< x < \u221e.\nLetY=g(X) =1\nX. Find the PDF of Y.\nExercise 14.\nA random variable Xhas the CDF\nFX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f30, x < 0,\nx\u03b1, 0\u2264x\u22641,\n1, x > 1,\n239", "255": "CHAPTER 4. CONTINUOUS RANDOM VARIABLES\nwith \u03b1 >0. Find the CDF of Yif\nY=g(X) =\u2212logX.\nExercise 15.\nEnergy efficiency is an important aspect of designing electrical systems. In some modern\nbuildings (e.g., airports), traditional escalators are being replaced by a new type of \u201csmart\u201d\nescalator which can automatically switch between a normal operating mode and a standby\nmode depending on the flow of pedestrians.\n(a) The arrival of pedestrians can be modeled as a Poisson random variable. Let Nbe the\nnumber of arrivals, and let \u03bbbe the arrival rate (people per minute). For a period of\ntminutes, show that the probability that there are narrivals is\nP(N=n) =(\u03bbt)n\nn!e\u2212\u03bbt.\n(b) Let Tbe a random variable denoting the interarrival time (i.e., the time between two\nconsecutive arrivals). Show that\nP(T > t ) =e\u2212\u03bbt.\nAlso, determine FT(t) and fT(t). Sketch fT(t).\n(Hint: Note that P(T > t ) =P(no arrival in tminutes).)\n(c) Suppose that the escalator will go into standby mode if there are no pedestrians for\nt0= 30 seconds. Let Ybe a random variable denoting the amount of time that the\nescalator is in standby mode. That is, let\nY=(\n0, ifT\u2264t0,\nT\u2212t0, ifT > t 0.\nFindE[Y].\n240", "256": "Chapter 5\nJoint Distributions\nWhen you go to a concert hall, sometimes you may want to see a solo violin concert, but other\ntimes you may want to see a symphony. Symphonies are appealing because many instruments\nare playing together. Random variables are similar. While single random variables are useful\nfor modeling simple events, we use multiple random variables to describe complex events.\nThe multiple random variables can be either independent or correlated. When many random\nvariables are present in the problem, we enter the subject of joint distribution .\nWhat are joint distributions?\nIn the simplest sense, joint distributions are extensions of the PDFs and PMFs we studied\nin the previous chapters. We summarize them as follows.\nJoint distributions are high-dimensional PDFs (or PMFs or CDFs).\nWhat do we mean by a high-dimensional PDF? We know that a single random variable is\ncharacterized by a 1-dimensional PDF fX(x). If we have a pair of random variables, then\nwe use a 2-dimensional function fX,Y(x, y), and if we have a triplet of random variables,\nwe use a 3-dimensional function fX,Y,Z (x, y, z ). In general, the dimensionality of the PDF\ngrows as the number of variables:\nfX(x)|{z}\none variable=\u21d2fX1,X2(x1, x2)|{z }\ntwo variables=\u21d2 \u00b7\u00b7\u00b7 =\u21d2fX1,...,X N(x1, . . . , x N)| {z }\nNvariables.\nFor busy engineers like us, fX1,...,X N(x1, . . . , x N) is not a friendly notation. A more con-\ncise way to write fX1,...,X N(x1, . . . , x N) is to define a vector of random variables X=\n[X1, X2, . . . , X N]Twith a vector of states x= [x1, x2, . . . , x N]T, and to define the PDF as\nfX(x) =fX1,...,X N(x1, . . . , x N).\nUnder what circumstance will we encounter creatures like fX(x)? Believe it or not,\nthese high-dimensional PDFs are everywhere . In 2010, computer-vision scientists created\nthe ImageNet dataset, containing 14 million images with ground-truth class labels. This\nenormous dataset has enabled a great blossoming of machine learning over the past several\n241", "257": "CHAPTER 5. JOINT DISTRIBUTIONS\nFigure 5.1: Joint distributions are ubiquitous in modern data analysis. For example, an image from a\ndataset can be represented by a high-dimensional vector x. Each vector has a certain probability of\nbeing present. This probability is described by the high-dimensional joint PDF fX(x). The goal of this\nchapter is to understand the properties of this fX.\n0\n543 50.05\n2 4 1 3\ny2 0 1\nx-10.1\n0 -2 -1-2 -3-3 -4 -4 -5 -5\n-5 -4 -3 -2 -1 0 1 2 3 4 5\nx00.10.20.30.40.5\n-5 -4 -3 -2 -1 0 1 2 3 4 5\ny00.10.20.30.40.5\nFigure 5.2: A 2-dimensional PDF fX,Y(x, y)of a pair of random variables (X, Y)and their respective\n1D PDFs fX(x)andfY(y).\ndecades, in which many advances in deep learning have been made. Fundamentally, the\nImageNet dataset provides a large collection of samples drawn from a latent distribution\nthat is high-dimensional. Each sample in the ImageNet dataset is a 224 \u00d7224\u00d73 image (the\nthree numbers stand for the image\u2019s height, width, and color). If we convert this image into\na vector, then the sample will have a dimension of 224 \u00d7224\u00d73 = 150,528. In other words,\nthe sample is a vector x\u2208R150528 \u00d71. The probability of obtaining a particular sample x\nis determined by probability density function fX(x). For example, it is more likely to get\nan image containing trees than one containing a Ferrari. The manifold generated by fX(x)\ncan be extremely complex, as illustrated in Figure 5.1 .\nThe story of ImageNet is just one of the many instances for which we use a joint\ndistribution fX(x). Joint distributions are ubiquitous. If you do data science, you must\nunderstand joint distributions. However, extending a 1-dimensional function fX(x) to a\n2-dimensional function fX,Y(x, y) and then to a N-dimensional function fX(x) is not trivial.\nThe goal of this chapter is to guide you through these important steps.\nPlan of Part 1 of this chapter: Two variables\nThis chapter is broadly divided into two halves. In the first half, we will look at a pair of\nrandom variables .\n\u0088Definition of fX,Y(x, y). The first thing we need to learn is the definition of a joint\ndistribution with two variables. Since we have two variables, the joint probability\ndensity function (or probability mass function) is a 2-dimensional function. A point\n242", "258": "on this 2D function is the probability density evaluated by a pair of variables X=x\nandY=y, as illustrated in Figure 5.2 . However, how do we formally define this 2D\nfunction? How is it related to the probability measure? Is there a way we can retrieve\nfX(x) and fY(y) from fX,Y(x, y), as illustrated on the right-hand sides of Figure 5.2 ?\nThese questions will be answered in Section 5.1.\n\u0088Joint expectation E[XY]. When we have a pair of random variables, how should we\ndefine the expectation? In Section 5.2, we will show that the most natural way to define\nthe joint expectation is in terms of E[XY], i.e., the expectation of the product. There\nis a surprising and beautiful connection between this \u201cexpectation of the product\u201d and\nthe cosine angle between two vectors, thereby showing that E[XY] is the correlation\nbetween XandY.\n\u0088The reason for studying a pair of random variables is to spell out the cause-effect\nrelationship between the variables. This cannot be done without conditional distri-\nbutions; this will be explained in Section 5.3. Conditional distributions provide an\nextremely important computational tool for decoupling complex events into simpler\nevents. Such decomposition allows us to solve difficult joint expectation problems via\nsimple conditional expectations ; this subject will be covered in Section 5.4.\n\u0088If you recall our discussions about the origin of a Gaussian random variable, we claimed\nthat the PDF of X+Yis the convolution between fXandfY. Why is this so? We\nwill answer this question in terms of joint distributions in Section 5.5.\nPlan of Part 2 of this chapter: Nvariables\nThe second half of the chapter focuses on the general case of Nrandom variables. This\nrequires the definitions of a random vector X= [X1, . . . , X N]T, a joint distribution fX(x),\nand the corresponding expectations E[X]. To make our discussions concrete, we will focus\non the case of high-dimensional Gaussian random variables and discuss the following topics.\n\u0088Covariance matrices/correlation matrices . If a pair of random variables can define\nthe correlation through the expectation of the product E[X1X2], then for a vector of\nrandom variables we can consider a matrix of correlations in the form\nR=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0E[X1X1]E[X1X2]\u00b7\u00b7\u00b7E[X1XN]\nE[X2X1]E[X2X2]\u00b7\u00b7\u00b7E[X2XN]\n............\nE[XNX1]E[XNX2]\u00b7\u00b7\u00b7E[XNXN]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nWhat are the properties of the matrix? How does it affect the shape of the high-\ndimensional Gaussian? If we have a dataset of vectors, how do we estimate this matrix\nfrom the data? We will answer these questions in Section 5.6 and Section 5.7.\n\u0088Principal-component analysis . Given the covariance matrix, we can perform some\nvery useful data analyses, such as the principal-component analysis in Section 5.8.\nThe question we will ask is: Among the many components, which one is the principal\ncomponent? If we can find the principal component(s), we can effectively perform\ndimensionality reduction by projecting a high-dimensional vector into low-dimensional\nrepresentations. We will introduce an application for face detection.\n243", "259": "CHAPTER 5. JOINT DISTRIBUTIONS\nFigure 5.3: When there is a pair of random variables, we can regard the sample space as a set of\ncoordinates. The random variables are 2D mappings from a coordinate \u03c9in\u2126X\u00d7\u2126Yto another\ncoordinate X(\u03c9)inR2.\n5.1 Joint PMF and Joint PDF\nProbability is a measure of the size of a set. This principle applies to discrete random vari-\nables, continuous random variables, single random variables, and multiple random variables.\nIn situations with a pair of random variables, the measure should be applied to the coordi-\nnate ( X, Y) represented by the random variables XandY. Consequently, when measuring\nthe probability, we either count these coordinates or integrate the area covered by these\ncoordinates. In this section, we formalize this notion of measuring 2D events.\n5.1.1 Probability measure in 2D\nConsider two random variables XandY. Let the sample space of XandYbe \u2126 Xand\n\u2126Y, respectively. Define the Cartesian product of \u2126 Xand \u2126 Yas \u2126 X\u00d7\u2126Y={(x, y)|x\u2208\n\u2126Xandy\u2208\u2126Y}. That is, \u2126 X\u00d7\u2126Ycontains all possible pairs ( X, Y).\nExample 5.1 . If \u2126 X={1,2}and \u2126 Y={4,5}, then \u2126 X\u00d7\u2126Y={(1,4),(1,5),\n(2,4),(2,5)}.\n244", "260": "5.1. JOINT PMF AND JOINT PDF\nExample 5.2 . If \u2126 X= [3,4] and \u2126 Y= [1,2], then \u2126 X\u00d7\u2126Y= a rectangle with two\ndiagonal vertices as (3 ,1) and (4 ,2).\nRandom variables are mappings from the sample space to the real line. If \u03c9\u2208\u2126Xis\nmapped to X(\u03c9)\u2208R, and \u03be\u2208\u2126Yis mapped to Y(\u03be)\u2208R, then a coordinate \u03c9= (\u03c9, \u03be) in\nthe sample space \u2126 X\u00d7\u2126Yshould be mapped to a coordinate ( X(\u03c9), Y(\u03be)) in the 2D plane.\n\u03c9def=\u0014\u03c9\n\u03be\u0015\n7\u2212\u2192\u0014X(\u03c9)\nY(\u03be)\u0015\ndef=X(\u03c9).\nWe denote such a vector-to-vector mapping as X(\u00b7) : \u2126 X\u00d7\u2126Y\u2192R\u00d7R, as illustrated in\nFigure 5.3 .\nTherefore, if we have an event A \u2208R2, the probability that Ahappens is\nP[A] =P[{\u03c9|X(\u03c9)\u2208 A} ]\n=P\u0014\u001a\u0014\u03c9\n\u03be\u0015\f\f\f\f\u0014X(\u03c9)\nY(\u03be)\u0015\n\u2208 A\u001b\u0015\n=P\u0014\u001a\u0014\u03c9\n\u03be\u0015\n\u2208X\u22121(A)\u001b\u0015\n=P[\u03c9\u2208X\u22121(A)].\nIn other words, we take the coordinate X(\u03c9) and find its inverse image X\u22121(A). The size\nof this inverse image X\u22121(A) in the sample space \u2126 X\u00d7\u2126Yis then the probability. We\nsummarize this general principle as follows.\nHow to measure probability in 2D\nFor a pair of random variables X= (X, Y), the probability of an event Ais measured\nin the product space \u2126 X\u00d7\u2126Ywith the size\nP[{\u03c9|X\u22121(A)}].\nThis definition is quite abstract. To make it more concrete, we will look at discrete and\ncontinuous random variables.\n5.1.2 Discrete random variables\nSuppose that the random variables XandYare discrete. Let A={X(\u03c9) =x, Y(\u03be) =y}\nbe a discrete event. Then the above definition tells us that the probability of Ais\nP[A] =P\u0014\n(\u03c9, \u03be)\f\f\f\fX(\u03c9) =x,andY(\u03be) =y\u0015\n=P[X=xandY=y]| {z }\ndef=pX,Y(x,y).\nWe define this probability as the joint probability mass function (joint PMF) pX,Y(x, y).\n245", "261": "CHAPTER 5. JOINT DISTRIBUTIONS\nDefinition 5.1. LetXandYbe two discrete random variables. The joint PMF ofX\nandYis defined as\npX,Y(x, y) =P[X=xandY=y] =P\u0014\n(\u03c9, \u03be)\f\f\f\fX(\u03c9) =x,andY(\u03be) =y\u0015\n.(5.1)\nWe sometimes write the joint PMF as pX,Y(x, y) =P[X=x, Y =y].\nFigure 5.4: A joint PMF for a pair of discrete random variables consists of an array of impulses. To\nmeasure the size of the event A, we sum all the impulses inside A.\nFigure 5.4 shows a graphical portrayal of the joint PMF. In a nutshell, pX,Y(x, y)\ncan be considered as a 2D extension of a single variable PMF. The probabilities are still\nrepresented by the impulses, but the domain of these impulses is now a 2D plane. If we have\nan event A, then the size of the event is\nP[A] =X\n(x,y)\u2208ApX,Y(x, y).\nExample 5.3 . Let Xbe a coin flip, Ybe a die. The sample space of Xis{0,1},\nwhereas the sample space of Yis{1,2,3,4,5,6}. The joint PMF, according to our\ndefinition, is the probability P[X=xandY=y], where xtakes a binary state and Y\ntakes one of the 6 states. The following table summarizes all the 12 states of the joint\ndistribution.\nY\n1 2 3 4 5 6\nX = 01\n121\n121\n121\n121\n121\n12\nX = 11\n121\n121\n121\n121\n121\n12\nIn this table, since there are 12 coordinates, and each coordinate has an equal\nchance of appearing, the probability for each coordinate becomes 1 /12. Therefore, the\njoint PMF of XandYis\npX,Y(x, y) =1\n12, x = 0,1, y = 1,2,3,4,5,6.\n246", "262": "5.1. JOINT PMF AND JOINT PDF\nIn this example, we observe that if XandYare not interacting with each other (for-\nmally, independent ), the joint PMF is the product of the two individual probabilities.\nExample 5.4 . In the previous example, if we define A={X+Y= 3}, the probability\nP[A] is\nP[A] =X\n(x,y)\u2208ApX,Y(x, y) =pX,Y(0,3) +pX,Y(1,2)\n=2\n12.\nIfB={min(X, Y) = 1}, the probability P[B] is\nP[B] =X\n(x,y)\u2208BpX,Y(x, y)\n=pX,Y(1,1) +pX,Y(1,2) +pX,Y(1,3)\n+pX,Y(1,4) +pX,Y(1,5) +pX,Y(1,6)\n=6\n12.\n5.1.3 Continuous random variables\nThe continuous version of the joint PMF is called the joint probability density function\n(joint PDF ), denoted by fX,Y(x, y). A joint PDF is analogous to a joint PMF. For example,\nintegrating it will give us the probability.\nDefinition 5.2. LetXandYbe two continuous random variables. The joint PDF of\nXandYis a function fX,Y(x, y)that can be integrated to yield a probability\nP[A] =Z\nAfX,Y(x, y)dx dy, (5.2)\nfor any event A \u2286\u2126X\u00d7\u2126Y.\nPictorially, we can view fX,Yas a 2D function where the height at a coordinate ( x, y) is\nfX,Y(x, y), as can be seen from Figure 5.5 . To compute the probability that ( X, Y)\u2208 A,\nwe integrate the function fX,Ywith respect to the area covered by the set A. For example,\nif the set Ais a rectangular box A= [a, b]\u00d7[c, d], then the integration becomes\nP[A] =P[a\u2264X\u2264b, c\u2264Y\u2264d]\n=Zd\ncZb\nafX,Y(x, y)dx dy.\n247", "263": "CHAPTER 5. JOINT DISTRIBUTIONS\nFigure 5.5: A joint PDF for a pair of continuous random variables is a surface in the 2D plane. To\nmeasure the size of the event A, we integrate fX,Y(x, y)inside A.\nExample 5.5 . Consider a uniform joint PDF fX,Y(x, y) defined on [0 ,2]2withfX,Y(x, y) =\n1\n4. LetA= [a, b]\u00d7[c, d]. Find P[A].\nSolution .\nP[A] =P[a\u2264X\u2264b, c\u2264X\u2264d]\n=Zd\ncZb\nafX,Y(x, y)dx dy =Zd\ncZb\na1\n4dx dy =(d\u2212c)(b\u2212a)\n4.\nPractice Exercise 5.1 . In the previous example, let B={X+Y\u22642}. Find P[B].\nSolution .\nP[B] =Z\nBfX,Y(x, y)dx dy =Z2\n0Z2\u2212y\n0fX,Y(x, y)dx dy\n=Z2\n0Z2\u2212y\n01\n4dx dy =Z2\n02\u2212y\n4dy=1\n2.\nHere, the limits of the integration can be determined from Figure 5.6 . The inner\nintegration (with respect to x) should start from 0 and end at 2 \u2212y, which is the line\ndefining the set x+y\u22642. Since the inner integration is performed for every y, we\nneed to enumerate all the possible y\u2019s to complete the outer integration. This leads to\nthe outer limit from 0 to 2.\n5.1.4 Normalization\nThe normalization property of a two-dimensional PMF and PDF is the property that, when\nwe enumerate all outcomes of the sample space, we obtain 1.\n248", "264": "5.1. JOINT PMF AND JOINT PDF\nFigure 5.6: To calculate P[X+Y\u22642], we perform a 2D integration over a triangle.\nTheorem 5.1. Let\u2126 = \u2126 X\u00d7\u2126Y. All joint PMFs and joint PDFs satisfy\nX\n(x,y)\u2208\u2126pX,Y(x, y) = 1 orZ\n\u2126fX,Y(x, y)dx dy = 1. (5.3)\nExample 5.6 . Consider a joint uniform PDF defined in the shaded area [0 ,3]\u00d7[0,3]\nwith PDF defined below. Find the constant c.\nfX,Y(x, y) =(\nc if (x, y)\u2208[0,3]\u00d7[0,3],\n0 otherwise .\nSolution . To find the constant c, we note that\n1 =Z3\n0Z3\n0fX,Y(x, y)dx dy =Z3\n0Z3\n0c dx dy = 9c.\nEquating the two sides gives us c=1\n9.\nPractice Exercise 5.2 . Consider a joint PDF\nfX,Y(x, y) =(\nce\u2212xe\u2212y0\u2264y\u2264x <\u221e,\n0 otherwise .\nFind the constant c. Tip: Consider the area of integration as shown in Figure 5.7 .\nSolution . There are two ways to take the integration shown in Figure 5.7 . We choose\nthe inner integration w.r.t. yfirst.\nZ\n\u2126fX,Y(x, y)dx dy =Z\u221e\n0Zx\n0ce\u2212xe\u2212ydy dx =Z\u221e\n0ce\u2212x(1\u2212e\u2212x) =c\n2.\nTherefore, c= 2.\n249", "265": "CHAPTER 5. JOINT DISTRIBUTIONS\nFigure 5.7: To integrate the probability P[0\u2264Y\u2264X], we perform a 2D integration over a triangle.\nThe two subfigures show the two ways of integrating the triangle. [Left]R\ndxfirst, and thenR\ndy.\n[Right]R\ndyfirst, and thenR\ndx.\n5.1.5 Marginal PMF and marginal PDF\nIf we only sum / integrate for one random variable, we obtain the PMF / PDF of the other\nrandom variable. The resulting PMF / PDF is called the marginal PMF / PDF.\nDefinition 5.3. Themarginal PMF is defined as\npX(x) =X\ny\u2208\u2126YpX,Y(x, y)and pY(y) =X\nx\u2208\u2126XpX,Y(x, y), (5.4)\nand the marginal PDF is defined as\nfX(x) =Z\n\u2126YfX,Y(x, y)dyand fY(y) =Z\n\u2126XfX,Y(x, y)dx. (5.5)\nSince fX,Y(x, y) is a two-dimensional function, when integrating over yfrom\u2212\u221eto\u221e, we\nproject fX,Y(x, y) onto the x-axis. Therefore, the resulting function depends on xonly.\nExample 5.7 . Consider the joint PDF fX,Y(x, y) =1\n4shown below. Find the marginal\nPDFs.\n250", "266": "5.1. JOINT PMF AND JOINT PDF\nSolution . If we integrate over xandy, we have\nfX(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f33, if 1< x\u22642,\n1, if 2< x\u22643,\n0, otherwise .and fY(y) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f31, if 1< x\u22642,\n2, if 2< x\u22643,\n1, if 3< x\u22644,\n0, otherwise .\nSo the marginal PDFs are the projection of the joint PDFs onto the x- and y-axes.\nPractice Exercise 5.3 . A joint Gaussian random variable ( X, Y) has a joint PDF\ngiven by\nfX,Y(x, y) =1\n2\u03c0\u03c32exp\u001a\n\u2212((x\u2212\u00b5X)2+ (y\u2212\u00b5Y)2)\n2\u03c32\u001b\n.\nFind the marginal PDFs fX(x) and fY(y).\nSolution .\nfX(x) =Z\u221e\n\u2212\u221efX,Y(x, y)dy\n=Z\u221e\n\u2212\u221e1\n2\u03c0\u03c32exp\u001a\n\u2212((x\u2212\u00b5X)2+ (y\u2212\u00b5Y)2)\n2\u03c32\u001b\ndy\n=1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5X)2\n2\u03c32\u001b\n\u00b7Z\u221e\n\u2212\u221e1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(y\u2212\u00b5Y)2\n2\u03c32\u001b\ndy.\nRecognizing that the last integral is equal to unity because it integrates a Gaussian\nPDF over the real line, it follows that\nfX(x) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5X)2\n2\u03c32\u001b\n.\nSimilarly, we have\nfY(y) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(y\u2212\u00b5Y)2\n2\u03c32\u001b\n.\n5.1.6 Independent random variables\nTwo random variables are said to be independent if and only if the joint PMF or PDF can\nbe factorized as a product of the marginal PMF / PDFs.\nDefinition 5.4. Random variables XandYareindependent if and only if\npX,Y(x, y) =pX(x)pY(y),orfX,Y(x, y) =fX(x)fY(y).\nThis definition is consistent with the definition of independence of two events. Recall that\ntwo events AandBare independent if and only if P[A\u2229B] =P[A]P[B]. Letting A={X=x}\n251", "267": "CHAPTER 5. JOINT DISTRIBUTIONS\nandB={Y=y}, we see that if AandBare independent then P[X=x\u2229Y=y] is the\nproduct P[X=x]P[Y=y]. This is precisely the relationship pX,Y(x, y) =pX(x)pY(y).\nExample 5.8 . Consider two random variables with a joint PDF given by\nfX,Y(x, y) =1\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5X)2+ (y\u2212\u00b5Y)2\n2\u03c32\u001b\n.\nAreXandYindependent?\nSolution . We know that\nfX,Y(x, y) =1\u221a\n2\u03c0\u03c3exp\u001a\n\u2212(x\u2212\u00b5X)2\n2\u03c32\u001b\n| {z }\nfX(x)\u00d71\u221a\n2\u03c0\u03c3exp\u001a\n\u2212(y\u2212\u00b5Y)2\n2\u03c32\u001b\n| {z }\nfY(y).\nTherefore, the random variables XandYare independent.\nPractice Exercise 5.4 . Let Xbe a coin and Ybe a die. Then the joint PMF is given\nby the table below.\nY\n1 2 3 4 5 6\nX = 01\n121\n121\n121\n121\n121\n12\nX = 11\n121\n121\n121\n121\n121\n12\nAreXandYindependent?\nSolution . For any xandy, we have that\npX,Y(x, y) =1\n12=1\n2|{z}\npX(x)\u00d71\n6|{z}\npY(y).\nTherefore, the random variables XandYare independent.\nExample 5.9 . Consider two random variables XandYwith a joint PDF given bya\nfX,Y(x, y)\u221dexp\b\n\u2212(x\u2212y)2\t\n= exp\b\n\u2212x2+ 2xy\u2212y2\t\n= exp\b\n\u2212x2\t\n|{z}\nfX(x)exp{2xy}|{z}\nextra termexp\b\n\u2212y2\t\n|{z}\nfY(y).\nThis PDF cannot be factorized into a product of two marginal PDFs. Therefore, the\nrandom variables are dependent.\naWe use the notation \u201c \u221d\u201d to denote \u201cproportional to\u201d. It implies that the normalization constant\nis omitted.\n252", "268": "5.1. JOINT PMF AND JOINT PDF\nWe can extrapolate the definition of independence to multiple random variables. If\nthere are many random variables X1, X2, . . . , X N, they will have a joint PDF\nfX1,...,X N(x1, . . . , x N).\nIf these random variables X1, X2, . . . , X Nare independent, then the joint PDF can be\nfactorized as\nfX1,...,X N(x1, . . . , x N) =fX1(x1)\u00b7fX2(x2)\u00b7\u00b7\u00b7fXN(xN)\n=NY\nn=1fXn(xn).\nThis gives us the definition of independence for Nrandom variables.\nDefinition 5.5. A sequence of random variables X1, . . . , X Nisindependent if and\nonly if their joint PDF (or joint PMF) can be factorized.\nfX1,...,X N(x1, . . . , x N) =NY\nn=1fXn(xn). (5.6)\nExample 5.10 . Throw a die 4 times. Let X1,X2,X3andX4be the outcomes. Then,\nsince these four throws are independent, the probability mass function of any quadrable\n(x1, x2, x3, x4) is\npX1,X2,X3,X4(x1, x2, x3, x4) =pX1(x1)pX2(x2)pX3(x3)pX4(x4).\nFor example, the probability of getting (1 ,5,2,6) is\npX1,X2,X3,X4(1,5,2,6) = pX1(1)pX2(5)pX3(2)pX4(6) =\u00121\n6\u00134\n.\nThe example above demonstrates an interesting phenomenon. If the Nrandom vari-\nables are independent, and if they all have the same distribution, then the joint PDF/PMF\nis just one of the individual PDFs taken to the power N. Random variables satisfying this\nproperty are known as independent and identically distributed random variables.\nDefinition 5.6 (Independent and Identically Distributed (i.i.d.) ).A collection of\nrandom variables X1, . . . , X Nis called independent and identically distributed (i.i.d.)\nif\n\u0088AllX1, . . . , X Nare independent; and\n\u0088AllX1, . . . , X Nhave the same distribution, i.e., fX1(x) =\u00b7\u00b7\u00b7=fXN(x).\nIfX1, . . . , X Nare i.i.d., we have that\nfX1,...,X N(x1, . . . , x 1) =NY\nn=1fX1(xn),\n253", "269": "CHAPTER 5. JOINT DISTRIBUTIONS\nwhere the particular choice of X1is unimportant because fX1(x) =\u00b7\u00b7\u00b7=fXN(x).\nWhy is i.i.d. so important?\n\u0088If a set of random variables are i.i.d., then the joint PDF can be written as a\nproduct of PDFs.\n\u0088Integrating a joint PDF is difficult. Integrating a product of PDFs is much easier.\nExample 5.11 . Let X1, X2, . . . , X Nbe a sequence of i.i.d. Gaussian random variables\nwhere each Xihas a PDF\nfXi(x) =1\u221a\n2\u03c0exp\u001a\n\u2212x2\n2\u001b\n.\nThe joint PDF of X1, X2, . . . , X Nis\nfX1,...,X N(x1, . . . , x N) =NY\ni=1\u001a1\u221a\n2\u03c0exp\u001a\n\u2212x2\ni\n2\u001b\u001b\n=\u00121\u221a\n2\u03c0\u0013N\nexp(\n\u2212NX\ni=1x2\ni\n2)\n,\nwhich is a function depending not on the individual values of x1, x2, . . . , x Nbut on the\nsumPN\ni=1x2\ni. So we have \u201ccompressed\u201d an N-dimensional function into a 1D function.\nExample 5.12 . Let \u03b8be a deterministic number that was sent through a noisy channel.\nWe model the noise as an additive Gaussian random variable with mean 0 and variance\n\u03c32. Supposing we have observed measurements Xi=\u03b8+Wi, for i= 1, . . . , N , where\nWi\u223cGaussian(0 , \u03c32), then the PDF of each Xiis\nfXi(x) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u03b8)2\n2\u03c32\u001b\n.\nThus the joint PDF of ( X1, X2, . . . , X N) is\nfX1,...,X N(x1, . . . , x N) =NY\ni=1\u001a1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(xi\u2212\u03b8)2\n2\u03c32\u001b\u001b\n=\u00121\u221a\n2\u03c0\u03c32\u0013N\nexp(\n\u2212NX\ni=1(xi\u2212\u03b8)2\n2\u03c32)\n.\nEssentially, this joint PDF tells us the probability density of seeing sample data\nx1, . . . , x N.\n254", "270": "5.1. JOINT PMF AND JOINT PDF\n5.1.7 Joint CDF\nWe now introduce the cumulative distribution function (CDF) for multiple variables.\nDefinition 5.7. LetXandYbe two random variables. The joint CDF ofXandY\nis the function FX,Y(x, y)such that\nFX,Y(x, y) =P[X\u2264x\u2229Y\u2264y]. (5.7)\nThis definition can be more explicitly written as follows.\nDefinition 5.8. IfXandYare discrete, then\nFX,Y(x, y) =X\ny\u2032\u2264yX\nx\u2032\u2264xpX,Y(x\u2032, y\u2032). (5.8)\nIfXandYare continuous, then\nFX,Y(x, y) =Zy\n\u2212\u221eZx\n\u2212\u221efX,Y(x\u2032, y\u2032)dx\u2032dy\u2032. (5.9)\nIf the two random variables are independent , then we have\nFX,Y(x, y) =Zx\n\u2212\u221efX(x\u2032)dx\u2032Zy\n\u2212\u221efY(y\u2032)dy\u2032=FX(x)FY(y).\nExample 5.13 . Let XandYbe two independent uniform random variables\nUniform(0 ,1). Find the joint CDF.\nSolution .\nFX,Y(x, y) =FX(x)FY(y) =Zx\n0fX(x\u2032)dx\u2032Zy\n0fY(y\u2032)dy\u2032\n=Zx\n01dx\u2032Zy\n01dy\u2032=xy.\nPractice Exercise 5.5 . Let XandYbe two independent uniform random variables\nGaussian( \u00b5, \u03c32). Find the joint CDF.\nSolution . Let \u03a6( \u00b7) be the CDF of the standard Gaussian.\nFX,Y(x, y) =FX(x)FY(y)\n=Zx\n\u2212\u221efX(x\u2032)dx\u2032Zy\n\u2212\u221efY(y\u2032)dy\u2032= \u03a6\u0012x\u2212\u00b5\n\u03c3\u0013\n\u03a6\u0012y\u2212\u00b5\n\u03c3\u0013\n.\n255", "271": "CHAPTER 5. JOINT DISTRIBUTIONS\nHere are a few properties of the CDF:\nFX,Y(x,\u2212\u221e) =Z\u2212\u221e\n\u2212\u221eZx\n\u2212\u221efX,Y(x\u2032, y\u2032)dx\u2032dy\u2032=Zx\n\u2212\u221e0dx\u2032= 0,\nFX,Y(\u2212\u221e, y) =Zy\n\u2212\u221eZ\u2212\u221e\n\u2212\u221efX,Y(x\u2032, y\u2032)dx\u2032dy\u2032=Zy\n\u2212\u221e0dy\u2032= 0,\nFX,Y(\u2212\u221e,\u2212\u221e) =Z\u2212\u221e\n\u2212\u221eZ\u2212\u221e\n\u2212\u221efX,Y(x\u2032, y\u2032)dx\u2032dy\u2032= 0,\nFX,Y(\u221e,\u221e) =Z\u221e\n\u2212\u221eZ\u221e\n\u2212\u221efX,Y(x\u2032, y\u2032)dx\u2032dy\u2032= 1.\nIn addition, we can obtain the marginal CDF as follows.\nProposition 5.1. LetXandYbe two random variables. The marginal CDF is\nFX(x) =FX,Y(x,\u221e), (5.10)\nFY(y) =FX,Y(\u221e, y). (5.11)\nProof . We prove only the first case. The second case is similar.\nFX,Y(x,\u221e) =Zx\n\u2212\u221eZ\u221e\n\u2212\u221efX,Y(x\u2032, y\u2032)dy\u2032dx\u2032=Zy\n\u2212\u221efX(x\u2032)dx\u2032=FX(x).\u25a1\nBy the fundamental theorem of calculus, we can derive the PDF from the CDF.\nDefinition 5.9. LetFX,Y(x, y)be the joint CDF of XandY. Then, the joint PDF\nis\nfX,Y(x, y) =\u22022\n\u2202y \u2202xFX,Y(x, y). (5.12)\nThe order of the partial derivatives can be switched, yielding a symmetric result:\nfX,Y(x, y) =\u22022\n\u2202x \u2202yFX,Y(x, y).\nExample 5.14 . Let XandYbe two uniform random variables with joint CDF\nFX,Y(x, y) =xyfor 0\u2264x\u22641 and 0 \u2264y\u22641. Find the joint PDF.\nSolution .\nfX,Y(x, y) =\u22022\n\u2202x\u2202yFX,Y(x, y) =\u22022\n\u2202x\u2202yxy= 1,\nwhich is consistent with the definition of a joint uniform random variable.\n256", "272": "5.2. JOINT EXPECTATION\nPractice Exercise 5.6 . Let XandYbe two exponential random variables with joint\nCDF\nFX,Y(x, y) = (1 \u2212e\u2212\u03bbx)(1\u2212e\u2212\u03bby), x \u22650, y\u22650.\nFind the joint PDF.\nSolution .\nfX,Y(x, y) =\u22022\n\u2202x\u2202yFX,Y(x, y) =\u22022\n\u2202x\u2202y(1\u2212e\u2212\u03bbx)(1\u2212e\u2212\u03bby)\n=\u2202\n\u2202x\u0000\n(1\u2212e\u2212\u03bbx)(\u03bbe\u2212\u03bby)\u0001\n=\u03bbe\u2212\u03bbx\u03bbe\u2212\u03bby.\nwhich is consistent with the definition of a joint exponential random variable.\n5.2 Joint Expectation\n5.2.1 Definition and interpretation\nWhen we have a single random variable, the expectation is defined as\nE[X] =Z\n\u2126xfX(x)dx.\nFor a pair of random variables, what would be a good way of defining the expectation?\nCertainly, we cannot just replace fX(x) by fX,Y(x, y) because the integration has to be-\ncome a double integration. However, if it is a double integration, where should we put the\nvariable y? It turns out that a useful way of defining the expectation for XandYis as\nfollows.\nDefinition 5.10. LetXandYbe two random variables. The joint expectation is\nE[XY] =X\ny\u2208\u2126YX\nx\u2208\u2126Xxy\u00b7pX,Y(x, y) (5.13)\nifXandYare discrete, or\nE[XY] =Z\ny\u2208\u2126YZ\nx\u2208\u2126Xxy\u00b7fX,Y(x, y)dx dy (5.14)\nifXandYare continuous. Joint expectation is also called correlation .\nThe double summation and integration on the right-hand side of the equation is nothing\nbut the state times the probability. Here, the state is the product xy, and the probability is\nthe joint PMF pX,Y(x, y) (or PDF). Therefore, as long as you agree that joint expectation\nshould be defined as E[XY], the double summation and the double integration make sense.\n257", "273": "CHAPTER 5. JOINT DISTRIBUTIONS\nThe biggest mystery here is E[XY]. You may wonder why the joint expectation should\nbe defined as the expectation of the product E[XY]. Why not the sum E[X+Y], or the\ndifference E[X\u2212Y], or the quotient E[X/Y ]? Why are we so deeply interested in Xtimes Y?\nThese are excellent questions. That the joint expectation is defined as the product has to do\nwith the correlation between two random variables. We will take a small detour into linear\nalgebra.\nLet us consider two discrete random variables XandY, both with Nstates. So X\nwill take the states {x1, x2, . . . , x N}andYwill take the states {y1, y2, . . . , y N}. Let\u2019s define\nthem as two vectors: xdef= [x1, . . . , x N]Tandydef= [y1, . . . , y N]T. Since XandYare random\nvariables, they have a joint PMF pX,Y(x, y). The array of the PMF values can be written\nas a matrix:\nPMF as a matrix = Pdef=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0pX,Y(x1, y1)pX,Y(x1, y2)\u00b7\u00b7\u00b7 pX,Y(x1, yN)\npX,Y(x2, y1)pX,Y(x2, y2)\u00b7\u00b7\u00b7 pX,Y(x2, yN)\n............\npX,Y(xN, y1)pX,Y(xN, y2)\u00b7\u00b7\u00b7pX,Y(xN, yN)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nLet\u2019s try to write the joint expectation in terms of matrices and vectors. The definition\nof a joint expectation tells us that\nE[XY] =NX\ni=1NX\nj=1xiyj\u00b7pX,Y(xi, yj),\nwhich can be written as\nE[XY] =\u0002x1\u00b7\u00b7\u00b7xN\u0003\n|{z }\nxT\uf8ee\n\uf8ef\uf8f0pX,Y(x1, y1)\u00b7\u00b7\u00b7 pX,Y(x1, yN)\n.........\npX,Y(xN, y1)\u00b7\u00b7\u00b7pX,Y(xN, yN)\uf8f9\n\uf8fa\uf8fb\n| {z }\nP\uf8ee\n\uf8ef\uf8f0y1\n...\nyN\uf8f9\n\uf8fa\uf8fb\n|{z}\ny=xTPy.\nThis is a weighted inner product between xandyusing the weight matrix P.\nWhy correlation is defined as E[XY]\n\u0088E[XY] is a weighted inner product between the states:\nE[XY] =xTPy.\n\u0088xandyare the states of the random variables XandY.\n\u0088The inner product measures the similarity between two vectors.\nExample 5.15 . Let Xbe a discrete random variable with Nstates, where each state\nhas an equal probability. Thus, pX(x) = 1 /Nfor all x. Let Y=Xbe another variable.\n258", "274": "5.2. JOINT EXPECTATION\nThen the joint PMF of ( X, Y) is\npX,Y(x, y) =(\n1\nN, x =y,\n0, x \u0338=y.\nIt follows that the joint expectation is\nE[XY] =NX\ni=1NX\nj=1xiyj\u00b7pX,Y(xi, yj) =1\nNNX\ni=1xiyi.\nEquivalently, we can obtain the result via the inner product by defining\nP=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f01\nN0\u00b7\u00b7\u00b7 0\n01\nN\u00b7\u00b7\u00b7 0\n............\n0\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b71\nN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=1\nNI.\nIn this case, the weighted inner product is\nxTPy=xTy\nN=1\nNNX\ni=1xiyi=E[XY].\nHow do we understand the inner product? Ignoring the matrix Pfor a moment, we\nrecall an elementary result in linear algebra.\nDefinition 5.11. Letx\u2208RNandy\u2208RNbe two vectors. Define the cosine angle\ncos\u03b8as\ncos\u03b8=xTy\n\u2225x\u2225\u2225y\u2225, (5.15)\nwhere \u2225x\u2225=qPN\ni=1x2\niis the norm of the vector x, and \u2225y\u2225=qPN\ni=1y2\niis the\nnorm of the vector y.\nThis definition can be understood as the geometry between two vectors, as illustrated in\nFigure 5.8 . If the two vectors xandyare parallel so that x=\u03b1yfor some \u03b1, then the\nangle \u03b8= 0. If xandyare orthogonal so that xTy= 0, then \u03b8=\u03c0/2. Therefore, the inner\nproduct xTytells us the degree of correlation between the vectors xandy.\nNow let\u2019s come back to our discussion about the joint expectation. The cosine angle\ndefinition tells us that if E[XY] =xTPy, the following form would make sense:\ncos\u03b8=xTPy\n\u2225x\u2225\u2225y\u2225=E[XY]\n\u2225x\u2225\u2225y\u2225.\nThat is, as long as we can find out the norms \u2225x\u2225and\u2225y\u2225, we will be able to interpret\nE[XY] from the cosine angle perspective. But what would be a reasonable definition of \u2225x\u2225\n259", "275": "CHAPTER 5. JOINT DISTRIBUTIONS\nFigure 5.8: The geometry of joint expectation. E[XY]gives us the cosine angle between the two random\nvariables. This, in turn, tells us the correlation between the two random variables.\nand\u2225y\u2225? We define the norm by first considering the variance of the random variable X\nandY:\nE[X2] =NX\ni=1xixi\u00b7pX(xi)\n=\u0002\nx1\u00b7\u00b7\u00b7xN\u0003\n|{z }\nxT\uf8ee\n\uf8ef\uf8f0pX(x1)\u00b7\u00b7\u00b7 0\n.........\n0 \u00b7\u00b7\u00b7pX(xN)\uf8f9\n\uf8fa\uf8fb\n| {z }\nPX\uf8ee\n\uf8ef\uf8f0x1\n...\nxN\uf8f9\n\uf8fa\uf8fb\n|{z}\nx\n=xTPXx=\u2225x\u22252\nPX,\nwhere PXis the diagonal matrix storing the probability masses of the random variable X.\nIt is not difficult to show that PX= diag( P1) by following the definition of the marginal\ndistributions (which are the column and row sums of the joint PMF). Similarly we can define\nE[Y2] =NX\nj=1yjyj\u00b7pY(yj)\n=\u0002\ny1\u00b7\u00b7\u00b7yN\u0003\n|{z }\nyT\uf8ee\n\uf8ef\uf8f0pY(y1)\u00b7\u00b7\u00b7 0\n.........\n0 \u00b7\u00b7\u00b7pY(yN)\uf8f9\n\uf8fa\uf8fb\n| {z }\nPY\uf8ee\n\uf8ef\uf8f0y1\n...\nyN\uf8f9\n\uf8fa\uf8fb\n|{z}\ny\n=yTPYy=\u2225y\u22252\nPY.\nTherefore, one way to define the cosine angle is to start with\ncos\u03b8=xTPXYy\n\u2225x\u2225PX\u2225y\u2225PY,\nwhere PXY=P,\u2225x\u2225PX=p\nxTPXxand\u2225y\u2225PY=p\nyTPYy. But writing it in terms of\nthe expectation, we observe that this cosine angle is exactly\ncos\u03b8=xTPXYy\n\u2225x\u2225PX\u2225y\u2225PY=E[XY]p\nE[X2]p\nE[Y2].\n260", "276": "5.2. JOINT EXPECTATION\nTherefore, E[XY] defines the cosine angle between the two random variables, which, in turn,\ndefines the correlation between the two. A large |E[XY]|means that XandYare highly\ncorrelated, and a small |E[XY]|means that XandYare not very correlated. If E[XY] = 0,\nthen the two random variables are uncorrelated. Therefore, E[XY] tells us how the two\nrandom variables are related to each other.\nTo further convince you thatE[XY]\u221a\nE[X2]\u221a\nE[Y2]can be interpreted as a cosine angle, we\nshow that\n\u22121\u2264E[XY]p\nE[X2]p\nE[Y2]\u22641,\nbecause if this ratio can go beyond +1 and \u22121, it makes no sense to call it a cosine angle.\nThe argument follows from a very well-known inequality in probability, called the Cauchy-\nSchwarz inequality (for expectation), which states that \u22121\u2264E[XY]\u221a\nE[X2]\u221a\nE[Y2]\u22641:\nTheorem 5.2 (Cauchy-Schwarz inequality ).For any random variables XandY,\n(E[XY])2\u2264E[X2]E[Y2]. (5.16)\nThe following proof can be skipped if you are reading the book the first time.\nProof . Let t\u2208Rbe a constant. Consider E[(X+tY)2] =E[X2+ 2tXY +t2Y2]. Since\nE[(X+tY)2]\u22650 for any t, it follows that\nE[X2+ 2tXY +t2Y2]\u22650.\nExpanding the left-hand side yields t2E[Y2] + 2tE[XY] +E[X2]\u22650. This is a quadratic\nequation in t, and we know that for any quadratic equation at2+bt+c\u22650 we must have\nb2\u22124ac\u22640. Therefore, in our case, we have that\n(2E[XY])2\u22124E[Y2]E[X2]\u22640,\nwhich means ( E[XY])2\u2264E[X2]E[Y2]. The equality holds when E[(X+tY)2] = 0. In this\ncase, X=\u2212tYfor some t, i.e., the random variable Xis a scaled version of Yso that the\nvector formed by the states of Xis parallel to that of Y.\n\u25a1\nEnd of the proof.\n5.2.2 Covariance and correlation coefficient\nIn many practical problems, we prefer to work with central moments, i.e., E[(X\u2212\u00b5X)2] in-\nstead of E[X2]. This essentially means that we subtract the mean from the random variable.\nIf we adopt such a centralized random variable, we can define the covariance as follows.\n261", "277": "CHAPTER 5. JOINT DISTRIBUTIONS\nDefinition 5.12. LetXandYbe two random variables. Then the covariance ofX\nandYis\nCov(X, Y) =E[(X\u2212\u00b5X)(Y\u2212\u00b5Y)], (5.17)\nwhere \u00b5X=E[X]and\u00b5Y=E[Y].\nIt is easy to show that if X=Y, then the covariance simplifies to the variance:\nCov(X, X ) =E[(X\u2212\u00b5X)(X\u2212\u00b5X)]\n= Var[ X].\nThus, covariance is a generalization of variance. The former can handle a pair of variables,\nwhereas the latter is only for a single variable. We can also demonstrate the following result.\nTheorem 5.3. LetXandYbe two random variables. Then\nCov(X, Y) =E[XY]\u2212E[X]E[Y] (5.18)\nProof . Just apply the definition of covariance:\nCov(X, Y) =E[(X\u2212\u00b5X)(Y\u2212\u00b5Y)]\n=E[XY\u2212X\u00b5Y\u2212Y \u00b5X+\u00b5X\u00b5Y]\n=E[XY]\u2212\u00b5X\u00b5Y.\n\u25a1\nThe next theorem concerns the sum of two random variables.\nTheorem 5.4. For any XandY,\na.E[X+Y] =E[X] +E[Y].\nb.Var[X+Y] = Var[ X] + 2Cov( X, Y) + Var[ Y].\nProof . Recall the definition of joint expectation:\nE[X+Y] =X\nyX\nx(x+y)pX,Y(x, y)\n=X\nyX\nxxpX,Y(x, y) +X\nyX\nxypX,Y(x, y)\n=X\nxx X\nypX,Y(x, y)!\n+X\nyy X\nxpX,Y(x, y)!\n=X\nxxpX(x) +X\nyypY(y)\n=E[X] +E[Y].\n262", "278": "5.2. JOINT EXPECTATION\nSimilarly,\nVar[X+Y] =E[(X+Y)2]\u2212E[X+Y]2\n=E[(X+Y)2]\u2212(\u00b5X+\u00b5Y)2\n=E[X2+ 2XY+Y2]\u2212(\u00b52\nX+ 2\u00b5X\u00b5Y+\u00b52\nY)\n=E[X2]\u2212\u00b52\nX+E[Y2]\u2212\u00b52\nY+ 2(E[XY]\u2212\u00b5X\u00b5Y)\n= Var[ X] + 2Cov( X, Y) + Var[ Y].\n\u25a1\nWith covariance defined, we can now define the correlation coefficient \u03c1, which is the\ncosine angle of the centralized variables. That is,\n\u03c1= cos \u03b8\n=E[(X\u2212\u00b5X)(Y\u2212\u00b5Y)]p\nE[(X\u2212\u00b5X)2]E[(Y\u2212\u00b5Y)2].\nRecognizing that the denominator of this expression is just the variance of XandY, we\ndefine the correlation coefficient as follows.\nDefinition 5.13. LetXandYbe two random variables. The correlation coefficient\nis\n\u03c1=Cov(X, Y)p\nVar[X]Var[Y]. (5.19)\nSince \u22121\u2264cos\u03b8\u22641,\u03c1is also between \u22121 and 1. The difference between \u03c1andE[XY]\nis that \u03c1isnormalized with respect to the variance of XandY, whereas E[XY] is not\nnormalized. The correlation coefficient has the following properties:\n\u0088\u03c1is always between \u22121 and 1, i.e., \u22121\u2264\u03c1\u22641. This is due to the cosine angle\ndefinition.\n\u0088When X=Y(fully correlated), \u03c1= +1.\n\u0088When X=\u2212Y(negatively correlated), \u03c1=\u22121.\n\u0088When XandYare uncorrelated, \u03c1= 0.\n5.2.3 Independence and correlation\nIf two random variables XandYare independent, the joint expectation can be written as\na product of two individual expectations.\nTheorem 5.5. IfXandYare independent, then\nE[XY] =E[X]E[Y]. (5.20)\n263", "279": "CHAPTER 5. JOINT DISTRIBUTIONS\nProof . We only prove the discrete case because the continuous can be proved similarly. If\nXandYare independent, we have pX,Y(x, y) =pX(x)pY(y). Therefore,\nE[XY] =X\nyX\nxxypX,Y(x, y) =X\nyX\nxxypX(x)pY(y)\n= X\nxxpX(x)! X\nyypY(y)!\n=E[X]E[Y].\n\u25a1\nIn general, for any two independent random variables and two functions fandg,\nE[f(X)g(Y)] =E[f(X)]E[g(Y)].\nThe following theorem illustrates a few important relationships between independence\nand correlation.\nTheorem 5.6. Consider the following two statements:\na.XandYare independent;\nb.Cov(X, Y) = 0 .\nStatement (a) implies statement (b), but (b) does not imply (a). Thus, independence\nis a stronger condition than correlation.\nProof . We first prove that (a) implies (b). If XandYare independent, then E[XY] =\nE[X]E[Y]. In this case,\nCov(X, Y) =E[XY]\u2212E[X]E[Y] =E[X]E[Y]\u2212E[X]E[Y] = 0.\nTo prove that (b) does not imply (a), we show a counterexample. Consider a discrete\nrandom variable Zwith PMF\npZ(z) =\u00021\n41\n41\n41\n4\u0003\n.\nLetXandYbe\nX= cos\u03c0\n2Zand Y= sin\u03c0\n2Z.\nThen we can show that E[X] = 0 and E[Y] = 0. The covariance is\nCov(X, Y) =E[(X\u22120)(Y\u22120)]\n=Eh\ncos\u03c0\n2Zsin\u03c0\n2Zi\n=E\u00141\n2sin\u03c0Z\u0015\n=1\n2\u0014\n(sin\u03c00)1\n4+ (sin \u03c01)1\n4+ (sin \u03c02)1\n4+ (sin \u03c03)1\n4\u0015\n= 0.\n264", "280": "5.2. JOINT EXPECTATION\nThe next step is to show that XandYare dependent. To this end, we only need to show\nthatpX,Y(x, y)\u0338=pX(x)pY(y). The joint PMF pX,Y(x, y) can be found by noting that\nZ= 0\u21d2X= 1, Y= 0,\nZ= 1\u21d2X= 0, Y= 1,\nZ= 2\u21d2X=\u22121, Y= 0,\nZ= 3\u21d2X= 0, Y=\u22121.\nThus, the PMF is\npX,Y(x, y) =\uf8ee\n\uf8f001\n40\n1\n401\n4\n01\n40\uf8f9\n\uf8fb.\nThe marginal PMFs are\npX(x) =\u00021\n41\n21\n4\u0003\n, p Y(y) =\u00021\n41\n21\n4\u0003\n.\nThe product pX(x)pY(y) is\npX(x)pY(y) =\uf8ee\n\uf8ef\uf8f01\n161\n81\n16\n1\n81\n41\n8\n1\n161\n81\n16\uf8f9\n\uf8fa\uf8fb.\nTherefore, pX,Y(x, y)\u0338=pX(x)pY(y), although E[XY] =E[X]E[Y].\n\u25a1\nWhat is the relationship between independent and uncorrelated?\n\u0088Independent \u21d2uncorrelated.\n\u0088Independent \u21cduncorrelated.\n5.2.4 Computing correlation from data\nWe close this section by discussing a very practical problem: Given a dataset containing two\ncolumns of data points, how do we determine whether the two columns are correlated?\nRecall that the correlation coefficient is defined as\n\u03c1=E[XY]\u2212\u00b5X\u00b5Y\n\u03c3X\u03c3Y.\nIf we have a dataset containing ( xn, yi)N\nn=1, then the correlation coefficient can be approxi-\nmated by\nb\u03c1=1\nNPN\nn=1xnyn\u2212xyq\n1\nNPN\nn=1(xn\u2212x)2q\n1\nNPN\nn=1(yn\u2212y)2,\nwhere x=1\nNPN\nn=1xnandy=1\nNPN\nn=1ynare the means. This equation should not be a\nsurprise because essentially all terms are the empirical estimates. Thus, b\u03c1is the empirical\ncorrelation coefficient determined from the dataset. As N\u2192 \u221e , we expect b\u03c1\u2192\u03c1.\n265", "281": "CHAPTER 5. JOINT DISTRIBUTIONS\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n(a)b\u03c1=\u22120.0038 (b) b\u03c1= 0.5321 (c) b\u03c1= 0.9656\nFigure 5.9: Visualization of correlated variables. Each of these figures represent a scattered plot of a\ndataset containing (xn, yn)N\nn=1. (a) is uncorrelated. (b) is somewhat correlated. (c) is strongly correlated.\nFigure 5.9 shows three example datasets. We plot the ( xn, yn) pairs as coordinates in\nthe 2D plane. The first dataset contains samples that are almost uncorrelated. We can see\nthat xndoes not tell us anything about yn. The second dataset is moderately correlated.\nThe third dataset is highly correlated: If we know xn, we are almost certain to know the\ncorresponding yn, with a small number of perturbations.\nOn a computer, computing the correlation coefficient can be done using built-in com-\nmands such as corrcoef in MATLAB and stats.pearsonr in Python. The codes to gen-\nerate the results in Figure 5.9 (b) are shown below.\n% MATLAB code to compute the correlation coefficient\nx = mvnrnd([0,0],[3 1; 1 1],1000);\nfigure(1); scatter(x(:,1),x(:,2));\nrho = corrcoef(x)\n# Python code to compute the correlation coefficient\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nx = stats.multivariate_normal.rvs([0,0], [[3,1],[1,1]], 10000)\nplt.figure(); plt.scatter(x[:,0],x[:,1])\nrho,_ = stats.pearsonr(x[:,0],x[:,1])\nprint(rho)\n5.3 Conditional PMF and PDF\nWhenever we have a pair of random variables XandYthat are correlated, we can define\ntheir conditional distributions, which quantify the probability of X=xgiven Y=y. In\nthis section, we discuss the concepts of conditional PMF and PDF.\n266", "282": "5.3. CONDITIONAL PMF AND PDF\n5.3.1 Conditional PMF\nWe start by defining the conditional PMF for a pair of discrete random variables.\nDefinition 5.14. LetXandYbe two discrete random variables. The conditional\nPMF ofXgiven Yis\npX|Y(x|y) =pX,Y(x, y)\npY(y). (5.21)\nThe simplest way to understand this is to view pX|Y(x|y) asP[X=x|Y=y]. That is,\ngiven that Y=y, what is the probability for X=x? To see why this perspective makes\nsense, let us recall the definition of a conditional probability:\npX|Y(x|y) =pX,Y(x, y)\npY(y)\n=P[X=x\u2229Y=y]\nP[Y=y]=P[X=x|Y=y].\nAs we can see, the last two equalities are essentially the definitions of conditional probability\nand the joint PMF.\nHow should we understand the notation pX|Y(x|y)? Is it a one-variable function in xor\na two-variable function in ( x, y)? What does pX|Y(x|y) tell us? To answer these questions,\nlet us first try to understand the randomness exhibited in a conditional PMF. In pX|Y(x|y),\nthe random variable Yisfixed to a specific value Y=y. Therefore there is nothing random\nabout Y. All the possibilities of Yhave already been taken care of by the denominator\npY(y). Only the variable xinpX|Y(x|y) has randomness. What do we mean by \u201cfixed at a\nvalue Y=y\u201d? Consider the following example.\nExample 5.16 . Suppose there are two coins. Let\nX= the sum of the values of two coins ,\nY= the value of the first coin .\nClearly, Xhas 3 states: 0, 1, 2, and Yhas two states: either 0 or 1. When we say\npX|Y(x|1), we refer to the probability mass function of Xwhen fixing Y= 1. If we do\nnot impose this condition, the probability mass of Xis simple:\npX(x) =\u00141\n4,1\n2,1\n4\u0015\n.\nHowever, if we include the conditioning, then\npX|Y(x|1) =pX,Y(x,1)\npY(1)\n=\u0002\n0,2\n4,1\n4\u0003\n1\n6=\u0014\n0,2\n3,1\n3\u0015\n.\n267", "283": "CHAPTER 5. JOINT DISTRIBUTIONS\nTo put this in plain words, when Y= 1, there is no way for Xto take the state 0. The\nchance for Xto take the state 1 is 2 /3 because either (0 ,1) or (1 ,0) can give X= 1.\nThe chance for Xto take the state 2 is 1 /3 because it has to be (1 ,1) in order to give\nX= 2. Therefore, when we say \u201cconditioned on Y= 1\u201d, we mean that we limit our\nobservations to cases where Y= 1. Since Yis already fixed at Y= 1, there is nothing\nrandom about Y. The only variable is X. This example is illustrated in Figure 5.10 .\nFigure 5.10: Suppose Xis the sum of two coins with PMF 0.25,0.5,0.25. Let Ybe the first coin.\nWhen Xis unconditioned, the PMF is just [0.25,0.5,0.25]. When Xis conditioned on Y= 1,\nthen \u201c X= 0\u201d cannot happen. Therefore, the resulting PMF pX|Y(x|1)only has two states. After\nnormalization we obtain the conditional PMF [0,0.66,0.33].\nSince Yis already fixed at a particular value Y=y,pX|Y(x|y) is a probability mass\nfunction of x(we want to emphasize again that it is xand not y). So pX|Y(x|y) is a one-\nvariable function in x. It is not the same as the usual PMF pX(x).pX|Y(x|y) is conditioned\nonY=y. For example, pX|Y(x|1) is the PMF of Xrestricted to the condition that Y= 1.\nIn fact, it follows that\nX\nx\u2208\u2126XpX|Y(x|y) =X\nx\u2208\u2126XpX,Y(x, y)\npY(y)=P\nx\u2208\u2126XpX,Y(x, y)\npY(y)=pY(y)\npY(y)= 1,\nbut this tells us that pX|Y(x|y) is a legitimate probability mass of X. If we sum over the y\u2019s\ninstead, then we will hit a bump:\nX\ny\u2208\u2126YpX|Y(x|y) =X\ny\u2208\u2126YpX,Y(x, y)\npY(y)\u0338= 1.\nTherefore, while pX|Y(x|y) is a legitimate probability mass function of X, it is not a prob-\nability mass function of Y.\nExample 5.17 . Consider a joint PMF given in the following table. Find the conditional\nPMF pX|Y(x|1) and the marginal PMF pX(x).\nY=\n1 2 3 4\nX = 11\n201\n201\n200\n20\n21\n202\n203\n201\n20\n31\n202\n203\n201\n20\n40\n201\n201\n201\n20\n268", "284": "5.3. CONDITIONAL PMF AND PDF\nSolution . To find the marginal PMF, we sum over all the y\u2019s for every x:\nx= 1 : pX(1) =4X\ny=1pX,Y(1, y) =1\n20+1\n20+1\n20+0\n20=3\n20,\nx= 2 : pX(2) =4X\ny=1pX,Y(2, y) =1\n20+2\n20+2\n20+1\n20=6\n20,\nx= 3 : pX(3) =4X\ny=1pX,Y(3, y) =1\n20+3\n20+3\n20+1\n20=8\n20,\nx= 4 : pX(4) =4X\ny=1pX,Y(4, y) =0\n20+1\n20+1\n20+1\n20=3\n20.\nHence, the marginal PMF is\npX(x) =\u00023\n206\n208\n203\n20\u0003\n.\nThe conditional PMF pX|Y(x|1) is\npX|Y(x|1) =pX,Y(x,1)\npY(1)=\u00021\n201\n201\n200\n20\u0003\n3\n20=\u00021\n31\n31\n30\u0003\n.\nPractice Exercise 5.7 . Consider two random variables XandYdefined as follows.\nY=(\n102, with prob 5 /6,\n104, with prob 1 /6.X=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f310\u22124Y, with prob 1 /2,\n10\u22123Y, with prob 1 /3,\n10\u22122Y, with prob 1 /6.\nFind pX|Y(x|y),pX(x) and pX,Y(x, y).\nSolution . Since Ytakes two different states, we can enumerate Y= 102andY= 104.\nThis gives us\npX|Y(x|102) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f31/2, ifx= 0.01,\n1/3, ifx= 0.1,\n1/6, ifx= 1.\npX|Y(x|104) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f31/2, ifx= 1,\n1/3, ifx= 10,\n1/6, ifx= 100 .\n269", "285": "CHAPTER 5. JOINT DISTRIBUTIONS\nThe joint PMF pX,Y(x, y) is\npX,Y(x,102) =pX|Y(x|102)pY(102) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\u00001\n2\u0001\u00005\n6\u0001\n, x = 0.01,\u00001\n3\u0001\u00005\n6\u0001\n, x = 0.1,\u00001\n6\u0001\u00005\n6\u0001\n, x = 1.\npX,Y(x,104) =pX|Y(x|104)pY(104) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\u00001\n2\u0001\u00001\n6\u0001\n, x = 1,\u00001\n3\u0001\u00001\n6\u0001\n, x = 10,\u00001\n6\u0001\u00001\n6\u0001\n, x = 100 .\nTherefore, the joint PMF is given by the following table.\n1040 01\n121\n181\n36\n102 5\n125\n185\n360 0\n0.01 0.1 1 10 100\nThe marginal PMF pX(x) is thus\npX(x) =X\nypX,Y(x, y) =\u00025\n125\n182\n91\n181\n36\u0003\n.\nIn the previous two examples, what is the probability P[X\u2208A|Y=y] or the proba-\nbilityP[X\u2208A] for some events A? The answers are giving by the following theorem.\nTheorem 5.7. LetXandYbe two discrete random variables, and let Abe an event.\nThen\n(i)P[X\u2208A|Y=y] =X\nx\u2208ApX|Y(x|y)\n(ii)P[X\u2208A] =X\nx\u2208AX\ny\u2208\u2126YpX|Y(x|y)pY(y) =X\ny\u2208\u2126YP[X\u2208A|Y=y]pY(y).\nProof . The first statement is based on the fact that if Acontains a finite number of elements,\nthenP[X\u2208A] is equivalent to the sumP\nx\u2208AP[X=x]. Thus,\nP[X\u2208A|Y=y] =P[X\u2208A\u2229Y=y]\nP[Y=y]\n=P\nx\u2208AP[X=x\u2229Y=y]\nP[Y=y]\n=X\nx\u2208ApX|Y(x|y).\nThe second statement holds because the inner summationP\ny\u2208\u2126YpX|Y(x|y)pY(y) is just\nthe marginal PMF pX(x). Thus the outer summation yields the probability.\n\u25a1\n270", "286": "5.3. CONDITIONAL PMF AND PDF\nExample 5.18 . Let us follow up on Example 5.17. What is the probability that\nP[X > 2|Y= 1]? What is the probability that P[X > 2]?\nSolution . Since the problem asks about the conditional probability, we know that it\ncan be computed by using the conditional PMF. This gives us\nP[X > 2|Y= 1] =X\nx>2pX|Y(x|1)\n=\u0018\u0018\u0018\u0018\u0018pX|Y(1|1) +\u0018\u0018\u0018\u0018\u0018pX|Y(2|1) +pX|Y(3|1)|{z}\n1\n3+pX|Y(4|1)|{z}\n0=1\n3.\nThe other probability is\nP[X > 2] =X\nx>2pX(x)\n=\u0018\u0018\u0018pX(1) +\u0018\u0018\u0018pX(2) + pX(3)|{z}\n8\n20+pX(4)|{z}\n3\n20=11\n20.\nWhat is the rule of thumb for conditional distribution?\n\u0088The PMF/PDF should match with the probability you are finding.\n\u0088If you want to find the conditional probability P[X\u2208A|Y=y], use the condi-\ntional PMF pX|Y(x|y).\n\u0088If you want to find the probability P[X\u2208A], use the marginal PMF pX(x).\nFinally, we define the conditional CDF for discrete random variables.\nDefinition 5.15. LetXandYbe discrete random variables. Then the conditional\nCDF ofXgiven Y=yis\nFX|Y(x|y) =P[X\u2264x|Y=y] =X\nx\u2032\u2264xpX|Y(x\u2032|y). (5.22)\n5.3.2 Conditional PDF\nWe now discuss the conditioning of a continuous random variable.\nDefinition 5.16. LetXandYbe two continuous random variables. The conditional\nPDF ofXgiven Yis\nfX|Y(x|y) =fX,Y(x, y)\nfY(y). (5.23)\n271", "287": "CHAPTER 5. JOINT DISTRIBUTIONS\nExample 5.19 . Let XandYbe two continuous random variables with a joint PDF\nfX,Y(x, y) =(\n2e\u2212xe\u2212y, 0\u2264y\u2264x <\u221e,\n0, otherwise .\nFind the conditional PDFs fX|Y(x|y) and fY|X(y|x).\nSolution . We first find the marginal PDFs.\nfX(x) =Z\u221e\n\u2212\u221efX,Y(x, y)dy=Zx\n02e\u2212xe\u2212ydy= 2e\u2212x(1\u2212e\u2212x),\nfY(y) =Z\u221e\n\u2212\u221efX,Y(x, y)dx=Z\u221e\ny2e\u2212xe\u2212ydx= 2e\u22122y.\nThus, the conditional PDFs are\nfX|Y(x|y) =fX,Y(x, y)\nfY(y)\n=2e\u2212xe\u2212y\n2e\u22122y=e\u2212(x+y), x\u2265y,\nfY|X(y|x) =fX,Y(x, y)\nfX(x)\n=2e\u2212xe\u2212y\n2e\u2212x(1\u2212e\u2212x)=e\u2212y\n1\u2212e\u2212x,0\u2264y < x.\nWhere does the conditional PDF come from? We cannot duplicate the argument\nwe used for the discrete case because the denominator of a conditional PMF becomes\nP[Y=y] = 0 when Yis continuous. To answer this question, we first define the conditional\nCDF for continuous random variables.\nDefinition 5.17. LetXandYbe continuous random variables. Then the conditional\nCDF ofXgiven Y=yis\nFX|Y(x|y) =Rx\n\u2212\u221efX,Y(x\u2032, y)dx\u2032\nfY(y). (5.24)\nWhy should the conditional CDF of continuous random variable be defined in this way? One\nway to interpret FX|Y(x|y) is as the limiting perspective. We can define the conditional CDF\nas\nFX|Y(x|y) = lim\nh\u21920P(X\u2264x|y\u2264Y\u2264y+h)\n= lim\nh\u21920P(X\u2264x\u2229y\u2264Y\u2264y+h)\nP[y\u2264Y\u2264y+h].\n272", "288": "5.3. CONDITIONAL PMF AND PDF\nWith some calculations, we have that\nlim\nh\u21920P(X\u2264x\u2229y\u2264Y\u2264y+h)\nP[y\u2264Y\u2264y+h]= lim\nh\u21920Rx\n\u2212\u221eRy+h\nyfX,Y(x\u2032, y\u2032)dy\u2032dx\u2032\nRy+h\nyfY(y\u2032)dy\u2032\n= lim\nh\u21920Rx\n\u2212\u221efX,Y(x\u2032, y\u2032)dx\u2032\u00b7h\nfY(y)\u00b7h\n=Rx\n\u2212\u221efX,Y(x\u2032, y\u2032)dx\u2032\nfY(y).\nThe key here is that the small step size hin the numerator and the denominator will\ncancel each other out. Now, given the conditional CDF, we can verify the definition of the\nconditional PDF. It holds that\nfX|Y(x|y) =d\ndxFX|Y(x|y)\n=d\ndx(Rx\n\u2212\u221efX,Y(x\u2032, y)dx\u2032\nfY(y))\n(a)=fX,Y(x, y)\nfY(y),\nwhere (a) follows from the fundamental theorem of calculus.\nJust like the conditional PMF, we can calculate the probabilities using the conditional\nPDFs. In particular, if we evaluate the probability where X\u2208Agiven that Ytakes a\nparticular value Y=y, then we can integrate the conditional PDF fX|Y(x|y), with respect\ntox.\nTheorem 5.8. LetXandYbe continuous random variables, and let Abe an event.\n(i)P[X\u2208A|Y=y] =R\nAfX|Y(x|y)dx,\n(ii)P[X\u2208A] =R\n\u2126YP[X\u2208A|Y=y]fY(y)dy.\nExample 5.20 . Let Xbe a random bit such that\nX=(\n+1,with prob 1 /2,\n\u22121,with prob 1 /2.\nSuppose that Xis transmitted over a noisy channel so that the observed signal is\nY=X+N,\nwhere N\u223cGaussian(0 ,1) is the noise, which is independent of the signal X. Find the\nprobabilities P[X= +1|Y >0] and P[X=\u22121|Y >0].\nSolution . First, we know that\nfY|X(y|+ 1) =1\u221a\n2\u03c0e\u2212(y\u22121)2\n2 and fY|X(y| \u22121) =1\u221a\n2\u03c0e\u2212(y+1)2\n2.\n273", "289": "CHAPTER 5. JOINT DISTRIBUTIONS\nTherefore, integrating yfrom 0 to \u221egives us\nP[Y >0|X= +1] =Z\u221e\n01\u221a\n2\u03c0e\u2212(y\u22121)2\n2dy\n= 1\u2212Z0\n\u2212\u221e1\u221a\n2\u03c0e\u2212(y\u22121)2\n2dy\n= 1\u2212\u03a6\u00120\u22121\n1\u0013\n= 1\u2212\u03a6(\u22121).\nSimilarly, we have P[Y >0|X=\u22121] = 1 \u2212\u03a6(+1). The probability we want to find is\nP[X= +1|Y >0], which can be determined using Bayes\u2019 theorem.\nP[X= +1|Y >0] =P[Y >0|X= +1]P[X= +1]\nP[Y >0].\nThe denominator can be found by using the law of total probability:\nP[Y >0] =P[Y >0|X= +1]P[X= +1]\n+P[Y >0|X=\u22121]P[X=\u22121]\n= 1\u22121\n2(\u03a6(+1) + \u03a6( \u22121))\n=1\n2,\nsince \u03a6(+1) + \u03a6( \u22121) = \u03a6(+1) + 1 \u2212\u03a6(+1) = 1. Therefore,\nP[X= +1|Y >0] = 1 \u2212\u03a6(\u22121)\n= 0.8413.\nThe implication is that if Y > 0, the probability P[X= +1|Y > 0] = 0 .8413. The\ncomplement of this result gives P[X=\u22121|Y >0] = 1 \u22120.8413 = 0 .1587.\nPractice Exercise 5.8 . Find P[Y > y ], where\nX\u223cUniform[1 ,2], Y|X\u223cExponential( x).\nSolution . The tricky part of this problem is the tendency to confuse the two variables\nXandY. Once you understand their roles the problem becomes easy. First notice that\nY|X\u223cExponential( x) is a conditional distribution. It says that given X=x, the\nprobability distribution of Yis exponential, with the parameter x. Thus, we have that\nfY|X(y|x) =xe\u2212xy.\nWhy? Recall that if Y\u223cExponential( \u03bb) then fY(y) =\u03bbe\u2212\u03bby. Now if we replace \u03bb\nwith x, we have xe\u2212xy. So the role of xin this conditional density function is as a\nparameter.\n274", "290": "5.4. CONDITIONAL EXPECTATION\nGiven this property, we can compute the conditional probability:\nP[Y > y |X=x] =Z\u221e\nyfY|X(y\u2032|x)dy\u2032\n=Z\u221e\nyxe\u2212xy\u2032dy\u2032=\u0014\n\u2212e\u2212xy\u2032\u0015\u221e\ny\u2032=y=e\u2212xy.\nFinally, we can compute the marginal probability:\nP[Y > y ] =Z\n\u2126XP[Y >0|X=x\u2032]fX(x\u2032)dx\u2032\n=Z1\n0e\u2212x\u2032ydx\u2032\n=\u00141\nye\u2212x\u2032y\u0015x\u2032=1\nx\u2032=0=1\ny\u0000\n1\u2212e\u2212y\u0001\n.\nWe can double-check this result by noting that the problem asks about the probability\nP[Y > y ]. Thus, the answer must be a function of ybut not of x.\n5.4 Conditional Expectation\n5.4.1 Definition\nWhen dealing with two dependent random variables, at times we would like to determine\nthe expectation of a random variable when the second random variable takes a particular\nstate. The conditional expectation is a formal way of doing so.\nDefinition 5.18. Theconditional expectation ofXgiven Y=yis\nE[X|Y=y] =X\nxxpX|Y(x|y) (5.25)\nfor discrete random variables, and\nE[X|Y=y] =Z\u221e\n\u2212\u221exfX|Y(x|y)dx (5.26)\nfor continuous random variables.\nThere are two points to note here. First, the expectation of E[X|Y=y] is taken with respect\ntofX|Y(x|y). We assume that the random variable Yis already fixed at the state Y=y.\nThus, the only source of randomness is X. Secondly, since the expectation E[X|Y=y] has\neliminated the randomness of X, the resulting function is in y.\n275", "291": "CHAPTER 5. JOINT DISTRIBUTIONS\nWhat is conditional expectation?\n\u0088E[X|Y=y] is the expectation using fX|Y(x|y).\n\u0088The integration is taken w.r.t. x, because Y=yis given and fixed.\n5.4.2 The law of total expectation\nTheorem 5.9. The law of total expectation states that\nE[X] =X\nyE[X|Y=y]pY(y),orE[X] =Z\u221e\n\u2212\u221eE[X|Y=y]fY(y)dy. (5.27)\nProof . We will prove the discrete case only, as the continuous case can be proved by replacing\nsummation with integration.\nE[X] =X\nxxpX(x) =X\nxx X\nypX,Y(x, y)!\n=X\nxX\nyxpX|Y(x|y)pY(y)\n=X\ny X\nxxpX|Y(x|y)!\npY(y) =X\nyE[X|Y=y]pY(y).\n\u25a1\nFigure 5.11 illustrates the idea behind the proof. Essentially, we decompose the expec-\ntation E[X] into \u201csubexpectations\u201d E[X|Y=y]. The probability of each subexpectation is\npY(y). By summing the subexpectation multiplied by pY(y), we obtain the overall expecta-\ntion.\nFigure 5.11: The expectation E[X]can be decomposed into a set of subexpectations. This gives us\nE[X] =P\nyE[X|Y=y]pY(y).\n276", "292": "5.4. CONDITIONAL EXPECTATION\nWhat is the law of total expectation?\n\u0088The law of total expectation is a decomposition rule.\n\u0088It decomposes E[X] into smaller/easier conditional expectations.\nThis law can also be written in a more compact form.\nCorollary 5.1. LetXandYbe two random variables. Then\nE[X] =EY\u0002\nEX|Y[X|Y]\u0003\n. (5.28)\nProof . The previous theorem states that E[X] =P\nyE[X|Y=y]pY(y). If we treat E[X|Y=\ny] as a function of y, for instance h(y), then\nE[X] =X\nyE[X|Y=y]pY(y) =X\nyh(y)pY(y) =E[h(Y)] =E[E[X|Y]].\n\u25a1\nExample 5.21 . Suppose there are two classes of cars. Let Xbe the speed of a car\nandCbe the class. When C= 1, we know that X\u223cGaussian( \u00b51, \u03c31). We know that\nP[C= 1] = p. When C= 2, X\u223cGaussian( \u00b52, \u03c32). Also, P[C= 2] = 1 \u2212p. If you see\na car on the freeway, what is its average speed?\nSolution . The problem has given us everything we need. In particular, we know that\nthe conditional PDFs are:\nfX|C(x|1) =1p\n2\u03c0\u03c32\n1exp\u001a\n\u2212(x\u2212\u00b51)2\n2\u03c32\n1\u001b\n,\nfX|C(x|2) =1p\n2\u03c0\u03c32\n2exp\u001a\n\u2212(x\u2212\u00b52)2\n2\u03c32\n2\u001b\n.\nTherefore, conditioned on C, we have two expectations:\nE[X|C= 1] =Z\u221e\n\u2212\u221ex fX|C(x|1)dx=\u00b51,\nE[X|C= 2] =Z\u221e\n\u2212\u221ex fX|C(x|2)dx=\u00b52.\nThe overall expectation E[X] is\nE[X] =2X\nc=1E[X|C=c]pC(c)\n=E[X|C= 1]P[C= 1] + E[X|C= 2]P[C= 2]\n=p\u00b51+ (1\u2212p)\u00b52.\n277", "293": "CHAPTER 5. JOINT DISTRIBUTIONS\nPractice Exercise 5.9 . Consider a joint PMF given by the following table. Find\nE[X|Y= 102] andE[X|Y= 104].\nY1040 01\n121\n181\n36\n102 5\n125\n185\n360 0\n0.01 0.1 1 10 100\nX\nSolution . To find the conditional expectation, we first need to know the conditional\nPMF.\npX|Y(x|102) =\u00021\n21\n31\n60 0\u0003\n,\npX|Y(x|104) =\u0002\n0 01\n21\n31\n6\u0003\n.\nTherefore, the conditional expectations are\nE[X|Y= 102] = (10\u22122)\u00121\n2\u0013\n+ (10\u22121)\u00121\n3\u0013\n+ (1)\u00121\n6\u0013\n=123\n600,\nE[X|Y= 104] = (1)\u00121\n2\u0013\n+ (10)\u00121\n3\u0013\n+ (100)\u00121\n6\u0013\n=123\n6.\nFrom the conditional expectations we can also find E[X]:\nE[X] =E[X|Y= 102]pY(102)\n+E[X|Y= 104]pY(104)\n=\u0012123\n600\u0013\u00125\n6\u0013\n+\u0012123\n6\u0013\u00121\n6\u0013\n= 3.5875.\nExample 5.22 . Consider two random variables XandY. The random variable X\nis Gaussian-distributed with X\u223cGaussian( \u00b5, \u03c32). The random variable Yhas a\nconditional distribution Y|X\u223cGaussian( X, X2). Find E[Y].\nSolution . The notation Y|X\u223cGaussian( X, X2) means that given the variable X,\nthe other variable Yhas a conditional distribution Gaussian( X, X2). That is, the\nvariable Yis a Gaussian with mean Xand variance X2. How can the mean be a\nrandom variable Xand the variance be another random variable X2? Because Xis\nthe conditional variable. Y|Xmeans that you have already chosen one state of X.\nGiven that particular state, the distribution of Yfollows fY|X. Therefore, for this\n278", "294": "5.4. CONDITIONAL EXPECTATION\nproblem, we know the PDFs:\nfX(x) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5)2\n2\u03c32\u001b\n,\nfY|X(y|x) =1\u221a\n2\u03c0x2exp\u001a\n\u2212(y\u2212x)2\n2x2\u001b\n.\nThe conditional expectation of Ygiven Xis\nE[Y|X=x] =Z\u221e\n\u2212\u221ey1\u221a\n2\u03c0x2exp\u001a\n\u2212(y\u2212x)2\n2x2\u001b\ndy\n=E[Gaussian( x, x2)] =x.\nThe last equality holds because we are computing the expectation of a Gaussian ran-\ndom variable with mean x. Finally, applying the law of total expectation, we can show\nthat\nE[Y] =Z\u221e\n\u2212\u221eE[Y|X=x]fX(x)dx\n=Z\u221e\n\u2212\u221ex1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5)2\n2\u03c32\u001b\ndx\n=E[Gaussian( \u00b5, \u03c32)] =\u00b5,\nwhere the last equality is based on the fact that it is the mean of a Gaussian.\nPractice Exercise 5.10 . Find E[sin(X+Y)], if X\u223cGaussian(0 ,1), and Y|X\u223c\nUniform[ x\u2212\u03c0, x+\u03c0].\nSolution . We know that the conditional density is\nfY|X(y|x) =1\n2\u03c0, x \u2212\u03c0\u2264y\u2264x+\u03c0.\nTherefore, we can compute the probability\nE[sin(X+Y)|X=x] =Zx+\u03c0\nx\u2212\u03c0sin(x+y)fY|X(y|x)dy\n=1\n2\u03c0Zx+\u03c0\nx\u2212\u03c0sin(x+y)dy\n| {z }\n=0= 0.\nHence, the overall expectation is\nE[sin(X+Y)] =Z1\n0E[sin(X+Y)|X=x]| {z }\n=01\u221a\n2\u03c0e\u2212x2\n2dx= 0.\n279", "295": "CHAPTER 5. JOINT DISTRIBUTIONS\n5.5 Sum of Two Random Variables\nOne typical problem we encounter in engineering is to determine the PDF of the sum of\ntwo random variables XandY, i.e., X+Y. Such a problem arises naturally when we want\nto evaluate the average of many random variables, e.g., the sample mean of a collection of\ndata points. This section will discuss a general principle for determining the PDF of a sum\nof two random variables.\n5.5.1 Intuition through convolution\nFirst, consider two random variables, XandY, both discrete uniform random variables\nin the range of 0 ,1,2,3. That is, pX(x) =pY(y) = [1 /4,1/4,1/4,1/4]. Since this is such a\nsimple problem we can enumerate all the possible cases of the sum Z=X+Y. The resulting\nprobabilities are shown in the following table.\nZ=X+YCases, written in terms of (X, Y) Probability\n0 (0,0) 1/16\n1 (0,1), (1,0) 2/16\n2 (1,1), (2,0), (0,2) 3/16\n3 (3,0), (2,1), (1,2), (0,3) 4/16\n4 (3,1), (2,2), (1,3) 3/16\n5 (3,2), (2,3) 2/16\n6 (3,3) 1/16\nClearly, the PMF of Zis not fZ(z) =fX(x)+fY(y). (Caution! Do not write this.) The\nPMF of Zlooks like a triangle distribution. How can we get to this triangle distribution\nfrom two uniform distributions? The key is the idea of convolution. Let us start with the\nPMF of X, which is pX(x). Let us also flip pY(y) over the y-axis. As we shift the flipped pY,\nwe multiply and add the PMF values as shown in Figure 5.12 . This gives us\npZ(0) =P[X+Y= 0]\n=P[(X, Y) = (0 ,0)]\n=pX(0)pY(0)\n=1\n16.\nNow, if we shift towards the right by 1, we have\npZ(1) =P[X+Y= 1]\n=P[(X, Y) = (0 ,1)\u222a(0,1)]\n=pX(0)pY(1) + pX(1)pY(0) =2\n16.\nBy continuing our argument, you can see that we will obtain the same PMF as the one\nshown in the table.\n280", "296": "5.5. SUM OF TWO RANDOM VARIABLES\nFigure 5.12: When summing two random variables XandY, we are effectively taking the convolutions\nof the two respective PMF / PDFs.\n5.5.2 Main result\nWe can show that for any arbitrary random variable XandY, the sum Z=X+Yhas a\ndistribution that is the convolution of two individual PDFs.\nTheorem 5.10. LetXandYbe two independent random variables with PDFs fX(x)\nandfY(y)respectively. Let Z=X+Y. The PDF of Zis given by\nfZ(z) = (fX\u2217fY)(z) =Z\u221e\n\u2212\u221efX(z\u2212y)fY(y)dy, (5.29)\nwhere \u201c \u2217\u201d denotes the convolution.\nProof . We begin by analyzing the CDF of Z. The CDF of Zis\nFZ(z) =P[Z\u2264z] =P[X+Y\u2264z].\nWe now draw a picture to illustrate the line under which we want to integrate. As shown in\nFigure 5.13 , the equation X+Y\u2264zdefines a straight line in the xyplane. You can think\nof it as Y\u2264 \u2212X+z, so that the slope is \u22121 and the y-intercept is z.\nNow, shall we take the upper half of the triangle or the lower half? Since the equation\nisY\u2264 \u2212X+z, a value of Yhas to be less than that of the line. Another easy way to check\nis to assume z >0 so that we have a positive y-intercept. Then we check where the origin\n281", "297": "CHAPTER 5. JOINT DISTRIBUTIONS\nFigure 5.13: The shaded region highlights the set X+Y\u2264Z. To integrate the PDF over this region,\nwe first take the inner integration over dxand then take the outer integration over dy.\n(0,0) belongs. In this case, if z >0, the origin (0 ,0) will satisfy the equation Y\u2264 \u2212X+z,\nand so it must be included. Thus, we conclude that the area is below the line.\nOnce we have determined the area to be integrated, we can write down the integration:\nP[X+Y\u2264z] =Z\u221e\n\u2212\u221eZz\u2212y\n\u2212\u221efX,Y(x, y)dx dy\n=Z\u221e\n\u2212\u221eZz\u2212y\n\u2212\u221efX(x)fY(y)dx dy, (independence)\nwhere the integration limits are just a rewrite of X+Y\u2264z(in this case since we are\nintegrating xfirst we have X\u2264 \u2212Y+z). Then, by the fundamental theorem of calculus,\nwe can show that\nfZ(z) =d\ndzFZ(z) =d\ndzZ\u221e\n\u2212\u221eZz\u2212y\n\u2212\u221efX(x)fY(y)dx dy\n=Z\u221e\n\u2212\u221e\u0012d\ndzZz\u2212y\n\u2212\u221efX(x)fY(y)dx\u0013\ndy\n=Z\u221e\n\u2212\u221efX(z\u2212y)fY(y)dy= (fX\u2217fY)(z),\nwhere \u201c \u2217\u201d denotes the convolution.\nHow is convolution related to random variables?\n\u0088If you sum XandY, the resulting PDF is the convolution of fXandfY.\n\u0088E.g., convolving two uniform random variables gives you a triangle PDF.\n5.5.3 Sum of common distributions\nTheorem 5.11 (Sum of two Poissons) .LetX1\u223cPoisson (\u03bb1)andX2\u223cPoisson (\u03bb2).\nThen\nX1+X2\u223cPoisson (\u03bb1+\u03bb2). (5.30)\n282", "298": "5.5. SUM OF TWO RANDOM VARIABLES\nProof . Let us apply the convolution principle.\npY(k) =P[X1+X2=k]\n=P[X1=\u2113\u2229X2=k\u2212\u2113]\n=kX\n\u2113=0\u03bb\u2113\n1e\u2212\u03bb1\n\u2113!\u00b7\u03bbk\u2212\u2113\n2e\u2212\u03bb2\n(k\u2212\u2113)!\n=e\u2212(\u03bb1+\u03bb2)kX\n\u2113=0\u03bb\u2113\n1\n\u2113!\u00b7\u03bbk\u2212\u2113\n2\n(k\u2212\u2113)!\n=e\u2212(\u03bb1+\u03bb2)\u00b71\nk!kX\n\u2113=0k!\n\u2113!(k\u2212\u2113)!\u03bb\u2113\n1\u03bbk\u2212\u2113\n2\n| {z }\n=Pk\n\u2113=0(k\n\u2113)\u03bb\u2113\n1\u03bbk\u2212\u2113\n2\n=(\u03bb1+\u03bb2)k\nk!e\u2212(\u03bb1+\u03bb2),\nwhere the last step is based on the binomial identityPk\n\u2113=0\u0000k\n\u2113\u0001\na\u2113bk\u2212\u2113= (a+b)k.\n\u25a1\nTheorem 5.12 (Sum of two Gaussians ).LetX1andX2be two Gaussian random\nvariables such that\nX1\u223cGaussian (\u00b51, \u03c32\n1)and X 2\u223cGaussian (\u00b52, \u03c32\n2).\nThen\nX1+X2\u223cGaussian (\u00b51+\u00b52, \u03c32\n1+\u03c32\n2). (5.31)\nProof . Let us apply the convolution principle.\nfZ(z) =Z\u221e\n\u2212\u221efX(t)fY(z\u2212t)dt\n=Z\u221e\n\u2212\u221e1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(t\u2212\u00b51)2\n2\u03c32\u001b\n\u00b71\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(z\u2212t\u2212\u00b52)2\n2\u03c32\u001b\ndt\n=1\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221e1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(t\u2212\u00b51)2+ (z\u2212t\u2212\u00b52)2\n2\u03c32\u001b\ndt.\nWe now complete the square:\n(t\u2212\u00b51)2+ (z\u2212t\u2212\u00b52)2= [t2\u22122\u00b51t+\u00b52\n1] + [t2+ 2t(\u00b52\u2212z) + (\u00b52\u2212z)2]\n= 2t2\u22122t(\u00b51\u2212\u00b52+z) +\u00b52\n1+ (\u00b52\u2212z)2\n= 2\u0014\nt2\u22122t\u00b7\u00b51\u2212\u00b52+z\n2\u0015\n+\u00b52\n1+ (\u00b52\u2212z)2\n= 2\u0014\nt\u2212\u00b51\u2212\u00b52+z\n2\u00152\n\u22122\u0014\u00b51\u2212\u00b52+z\n2\u00152\n+\u00b52\n1+ (\u00b52\u2212z)2.\n283", "299": "CHAPTER 5. JOINT DISTRIBUTIONS\nThe last term can be simplified to\n\u22122\u0014\u00b51\u2212\u00b52+z\n2\u00152\n+\u00b52\n1+ (\u00b52\u2212z)2\n=\u2212\u00b52\n1\u22122\u00b51(\u00b52\u2212z) + (\u00b52\u2212z)2\n2+\u00b52\n1+ (\u00b52\u2212z)2\n=\u00b52\n1+ 2\u00b51(\u00b52\u2212z) + (\u00b52\u2212z)2\n2=(\u00b51+\u00b52\u2212z)2\n2.\nSubstituting these into the integral, we can show that\nfZ(z) =1\u221a\n2\u03c0\u03c32Z\u221e\n\u2212\u221e1\u221a\n2\u03c0\u03c32exp(\n\u22122\u0002\nt\u2212\u00b51\u2212\u00b52+z\n2\u00032+(\u00b51+\u00b52\u2212z)2\n2\n2\u03c32)\ndt\n=1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(\u00b51+\u00b52\u2212z)2\n2(2\u03c32)\u001bZ\u221e\n\u2212\u221e1\u221a\n2\u03c0\u03c32exp(\n\u2212\u0002\nt\u2212\u00b51\u2212\u00b52+z\n2\u00032\n\u03c32)\ndt\n| {z }\n=1\u221a\n2\n=1p\n2\u03c0(2\u03c3)2exp\u001a\n\u2212(\u00b51+\u00b52\u2212z)2\n2(2\u03c32)\u001b\n.\nTherefore, we have shown that the resulting distribution is a Gaussian with mean \u00b51+\u00b52\nand variance 2 \u03c32. \u25a1\nPractice Exercise 5.11 . Let XandYbe independent, and let\nfX(x) =(\nxe\u2212x, x \u22650,\n0, x < 0,and fY(y) =(\nye\u2212y, y \u22650,\n0, y < 0.\nFind the PDF of Z=X+Y.\nSolution . Using the results derived above, we see that\nfZ(z) =Z\u221e\n\u2212\u221efX(z\u2212y)fY(y)dy\n=Zz\n\u2212\u221efX(z\u2212y)fY(y)dy,\nwhere the upper limit zcame from the fact that x\u22650. Therefore, since Z=X+Y, we\nmust have Z\u2212Y=X\u22650 and so Z\u2265Y. This is portrayed graphically in Figure 5.14 .\nSubstituting the PDFs into the integration yields\nfZ(z) =Zz\n0(z\u2212y)e\u2212(z\u2212y)ye\u2212ydy=z3\n6e\u2212z, z\u22650.\nForz <0,fZ(z) = 0.\nThe functions of two random variables are not limited to summation. The following\nexample illustrates the case of the product of two random variables.\n284", "300": "5.5. SUM OF TWO RANDOM VARIABLES\nFigure 5.14: [Left] The outer integral goes from 0 to zbecause the triangle stops at y=z. [Right] If\nthe triangle is unbounded, then the integral goes from \u2212\u221e to\u221e.\nExample 5.23 . Let XandYbe two independent random variables such that\nfX(x) =(\n2x, if 0\u2264x\u22641,\n0, otherwise ,and fY(y) =(\n1, if 0\u2264y\u22641,\n0, otherwise .\nLetZ=XY. Find fZ(z).\nSolution . The CDF of Zcan be evaluated as\nFZ(z) =P[Z\u2264z] =P[XY\u2264z] =Z\u221e\n\u2212\u221eZz\ny\n\u2212\u221efX(x)fY(y)dx dy.\nTaking the derivative yields\nfZ(z) =d\ndzFZ(z) =d\ndzZ\u221e\n\u2212\u221eZz\ny\n\u2212\u221efX(x)fY(y)dx dy\n(a)=Z\u221e\n\u2212\u221e1\nyfX\u0012z\ny\u0013\nfY(y)dy,\nwhere (a) holds by the fundamental theorem of calculus. The upper and lower limit of\nthis integration can be determined by noting that\n0\u2264z\ny=x\u22641,\nwhich implies that z\u2264y. Since y\u22641, we have that z\u2264y\u22641. Therefore, the PDF is\nfZ(z) =Z1\nz1\nyfX\u0012z\ny\u0013\nfY(y)dy\n=Z1\nz2z\ny2dy= 2(1 \u2212z), z\u22650.\nForz <0,fZ(z) = 0.\n285", "301": "CHAPTER 5. JOINT DISTRIBUTIONS\nClosing remark . For some random variables, summing two i.i.d. copies remain the same\nrandom variable (but with different parameters). For other random variables, summing\ntwo i.i.d. copies gives a different random variable. Table 5.1 summarizes some of the most\ncommonly used random variable pairs.\nX1 X2 Sum X1+X2\nBernoulli( p) Bernoulli( p) Binomial(2 , p)\nBinomial( n, p) Binomial( m, p) Binomial( m+n, p)\nPoisson( \u03bb1) Poisson( \u03bb2) Poisson( \u03bb1+\u03bb2)\nExponential( \u03bb) Exponential( \u03bb) Erlang(2 , \u03bb)\nGaussian( \u00b51, \u03c32\n1) Gaussian( \u00b52, \u03c32\n2) Gaussian( \u00b51+\u00b52, \u03c32\n1+\u03c32\n2)\nTable 5.1: Common distributions of the sum of two random variables.\n5.6 Random Vectors and Covariance Matrices\nWe now enter the second part of this chapter. In the first part, we were mainly interested\nin a pair of random variables. In the second part, however, we will study vectors of N\nrandom variables. To understand a vector of random variables, we will not drill down to\nthe integrations of the PDFs (which you would certainly not enjoy). Instead, we will blend\nlinear algebra tools and probabilistic tools to learn a few practical data analysis techniques.\n5.6.1 PDF of random vectors\nJoint distributions can be generalized to more than two random variables. The most conve-\nnient way is to consider a vector of random variables and their corresponding states.\nX=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0X1\nX2\n...\nXN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fband x=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0x1\nx2\n...\nxN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nOur notation here is unconventional since bold upper case letters usually represent matrices.\nHere, Xdenotes a vector, specifically a random vector. Its state is a vector x. In this chapter,\nwe will use the following notational convention: XandYrepresent random vectors while\nArepresents a matrix.\nOne way to think about Xis to imagine that if you put your hand into the sample\nspace, you will pick up a vector x. This random realization xhasNentries, and so you\nneed to specify the probability of getting all these entries simultaneously . Accordingly, we\nshould expect that Xis characterized by an N-dimensional PDF\nfX(x) =fX1,X2,...,X N(x1, x2, . . . , x N).\n286", "302": "5.6. RANDOM VECTORS AND COVARIANCE MATRICES\nEssentially, this PDF tells us the probability density for random variable X1=x1, random\nvariable X2=x2, etc. It is a coordinate-wise description. For example, if Xcontains three\nelements such that X= [X1, X2, X3]T, and if the state we are looking at is x= [3,1,7]T,\nthen fX(x) is the probability density such that this 3D coordinate ( X1, X2, X3) takes the\nvalue [3 ,1,7]T.\nTo compute the probability, we can integrate fX(x) with respect to x. LetAbe the\nevent. Then\nP[X\u2208 A] =Z\nAfX(x)dx\n=Z\n\u00b7\u00b7\u00b7Z\nAfX1,...,X N(x1, . . . , x N)dx1. . . dx N.\nIf the random coordinates X1, . . . , X Nareindependent , the PDF can be written as a prod-\nuct of Nindividual PDFs:\nfX1,...,X N(x1, . . . , x N) =fX1(x1)fX2(x2)\u00b7\u00b7\u00b7fXN(xN),and so\nP[X\u2208 A] =Z\n\u00b7\u00b7\u00b7Z\nAfX1(x1)fX2(x2)\u00b7\u00b7\u00b7fXN(xN)dx1\u00b7\u00b7\u00b7dxN.\nHowever, this does not necessarily simplify the calculation unless Ais separable, e.g., A=\n[a1, b1]\u00d7[a2, b2]\u00d7 \u00b7\u00b7\u00b7 \u00d7 [aN, bN]. In this case the integration becomes\nP[X\u2208 A] =NY\ni=1\"Zbi\naifXi(xi)dxi#\n,\nwhich is obviously manageable.\nExample 5.24 . LetX= [X1, . . . , X N]Tbe a vector of zero-mean unit variance Gaus-\nsian random vectors. Let A= [\u22121,2]N. Then\nP[X\u2208 A] =Z\nAfX(x)dx\n=Z\n\u00b7\u00b7\u00b7Z\nAfX1,\u00b7\u00b7\u00b7,XN(x1, . . . , x N)dx1\u00b7\u00b7\u00b7dxN\n=\u0014Z2\n\u22121fX1(x1)dx1\u0015N\n= [\u03a6(2) \u2212\u03a6(\u22121)]N,\nwhere \u03a6( \u00b7) is the standard Gaussian CDF.\nAs you can see from the definition of a vector random variable, computing the proba-\nbility typically involves integrating a high-dimensional function, which is tedious. However,\nthe good news is that in practice we seldom need to perform such calculations. Often we are\nmore interested in the mean and the covariance of the random vectors because they usually\ncarry geometric meanings. The next subsection explores this topic.\n287", "303": "CHAPTER 5. JOINT DISTRIBUTIONS\n5.6.2 Expectation of random vectors\nLetX= [X1, . . . , X N]Tbe a random vector. We define the expectation of a random vector\nas follows.\nDefinition 5.19. LetX= [X1, . . . , X N]Tbe a random vector. The expectation is\n\u00b5def=E[X] =\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0E[X1]\nE[X2]\n...\nE[XN]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb. (5.32)\nThe resulting vector is called the mean vector . Since the mean vector is a vector of\nindividual elements, we need to compute the marginal PDFs before computing the expec-\ntations:\nE[X] =\uf8ee\n\uf8ef\uf8f0E[X1]\n...\nE[XN]\uf8f9\n\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8f0R\n\u2126x1fX1(x1)dx1\n...R\n\u2126xNfXN(xN)dxN\uf8f9\n\uf8fa\uf8fb,\nwhere the marginal PDF is determined by\nfXn(xn) =Z\n\u2126fX\\n(x\\n)dx\\n.\nIn the equation above, x\\n= [x1, . . . , x n\u22121, xn+1, . . . , x N]Tcontains all the elements with-\noutxn. For example, if the PDF is fX1,X2,X3(x1, x2, x3), then\nE[X1] =Z\nx1Z\nfX1,X2,X3(x1, x2, x3)dx2dx3\n| {z }\nfX1(x1)dx1.\nAgain, this will become tedious when there are many variables.\nWhile the definition of the expectation may be challenging to understand, some prob-\nlems using it are straightforward. We will first demonstrate the case of independent Poisson\nrandom variables, and then we will discuss joint Gaussians.\nExample 5.25 . LetX= [X1, . . . , X N]Tbe a random vector such that Xnare inde-\npendent Poissons with Xn\u223cPoisson( \u03bbn). Then\nE[X] =\uf8ee\n\uf8ef\uf8f0E[X1]\n...\nE[XN]\uf8f9\n\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8f0P\u221e\nk=0k\u00b7\u03bbk\n1e\u2212\u03bb1\nk!...P\u221e\nk=0k\u00b7\u03bbk\nNe\u2212\u03bbN\nk!\uf8f9\n\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8f0\u03bb1\n...\n\u03bbN\uf8f9\n\uf8fa\uf8fb.\nOn computers, computing the mean vector can be done using built-in commands such\nasmean in MATLAB and np.mean in Python. However, caution is needed when performing\nthe calculation. In MATLAB, mean computes along first dimension (rows index). Thus, if we\n288", "304": "5.6. RANDOM VECTORS AND COVARIANCE MATRICES\nhave an N\u00d72 array, applying mean will give us a 1 \u00d72 vector. To obtain the column mean\nvector of size N\u00d71, we need to specify the direction as mean(X,2) . Similarly, in Python,\nwhen calling np.mean , we need to specify the axis.\n% MATLAB code to compute a mean vector\nX = randn(100,2);\nmX = mean(X,2);\n# Python code to compute a mean vector\nimport numpy as np\nimport scipy.stats as stats\nX = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100)\nmX = np.mean(X,axis=1)\n5.6.3 Covariance matrix\nDefinition 5.20. Thecovariance matrix of a random vector X= [X1, . . . , X N]Tis\n\u03a3def= Cov( X) =\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0Var[X1] Cov( X1, X2)\u00b7\u00b7\u00b7 Cov(X1, XN)\nCov[X2, X1] Var[ X2]\u00b7\u00b7\u00b7 Cov(X2, XN)\n............\nCov(XN, X1) Cov( XN, X2)\u00b7\u00b7\u00b7 Var[XN]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb. (5.33)\nA more compact way of writing the covariance matrix is\n\u03a3= Cov( X) =E[(X\u2212\u00b5)(X\u2212\u00b5)T],\nwhere \u00b5=E[X] is the mean vector. The notation abTmeans the outer product , defined\nas\nabT=\uf8ee\n\uf8ef\uf8f0a1\n...\naN\uf8f9\n\uf8fa\uf8fb\u0002b1\u00b7\u00b7\u00b7bN\u0003\n=\uf8ee\n\uf8ef\uf8f0a1b1a1b2\u00b7\u00b7\u00b7 a1bN\n............\naNb1aNb2\u00b7\u00b7\u00b7aNbN\uf8f9\n\uf8fa\uf8fb.\nIt is easy to show that Cov( X) = Cov( X)T, i.e., they are symmetric.\nTheorem 5.13. If the coordinates X1, . . . , X Nare independent, then the covariance\nmatrix Cov(X) =\u03a3is a diagonal matrix:\n\u03a3= Cov( X) =\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0Var[X1] 0 \u00b7\u00b7\u00b7 0\n0 Var[ X2]\u00b7\u00b7\u00b7 0\n............\n0 0 \u00b7\u00b7\u00b7 Var[XN]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\n289", "305": "CHAPTER 5. JOINT DISTRIBUTIONS\nProof . If all Xi\u2019s are independent, then Cov( Xi, Xj) = 0 for all i\u0338=j. Substituting this\ninto the definition of the covariance matrix, we obtain the result.\n\u25a1\nIf we ignore the mean vector \u00b5, we obtain the autocorrelation matrix R.\nDefinition 5.21. LetX= [X1, . . . , X N]Tbe a random vector. The autocorrelation\nmatrix is\nR=E[XXT] =\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0E[X1X1]E[X1X2]\u00b7\u00b7\u00b7E[X1XN]\nE[X2X1]E[X2X2]\u00b7\u00b7\u00b7E[X2XN]\n............\nE[XNX1]E[XNX2]\u00b7\u00b7\u00b7E[XNXN]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb. (5.34)\nWe state without proof that\n\u03a3=R\u2212\u00b5\u00b5T,\nwhich corresponds to the single-variable case where \u03c32=E[X2]\u2212\u00b52.\nOn computers, computing the covariance matrix is done using built-in commands cov\nin MATLAB and np.cov in Python. Like the mean vectors, when computing the covariance,\nwe need to specify the direction. For example, for an N\u00d72 data matrix X, the covariance\nneeds to be a 2 \u00d72 matrix. If we compute the covariance along the wrong direction, we will\nobtain an N\u00d7Nmatrix, which is incorrect.\n% MATLAB code to compute covariance matrix\nX = randn(100,2);\ncovX = cov(X);\n# Python code to compute covariance matrix\nimport numpy as np\nimport scipy.stats as stats\nX = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100)\ncovX = np.cov(X,rowvar=False)\nprint(covX)\n5.6.4 Multidimensional Gaussian\nWith the above tools in hand, we can now define a high-dimensional Gaussian. The PDF of\na high-dimensional Gaussian is defined as follows.\nDefinition 5.22. Ad-dimensional joint Gaussian has the PDF\nfX(x) =1p\n(2\u03c0)d|\u03a3|exp\u001a\n\u22121\n2(x\u2212\u00b5)T\u03a3\u22121(x\u2212\u00b5)\u001b\n, (5.35)\nwhere ddenotes the dimensionality of the vector x.\n290", "306": "5.6. RANDOM VECTORS AND COVARIANCE MATRICES\nThe mean vector and the covariance matrix of a joint Gaussian is readily available from the\ndefinition.\nE[X] =\u00b5 and Cov( X) =\u03a3.\nIt is easy to show that if Xis a scalar X, then d= 1,\u00b5=\u00b5, and \u03a3=\u03c32. Substituting\nthese into the above definition returns us the familiar 1D Gaussian.\nThed-dimensional Gaussian is a generalization of the 1D Gaussian(s). Suppose that Xi\nandXjareindependent for all i\u0338=j. Then E[XiXj] =E[Xi]E[Xj] and hence Cov( Xi, Xj) =\n0. Consequently, the covariance matrix \u03a3is a diagonal matrix:\n\u03a3=\uf8ee\n\uf8ef\uf8f0\u03c32\n1\u00b7\u00b7\u00b7 0\n.........\n0\u00b7\u00b7\u00b7\u03c32\nd\uf8f9\n\uf8fa\uf8fb,\nwhere \u03c32\ni= Var[ Xi]. When this occurs, the exponential term in the Gaussian PDF is\n(x\u2212\u00b5)T\u03a3\u22121(x\u2212\u00b5) =\uf8ee\n\uf8ef\uf8f0x1\u2212\u00b51\n...\nxd\u2212\u00b5d\uf8f9\n\uf8fa\uf8fbT\uf8ee\n\uf8ef\uf8f0\u03c32\n1\u00b7\u00b7\u00b7 0\n.........\n0\u00b7\u00b7\u00b7\u03c32\nd\uf8f9\n\uf8fa\uf8fb\u22121\uf8ee\n\uf8ef\uf8f0x1\u2212\u00b51\n...\nxd\u2212\u00b5d\uf8f9\n\uf8fa\uf8fb=dX\ni=1(xi\u2212\u00b5i)2\n\u03c32\ni.\nMoreover, the determinant |\u03a3|is\n|\u03a3|=\f\f\f\f\f\f\f\uf8ee\n\uf8ef\uf8f0\u03c32\n1\u00b7\u00b7\u00b7 0\n.........\n0\u00b7\u00b7\u00b7\u03c32\nd\uf8f9\n\uf8fa\uf8fb\f\f\f\f\f\f\f=dY\ni=1\u03c32\ni.\nSubstituting these results into the joint Gaussian PDF, we obtain\nfX(x) =nY\ni=11p\n(2\u03c0)\u03c32\niexp\u001a\n\u2212(x\u2212\u00b5i)2\n2\u03c32\ni\u001b\n,\nwhich is a product of individual Gaussians.\nThe Gaussian has different offsets and orientations for different choices of \u00b5and\u03a3.\nFigure 5.15 shows a few examples. Note that for \u03a3to be valid \u03a3has to be \u201csymmetric\npositive semi-definite\u201d, the meaning of which will be explained shortly.\nGenerating random numbers from a multidimensional Gaussian can be done by calling\nbuilt-in commands. In MATLAB, we use mvnrnd . In Python, we have a similar command.\n% MATLAB code to generate random numbers from multivariate Gaussian\nmu = [0 0];\nSigma = [.25 .3; .3 1];\nX = mvnrnd(mu,Sigma,100);\n# Python code to generate random numbers from multivariate Gaussian\nimport numpy as np\nimport scipy.stats as stats\nX = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],100)\n291", "307": "CHAPTER 5. JOINT DISTRIBUTIONS\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n(\u00b5,\u03a3) =\u00140\n2\u0015\n,\u00145 0\n0 0.5\u0015\n(\u00b5,\u03a3) =\u00141\n2\u0015\n,\u00141\u22120.5\n\u22120.5 1\u0015\n(\u00b5,\u03a3) =\u00140\n0\u0015\n,\u00142 1 .9\n1.9 2\u0015\nFigure 5.15: Visualization of 2D Gaussians with different means and covariances.\nTo display the data points and overlay with the contour, we can use MATLAB com-\nmands such as contour . The resulting plot looks like the one shown in Figure 5.16 . In\nPython the corresponding command is plt.contour . To set up the plotting environment\nwe use the commands np.meshgrid . The grid points are used to evaluate the PDF values,\nthus giving us the contour.\n% MATLAB code: Overlay random numbers with the Gaussian contour.\nX = mvnrnd([0 0],[.25 .3; .3 1],1000);\nx1 = -2.5:.01:2.5;\nx2 = -3.5:.01:3.5;\n[X1,X2] = meshgrid(x1,x2);\nF = mvnpdf([X1(:) X2(:)],[0 0],[.25 .3; .3 1]);\nF = reshape(F,length(x2),length(x1));\nfigure(1);\nscatter(x(:,1),x(:,2),\u2019rx\u2019, \u2019LineWidth\u2019, 1.5); hold on;\ncontour(x1,x2,F,[.001 .01 .05:.1:.95 .99 .999], \u2019LineWidth\u2019, 2);\n# Python code: Overlay random numbers with the Gaussian contour.\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nX = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],1000)\nx1 = np.arange(-2.5, 2.5, 0.01)\nx2 = np.arange(-3.5, 3.5, 0.01)\nX1, X2 = np.meshgrid(x1,x2)\nXpos = np.empty(X1.shape + (2,))\nXpos[:,:,0] = X1\nXpos[:,:,1] = X2\nF = stats.multivariate_normal.pdf(Xpos,[0,0],[[0.25,0.3],[0.3,1.0]])\nplt.scatter(X[:,0],X[:,1])\nplt.contour(x1,x2,F)\n292", "308": "5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS\n-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5\nx-3-2-10123y\nFigure 5.16: 1000 random numbers drawn from a 2D Gaussian, overlaid with the contour plot.\n5.7 Transformation of Multidimensional Gaussians\nAs we have seen in Figure 5.15 , the shape and orientation of a multidimensional Gaussian\nare determined by the mean vector \u00b5and the covariance matrix \u03a3. This means that if we\ncan somehow transform the mean vector and the covariance matrix, we will get another\nGaussian. A few practical questions are:\n\u0088How do we shift and rotate a Gaussian random variable?\n\u0088If we have an arbitrary Gaussian, how do we go back to zero-mean unit-variance\nGaussian?\n\u0088How do we generate random vectors according to a predefined Gaussian?\nThese questions come up frequently in data analysis. Answering the first two questions will\nhelp us transform Gaussians back and forth, while answering the last question will help us\nwith generating random samples.\n5.7.1 Linear transformation of mean and covariance\nSuppose we have an arbitrary (not necessarily a Gaussian) random vector X= [X1, . . . , X N]T\nwith mean \u00b5Xand covariance \u03a3X. Entries of Xare not necessarily independent. Let\nA\u2208RN\u00d7Nbe a transformation, and let Y=AX. That is,\nY=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0Y1\nY2\n...\nYN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0a11a12\u00b7\u00b7\u00b7 a1N\na21a22\u00b7\u00b7\u00b7 a2N\n............\naN1aN2\u00b7\u00b7\u00b7aNN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0X1\nX2\n...\nXN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=AX.\nThen we can show the following result.\n293", "309": "CHAPTER 5. JOINT DISTRIBUTIONS\nTheorem 5.14. The mean vector and covariance matrix of Y=AX are\n\u00b5Y=A\u00b5X, \u03a3Y=A\u03a3XAT. (5.36)\nProof . We first show the mean. Consider the nth element of Y:\nE[Yn] =E\"NX\nk=1ankXk#\n=NX\nk=1ankE[Xk].\nTherefore,\n\u00b5Y=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0E[Y1]\nE[Y2]\n...\nE[YN]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0PN\nk=1a1kE[Xk]PN\nk=1a2kE[Xk]\n...PN\nk=1aNkE[Xk]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0a11a12\u00b7\u00b7\u00b7 a1N\na21a22\u00b7\u00b7\u00b7 a2N\n............\naN1aN2\u00b7\u00b7\u00b7aNN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0E[X1]\nE[X2]\n...\nE[XN]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=A\u00b5X.\nThe covariance matrix follows from the fact that\n\u03a3Y=E[(Y\u2212\u00b5Y)(Y\u2212\u00b5Y)T]\n=E[(AX\u2212A\u00b5X)(AX\u2212A\u00b5X)T]\n=E[A(X\u2212\u00b5X)(X\u2212\u00b5X)TAT]\n=AE[(X\u2212\u00b5X)(X\u2212\u00b5X)T]AT\n=A\u03a3XAT.\n\u25a1\nWhat if we shift the random vector by defining Y=X+b? We state the following\nresult without proof (try proving it as an exercise).\nTheorem 5.15. The mean vector and covariance matrix of Y=X+bare\n\u00b5Y=\u00b5X+b, \u03a3Y=\u03a3X. (5.37)\nFor a Gaussian random vector, the linear transformations either shifts the Gaussian or\nrotates the Gaussian, as shown in Figure 5.17 :\n\u0088If we add btoX, the resulting operation is a translation.\n\u0088If we multiply AbyX, then the resulting operation is a rotation and scaling.\n294", "310": "5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS\nFigure 5.17: Transforming a Gaussian. [Left] Translation by a vector b. [Right] Rotation and scaling by\na matrix X.\nHow to rotate, scale, and translate a Gaussian random variable\n\u0088We rotate and scale a Gaussian by Y=AX.\n\u0088We translate a Gaussian by Y=X+b.\n5.7.2 Eigenvalues and eigenvectors\nAs our next step, we need to understand eigendecomposition . You can easily find relevant\nbackground in any undergraduate linear algebra textbook. Here we provide a summary for\ncompleteness.\nWhen applying a matrix Ato a vector x, a typical engineering question is: what x\nwould be invariant to A? Or in other words, for what xcan we make sure that Ax=\u03bbx,\nfor some scalar \u03bb? If we can find such a vector x, we say that xis the eigenvector ofA.\nEigenvectors are useful for seeking principal components of datasets or finding efficient signal\nrepresentations. They are defined as follows:\nDefinition 5.23. Given a square matrix A\u2208RN\u00d7N, the vector u\u2208RN(with u\u0338=0)\nis called the eigenvector ofAif\nAu=\u03bbu, (5.38)\nfor some \u03bb\u2208R. The scalar \u03bbis called the eigenvalue associated with u.\nAnN\u00d7Nmatrix has Neigenvectors and Neigenvalues. Therefore, the above equation can\nbe generalized to\nAui=\u03bbiui,\nfori= 1, . . . , N , or more compactly as AU = \u039bU. The eigenvalues \u03bb1, . . . , \u03bb Nare not\nnecessarily distinct. There are matrices with identical eigenvalues, the identity matrix being\na trivial example. On the other hand, not all square matrices have eigenvectors. For example,\nthe matrix\u00140 1\n0 0\u0015\ndoes not have an eigenvalue. Matrices that have eigenvalues must be\ndiagonalizable .\n295", "311": "CHAPTER 5. JOINT DISTRIBUTIONS\nThere are a number of equivalent conditions for \u03bbto be an eigenvalue:\n\u2022 There exists u\u0338= 0 such that Au=\u03bbu;\n\u2022 There exists u\u0338= 0 such that ( A\u2212\u03bbI)u=0;\n\u2022 (A\u2212\u03bbI) is not invertible;\n\u2022 det(A\u2212\u03bbI) = 0.\nWe are mostly interested in symmetric matrices. If Ais symmetric, then all the eigen-\nvalues are real, and the following result holds.\nTheorem 5.16. IfAis symmetric, all the eigenvalues are real, and there exists U\nsuch that UTU=IandA=U\u039bUT. Then\n\uf8ee\n\uf8f0| | |\na1a2\u00b7\u00b7\u00b7aN\n| | |\uf8f9\n\uf8fb\n| {z }\nA=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0| | |\nu1u2\u00b7\u00b7\u00b7uN\n| | |\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\nU\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03bb1\n\u03bb2\n...\n\u03bbN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\n\u039b\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u2014uT\n1\u2014\n\u2014uT\n2\u2014\n...\n\u2014uT\nN \u2014\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\nUT.\n(5.39)\nWe call such a decomposition the eigendecomposition . In MATLAB, we can compute the\neigenvalues of a matrix by using the eigcommand. In Python, the corresponding command\nisnp.linalg.eig . Note that in our demonstration below we symmetrize the matrix. This\nstep is needed, for otherwise the eigenvalues will contain complex numbers.\n% MATLAB Code to perform eigendecomposition\nA = randn(100,100);\nA = (A + A\u2019)/2; % symmetrize because A is not symmetric\n[U,S] = eig(A); % eigendecomposition\ns = diag(S); % extract eigenvalue\n# Python Code to perform eigendecomposition\nimport numpy as np\nA = np.random.randn(100,100)\nA = (A + np.transpose(A))/2\nS, U = np.linalg.eig(A)\ns = np.diag(S)\nThe condition that UTU=Iis the result of an orthonormal matrix. Equivalently,\nuT\niuj= 1 if i=janduT\niuj= 0 if i\u0338=j. Since {ui}N\ni=1is orthonormal, it can serve as a\nbasis of any vector in Rn:\nx=NX\nj=1\u03b1juj,\nwhere \u03b1j=uT\njxis called the basis coefficient . Basis vectors are useful in that they can\nprovide alternative representations of a vector.\n296", "312": "5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS\nFigure 5.18: The center and the radius of the ellipse is determined by \u00b5and\u03a3.\nThe geometry of the joint Gaussian is determined by its eigenvalues and eigenvectors.\nConsider the eigendecomposition of \u03a3:\n\u03a3=U\u039bUT\n=\uf8ee\n\uf8f0| | |\nu1u2\u00b7\u00b7\u00b7ud\n| | |\uf8f9\n\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03bb10\u00b7\u00b7\u00b7 0\n0\u03bb2\u00b7\u00b7\u00b7 0\n............\n0\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u03bbd\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u2212uT\n1\u2212\n\u2212uT\n2\u2212\n...\n\u2212uT\nd\u2212\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nfor some unitary matrix Uand diagonal matrix \u039b. The columns of Uare called the eigen-\nvectors, and the entries of \u039bare called the eigenvalues. Since \u03a3is symmetric, all \u03bbi\u2019s are\nreal. In addition, since \u03a3is positive semi-definite, all \u03bbi\u2019s are non-negative. Accordingly, the\nvolume defined by the multidimensional Gaussian is always a convex object, e.g., an ellipse\nin 2D or an ellipsoid in 3D.\nThe orientation of the axes is defined by the column vectors ui. In the case of d= 2,\nthe major axis is defined by u1and the minor axis is defined by u2. The corresponding radii\nof each axis are specified by the eigenvalues \u03bb1and\u03bb2.Figure 5.18 provides an illustration.\n5.7.3 Covariance matrices are always positive semi-definite\nThe following subsection about positive semi-definite matrices can be skipped if it is your\nfirst time reading the book.\nNow that we understand eigendecomposition, what can we do with it? Here is one practical\nproblem. Given a matrix \u03a3, how do you know whether this \u03a3is valid? For example, if we\ngive you a singular matrix, then \u03a3\u22121may not exist. Checking the validity of \u03a3requires the\nconcept of positive semi-definite .\nGiven a square matrix A\u2208RN\u00d7N, it is important to check the positive semi-definiteness\nofA. There are two practical scenarios where we need positive semi-definiteness. (1) If\nwe are estimating the covariance matrix \u03a3from a dataset, we need to ensure that \u03a3=\nE[(X\u2212\u00b5)(X\u2212\u00b5)T] is positive semi-definite because all covariance matrices are positive\n297", "313": "CHAPTER 5. JOINT DISTRIBUTIONS\nsemi-definite. Otherwise, the matrix we estimate is not a legitimate covariance matrix. (2)\nIf we solve an optimization problem involving a function f(x) =xTAx, then having A\nbeing positive semi-definite, we can guarantee that the problem is convex. Convex problems\nensure that a local minimum is also global, and convex problems can be solved efficiently\nusing known algorithms.\nDefinition 5.24 (Positive Semi-Definite ).A matrix A\u2208RN\u00d7Nis positive semi-\ndefinite if\nxTAx\u22650 (5.40)\nfor any x\u2208RN.Aispositive definite ifxTAx>0for any x\u2208RN.\nUsing eigendecomposition, it is not difficult to show that positive semi-definiteness is equiv-\nalent to having non-negative eigenvalues.\nTheorem 5.17. A matrix A\u2208RN\u00d7Nispositive semi-definite if and only if\n\u03bbi(A)\u22650 (5.41)\nfor all i= 1, . . . , N , where \u03bbi(A)denotes the ith eigenvalue of A.\nProof . By the definitions of eigenvalue and eigenvector, we have that\nAui=\u03bbiui,\nwhere \u03bbiis the eigenvalue and uiis the corresponding eigenvector. If Ais positive semi-\ndefinite, then uT\niAui\u22650 since uiis a particular vector in Rn. So we have\n0\u2264uT\niAui=\u03bb\u2225ui\u22252,\nand hence \u03bbi\u22650. Conversely, if \u03bbi\u22650 for all i, then since A=PN\ni=1\u03bbiuiuT\niwe can\nconclude that\nxTAx=xT NX\ni=1\u03bbiuiuT\ni!\nx=NX\ni=1\u03bbi(uT\nix)2\u22650.\n\u25a1\nThe following corollary shows that if A\u2208Rn\u00d7nis positive definite, it must be invert-\nible. Being invertible also means that the columns of Aare linearly independent.\nCorollary 5.2. If a matrix A\u2208RN\u00d7Nispositive definite (but not semi-definite),\nthenAmust be invertible, i.e., there exists A\u22121\u2208RN\u00d7Nsuch that\nA\u22121A=AA\u22121=I. (5.42)\nThe next theorem tells us that the covariance matrix is always positive semi-definite.\n298", "314": "5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS\nTheorem 5.18. The covariance matrix Cov(X) =\u03a3issymmetric positive semi-\ndefinite , i.e.,\n\u03a3T=\u03a3,and vT\u03a3v\u22650,\u2200v\u2208Rd.\nProof . Symmetry follows immediately from the definition, because Cov( Xi, Xj) = Cov( Xj, Xi).\nThe positive semi-definiteness comes from the fact that\nvT\u03a3v=vTE[(X\u2212\u00b5)(X\u2212\u00b5)T]v\n=E[vT(X\u2212\u00b5)(X\u2212\u00b5)Tv]\n=E[bTb] =E[\u2225b\u22252]\u22650,\nwhere b= (X\u2212\u00b5)Tv. \u25a1\nEnd of the discussion.\n5.7.4 Gaussian whitening\nBesides checking positive semi-definiteness, another typical problem we encounter is how to\ngenerate random samples according to some Gaussian distributions.\nFrom Gaussian (0,I)to Gaussian (\u00b5,\u03a3). If we are given zero-mean unit-variance Gaus-\nsianX\u223cGaussian( 0,I), how do we generate Y\u223cGaussian( \u00b5,\u03a3) from X?\nThe idea is to define a transformation\nY=\u03a31\n2X+\u00b5,\nwhere \u03a31\n2=U\u039b1\n2UT. Then the mean of Yis\nE[Y] =E[\u03a31\n2X+\u00b5] =\u03a31\n2E[X] +\u00b5=\u03a31\n20+\u00b5=\u00b5,\nand the covariance matrix is\nE[(Y\u2212\u00b5)(Y\u2212\u00b5)T] =E[(\u03a31\n2X+\u00b5\u2212\u00b5)(\u03a31\n2X+\u00b5\u2212\u00b5)T]\n=E[(\u03a31\n2X)(\u03a31\n2X)T] =\u03a31\n2E[XXT]\u03a31\n2\n=\u03a31\n2I\u03a31\n2=\u03a3.\nThe following theorem summarizes this result.\nTheorem 5.19. LetXbeX\u223cGaussian (0,I). Consider a mean vector \u00b5and a\ncovariance matrix \u03a3with eigendecomposition \u03a3=U\u039bUT. If\nY=\u03a31\n2X+\u00b5, (5.43)\nwhere \u03a31\n2=U\u039b1\n2UT, then Y\u223cGaussian (\u00b5,\u03a3).\n299", "315": "CHAPTER 5. JOINT DISTRIBUTIONS\nTherefore, the two steps for doing this Gaussian whitening are:\n\u0088Step 1: Generate samples {x1, . . . ,xN}that are distributed according to Gaussian( 0,I).\n\u0088Step 2: Define ynwhere\nyn=\u03a31\n2xn+\u00b5.\nThese two steps are portrayed in Figure 5.19 .\nFigure 5.19: Generating an arbitrary Gaussian from Gaussian (0,I).\nExample 5.26 . Consider a set of N= 1000 i.i.d. Gaussian( 0,I) data points as shown\ninFigure 5.20 , for example,\nx1=\u00140.5377\n1.8399\u0015\n,x2=\u0014\u22122.2588\n0.8622\u0015\n, . . . , x1000=\u00140.3188\n\u22121.3077\u0015\n.\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n(a) Before (b) After\nFigure 5.20: Generating arbitrary Gaussian random variables from Gaussian (0,I).\nTransform these data points so that the new distribution is a Gaussian with\n\u00b5=\u00141\n\u22122\u0015\nand \u03a3=\u00143\u22120.5\n\u22120.5 1\u0015\n.\n300", "316": "5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS\nSolution . To perform the transformation, we first perform eigendecomposition of \u03a3=\nU\u039bUT. Then \u03a31\n2=U\u039b1\n2UT. For our problem, we compute\n\u03a31\n2=\u00141.722 \u22120.1848\n\u22120.1848 0 .9828\u0015\n.\nMultiplying this matrix to yield yn=\u03a31\n2xn+\u00b5, we obtain\ny1=\u00141.5870\n\u22120.2971\u0015\n,y2=\u0014\u22123.0495\n\u22120.7351\u0015\n, . . . , y1000=\u00141.7907\n\u22123.3441\u0015\n.\nIn MATLAB, the above whitening procedure can be realized using the following com-\nmands.\n% MATLAB code to perform the whitening\nx = mvnrnd([0,0],[1 0; 0 1],1000);\nSigma = [3 -0.5; -0.5 1];\nmu = [1; -2];\ny = Sigma^(0.5)*x\u2019 + mu;\nThe Python implementation is similar, although one needs to be careful with the\nmore complicated syntax. For example, Sigma^(0.5) in MATLAB does the eigen-based\nmatrix power automatically, whereas in Python we need to call a specific built-in command\nfractional_matrix_power . In MATLAB, broadcasting a vector to a matrix can be rec-\nognized. In Python, we need to call repmat explicitly to control the shape of the mean\nvectors.\n# Python code to perform the whitening\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.linalg import fractional_matrix_power\nx = np.random.multivariate_normal([0,0],[[1,0],[0,1]],1000)\nmu = np.array([1,-2])\nSigma = np.array([[3, -0.5],[-0.5, 1]])\nSigma2 = fractional_matrix_power(Sigma,0.5)\ny = np.dot(Sigma2, x.T) + np.matlib.repmat(mu,1000,1).T\nFrom Gaussian (\u00b5,\u03a3)to Gaussian (0,I). The reverse direction can be done as follows.\nSupposing that we have Y\u223cGaussian( \u00b5,\u03a3), we define\nX=\u03a3\u22121\n2(Y\u2212\u00b5). (5.44)\nThen\nE[X] =E[\u03a3\u22121\n2(Y\u2212\u00b5)]\n=\u03a3\u22121\n2(E[Y]\u2212\u00b5) =0.\n301", "317": "CHAPTER 5. JOINT DISTRIBUTIONS\nThe covariance is\nCov(X) =E[(X\u2212\u00b5X)(X\u2212\u00b5X)T]\n=E[XXT]\n=Eh\n\u03a3\u22121\n2(Y\u2212\u00b5)(Y\u2212\u00b5)T\u03a3\u2212T\n2i\n=\u03a3\u22121\n2E\u0002\n(Y\u2212\u00b5)(Y\u2212\u00b5)T\u0003\n\u03a3\u2212T\n2\n=\u03a3\u22121\n2\u03a3\u03a3\u22121\n2=I.\nThe following theorem summarizes this result.\nTheorem 5.20. LetYbe a Gaussian Y\u223cGaussian (\u00b5,\u03a3). If\nX=\u03a3\u22121\n2(Y\u2212\u00b5), (5.45)\nthenX\u223cGaussian (0,I).\nThus the two steps of doing this reversed Gaussian whitening are:\n\u0088Step 1: Assuming that y1, . . . ,yNare distributed as Gaussian( \u00b5,\u03a3), estimate \u00b5\nand\u03a3.\n\u0088Step 2: Define xnwhere\nxn=\u03a31\n2(yn\u2212\u00b5). (5.46)\nThese two steps are shown pictorially in Figure 5.21 .\nFigure 5.21: Converting an arbitrary Gaussian back to Gaussian (0,I).\nIn practice, if we are given {yn}N\nn=1, we need to estimate \u00b5and\u03a3. The estimations\nare quite straightforward.\nb\u00b5=1\nNNX\nn=1yn,\nb\u03a3=1\nNNX\nn=1(yn\u2212b\u00b5)(yn\u2212b\u00b5)T.\n302", "318": "5.8. PRINCIPAL-COMPONENT ANALYSIS\nOn computers, these can be obtained using the command mean and cov. Once we have\ncalculated b\u00b5andb\u03a3, we can define xnas\nxn=b\u03a3\u22121\n2(yn\u2212b\u00b5).\nOn computers, the codes for the whitening procedure that uses the estimated mean\nand covariance are shown below.\n% MATLAB code to perform whitening\ny = mvnrnd([1; -2],[3 -0.5; -0.5 1],100);\nmY = mean(y);\ncovY = cov(y);\nx = covY^(-0.5)*(y-mY)\u2019;\n# Python code to perform whitening\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.linalg import fractional_matrix_power\ny = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],100)\nmY = np.mean(y,axis=0)\ncovY = np.cov(y,rowvar=False)\ncovY2 = fractional_matrix_power(covY,-0.5)\nx = np.dot(covY2, (y-np.matlib.repmat(mY,100,1)).T)\n5.8 Principal-Component Analysis\nWe have studied the covariance matrix \u03a3in some depth. It has many other uses besides\ntransforming Gaussian random variables, and in this section we present one of them, called\ntheprincipal-component analysis (PCA). PCA is a widely used tool for dimension reduc-\ntion. Instead of using Nfeatures to describe a data point, PCA allows us to use the leading\npprincipal components to describe the same data point. In many problems in machine\nlearning, this makes the learning task easier and the inference task more efficient.\n5.8.1 The main idea: Eigendecomposition\nPCA can be summarized in one sentence:\nThe key idea of PCA is the eigendecomposition of the covariance matrix \u03a3.\nThis is a condensed summary of PCA: It is just the eigendecomposition of the co-\nvariance. However, before we discuss the computational procedure, we will explain why we\nwould want to perform the eigendecomposition of the covariance matrix.\n303", "319": "CHAPTER 5. JOINT DISTRIBUTIONS\nConsider a set of data points {x(1), . . . ,x(N)}, where each x(n)\u2208Rdis ad-dimensional\nvector. The dimension dis often high. For example, if we have an image of size 1024 \u00d71024\u00d73,\nthen d= 3,145,728 \u2014 not a huge number, but enough to make you feel dizzy. The goal\nof PCA is to find a low-dimensional representation inRpwhere p\u226ad. If we can find\nthis low-dimensional representation, we can represent the d-dimensional input using only p\ncoefficients. Since p\u226ad, we can \u201ccompress\u201d the data by using a compact representation. In\nmodern data science, such a dimension reduction scheme is useful for handling large-scale\ndatasets.\nMathematically, we define a set of basis vector v1, . . . ,vp, where each vi\u2208Rd. Our\ngoal is to approximate an input data point x(n)\u2208Rdby these basis vectors:\nx(n)\u2248pX\ni=1\u03b1ivi,\nwhere {\u03b1i}p\ni=1are called the representation coefficients . The representation described by\nthis equation is a linear representation. Linear representation is extremely common in prac-\ntice. For example, a data point x(n)= [7,1,4]Tcan be represented as\n\uf8ee\n\uf8f07\n1\n4\uf8f9\n\uf8fb\n|{z}\nx(n)= 3|{z}\n\u03b11\uf8ee\n\uf8f01\n\u22121\n0\uf8f9\n\uf8fb\n|{z}\nv1+ 4|{z}\n\u03b12\uf8ee\n\uf8f01\n1\n1\uf8f9\n\uf8fb\n|{z}\nv2.\nTherefore, the 3-dimensional input x(n)can now be represented by two coefficients \u03b11= 3\nand\u03b12= 4. This is called dimensionality reduction .\nPictorially, if we have already determined the basis vectors, we can compute the co-\nefficients for every data point in the dataset. However, not all basis vectors are good. As\nillustrated in Figure 5.22 , an elongated dataset will be of the greatest benefit if the basis\nvectors are oriented according to the data geometry. If we can find such basis vectors, then\nthe data points will have a large coefficient and a small coefficient, corresponding to the\nmajor and the minor axes. Dimensionality reduction can thus be achieved by, for example,\nonly keeping the larger coefficients.\nFigure 5.22: PCA aims at finding a low-dimensional representation of a high-dimensional dataset. In\nthis figure, the 2D data points can be well represented by the 1D space spanned by v1.\nThe challenge here is that, given the dataset {x(1), . . . ,x(N)}, we need to determine\nboth the basis vectors {vi}p\ni=1and the coefficients {\u03b1i}p\ni=1. Fortunately, this can be formu-\nlated as an eigendecomposition problem.\n304", "320": "5.8. PRINCIPAL-COMPONENT ANALYSIS\nTo see how this problem can be thus formulated, we consider the simplest case as\nillustrated in Figure 5.22 , where we want to find theleading principal component. That is,\nwe find ( \u03b1,v) such that x\u2248\u03b1v. This amounts to solving the optimization problem\n(bv,b\u03b1) = argmin\n\u2225v\u22252=1,\u03b1\r\r\r\r\r\r\uf8ee\n\uf8f0|\nx\n|\uf8f9\n\uf8fb\u2212\u03b1\uf8ee\n\uf8f0|\nv\n|\uf8f9\n\uf8fb\r\r\r\r\r\r2\n.\nThe notation \u201cargmin\u201d means the argument that minimizes the function. The equation\nsays that we find the ( \u03b1,v) that minimizes the distance between xand\u03b1v. The constraint\n\u2225v\u22252= 1 limits the search to within a unit circle; otherwise our solution will not be unique.\nSolving the optimization problem is not difficult. If we take the derivative w.r.t. \u03b1and\nset it to zero, we have that\n2vT(x\u2212\u03b1v) = 0 \u21d2 \u03b1=vTx.\nSubstituting \u03b1=xTvinto the objective function again, we show that\nargmin\n\u2225v\u22252=1\u2225x\u2212\u03b1v\u22252= argmin\n\u2225v\u22252=1\u001a\nxTx\u22122\u03b1xTv+\u03b12\b\b\bvTv\u001b\n,\u2225v\u22252= 1\n= argmin\n\u2225v\u22252=1\u001a\n\u22122\u03b1xTv+\u03b12\u001b\n, dropxTx\n= argmin\n\u2225v\u22252=1\u001a\n\u22122(xTv)xTv+ (xTv)2\u001b\n, substitute \u03b1=xTv\n= argmax\n\u2225v\u22252=1\u001a\nvTxxTv\u001b\n, change min to max .\nLet us pause for a second. We have shown that if we have onedata point x, the leading\nprincipal component vcan be determined by maximizing vTxxTv. What have we gained?\nWe have transformed the original optimization, which contains two variables ( v, \u03b1), to a new\noptimization that contains one variable v. Thus if we know how to solve the one-variable\nproblem we are done.\nHowever, there is one more issue we need to address before we discuss how to solve\nfor the problem. The issue is that the formulation is about one data sample , not the entire\ndataset. To include all the samples, we need to assume that xis a realization of a random\nvector X. Then the above optimization can be formulated in the expectation sense as\nargmin\n\u2225v\u22252=1E\u2225X\u2212\u03b1v\u22252= argmax\n\u2225v\u22252=1vTE\u001a\nXXT\u001b\nv\n= argmax\n\u2225v\u22252=1vT\u03a3v,\nwhere \u03a3def=E[XTX].1Therefore, if we can maximize vT\u03a3vwe will be able to determine\nthe principal component.\nNow comes the main result. The following theorem shows that the maximization is\nequivalent to eigendecomposition. The proof requires Lagrange multipliers, which are beyond\nthe scope of this book.\n1Here we assume that Xis zero-mean, i.e., E[X] = 0. If it is not, then we can subtract the mean by\nconsidering argmax\n\u2225v\u22252=1vTE\u001a\n(X\u2212\u00b5)(X\u2212\u00b5)T\u001b\nv.\n305", "321": "CHAPTER 5. JOINT DISTRIBUTIONS\nTheorem 5.21. Let\u03a3be ad\u00d7dmatrix with eigendecomposition \u03a3=USUT. Then\nthe optimization\nbv=argmax\n\u2225v\u22252=1vT\u03a3v (5.47)\nhas a solution bv=u1, i.e., the first column of the eigenvector matrix U.\nThe following proof requires an understanding of Lagrange multipliers and constrained\noptimizations. It is not essential for understanding this chapter.\nWe want to prove that the solution to the problem\nbv= argmax\n\u2225v\u22252=1vT\u03a3v\nis the eigenvector of the matrix \u03a3. To show that, we first write down the Lagrangian:\nL(v, \u03bb) =vT\u03a3v\u2212\u03bb(\u2225v\u22252\u22121)\nTaking the derivative w.r.t. vand setting to zero yields\n\u2207vL(v, \u03bb) = 2\u03a3v\u22122\u03bbv=0.\nThis is equivalent to \u03a3v=\u03bbv. So if \u03a3=USUT, then by letting v=uiand\u03bb=siwe can\nsatisfy the condition since \u03a3ui=USUTui=USe i=siui.\nEnd of the proof.\nThis theorem can be extended to the second (and other) principal components of\nthe covariance matrix. In fact, given the covariance matrix \u03a3we can follow the procedure\noutlined in Figure 5.23 to determine the principal components. The eigendecomposition of a\nd\u00d7dmatrix \u03a3will give us a d\u00d7deigenvector matrix Uand an eigenvalue matrix S. To keep\nthepleading eigenvectors, we truncate the Umatrix to only use the first peigenvectors. Here,\nwe assume that the eigenvectors are ordered according to the magnitude of the eigenvalues,\nfrom large to small.\nIn practice, if we are given a dataset {x(1), . . . ,x(N)}, we can first estimate the covari-\nance matrix \u03a3by\nb\u03a3=1\nNNX\nn=1(x(n)\u2212b\u00b5)(x(n)\u2212b\u00b5)T,\nwhere b\u00b5=1\nNPN\nn=1x(n)is the mean vector. Afterwards, we can compute the eigendecom-\nposition of b\u03a3by\n[U,S] = eig( b\u03a3).\nOn a computer, the principal components are obtained through eigendecomposition.\nA MATLAB example and a Python example are shown below. We explicitly show the two\nprincipal components in this example. The magnitudes of these two vectors are determined\nby the eigenvalues diag(s) .\n306", "322": "5.8. PRINCIPAL-COMPONENT ANALYSIS\nFigure 5.23: The principal components are the eigenvectors of the covariance matrix. In this figure \u03a3\ndenotes the covariance matrix, u1, . . . ,updenote the pleading eigenvectors, and sdenotes the diagonal\nof the eigenvalue matrix.\n% MATLAB code to perform the principal-component analysis\nx = mvnrnd([0,0],[2 -1.9; -1.9 2],1000);\ncovX = cov(x);\n[U,S] = eig(covX);\nu(:,1) % Principle components\nu(:,2) % Principle components\n# Python code to perform the principal-component analysis\nimport numpy as np\nx = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],1000)\ncovX = np.cov(x,rowvar=False)\nS, U = np.linalg.eig(covX)\nprint(U)\nExample 5.27 . Suppose we have a dataset containing N= 1000 samples, drawn from\nan unknown distribution. The first few samples are\nx1=\u00140.5254\n\u22120.6930\u0015\n,x2=\u0014\u22120.4040\n0.3724\u0015\n, . . . , x1000=\u00141.4165\n\u22121.5463\u0015\n.\nWe can compute the mean and covariance using MATLAB commands mean andcov.\nThis will return us\nb\u00b5=\u00140.0561\n\u22120.0303\u0015\nandb\u03a3=\u00142.0460 \u22121.9394\n\u22121.9394 2 .0426\u0015\n.\nApplying eigendecomposition on b\u03a3, we show that\n[U,S] = eig( b\u03a3),\n=\u21d2U=\u0014\u22120.7068 \u22120.7074\n\u22120.7074 0 .7068\u0015\nand S=\u00140.1049 0\n0 3 .9837\u0015\n.\n307", "323": "CHAPTER 5. JOINT DISTRIBUTIONS\nTherefore, we have obtained two principal components\nu1=\u0014\u22120.7068\n\u22120.7074\u0015\nand u2=\u0014\u22120.7074\n0.7068\u0015\n.\nAs seen in the figure below, these two principal components make sense. The vector\nu1is the orange line and is the minor axis. The vector u2is the blue line and is the\nmajor axis. Again, the ordering of the vectors is determined by the eigenvalues. Since\nu2has a larger eigenvalue (=3.9837), it is the leading principal component.\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\nFigure 5.24: To determine the representation coefficients, we solve an inverse problem by finding the\nvector \u03b1in the equation x(n)=Up\u03b1(n).\nWhy do we call our method principal component analysis ? The analysis part comes\nfrom the fact that we can compress a data vector x(n)from a high dimension dto a low\ndimension p. Defining Up= [u1, . . . ,up], a matrix containing the pleading eigenvectors of\nthe matrix U, we solve the inverse problem:\nx(n)=Up\u03b1(n),\nwhere the goal is to determine the coefficient vector \u03b1(n)\u2208Rp. Since Upis an orthonormal\nmatrix (i.e., UT\npUp=I), it follows that\nUT\npx(n)=UT\npUp|{z}\n=I\u03b1(n),\n308", "324": "5.8. PRINCIPAL-COMPONENT ANALYSIS\nas illustrated in Figure 5.24 . Hence,\n\u03b1(n)=UT\npx(n).\nThis equation is a projection operation that projects a data point x(n)onto the space\nspanned by the pleading principal components. Repeating the procedure for all the data\npoints x(1), . . . ,x(N)in the dataset, we have compressed the dataset.\nExample 5.28 . Using the example above, we can show that\n\u03b1(1)=UTx(1)=\u00140.1189\n\u22120.8615\u0015\n,\u03b1(2)=\u00140.0221\n0.5491\u0015\n, . . . , \u03b1(1000)=\u00140.0927\n\u22122.0950\u0015\n.\nThe principal-component analysis says that since the leading components represent the\ndata, we only need to keep the blue-colored values because they are the coefficients\nassociated with the leading principal component.\n5.8.2 The eigenface problem\nAs a concrete example of PCA, we consider a computer vision problem called the eigen-\nface problem. In 2001, researchers at Yale University published the Yale Database, and\na few years later they extended it to a larger one ( http://vision.ucsd.edu/ ~leekc/\nExtYaleDatabase/ExtYaleB.html ). The dataset, now known as the Yale Face Dataset, con-\ntains 16,128 images of 28 human subjects under nine poses and 64 illumination conditions.\nThe sizes of the images are d= 168 \u00d7192 = 32,256 pixels. Treating these N=16,128 images\nas vectors in R32,256\u00d71, we have 16,128 of these vectors. Let us call them {x(1), . . . ,x(N)}.\nFollowing the procedure we described above, we estimate the covariance matrix by\ncomputing\nb\u03a3=E[(X\u2212b\u00b5)(X\u2212b\u00b5)T]\u22481\nNNX\nn=1(x(n)\u2212b\u00b5)(x(n)\u2212b\u00b5)T, (5.48)\nwhere b\u00b5=E[X]\u22481\nNPN\nn=1x(n)is the mean vector. Note that the size of b\u00b5is 32,256 \u00d71\nand the size of b\u03a3is 32,256 \u00d732,256.\nFigure 5.25: The extended Yale Face Database B.\nOnce we obtain an estimate of the covariance matrix, we can perform an eigendecom-\nposition to get\n[U,S] = eig( b\u03a3).\nThe columns of U, i.e.,{ui}d\ni=1, are the eigenvectors of b\u03a3. These eigenvectors are the basis\nof a testing face image.\n309", "325": "CHAPTER 5. JOINT DISTRIBUTIONS\nFigure 5.26: Given a face image, the learned basis vectors (from the eigendecomposition of the covari-\nance matrix) can be used to compress the image xinto a feature vector \u03b1where the dimension of \u03b1is\nsignificantly lower than that of x.\nWith the basis vectors u1, . . . ,upwe can project every image in the dataset using a\nlow-dimensional representation. Specifically, for an image xwe compute the coefficients\n\u03b1i=uT\nix, i = 1, . . . , p\nor more compactly \u03b1=UTx. Note that the dimension of xisd\u00d71 (which in our case is\nd= 32,526), and the dimensions of \u03b1can be as few as p= 100. Therefore, we are using a\n100-dimensional vector to represent a 32,526-dimensional data. This is a huge dimensionality\nreduction.\nThe process repeats for all the samples x(1), . . . ,x(N). This gives us a collection of rep-\nresentation coefficients \u03b1(1), . . . ,\u03b1(N), where each \u03b1(n)is 100-dimensional (see Figure 5.26 ).\nNotice that the basis vectors uiappear more or less \u201cface images,\u201d but they are the features\nof the faces. PCA says that a real face can be written as a linear combination of these basis\nvectors.\nHow to solve the eigenface problem\n\u0088Compute the covariance matrix of all the images.\n\u0088Apply eigendecomposition to the covariance matrix.\n\u0088Project onto the basis vectors and find the coefficients.\n\u0088The coefficients are the low-dimensional representation of the images.\n\u0088We use the coefficients to perform downstream tasks, such as classification.\n310", "326": "5.8. PRINCIPAL-COMPONENT ANALYSIS\n5.8.3 What cannot be analyzed by PCA?\nPCA is a dimension reduction tool. It compresses a raw data vector x\u2208Rdinto a smaller\nfeature vector \u03b1\u2208Rp. The advantage is that the downstream learning problems are much\neasier because p\u226ad. For example, classification using \u03b1is more efficient than classification\nusing xsince there is very little information loss from xto\u03b1.\nThere are three limitations of PCA:\n\u0088PCS fails when the raw data are not orthogonal . The basis vectors uireturned\nby PCA are orthogonal , meaning that uT\niuj= 0 as long as i\u0338=j. As a result, if\nthe data intrinsically have this orthogonality property, then PCA will work very well.\nHowever, if the data live in a space such as a donut shape as illustrated in Figure 5.27 ,\nthen PCA will fail. Here, by failure, we mean that pis not much smaller than d. To\nhandle datasets behaving like Figure 5.27 we need advanced tools. One of these is the\nkernel-PCA. The idea is to apply a nonlinear transformation to the data before you\nrun PCA.\nFigure 5.27: [Left] PCA works when the data has redundant dimensions or is living on orthogonal\nspaces. [Right] PCA fails when the data does not have easily decomposable spaces.\n\u0088Basis vectors returned by PCA are not interpretable . A temptation with PCA is to\nthink that the basis vectors uioffer meaningful information because they are the \u201cprin-\ncipal components\u201d. However, since PCA is the eigendecomposition of the covariance\nmatrix, which is purely a mathematical operation, there is no guarantee that the basis\nvectors contain any semantic meaning. If we look at the basis vectors shown in Fig-\nure 5.26 , there is almost no information one can draw. Therefore, in the data-science\nliterature alternative methods such as non-negative matrix factorization and the more\nrecent deep neural network embedding are more attractive because the feature vectors\nsometimes (not always) have meanings.\n\u0088PCA does not return you the most influential \u201ccomponent\u201d . Imagine that you\nare analyzing medical data for research on a disease, in which each data vector x(n)\ncontains height, weight, BMI, blood pressure, etc. When you run PCA on the dataset,\nyou will obtain some \u201cprincipal components\u201d. However, these principal components\nwill likely have everything, e.g., the height entry of the principal component will have\nsome values, the weight will have some values, etc. If you have found a principal\ncomponent, it does not mean that you have identified the leading risk factor of the\ndisease. If you want to identify the leading risk factor of the disease, e.g., whether\nthe height or weight is more important, you need to resort to advanced tools such as\nvariable selection or the LASSO type of regression analysis (see Chapter 7).\n311", "327": "CHAPTER 5. JOINT DISTRIBUTIONS\nClosing remark . PCAs are powerful computational tools based on the simplest concept of\ncovariance matrices because, as our derivation showed, covariance matrices encode the \u201cvari-\nation\u201d of the data. Therefore, by finding a vector that aligns with the maximum variation\nof the data, we can find the principal component.\n5.9 Summary\nAs you were reading this chapter, you may have felt that the first and second parts discuss\ndistinctly different subjects, and in fact many books treat them as separate topics. We take\na different approach. We think that they are essentially the same thing if you understand\nthe following chain of distributions:\nfX(x)|{z}\none variable=\u21d2fX1,X2(x1, x2)|{z }\ntwo variables=\u21d2 \u00b7\u00b7\u00b7 =\u21d2fX1,...,X N(x1, . . . , x N)| {z }\nNvariables.\nThe first part exclusively deals with two variables. The generalization from two variables to\nNvariables is straightforward for PDFs and CDFs:\n\u0088PDF: fX1,X2(x1, x2) =\u21d2fX1,...,X N(x1, . . . , x N).\n\u0088CDF: FX1,X2(x1, x2) =\u21d2FX1,...,X N(x1, . . . , x N).\nThe joint expectation can also be generalized from two variables to Nvariables:\n\u0014Var[X2\n1] Cov( X1, X2)\nCov(X2, X1) Var[ X2\n2]\u0015\n=\u21d2\uf8ee\n\uf8ef\uf8f0Var[X2\n1]\u00b7\u00b7\u00b7 Cov(X1, XN)\n.........\nCov(XN, X1)\u00b7\u00b7\u00b7 Var[X2\nN]\uf8f9\n\uf8fa\uf8fb.\nConditional PDFs and conditional expectations are powerful tools for decomposing\ncomplex events into simpler events. Specifically, the law of total expectation,\nE[X] =Z\nE[X|Y=y]fY(y)dy=EY[EX|Y[X|Y]],\nis instrumental for evaluating variables defined through conditional relationships. The idea\nis also extendable to more random variables, such as\nE[X1] =Z Z\nE[X1|X2=x2, X3=x3]fX2,X3(x2, x3)dx2dx3,\nwhere E[X1|X2=x2, X3=x3] can be evaluated through\nE[X1|X2=x2, X3=x3] =Z\nx1fX1|X2,X3(x1|x2, x3)dx1.\nThis type of chain relationship can generalize to other high-order cases.\nIt is important to remember that for any high-dimensional random variables, the char-\nacterization is always made by the PDF fX(x) (or the CDF). We did not go into the details\n312", "328": "5.10. REFERENCES\nof analyzing fX(x) but have only discussed the mean vector E[X] =\u00b5and the covariance\nmatrix Cov( X) =\u03a3. We have been focusing exclusively on the high-dimensional Gaussian\nrandom variables\nfX(x) =1p\n(2\u03c0)d|\u03a3|exp\u001a\n\u22121\n2(x\u2212\u00b5)T\u03a3(x\u2212\u00b5)\u001b\n,\nbecause they are ubiquitous in data science today. We discussed the linear transformations\nfrom a zero-mean unit-variance Gaussian to another Gaussian, and vice versa.\n5.10 References\nJoint Distributions and Correlation\n5-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapters 2.5, 3.4, 4.2.\n5-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapters 5.1 \u2013 5.6.\n5-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapters 6.1 \u2013 6.4.\n5-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapters 7.1 \u2013 7.2.\n5-5 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapters\n6.1 \u2013 6.3.\n5-6 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Chapter 2.6.\nConditional Distributions and Expectations\n5-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapters 2.6, 3.5, 3.6, 4.3.\n5-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 5.7.\n5-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapters 6.6 \u2013 6.7.\n5-10 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapters 7.3 \u2013 7.5.\n5-11 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapters\n7.5 \u2013 7.6.\n5-12 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Chapter 4.2.\n313", "329": "CHAPTER 5. JOINT DISTRIBUTIONS\nSum of Random Variables\n5-13 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 4.5.\n5-14 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 7.1.\n5-15 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Chapters 3.3 and 3.4.\nVector Random Variables\n5-16 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapters 6.1 \u2013 6.6.\n5-17 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapters 8.1 \u2013 8.3, 9.\n5-18 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Chapters 5.1 \u2013 5.6.\nPrincipal-Component Analysis\nPCA is often taught in machine learning courses. For first-time readers, we suggest reviewing\nthe linear algebraic tools in Moon and Stirling. Then, the tutorial by Shlens and the chapter\nin Bishop would be sufficient to cover most of the materials. More advanced topics, such as\nkernel PCA, can be found in the following references.\n5-19 Todd K. Moon and Wynn C. Stirling, Mathematical Methods and Algorithms for Signal\nProcessing , Prentice-Hall, 2000. Chapter 7.\n5-20 Christopher Bishop, Pattern Recognition and Machine Leanring , Springer, 2006. Chap-\nter 12.\n5-21 Jonathon Shlens (2014) \u201cA Tutorial on Principal Component Analysis\u201d, https://\narxiv.org/pdf/1404.1100.pdf\n5-22 Paul Honeine (2014), \u201cAn eigenanalysis of data centering in machine learning\u201d, https:\n//arxiv.org/pdf/1407.2904.pdf\n5-23 Quan Wang (2012), \u201cKernel Principal Component Analysis and its Applications\u201d,\nhttps://arxiv.org/abs/1207.3538\n5-24 Sch\u00a8 olkopf et al. (2005), \u201cKernel Principal Component Analysis\u201d, https://link.springer.\ncom/chapter/10.1007/BFb0020217\n5.11 Problems\nExercise 1. (Video Solution)\nAlex and Bob each flips a fair coin twice. Use \u201c1\u201d to denote heads and \u201c0\u201d to denote tails.\nLetXbe the maximum of the two numbers Alex gets, and let Ybe the minimum of the\ntwo numbers Bob gets.\n314", "330": "5.11. PROBLEMS\n(a) Find and sketch the joint PMF pX,Y(x, y).\n(b) Find the marginal PMF pX(x) and pY(y).\n(c) Find the conditional PMF PX|Y(x|y). Does PX|Y(x|y) =PX(x)? Why or why not?\nExercise 2.\nTwo fair dice are rolled. Find the joint PMF of XandYwhen\n(a)Xis the larger value rolled, and Yis the sum of the two values.\n(b)Xis the smaller, and Yis the larger value rolled.\nExercise 3.\nThe amplitudes of two signals XandYhave joint PDF\nfXY(x, y) =e\u2212x/2ye\u2212y2\nforx >0, y > 0.\n(a) Find the joint CDF.\n(b) Find P(X1/2> Y).\n(c) Find the marginal PDFs.\nExercise 4. (Video Solution)\nFind the marginal CDFs FX(x) and FY(y) and determine whether or not XandYare\nindependent, if\nFXY(x, y) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3x\u22121\u2212e\u2212y\u2212e\u2212xy\ny,if 1\u2264x\u22642, y\u22650\n1\u2212e\u2212y\u2212e\u22122y\ny, ifx >2, y\u22650,\n0, otherwise .\nExercise 5. (Video Solution)\n(a) Find the marginal PDF fX(x) if\nfXY(x, y) =exp{\u2212|y\u2212x| \u2212x2/2}\n2\u221a\n2\u03c0.\n(b) Find the marginal PDF fY(y) if\nfXY(x, y) =4e\u2212(x\u2212y)2/2\ny2\u221a\n2\u03c0.\n315", "331": "CHAPTER 5. JOINT DISTRIBUTIONS\nExercise 6. (Video Solution)\nLetX, Y be two random variables with joint CDF\nFX,Y(x, y) =y+e\u2212x(y+1)\ny+ 1.\nShow that\n\u22022\n\u2202x\u2202yFX,Y(x, y) =\u22022\n\u2202y\u2202xFX,Y(x, y).\nWhat is the implication of this result?\nExercise 7. (Video Solution)\nLetXandYbe two random variables with joint PDF\nfX,Y(x, y) =1\n2\u03c0e\u22121\n2(x2+y2).\n(a) Find the PDF of Z= max( X, Y).\n(b) Find the PDF of Z= min( X, Y).\nYou may leave your answers in terms of the \u03a6( \u00b7) function.\nExercise 8.\nThe random vector ( X, Y) has a joint PDF\nfXY(x, y) = 2 e\u2212xe\u22122y\nforx >0, y > 0. Find the probability of the following events:\n(a){X+Y\u22648}.\n(b){X\u2212Y\u226410}.\n(c){X2< Y}.\nExercise 9.\nLetXandYbe zero-mean, unit-variance independent Gaussian random variables. Find the\nvalue of rfor which the probability that ( X, Y) falls inside a circle of radius ris 1/2.\nExercise 10.\nThe input Xto a communication channel is +1 or \u22121 with probabilities pand 1 \u2212p,\nrespectively. The received signal Yis the sum of Xand noise N, which has a Gaussian\ndistribution with zero mean and variance \u03c32= 0.25.\n(a) Find the joint probability P(X=j, Y\u2264y).\n(b) Find the marginal PMF of Xand the marginal PDF of Y.\n(c) Suppose we are given that Y >0. Which is more likely, X= 1 or X=\u22121?\n316", "332": "5.11. PROBLEMS\nExercise 11. (Video Solution)\nLet\nfX,Y(x, y) =(\nce\u2212xe\u2212y,if 0\u2264y\u2264x <\u221e,\n0,otherwise .\n(a) Find c.\n(b) Find fX(x) and fY(y).\n(c) Find E[X] andE[Y], Var[ X] and Var[ Y].\n(d) Find E[XY], Cov( X, Y) and \u03c1.\nExercise 12. (Video Solution)\nIn class, we have used the Cauchy-Schwarz inequality to show that \u22121\u2264\u03c1\u22641. This exercise\nasks you to prove the Cauchy-Schwarz inequality:\n(E[XY])2\u2264E[X2]E[Y2].\nHint: Consider the expectation E[(tX+Y)2]. Note that this is a quadratic equation in tand\nE[(tX+Y)2]\u22650 for all t. Consider the discriminant of this quadratic equation.\nExercise 13. (Video Solution)\nLet \u0398 \u223cUniform[0 ,2\u03c0].\n(a) If X= cos \u0398, Y= sin \u0398. Are XandYuncorrelated?\n(b) If X= cos(\u0398 /4),Y= sin(\u0398 /4). Are XandYuncorrelated?\nExercise 14. (Video Solution)\nLetXandYhave a joint PDF\nfX,Y(x, y) =c(x+y),\nfor 0\u2264x\u22641 and 0 \u2264y\u22641.\n(a) Find c,fX(x),fY(y), and E[Y].\n(b) Find fY|X(y|x).\n(c) Find P[Y > X |X > 1/2].\n(d) Find E[Y|X=x].\n(e) Find E[E[Y|X]], and compare with the E[Y] computed in (a).\nExercise 15. (Video Solution)\nUse the law of total expectation to compute the following:\n317", "333": "CHAPTER 5. JOINT DISTRIBUTIONS\n1.E[sin(X+Y)], where X\u223c N(0,1), and Y|X\u223cUniform[ x\u2212\u03c0, x+\u03c0]\n2.P[Y < y ], where X\u223cUniform[0 ,1], and Y|X\u223cExponential( x)\n3.E[XeY], where X\u223cUniform[ \u22121,1], and Y|X\u223c N(0, x2)\nExercise 16.\nLetY=X+N, where Xis the input, Nis the noise, and Yis the output of a system. Assume\nthat XandNare independent random variables. It is given that E[X] = 0, Var[ X] =\u03c32\nX,\nE[N] = 0, and Var[ N] =\u03c32\nN.\n(a) Find the correlation coefficient \u03c1between the input Xand the output Y.\n(b) Suppose we estimate the input Xby a linear function g(Y) =aY. Find the value of\nathat minimizes the mean squared error E[(X\u2212aY)2].\n(c) Express the resulting mean squared error in terms of \u03b7=\u03c32\nX/\u03c32\nN.\nExercise 17. (Video Solution)\nTwo independent random variables XandYhave PDFs\nfX(x) =(\ne\u2212x, x \u22650,\n0, x < 0,fY(y) =(\n0, y > 0,\ney, y \u22640.\nFind the PDF of Z=X\u2212Y.\nExercise 18.\nLetXandYbe two independent random variables with densities\nfX(x) =(\nxe\u2212x, x \u22650,\n0, x < 0,and fY(y) =(\nye\u2212y, y \u22650,\n0, y < 0.\nFind the PDF of Z=X+Y.\nExercise 19.\nThe random variables XandYhave the joint PDF\nfXY(x, y) =e\u2212(x+y)\nfor 0 < y < x < 1. Find the PDF of Z=X+Y.\nExercise 20.\nThe joint density function of XandYis given by\nfXY(x, y) =e\u2212(x+y)\nforx >0, y > 0. Find the PDF of the random variable Z=X/Y .\n318", "334": "Chapter 6\nSample Statistics\nWhen we think about probability, the first thing that likely comes to mind is flipping a coin,\nthrowing a die, or playing a card game. These are excellent examples of the subject. However,\nthey seldom fit in the context of modern data science, which is concerned with drawing\nconclusions from data. In our opinion, the power of probability is its ability to summarize\nmicrostates using macro descriptions . This statement will take us some effort to elaborate.\nWe study probability because we want to analyze the uncertainties. However, when we\nhave many data points, analyzing the uncertainties of each data point (the microstates)\nis computationally very difficult. Probability is useful here because it allows us to bypass\nthe microstates and summarize the macro behavior. Instead of reporting the states of each\nindividual, we report their sample average. Instead of offering the worst-case guarantee,\nwe offer a probabilistic guarantee. You ask: so what? If we can offer you a performance\nguarantee at 99.99% confidence but one-tenth of the cost of a 100% performance guarantee,\nwould you consider our offer? The goal of this chapter is to outline the concepts of these\nprobabilistic arguments.\nThe significance of sample average\nImagine that you have a box containing many tiny magnets. (You can also think of a dataset\ncontaining two classes of labels.) In condensed matter physics, these are known as the spin\nglasses . The orientations of the magnets depend on the magnetic field. Under an extreme\ncondition where the magnetic field is strong, all magnets will point in the same direction.\nWhen the magnetic field is not as strong, some will align with the field but some will not,\nas we show in Figure 6.1 .\nIf we try to study every single magnet in this box, the correlation of the magnets will\nforce us to consider a joint distribution, since if one magnet points to the right it is likely\nthat another magnet will also point to the right. The simultaneous description of all magnets\nis modeled through a joint probability distribution\nfX1,X2,...,X N(x1, x2, . . . , x N).\nLike any joint PDF, this PDF tells us the probability density that the magnets will take\na collection of states simultaneously. If Nis large (say, on the order of millions), this joint\ndistribution will be very complicated.\n319", "335": "CHAPTER 6. SAMPLE STATISTICS\nFigure 6.1: Imagine that we have a box of magnets and we want to measure their orientation angles.\nThe data points have individual randomness and correlations. Studying each one individually could be\ncomputationally infeasible, as we need to estimate the joint PDF fX1,...,X N(x1, . . . , x N)across all the\ndata points. Probability offers a tool to summarize these individual states using a macro description.\nFor example, we can analyze the sample average XNof the data points and derive conclusions from\nthe PDF of XN, i.e., fXN(x). The objective of this chapter is to present a few probabilistic tools to\nanalyze macro descriptions, such as the sample average.\nSince the joint PDF is very difficult to obtain computationally, physicists proposed\nto study the sample statistics. Instead of looking at the individual states, they look at the\nsample average of the states. If we define X1, . . . , X Nas the states of the magnets, then\nthe sample average is\nXN=1\nNNX\nn=1Xn.\nSince each magnet is random, the sample average is also random, and therefore it is granted\na PDF:\nfXN(x).\nThus, XNhas a PDF, a mean, a variance, and so on.\nWe call XNa sample statistic. It is called a statistic because it is a summary of the\nmicrostates, and a sample statistic because the statistic is based on random samples, not on\nthe underlying theoretical distributions. We are interested in knowing the behavior of XN\nbecause it is the summary of the observations. If we know the PDF of XN, we will know\nthe mean, the variance, and the value of XNwhen the magnetic field increases or decreases.\nWhy study the sample average XN?\n\u0088Analyzing individual variables is not feasible because the joint PDF can be ex-\ntremely high-dimensional.\n\u0088Sample average is a macro description of the data.\n\u0088If you know the behavior of the sample average, you know most of the data.\nProbabilistic guarantee versus worst-case guarantee\nBesides the sample average, we are also interested in the difference between a probabilistic\nguarantee and a deterministic guarantee.\n320", "336": "Consider the birthday paradox (see Chapter 1 for details). Suppose there are 50 stu-\ndents in a room. What is the probability that at least two students have the same birthday?\nA naive thought would suggest that we need 366 students to guarantee a pair of the same\nbirthday because there are 365 days. So, with only 50 students, it would seem unlikely to\nhave a pair with the same birthday. However, it turns out that with just 50 students, the\nprobability of having at least one pair with the same birthday is more than 97%. Figure 6.2\nbelow shows a calculation by a computer, where we plot the estimated probability as a func-\ntion of the number of students. What is more surprising is that with as few as 23 students,\nthe probability is greater than 50%. There is no need for there to be 365 students in order\nto offer a guarantee.\n0 10 20 30 40 50 60 70 80 90 100\nNumber of people00.10.20.30.40.50.60.70.80.91Probability\nFigure 6.2: The birthday paradox asks the question of how many people we need to ask in order to have\nat least two of them having the same birthday. While we tend to think that the answer is 366 (because\nthere are 365 days), the actual probability, as we have calculated (see Chapter 1), is more than 97%,\neven if we have only asked 50 people. The curve above shows the probability of having at least one pair\nof people having the same birthday as a function of the number of people. The plot highlights the gap\nbetween the worst-case performance and an average-case performance.\nWhy does this happen? Certainly, we can trace back to the formulae in Chapter 1 and\nargue through the lens of combinations and permutations. However, the more important\nmessage is about the difference between the worst-case guarantee and the average-case\nguarantee .\nWorst case versus average case\n\u0088Worst-case guarantee: You need to ensure that the worst one is protected. This\nrequires an exhaustive search until hitting 100%. It is a deterministic guarantee.\n\u0088Average-case guarantee: You guarantee that with a high probability (e.g., 99.99%),\nthe undesirable event does not happen. This is a probabilistic guarantee.\nIs there a difference between 99.99% and 100%? If the probability is 99.99%, there is\none failure every 10,000 trials on average. You are unlikely to fail, but it is still possible.\nA 100% guarantee says that no matter how many trials you make you will not fail. The\n99.99% guarantee is much weaker (yes, much weaker, not just a little bit weaker) than the\ndeterministic guarantee. However, in practice, people might be willing to pay for the risk in\nexchange for efficiency. This is the principle behind insurance. Automobile manufacturing\n321", "337": "CHAPTER 6. SAMPLE STATISTICS\nalso uses this principle \u2014 your chance of purchasing a defective car is non-zero, but if the\nmanufacturer can sell enough cars to compensate for the maintenance cost of fixing your\ncar, they might be willing to offer a limited warranty in exchange for a lower selling price.\nHow do we analyze the probabilistic guarantee, e.g., for the sample average? Remember\nthat the sample average XNis a random variable. Since it is a random variable, it has a\nmean, variance, and PDF.1To measure the probabilistic guarantee, we consider the event\nBdef={|XN\u2212\u00b5| \u2265\u03f5},\nwhere \u00b5=E[XN] is the true population mean, and \u03f5 >0 is a very small number. This\nprobability is illustrated in Figure 6.3 , assuming that XNhas the PDF of a Gaussian. The\nprobability of Bis the two tails under the PDF. Therefore, Bis abadevent because in\nprinciple XNshould be close to \u00b5. The probability P[B] measures situations where XN\nstays very far from \u00b5. If we can show that P[B] is small (e.g., <0.01%), then we can say\nthat we have obtained a probabilistic guarantee at 99.99%.\nFigure 6.3: The probabilistic guarantee of a sample average XNis established by computing the\nprobability of the tails. In this example, we assume that fXN(x)take a Gaussian shape, and we define\n\u03f5= 1. Anything belonging to |XN\u2212\u00b5| \u2265\u03f5is called a undesired event B. If the probability of a\nundesired event is small, we say that we can offer a probabilistic guarantee.\nThe moment we compute P[|XN\u2212\u00b5| \u2265\u03f5], we enter the race of probabilistic guarantee\n(e.g., 99.99%). Why? If the probability P[|XN\u2212\u00b5| \u2265\u03f5] is less than 0.01%, it still does not\nexclude the possibility that something bad will happen once every 10,000 trials on average.\nThe chance is low, but it is still possible. We will learn some mathematical tools for analyzing\nthis type of probabilistic guarantee.\nPlan for this chapter\nWith these two main themes in mind, we now discuss the organization of this chapter. There\nare four sections: two for mathematical tools and two for main results.\n\u0088Moment-generating functions : We have seen in Chapter 5 that the PDF of a sum of\ntwo random variables X+Yis the convolution of the two PDFs fX\u2217fY. Convolutions\nare non-trivial, especially when we have more random variables to sum. The moment-\ngenerating functions provide a convenient way of summing Nrandom variables. They\nare the transform domain techniques (e.g., Fourier transforms). Since convolutions in\n1Not all random variables have mean and variance, e.g., a Cauchy random variable, but most of them\ndo.\n322", "338": "time are multiplications in frequency, the moment-generating functions allow us to\nmultiply PDFs in the transformed space. In this way, we can sum as many random\nvariables as we want. We will discuss this idea in Section 6.1.\nKey Concept 1: Why study moment-generating functions?\nMoment-generating functions help us determine the PDF of X1+X2+\u00b7\u00b7\u00b7+XN.\n\u0088Probability inequalities : When analyzing sample statistics such as XN, evaluating the\nexact probability could be difficult because it requires integrating the PDFs. However,\nif our ultimate goal is to estimate the probability, deriving an upper bound might be\nsufficient to achieve the goal. The probability inequalities are designed for this purpose.\nIn Section 6.2, we discuss several of the most basic probability inequalities. We will\nuse some of them to prove the law of large numbers.\nKey Concept 2: How can probability inequalities be useful?\nProbability inequalities help us upper-bound the bad event P[|XN\u2212\u00b5| \u2265\u03f5].\n\u0088Law of large numbers : This is the first main result of the chapter. The law of large\nnumbers says that the sample average XNconverges to the population mean \u00b5when\nthe number of samples grows to infinity. The law of large numbers comes in two\nversions: the weak law of large numbers and the strong law of large numbers. The\ndifference is the type of convergence they guarantee. The weak law is based on con-\nvergence in probability , whereas the strong law is based on almost sure convergence .\nWe will discuss these types of convergence in Section 6.3.\nKey Concept 3: What is the law of large numbers?\nThere is a weak law and a strong law of large numbers. The weak law of large\nnumbers says that XNconverges to the true mean \u00b5, asNgrows:\nlim\nN\u2192\u221eP[|XN\u2212\u00b5|> \u03f5] = 0.\n\u0088Central Limit Theorem : The Central Limit Theorem says that the probability of\nXNcan be approximated by the probability of a Gaussian. You can also think of\nthis as saying that the PDF of XNis converging to a distribution that can be well\napproximated by a bell-shaped Gaussian. If we have many random variables and their\nsum is becoming a Gaussian, we can ignore the individual PDFs and focus on the\nGaussian. Thus it explains why Gaussian is so popular. We will discuss this theorem\nin detail in Section 6.4.\nKey Concept 4: What is the Central Limit Theorem?\nThe CDF of XNcan be approximated by the CDF of a Gaussian, as Ngrows.\n323", "339": "CHAPTER 6. SAMPLE STATISTICS\n6.1 Moment-Generating and Characteristic Functions\nConsider two independent random variables XandYwith PDFs fX(x) and fY(y), respec-\ntively. Let Z=X+Ybe the sum of the two random variables. We know from Chapter 5\nthat the PDF of Z,fZ, is the convolution of fXandfY. However, we think you will agree\nthat convolutions are not easy to compute. Especially when the sum involves more random\nvariables, computing the convolution would be tedious. So how should we proceed in this\ncase? One approach is to use some kind of \u201cfrequency domain\u201d method that transforms\nthe PDFs to another domain and then perform multiplication instead of the convolution\nto make the calculations easy or at least easier. The moment-generating functions and the\ncharacteristic functions are designed for this purpose.\n6.1.1 Moment-generating function\nDefinition 6.1. For any random variable X, themoment-generating function (MGF)\nMX(s)is\nMX(s) =E\u0002\nesX\u0003\n. (6.1)\nThe definition says that the moment-generating function (MGF) is the expectation of the\nrandom variable taken to the power esXfor some s. Effectively, it is the expectation of a\nfunction of random variables. The meaning of the expectation can be seen by writing out\nthe definition. For the discrete case, the MGF is\nMX(s) =X\nx\u2208\u2126esxpX(x), (6.2)\nwhereas in the continuous case, the MGF is\nMX(s) =Z\u221e\n\u2212\u221eesxfX(x)dx. (6.3)\nThe continuous case should remind us of the definition of a Laplace transform. For any\nfunction f(t), the Laplace transform is\nL[f](s) =Z\u221e\n\u2212\u221ef(t)estdt.\nFrom this perspective, we can interpret the MGF as the Laplace transform of the PDF.\nThe argument sof the output can be regarded as the coordinate in the Laplace space. If\ns=\u2212j\u03c9, then MX(j\u03c9) becomes the Fourier transform of the PDF.\nExample 6.1 . Consider a random variable Xwith three states 0 ,1,2 and with prob-\nability masses2\n6,3\n6,1\n6respectively. Find the MGF.\n324", "340": "6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS\nSolution . The moment-generating function is\nMX(s) =E[esX] =es0\u00b72\n6+es1\u00b73\n6+es2\u00b71\n6\n=1\n3+es\n2+e2s\n6.\nPractice Exercise 6.1 . Find the MGF for a Poisson random variable.\nSolution . The MGF of Poisson random variable can be found as\nMX(s) =E[esX] =\u221eX\nx=0esx\u03bbxe\u2212\u03bb\nx!=\u221eX\nx=0(\u03bbes)x\nx!e\u2212\u03bb=e\u03bbese\u2212\u03bb.\nPractice Exercise 6.2 . Find the MGF for an exponential random variable.\nSolution . The MGF of an exponential random variable can be found as\nMX(s) =E[esX] =Z\u221e\n0esx\u03bbe\u2212\u03bbxdx=Z\u221e\n0\u03bbe(s\u2212\u03bb)xdx=\u03bb\n\u03bb\u2212s, if\u03bb > s.\nWhy are moment-generating functions so called? The following theorem reveals the\nreason.\nTheorem 6.1. The MGF has the properties that\n\u0088MX(0) = 1 ,\n\u0088d\ndsMX(s)|s=0=E[X],d2\nds2MX(s)|s=0=E[X2],\n\u0088dk\ndskMX(s)|s=0=E[Xk], for any positive integer k.\nProof . The first property can be proved by noting that\nMX(0) =E[e0X] =E[1] = 1 .\nThe third property holds because\ndk\ndskMX(s) =Z\u221e\n\u2212\u221edk\ndskesxfX(x)dx=Z\u221e\n\u2212\u221exkesxfX(x)dx.\nSetting s= 0 yields\ndk\ndskMX(s)|s=0=Z\u221e\n\u2212\u221exkfX(x)dx=E[Xk].\nThe second property is a special case of the third property.\n\u25a1\n325", "341": "CHAPTER 6. SAMPLE STATISTICS\nThe theorem tells us that if we take the derivative of the MGF and set s= 0, we will\nobtain the moment. The order of the moment depends on the order of the derivative. As a\nresult, the MGF can \u201cgenerate moments\u201d by taking derivatives. This happens because of\nthe exponential function esx. Sinced\ndsesx=xesx, the variable xappears whenever we take\nthe derivative.\nPractice Exercise 6.3 . Let Xbe a Bernoulli random variable with parameter p.\nFind the first two moments using MGF.\nSolution . The MGF of a Bernoulli random variable is\nMX(s) =E[esX]\n=es0pX(0) + es1pX(1)\n= (1)(1 \u2212p) + (es)(p)\n= 1\u2212p+pes.\nThe first and the second moment, using the derivative approach, are\nE[X] =d\ndsMX(s)\f\f\f\f\ns=0=d\nds\u0012\n1\u2212p+pes\u0013\f\f\f\f\ns=0=pes\f\f\f\f\ns=0=p,\nE[X2] =d2\nds2MX(s)\f\f\f\f\ns=0=d2\nds2\u0012\n1\u2212p+pes\u0013\f\f\f\f\ns=0=pes\f\f\f\f\ns=0=p.\nTo facilitate our discussions of MGF, we summarize a few MGFs in the table below.\nDistribution PMF / PDF E[X] Var[ X] MX(s)\nBernoulli pX(1) = pandpX(0) = 1 \u2212p p p (1\u2212p) 1 \u2212p+pes\nBinomial pX(k) =\u0000n\nk\u0001\npk(1\u2212p)n\u2212knp np (1\u2212p) (1 \u2212p+pes)n\nGeometric pX(k) =p(1\u2212p)k\u221211\np1\u2212p\np2pes\n1\u2212(1\u2212p)es\nPoisson pX(k) =\u03bbke\u2212\u03bb\nk!\u03bb \u03bb e\u03bb(es\u22121)\nGaussian fX(x) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5)2\n2\u03c32\u001b\n\u00b5 \u03c32exp\u001a\n\u00b5s+\u03c32s2\n2\u001b\nExponential fX(x) =\u03bbexp{\u2212\u03bbx}1\n\u03bb1\n\u03bb2\u03bb\n\u03bb\u2212s\nUniform fX(x) =1\nb\u2212aa+b\n2(b\u2212a)2\n12esb\u2212esa\ns(b\u2212a)\nTable 6.1: Moment-generating functions of common random variables.\n326", "342": "6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS\n6.1.2 Sum of independent variables via MGF\nMGFs are most useful when analyzing the PDF of a sum of two random variables. The\nfollowing theorem highlights the result.\nTheorem 6.2. LetXandYbe independent random variables. Let Z=X+Y. Then\nMZ(s) =MX(s)MY(s). (6.4)\nProof . By the definition of MGF, we have that\nMZ(s) =Eh\nes(X+Y)i(a)=E\u0002\nesX\u0003\nE\u0002\nesY\u0003\n=MX(s)MY(s),\nwhere (a) is valid because XandYare independent.\n\u25a1\nCorollary 6.1. Consider independent random variables X1, . . . , X N. Let Z=PN\nn=1Xn\nbe the sum of random variables. Then the MGF of Zis\nMZ(s) =NY\nn=1MXn(s). (6.5)\nIf these random variables are further assumed to be identically distributed, the MGF is\nMZ(s) = (MX1(s))N. (6.6)\nProof . This follows immediately from the previous theorem:\nMZ(s) =E[es(X1+\u00b7\u00b7\u00b7+XN)] =E[esX1]E[esX2]\u00b7\u00b7\u00b7E[esXN] =NY\nn=1MXn(s).\nIf the random variables X1, . . . , X Nare i.i.d., then the product simplifies to\nNY\nn=1MXn(s) =NY\nn=1MX1(s) = (MX1(s))N.\n\u25a1\nTheorem 6.3 (Sum of Bernoulli = binomial ).LetX1, . . . , XNbe a sequence of\ni.i.d. Bernoulli random variables with parameter p. Let Z=X1+\u00b7\u00b7\u00b7+XNbe the sum.\nThen Zis a binomial random variable with parameters (N, p).\nProof . Let us consider a sequence of i.i.d. Bernoulli random variables Xn\u223cBernoulli( p)\nforn= 1, . . . , N . Let Z=X1+\u00b7\u00b7\u00b7+XN. The moment-generating function of Zis\nMZ(s) =E[es(X1+\u00b7\u00b7\u00b7+XN)] =NY\nn=1E[esXn]\n=NY\nn=1\u0000\npes1+ (1\u2212p)es0\u0001\n= (pes+ (1\u2212p))N.\n327", "343": "CHAPTER 6. SAMPLE STATISTICS\nNow, let us check the moment-generating function of a binomial random variable: If Z\u223c\nBinomial( N, p), then\nMZ(s) =E[esZ] =NX\nn=0esk\u0012N\nk\u0013\npk(1\u2212p)N\u2212k\n=NX\nn=0\u0012N\nk\u0013\n(pes)k(1\u2212p)N\u2212k= (pes+ (1\u2212p))N,\nwhere the last equality holds becausePN\nn=0\u0000N\nk\u0001\nakbN\u2212k= (a+b)N. Therefore, the two\nmoment-generating functions are identical. \u25a1\nTheorem 6.4 (Sum of binomial = binomial ).LetX1, . . . , XNbe a sequence of\ni.i.d. binomial random variables with parameters (n, p). Let Z=X1+\u00b7\u00b7\u00b7+XNbe the\nsum. Then Zis a binomial random variable with parameters (Nn, p ).\nProof . The MGF of a binomial random variable is\nMXi(s) = (pes+ (1\u2212p))n.\nIf we have Nof these random variables, then Z=X1+\u00b7\u00b7\u00b7+XNwill have the MGF\nMZ(s) =NY\ni=1MXi(s) = (pes+ (1\u2212p))Nn.\nNote that this is just the MGF of another binomial random variable with parameter ( Nn, p ).\n\u25a1\nTheorem 6.5 (Sum of Poisson = Poisson ).LetX1, . . . , XNbe a sequence of\ni.i.d. Poisson random variables with parameter \u03bb. Let Z=X1+\u00b7\u00b7\u00b7+XNbe the sum.\nThen Zis a Poisson random variable with parameters N\u03bb.\nProof . The MGF of a Poisson random variable is\nMX(s) =E[esX] =\u221eX\nk=0esk\u03bbk\nk!e\u2212\u03bb\n=e\u2212\u03bb\u221eX\nk=0(\u03bbes)k\nk!\n=e\u2212\u03bbe\u03bbes=e\u03bb(es\u22121).\nAssume that we have a sum of Ni.i.d. Poisson random variables. Then, by the main theorem,\nwe have that\nMZ(s) = [MX(s)]N=eN\u03bb(es\u22121).\nTherefore, the resulting random variable Zis a Poisson with parameter N\u03bb. \u25a1\n328", "344": "6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS\nTheorem 6.6 (Sum of Gaussian = Gaussian ).LetX1, . . . , XNbe a sequence of\nindependent Gaussian random variables with parameters (\u00b51, \u03c32\n1), . . . , (\u00b5N, \u03c32\nN). Let\nZ=X1+\u00b7\u00b7\u00b7+XNbe the sum. Then Zis a Gaussian random variable:\nZ=Gaussian\u0012NX\nn=1\u00b5n,NX\nn=1\u03c32\nn\u0013\n. (6.7)\nProof . We skip the proof of the MGF of a Gaussian. It can be shown that\nMX(s) = exp\u001a\n\u00b5s+\u03c32s2\n2\u001b\n.\nWhen we have a sequence of Gaussian random variables, then\nMZ(s) =E[es(X1+\u00b7\u00b7\u00b7+XN)]\n=MX1(s)\u00b7\u00b7\u00b7MXN(s)\n=\u0012\nexp\u001a\n\u00b51s+\u03c32\n1s2\n2\u001b\u0013\n\u00b7\u00b7\u00b7\u0012\nexp\u001a\n\u00b5Ns+\u03c32\nNs2\n2\u001b\u0013\n= exp( NX\nn=1\u00b5n!\ns+ NX\nn=1\u03c32\nn!\ns2\n2)\n.\nTherefore, the resulting random variable Zis also a Gaussian. The mean and variance of Z\narePN\nn=1\u00b5nandPN\nn=1\u03c32\nn, respectively.\n\u25a1\n6.1.3 Characteristic functions\nMoment-generating functions are the Laplace transforms of the PDFs. However, since the\nLaplace transform is defined on the entire right half-plane, not all PDFs can be transformed.\nOne way to mitigate this problem is to restrict sto the imaginary axis, s=j\u03c9. This will\ngive us the characteristic function .\nDefinition 6.2 (Usual definition) .Thecharacteristic function of a random variable\nXis\n\u03a6X(j\u03c9) =E[ej\u03c9X]. (6.8)\nHowever, we note that since \u03c9can take any value in ( \u2212\u221e,\u221e), it does not matter if we\nconsider E[e\u2212j\u03c9X] orE[ej\u03c9X]. This leads to the following equivalent definition of the char-\nacteristic function:\nDefinition 6.3 (Alternative definition (for this book)) .Thecharacteristic function\nof a random variable Xis\n\u03a6X(j\u03c9) =E[e\u2212j\u03c9X]. (6.9)\n329", "345": "CHAPTER 6. SAMPLE STATISTICS\nIf we follow this definition, we see that the characteristic function can be written as\n\u03a6X(j\u03c9) =E[e\u2212j\u03c9X] =Z\u221e\n\u2212\u221ee\u2212j\u03c9xfX(x)dx. (6.10)\nThis is exactly the Fourier transform of the PDF. The reason for introducing this alternative\ncharacteristic function is that E[e\u2212j\u03c9X] is the Fourier transform of fX(x) butE[ej\u03c9X] is the\ninverse Fourier transform of fX(x). The former is more convenient (in terms of notation)\nfor students who have taken a course in signals and systems. However, we should stress that\nthe usual way of defining the characteristic function is E[ej\u03c9X].\nA list of common Fourier transforms is shown in the table below. Additional identities\ncan be found in standard signals and systems textbooks.\nFourier Transforms\nf(t)\u2190\u2192F(\u03c9) f(t)\u2190\u2192F(\u03c9)\n1.e\u2212atu(t)\u2190\u21921\na+j\u03c9,a >0 10. sinc2(Wt\n2)\u2190\u21922\u03c0\nW\u2206(\u03c9\n2W)\n2.eatu(\u2212t)\u2190\u21921\na\u2212j\u03c9,a >0 11. e\u2212atsin(\u03c90t)u(t)\u2190\u2192\u03c90\n(a+j\u03c9)2+\u03c92\n0\n3.e\u2212a|t|\u2190\u21922a\na2+\u03c92,a >0 12. e\u2212atcos(\u03c90t)u(t)\u2190\u2192a+j\u03c9\n(a+j\u03c9)2+\u03c92\n0\n4.a2\na2+t2\u2190\u2192\u03c0ae\u2212a|\u03c9|,a >0 13. e\u2212t2\n2\u03c32\u2190\u2192\u221a\n2\u03c0\u03c3e\u2212\u03c32\u03c92\n2\n5.te\u2212atu(t)\u2190\u21921\n(a+j\u03c9)2,a >0 14. \u03b4(t)\u2190\u21921\n6.tne\u2212atu(t)\u2190\u2192n!\n(a+j\u03c9)n+1,a >0 15. 1 \u2190\u21922\u03c0\u03b4(\u03c9)\n7.rect(t\n\u03c4)\u2190\u2192\u03c4sinc(\u03c9\u03c4\n2) 16. \u03b4(t\u2212t0)\u2190\u2192e\u2212j\u03c9t0\n8. sinc( Wt)\u2190\u2192\u03c0\nWrect(w\n2W) 17. ej\u03c90t\u2190\u21922\u03c0\u03b4(\u03c9\u2212\u03c90)\n9.\u2206(t\n\u03c4)\u2190\u2192\u03c4\n2sinc2(\u03c9\u03c4\n4) 18. f(t)ej\u03c90t\u2190\u2192F(\u03c9\u2212\u03c90)\nTable 6.2: Fourier transform pairs of commonly used functions.\nExample 6.2 . Let Xbe a random variable with PDF fX(x) =\u03bbe\u2212\u03bbxforx\u22650. Find\nthe characteristic function.\nSolution . The Fourier transform pair is\n\u03bbe\u2212\u03bbx\u2212\u2192\u03bb\u00b7 F\u001a\ne\u2212\u03bbx\u001b\n=\u03bb\u00b71\n\u03bb+j\u03c9.\nTherefore, the characteristic function is \u03a6 X(j\u03c9) =\u03bb\n\u03bb+j\u03c9.\n330", "346": "6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS\nExample 6.3 . Let XandYbe independent, and let\nfX(x) =(\n\u03bbe\u2212\u03bbx, x \u22650,\n0, x < 0,fY(y) =(\n\u03bbe\u2212\u03bby, y \u22650,\n0, y < 0.\nFind the PDF of Z=X+Y.\nSolution . The characteristic function of XandYcan be found from the Fourier table:\n\u03a6X(j\u03c9) =\u03bb\n\u03bb+j\u03c9and \u03a6 Y(j\u03c9) =\u03bb\n\u03bb+j\u03c9.\nTherefore, the characteristic function of Zis\n\u03a6Z(j\u03c9) = \u03a6 X(j\u03c9)\u03a6Y(j\u03c9) =\u03bb2\n(\u03bb+j\u03c9)2.\nBy inverse Fourier transform, we have that\nfZ(z) =F\u22121\u001a\u03bb2\n(\u03bb+j\u03c9)2\u001b\n=\u03bb2ze\u2212\u03bbz, z\u22650.\nWhy \u03a6X(j\u03c9)but not MX(s)?As we said, the function is not always defined. Recall\nthat the expectation E[X] exists only when fX(x) is absolutely integrable, or E[|X|]<\u221e.\nFor a characteristic function, the expectation is valid because E[|ej\u03c9X|] =E[1] = 1. However,\nfor a function, E[|esX|] could be unbounded. To see a counterexample, we consider the\nCauchy distribution.\nTheorem 6.7. Consider the Cauchy distribution with PDF\nfX(x) =1\n\u03c0(x2+ 1). (6.11)\nThe MGF of Xis undefined but the characteristic function is well defined.\nProof . The MGF is\nMX(s) =Z\u221e\n\u2212\u221eesx 1\n\u03c0(x2+ 1)dx\u2265Z\u221e\n1esx 1\n\u03c0(x2+ 1)dx\n\u2265Z\u221e\n1(sx)3\n6\u03c0(x2+ 1)dx, because esx\u2265(sx)3\n6\n\u2265Z\u221e\n1(sx)3\n6\u03c0(2x2)dx=s3\n12\u03c0Z\u221e\n1x dx =\u221e.\nTherefore, the MGF is undefined. On the other hand, by the Fourier table we know that\n\u03a6X(j\u03c9) =F\u001a1\n\u03c0(x2+ 1)\u001b\n=e\u2212|\u03c9|.\n\u25a1\n331", "347": "CHAPTER 6. SAMPLE STATISTICS\nExample 6.4 . Let X0, X1, . . .be a sequence of independent random variables with\nPDF\nfXk(x) =ak\n\u03c0(a2\nk+x2), a k=1\n2k+1fork= 0,1, . . . .\nFind the PDF of Y, where Y=P\u221e\nk=0Xk.\nSolution . From the Fourier transform table, we know that\nak\n\u03c0(a2\nk+x2)=1\nak\u03c0\u00b7a2\nk\n(a2\nk+x2)F\u2190\u21921\nak\u03c0\u00b7\u03c0ake\u2212ak|\u03c9|=e\u2212ak|\u03c9|.\nThe characteristic function of Yis\n\u03a6Y(j\u03c9) =\u221eY\nk=0\u03a6Xk(j\u03c9) = exp(\n\u2212|\u03c9|\u221eX\nk=0ak)\n.\nSinceP\u221e\nk=0ak=P\u221e\nk=01\n2k+1=1\n2+1\n4+\u00b7\u00b7\u00b7= 1, the characteristic function becomes\n\u03a6Y(j\u03c9) =e\u2212|\u03c9|. The inverse Fourier transform gives us\ne\u2212|\u03c9|=1\n\u03c0\u00b7\u03c0e\u2212|\u03c9|F\u2190\u21921\n\u03c0\u00b71\n1 +x2.\nTherefore the PDF of Yis\nfY(y) =1\n\u03c0(1 +y2).\nExample 6.5 . Two random variables XandYhave the PDFs\nfX(x) =(\ne\u2212x, x \u22650,\n0, x < 0,and fY(y) =(\ne\u2212y, y \u22650,\n0, y < 0.\nFind the PDF of Z= max( X, Y)\u2212min(X, Y).\nSolution . We first show that\nZ= max( X, Y)\u2212min(X, Y) =|X\u2212Y|.\nSuppose X > Y , then max( X, Y) =Xand min( X, Y) =Y. SoZ=X\u2212Y. IfX < Y ,\nthen max( X, Y) =Yand min( X, Y) =X. SoZ=Y\u2212X. Combining the two cases\ngives us Z=|X\u2212Y|. Now, consider the Fourier transform of the PDFs:\ne\u2212xF\u2190\u21921\n1 +j\u03c9.\n332", "348": "6.2. PROBABILITY INEQUALITIES\nLetU=X\u2212Y, and let Z=|U|. The characteristic function is\n\u03a6U(j\u03c9) =E[e\u2212j\u03c9(X\u2212Y)] =E[e\u2212j\u03c9X]E[ej\u03c9Y]\n=1\n1 +j\u03c9\u00b71\n1\u2212j\u03c9=1\n1 +\u03c92F\u2190\u2192 fU(u) =1\n2e\u2212|u|.\nWith the PDF of U, we can find the CDF of Z:\nFZ(z) =P[Z\u2264z] =P[|U| \u2264z]\n=Zz\n\u2212zfU(u)du\n=Zz\n\u2212z1\n2e\u2212|u|du\n= 2Zz\n01\n2e\u2212udu= 1\u2212e\u2212z.\nHence, the PDF is\nfZ(z) =d\ndzFZ(z) =e\u2212z.\nClosing remark . Moment-generating functions and characteristic functions are useful\nmathematical tools. In this section, we have confined our discussion to using them to com-\npute the sum of two random variables. Later sections and chapters will explain further uses\nfor these functions. For example, we use the MGFs when proving Chernoff\u2019s bound and\nproving the Central Limit Theorem.\n6.2 Probability Inequalities\nMoment-generating functions and characteristic functions are powerful tools for handling the\nsum of random variables. We now introduce another set of tools, known as the probability\ninequalities , that allow us to do approximations. We will highlight a few basic probability\ninequalities in this section.\n6.2.1 Union bound\nThe first inequality is the union bound we had introduced when we discussed the axioms of\nprobabilities. The union bound states the following:\nTheorem 6.8 (Union Bound ).LetA1, . . . , A Nbe a collection of sets. Then\nP\"N[\nn=1An#\n\u2264NX\nn=1P[An]. (6.12)\n333", "349": "CHAPTER 6. SAMPLE STATISTICS\nProof . We can prove this by induction. First, if N= 2,\nP[A1\u222aA2] =P[A1] +P[A2]\u2212P[A1\u2229A2]\u2264P[A1] +P[A2],\nbecause P[A1\u2229A2] is a probability and so it must be non-negative. Thus we have proved\nthe base case. Assume that the statement is true for N=K. We need to prove that the\nstatement is also true for N=K+ 1. To this end, we note that\nP\"K+1[\nn=1An#\n=P\" K[\nn=1An!\n\u222aAK+1#\n=P\"K[\nn=1An#\n+P[AK+1]\u2212P\" K[\nn=1An!\n\u2229AK+1#\n\u2264P\"K[\nn=1An#\n+P[AK+1].\nThen, according to our hypothesis for N=K, it follows that\nP\"K[\nn=1An#\n\u2264KX\nn=1P[An].\nPutting these together,\nP\"K+1[\nn=1An#\n\u2264KX\nn=1P[An] +P[AK+1] =K+1X\nn=1P[An].\nTherefore, by the principle of induction, we have proved the statement.\n\u25a1\nRemark . The tightness of the union bound depends on the amount of overlapping between\nthe events A1, . . . , A n, as illustrated in Figure 6.4 . If the events are disjoint, the union bound\nis tight. If the events are overlapping significantly, the union is loose. The idea of the union\nbound is the principle of divide and conquer. We decompose the system into smaller events\nfor a system of nvariables and use the union bound to upper-limit the overall probability. If\nthe probability of each event is small, the union bound tells us that the overall probability\nof the system will also be small.\nFigure 6.4: Conditions under which the union bound is loose or tight. [Left] The union bound is loose\nwhen the sets are overlapping. [Right] The union bound is tight when the sets are (nearly) disjoint.\n334", "350": "6.2. PROBABILITY INEQUALITIES\nExample 6.6 . Let X1, . . . , X Nbe a sequence of i.i.d. random variables with CDF\nFXn(x) and let Z= min( X1, . . . , X N). Find an upper bound on the CDF.\nSolution . Note that Z= min( X1, . . . , X N)\u2264zis equivalent to at least one of the\nXn\u2019s being less than z. Thus, we have that\nZ= min( X1, . . . , X N)\u2264z\u21d4X1\u2264z\u222a \u00b7\u00b7\u00b7 \u222a XN\u2264z.\nSubstituting this result into the CDF,\nFZ(z) =P[Z\u2264z]\n=P[min( X1, . . . , X N)\u2264z]\n=P[X1\u2264z\u222a \u00b7\u00b7\u00b7 \u222a XN\u2264z]\n\u2264P[X1\u2264z] +\u00b7\u00b7\u00b7+P[XN\u2264z]\n=N\u00b7FX(z).\n6.2.2 The Cauchy-Schwarz inequality\nThe second inequality we study here is the Cauchy-Schwarz inequality , which we previously\nmentioned in Chapter 5. We review it for the sake of completeness.\nTheorem 6.9 (Cauchy-Schwarz inequality ).LetXandYbe two random variables.\nThen\nE[XY]2\u2264E[X2]E[Y2]. (6.13)\nProof . Let f(s) =E[(sX+Y)2] for any real s. Then\nf(s) =E[(sX+Y)2]\n=E[s2X2+ 2sXY +Y2]\n=E[X2]s2+ 2E[XY]s+E[Y2].\nThis is a quadratic equation, and f(s)\u22650 for all sbecause E[(sX+Y)2]\u22650.\nRecall that for a quadratic equation \u03d5(x) =ax2+bx+c, the function \u03d5(x)\u22650 if and\nonly if b2\u22124ac\u22640. Substituting this result into our problem, we show that\n(2E[XY])2\u22124E[X2]E[Y2]\u22640.\nThis implies that\nE[XY]2\u2264E[X2]E[Y2],\nwhich completes the proof.\n\u25a1\nRemark . As shown in Chapter 5, the Cauchy-Schwarz inequality is useful in analyzing\nE[XY]. For example, we can use the Cauchy-Schwarz inequality to prove that the correlation\ncoefficient \u03c1is bounded between \u22121 and 1.\n335", "351": "CHAPTER 6. SAMPLE STATISTICS\n6.2.3 Jensen\u2019s inequality\nOur next inequality is Jensen\u2019s inequality . To motivate the inequality, we recall that\nVar[X] =E[X2]\u2212E[X]2.\nSince Var[ X]\u22650 for any X, it follows that\nE[X2]|{z}\n=E[g(X)]\u2265E[X]2\n|{z}\n=g(E[X]). (6.14)\nJensen\u2019s inequality is a generalization of the above result by recognizing that the inequality\ndoes not only hold for the function g(X) =X2but also for any convex function g. The\ntheorem is stated as follows:\nTheorem 6.10 (Jensen\u2019s inequality ).LetXbe a random variable, and let g:R\u2192R\nbe aconvex function. Then\nE[g(X)]\u2265g(E[X]). (6.15)\nIf the function gisconcave , then the inequality sign is flipped: E[g(X)]\u2264g(E[X]). The\nway to remember this result is to remember that E[X2]\u2212E[X]2= Var[ X]\u22650.\nNow, what is a convex function? Informally, a function gisconvex if, when we pick any\ntwo points on the function and connect them with a straight line, the line will be above the\nfunction for that segment. This definition is illustrated in Figure 6.5 . Consider an interval\n[x, y], and the line segment connecting g(x) and g(y). If the function g(\u00b7) is convex, then\nthe entire line segment should be above the curve.\nFigure 6.5: Illustration of a convex function, a concave function, and a function that is neither convex\nnor concave.\nThe definition of a convex function essentially follows the above picture:\nDefinition 6.4. A function gisconvex if\ng(\u03bbx+ (1\u2212\u03bb)y)\u2264\u03bbg(x) + (1 \u2212\u03bb)g(y), (6.16)\nfor any 0\u2264\u03bb\u22641.\nHere \u03bbrepresents a \u201csweeping\u201d constant that goes from xtoy. When \u03bb= 1 then \u03bbx+(1\u2212\u03bb)y\nsimplifies to x, and when \u03bb= 0 then \u03bbx+ (1\u2212\u03bb)ysimplifies to y.\n336", "352": "6.2. PROBABILITY INEQUALITIES\nThe definition is easy to understand. The left-hand side g(\u03bbx+(1\u2212\u03bb)y) is the function\nevaluated at any points in the interval [ x, y]. The right-hand side is the red straight line we\nplotted in Figure 6.5 . It connects the two points g(x) and g(y). Convexity means that the\nred line is entirely above the curve.\nFor twice-differentiable 1D functions, convexity can be described by the curvature of\nthe function. A function is convex if\ng\u2032\u2032(x)\u22650. (6.17)\nThis is self-explanatory because if the curvature is non-negative for all x, then the slope of\nghas to keep increasing.\nExample 6.7 . The following functions are convex or concave:\n\u0088g(x) = log xis concave, because g\u2032(x) =1\nxandg\u2032\u2032(x) =\u22121\nx2\u22640 for all x.\n\u0088g(x) =x2is convex, because g\u2032(x) = 2 xandg\u2032\u2032(x) = 2 is positive.\n\u0088g(x) =e\u2212xis convex, because g\u2032(x) =\u2212e\u2212xandg\u2032\u2032(x) =e\u2212x\u22650.\nWhy is Jensen inequality valid for a convex function? Consider the illustration in\nFigure 6.6 . Suppose we have a random variable Xtaking some PDF fX(x). There is a\nconvex function g(\u00b7) that maps the random variable Xtog(X). Since g(\u00b7) is convex, a PDF\nlike the one we see in Figure 6.6 will become skewed. (You can map the left tail to the new\nleft tail, the peak to the new peak, and the right tail to the new right tail.) As you can see\nfrom the figure, the new random variable g(X) has a mean E[g(X)] that is greater than the\nmapped old mean g(E[X]). Jensen\u2019s inequality captures this phenomenon by stating that\nE[g(X)]\u2265g(E[X]) for any convex function g(\u00b7).\nFigure 6.6: Jensen\u2019s inequality states that if there is a convex function g(\u00b7)that maps a random variable\nXto a new random variable g(X), the new mean E[g(X)]will be greater than the mapped old mean\ng(E[X]).\nProving Jensen\u2019s inequality is straightforward for a two-state discrete random variable.\nDefine a random variable Xwith states xandy. The probabilities for these two states are\nP[X=x] =\u03bbandP[X=y] = 1\u2212\u03bb. Then\nE[X] =X\nx\u2032\u2208{x,y}x\u2032pX(x\u2032) =\u03bbx+ (1\u2212\u03bb)y.\n337", "353": "CHAPTER 6. SAMPLE STATISTICS\nNow, let g(\u00b7) be a convex function. We know from the expectation that\nE[g(X)] =X\nx\u2032\u2208{x,y}g(x\u2032)pX(x\u2032) =g(x)\u03bb+ (1\u2212\u03bb)g(y).\nBy convexity of the function g(\u00b7), it follows that\ng(\u03bbx+ (1\u2212\u03bb)y)| {z }\n=g(E[X])\u2264\u03bbf(x) + (1 \u2212\u03bb)g(y)| {z }\n=E[g(X)],\nwhere in the underbrace we substitute the definitions using the expectation. Therefore,\nfor any two-state discrete random variables, the proof of Jensen\u2019s inequality follows directly\nfrom the convexity. If the discrete random variable takes more than two states, we can prove\nthe theorem by induction. For continuous random variables, we can prove the theorem using\nthe following approach.\nYou may skip the proof of Jensen\u2019s inequality if this is your first time reading the book.\nHere we present an alternative proof of Jensen\u2019s inequality that does not require proof\nby induction. The idea is to recognize that if the function gis convex we can find a tangent\nlineL(X) =aX+bat the point E[X] that is uniformly lower than g(X), i.e., g(X)\u2265L(X)\nfor all X. Then we can prove the result with a simple geometric argument. Figure 6.7\nillustrates this idea.\nFigure 6.7: Geometric illustration of the proof of Jensen\u2019s inequality. Suppose g(\u00b7)is a convex function.\nFor any point Xong(\u00b7), we can find a tangent line L(X) =aX+b. Since the black curve is always\nabove the tangent, it follows that E[g(X)]\u2265E[L(X)]for any X. Also, note that at a particular point\nE[X], the black curve and the red line touch, and so we have L(E[X]) =g(E[X]).\nProof of Jensen\u2019s inequality . Consider L(X) as defined above. Since gis convex, g(X)\u2265\nL(X) for all X. Therefore,\nE[g(X)]\u2265E[L(X)]\n=E[aX+b]\n=aE[X] +b\n=L(E[X]) =g(E[X]),\nwhere the last equality holds because Lis a tangent line to gwhere they meet at E[X].\n\u25a1\n338", "354": "6.2. PROBABILITY INEQUALITIES\nWhat are ( a, b) in the proof? By Taylor expansion,\ng(X)\u2248g(E[X]) +g\u2032(E[X])(X\u2212E[X])\ndef=L(X).\nTherefore, if we want to be precise, then a=g\u2032(E[X]) and b=g(E[X])\u2212g\u2032(E[X])E[X].\nThe end of the proof.\nExample 6.8 . By Jensen\u2019s inequality, we have that\n(a)E[X2]\u2265E[X]2, because g(x) =x2is convex.\n(b)E\u00021\nX\u0003\n\u22651\nE[X], because g(x) =1\nxis convex.\n(c)E[logX]\u2264logE[X], because g(x) = log xis concave.\n6.2.4 Markov\u2019s inequality\nOur next inequality, Markov\u2019s inequality , is an elementary inequality that links probability\nand expectation.\nTheorem 6.11 (Markov\u2019s inequality ).LetX\u22650be a non-negative random variable.\nThen, for any \u03b5 >0, we have\nP[X\u2265\u03b5]\u2264E[X]\n\u03b5. (6.18)\nMarkov\u2019s inequality concerns the tailof the random variable. As illustrated in Figure 6.8 ,\nP[X\u2265\u03b5] measures the probability that the random variable takes a value greater than \u03b5.\nMarkov\u2019s inequality asserts that this probability P[X\u2265\u03b5] is upper-bounded by the ratio\nE[X]/\u03b5. This result is useful because it relates the probability and the expectation. In many\nproblems the probability P[X\u2265\u03b5] could be difficult to evaluate if the PDF is complicated.\nThe expectation, on the other hand, is usually easier to evaluate.\nProof . Consider \u03b5P[X\u2265\u03b5]. It follows that\n\u03b5P[X\u2265\u03b5] =Z\u221e\n\u03b5\u03b5 fX(x)dx\u2264Z\u221e\n\u03b5xfX(x)dx,\nwhere the inequality is valid because for any x\u2265\u03b5the integrand (which is non-negative)\nwill always increase (or at least not decrease). It then follows that\nZ\u221e\n\u03b5xfX(x)dx\u2264Z\u221e\n0xfX(x)dx=E[X].\u25a1\nA pictorial interpretation of Markov\u2019s inequality is shown in Figure 6.9 . For X > 0, it\nis not difficult to show that E[X] =R\u221e\n01\u2212FX(x)dx. Then, in the CDF plot, we see that\n\u03b5\u00b7P[X\u2265\u03b5] is a rectangle covering the top left corner. This area is clearly smaller than the\narea covered by the function 1 \u2212FX(x).\n339", "355": "CHAPTER 6. SAMPLE STATISTICS\nFigure 6.8: Markov\u2019s inequality provides an upper bound to the tail of a random variable. The inequality\nstates that the probability P[X\u2265\u03b5]is upper bounded by the ratio E[X]/\u03b5.\nFigure 6.9: The proof of Markov\u2019s inequality follows from the fact that \u03b5\u00b7P[X\u2265\u03b5]occupies the\ntop left corner marked by the yellow rectangle. The expectation is the area above the CDF so that\nE[X] =R\u221e\n01\u2212FX(x)dx. Since the yellow rectangle is smaller than the orange shaded area, it follows\nthat\u03b5\u00b7P[X\u2265\u03b5]\u2264E[X], which is Markov\u2019s inequality.\nPractice Exercise 6.4 . Prove that if X > 0, then E[X] =R\u221e\n01\u2212FX(x)dx.\nSolution . We start from the right-hand side:\nZ\u221e\n01\u2212FX(x)dx=Z\u221e\n01\u2212P[X\u2264x]dx\n=Z\u221e\n0P[X\u2265x]dx\n=Z\u221e\n0Z\u221e\nxfX(t)dt dx\n=Z\u221e\n0Zt\n0fX(t)dx dt\n=Z\u221e\n0tfX(t)dt=E[X].\nThe change in the integration order is illustrated below.\n340", "356": "6.2. PROBABILITY INEQUALITIES\nHow tight is Markov\u2019s inequality? It is possible to create a random variable such that\nthe equality is met (see Exercise 6.14). However, in general, the estimate provided by the\nupper bound is not tight. Here is an example.\nPractice Exercise 6.5 . Let X\u223cUniform(0 ,4). Verify Markov\u2019s inequality for P[X\u2265\n2],P[X\u22653] and P[X\u22654].\nSolution . First, we observe that E[X] = 2. Then\nP[X\u22652] = 0 .5,E[X]\n2= 1,\nP[X\u22653] = 0 .25,E[X]\n3= 0.67,\nP[X\u22654] = 0 ,E[X]\n4= 0.5.\nTherefore, although the upper bounds are all valid, they are very loose.\nIf Markov\u2019s inequality is not tight, why is it useful? It turns out that while Markov\u2019s\ninequality is not tight, its variations can be powerful. We will come back to this point when\nwe discuss Chernoff\u2019s bound.\n6.2.5 Chebyshev\u2019s inequality\nThe next inequality is a simple extension of Markov\u2019s inequality. The result is known as\nChebyshev\u2019s inequality.\nTheorem 6.12 (Chebyshev\u2019s inequality ).LetXbe a random variable with mean \u00b5.\nThen for any \u03b5 >0we have\nP[|X\u2212\u00b5| \u2265\u03b5]\u2264Var[X]\n\u03b52. (6.19)\nThe tail measured by Chebyshev\u2019s inequality is illustrated in Figure 6.10 . Since the\nevent |X\u2212\u00b5| \u2265\u03b5involves an absolute value, the probability measures the two-sided tail.\nChebyshev\u2019s inequality states that this tail probability is upper-bounded by Var[ X]/\u03b52.\n341", "357": "CHAPTER 6. SAMPLE STATISTICS\nFigure 6.10: Chebyshev\u2019s inequality states that the two-sided tail probability P[|X\u2212\u00b5| \u2265\u03b5]is upper-\nbounded by Var[X]/\u03b52\nProof . We apply Markov\u2019s inequality to show that\nP[|X\u2212\u00b5| \u2265\u03b5] =P[(X\u2212\u00b5)2\u2265\u03b52]\n\u2264E[(X\u2212\u00b5)2]\n\u03b52=Var[X]\n\u03b52.\n\u25a1\nAn alternative form of Chebyshev\u2019s inequality is obtained by letting \u03b5=k\u03c3. In this\ncase, we have\nP[|X\u2212\u00b5| \u2265k\u03c3]\u2264\u03c32\nk2\u03c32=1\nk2.\nTherefore, if a random variable is ktimes the standard deviation away from the mean, then\nthe probability bound drops to 1 /k2.\nPractice Exercise 6.6 . Let X\u223cUniform(0 ,4). Find the bound of Chebyshev\u2019s\ninequality for the probability P[|X\u2212\u00b5| \u22651].\nSolution . Note that E[X] = 2 and \u03c32= 42/12 = 4 /3. Therefore, we have\nP[|X\u2212\u00b5| \u22651]\u2264\u03c32\n\u03b52=4\n3,\nwhich is a valid upper bound, but quite conservative.\nPractice Exercise 6.7 . Let X\u223cExponential(1). Find the bound of Chebyshev\u2019s\ninequality for the probability P[X\u2265\u03b5].\nSolution . Note that E[X] = 1 and \u03c32= 1. Thus we have\nP[X\u2265\u03b5] =P[X\u2212\u00b5\u2265\u03b5\u2212\u00b5]\u2264P[|X\u2212\u00b5| \u2265\u03b5\u2212\u00b5]\n\u2264\u03c32\n(\u03b5\u2212\u00b5)2=1\n(\u03b5\u22121)2.\n342", "358": "6.2. PROBABILITY INEQUALITIES\nWe can compare this with the exact probability, which is\nP[X\u2265\u03b5] = 1\u2212FX(\u03b5) =e\u2212\u03b5.\nAgain, the estimate given by Chebyshev\u2019s inequality is acceptable but too conservative.\nCorollary 6.2. LetX1, . . . , X Nbe i.i.d. random variables with mean E[Xn] =\u00b5and\nvariance Var[Xn] =\u03c32. Let XN=1\nNPN\nn=1Xnbe the sample mean. Then\nP\u0014\f\fXN\u2212\u00b5\f\f> \u03f5\u0015\n\u2264\u03c32\nN\u03f52. (6.20)\nProof . We can first show that E[XN] =\u00b5and Var[ XN] satisfies\nVar[XN] =1\nN2NX\nn=1Var[Xn] =\u03c32\nN.\nThen by Chebyshev\u2019s inequality,\nP\u0014\f\fXN\u2212\u00b5\f\f> \u03f5\u0015\n\u2264Var[XN]\n\u03f52=\u03c32\nN\u03f52.\n\u25a1\nThe consequence of this corollary is that the upper bound \u03c32N/\u03f52will converge to zero\nasN\u2192 \u221e . Therefore, the probability of getting the event {\f\fXN\u2212\u00b5\f\f> \u03f5}is vanishing.\nIt means that the sample average XNis converging to the true population mean \u00b5, in the\nsense that the probability of failing is shrinking.\n6.2.6 Chernoff\u2019s bound\nWe now introduce a powerful inequality or a set of general procedures that gives us some\nhighly useful inequalities. The idea is named for Herman Chernoff, although it was actually\ndue to his colleague Herman Rubin.\nTheorem 6.13 (Chernoff\u2019s bound ).LetXbe a random variable. Then, for any\n\u03b5\u22650, we have that\nP[X\u2265\u03b5]\u2264e\u2212\u03c6(\u03b5), (6.21)\nwherea\n\u03c6(\u03b5) = max\ns>0\u001a\ns\u03b5\u2212logMX(s)\u001b\n, (6.22)\nandMX(s)is the moment-generating function.\na\u03c6(\u03b5) is called the Fenchel-Legendre dual function of log MX. See references [6-14].\n343", "359": "CHAPTER 6. SAMPLE STATISTICS\nProof . There are two tricks in the proof of Chernoff\u2019s bound. The first trick is a nonlinear\ntransformation. Since esxis an increasing function for any s >0 and x, we have that\nP[X\u2265\u03b5] =P[esX\u2265es\u03b5]\n(a)\n\u2264E[esX]\nes\u03b5\n(b)=e\u2212s\u03b5MX(s)\n=e\u2212s\u03b5+log MX(s),\nwhere the inequality (a) is due to Markov\u2019s inequality. Step (b) just uses the definition of\nMGF that E[esX] =MX(s).\nNow for the second trick. Note that the above result holds for all s. That means it\nmust also hold for the sthat minimizes e\u2212s\u03b5+log MX(s). This implies that\nP[X\u2265\u03b5]\u2264min\ns>0n\ne\u2212s\u03b5+log MX(s)o\n.\nAgain, since exis increasing, the minimizer of the above probability is also the maximizer\nof this function:\n\u03c6(\u03b5) = max\ns>0\u001a\ns\u03b5\u2212logMX(s)\u001b\n.\nThus, we conclude that P[X\u2265\u03b5]\u2264e\u2212\u03c6(\u03b5).\n\u25a1\n6.2.7 Comparing Chernoff and Chebyshev\nLet\u2019s consider an example of how Chernoff\u2019s bound can be useful.\nSuppose that we have a random variable X\u223cGaussian(0 , \u03c32/N). The number Ncan\nbe regarded as the number of samples. For example, if Y1, . . . , Y NareNGaussian random\nvariables with mean 0 and variance \u03c32, then the average X=1\nNPN\nn=1Ynwill have mean\n0 and variance \u03c32/N. Therefore, as Ngrows, the variance of Xwill become smaller and\nsmaller.\nFirst, since the random variable is Gaussian, we can show the following:\nLemma 6.1. LetX\u223cGaussian (0,\u03c32\nN)be a Gaussian random variable. Then, for any\n\u03b5 >0,\nP[X\u2265\u03b5] = 1\u2212\u03a6 \u221a\nN\u03b5\n\u03c3!\n, (6.23)\nwhere \u03a6is the standard Gaussian\u2019s CDF.\nNote that this is the exact result: If you tell me \u03b5,N, and \u03c3, then the probability P[X\u2265\u03b5]\nis exactly the one shown on the right-hand side. No approximation, no randomness.\n344", "360": "6.2. PROBABILITY INEQUALITIES\nProof . Since Xis Gaussian, the probability is\nP[X\u2265\u03b5] =Z\u221e\n\u03b51p\n2\u03c0(\u03c32/N)exp\u001a\n\u2212x2\n2(\u03c32/N)\u001b\ndx\n= 1\u2212Z\u03b5\n\u2212\u221e1p\n2\u03c0(\u03c32/N)exp\u001a\n\u2212x2\n2(\u03c32/N)\u001b\ndx\n= 1\u2212Z \u03b5\u221a\n\u03c32/N\n\u2212\u221e1\u221a\n2\u03c0exp\u001a\n\u2212x2\n2\u001b\ndx\n= 1\u2212\u03a6 \n\u03b5p\n\u03c32/N!\n= 1\u2212\u03a6 \u221a\nN\u03b5\n\u03c3!\n.\n\u25a1\nLet us compute the bound given by Chebyshev\u2019s inequality.\nLemma 6.2. LetX\u223cGaussian (0,\u03c32\nN)be a Gaussian random variable. Then, for any\n\u03b5 >0, Chebyshev\u2019s inequality implies that\nP[X\u2265\u03b5]\u2264\u03c32\nN\u03b52. (6.24)\nProof . We apply Chebyshev\u2019s inequality by assuming that \u00b5= 0:\nP[X\u2265\u03b5] =P[X\u2212\u00b5\u2265\u03b5\u2212\u00b5]\u2264P[|X\u2212\u00b5| \u2265\u03b5\u2212\u00b5]\n\u2264E[(X\u2212\u00b5)2]\n(\u03b5\u2212\u00b5)2=\u03c32\nN\u03b52.\n\u25a1\nWe now compute Chernoff\u2019s bound.\nTheorem 6.14. LetX\u223cGaussian (0,\u03c32\nN)be a Gaussian random variable. Then, for\nany\u03b5 >0, Chernoff\u2019s bound implies that\nP[X\u2265\u03b5]\u2264exp\u001a\n\u2212\u03b52N\n2\u03c32\u001b\n. (6.25)\nProof . The MGF of a zero-mean Gaussian random variable with variance \u03c32/NisMX(s) =\nexpn\n\u03c32s2\n2No\n. Therefore, the function \u03c6can be written as\n\u03c6(\u03b5) = max\ns>0\u001a\ns\u03b5\u2212logMX(s)\u001b\n= max\ns>0\u001a\ns\u03b5\u2212\u03c32s2\n2N\u001b\n.\nTo maximize the function we take the derivative and set it to zero. This yields\nd\nds\u001a\ns\u03b5\u2212\u03c32s2\n2N\u001b\n= 0 \u21d2 s\u2217=N\u03b5\n\u03c32.\n345", "361": "CHAPTER 6. SAMPLE STATISTICS\nNote that this s\u2217is a maximizer because s\u03b5\u2212\u03c32s2\n2Nis a concave function.\nSubstituting s\u2217into\u03c6(\u03b5),\n\u03c6(\u03b5) = max\ns>0\u001as\u03b5\u2212\u03c32s2\n2N\u001b\n=s\u2217\u03b5\u2212\u03c32(s\u2217)2\n2N=\u0012N\u03b5\n\u03c32\u0013\n\u03b5\u2212\u03c32\n2N\u0012N\u03b5\n\u03c32\u00132\n=\u03b52N\n2\u03c32,\nand hence\nP[X\u2265\u03b5]\u2264e\u2212\u03c6(\u03b5)= exp\u001a\n\u2212\u03b52N\n2\u03c32\u001b\n.\n\u25a1\nFigure 6.11 shows the comparison between the exact probability, the bound provided\nby Chebyshev\u2019s inequality, and Chernoff\u2019s bound:\n\u0088Exact :P[X\u2265\u03b5] = 1\u2212\u03a6\u0010\u221a\nN\u03b5\n\u03c3\u0011\n.\n\u0088Chebyshev :P[X\u2265\u03b5]\u2264\u03c32\nN\u03b52,\n\u0088Chernoff :P[X\u2265\u03b5]\u2264expn\n\u2212\u03b52N\n2\u03c32o\n.\nIn this numerical experiment, we set \u03b5= 0.1, and \u03c3= 1. We vary the number N. As we can\nsee from the figure, the bound provided by Chebyshev is valid but very loose. It does not\neven capture the tail as Ngrows. On the other hand, Chernoff\u2019s bound is reasonably tight.\nHowever, one should note that the tightness of Chernoff is only valid for large N. When N\nis small, it is possible to construct random variables such that Chebyshev is tighter.\nThe MATLAB code used to generate this plot is illustrated below.\n% MATLAB code to compare the probability bounds\nepsilon = 0.1;\nsigma = 1;\nN = logspace(1,3.9,50);\np_exact = 1-normcdf(sqrt(N)*epsilon/sigma);\np_cheby = sigma^2./(epsilon^2*N);\np_chern = exp(-epsilon^2*N/(2*sigma^2));\nloglog(N, p_exact, \u2019-o\u2019, \u2019Color\u2019, [1 0.5 0], \u2019LineWidth\u2019, 2); hold on;\nloglog(N, p_cheby, \u2019-\u2019, \u2019Color\u2019, [0.2 0.7 0.1], \u2019LineWidth\u2019, 2);\nloglog(N, p_chern, \u2019-\u2019, \u2019Color\u2019, [0.2 0.0 0.8], \u2019LineWidth\u2019, 2);\nWhat could go wrong if we insist on using Chebyshev\u2019s inequality? Consider the fol-\nlowing example.\nExample 6.9 . Let X\u223cGaussian(0 , \u03c32/N). Suppose that we want the probability to\nbe no greater than a confidence level of \u03b1:\nP[X\u2265\u03b5]\u2264\u03b1.\n346", "362": "6.2. PROBABILITY INEQUALITIES\n101102103\nN10-1510-1010-5100Probability\nExact\nChebyshev\nChernoff\nFigure 6.11: Comparison between Chernoff\u2019s bound and Chebyshev\u2019s bound. The random variable we\nuse is X\u223cGaussian (0, \u03c32/N). As Ngrows, we show the probability bounds predicted by the two\nmethods.\nLet\u03b1= 0.05,\u03b5= 0.1, and \u03c3= 1. Find the Nusing (i) Chebyshev\u2019s inequality and (ii)\nChernoff\u2019s inequality.\nSolution : (i) Chebyshev\u2019s inequality implies that\nP[X\u2265\u03b5]\u2264\u03c32\nN\u03b52\u2264\u03b1,\nwhich means that\nN\u2265\u03c32\n\u03b1\u03b52.\nIf we plug in \u03b1= 0.05,\u03b5= 0.1, and \u03c3= 1, then N\u22652000.\n(ii) For Chernoff\u2019s inequality, it holds that\nP[X\u2265\u03b5]\u2264exp\u001a\n\u2212\u03b52N\n2\u03c32\u001b\n\u2264\u03b1,\nwhich means that\nN\u2265 \u22122\u03c32\n\u03b52log\u03b1\nPlugging in \u03b1= 0.05,\u03b5= 0.1, and \u03c3= 1, we have that N\u2265600. This is more than 3\ntimes smaller than the one predicted by Chebyshev\u2019s inequality. Which one is correct?\nBoth are correct but Chebyshev\u2019s inequality is overly conservative. If N\u2265600 can\nmakeP[X\u2265\u03b5]\u2264\u03b1, then certainly N\u22652000 will work too. However, N\u22652000 is too\nloose.\n347", "363": "CHAPTER 6. SAMPLE STATISTICS\n6.2.8 Hoeffding\u2019s inequality\nChernoff\u2019s bound can be used to derive many powerful inequalities. Here we present an\ninequality for bounded random variables. This result is known as Hoeffding\u2019s inequality.\nTheorem 6.15 (Hoeffding\u2019s inequality ).LetX1, . . . , X Nbe i.i.d. random variables\nwith 0\u2264Xn\u22641, andE[Xn] =\u00b5. Then\nP\u0014\f\fXN\u2212\u00b5\f\f> \u03f5\u0015\n\u22642e\u22122\u03f52N, (6.26)\nwhere XN=1\nNPN\nn=1Xn.\nYou may skip the proof of Hoeffding\u2019s inequality if this is your first time reading the book.\nProof . (Hoeffding\u2019s inequality) First, we show that\nP\u0002\nXN\u2212\u00b5 > \u03f5\u0003\n=P\"\n1\nNNX\nn=1Xn\u2212\u00b5 > \u03f5#\n=P\"NX\nn=1(Xn\u2212\u00b5)> N\u03f5#\n=Ph\nesPN\nn=1(Xn\u2212\u00b5)\u2265es\u03f5Ni\n\u2264E[esPN\nn=1(Xn\u2212\u00b5)]\nes\u03f5N=\u0012E[es(Xn\u2212\u00b5)]\nes\u03f5\u0013N\n.\nLetZn=Xn\u2212\u00b5. Then \u2212\u00b5\u2264Zn\u22641\u2212\u00b5. At this point we use Hoeffding Lemma (see\nbelow) that E[esZn]\u2264es2\n8because b\u2212a= (1\u2212\u00b5)\u2212(\u2212\u00b5) = 1. Thus,\nP\u0002\nXN\u2212\u00b5 > \u03f5\u0003\n\u2264\u0012E[esZn]\nes\u03f5\u0013N\n\u2264 \nes2\n8\nes\u03f5!N\n=es2N\n8\u2212s\u03f5N,\u2200s.\nThis result holds for all s, and thus it holds for the sthat minimizes the right-hand side.\nThis implies that\nP\u0002\nXN\u2212\u00b5 > \u03f5\u0003\n\u2264min\ns\u001a\nexp\u001as2N\n8\u2212s\u03f5N\u001b\u001b\n.\nMinimizing the exponent givesd\ndsn\ns2N\n8\u2212s\u03f5No\n=sN\n4\u2212\u03f5N= 0. Thus we have s= 4\u03f5.\nHence,\nP\u0002\nXN\u2212\u00b5 > \u03f5\u0003\n\u2264exp\u001a(4\u03f5)2N\n8\u2212(4\u03f5)\u03f5N\u001b\n=e\u22122\u03f52N.\nBy symmetry, P\u0002\nXN\u2212\u00b5 <\u2212\u03f5\u0003\n\u2264e\u22122\u03f52N. Then by union bound we show that\nP\u0002\n|XN\u2212\u00b5|> \u03f5\u0003\n=P\u0002\nXN\u2212\u00b5 > \u03f5\u0003\n+P\u0002\nXN\u2212\u00b5 <\u2212\u03f5\u0003\n\u2264e\u22122\u03f52N+e\u22122\u03f52N\n= 2e\u22122\u03f52N.\u25a1\n348", "364": "6.2. PROBABILITY INEQUALITIES\nLemma 6.3 (Hoeffding\u2019s lemma ).Leta\u2264X\u2264bbe a random variable with\nE[X] = 0. Then\nMX(s)def=E\u0002\nesX\u0003\n\u2264exp\u001as2(b\u2212a)2\n8\u001b\n. (6.27)\nProof . Since a\u2264X\u2264b, we can write Xas a linear combination of aandb:\nX=\u03bbb+ (1\u2212\u03bb)a,\nwhere \u03bb=X\u2212a\nb\u2212a. Since exp( \u00b7) is a convex function, it follows that e\u03bbb+(1\u2212\u03bb)a\u2264\u03bbeb+(1\u2212\u03bb)ea.\n(Recall that his convex if h(\u03bbx+ (1\u2212\u03bb)y)\u2264\u03bbh(x) + (1 \u2212\u03bb)h(y).) Therefore, we have\nesX\u2264\u03bbesb+ (1\u2212\u03bb)esa\n=X\u2212a\nb\u2212aesb+b\u2212X\nb\u2212aesa.\nTaking expectations on both sides of the equation,\nE[esX]\u2264\u2212a\nb\u2212aesb+b\nb\u2212aesa,\nbecause E[X] = 0. Now, if we let \u03b8=\u2212a\nb\u2212a, then\n\u2212a\nb\u2212aesb+b\nb\u2212aesa=\u03b8esb+ (1\u2212\u03b8)esa\n=esa\u0010\n1\u2212\u03b8+\u03b8es(b\u2212a)\u0011\n=\u0010\n1\u2212\u03b8+\u03b8es(b\u2212a)\u0011\ne\u2212s\u03b8(b\u2212a)\n= (1\u2212\u03b8+\u03b8eu)e\u2212\u03b8u=e\u2212\u03b8u+log(1 \u2212\u03b8+\u03b8eu),\nwhere we let u=s(b\u2212a). This can be simplified as E[esX]\u2264E[e\u03d5(u)] by defining\n\u03d5(u) =\u2212\u03b8u+ log(1 \u2212\u03b8+\u03b8eu).\nThe final step is to approximate \u03d5(u). To this end, we use Taylor approximation:\n\u03d5(u) =\u03d5(0) + u\u03d5\u2032(0) +u2\n2\u03d5\u2032\u2032(\u03be),\nfor some \u03be\u2208[a, b]. Since \u03d5(0) = 0, \u03d5\u2032(0) = 0, and \u03d5\u2032\u2032(u)\u22641\n4for all u, it follows that\n\u03d5(u) =u2\n2\u03d5\u2032\u2032(\u03be)\u2264u2\n8=s2(b\u2212a)2\n8.\u25a1\nEnd of the proof.\n349", "365": "CHAPTER 6. SAMPLE STATISTICS\nWhat is so special about the Hoeffding\u2019s inequality?\n\u0088Since Hoeffding\u2019s inequality is derived from Chernoff\u2019s bound, it inherits the\ntightness. Hoeffding\u2019s inequality is much stronger than Chebyshev\u2019s inequality\nin bounding the tail distributions.\n\u0088Hoeffding\u2019s inequality is one of the few inequalities that do not require E[X] and\nVar[X] on the right-hand side.\n\u0088A downside of the inequality is that boundedness is not always easy to satisfy.\nFor example, if Xnis a Gaussian random variable, Hoeffding does not apply.\nThere are more advanced inequalities for situations like these.\nInterpreting Hoeffding\u2019s inequality . One way to interpret Hoeffding\u2019s inequality is to\nwrite the equation as\nP\u0002\f\fXN\u2212\u00b5\f\f> \u03f5\u0003\n\u22642e\u22122\u03f52N\n|{z}\n\u03b4,\nwhich is equivalent to\nP\u0002\f\fXN\u2212\u00b5\f\f\u2264\u03f5\u0003\n\u22651\u2212\u03b4.\nThis means that with a probability at least 1 \u2212\u03b4, we have\nXN\u2212\u03f5\u2264\u00b5\u2264XN+\u03f5.\nIf we let \u03b4= 2e\u22122\u03f52N, this becomes\nXN\u2212r\n1\n2Nlog2\n\u03b4\u2264\u00b5\u2264XN+r\n1\n2Nlog2\n\u03b4. (6.28)\nThis inequality is a confidence interval (see Chapter 9). It says that with probability at\nleast 1 \u2212\u03b4, the interval [ XN\u2212\u03f5,XN+\u03f5] includes the true population mean \u00b5.\nThere are two questions one can ask about the confidence interval:\n\u0088Given Nand\u03b4, what is the confidence interval? Equation (6.28) tells us that if we\nknow N, to achieve a probability of at least 1 \u2212\u03b4the confidence interval will follow\nEquation (6.28). For example, if N= 10,000 and \u03b4= 0.01,q\n1\n2Nlog2\n\u03b4= 0.016.\nTherefore, with a probability at least 99%, the true population mean \u00b5will be included\nin the interval\nXN\u22120.16\u2264\u00b5\u2264XN+ 0.16.\n\u0088If we want to achieve a certain confidence interval, what is the Nwe need? If we are\ngiven \u03f5and\u03b4, the Nwe need is\n\u03b4\u22642e\u22122\u03f52N\u21d2 N\u2265log2\n\u03b4\n2\u03f52.\nFor example, if \u03b4= 0.01 and \u03f5= 0.01, the Nwe need is N\u226526,500.\nWhen is Hoeffding\u2019s inequality used? Hoeffding\u2019s inequality is fundamental in modern\nmachine learning theory. In this field, one often wants to quantify how well a learning\n350", "366": "6.3. LAW OF LARGE NUMBERS\nalgorithm performs with respect to the complexity of the model and the number of training\nsamples. For example, if we choose a complex model, we should expect to use more training\nsamples or overfit otherwise. Hoeffding\u2019s inequality provides an asymptotic description of\nthe training error, testing error, and the number of training samples. The inequality is\noften used to compare the theoretical performance limit of one model versus another model.\nTherefore, although we do not need to use Hoeffding\u2019s inequality in this book, we hope you\nappreciate its tightness.\nClosing Remark . We close this section by providing the historic context of Chernoff\u2019s\ninequality. Herman Chernoff, the discoverer of Chernoff\u2019s inequality, wrote the following\nmany years after the publication of the original paper in 1952.\n\u201cIn working on an artificial example, I discovered that I was using the Central Limit\nTheorem for large deviations where it did not apply. This led me to derive the asymptotic\nupper and lower bounds that were needed for the tail probabilities. [Herman] Rubin claimed\nhe could get these bounds with much less work, and I challenged him. He produced a rather\nsimple argument, using Markov\u2019s inequality, for the upper bound. Since that seemed to be\na minor lemma in the ensuing paper I published (Chernoff, 1952), I neglected to give him\ncredit. I now consider it a serious error in judgment, especially because his result is stronger\nfor the upper bound than the asymptotic result I had derived. \u201d \u2014 Herman Chernoff, \u201cA\ncareer in statistics,\u201d in Lin et al., Past, Present, and Future of Statistical Science (2014),\np. 35.\n6.3 Law of Large Numbers\nIn this section, we present our first main result: the law of large numbers. We will discuss\ntwo versions of the law: the weak law and the strong law. We will also introduce two forms\nof convergence: convergence in probability and almost sure convergence.\n6.3.1 Sample average\nThe law of large numbers is a probabilistic statement about the sample average . Suppose\nthat we have a collection of i.i.d. random variables X1, . . . , X N. The sample average of these\nNrandom variables is defined as follows:\nDefinition 6.5. Thesample average of a sequence of random variables X1, . . . , X N\nis\nXN=1\nNNX\nn=1Xn. (6.29)\nIf the random variables X1, . . . , X Nare i.i.d. so that they have the same population\nmean E[Xn] =\u00b5(forn= 1, . . . , N ), then by the linearity of the expectation,\nE\u0002\nXN\u0003\n=1\nNNX\nn=1E[Xn] =\u00b5.\n351", "367": "CHAPTER 6. SAMPLE STATISTICS\nTherefore, the mean of XNis the population mean \u00b5.\nThe sample average, XN, plays an important role in statistics. For example, by sur-\nveying 10,000 Americans, we can find a sample average of their ages. Since we never have\naccess to the true population mean, the sample average is an estimate, and since XNis only\nan estimate, we need to ask how good the estimate is.\nOne reason we ask this question is that XNis a finite-sample \u201capproximation\u201d of \u00b5.\nMore importantly, the root of the problem is that XNitself is a random variable because\nX1, . . . , X Nare all random variables. Since XNis a random variable, there is a PDF of\nXN; there is a CDF of XN; there is E[XN]; and there is Var[ XN]. Since XNis a random\nvariable, it has uncertainty. To say that we are confident about XN, we need to ensure that\nthe uncertainty is within some tolerable range.\nHow do we control the uncertainty? We can compute the variance. If X1, . . . , X Nare\ni.i.d. random variables with the same variance Var[ Xn] =\u03c32(forn= 1, . . . , N ), then\nVar\u0002\nXN\u0003\n=1\nN2NX\nn=1Var[Xn] =1\nN2NX\nn=1\u03c32=\u03c32\nN.\nTherefore, the variance will shrink to 0 as Ngrows. In other words, the more samples we\nuse to construct the sample average, the less deviation the random variable XNwill have.\nVisualizing the sample average\nTo help you visualize the randomness of XN, we consider an experiment of drawing N\nBernoulli random variables X1, . . . , X Nwith parameter p= 1/2. Since Xnis Bernoulli, it\nfollows that\nE[Xn] =p and Var[ Xn] =p(1\u2212p).\nWe construct a sample average XN=1\nNPN\nn=1Xn. Since Xnis a Bernoulli random variable,\nwe know everything about XN. First, XNis a binomial random variable, since XNis the\nsum of Bernoulli random variables. Second, the mean and variance of XNare respectively\n\u00b5XNdef=E[XN] =1\nNNX\nn=1E[Xn] =p,\n\u03c32\nXNdef= Var[ XN] =1\nN2NX\nn=1Var[Xn] =p(1\u2212p)\nN.\nInFigure 6.12 , we plot the random variables XN(the black crosses) for every N. You\ncan see that at each N, e.g., N= 100, there are many possible observations for XNbecause\nXNitself is a random variable. As Nincreases, we see that the deviation of the random\nvariables becomes smaller. In the same plot, we show the bounds \u00b5\u00b13\u03c3XN, which are three\nstandard deviations from the mean. We can see clearly that the bounds provide a very good\nenvelope covering the random variables. As Ngoes to infinity, we can see that the standard\ndeviation goes to zero, and so XNapproaches the true mean.\nFor your reference, the MATLAB code and the Python code we used to generate the\nplot are shown below.\n% MATLAB code to illustrate the weak law of large numbers\nNset = round(logspace(2,5,100));\n352", "368": "6.3. LAW OF LARGE NUMBERS\n102103104105\nN0.30.40.50.60.7sample average\nFigure 6.12: The weak law of large numbers. In this plot, we assume that X1, . . . , X Nare i.i.d. Bernoulli\nrandom variables with a parameter p. The black crosses in the plot are the sample averages\nXN=1\nNPN\nn=1Xn. The red curves are the ideal bounds \u00b5XN\u00b13\u03c3XN, where \u00b5XN=pand\n\u03c3XN=p\np(1\u2212p)/N. As Ngrows, we observe that the variance shrinks to zero. Therefore, the\nsample average is converging to the true population mean.\nfor i=1:length(Nset)\nN = Nset(i);\np = 0.5;\nx(:,i) = binornd(N, p, 1000,1)/N;\nend\ny = x(1:10:end,:)\u2019;\nsemilogx(Nset, y, \u2019kx\u2019); hold on;\nsemilogx(Nset, p+3*sqrt(p*(1-p)./Nset), \u2019r\u2019, \u2019LineWidth\u2019, 4);\nsemilogx(Nset, p-3*sqrt(p*(1-p)./Nset), \u2019r\u2019, \u2019LineWidth\u2019, 4);\n# Python code to illustrate the weak law of large numbers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport numpy.matlib\np = 0.5\nNset = np.round(np.logspace(2,5,100)).astype(int)\nx = np.zeros((1000,Nset.size))\nfor i in range(Nset.size):\nN = Nset[i]\nx[:,i] = stats.binom.rvs(N, p, size=1000)/N\nNset_grid = np.matlib.repmat(Nset, 1000, 1)\nplt.semilogx(Nset_grid, x,\u2019ko\u2019);\nplt.semilogx(Nset, p + 3*np.sqrt((p*(1-p))/Nset), \u2019r\u2019, linewidth=6)\nplt.semilogx(Nset, p - 3*np.sqrt((p*(1-p))/Nset), \u2019r\u2019, linewidth=6)\n353", "369": "CHAPTER 6. SAMPLE STATISTICS\nNote the outliers for each NinFigure 6.12 . For example, at N= 102we see a point\nlocated near 0.7 on the y-axis. This point is outside three standard deviations. Is it normal?\nYes. Being outside three standard deviations only says that the probability of having this\noutlier is small . It does not say that the outlier is impossible . Having a small probability does\nnot exclude the possibility. By contrast, if you say that something will surely not happen you\nmean that there is not even a small probability. The former is a weaker statement than the\nlatter. Therefore, even though we establish a three standard deviation envelope, there are\npoints falling outside the envelope. As Ngrows, the chance of having a bad outlier becomes\nsmaller. Therefore, the greater the N, the smaller the chance we will get an outlier.\nIf the random variables Xnare i.i.d., the above phenomenon is universal. Below is an\nexample of the Poisson case.\nPractice Exercise 6.8 . Let Xn\u223cPoisson( \u03bb). Define the sample average as XN=\n1\nNPN\nn=1Xn. Find the mean and variance of XN.\nSolution . Since Xnis Poisson, we know that E[Xn] =\u03bband Var[ Xn] =\u03bb. So\nE[XN] =1\nNNX\nn=1E[Xn] =1\nNNX\nn=1\u03bb=\u03bb,\nVar[XN] =1\nN2NX\nn=1Var[Xn] =1\nN2NX\nn=1\u03bb=\u03bb\nN.\nTherefore, as N\u2192 \u221e , the variance Var[ XN]\u21920.\n6.3.2 Weak law of large numbers (WLLN)\nThe analysis of Figure 6.12 shows us something important, namely that the convergence\nin a probabilistic way is different from that in a deterministic way. We now describe one\nfundamental result related to probabilistic convergence, known as the weak law of large\nnumbers.\nTheorem 6.16 (Weak law of large numbers ).LetX1, . . . , X Nbe a set of i.i.d. ran-\ndom variables with mean \u00b5and variance \u03c32. Assume E[X2]<\u221e. Let XN=1\nNPN\nn=1Xn.\nThen for any \u03b5 >0,\nlim\nN\u2192\u221eP\u0014\n|XN\u2212\u00b5|> \u03b5\u0015\n= 0. (6.30)\nProof . By Chebyshev\u2019s inequality,\nP\u0002\n|XN\u2212\u00b5|> \u03b5\u0003\n\u2264Var[XN]\n\u03b52=Var[Xn]\nN\u03b52.\nTherefore, setting N\u2192 \u221e we have\nlim\nN\u2192\u221eP\u0002\n|XN\u2212\u00b5|> \u03b5\u0003\n= lim\nN\u2192\u221eVar[Xn]\nN\u03b52= 0.\n\u25a1\n354", "370": "6.3. LAW OF LARGE NUMBERS\nExample 6.10 . Consider a set of i.i.d. random variables X1, . . . , X Nwhere\nXn\u223cGaussian( \u00b5, \u03c32).\nVerify that the sample average XN=1\nNPN\nn=1Xnfollows the weak law of large num-\nbers.\nSolution : Since Xnis a Gaussian, the sample average XNis also a Gaussian:\nXN\u223cGaussian\u0012\n\u00b5,\u03c32\nN\u0013\n.\nConsider the probability P\u0002\n|XN\u2212\u00b5|> \u03b5\u0003\nfor each N:\n\u03b4Ndef=P\u0014\n|XN\u2212\u00b5|> \u03b5\u0015\n=P\u0014\nXN\u2212\u00b5 > \u03b5\u0015\n+P\u0014\nXN\u2212\u00b5 <\u2212\u03b5\u0015\n= 1\u2212\u03a6 \n\u03b5\u221a\nN\n\u03c3!\n+ \u03a6 \n\u2212\u03b5\u221a\nN\n\u03c3!\n= 2\u03a6 \n\u2212\u03b5\u221a\nN\n\u03c3!\n.\nIf we set \u03c3= 1 and \u03b5= 0.1, then\n\u03b41= 2\u03a6\u0012\n\u22120.1\u00b71\n1\u0013\n= 0.9203, \u03b4 5= 2\u03a6 \n\u22120.1\u00b7\u221a\n5\n1!\n= 0.8231,\n\u03b410= 2\u03a6 \n\u22120.1\u00b7\u221a\n10\n1!\n= 0.7518, \u03b4 100= 2\u03a6 \n\u22120.1\u00b7\u221a\n100\n1!\n= 0.3173,\n\u03b41000= 2\u03a6 \n\u22120.1\u00b7\u221a\n1000\n1!\n= 0.0016.\nAs you can see, the the sequence \u03b41, \u03b42, . . . , \u03b4 N, . . .rapidly converges to 0 as Ngrows.\nIn fact, since \u03a6( z) is a increasing function for z <0 with \u03a6( \u2212\u221e) = 0, it follows that\nlim\nN\u2192\u221eP\u0014\n|XN\u2212\u00b5|> \u03b5\u0015\n= lim\nN\u2192\u221e2\u03a6 \n\u2212\u03b5\u221a\nN\n\u03c3!\n= 0.\nThe weak law of large numbers is portrayed graphically in Figure 6.13 . In this figure\nwe draw several PDFs of the sample average XN. The shapes of the PDFs are getting\nnarrower as the variance of the random variable shrinks. Since the PDFs become narrower,\nthe probability P[|XN\u2212\u00b5|> \u03b5] becomes more unlikely. At the limit when N\u2192 \u221e , the\nprobability vanishes. The weak law of large numbers asserts that this happens for any set of\ni.i.d. random variables. It says that the sequence of probability values \u03b4Ndef=P[|XN\u2212\u00b5|> \u03b5]\n355", "371": "CHAPTER 6. SAMPLE STATISTICS\nwill converge to zero.\nFigure 6.13: The weak law of large numbers states that as Nincreases, the variance of the sample\naverage XNshrinks. As a result, the probability P[|XN\u2212\u00b5|> \u03b5]decreases and eventually vanishes.\nNote that the convergence here is that of the sequence of probabilities P[|XN\u2212\u00b5|> \u03b5], which is just\na sequence of numbers.\nWhat is the weak law of large numbers?\nLetXNbe the sample average of i.i.d. random variables X1, . . . , X N.\nlim\nN\u2192\u221eP\u0014\n|XN\u2212\u00b5|> \u03b5\u0015\n= 0. (6.31)\n\u0088For details, see Theorem 6.16.\n\u0088The WLLN concerns the sequence of probability values \u03b4N=P[|XN\u2212\u00b5|> \u03b5].\n\u0088The probabilities converge to zero as Ngrows.\n\u0088It is weak because having a small probability does not exclude the possibility of\nhappening.\n6.3.3 Convergence in probability\nThe example above tells us that in order to show convergence, we need to first compute the\nprobability \u03b4nof each event and then take the limit of the sequence, e.g., the one shown in\nthe table below:\n\u03b41 \u03b45 \u03b410 \u03b4100 \u03b41000 \u03b410000\n0.9203 0.8231 0.7518 0.3173 0.0016 1.5240 \u00d710\u221223\nTherefore, the convergence is the convergence of the probability . Since {\u03b41, \u03b42, . . .}is a\nsequence of real numbers (between 0 and 1), any convergence results for real numbers apply\nhere.\nNote that the convergence controls only the probabilities. Probability means chance.\nTherefore, having the limit converging to zero only means that the chance of happening is\nbecoming smaller and smaller. However, at any N, there is still a chance that some bad\nevent can happen.\n356", "372": "6.3. LAW OF LARGE NUMBERS\nWhat do we mean by a bad event? Assume that Xnare fair coins. The sample average\nXN= (1/N)PN\nn=1Xnis more or less equal to 1/2 as Ngrows. However, even if Nis a\nlarge number, say N= 1000, we are still not certain that the sample average is exactly 1/2.\nIt is possible, though very unlikely, that we obtain 1000 heads or 1000 tails (so that the\nsample average is \u201c1\u201d or \u201c0\u201d). The bottom line is: Having a probability converging to zero\nonly means that for any tolerance level we can always find an Nlarge enough so that the\nprobability is smaller than that tolerance.\nThe type of convergence described by the weak law of large numbers is known as the\nconvergence in probability .\nDefinition 6.6. A sequence of random variables A1, . . . , A Nconverges in probability\nto a deterministic number \u03b1if for every \u03b5 >0,\nlim\nN\u2192\u221eP[|AN\u2212\u03b1|> \u03b5] = 0. (6.32)\nWe write ANp\u2192\u03b1to denote convergence in probability.\nThe following two examples illustrate how to prove convergence in probability.\nExample 6.11 . Let X1, . . . , X Nbe i.i.d. random variables with Xn\u223cUniform(0 ,1).\nDefine AN= min( X1, . . . , X N). Show that ANconverges in probability to zero.\nSolution . (Without determining the PDF of AN, we notice that as Nincreases, the\nvalue of ANwill likely decrease. Therefore, we should expect ANto converge to zero.)\nPick an \u03b5 >0. It follows that\nP[|AN\u22120| \u2265\u03b5] =P[min( X1, . . . , X N)\u2265\u03b5], because Xn\u22650\n=P[X1\u2265\u03b5and\u00b7\u00b7\u00b7andXN\u2265\u03b5]\n=P\u0000\nX1\u2265\u03b5\u0001\n\u00b7\u00b7\u00b7P\u0000\nXN\u2265\u03b5\u0001\n= (1\u2212\u03b5)N.\nSetting the limit of N\u2192 \u221e , we conclude that\nlim\nN\u2192\u221eP[|AN\u22120| \u2265\u03b5] = lim\nN\u2192\u221e(1\u2212\u03b5)N= 0.\nTherefore, ANconverges to zero in probability.\nPractice Exercise 6.9 . Let X\u223cExponential(1). By evaluating the CDF, we know\nthatP[X\u2265x] =e\u2212x. Let AN=X/N . Prove that ANconverges to zero in probability.\nSolution . For any \u03b5 >0,\nP[|AN\u22120| \u2265\u03b5] =P[AN\u2265\u03b5]\n=P[X\u2265N\u03b5]\n=e\u2212N\u03b5.\n357", "373": "CHAPTER 6. SAMPLE STATISTICS\nPutting N\u2192 \u221e on both sides of the equation gives us\nlim\nN\u2192\u221eP[|AN\u22120| \u2265\u03b5] = lim\nN\u2192\u221ee\u2212N\u03b5= 0.\nThus, ANconverges to zero in probability.\nExample 6.12 . Construct an example such that ANconverges in probability to some-\nthing, but E[AN] does not converge to the same thing.\nSolution . Consider a sequence of random variables ANsuch that\nP[AN=\u03b1] =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f31\u22121\nN, \u03b1 = 0,\n1\nN, \u03b1 =N2,\n0, otherwise .\nThe PDF of the random variable ANis shown in Figure 6.14 .\nFigure 6.14: Probability density function of the random variable AN.\nWe first show that ANconverges in probability to zero. Let \u03b5 >0 be a fixed\nconstant. Since \u03b5 >0,P[AN\u2265\u03b5] =1\nNfor any N >\u221a\u03b5. Therefore, we have that\nlim\nN\u2192\u221eP[|AN\u22120| \u2265\u03b5] = lim\nN\u2192\u221eP[AN\u2265\u03b5]\n= lim\nN\u2192\u221e1\nN= 0.\nHence, ANconverges to 0 in probability.\nHowever, E[AN] does not converge to zero, because\nE[AN] = 0\u00b7\u0012\n1\u22121\nN\u0013\n+N2\u00b71\nN=N.\nSoE[AN] goes to infinity as Ngrows.\n6.3.4 Can we prove WLLN using Chernoff\u2019s bound?\nThe following discussion of using Chernoff\u2019s bound to prove WLLN can be skipped if this\nis your first time reading the book.\n358", "374": "6.3. LAW OF LARGE NUMBERS\nIn proving WLLN we use Chebyshev\u2019s inequality. Can we use Chernoff\u2019s inequality (or\nHoeffding\u2019s) to prove the result? Yes, we can use them. However, notice that the task here is\nto prove convergence, not to find the bestconvergence. Finding the best convergence means\nfinding the fastest decay rate of the probability sequence. Chernoff\u2019s bound (and Hoeffding\u2019s\ninequality) offers a better decay rate. However, Chernoff\u2019s bound needs to be customized for\nindividual random variables. For example, Chernoff\u2019s bound for Gaussian is different from\nChernoff\u2019s bound for exponential. This result makes Chebyshev the most convenient bound\nbecause it only requires the variance to be bounded.\nWhat if we insist on using Chernoff\u2019s bound in proving the WLLN? We can do that for\nspecific random variables. Let\u2019s consider two examples. The first example is the Gaussian\nrandom variable where Xn\u223c N(0, \u03c32). We know that XN\u223c N(0, \u03c32/N). Chernoff\u2019s bound\nshows that\nP\u0002\n|XN\u2212\u00b5|> \u03b5\u0003\n\u22642 exp\u001a\n\u2212\u03b52N\n2\u03c32\u001b\n,\nTaking the limit on both sides, we have\nlim\nN\u2192\u221eP\u0002\n|XN\u2212\u00b5|> \u03b5\u0003\n= lim\nN\u2192\u221e2 exp\u001a\n\u2212\u03b52N\n2\u03c32\u001b\n= 0.\nNote that the rate of convergence here is exponential. The rate of convergence offered by\nChebyshev is only linear. Of course, you may argue that since Xnis Gaussian we have\nclosed-form expressions about the probability, so we do not need Chernoff\u2019s bound. This is\na legitimate point, and so here is an example where we do not have a closed-form expression\nfor the probability.\nConsider a sequence of arbitrary i.i.d. random variables X1, . . . , X Nwith 0 \u2264Xn\u22641.\nThen Hoeffding\u2019s inequality tells us that\nP\u0002\n|XN\u2212\u00b5|> \u03b5\u0003\n\u22642 exp\b\n\u22122\u03b52N\t\n.\nTaking the limit on both sides, we have\nlim\nN\u2192\u221eP\u0002\n|XN\u2212\u00b5|> \u03b5\u0003\n= lim\nN\u2192\u221e2 exp\b\n\u22122\u03b52N\t\n= 0.\nAgain, we obtain a WLLN result, this time for i.i.d. random variables X1, . . . , X Nwith\n0\u2264Xn\u22641.\nAs you can see from these two examples, WLLN can be proved in multiple ways\ndepending on how general the random variables need to be.\nEnd of the discussions.\n6.3.5 Does the weak law of large numbers always hold?\nThe following discussion of the failure of the weak law of large numbers can be skipped if\nthis is your first time reading the book.\n359", "375": "CHAPTER 6. SAMPLE STATISTICS\nThe weak law of large numbers does not always hold. Recall that when we prove the\nweak law of large numbers using Chebyshev\u2019s inequality, we implicitly require that the\nvariance Var[ XN] is finite. (Look at the condition that E[X2]<\u221e.) Thus for distributions\nwhose variance is unbounded, Chebyshev\u2019s inequality does not hold. One example is the\nCauchy distribution. The PDF of a Cauchy distribution is\nfX(x) =\u03b3\n\u03c0(\u03b32+x2),\nwhere \u03b3is a parameter. Letting \u03b3= 1,\nE[X2] =Z\u221e\n\u2212\u221ex2\n\u03c0(1 +x2)dx=1\n\u03c0Z\u221e\n\u2212\u221e1\u22121\n1 +x2dx\n=1\n\u03c0Z\u221e\n\u2212\u221edx\u22121\n\u03c0Z\u221e\n\u2212\u221e1\n1 +x2dx=1\n\u03c0\u0014\nx\u2212tan\u22121(x)\u0015\f\f\f\f\u221e\nx=\u2212\u221e=\u221e.\nSince the second moment is unbounded, the variance of Xwill also be unbounded.\nA perceptive reader may observe that even if E[X2] is unbounded, it does not mean\nthat the tail probability is unbounded. This is correct. However, for Cauchy distributions,\nwe can show that the sample average XNdoes not converge to the mean when N\u2192 \u221e\n(and so the WLLN fails). To see this, we note that the characteristic function of a Cauchy\nrandom variable Xis\n1\n\u03c0(1 +x2)\u2194e\u2212|\u03c9|.\nSo for the sample average XN=1\nNPN\nn=1Xn, the characteristic function is\nE[e\u2212j\u03c9XN] =E[e\u2212j\u03c9\nNPN\nn=1Xn] =NY\nn=1E[e\u2212j\u03c9\nNXn] =h\ne\u2212|\u03c9|\nNiN\n=e\u2212|\u03c9|,\nwhich remains a Cauchy distribution with \u03b3= 1. Therefore, we have that\nP[|XN| \u2264\u03b5] =Z\u03b5\n\u2212\u221e1\n\u03c0(1 +x2)dx\n=Z0\n\u2212\u221e1\n\u03c0(1 +x2)dx+Z\u03b5\n01\n\u03c0(1 +x2)dx=1\n2+1\n\u03c0tan\u22121(\u03b5).\nThus no matter how many samples we have, P[|XN| \u2264\u03b5] will never converge to 1 (so\nP[|XN|> \u03b5] will never converge to 0). Therefore, WLLN does not hold.\nEnd of the discussion.\n6.3.6 Strong law of large numbers\nSince there is a \u201cweak\u201d law of large numbers, you will not be surprised to learn that there\nis a strong law of large numbers. The strong law is more restrictive than the weak law. Any\nsequence satisfying the strong law will satisfy the weak law, but not vice versa. Since the\nstrong law is \u201cstronger\u201d, the proof is more involved.\n360", "376": "6.3. LAW OF LARGE NUMBERS\nTheorem 6.17 (Strong law of large numbers ).LetX1, . . . , X Nbe a sequence of\ni.i.d. random variables with common mean \u00b5and variance \u03c32. Assume E[X4]<\u221e.\nLetXN=1\nNPN\nn=1Xnbe the sample average. Then\nPh\nlim\nN\u2192\u221eXN=\u00b5i\n= 1. (6.33)\nThe strong law flips the order of limit and probability . As you can see, the difference\nbetween the strong law and the weak law is the order of the limit and the probability. In the\nweak law, the limit is outside the probability, whereas, in the strong law, the limit is inside\nthe probability. This switch in order makes the interpretation of the result fundamentally\ndifferent. In the final analysis, the weak law concerns the limit of a sequence of probabilities\n(which are just real numbers between 0 and 1). However, the strong law concerns the limit\nof a sequence of random variables. The strong law answers the question, what is the limiting\nobject of the sample average as Ngrows?\nThe strong law concerns the limiting object, not a sequence of numbers . What\nis the \u201climiting object\u201d? If we denote XNas the sample average using Nsamples, then\nwe know that X1is a random variable, X2is a random variable, and all Xn\u2019s are random\nvariables. So we have a sequence of random variables. As Ngoes to infinity, we can ask about\nthe limiting object lim N\u2192\u221eXN. However, even without any deep analysis, you should be\nable to see that lim N\u2192\u221eXNis another random variable. The strong law says that this\nlimiting object will \u201csuccessfully\u201d become a deterministic number \u00b5, after a finite number\nof \u201cfailures\u201d.\nThe strong law asserts that there are a finite number of failures . Let us explain\n\u201csuccess\u201d and \u201cfailure\u201d. XNis a random variable, so it fluctuates. However, as Ngoes to\ninfinity, the strong law says that the number of times where XN\u0338=\u00b5will be zero. That\nis, there is a finite number of times where XN\u0338=\u00b5(i.e., fail), and afterward, you will be\nperfectly fine (i.e., success). Yes, perfectly fine means 100%. The weak law only guarantees\n99.99%.\nA good example for differentiating the weak law and the strong law is an electronic\ndictionary that improves itself every time you use it. The weak law says that if you use\nthe dictionary for a long period, the probability of making an error will become small. You\nwill still get an error once in a while, but the probability is very small. This is a 99.99%\nguarantee, and it is the weak law. The strong law says that the number of failures is finite.\nAfter you have gone through this finite number of failures, you will be completely free of\nerror. This is a 100% guarantee by the strong law. When will you hit this magical number?\nThe strong law does not say when; it only asserts the existence of this number. However, this\nexistence is already good enough in many ways. It gives a certificate of assurance, whereas\nthe weak law still has uncertainty.\nStrong law \u0338=deterministic. If the strong law offers a 100% guarantee, does it mean\nthat it is a deterministic guarantee? No, the strong law is still a probabilistic statement\nbecause we are still using P[\u00b7] to measure an event. The event can include measure-zero\nsubsets, and the measure-zero subsets can be huge. For example, the set of rational numbers\non the real line is a measure-zero set when measuring the probability using an integration.\nThe strong law does not handle those measure-zero subsets.\n361", "377": "CHAPTER 6. SAMPLE STATISTICS\n6.3.7 Almost sure convergence\nThe discussion below can be skipped if this is your first time reading the book.\nThe type of convergence used by the strong law of large numbers is the almost sure\nconvergence . It is defined formally as follows.\nDefinition 6.7. A sequence of random variables A1, . . . , A Nconverges almost surely\nto\u03b1if\nPh\nlim\nN\u2192\u221eAN=\u03b1i\n= 1. (6.34)\nWe write ANa.s.\u2192\u03b1to denote almost sure convergence.\nTo prove almost sure convergence, one needs to show that the sequence ANwill demonstrate\nAN\u0338=\u03b1for a finite number of times. Afterward, ANneeds to demonstrate AN=\u03b1.\nExample 6.13 .aConstruct a sequence of events that converges almost surely.\nSolution . Let X1, . . . , X Nbe i.i.d. random variables such that Xn\u223cUniform(0 ,1).\nDefine AN= min( X1, . . . , X N). Since ANis nonincreasing and is bounded below by\nzero, it must have a limit. Let us call this limit\nAdef= lim\nN\u2192\u221eAN.\nThen we can show that\nP[A\u2265\u03f5] =P[min( X1, X2, . . .)\u2265\u03f5]\n(a)\n\u2264P[min( X1, X2, . . . , X N)\u2265\u03f5]\n(b)=P[X1\u2265\u03f5andX2\u2265\u03f5and\u00b7\u00b7\u00b7andXN\u2265\u03f5]\n= (1\u2212\u03f5)N,\nwhere ( a) holds because there are more elements in ( X1, X2, . . .) than in\n(X1, X2, . . . , X N). Therefore, the minimum value of the former is less than the mini-\nmum value of the latter. ( b) holds because if min( X1, X2, . . . , X N)\u2265\u03f5, then Xn\u2265\u03f5\nfor all n.\nSinceP[A\u2265\u03f5]\u2264(1\u2212\u03f5)Nfor any N, the statement still holds as N\u2192 \u221e . Thus,\nP[A\u2265\u03f5]\u2264lim\nN\u2192\u221e(1\u2212\u03f5)N= 0.\nThis shows P[A\u2265\u03f5] = 0 for any positive \u03f5. SoP[A > \u03f5 ] = 0, and hence P[A= 0] = 1.\nSince Ais the limit of AN, we conclude that\nPh\nlim\nN\u2192\u221eAN= 0i\n=P[A= 0] = 1 .\n362", "378": "6.3. LAW OF LARGE NUMBERS\nSoANconverges to 0 almost surely.\naThis example is modified from Bertsekas and Tsitsiklis, Introduction to Probability , Chapter 5.5.\nExample 6.14 .aConstruct an example where a sequence of events converges in prob-\nability but does not converge almost surely.\nSolution . Consider a discrete time arrival process. The set of times is partitioned into\nconsecutive intervals of the form\nI1={2,3},\nI2={4,5,6,7},\nI3={8,9,10, . . . , 15},\n...\nIk={2k,2k+ 1, . . . , 2k+1\u22121}.\nTherefore, the length of each interval is |I1|= 2,|I2|= 4, . . . , |Ik|= 2k.\nDuring each interval, there is exactly one arrival. Define Ynas a binary random\nvariable such that for every n\u2208Ik,\nYn=(\n1, with probability1\n|Ik|,\n0, with probability 1 \u22121\n|Ik|.\nFor example, if n\u2208 {2,3}, then P[Yn= 1] =1\n2. Ifn\u2208 {4,5,6,7}, then P[Yn= 1] =1\n4.\nIn general, we have that\nlim\nn\u2192\u221eP[Yn= 1] = lim\nn\u2192\u221e1\n|Ik|= lim\nn\u2192\u221e1\n2k= 0,\nand hence\nlim\nn\u2192\u221eP[Yn= 0] = lim\nn\u2192\u221e1\u22121\n2k= 1.\nTherefore, Ynconverges to 0 in probability.\nHowever, when we carry out the experiment, there is exactly one arrival per\ninterval according to the problem conditions. Since we have an infinite number of\nintervals I1, I2, . . ., we will have an infinite number of arrivals in total. As a result,\nYn= 1 for infinitely many times. We do not know which Ynwill equal 1 and which\nYnwill equal to 0. However, we know that there are infinitely many Ynthat are equal\nto 1. Therefore, in the sequence Y1, Y2, . . . , Y n, . . ., we must have that the tail of the\nsequence is 1. (If Ynstops being 1 after some n, then we will not have an infinite\nnumber of arrivals in total.)\nSince Yn= 1 when nis large enough, it follows that\nPh\nlim\nn\u2192\u221eYn= 1i\n= 1.\n363", "379": "CHAPTER 6. SAMPLE STATISTICS\nEquivalently, we can say that the sequence Ynwill never take the value 0 when nis\nlarge enough. Thus,\nPh\nlim\nn\u2192\u221eYn= 0i\n= 0.\nTherefore, Yndoes not converge to 0 almost surely.\naThis example is modified from Bertsekas and Tsitsiklis, Introduction to Probability , Chapter 5.5.\nEnd of the discussions.\n6.3.8 Proof of the strong law of large numbers\nThe strong law of large numbers can be proved in several ways. We present a proof based on\nBertsekas and Tsitsiklis, Introduction to Probability , Problems 5.16 and 5.17, which require\na finite fourth moment E[X4\nn]<\u221e. An alternative proof that requires only E[Xn]<\u221eis\nfrom Billingsley, Probability and Measure , Theorem 22.1.\nThe proof of the strong law of large numbers is beyond the scope of this book. This\nsection is optional.\nLemma 6.4. Consider non-negative random variables X1, . . . , X N. Assume that\nE\"\u221eX\nn=1Xn#\n<\u221e. (6.35)\nThen Xna.s.\u21920.\nProof . Let S=PN\nn=1Xn. Note that Sis a random variable, and our assumption is that\nE[S]<\u221e. Thus, we argue that S <\u221ewith probability 1. If not, then Swill have a positive\nprobability of being \u221e. But if this happens, we will have E[S] =\u221ebecause (by the law of\ntotal expectation):\nE[S] =E[S|S= infinite]| {z }\n=\u221eP[S= infinite] + E[S|S= finite] P[S= finite] .\nNow, since Sis finite, the sequence {X1, . . . , X N, . . .}must converge to zero. Otherwise,\nifXnis converging to some constants c >0, then summing the tail of the sequence (which\ncontains infinitely many terms) gives infinity:\nS=X1+\u00b7\u00b7\u00b7|{z}\n=finite+XN+\u00b7\u00b7\u00b7+|{z}\n=infinite.\nSince the probability of Sbeing finite is 1, it follows that {X1, . . . , X N}is converging\nto zero with probability 1.\n\u25a1\n364", "380": "6.3. LAW OF LARGE NUMBERS\nTheorem 6.18 (Strong law of large numbers ).LetX1, . . . , X Nbe a sequence of\ni.i.d. random variables with common mean \u00b5and variance \u03c32. Assume E[X4\nn]<\u221e.\nLetXN=1\nNPN\nn=1Xnbe the sample average. Then\nPh\nlim\nN\u2192\u221eXN=\u00b5i\n= 1. (6.36)\nProof . We first prove the case where E[Xn] = 0. To establish that XN\u21920 with probabil-\nity 1, we use the lemma to show that\nE\"\u221eX\nN=1|XN|#\n<\u221e.\nBut to show E[P\u221e\nN=1|XN|]<\u221e, we note that |x| \u22641 +x4. Therefore, E[P\u221e\nN=1|XN|]\u2264\n1 +E[P\u221e\nN=1X4\nN], and hence we just need to show that\nE\"\u221eX\nN=1X4\nN#\n<\u221e.\nLet us expand the term E[X4\nN] as follows:\nE[X4\nN] =1\nN4NX\nn1=1NX\nn2=1NX\nn3=1NX\nn4=1E[Xn1Xn2Xn3Xn4].\nThere are five possibilities for E[Xn1Xn2Xn3Xn4]:\n\u0088All indices are different. Then\nE[Xn1Xn2Xn3Xn4] =E[Xn1]E[Xn2]E[Xn3]E[Xn4] = 0\u00b70\u00b70\u00b70 = 0 .\n\u0088One index is different from other three indices. For example, if n1is different from\nn2, n3, n4, then\nE[Xn1Xn2Xn3Xn4] =E[Xn1]E[Xn2Xn3Xn4] = 0\u00b7E[Xn2Xn3Xn4] = 0.\n\u0088Two indices are identical. For example, if n1=n3, and n2=n4, then\nE[Xn1Xn2Xn3Xn4] =E[Xn1Xn3]E[Xn2Xn4] =E[X2\nn1X2\nn2].\nThere are altogether 3 N(N\u22121) of these cases: N(N\u22121) comes from choosing N\nfollowed by choosing N\u22121, and 3 accounts for n1=n2\u0338=n3=n4,n1=n3\u0338=n2=n4,\nandn1=n4\u0338=n2=n3.\n\u0088Two indices are identical, and two indices are different. For example, if n1=n3but\nn2andn4are different. Then\nE[Xn1Xn2Xn3Xn4] =E[Xn1Xn3]E[Xn2]E[Xn4] =E[X2\nn1]\u00b70\u00b70 = 0 .\n365", "381": "CHAPTER 6. SAMPLE STATISTICS\n\u0088All indices are identical. If n1=n2=n3=n4, then\nE[Xn1Xn2Xn3Xn4] =E[X4\nn1].\nThere are altogether Ncases of this.\nTherefore, it follows that\nE[X4\nN] =NE[X4\n1] + 3N(N\u22121)E[X2\n1X2\n2]\nN4.\nSince xy\u2264(x2+y2)/2, it follows that\nE[X2\n1X2\n2]\u2264E[(X2\n1)2+ (X2\n2)2]/2 =E[X4\n1+X4\n2]/2 =E[X4\n1].\nSubstituting into the previous result,\nE[X4\nN]\u2264NE[X4\n1] + 3N(N\u22121)E[X4\n1]\nN4\u22643N2\nN4E[X4\n1] =3\nN2E[X4\n1].\nNow, let us complete the proof.\nE\"\u221eX\nN=1X4\nN#\n\u2264E\"\u221eX\nN=13\nN2E[X4\n1]#\n<\u221e,\nbecauseP\u221e\nN=1(1/N2) is the Bassel problem with a solution thatP\u221e\nN=1(1/N2) =\u03c02/6.\nConsequently, we have shown that EhP\u221e\nN=1X4\nNi\n<\u221e, which implies E\u0002P\u221e\nN=1|XN|\u0003\n<\u221e.\nThen, by the lemma, we have XNconverging to 0 with probability 1, which proves the result.\nIfE[Xn] =\u00b5, then just replace Xnwith Yn=Xn\u2212\u00b5in the above arguments. Then we\ncan show that YNconverges to 0 with probability 1, which is equivalent to XNconverging\nto\u00b5with probability 1.\nEnd of the proof of strong law of large numbers.\n6.4 Central Limit Theorem\nThe law of large numbers tells us the mean of the sample average XN= (1/N)PN\nn=1Xn.\nHowever, if you recall our experiment of throwing Ndice and inspecting the PDF of the\nsum of the numbers, you may remember that the convolution of an infinite number of\nuniform distributions gives us a Gaussian distribution. For example, we show a sequence\nof experiments in Figure 6.15 . In each experiment, we throw Ndice and count the sum.\nTherefore, if each face of the die is denoted as Xn, then the sum is X1+\u00b7\u00b7\u00b7+XN. We plot\nthe PDF of the sum. As you can see in the figure, X1+\u00b7\u00b7\u00b7+XNconverges to a Gaussian.\nThis phenomenon is explained by the Central Limit Theorem (CLT) .\nWhat does the Central Limit Theorem say? Let XNbe the sample average, and let\nZN=\u221a\nN\u0010\nXN\u2212\u00b5\n\u03c3\u0011\nbe the normalized variable. The Central Limit Theorem is as follows:\n366", "382": "6.4. CENTRAL LIMIT THEOREM\nFigure 6.15: Pictorial illustration of the Central Limit Theorem. Suppose we throw a die and record the\nface. [Left] If we only have one die, then the distribution of the face is uniform. [Middle] If we throw\ntwo dice, the distribution is the convolution of two uniform distributions. This will give us a triangle\ndistribution. [Right] If we throw five dice, the distribution is becoming similar to a Gaussian. The Central\nLimit Theorem says that as Ngoes to infinity, the distribution of the sum will converge to a Gaussian.\nCentral Limit Theorem :\nThe CDF of ZNis converging pointwise to the CDF of Gaussian(0,1).\nNote that we are very careful here. We are not saying that the PDF of ZNis converging to\nthe PDF of a Gaussian, nor are we saying that the random variable ZNis converging to a\nGaussian random variable. We are only saying that the values of the CDF are converging\npointwise. The difference is subtle but important.\nTo understand the difficulty and the core ideas, we first present the concept of conver-\ngence in distribution.\n6.4.1 Convergence in distribution\nDefinition 6.8. LetZ1, . . . , Z Nbe random variables with CDFs FZ1, . . . , F ZNrespec-\ntively. We say that a sequence of Z1, . . . , Z Nconverges in distribution to a random\nvariable Zwith CDF FZif\nlim\nN\u2192\u221eFZN(z) =FZ(z), (6.37)\nfor every continuous point zofFZ. We write ZNd\u2192Zto denote convergence in\ndistribution.\nThis definition involves many concepts, which we will discuss one by one. However, the\ndefinition can be summarized in a nutshell as follows.\nConvergence in distribution = values of the CDF converge.\nExample 1 . (Bernoulli ) Consider flipping a fair coin Ntimes. Denote each coin flip as a\nBernoulli random variable Xn\u223cBernoulli( p), where n= 1,2, . . . , N . Define ZNas the sum\n367", "383": "CHAPTER 6. SAMPLE STATISTICS\nofNBernoulli random variables, so that\nZN=NX\nn=1Xn.\nWe know that the resulting random variable ZNis a binomial random variable with mean\nNpand variance Np(1\u2212p). Let us plot the PDF fZN(z) as shown in Figure 6.16 .\nFigure 6.16: Convergence in distribution. The convergence in distribution concerns the convergence of\nthe values of the CDF (not the PDF). In this figure, we let ZN=X1+\u00b7\u00b7\u00b7+XN, where XNis a\nBernoulli random variable with parameter p. Since a sum of Bernoulli random variables is a binomial,\nZNis a binomial random variable with parameters (N, p). We plot the PDF of ZN, which is a train of\ndelta functions, and compare it with the Gaussian PDF. Observe that the error, max z|fZN(z)\u2212fZ(z)|,\ndoes notconverge to 0. The PDF of ZNis a binomial. A binomial is always a binomial. It will not turn\ninto a Gaussian.\nThe first thing we notice in the figure is that as Nincreases, the PDF of the binomial\nhas an envelope that is \u201cvery Gaussian\u201d. So one temptation is to say that the random\nvariable ZNis converging to another random variable Z. In addition, we would think that\nthe PDFs converge in the sense that for allz,\nfZN(z) =\u0012N\nz\u0013\npz(1\u2212p)N\u2212z\u2212\u2192 fZ(z) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(z\u2212\u00b5)2\n2\u03c32\u001b\n,\nwhere \u00b5=Npand\u03c32=Np(1\u2212p).\nUnfortunately this argument does not work, because fZ(z) is continuous but fZN(z)\nis discrete. The sample space of ZNand the sample space of Zare completely different. In\nfact, if we write fZNas an impulse train, we observe that\nfZN(z) =NX\ni=0\u0012N\ni\u0013\npi(1\u2212p)N\u2212i\u03b4(z\u2212i).\nClearly, no matter how big the Nis, the difference |fZN(z)\u2212fZ(z)|will never go to zero\nfor non-integer values of z. Mathematically, we can show that\nmax\nz|fZN(z)\u2212fZ(z)| \u0338\u2212\u2192 0,\nasN\u2192 \u221e .ZNis a binomial random variable regardless of N. It will not become a Gaussian.\n368", "384": "6.4. CENTRAL LIMIT THEOREM\nIffZN(z) is not converging to a Gaussian PDF, how do we explain the convergence?\nThe answer is to look at the CDF. For discrete PDFs such as a binomial random variable,\nthe CDF is a staircase function. What we can show is that\nFZN(z) =zX\ni=0\u0012N\ni\u0013\npi(1\u2212p)N\u2212i\u2212\u2192 FZ(z) =Zz\n\u2212\u221e1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(t\u2212\u00b5)2\n2\u03c32\u001b\ndt.\nThe difference between the PDF convergence and the CDF convergence is that the PDF\ndoes not allow a meaningful \u201cdistance\u201d between a discrete function and continuous function.\nFor CDF, the distance is well defined by taking the difference between the staircase function\nand the continuous function. For example, we can compute\n|FZN(z)\u2212FZ(z)|, for all continuous points zofFZ,\nand show that\nmax\nz|FZN(z)\u2212FZ(z)| \u2212\u2192 0.\nFigure 6.17: Convergence in distribution. This is the same as Figure 6.16 , but this time we plot the\nCDF of ZN. The CDF is a staircase function. We compare it with the Gaussian CDF. Observe that the\nerror, max z|FZN(z)\u2212FZ(z)|, converges to zero as Ngrows. Convergence in distribution says that the\nsequence of CDFs FZN(z)will converge to the limiting CDF FZ(z), at all continuous points of FZ(z).\nWe need to pay attention to the set of z\u2019s. We do not evaluate all z\u2019s but only the z\u2019s\nthat are continuous points of FZ. IfFZis Gaussian, this does not matter because all z\u2019s\nare continuous. However, for CDFs containing discontinuous points, our definition of con-\nvergence in distribution will ignore these discontinuous points because they have a measure\nzero.\nExample 2 . (Poisson ) Consider Xn\u223cPoisson( \u03bb), and consider X1, . . . , X N. Define ZN=PN\nn=1Xn. It follows that E[ZN] =PN\nn=1E[Xn] =N\u03bband Var[ ZN] =PN\nn=1Var[Xn] =N\u03bb.\nMoreover, we know that the sum of Poissons remains a Poisson. Therefore, the PDF of ZN\nis\nfZN(z) =\u221eX\nk=0(N\u03bb)k\nk!e\u2212N\u03bb\u03b4(z\u2212k) and fZ(z) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(z\u2212\u00b5)2\n2\u03c32\u001b\n,\n369", "385": "CHAPTER 6. SAMPLE STATISTICS\n0 2 4 6 800.050.10.150.2\nPoisson\nGaussian\n0 5 10 15 2000.050.10.15\nPoisson\nGaussian\n0 20 40 60 80 10000.020.040.06\nPoisson\nGaussian\n0 2 4 6 800.20.40.60.81\nPoisson\nGaussian\n0 5 10 15 2000.20.40.60.81\nPoisson\nGaussian\n0 20 40 60 80 10000.20.40.60.81\nPoisson\nGaussian\n(a)N= 4 (b) N= 10 (c) N= 50\nFigure 6.18: Convergence in distribution for a sum of Poisson random variables. Here we assume that\nX1, . . . , X Nare i.i.d. Poisson with a parameter \u03bb. We let ZN=PN\nn=1Xnbe the sum, and compute the\ncorresponding PDF (top row) and CDFs (bottom row). Just as with the binomial example, the PDFs of\nthe Poisson do not converge but the CDFs of the Poisson converge to the CDF of a Gaussian.\nwhere \u00b5=N\u03bband\u03c32=N\u03bb. Again, fZNdoes not converge to fZ. However, if we compare\nthe CDF, we can see from Figure 6.18 that the CDF of the Poisson is becoming better\napproximated by the Gaussian.\nInterpreting \u201cconvergence in distribution\u201d . After seeing two examples, you should\nnow have some idea of what \u201cconvergence in distribution\u201d means. This concept applies to\nthe CDFs. When we write\nlim\nN\u2192\u221eFZN(z) =FZ(z), (6.38)\nwe mean that FZN(z) is converging to the value FZ(z), and this relationship holds for all\nthe continuous z\u2019s ofFZ. It does not say that the random variable ZNis becoming another\nrandom variable Z.\nZNd\u2212\u2192Zis equivalent to lim N\u2192\u221eFZN(z) =FZ(z).\nExample 3 . (Exponential ) So far, we have studied the sum of discrete random variables.\nNow, let\u2019s take a look at continuous random variables. Consider Xn\u223cExponential( \u03bb), and\nletX1, . . . , X Nbe i.i.d. copies. Define ZN=PN\nn=1Xn. Then E[ZN] =PN\nn=1E[Xn] =N/\u03bb\nand Var[ ZN] =N\n\u03bb2. How about the PDF of ZN? Using the characteristic functions, we know\nthat\nfXn(x) =\u03bbe\u2212\u03bbx F\u2190\u2192 \u03a6Xn(j\u03c9) =\u03bb\n\u03bb+j\u03c9.\n370", "386": "6.4. CENTRAL LIMIT THEOREM\nTherefore, the product is\n\u03a6ZN(j\u03c9) =NY\nn=1\u03a6Xn(j\u03c9) =\u03bbN\n(\u03bb+j\u03c9)N=\u03bbN\n(\u03bb+j\u03c9)N\u00d7(N\u22121)!\n(N\u22121)!\n=\u03bbN\n(N\u22121)!\u00b7(N\u22121)!\n(\u03bb+j\u03c9)NF\u2190\u2192\u03bbN\n(N\u22121)!zN\u22121e\u2212\u03bbz=fZN(z).\nThis resulting PDF fZN(z) =\u03bbN\n(N\u22121)!zN\u22121e\u2212\u03bbzis known as the Erlang distribution . The\nCDF of the Erlang distribution is\nFZN(z) =Zz\n\u2212\u221efZN(t)dt\n=Zz\n0\u03bbN\n(N\u22121)!tN\u22121e\u2212\u03bbtdt\n= Gamma function( z, N),\nwhere the last integral is known as the incomplete gamma function, evaluated at z.\nGiven all these, we can now compare the PDF and the CDF of ZNversus Z.Figure 6.19\nshows the PDFs and the CDFs of ZNfor various Nvalues. In this experiment we set \u03bb= 1.\nAs we can see from the experiment, the Erlang distribution\u2019s PDF and CDF converge to\na Gaussian. In fact, for continuous random variables such as exponential random variables,\nwe indeed have the random variable ZNconverging to the random variable Z. This is quite\ndifferent from discrete random variables, where ZNdoes not converge to Zbut only FZN\nconverges to FZ.\n0 2 4 6 800.050.10.150.20.25\nSum of Exponential\nGaussian\n0 5 10 15 2000.050.10.15\nSum of Exponential\nGaussian\n0 20 40 60 80 10000.020.040.06\nSum of Exponential\nGaussian\n0 2 4 6 800.20.40.60.81\nSum of Exponential\nGaussian\n0 5 10 15 2000.20.40.60.81\nSum of Exponential\nGaussian\n0 20 40 60 80 10000.20.40.60.81\nSum of Exponential\nGaussian\n(a)N= 4 (b) N= 10 (c) N= 50\nFigure 6.19: Convergence in distribution for a sum of exponential random variables. Here we assume\nthatX1, . . . , X Nare i.i.d. exponentials with a parameter \u03bb. We define ZN=PN\nn=1Xnbe the sum.\nIt is known that the sum of exponentials is an Erlang. We compute the corresponding PDF (top row)\nand CDFs (bottom row). Unlike the previous two examples, in this example we see that both PDFs and\nCDFs of the Erlang distribution are converging to a Gaussian.\n371", "387": "CHAPTER 6. SAMPLE STATISTICS\nIsd\u2212\u2192stronger thanp\u2212\u2192?Convergence in distribution is actually weaker than con-\nvergence in probability. Consider a continuous random variable Xwith a symmetric PDF\nfX(x) such that fX(x) =fX(\u2212x). It holds that the PDF of \u2212Xhas the same PDF. If\nwe define the sequence ZN=XifNis odd and ZN=\u2212XifNis even, and let Z=X,\nthen FZN(z) =FZ(z) for every zbecause the PDF of Xand\u2212Xare identical. There-\nfore, ZNd\u2192Z. However, ZN\u0338p\u2192Zbecause ZNoscillates between the random variables X\nand\u2212X. These two random variables are different (although they have the same CDF)\nbecause P[X=\u2212X] =P[{\u03c9:X(\u03c9) =\u2212X(\u03c9)}] =P[{\u03c9:X(\u03c9) = 0}] = 0.\n6.4.2 Central Limit Theorem\nTheorem 6.19 (Central Limit Theorem ).LetX1, . . . , X Nbe i.i.d. random variables\nof mean E[Xn] =\u00b5and variance Var[Xn] =\u03c32. Also, assume that E[|X3\nn|]<\u221e. Let\nXN= (1/N)PN\nn=1Xnbe the sample average, and let ZN=\u221a\nN\u0010\nXN\u2212\u00b5\n\u03c3\u0011\n. Then\nlim\nN\u2192\u221eFZN(z) =FZ(z), (6.39)\nwhere Z=Gaussian (0,1).\nIn plain words, the Central Limit Theorem says that the sample average (which is\na random variable) has a CDF converging to the CDF of a Gaussian. Therefore, if we\nwant to evaluate probabilities associated with the sample average, we can approximate the\nprobability by the probability of a Gaussian.\nAs we discussed above, the Central Limit Theorem does not mean that the random\nvariable ZNis converging to a Gaussian random variable, nor does it mean that the PDF\nofZNis converging to the PDF of a Gaussian. It only means that the CDF of ZNis\nconverging to the CDF of a Gaussian. Many people think that the Central Limit Theorem\nmeans \u201csample average converges to Gaussian\u201d. This is incorrect for the above reasons.\nHowever, it is not completely wrong. For continuous random variables where both PDF\nand CDF are continuous, we will not run into situations where the PDF is a train of delta\nfunctions. In this case, convergence in CDF can be translated to convergence in PDF.\nThe power of the Central Limit Theorem is that the result holds for anydistribution\nofX1, . . . , X N. That is, regardless of the distribution of X1, . . . , X N, the CDF of XNis\napproaching a Gaussian.\nSummary of the Central Limit Theorem\n\u0088X1, . . . , X Nare i.i.d. random variables, with mean \u00b5and variance \u03c32. They are\nnot necessarily Gaussians.\n\u0088Define the sample average as XN= (1/N)PN\nn=1Xn, and let ZN=\u221a\nN\u0010\nX\u2212\u00b5\n\u03c3\u0011\n.\n\u0088The Central Limit Theorem says ZNd\u2212\u2192Gaussian(0 ,1). Equivalently, the the-\norem says that NXNd\u2212\u2192Gaussian( \u00b5, \u03c32).\n\u0088So if we want to evaluate the probability of XN\u2208 A for some set A, we can\n372", "388": "6.4. CENTRAL LIMIT THEOREM\napproximate the probability by evaluating the Gaussian:\nP[XN\u2208 A]\u2248Z\nA1p\n2\u03c0(\u03c32/N)exp\u001a\n\u2212(y\u2212\u00b5)2\n2(\u03c32/N)\u001b\ndy.\n\u0088CLT does notsay that the PDF of XNis becoming a Gaussian PDF.\n\u0088CLT only says that the CDF of XNis becoming a Gaussian CDF.\nIf the set Ais an interval, we can use the standard Gaussian CDF to compute the\nprobability.\nCorollary 6.3. LetX1, . . . , X Nbe i.i.d. random variables with mean \u00b5and vari-\nance \u03c32. Define the sample average as XN= (1/N)PN\nn=1Xn. Then\nP[a\u2264XN\u2264b]\u2248\u03a6\u0012\u221a\nNb\u2212\u00b5\n\u03c3\u0013\n\u2212\u03a6\u0012\u221a\nNa\u2212\u00b5\n\u03c3\u0013\n, (6.40)\nwhere \u03a6(z) =Rz\n\u2212\u221e1\u221a\n2\u03c0e\u2212x2\n2dxis the CDF of the standard Gaussian.\nProof . By the Central Limit Theorem, we know that XNd\u2212\u2192Gaussian( \u00b5,\u03c32\nN). Therefore,\nP[a\u2264XN\u2264b]\u2248Zb\na1p\n2\u03c0(\u03c32/N)exp\u001a\n\u2212(y\u2212\u00b5)2\n2(\u03c32/N)\u001b\ndy\n=Z\u221a\nNb\u2212\u00b5\n\u03c3\n\u221a\nNa\u2212\u00b5\n\u03c31\u221a\n2\u03c0e\u2212y2\n2dy= \u03a6\u0012\u221a\nNb\u2212\u00b5\n\u03c3\u0013\n\u2212\u03a6\u0012\u221a\nNa\u2212\u00b5\n\u03c3\u0013\n.\n\u25a1\nFigure 6.20: The Central Limit Theorem says that if we want to evaluate the probability P[a\u2264XN\u2264b],\nwhere XN= (1/N)PN\nn=1Xnis the sample average of i.i.d. random variables X1, . . . , X N, we can\napproximate the probability by integrating the Gaussian PDF.\nA graphical illustration of the CLT is shown in Figure 6.20 , where we use a binomial\nrandom variable (which is the sum of i.i.d. Bernoulli) as an example. The CLT does not say\n373", "389": "CHAPTER 6. SAMPLE STATISTICS\nthat the binomial random variable is becoming a Gaussian. It only says that the probability\ncovered by the binomial can be approximated by the Gaussian.\nThe following proof of the Central Limit Theorem can be skipped if this is your first time\nreading the book.\nProof of the Central Limit Theorem . We now give a \u201cproof\u201d of the Central Limit\nTheorem. Technically speaking, this proof does not prove the convergence of the CDF as the\ntheorem claims; it only proves that the moment-generating function converges. The actual\nproof of the CDF convergence is based on the Berry-Esseen Theorem, which is beyond the\nscope of this book. However, what we prove below is still useful because it gives us some\nintuition about why Gaussian is the limiting random variable we should consider in the first\nplace.\nLetZN=\u221a\nN\u0010\nXN\u2212\u00b5\n\u03c3\u0011\n. It follows that E[ZN] = 0 and Var[ ZN] = 1. Therefore, if we\ncan show that ZNis converging to a standard Gaussian random variable Z\u223cGaussian(0 ,1),\nthen by the linear transformation property of Gaussian, Y=\u03c3\u221a\nNZ+\u00b5will be Gaussian( \u00b5, \u03c32/N).\nOur proof is based on analyzing the moment-generating function of ZN. In particular,\nMZN(s)def=E[esZN] =E\u0014\nes\u221a\nN\u0010XN\u2212\u00b5\n\u03c3\u0011\u0015\n=NY\nn=1Eh\nes\n\u03c3\u221a\nN(Xn\u2212\u00b5)i\n.\nExpanding the exponential term using the Taylor expansion (Chapter 1.2),\nNY\nn=1Eh\nes\n\u03c3\u221a\nN(Xn\u2212\u00b5)i\n=NY\nn=1E\u0014\n1 +s\n\u03c3\u221a\nN(Xn\u2212\u00b5) +s2\n2\u03c32N(Xn\u2212\u00b5)2+O\u0012(Xn\u2212\u00b5)3\n\u03c33N\u221a\nN\u0013\u0015\n=NY\nn=1\u0014\n1 +s\n\u03c3\u221a\nNE[Xn\u2212\u00b5] +s2\n2\u03c32NE\u0002\n(Xn\u2212\u00b5)2\u0003\u0015\n=\u0012\n1 +s2\n2N\u0013N\n.\nIt remains to show that\u0010\n1 +s2\n2N\u0011N\n\u2192es2/2. If we can show that, we have shown that the\nMGF of ZNis also the MGF of Gaussian(0 ,1). To this end, we consider log(1 + x). By the\nTaylor approximation, we have that\nlog(1 + x)\u2248log(1) +\u0012d\ndxlogx|x=1\u0013\nx+\u0012d2\ndx2logx|x=1\u0013x2\n2+O(x3).\nTherefore, we have log\u0010\n1 +s2\n2N\u0011\n\u2248s2\n2N\u2212s4\n4N2. AsN\u2192 \u221e , the limit becomes\nlim\nN\u2192\u221eNlog\u0012\n1 +s2\n2N\u0013\n\u2248s2\n2\u2212lim\nN\u2192\u221es4\n4N=s2\n2,\nand so taking the exponential on both sides yields lim N\u2192\u221e\u0010\n1 +s2\n2N\u0011N\n=es2\n2. Therefore,\nwe conclude that lim N\u2192\u221eMZN(s) =es2\n2, and so ZNis converging to a Gaussian.\n374", "390": "6.4. CENTRAL LIMIT THEOREM\n\u25a1\nLimitation of our proof . The limitation of our proof lies in the issue of whether the\nintegration and the limit are interchangeable:\nlim\nN\u2192\u221eMZN(s) = lim\nN\u2192\u221e\u001aZ\nfZN(z)eszdz\u001b\n?=Z\u0010\nlim\nN\u2192\u221efZN(z)\u0011\neszdz.\nIf they were, then proving lim N\u2192\u221eMZN(s) =MZ(s) is sufficient to claim fZN(z)\u2192fZ(z).\nHowever, we know that the latter is not true in general. For example, if fZN(z) is a train of\ndelta functions, then the limit and the integration are not interchangeable.\nBerry-Esseen Theorem . The formal way of proving the Central Limit Theorem is to\nprove the Berry-Esseen Theorem. The theorem states that\nsup\nz\u2208R\f\f\f\fFZN(z)\u2212FZ(z)\f\f\f\f\u2264C\u03b2\n\u03c33\u221a\nN,\nwhere \u03b2andCare universal constants. Here, you can more or less treat the supremum\noperator as the maximum. The left-hand side represents the worst-case error of the CDF\nFZNcompared to the limiting CDF FZ. The right-hand side involves several constants C,\n\u03b2, and \u03c3, but they are fixed.\nAsNgoes to infinity, the right-hand side will converge to zero. Therefore, if we can\nprove this result, then we have proved the actual Central Limit Theorem. In addition,\nwe have found the rate of convergence since the right-hand side tells us that the error\ndrops at the rate of 1 /\u221a\nN, which is not particularly fast but is sufficient for our purpose.\nUnfortunately, proving the Berry-Esseen theorem is not easy. One of the difficulties, for\nexample, is that one needs to deal with the infinite convolutions in the time domain or the\nfrequency domain.\nInterpreting our proof . If our proof is not completely valid, why do we mention it?\nFor one thing, it provides us with some useful intuition. For most of the (well-behaving)\nrandom variables whose moments are finite, the exponential term in the moment-generating\nfunction can be truncated to the second-order polynomial. Since a second-order polynomial\nis a Gaussian, it naturally concludes that as long as we can perform such truncation the\ntruncated random variable will be Gaussian.\nTo convince you that the Gaussian MGF is the second-order approximation to other\nMGFs, we use Bernoulli as an example. Let X1, . . . , X Nbe i.i.d. Bernoulli with a parame-\nterp. Then the moment-generating function of XN= (1/N)PN\nn=1Xnwould be:\nMXN(s) =E[esX] =E[es1\nNPN\nn=1Xn] =NY\nn=1E[es\nNXn]\n= (1\u2212p+pes\nN)N\u2248\u0012\n1\u2212p+p\u0012\n1 +s\nN+s2\n2N2\u0013\u0013N\n=\u0012\n1 +sp\nN+s2p\n2N2\u0013N\n.\n375", "391": "CHAPTER 6. SAMPLE STATISTICS\nUsing the logarithmic approximation, it follows that\nlogMXN(s) =Nlog\u0012\n1 +sp\nN+s2p\n2N2\u0013\n\u2248N\u0012sp\nN+s2p\n2N2\u0013\n\u2212N\n2\u0012sp\nN+s2p\n2N2\u00132\n\u2248sp+s2p(1\u2212p)\n2Ndef= log MY(s).\nTaking the exponential on both sides, we have that\nMY(s) = exp\u001a\nsp+s2p(1\u2212p)\n2N\u001b\n,\nwhich is the MGF of a Gaussian random variable Y\u223cGaussian\u0010\np,p(1\u2212p)\nN\u0011\n.\nFigure 6.21 shows several MGFs. In each of the subfigures we plot the exact MGF\nMXN(s) = (1 \u2212p+pes\nN)Nas a function of s. (The parameter pin this example is p= 0.5.) We\nvary the number N, and we inspect how the shape of MXN(s) changes. On top of the exact\nMGFs, we plot the Gaussian approximations MY(s) = expn\nsp+s2p(1\u2212p)\n2No\n. According to\nour calculation, this Gaussian approximation is the second-order approximation to the exact\nMGF. The figures show the effect of the second-order approximation. For example, in (a)\nwhen N= 2 the Gaussian is a quadratic approximation of the exact MGF. For (b) and (c),\nasNincreases, the approximation improves.\n-10 -5 0 5 1010-210-1100101102103104105\nBinomial MGF\nGaussian MGF\n-10 -5 0 5 1010-210-1100101102103104105\nBinomial MGF\nGaussian MGF\n-10 -5 0 5 1010-210-1100101102103104105\nBinomial MGF\nGaussian MGF\n(a)N= 2 (b) N= 4 (c) N= 10\nFigure 6.21: Explanation of the Central Limit Theorem using the function. In this set of plots, we show\nthe MGF of the random variable XN= (1/N)PN\nn=1Xn, where X1, . . . , X Nare i.i.d. Bernoulli random\nvariables. The exact MGF of XNis the binomial, whereas the approximated MGF is the Gaussian. We\nobserve that as Nincreases, the Gaussian approximation to the exact MGF improves.\nThe reason why the second-order approximation works for Gaussian is that when N\nincreases, the higher order moments of XNvanish and only the leading first two moments\nsurvive. The MGFs are becoming flat because MY(s) = expn\nsp+s2p(1\u2212p)\n2No\nconverges to\nexp{sp}when N\u2192 \u221e . Taking the inverse Laplace transform, MY(s) = exp {sp}corresponds\nto a delta function. This makes sense because as Ngrows, the variance of the Xshrinks.\nEnd of the discussion.\n376", "392": "6.4. CENTRAL LIMIT THEOREM\n6.4.3 Examples\nExample 6.15 . Prove the equivalence of a few statements.\n\u0088\u221a\nN\u0010\nXN\u2212\u00b5\n\u03c3\u0011d\u2192Gaussian(0 ,1)\n\u0088\u221a\nN(XN\u2212\u00b5)d\u2192Gaussian(0 , \u03c32)\n\u0088\u221a\nNXNd\u2192Gaussian( \u00b5, \u03c32)\nSolution . The proof is based on the linear transformation property of Gaussian ran-\ndom variables. For example, if the first statement is true, then the second statement\nis also true because\nlim\nN\u2192\u221eF\u221a\nN(XN\u2212\u00b5)(z) = lim\nN\u2192\u221eP[\u221a\nN(XN\u2212\u00b5)\u2264z]\n= lim\nN\u2192\u221eP\u0014\u221a\nN\u0012XN\u2212\u00b5\n\u03c3\u0013\n\u2264z\n\u03c3\u0015\n=Zz/\u03c3\n\u2212\u221e1\u221a\n2\u03c0e\u2212t2\n2dt\n=Zz\n\u2212\u221e1\u221a\n2\u03c0\u03c32e\u2212t2\n2\u03c32dt.\nThe other results can be proved similarly.\nExample 6.16 . Suppose Xn\u223cPoisson(10) for n= 1, . . . , N , and let XNbe the\nsample average. Use the Central Limit Theorem to approximate P[9\u2264XN\u226411] for\nN= 20.\nSolution . We first show that\nE[XN] =E\"\n1\nNNX\nn=1Xn#\n=1\nNNX\nn=1E[Xn] = 10 ,\nVar[XN] =1\nN2NX\nn=1Var [Xn] =1\nNVar[Xn] =10\n20=1\n2.\nTherefore, the Central Limit Theorem implies that XNd\u2212\u2192Gaussian\u0000\n10,1\n2\u0001\n. The\nprobability is\nP[9\u2264XN\u226411]\u2248\u03a6 \n11\u221210p\n1/2!\n\u2212\u03a6 \n9\u221210p\n1/2!\n= \u03a6\u00121\u221a\n0.5\u0013\n\u2212\u03a6\u0012\n\u22121\u221a\n0.5\u0013\n= 0.9214\u22120.0786 = 0 .8427.\n377", "393": "CHAPTER 6. SAMPLE STATISTICS\nWe can also do an exact calculation to verify our approximation. Let SN=PN\nn=1Xnso that XN=SN\nN. Since a sum of Poisson remains a Poisson, it follows that\nSN\u223cPoisson(10 N) = Poisson(200) .\nConsequently,\nP[9\u2264XN\u226411] =P[180\u2264SN\u2264220]\n=220X\n\u2113=0200\u2113e\u2212200\n\u2113!\u2212180X\n\u2113=0200\u2113e\u2212200\n\u2113!= 0.9247\u22120.0822 = 0 .8425.\nNote that this is an exact calculation subject to numerical errors when evaluating the\nfinite sums. The proximity to the Gaussian approximation shows the convenience of\nthe Central Limit Theorem.\nExample 6.17 . Suppose you have collected N= 100 data points from an unknown\ndistribution. The only thing you know is that the true population mean is \u00b5= 500\nand the standard deviation is \u03c3= 80. (Note that this distribution is not necessarily a\nGaussian.)\n(a) Find the probability that the sample mean will be inside the interval (490 ,510).\n(b) Find an interval such that 95% of the sample average is covered.\nSolution . To solve (a), we note that XNd\u2192Gaussian\u0012\n500,\u0010\n80\u221a\n100\u00112\u0013\n. Therefore,\nP[490\u2264XN\u2264510] = \u03a6 \n510\u2212500\n80\u221a\n100!\n\u2212\u03a6 \n490\u2212500\n80\u221a\n100!\n= \u03a6(1 .25)\u2212\u03a6(\u22121.25) = 0 .7888.\nTo solve (b), we know that \u03a6( x) = 0 .025 implies that x=\u22121.96, and \u03a6( x) = 0 .975\nimplies that x= +1 .96. So\ny\u2212500\n80\u221a\n100=\u00b11.96\u21d2 y= 484 .32 or y= 515 .68.\nTherefore, P[484.32\u2264XN\u2264515.68] = 0 .95.\n6.4.4 Limitation of the Central Limit Theorem\nIf we recall the statement of the Central Limit Theorem (Berry-Esseen), we observe that\nthe theorem states only that\nlim\nN\u2192\u221eP\u0014\u221a\nN\u0012XN\u2212\u00b5\n\u03c3\u0013\n\u2264\u03b5\u0015\n= lim\nN\u2192\u221eFZN(\u03b5) =FZ(\u03b5) = \u03a6( \u03b5).\n378", "394": "6.4. CENTRAL LIMIT THEOREM\nRearranging the terms,\nlim\nN\u2192\u221eP\u0014\nXN\u2264\u00b5+\u03c3\u03b5\u221a\nN\u0015\n= \u03a6(\u03b5).\nThis implies that the approximation is good only when the deviation \u03b5issmall .\nLet us consider an example to illustrate this idea. Consider a set of i.i.d. exponential\nrandom variables X1, . . . , X N, where Xn\u223cExponential( \u03bb). Let SN=X1+\u00b7\u00b7\u00b7+XNbe\nthe sum, and let X=SN/Nbe the sample average. Then, according to Chapter 6.4.1, SN\nis an Erlang distribution SN\u223cErlang( N, \u03bb) with a PDF\nfSN(x) =\u03bbN\n(N\u22121)!xN\u22121e\u2212\u03bbx.\nPractice Exercise 6.10 . Let SN\u223cErlang( N, \u03bb) with a PDF fSN(x). Show that if\nYN=aSN+bfor any constants aandb, then\nfYN(y) =1\nafSN\u0012y\u2212b\na\u0013\n.\nSolution : This is a simple transformation of random variables:\nFYN(y) =P[Y\u2264y] =P[aSN+b\u2264y] =P\u0014\nSN\u2264y\u2212b\na\u0015\n=Zy\u2212b\na\n\u2212\u221efSN(x)dx.\nHence, using the fundamental theorem of calculus,\nfYN(y) =d\ndyZy\u2212b\na\n\u2212\u221efSN(x)dx=1\nafSN\u0012y\u2212b\na\u0013\n.\nWe are interested in knowing the statistics of XNand comparing it with a Gaussian.\nTo this end, we construct a normalized variable\nZN=XN\u2212\u00b5\n\u03c3/\u221a\nN,\nwhere \u00b5=E[Xn] =1\n\u03bband\u03c32= Var[ Xn] =1\n\u03bb2. Then\nZN=SN/N\u2212\u00b5\n\u03c3/\u221a\nN=SN\u2212N\u00b5\n\u03c3\u221a\nN=\u03bb\u221a\nNSN\u2212\u221a\nN\nUsing the result of the practice exercise, by mapping a=\u03bb\u221a\nNandb=\u2212\u221a\nN, it follows that\nfZN(z) =\u221a\nN\n\u03bbfSN \nz+\u221a\nN\n\u03bb\u221a\nN!\n.\nNow we compare ZNwith the standard Gaussian Z\u223cGaussian(0 ,1). According to the\nCentral Limit Theorem, the standard Gaussian is a good approximation to the normalized\n379", "395": "CHAPTER 6. SAMPLE STATISTICS\nsample average ZN. To compare the two results, we conduct a numerical experiment. We\nlet\u03bb= 1 and we vary N. We plot the PDF fZN(z) as a function of z, for different N\u2019s, in\nFigure 6.22 . In addition, we plot the PDF fZ(z), which is the standard Gaussian.\nThe plot in Figure 6.22 shows that while the Central Limit Theorem provides a good\napproximation, the approximation is only good for values that are close to the mean. For\nthetails, the Gaussian approximation is not as good.\n-1 0 1 2 3 4 510-610-410-2100\nN = 1\nN = 10\nN = 100\nN = 1000\nGaussian\nFigure 6.22: CLT fails at the tails. We note that X1, . . . , X Nare i.i.d. exponential with a parameter\n\u03bb= 1. We plot the PDFs of the normalized sample average ZN=XN\u2212\u00b5\n\u03c3/\u221a\nNby varying N. We plot the PDF\nof the standard Gaussian Z\u223cGaussian (0,1)on the same grid. Note that the Gaussian approximation\nis good for values that are close to the mean. For the tails, the Gaussian approximation is not very\naccurate.\nThe limitation of the Central Limit Theorem is attributable to the fact that Gaussian\nis a second-order approximation. If a random variable has a very large third moment, the\nsecond-order approximation may not be sufficient. In this case, we need a much larger Nto\ndrive the third moment to a small value and make the Gaussian approximation valid.\nWhen will the Central Limit Theorem fail?\n\u0088The Central Limit Theorem fails when Nis small.\n\u0088The Central Limit Theorem fails if the third moment is large. As an extreme\ncase, a Cauchy random variable does not have a finite third moment. The Central\nLimit Theorem is not valid for this case.\n\u0088The Central Limit Theorem can only approximate the probability for input val-\nues near the mean. It does not approximate the tails, for which we need to use\nChernoff\u2019s bound.\n6.5 Summary\nWhy do we need to study the sample average? Because it is the summary of the dataset. In\nmachine learning, one of the most frequently asked questions is about the number of training\n380", "396": "6.6. REFERENCES\nsamples required to train a model. The answer can be found by analyzing the average number\nof successes and failures as the number of training samples grows. For example, if we define\nfas the classifier that takes a data point xnand predicts a label f(xn), we hope that it\nwill match with the true label yn. If we define an error\nEn=(\n1, f (xn) =yn correct classification ,\n0, f (xn)\u0338=yn incorrect classification ,\nthen Enis a Bernoulli random variable, and the total loss E=1\nNPN\nn=1Enwill be the\ntraining loss. But what is1\nNPN\nn=1En? It is exactly the sample average of En. Therefore, by\nanalyzing the sample average Ewe will learn something about the generalization capability\nof our model.\nHow should we study the sample average? By understanding the law of large numbers\nand the Central Limit Theorem, as we have seen in this chapter.\n\u0088Law of large numbers: Xconverges to the true mean \u00b5asNgrows.\n\u0088Central Limit Theorem: The CDF of Xcan be approximated by the CDF of a\nGaussian, as Ngrows.\nPerformance guarantee? The other topic we discussed in this chapter is the concept\nof convergence type. There are essentially four types of convergence, ranked in the order of\nrestrictions.\n\u0088Deterministic convergence : A sequence of deterministic numbers converges to\nanother deterministic number. For example, the sequence 1 ,1\n2,1\n3,1\n4, . . .converges\nto 0 deterministically. There is nothing random about it.\n\u0088Almost sure convergence : Randomness exists, and there is a probabilistic con-\nvergence. Almost sure convergence means that there is zero probability of failure\nafter a finite number of failures.\n\u0088Convergence in probability : The sequence of probability values converges, i.e.,\nthe chance of failure is going to zero. However, you can still fail even if your N\nis large.\n\u0088Convergence in distribution : The probability values can be approximated by\nthe CDF of a Gaussian.\n6.6 References\nMoment-Generating and Characteristic Functions\n6-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 4.4.\n381", "397": "CHAPTER 6. SAMPLE STATISTICS\n6-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapters 4.5 and 4.7.\n6-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapters 5.5 and 7.2.\n6-4 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001. Chapters 4.5 and 4.7.\n6-5 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n7.7.\n6-6 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapter 4.3.\nBasic probability inequality\n6-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 5.1.\n6-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapters 6 and 8.\n6-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 7.4.\n6-10 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n8.2.\n6-11 Larry Wasserman, All of Statistics , Springer 2003. Chapter 4.\nConcentration inequalities\n6-12 Larry Wasserman, All of Statistics , Springer 2003. Chapter 4.\n6-13 Martin Wainwright, High-Dimensional Statistics , Cambridge University Press, 2019.\nChapter 2.1.\n6-14 Stephane Boucheron, Gabor Lugosi and Pascal Massart, Concentration Inequalities ,\nOxford University Press, 2013. Chapters 2.1 and 2.2.\nLaw of large numbers\n6-15 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapters 5.2, 5.3, 5.5.\n6-16 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapters 7.1, 7.2, 7.4\n6-17 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 7.4.\n382", "398": "6.7. PROBLEMS\n6-18 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n8.2, 8.4.\n6-19 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapters 3.3, 14.1, 14.3.\n6-20 Larry Wasserman, All of Statistics , Springer 2003. Chapter 5.1 - 5.3.\n6-21 Patrick Billingsley, Probability and Measure , Wiley 1995. Section 22.\nCentral Limit Theorem\n6-22 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 5.4.\n6-23 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 7.3.\n6-24 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 7.4.\n6-25 Sheldon Ross, A First Course in Probability , Prentice Hall, 8th Edition, 2010. Chapter\n8.3.\n6-26 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, 2006. Chapters 5.6, 14.2.\n6-27 Larry Wasserman, All of Statistics , Springer 2003. Chapter 5.4.\n6-28 Patrick Billingsley, Probability and Measure , Wiley 1995. Section 27.\n6.7 Problems\nExercise 1. (Video Solution)\nLetX,Y,Zbe three independent random variables:\nX\u223cBernoulli( p), Y \u223cExponential( \u03b1), Z \u223cPoisson( \u03bb)\nFind the function for the following random variables.\n(a)U=Y+Z\n(b)U= 2Z+ 3\n(c)U=XY\n(d)U= 2XY+ (1\u2212X)Z\n383", "399": "CHAPTER 6. SAMPLE STATISTICS\nExercise 2. (Video Solution)\nTwo random variables XandYhave the joint PMF\nP(X=n, Y=m) =\u03bbn+m\n1\u03bbm\n2\n(n+m)!m!e\u2212(\u03bb1+\u03bb2), m = 0,1,2, . . . , n \u2265 \u2212m.\nLetZ=X+Y. Find the function MZ(s) and the PMF of Z.\nExercise 3. (Video Solution)\nLetX0, X1, . . .be a sequence of independent random variables with PDF\nfXk(x) =ak\n\u03c0(a2\nk+x2), a k=1\n2k+1,\nfork= 0,1, . . .. Find the PDF of Y, where\nY=\u221eX\nk=0Xk.\nHint: You may find the characteristic function useful.\nExercise 4.\nThe random variables XandYare independent and have PDFs\nfX(x) =(\ne\u2212x, x \u22650,\n0, x < 0,and fY(y) =(\n0, y > 0,\net, y \u22640.\nFind the PDF of Z=X+Y. (Hint: Use the characteristic function and the moment-\ngenerating function.)\nExercise 5.\nA discrete random variable Xhas a PMF\npX(k) =1\n2k, k = 1,2, . . . .\nFind the characteristic function \u03a6 X(j\u03c9).\nExercise 6.\nLetT1, T2, . . .be i.i.d. random variables with PDF\nfTk(t) =(\n\u03bbe\u2212\u03bbt, t \u22650,\n0, t < 0,\nfork= 1,2,3, . . .. Let Sn=Pn\nk=1Tk. Find the PDF of Sn.\nExercise 7. (Video Solution)\nIn this exercise we will prove a variant of Chebyshev when the variance \u03c32is unknown but\nXis bounded between a\u2264X\u2264b.\n384", "400": "6.7. PROBLEMS\n(a) Let \u03b3\u2208R. Find a \u03b3that minimizes E[(X\u2212\u03b3)2]. Hence, show that E[(X\u2212\u03b3)2]\u2265Var[X]\nfor any \u03b3.\n(b) Let \u03b3= (a+b)/2. Show that\nE[(X\u2212\u03b3)2] =E[(X\u2212a)(X\u2212b)] +(b\u2212a)2\n4.\n(c) From (a) and (b), show that Var[ X]\u2264(b\u2212a)2\n4.\n(d) Show that for any \u03b5 >0,\nP[|X\u2212\u00b5|> \u03b5]\u2264(b\u2212a)2\n4\u03b52.\nExercise 8.\nThe random variables XandYare independent with PDFs\nfX(x) =1\n\u03c0(1 +x2)and fY(y) =1\n\u03c0(1 +y2),\nrespectively. Find the PDF of Z=X\u2212Y. (Hint: Use the characteristic function.)\nExercise 9.\nA random variable Xhas the characteristic function\n\u03a6X(j\u03c9) =e\u2212j\u03c9/(1\u2212j\u03c9).\nFind the mean and variance of X.\nExercise 10.\nShow that for any random variables XandY,\nP[|X\u2212Y|> \u03f5]\u22641\n\u03f52E[(X\u2212Y)2].\nExercise 11.\nLetXbe an exponential random variable with a parameter \u03bb. Let \u00b5=E[X] and \u03c32=\nVar[X]. Compute P[|X\u2212\u00b5| \u2265k\u03c3] for any k >1. Compare this to the bound obtained by\nChebyshev\u2019s inequality.\nExercise 12.\nLetX1, . . . , X Nbe i.i.d. Bernoulli with a parameter p. Let \u03b1 >0 and define\n\u03f5=s\n1\n2Nlog\u00122\n\u03b1\u0013\n.\n385", "401": "CHAPTER 6. SAMPLE STATISTICS\nLetXN=1\nNPN\nn=1Xn. Define an interval\nI=\u0002\nXN\u2212\u03f5,XN+\u03f5\u0003\n.\nUse Hoeffding\u2019s inequality to show that\nP[Icontains p]\u22651\u2212\u03b1.\nExercise 13.\nLetZ\u223cGaussian(0 ,1). Prove that for any \u03f5 >0,\nP[|Z|> \u03f5]\u2264r\n2\n\u03c0e\u2212\u03f52\n2\n\u03f5.\nHint: Note that \u03f5P[|Z|> \u03f5] = 2 \u03f5P[Z > \u03f5 ], and then follow the procedure we used to prove\nMarkov\u2019s inequality.\nExercise 14.\n(a) Give a non-negative random variable X\u22650 such that Markov\u2019s inequality is met with\nequality. Hint: Consider a discrete random variable.\n(b) Give a random variable Xsuch that Chebyshev\u2019s inequality is met with equality.\nExercise 15.\nConsider a random variable Xsuch that\nE[esX]\u2264es2\u03c32\n2.\n(a) Show that for any t,\nP[X\u2265t]\u2264exp\u001a\n\u2212t2\n2\u03c32\u001b\n.\nHint: Use Chernoff\u2019s bound.\n(b) Show that\nE[X2]\u22644\u03c32.\nHint: First prove that E[X2] =R\u221e\n0P[X2\u2265t]dt. Then use part (a) above.\nExercise 16.\nLetX1, . . . , X Nbe i.i.d. uniform random variables distributed over [0 ,1]. Suppose Y1, . . . , Y N\nare defined as follows.\n(a)Yn=Xn/n\n(b)Yn= (Xn)n\n(c)Yn= max( X1, . . . , X n)\n386", "402": "6.7. PROBLEMS\n(d)Yn= min( X1, . . . , X n)\nFor (a), (b), (c), and (d), show that Ynconverges in probability to some limit. Identify the\nlimit in each case.\nExercise 17.\nLet\u03bbn=1\nnforn= 1,2, . . .. Let Xn\u223cPoisson( \u03bbn). Show that Xnconverges in probability\nto 0.\nExercise 18.\nLetY1, Y2, . . .be a sequence of random variables such that\nYn=(\n0, with probability 1 \u22121\nn,\n2n, with probability1\nn.\nDoes Ynconverge in probability to 0?\nExercise 19. (Video Solution)\nA Laplace random variable has a PDF\nfX(x) =\u03bb\n2e\u2212\u03bb|x|, \u03bb > 0,\nand the variance is Var[ X] =2\n\u03bb2. Let X1, . . . , X 500be a sequence of i.i.d. Laplace random\nvariables. Let\nM500=X1+\u00b7\u00b7\u00b7+X500\n500.\n(a) Find E[X]. Express your answer in terms of \u03bb.\n(b) Let \u03bb= 10. Using Chebyshev\u2019s inequality, find a lower bound of\nP[\u22120.1\u2264M500\u22640.1].\n(c) Let \u03bb= 10. Using the Central Limit Theorem, find the probability\nP[\u22120.1\u2264M500\u22640.1].\nYou may leave your answer in terms of the \u03a6( \u00b7) function.\nExercise 20. (Video Solution)\nLetX1, . . . , X nbe a sequence of i.i.d. random variables such that Xi=\u00b11 with equal\nprobability. Let\nYn=1\u221annX\ni=1Xi.\nProve the Central Limit Theorem for this particular sequence of random variables by showing\nthat\n387", "403": "CHAPTER 6. SAMPLE STATISTICS\n(a)E[Yn] = 0, Var[ Yn] = 1.\n(b) The function of YnisMYn(s)\u2192es2\n2asn\u2192 \u221e .\nExercise 21. (Video Solution)\nLetX1, . . . , X Nbe a sequence of i.i.d. random variables with mean and variance\nE[Xn] =\u00b5and Var[ Xn] =\u03c32, n = 1, . . . , N.\nThe distribution of Xnis, unknown. Let\nMN=1\nNNX\nn=1Xn.\nUse the Central Limit Theorem to estimate the probability P[MN>2\u00b5].\n388", "404": "Chapter 7\nRegression\nStarting with this chapter, we will discuss several combat skills \u2014 techniques that we use\nto do the actual data analysis. The theme of this topic is learning andinference , which are\nboth at the core of modern data science. The word \u201clearning\u201d can be broadly interpreted\nas seeking the best model to explain the data, and the word \u201cinference\u201d refers to prediction\nand recovery. Here, prediction means that we use the observed data to forecast or generalize\nto unseen situations, whereas recovery means that we try to restore the missing data in our\ncurrent observations. In this chapter we will learn regression , one of the most widely used\nlearning and inference techniques.\nRegression is a process for finding the relationship between the inputs and the outputs.\nIn a regression problem, we consider a set of input data {x1, . . . , x N}and a set of output\ndata{y1, . . . , y N}. We call the set of these input-output pairs Ddef={(x1, y1), . . . , (xN, yN)}\nthetraining data . The true relationship between an xnand a ynis unknown. We do not\nknow, you do not know, only God knows. We denote this unknown relationship as a mapping\nf(\u00b7) that takes xnand maps it to yn,\nyn=f(xn),\nas illustrated in Figure 7.1 .\nFigure 7.1: A regression problem is about finding the best approximation to the input-output relationship\nof the data.\nSince we do not know f(\u00b7), finding it from a set of finite number of data points D=\n{(x1, y1), . . . , (xN, yN)}is infeasible \u2014 there are infinitely many ways we can make yn=\nf(xn) for every n= 1, . . . , N . The idea of regression is to add a structure to the problem.\nInstead of looking for f(\u00b7), we find a proxy g\u03b8(\u00b7). This proxy g\u03b8(\u00b7) takes a certain parametric\nform. For example, we can postulate that ( xn, yn) has a linear relationship, and so\ng\u03b8(xn) = \u03b81|{z}\nparameterxn+ \u03b80|{z}\nparameter, n = 1, . . . , N.\n389", "405": "CHAPTER 7. REGRESSION\nThis equation is a straight line with a slope \u03b81and a y-intercept \u03b80. We call \u03b8= [\u03b81, \u03b80]\ntheparameter of the model f(\u00b7). To emphasize that the function we are using here is\nparameterized by\u03b8, we denote the function by g\u03b8(\u00b7).\nOf course, any model we choose is our guess . It will never be the true model. There is\nalways a difference between what our model tells us and what we have observed. We denote\nthis \u201cdifference\u201d or \u201cerror\u201d by enand define it as:\nen=yn\u2212g\u03b8(xn), n = 1, . . . , N.\nThe purpose of regression is to find the best \u03b8such that the error is minimized. For example,\nconsider a minimization of the sum-square error:\nb\u03b8= argmin\n\u03b8\u2208RdNX\nn=1(yn\u2212g\u03b8(xn))2\n| {z }\ntraining loss Etrain(\u03b8).\nThe sum of the squared error is just one of the many possible ways we can define the training\nlossEtrain(\u03b8). We will discuss different ways to define the training loss in this chapter, but the\npoint should be evident. For a given dataset D={(x1, y1), . . . , (xN, yN)}, regression tries\nto find a function g\u03b8(\u00b7) such that the training loss is minimized. The optimization variable\nis the parameter \u03b8. If the function g\u03b8(\u00b7) is a linear function in \u03b8, we call the regression a\nlinear regression .\nFigure 7.2: A regression problem involves several steps: picking a model g\u03b8, defining the training loss\nEtrain(\u03b8), and solving the optimization to update \u03b8.\nA summary of the regression process is shown in Figure 7.2 . Given the training data\nD={(x1, y1), . . . , (xN, yN)}, the user picks a model g\u03b8(\u00b7) to make a prediction. We com-\npare the predicted value g\u03b8(xn) with the observed value yn, and compute the training loss\nEtrain(\u03b8). The training loss Etrain(\u03b8) is a function of the model parameter \u03b8. Different model\nparameters \u03b8give different training loss. We solve an optimization problem to find the best\nmodel parameter. In practice, we often iterate the process for a few times until the training\nloss is settled down.\n390", "406": "What is regression?\nGiven the data points ( x1, y1), . . . , (xN, yN), regression is the process of finding\nthe parameter \u03b8of a function g\u03b8(\u00b7) such that the training loss is minimized:\nb\u03b8= argmin\n\u03b8\u2208RdNX\nn=1L(yn, g\u03b8(xn))\n| {z }\ntraining loss Etrain(\u03b8), (7.1)\nwhere L(\u00b7,\u00b7) is the loss between a pair of true observation ynand the prediction g\u03b8(xn).\nOne common choice of L(\u00b7,\u00b7) isL(g\u03b8(xn), yn) = (g\u03b8(xn)\u2212yn)2.\nExample 1. Fitting the data\nSuppose we have a set of data points ( x1, y1), (x2, y2), . . . , ( xN, yN), where xn\u2019s are the\ninputs and yn\u2019s are the outputs. These pairs of data points can be plotted in a scatter plot,\nas shown in Figure 7.3 . We want to find the curve that best fits the data.\nTo solve this problem, we first need to choose a model, for example\ng\u03b8(xn) =\u03b80+\u03b81xn+\u03b82x2\nn+\u03b83x3\nn+\u03b84x4\nn.\nWe call the coefficients \u03b8= [\u03b80, \u03b81, \u03b82, \u03b83, \u03b84] the regression coefficients . They can be found\nby solving the optimization problem\nminimize\n\u03b80,\u03b81,\u03b82,\u03b83,\u03b84NX\nn=1\u0012\nyn\u2212(\u03b80+\u03b81xn+\u03b82x2\nn+\u03b83x3\nn+\u03b84x4\nn)\u00132\n.\n-1 -0.5 0 0.5 1-3-2-101234\ndata\nfitted curve\nFigure 7.3: Regression can be used to fit the dataset using curves. In this example, we use a fourth-th\norder polynomial g\u03b8(x) =P4\np=0\u03b8pxp\nnto fit a 50-point dataset.\nThis optimization asks for the best \u03b8= [\u03b80, . . . , \u03b8 4]Tsuch that the training loss is\nminimized. Solving the minimization problem would require some effort, but if we imagine\nthat we have solved it we can find the best curve, which is g\u03b8(x) =P4\np=0\u03b8pxp\nnwith the\noptimal \u03b8plugged in. The red curve in Figure 7.3 shows an example in which we have used\na fourth-order polynomial to fit a dataset comprising 50 data points. We will learn how to\nsolve the problem in this chapter.\n391", "407": "CHAPTER 7. REGRESSION\nExample 2. Predicting the stock market\nImagine that you have bought some shares in the stock market. You have looked at the past\ndata, and you want to predict the price of the shares over the next few days. How would\nyou do it besides just eyeballing the data?\nFirst, you would plot the data points on a graph. Mathematically, we can denote these\ndata points as {x1, x2, . . . , x N}, where the indices n= 1,2, . . . , N can be treated as time\nstamps. We assume a simple model to describe the relationship between the xn\u2019s, say\nxn\u2248axn\u22121+bxn\u22122,\nfor some parameters \u03b8= (a, b).1This model assumes that the current value xncan be\napproximated by a linear combination of two previous values xn\u22121andxn\u22122. Therefore, if\nwe have x1andx2we should be able to predict x3, and if we have x2andx3we should be\nable to predict x4, etc. The magic of this prediction comes from the parameters aandb. If\nwe know aandb, the prediction can be done by simply plugging in the numbers.\nThe regression problem here is to estimate the parameters aandbfrom the data. Since\nwe are given a set of training data {x1, x2, . . . , x N}, we can check whether our predicted\nvaluebx3is close to the true x3, and whether our predicted value bx4is close to the true x4,\netc. This leads to the optimization\n(ba,bb) = argmin\na,bNX\nn=1\u0012\nxn\u2212(axn\u22121+bxn\u22122)| {z }\n=prediction\u00132\n,\nwhere we use initial conditions that x0=x\u22121= 0. The optimization problem requires us\nto minimize the disparity between xnand the predicted value axn\u22121+bxn\u22122, for all n.\nBy finding the ( a, b) that minimizes this objective function, we will accomplish our goal of\nestimating the best ( a, b).\nFigure 7.4 shows an example of predicting a random process using the above model.\nIf the parameters aandbare properly determined, we will obtain a reasonably well-fitted\ncurve to the data. A simple extrapolation to the future timestamp would suffice for the\nforecast task.\nPlan for this chapter\nWhat are the key ingredients of regression?\n\u0088Learning : Formulate the regression problem as an optimization problem, and\nsolve it by finding the best parameters.\n\u0088Inference : Use the estimated parameters and models to predict the unseen data\npoints.\nRegression is too broad a topic to be covered adequately in a single chapter. Accord-\ningly, we will present a few principles and a few practical algorithmic techniques that are\nbroadly applicable to many (definitely not all) regression tasks. These include the following.\n1Caution: If you lose money in the stock market by following this naive model, please do not cry. This\nmodel is greatly oversimplified and probably wrong.\n392", "408": "0 0.2 0.4 0.6 0.8 1-1-0.500.511.5\ndata\nbest fit\ncandidateFigure 7.4: An autoregression model aims at learning the model parameters based on the previous\nsamples. This example illustrates fitting the data using the model xn=axn\u22121+bxn\u22122, forn= 1, . . . , N .\n\u0088Theprinciple of regression (Section 7.1). We explain the formulation of a regression\nproblem via optimization. There are a few steps involved in developing this concept.\nFirst, we will exclusively focus on linear models because these models are easier to\nanalyze than nonlinear models but are still rich enough for many practical problems.\nWe will discuss how to solve the linear regression problem and some applications of\nthe solutions. We then address the issue of outliers using a concept called the robust\nlinear regression .\n\u0088Overfitting (Section 7.2). The biggest practical challenge of regression is overfitting .\nOverfitting occurs when a model fits too closely to the training samples so that it\nfails to generalize . We will delve deeply into the roots of overfitting and show that\noverfitting depends on three factors: the number of training samples N, the model\ncomplexity d, and the magnitude of noise \u03c32.\n\u0088Bias-variance trade-off (Section 7.3). We will present one of the most fundamental\nresults in learning theory, known as the bias-variance trade-off. It applies to allregres-\nsion problems, not just to linear models. Understanding this trade-off will help you\nunderstand the fundamental limits of your problem so that you know what to expect\nfrom the model.\n\u0088Regularization (Section 7.4). In this section we discuss a technique for combatting\noverfitting known as regularization . Regularization is carried out by adding an extra\nterm to the regression objective function. By solving the modified optimization, the\nregression solution is improved in two ways: (i) regularization makes the regression\nsolution less sensitive to noise perturbations, and (ii) it alleviates the fitting difficulty\nwhen we have only a few training samples. We will discuss two regularization strategies:\ntheridge regression and the LASSO regression .\nMuch of this chapter deals with optimization. If this is your first time reading this\nbook, we encourage you to have a reference book on linear algebra at hand.\n393", "409": "CHAPTER 7. REGRESSION\n7.1 Principles of Regression\nWe start by recalling our discussion in the introduction. The purpose of regression can be\nsummarized in a simple statement:\nGiven the data points ( x1, y1), . . . , (xN, yN), find the parameter \u03b8of a function g\u03b8(\u00b7)\nsuch that the training loss is minimized:\nb\u03b8= argmin\n\u03b8\u2208RdNX\nn=1L(yn, g\u03b8(xn))\n| {z }\ntraining loss Etrain(\u03b8), (7.2)\nwhere L(\u00b7,\u00b7) is the loss between a pair of true observation ynand the prediction g\u03b8(xn).\nWhen the context makes it clear, we will drop the subscript \u03b8ing\u03b8(\u00b7) with the understanding\nthat the function g(\u00b7) is parameterized by \u03b8.\nAs you can see, regression finds a function g(\u00b7) that best approximates the input-output\nrelationship between xnandyn. There are two choices we need to make when formulating\na regression problem:\n\u0088Function g(\u00b7): What is the family of functions we want to use? This could be a line, a\npolynomial, or a set of basis functions. If it is a polynomial, what is its order? We need\nto make all these decisions before running the regression. A poor choice of function\nfamily can lead to a poor regression result.\n\u0088Loss \u201cL(\u00b7,\u00b7)\u201d: How do we measure the closeness between ynandg(xn)? Are we measur-\ning in terms of the squared error ( yn\u2212g(xn))2, or the absolute difference |yn\u2212g(xn)|,\nor something else? Again, a poor choice of distance function can create a false sense\nof closeness because you might be optimizing for a wrong objective.\nBefore we delve into the details, we need to discuss briefly the connection between\nregression and probability. A regression problem can be solved without knowing probability,\nso why is regression discussed in a book on probability?\nThis question is related to how much we know about the statistical model and what\nkind of optimality we are seeking. A full answer requires some understanding of maximum\nlikelihood estimation and maximum a posteriori estimation, which will be explained in\nChapter 8. As a quick preview of our results, we summarize the key ideas below:\nHow is regression related to probability?\n\u0088If you know the statistical relationship between xnandyn, then we can construct\na regression problem that maximizes the likelihood of the underlying distribu-\ntion. Such regression solution is optimal with respect to the likelihood.\n\u0088We can construct a regression problem that can minimize the expectation of the\n394", "410": "7.1. PRINCIPLES OF REGRESSION\nsquared error. This regression solution is mean-squared optimal .\n\u0088If you are a Bayesian and you know the prior distribution of xn, then we can\nconstruct a regression problem that maximizes the posterior distribution. The\nsolution to this regression problem is Bayesian optimal .\n\u0088If you know nothing about the statistics of xnandyn, you can still run the\nregression and get something, and this \u201csomething\u201d can be very useful. However,\nyou cannot claim statistical optimality of this \u201csomething\u201d.\nSee Chapter 8 for additional discussion.\nIt is important to understand that a regression problem is at the intersection of op-\ntimization andstatistics . The need for optimization is clear because we need to minimize\nthe error. The statistical need is to generalize to unknown data. If there is no statistical\nrelationship between xnandyn(for all n), whatever model we obtain from the regression\nwill only work for the Ntraining samples. The model will not generalize because knowing\nxnwill not help us know yn. In other words, if there is no statistical relationship between\nxnandyn, you can fit perfectly to the training data but you will fail miserably to fit the\ntesting data.\n7.1.1 Intuition: How to fit a straight line?\nIn this subsection we want to give you the basic idea of how regression is formulated. To\nkeep things simple, we will discuss how to fit data using a straight line.\nConsider a collection of data points D={(x1, y1), . . . , (xN, yN)}, where xn\u2019s are the\ninputs and yn\u2019s are the observations, for example, in the table below.\nn x n yn\n1 0.6700 3.0237\n2 0.3474 2.3937\n3 0.6695 3.5548\n.........\nN\u22121 0.2953 2.6396\nN 0.6804 3.2536\nLet us consider the linear regression problem. The goal of linear regression is to find\nthestraight line that best fits the datasets. All straight lines on a 2D graph are plots of the\nequation\ng(x) =ax+b,\nwhere ais the slope of the line and bis the y-intercept of the line. We denote this line\nbyg(\u00b7). Note that this function gis characterized by two parameters ( a, b) because once\n(a, b) are known the line is determined. If we change ( a, b), the line will change as well.\nTherefore, by finding the bestline we are essentially searching for the best ( a, b) such that\nthe training error is minimized.\nThe pictorial meaning of linear regression can easily be seen in Figure 7.5 , which\nshows N= 50 data points according to some latent distributions. Given these 50 data\npoints, we construct several possible candidates for the regression model. These candidates\n395", "411": "CHAPTER 7. REGRESSION\nare characterized by the parameters ( a, b). For example, the parameters ( a, b) = (1 ,2) and\n(a, b) = (\u22122,3) represent two different straight lines in the candidate pool. The goal of the\nregression is to find the best line from these candidates. Note that since we limit ourselves\nto straight lines, the candidate set will not include polynomials or trigonometric functions.\nThese functions are outside the family we are considering.\n0 0.2 0.4 0.6 0.8 11.522.533.544.5\ndata\nbest fit\ncandidate\nFigure 7.5: The objective of least squares fitting (or linear regression) is to find a line that best fits the\ndataset.\nGiven these candidate functions, we need to measure the the training loss. This can\nbe defined in multiple ways, such as\n\u0088Sum-squared loss Etrain(\u03b8) =PN\nn=1(yn\u2212g(xn))2.\n\u0088Sum-absolute loss Etrain(\u03b8) =PN\nn=1|yn\u2212g(xn)|.\n\u0088Cross-entropy loss Etrain(\u03b8) =\u2212PN\nn=1(ynlogg(xn) + (1 \u2212yn) log(1 \u2212g(xn))).\n\u0088Perceptual loss Etrain(\u03b8) =PN\nn=1max(\u2212yng(xn),0), when ynandg(xn) are binary\ntaking values \u00b11. This is a reasonable training error because if ynmatches with g(xn),\nthen yng(xn) = 1 and so max( \u2212yng(xn),0) = 0. But if yndoes not match with g(xn),\nthen yng(xn) =\u22121 and hence max( \u2212yng(xn),0) = 1. Thus, the loss captures the sum\nof all the mismatched pairs.\nChoosing the loss function is problem-specific. It is also where probability enters the picture\nbecause, without any knowledge about the distributions of xnandyn, there is no way to\nchoose the best training loss. You can still pick one, as we will do, but it will not be granted\nany probabilistic guarantees.\nAmong these possible choices of the training error, we are going to focus on the sum-\nsquared loss because it is convex anddifferentiable . This makes the computation easy,\nsince we can run any textbook optimization algorithm. The regression problem under the\nsum-squared loss is:\n\u0010\nba,bb\u0011\n= argmin\n(a,b)NX\nn=1\u0012\nyn\u2212(axn+b)|{z}\n=g(xn)\u00132\n. (7.3)\nIn this equation, the symbol \u201cargmin\u201d means \u201cargument minimize\u201d, which returns the ar-\ngument that minimizes the cost function on the right. The interpretation of the equation is\n396", "412": "7.1. PRINCIPLES OF REGRESSION\nthat we seek the ( a, b) that minimize the sumPN\nn=1(yn\u2212(axn+b))2. Since we are mini-\nmizing the squared error, this linear regression problem is also known as the least squares\nfitting problem. The idea is summarized in the following box.\nWhat is linear least squares fitting?\n\u0088Find a line g(x) =ax+bthat best fits the training data {(xn, yn)}N\nn=1.\n\u0088The optimality criterion is to minimize the squared error\nEtrain(\u03b8) =NX\nn=1\u0012\nyn\u2212g(xn)\u00132\n, (7.4)\nwhere \u03b8= (a, b) is the model parameter.\n\u0088There exist other optimality criteria. Squared error is convex and differentiable.\n7.1.2 Solving the linear regression problem\nLet\u2019s consider how to solve the linear regression problem given by Equation (7.3). The\nproblem is the following:\u0010\nba,bb\u0011\n= argmin\n(a,b)Etrain(a, b). (7.5)\nAs with any two-dimensional optimization problem, the optimal point ( ba,bb) should\nhave a zero gradient, meaning that\n\u2202\n\u2202aEtrain(a, b) = 0 and\u2202\n\u2202bEtrain(a, b) = 0 .\nThis should be familiar to you, even if you have only learned basic calculus. This pair of\nequations says that, at a minimum point, the directional slopes should be zero no matter\nwhich direction you are looking at.\nThe derivative with respect to ais\n\u2202\n\u2202aEtrain(a, b)\n=\u2202\n\u2202a\u001aNX\nn=1\u0012\nyn\u2212(axn+b)\u00132\u001b\n=\u2202\n\u2202a\u001a\u0012\ny1\u2212(ax1+b)\u00132\n+\u0012\ny2\u2212(ax2+b)\u00132\n+\u00b7\u00b7\u00b7+\u0012\nyN\u2212(axN+b)\u00132\u001b\n= 2\u0012\ny1\u2212(ax1+b)\u0013\n(\u2212x1) +\u00b7\u00b7\u00b7+ 2\u0012\nyN\u2212(axN+b)\u0013\n(\u2212xN)\n= 2 \n\u2212NX\nn=1xnyn+aNX\nn=1x2\nn+bNX\nn=1xn!\n.\n397", "413": "CHAPTER 7. REGRESSION\nSimilarly, the derivative with respect to bis\n\u2202\n\u2202bEtrain(a, b) =\u2202\n\u2202b\u001aNX\nn=1\u0012\nyn\u2212(axn+b)\u00132\u001b\n= 2\u0012\ny1\u2212(ax1+b)\u0013\n(\u22121) +\u00b7\u00b7\u00b7+ 2\u0012\nyN\u2212(axN+b)\u0013\n(\u22121)\n= 2 \n\u2212NX\nn=1yn+aNX\nn=1xn+bNX\nn=11!\n.\nSetting these two equations to zero, we have that\n2 \n\u2212NX\nn=1ynxn+aNX\nn=1x2\nn+bNX\nn=1xn!\n= 0,\n2 \n\u2212NX\nn=1yn+aNX\nn=1xn+bNX\nn=11!\n= 0.\nRearranging the terms, the pair can be equivalently written as\n\uf8ee\n\uf8ef\uf8ef\uf8f0NP\nn=1x2\nnNP\nn=1xn\nNP\nn=1xn N\uf8f9\n\uf8fa\uf8fa\uf8fb\u0014a\nb\u0015\n=\uf8ee\n\uf8ef\uf8ef\uf8f0NP\nn=1xnyn\nNP\nn=1yn\uf8f9\n\uf8fa\uf8fa\uf8fb.\nTherefore, if we can solve this system of linear equations, we will have the linear regression\nsolution.\nRemark . It is easy to see that the solution achieves the minimum instead of the maximum,\nsince the second-order derivatives are positive:\n\u22022\n\u2202a2Etrain(a, b) =NX\nn=1x2\nn\u22650 and\u22022\n\u2202b2Etrain(a, b) =NX\nn=11>0.\nThe following theorem summarizes this intermediate result.\nTheorem 7.1. The solution of the problem Equation (7.5)\n\u0010\nba,bb\u0011\n=argmin\n(a,b)NX\nn=1\u0012\nyn\u2212(axn+b)\u00132\nsatisfies the equation\n\uf8ee\n\uf8ef\uf8ef\uf8f0NP\nn=1x2\nnNP\nn=1xn\nNP\nn=1xn N\uf8f9\n\uf8fa\uf8fa\uf8fb\u0014ba\nbb\u0015\n=\uf8ee\n\uf8ef\uf8ef\uf8f0NP\nn=1xnyn\nNP\nn=1yn\uf8f9\n\uf8fa\uf8fa\uf8fb. (7.6)\n398", "414": "7.1. PRINCIPLES OF REGRESSION\nMatrix-vector form of linear regression\nSolving this linear regression requires some basic linear algebra. The regression can be\nwritten as \uf8ee\n\uf8ef\uf8f0y1\n...\nyN\uf8f9\n\uf8fa\uf8fb\n|{z}\ny=\uf8ee\n\uf8ef\uf8f0x11\n......\nxN1\uf8f9\n\uf8fa\uf8fb\n|{z}\nX\u0014a\nb\u0015\n|{z}\n\u03b8+\uf8ee\n\uf8ef\uf8f0e1\n...\neN\uf8f9\n\uf8fa\uf8fb\n|{z}\ne.\nWith X,y,\u03b8ande, we can write the linear regression problem compactly as\ny=X\u03b8+e.\nTherefore, the training loss Etrain(\u03b8) can be defined as\nEtrain(\u03b8) =\u2225y\u2212X\u03b8\u22252\n=\r\r\r\r\r\r\r\uf8ee\n\uf8ef\uf8f0y1\n...\nyN\uf8f9\n\uf8fa\uf8fb\u2212\uf8ee\n\uf8ef\uf8f0x11\n......\nxN1\uf8f9\n\uf8fa\uf8fb\u0014a\nb\u0015\r\r\r\r\r\r\r2\n=NX\nn=1\u0012\nyn\u2212(axn+b)\u00132\n.\nNow, taking the gradient with respect to \u03b8yields2\n\u2207\u03b8Etrain(\u03b8) =\u2207\u03b8\u001a\n\u2225y\u2212X\u03b8\u22252\u001b\n=\u22122XT(y\u2212X\u03b8).\nEquating this to zero, we obtain\nXT(y\u2212X\u03b8) = 0 \u21d0\u21d2 XTX\u03b8=XTy. (7.7)\nEquation (7.7) is called the normal equation .\nThe normal equation is a convenient way of constructing the system of linear equations.\nUsing the 2-by-2 system shown in Equation (7.6) as an example, we note that\nXTX=\u0014\nx1\u00b7\u00b7\u00b7xN\n1\u00b7\u00b7\u00b7 1\u0015\uf8ee\n\uf8ef\uf8f0x11\n......\nxN1\uf8f9\n\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8f0NP\nn=1x2\nnNP\nn=1xn\nNP\nn=1xn N\uf8f9\n\uf8fa\uf8fa\uf8fb,\nXTy=\u0014\nx1\u00b7\u00b7\u00b7xN\n1\u00b7\u00b7\u00b7 1\u0015\uf8ee\n\uf8ef\uf8f0y1\n...\nyN\uf8f9\n\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8f0NP\nn=1xnyn\nNP\nn=1yn\uf8f9\n\uf8fa\uf8fa\uf8fb.\nTherefore, as long as you can construct the Xmatrix, forming the 2-by-2 system in Equa-\ntion (7.6) is straightforward: start with y=X\u03b8and then multiply the matrix transpose\nXTto both sides. The resulting system is what you need. There is nothing to memorize.\n2This is a basic vector calculus result. For details, you may consult standard texts such as the University\nof Waterloo\u2019s matrix cookbook. https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf\n399", "415": "CHAPTER 7. REGRESSION\nRunning linear regression on a computer\nOn a computer, solving the linear regression for a line is straightforward. Let us look at the\nMATLAB code first.\n% MATLAB code to fit data points using a straight line\nN = 50;\nx = rand(N,1)*1;\na = 2.5; % true parameter\nb = 1.3; % true parameter\ny = a*x + b + 0.2*rand(size(x)); % Synthesize training data\nX = [x(:) ones(N,1)]; % construct the X matrix\ntheta = X\\y(:); % solve y = X theta\nt = linspace(0, 1, 200); % interpolate and plot\nyhat = theta(1)*t + theta(2);\nplot(x,y,\u2019o\u2019,\u2019LineWidth\u2019,2); hold on;\nplot(t,yhat,\u2019r\u2019,\u2019LineWidth\u2019,4);\nIn this piece of MATLAB code, we need to define the data matrix X. Here, x(:) is the\ncolumn vector that stores all the values ( x1, . . . , x N). The all-one vector ones(N,1) is the\nsecond column in our Xmatrix. The command X\\y(:) is equivalent to solving the normal\nequation\nXTX\u03b8=XTy.\nThe last few lines are used to plot the predicted curve. Note that theta(1) andtheta(2)\nare the entries of the solution \u03b8. The result of this program is exactly the plot shown in\nFigure 7.5 above.\nIn Python, the program is quite similar. The command we use to solve the inversion\nisnp.linalg.lstsq .\n# Python code to fit data points using a straight line\nimport numpy as np\nimport matplotlib.pyplot as plt\nN = 50\nx = np.random.rand(N)\na = 2.5 # true parameter\nb = 1.3 # true parameter\ny = a*x + b + 0.2*np.random.randn(N) # Synthesize training data\nX = np.column_stack((x, np.ones(N))) # construct the X matrix\ntheta = np.linalg.lstsq(X, y, rcond=None)[0] # solve y = X theta\nt = np.linspace(0,1,200) # interpolate and plot\nyhat = theta[0]*t + theta[1]\nplt.plot(x,y,\u2019o\u2019)\nplt.plot(t,yhat,\u2019r\u2019,linewidth=4)\n400", "416": "7.1. PRINCIPLES OF REGRESSION\n7.1.3 Extension: Beyond a straight line\nRegression is a powerful technique. Although we have discussed its usefulness for fitting\nstraight lines, the same concept can fit other curves.\nTo generalize the regression formulation, we consider a d-dimensional regression coef-\nficient vector \u03b8= [\u03b80, . . . , \u03b8 d\u22121]T\u2208Rdand a general linear model\ng\u03b8(xn) =d\u22121X\np=0\u03b8p\u03d5p(xn).\nHere, the mappings {\u03d5p(\u00b7)}d\u22121\np=0can be considered as a nonlinear transformation that takes\nthe input xnand maps it to another value. For example, \u03d5p(\u00b7) = (\u00b7)pwill map an input x\nto apth power xp.\nWe can now write the system of linear equations as\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\ny=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03d50(x1)\u03d51(x1)\u00b7\u00b7\u00b7 \u03d5d\u22121(x1)\n\u03d50(x2)\u03d51(x2)\u00b7\u00b7\u00b7 \u03d5d\u22121(x2)\n... \u00b7\u00b7\u00b7......\n\u03d50(xN)\u03d51(xN)\u00b7\u00b7\u00b7\u03d5d\u22121(xN)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\nX\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b80\n\u03b81\n...\n\u03b8d\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n\u03b8+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\ne. (7.8)\nLet us look at some examples.\nExample 7.1 . (Quadratic fitting ) Consider the linear regression problem using a\nquadratic equation:\nyn=ax2\nn+bxn+c, n = 1, . . . , N.\nExpress this equation in matrix-vector form.\nSolution . The matrix-vector expression is\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0x2\n1x11\nx2\n2x21\n.........\nx2\nNxN1\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8f0a\nb\nc\uf8f9\n\uf8fb+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nThis is again in the form of y=X\u03b8+e.\nThe MATLAB and Python programs for Example 7.1 are shown below. A numerical\nexample is illustrated in Figure 7.6 .\n% MATLAB code to fit data using a quadratic equation\nN = 50;\nx = rand(N,1)*1;\na = -2.5;\nb = 1.3;\nc = 1.2;\n401", "417": "CHAPTER 7. REGRESSION\n0 0.2 0.4 0.6 0.8 100.511.522.5\ndata\nfitted curve\nFigure 7.6: Example: Our goal is to fit the dataset of 50 data points shown above. The model we use\nisg\u03b8(xn) =ax2\nn+bxn+c, forn= 1, . . . , N .\ny = a*x.^2 + b*x + c + 1*rand(size(x));\nN = length(x);\nX = [ones(N,1) x(:) x(:).^2];\nbeta = X\\y(:);\nt = linspace(0, 1, 200);\nyhat = theta(3)*t.^2 + theta(2)*t + theta(1);\nplot(x,y, \u2019o\u2019,\u2019LineWidth\u2019,2); hold on;\nplot(t,yhat,\u2019r\u2019,\u2019LineWidth\u2019,6);\n# Python code to fit data using a quadratic equation\nimport numpy as np\nimport matplotlib.pyplot as plt\nN = 50\nx = np.random.rand(N)\na = -2.5\nb = 1.3\nc = 1.2\ny = a*x**2 + b*x + c + 0.2*np.random.randn(N)\nX = np.column_stack((np.ones(N), x, x**2))\ntheta = np.linalg.lstsq(X, y, rcond=None)[0]\nt = np.linspace(0,1,200)\nyhat = theta[0] + theta[1]*t + theta[2]*t**2\nplt.plot(x,y,\u2019o\u2019)\nplt.plot(t,yhat,\u2019r\u2019,linewidth=4)\nThe generalization to polynomials of arbitrary order is to replace the model with\ng\u03b8(xn) =d\u22121X\np=0\u03b8pxp,\n402", "418": "7.1. PRINCIPLES OF REGRESSION\nwhere p= 0,1, . . . , d \u22121 represent the orders of the polynomials and \u03b80, . . . , \u03b8 d\u22121are the\nregression coefficients . In this case, the matrix system is\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f01x1\u00b7\u00b7\u00b7xd\u22121\n1\n1x2\u00b7\u00b7\u00b7xd\u22121\n2...\u00b7\u00b7\u00b7......\n1xN\u00b7\u00b7\u00b7xd\u22121\nN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b80\n\u03b81\n...\n\u03b8d\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nwhich again is in the form of y=X\u03b8+e.\nExample 7.2 . (Legendre polynomial fitting ) Let {Lp(\u00b7)}d\u22121\np=0be a set of Legendre\npolynomials (see discussions below), and consider the linear regression problem using\nyn=d\u22121X\np=0\u03b8pLp(x), n = 1, . . . , N.\nExpress this equation in matrix-vector form.\nSolution . The matrix-vector expression is\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0L0(x1)L1(x1)\u00b7\u00b7\u00b7 Ld\u22121(x1)\nL0(x2)L1(x2)\u00b7\u00b7\u00b7 Ld\u22121(x2)\n... \u00b7\u00b7\u00b7......\nL0(xN)L1(xN)\u00b7\u00b7\u00b7Ld\u22121(xN)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b80\n\u03b81\n...\n\u03b8d\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nLegendre polynomials are orthogonal polynomials. In conventional polynomials, the\nfunctions {x, x2, x3, . . . , xp}are not orthogonal. As we increase p, the set of functions\n{x, x2, x3, . . . , xp}will have redundancy, which will eventually result in the matrix Xbeing\nnoninvertible.\nThepth-order Legendre polynomial is denoted by Lp(x). Using the Legendre polyno-\nmials as the building block of the regression problem, the model is expressed as\ng\u03b8(x)def=d\u22121X\np=0\u03b8pLp(x)\n=\u03b80L0(x) +\u03b81L1(x)|{z}\n=x+\u03b82L2(x)|{z}\n=1\n2(3x2\u22121)+\u00b7\u00b7\u00b7+\u03b8d\u22121Ld\u22121(x),\nwhere L0(\u00b7),L1(\u00b7) and L2(\u00b7) are the Legendre polynomials of order 0, 1 and 2, respectively.\nAs an example, the first few leading Legendre polynomials are\nL0(x) = 1 ,\nL1(x) =x,\nL2(x) =1\n2(3x2\u22121),\nL3(x) =1\n2(5x3\u22123x).\n403", "419": "CHAPTER 7. REGRESSION\nThe order of the Legendre polynomials is always the same as that of the ordinary polyno-\nmials. The shapes of these polynomials are shown in Figure 7.7 (a).\n-1 -0.5 0 0.5 1-1-0.500.51\nL0(x)\nL1(x)\nL2(x)\nL3(x)\nL4(x)\n-1 -0.5 0 0.5 1-2-101234\ndata\nLegendre basis\nPolynomial basis\n(a) (b)\nFigure 7.7: (a) The first 5 leading Legendre polynomials plotted in the range of \u22121\u2264x\u22641. (b) Fitting\nthe data using an ordinary polynomial and a Legendre polynomial.\nFigure 7.7 (b) demonstrates a fitting problem using the Legendre polynomials. You\ncan see that the fitting is just as good as that of the ordinary polynomials (which should\nbe the case). However, if we compare the coefficients, we observe that the magnitude of\nthe Legendre coefficients is smaller (see Table 7.1 ). In general, as the order of polynomials\nincreases and the noise grows, the ordinary polynomials will become increasingly difficult to\nfit the data.\n\u03b84 \u03b83 \u03b82 \u03b81 \u03b80\nOrdinary polynomials 5.3061 3.3519 \u22123.6285 \u22121.8729 0.1540\nLegendre polynomials 1.2128 1.3408 0.6131 0.1382 0.0057\nTable 7.1: The regression coefficients of an ordinary polynomial and a Legendre polynomial. Note that\nwhile both polynomials can fit the data, the Legendre polynomial coefficients have smaller magnitudes.\nCalling Legendre polynomials for regression is not difficult in MATLAB and Python.\nSpecifically, one can call legendreP in MATLAB and scipy.special.eval_legendre in\nPython.\n% MATLAB code to fit data using Legendre polynomials\nN = 50;\nx = 1*(rand(N,1)*2-1);\na = [-0.001 0.01 +0.55 1.5 1.2];\ny = a(1)*legendreP(0,x) + a(2)*legendreP(1,x) + ...\n+ a(3)*legendreP(2,x) + a(4)*legendreP(3,x) + ...\n+ a(5)*legendreP(4,x) + 0.5*randn(N,1);\nX = [legendreP(0,x(:)) legendreP(1,x(:)) ...\nlegendreP(2,x(:)) legendreP(3,x(:)) ...\n404", "420": "7.1. PRINCIPLES OF REGRESSION\nlegendreP(4,x(:))];\nbeta = X\\y(:);\nt = linspace(-1, 1, 200);\nyhat = beta(1)*legendreP(0,t) + beta(2)*legendreP(1,t) + ...\n+ beta(3)*legendreP(2,t) + beta(4)*legendreP(3,t) + ...\n+ beta(5)*legendreP(4,t);\nplot(x,y,\u2019ko\u2019,\u2019LineWidth\u2019,2,\u2019MarkerSize\u2019,10); hold on;\nplot(t,yhat,\u2019LineWidth\u2019,6,\u2019Color\u2019,[0.9 0 0]);\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import eval_legendre\nN = 50\nx = np.linspace(-1,1,N)\na = np.array([-0.001, 0.01, 0.55, 1.5, 1.2])\ny = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \\\na[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \\\na[4]*eval_legendre(4,x) + 0.2*np.random.randn(N)\nX = np.column_stack((eval_legendre(0,x), eval_legendre(1,x), \\\neval_legendre(2,x), eval_legendre(3,x), \\\neval_legendre(4,x)))\ntheta = np.linalg.lstsq(X, y, rcond=None)[0]\nt = np.linspace(-1, 1, 50);\nyhat = theta[0]*eval_legendre(0,t) + theta[1]*eval_legendre(1,t) + \\\ntheta[2]*eval_legendre(2,t) + theta[3]*eval_legendre(3,t) + \\\ntheta[4]*eval_legendre(4,t)\nplt.plot(x,y,\u2019o\u2019,markersize=12)\nplt.plot(t,yhat, linewidth=8)\nplt.show()\nThe idea of fitting a set of data using the Legendre polynomials belongs to the larger\nfamily of basis functions . In general, we can use a set of basis functions to model the data:\ng\u03b8(x)def=d\u22121X\np=0\u03b8p\u03d5p(x),\nwhere {\u03d5p(x)}d\u22121\np=0are the basis functions and {\u03b8p}d\u22121\np=0are the regression coefficients. The\nconstant \u03b80is often called the biasof the regression.\nChoice of the \u03d5p(x) can be extremely broad. One can choose the ordinary polynomials\n\u03d5p(x) =xpor the Legendre polynomial \u03d5p(x) =Lp(x). Other choices are also available:\n\u0088Fourier basis: \u03d5p(x) =ej\u03c9px, where \u03c9pis the pth carrier frequency.\n\u0088Sinusoid basis: \u03d5p(x) = sin( \u03c9px), which is same as the Fourier basis but taking the\nimaginary part.\n405", "421": "CHAPTER 7. REGRESSION\n\u0088Gaussian basis: \u03d5p(x) =1\u221a\n2\u03c0\u03c32pexpn\n\u2212(x\u2212\u00b5p)2\n2\u03c32po\n, where ( \u00b5p, \u03c3p) are the model param-\neters.\nEvidently, by choosing different basis functions we have different ways to fit the data. There\nis no definitive answer as to which functions are better. Statistical techniques such as model\nselections are available, but experience will tell you to align with one and not the other. It\nis frequently more useful to have some domain knowledge rather than resorting to various\ncomputational techniques.\nHow to fit data using basis functions\n\u0088Construct this equation:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\ny=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03d50(x1)\u03d51(x1)\u00b7\u00b7\u00b7 \u03d5d\u22121(x1)\n\u03d50(x2)\u03d51(x2)\u00b7\u00b7\u00b7 \u03d5d\u22121(x2)\n... \u00b7\u00b7\u00b7......\n\u03d50(xN)\u03d51(xN)\u00b7\u00b7\u00b7\u03d5d\u22121(xN)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\nX\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b80\n\u03b81\n...\n\u03b8d\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n\u03b8+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\ne, (7.9)\n\u0088The functions \u03d5p(x) are the basis functions, e.g., \u03d5p(x) =xpfor ordinary poly-\nnomials.\n\u0088You can replace the polynomials with the Legendre polynomials.\n\u0088You can also replace the polynomials with other basis functions.\n\u0088Solve for \u03b8by\nb\u03b8= argmin\n\u03b8\u2225y\u2212X\u03b8\u22252.\nExample 7.3 . (Autoregressive model ) Consider a two-tap autoregressive model:\nyn=ayn\u22121+byn\u22122, n = 1,2, . . . , N\nwhere we assume y0=y\u22121= 0. Express this equation in the matrix-vector form.\nSolution . The matrix-vector form of the equation is\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=y=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y0 y\u22121\ny1 y0\n......\nyN\u22121yN\u22122\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\n=X\u0014a\nb\u0015\n|{z}\n=\u03b8+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nIn general, we can append more previous samples to predict the future. The general\n406", "422": "7.1. PRINCIPLES OF REGRESSION\nexpression is\nyn=LX\n\u2113=1\u03b8\u2113yn\u2212\u2113, n = 1,2, . . . , N,\nwhere \u2113= 1,2, . . . , L denote the previous Lsamples of the data and {\u03b81, . . . , \u03b8 L}are the\nregression coefficients. If we do this we see that the matrix expression is\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\ny3\ny4\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=y=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0y0 y\u22121y\u22122\u00b7\u00b7\u00b7 y1\u2212L\ny1 y0 y\u22121\u00b7\u00b7\u00b7 y2\u2212L\ny2 y1 y0\u00b7\u00b7\u00b7 y3\u2212L\ny3 y2 y1\u00b7\u00b7\u00b7 y4\u2212L\n...............\nyN\u22121yN\u22122yN\u22123...yN\u2212L\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\n=X\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b81\n\u03b82\n...\n\u03b8L\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=\u03b8+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\ne3\ne4\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb.\nObserve the pattern associated with this matrix X. Each column is a one-entry shifted\nversion of the previous column. This matrix is called a Toeplitz matrix .\nThe MATLAB (and Python) code for calling and using the Toeplitz matrix is shown\nbelow.\n% MATLAB code for auto-regressive model\nN = 500;\ny = cumsum(0.2*randn(N,1)) + 0.05*randn(N,1); % generate data\nL = 100; % use previous 100 samples\nc = [0; y(1:400-1)];\nr = zeros(1,L);\nX = toeplitz(c,r); % Toeplitz matrix\ntheta = X\\y(1:400); % solve y = X theta\nyhat = X*theta; % prediction\nplot(y(1:400), \u2019ko\u2019,\u2019LineWidth\u2019,2);hold on;\nplot(yhat(1:400),\u2019r\u2019,\u2019LineWidth\u2019,4);\n# Python code for auto-regressive model\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import toeplitz\nN = 500\ny = np.cumsum(0.2*np.random.randn(N)) + 0.05*np.random.randn(N)\nL = 100\nc = np.hstack((0, y[0:400-1]))\nr = np.zeros(L)\nX = toeplitz(c,r)\ntheta = np.linalg.lstsq(X, y[0:400], rcond=None)[0]\nyhat = np.dot(X, theta)\n407", "423": "CHAPTER 7. REGRESSION\nplt.plot(y[0:400], \u2019o\u2019)\nplt.plot(yhat[0:400],linewidth=4)\nThe plots generated by the above programs are shown in Figure 7.8 (a). Note that we\nare doing an interpolation , because we are predicting the values within the training dataset.\n0 100 200 300 400 500-6-4-202\n0 100 200 300 400 500-6-4-202\n(a) (b)\nFigure 7.8: Autoregressive model on a simulated dataset, using L= 100 coefficients. (a) Training data.\nNote that the model trains very well on this dataset. (b) Testing data. When tested on future data, the\nautoregressive model can still predict for a few samples but loses track when the time elapsed grows.\nWe now consider extrapolation . Given the training data, we can find the regression\ncoefficients by solving the above linear equation. This gives us \u03b8. To predict the future\nsamples we need to return to the equation\nbyn=LX\n\u2113=1\u03b8\u2113 byn\u2212\u2113|{z}\n=previous estimate, n = 1,2, . . . , N,\nwherebyn\u2212\u2113are the previous estimates. For example, if we are given 100 days of stock prices,\nthen predicting the 101st day\u2019s price should be based on the Ldays before the 101st. A\nsimple for-loop suffices for such a calculation.\nFigure 7.8 (b) shows a numerical example of extrapolating data using the autoregressive\nmodel. In this experiment we use N= 400 samples to train an autoregressive model of order\nL= 100. We then predict the data for another 100 data points. As you can see from the\nfigure, the first few samples still look reasonable. However, as time increases, the model\nstarts to lose track of the real trend.\nIs there any way we can improve the autoregressive model? A simple way is to increase\nthe memory Lso that we can use a long history to predict the future. This boils down to\nthe long-term running average of the curve, which works well in many cases. However, if\nthe testing data does not follow the same distribution as the training data (which is often\nthe case in the real stock market because unexpected news can change the stock price),\nthen even the long-term average will not be a good forecast. That is why data scientists on\nWall Street make so much money: they have advanced mathematical tools for modeling the\nstock market. Nevertheless, we hope that the autoregressive model provides you with a new\nperspective for analyzing data.\nThe summary below highlights the main ideas of the autoregressive model.\n408", "424": "7.1. PRINCIPLES OF REGRESSION\nWhat is the autoregressive model?\n\u0088It solves this problem\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\ny3\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=y=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0y0 y\u22121y\u22122\u00b7\u00b7\u00b7 y1\u2212L\ny1 y0 y\u22121\u00b7\u00b7\u00b7 y2\u2212L\ny2 y1 y0\u00b7\u00b7\u00b7 y3\u2212L\n...............\nyN\u22121yN\u22122yN\u22123...yN\u2212L\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\n=X\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b81\n\u03b82\n...\n\u03b8L\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=\u03b8+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\ne3\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=e. (7.10)\n\u0088The number of taps in the past history would affect the memory and hence the\nlong-term forecast.\n\u0088Solve for \u03b8by\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225y\u2212X\u03b8\u22252. (7.11)\n7.1.4 Overdetermined and underdetermined systems\nThe sub-section requires knowledge of some concepts in linear algebra that can be found\nin standard references.a\naCarl Meyer, Matrix Analysis and Applied Linear Algebra , SIAM, 2000.\nLet us now consider the theoretical properties of the least squares linear regression\nproblem, which is an optimization:\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225y\u2212X\u03b8\u22252. (P1)\nWe observe that the objective value of this optimization problem can go to zero if and only\nif the minimizer b\u03b8is the solution of the system of linear equations\nFind\u03b8such that y=X\u03b8. (P2)\nWe emphasize that Problem (P1) and Problem (P2) are two different problems. Even if we\ncannot solve Problem (P2), Problem (P1) is still well defined, but the objective value will\nnot go to zero. This subsection aims to draw the connection between the two problems and\ndiscuss the respective solutions. We will start with Problem (P2) by considering two shapes\nof the matrix X.\nOverdetermined system\nProblem (P2) is called overdetermined ifX\u2208RN\u00d7dis tall and skinny, i.e., N > d . This hap-\npens when you have more rows than columns, or equivalently when you have more equations\nthan unknowns. When N > d , Problem (P2) has a unique solution b\u03b8= (XTX)\u22121XTyif\n409", "425": "CHAPTER 7. REGRESSION\nand only if XTXis invertible, or equivalently if and only if the columns of Xare linearly in-\ndependent. A technical description of this is that Xhas a full rank , denoted by rank( X) =d.\nWhen rank( X) =d, Problem (P1) has a unique global minimizer b\u03b8= (XTX)\u22121XTy, which\nis the same as the unique solution of Problem (P2).\nFigure 7.9: Hierarchy of the solutions of an overdetermined system. An overdetermined system uses\na tall and skinny matrix X. The rank of a matrix Xis defined as the largest number of independent\ncolumns we can find in X. If rank (X) =d, the matrix XTXis invertible, and Problem (P2) will have\na unique solution. If rank (X)< d, then the solution depends on whether the particular observation y\nlives in the range space of X. If yes, Problem (P2) will have infinitely many solutions because there is\na nontrivial null space. If no, Problem (P2) will have no solution because the system is incompatible.\nIf the columns of Xare linearly dependent so that XTXis not invertible, we say\nthatXisrank-deficient (denoted as rank( X)< d). In this case, Problem (P2) may not\nhave a solution. We say that it may not have a solution because it is still possible to have a\nsolution. It all depends on whether ycan be written as a linear combination of the linearly\nindependent columns of X.\nIf yes, we say that ylives in the range space ofX. The range space of Xis defined\nas the set of vectors {z|z=X\u03b1,for some \u03b1}. If rank( X) =d, allywill live in the range\nspace of X. But if rank( X)< d, only some of the ywill live in the range space of X.\nWhen this happens, the matrix Xmust have a nontrivial null space . The null space of X\nis defined as the set of vectors {z|Xz= 0}. A nontrivial null space will give us infinitely\nmany solutions to Problem (P2). This is because if \u03b1is the solution found in the range\nspace so that y=X\u03b1, then we can pick any zfrom the null space such that Xz= 0.\nThis will lead to another solution \u03b1+zsuch that X(\u03b1+z) =X\u03b1+ 0 = y. Since we have\ninfinitely many choices of such z\u2019s, there will be infinitely many solutions to Problem (P2).\nAlthough there are infinitely many solutions to Problem (P2), all of them are the global\nminimizers of Problem (P1). They can make the objective value equal to zero because the\nequality y=X\u03b8holds. However, the solutions to Problem (P2) are not unique since the\nobjective function is convex but not strictly convex.\nIfydoes not live in the range space of X, we say that Problem (P2) is incompatible .\nIf a system of linear equations is incompatible, there is no solution. However, even when\nthis happens, we can still solve the optimization Problem (P1), but the objective value will\nnot reach 0. The minimizer is a global minimizer because the objective function is convex,\n410", "426": "7.1. PRINCIPLES OF REGRESSION\nbut the minimizer is not unique.\nUnderdetermined system\nProblem (P2) is called underdetermined ifXis fat and short, i.e., N < d . This happens\nwhen you have more columns than rows, or equivalently when you have more unknowns than\nequations. In this case, XTXis not invertible, and so we cannot use b\u03b8= (XTX)\u22121XTy\nas the solution. However, if rank( X) =N, then anyywill live in the range space of X. But\nbecause Xis fat and short, there exists a nontrivial null space. Therefore, Problem (P2)\nwill have infinitely many solutions, attributed to the vectors generated by the null space.\nFor this set of infinitely many solutions, the corresponding Problem (P1) will have a global\nminimizer, and the objective value will be zero. However, the minimizer is not unique. This\nis the first case in Figure 7.10 .\nFigure 7.10: Hierarchy of the solutions of an underdetermined system. An underdetermined system uses\na fat and short matrix X. The rank of a matrix Xis defined as the largest number of independent\ncolumns we can find in X. If rank (X) =N, we will have infinitely many solutions. If rank (X)< N ,\nthen the solutions depends on whether the particular observation ylives in the range space of X. If yes,\nProblem (P2) will have infinitely many solutions because there is a nontrivial null space. If no, Problem\n(P2) will have no solution because the system is incompatible.\nThere are two other cases in Figure 7.10 , which occur when rank( X)< N:\n\u0088(i) Ifyis in the range space of X, Problem (P2) will have infinitely many solutions.\nSince Problem (P2) remains feasible, the objective function of Problem (P1) will go\nto zero.\n\u0088(ii) If yis not in the range space of X, the system in Problem (P2) is incompatible\nand there will be no solution. The objective value of Problem (P1) will not go to zero.\nIf an underdetermined system has infinitely many solutions, we need to pick and choose.\nOne of the possible approaches is to consider the optimization\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225\u03b8\u22252subject to X\u03b8=y. (P3)\nThis optimization is different from Problem (P1), which is an unconstrained optimization.\nOur goal is to minimize the deviation between X\u03b8andy. Problem (P3) is constrained . Since\n411", "427": "CHAPTER 7. REGRESSION\nwe assume that Problem (P2) has infinitely many solutions, the constraint set y=X\u03b8\nis feasible. Among all the feasible choices, we pick the one that minimizes the squared\nnorm. Therefore, the solution to Problem (P3) is called the minimum-norm least squares.\nTheorem 7.2 below summarizes the solution. If ydoes not live in the range space of X,\nthen Problem (P2) does not have a solution. Therefore, the constraint in P3 is infeasible,\nand hence the optimization problem does not have a minimizer.\nTheorem 7.2. Consider the underdetermined linear regression problem where N < d :\nb\u03b8=argmin\n\u03b8\u2208Rd\u2225\u03b8\u22252subject to y=X\u03b8,\nwhere X\u2208RN\u00d7d,\u03b8\u2208Rd, and y\u2208RN. If rank (X) =N, then the linear regression\nproblem will have a unique global minimum\nb\u03b8=XT(XXT)\u22121y. (7.12)\nThis solution is called the minimum-norm least-squares solution.\nProof . The proof of the theorem requires some knowledge of constrained optimization.\nConsider the Lagrangian of the problem:\nL(\u03b8,\u03bb) =\u2225\u03b8\u22252+\u03bbT(X\u03b8\u2212y),\nwhere \u03bbis called the Lagrange multiplier. The solution of the constrained optimization is\nthe stationary point of the Lagrangian. To find the stationary point, we take the derivatives\nwith respect to \u03b8and\u03bb. This yields\n\u2207\u03b8L= 2\u03b8+XT\u03bb= 0,\n\u2207\u03bbL=X\u03b8\u2212y= 0.\nThe first equation gives us \u03b8=\u2212XT\u03bb/2. Substituting it into the second equation, and\nassuming that rank( X) =Nso that XTXis invertible, we have\nX\u0010\n\u2212XT\u03bb/2\u0011\n\u2212y= 0,\nwhich implies that \u03bb=\u22122(XXT)\u22121y. Therefore, \u03b8=XT(XXT)\u22121y. \u25a1\nThe end of this subsection. Please join us again.\n7.1.5 Robust linear regression\nThis subsection is optional for a first reading of the book.\nThe linear regression we have discussed so far is based on an important criterion,\nnamely the squared error criterion. We chose the squared error as the training loss because\n412", "428": "7.1. PRINCIPLES OF REGRESSION\nit is differentiable and convex. Differentiability allows us to take the derivative and locate\nthe minimum point. Convexity allows us to claim a global minimizer (also unique if the\nobjective function is strictly convex). However, such a nice criterion suffers from a serious\ndrawback: the issue of outliers .\nConsider Figure 7.11 . InFigure 7.11 (a), we show a regression problem for N= 50\ndata points. Our basis functions are the ordinary polynomials in the fourth order. Everything\nlooks fine in the figure. We intervene in the data by randomly altering a few of them so that\ntheir values are off. There are only a handful of these outliers. We run the same regression\nanalysis again, but we observe (see Figure 7.11 (b)) that our fitted curve has been distorted\nquite significantly.\n-1 -0.5 0 0.5 1-2-101234\ndata\nfitted curve\n-1 -0.5 0 0.5 1-2-1012345\ndata\nfitted curve\n(a) (\u00b7)2without outlier (b) ( \u00b7)2with outlier\nFigure 7.11: Linear regression using the squared error as the training loss suffers from outliers. (a) The\nregression performs well when there is no outlier. (b) By adding only a few outliers, the regression curve\nhas already been distorted.\nThis occurs because of the squared error. By the definition of a squared error, our\ntraining loss is\nEtrain(\u03b8) =NX\nn=1\u0012\nyn\u2212g\u03b8(xn)\u00132\n.\nWithout loss of generality, let us assume that one of these error terms is large because of an\noutlier. Then the training loss becomes\nEtrain(\u03b8) =\u0012\ny1\u2212g\u03b8(x1)\u00132\n| {z }\nsmall+\u0012\ny2\u2212g\u03b8(x2)\u00132\n| {z }\nsmall+\u0012\ny3\u2212g\u03b8(x3)\u00132\n| {z }\nlarge+\u00b7\u00b7\u00b7+\u0012\nyN\u2212g\u03b8(xN)\u00132\n| {z }\nsmall.\nHere is the daunting fact: If one or a few of these individual error terms are large, the\nsquare operation will amplify them. As a result, the error you see is not just large but large2.\nMoreover, since we put the squares to the small errors as well, we have small2instead of\nsmall. When you try to weigh the relative significance between the outliers and the normal\ndata points, the outliers suddenly have a very large contribution to the error. Since the goal\nof linear regression is to minimize the total loss, the presence of the outliers will drive the\noptimization solution to compensate for the large error.\n413", "429": "CHAPTER 7. REGRESSION\nOne possible solution is to replace the squared error by the absolute error , such that\nEtrain(\u03b8) =NX\nn=1\f\f\f\fyn\u2212g\u03b8(xn)\f\f\f\f.\nThis is a simple modification, but it is very effective. The reason is that the absolute error\nkeeps the small just as small, and keeps the large just as large. There is no amplification.\nTherefore, while the outliers still contribute to the overall loss, their contributions are less\nprominent. (If you have a lot of strong outliers, even the absolute error will fail. If this\nhappens, you should go back to your data collection process and find out what has gone\nwrong.)\nWhen we use the absolute error as the training loss, the resulting regression problem is\ntheleast absolute deviation regression (or simply the robust regression ). The tricky thing\nabout the least absolute deviation is that the training loss is not differentiable. In other\nwords, we cannot take the derivative and find the optimal solution. The good news is that\nthere exists an alternative approach for solving this problem: using linear programming\n(implemented via the simplex method ).\nSolving the robust regression problem\nLet us focus on the linear model\ng\u03b8(xn) =xT\nn\u03b8,\nwhere xn= [\u03d50(xn), . . . , \u03d5 d\u22121(xn)]T\u2208Rdis the nth input vector for some basis functions\n{\u03d5p}d\u22121\np=0, and \u03b8= [\u03b80, . . . , \u03b8 d\u22121]T\u2208Rdis the parameter. Substituting this into the training\nloss, the optimization problem is\nminimize\n\u03b8\u2208RdNX\nn=1\f\f\f\fyn\u2212xT\nn\u03b8\f\f\f\f.\nHere is an important trick. The idea is to express the problem as an equivalent problem\nminimize\n\u03b8\u2208Rd,u\u2208RNNX\nn=1un\nsubject to un=|yn\u2212xT\nn\u03b8|, n = 1, . . . , N.\nThere is a small but important difference between this problem and the previous one. In the\nfirst problem, there is only one optimization variable \u03b8. In the new problem, we introduce an\nadditional variable u= [u1, . . . , u N]Tand add a constraint un=|yn\u2212xT\nn\u03b8|forn= 1, . . . , N .\nWe introduce uso that we can have some additional degrees of freedom. At the optimal\nsolution, unmust equal to |yn\u2212xT\nn\u03b8|, and so the corresponding \u03b8is the solution of the\noriginal problem.\nNow we note that x=|a|is equivalent to x\u2265aandx\u2265 \u2212a. Therefore, the constraint\ncan be equivalently written as\nminimize\n\u03b8\u2208Rd,u\u2208RNNX\nn=1un, (7.13)\nsubject to un\u2265 \u2212(yn\u2212xT\nn\u03b8), n = 1, . . . , N\nun\u2265(yn\u2212xT\nn\u03b8), n = 1, . . . , N.\n414", "430": "7.1. PRINCIPLES OF REGRESSION\nIn other words, we have rewritten the equality constraint as a pair of inequality constraints\nby removing the absolute signs.\nThe optimization in Equation (7.13) is in the form of a standard linear programming\nproblem. A linear programming problem takes the form of\nminimize\nx\u2208RkcTx (7.14)\nsubject to Ax\u2264b,\nfor some vectors c\u2208Rk,b\u2208Rm, and matrix A\u2208Rm\u00d7k. Linear programming is a stan-\ndard optimization problem that you can find in most optimization textbooks. On a com-\nputer, if we know c,bandA, solving the linear programming problem can be done using\nbuilt-in commands. For MATLAB, the command is linprog . For Python, the command is\nscipy.optimize.linprog . We will discuss a concrete example shortly.\n% MATLAB command for linear programming\nx = linprog(c, A, b);\n# Python command for linear programming\nlinprog(c, A, b, bounds=(None,None), method=\"revised simplex\")\nGiven Equation (7.13), the question becomes how to convert it into the standard linear\nprogramming format. This requires two steps. The first step uses the objective function :\nNX\nn=1un=d\u22121X\np=0(0)(\u03b8p) +NX\nn=1(1)(un)\n=\u00020 0 \u00b7\u00b7\u00b7 0 1 1 \u00b7\u00b7\u00b7 1\u0003\n| {z }\n=cT\u0014\n\u03b8\nu\u0015\n.\nTherefore, the vector chasd0\u2019s followed by N1\u2019s.\nThe second step concerns the constraint . It can be shown that un\u2265 \u2212(yn\u2212xT\nn\u03b8) is\nequivalent to xT\nn\u03b8\u2212un\u2264yn. Written in the matrix form, we have\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0xT\n1\u22121 0 \u00b7\u00b7\u00b7 0\nxT\n20\u22121\u00b7\u00b7\u00b7 0\n.........\u00b7\u00b7\u00b7...\nxT\nN0 0 \u00b7\u00b7\u00b7 \u2212 1\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b8\nu1\n...\nuN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\u2264\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nwhich is equivalent to\n\u0002\nX\u2212I\u0003\u0014\u03b8\nu\u0015\n\u2264y, (7.15)\nwhere I\u2208RN\u00d7Nis the identity matrix.\nSimilarly, the other constraint un\u2265(yn\u2212xT\nn\u03b8) is equivalent to \u2212xT\nn\u03b8\u2212un\u2264 \u2212yn.\nWritten in the matrix form, we have\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u2212xT\n1\u22121 0 \u00b7\u00b7\u00b7 0\n\u2212xT\n20\u22121\u00b7\u00b7\u00b7 0\n.........\u00b7\u00b7\u00b7...\n\u2212xT\nN0 0 \u00b7\u00b7\u00b7 \u2212 1\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b8\nu1\n...\nuN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\u2264\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u2212y1\n\u2212y2\n...\n\u2212yN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\n415", "431": "CHAPTER 7. REGRESSION\nwhich is equivalent to\n\u0002\u2212X\u2212I\u0003\u0014\u03b8\nu\u0015\n\u2264 \u2212y\nPutting everything together, we have finally arrived at the linear programming problem\nminimize\n\u03b8\u2208Rd,u\u2208RN\u00020d1N\u0003\u0014\u03b8\nu\u0015\nsubject to\u0014\nX\u2212I\n\u2212X\u2212I\u0015\u0014\n\u03b8\nu\u0015\n\u2264\u0014\ny\n\u2212y\u0015\n,\nwhere 0d\u2208Rdis an all-zero vector, and 1N\u2208RNis an all-one vector. It is this problem\nthat solves the robust linear regression.\nLet us look at how to implement linear programming to solve the robust regression\noptimization. As an example, we continue with the polynomial fitting problem in which\nthere are outliers. We choose the ordinary polynomials as the basis functions. To construct\nthe linear programming problem, we need to define the matrix Aand the vectors cand\nbaccording to the linear programming form. This is done using the following MATLAB\nprogram.\n% MATLAB code to demonstrate robust regression\nN = 50;\nx = linspace(-1,1,N)\u2019;\na = [-0.001 0.01 0.55 1.5 1.2];\ny = a(1)*legendreP(0,x) + a(2)*legendreP(1,x) + ...\na(3)*legendreP(2,x) + a(4)*legendreP(3,x) + ...\na(5)*legendreP(4,x) + 0.2*randn(N,1);\nidx = [10, 16, 23, 37, 45];\ny(idx) = 5;\nX = [x(:).^0 x(:).^1 x(:).^2 x(:).^3 x(:).^4];\nA = [X -eye(N); -X -eye(N)];\nb = [y(:); -y(:)];\nc = [zeros(1,5) ones(1,N)]\u2019;\ntheta = linprog(c, A, b);\nt = linspace(-1,1,200)\u2019;\nyhat = theta(1) + theta(2)*t(:) + ...\ntheta(3)*t(:).^2 + theta(4)*t(:).^3 + ...\ntheta(5)*t(:).^4;\nplot(x,y, \u2019ko\u2019,\u2019LineWidth\u2019,2); hold on;\nplot(t,yhat,\u2019r\u2019,\u2019LineWidth\u2019,4);\nIn this set of commands, the basis vectors are defined as xT\nn= [\u03d54(xn), . . . , \u03d5 0(xn)]T, for\nn= 1, . . . , N . The matrix Iis constructed by using the command eye(N) , which constructs\nthe identity matrix of size N\u00d7N. The rest of the commands are self-explanatory. Note that\nthe solution to the linear programming problem consists of both \u03b8andu. To squeeze \u03b8we\nneed to locate the first dentries. The remainder is u.\nCommands for Python are similar, although we need to call np.hstack andnp.vstack\nto construct the matrices and vectors. The main routine is linprog in the scipy.optimize\n416", "432": "7.1. PRINCIPLES OF REGRESSION\nlibrary. Note that for this particular example, the bounds are bounds=(None,None) , or\notherwise Python will search in the positive quadrant.\n# Python code to demonstrate robust regression\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import eval_legendre\nfrom scipy.optimize import linprog\nN = 50\nx = np.linspace(-1,1,N)\na = np.array([-0.001, 0.01, 0.55, 1.5, 1.2])\ny = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \\\na[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \\\na[4]*eval_legendre(4,x) + 0.2*np.random.randn(N)\nidx = [10,16,23,37,45]\ny[idx] = 5\nX = np.column_stack((np.ones(N), x, x**2, x**3, x**4))\nA = np.vstack((np.hstack((X, -np.eye(N))), np.hstack((-X, -np.eye(N)))))\nb = np.hstack((y,-y))\nc = np.hstack((np.zeros(5), np.ones(N)))\nres = linprog(c, A, b, bounds=(None,None), method=\"revised simplex\")\ntheta = res.x\nt = np.linspace(-1,1,200)\nyhat = theta[0]*np.ones(200) + theta[1]*t + theta[2]*t**2 + \\\ntheta[3]*t**3 + theta[4]*t**4\nplt.plot(x,y,\u2019o\u2019,markersize=12)\nplt.plot(t,yhat, linewidth=8)\nplt.show()\nThe result of this experiment is shown in Figure 7.12 . It is remarkable to see that the\nrobust regression result is almost as good as the result would be without outliers.\nIf robust linear regression performs so well, why don\u2019t we use it all the time? Why\nis least squares regression still more popular? The answer has a lot to do with the com-\nputational complexity and the uniqueness of the solution. Linear programming requires an\nalgorithm for a solution. While we have very fast linear programming solvers today, the com-\nputational cost of solving a linear program is still much higher than solving a least-squares\nproblem (which is essentially a matrix inversion).\nThe other issue with robust linear regression is the uniqueness of the solution. Lin-\near programming is known to have degenerate solutions when the constraint set (a high-\ndimensional polygon) touches the objective function (which is a line) at one of its edges.\nThe least-squares fitting does not have this problem because the optimization surface is a\nparabola. Unless the matrix XTXis noninvertible, the solution is guaranteed to be the\nunique global minimum. Linear programming does not have this convenient property. We\ncan have multiple solutions \u03b8that give the same objective value. If you try to interpret your\nresult by inspecting the magnitude of the \u03b8\u2019s, the nonuniqueness of the solution would cause\nproblems because your interpretation can be swiped immediately if the linear programming\ngives you a nonunique solution.\n417", "433": "CHAPTER 7. REGRESSION\n-1 -0.5 0 0.5 1-2-1012345\n-1 -0.5 0 0.5 1-2-1012345\n(a) Ordinary ( \u00b7)2regression with outliers (b) Robust | \u00b7 |regression with outliers\nFigure 7.12: (a) Ordinary linear regression using (\u00b7)2as the training loss. In the absence of any outlier,\nthe regression performs well. (b) Robust linear regression using | \u00b7 |as the training loss. Note that even\nin the presence of outliers, the robustness regression perform reasonably well.\nEnd of this subsection. Please join us again.\nClosing remark . The principle of linear regression is primarily to set up a function to fit\nthe data. This, in turn, is about finding a set of good basis functions and minimizing the\nappropriate training loss. Selecting the basis is usually done in several ways:\n\u0088The problem forces you to choose certain basis functions. For example, suppose you\nare working on a disease dataset. The variates are height, weight, and BMI. You do\nnot have any choice here because your goal is to see which factor contributes the most\nto the cause of the disease.\n\u0088There are known basis functions that work. For example, suppose you are working on\na speech dataset. Physics tells us that Fourier bases are excellent representations of\nthese sinusoidal functions. So it would make more sense to use the Fourier basis than\nthe polynomials.\n\u0088Sometimes the basis can be learned from the data. For example, you can run principal-\ncomponent analysis (PCA) to extract the basis. Then you can run the linear regression\nto compute the coefficients. This is a data-driven approach and could apply to some\nproblems.\n7.2 Overfitting\nThe regression principle we have discussed in the previous section is a powerful technique\nfor data analysis. However, there are many ways in which things can fall apart. We have\nseen the problem of outliers, where perturbations of one or a few data points would result\nin a big change in the regression result, and we discussed some techniques to overcome the\n418", "434": "7.2. OVERFITTING\noutlier problem, e.g., using robust regression. In addition to outliers, there are other causes\nof the failure of the regression.\nIn this section, we examine the relationship between the number of training samples\nand the complexity of the model. For example, if we decide to use polynomials as the basis\nfunctions and we have only N= 20 data points, what should be the order of the polynomials?\nShall we use the 5th-order polynomial, or shall we use the 20th-order? Our goal in this section\nis to acquire an understanding of the general problem known as overfitting . Then we will\ndiscuss methods for mitigating overfitting in Section 7.4.\n7.2.1 Overview of overfitting\nImagine that we have a dataset containing N= 20 training samples. We know that the data\nare generated from a fourth-order polynomial with Legendre polynomials as the basis. On\ntop of these samples, we also know that a small amount of noise corrupts each sample, for\nexample, Gaussian noise of standard deviation \u03c3= 0.1.\nWe have two options here for fitting the data:\n\u0088Option 1: h(x) =P4\np=0\u03b8pLp(x), which is a 4th-order polynomial.\n\u0088Option 2: g(x) =P50\np=0\u03b8pLp(x), which is a 50th-order polynomial.\nModel 2 is more expressive because it has more degrees of freedom. Let us fit the data using\nthese two models. Figure 7.13 shows the results. However, what is going on with the 50th-\norder polynomial? It has gone completely wild. How can the regression ever choose such a\nterrible model?\n-1 -0.5 0 0.5 1-3-2-10123\ndata\nfitted curve\n-1 -0.5 0 0.5 1-3-2-10123\ndata\nfitted curve\n(a) 4th-order polynomial (b) 50th-order polynomial\nFigure 7.13: Fitting data using a 4th-order polynomial and a 50th-order polynomial.\nHere is an even bigger surprise: If we compute the training loss, we get\nEtrain(h) =1\nNNX\nn=1\u0012\nyn\u2212h(xn)\u00132\n= 0.0063,\nEtrain(g) =1\nNNX\nn=1\u0012\nyn\u2212g(xn)\u00132\n= 5.7811\u00d710\u221224.\n419", "435": "CHAPTER 7. REGRESSION\nThus, while Model 2 looks wild in the figure, it has a much lower training loss than Model 1.\nSo according to the training loss, Model 2 fits better.\nAny sensible person at this point will object, since Model 2 cannot possibly be better,\nfor the following reason. It is not because it \u201clooks bad\u201d, but because if you test the model\nwith an unseen sample it is almost certain that the testing error will explode. For example,\ninFigure 7.13 (a) if we look at x= 0, we would expect the predicted value to be close\ntoy= 0. However, Figure 7.13 (b) suggests that the predicted value is going to negative\ninfinity. It would be hard to believe that the negative infinity is a better prediction than the\nother one. We refer to this general phenomenon of fitting very well to the training data but\ngeneralizing poorly to the testing data as overfitting .\nWhat is overfitting?\nOverfitting means that a model fits too closely to the training samples so that it\nfails to generalize.\nOverfitting occurs as a consequence of an imbalance between the following three factors:\n\u0088Number of training samples N. If you have many training samples, you should learn\nvery well, even if the model is complex. Conversely, if the model is complex but does\nnot have enough training samples, you will overfit it. The most serious problem in\nregression is often insufficient training data.\n\u0088Model order d. This refers to the complexity of the model. For example, if your model\nuses a polynomial, drefers to the order of the polynomial. If your training set is too\nsmall, you need to use a less complex model. The general rule of thumb is: \u201cless is\nmore\u201d.\n\u0088Noise variance \u03c32. This refers to the variance of the error enyou add to the data.\nThe model we assumed in the previous numerical experiment is that\nyn=g(xn) +en, n = 1, . . . , N.\nwhere en\u223cGaussian(0 , \u03c32). If \u03c3increases, it is inevitable that the fitting will be-\ncome more difficult. Hence it would require more training samples, and perhaps a less\ncomplex model would work better.\n7.2.2 Analysis of the linear case\nLet us spell out the details of these factors one by one. To make our discussion concrete, we\nwill use linear regression as a case study. The general analysis will be presented in the next\nsection.\nNotations\n\u0088Ground Truth Model . To start with, we assume that we have a population set D\ncontaining infinitely many samples ( x, y) drawn according to some latent distributions.\nThe relationship between xandyis defined through an unknown target function\ny=f(x) +e,\n420", "436": "7.2. OVERFITTING\nwhere e\u223cGaussian(0 , \u03c32) is the noise. For our analysis, we assume that f(\u00b7) is linear,\nso that\nf(x) =xT\u03b8\u2217,\nwhere \u03b8\u2217\u2208Rdis the ground truth model parameter. Notice that f(\u00b7) is deterministic,\nbuteis random. Therefore, any randomness we see in yis due to e.\n\u0088Training and Testing Set . From D, we construct two datasets: the training data set\nDtrainthat contains training samples {(x1, y1), . . . , (xN, yN)}and the testing dataset\nDtestthat contains {(x1, y1), . . . , (xM, yM)}. Both DtrainandDtestare subsets of D.\n\u0088Predictive Model . We consider a predictive model g\u03b8(\u00b7). For simplicity, we assume\nthatg\u03b8(\u00b7) is also linear:\ng\u03b8(x) =xT\u03b8.\nGiven the training dataset D={(x1, y1), . . . , (xN, yN)}, we construct a linear regres-\nsion problem:\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252.\nThroughout our analysis, we assume that N\u2265dso that we have more training data\nthan the number of unknowns. We further assume that XTXis invertible, and so\nthere is a unique global minimizer\nb\u03b8= (XTX)\u22121XTy.\n\u0088Training Error . Given the estimated model parameter b\u03b8, we define the in-sample\nprediction as\nbytrain=Xtrainb\u03b8,\nwhere Xtrain=Xis the training data matrix. The in-sample prediction is the pre-\ndicted value using the trained model for the training data. The corresponding error\nwith respect to the ground truth is called the training error :\nEtrain(b\u03b8) =Ee\u00141\nN\u2225bytrain\u2212y\u22252\u0015\n,\nwhere Nis the number of training samples in the training dataset. Note that the\nexpectation is taken with respect to the noise vector e, which follows the distribution\nGaussian(0 , \u03c32I).\n\u0088Testing Error . During testing, we construct a testing matrix Xtest. This gives us the\nestimated values bytest:\nbytest=Xtestb\u03b8.\nThe out-sample prediction is the predicted value using the trained model for the testing\ndata. The corresponding error with respect to the ground truth is called the testing\nerror:\nEtest(b\u03b8) =Ee\u00141\nM\u2225bytest\u2212y\u22252\u0015\n,\nwhere Mis the number of testing samples in the testing dataset.\n421", "437": "CHAPTER 7. REGRESSION\nAnalysis of the training error\nWe first analyze the training error, which we defined as\nEtrain=Ee\u00141\nN\u2225by\u2212y\u22252\u0015\ndef= MSE( by,y). (7.16)\nFor this particular choice of the training error, we call it the mean squared error (MSE). It\nmeasures the difference between byandy.\nTheorem 7.3. Let\u03b8\u2217\u2208Rdbe the ground truth linear model parameter, and X\u2208\nRN\u00d7dbe a matrix such that N\u2265dandXTXis invertible. Assume that the data\nfollows the linear model y=X\u03b8\u2217+ewhere e\u223cGaussian (0, \u03c32I). Consider the linear\nregression problem b\u03b8=argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252, and the predicted value by=Xb\u03b8. The\nmean squared training error of this linear model is\nEtraindef=MSE(by,y) =Ee\u00141\nN\u2225by\u2212y\u22252\u0015\n=\u03c32\u0012\n1\u2212d\nN\u0013\n. (7.17)\nThe proof below depends on some results from linear algebra that may be difficult for\nfirst-time readers. We recommend you read the proof later.\nProof . Recall that the least squares linear regression solution is b\u03b8= (XTX)\u22121XTy. Since\ny=X\u03b8\u2217+e, we can substitute this into the predicted value byto show that\nby=Xb\u03b8=X(XTX)\u22121XT\n| {z }\n=Hy=X(XTX)\u22121XT(X\u03b8\u2217+e) =X\u03b8\u2217+He.\nTherefore, substituting by=X\u03b8\u2217+Heinto the MSE,\nMSE(by,y)def=Ee\u00141\nN\u2225by\u2212y\u22252\u0015\n=Ee\u00141\nN\u2225X\u03b8\u2217+He\u2212X\u03b8\u2217\u2212e\u22252\u0015\n=Ee\u00141\nN\u2225(H\u2212I)e\u22252\u0015\n.\nAt this point we need to use a tool from linear algebra. One useful identity3is that for any\nv\u2208RN,\n\u2225v\u22252= Tr(vvT).\n3The reason for this identity is that\nv=NX\nn=1v2\nn= Tr\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0v2\n1v1v2\u00b7\u00b7\u00b7 v1vN\nv2v1 v2\n2\u00b7\u00b7\u00b7 v2vN\n............\nvNv1vNv2\u00b7\u00b7\u00b7 v2\nN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\uf8fc\n\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8fe= Trn\nvvTo\n.\n422", "438": "7.2. OVERFITTING\nUsing this identity, we have that\nEe\u00141\nN\u2225(H\u2212I)e\u22252\u0015\n=1\nNEe\u0014\nTr\u001a\n(H\u2212I)eeT(H\u2212I)T\u001b\u0015\n=1\nNTr\u001a\n(H\u2212I)Ee\u0002\neeT\u0003\n(H\u2212I)T\u001b\n=\u03c32\nNTr\u001a\n(H\u2212I)(H\u2212I)T\u001b\n,\nwhere we used the fact that E[eeT] =\u03c32I. The special structure of Htells us that HT=H\nandHTH=H. Thus, we have ( H\u2212I)T(H\u2212I) =I\u2212H. In addition, using the cyclic\nproperty of trace Tr( AB) = Tr( BA), we have that\nTr(H) = Tr( X(XTX)\u22121XT)\n= Tr(( XTX)\u22121XTX) = Tr( I) =d.\nConsequently,\n\u03c32\nNTr\u001a\n(H\u2212I)(H\u2212I)T\u001b\n=\u03c32\nNTr\u001a\nI\u2212H\u001b\n=\u03c32\u0012\n1\u2212d\nN\u0013\n.\nThis completes the proof.\n\u25a1\nThe end of the proof. Please join us again.\nPractice Exercise 1 . In the theorem above, we proved the MSE of the prediction y.\nIn this example, we would like to prove the MSE for the parameter . Prove that\nMSE(b\u03b8,\u03b8\u2217)def=Ee\u0014\r\r\rb\u03b8\u2212\u03b8\u2217\r\r\r2\u0015\n=\u03c32Tr\u001a\n(XTX)\u22121\u001b\n.\nSolution . Let us start with the definition:\nMSE(b\u03b8,\u03b8\u2217) =Ee\u0014\r\r\r(XTX)\u22121XTy\u2212\u03b8\u2217\r\r\r2\u0015\n=Ee\u0014\r\r\r(XTX)\u22121XT(X\u03b8\u2217+e)\u2212\u03b8\u2217\r\r\r2\u0015\n=Ee\u0014\r\r\r\u03b8\u2217+ (XTX)\u22121XTe\u2212\u03b8\u2217\r\r\r2\u0015\n=Ee\u0014\r\r\r(XTX)\u22121XTe\r\r\r2\u0015\n.\n423", "439": "CHAPTER 7. REGRESSION\nContinuing the calculation,\nEe\u0014\r\r\r(XTX)\u22121XTe\r\r\r2\u0015\n=Ee\u0014\nTr\u001a\n(XTX)\u22121XTe eTX(XTX)\u22121\u001b\u0015\n= Tr\u001a\n(XTX)\u22121XTEe\u0014\neeT\u0015\nX(XTX)\u22121\u001b\n= Tr\u001a\n(XTX)\u22121XT\u00b7\u03c32I\u00b7X(XTX)\u22121\u001b\n=\u03c32Tr\u001a\n(XTX)\u22121XT\u00b7X(XTX)\u22121\u001b\n=\u03c32Tr\u001a\n(XTX)\u22121\u001b\n.\nAnalysis of the testing error\nSimilarly to the training error, we can analyze the testing error. The testing error is defined\nas\nEtest= MSE( by,y\u2032)def=Ee,e\u2032\u00141\nM\u2225by\u2212y\u2032\u22252\u0015\n, (7.18)\nwhereby= [by1, . . . ,byM]Tis a vector of Mpredicted values and y\u2032= [y\u2032\n1, . . . , y\u2032\nM]Tis a vector\nofMtrue values in the testing data.4\nWe would like to derive something concrete. To make our analysis simple, we consider\na special case in which the testing set contains ( x1, y\u2032\n1), . . . , (xN, y\u2032\nN). That is, the inputs\nx1, . . . , x Nare identical for both training and testing (for example, suppose that you measure\nthe temperature on two different days but at the same time stamps.) In this case, we have\nM=N, and we have Xtest=Xtrain=X. However, the noise added to the testing data is\nstill different from the noise added to the training data.\nWith these simplifications, we can derive the testing error as follows.\nTheorem 7.4. Let\u03b8\u2217\u2208Rdbe the ground truth linear model parameter, and X\u2208\nRN\u00d7dbe a matrix such that N\u2265dandXTXis invertible. Assume that the training\ndata follows the linear model y=X\u03b8\u2217+e, where e\u223cGaussian (0, \u03c32I). Consider\nthe linear regression problem b\u03b8= (XTX)\u22121XTy, and let by=Xb\u03b8. LetXtest=X\nbe the testing input data matrix, and define y\u2032=Xtest\u03b8\u2217+e\u2032\u2208RN, with e\u2032\u223c\nGaussian (0, \u03c32I), be the testing output. Then, the mean squared testing error of this\nlinear model is\nEtestdef=MSE(by,y\u2032) =Ee,e\u2032\u00141\nN\u2225by\u2212y\u2032\u22252\u0015\n=\u03c32\u0012\n1 +d\nN\u0013\n. (7.19)\nIn this definition, the expectation is taken with respect to a joint distribution of ( e,e\u2032).\nThis is because, in testing, the trained model is based on yof which the randomness is e.\nHowever, the testing data is based on y\u2032, where the randomness comes from e\u2032. We assume\nthateande\u2032are independent i.i.d. Gaussian vectors.\n4In practice, the number of testing samples Mcan be much larger than the number of training samples N.\nThis probably does not agree with your experience, in which the testing dataset is often much smaller than\nthe training dataset. The reason for this paradox is that the practical testing data set is only a finite subset\nof all the possible testing samples available. So the \u201ctesting error\u201d we compute in practice approximates the\ntrue testing error. If you want to compute the true testing error, you need a very large testing dataset.\n424", "440": "7.2. OVERFITTING\nAs with the previous proof, we recommend you study this proof later.\nProof . The MSE can be derived from the definition:\nMSE(by,y\u2032) =Ee,e\u2032\u00141\nN\u2225by\u2212y\u2032\u22252\u0015\n=1\nNEe,e\u2032\u0014\n\u2225X\u03b8\u2217+He\u2212X\u03b8\u2217\u2212e\u2032\u22252\u0015\n=1\nNEe,e\u2032\u0014\n\u2225He\u2212e\u2032\u22252\u0015\n.\nSince each noise term enande\u2032\nnis an i.i.d. copy of the same Gaussian random variable, by\nusing the fact that\nTr(H) = Tr( X(XTX)\u22121XT)\n= Tr(( XTX)\u22121XTX) = Tr( I) =d,\nwe have that\nEe,e\u2032h\n\u2225He\u2212e\u2032\u22252i\n=Ee\u0002\n\u2225He\u22252\u0003\n\u2212Ee,e\u2032h\n2eTHTe\u2032i\n| {z }\n=0+Ee\u2032\u0002\n\u2225e\u2032\u22252\u0003\n=Eeh\nTrn\nHeeTHToi\n+Ee\u2032\u0002\nTr\b\ne\u2032e\u2032T\t\u0003\n= Trn\nHEe\u0002\neeT\u0003\nHTo\n+ Tr{Ee\u2032\u0002\ne\u2032e\u2032T\u0003\n}\n= Trn\nH\u00b7\u03c32IN\u00d7N\u00b7HTo\n+ Tr\b\n\u03c32IN\u00d7N\t\n=\u03c32Trn\nHHTo\n+ Tr\b\n\u03c32IN\u00d7N\t\n=\u03c32Tr(Id\u00d7d) +\u03c32Tr{IN\u00d7N}=\u03c32(d+N).\nCombining all the terms,\nMSE(by,y\u2032) =Ee,e\u2032\u00141\nN\u2225by\u2212y\u2032\u22252\u0015\n=\u03c32\u0012\n1 +d\nN\u0013\n,\nwhich completes the proof.\n\u25a1\nThe end of the proof.\n7.2.3 Interpreting the linear analysis results\nLet us summarize the two main theorems. They state that, for N\u2265d,\nEtraindef= MSE( by,y) =Ee\u00141\nN\u2225by\u2212y\u22252\u0015\n=\u03c32\u0012\n1\u2212d\nN\u0013\n, (7.20)\nEtestdef= MSE( by,y\u2032) =Ee,e\u2032\u00141\nN\u2225by\u2212y\u2032\u22252\u0015\n=\u03c32\u0012\n1 +d\nN\u0013\n. (7.21)\nThis pair of equations tells us everything about the overfitting issue.\n425", "441": "CHAPTER 7. REGRESSION\nHow do EtrainandEtestchange w.r.t. \u03c32?\n\u0088Etrain\u2191as\u03c32\u2191. Thus noisier data are harder to fit.\n\u0088Etest\u2191as\u03c32\u2191. Thus a noiser model is more difficult to generalize.\nThe reasons for these results should be clear from the following equations:\nEtrain=\u03c32\u0012\n1\u2212d\nN\u0013\n,\nEtest=\u03c32\u0012\n1 +d\nN\u0013\n.\nAs\u03c32increases, the training error Etraingrows linearly w.r.t. \u03c32. Since the training error\nmeasures how good your model is compared with the training data, a larger Etrainmeans it\nis more difficult to fit. For the testing case, Etestalso grows linearly w.r.t. \u03c32. This implies\nthat the model would be more difficult to generalize if the model were trained using noisier\ndata.\nHow do EtrainandEtestchange w.r.t. N?\n\u0088Etrain\u2191asN\u2191. Thus more training samples make fitting harder.\n\u0088Etest\u2193asN\u2191. Thus more training samples improve generalization.\nThe reason for this should also be clear from the following equations:\nEtrain=\u03c32\u0012\n1\u2212d\nN\u0013\n,\nEtest=\u03c32\u0012\n1 +d\nN\u0013\n.\nAsNincreases, the model sees more training samples. The goal of the model is to minimize\nthe error with all the training samples. Thus the more training samples we have, the harder\nit will be to make everyone happy, so the training error grows as Ngrows. For testing, if the\nmodel is trained with more samples it is more resilient to noise. Hence the generalization\nimproves.\nHow do EtrainandEtestchange w.r.t. d?\n\u0088Etrain\u2193asd\u2191. Thus a more complex model makes fitting easier.\n\u0088Etest\u2191asd\u2191. Thus a more complex model makes generalization harder.\nThese results are perhaps less obvious than the others. The following equations tell us that\nEtrain=\u03c32\u0012\n1\u2212d\nN\u0013\n,\nEtest=\u03c32\u0012\n1+d\nN\u0013\n. (7.22)\n426", "442": "7.2. OVERFITTING\nFor this linear regression model to work, dhas to be less than N; otherwise, the matrix\ninversion ( XTX)\u22121is invalid. However, as dgrows while Nremains fixed, we ask the\nlinear regression to fit a larger and larger model while not providing any additional training\nsamples. Equation (7.22) says that Etrainwill drop as dincreases but Etestwill increase as d\nincreases. Therefore, a larger model will not generalize as well if Nis fixed.\nIfd > N , then the optimization\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252\nwill have many global minimizers (see Figure 7.10 ), implying that the training error can go\nto zero. Our analysis of EtrainandEtestdoes not cover this case because our proofs require\n(XTX)\u22121to exist. However, we can still extrapolate what will happen. When the training\nerror is zero, it only means that we fit perfectly into the training data. Since the testing\nerror grows as dgrows (though not in the particular form shown in Equation (7.22)), we\nshould expect the testing error to become worse.\nLearning curve\nThe results we derived above can be summarized in the learning curve shown in Figure 7.14 .\nIn this figure we consider a simple problem where\nyn=\u03b80+\u03b81xn+en,\nforen\u223cGaussian(0 ,1). Therefore, according to our theoretical derivations, we have \u03c3= 1\nandd= 2. For every N, we compute the average training error Etrainand the average testing\nerrorEtest, and then mark them on the figure. These are our empirical training and testing\nerrors. On the same figure, we calculate the theoretical training and testing error according\nto Equation (7.22).\nThe MATLAB and Python codes used to generate this learning curve are shown below.\nNset = round(logspace(1,3,20));\nE_train = zeros(1,length(Nset));\nE_test = zeros(1,length(Nset));\na = [1.3, 2.5];\nfor j = 1:length(Nset)\nN = Nset(j);\nx = linspace(-1,1,N)\u2019;\nE_train_temp = zeros(1,1000);\nE_test_temp = zeros(1,1000);\nX = [ones(N,1), x(:)];\nfor i = 1:1000\ny = a(1) + a(2)*x + randn(size(x));\ny1 = a(1) + a(2)*x + randn(size(x));\ntheta = X\\y(:);\nyhat = theta(1) + theta(2)*x;\nE_train_temp(i) = mean((yhat(:)-y(:)).^2);\nE_test_temp(i) = mean((yhat(:)-y1(:)).^2);\nend\nE_train(j) = mean(E_train_temp);\n427", "443": "CHAPTER 7. REGRESSION\nE_test(j) = mean(E_test_temp);\nend\nsemilogx(Nset, E_train, \u2019kx\u2019, \u2019LineWidth\u2019, 2, \u2019MarkerSize\u2019, 16); hold on;\nsemilogx(Nset, E_test, \u2019ro\u2019, \u2019LineWidth\u2019, 2, \u2019MarkerSize\u2019, 8);\nsemilogx(Nset, 1-2./Nset, \u2019k\u2019, \u2019LineWidth\u2019, 4);\nsemilogx(Nset, 1+2./Nset, \u2019r\u2019, \u2019LineWidth\u2019, 4);\nimport numpy as np\nimport matplotlib.pyplot as plt\nNset = np.logspace(1,3,20)\nNset = Nset.astype(int)\nE_train = np.zeros(len(Nset))\nE_test = np.zeros(len(Nset))\nfor j in range(len(Nset)):\nN = Nset[j]\nx = np.linspace(-1,1,N)\na = np.array([1, 2])\nE_train_tmp = np.zeros(1000)\nE_test_tmp = np.zeros(1000)\nfor i in range(1000):\ny = a[0] + a[1]*x + np.random.randn(N)\nX = np.column_stack((np.ones(N), x))\ntheta = np.linalg.lstsq(X, y, rcond=None)[0]\nyhat = theta[0] + theta[1]*x\nE_train_tmp[i] = np.mean((yhat-y)**2)\ny1 = a[0] + a[1]*x + np.random.randn(N)\nE_test_tmp[i] = np.mean((yhat-y1)**2)\nE_train[j] = np.mean(E_train_tmp)\nE_test[j] = np.mean(E_test_tmp)\nplt.semilogx(Nset, E_train, \u2019kx\u2019)\nplt.semilogx(Nset, E_test, \u2019ro\u2019)\nplt.semilogx(Nset, (1-2/Nset), linewidth=4, c=\u2019k\u2019)\nplt.semilogx(Nset, (1+2/Nset), linewidth=4, c=\u2019r\u2019)\nThe training error curve and the testing error curve behave in opposite ways as N\nincreases. The training error Etrain increases as Nincreases, because when we have more\ntraining samples it becomes harder for the model to fit all the data. By contrast, the testing\nerrorEtestdecreases as Nincreases, because when we have more training samples the model\nbecomes more robust to noise and unseen data. Therefore, the testing error improves.\nAsNgoes to infinity, both the training error and the testing error converge. This is\ndue to the law of large numbers, which says that the empirical training and testing errors\nshould converge to their respective expected values. If the training error and the testing error\nconverge to the same value, the training can generalize to testing. If they do not converge to\nthe same value, there is a mismatch between the training samples and the testing samples.\nIt is important to pay attention to the gap between the converged values. We often\nassume that the training samples and the testing samples are drawn from the same distri-\nbution, and therefore the training samples are good representatives of the testing samples.\n428", "444": "7.3. BIAS AND VARIANCE TRADE-OFF\n101102103\nNumber of training samples, N0.80.850.90.9511.051.11.151.2ErrorTraining Error\nTesting Error\nFigure 7.14: The learning curve is a pair of functions representing the training error and the testing\nerror. As Nincreases we expect the training error to increase and the testing error to decrease. The two\nfunctions will converge to the same value as Ngoes to infinity. If they do not converge to the same\nvalue, there is an intrinsic mismatch between the training samples and the testing samples, e.g., the\ntraining samples are not representative enough for the dataset.\nIf the assumption is not true, there will be a gap between the converged training error and\nthe testing error. Thus, what you claim in training cannot be transferred to the testing.\nConsequently, the learning curve provides you with a useful debugging tool to check how\nwell your training compares with your testing.\nClosing remark . In this section we have studied a very important concept in regression,\noverfitting. We emphasize that overfitting is not only caused by the complexity of the model\nbut a combination of the three factors \u03c32,N, and d. We close this section by summarizing\nthe causes of overfitting:\nWhat is the source of overfitting?\n\u0088Overfitting occurs because you have an imbalance between \u03c32,Nandd.\n\u0088Selecting the correct complexity for your model is the key to avoid overfitting.\n7.3 Bias and Variance Trade-Off\nOur linear analysis has provided you with a rough understanding of what we experience in\noverfitting. However, for general regression problems where the models are not necessarily\nlinear, we need to go deeper. The goal of this section is to explain the trade-off between\nbias and variance. This analysis requires some patience as it involves many equations. We\nrecommend skipping this section on a first reading and then returning to it later.\n429", "445": "CHAPTER 7. REGRESSION\nIf it is your first time reading it, we recommend you go through it slowly.\n7.3.1 Decomposing the testing error\nNotations\nAs we did at the beginning of Section 7.2, we consider a ground truth model that relates\nan input xand an output y:\ny=f(x) +e,\nwhere e\u223cGaussian(0 , \u03c32) is the noise. For example, if we use a linear model, then fcould\nbef(x) =\u03b8Tx, for some regression coefficients \u03b8.\nDuring training , we pick a prediction model g\u03b8(\u00b7) and try to predict the output when\ngiven a training sample x:\nby=g\u03b8(x).\nFor example, we may choose g\u03b8(x) =\u03b8Tx, which is also a linear model. We may also choose\na linear model in another basis, e.g., g\u03b8(x) =\u03b8T\u03d5(x) for some transformations \u03d5(\u00b7). In any\ncase, the goal of training is to minimize the training error:\nb\u03b8= argmin\n\u03b81\nNNX\nn=1(g\u03b8(xn)\u2212yn)2,\nwhere the sum is taken over the training samples Dtrain={(x1, y1), . . . , (xN, yN)}. Because\nthe model parameter b\u03b8is learned from the training dataset Dtrain, the prediction model\ndepends on Dtrain. To emphasize this dependency, we write\ng(Dtrain)= the model trained from\u001a\n(x1, y1), . . . , (xN, yN)\u001b\n.\nDuring testing , we consider a testing dataset Dtest={(x\u2032\n1, y\u2032\n1), . . . , (x\u2032\nM, y\u2032\nM)}. We put\nthese testing samples into the trained model to predict an output:\nby\u2032\nm=g(Dtrain)(x\u2032\nm), m = 1, . . . , M. (predicted value)\nSince the goal of regression is to make g(Dtrain)as close to fas possible, it is natural to\nexpect by\u2032\nmto be close to y\u2032\nm.\nTesting error decomposition (noise-free)\nSo we can now compute the testing error \u2014 the error that we ultimately care about. In the\nnoise-free condition, i.e., e= 0, the testing error is defined as\nE(Dtrain)\ntest =Ex\u2032h\u0000\ng(Dtrain)(x\u2032)\u2212f(x\u2032)\u00012i\n(7.23)\n\u22481\nMMX\nm=1\u0012\ng(Dtraing )(x\u2032\nm)\u2212f(x\u2032\nm)\u00132\n.\nThere are several components in this equation. First, x\u2032is a testing sample drawn from a\ncertain distribution. You can think of Dtestas a finite subset drawn from this distribution.\n430", "446": "7.3. BIAS AND VARIANCE TRADE-OFF\nSecond, the error\u0000\ng(Dtrain)(x\u2032)\u2212f(x\u2032)\u00012measures the deviation between our predicted value\nand the true value. Note that this error term is specific to one testing sample x\u2032. Therefore,\nwe take expectation Ex\u2032to find the average of the error for the distribution of x\u2032.\nThe testing error E(Dtrain)\ntest is a function that is dependent on the training set Dtrain,\nbecause the model g(Dtrain)is trained from Dtrain. Therefore, as we change the training\nset, we will have a different model gand hence a different testing error. To eliminate the\nrandomness of the training set, we define the overall testing error as\nEtest=EDtrain\u0014\nE(Dtrain)\ntest\u0015\n=EDtrain\u0014\nEx\u2032h\u0000\ng(Dtrain)(x\u2032)\u2212f(x\u2032)\u00012i\u0015\n. (7.24)\nNote that this definition of the testing error is consistent with the special case in Equa-\ntion (7.18), in which the testing error involves a joint expectation over eande\u2032. The ex-\npectation over eaccounts for the training samples, and the expectation over e\u2032accounts for\nthe testing samples.\nLet us try to extract some meaning from the testing error. Our method will be to\ndecompose the testing error into biasandvariance .\nTheorem 7.5. Assume a noise-free condition. The testing error of a regression prob-\nlem is given by\nEtest=Ex\u2032\u0014\n(g(x\u2032)\u2212f(x\u2032))2\n| {z }\n=bias(x\u2032)+EDtrain[(g(Dtrain)(x\u2032)\u2212g(x\u2032))2]| {z }\n=var(x\u2032)\u0015\n, (7.25)\nwhere g(x\u2032)def=EDtrain[g(Dtrain)(x\u2032)].\nProof . To simplify our notation, we will drop the subscript \u201ctrain\u201d in Dtrain when the\ncontext is clear. We have that\nEtest=EDh\nEx\u2032h\n(g(D)(x\u2032)\u2212f(x\u2032))2ii\n=Ex\u2032h\nEDh\n(g(D)(x\u2032)\u2212f(x\u2032))2ii\n.\nContinuing the calculation,\nEtest=Ex\u2032h\nEDh\n(g(D)(x\u2032)\u2212g(x\u2032) +g(x\u2032)\u2212f(x\u2032))2ii\n=Ex\u2032\u0014\nEDh\n(g(D)(x\u2032)\u2212g(x\u2032))2i\n+ 2EDh\n(g(D)(x\u2032)\u2212g(x\u2032))(g(x\u2032)\u2212f(x\u2032))i\n+EDh\n(g(x\u2032)\u2212f(x\u2032))2i\u0015\n.\nSince g(x\u2032)def=ED[g(D)(x\u2032)], it follows that\n2EDh\n(g(D)(x\u2032)\u2212g(x\u2032))(g(x\u2032)\u2212f(x\u2032))i\n= 0\n431", "447": "CHAPTER 7. REGRESSION\nbecause g(x\u2032)\u2212f(x\u2032) is independent of D, and\nEDh\n(g(x\u2032)\u2212f(x\u2032))2i\n= (g(x\u2032)\u2212f(x\u2032))2.\nTherefore,\nEtest=Ex\u2032\u0014\nEDh\n(g(D)(x\u2032)\u2212g(x\u2032))2i\n+h\n(g(x\u2032)\u2212f(x\u2032))2i\u0015\n.\nThus, by defining two following terms we have proved the theorem.\nbias(x\u2032)def= (g(x\u2032)\u2212f(x\u2032))2,\nvar(x\u2032)def=ED[(g(D)(x\u2032)\u2212g(x\u2032))2].\n\u25a1\nLet\u2019s consider what this theorem implies. This result is a decomposition of the testing\nerror into bias andvariance . It is a universal result that applies to allregression models,\nnot only linear cases. To summarize the meanings of bias and variance:\nWhat are bias and variance?\n\u0088Bias = how far your average is from the truth.\n\u0088Variance = how much fluctuation you have around the average.\nFigure 7.15 gives a pictorial representation of bias and variance. In this figure, we\nconstruct four scenarios of bias and variance. Each cross represents the predictor g(Dtrain),\nwith the true predictor fat the origin. Figure 7.15 (a) shows the case with a low bias and\na low variance. All these predictors g(Dtrain)are very close to the ground truth, and they\nhave small fluctuations around their average. Figure 7.15 (b) shows the case of a high bias\nand a low variance. It has a high bias because the entire group of g(Dtrain)is shifted to the\ncorner. The bias, which is the distance from the truth to the average, is therefore large. The\nvariance remains small because the fluctuation around the average is small. Figure 7.15 (c)\nshows the case of a low bias but high variance. In this case, the fluctuation around the\naverage is large. Figure 7.15 shows the case of high bias and high variance. We want to\navoid this case.\nBias low Bias high Bias low Bias high\nVar low Var low Var high Var high\n(a) (b) (c) (d)\nFigure 7.15: Imagine that you are throwing a dart with a target at the center. The four subfigures show\nthe levels of bias and variance.\n432", "448": "7.3. BIAS AND VARIANCE TRADE-OFF\nTesting error decomposition (noisy case)\nLet us consider a situation when there is noise. In the presence of noise, the training and\ntesting samples will follow the relationship\ny=f(x) +e,\nwhere e\u223cGaussian(0 , \u03c32). We assume that the noise is Gaussian to make the proof easier.\nWe can consider other types of noise in theory, but the theoretical results will need to be\nmodified.\nIn the presence of noise, the testing error is\nEtest(x\u2032)def=EDtrain,e\u0014\u0010\ng(Dtrain)(x\u2032)\u2212f(x\u2032) +e\u00112\u0015\n=EDtrain,e\u0014\u0010\ng(Dtrain)(x\u2032)\u2212g(x\u2032) +g(x\u2032)\u2212f(x\u2032) +e\u00112\u0015\n,\nwhere we take the joint expectation over the training dataset Dtrainand the error e. Con-\ntinuing the calculation, and using the fact that Dtrainandeare independent (and E[e] = 0),\nit follows that\nEtest(x\u2032) =EDtrain,e\u0014\u0010\ng(Dtrain)(x\u2032)\u2212g(x\u2032) +g(x\u2032)\u2212f(x\u2032) +e\u00112\u0015\n=EDtrain,e\u0014\u0010\ng(D)(x\u2032)\u2212g(x\u2032)\u00112\n+\u0010\ng(x\u2032)\u2212f(x\u2032)\u00112\n+e2\u0015\n=EDtrain\u0014\u0010\ng(Dtrain)(x\u2032)\u2212g(x\u2032)\u00112\u0015\n| {z }\n=var(x\u2032)+\u0010\ng(x\u2032)\u2212f(x\u2032)\u00112\n| {z }\n=bias( x\u2032)+Eeh\ne2i\n|{z}\n=noise.\nTaking the expectation of x\u2032over the entire testing distribution gives us\nEtest=Ex\u2032[Etest(x\u2032)] =Ex\u2032[var(x\u2032)]|{z}\nvar+Ex\u2032[bias(x\u2032)]|{z}\nbias+\u03c32.\nThe theorem below summarizes the results:\nTheorem 7.6. Assume a noisy condition where y=f(x)+efor some i.i.d. Gaussian\nnoise e\u223cGaussian (0, \u03c32). The testing error of a regression problem is given by\nEtest=Ex\u2032\u0014\n(g(x\u2032)\u2212f(x\u2032))2\n| {z }\n=bias(x\u2032)\u0015\n+Ex\u2032\u0014\nEDtrain[(g(Dtrain)(x\u2032)\u2212g(x\u2032))2]| {z }\n=var(x\u2032)\u0015\n+\u03c32,(7.26)\nwhere g(x\u2032)def=EDtrain[g(Dtrain)(x\u2032)].\n7.3.2 Analysis of the bias\nLet us examine the bias and variance in more detail. To discuss bias we must first understand\nthe quantity\ng(x\u2032)def=EDtrain[g(Dtrain)(x\u2032)], (7.27)\n433", "449": "CHAPTER 7. REGRESSION\nwhich is known as the average predictor . The average predictor, as the equation suggests, is\nthe expectation of the predictor g(Dtrain). Remember that g(Dtrain)is a predictor constructed\nfrom a specific training set Dtrain. If tomorrow our training set Dtraincontains other data\n(that come from the same underlying distribution), g(Dtrain)will be different. The average\npredictor gis the average across these random fluctuations of the dataset Dtrain. Here is an\nexample:\nSuppose we use a linear model with the ordinary polynomials as the bases. The data\npoints are generated according to\nyn=d\u22121X\np=0\u03b8pxp\nn\n|{z}\ndef=f(xn)=\u03b8Txn+en. (7.28)\nIf we use a particular training set Dtrainand run the regression, we will be able to obtain\none of the regression lines, as shown in Figure 7.16 . Let us call this line g(1). We repeat the\nexperiment by drawing another dataset, and call it g(2). We continue and eventually we will\nfind a set of regression lines g(1), g(2), . . . , g(K), where Kdenotes the number of training sets\nyou are using to generate all the gray curves. The average predictor gis defined as\ng(x\u2032) =EDtrain[g(Dtrain)]\u22481\nKKX\nk=1g(k)(x\u2032).\nThus if we take the average of all these gray curves we will obtain the average predictor,\nwhich is the red curve shown in Figure 7.16 .\n-1 -0.5 0 0.5 1-10123\nFigure 7.16: We run linear regression many times for different training datasets. Each one consists of\ndifferent random realizations of noise. The gray curves are the regression lines returned by each of the\ntraining datasets. We then take the average of these gray curves to obtain the red curve, which is the\naverage predictor.\nIf you are curious about how this plot was generated, the MATLAB and Python codes\nare given below.\n% MATLAB code to visualize the average predictor\nN = 20;\n434", "450": "7.3. BIAS AND VARIANCE TRADE-OFF\na = [5.7, 3.7, -3.6, -2.3, 0.05];\nx = linspace(-1,1,N);\nyhat = zeros(100,50);\nfor i=1:100\nX = [x(:).^0, x(:).^1, x(:).^2, x(:).^3, x(:).^4];\ny = X*a(:) + 0.5*randn(N,1);\ntheta = X\\y(:);\nt = linspace(-1, 1, 50);\nyhat(i,:) = theta(1) + theta(2)*t(:) + theta(3)*t(:).^2 ...\n+ theta(4)*t(:)^3 + theta(5)*t(:).^4;\nend\nfigure;\nplot(t, yhat, \u2019color\u2019, [0.6 0.6 0.6]); hold on;\nplot(t, mean(yhat), \u2019LineWidth\u2019, 4, \u2019color\u2019, [0.8 0 0]);\naxis([-1 1 -2 2]);\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import eval_legendre\nnp.set_printoptions(precision=2, suppress=True)\nN = 20\nx = np.linspace(-1,1,N)\na = np.array([0.5, -2, -3, 4, 6])\nyhat = np.zeros((50,100))\nfor i in range(100):\ny = a[0] + a[1]*x + a[2]*x**2 + \\\na[3]*x**3 + a[4]*x**4 + 0.5*np.random.randn(N)\nX = np.column_stack((np.ones(N), x, x**2, x**3, x**4))\ntheta = np.linalg.lstsq(X, y, rcond=None)[0]\nt = np.linspace(-1,1,50)\nXhat = np.column_stack((np.ones(50), t, t**2, t**3, t**4))\nyhat[:,i] = np.dot(Xhat, theta)\nplt.plot(t, yhat[:,i], c=\u2019gray\u2019)\nplt.plot(t, np.mean(yhat, axis=1), c=\u2019r\u2019, linewidth=4)\nWe now show an analytic calculation to verify Figure 7.16 .\nExample 7.4 . Consider a linear model such that\ny=xT\u03b8+e. (7.29)\nWhat is the predictor g(Dtrain)(x\u2032)? What is the average predictor g(x\u2032)?\nSolution . First, consider a training dataset Dtrain ={(x1, y1), . . . , (xN, yN)}. We\nassume that the xn\u2019s are deterministic and fixed. Therefore, the source of randomness\nin the training set is caused by the noise e\u223cGaussian(0 , \u03c32) and hence by the noisy\n435", "451": "CHAPTER 7. REGRESSION\nobservation y.\nThe training set gives us the equation y=X\u03b8+e, where Xis the matrix\nconstructed from xn\u2019s. The regression solution to this dataset is\nb\u03b8= (XTX)\u22121XTy,\nwhich should actually be b\u03b8(Dtrain)because yis a dataset-dependent vector.\nConsequently,\ng(Dtrain)(x\u2032) =b\u03b8Tx\u2032= (x\u2032)T(XTX)\u22121XTy\n= (x\u2032)T(XTX)\u22121XT(X\u03b8+e)\n= (x\u2032)T\u03b8+ (x\u2032)T(XTX)\u22121XTe.\nSince the randomness of Dtrainis caused by the noise, it follows that\ng(x\u2032) =EDtrain[g(Dtrain)(x\u2032)] =Ee[(x\u2032)T\u03b8+ (x\u2032)T(XTX)\u22121XTe]\n= (x\u2032)T\u03b8+ (x\u2032)T(XTX)\u22121XTEe[e]\n= (x\u2032)T\u03b8+ 0 = f(x\u2032).\nSo the average predictor will return the ground truth. However, note that not all\npredictors will return the ground truth.\nIn the above example, we obtained an interesting result, namely that g(x\u2032) =f(x\u2032).\nThat is, the average predictor equals the true predictor. However, in general, g(x\u2032) does\nnot necessarily equal f(x\u2032). If this occurs, we have a deviation ( g(x\u2032)\u2212f(x\u2032))2>0. This\ndeviation is called the bias. Bias is independent of the number of training samples because\nwe have taken the average of the predictors. Therefore, bias is more of an intrinsic (or\nsystematic) error due to the choice of the model.\nWhat is bias?\n\u0088Bias is defined as bias = Ex\u2032[(g(x\u2032)\u2212f(x\u2032))2], where x\u2032is a testing sample.\n\u0088It is the deviation from the average predictor to the true predictor.\n\u0088Bias is not necessarily a bad thing. A good predictor can have some bias as long\nas it helps to reduce the variance.\n7.3.3 Variance\nThe other quantity in the game is the variance . Variance at a testing sample x\u2032is defined\nas\nvar(x\u2032)def=EDtrain[(g(Dtrain)(x\u2032)\u2212g(x\u2032))2]. (7.30)\nAs the equation suggests, the variance measures the fluctuation between the predictor\ng(Dtrain)and the average predictor g.Figure 7.17 illustrates the polynomial-fitting prob-\nlem we discussed above. In this figure we consider two levels of variance by varying the\n436", "452": "7.3. BIAS AND VARIANCE TRADE-OFF\nnoise strength of en. The figure shows that as the observation becomes noisier, the predictor\ng(Dtrain)will have a larger fluctuation for the average predictor.\n-1 -0.5 0 0.5 1-2-1012\n-1 -0.5 0 0.5 1-2-1012\n(a) small variance (b) large variance\nFigure 7.17: Variance measures the magnitude of fluctuation between the particular predictor g(Dtrain)\nand the average predictor g.\nExample 7.5 . Continuing with Example 7.4, we ask: What is the variance?\nSolution . We first determine the predictor and its average:\ng(Dtrain)= (XTX)\u22121XTy=\u03b8+ (XTX)\u22121XTe\ng=E[g(Dtrain)] =Ee[\u03b8+ (XTX)\u22121XTe] =\u03b8,\nso the prediction at a testing sample x\u2032is\ng(Dtrain)(x\u2032) = (x\u2032)T\u03b8+ (x\u2032)T(XTX)\u22121XTe\ng(x\u2032) = (x\u2032)T\u03b8,\nConsequently, the variance is\nEDtrain\u0014\u0010\ng(Dtrain)(x\u2032)\u2212g(x\u2032)\u00112\u0015\n=Ee\u0014\u0010\n(x\u2032)T\u03b8+ (x\u2032)T(XTX)\u22121XTe\u2212(x\u2032)T\u03b8\u00112\u0015\n=Ee\u0014\u0010\n(x\u2032)T(XTX)\u22121XTe\u00112\u0015\n.\nContinuing the calculation,\nEDtrain\u0014\u0010\ng(Dtrain)(x\u2032)\u2212g(x\u2032)\u00112\u0015\n= (x\u2032)T(XTX)\u22121XTEe[eeT]X(XTX)\u22121x\u2032\n= (x\u2032)T(XTX)\u22121XT\u03c32IX(XTX)\u22121x\u2032\n=\u03c32(x\u2032)T(XTX)\u22121x\u2032\n=\u03c32Trn\n(XTX)\u22121(x\u2032)(x\u2032)To\n.\n437", "453": "CHAPTER 7. REGRESSION\nWhat will happen if we use more samples so that Ngrows? As Ngrows, the matrix Xwill\nhave more rows. Assuming that the magnitude of the entries remains unchanged, more rows\ninXwill increase the magnitude of XTXbecause we are summing more terms. Consider\na 2\u00d72 ordinary polynomial system where\nXTX=\uf8ee\n\uf8f0PN\nn=1x2\nnPN\nn=1xn\nPN\nn=1xn N\uf8f9\n\uf8fb.\nAsNgrows, all the entries in the matrix grow. As a result, ( XTX)\u22121will shrink in mag-\nnitude and thus drive the variance \u03c32Trn\n(XTX)\u22121(x\u2032)(x\u2032)To\nto zero.\nWhat is variance?\n\u0088Variance is the deviation between the predictor g(Dtrain)and its average g.\n\u0088It can be reduced by using more training samples.\n7.3.4 Bias and variance on the learning curve\nThe decomposition of the testing error into bias and variance is portrayed visually by the\nlearning curve shown in Figure 7.18 . This figure shows the testing error and the training\nerror as functions of the number of training samples. As Nincreases, we observe that both\ntesting and training errors converge to the same value. At any fixed N, the testing error is\ncomposed of bias and variance:\n\u0088The bias is the distance from the ground to the steady-state level. This value is fixed\nand is a constant w.r.t. N. In other words, regardless of how many training samples\nyou have, the bias is always there. It is the best outcome you can achieve.\n\u0088The variance is the fluctuation from the steady-state level to the instantaneous state.\nIt drops as Nincreases.\nFigure 7.18: The learning curve can be decomposed into the sum of the bias and the variance. The bias\nis the testing error when N=\u221e. For finite N, the difference between the testing error and the bias is\nthe variance.\n438", "454": "7.3. BIAS AND VARIANCE TRADE-OFF\nFigure 7.19 compares the learning curve of two models. The first case requires us to\nfit the data using a simple model (marked in purple). The training error and the testing\nerror have small fluctuations around the steady-state because, for simple models, you need\nonly a small number of samples to make the model happy. The second case requires us to\nfit the data using a complex model (marked in green). This set of curves has a much wider\nfluctuation because it is harder to train and harder to generalize. However, when we have\nenough training samples, the training error and the testing error will converge to a lower\nsteady-state value. Therefore, you need to pay the price of using a complex model, but if\nyou do, you will enjoy a lower testing error.\n101102103\nNumber of training samples, N00.511.522.5ErrorSimple Model - Training Error\nSimple Model - Testing Error\nComplex Model - Training Error\nComplex Model - Testing Error\nFigure 7.19: The generalization capability of a model is summarized by the training and testing errors\nof the model. If we use a simple model we will have an easier time with the training but the steady-state\ntesting error will be high. In contrast, if we use a complex model we need to have a sufficient number\nof training samples to train the model well. However, when the complex model is well trained, the\nsteady-state error will be lower.\nThe implication of all this is that you should choose the model by considering the\nnumber of data points. Never buy an expensive toy when you do not have the money! If\nyou insist on using a complex model while you do not have enough training data, you will\nsuffer from a poor testing error even if you feel good about it.\nClosing remark . We close this section by revisiting the bias-variance trade-off:\nEtest=Ex\u2032\u0014\n(g(x\u2032)\u2212f(x\u2032))2\n| {z }\n=bias( x\u2032)\u0015\n+Ex\u2032\u0014\nEDtrain[(g(Dtrain)(x\u2032)\u2212g(x\u2032))2]| {z }\n=var(x\u2032)\u0015\n+\u03c32. (7.31)\nThe relationship among the three terms is summarized below:\nWhat is the trade-off offered by the bias-variance analysis?\n\u0088Overfitting improves if N\u2191: Variance drops as Ngrows. Bias is unchanged.\n\u0088Overfitting worsens if \u03c32\u2191. If training noise grows, g(Dtrain)will have more fluc-\ntuations, so variance will grow. If testing noise grows, e2grows.\n439", "455": "CHAPTER 7. REGRESSION\n\u0088Overfitting worsens if the target function fis too complicated to be approximated\nbyg.\nEnd of the section. Please join us again.\n7.4 Regularization\nHaving discussed the source of the overfitting problem, we now discuss methods to allevi-\nate overfitting. The method we focus on here is regularization . Regularization means that\ninstead of seeking the model parameters by minimizing the training loss alone, we add a\npenalty term to force the parameters to\u201cbehave better\u201d. As a preview of the technique, we\nchange the original training loss\nEtrain(\u03b8) =NX\nn=1\u0012\nyn\u2212d\u22121X\np=0\u03b8p\u03d5p(xn)\u00132\n| {z }\ndata fidelity, (7.32)\nwhich consists of only the data fidelity term, to a modified training loss\nEtrain(\u03b8) =NX\nn=1\u0012\nyn\u2212d\u22121X\np=0\u03b8p\u03d5p(xn)\u00132\n| {z }\nF(\u03b8),data fidelity+ \u03bb\u00b7d\u22121X\np=0\u03b82\np\n|{z}\n\u03bb\u00b7R(\u03b8),regularization. (7.33)\nPutting this into the matrix form, we define the data fidelity term as\nF(\u03b8) =\u2225X\u03b8\u2212y\u22252. (7.34)\nThe newly added term R(\u03b8) is called the regularization function or the penalty function .\nIt can take a variety of forms, e.g.,\n\u0088Ridge regression: R(\u03b8) =Pd\u22121\np=0\u03b82\np=\u2225\u03b8\u22252.\n\u0088LASSO regression: R(\u03b8) =Pd\u22121\np=0|\u03b8p|=\u2225\u03b8\u22251.\nIn this section we aim to understand the role of the regularization functions by studying\nthese two examples of R(\u03b8).\n7.4.1 Ridge regularization\nTo explain the meaning of Equation (7.33) we write it in terms of matrices and vectors:\nminimize\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22252, (7.35)\nwhere \u03bbis called the regularization parameter .It needs to be tuned by the user. We refer\nto Equation (7.35) as the ridge regression .5\n5In signal processing and optimization, Equation (7.35) is called the Tikhonov regularization. We follow\nthe statistics community in calling it the ridge regression.\n440", "456": "7.4. REGULARIZATION\nHow can the regularization function help to mitigate the overfitting problem? First\nlet\u2019s find the solution to this problem.\nPractice Exercise 1 . Prove that the solution to Equation (7.35) is\nb\u03b8= (XTX+\u03bbI)\u22121XTy. (7.36)\nSolution . Take the derivative with respect to \u03b8.aThis yields\n\u2207\u03b8\u001a\n\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22252\u001b\n= 2XT(X\u03b8\u2212y) + 2\u03bb\u03b8= 0.\nRearranging the terms gives\n(XTX+\u03bbI)\u03b8=XTy.\nTaking the inverse of the matrix on both sides yields the solution.\naThe solution here requires some basic matrix calculus. You may refer to the University of Water-\nloo\u2019s Matrix Cookbook https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf .\nLet us compare the ridge regression solution with the vanilla regression solutions:\nb\u03b8vanilla = (XTX)\u22121XTy,\nb\u03b8ridge(\u03bb) = (XTX+\u03bbI)\u22121XTy.\nClearly, the only difference is the presence of the parameter \u03bb:\n\u0088If\u03bb\u21920, then b\u03b8ridge(0) =b\u03b8vanilla . This is because\nEtrain(\u03b8) =\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22252\n|{z}\n=0.\nHence, when \u03bb\u21920, the regression problem goes back to the vanilla version, and so\ndoes the solution.\n\u0088\u03bb\u2192 \u221e , then b\u03b8ridge(\u221e) = 0. This happens because\nEtrain(\u03b8) =1\n\u03bb\u2225X\u03b8\u2212y\u22252\n|{z}\n=0+\u2225\u03b8\u22252.\nSince we are now minimizing \u2225\u03b8\u22252, the solution will be \u03b8= 0 because zero is the\nsmallest value a squared function can achieve.\nFor any 0 < \u03bb < \u221e, the net effect of ( XTX+\u03bbI) is the constant \u03bbadded to all the\neigenvalues of XTX. By taking the eigendecomposition of XTX,\n[U,S] =eig(XTX),\nwe have that\nXTX+\u03bbI=USUT+\u03bbI\n=USUT+\u03bbUUT=U(S+\u03bbI)UT.\n441", "457": "CHAPTER 7. REGRESSION\nTherefore, if the eigenvalue matrix Shas a zero eigenvalue it will be offset by \u03bb:\nS=\uf8ee\n\uf8ef\uf8ef\uf8f0\u2663\n\u2661\n\u2660\n0\uf8f9\n\uf8fa\uf8fa\uf8fb\u2212\u2192 S+\u03bbI=\uf8ee\n\uf8ef\uf8ef\uf8f0\u2663+\u03bb\n\u2661+\u03bb\n\u2660+\u03bb\n\u03bb\uf8f9\n\uf8fa\uf8fa\uf8fb\nAs a result, even if XTXis not invertible (or close to not invertible), the new matrix\nXTX+\u03bbIis guaranteed to be invertible.\nPractice Exercise 2. You may be wondering what happens if XTXhas a negative\neigenvalue so that when we add a positive \u03bb, the resulting matrix may have a zero\neigenvalue. Prove that XTXwill never have a negative eigenvalue, and XTX+\u03bbI\nalways has positive eigenvalues.\nSolution . Eigenvalues of a matrix Aare nonnegative if and only if vTAv\u22650 for\nanyv. Thus we need to check whether vTXTXv\u22650 for all v. However, this is easy:\nvTXTXv=\u2225Xv\u22252,\nwhich must be nonnegative for any v. Matrices satisfying this property are called\npositive semidefinite . Therefore, XTXis positive semidefinite.\nImplementation\nSolving the ridge regression is easy. First, we observe that the regularization function R(\u03b8) =\n\u2225\u03b8\u22252is a quadratic function. Therefore, it can be combined with the data fidelity term as\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22252\n= argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252+\u2225\u221a\n\u03bbI\u03b8\u22120\u22252\n= argmin\n\u03b8\u2208Rd\r\r\r\r\u0014X\u221a\n\u03bbI\u0015\n\u03b8\u2212\u0014y\n0\u0015\r\r\r\r2\n.\nTherefore, all we need to do is to concatenate the matrix Xwith a d\u00d7didentity operator\u221a\n\u03bbI, and concatenate ywith a d\u00d71 all-zero vector.\nIn MATLAB and Python, the implementation of the ridge regression is done by defining\na new matrix Aand a new vector b, as shown below:\n% MATLAB command for ridge regression\nA = [X; sqrt(lambda)*eye(d)];\nb = [y(:); zeros(d,1)];\ntheta = A\\b;\n% MATLAB command for ridge regression\nA = np.vstack((X, np.sqrt(lambd)*np.eye(d)))\nb = np.hstack((y, np.zeros(d)))\ntheta = np.linalg.lstsq(A, b, rcond=None)[0]\n442", "458": "7.4. REGULARIZATION\nExample 7.6 . Consider a dataset of N= 20 data points. These data points are\nconstructed from the model\nyn= 0.5\u22122xn\u22123x2\nn+ 4x3\nn+ 6x4\nN+en, n = 1, . . . , N,\nwhere en\u223cGaussian(0 ,0.252) is the noise. Fit the data using\n(a) Vanilla linear regression with a 4th-order polynomial.\n(b) Vanilla linear regression with a 20th-order polynomial.\n(c) Ridge regression with a 20th-order polynomial, by considering three choices of \u03bb:\n\u03bb= 10\u22126,\u03bb= 10\u22123, and \u03bb= 10.\nSolution .\n(a) We first fit the data using a 4th-order polynomial. This fitting is relatively\nstraightforward. In the MATLAB / Python programs below, set d= 4 and\n\u03bb= 0. The result is shown in Figure 7.20 (a).\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n(a) Vanilla, 4th-order polynomial (b) Vanilla, 20th-order polynomial\nFigure 7.20: Overfitting occurs when the model is too complex for the number of training samples.\nWhen using a vanilla regression with a 20th-order polynomial, the curve overfits the data and\ncauses a catastrophic fitting error.\n(b) Suppose we use a 20th-order polynomial g(x) =P20\np=0\u03b8pxpto fit the data. We\nplot the result in Figure 7.20 (b). Since the order of the polynomial is very high\nrelative to the number of training samples, it comes as no surprise that the fitting\nis poor. This is overfitting, and we know the reason.\n(c) Next, we consider a ridge regression using three choices of \u03bb. The result is shown\ninFigure 7.21 . If\u03bbis too small, we observe that some overfitting still occurs. If\n\u03bbis too large, then the curve underfits the data. For an appropriately chosen \u03bb,\nit can be seen that the fitting is reasonably good.\n443", "459": "CHAPTER 7. REGRESSION\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n(a) Ridge, \u03bb= 10\u22126(b) Ridge, \u03bb= 10\u22123(c) Ridge, \u03bb= 10\nFigure 7.21: Ridge regression addresses the overfitting problem by adding a regularization term\nto the training loss. Depending on the strength of the parameter \u03bb, the fitted curve can vary from\noverfitting to underfitting.\nThe MATLAB and Python codes used to generate the above plots are shown below.\n% MATLAB code to demonstrate a ridge regression example\n% Generate data\nN = 20;\nx = linspace(-1,1,N);\na = [0.5, -2, -3, 4, 6];\ny = a(1)+a(2)*x(:)+a(3)*x(:).^2+a(4)*x(:).^3+a(5)*x(:).^4+0.25*randn(N,1);\n% Ridge regression\nlambda = 0.1;\nd = 20;\nX = zeros(N, d);\nfor p=0:d-1\nX(:,p+1) = x(:).^p;\nend\nA = [X; sqrt(lambda)*eye(d)];\nb = [y(:); zeros(d,1)];\ntheta = A\\b;\n% Interpolate and display results\nt = linspace(-1, 1, 500);\nXhat = zeros(length(t), d);\nfor p=0:d-1\nXhat(:,p+1) = t(:).^p;\nend\nyhat = Xhat*theta;\nplot(x,y, \u2019ko\u2019,\u2019LineWidth\u2019,2, \u2019MarkerSize\u2019, 10); hold on;\nplot(t,yhat,\u2019LineWidth\u2019,4,\u2019Color\u2019,[0.2 0.2 0.9]);\n# Python code to demonstrate a ridge regression example\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import eval_legendre\nnp.set_printoptions(precision=2, suppress=True)\n444", "460": "7.4. REGULARIZATION\nN = 20\nx = np.linspace(-1,1,N)\na = np.array([0.5, -2, -3, 4, 6])\ny = a[0] + a[1]*x + a[2]*x**2 + \\\na[3]*x**3 + a[4]*x**4 + 0.25*np.random.randn(N)\nd = 20\nX = np.zeros((N, d))\nfor p in range(d):\nX[:,p] = x**p\nlambd = 0.1\nA = np.vstack((X, np.sqrt(lambd)*np.eye(d)))\nb = np.hstack((y, np.zeros(d)))\ntheta = np.linalg.lstsq(A, b, rcond=None)[0]\nt = np.linspace(-1, 1, 500)\nXhat = np.zeros((500,d))\nfor p in range(d):\nXhat[:,p] = t**p\nyhat = np.dot(Xhat, theta)\nplt.plot(x,y,\u2019o\u2019,markersize=12)\nplt.plot(t,yhat, linewidth=4)\nplt.show()\nWhy does ridge regression work?\n\u0088The penalty term \u2225\u03b8\u22252in\nb\u03b8ridge= argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22252\ndoes not allow solutions with very \u2225\u03b8\u22252.\n\u0088The penalty term adds a positive offset to the eigenvalues of XTX.\n\u0088Since the denominator in ( XTX+\u03bbI)\u22121XTybecomes larger than that of\n(XTX)\u22121XTy, noise in yis less amplified.\nChoosing the parameter\nHow should we choose the parameter \u03bb? The honest answer is that there is no answer\nbecause the optimal \u03bbcan only be found if we have access to the testing samples. If we do,\nwe can plot the MSE (the testing error) with respect to \u03bb, as shown in Figure 7.22 (a).\nOf course in reality we do not have access to the testing data. However, we can reserve\na small portion of the training samples and treat them as validation samples . Then we\nrun the ridge regression for different choices of \u03bb. The \u03bbthat minimizes the error on these\nvalidation samples is the one that you should deploy. If the training set is small, we can\n445", "461": "CHAPTER 7. REGRESSION\n10-1010-510010-310-210-1\n10010500.050.10.150.20.250.3\n(a) Testing error vs \u03bb (b)F(b\u03b8\u03bb) vsR(b\u03b8\u03bb)\nFigure 7.22: (a) Determining the optimal \u03bbrequires knowledge of the testing samples. In practice, we\ncan replace the testing samples with the validation samples, which are subsets of the training data. Then\nby plotting the validation error as a function of \u03bbwe can determine the optimal \u03bb. (b) The alternative\nis to plot F(b\u03b8\u03bb)versus R(b\u03b8\u03bb). The optimal \u03bbcan be found by locating the elbow point.\nshuffle the validation samples randomly and compute the average. This scheme is known as\ncross-validation .\nFor some problems, there are \u201ctactics\u201d you may be able to employ for determining\nthe optimal \u03bb. The first approach is to ask yourself what would be the reasonable range\nof\u2225\u03b8\u22252or\u2225X\u03b8\u2212y\u22252? Are you expecting them to be large or small? Approximately in\nwhat order of magnitude? If you have some clues about this, then you can plot the function\nF(b\u03b8\u03bb) =\u2225Xb\u03b8\u03bb\u2212y\u22252as a function of R(b\u03b8\u03bb) =\u2225b\u03b8\u03bb\u22252, where b\u03b8\u03bbis a shorthand notation\nforb\u03b8ridge(\u03bb), which is the estimated parameter using a specific value of \u03bb.Figure 7.22 (b)\nshows an example of such a plot. As you can see, by varying \u03bbwe have different values of\nF(b\u03b8\u03bb) and R(b\u03b8\u03bb).\nIf you have some ideas about what \u2225\u03b8\u22252should be, say you want \u2225\u03b8\u22252\u2264\u03c4, you can go\nto the F(b\u03b8\u03bb) versus R(b\u03b8\u03bb) curve and find a point such that R(b\u03b8\u03bb)\u2264\u03c4. On the other hand,\nif you want \u2225X\u03b8\u2212y\u22252\u2264\u03f5, you can also go to the F(b\u03b8\u03bb) versus R(b\u03b8\u03bb) curve and find a\npoint such that \u2225X\u03b8\u2212y\u22252\u2264\u03f5. In either case, you have the freedom to shift the difficulty\nof finding \u03bbto that of finding \u03c4or\u03f5. Note that \u03c4and\u03f5have better physical interpretations.\nThe quantity \u03f5tells us the upper bound of the prediction error, and \u03c4tells us the upper\nbound of the parameter magnitude. If you have been working on your dataset long enough,\nthe historical data (and your experience) will help you determine these values.\nAnother feasible option suggested in the literature is finding the anchor point of the\nF(b\u03b8\u03bb) and R(b\u03b8\u03bb). The idea is that if the curve has a sharp elbow, the turning point would\nindicate a rapid increase/decrease in F(b\u03b8\u03bb) (or R(b\u03b8\u03bb)).\nHow to determine \u03bb\n\u0088Cross-validation: Reserve a few training samples as validation samples. Check\nthe prediction error w.r.t. these validation samples. The \u03bbthat minimizes the\nvalidation error is the one you deploy.\n\u0088\u2225\u03b8\u22252\u2264\u03c4: Plot the F(b\u03b8\u03bb) and R(b\u03b8\u03bb). Then go along the R-axis to find the\n446", "462": "7.4. REGULARIZATION\nposition where R(b\u03b8\u03bb)\u2264\u03c4.\n\u0088\u2225X\u03b8\u2212y\u22252\u2264\u03c4: Plot the F(b\u03b8\u03bb) and R(b\u03b8\u03bb). Then go along the F-axis to find\nthe position where F(b\u03b8\u03bb)\u2264\u03f5.\n\u0088Find the elbow point of F(b\u03b8\u03bb) and R(b\u03b8\u03bb).\nBias and variance trade-off for ridge regression\nWe now discuss the bias and variance trade-off of the ridge regression.\nTheorem 7.7. Lety=X\u03b8+ebe the training data, where eis zero-mean and has a\ncovariance \u03c32I. Consider the ridge regression\nb\u03b8\u03bb=argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22252. (7.37)\nThen the estimate has the properties that\nb\u03b8\u03bb= (XTX+\u03bbI)\u22121XTX\u03b8+ (XTX+\u03bbI)\u22121XTe,\nE[b\u03b8\u03bb] = (XTX+\u03bbI)\u22121XTX\u03b8=W\u03bb\u03b8,\nCov[b\u03b8\u03bb] =\u03c32(XTX+\u03bbI)\u22121XTX(XTX+\u03bbI)\u22121,\nMSE(b\u03b8\u03bb,\u03b8) =\u03c32Tr\u001a\nW\u03bb(XTX)\u22121WT\n\u03bb\u001b\n+\u03b8T(W\u03bb\u2212I)T(W\u03bb\u2212I)\u03b8,\nwhere W\u03bb= (XTX+\u03bbI)\u22121XTX.\nProof . The proof of this theorem involves some tedious matrix operations that will be\nomitted here. If you are interested in the proof you can consult van Wieringen\u2019s \u201cLecture\nnotes on ridge regression\u201d, https://arxiv.org/pdf/1509.09169.pdf .\n\u25a1\nThe results of this theorem provide a way to assess the bias and variance. Specifically,\nfrom the MSE we know that\nMSE(b\u03b8\u03bb,\u03b8) =Eeh\n\u2225b\u03b8\u03bb\u2212\u03b8\u22252i\n=\u2225Ee[b\u03b8\u03bb]\u2212\u03b8\u22252+ Trn\nCov[b\u03b8\u03bb]o\n=\u03b8T(W\u03bb\u2212I)T(W\u03bb\u2212I)\u03b8| {z }\nbias+\u03c32Tr\u001a\nW\u03bb(XTX)\u22121WT\n\u03bb\u001b\n| {z }\nvariance.\nThe bias and variance are defined respectively as\nBias(b\u03b8\u03bb,\u03b8) =\u03b8T(W\u03bb\u2212I)T(W\u03bb\u2212I)\u03b8,\nVar(b\u03b8\u03bb,\u03b8) =\u03c32Tr\u001a\nW\u03bb(XTX)\u22121WT\n\u03bb\u001b\n.\nWe can then plot the bias and variance as a function of \u03bb. An example is shown in Fig-\nure 7.23 .\n447", "463": "CHAPTER 7. REGRESSION\n10-410-310-210-110-1100101102103\nMSE\nBias\nVariance\nFigure 7.23: The bias and variance of the ridge regression behave in opposite ways as \u03bbincreases. The\nMSE is the sum of bias and variance.\nThe result in Figure 7.23 can be summarized in three points:\n\u0088Bias\u2191as\u03bb\u2191. This is because a large \u03bbpushes the solution towards \u03b8= 0. Therefore,\nthe bias with respect to the ground truth \u03b8will increase.\n\u0088Variance \u2193as\u03bb\u2191. Since variance is caused by noise, increasing \u03bbforces the solution\n\u03b8to be small. Hence, it becomes less sensitive to noise.\n\u0088MSE reaches a minimum point somewhere in the middle. The MSE is the sum of bias\nand variance. Therefore, it drops to the minimum and then rises again as \u03bbincreases.\nWith appropriate choice of \u03bb, we can show that the ridge regression can have a\nlower mean squared error than the vanilla regression. The following result is due to C. M.\nTheobald:6\nTheorem 7.8. For\u03bb <2\u03c32\u2225\u03b8\u2225\u22122,\nMSE\u0010\nb\u03b8ridge(\u03bb),\u03b8\u0011\n<MSE\u0010\nb\u03b8vanilla ,\u03b8\u0011\n. (7.38)\nThis theorem says that as long as \u03bbis small enough, the ridge regression will have a lower\nMSE than the vanilla regression. Thus ridge regression is almost always helpful. Of course,\nthe optimal \u03bbis not provided by the theorem, which only tells us where to search for a\ngood \u03bb.\nWhy does ridge regression reduce the testing error?\n\u0088The regularization reduces the variance (see Figure 7.23 when \u03bb >0)\n\u0088It pays the price of increasing the bias.\n6Theobald, C. M. (1974). Generalizations of mean square error applied to ridge regression. Journal of\nthe Royal Statistical Society . Series B (Methodological), 36(1), 103-106.\n448", "464": "7.4. REGULARIZATION\n\u0088Usually, the drop in variance outweighs the increase in bias. So the overall MSE\ndrops.\n\u0088Bias is not always a bad thing.\n7.4.2 LASSO regularization\nThe ridge regression we discussed in the previous subsection is just one of the many possible\nways of doing regularization. One alternative is to replace \u2225\u03b8\u22252by\u2225\u03b8\u22251, where\n\u2225\u03b8\u22251=d\u22121X\np=0|\u03b8p|. (7.39)\nThis change from the sum-squares to sum-absolute-values has been main driving force in\ndata science, machine learning, and signal processing for at least the past two decades. The\noptimization associated with \u2225\u03b8\u22251is\nminimize\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22251, (7.40)\nor\nEtrain(\u03b8) =NX\nn=1\u0012\nyn\u2212d\u22121X\np=0\u03b8p\u03d5p(xn)\u00132\n| {z }\nF(\u03b8),data fidelity+ \u03bb\u00b7d\u22121X\np=0|\u03b8p|\n|{z}\n\u03bb\u00b7R(\u03b8),regularization. (7.41)\nSeeking a sparse solution\nTo understand the choice of \u2225 \u00b7 \u2225 1, we need to introduce the concept of sparsity .\nDefinition 7.1. A vector \u03b8is called sparse if it has only a few non-zero elements.\nAs illustrated in Figure 7.24 , a sparse \u03b8ensures that only a very few columns of the data\nmatrix Xare active. This is an attractive property because, in some of the regression\nproblems, it is indeed possible to have just a few dominant factors. The LASSO regression\nsays that if our problem possesses this sparse solution, then the \u2225 \u00b7 \u2225 1can help us find the\nsparse solution.\nFigure 7.24: A vector \u03b8is sparse if it only contains a few non-zero elements. If \u03b8is sparse, then the\nobservation yis determined by a few active components.\n449", "465": "CHAPTER 7. REGRESSION\nHow can \u2225\u03b8\u22251promote sparsity? If we consider the sets\n\u21261={\u03b8| \u2225\u03b8\u22251\u2264\u03c4}={(\u03b81, \u03b82)| |\u03b81|+|\u03b82| \u2264\u03c4},\n\u21262={\u03b8| \u2225\u03b8\u22252\u2264\u03c4}={(\u03b81, \u03b82)|\u03b82\n1+\u03b82\n2\u2264\u03c4},\nwe note that \u2126 1has a diamond shape whereas \u2126 2has a circular shape. Since the data\nfidelity term \u2225X\u03b8\u2212y\u22252is an ellipsoid, seeking the optimal value in the presence of the\nregularization term can be viewed as moving the ellipsoid until it touches the set defined\nby the regularization. As illustrated in Figure 7.25 , since {\u03b8| \u2225\u03b8\u22252\u2264\u03c4}is a circle, the\nsolution will be somewhere in the middle. On the other hand, since {\u03b8| \u2225\u03b8\u22251\u2264\u03c4}is a\ndiamond, the solution will be one of the vertices. The difference between \u201csomewhere in\nthe middle\u201d and \u201ca vertex\u201d is that the vertex is a sparse solution, since by the definition of\na vertex one coordinate must be zero and the other coordinate must be non-zero. We can\neasily extrapolate this idea to the higher-dimensional spaces. In this case, we will see that\nthe solution for the \u2225 \u00b7 \u2225 1problem has only a few non-zero entries.\nFigure 7.25: A vector \u03b8is sparse if it contains only a few non-zero elements. If \u03b8is sparse, then the\nobservation yis determined by a few active components.\nThe optimization formulated in Equation (7.41) is known as the least absolute shrink-\nage and selection operator (LASSO). LASSO problems are difficult, but over the past two\ndecades we have increased our understanding of the problem. The most significant break-\nthrough is that we now have algorithms to solve the LASSO problem efficiently. This is\nimportant because, unlike the ridge regression, where we have a (very simple) closed-form\nsolution, the LASSO problem can only be solved using iterative algorithms.\nWhat is so special about LASSO?\n\u0088LASSO regularization promotes a sparse solution.\n\u0088If the underlying model has a sparse solution, e.g., you choose a 50th-order\npolynomial, but the underlying model is a third-order polynomial, then there\nshould only be three non-zero regression coefficients in your 50th-order polyno-\nmial. LASSO will help in this case.\n450", "466": "7.4. REGULARIZATION\n\u0088If the underlying model has a dense solution, then LASSO is of limited value. A\nridge regression could be better.\n\u0088While \u2225\u03b8\u22251is not differentiable (at 0), there exist polynomial-time convex algo-\nrithms to solve the problem, e.g., interior-point methods.\nSolving the LASSO problem\nToday, there are many open-source packages to solve the LASSO problem. They are mostly\ndeveloped in the convex optimization literature. One of the most user-friendly packages is\ntheCVXpackage developed by S. Boyd and colleagues at Stanford University.7Once you\nhave downloaded and installed the package, solving the optimization can be done literally\nby typing in the data fidelity term and the regularization term. An example is given below.\ncvx_begin\nvariable theta(d)\nminimize(sum_square(X*theta-y) + lambda*norm(theta,1))\ncvx_end\nAs you can see, the program is extremely simple. You start by calling cvx_begin\nand end it with cvx_end . Inside the box we create a variable beta(d) , where ddenotes\nthe dimension of the vector theta . The main command is minimize . However, this line is\nalmost self-explanatory. As long as you follow the syntax given by the user guidelines, you\nwill be able to set it up properly.\nIn Python, we can call the cvxpy library.\nimport cvxpy as cvx\ntheta = cvx.Variable(d)\nobjective = cvx.Minimize( cvx.sum_squares(X*theta-y) \\\n+ lambd*cvx.norm1(theta) )\nprob = cvx.Problem(objective)\nprob.solve()\nTo see a concrete example, we use the crime rate data obtained from https://web.\nstanford.edu/ ~hastie/StatLearnSparsity/data.html . A snapshot of the data is shown\nin the table below. In this dataset, the vector yis the crime rate, which is the last column\nof the table. The feature/basis vectors are funding ,hs,not-hs ,college .\ncity crime rate funding hs no-hs college\n1 478 40 74 11 31\n2 494 32 72 11 43\n3 643 57 71 18 16\n4 341 31 71 11 25\n..................\n50 940 66 67 26 18\n7The MATLAB version is here: http://cvxr.com/cvx/ . The Python version is here: https://cvxopt.\norg/. Follow the instructions to install the package.\n451", "467": "CHAPTER 7. REGRESSION\nWe consider two optimizations:\nb\u03b81(\u03bb) = argmin\n\u03b8E1(\u03b8)def=\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22251,\nb\u03b82(\u03bb) = argmin\n\u03b8E2(\u03b8)def=\u2225X\u03b8\u2212y\u22252+\u03bb\u2225\u03b8\u22252.\nAs we have discussed, the first optimization uses the \u2225\u00b7\u22251regularized least squares, which is\nthe LASSO problem. The second optimization is the standard \u2225\u00b7\u22252regularized least squares.\nSince both solutions depend on the parameter \u03bb, we parameterize the solutions in terms of\n\u03bb. Note that the optimal \u03bbforb\u03b81is not necessarily the optimal \u03bbforb\u03b82.\nOne thing we would like to demonstrate in this example is visualizing the linear re-\ngression coefficients b\u03b81(\u03bb) andb\u03b82(\u03bb) as\u03bbchanges. To solve the optimization, we use CVX\nwith the MATLAB and Python implementation is shown below.\ndata = load(\u2019./dataset/data_crime.txt\u2019);\ny = data(:,1); % The observed crime rate\nX = data(:,3:end); % Feature vectors\n[N,d]= size(X);\nlambdaset = logspace(-1,8,50);\ntheta_store = zeros(d,50);\nfor i=1:length(lambdaset)\nlambda = lambdaset(i);\ncvx_begin\nvariable theta(d)\nminimize( sum_square(X*theta-y) + lambda*norm(theta,1) )\n% minimize( sum_square(X*theta-y) + lambda*sum_square(theta) )\ncvx_end\ntheta_store(:,i) = theta(:);\nend\nfigure(1);\nsemilogx(lambdaset, theta_store, \u2019LineWidth\u2019, 4);\nlegend(\u2019funding\u2019,\u2019% high\u2019, \u2019% no high\u2019, \u2019% college\u2019, ...\n\u2019% graduate\u2019, \u2019Location\u2019,\u2019NW\u2019);\nxlabel(\u2019lambda\u2019);\nylabel(\u2019feature attribute\u2019);\nimport cvxpy as cvx\nimport numpy as np\nimport matplotlib.pyplot as plt\ndata = np.loadtxt(\"/content/data_crime.txt\")\ny = data[:,0]\nX = data[:,2:7]\nN,d = X.shape\nlambd_set = np.logspace(-1,8,50)\n452", "468": "7.4. REGULARIZATION\ntheta_store = np.zeros((d,50))\nfor i in range(50):\nlambd = lambd_set[i]\ntheta = cvx.Variable(d)\nobjective = cvx.Minimize( cvx.sum_squares(X*theta-y) \\\n+ lambd*cvx.norm1(theta) )\n# objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \\\n+ lambd*cvx.sum_squares(theta) )\nprob = cvx.Problem(objective)\nprob.solve()\ntheta_store[:,i] = theta.value\nfor i in range(d):\nplt.semilogx(lambd_set, theta_store[i,:])\n10-2100102104106108\nlambda-202468101214feature attributefunding\n% high\n% no high\n% college\n% graduate\n10-2100102104106108\nlambda-202468101214feature attributefunding\n% high\n% no high\n% college\n% graduate\n(a) LASSO (b) Ridge\nFigure 7.26: Ridge and LASSO regression on the crime-rate dataset. (a) The LASSO regression suggests\nthat there are only a few active components as we change \u03bb. (b) The ridge regression returns a set of\ndense solutions for all choices of \u03bb.\nFigure 7.26 shows some interesting differences between the two regression models.\n\u0088Trajectory . For the \u2225 \u00b7 \u22252estimate b\u03b82(\u03bb), the trajectory of the regression coefficients\nis smooth. This is attributable to the fact that the training loss E2(\u03b8) is continuously\ndifferentiable in \u03b8, and so the solution trajectory is smooth. By contrast, the \u2225 \u00b7 \u2225 1\nestimate b\u03b81(\u03bb) has a more disruptive trajectory.\n\u0088Active members . For the LASSO problem, b\u03b81(\u03bb) switches the active member as \u03bb\nchanges. For example, the feature high-school is the first one being activated when\n\u03bb\u2193. This implies that if we limit ourselves to only onefeature, then high-school is\nthe feature we should select. The ridge regression does not have this feature-selection\nproperty. How about when \u03bb= 106? In this case, the LASSO has two active members:\nfunding and high-school . This suggests that if there are two contributing factors,\nfunding andhigh-school are the two. As \u03bb= 104, we see that in LASSO, the green\ncurve goes to zero but then the red curve rises. This means a correlation between\n453", "469": "CHAPTER 7. REGRESSION\nhigh school andno high school , which should not be a surprise because they are\ncomplementary to each other.\n\u0088Magnitude of solutions . The magnitude of the solutions does not necessarily convey\na clear conclusion because the feature vectors (e.g., high school ) and the observable\ncrime rate have different units.\n\u0088Limiting solutions . As\u03bb\u21920, both b\u03b81(\u03bb) andb\u03b82(\u03bb) reach the same solution, because\nthe training losses are identical when \u03bb= 0.\nLASSO for overfitting\nDoes LASSO help to mitigate the overfitting problem? Not always, but it often does. In\nFigure 7.27 we consider fitting a dataset of N= 20 data points. The ground truth model\nwe use is\nyn=L0(xn) + 0.5L1(xn) + 0.5L2(xn) + 1.5L3(xn) +L4(xn) +en,\nwhere en\u223cGaussian(0 , \u03c32) for \u03c3= 0.25. When fitting the data, we purposely choose a\n20th-order Legendre polynomial as the regression model. With only N= 20 data points, we\ncan be almost certain that there is overfitting.\nThe MATLAB and Python codes for solving this LASSO problem are shown below.\n% MATLAB code to demonstrate overfitting and LASSO\n% Generate data\nN = 20;\nx = linspace(-1,1,N)\u2019;\na = [1, 0.5, 0.5, 1.5, 1];\ny = a(1)*legendreP(0,x)+a(2)*legendreP(1,x)+a(3)*legendreP(2,x)+ ...\na(4)*legendreP(3,x)+a(5)*legendreP(4,x)+0.25*randn(N,1);\n% Solve LASSO using CVX\nd = 20;\nX = zeros(N, d);\nfor p=0:d-1\nX(:,p+1) = reshape(legendreP(p,x),N,1);\nend\nlambda = 2;\ncvx_begin\nvariable theta(d)\nminimize( sum_square( X*theta - y ) + lambda * norm(theta , 1) )\ncvx_end\n% Plot results\nt = linspace(-1, 1, 200);\nXhat = zeros(length(t), d);\nfor p=0:d-1\nXhat(:,p+1) = reshape(legendreP(p,t),200,1);\nend\nyhat = Xhat*theta;\n454", "470": "7.4. REGULARIZATION\nplot(x,y, \u2019ko\u2019,\u2019LineWidth\u2019,2, \u2019MarkerSize\u2019, 10); hold on;\nplot(t,yhat,\u2019LineWidth\u2019,6,\u2019Color\u2019,[0.2 0.5 0.2]);\n# Python code to demonstrate overfitting and LASSO\nimport cvxpy as cvx\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Setup the problem\nN = 20\nx = np.linspace(-1,1,N)\na = np.array([1, 0.5, 0.5, 1.5, 1])\ny = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \\\na[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \\\na[4]*eval_legendre(4,x) + 0.25*np.random.randn(N)\n# Solve LASSO using CVX\nd = 20\nlambd = 1\nX = np.zeros((N, d))\nfor p in range(d):\nX[:,p] = eval_legendre(p,x)\ntheta = cvx.Variable(d)\nobjective = cvx.Minimize( cvx.sum_squares(X*theta-y) \\\n+ lambd*cvx.norm1(theta) )\nprob = cvx.Problem(objective)\nprob.solve()\nthetahat = theta.value\n# Plot the curves\nt = np.linspace(-1, 1, 500)\nXhat = np.zeros((500,d))\nfor p in range(P):\nXhat[:,p] = eval_legendre(p,t)\nyhat = np.dot(Xhat, thetahat)\nplt.plot(x, y, \u2019o\u2019)\nplt.plot(t, yhat, linewidth=4)\nLet us compare the various regression results. Figure 7.27 (b) shows the vanilla regres-\nsion, which as you can see fits the N= 20 data points very well. However, no one would\nbelieve that such a fitting curve can generalize to unseen data. Figure 7.27 (c) shows the\nridge regression result. When performing the analysis, we sweep a range of \u03bband pick the\nvalue \u03bb= 0.5 so that the fitted curve is neither too \u201cwild\u201d nor too \u201cflat\u201d. We can see that\nthe fitting is improved. However, since the ridge regression only penalizes large-magnitude\ncoefficients, the fitting is still not ideal. Figure 7.27 (d) shows the LASSO regression result.\nSince the true model is a 4th-order polynomial and we use a 20th-order polynomial, the true\nsolution is sparse. Therefore, LASSO is helpful, and hence we can pick a sparse solution.\nThe significance of LASSO is often not about the fitting of the data points but the\n455", "471": "CHAPTER 7. REGRESSION\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n(a) Ground truth model (b) Vanilla regression\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n-1 -0.5 0 0.5 1-101234\ndata\nfitted curve\n(c) Ridge (d) LASSO\nFigure 7.27: We fit a dataset of N= 20 data points. (a) The ground truth model that generates\nthe data. The model is a 4th-order ordinary polynomial. (b) Vanilla regression result, without any\nregularization. Note that there is severe overfitting because the model complexity is too high. (c) Ridge\nregression result, by setting \u03bb= 0.5. (d) LASSO regression result, by setting \u03bb= 2.\nnumber of active coefficients. In Figure 7.28 we show a comparison between the ground\ntruth coefficients, the vanilla regression coefficients, the ridge regression coefficients, and the\nLASSO regression coefficients. It is evident that the LASSO solution contains a much smaller\nnumber of non-zeros compared to the ridge regression. Most of the high-order coefficients\nare zero. By contrast, the vanilla regression coefficients are wild. The ridge regression is\nbetter, but there are many non-zero high-order coefficients.\nClosing remark . In this section, we discussed two regularization techniques: ridge regres-\nsion and LASSO regression. Both techniques are about adding a penalty term to the training\nloss to constrain the regression coefficients. In the optimization literature, writings on ridge\nand LASSO regression are abundant, covering both algorithms and theoretical properties.\nAn example of a theoretical question addressed in the literature is: Under what conditions\nis LASSO guaranteed to recover the correct support of the solution, i.e., locating the correct\npositions of the non-zeros? Problems like these are beyond the scope of this book.\n456", "472": "7.5. SUMMARY\n-2-1012\n5 10 15 20\n-15-10-5051015\n5 10 15 20\n(a) Ground truth model (b) Vanilla regression\n-2-1012\n5 10 15 20\n-2-1012\n5 10 15 20\n(c) Ridge (d) LASSO\nFigure 7.28: Coefficients of the regression models. (a) The ground truth model, which is a 4th-order\npolynomial. There are only 5 non-zero coefficients. (b) The vanilla regression coefficients. Note that\nthe values are wild and large, although the curve fits the training data points very well. (c) The ridge\nregression coefficients. While the overall magnitudes are significantly improved from the vanilla, some\nhigh-order coefficients are still non-zero. (d) The LASSO regression coefficients. There are very few\nnon-zeros, and the non-zeros match well with the ground truth.\n7.5 Summary\nRegression is one of the most widely used techniques in data science. The formulation of the\nregression problem is as simple as setting up a system of linear equations:\nminimize\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252, (7.42)\nwhich has a closed-form solution. The biggest problems in practice are outliers, lack of\ntraining samples, and poor choice of the regression model.\n\u0088Outliers : We always recommend plotting the data whenever possible to check if there\nare obvious outliers. There are also statistical tests in which you can evaluate the\nvalidity of your samples. One simple way to debug outliers is to run the regression\n457", "473": "CHAPTER 7. REGRESSION\nand check the prediction error against each training sample. If you have an outlier,\nand if your model is of reasonably low complexity, then a sample with an excessively\nlarge prediction error is an outlier. For example, if most of the training samples are\nwithin one standard deviation from your prediction but a few are substantially off,\nyou will know which ones are the outliers. Robust linear regression is one technique for\ncountering outliers, but an experienced data scientist can often reject outliers before\nrunning any regression algorithms. Domain knowledge is of great value for this purpose.\n\u0088Lack of training samples : As we have discussed in the overfitting section, it is ex-\ntremely important to ensure that your model complexity is appropriate for the number\nof training samples. If the training set is small, do not use a complex model. Regu-\nlarization techniques are valuable tools to mitigate overfitting. However, choosing a\ngood regularization requires domain knowledge. For example, if you know that some\nfeatures are not important, you need to scale them properly so as not to over-influence\nthe regression solution.\n\u0088Wrong model : We have mentioned several times that regression can always return you\na result because regression is an optimization problem. However, whether that result\nis meaningful depends on how meaningful your regression problem is. For example, if\nthe noise is i.i.d. Gaussian, a data fidelity term with \u2225 \u00b7 \u22252would be a good choice;\nhowever, if the noise is i.i.d. Poisson, \u2225 \u00b7 \u22252would become a very bad model. We need\na tighter connection with the statistics of the underlying data-generation model for\nproblems like these. This is the subject of our next chapter, on parameter estimation.\n7.6 References\nLinear regression\nTreatment of standard linear regression is abundant. In the context of machine learning and\ndata science, the following references are useful.\n7-1 Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction\nto Statistical Learning with Applications in R , Springer 2013, Chapter 3.\n7-2 Stephen Boyd and Lieven Vandenberghe, Convex Optimization , Cambridge University\nPress, 2004. Chapter 6.\n7-3 Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical\nLearning , Springer, 2001. Chapter 3.\n7-4 Christopher Bishop, Pattern Recognition and Machine Learning , Springer 2006. Chap-\nter 3.1.\n7-5 Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin, Learning from Data ,\nAML Book, 2012. Chapter 3.2\nOverfitting and Bias/Variance\nThe theory of overfitting and the trade-off between bias and variance can be found in multiple\nreferences. The following are basic treatments of the subject.\n458", "474": "7.7. PROBLEMS\n7-6 Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin, Learning from Data ,\nAML Book, 2012. Chapter 4.\n7-7 Christopher Bishop, Pattern Recognition and Machine Learning , Springer 2006. Chap-\nter 3.2.\nRidge and LASSO regression\nRidge and LASSO regression are important tools in statistical learning today. The following\ntwo textbooks cover some of the perspectives of the statistical community and the signal\nprocessing community.\n7-8 Trevor Hastie, Robert Tibshirani, and Martin Wainwright, Statistical Learning with\nSparsity: The LASSO and Generalizations , CRC Press, 2015.\n7-9 Michael Elad, Sparse and Redundant Representations , Springer, 2010. Chapters 1\nand 3.\n7.7 Problems\nExercise 1.\n(a) Construct a dataset with N= 20 samples, following the model\nyn=d\u22121X\np=0\u03b8pLp(xn) +en, (7.43)\nwhere \u03b80= 1, \u03b81= 0.5,\u03b82= 0.5,\u03b83= 1.5,\u03b84= 1, for \u22121< x < 1. Here, Lp(x) is the\nLegendre polynomial of the pth order. The N= 20 samples are random uniformly sam-\npled from the interval [ \u22121,1]. The noise samples enare i.i.d. Gaussian with variance\n\u03c32= 0.252. Plot the dataset using the MATLAB or Python command scatter .\n(b) Run the regression using the same model where d= 5, without any regularization.\nPlot the predicted curve and overlay with the training samples.\n(c) Repeat (b) by running the regression with d= 20. Explain your observations.\n(d) Increase the number of training samples NtoN= 50, N= 500, and N= 5000, and\nrepeat (c). Explain your observations.\n(e) Construct a testing dataset with M= 1000 testing samples. For each of the regression\nmodels trained in (b)-(d), compute the testing error.\nExercise 2.\nConsider a data generation model\nxn=N\u22121X\nk=0cke\u2212j2\u03c0kn\nN, n= 0, . . . , N \u22121.\n459", "475": "CHAPTER 7. REGRESSION\n(a) Write the above equation in matrix-vector form\nx=Wc.\nWhat are the vectors candx, and what is the matrix W?\n(b) Show that Wis orthogonal, i.e.,, WHW=I, where WHis the conjugate transpose\nofW.\n(c) Using (b), derive the least squares regression solution.\nExercise 3.\nConsider a simplified LASSO regression problem:\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225y\u2212\u03b8\u22252+\u03bb\u2225\u03b8\u22251. (7.44)\nShow that the solution is given by\nb\u03b8= sign( y)\u00b7max (|y| \u2212\u03bb,0), (7.45)\nwhere \u00b7is the elementwise multiplication.\nExercise 4.\nA one-dimensional signal is corrupted by blur and noise:\nyn=L\u22121X\n\u2113=0h\u2113xn\u2212\u2113+en.\n(a) Formulate the least squares regression problem in matrix-vector form y=Hx+e.\nFindx,yandH.\n(b) Consider a regularization function\nR(x) =NX\nn=2(xn\u2212xn\u22121)2.\nShow that this regularization is equivalent to R(x) =\u2225Dx\u22252for some D. Find D.\n(c) Using the regularization in (b), derive the regularized least squares regression result:\nminimize\nx\u2225y\u2212Hx\u22252+\u03bb\u2225Dx\u22252.\nExercise 5.\nLet\u03c3(\u00b7) be the sigmoid function\n\u03c3(a) =1\n1 +ea.\nWe want to use \u03c3(a) as a basis function.\n460", "476": "7.7. PROBLEMS\n(a) Show that the tanh function and the sigmoid function are related by\ntanh( a) = 2 \u03c3(2a)\u22121.\n(b) Show that a linear combination of sigmoid functions\nyn=\u03b80+d\u22121X\np=1\u03b8p\u03c3\u0012xn\u2212\u00b5j\ns\u0013\nis equivalent to a linear combination of tanh functions\nyn=\u03b10+d\u22121X\np=1\u03b1ptanh\u0012xn\u2212\u00b5j\n2s\u0013\n.\n(c) Find the relationship between \u03b8pand\u03b1p.\nExercise 6. (NHANES Part 1)(Data download)\nThe National Health and Nutrition Examination Survey (NHANES) is a program to assess\nthe health and nutritional status of adults and children in the United States8. The complete\nsurvey result contains over 4,000 samples of health-related data of individuals who partici-\npated in the survey between 2011 and 2014. In the following exercises, we will focus on two\ncategories of the data for each individual: height (in mm) and body mass index (BMI). The\ndata is divided into two classes based on gender. Table 1 contains snippets of the data.\nindex female bmi female stature mm\n0 28.2 1563\n1 22.2 1716\n2 27.1 1484\n3 28.1 1651index male bmi male stature mm\n0 30 1679\n1 25.6 1586\n2 24.2 1773\n3 27.4 1816\nTable 7.2: Male and Female Data Snippets\nUsecsv.reader to read the training data files for the two data classes.\nImportant! Before proceeding to the problems,\n\u0088normalize the number in male_stature_mm andfemale_stature_mm by dividing them\nby 1000, and\n\u0088normalize that of male_bmi andfemale_bmi by dividing them by 10.\nThis will significantly reduce the numerical error.\nConsider a linear model:\ng\u03b8=\u03b8Tx, (7.46)\n8https://www.cdc.gov/nchs/nhanes/index.htm\n461", "477": "CHAPTER 7. REGRESSION\nThe regression problem we want to solve is\nb\u03b8= argmin\n\u03b8\u2208RdNX\nn=1(yn\u2212g\u03b8(xn))2,\nwhere D={(xn, yn)}N\nn=1is the training dataset. Putting the equation into the matrix form,\nwe know that the optimization is equivalent to\nb\u03b8= argmin\n\u03b8\u2208Rd\u2225y\u2212X\u03b8\u22252\n|{z}\nEtrain(\u03b8).\n(a) Derive the solution b\u03b8. State the conditions under which the solution is the unique\nglobal minimum in terms of the rank of X. Suggest two techniques that can be used\nwhen XTXis not invertible.\n(b) For the NHANES dataset, assign yn= +1 if the nth sample is a male and yn=\u22121\nif the nth sample is a female. Implement your answer in (a) with Python to solve the\nproblem. Report your answer.\n(c) Repeat (b), but this time use CVXPY. Report your answer, and compare with (b).\nExercise 7. (NHANES Part 2)(Data download)\nWe want to do a classification based on the linear model we found in the previous exercise.\nThe classifier we will use is\npredicted label = sign( g\u03b8(x)), (7.47)\nwhere x\u2208Rdis the a test sample. Here, we label +1 for male and \u22121 for female. Because\nthe dataset we consider in this exercise has only two columns, the linear model is\ng\u03b8(x) =\u03b80+\u03b81x1+\u03b82x2,\nwhere x= [1, x1, x2]Tis the input data and \u03b8= [\u03b80, \u03b81, \u03b82]Tis the parameter vector.\n(a) First, we want to visualize the classifier.\n(i) Plot the training data points of the male and female classes. Mark the male class\nwith blue circles and the female class with red dots.\n(ii) Plot the decision boundary g\u03b8(\u00b7) and overlay it with the data plotted in (a).\nHint: g\u03b8(\u00b7) is a straight line in 2D. You can express x2in terms of x1and other\nparameters.\n(b) (This problem requires knowledge of the content of Chapter 9). Report the classifica-\ntion accuracy. To do so, take testing data xand compute the prediction according to\nEquation (7.47).\n(i) What is the Type 1 error (False Alarm) of classifying males? That is, what is the\npercentage of testing samples that should be female but a male was predicted?\n(ii) What is the Type 2 error (Miss) of classifying males? That is, what is the per-\ncentage of testing samples that should be male but a female was predicted?\n462", "478": "7.7. PROBLEMS\n(iii) What is the precision and recall for this classifier? For the definitions of precision\nand recall, refer to Chapter 9.5.4.\nExercise 8. (NHANES Part 3)(Data download)\nThis exercise requires some background in optimization. Please refer to Reference [7.2, Chap-\nter 9 and 10]. Consider the following three optimization problems:\nb\u03b8\u03bb= argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252\n2+\u03bb\u2225\u03b8\u22252\n2, (7.48)\nb\u03b8\u03b1= argmin\n\u03b8\u2208Rd\u2225X\u03b8\u2212y\u22252\n2subject to \u2225\u03b8\u22252\n2\u2264\u03b1, (7.49)\nb\u03b8\u03f5= argmin\n\u03b8\u2208Rd\u2225\u03b8\u22252\n2subject to \u2225X\u03b8\u2212y\u22252\n2\u2264\u03f5. (7.50)\n(a) Set lambd = np.arange(0.1,10,0.1) . Plot\n\u0088\u2225Xb\u03b8\u03bb\u2212y\u22252\n2as a function of \u2225b\u03b8\u03bb\u22252\n2.\n\u0088\u2225Xb\u03b8\u03bb\u2212y\u22252\n2as a function of \u03bb.\n\u0088\u2225b\u03b8\u03bb\u22252\n2as a function of \u03bb.\n(b) (i) Write down the Lagrangian for each of the three problems. Note that the first\nproblem does not have any Lagrange multiplier. For the second and third prob-\nlems you may use the following notations:\n\u0088\u03b3\u03b1= the Lagrange multiplier of Equation (7.49), and\n\u0088\u03b3\u03f5= the Lagrange multiplier of Equation (7.50).\n(ii) State the first-order optimality conditions (the Karush-Kuhn-Tucker or KKT\nconditions) for each of the three problems. Express your answers in terms of X,\n\u03b8,y,\u03bb,\u03b1,\u03f5, and the two Lagrange multipliers \u03b3\u03b1,\u03b3\u03f5.\n(iii) Fix \u03bb >0. We can solve Equation (7.48) to obtain b\u03b8\u03bb. Find \u03b1and the Lagrange\nmultiplier \u03b3\u03b1in Equation (7.49) such that b\u03b8\u03bbwould satisfy the KKT conditions\nof Equation (7.49).\n(iv) Fix \u03bb >0. We can solve Equation (7.48) to obtain b\u03b8\u03bb. Find \u03f5and the Lagrange\nmultiplier \u03b3\u03f5in Equation (7.50) such that b\u03b8\u03bbwould satisfy the KKT conditions\nof Equation (7.50).\n(v) Fix \u03bb >0. By using the \u03b1and\u03b3\u03b1you found in (iii), you can show that b\u03b8\u03bbwould\nsatisfy the KKT conditions of Equation (7.49). Is it enough to claim that b\u03b8\u03bbis\nthe solution of Equation (7.49)? If yes, why? If no, what else do we need to show?\nPlease elaborate through a proof, if needed.\nExercise 9.\nConsider a training dataset Dtrain={(x1, y1), . . . , (xN, yN)}and a weight w= [w1, . . . , w N]T.\nFind the regression solution to the following problem and discuss how you would choose the\nweight.\nb\u03b8= argmin\n\u03b8\u2208RdNX\nn=1wn\u0000\nyn\u2212xT\nn\u03b8\u00012. (7.51)\n463", "479": "CHAPTER 7. REGRESSION\nExercise 10.\nConsider a training dataset Dtrain={(x1, y1), . . . , (xN, yN)}. Suppose that the input data\nxnis corrupted by i.i.d. Gaussian noise en\u223cGaussian(0 , \u03c32Id) so that the training set\nbecomes Dtrain={(x1+e1, y1), . . . , (xN+eN, yN)}. Show that the (vanilla) least squares\nlinear regression by taking the expectation over en,\nb\u03b8= argmin\n\u03b8\u2208RdNX\nn=1Eenh\u0000\nyn\u2212(xn+en)T\u03b8\u00012i\n, (7.52)\nis equivalent to a ridge regression.\n464", "480": "Chapter 8\nEstimation\nIn this chapter, we discuss another set of important combat skills in data science, namely es-\ntimation . Estimation has a close relationship with regression. Regression primarily takes the\noptimization route, while estimation takes the probabilistic route. As we will see, at a cer-\ntain point the two will merge. That is, under some specific statistical conditions, estimation\nprocesses will coincide with the regression.\nEstimation is summarized pictorially in Figure 8.1 . Imagine that we have some random\nsamples X1, . . . , X N. These samples are drawn from a distribution fX(x;\u03b8), where \u03b8is a\nparameter that characterizes the distribution. The parameter \u03b8is not known to us. The\ngoal of estimation is to solve an inverse problem to recover the parameter based on the\nobservations X1, . . . , X N.\nFigure 8.1: Estimation is an inverse problem of recovering the unknown parameters that were used by\nthe distribution. In this figure, the PDF of Xusing a parameter \u03b8is denoted as fX(x;\u03b8). The forward\ndata-generation process takes the parameter \u03b8and creates the random samples X1, . . . , X N. Estimation\ntakes these observed random samples and recovers the underlying model parameter \u03b8.\nWhat is estimation?\nEstimation is an inverse problem with the goal of recovering the underlying pa-\nrameter \u03b8of a distribution fX(x;\u03b8) based on the observed samples X1, . . . , X N.\n465", "481": "CHAPTER 8. ESTIMATION\nWhat are parameters?\nBefore we discuss the methods of estimation, let us clarify the meaning of the parameter \u03b8.\nAll probability density functions (PDFs) have parameters. For example, a Bernoulli random\nvariable is characterized by a parameter pthat defines the probability of getting a \u201chead\u201d. A\nGaussian random variable is characterized by two parameters: the mean \u00b5and variance \u03c32.\nExample 8.1 . (Parameter of a Bernoulli ) IfXnis a Bernoulli random variable, then\nthe PMF has a parameter \u03b8:\npXn(xn;\u03b8) =\u03b8xn(1\u2212\u03b8)1\u2212xn.\nRemark . The PMF is expressed in this form because xnis either 1 or 0:\npXn(xn;\u03b8) =(\n\u03b81(1\u2212\u03b8)1\u22121=\u03b8, if xn= 1,\n\u03b80(1\u2212\u03b8)1\u22120= 1\u2212\u03b8, if xn= 0.\nExample 8.2 . (Parameter of a Gaussian ) IfXnis a Gaussian random variable, the\nPDF is\nfXn(xn;\u03b8|{z}\n=(\u00b5,\u03c3)) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(xn\u2212\u00b5)2\n2\u03c32\u001b\n,\nwhere \u03b8= [\u00b5, \u03c3] consists of both the mean and the variance. We can also designate\nthe parameter \u03b8to be the mean only. For example, if we know that \u03c3= 1, then the\nPDF is\nfXn(xn;\u03b8|{z}\n=\u00b5) =1\u221a\n2\u03c0exp\u001a\n\u2212(xn\u2212\u00b5)2\n2\u001b\n,\nwhere \u03b8is the mean.\nSince all probability density functions have parameters, estimating them from the\nobserved random variables is a well-defined inverse problem. Of course, there are better\nestimates and there are worse estimates. Let us look at the following example to develop\nour intuitions about estimation.\nFigure 8.2 shows a dataset containing 1000 data points generated from a 2D Gaussian\ndistribution with an unknown mean vector \u00b5and an unknown covariance matrix \u03a3. We\nduplicate this dataset in the four subfigures. The estimation problem is to recover the\nunknown mean vector \u00b5and the covariance matrix \u03a3. In the subfigures we propose four\ncandidates, each with a different mean vector and a different covariance matrix. We draw\nthe contour lines of the corresponding Gaussians. It can be seen that some Gaussians fit the\ndata better than others. The goal of this chapter is to develop a systematic way of finding\nthe best fit for the data.\nPlan for this chapter\nThe discussions in this chapter concern the three elementary distributions:\n466", "482": "-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345\n-5 -4 -3 -2 -1 0 1 2 3 4 5-5-4-3-2-1012345Bad estimate Bad estimate Bad estimate Good estimate\n\u00b5=\u0014\n2\n\u22120.5\u0015\n\u00b5=\u0014\n0\n\u22121.5\u0015\n\u00b5=\u0014\n\u22120.5\n\u22120.7\u0015\n\u00b5=\u0014\n0\n0\u0015\n\u03a3=\u00140.25 0 .2\n0.2 1\u0015\n\u03a3=\u00141\u22120.2\n\u22120.2 0 .1\u0015\n\u03a3=\u00141 0\n0 1\u0015\n\u03a3=\u00140.25 0 .3\n0.3 1\u0015\nFigure 8.2: An estimation problem. Given a set of 1000 data points drawn from a Gaussian distribution\nwith unknown mean \u00b5and covariance \u03a3, we propose several candidate Gaussians and see which one\nwould be the best fit to the data. Visually, we observe that the right-most Gaussian has the best fit.\nThe goal of this chapter is to develop a systematic way of solving estimation problems of this type.\n\u0088Likelihood: fX|\u0398(x|\u03b8), which is the conditional PDF of Xgiven that the parameter\nis\u0398.\n\u0088Prior: f\u0398(\u03b8), which is the PDF of \u0398.\n\u0088Posterior: f\u0398|X(\u03b8|x), which is the conditional PDF of \u0398given the data X.\nEach of these density functions has its respective meaning, and consequently a set of different\nestimation techniques. In Section 8.1 we introduce the concept of maximum-likelihood (ML)\nestimation. As the name suggests, the estimate is constructed by maximizing the likelihood\nfunction. We will discuss a few examples of ML estimation and draw connections between\nML estimation and regression. In Section 8.2 we will discuss several basic properties of an\nML estimate. Specifically, we will introduce the ideas of unbiasedness, consistency, and the\ninvariance principle.\nThe second topic discussed in this chapter is the maximum-a-posteriori (MAP) esti-\nmation, detailed in Section 8.3. In MAP, the parameter \u0398is a random variable. Since \u0398is a\nrandom variable, it has its own probability density function f\u0398(\u03b8), which we call the prior.\nGiven the likelihood and the prior, we can define the posterior . The MAP estimation finds\nthe peak of the posterior distribution as a way to \u201cexplain\u201d the data. Several important\ntopics will be covered in Section 8.3. For example, we will discuss the choice of the prior\nvia the concept of conjugate prior . We will also discuss how MAP is related to regularized\nregressions such as the ridge and LASSO regressions.\nThe third topic is the minimum mean-square estimation (MMSE), outlined in Sec-\ntion 8.4. The MMSE is a Bayesian approach. An important result that will be demonstrated\nis that the MMSE estimate is the conditional expectation of the posterior distribution. In\nother words, it is the mean of the posterior. An MMSE estimate has an important difference\ncompared to a MAP estimate, namely that while an MMSE estimate is the mean of the\nposterior, a MAP estimate is the mode of the posterior. We discuss the formulation of the\nestimation problem and ways of solving the problem. We also discuss how the MMSE can\nbe performed for multidimensional Gaussian distributions.\n467", "483": "CHAPTER 8. ESTIMATION\n8.1 Maximum-Likelihood Estimation\nMaximum-likelihood (ML) estimation, as the name suggests, is an estimation method that\n\u201cmaximizes\u201d the \u201clikelihood\u201d. Therefore, to understand the ML estimation, we first need to\nunderstand the meaning of likelihood, and why maximizing the likelihood would be useful.\n8.1.1 Likelihood function\nConsider a set of Ndata points D={x1, x2, . . . , x N}. We want to describe these data points\nusing a probability distribution. What would be the most general way of defining such a\ndistribution?\nSince we have Ndata points, and we do not know anything about them, the most gen-\neral way to define a distribution is as a high-dimensional probability density function (PDF)\nfX(x). This is a PDF of a random vector X= [X1, . . . , X N]T. A particular realization of\nthis random vector is x= [x1, . . . , x N]T.\nfX(x) is the most general description for the Ndata points because fX(x) is the\njoint PDF of all variables. It provides the complete statistical description of the vector X.\nFor example, we can compute the mean vector E[X], the covariance matrix Cov( X), the\nmarginal distributions, the conditional distribution, the conditional expectations, etc. In\nshort, if we know fX(x), we know everything about X.\nThe joint PDF fX(x) is always parameterized by a certain parameter \u03b8. For example, if\nwe assume that Xis drawn from a joint Gaussian distribution, then fX(x) is parameterized\nby the mean vector \u00b5and the covariance matrix \u03a3. So we say that the parameter \u03b8is\n\u03b8= (\u00b5,\u03a3). To state the dependency on the parameter explicitly, we write\nfX(x;\u03b8) = PDF of the random vector Xwith a parameter \u03b8.\nWhen you express the joint PDF as a function of xand\u03b8, you have two variables to\nplay with. The first variable is the observation x, which is given by the measured data. We\nusually think about the probability density function fX(x) in terms of x, because the PDF\nis evaluated at X=x. In estimation, however, xis something that you cannot control.\nWhen your boss hands a dataset to you, xis already fixed. You can consider the probability\nof getting this particular x, but you cannot change x.\nThe second variable stated in fX(x;\u03b8) is the parameter \u03b8. This parameter is what\nwe want to find out, and it is the subject of interest in an estimation problem. Our goal is\nto find the optimal \u03b8that can offer the \u201cbest explanation\u201d to data x, in the sense that it\ncan maximize fX(x;\u03b8).\nThelikelihood function is the PDF that shifts the emphasis to \u03b8:\nDefinition 8.1. LetX= [X1, . . . , X N]Tbe a random vector drawn from a joint PDF\nfX(x;\u03b8), and let x= [x1, . . . , x N]Tbe the realizations. The likelihood function is a\n468", "484": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nfunction of the parameter \u03b8given the realizations x:\nL(\u03b8|x)def=fX(x;\u03b8). (8.1)\nA word of caution: L(\u03b8|x) isnota conditional PDF because \u03b8is not a random variable.\nThe correct way to interpret L(\u03b8|x) is to view it as a function of \u03b8. This function changes\nits shape according the observed data x. We will return to this point shortly.\nIndependent observations\nWhile fX(x) provides us with a complete picture of the random vector X, using fX(x) is\ntedious. We need to describe how each Xnis generated and describe how Xnis related to\nXmfor all pairs of nandm. If the vector Xcontains Nentries, then there are N2/2 pairs\nof correlations we need to compute. When Nis large, finding fX(x) would be very difficult\nif not impossible.\nIn practice, fX(x) may sometimes be overkill. For example, if we measure the inter-\narrival time of a bus for several days, it is quite likely that the measurements will not be\ncorrelated. In this case, instead of using the full fX(x), we can make assumptions about\nthe data points. The assumption we will make is that all the data points are independent\nand that they are drawn from an identical distribution fX(x). The assumption that the\ndata points are independently and identically distributed (i.i.d.) significantly simplifies the\nproblem so that the joint PDF fXcan be written as a product of single PDFs fXn:\nfX(x) =fX1,...,X N(x1, . . . , x N) =NY\nn=1fXn(xn).\nIf you prefer a visualization, we can take a look at the covariance matrix, which goes\nfrom a full covariance matrix to a diagonal matrix and then to an identity matrix:\n\uf8ee\n\uf8ef\uf8f0Var[X1] Cov( X1, X2)\u00b7\u00b7\u00b7 Cov(X1, XN)\nCov[X2, X1] Var[ X2] \u00b7\u00b7\u00b7 Cov(X2, XN)\n............\nCov(XN, X1) Cov( XN, X2)\u00b7\u00b7\u00b7 Var[XN]\uf8f9\n\uf8fa\uf8fb=\u21d2\nindependent\uf8ee\n\uf8ef\uf8f0Var[X1] 0 \u00b7\u00b7\u00b7 0\n0 Var[ X2]\u00b7\u00b7\u00b7 0\n............\n0 0 \u00b7\u00b7\u00b7 Var[XN]\uf8f9\n\uf8fa\uf8fb\n=\u21d2\nidentical\uf8ee\n\uf8ef\uf8f0\u03c320\u00b7\u00b7\u00b7 0\n0\u03c32\u00b7\u00b7\u00b7 0\n............\n0 0 \u00b7\u00b7\u00b7 \u03c32\uf8f9\n\uf8fa\uf8fb.\nThe assumption of i.i.d. is strong. Not all data can be modeled as i.i.d. (For example,\nphotons passing through a scattering medium have correlated statistics.) However, if the\ni.i.d. assumption is valid, we can simplify the model significantly.\nIf the data points are i.i.d., then we can write the joint PDF as\nfX(x;\u03b8) =NY\nn=1fXn(xn;\u03b8).\nThis gives us a simplified form of the likelihood function, written as a product of the indi-\nvidual PDFs.\n469", "485": "CHAPTER 8. ESTIMATION\nDefinition 8.2. Given i.i.d. random variables X1, . . . , X Nthat all have the same PDF\nfXn(xn), the likelihood function is\nL(\u03b8|x)def=NY\nn=1fXn(xn;\u03b8). (8.2)\nIn computation we often take the log of the likelihood function. We call the resulting function\nthelog-likelihood .\nDefinition 8.3. Given a set of i.i.d. random variables X1, . . . , X Nwith PDF fXn(x; ;\u03b8),\nthelog-likelihood is defined as\nlogL(\u03b8|x) = log fX(x;\u03b8) =NX\nn=1logfXn(xn;\u03b8). (8.3)\nExample 8.3 . Find the log-likelihood of a sequence of i.i.d. Gaussian random variables\nX1, . . . , X Nwith mean \u00b5and variance \u03c32.\nSolution . Since the random variables X1, . . . , X Nare i.i.d. Gaussian, the PDF is\nfX(x;\u00b5, \u03c32) =NY\nn=1\u001a1\u221a\n2\u03c0\u03c32e\u2212(xn\u2212\u00b5)2\n2\u03c32\u001b\n. (8.4)\nTaking the log on both sides yields the log-likelihood function:\nlogL(\u00b5, \u03c32|x) = log fX(x;\u00b5, \u03c32)\n= log(NY\nn=1\u001a1\u221a\n2\u03c0\u03c32e\u2212(xn\u2212\u00b5)2\n2\u03c32\u001b)\n=NX\nn=1log\u001a1\u221a\n2\u03c0\u03c32e\u2212(xn\u2212\u00b5)2\n2\u03c32\u001b\n=NX\nn=1\u001a\n\u22121\n2log(2\u03c0\u03c32)\u2212(xn\u2212\u00b5)2\n2\u03c32\u001b\n=\u2212N\n2log(2\u03c0\u03c32)\u22121\n2\u03c32NX\nn=1(xn\u2212\u00b5)2.\nPractice Exercise 8.1 . Find the log-likelihood of a sequence of i.i.d. Bernoulli random\nvariables X1, . . . , X Nwith parameter \u03b8.\n470", "486": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nSolution . IfX1, . . . , X Nare i.i.d. Bernoulli random variables, we have\nfX(x;\u03b8) =NY\nn=1\u001a\n\u03b8xn(1\u2212\u03b8)1\u2212xn\u001b\n.\nTaking the log on both sides of the equation yields the log-likelihood function:\nlogL(\u03b8|x) = log(NY\nn=1\u001a\n\u03b8xn(1\u2212\u03b8)1\u2212xn\u001b)\n=NX\nn=1log\u001a\n\u03b8xn(1\u2212\u03b8)1\u2212xn\u001b\n=NX\nn=1xnlog\u03b8+ (1\u2212xn) log(1 \u2212\u03b8)\n= NX\nn=1xn!\n\u00b7log\u03b8+ \nN\u2212NX\nn=1xn!\n\u00b7log(1\u2212\u03b8).\nVisualizing the likelihood function\nThe likelihood function L(\u03b8|x) is a function of \u03b8, but its value also depends on the under-\nlying measurements x. It is extremely important to keep in mind the presence of both.\nTo help you visualize the effect of \u03b8andx, we consider a set of i.i.d. Bernoulli random\nvariables. As we have just shown in the practice exercise, the likelihood function of these\ni.i.d. random variables is\nlogL(\u03b8|x) = NX\nn=1xn!\n|{z}\nS\u00b7log\u03b8+ \nN\u2212NX\nn=1xn!\n|{z }\nN\u2212S\u00b7log(1\u2212\u03b8), (8.5)\nwhere we define S=PN\nn=1xnas the sum of the (binary) measurements.\nTo make the dependency on Sand\u03b8explicit, we write L(\u03b8|x) as\nlogL(\u03b8|S) =Slog\u03b8+ (N\u2212S) log(1 \u2212\u03b8), (8.6)\nwhich emphasizes the role of Sin defining the log-likelihood function. We plot the surface\nofL(\u03b8|S) as a function of Sand\u03b8, assuming that N= 50. As shown on the left-hand side\nofFigure 8.3 , the surface L(\u03b8|S) has a saddle shape. Along one direction the function goes\nup, whereas along another direction the function goes down. In the middle of Figure 8.3 ,\nwe show a bird\u2019s-eye view of the surface, with the color-coding matched with the surface\nplot. As you can see, when plotted as a function of \u03b8andx(in our case, we use a summary\nstatistic S=PN\nn=1xn), the two-dimensional plot tells us how the log-likelihood function\nchanges when Schanges. On the right-hand side of Figure 8.3 , we show two particular\ncross sections of the two-dimensional plot. One cross section is taken from S= 25 and the\nother cross section is taken from S= 12. Since the total number of heads in this numerical\nexperiment is assumed to be N= 50, the first cross section at S= 25 is obtained when\n471", "487": "CHAPTER 8. ESTIMATION\nFigure 8.3: We plot the log-likelihood function as a function of S=PN\nn=1xnand\u03b8. [Left] We show\nthe surface plot of L(\u03b8|S) =Slog\u03b8+ (N\u2212S) log(1 \u2212\u03b8). Note that the surface has a saddle shape.\n[Middle] By taking a bird\u2019s-eye view of the surface plot, we obtain a 2-dimensional contour plot of the\nsurface, where the color code matches the height of the log-likelihood function. [Right] We take two\ncross sections along S= 25 andS= 12 . Observe how the shape changes.\nhalf of the Bernoulli measurements are \u201c1\u201d, whereas the second cross section at S= 12 is\nobtained when a quarter of the Bernoulli measurements are \u201c1\u201d.\nThe cross sections tell us the log-likelihood function log L(\u03b8|S) is a function defined\nspecifically for a given measurement x. As you can see from Figure 8.3 , the log-likelihood\nfunction changes when Schanges. Therefore, if our goal is to \u201cfind a \u03b8that maximizes the\nlog-likelihood function\u201d, then for a different xwe will have a different answer. For example,\naccording to Figure 8.3 , the maximum for log L(\u03b8|S= 25) occurs when \u03b8\u22480.5, and the\nmaximum for log L(\u03b8|S= 12) occurs when \u03b8\u22480.24. These are the maximum-likelihood\nestimates for the respective measurements.\nWe use the following MATLAB code to generate the surface plot:\n% MATLAB code to generate the surface plot\nN = 50;\nS = 1:N;\ntheta = linspace(0.1,0.9,100);\n[S_grid, theta_grid] = meshgrid(S, theta);\nL = S_grid.*log(theta_grid) + (N-S_grid).*log(1-theta_grid);\ns = surf(S,theta,L);\ns.LineStyle = \u2019-\u2019;\ncolormap jet\nview(65,15)\nFor the bird\u2019s-eye view plot, we replace surf with imagesc(S,theta,L) . For the cross\nsection plots, we call the commands plot(theta, L(:,12)) andplot(theta, L(:,25)) .\n8.1.2 Maximum-likelihood estimate\nThe likelihood is the PDF of Xbut viewed as a function of \u03b8. The optimization problem\nof maximizing L(\u03b8|x) is called the maximum-likelihood (ML) estimation:\n472", "488": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nDefinition 8.4. LetL(\u03b8)be the likelihood function of the parameter \u03b8given the\nmeasurements x= [x1, . . . , x N]T. The maximum-likelihood estimate of the parameter\n\u03b8is a parameter that maximizes the likelihood:\nb\u03b8MLdef=argmax\n\u03b8L(\u03b8|x). (8.7)\nExample 8.4 . Find the ML estimate for a set of i.i.d. Bernoulli random variables\n{X1, . . . , X N}with Xn\u223cBernoulli( \u03b8) for n= 1, . . . , N .\nSolution . We know that the log-likelihood function of a set of i.i.d. Bernoulli random\nvariables is given by\nlogL(\u03b8|x) = NX\nn=1xn!\n\u00b7log\u03b8+ \nN\u2212NX\nn=1xn!\n\u00b7log(1\u2212\u03b8). (8.8)\nThus, to find the ML estimate, we need to solve the optimization problem\nb\u03b8ML= argmax\n\u03b8( NX\nn=1xn!\n\u00b7log\u03b8+ \nN\u2212NX\nn=1xn!\n\u00b7log(1\u2212\u03b8))\n.\nTaking the derivative with respect to \u03b8and setting it to zero, we obtain\nd\nd\u03b8( NX\nn=1xn!\n\u00b7log\u03b8+ \nN\u2212NX\nn=1xn!\n\u00b7log(1\u2212\u03b8))\n= 0.\nThis gives us\n\u0010PN\nn=1xn\u0011\n\u03b8\u2212N\u2212PN\nn=1xn\n1\u2212\u03b8= 0.\nRearranging the terms yields\nb\u03b8ML=1\nNNX\nn=1xn.\nLet\u2019s do a sanity check to see if this result makes sense. The solution to this problem\nsays that b\u03b8MLis the empirical average of the measurements. Assume that N= 50. Let us\nconsider two particular scenarios as illustrated in Figure 8.4 .\n\u0088Scenario 1 :xis a vector of measurements such that Sdef=PN\nn=1xn= 25. Since\nN= 50, the formula tells us that b\u03b8ML=25\n50= 0.5. This is the bestguess based on the\n50 measurements where 25 are heads. If you look at Figure 8.3 andFigure 8.4 , when\nS= 25, we are looking at a particular cross section in the 2D plot. The likelihood\nfunction we are inspecting is L(\u03b8|S= 25). For this likelihood function, the maximum\noccurs at \u03b8= 0.5.\n473", "489": "CHAPTER 8. ESTIMATION\nFigure 8.4: Illustration of how the maximum-likelihood estimate of a set of i.i.d. Bernoulli random\nvariables is determined. The subfigures above show two particular scenarios at S= 25 andS= 12 ,\nassuming that N= 50 . When S= 25 , the likelihood function has a quadratic shape centered at\n\u03b8= 0.5. This point is also the peak of the likelihood function when S= 25 . Therefore, the ML estimate\nisb\u03b8ML= 0.5. The second case is when S= 12 . The quadratic likelihood is shifted toward the left. The\nML estimate is b\u03b8ML= 0.24.\n\u0088Scenario 2 :xis a vector of measurements such that Sdef=PN\nn=1xn= 12. The formula\ntells us that b\u03b8ML=12\n50= 0.24. This is again the bestguess based on the 50 measure-\nments where 12 are heads. Referring to Figure 8.3 andFigure 8.4 , we can see that\nthe likelihood function corresponds to another cross section L(\u03b8|S= 12) where the\nmaximum occurs at \u03b8= 0.24.\nAt this point, you may wonder why the shape of the likelihood function L(\u03b8|x) changes\nso radically as xchanges? The answer can be found in Figure 8.5 . Imagine that we have\nN= 50 measurements of which S= 40 give us heads. If these i.i.d. Bernoulli random\nvariables have a parameter \u03b8= 0.5, it is quite unlikely that we will get 40 out of 50\nmeasurements to be heads. (If it were \u03b8= 0.5, we should get more or less 25 out of 50\nheads.) When S= 40, and without any additional information about the experiment, the\nmost logical guess is that the Bernoulli random variables have a parameter \u03b8= 0.8. Since\nthe measurement Scan be as extreme as 0 out of 50 or 50 out of 50, the likelihood function\nL(\u03b8|x) has to reflect these extreme cases. Therefore, as we change x, we observe a big\nchange in the shape of the likelihood function.\nAs you can see from Figure 8.5 ,S= 40 corresponds to the marked vertical cross\nsection. As we determine the maximum-likelihood estimate, we search among all the possi-\nbilities, such as \u03b8= 0.2,\u03b8= 0.5,\u03b8= 0.8, etc. These possibilities correspond to the horizontal\nlines we drew in the figure. Among those horizontal lines, it is clear that the best estimate\noccurs when \u03b8= 0.8, which is also the ML estimate.\n474", "490": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nFigure 8.5: Suppose that we have a set of measurements such that S= 40 . To determine the ML\nestimate, we look at the vertical cross section at S= 40 . Among the different candidate parameters,\ne.g.,\u03b8= 0.2,\u03b8= 0.5and\u03b8= 0.8, we pick the one that has the maximum response to the likelihood\nfunction. For S= 40 , it is more likely that the underlying parameter is \u03b8= 0.8than \u03b8= 0.2or\u03b8= 0.5.\nVisualizing ML estimation as Ngrows\nMaximum-likelihood estimation can also be understood directly from the PDF instead of\nthe likelihood function. To explain this perspective, let\u2019s do a quick exercise.\nPractice Exercise 8.2 . Suppose that Xnis a Gaussian random variable. Assume\nthat\u03c3= 1 is known but the mean \u03b8is unknown. Find the ML estimate of the mean.\nSolution . The ML estimate b\u03b8MLis\nb\u03b8ML= argmax\n\u03b8logL(\u03b8|x)\n= argmax\n\u03b8log(NY\nn=11\u221a\n2\u03c0exp\u001a\n\u2212(xn\u2212\u03b8)2\n2\u001b)\n= argmax\n\u03b8\u2212N\n2log(2\u03c0)\u22121\n2NX\nn=1(xn\u2212\u03b8)2.\nTaking the derivative with respect to \u03b8, we obtain\nd\nd\u03b8(\n\u2212N\n2log(2\u03c0)\u22121\n2NX\nn=1(xn\u2212\u03b8)2)\n= 0.\nThis gives usPN\nn=1(xn\u2212\u03b8) = 0. Therefore, the ML estimate is\nb\u03b8ML=1\nNNX\nn=1xn.\nNow we will draw the PDF and compare it with the measured data points. Our focus\n475", "491": "CHAPTER 8. ESTIMATION\nis to analyze how the ML estimate changes as Ngrows.\nWhen N= 1.There is only one observation x1. The best Gaussian that fits this sample\nmust be the one that is centered at x1. In fact, the optimization is1\nb\u03b8ML= argmax\n\u03b8logL(\u03b8|x1) = argmax\n\u03b8log\u001a1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x1\u2212\u03b8)2\n2\u03c32\u001b\u001b\n= argmax\n\u03b8\u2212(x1\u2212\u03b8)2=x1.\nTherefore, the ML estimate is b\u03b8ML=x1.Figure 8.6 illustrates this case. As we conduct the\nML estimation, we imagine that there are a few candidate PDFs. The ML estimation says\nthat among all these candidate PDFs we need to find one that can maximize the probability\nof obtaining the observation x1. Since we only have one observation, we have no choice but\nto pick a Gaussian centered at x1. Certainly the sample X1=x1could be bad, and we may\nfind a wrong Gaussian. However, with only one sample there is no way for us to make better\ndecisions.\n-5 -4 -3 -2 -1 0 1 2 3 4 5\nx00.050.10.150.20.250.30.350.40.450.5\nData Point\nCandidate PDF\nEstimated PDF\nFigure 8.6: N= 1. Suppose that we are given one observed data point located around x=\u22122.1. To\nconduct the ML estimation we propose a few candidate PDFs, each being a Gaussian with unit variance\nbut a different mean \u03b8. The ML estimate is a parameter \u03b8such that the corresponding PDF matches\nthe best with the observed data. In this example the best match happens when the estimated Gaussian\nPDF is centered at x1.\nWhen N= 2.In this case we need to find a Gaussian that fits both x1andx2. The\nprobability of simultaneously observing x1andx2is determined by the joint distribution.\nBy independence we then have\nb\u03b8ML= argmax\n\u03b8log(\u00121\u221a\n2\u03c0\u03c32\u00132\nexp\u001a\n\u2212(x1\u2212\u03b8)2+ (x2\u2212\u03b8)2)\n2\u03c32\u001b)\n= argmax\n\u03b8\u001a\n\u2212(x1\u2212\u03b8)2+ (x2\u2212\u03b8)2\n2\u03c32\u001b\n=x1+x2\n2,\n1We skip the step of checking whether the stationary point is a maximum or a minimum, which can be\ndone by evaluating the second-order derivative. In fact, since the function \u2212(x1\u2212\u03b8)2is concave in \u03b8, a\nstationary point must be a maximum.\n476", "492": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nwhere the last step is obtained by taking the derivative:\nd\nd\u03b8\b\n(x1\u2212\u03b8)2+ (x2\u2212\u03b8)2\t\n= 2(x1\u2212\u03b8) + 2( x2\u2212\u03b8).\nEquating this with zero yields the solution \u03b8=x1+x2\n2. Therefore, the best Gaussian that\nfits the observations is Gaussian(x1+x2\n2, \u03c32).\n-5 -4 -3 -2 -1 0 1 2 3 4 5\nx00.050.10.150.20.250.30.350.40.450.5\nData Point\nCandidate PDF\nEstimated PDF\nFigure 8.7: N= 2. Suppose that we are given two observed data points located around x1=\u22120.98\nandx2=\u22121.15. To conduct the ML estimation we propose a few candidate PDFs, each being a\nGaussian with unit variance but a different mean \u03b8. The ML estimate is a parameter \u03b8such that the\ncorresponding PDF best matches the observed data. In this example the best match happens when the\nestimated Gaussian PDF is centered at (x1+x2)/2\u2248 \u22121.07.\nDoes this result make sense? When you have two data points x1andx2, the ML\nestimation is trying to find a Gaussian that can best fit both of these two data points.\nYour best bet here is b\u03b8ML= (x1+x2)/2, because there are no other choices. If you choose\nb\u03b8ML=x1orb\u03b8ML=x2, it cannot be a good estimate because you are not using both data\npoints. As shown in Figure 8.7 , for these two observed data points x1andx2, the PDF\nmarked in red (which is a Gaussian centered at ( x1+x2)/2) is indeed the best fit.\nWhen N= 10 andN= 100 .We can continue the above calculation for N= 10 and\nN= 100. In this case the MLE is\nb\u03b8ML= argmax\n\u03b8log(\u00121\u221a\n2\u03c0\u03c32\u0013N\nexp\u001a\n\u2212(x1\u2212\u03b8)2+\u00b7\u00b7\u00b7+ (xN\u2212\u03b8)2\n2\u03c32\u001b)\n= argmax\n\u03b8\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32=1\nNNX\nn=1xn.\nwhere the optimization is solved by taking the derivative:\nd\nd\u03b8NX\nn=1(xn\u2212\u03b8)2=\u22122NX\nn=1(xn\u2212\u03b8)\nEquating this with zero yields the solution \u03b8=1\nNPN\nn=1xn.\nThe result suggests that for an arbitrary number of training samples the ML estimate\nis the sample average. These cases are illustrated in Figure 8.8 . As you can see, the red\ncurves (the estimated PDF) are always trying to fit as many data points as possible.\nThe above experiment tells us something about the ML estimation:\n477", "493": "CHAPTER 8. ESTIMATION\n-5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.40.450.5\nData Point\nCandidate PDF\nEstimated PDF\n-5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.40.450.5\nData Point\nCandidate PDF\nEstimated PDF\n(c)N= 10 (d) N= 100\nFigure 8.8: When N= 10 andN= 100 , the ML estimation continues to evaluate the different\ncandidate PDFs. For a given set of data points, the ML estimation picks the best PDF to fit the data\npoints. In this Gaussian example it was shown that the optimal parameter is b\u03b8ML= (1/N)PN\nn=1xn,\nwhich is the sample average.\nHow does ML estimation work, intuitively?\n\u0088The likelihood function L(\u03b8|x) measures how \u201clikely\u201d it is that we will get xif\nthe underlying parameter is \u03b8.\n\u0088In the case of a Gaussian with an unknown mean, you move around the Gaussian\nuntil you find a good fit.\n8.1.3 Application 1: Social network analysis\nML estimation has extremely broad applicability. In this subsection and the next we discuss\ntwo real examples. We start with an example in social network analysis.\nIn Chapter 3, when we discussed the Bernoulli random variables, we introduced the\nErd\u02dd os-R\u00b4 enyi graph \u2014 one of the simplest models for social networks. The Erd\u02dd os-R\u00b4 enyi graph\nis a single-membership network that assumes that all users belong to the same cluster. Thus\nthe connectivity between users is specified by a single parameter, which is also the probability\nof the Bernoulli random variable.\nIn our discussions in Chapter 3 we defined an adjacency matrix to represent a graph.\nThe adjacency matrix is a binary matrix, with the ( i, j)th entry indicating an edge connect-\ning nodes iandj. Since the presence and absence of an edge is binary and random, we may\nmodel each element of the adjacency matrix as a Bernoulli random variable\nXij\u223cBernoulli( p).\nIn other words, the edge Xijlinking user iand user jin the network is either Xij= 1 with\nprobability p, orXij= 0 with probability 1 \u2212p. In terms of notation, we define the matrix\nX\u2208RN\u00d7Nas the adjacency matrix, with the ( i, j)th element being Xij.\nA few examples of a single-membership Erd\u02dd os-R\u00b4 enyi graph are shown in Figure 8.9 . As\nthe figure shows, the network connectivity increases as the Bernoulli parameter pincreases.\nThis happens because pdefines the \u201cdensity\u201d of the edges. If pis large, we have a greater\nchance of getting Xij= 1, and so there is a higher probability that an edge is present\nbetween node iand node j. Ifpis small, the probability is lower.\n478", "494": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\n-2 0 2-3-2-1012p = 0.3\n  1\n  2  3\n  4\n  5  6  7\n  8  9  10\n  11  12\n  13\n  14  15  16  17  18  19  20\n  21  22  23  24\n  25  26  27\n  28  29\n  30  31\n  32  33\n  34  35\n  36\n  37\n  38  39\n  40\n-2 0 2-4-3-2-10123p = 0.5\n  1\n  2  3  4\n  5  6  7\n  8\n  9\n  10  11\n  12  13  14  15\n  16  17\n  18   19  20\n  21\n  22\n  23  24  25\n  26  27  28\n  29  30\n  31  32  33\n  34\n  35\n  36\n  37  38\n  39  40\n-4 -2 0 2 4-4-3-2-10123p = 0.7\n  1\n  2\n  3  4\n  5  6\n  7\n  8  9  10\n  11  12\n  13  14\n  15  16  17\n  18\n  19  20  21\n  22  23\n  24\n  25  26  27  28\n  29  30  31\n  32  33\n  34  35  36\n  37  38  39\n  40\n-4 -2 0 2 4-4-2024p = 0.9\n  1\n  2  3\n  4  5\n  6  7\n  8\n  9  10\n  11  12  13\n  14  15\n  16\n  17  18\n  19  20\n  21  22\n  23  24\n  25\n  26  27  28\n  29\n  30  31  32  33  34  35\n  36\n  37  38\n  39  40\n(a) Graph representations of Erd\u02dd os-R\u00b4 enyi graphs at different p.\n(b) Adjacent matrices of the corresponding graphs.\nFigure 8.9: A single-membership Erd\u02dd os-R\u00b4 enyi graph is a graph structure in which the edge between\nnode iand node jis defined as a Bernoulli random variable with parameter p. Aspincreases, the graph\nhas a higher probability of having more edges. The adjacent matrices shown in the bottom row are the\nmathematical representations of the graphs.\nSuppose that we are given onesnapshot of the network, i.e., one realization x\u2208RN\u00d7N\nof the adjacency matrix X\u2208RN\u00d7N. The problem of recovering the latent parameter pcan\nbe formulated as an ML estimation.\nExample 8.5 . Write down the log-likelihood function of the single-membership Erd\u02dd os-\nR\u00b4 enyi graph ML estimation problem.\nSolution . Based on the definition of the graph model, we know that\nXij\u223cBernoulli( p).\nTherefore, the probability mass function of Xijis\nP[Xij= 1] = p and P[Xij= 0] = 1 \u2212p.\nThis can be compactly expressed as\nfX(x;p) =NY\ni=1NY\nj=1pxij(1\u2212p)1\u2212xij.\nHence, the log-likelihood is\nlogL(p|x) =NX\ni=1NX\nj=1{xijlogp+ (1\u2212xij) log(1 \u2212p)}.\n479", "495": "CHAPTER 8. ESTIMATION\nNow that we have the log-likelihood function, we can proceed to estimate the param-\neterp. The solution to this is the ML estimate.\nPractice Exercise 8.3 . Solve the ML estimation problem:\nbpML= argmax\nplogL(p|x).\nSolution . Using the log-likelihood we just derived, we have that\nbpML=NX\ni=1NX\nj=1{xijlogp+ (1\u2212xij) log(1 \u2212p)}.\nTaking the derivative and setting it to zero,\nd\ndplogL(p|x) =d\ndp\uf8f1\n\uf8f2\n\uf8f3NX\ni=1NX\nj=1{xijlogp+ (1\u2212xij) log(1 \u2212p)}\uf8fc\n\uf8fd\n\uf8fe\n=NX\ni=1NX\nj=1\u001axij\np\u22121\u2212xij\n1\u2212p\u001b\n= 0.\nLetS=PN\ni=1PN\nj=1xij. The equation above then becomes\nS\np\u2212N2\u2212S\n1\u2212p= 0.\nRearranging the terms yields (1 \u2212p)S=p(N2\u2212S), which gives us\nbpML=S\nN2=1\nN2NX\ni=1NX\nj=1xij. (8.9)\nOn computers, visualizing the graphs and computing the ML estimates are reasonably\nstraightforward. In MATLAB, you can call the command graph to build a graph from the\nadjacency matrix A. This will allow you to plot the graph. The computation, however, is done\ndirectly by the adjacency matrix. In the code below, you can see that we call rand to generate\nthe Bernoulli random variables. The command triu extracts the upper triangular matrix\nfrom the matrix A. This ensures that we do not pick the diagonals. The symmetrization of\nA+A\u2019 ensures that the graph is indirectional, meaning that itojis the same as jtoi.\n% MATLAB code to visualize a graph\nn = 40; # Number of nodes\np = 0.3 # probability\nA = rand(n,n)<p;\nA = triu(A,1);\nA = A+A\u2019; # Adj matrix\nG = graph(A); # Graph\n480", "496": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nplot(G); # Drawing\np_ML = mean(A(:)); # ML estimate\nIn Python, the computation is done similarly with the help of the networkx library.\nThe number of edges mis defined as m=pn2\n2. This is because for a graph with nnodes, there\nare at mostn2\n2unique pairs of indirected edges. Multiplying this number by the probability\npwill give us the number of edges m.\n# Python code to visualize a graph\nimport networkx as nx\nimport numpy as np\nn = 40 # Number of nodes\np = 0.3 # probability\nm = np.round(((n ** 2)/2)*p) # Number of edges\nG = nx.gnm_random_graph(n,m) # Graph\nA = nx.adjacency_matrix(G) # Adj matrix\nnx.draw(G) # Drawing\np_ML = np.mean(A) # ML estimate\nAs you can see in both the MATLAB and the Python code, the ML estimate bpMLis de-\ntermined by taking the sample average. Thus the ML estimate, according to our calculation,\nisbpML=1\nN2PN\ni=1PN\nj=1xij.\n8.1.4 Application 2: Reconstructing images\nBeing able to see in the dark is the holy grail of imaging. Many advanced sensing technologies\nhave been developed over the past decade. In this example, we consider a single-photon image\nsensor. This is a counting device that counts the number of photons arriving at the sensor.\nPhysicists have shown that a Poisson process can model the arrival of the photons. For\nsimplicity we assume a homogeneous pattern of Npixels. The underlying intensity of the\nhomogeneous pattern is a constant \u03bb.\nSuppose that we have a sensor with Npixels X1, . . . , X N. According to the Poisson\nstatistics, the probability of observing a pixel value is determined by the Poisson probability:\nXn\u223cPoisson( \u03bb), n = 1, . . . , N,\nor more explicitly,\nP[Xn=xn] =\u03bbxn\nxn!e\u2212\u03bb,\nwhere xnis the nth observed pixel value, and is an integer.\nA single-photon image sensor is slightly more complicated in the sense that it does\nnot report Xnbut instead reports a truncated version of Xn. Depending on the number of\nincoming photons, the sensor reports\nYn=(\n1, X n\u22651,\n0, X n= 0.(8.10)\nWe call this type of sensors a one-bit single-photon image sensor (see Figure 8.10 ). Our\nquestion is: If we are given the measurements X1, . . . , X N, can we estimate the underlying\nparameter \u03bb?\n481", "497": "CHAPTER 8. ESTIMATION\nFigure 8.10: A one-bit single-photon image sensor captures an image with binary bits: It reports a \u201c1\u201d\nwhen the number of photons exceeds certain threshold, and \u201c0\u201d otherwise. The recovery problem here\nis to estimate the underlying image from the measurements.\nExample 8.6 . Derive the log-likelihood function of the estimation problem for the\nsingle-photon image sensors.\nSolution . Since Ynis a binary random variable, its probability is completely specified\nby the two states it takes:\nP[Yn= 0] = P[Xn= 0] = e\u2212\u03bb\nP[Yn= 1] = P[Xn\u0338= 0] = 1 \u2212e\u2212\u03bb.\nThus, Ynis a Bernoulli random variable with probability 1 \u2212e\u2212\u03bbof getting a value\nof 1, and probability e\u2212\u03bbof getting a value of 0. By defining ynas a binary number\ntaking values of either 0 or 1, it follows that the log-likelihood is\nlogL(\u03bb|y) = log\u001aNY\nn=1\u0000\n1\u2212e\u2212\u03bb\u0001yn\u0000\ne\u2212\u03bb\u00011\u2212yn\u001b\n=NX\nn=1\u001a\nynlog(1\u2212e\u2212\u03bb)\u2212\u03bb(1\u2212yn)\u001b\n.\nPractice Exercise 8.4 . Solve the ML estimation problem\nb\u03bbML= argmax\n\u03bblogL(\u03bb|y). (8.11)\nSolution . First, we define S=PN\nn=1yn. This simplifies the log-likelihood function to\nlogL(\u03bb|y) =NX\nn=1\u001a\nynlog(1\u2212e\u2212\u03bb)\u2212\u03bb(1\u2212yn)\u001b\n=Slog(1\u2212e\u2212\u03bb)\u2212\u03bb(N\u2212S).\n482", "498": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nThe ML estimation is\nb\u03bbML= argmax\n\u03bbSlog(1\u2212e\u2212\u03bb)\u2212\u03bb(N\u2212S).\nTaking the derivative w.r.t. \u03bbyields\nd\nd\u03bb\u001a\nSlog(1\u2212e\u2212\u03bb)\u2212\u03bb(N\u2212S)\u001b\n=S\n1\u2212e\u2212\u03bbe\u2212\u03bb\u2212(N\u2212S).\nMoving around the terms, it follows that\nS\n1\u2212e\u2212\u03bbe\u2212\u03bb\u2212(N\u2212S) = 0 = \u21d2 \u03bb=\u2212log\u0012\n1\u2212S\nN\u0013\n.\nTherefore, the ML estimate is\nb\u03bbML=\u2212log \n1\u22121\nNNX\nn=1yn!\n. (8.12)\nFor real images, you can extrapolate the idea from yntoyi,j,t, which denotes the ( i, j)th\npixel located at time t. Defining yt\u2208RN\u00d7Nas the tth frame of the observed data, we can\nuseTframes to recover one image b\u03bbML\u2208RN\u00d7N. It follows from the above derivation that\nthe ML estimate is\nb\u03bbML=\u2212log \n1\u22121\nTTX\nt=1yt!\n. (8.13)\nFigure 8.11 shows a pair of input-output images of a 256 \u00d7256 image.\n(a) Observed data (1-frame) (b) ML estimate (using 100 frames)\nFigure 8.11: ML estimation for a single-photon image sensor problem. The observed data consists of\n100 frames of binary measurements y1, . . . ,yT, where T= 100 . The ML estimate is constructed by\n\u03bb=\u2212log(1\u22121\nTPT\nt=1yt).\nOn a computer the ML estimation can be done in a few lines of MATLAB code. The\ncode in Python requires more work, as it needs to read images using the openCV library.\n483", "499": "CHAPTER 8. ESTIMATION\n% MATLAB code to recover an image from binary measurements\nlambda = im2double(imread(\u2019cameraman.tif\u2019));\nT = 100; % 100 frames\nx = poissrnd( repmat(lambda, [1,1,T]) ); % generate Poisson r.v.\ny = (x>=1); % binary truncation\nlambdahat = -log(1-mean(y,3)); % ML estimation\nfigure(1); imshow(x(:,:,1));\nfigure(2); imshow(lambdahat);\n# Python code to recover an image from binary measurements\nimport cv2\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nlambd = cv2.imread(\u2019./cameraman.tif\u2019) # read image\nlambd = cv2.cvtColor(lambd, cv2.COLOR_BGR2GRAY)/255 # gray scale\nT = 100\nlambdT = np.repeat(lambd[:, :, np.newaxis], T, axis=2) # repeat image\nx = stats.poisson.rvs(lambdT) # Poisson statistics\ny = (x>=1).astype(float) # binary truncation\nlambdhat = -np.log(1-np.mean(y,axis=2)) # ML estimation\nplt.imshow(lambdhat,cmap=\u2019gray\u2019)\n8.1.5 More examples of ML estimation\nBy now you should be familiar with the procedure for solving the ML estimation problem.\nWe summarize the two steps as follows.\nHow to solve an ML estimation problem\n\u0088Write down the likelihood L(\u03b8|x).\n\u0088Maximize the likelihood by solving b\u03b8ML= argmax\n\u03b8logL(\u03b8|x).\nPractice Exercise 8.5 (Gaussian ). Suppose that we are given a set of i.i.d. Gaus-\nsian random variables X1, . . . , X N, where both the mean \u00b5and the variance \u03c32are\nunknown. Let \u03b8= [\u00b5, \u03c32]Tbe the parameter. Find the ML estimate of \u03b8.\nSolution . We first write down the likelihood. The likelihood of these i.i.d. Gaussian\nrandom variables is\nL(\u03b8|x) =\u00121\u221a\n2\u03c0\u03c32\u0013N\nexp(\n\u22121\n2\u03c32NX\nn=1(xn\u2212\u00b5)2)\n.\n484", "500": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nTo solve the ML estimation problem, we maximize the log-likelihood:\nb\u03b8MLdef= argmax\n\u03b8L(\u03b8|x)\n= argmax\n\u00b5,\u03c32(\n\u2212N\n2log(2\u03c0\u03c32)\u22121\n2\u03c32NX\nn=1(xn\u2212\u00b5)2)\n.\nSince we have two parameters, we need to take the derivatives for both.\nd\nd\u00b5(\n\u2212N\n2log(2\u03c0\u03c32)\u22121\n2\u03c32NX\nn=1(xn\u2212\u00b5)2)\n= 0,\nd\nd\u03c32(\n\u2212N\n2log(2\u03c0\u03c32)\u22121\n2\u03c32NX\nn=1(xn\u2212\u00b5)2)\n= 0.\n(Note that the derivative of the second equation is taken w.r.t. to \u03c32and not \u03c3.) This\npair of equations gives us\n1\n\u03c32NX\nn=1(xn\u2212\u00b5) = 0 ,and\u2212N\n2\u00b71\n2\u03c0\u03c32\u00b7(2\u03c0) +1\n2\u03c34NX\nn=1(xn\u2212\u00b5)2= 0.\nRearranging the equations, we find that\nb\u00b5ML=1\nNNX\nn=1xn and b\u03c32\nML=1\nNNX\nn=1(xn\u2212b\u00b5ML)2. (8.14)\nPractice Exercise 8.6 . (Poisson ) Given a set of i.i.d. Poisson random variables\nX1, . . . , X Nwith an unknown parameter \u03bb, find the ML estimate of \u03bb.\nSolution . For a Poisson random variable, the likelihood function is\nL(\u03bb|x) =NY\nn=1\u001a\u03bbxn\nxn!e\u2212\u03bb\u001b\n. (8.15)\nTo solve the ML estimation problem, we note that\nb\u03bbML= argmax\n\u03bbL(\u03bb|x) = argmax\n\u03bblog(NY\nn=1\u03bbxn\nxn!e\u2212\u03bb)\n= argmax\n\u03bblog\u001a\u03bbP\nnxn\nQ\nnxn!e\u2212N\u03bb\u001b\n.\nSinceQ\nnxn! is independent of \u03bb, its presence or absence will not affect the optimization\n485", "501": "CHAPTER 8. ESTIMATION\nproblem. Consequently we can drop the term. It follows that\nb\u03bbML= argmax\n\u03bblogn\n\u03bbP\nnxne\u2212N\u03bbo\n= argmax\n\u03bb X\nnxn!\nlog\u03bb\u2212N\u03bb.\nTaking the derivative and setting it to zero yields\nd\nd\u03bb( X\nnxn!\nlog\u03bb\u2212N\u03bb)\n=P\nnxn\n\u03bb\u2212N= 0.\nRearranging the terms yields\nb\u03bbML=1\nNNX\nn=1xn. (8.16)\nThe idea of ML estimation can also be extended to vector observations.\nExample 8.7 . (High-dimensional Gaussian ) Suppose that we are given a set of i.i.d.\nd-dimensional Gaussian random vectors X1, . . . ,XNsuch that\nXn\u223cGaussian( \u00b5,\u03a3).\nWe assume that \u03a3is fixed and known, but \u00b5is unknown. Find the ML estimate of \u00b5.\nSolution . The likelihood function is\nL(\u00b5|{xn}N\nn=1) =NY\nn=1fXn(xn;\u00b5)\n=NY\nn=1(\n1p\n(2\u03c0)d|\u03a3|exp\u001a\n\u22121\n2(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5)\u001b)\n= \n1p\n(2\u03c0)d|\u03a3|!N\nexp(\n\u22121\n2NX\nn=1(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5))\n.\nThus the log-likelihood function is\nlogL(\u00b5|{xn}N\nn=1) =N\n2log|\u03a3|+N\n2log(2\u03c0)d+NX\nn=1\u001a1\n2(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5)\u001b\n.\nThe ML estimate is found by maximizing this log-likelihood function:\nb\u00b5ML= argmax\n\u00b5logL(\u00b5|{xn}N\nn=1).\n486", "502": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nTaking the gradient of the function and setting it to zero, we have that\nd\nd\u00b5(\nN\n2log|\u03a3|+N\n2log(2\u03c0)d+NX\nn=1\u001a1\n2(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5)\u001b)\n= 0.\nThe derivatives of the first two terms are zero because they do not depend on \u00b5). Thus\nwe have that:\nNX\nn=1\u001a\n\u03a3\u22121(xn\u2212\u00b5)\u001b\n= 0.\nRearranging the terms yields the ML estimate b\u00b5ML=1\nNPN\nn=1xn.\nExample 8.8 . (High-dimensional Gaussian ) Assume the same problem setting as in\nExample 8.7, except that this time we assume that both the mean vector \u00b5and the\ncovariance matrix \u03a3are unknown. Find the ML estimate for \u03b8= (\u00b5,\u03a3).\nSolution . The log-likelihood follows from Example 8.7:\nlogL(\u00b5|{xn}N\nn=1) =N\n2log|\u03a3|+N\n2log(2\u03c0)d+NX\nn=1\u001a1\n2(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5)\u001b\n.\nFinding the ML estimate requires taking the derivative with respect to both \u00b5and\u03a3:\nd\nd\u00b5(\nN\n2log|\u03a3|+N\n2log(2\u03c0)d+NX\nn=1\u001a1\n2(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5)\u001b)\n= 0,\nd\nd\u03a3(\nN\n2log|\u03a3|+N\n2log(2\u03c0)d+NX\nn=1\u001a1\n2(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5)\u001b)\n= 0.\nAfter some tedious algebraic steps (see Duda et al., Pattern Classification , Problem\n3.14), we have that\nb\u00b5ML=1\nNNX\nn=1xn, (8.17)\nb\u03a3ML=1\nNNX\nn=1(xn\u2212b\u00b5ML)(xn\u2212b\u00b5ML)T. (8.18)\n8.1.6 Regression versus ML estimation\nML estimation is closely related to regression. To understand the connection, we consider a\nlinear model that we studied in Chapter 7. This model describes the relationship between\n487", "503": "CHAPTER 8. ESTIMATION\nthe inputs x1, . . . ,xNand the observed outputs y1, . . . , y N, via the equation\nyn=d\u22121X\np=0\u03b8p\u03d5p(xn) +en, n = 1, . . . , N. (8.19)\nIn this expression, \u03d5p(\u00b7) is a transformation that extracts the \u201cfeatures\u201d of the input vector\nxto produce a scalar. The coefficient \u03b8pdefines the relative weight of the feature \u03d5p(xn) in\nconstructing the observed variable yn. The error endefines the modeling error between the\nobservation ynand the predictionPd\u22121\np=0\u03b8p\u03d5p(xn). We call this equation a linear model.\nExpressed in matrix form, the linear model is\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=y=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03d50(x1)\u03d51(x1)\u00b7\u00b7\u00b7 \u03d5d\u22121(x1)\n\u03d50(x2)\u03d51(x2)\u00b7\u00b7\u00b7 \u03d5d\u22121(x2)\n... \u00b7\u00b7\u00b7......\n\u03d50(xN)\u03d51(xN)\u00b7\u00b7\u00b7\u03d5d\u22121(xN)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\n=X\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b80\n\u03b81\n...\n\u03b8d\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=\u03b8+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=e,\nor more compactly as y=X\u03b8+e. Rearranging the terms, it is easy to show that\nNX\nn=1e2\nn=NX\nn=1 \nyn\u2212d\u22121X\np=0\u03b8p\u03d5p(xn)!2\n=NX\nn=1(yn\u2212[X\u03b8]n)2=\u2225y\u2212X\u03b8\u22252.\nNow we make an assumption : that each noise enis an i.i.d. copy of a Gaussian random\nvariable with zero mean and variance \u03c32. In other words, the error vector eis distributed\naccording to e\u223cGaussian( 0, \u03c32I). This assumption is not always true because there are\nmany situations in which the error is not Gaussian. However, this assumption is necessary\nfor us to make the connection between ML estimation and regression.\nWith this assumption, we ask, given the observations y1, . . . , y N, what would be the\nML estimate of the unknown parameter \u03b8? We answer this question in two steps.\nExample 8.9 . Find the likelihood function of \u03b8, given y= [y1, . . . , y N]T.\nSolution . The PDF of yis given by a Gaussian:\nfY(y;\u03b8) =NY\nn=1\u001a1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(yn\u2212[X\u03b8]n)2\n2\u03c32\u001b\u001b\n=1p\n(2\u03c0\u03c32)Nexp(\n\u22121\n2\u03c32NX\nn=1(yn\u2212[X\u03b8]n)2)\n=1p\n(2\u03c0\u03c32)Nexp\u001a\n\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252\u001b\n. (8.20)\n488", "504": "8.1. MAXIMUM-LIKELIHOOD ESTIMATION\nTherefore, the log-likelihood function is\nlogL(\u03b8|y) = log(\n1p\n(2\u03c0\u03c32)Nexp\u001a\n\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252\u001b)\n=\u2212N\n2log(2\u03c0\u03c32)\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252.\nThe next step is to solve the ML estimation by maximizing the log-likelihood.\nExample 8.10 . Solve the ML estimation problem stated in Example 8.9. Assume that\nXTXis invertible.\nSolution .\nb\u03b8ML= argmax\n\u03b8logL(\u03b8|y)\n= argmax\n\u03b8\u001a\n\u2212N\n2log(2\u03c0\u03c32)\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252\u001b\n.\nTaking the derivative w.r.t. \u03b8yields\nd\nd\u03b8\u001a\n\u2212N\n2log(2\u03c0\u03c32)\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252\u001b\n= 0.\nSinced\nd\u03b8\u03b8TA\u03b8=A+AT, it follows from the chain rule that\nd\nd\u03b8\u001a\n\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252\u001b\n=d\nd\u03b8\u001a\n\u22121\n2\u03c32(y\u2212X\u03b8)T(y\u2212X\u03b8)\u001b\n=1\n\u03c32XT(X\u03b8\u2212y).\nSubstituting this result into the equation,\n1\n\u03c32XT(X\u03b8\u2212y) = 0 .\nRearranging terms we obtain XTX\u03b8=XTy, of which the solution is\nb\u03b8ML= (XTX)\u22121XTy. (8.21)\nSince the ML estimate in Equation (8.21) is the same as the regression solution (see\nChapter 7), we conclude that the regression problem of a linear model is equivalent to solving\nan ML estimation problem.\nThe main difference between a linear regression problem and an ML estimation problem\nis the underlying statistical model, as illustrated in Figure 8.12 . In linear regression, you\ndo not care about the statistics of the noise term en. We choose ( \u00b7)2as the error because it\nis differentiable and convenient. In ML estimation, we choose ( \u00b7)2as the error because the\nnoise is Gaussian. If the noise is not Gaussian, e.g., the noise follows a Laplace distribution,\nwe need to choose | \u00b7 |as the error. Therefore, you can always get a result by solving the\nlinear regression. However, this result will only become meaningful if you provide additional\n489", "505": "CHAPTER 8. ESTIMATION\nFigure 8.12: ML estimation is equivalent to a linear regression when the underlying statistical model\nfor ML estimation is a Gaussian. Specifically, if the error term e=y\u2212X\u03b8is an independent Gaussian\nvector with zero mean and covariance matrix \u03c32I, then the resulting ML estimation is the same as linear\nregression. If the underlying statistical model is not Gaussian, then solving the regression is equivalent\nto applying a Gaussian ML estimation to a non-Gaussian problem. This will still give us a result, but\nthat result will not maximize the likelihood, and thus it will not have any statistical guarantee.\ninformation about the problem. For example, if you know that the noise is Gaussian, then\nthe regression solution is also the ML solution. This is a statistical guarantee.\nIn practice, of course, we do not know whether the noise is Gaussian or not. At this\npoint we have two courses of action: (i) Use your prior knowledge/domain expertise to\ndetermine whether a Gaussian assumption makes sense, or (ii) select an alternative model\nand see if the alternative model fits the data better. In practice, we should also question\nwhether maximizing the likelihood is what we want. We may have some knowledge and\ntherefore prefer the parameter \u03b8, e.g., we want a sparse solution so that \u03b8only contains a\nfew non-zeros. In that case, maximizing the likelihood without any constraint may not be\nthe solution we want.\nML estimation versus regression\n\u0088ML estimation requires a statistical assumption, whereas regression does not.\n\u0088Suppose that you use a linear model yn=Pd\u22121\np=0\u03b8p\u03d5p(xn) +enwhere en\u223c\nGaussian(0 , \u03c32), for n= 1, . . . , N .\n\u0088Then the likelihood function in the ML estimation is\nL(\u03b8|y) =1p\n(2\u03c0\u03c32)Nexp\u001a\n\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252\u001b\n,\n\u0088The ML estimate b\u03b8MLisb\u03b8ML= (XTX)\u22121XTy, which is exactly the same as\nthe regression solution. If the above statistical assumptions do not hold, then the\nregression solution will not maximize the likelihood.\n490", "506": "8.2. PROPERTIES OF ML ESTIMATES\n8.2 Properties of ML Estimates\nML estimation is a very special type of estimation. Not all estimations are ML. If an estimate\nis ML, are there any theoretical properties we can analyze? For example, will ML estimates\nguarantee the recovery of the true parameter? If so, when will this happen? In this section\nwe investigate these theoretical questions so that you will acquire a better understanding of\nthe statistical nature of ML estimates.2\n8.2.1 Estimators\nWe know that an ML estimate is defined as\nb\u03b8ML(x) = argmax\n\u03b8L(\u03b8|x). (8.22)\nWe write b\u03b8ML(x) to emphasize that b\u03b8MLis a function of x. The dependency of b\u03b8ML(x) on\nxshould not be a surprise. For example, if the ML estimate is the sample average, we have\nthat\nb\u03b8ML(x1, . . . , x N) =1\nNNX\nn=1xn,\nwhere x= [x1, . . . , x N]T.\nHowever, in this setting we should always remember that x1, . . . , x Nare realizations\nof the i.i.d. random variables X1, . . . , X N. Therefore, if we want to analzye the randomness\nof the variables, it is more reasonable to write b\u03b8MLas a random variable b\u0398ML. For example,\nin the case of sample average, we have that\nb\u0398ML(X1, . . . , X N) =1\nNNX\nn=1Xn. (8.23)\nWe call b\u0398MLthe ML estimator of the true parameter \u03b8.\nEstimate versus estimator\n\u0088Anestimate is anumber , e.g.,b\u03b8ML=1\nNNX\nn=1xn. It is the random realization of\na random variable.\n\u0088Anestimator is arandom variable , e.g., b\u0398ML=1\nNNX\nn=1Xn. It takes a set of\nrandom variables and generates another random variable.\n2For notational simplicity, in this section we will focus on a scalar parameter \u03b8instead of a vector\nparameter \u03b8.\n491", "507": "CHAPTER 8. ESTIMATION\nThe ML estimators are one type of estimator, namely those that maximize the likeli-\nhood functions. If we do not want to maximize the likelihood we can still define an estimator.\nAn estimator is any function that takes the data points X1, . . . , X Nand maps them to a\nnumber (or a vector of numbers). That is, an estimator is\nb\u0398(X1, . . . , X N).\nWe call b\u0398 the estimator of the true parameter \u03b8.\nExample 8.11 . Let X1, . . . , X Nbe Gaussian i.i.d. random variables with unknown\nmean \u03b8and known variance \u03c32. Construct two possible estimators.\nSolution . We define two estimators:\nb\u03981(X1, . . . , X N) =1\nNNX\nn=1Xn,\nb\u03982(X1, . . . , X N) =X1,\nIn the first case, the estimator takes all the samples and constructs the sample average.\nThe second estimator takes all the samples and returns on the first element. Both are\nlegitimate estimators. However, b\u03981is the ML estimator, whereas b\u03982is not.\n8.2.2 Unbiased estimators\nWhile you can define estimators in any way you like, certain estimators are good and others\nare bad. By \u201cgood\u201d we mean that the estimator can provide you with the information about\nthe true parameter \u03b8; otherwise, why would you even construct such an estimator? However,\nthe difficulty here is that b\u0398 is a random variable because it is constructed from X1, . . . , X N.\nTherefore, we need to define different metrics to quantify the usefulness of the estimators.\nDefinition 8.5. An estimator b\u0398isunbiased if\nE[b\u0398] = \u03b8. (8.24)\nUnbiasedness means that the average of the random variable b\u0398 matches the true\nparameter \u03b8. In other words, while we allow b\u0398 to fluctuate, we expect the average to match\nthe true \u03b8. If this is not the case, using more measurements will not help us get closer to \u03b8.\nExample 8.12 . Let X1, . . . , X Nbe i.i.d. Gaussian random variables with a unknown\nmean \u03b8. It has been shown that the ML estimator is\nb\u0398ML=1\nNNX\nn=1Xn. (8.25)\nIs the ML estimator b\u0398MLunbiased?\n492", "508": "8.2. PROPERTIES OF ML ESTIMATES\nSolution : To check the unbiasedness, we look at the expectation:\nE[b\u0398ML] =1\nNNX\nn=1E[Xn] =1\nNNX\nn=1\u03b8=\u03b8.\nThus,b\u0398ML=1\nNPN\nn=1Xnis an unbiased estimator of \u03b8.\nExample 8.13 . Same as the example before, but this time we consider an estimator\nb\u0398 =X1+X2+ 5. (8.26)\nIs this estimator unbiased?\nSolution : In this case,\nE[b\u0398] =E[X1+X2+ 5] = E[X1] +E[X2] + 5 = 2 \u03b8+ 5\u0338=\u03b8.\nTherefore, the estimator is biased.\nExample 8.14 . Let X1, . . . , X Nbe i.i.d. Gaussian random variables with unknown\nmean \u00b5and unknown variance \u03c32. We have shown that the ML estimators are\nb\u00b5ML=1\nNNX\nn=1Xn and b\u03c32\nML=1\nNNX\nn=1(Xn\u2212b\u00b5ML)2.\nIt is easy to show that E[b\u00b5ML] =\u00b5. How about b\u03c32\nML? Is it an unbiased estimator?\nSolution : For simplicity we assume \u00b5= 0 so that E[X2\nn] =E[(Xn\u22120)2] =\u03c32.\nNote that\nE[b\u03c32\nML] =1\nNNX\nn=1\u001a\nE[X2\nn]\u22122E[b\u00b5MLXn] +E[b\u00b52\nML]\u001b\n=1\nNNX\nn=1\uf8f1\n\uf8f2\n\uf8f3\u03c32\u22122E\uf8ee\n\uf8f01\nNNX\nj=1XjXn\uf8f9\n\uf8fb+E\uf8ee\n\uf8f0 \n1\nNNX\nn=1Xn!2\uf8f9\n\uf8fb\uf8fc\n\uf8fd\n\uf8fe.\nBy independence, we observe that E[XjXn] =E[Xj]E[Xn] = 0, for any j\u0338=n. There-\nfore,\nE\uf8ee\n\uf8f01\nNNX\nj=1XjXn\uf8f9\n\uf8fb=1\nNE\u0014\nX1Xn+\u00b7\u00b7\u00b7+XNXn\u0015\n=1\nN(0 +\u00b7\u00b7\u00b7+\u03c32+\u00b7\u00b7\u00b7+ 0) =\u03c32\nN.\n493", "509": "CHAPTER 8. ESTIMATION\nSimilarly, we have that\nE\uf8ee\n\uf8f0 \n1\nNNX\nn=1Xn!2\uf8f9\n\uf8fb=1\nN2NX\nn=1\uf8f1\n\uf8f2\n\uf8f3E[X2\nn] +X\nj\u0338=nE[XjXn]\uf8fc\n\uf8fd\n\uf8fe\n=1\nN2NX\nn=1n\n\u03c32+ 0o\n=\u03c32\nN.\nCombining everything, we arrive at the result:\nE[b\u03c32\nML] =1\nNNX\nn=1\uf8f1\n\uf8f2\n\uf8f3\u03c32\u22122E\uf8ee\n\uf8f01\nNNX\nj=1XjXn\uf8f9\n\uf8fb+E\uf8ee\n\uf8f0 \n1\nNNX\nn=1Xn!2\uf8f9\n\uf8fb\uf8fc\n\uf8fd\n\uf8fe\n=1\nNNX\nn=1\u001a\n\u03c32\u22122\u03c32\nN+\u03c32\nN\u001b\n=N\u22121\nN\u03c32,\nwhich is not equal to \u03c32. Therefore, b\u03c32\nMLis a biased estimator of \u03c32.\nIn the previous example, it is possible to construct an unbiased estimator for the\nvariance. To do so, we can use\nb\u03c32\nunbias =1\nN\u22121NX\nn=1(Xn\u2212b\u00b5ML)2, (8.27)\nso that E[b\u03c32\nunbias ] =\u03c32. However, note that b\u03c32\nunbias does not maximize the likelihood, so while\nyou can get unbiasedness, you cannot maximize the likelihood. If you want to maximize the\nlikelihood, you cannot get unbiasedness.\nWhat is an unbiased estimator?\n\u0088An estimator b\u0398 is unbiased if E[b\u0398] = \u03b8.\n\u0088Unbiased means that the statistical average of b\u0398 is the true parameter \u03b8.\n\u0088IfXn\u223cGaussian( \u03b8, \u03c32), then b\u0398 = (1 /N)PN\nn=1Xnis unbiased, but b\u0398 = X1is\nbiased.\n8.2.3 Consistent estimators\nBy definition, an estimator b\u0398(X1, . . . , X N) is a function of Nrandom variables X1, . . . , X N.\nTherefore, b\u0398(X1, . . . , X N) changes as Ngrows. In this subsection we analyze how b\u0398 behaves\nwhen Nchanges. For notational simplicity we use the following notation:\nb\u0398N=b\u0398(X1, . . . , X N). (8.28)\nThus, as Nincreases, we use more random variables in defining b\u0398(X1, . . . , X N).\n494", "510": "8.2. PROPERTIES OF ML ESTIMATES\nDefinition 8.6. An estimator b\u0398Nisconsistent ifb\u0398Np\u2212\u2192\u03b8, i.e.,\nlim\nN\u2192\u221eP\u0014\f\f\fb\u0398N\u2212\u03b8\f\f\f\u2265\u03f5\u0015\n= 0. (8.29)\nThe definition here follows from our discussions of the law of large numbers in Chapter 6.\nThe specific type of convergence is known as the convergence in probability . It says that\nasNgrows, the estimator b\u0398 will be close enough to \u03b8so that the probability of getting a\nlarge deviation will diminish, as illustrated in Figure 8.13 .\n-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2\n-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2\n(a)N= 1 (b) N= 2\n-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2\n-5 -4 -3 -2 -1 0 1 2 3 4 500.20.40.60.811.2\n(c)N= 4 (d) N= 8\nFigure 8.13: The four subfigures here illustrate the probability of error P\u0002\n|b\u0398N\u2212\u03b8| \u2265\u03f5\u0003\n, which is\nrepresented by the areas shaded in blue. We assume that the estimator b\u0398Nis a Gaussian random\nvariable following a distribution Gaussian (0,\u03c32\nN), where we set \u03c3= 1. The threshold we use in this\nfigure is \u03f5= 1. AsNgrows, we see that the probability of error diminishes. If the probability of error\ngoes to zero, we say that the estimator is consistent .\nThe examples in Figure 8.13 are typical situations for an estimator based on the\nsample average. For example, if we assume that X1, . . . , X Nare i.i.d. Gaussian copies of\nGaussian(0 , \u03c32), then the estimator\nb\u0398(X1, . . . , X N) =1\nNNX\nn=1Xn\nwill follow a Gaussian distribution Gaussian(0 ,\u03c32\nN). (Please refer to Chapter 6 for the deriva-\ntion.) Then, as Ngrows, the PDF of b\u0398Nbecomes narrower and narrower. For a fixed \u03f5, it\nfollows that the probability of error will diminish to zero. In fact, we can prove that, for this\n495", "511": "CHAPTER 8. ESTIMATION\nexample,\nP\u0014\f\f\fb\u0398N\u2212\u03b8\f\f\f\u2265\u03f5\u0015\n=P\u0014\nb\u0398N\u2212\u03b8\u2265\u03f5\u0015\n+P\u0014\nb\u0398N\u2212\u03b8\u2264 \u2212\u03f5\u0015\n=Z\u221e\n\u03b8+\u03f5Gaussian\u0012\nz\f\f\u03b8,\u03c32\nN\u0013\ndz+Z\u03b8\u2212\u03f5\n\u2212\u221eGaussian\u0012\nz\f\f\u03b8,\u03c32\nN\u0013\ndz\n=Z\u221e\n\u03b8+\u03f51p\n2\u03c0\u03c32/Ne\u2212(z\u2212\u03b8)2\n2\u03c32/Ndz+Z\u03b8\u2212\u03f5\n\u2212\u221e1p\n2\u03c0\u03c32/Ne\u2212(z\u2212\u03b8)2\n2\u03c32/Ndz\n=Z\u221e\n\u03f5\n\u03c3/\u221a\nN1\u221a\n2\u03c0e\u2212z2\n2dz+Z\u2212\u03f5\n\u03c3/\u221a\nN\n\u2212\u221e1\u221a\n2\u03c0e\u2212z2\n2dz\n= 1\u2212\u03a6\u0012\u03f5\n\u03c3/\u221a\nN\u0013\n+ \u03a6\u0012\u2212\u03f5\n\u03c3/\u221a\nN\u0013\n= 2\u03a6\u0012\u2212\u03f5\n\u03c3/\u221a\nN\u0013\n.\nTherefore, as N\u2192 \u221e , it holds that\u2212\u03f5\n\u03c3/\u221a\nN\u2192 \u2212\u221e . Hence,\nlim\nN\u2192\u221eP\u0014\f\f\fb\u0398N\u2212\u03b8\f\f\f\u2265\u03f5\u0015\n= lim\nN\u2192\u221e2\u03a6\u0012\u2212\u03f5\n\u03c3/\u221a\nN\u0013\n= 0.\nThis explains why in Figure 8.13 the probability of error diminishes to zero as Ngrows.\nTherefore, we say that b\u0398Nisconsistent .\nIn general, there are two ways to check whether an estimator is consistent:\n\u0088Prove convergence in probability . This is based on the definition of a consistent\nestimator. If we can prove that\nlim\nN\u2192\u221eP\u0002\n|b\u0398N\u2212\u03b8| \u2265\u03f5\u0003\n= 0, (8.30)\nthen we say that the estimator is consistent.\n\u0088Prove convergence in mean squared error :\nlim\nN\u2192\u221eE[(b\u0398N\u2212\u03b8)2] = 0. (8.31)\nTo see why convergence in the mean squared error is sufficient to guarantee consistency,\nwe recall Chebyshev\u2019s inequality in Chapter 6, which says that\nP\u0002\n|b\u0398N\u2212\u03b8| \u2265\u03f5\u0003\n\u2264E[(b\u0398N\u2212\u03b8)2]\n\u03f52.\nThus, if lim N\u2192\u221eE[(b\u0398N\u2212\u03b8)2] = 0, convergence in probability will also hold. How-\never, since mean square convergence is stronger than convergence in probability, being\nunable to show mean square convergence does not imply that an estimator is incon-\nsistent.\nBe careful not to confuse a consistent estimator and an unbiased estimator. The two\nare different concepts; one does not imply the other.\n496", "512": "8.2. PROPERTIES OF ML ESTIMATES\nConsistent versus unbiased\n\u0088Consistent = If you have enough samples, then the estimator b\u0398 will converge to\nthe true parameter.\n\u0088Unbiasedness does not imply consistency. For example (Gaussian), if\nb\u0398 =X1,\nthenE[X1] =\u00b5. ButP[|b\u0398\u2212\u00b5|> \u03f5] does not converge to 0 as Ngrows. So this\nestimator is inconsistent. (See Example 8.16 below.)\n\u0088Consistency does not imply unbiasedness. For example (Gaussian), if\nb\u0398 =1\nNNX\nn=1(Xn\u2212\u00b5)2\nis a biased estimate for variance, but it is consistent. (See Example 8.17 below.)\nExample 8.15 . Let X1, . . . , X Nbe i.i.d. Gaussian random variables with an unknown\nmean \u00b5and known variance \u03c32. We know that the ML estimator for the mean is\nb\u00b5ML= (1/N)PN\nn=1Xn. Isb\u00b5MLconsistent?\nSolution . We have shown that the ML estimator is\nb\u00b5ML=1\nNNX\nn=1Xn.\nSinceE[b\u00b5ML] =\u00b5, andE[(b\u00b5ML\u2212\u00b5)2] = Var[ b\u00b5ML] =\u03c32\nN, it follows that\nP\u0002\n|b\u00b5ML\u2212\u00b5| \u2265\u03f5\u0003\n\u2264E[(b\u00b5ML\u2212\u00b5)2]\n\u03f52=\u03c32\nN\u03f52.\nThus, when Ngoes to infinity, the probability converges to zero, and hence the esti-\nmator is consistent.\nExample 8.16 . Let X1, . . . , X Nbe i.i.d. Gaussian random variables with an unknown\nmean \u00b5and known variance \u03c32. Define an estimator b\u00b5=X1. Show that the estimator\nis unbiased but inconsistent.\nSolution . We know that E[b\u00b5] =E[X1] =\u00b5. Sob\u00b5is an unbiased estimator. However,\nwe can show that\nE[(b\u00b5\u2212\u00b5)2] =E[(X1\u2212\u00b5)2] =\u03c32.\nSince this variance E[(b\u00b5\u2212\u00b5)2] does not shrink as Nincreases, it follows that no matter\n497", "513": "CHAPTER 8. ESTIMATION\nhow many samples we use we cannot make E[(b\u00b5\u2212\u00b5)2] go to zero. To be more precise,\nP\u0014\n|b\u00b5\u2212\u00b5| \u2265\u03f5\u0015\n=P\u0014\n|X1\u2212\u00b5| \u2265\u03f5\u0015\n=P\u0014\nX1\u2264\u00b5\u2212\u03f5\u0015\n+P\u0014\nX1\u2265\u00b5+\u03f5\u0015\n=Z\u00b5\u2212\u03f5\n\u2212\u221e1\u221a\n2\u03c0\u03c32e\u2212(x\u2212\u00b5)2\n2\u03c32dx+Z\u221e\n\u00b5+\u03f51\u221a\n2\u03c0\u03c32e\u2212(x\u2212\u00b5)2\n2\u03c32dx\n= 2\u03a6\u0012\u2212\u03f5\n\u03c3\u0013\n,\nwhich does not converge to zero as N\u2192 \u221e . So the estimator is inconsistent.\nExample 8.17 . Let X1, . . . , X Nbe i.i.d. Gaussian random variables with an unknown\nmean \u00b5and an unknown variance \u03c32. Is the ML estimate of the variance, i.e., b\u03c32\nML,\nconsistent?\nSolution . We know that the ML estimator for the mean is\nb\u00b5ML=1\nNNX\nn=1Xn,\nand we have shown that it is an unbiased and consistent estimator of the mean. For\nthe variance,\nb\u03c32\nML=1\nNNX\nn=1(Xn\u2212b\u00b5ML)2=1\nNNX\nn=1\u0002\nX2\nn\u22122b\u00b5MLXn+b\u00b52\nML\u0003\n=1\nNNX\nn=1X2\nn\u22122b\u00b5ML\u00b71\nNNX\nn=1Xn+b\u00b52\nML\n=1\nNNX\nn=1X2\nn\u2212b\u00b52\nML.\nNote that1\nNPN\nn=1X2\nnis the sample average of the second moment, and so by the\nweak law of large numbers it should converge in probability to E[X2\nn]. Similarly, b\u00b5ML\nwill converge in probability to \u00b5. Therefore, we have\nb\u03c32\nML=1\nNNX\nn=1X2\nn\u2212b\u00b52\nMLp\u2212\u2192(\u03c32+\u00b52)\u2212\u00b52=\u03c32.\nThus, we have shown that the ML estimator of the variance is biased but consistent.\n498", "514": "8.2. PROPERTIES OF ML ESTIMATES\nThe following discussions about the consistency of ML estimators can be skipped.\nAs we have said, there are many estimators. Some estimators are consistent and some\nare not. The ML estimators are special. It turns out that under certain regularity conditions\nthe ML estimators of i.i.d. observations are consistent.\nWithout proving this result formally, we highlight a few steps to illustrate the idea.\nSuppose that we have a set of i.i.d. data points x1, . . . ,xNdrawn from some distribution\nf(x,|\u03b8true). To formulate the ML estimation, we consider the log-likelihood function (di-\nvided by N):\n1\nNlogL(\u03b8|x) =1\nNNX\nn=1logf(xn;\u03b8). (8.32)\nHere, the variable \u03b8is unknown. We need to find it by maximizing the log-likelihood.\nBy the weak law of large numbers, we can show that the log-likelihood based on the\nNsamples will converge in probability to\n1\nNNX\nn=1logf(xn;\u03b8)\n| {z }\ngN(\u03b8)p\u2212\u2192E[logf(x;\u03b8)]. (8.33)\nThe expectation can be evaluated by integrating over the true distribution:\nE[logf(x;\u03b8)] =Z\nlogf(x;\u03b8)\u00b7f(x;\u03b8true)dx\n| {z }\ng(\u03b8).\nwhere f(x;\u03b8true) denotes the true distribution of the samples xn\u2019s. From these two results\nwe define two functions:\ngN(\u03b8)def=1\nNNX\nn=1logf(xn;\u03b8),andg(\u03b8)def=Z\nlogf(x;\u03b8)\u00b7f(x;\u03b8true)dx,\nand we know that gN(\u03b8)p\u2212\u2192g(\u03b8).\nWe also know that b\u03b8MLis the ML estimator, and so\nb\u03b8ML= argmax\n\u03b8gN(\u03b8).\nLet\u03b8\u2217be the maximizer of the limiting function, i.e.,\n\u03b8\u2217= argmax\n\u03b8g(\u03b8).\nBecause gN(\u03b8)p\u2192g(\u03b8), we can (loosely3) argue that b\u03b8MLp\u2192\u03b8\u2217. If we can show that\n\u03b8\u2217=\u03b8true, then we have shown that b\u03b8MLp\u2192\u03b8true, implying that b\u03b8MLis consistent.\n3To rigorously prove this statement we need some kind of regularity conditions on gNandg. A more\nformal proof can be found in H. Vincent Poor, An Introduction Signal Detection and Estimation , Springer,\n1998, Section IV.D.\n499", "515": "CHAPTER 8. ESTIMATION\nTo show that \u03b8\u2217=\u03b8true, we note that\nd\nd\u03b8Z\nlogf(x;\u03b8)\u00b7f(x;\u03b8true)dx=Zd\nd\u03b8logf(x;\u03b8)\u00b7f(x;\u03b8true)dx\n=Zf\u2032(x;\u03b8)\nf(x;\u03b8)\u00b7f(x;\u03b8true)dx.\nWe ask whether this is equal to zero. Putting \u03b8=\u03b8true, we have that\nZf\u2032(x;\u03b8true)\nf(x;\u03b8true)\u00b7f(x;\u03b8true)dx=Z\nf\u2032(x;\u03b8true)dx.\nHowever, this integral can be simplified to\nZ\nf\u2032(x;\u03b8true)dx=d\nd\u03b8Z\nf(x;\u03b8)dx\n|{z}\n=1\f\f\f\f\n\u03b8=\u03b8true= 0.\nTherefore, \u03b8trueis the maximizer for g(\u03b8), and so \u03b8true=\u03b8\u2217.\nEnd of the discussion. Please join us again.\n8.2.4 Invariance principle\nAnother useful property satisfied by the ML estimate is the invariance principle . The in-\nvariance principle says that a monotonic transformation of the true parameter is preserved\nfor the ML estimates.\nWhat is the invariance principle?\n\u0088There is a monotonic function h.\n\u0088There is an ML estimate b\u03b8MLfor\u03b8.\n\u0088The monotonic function hmaps the true parameter \u03b87\u2212\u2192h(\u03b8).\n\u0088Then the same function will map the ML estimate b\u03b8ML7\u2212\u2192h(b\u03b8ML).\nThe formal statement of the invariance principle is given by the theorem below.\nTheorem 8.1. Ifb\u03b8MLis the ML estimate of \u03b8, then for any one-to-one function h\nof\u03b8, the ML estimate of h(\u03b8)ish(b\u03b8ML).\nProof . Define the likelihood function L(\u03b8) (we have dropped xto simplify the notation).\nThen, for any monotonic function h, we have that\nL(\u03b8) =L(h\u22121(h(\u03b8))).\n500", "516": "8.2. PROPERTIES OF ML ESTIMATES\nLetb\u03b8MLbe the ML estimate:\nb\u03b8ML= argmax\n\u03b8L(\u03b8) = argmax\n\u03b8L(h\u22121(h(\u03b8))).\nBy the definition of ML, b\u03b8MLmust maximize the likelihood. Therefore, L(h\u22121(h(\u03b8))) is\nmaximized when h\u22121(h(\u03b8)) =b\u03b8ML. This implies that h(\u03b8) =h(b\u03b8ML) because his monotonic.\nSince h(\u03b8) is the parameter we try to estimate, the equality h(\u03b8) =h(b\u03b8ML) implies that\nh(b\u03b8ML) is the ML estimate of h(\u03b8). \u25a1\nExample 8.18 . Consider the single-photon image sensor example we discussed in\nSection 8.1. We consider a set of i.i.d. Bernoulli random variables with PMF\npXn(1) = 1 \u2212e\u2212\u03b7and pXn(0) = e\u2212\u03b7. (8.34)\nFind the ML estimate through (a) direct calculation and (b) the invariance principle.\nSolution . (a) Following the example in Equation (8.12), the ML estimate of \u03b7is\nb\u03b7ML= argmax\n\u03b7NY\nn=1(1\u2212e\u2212\u03b7)xn(e\u2212\u03b7)1\u2212xn=\u2212log \n1\u22121\nNNX\nn=1xn!\n.\n(b) We can obtain the same result using the invariance principle. Since Xnis a\nbinary random variable, we assume that it is a Bernoulli with parameter \u03b8. Then the\nML estimate of \u03b8is\nb\u03b8ML= argmax\n\u03b8NY\nn=1\u03b8xn(1\u2212\u03b8)1\u2212xn\n=1\nNNX\nn=1xn.\nThe relationship between \u03b8and\u03b7is that \u03b8= 1\u2212e\u2212\u03b7, or\u03b7=\u2212log(1\u2212\u03b8). So we let\nh(\u03b8) =\u2212log(1\u2212\u03b8). The invariance principle says that the ML estimate of h(\u03b8) is\nb\u03b7MLdef=dh(\u03b8)ML(i)=h(b\u03b8ML)\n=\u2212log \n1\u22121\nNNX\nn=1xn!\n,\nwhere (i) follows from the invariance principle.\nThe invariance principle can be very convenient, especially when the transformation his\ncomplicated, so that a direct evaluation of the ML estimate is difficult.\nThe invariance principle is portrayed in Figure 8.14 . We start with the Bernoulli log-\nlikelihood\nlogL(\u03b8|S) =Slog\u03b8+ (1\u2212S) log(1 \u2212\u03b8).\n501", "517": "CHAPTER 8. ESTIMATION\nFigure 8.14: The invariance principle is a transformation of the ML estimate. In this example, we\nconsider a Bernoulli log-likelihood function shown in the lowermost plot. For this log-likelihood, the ML\nestimate is b\u03b8ML= 0.4. On the left-hand side we show another log-likelihood, derived for a truncated\nPoisson random variable. Note that the ML estimate is b\u03b7ML= 0.5108. The invariance principle asserts\nthat, instead of computing these ML estimates directly, we can first derive the relationship between \u03b7\nand\u03b8for any \u03b8. Since we know that \u03b8= 1\u2212e\u2212\u03b7, it follows that \u03b7=\u2212log(1\u2212\u03b8). We define this\ntransformation as \u03b7=h(\u03b8) =\u2212log(1\u2212\u03b8). Then the ML estimate is b\u03b7ML=h(b\u03b8ML) =h(0.4) = 0 .5108.\nThe invariance principle saves us the trouble of computing the maximization of the more truncated\nPoisson likelihood.\nIn this particular example we let S= 20, where Sdenotes the sum of the N= 50 Bernoulli\nrandom variables. The other log-likelihood is the truncated Poisson, which is given by\nlogL(\u03b7|S) =Slog(1\u2212e\u2212\u03b7) + (1 \u2212S) log( e\u2212\u03b7).\nThe transformation between the two is the function \u03b7=h(\u03b8) =\u2212log(1\u2212\u03b8). Putting\neverything into the figure, we see that the ML estimate ( \u03b8= 0.4) is translated to \u03b7= 0.5108.\nThe invariance principle asserts that this calculation can be done by b\u03b7ML=h(b\u03b8ML) =\nh(0.4) =\u22120.5108.\n8.3 Maximum A Posteriori Estimation\nIn ML estimation, the parameter \u03b8is treated as a deterministic quantity. There are, however,\nmany situations where we have some prior knowledge about \u03b8. For example, we may not\nknow exactly the speed of a car, but we may know that the speed is roughly 65 mph\n502", "518": "8.3. MAXIMUM A POSTERIORI ESTIMATION\nwith a standard deviation of 5 mph. How do we incorporate such prior knowledge into the\nestimation problem?\nIn this section, we introduce the second estimation technique, known as the maximum\na posteriori (MAP) estimation. MAP estimation links the likelihood and the prior. The key\nidea is to treat the parameter \u03b8as a random variable (vector) \u0398with a PDF f\u0398(\u03b8).\n8.3.1 The trio of likelihood, prior, and posterior\nTo understand how the MAP estimation works, it is important first to understand the role\nof the parameter \u03b8, which changes from a deterministic quantity to a random quantity.\nRecall the likelihood function we defined in the ML estimation; it is\nL(\u03b8|x) =fX(x;\u03b8),\nif we assume that we have a set of i.i.d. observations x= [x1, . . . , x N]T. By writing the PDF\nofXasfX(x;\u03b8), we emphasize that \u03b8is a deterministic but unknown parameter. There\nis nothing random about \u03b8.\nIn MAP, we change the nature of \u03b8from deterministic to random. We replace \u03b8by\u0398\nand write\nfX(x;\u03b8)becomes=\u21d2 fX|\u0398(x|\u03b8). (8.35)\nThe difference between the left-hand side and the right-hand side is subtle but important.\nOn the left-hand side, fX(x;\u03b8) is the PDF of X. This PDF is parameterized by \u03b8. On the\nright-hand side, fX|\u0398(x|\u03b8) is a conditional PDF of Xgiven \u0398. The values they provide\nare exactly the same. However, in fX|\u0398(x|\u03b8),\u03b8is a realization of a random variable \u0398.\nBecause \u0398is now a random variable (vector), we can define its PDF (yes, the PDF\nof\u0398), and denote it by\nf\u0398(\u03b8), (8.36)\nwhich is called the prior distribution. The prior distribution of \u0398is unique in MAP estima-\ntion. There is nothing called a prior in ML estimation.\nMultiplying fX|\u0398(x|\u03b8) with the prior PDF f\u0398(\u03b8), and using Bayes\u2019 Theorem, we\nobtain the posterior distribution:\nf\u0398|X(\u03b8|x) =fX|\u0398(x|\u03b8)f\u0398(\u03b8)\nfX(x). (8.37)\nThe posterior distribution is the PDF of \u0398given the measurements X.\nThe likelihood, the prior, and the posterior can be confusing. Let us clarify their mean-\nings.\n\u0088Likelihood fX|\u0398(x|\u03b8): This is the conditional probability density of Xgiven the pa-\nrameter \u0398. Do not confuse the likelihood fX|\u0398(x|\u03b8) defined in the MAP context\nand the likelihood fX(x;|\u03b8) defined in the ML context. The former assumes that \u0398\nis random whereas the latter assumes that \u03b8is deterministic. They have the same\nvalues.\n\u0088Prior f\u0398(\u03b8): This is the prior distribution of \u0398. It does not come from the data X\nbut from our prior knowledge. For example, if we see a bike on the road, even before\nwe take any measurement we will have a rough idea of its speed. This is the prior\ndistribution.\n503", "519": "CHAPTER 8. ESTIMATION\n\u0088Posterior f\u0398|X(\u03b8|x): This is the posterior density of \u0398given that we have observed X.\nDo not confuse f\u0398|X(\u03b8|x) andL(\u03b8|x). The posterior distribution f\u0398|X(\u03b8|x) is a PDF\nof\u0398given X=x. The likelihood L(\u03b8|x) is not a PDF. If you integrate f\u0398|X(\u03b8|x)\nwith respect to \u03b8, you get 1, but if you integrate L(\u03b8|x) with respect to \u03b8, you do\nnot get 1.\nWhat is the difference between ML and MAP?\nLikelihood ML fX(x;\u03b8) The parameter \u03b8is deterministic.\nMAP fX|\u0398(x|\u03b8) The parameter \u0398is random.\nPrior ML There is no prior, because \u03b8is deterministic.\nMAP f\u0398(\u03b8) This is the PDF of \u0398.\nOptimization ML Find the peak of the likelihood fX(x;\u03b8).\nMAP Find the peak of the posterior f\u0398|X(\u03b8|x).\nMaximum a posteriori (MAP) estimation is a form of Bayesian estimation. Bayesian\nmethods emphasize our prior knowledge or beliefs about the parameters. As we will see\nshortly, the prior has something valuable to offer, especially when we have very few data\npoints.\n8.3.2 Understanding the priors\nSince the biggest difference between MAP and ML is the addition of the prior f\u0398(\u03b8), we\nneed to take a closer look at what they mean. In Figure 8.15 below, we show a set of six\ndifferent priors. We ask two questions: (1) What do they mean? (2) Which one should we\nuse?\nFigure 8.15: This figure illustrates six different examples of the prior distribution f\u0398(\u03b8), when the prior\nis a 1D parameter \u03b8. The prior distribution f\u0398(\u03b8)is the PDF of \u0398. (a) f\u0398(\u03b8) =\u03b4(\u03b8), which is a delta\nfunction. (b) f\u0398(\u03b8) =1\nb\u2212afora\u2264\u03b8\u2264b. This is a uniform distribution. (c) This is also a uniform\ndistribution, but the spread is very wide. (d) f\u0398(\u03b8) =Gaussian (0, \u03c32), which is a zero-mean Gaussian.\n(e) The same Gaussian, but with a different mean. (f) A Gaussian with zero mean, but a large variance.\n504", "520": "8.3. MAXIMUM A POSTERIORI ESTIMATION\nWhat does the shape of a prior tell us?\nIt tells us your belief as to how the underlying parameter \u0398should be distributed.\nThe meaning of this statement can be best understood from the examples shown in Fig-\nure 8.15 :\n\u0088Figure 8.15 (a). This is a delta prior f\u0398(\u03b8) =\u03b4(\u03b8) (or f\u0398(\u03b8) =\u03b4(\u03b8\u2212\u03b80)). If you use this\nprior, you are absolutely sure that the parameter \u0398 takes a specific value. There is no\nuncertainty about your belief. Since you are so confident about your prior knowledge,\nyou will ignore the likelihood that is constructed from the data. No one will use a\ndelta prior in practice.\n\u0088Figure 8.15 (b).f\u0398(\u03b8) =1\nb\u2212afora\u2264\u03b8\u2264b, and is zero otherwise. This is a bounded\nuniform prior. You do not have any preference for the parameter \u0398, but you do know\nfrom your prior experience that a\u2264\u0398\u2264b.\n\u0088Figure 8.15 (c). This prior is the same as (b) but is short and very wide. If you use\nthis prior, it means that you know nothing about the parameter. So you give up the\nprior and let the likelihood dominate the MAP estimate.\n\u0088Figure 8.15 (d).f\u0398(\u03b8) = Gaussian(0 , \u03c32). You use this prior when you know something\nabout the parameter, e.g., that it is centered at certain location and you have some\nuncertainty.\n\u0088Figure 8.15 (e). Same as (d), but the parameter is centered at some other location.\n\u0088Figure 8.15 (f). Same as (d), but you have less confidence about the parameter.\nAs you can see from these examples, the shape of the prior tells us how youwant \u0398 to be\ndistributed. The choice you make will directly influence the MAP optimization, and hence\nthe MAP estimate.\nSince the prior is a subjective quantity in the MAP framework, you as the user have\nthe freedom to choose whatever you like. For instance, if you have conducted a similar\nexperiment before, you can use the results of the previous experiments as the current prior.\nAnother strategy is to go with physics. For instance, we can argue that \u03b8should be sparse\nso that it contains as few non-zeros as possible. In this case, a sparsity-driven prior, such\nasf\u0398(\u03b8) = exp {\u2212\u2225\u03b8\u22251}, could be a choice. The third strategy is to choose a prior that is\ncomputationally \u201cfriendlier\u201d, e.g., in quadratic form so that the MAP is differentiable. One\nsuch choice is the conjugate prior . We will discuss this later in Section 8.3.6.\nWhich prior should we choose?\n\u0088Based on your preference, e.g., you know from historical data that the parameter\nshould behave in certain ways.\n\u0088Based on physics, e.g., the parameter has a physical interpretation, so you need\nto abide by the physical laws.\n\u0088Choose a prior that is computationally \u201cfriendlier\u201d. This is the topic of the\nconjugate prior , which is a prior that does not change the form of the posterior\ndistribution. (We will discuss this later in Section 8.3.6.)\n505", "521": "CHAPTER 8. ESTIMATION\n8.3.3 MAP formulation and solution\nOur next task is to study how to formulate the MAP problem and how to solve it.\nDefinition 8.7. LetX= [X1, . . . , X N]Tbe i.i.d. observations. Let \u0398be a random\nparameter. The maximum-a-posteriori estimate of \u0398is\nb\u03b8MAP =argmax\n\u03b8f\u0398|X(\u03b8|x). (8.38)\nPhilosophically speaking, ML and MAP have two different goals. ML considers a para-\nmetric model with a deterministic parameter. Its goal is to find the parameter that maximizes\nthe likelihood for the data we have observed. MAP also considers a parametric model but\nthe parameter \u0398is random. Because \u0398is random, we are finding one particular state \u03b8of\nthe parameter \u0398that offers the best explanation conditioned on the data Xwe observe.\nIn a sense, the two optimization problems are\nb\u03b8ML= argmax\n\u03b8fX|\u0398(x|\u03b8),\nb\u03b8MAP = argmax\n\u03b8f\u0398|X(\u03b8|x).\nThis pair of equations is interesting, as the pair tells us that the difference between the ML\nestimation and the MAP estimation is the flipped order of Xand\u0398.\nThere are two reasons we care about the posterior. First, in MAP the posterior allows\nus to incorporate the prior. ML does not allow a prior. A prior can be useful when the\nnumber of samples is small. Second, maximizing the posterior does have some physical\ninterpretations. MAP asks for the probability of \u0398=\u03b8after observing Ntraining samples\nX=x. ML asks for the probability of observing X=xgiven a parameter \u03b8. Both are\ncorrect and legitimate criteria, but sometimes we might prefer one over the other.\nTo solve the MAP problem, we notice that\nb\u03b8MAP = argmax\n\u03b8f\u0398|X(\u03b8|x)\n= argmax\n\u03b8fX|\u0398(x|\u03b8)f\u0398(\u03b8)\nfX(x)\n= argmax\n\u03b8fX|\u0398(x|\u03b8)f\u0398(\u03b8), f X(x) does not contain \u03b8\n= argmax\n\u03b8logfX|\u0398(x|\u03b8) + log f\u0398(\u03b8).\nTherefore, what MAP adds is the prior log f\u0398(\u03b8). If you use an uninformative prior, e.g., a\nprior with extremely wide support, then the MAP estimation will return more or less the\nsame result as the ML estimation.\nWhen does MAP = ML?\n\u0088The relation \u201c=\u201d does not make sense here, because \u03b8is random in MAP but\ndeterministic in ML.\n\u0088Solution of MAP optimization = solution of ML optimization, when f\u0398(\u03b8) is\nuniform over the parameter space.\n506", "522": "8.3. MAXIMUM A POSTERIORI ESTIMATION\n\u0088In this case, f\u0398(\u03b8) = constant and so it can be dropped from the optimization.\nExample 8.19 . Let X1, . . . , X Nbe i.i.d. random variables with a PDF fXn|\u0398(xn|\u03b8)\nfor all n, and \u0398 be a random parameter with PDF f\u0398(\u03b8):\nfXn|\u0398(xn|\u03b8) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(xn\u2212\u03b8)2\n2\u03c32\u001b\n,\nf\u0398(\u03b8) =1p\n2\u03c0\u03c32\n0exp\u001a\n\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0\u001b\n.\nFind the MAP estimate.\nSolution . The MAP estimate is\nb\u03b8MAP = argmax\n\u03b8\"NY\nn=11\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(xn\u2212\u03b8)2\n2\u03c32\u001b#\n\u00d7\"\n1p\n2\u03c0\u03c32\n0exp\u001a\n\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0\u001b#\n= argmax\n\u03b8\u00121\u221a\n2\u03c0\u03c32\u0013N\n\u00d71p\n2\u03c0\u03c32\n0exp(\n\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0)\n.\nSince the maximizer is not changed by any monotonic function, we apply logarithm\nto the above equations. This yields\nb\u03b8MAP = argmax\n\u03b8\u001a\n\u2212N\n2log\u0000\n2\u03c0\u03c32\u0001\n\u22121\n2log(2\u03c0\u03c32\n0)\n\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0\u001b\n.\nConstants in the maximization do not matter. So by dropping the constant terms we\nobtain\nb\u03b8MAP = argmax\n\u03b8(\n\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0)\n. (8.39)\nIt now remains to solve the maximization. To this end we take the derivative w.r.t. \u03b8\nand show that\nd\nd\u03b8(\n\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0)\n= 0.\nThis yields\nNX\nn=1(xn\u2212\u03b8)\n\u03c32\u2212\u03b8\u2212\u00b50\n\u03c32\n0= 0.\n507", "523": "CHAPTER 8. ESTIMATION\nRearranging the terms gives us the final result:\nb\u03b8MAP =\u03c32\n0\u0010\n1\nNPN\nn=1xn\u0011\n+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN. (8.40)\nPractice Exercise 8.7 . Prove that if f\u0398(\u03b8) =\u03b4(\u03b8\u2212\u03b80), the MAP estimate is b\u03b8MAP =\n\u03b80.\nSolution . Iff\u0398(\u03b8) =\u03b4(\u03b8\u2212\u03b80), then\nb\u03b8MAP = argmax\n\u03b8logfX|\u0398(x|\u03b8) + log f\u0398(\u03b8)\n= argmax\n\u03b8logfX|\u0398(x|\u03b8) + log \u03b4(\u03b8\u2212\u03b80)\n=\uf8f1\n\uf8f2\n\uf8f3argmax\n\u03b8logfX|\u0398(x|\u03b8)\u2212 \u221e, \u03b8\u0338=\u03b80.\nargmax\n\u03b8logfX|\u0398(x|\u03b8) + 0, \u03b8=\u03b80.\nThus, if b\u03b8MAP\u0338=\u03b80, the first case says that there is no solution, so we must go with\nthe second case b\u03b8MAP =\u03b80. But if b\u03b8MAP =\u03b80, there is no optimization because we\nhave already chosen b\u03b8MAP =\u03b80. This proves the result.\n8.3.4 Analyzing the MAP solution\nAs we said earlier, MAP offers something that ML does not. To see this, we will use the\nresult of the Gaussian random variables as an example and analyze the MAP solution as\nwe change the parameters Nand\u03c30. Recall that if X1, . . . , X Nare i.i.d. Gaussian random\nvariables with unknown mean \u03b8and known variance \u03c3, the ML estimate is\nb\u03b8ML=1\nNNX\nn=1xn.\nAssuming that the parameter \u0398 is distributed according to a PDF Gaussian( \u00b50, \u03c32\n0), we\nhave shown in the previous subsection that\nb\u03b8MAP =\u03c32\n0\u0010\n1\nNPN\nn=1xn\u0011\n+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN=\u03c32\n0b\u03b8ML+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN.\nIn what follows, we will take a look at the behavior of the MAP estimate b\u03b8MAP asN\nand\u03c30change. The results of our discussion are summarized in Figure 8.16 .\nFirst, let\u2019s look at the effect of N.\nHow does Nchange b\u03b8MAP?\n\u0088AsN\u2192 \u221e , the MAP estimate b\u03b8MAP\u2192b\u03b8ML: If we have enough samples, we\ntrust the data.\n508", "524": "8.3. MAXIMUM A POSTERIORI ESTIMATION\n(a) Effect of N (b) Effect of \u03c30\nFigure 8.16: The MAP estimate b\u03b8MAPswings between the ML estimate b\u03b8MLand the prior \u00b50. (a) When\nNincreases, the likelihood is more reliable and so we lean towards the ML estimate. If Nis small, we\nshould trust the prior more than the ML estimate. (b) When \u03c30decreases, we become more confident\nabout the prior and so we will use it. If \u03c30is large, we use more information from the ML estimate.\n\u0088AsN\u21920, the MAP estimate b\u03b8MAP\u2192\u03b80. If we do not have any samples, we\ntrust the prior.\nThese two results can be demonstrated by taking the limits. As N\u2192 \u221e , the MAP estimate\nconverges to\nlim\nN\u2192\u221eb\u03b8MAP = lim\nN\u2192\u221e\u03c32\n0b\u03b8ML+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN=b\u03b8ML. (8.41)\nThis result is not surprising. When we have infinitely many samples, we will completely\nrely on the data and make our estimate. Thus, the MAP estimate is the same as the ML\nestimate.\nWhen N\u21920, the MAP estimate converges to\nlim\nN\u21920b\u03b8MAP = lim\nN\u21920\u03c32\n0b\u03b8ML+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN=\u00b50. (8.42)\nThis means that, when we do not have any samples, the MAP estimate b\u03b8MAP will completely\nuse the prior distribution, which has a mean \u00b50.\nThe implication of this result is that MAP offers a natural swing between b\u03b8MLandb\u03b80,\ncontrolled by N. Where does this Ncome from? If we recall the derivation of the result, we\nnote that the Naffects the likelihood term through the number of samples:\nb\u03b8MAP = argmax\n\u03b8\u001a\n\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32\n|{z }\nNterms here\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0|{z}\n1 term\u001b\n.\nThus, as Nincreases, the influence of the data term grows, and so the result will gradually\nshift towards b\u03b8ML.\nFigure 8.17 illustrates a numerical experiment in which we draw Nrandom samples\nx1, . . . , x Naccording to a Gaussian distribution Gaussian( \u03b8, \u03c32), with \u03c3= 1. We assume\nthat the prior distribution is Gaussian( \u00b50, \u03c32\n0), with \u00b50= 0 and \u03c30= 0.25. The ML estimate\nof this problem is b\u03b8ML=1\nNPN\nn=1xn, whereas the MAP estimate is given by Equation (8.40).\n509", "525": "CHAPTER 8. ESTIMATION\nThe figure shows the resulting PDFs. A helpful analogy is that the prior and the likelihood\nare pulling a rope in two opposite directions. As Ngrows, the force of the likelihood increases\nand so the influence becomes stronger.\n(a)N= 1 (b) N= 50\nFigure 8.17: The subfigures show the prior distribution f\u0398(\u03b8)and the likelihood function fX|\u0398(x|\u03b8),\ngiven the observed data. (a) When N= 1, the estimated posterior distribution f\u0398|X(\u03b8|x)is pulled\ntowards the prior. (b) When N= 50 , the posterior is pulled towards the ML estimate. The analogy for\nthe situation is that each data point is acting as a small force against the big force of the prior. As N\ngrows, the small forces of the data points accumulate and eventually dominate.\nWe next look at the effect of \u03c30.\nHow does \u03c30change b\u03b8MAP?\n\u0088As\u03c30\u2192 \u221e , the MAP estimate b\u03b8MAP\u2192b\u03b8ML: If we have doubts about the prior,\nwe trust the data.\n\u0088As\u03c30\u21920, the MAP estimate b\u03b8MAP\u2192\u03b80. If we are absolutely sure about the\nprior, we ignore the data.\nWhen \u03c30\u2192 \u221e , the limit of b\u03b8MAP is\nlim\n\u03c30\u2192\u221eb\u03b8MAP = lim\n\u03c30\u2192\u221e\u03c32\n0b\u03b8ML+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN=b\u03b8ML. (8.43)\nThe reason why this happens is that \u03c30is the uncertainty level of the prior. If \u03c30is high,\nwe are not certain about the prior. In this case, MAP chooses to follow the ML estimate.\nWhen \u03c30\u21920, the limit of b\u03b8MAP is\nlim\n\u03c30\u21920b\u03b8MAP = lim\n\u03c30\u21920\u03c32\n0b\u03b8ML+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN=\u00b50. (8.44)\nNote that when \u03c30\u21920, we are essentially saying that we are absolutely sure about the\nprior. If we are so sure about the prior, there is no need to look at the data. In that case\nthe MAP estimate is \u00b50.\n510", "526": "8.3. MAXIMUM A POSTERIORI ESTIMATION\nThe way to understand the influence of \u03c30is to inspect the equation:\nb\u03b8MAP = argmax\n\u03b8\u001a\n\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32\n|{z }\nfixed w.r.t. \u03c30\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0|{z}\nchanges with \u03c30\u001b\n.\nSince \u03c30is purely a preference youdecide, you can control how much trust to put onto the\nprior.\n(a)\u03c30= 0.1 (b) \u03c30= 1\nFigure 8.18: The subfigures show the prior distribution f\u0398(\u03b8)and the likelihood function fX|\u0398(x|\u03b8),\ngiven the observed data. (a) When \u03c30= 0.1, the estimated posterior distribution f\u0398|X(\u03b8|x)is pulled\ntowards the prior. (b) When \u03c30= 1, the posterior is pulled towards the ML estimate. An analogy for\nthe situation is that the strength of the prior depends on the magnitude of \u03c30. If\u03c30is small the prior\nis strong, and so the influence is large. If \u03c30is large the prior is weak, and so the ML estimate will\ndominate.\nFigure 8.18 illustrates a numerical experiment in which we compare \u03c30= 0.1 and\n\u03c30= 1. If \u03c30is small, the prior distribution f\u0398(\u03b8) becomes similar to a delta function. We\ncan interpret it as a very confident prior, so confident that we wish to align with the prior.\nThe situation can be imagined as a game of tug-of-war between a powerful bull and a horse,\nwhich the bull will naturally win. If \u03c30is large the prior distribution will become flat. It\nmeans that we are not very confident about the prior so that we will trust the data. In this\ncase the MAP estimate will shift towards the ML estimate.\n8.3.5 Analysis of the posterior distribution\nWhen the likelihood is multiplied with the prior to form the posterior, what does the poste-\nrior distribution look like? To answer this question we continue our Gaussian example with\na fixed variance \u03c3and an unknown mean \u03b8. The posterior distribution is proportional to\nf\u0398|X(\u03b8|x) =fX|\u0398(x|\u03b8)f\u0398(\u03b8)\nfX(x)\u221dfX|\u0398(x|\u03b8)f\u0398(\u03b8)\n=\"NY\nn=11\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(xn\u2212\u03b8)2\n2\u03c32\u001b#\n\u00b7\"\n1p\n2\u03c0\u03c32\n0exp\u001a\n\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0\u001b#\n.(8.45)\n511", "527": "CHAPTER 8. ESTIMATION\nPerforming the multiplication and completing the squares,\nNX\nn=1(xn\u2212\u03b8)2\n2\u03c32+(\u03b8\u2212\u00b50)2\n2\u03c32\n0=(\u03b8\u2212b\u03b8MAP)2\n2\u03c32\nMAP,\nwhere\nb\u03b8MAP =\u03c32\n0b\u03b8ML+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN, and1\nb\u03c32\nMAP=1\n\u03c32\n0+N\n\u03c32. (8.46)\nIn other words, the posterior distribution f\u0398|X(\u03b8|x) is also a Gaussian with\nf\u0398|X(\u03b8|x) = Gaussian( b\u03b8MAP,b\u03c32\nMAP).\nIffX|\u0398(x|\u03b8) =Gaussian (x;\u03b8, \u03c3), and f\u0398(\u03b8) =Gaussian (\u03b8;\u00b50, \u03c32\n0), what is the\nposterior f\u0398|X(\u03b8|x)?\nThe posterior f\u0398|X(\u03b8|x) is Gaussian( b\u03b8MAP,b\u03c32\nMAP), where\nb\u03b8MAP =\u03c32\n0b\u03b8ML+\u03c32\nN\u00b50\n\u03c32\n0+\u03c32\nN,and1\nb\u03c32\nMAP=1\n\u03c32\n0+N\n\u03c32. (8.47)\nThe posterior tells us how Nand\u03c30will influence the MAP estimate. As Ngrows,\nthe posterior mean and variance becomes\nlim\nN\u2192\u221eb\u03b8MAP =b\u03b8ML=\u03b8,and lim\nN\u2192\u221eb\u03c3MAP = 0.\nAs a result, the posterior distribution f\u0398|X(\u03b8|x) will converge to a delta function centered\nat the ML estimate b\u03b8ML. Therefore, as we try to solve the MAP problem by maximizing the\nposterior, the MAP estimate has to improve because b\u03c3MAP\u21920.\nWe can plot the posterior distribution Gaussian( b\u03b8MAP,b\u03c32\nMAP) as a function of the\nnumber of samples N.Figure 8.19 illustrates this example using the following configurations.\nThe likelihood is Gaussian with \u00b5= 1, \u03c3= 0.25. The prior is Gaussian with \u00b50= 0 and\n\u03c3= 0.25. We construct the Gaussian according to Gaussian( b\u03b8MAP,b\u03c32\nMAP) by varying N.\nThe result shown in Figure 8.19 confirms our prediction: As Ngrows, the posterior becomes\nmore like a delta function whose mean is the true mean \u00b5. The posterior estimator b\u03b8MAP,\nfor each N, is the peak of the respective Gaussian.\nWhat is the pictorial interpretation of the MAP estimate?\n\u0088For every N, MAP has a posterior distribution f\u0398|X(\u03b8|x).\n\u0088AsNgrows, f\u0398|X(\u03b8|x) converges to a delta function centered at b\u03b8ML.\n\u0088MAP tries to find the peak of f\u0398|X(\u03b8|x). For large N, it returns b\u03b8ML.\n512", "528": "8.3. MAXIMUM A POSTERIORI ESTIMATION\n-1 -0.5 0 0.5 1 1.502468\nN = 0\nN = 1\nN = 2\nN = 5\nN = 8\nN = 12\nN = 20\nFigure 8.19: Posterior distribution f\u0398|X(\u03b8|x) =Gaussian (b\u03b8MAP, \u03c32\nMAP)asNgrows. When Nis small,\nthe posterior distribution is dominated by the prior. As Nincreases, the posterior distribution changes\nits mean and its variance.\n8.3.6 Conjugate prior\nChoosing the prior is an important topic in a MAP estimation. We have elaborated two\n\u201cengineering\u201d solutions: Use your prior experience or follow the physics. In this subsection,\nwe discuss the third option: to choose something computationally friendly. To explain what\nwe mean by \u201ccomputationally friendly\u201d, let us consider the following example, thanks to\nAvinash Kak.4\nConsider a Bernoulli distribution with a PDF\nfX|\u0398(x|\u03b8) =NY\nn=1\u03b8xn(1\u2212\u03b8)1\u2212xn. (8.48)\nTo compute the MAP estimate, we assume that we have a prior f\u0398(\u03b8). Therefore, the MAP\nestimate is given by\nb\u03b8MAP = argmax\n\u03b8fX|\u0398(x|\u03b8)f\u0398(\u03b8)\n= argmax\n\u03b8\"NY\nn=1\u03b8xn(1\u2212\u03b8)1\u2212xn#\n\u00b7f\u0398(\u03b8)\n= argmax\n\u03b8NX\nn=1xnlog\u03b8+ (1\u2212xn) log(1 \u2212\u03b8) + log f\u0398(\u03b8).\nLet us consider three options for the prior. Which one would you use?\n\u0088Candidate 1 :f\u0398(\u03b8) =1\u221a\n2\u03c0\u03c32expn\n\u2212(\u03b8\u2212\u00b5)2\n2\u03c32o\n, a Gaussian prior. If you choose this\nprior, the optimization problem will become\nb\u03b8MAP = argmax\n\u03b8NX\nn=1\u001a\nxnlog\u03b8+ (1\u2212xn) log(1 \u2212\u03b8)\u001b\n\u2212(\u03b8\u2212\u00b5)2\n2\u03c32.\n4Avinash Kak \u201cML, MAP, and Bayesian \u2014 The Holy Trinity of Parameter Estimation and Data Pre-\ndiction\u201d, https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf\n513", "529": "CHAPTER 8. ESTIMATION\nWe can still take the derivative and set it to zero. This gives\nPN\nn=1xn\n\u03b8\u2212N\u2212PN\nn=1xn\n1\u2212\u03b8=\u03b8\u2212\u00b5\n\u03c32.\nDefining S=PN\nn=1xnand moving the terms around, we have\n(1\u2212\u03b8)\u03c32S\u2212\u03b8\u03c32(N\u2212S) =\u03b8(1\u2212\u03b8)(\u03b8\u2212\u00b5).\nThis is a cubic polynomial problem that has a closed-form solution and is also solvable\nby a computer. But it\u2019s also tedious, at least to lazy engineers like ourselves.\n\u0088Candidate 2 :f\u0398(\u03b8) =\u03bb\n2e\u2212\u03bb|\u03b8|, a Laplace prior. In this case, the optimization problem\nbecomes\nb\u03b8MAP = argmax\n\u03b8NX\nn=1\u001a\nxnlog\u03b8+ (1\u2212xn) log(1 \u2212\u03b8)\u001b\n\u2212\u03bb|\u03b8|.\nWelcome to convex optimization! There is no closed-form solution. If you want to solve\nthis problem, you need to call a convex solver.\n\u0088Candidate 3 :f\u0398(\u03b8) =1\nC\u03b8\u03b1\u22121(1\u2212\u03b8)\u03b2\u22121, a beta prior. This prior looks very compli-\ncated, but let\u2019s plug it into our optimization problem:\nb\u03b8MAP = argmax\n\u03b8NX\nn=1\u001a\nxnlog\u03b8\n+ (1\u2212xn) log(1 \u2212\u03b8)\u001b\n+ (\u03b1\u22121) log \u03b8+ (\u03b2\u22121) log(1 \u2212\u03b8)\n= argmax\n\u03b8(S+\u03b1\u22121) log \u03b8+ (N\u2212S+\u03b2\u22121) log(1 \u2212\u03b8),\nwhere S=PN\nn=1xn. Taking the derivative and setting it to zero, we have\nS+\u03b1\u22121\n\u03b8=N\u2212S+\u03b2\u22121\n1\u2212\u03b8.\nRearranging the terms we obtain the final estimate:\nb\u03b8MAP =S+\u03b1\u22121\nN+\u03b2+\u03b1\u22122. (8.49)\nThere are a number of intuitions that we can draw from this beta prior, but most\nimportantly, we have obtained a very simple solution. That is because the posterior distri-\nbution remains in the same form as the prior, after multiplying by the prior. Specifically, if\nwe use the beta prior, the posterior distribution is\nf\u0398|X(\u03b8|x)\u221dfX|\u0398(x|\u03b8)f\u0398(\u03b8)\n=\"NY\nn=1\u03b8xn(1\u2212\u03b8)1\u2212xn#\n\u00b71\nC\u03b8\u03b1\u22121(1\u2212\u03b8)\u03b2\u22121\n=\u03b8S+\u03b1\u22121(1\u2212\u03b8)N\u2212S+\u03b2\u22121.\nThis is still in the form of \u03b8\u22c6\u22121(1\u2212\u03b8)\u25a0\u22121, which is the same as the prior. When this\nhappens, we call the prior a conjugate prior. In this example, the beta prior is a conjugate\nbefore the Bernoulli likelihood.\n514", "530": "8.3. MAXIMUM A POSTERIORI ESTIMATION\nWhat is a conjugate prior?\n\u0088It is a prior such that when multiplied by the likelihood to form the posterior,\nthe posterior f\u0398|X(\u03b8|x) takes the same form as the prior f\u0398(\u03b8).\n\u0088Every likelihood has its conjugate prior.\n\u0088Conjugate priors are not necessarily good priors. They are just computationally\nfriendly. Some of them have good physical interpretations.\nWe can make a few interpretations of the beta prior, in the context of Bernoulli likeli-\nhood. First, the beta distribution takes the form\nf\u0398(\u03b8) =1\nB(\u03b1, \u03b2)\u03b8\u03b1\u22121(1\u2212\u03b8)\u03b2\u22121, (8.50)\nwith B(\u03b1, \u03b2) is the beta function5. The shape of the beta distribution is shown in Fig-\nure 8.20 . For different choices of \u03b1and\u03b2, the distribution has a peak located towards\neither side of the interval [0 ,1]. For example, if \u03b1is large but \u03b2is small, the distribution\nf\u0398(\u03b8) leans towards 1 (the yellow curve).\n0 0.2 0.4 0.6 0.8 101234\n = 2,  = 8\n = 3,  = 7\n = 8,  = 2\nFigure 8.20: Beta distribution f\u0398(\u03b8)for various choices of \u03b1and\u03b2. When (\u03b1, \u03b2) = (2 ,8), the beta\ndistribution favors small \u03b8. When (\u03b1, \u03b2) = (8 ,2), the beta distribution favors large \u03b8. By swinging\nbetween the (\u03b1, \u03b2)pairs, we obtain a prior that has a preference over \u03b8.\nAs a user, you have the freedom to pick f\u0398(\u03b8). Even if you are restricted to the beta\ndistribution, you still have plenty of degrees of freedom in choosing \u03b1and\u03b2so that your\nchoice matches your belief. For example, if you know ahead of time that the Bernoulli\nexperiment is biased towards 1 (e.g., the coin is more likely to come up heads), you can\nchoose a large \u03b1and a small \u03b2. By contrast, if you believe that the coin is fair, you choose\n\u03b1=\u03b2. The parameters \u03b1and\u03b2are known as the hyperparameters of the prior distribution.\nHyperparameters are parameters for f\u0398(\u03b8).\n5The beta function is defined as B(\u03b1, \u03b2) =\u0393(\u03b1)\u0393(\u03b2)\n\u0393(\u03b1+\u03b2), where \u0393 is the gamma function. For integer n,\n\u0393(n) = (n\u22121)!\n515", "531": "CHAPTER 8. ESTIMATION\nExample 8.20 . (Prior for Gaussian mean) Consider a Gaussian likelihood for a fixed\nvariance \u03c32and unknown mean \u03b8:\nfX|\u0398(x|\u03b8) =\u00121\u221a\n2\u03c0\u03c32\u0013N\nexp(\n\u2212NX\nn=1(xn\u2212\u03b8)2\n2\u03c32)\n.\nShow that the conjugate prior is given by\nf\u0398(\u03b8) =1p\n2\u03c0\u03c32\n0exp\u001a\n\u2212(\u03b8\u2212\u00b50)2\n2\u03c32\n0\u001b\n. (8.51)\nSolution . We have shown this result previously. By some (tedious) completing squares,\nwe show that\nf\u0398|X(\u03b8|x) =1p\n2\u03c0\u03c32\nNexp\u001a\n\u2212(\u03b8\u2212\u00b5N)2\n2\u03c32\nN\u001b\n,\nwhere\n\u00b5N=\u03c32\nN\u03c32\n0+\u03c32\u00b50+N\u03c32\n0\nN\u03c32\n0+\u03c32b\u03b8MLand \u03c32\nN=\u03c32\u03c32\n0\n\u03c32+N\u03c32\n0.\nSince f\u0398|X(\u03b8|x) is in the same form as f\u0398(\u03b8), we know that f\u0398(\u03b8) is a conjugate prior.\nExample 8.21 . (Prior for Gaussian variance ) Consider a Gaussian likelihood for a\nmean \u00b5and unknown variance \u03c32:\nfX|\u03c3(x|\u03c3) =\u00121\u221a\n2\u03c0\u03c32\u0013N\nexp(\n\u2212NX\nn=1(xn\u2212\u00b5)2\n2\u03c32)\n.\nFind the conjugate prior.\nSolution . We first define the precision \u03b8=1\n\u03c32. The likelihood is\nfX|\u0398(x|\u03b8) =\u00121\u221a\n2\u03c0\u03c32\u0013N\nexp(\n\u2212NX\nn=1(xn\u2212\u00b5)2\n2\u03c32)\n=1\n(2\u03c0)N/2\u03b8N/2exp(\n\u2212\u03b8\n2NX\nn=1(xn\u2212\u00b5)2)\n.\nWe propose to choose the prior f\u0398(\u03b8) as\nf\u0398(\u03b8) =1\n\u0393(a)ba\u03b8a\u22121exp{\u2212b\u03b8},\nfor some aandb. This f\u0398(\u03b8) is called the Gamma distribution Gamma( \u03b8|a, b). We can\nshow that E[\u0398] =a\nband Var[\u0398] =a\nb2. With some (tedious) completing squares, we\n516", "532": "8.3. MAXIMUM A POSTERIORI ESTIMATION\nshow that the posterior is\nf\u0398|X(\u03b8|x)\u221d\u03b8(a0+N/2)\u22121exp(\n\u2212 \nb0+1\n2NX\nn=1(xn\u2212\u00b5)2!\n\u03b8)\n,\nwhich is in the same form as the prior. So we know that our proposed f\u0398(\u03b8) is a\nconjugate prior.\nThe story of conjugate priors is endless because every likelihood has its conjugate prior.\nTable 8.1 summarizes a few commonly used conjugate priors, their likelihoods, and their\nposteriors. The list can be expanded further to distributions with multiple parameters. For\nexample, if a Gaussian has both unknown mean and variance, then there exists a conjugate\nprior consisting of a Gaussian multiplied by a Gamma. Conjugate priors also apply to multi-\ndimensional distributions. For example, the prior for the mean vector of a high-dimensional\nGaussian is another high-dimensional Gaussian. The prior for the covariance matrix of a\nhigh-dimensional Gaussian is the Wishart prior. The prior for both the mean vector and the\ncovariance matrix is the normal Wishart.\nTable of Conjugate Priors\nLikelihood Conjugate Prior Posterior\nfX|\u0398(x|\u03b8) f\u0398(\u03b8) f\u0398|X(\u03b8|x)\nBernoulli( \u03b8) Beta( \u03b1, \u03b2) Beta( \u03b1+S, \u03b2+N\u2212S)\nPoisson( \u03b8) Gamma( \u03b1, \u03b2) Gamma\u0010\n\u03b1+S,\u03b2\n1+N\u0011\nExponential( \u03b8) Gamma( \u03b1, \u03b2) Gamma\u0010\n\u03b1+N,\u03b2\n1+\u03b2S\u0011\nGaussian( \u03b8, \u03c32) Gaussian( \u00b50, \u03c32\n0) Gaussian\u0010\n\u00b50/\u03c32\n0+S/\u03c32\n1/\u03c32\n0+N/\u03c32\u0011\nGaussian( \u00b5, \u03b82) Inv. Gamma( \u03b1, \u03b2) Gamma\u0010\n\u03b1+N\n2, \u03b2+1\n2PN\nn=1(xn\u2212\u00b5)2\u0011\nTable 8.1: Commonly used conjugate priors.\n8.3.7 Linking MAP with regression\nML and regression represent the statistics and the optimization aspects of the same problem.\nWith the parallel argument, MAP is linked to the regularized regression. The reason follows\nimmediately from the definition of MAP:\nb\u03b8MAP = argmax\n\u03b8logfX|\u0398(x|\u03b8)|{z }\ndata fidelity+ log f\u0398(\u03b8)|{z}\nregularization.\n517", "533": "CHAPTER 8. ESTIMATION\nTo make this more explicit, we consider following linear regression problem:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0y1\ny2\n...\nyN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=y=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03d50(x1)\u03d51(x1)\u00b7\u00b7\u00b7 \u03d5d\u22121(x1)\n\u03d50(x2)\u03d51(x2)\u00b7\u00b7\u00b7 \u03d5d\u22121(x2)\n... \u00b7\u00b7\u00b7......\n\u03d50(xN)\u03d51(xN)\u00b7\u00b7\u00b7\u03d5d\u22121(xN)\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n| {z }\n=X\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\u03b80\n\u03b81\n...\n\u03b8d\u22121\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=\u03b8+\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0e1\ne2\n...\neN\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n|{z}\n=e.\nIf we assume that e\u223cGaussian(0 , \u03c32I), the likelihood is defined as\nfY|\u0398(y|\u03b8) =1p\n(2\u03c0\u03c32)Nexp\u001a\n\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252\u001b\n. (8.52)\nIn the ML setting, the ML estimate is the maximizer of the likelihood:\nb\u03b8ML= argmax\n\u03b8logfY|\u0398(y|\u03b8)\n= argmax\n\u03b8\u22121\n2\u03c32\u2225y\u2212X\u03b8\u22252.\nFor MAP, we add a prior term so that the optimization becomes\nb\u03b8MAP = argmax\n\u03b8logfY|\u0398(y|\u03b8) + log f\u0398(\u03b8)\n= argmin\n\u03b81\n2\u03c32\u2225y\u2212X\u03b8\u22252\u2212logf\u0398(\u03b8).\nTherefore, the regularization of the regression is exactly \u2212logf\u0398(\u03b8). We can perform reverse\nengineering to find out the corresponding prior for our favorite choices of the regularization.\nRidge regression . Suppose that\nf\u0398(\u03b8) = exp\u001a\n\u2212\u2225\u03b8\u22252\n2\u03c32\n0\u001b\n.\nTaking the negative log on both sides yields\n\u2212logf\u0398(\u03b8) =\u2225\u03b8\u22252\n2\u03c32\n0.\nPutting this into the MAP estimate,\nb\u03b8MAP = argmin\n\u03b81\n2\u03c32\u2225y\u2212X\u03b8\u22252+1\n2\u03c32\n0\u2225\u03b8\u22252\n= argmin\n\u03b8\u2225y\u2212X\u03b8\u22252+\u03c32\n\u03c32\n0|{z}\n=\u03bb\u2225\u03b8\u22252,\nwhere \u03bbis the corresponding ridge regularization parameter. Therefore, the ridge regression\nis equivalent to a MAP estimation using a Gaussian prior.\n518", "534": "8.3. MAXIMUM A POSTERIORI ESTIMATION\nHow is MAP related to ridge regression?\n\u0088In MAP, define the prior as a Gaussian:\nf\u0398(\u03b8) = exp\u001a\n\u2212\u2225\u03b8\u22252\n2\u03c32\n0\u001b\n. (8.53)\n\u0088The prior says that the solution \u03b8is naturally distributed according to a Gaussian\nwith mean zero and variance \u03c32\n0.\nLASSO regression . Suppose that\nf\u0398(\u03b8) = exp\u001a\n\u2212\u2225\u03b8\u22251\n\u03b1\u001b\n.\nTaking the negative log on both sides yields\n\u2212logf\u0398(\u03b8) =\u2225\u03b8\u22251\n\u03b1.\nPutting this into the MAP estimate we can show that\nb\u03b8MAP = argmin\n\u03b81\n2\u03c32\u2225y\u2212X\u03b8\u22252+1\n\u03b1\u2225\u03b8\u22251\n= argmin\n\u03b81\n2\u2225y\u2212X\u03b8\u22252+\u03c32\n\u03b1|{z}\n=\u03bb\u2225\u03b8\u22251.\nTo summarize:\nHow is MAP related to LASSO regression?\n\u0088LASSO is a MAP using the prior\nf\u0398(\u03b8) = exp\u001a\n\u2212\u2225\u03b8\u22251\n\u03b1\u001b\n. (8.54)\nAt this point, you may be wondering what MAP buys us when regularized regression\ncan already do the job. The answer is about the interpretation. While regularized regression\ncan always return us a result, that is just a result. However, if you know that the parameter \u03b8\nis distributed according to some distributions f\u0398(\u03b8), MAP offers a statistical perspective of\nthe solution in the sense that it returns the peak of the posterior f\u0398|X(\u03b8|x). For example, if\nwe know that the data is generated from a linear model with Gaussian noise, and if we know\nthat the true regression coefficients are drawn from a Gaussian, then the ridge regression is\nguaranteed to be optimal in the posterior sense. Similarly, if we know that there are outliers\nand have some ideas about the outlier statistics, perhaps the LASSO regression is a better\nchoice.\nIt is also important to note the different optimalities offered by MAP versus ML versus\nregression. The optimality offered by regression is the training loss, which can always give\nus a result even if the underlying statistics do not match the optimization formulation,\n519", "535": "CHAPTER 8. ESTIMATION\ne.g., there are outliers, and you use unregularized least-squares minimization. You can get a\nresult, but the outliers will heavily influence your solution. On the other hand, if you know\nthe data statistics and choose to follow the ML, then the ML solution is optimal in the sense\nof optimizing the likelihood fX|\u0398(x|\u03b8). If you further know the prior statistics, the MAP\nsolution will be optimal, but this time it is optimal w.r.t. to the posterior f\u0398|X(\u03b8|x). Since\neach of these is optimizing for a different goal, they are only good for their chosen objectives.\nFor example, b\u03b8MAP can be a biased estimate if our goal is to maximize the likelihood. The\nb\u03b8MLis optimal for the likelihood but can be a bad choice for the posterior. Both b\u03b8MAP and\nb\u03b8MLcan possibly achieve a reasonable mean-squared error, but their results may not make\nsense (e.g., if \u03b8is an image then b\u03b8MAP may over-smooth the image whereas b\u03b8MLamplifies\nnoise). So it\u2019s incorrect to think that b\u03b8MAP is superior to b\u03b8MLbecause it is more general.\nHere are some rules of thumb for MAP, ML, and regression:\nWhen should I use regression, ML and MAP?\n\u0088Regression : If you are lazy and you know nothing about the statistics, do the\nregression with whatever regularization you prefer. It will give you a result. See\nif it makes sense with your data.\n\u0088MAP : If you know the statistics of the data, and if you have some preference for\nthe prior distribution, go with MAP. It will offer you the optimal solution w.r.t.\nfinding the peak of the posterior.\n\u0088ML: If you are interested in some simple-form solution, and you want those nice\nproperties such as consistency and unbiasedness, then go with ML. It usually\npossesses the \u201cfriendly\u201d properties so that you can derive the performance limit.\n8.4 Minimum Mean-Square Estimation\nFirst-time readers are often tempted to think that the maximum-likelihood estimation or\nthe maximum a posteriori estimation are thebest methods to estimate parameters. In some\nsense, this is true because both estimation procedures offer some form of optimal explanation\nfor the observed variables. However, as we said above, being optimal with respect to the\nlikelihood or the posterior only means optimal under the respective criteria. An ML estimate\nis not necessarily optimal for the posterior, whereas a MAP estimate is not necessarily\noptimal for the likelihood. Therefore, as we proceed to the third commonly used estimation\nstrategy, we need to remind ourselves of the specific type of optimality we seek.\n8.4.1 Positioning the minimum mean-square estimation\nMean-square error estimation, as it is termed, uses the mean-square error as the optimality\ncriterion. The corresponding estimation process is known as the minimum mean-square\nestimation (MMSE) . MMSE is a Bayesian approach, meaning that it uses the prior f\u0398(\u03b8)\nas well as the likelihood fX|\u0398(x|\u03b8). As we will show shortly, the MMSE estimate of a set\n520", "536": "8.4. MINIMUM MEAN-SQUARE ESTIMATION\nof i.i.d. observation X= [X1, . . . , X N]Tis\nb\u03b8MMSE (x)(a)=E\u0398|X[\u0398|X=x] ( a) : We will discuss this.\n=Z\n\u03b8\u00b7f\u0398|X(\u03b8|x)d\u03b8. (8.55)\nYou may find this equation very surprising, because it says that the MMSE estimate is\nthemean of the posterior distribution f\u0398|X(\u03b8|x). Let\u2019s compare this result with the ML\nestimate and the MAP estimate:\nb\u03b8ML= peak of fX|\u0398(x|\u03b8),\nb\u03b8MAP = peak of f\u0398X|(\u03b8|x),\nb\u03b8MMSE = average of f\u0398|X(\u03b8|x).\nTherefore, an MMSE estimate is not by any means universally superior or inferior to a MAP\nestimate or an ML estimate. It is just a different estimate with a different goal.\nSo how exactly are these estimates different? Figure 8.21 illustrates a typical situation\nof asymmetric distribution. Here, we plot both the likelihood function fX|\u0398(x|\u03b8) and the\nposterior function f\u0398X|(\u03b8|x).\nFigure 8.21: A typical example of an ML estimate, a MAP estimate and an MMSE estimate.\nAs shown in the figure, the ML estimate is the peak of the likelihood, whereas the MAP\nestimate is the peak of the posterior. The third estimate is the MMSE estimate, which is\nthe average of the posterior distribution. It is easy to see that if the posterior distribution\nis symmetric and has a single peak, the peak is always the mean. Therefore, for single-peak\nsymmetric distributions, MMSE and MAP estimates are identical.\nWhat is so special about the MMSE estimate?\n\u0088MMSE is a Bayesian estimation, so it requires a prior.\n\u0088An MMSE estimate is the mean of the posterior distribution.\n\u0088MMSE estimate = MAP estimate if the posterior distribution is symmetric and\nhas a single peak.\n521", "537": "CHAPTER 8. ESTIMATION\n8.4.2 Mean squared error\nThe MMSE is based on minimizing the mean squared error (MSE). In this subsection we\ndiscuss the mean squared error in the Bayesian setting. In the deterministic setting, given\nan estimate b\u03b8and a ground truth \u03b8, the MSE is defined as\nMSE( \u03b8|{z}\nground truth,b\u03b8|{z}\nestimate) = (\u03b8\u2212b\u03b8)2. (8.56)\nIn any estimation problem, the estimate b\u03b8is always a function of the observed variables.\nThus, we have\nb\u03b8(X) =g(X), where X= [X1, . . . , X N]T,\nfor some function g(\u00b7). Substituting this into the definition of MSE, and recognizing that X\nis drawn from a distribution fX(x), we take the expectation to define the MSE as\nMSE( \u03b8,b\u03b8) = (\u03b8\u2212b\u03b8)2\n\u21d3replace b\u03b8byg(X)\nMSE( \u03b8,b\u03b8) = (\u03b8\u2212g(X))2\n\u21d3take expectation over X\nMSE( \u03b8,b\u03b8) =EX\u0002\n(\u03b8\u2212g(X))2\u0003\n.\nThus we have arrived at the definition of MSE. We call this the frequentist version, because\nthe parameter \u03b8is deterministic.\nDefinition 8.8 (Mean squared error, frequentist ).The mean squared error of an\nestimate g(X)w.r.t. the true parameter \u03b8is\nMSE freq(\u03b8, g(\u00b7)) =EX\u0002\n(\u03b8\u2212g(X))2\u0003\n. (8.57)\nIf the parameter \u03b8is high-dimensional, so is the estimate g(X), and the MSE is\nMSE freq(\u03b8,g(\u00b7)) =EX\u0002\n\u2225\u03b8\u2212g(X)\u22252\u0003\n. (8.58)\nNote that in the above definition the MSE is measured between the true parameter \u03b8and\nthe estimator g(\u00b7). We use the function g(\u00b7) here because we have taken the expectation\nof all the possible inputs X. So we are not comparing \u03b8with a value g(X) but with the\nfunction g(\u00b7).\nIf we take a Bayesian approach such as the MAP, then \u03b8itself is a random variable \u0398.\nTo compute the MSE, we then need to take the average across all the possible choices of\nground truth \u0398. This leads to\nMSE( \u03b8,b\u03b8) =EX\u0002\n(\u03b8\u2212g(X))2\u0003\n\u21d3replace \u03b8by \u0398\nMSE( \u03b8,b\u03b8) =EX\u0002\n(\u0398\u2212g(X))2\u0003\n\u21d3take expectation over \u0398\nMSE( \u03b8,b\u03b8) =EX,\u0398\u0002\n(\u0398\u2212g(X))2\u0003\n.\nTherefore, we have arrived at our definition of the MSE, in the Bayesian setting.\n522", "538": "8.4. MINIMUM MEAN-SQUARE ESTIMATION\nDefinition 8.9 (Mean squared error, Bayesian ).The mean squared error of an es-\ntimate g(X)w.r.t. the true parameter \u0398is\nMSE Bayes (\u0398, g(\u00b7)) =E\u0398,X\u0002\n(\u0398\u2212g(X))2\u0003\n. (8.59)\nIf the parameter \u0398is high-dimensional, so is the estimate g(X), and the MSE is\nMSE Bayes (\u0398,g(\u00b7)) =E\u0398,X\u0002\n\u2225\u0398\u2212g(X)\u22252\u0003\n. (8.60)\nThe difference between the Bayesian MSE and the frequentist MSE is the expectation over \u0398.\nPractically speaking, the frequentist MSE is more of an evaluation metric than an objective\nfunction for solving an inverse problem. The reason is that in an inverse problem, we never\nhave access to the true parameter \u03b8. (If we knew \u03b8, there would be no problem to solve.)\nBayesian MSE is more meaningful. It says that we do not know the true parameter \u03b8, but\nwe know its statistics. We are trying to find the best g(\u00b7) that minimizes the error. Our\nsolution will depend on the statistics of \u0398 but not on the unknown true parameter \u03b8.\nWhen we say minimum mean squared error estimation, we typically refer to the\nBayesian MMSE. In this case, the problem we solve is\ng(\u00b7) = argmin\ng(\u00b7)E\u0398,X\u0002\n(\u0398\u2212g(X))2\u0003\n. (8.61)\nAs you can see from Definition 8.9, the goal of the Bayesian MMSE is to find a function\ng:RN\u2192Rsuch that the joint expectation E\u0398,X\u0002\n(\u0398\u2212g(X))2\u0003\nis minimized. In the case\nwhere \u0398is a vector, the problem becomes\ng(\u00b7) = argmin\ng(\u00b7)E\u0398,X\u0002\n\u2225\u0398\u2212g(X)\u22252\u0003\n, (8.62)\nwhere g(\u00b7) :RN\u00d7d\u2192Rdif\u0398is ad-dimensional vector. The function gwill take a sequence\nofNobserved numbers and estimate the parameter \u0398.\nWhat is the Bayesian MMSE estimate?\nThe Bayesian MMSE estimate is obtained by minimizing the MSE:\ng(\u00b7) = argmin\ng(\u00b7)E\u0398,X\u0002\n(\u0398\u2212g(X))2\u0003\n. (8.63)\n8.4.3 MMSE estimate = conditional expectation\nTheorem 8.2. TheBayesian MMSE estimate is\nb\u03b8MMSE =argmin\ng(\u00b7)E\u0398,X\u0002\n(\u0398\u2212g(X))2\u0003\n=E\u0398|X[\u0398|X=x]. (8.64)\n523", "539": "CHAPTER 8. ESTIMATION\nProof . First of all, we decompose the joint expectation:\nE\u0398,X\u0002\n(\u0398\u2212g(X))2\u0003\n=Z\nE\u0398|X\u0002\n(\u0398\u2212g(X))2|X=x\u0003\nfX(x)dx.\nSince fX(x)\u22650 for all x, andE\u0398|X\u0002\n(\u0398\u2212g(X))2|X=x\u0003\n\u22650 because it is a square, it\nfollows that the integral is minimized when E\u0398|X\u0002\n(\u0398\u2212g(X))2|X=x\u0003\nis minimized.\nThe conditional expectation can be evaluated as\nE\u0398|X[(\u0398\u2212g(X))2|X=x]\n=E\u0398|X\u0014\n\u03982\u22122\u0398g(X) +g(X)2\f\f\f\fX=x\u0015\n=E\u0398|X\u0014\n\u03982\f\f\fX=x\u0015\n| {z }\ndef=V(x)\u22122E\u0398|X\u0014\n\u0398\f\f\fX=x\u0015\n| {z }\ndef=u(x)g(x) +g(x)2\n=V(x)\u22122u(x)g(x) +g(x)2+u(x)2\u2212u(x)2\n=V(x)\u2212u(x)2+ (u(x)\u2212g(x))2\n\u2265V(x)\u2212u(x)2,\u2200g(\u00b7),\nwhere the last inequality holds because no matter what g(\u00b7) we choose, the square term\n(u(x)\u2212g(x))2is non-negative. Therefore, E\u0398|X[(\u0398\u2212g(X))2|X=x] is lower-bounded by\nV(x)\u2212u(x)2, which is a bound that is independent of g(\u00b7). If we can find a g(\u00b7) such that\nthis lower bound can be met, the corresponding g(\u00b7) is the minimizer.\nTo this end we only need to make E\u0398|X[(\u0398\u2212g(X))2|X=x] equal V(x)\u2212u(x)2,\nbut this is easy: the equality holds if and only if ( u(x)\u2212g(x))2= 0. In other words, if we\nchoose g(\u00b7) such that g(x) =u(x), the corresponding g(\u00b7) is the minimizer. This g(\u00b7), by\nsubstituting the definition of u(x), is\ng(x) =E\u0398|X\u0014\n\u0398\f\f\fX=x\u0015\n. (8.65)\nThis completes the proof.\n\u25a1\nWhat is the MMSE estimate?\nThe MMSE estimate is\nb\u03b8MMSE (x) =E\u0398|X[\u0398|X=x]. (8.66)\nWe emphasize that b\u03b8MMSE (x) is a function of x, because for a different set of observations\nxwe will have a different estimated value. Since xis a random realization of the random\nvector X, we can also define the MMSE estimator as\nb\u0398MMSE (X) =E\u0398|X[\u0398|X]. (8.67)\nIn this notation, we emphasize that the estimator b\u0398MMSE returns a random parameter. The\ninput to the estimator is the random vector X. Because we are not looking at a particular\nrealization X=xbut the general X,b\u0398MMSE is a function of Xand not x.\n524", "540": "8.4. MINIMUM MEAN-SQUARE ESTIMATION\nConditional expectation of what?\nAn MMSE estimator is the conditional expectation of \u0398 given X=x:\nE\u0398|X\u0014\n\u0398\f\f\fX=x\u0015\n=Z\n\u03b8 f\u0398|X(\u03b8|x)d\u03b8. (8.68)\nThis is the expectation using the posterior distribution f\u0398|X(\u03b8|x). It should be compared\nto the peak of the posterior, which returns us the MAP estimate. The posterior distribution\nis constructed through Bayes\u2019 theorem:\nf\u0398|X(\u03b8|x) =fX|\u0398(x|\u03b8)f\u0398(\u03b8)\nfX(x). (8.69)\nTherefore, to evaluate the expectation of the condition distribution, we need to include the\nnormalization constant fX(x), which was omitted in MAP.\nThe discussion about the mean squared error and the vector estimates can be skipped if\nthis is your first time reading the book.\nWhat is the mean squared error when using the MMSE estimator?\n\u0088The mean squared error conditioned on the observation is\nMSE(\u0398 ,b\u0398MMSE (X))def=E\u0398|X[(\u0398\u2212b\u0398MMSE (X))2|X]\n= Var \u0398|X[\u0398|X],\nwhich is the conditional variance.\n\u0088The overall mean squared error, unconditioned, is\nMSE(\u0398 ,b\u0398MMSE (\u00b7)) =EX\u0002\nVar\u0398|X[\u0398|X]\u0003\n= Var \u0398[\u0398].\nProof . Let us prove these two statements. The resulting MSE is obtained by substituting\nb\u0398MMSE (x) =E\u0398|X\u0002\n\u0398\f\fX\u0003\ninto the MSE(\u0398 ,b\u0398MMSE (X)). To this end, we have that\nE\u0398|X[(\u0398\u2212b\u0398MMSE (X))2|X] =V(X)\u2212u(X)2\n+ ( u(X)\u2212b\u0398MMSE (X))2\n| {z }\n=0,because b\u0398MMSE (X)=E\u0398|X[\u0398|X]=u(X).\nThe variables Vanduare defined as\nV(X) =E\u0398|X\u0002\n\u03982\f\f\fX\u0003\n= 2nd moment of \u0398 using f\u0398|X(\u03b8|x),\nu(X) =E\u0398|X\u0002\n\u0398\f\fX\u0003\n= 1st moment of \u0398 using f\u0398|X(\u03b8|x).\n525", "541": "CHAPTER 8. ESTIMATION\nSince Var[ Z] =E[Z2]\u2212E[Z]2for any random variable Z, it follows that\nE\u0398|X[(\u0398\u2212b\u0398MMSE (X))2|X] =V(X)\u2212u(X)2\n=E\u0398|X\u0002\n\u03982\f\f\fX\u0003\n\u2212\u0000\nE\u0398|X\u0002\n\u0398\f\fX\u0003\u00012\n= variance of \u0398 using f\u0398|X(\u03b8|x)\ndef= Var \u0398|X[\u0398|X].\nSubstituting this conditional variance into the MSE definition,\nMSE(\u0398 ,b\u0398MMSE (\u00b7)) =Z\nE\u0398|X[(\u0398\u2212b\u0398MMSE (X))2|X=x]fX(x)dx\n=Z\nVar\u0398|X[\u0398|X=x]fX(x)dx\n= Var \u0398[\u0398].\n\u25a1\nWhat happens if the parameter is a vector?\n\u0088The MMSE estimate is b\u03b8MMSE (x) =E\u0398|X[\u0398|X=x].\n\u0088The MSE is\nMSE( \u0398,b\u0398MMSE (\u00b7)) = Tr\u001a\nEXn\nCov(\u0398|X)o\u001b\n. (8.70)\nProof . The first statement, that the MMSE estimate is\nb\u03b8MMSE (x) =E\u0398|X[\u0398|X=x],\nis easy to understand since it just follows from the scalar case. The estimator is b\u0398MMSE (X) =\nE\u0398|X[\u0398|X]. The corresponding MSE is\nMSE( \u0398,b\u0398MMSE (\u00b7)) =E\u0398,X[\u2225\u0398\u2212b\u0398MMSE (X)\u22252]\n=EX\u001a\nE\u0398|X[\u2225\u0398\u2212b\u0398MMSE (X)\u22252|X]\u001b\n,\nwhere we have used the law of total expectation to decompose the joint expectation. Using\nthe matrix identity below, we have that\nEX\u001a\nE\u0398|X[\u2225\u0398\u2212b\u0398MMSE (X)\u22252|X]\u001b\n=EX\u001a\nE\u0398|Xh\nTrn\n(\u0398\u2212b\u0398MMSE (X))(\u0398\u2212b\u0398MMSE (X))To\n|Xi\u001b\n= Tr\u001a\nEX\u001a\nE\u0398|Xh\n(\u0398\u2212b\u0398MMSE (X))(\u0398\u2212b\u0398MMSE (X))T|Xi\u001b\u001b\n.\n526", "542": "8.4. MINIMUM MEAN-SQUARE ESTIMATION\nHowever, since the MMSE estimator is the condition expectation of the posterior, it follows\nthat the inner expectation is the conditional covariance. Therefore, we arrive at the second\nstatement:\nMSE( \u0398,b\u0398MMSE (\u00b7)) = Tr\u001a\nEX\u001a\nE\u0398|Xh\n(\u0398\u2212b\u0398MMSE (X))(\u0398\u2212b\u0398MMSE (X))T|Xi\u001b\u001b\n= Tr\u001a\nEXn\nCov(\u0398|X)o\u001b\n.\n\u25a1\nTo prove the two statements above, we need some tools from linear algebra. The two\nspecific matrix identities are given by the following lemma:\nLemma 8.1. The following are matrix identities:\n\u0088For any random vector \u0398\u2208Rd,\n\u2225\u0398\u22252=Tr(\u0398T\u0398) =Tr(\u0398\u0398T).\n\u0088For any random vector \u0398\u2208Rd,\nE\u0398[Tr(\u0398\u0398T)] =Tr(E\u0398[\u0398\u0398T]).\nThe proof of these two results is straightforward. The first is due to the cyclic property of\nthe trace operator. The second statement is true because the trace is a linear operator that\nsums the diagonal of a matrix.\nThe end of the discussion. Please join us again.\nExample 8.22 . Let\nfX|\u0398(x|\u03b8) =(\n\u03b8e\u2212\u03b8x, x \u22650,\n0, x < 0,and f\u0398(\u03b8) =(\n\u03b1e\u2212\u03b1\u03b8, \u03b8 \u22650,\n0, \u03b8 < 0.\nFind the ML, MAP, and MMSE estimates for a single observation X=x.\nSolution . We first find the posterior distribution:\nf\u0398|X(\u03b8|x) =fX|\u0398(x|\u03b8)f\u0398(\u03b8)\nfX(x)\n=\u03b1\u03b8e\u2212(\u03b1+x)\u03b8\nR\u221e\n0\u03b1\u03b8e\u2212(\u03b1+x)\u03b8d\u03b8\n=\u03b1\u03b8e\u2212(\u03b1+x)\u03b8\n\u03b1\n(\u03b1+x)2\n= (\u03b1+x)2\u03b8e\u2212(\u03b1+x)\u03b8.\n527", "543": "CHAPTER 8. ESTIMATION\nThe MMSE estimate is the conditional expectation of the posterior:\nb\u03b8MMSE (x) =E\u0398|X[\u0398|X=x]\n=Z\u221e\n0\u03b8f\u0398|X(\u03b8|x)d\u03b8\n=Z\u221e\n0\u03b8(\u03b1+x)2\u03b8e\u2212(\u03b1+x)\u03b8d\u03b8\n= (\u03b1+x)Z\u221e\n0\u03b82\u00b7(\u03b1+x)e\u2212(\u03b1+x)\u03b8d\u03b8\n| {z }\n2nd moment of exponential distribution\n= (\u03b1+x)\u00b72\n(\u03b1+x)2=2\n\u03b1+x.\nThe MAP estimate is the peak of the posterior:\nb\u03b8MAP(x) = argmax\n\u03b8logfX|\u0398(x|\u03b8) + log f\u0398(\u03b8)\n= argmax\n\u03b8\u2212\u03b8x+ log \u03b8\u2212\u03b1\u03b8+ log \u03b1.\nTaking the derivative and setting it to zero yields \u2212x+1\n\u03b8\u2212\u03b1= 0. This implies that\nb\u03b8MAP(x) =1\n\u03b1+x.\nFinally, the ML estimate is\nb\u03b8ML(x) = argmax\n\u03b8logfX|\u0398(x|\u03b8) =1\nx.\nPractice Exercise 8.8 . Following the previous example, derive the estimates for\nmultiple observations X=x.\nSolution . The posterior is\nf\u0398|X(\u03b8|x) =fX|\u0398(x|\u03b8)f\u0398(\u03b8)\nfX(x)\n=(QN\nn=1fX|\u0398(xn|\u03b8))f\u0398(\u03b8)\nfX(x)\n=\u03b1\u03b8e\u2212(\u03b1+PN\nn=1xn)\u03b8\nR\u221e\n0\u03b1\u03b8e\u2212(\u03b1+PN\nn=1xn)\u03b8d\u03b8\n= \n\u03b1+NX\nn=1xn!2\n\u03b8e\u2212(\u03b1+PN\nn=1xn)\u03b8.\n528", "544": "8.4. MINIMUM MEAN-SQUARE ESTIMATION\nTherefore, we are only replacing xby the sumPN\nn=1xnin the posterior. Hence, the\nestimates are:\nb\u03b8MMSE (x) =2\n\u03b1+PN\nn=1xn,\nb\u03b8MAP(x) =1\n\u03b1+PN\nn=1xn,\nb\u03b8ML(x) =1PN\nn=1xn.\nThis example shows that as N\u2192 \u221e , the ML estimate b\u03b8ML(x)\u21920. The reason is that the\nlikelihood is an exponential distribution. Therefore, the peak is always at 0. The posterior is\nan Erlang distribution, and therefore the peak is offset by \u03b1in the denominator. However,\nasN\u2192 \u221e the posterior distribution is dominated by the likelihood, so the peak is shifted\ntowards 0. Finally, since the Erlang distribution is asymmetric, the mean is different from\nthe peak. Hence, the MMSE estimate is different from the MAP estimate.\n8.4.4 MMSE estimator for multidimensional Gaussian\nThe multidimensional Gaussian has some very important uses in data science. Accordingly,\nwe devote this subsection to the discussion of the MMSE estimate of a Gaussian. The main\nresult is stated as follows.\nWhat is the MMSE estimator for a multi-dimensional Gaussian?\nTheorem 8.3. Suppose \u0398\u2208RdandX\u2208RNare jointly Gaussian with a joint PDF\n\u0014\u0398\nX\u0015\n\u223cGaussian\u0012\u0014\u00b5\u0398\n\u00b5X\u0015\n,\u0014\u03a3\u0398\u0398\u03a3\u0398X\n\u03a3X\u0398\u03a3XX\u0015\u0013\n.\nThe MMSE estimator is\nb\u0398MMSE (X) =\u00b5\u0398+\u03a3\u0398X\u03a3\u22121\nXX(X\u2212\u00b5X). (8.71)\nThe proof of this result is not difficult but it is tedious. The flow of the argument is:\n\u0088Step 1: Show that the posterior distribution f\u0398|X(\u03b8|x) is a Gaussian.\n\u0088Step 2: To do so we need to complete the squares for matrices.\n\u0088Step 3: Once we have the f\u0398|X(\u03b8|x), the posterior mean is the MMSE estimator.\nThe proof below can be skipped if this is your first time reading the book.\n529", "545": "CHAPTER 8. ESTIMATION\nProof . The posterior PDF is\nf\u0398|X(\u03b8|x) =f\u0398,X(\u03b8,x)\nfX(x)\n=1\u221a\n(2\u03c0)d+N|\u03a3|exp(\n\u22121\n2\u0014\u03b8\u2212\u00b5\u0398\nx\u2212\u00b5X\u0015T\u0014\u03a3\u0398\u0398\u03a3\u0398X\n\u03a3X\u0398\u03a3XX\u0015\u22121\u0014\u03b8\u2212\u00b5\u0398\nx\u2212\u00b5X\u0015)\n1\u221a\n(2\u03c0)N|\u03a3XX|expn\n\u22121\n2\u0002x\u2212\u00b5X\u0003T\u03a3\u22121\nXX\u0002x\u2212\u00b5X\u0003o .\nWithout loss of generality, we assume that \u00b5X=\u00b5\u0398= 0. Then the posterior becomes\nf\u0398|X(\u03b8|x) =1p\n(2\u03c0)d|\u03a3|/|\u03a3XX|\n\u00d7exp(\n\u22121\n2\u0014\u03b8\nx\u0015T\u0014\u03a3\u0398\u0398\u03a3\u0398X\n\u03a3X\u0398\u03a3XX\u0015\u22121\u0014\u03b8\nx\u0015\n+1\n2xT\u03a3\u22121\nXXx)\n| {z }\nH(\u03b8,x).\nThe tedious task here is to simplify H(\u03b8,x).\nRegardless of what the 2-by-2 matrix inverse is, the matrix will take the form\n\u0014\u03a3\u0398\u0398\u03a3\u0398X\n\u03a3X\u0398\u03a3XX\u0015\u22121\n=\u0014A B\nC D\u0015\n,\nfor some choices of matrices A,B,CandD. Therefore, the function H(\u03b8,x) can be written\nas\nH(\u03b8,x) =\u22121\n2n\n\u03b8TA\u03b8+\u03b8TBx+xTC\u03b8+xTDx\u2212xT\u03a3\u22121\nXXxo\n. (8.72)\nOur goal is to complete the square for H(\u03b8,x). To this end, we propose to write\nH(\u03b8,x) =\u22121\n2n\n(\u03b8\u2212Gx)TA(\u03b8\u2212Gx) +Q(x)o\n, (8.73)\nfor some matrix Gand function Q(\u00b7) ofxonly. If we compare Equation (8.72) and Equa-\ntion (8.73), we observe that Gmust satisfy\nG=\u2212A\u22121B.\nTherefore, if we can determine AandB, we will know G. If we know G, we have completed\nthe square for H(\u03b8,x). If we can complete the square for H(\u03b8,x), we can write\nf\u0398|X(\u03b8|x) =exp{\u2212Q(x)/2}p\n(2\u03c0)d|\u03a3|/|\u03a3XX|| {z }\nconstant in \u03b8\u00d7exp\u001a\n\u22121\n2(\u03b8\u2212Gx)TA(\u03b8\u2212Gx)\u001b\n| {z }\na Gaussian.\nHence, the MMSE estimate, which is the posterior mean E[\u0398|X=x], is simply Gx:\nb\u03b8MMSE (x) =E[\u0398|X=x]\n=Gx\n=\u2212A\u22121Bx.\n530", "546": "8.4. MINIMUM MEAN-SQUARE ESTIMATION\nSo it remains to determine AandBby solving the tedious matrix inversion problem. The\nresult is:6\nA= (\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121,\nB=\u2212(\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121\u03a3\u0398X\u03a3\u22121\nXX,\nC= (\u03a3XX\u2212\u03a3X\u0398\u03a3\u22121\n\u0398\u0398\u03a3\u0398X)\u22121\u03a3X\u0398\u03a3\u22121\n\u0398\u0398,\nD= (\u03a3XX\u2212\u03a3X\u0398\u03a3\u22121\n\u0398\u0398\u03a3\u0398X)\u22121.\nTherefore, plugging everything into the equation,\nb\u03b8MMSE (x) =\u2212A\u22121Bx\n=\u03a3\u0398,X\u03a3\u22121\nXXx.\nFor non-zero means, we can repeat the same arguments above and show that\nb\u03b8MMSE (x) =\u00b5\u0398+\u03a3\u0398,X\u03a3\u22121\nXX(x\u2212\u00b5X).\n\u25a1\nEnd of the proof. Please join us again.\nPractice Exercise 8.9 . Suppose \u0398\u2208RdandX\u2208RNare jointly Gaussian with a\njoint PDF\u0014\u0398\nX\u0015\n\u223cGaussian\u0012\u0014\u00b5\u0398\n\u00b5X\u0015\n,\u0014\u03a3\u0398\u0398\u03a3\u0398X\n\u03a3X\u0398\u03a3XX\u0015\u0013\n.\nWe know that the MMSE estimator is\nb\u0398MMSE (X) =\u00b5\u0398+\u03a3\u0398X\u03a3\u22121\nXX(X\u2212\u00b5X). (8.74)\nFind the mean squared error when using the MMSE estimator.\nSolution . Conditioned on X=x, according to Equation (8.70), the MMSE is\nMSE( \u0398,b\u0398(X)) = Tr {Cov[\u0398|X]}.\nThe conditional covariance Cov[ \u0398|X] is the covariance of the posterior distribution\nf\u0398|X(\u03b8|x), which is\nTr{Cov[\u0398|X]}= Tr{A}\n= Tr\b\n(\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121\t\n.\n6See Matrix Cookbook https://www.math.uwaterloo.ca/ ~hwolkowi/matrixcookbook.pdf Section 9.1.5\non the Schur complement.\n531", "547": "CHAPTER 8. ESTIMATION\nThe overall mean squared error is\nMSE\u0010\n\u0398,b\u0398(\u00b7)\u0011\n=EX\u0014\nMSE( \u0398,b\u0398(X))\u0015\n=Z\nMSE( \u0398,b\u0398(x))fX(x)dx\n=Z\nTr{Cov[\u0398|X]}fX(x)dx\n=Z\nTr\b\n(\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121\t\nfX(x)dx\n= Tr\b\n(\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121\tZ\nfX(x)dx\n= Tr\b\n(\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121\t\n.\nFor multidimensional Gaussian, does MMSE = MAP?\nThe answer is YES.\nTheorem 8.4. Suppose \u0398\u2208RdandX\u2208RNare jointly Gaussian with a joint PDF\n\u0014\u0398\nX\u0015\n\u223cGaussian\u0012\u0014\u00b5\u0398\n\u00b5X\u0015\n,\u0014\u03a3\u0398\u0398\u03a3\u0398X\n\u03a3X\u0398\u03a3XX\u0015\u0013\n.\nThe MAP estimate is\nb\u0398MAP(X) =\u00b5\u0398+\u03a3\u0398X\u03a3\u22121\nXX(X\u2212\u00b5X). (8.75)\nProof . The proof of this result is straightforward. If we return to the proof of the MMSE\nresult, we note that\nf\u0398|X(\u03b8|x) =exp{\u2212Q(x)/2}p\n(2\u03c0)d|\u03a3|/|\u03a3XX|| {z }\nconstant in \u03b8\u00d7exp\u001a\n\u22121\n2(\u03b8\u2212Gx)TA(\u03b8\u2212Gx)\u001b\n| {z }\na Gaussian.\nTherefore, the maximizer of this posterior distribution, which is the MAP estimate, is\nb\u03b8MAP(x) = argmax\n\u03b8f\u0398|X(\u03b8|x)\n= argmax\n\u03b8\u22121\n2(\u03b8\u2212Gx)TA(\u03b8\u2212Gx).\nTaking the derivative w.r.t. \u03b8and setting it zero, we have\nb\u03b8MAP(x) =Gx=\u03a3\u0398,X\u03a3\u22121\nXXx.\nIf the mean vectors are non-zero, we have b\u03b8MAP(x) =\u00b5\u0398+\u03a3\u0398X\u03a3\u22121\nXX(x\u2212\u00b5X).\n\u25a1\n532", "548": "8.4. MINIMUM MEAN-SQUARE ESTIMATION\n8.4.5 Linking MMSE and neural networks\nThe blossoming of deep neural networks since 2010 has created a substantial impact on\nmodern data science. The basic idea of a neural network is to train a stack of matrices and\nnonlinear functions (known as the network weights and the neuron activation functions,\nrespectively), among other innovative ideas, so that a certain training loss is minimized.\nExpressing this by equations, the goal of the learning is equivalent to solving the optimization\nproblem\nbg(\u00b7) = argmin\ng(\u00b7)EX,\u0398\u0014\n\u2225\u0398\u2212g(X)\u22252\u0015\n, (8.76)\nwhere X\u2208RMis the input data and \u0398\u2208Rdis the ground truth prediction. We want to\nfindg(\u00b7) such that the error is minimized.\nThe error we choose here is the \u21132-norm error \u2225 \u00b7 \u22252. It is only one of many possi-\nble choices. You may recognize that this is exactly the same as the MMSE optimization.\nTherefore, the neural network we are finding here is the MMSE estimator. Since the MMSE\nestimator is the conditional expectation of the posterior distribution, the neural network\napproximates the mean of the posterior distribution.\nOften the struggle we have with deep neural networks is whether we can find the\noptimal network parameters via optimization algorithms such as the stochastic gradient\ndescent algorithms. However, if we think about this problem more deeply, the equivalence\nbetween the MMSE estimator and the posterior mean tells us that the hard part is related\nto the posterior distribution. In the high-dimensional landscape, it is close to impossible to\ndetermine the posterior and its mean. If we add to these difficulties and the nonconvexity\nof the function g, training a network is very challenging.\nOne misconception about neural networks is that if we can achieve a low training error,\nand if the model can also achieve a low testing error, then the network is good. This is a false\nsense of satisfaction. If a model can achieve very good training and testing errors, then the\nmodel is only good with respect to the error you choose. For example, if we choose the \u21132-\nnorm error \u2225\u00b7\u22252and if our model achieves good training and testing errors (in terms of \u2225\u00b7\u22252),\nwe can conclude that the model does well with respect to \u2225 \u00b7 \u22252. The more serious problem\nhere, unfortunately, is that \u2225 \u00b7 \u22252is not necessarily a good metric of performance (for both\ntraining and testing) because training with \u2225\u00b7\u22252is equivalent to approximating the posterior\nmean. There is absolutely no reason to believe that in the high-dimensional landscape, the\nposterior mean is theoptimal. If we choose the posterior mode or the posterior median ,\nwe will also obtain a result. Why are the modes and medians \u201cworse\u201d than the mean ?\nIn practice, it has been observed that training deep neural networks for image-processing\ntasks generally leads to over-smoothed images. This demonstrates how minimizing the mean\nsquared error \u2225 \u00b7 \u22252can be a fundamental mismatch with the problem.\nIs minimizing the MSE the best option?\n\u0088No. Minimizing the MSE is equivalent to finding the mean of the posterior. There\nis no reason why the mean is the \u201cbest\u201d.\n\u0088You can find the mode of the posterior, in which case you will get a MAP\nestimator.\n\u0088You can also find the median of the posterior, in which case you will get the\nminimum absolute error estimator.\n533", "549": "CHAPTER 8. ESTIMATION\n\u0088Ultimately, you need to define what is \u201cgood\u201d and what is \u201cbad\u201d.\n\u0088The same principle applies to deep neural networks. Especially in the regression\nsetting, why is \u2225 \u00b7 \u22252a good evaluation metric for testing (not just training)?\n8.5 Summary\nIn this chapter, we have discussed the basic principles of parameter estimation. The three\nbuilding blocks are:\n\u0088Likelihood fX|\u0398(x|\u03b8): the PDF that we observe samples Xconditioned on the un-\nknown parameter \u0398. In the frequentist world, \u0398is a deterministic quantity. In the\nBayesian world, \u0398is random and so it has a PDF.\n\u0088Prior f\u0398(\u03b8): the PDF of \u0398. The prior f\u0398(\u03b8) is used by all Bayesian computation.\n\u0088Posterior f\u0398|X(\u03b8|x): the PDF that the underlying parameter is \u0398=\u03b8given that we\nhave observed X=x.\nThe three building blocks give us several strategies to estimate the parameters:\n\u0088Maximum likelihood (ML) estimation: Maximize fX|\u0398(x|\u03b8).\n\u0088Maximum a posteriori (MAP) estimation: Maximize f\u0398|X(\u03b8|x).\n\u0088Minimum mean-square estimation (MMSE): Minimize the mean squared error, which\nis equivalent to finding the mean of f\u0398|X(\u03b8|x).\nAs discussed in this chapter, no single estimation strategy is universally \u201cbetter\u201d because\none needs to specify the optimality criterion. If the goal is to minimize the mean squared\nerror, then the MMSE estimator is the optimal strategy. If the goal is to maximize the\nlikelihood without assuming any prior knowledge, the ML estimator would be the optimal\nstrategy. It may appear that if we knew the ground truth parameter \u03b8\u2217we could minimize\nthe distance between the estimated parameter \u03b8and the true value \u03b8\u2217. If the parameter\nis a scalar, this will work. However, if the parameter is a vector, the noise of the distance\nbecomes an issue. For example, if one cares about the mean absolute error (MAE), the\noptimal estimator would be the median of the posterior distribution instead of the mean of\nthe posterior in the MMSE case. Therefore, it is the end user\u2019s responsibility to specify the\noptimality criterion.\nWhenever we consider parameter estimation, we tend to think that it is about estimat-\ning the model parameters, such as the mean of a Gaussian PDF. While in many statistics\nproblems this is indeed the case, parameter estimation can be much broader if we link it\nwith regression. Specifically, a regularized linear regression problem can be formulated as a\nMAP estimation\n\u03b8\u2217= argmax\n\u03b8\u2225X\u03b8\u2212y\u22252\n|{z}\n\u2212logfX|\u0398(x|\u03b8)+\u03bbR(\u03b8)|{z}\n\u2212logf\u0398(\u03b8), (8.77)\nfor some regularization R(\u03b8), which is also the negative log of the prior. Expressed in this\nway, we recognize that the MAP estimation can be used to recover signals. For example, we\n534", "550": "8.6. REFERENCES\ncan model Xas a linear degradation process of certain imaging systems. Then solving the\nMAP estimation is equivalent to finding the best signal explaining the degraded observation\nusing the posterior as the criterion. There is rich literature dealing with solving MAP esti-\nmation problems similar to these in subjects such as computational imaging, communication\nsystems, remote sensing, radar engineering, and recommendation systems, to name a few.\n8.6 References\nBasic\n8-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 8 and Chapter 9.\n8-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 6 and Chapter 8.\n8-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 8.\n8-4 Henry Stark and John W. Woods, Probability and Random Processes with Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2002. Chapter 5.\n8-5 Todd K. Moon and Wynn C. Stirling, Mathematical Methods and Algorithms for Signal\nProcessing , Prentice-Hall, 2000. Chapter 12.\nTheoretical analysis\n8-6 H. Vincent Poor, An Introduction Signal Detection and Estimation , Springer, 1998.\n8-7 Steven M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory ,\nPrentice-Hall, 1993.\n8-8 Bernard C. Levy, Principles of Signal Detection and Parameter Estimation , Springer,\n2008.\n8-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 2001. Chapter 8.\n8-10 Larry Wasserman, All of Statistics: A Concise Course in Statistical Inference , Springer,\n2010.\n8-11 Erich L. Lehmann, Elements of Large-Sample Theory , Springer, 1999. Chapter 7.\n8-12 George Casella and Roger L. Berger Statistical Inference , Duxbury, 2002. Chapter 7.\n535", "551": "CHAPTER 8. ESTIMATION\nMachine-learning\n8-13 Christopher Bishop, Pattern Recognition and Machine Learning , Springer, 2006. Chap-\nter 2 and Chapter 3.\n8-14 Richard O. Duda, Peter E. Hart and David G. Stork, Pattern Classification , Wiley\n2001. Chapter 3.\n8.7 Problems\nExercise 1.\nLetX1, . . . , X Nbe a sequence of i.i.d. Bernoulli random variables with P[Xn= 1] = \u03b8.\nSuppose that we have observed x1, . . . , x N.\n(a) Show that the PMF of XnispXn(xn|\u03b8) =\u03b8xn(1\u2212\u03b8)1\u2212xn. Find the joint PMF\npX1,...,X N(x1, . . . , x N).\n(b) Find the maximum likelihood estimate b\u03b8, i.e.,\nb\u03b8ML= argmax\n\u03b8logpX1,...,X N(x1, . . . , x N).\nExpress your answer in terms of x1, . . . , x N.\n(c) Let \u03b8= 1/2. Use Chebyshev\u2019s inequality to find an upper bound for P[|b\u0398ML\u2212\u03b8|>0.1].\nExercise 2.\nLetYn=\u03b8+Wnbe the output of a noisy channel where the input is a scalar \u03b8and\nWn\u223c N(0,1) is an i.i.d. Gaussian noise. Suppose that we have observed y1, . . . , y N.\n(a) Express the PDF of Ynin terms of \u03b8andyn. Find the joint PDF of Y1, . . . , Y N.\n(b) Find the maximum likelihood estimate b\u03b8ML. Express your answer in terms of y1, . . . , y N.\n(c) Find E[b\u0398ML].\nExercise 3.\nLetX1, . . . , X Nbe a sequence of i.i.d. Gaussian random variables with unknown mean \u03b81\nand variance \u03b82. Suppose that we have observations x1, . . . , x N.\n(a) Express the PDF of Xnin terms of xn,\u03b81and\u03b82. Find the joint PDF of X1, . . . , X N.\n(b) Find the maximum likelihood estimates of \u03b81and\u03b82.\n536", "552": "8.7. PROBLEMS\nExercise 4.\nIn this problem we study a single-photon image sensor. First, recall that photons arrive\naccording to a Poisson distribution, i.e., the probability of observing kphotons is\nP[Y=k] =\u03bbke\u2212\u03bb\nk!,\nwhere \u03bbis the (unknown) underlying photon arrival rate. When photons arrive at the single-\nphoton detector, the detector generates a binary response \u201c1\u201d when one or more photons\nare detected, and \u201c0\u201d when no photon is detected.\n(a) Let Bbe the random variable denoting the response of the single-photon detector.\nThat is,\nB=(\n1, Y \u22651,\n0, Y = 0.\nFind the PMF of B.\n(b) Suppose we have obtained Tindependent measurements with realizations B1=b1,\nB2=b2, ...,BT=bT. Show that the underlying photon arrival rate \u03bbcan be estimated\nby\n\u03bb=\u2212log \n1\u2212PT\nt=1bt\nT!\n.\n(c) Get a random image from the internet and turn it into a grayscale array with values\nbetween 0 and 1. Write a MATLAB or Python program to synthetically generate a\nsequence of T= 1000 binary images. Then use the previous result to reconstruct the\ngrayscale image.\nExercise 5.\nConsider a deterministic vector s\u2208Rdand random vectors\nfY|\u0398(y|\u03b8) = Gaussian( s\u03b8,\u03a3),\nf\u0398(\u03b8) = Gaussian( \u00b5, \u03c32).\n(a) Show that the posterior distribution is given by\nf\u0398|Y(\u03b8|y) = Gaussian( m, q2), (8.78)\nwhere\nd2=sT\u03a3\u22121s,\nm=\u0012\nd2+1\n\u03c32\u0013\u22121\u0010\nsT\u03a3\u22121y+\u00b5\n\u03c32\u0011\n,\nq2=1\nd2+1\n\u03c32.\n(b) Show that the MMSE estimate b\u03b8MMSE (y) is given by\nb\u03b8MMSE (y) =\u03c32sT\u03a3\u22121y+\u00b5\n\u03c32d2+ 1. (8.79)\n537", "553": "CHAPTER 8. ESTIMATION\n(c) Show that the MSE is given by\nMSE(\u0398 ,b\u0398MMSE (Y)) =1\nd2+1\n\u03c32. (8.80)\nWhat happens when \u03c3\u21920?\n(d) Give an interpretation of d2. What happens when d2\u21920 and when d2\u2192 \u221e ?\nExercise 6.\nProve the following identity:\n\u0014\u03a3\u0398\u0398\u03a3\u0398X\n\u03a3X\u0398\u03a3XX\u0015\u22121\n=\u0014(\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121\u2212(\u03a3\u0398\u0398\u2212\u03a3\u0398X\u03a3\u22121\nXX\u03a3X\u0398)\u22121\u03a3\u0398X\u03a3\u22121\nXX\n(\u03a3XX\u2212\u03a3X\u0398\u03a3\u22121\n\u0398\u0398\u03a3\u0398X)\u22121\u03a3X\u0398\u03a3\u22121\n\u0398\u0398 (\u03a3XX\u2212\u03a3X\u0398\u03a3\u22121\n\u0398\u0398\u03a3\u0398X)\u22121\u0015\n.\nHint: You can perform reverse engineering by checking whether the product of the left-hand\nside and the right-hand side would give you the identity matrix.\nExercise 7.\nLetX1,X2,X3andX4be four i.i.d. Poisson random variables with mean \u03b8= 4. Find the\nmean and variance of the following estimators b\u0398(X) for \u03b8and determine whether they are\nbiased or unbiased.\n\u0088b\u0398(X) = (X1+X2)/2\n\u0088b\u0398(X) = (X3+X4)/2\n\u0088b\u0398(X) = (X1+ 2X2)/3\n\u0088b\u0398(X) = (X1+X2+X3+X4)/4\nExercise 8.\nLetX1, . . . , X Nbe i.i.d. random variables with a uniform distribution of [0 , \u03b8]. Consider the\nfollowing estimator:\nb\u0398(X) = max( X1, . . . , X N). (8.81)\n(a) Show that the PDF of b\u0398 is fb\u0398(\u03b8) =N[FX(x)]N\u22121fX(x), where fXandFXare re-\nspectively the PDF and CDF of Xn.\n(b) Show that b\u0398 is a biased estimator.\n(c) Find the variance of b\u0398. Is it a consistent estimator?\n(d) Find a constant cso that cb\u0398 is unbiased.\nExercise 9.\nLetX1, . . . , X Nbe i.i.d. Gaussian random variables with unknown mean \u03b8and known\nvariance \u03c3= 1.\n538", "554": "8.7. PROBLEMS\n(a) Show that the log-likelihood function is\nlogL(\u03b8|x) =\u2212N\n2log(2\u03c0)\u22121\n2NX\nn=1(xn\u2212\u03b8)2. (8.82)\n(b) Let X2=1\nNPN\nn=1x2\nnandX=1\nNPN\nn=1xn. Show that X2>(X)2if and only ifPN\nn=1(xn\u2212\u03b8)2\u22650 for all \u03b8.\n(c) Use Python to plot the function log L(\u03b8|x), when X= 2 and X2= 1.\nExercise 10.\nLetX1, . . . , X Nbe i.i.d. uniform random variables over the interval [0 , \u03b8].\nLetT= max( X1, . . . , X N).\n(a) Consider the estimator h(X) =1\nNPN\nn=1Xn. Ish(\u00b7) an unbiased estimator?\n(b) Consider the estimator g(X) =1\nNPN\nn=1Xn. Isg(\u00b7) an unbiased estimator?\n(c) Show that\nE[g(X)|T=t] =\u0012N+ 1\nN\u0013\nt.\n(d) Let bg(X) =E[g(X)|T] =\u0000N+1\nN\u0001\nT. Show that\nE[bg(X)2] =\u0012(N+ 1)2\nN(N+ 2)\u0013\n\u03b82.\n(e) Show that\nE[(bg(X)\u2212\u03b8)2] =\u00121\nN(N+ 2)\u0013\n\u03b82.\nExercise 11.\nThe Kullback-Leibler divergence between two distributions p1(x) and p2(x) is defined as\nKL(p1\u2225p2) =Z\np1(x) logp1(x)\np2(x)dx. (8.83)\nSuppose we approximate p1using a distribution p2. Let us choose p2= Gaussian( \u00b5,\u03a3).\nShow that \u00b5and\u03a3, which minimize the KL divergence, are such that\n\u00b5=Ex\u223cp1(x)[x] and \u03a3=Ex\u223cp1(x)[(x\u2212\u00b5)(x\u2212\u00b5)T].\nExercise 12.\n(a) Recall that the trace operator is defined as tr[ A] =Pd\ni=1[A]i,i. Prove the matrix\nidentity\nxTAx= tr[AxxT], (8.84)\nwhere A\u2208Rd\u00d7d.\n539", "555": "CHAPTER 8. ESTIMATION\n(b) Show that the likelihood function\np(D |\u03a3) =NY\nn=1\u001a1\n(2\u03c0)d/2|\u03a3|1/2expn\n\u22121\n2(xn\u2212\u00b5)T\u03a3\u22121(xn\u2212\u00b5)o\u001b\n(8.85)\ncan be written as\np(D|\u03a3) =1\n(2\u03c0)Nd/2|\u03a3\u22121|N/2exp(\n\u22121\n2tr\"\n\u03a3\u22121NX\nn=1(xn\u2212\u00b5)(xn\u2212\u00b5)T#)\n.(8.86)\n(c) Let A=\u03a3\u22121b\u03a3ML, and \u03bb1, ..., \u03bb dbe the eigenvalues of A. Show that the result from\npart (b) leads to\np(D|\u03a3) =1\n(2\u03c0)Nd/2|b\u03a3ML|N/2 dY\ni=1\u03bbi!N/2\nexp(\n\u2212N\n2dX\ni=1\u03bbi)\n. (8.87)\nHint: For matrix Awith eigenvalues \u03bb1, ..., \u03bb d, tr[A] =Pd\ni=1\u03bbi.\n(d) Find \u03bb1, . . . , \u03bb dsuch that Equation (8.87) is maximized.\n(e) With the choice of \u03bbigiven in (d), derive the ML estimate b\u03a3ML.\n(f) What would be the alternative way of finding b\u03a3ML? You do not need to prove it. Just\nbriefly describe the idea.\n(g)b\u03a3MLis abiased estimate of the covariance matrix because E[b\u03a3ML]\u0338=\u03a3. Can you\nsuggest an unbiased estimate b\u03a3unbias such that E[b\u03a3unbias ] =\u03a3? You don\u2019t need to\nprove it. Just state the result.\n540", "556": "Chapter 9\nConfidence and Hypothesis\nIn Chapters 7 and 8 we learned about regression and estimation, which allow us to determine\nthe underlying parameters of our statistical models. After obtaining the estimates, we would\nlike to quantify the accuracy of the estimates and draw statistical conclusions. Additionally,\nwe would like to understand the confidence of these estimates along with their statistical\nsignificance . This chapter presents a few principles that involve analyzing the confidence of\nthe estimates and conducting hypothesis testing. There are two main questions that we will\naddress:\n\u0088How good is our estimate? This is a fundamental question about the estimator b\u0398, a\nrandom variable with a PDF, a mean, and a variance.1The estimator we construct\ntoday may be different from the estimator we construct tomorrow due to variations in\nthe observed data. Therefore, the quality of the estimator depends on the randomness\nand the number of samples used to construct it. To measure the quality of the estimator\nwe need to introduce an important concept known as the confidence .\n\u0088Is there statistical significance? Suppose that we ran a campaign and observed that\nthere is a change in the statistics. On what basis do we claim that the change is\nstatistically significant? How should the cutoff be determined? If we claim that a\nresult is statistically significant but there is no significance in reality, how much error\nwill we suffer? These questions are the subjects of hypothesis testing .\nThese two principal questions are critical for modern data science. If they are not properly\nanswered, our statistical conclusions could potentially be flawed. A toy example:\nImagine that you are developing a COVID-19 vaccine. You tested the vaccine on three\npatients, and all of them show positive responses to the vaccine. You felt excited because\nyour vaccine has a 100% success rate. You submit your vaccine application to FDA. Within\n1 second your application is rejected. Why? The answer is obvious. You only have three\ntesting samples. How reliable can these three samples be?\nWhile you are laughing at this toy example, it raises deep statistical questions. First,\nwhy are three samples not enough? Well, it is because the variance of the estimator can\npotentially be huge. More samples are better because if the estimator is the sample average of\nthe individual responses, the estimator will behave like a Gaussian according to the Central\n1Not all random variables have a well-defined PDF, mean, and variance. E.g., a Cauchy variable does\nnot have a mean.\n541", "557": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nLimit Theorem. The variance of this Gaussian will diminish as we have more samples.\nTherefore, if we want to control the variance of the estimator, we need more samples. Second,\neven if we have many samples, how confident is this estimator with respect to the unknown\npopulation parameter? Note that the population parameter is unknown, and so we cannot\nmeasure things such as the mean squared error. We need a tool to report confidence. Third,\nfor simple estimators such as the sample average, we can approximate it by a Gaussian .\nHowever, if the estimator is more complicated, e.g., the sample median, how do we estimate\nthe variance and the confidence? Fourth, suppose that we have expanded the vaccine test\nto, say, 951 patients, and we have obtained some statistics. To what extent can we declare\nthat the vaccine is effective? We need a decision rule that turns the statistics into a binary\ndecision. Finally, even if we declare that the vaccine is effective with a confidence of 95%,\nwhat about the remaining 5%? What if we want to push the confidence to 99%? What is\nthe trade-off?\nAs you can see, these questions are the recurring themes of all data science problems.\nNo matter if you are developing a medical diagnostic system, a computer vision algorithm,\na speech recognition system, a recommendation system, a search engine, stock forecast,\nfraud detection, or robotics controls, you need to answer these questions. This chapter will\nintroduce useful concepts related to data analysis in the form of five basic principles :\n1.Confidence interval (Section 9.1). A confidence interval is a random interval that\nincludes the true parameter. We will discuss how a confidence interval is constructed\nand the correct way to interpret the confidence interval.\n2.Bootstrapping (Section 9.2). When constructing the confidence interval, we need the\nvariance of the estimator. However, since we do not know the true distribution, we\nneed an alternative way to estimate the variance. Bootstrapping is designed for this\npurpose.\n3.Hypothesis testing (Section 9.3). Many statistical tasks require a binary decision at\nthe end, e.g., there is a disease versus there is no disease. Hypothesis testing is a\nprinciple for making a systematic decision with statistical guarantees.\n4.Neyman-Pearson decision (Section 9.4). The simple hypothesis testing procedure has\nmany limitations that can only be resolved if we understand a more general framework.\nWe will study such a framework, called the Neyman-Pearson decision rule.\n5.ROC and PR curves (Section 9.5). No decision rule is perfect. There is always a\ntrade-off between how much we can detect and how much we will miss. The receiver\noperating characteristic (ROC) curve and the precision-recall (PR) curve can give us\nmore insight into this trade-off. We will establish the equivalence between the ROC\nand the PR curve and correct any misconceptions about them.\nAfter reading this chapter, we hope that you will be able to apply these principles\nto your favorite data analysis problems correctly. With these principles, you can tell your\ncustomers or bosses the statistical significance of your conclusions. You will also be able to\nhelp your friends understand the many misconceptions that they may find on the internet.\n542", "558": "9.1. CONFIDENCE INTERVAL\n9.1 Confidence Interval\nThe first topic we discuss in this chapter is the confidence interval . At a high level, the\nconfidence interval tells us the quality of our estimator with respect to the number of sam-\nples. We begin this section by reviewing the randomness of an estimator. Then we develop\nthe concept of the confidence interval. We discuss several methods for constructing and\ninterpreting these confidence intervals.\n9.1.1 The randomness of an estimator\nImagine that we have a dataset X={X1, . . . , X N}, where we assume that Xnare i.i.d.\ncopies drawn from a distribution fX(x;\u03b8). We want to construct an estimator b\u0398 of \u03b8from\nthe dataset X. For example, if fXis a Gaussian distribution with an unknown mean \u03b8, we\nwould like to estimate \u03b8using the sample average b\u0398. In statistics, an estimator b\u0398 is also\nknown as a statistic , which is constructed from the samples. In this book we use the terms\n\u201cestimator\u201d and \u201cstatistic\u201d interchangeably. Written as equations, an estimator is a function\nof the samples:\nb\u0398|{z}\nestimator=g(X1, . . . , X N)|{z }\nfunction of X,\nwhere gis a function that takes the samples X1, . . . , X Nand returns a random variable b\u0398.\nFor example, the sample average\nb\u0398 =1\nNNX\nn=1Xn\n|{z}\ng(X1,...,X N)\nis an estimator because it is computed by summing the samples X1, . . . , X Nand dividing it\nbyN.\nWhat is an estimator?\n\u0088An estimator b\u0398 is a function of the samples X1, . . . , X N:\nb\u0398 =g(X1, . . . , X N). (9.1)\n\u0088b\u0398 is a random variable. It has a PDF, CDF, mean, variance, etc.\nBy construction, b\u0398 is a random variable because it is a function of the random samples.\nTherefore, b\u0398 has its own PDF, CDF, mean, variance, etc. Since b\u0398 is a random variable,\nwe should report both the estimator\u2019s value and the estimator\u2019s confidence when reporting\nits performance. The confidence measures the quality of b\u0398 when compared to the true\nparameter \u03b8. It provides a measure of the reliability of the estimator b\u0398. Ifb\u0398 fluctuates a\ngreat deal we may not be confident of our estimates. Let\u2019s consider the following example.\n543", "559": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nExample 9.1 . A class of 1000 students took a test. The distribution of the score is\nroughly a Gaussian with mean 50 and standard deviation 20. A teaching assistant\nwas too lazy to calculate the true population mean. Instead, he sampled a subset of 5\nscores listed as follows:\nStudent ID 1 2 3 4 5\nScores 11 97 1 78 82\nHe calculated the average, which is 53.8. This is a very good estimate of the class\naverage (which is 50). What is wrong with his procedure?\nSolution . He was just lucky. It quite possible that if he sampled another 5 scores, he\nwould get something very different. For example, if he looks at the 11 to 15 student\nscores, he could get:\nStudent ID 11 12 13 14 15\nScores 44 29 19 27 15\nIn this case the average is 26.8.\nBoth 53.8 and 26.8 are legitimate estimates, but they are the random realizations\nof a random variable b\u0398. This b\u0398 has a PDF, CDF, mean, variance, etc. It may be\nmisleading to simply report the estimated value from a particular instant, so the\nconfidence of the estimator must be specified.\nDistributions of b\u0398. We next discuss the distribution of b\u0398.Figure 9.1 illustrates several\nkey ideas. Suppose that the population distribution fX(x) is a mixture of two Gaussians.\nLet\u03b8be the mean of this distribution (somewhere between the two peak locations). We\nsample N= 50 data points X1, . . . , X Nfrom this distribution. However, the 50 data points\nwe sample today could differ from the 50 data points we sample tomorrow. If we compute\nthe sample average from each of these finite-sample distributions, we will obtain a set of\nsample averages b\u0398. Notably, we have a setofb\u0398 because today we have one b\u0398 and tomorrow\nwe have another b\u0398. By plotting the histogram of the sample averages b\u0398, we will have a\ndistribution.\nThe histogram of b\u0398 depends on several factors. According to Central Limit Theorem,\nthe shape of fb\u0398(\u03b8) is a Gaussian because b\u0398 is the average of Ni.i.d. random variables.\nIfb\u0398 is not the average of i.i.d. random variables, the shape is not necessarily a Gaussian.\nThis results in additional complications, so we will discuss some tools for dealing with this\nproblem. The spread of the sample distribution is mainly driven by the number of samples\nwe have in each subdataset. As you can imagine, the more samples we have in a subdataset\nthe more accurate the distribution. Thus you will have a more accurate sample average. The\nfluctuation of the sample average will also be smaller.\nBefore we continue, let\u2019s summarize the randomness of b\u0398:\nWhat is the randomness of b\u0398?\n\u0088b\u0398 is generated from a finite-sample dataset. Each time we draw a finite-sample\ndataset, we introduce randomness.\n544", "560": "9.1. CONFIDENCE INTERVAL\nFigure 9.1: Pictorial illustration of the randomness of the estimator b\u0398. Given a population, our datasets\nare usually a subset of the population. Computing the sample average from these finite-sample distribu-\ntions introduces the randomness to b\u0398. If we plot the histogram of the sample averages, we will obtain\na distribution. The mean of this distribution is the population mean, but there is a nontrivial amount of\nfluctuation. The purpose of the concept of confidence interval is to quantify this fluctuation.\n\u0088Ifb\u0398 is the sample average, the PDF is (roughly) a Gaussian. If b\u0398 is not a sample\naverage, the PDF is not necessarily a Gaussian.\n\u0088The spread of the fluctuation depends on the number of samples in each sub-\ndataset.\n9.1.2 Understanding confidence intervals\nThe confidence interval is a probabilistic statement about b\u0398. Instead of studying b\u0398 as a\npoint , we construct an interval\nI=h\nb\u0398\u2212\u03f5,b\u0398 +\u03f5i\n, (9.2)\nfor some \u03f5to be determined. Note that this interval is a random interval : If we have a\ndifferent realization of b\u0398, we will have a different I. We call Itheconfidence interval for\nthe estimator b\u0398.\nGiven this random interval, we ask: What is the probability that Iincludes \u03b8? That\nmeans that we want to evaluate the probability\nP[\u03b8\u2208 I] =Ph\nb\u0398\u2212\u03f5\u2264\u03b8\u2264b\u0398 +\u03f5i\n.\nWe emphasize that the randomness in this probability is caused by b\u0398, not \u03b8. This is because\nthe interval Ichanges when we conduct a different experiment to obtain a different b\u0398. The\n545", "561": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nsituation is similar to that illustrated on the left-hand side of Figure 9.2 . The confidence\ninterval Ichanges but the true parameter \u03b8is fixed.\nFigure 9.2: Confidence interval is the random interval I= [b\u0398\u2212\u03f5,b\u0398 +\u03f5], not the deterministic interval\n[\u03b8\u2212\u03f5, \u03b8+\u03f5]. The random interval in the former case does not require any knowledge about the true\nparameter \u03b8, whereas the latter requires \u03b8. By claiming a 95% confidence interval, we say that there\nis 95% chance that the random interval will include the true parameter. So if you have 100 random\nrealizations of the confidence intervals, then 95 on average will include the true parameter.\nConfidence intervals can be confusing. Often the confusion arises because of the fol-\nlowing identity:\nPh\nb\u0398\u2212\u03f5\u2264\u03b8\u2264b\u0398 +\u03f5i\n=Ph\n\u2212\u03f5\u2264\u03b8\u2212b\u0398\u2264\u03f5i\n=Ph\n\u2212\u03f5\u2212\u03b8\u2264 \u2212b\u0398\u2264\u03f5\u2212\u03b8i\n=Ph\n\u03b8\u2212\u03f5\u2264b\u0398\u2264\u03b8+\u03f5i\n. (9.3)\nAlthough the values of the two probabilities are the same, the two events are interpreted\ndifferently. The right-hand side of Figure 9.2 illustrates P[\u03b8\u2212\u03f5\u2264b\u0398\u2264\u03b8+\u03f5]. The interval\n[\u03b8\u2212\u03f5, \u03b8+\u03f5] is fixed. What is the probability that the estimator b\u0398 lies within this deterministic\ninterval? To find this probability, we need to know the true parameter \u03b8, which is not\navailable. By contrast, the other probability P[b\u0398\u2212\u03f5\u2264\u03b8\u2264b\u0398 +\u03f5] does not require any\nknowledge about the true parameter \u03b8. What is the probability that the true parameter is\nincluded inside the random interval? If the probability is high, we say that there is a good\nchance that our confidence interval will contain the true parameter. This is observed in the\nleft-hand side of Figure 9.2 .\nIn practice we often set P[b\u0398\u2212\u03f5\u2264\u03b8\u2264b\u0398 +\u03f5] to be greater than a certain confidence\nlevel, say 95%, and then we determine \u03f5. Once we have determined \u03f5, we can claim that\n546", "562": "9.1. CONFIDENCE INTERVAL\nwith 95% probability the interval [ b\u0398\u2212\u03f5,b\u0398 +\u03f5] will include the unknown parameter \u03b8. We\ndo not need to know \u03b8at any point in this process.\nTo make this more general, we define 1 \u2212\u03b1as the confidence level for some parame-\nter\u03b1. For example, if we would like to have a 95% confidence level, we set \u03b1= 0.05. Then\nthe probability inequality\nPh\nb\u0398\u2212\u03f5\u2264\u03b8\u2264b\u0398 +\u03f5i\n\u22651\u2212\u03b1 (9.4)\ntells us that there is at least a 95% chance that the random interval I= [b\u0398\u2212\u03f5,b\u0398 +\u03f5] will\ninclude the true parameter \u03b8. In this case we say that Iis a \u201c 95% confidence interval \u201d.\nWhat is a 95% confidence interval?\n\u0088It is a random interval [ b\u0398\u2212\u03f5,b\u0398 +\u03f5] such that there is 95% probability for it to\ninclude the true parameter \u03b8.\n\u0088It is not the deterministic interval [ \u03b8\u2212\u03f5, \u03b8+\u03f5], because we never know \u03b8.\nLet\u2019s consider the following two examples to clarify any misconceptions.\nExample 9.2 . After analyzing the life expectancy of people in the United States, it\nwas concluded that the 95% confidence interval is (77.8, 79.1) years old. Is the following\nclaim valid?\nAbout 95% of the people in the United States have a life expectancy between 77.8\nyears old and 79.1 years old.\nSolution . No. The confidence interval tells us that with 95% probability the random\ninterval (77 .8,79.1) will include the true average. We emphasize that (77 .8,79.1) is\nrandom because it is constructed from a small set of data points. If we survey another\nset of people we will have another interval.\nSince we do not know the true average, we do not know the percentage of people\nwhose life expectancy is between 77.8 years old and 79.1 years old. It could be that the\ntrue average is 80 years old, which is out of the range. It could also be that the true\naverage is 77.9 years old, which is within the range, but only 10% of the population\nmay have life expectancy in (77 .8,79.1).\nExample 9.3 . After studying the SAT scores of 1000 high school students, it was\nconcluded that the 95% confidence interval is (1134, 1250) points. Is the following\nclaim valid?\nThere is a 95% probability that the average SAT score in the population is in the\nrange 1134 and 1250.\nSolution . Yes, but it can be made clearer. The average SAT score in the population\nremains unknown. It is a constant and it is deterministic, so there is no probability\nassociated with it. A better way to say this is: \u201cThere is 95% probability that the\n547", "563": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nrandom interval 1134 and 1250 will include the average SAT score.\u201d We emphasize\nthat the 95% probability is about the random interval, not the unknown parameter.\n9.1.3 Constructing a confidence interval\nLet\u2019s consider an example. Suppose that we have a set of i.i.d. observations X1, . . . , X N\nthat are Gaussians with an unknown mean \u03b8and a known variance \u03c32. We consider the\nmaximum-likelihood estimator, which is the sample average:\nb\u0398 =1\nNNX\nn=1Xn.\nOur goal is to construct a confidence interval.\nFigure 9.3: Conceptual illustration of how to construct a confidence interval. Starting with the pop-\nulation, we draw random subsets. Each random subset gives us an estimator, and correspondingly an\ninterval.\nBefore we consider the equations, let\u2019s look at a graph illustrating what we want to\nachieve. Figure 9.3 shows a population distribution, which is a Gaussian in this example.\nWe draw Nsamples from the Gaussian to construct a random subset. Based on this random\nsubset we construct the estimator b\u0398. Since this estimator is based on the particular random\nsubset we have, we can follow the same approach by drawing another random subset. To\ndifferentiate the estimators constructed by the different random subsets, let\u2019s call the esti-\nmators b\u0398(1)andb\u0398(2), respectively. For each estimator we construct an interval [ b\u0398\u2212\u03f5,b\u0398+\u03f5]\nto obtain two different intervals:\nI1= [b\u0398(1)\u2212\u03f5,b\u0398(1)+\u03f5] and I2= [b\u0398(2)\u2212\u03f5,b\u0398(2)+\u03f5].\n548", "564": "9.1. CONFIDENCE INTERVAL\nIf we can determine \u03f5, we have found the confidence interval.\nWe can determine the confidence interval by observing the histogram of b\u0398, which in\nour case is the histogram of the sample average, since the histogram of b\u0398 is well-defined,\nespecially if we are looking at the sample average. The histogram of the sample average is a\nGaussian because the average of Ni.i.d. Gaussian random variables is Gaussian. Therefore,\nthe width of this Gaussian is determined by the answer to this question:\nFor what \u03f5can we cover 95% of the histogram of b\u0398?\nTo find the answer, we set up the following probability inequality:\nP\uf8ee\n\uf8f0|b\u0398\u2212E[b\u0398]|q\nVar[b\u0398]\u2264\u03f5\uf8f9\n\uf8fb\u22651\u2212\u03b1.\nThis probability says that we want to find an \u03f5such that the majority of b\u0398 is living close\nto its mean. The level 1 \u2212\u03b1is our confidence level, which is typically 95%. Equivalently, we\nlet\u03b1= 0.05.\nIn the above equation, we can define the quotient as\nbZdef=b\u0398\u2212E[b\u0398]q\nVar[b\u0398].\nWe know that bZis a zero-mean unit-variance Gaussian because it is the standardized vari-\nable. [Note: Not all normalized variables are Gaussian, but if b\u0398 is a Gaussian the normalized\nvariable will remain a Gaussian.] Thus, the probability inequality we are looking at is\nPh\n|bZ| \u2264\u03f5i\n|{z}\ntwo tails of a standard Gaussian\u2265 1\u2212\u03b1.\nThe PDF of bZis shown in Figure 9.4 . As you can see, to achieve 95% confidence we need\nto pick an appropriate \u03f5such that the shaded area is less than 5%.\n-3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 300.050.10.150.20.250.30.350.4\nFigure 9.4: PDF of the random variable bZ= (b\u0398\u2212E[b\u0398])/q\nVar[b\u0398]. The shaded area denotes the\n\u03b1= 0.05confidence level.\n549", "565": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nSinceP[bZ\u2264\u03f5] is the CDF of a Gaussian, it follows that\nP[|bZ| \u2264\u03f5] =P[\u2212\u03f5\u2264bZ\u2264\u03f5]\n=P[bZ\u2264\u03f5]\u2212P[bZ\u2264 \u2212\u03f5]\n= \u03a6 ( \u03f5)\u2212\u03a6 (\u2212\u03f5).\nUsing the symmetry of the Gaussian, it follows that \u03a6 ( \u2212\u03f5) = 1\u2212\u03a6 (\u03f5) and hence\nP[|bZ| \u2264\u03f5] = 2\u03a6 ( \u03f5)\u22121.\nEquating this result with the probability inequality P[|bZ| \u2264\u03f5]\u22651\u2212\u03b1, we have that\n\u03f5\u2265\u03a6\u22121\u0010\n1\u2212\u03b1\n2\u0011\n.\nThe remainder of this problem is solvable on a computer. On MATLAB, we can call\nicdf to compute the inverse CDF of a standard Gaussian. On Python, the command is\nstats.norm.ppf . The commands are as shown below.\n% MATLAB code to compute the width of the confidence interval\nalpha = 0.05;\nmu = 0; sigma = 1; % Standard Gaussian\nepsilon = icdf(\u2019norm\u2019,1-alpha/2,mu,sigma)\n# Python code to compute the width of the confidence interval\nimport scipy.stats as stats\nalph = 0.05;\nmu = 0; sigma = 1; # Standard Gaussian\nepsilon = stats.norm.ppf(1-alph/2, mu, sigma)\nprint(epsilon)\nIf everything is done properly, we see that for a 95% confidence level ( \u03b1= 0.05) the corre-\nsponding \u03f5is\u03f5= 1.96.\nAfter determining \u03f5, it remains to determine E[b\u0398] and Var[ b\u0398] in order to complete the\nprobability inequality. To this end, we note that\nE[b\u0398] =E\"\n1\nNNX\nn=1Xn#\n=\u03b8,\nVar[b\u0398] = Var\"\n1\nNNX\nn=1Xn#\n=\u03c32\nN,\nif we assume that the population distribution is Gaussian( \u03b8, \u03c32), where \u03b8is unknown but \u03c3\nis known. Substituting these into the probability inequality, we have that\nP\uf8ee\n\uf8f0|b\u0398\u2212E[b\u0398]|q\nVar[b\u0398]\u2264\u03f5\uf8f9\n\uf8fb=P\u0014\nb\u0398\u2212\u03f5\u03c3\u221a\nN\u2264\u03b8\u2264b\u0398 +\u03f5\u03c3\u221a\nN\u0015\n=P\u0014\nb\u0398\u22121.96\u03c3\u221a\nN\u2264\u03b8\u2264b\u0398 + 1 .96\u03c3\u221a\nN\u0015\n,\n550", "566": "9.1. CONFIDENCE INTERVAL\nwhere we let \u03f5= 1.96 for a 95% confidence level. Therefore, the 95% confidence interval is\n\u0014\nb\u0398\u22121.96\u03c3\u221a\nN,b\u0398 + 1 .96\u03c3\u221a\nN\u0015\n. (9.5)\nAs you can see, we do not need to know the value of \u03b8at any point of the derivation because\nthe confidence interval in Equation (9.5) does not involve \u03b8. This is an important difference\nwith the other probability P[\u03b8\u2212\u03f5\u2264b\u0398\u2264\u03b8+\u03f5], which requires \u03b8.\nHow to construct a confidence interval\n\u0088Compute the estimator b\u0398.\n\u0088Determine the width of the confidence interval \u03f5by inspecting the confidence\nlevel 1 \u2212\u03b1. Ifb\u0398 is Gaussian, then \u03f5= \u03a6\u22121(1\u2212\u03b1\n2).\n\u0088Ifb\u0398 is not a Gaussian, replace the Gaussian CDF by the CDF of b\u0398.\n\u0088The confidence interval is [ b\u0398\u2212\u03f5,b\u0398 +\u03f5].\n9.1.4 Properties of the confidence interval\nSome important properties of the confidence interval are listed below.\n\u0088Probability of b\u0398is the same as probability of bZ. First, the two random variables b\u0398\nandbZhave a one-to-one correspondence. We proved the following in Chapter 6:\nIfb\u0398\u223cGaussian( \u03b8,\u03c32\nN), then\nbZdef=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN\u223cGaussian(0 ,1). (9.6)\nFor example, if b\u0398\u223cGaussian( \u03b8,\u03c32\nN) with N= 1, \u03b8= 1 and \u03c3= 2, then a 95%\nconfidence level is\n0.95\u2248P[\u22121.96\u2264bZ\u22641.96], (bZis within 1.96 std from bZ\u2019s mean)\n=P[\u22121.96\u2264b\u0398\u2212\u03b8\n\u03c3/\u221a\nN\u22641.96]\n=P\u0014\n\u03b8\u22121.96\u03c3\u221a\nN\u2264b\u0398\u2264\u03b8+ 1.96\u03c3\u221a\nN\u0015\n=P[\u22122.92\u2264b\u0398\u22644.92]. (b\u0398 is within 1.96 std from b\u0398\u2019s mean)\nNote that while the range for bZis different from the range for b\u0398, they both return the\nsame probability. The only difference is that b\u0398 is constructed before the normalization\nandbZis constructed after the normalization.\n551", "567": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\n\u0088Standard error . In this estimation problem we know that b\u0398 is the sample average. We\nassume that the mean \u03b8is unknown but the variance Var[ b\u0398] is known. The standard\ndeviation of b\u0398 is called the standard error :\nse=q\nVar[b\u0398] =\u03c3\u221a\nN. (9.7)\n\u0088Critical value . The value 1 .96 in our example is often known as the critical value . It\nis defined as\nz\u03b1= \u03a6\u22121\u0010\n1\u2212\u03b1\n2\u0011\n. (9.8)\nThez\u03b1value gives us a multiplier applied to the standard error that will result in a\nvalue within the confidence interval. This is because, by the definition of the confidence\ninterval, the interval is\n\u0014\nb\u0398\u22121.96\u03c3\u221a\nN,b\u0398 + 1 .96\u03c3\u221a\nN\u0015\n=h\nb\u0398\u2212z\u03b1se,b\u0398 +z\u03b1sei\n\u0088Margin of error . The margin of error is defined as\nmargin of error = z\u03b1\u03c3\u221a\nN. (9.9)\nThe margin of error is also the width of the confidence interval. As the name implies,\nthe margin of error tells us how much error the confidence interval includes when\npredicting the population parameter.\nPractice Exercise 9.1 . Suppose that the number of photos a Facebook user uploads\nper day is a random variable with \u03c3= 2. In a set of 341 users, the sample average is\n2.9. Find the 90% confidence interval of the population mean.\nSolution . We set \u03b1= 0.1. The z\u03b1-value is\nz\u03b1= \u03a6\u22121\u0010\n1\u2212\u03b1\n2\u0011\n= 1.6449.\nThe 90% confidence interval is then\n\u0014\nb\u0398\u22121.642\u221a\n341,b\u0398 + 1 .642\u221a\n341\u0015\n= [2.72,3.08].\nTherefore, with 90% probability, the interval [2 .72,3.08] includes the population mean.\nExample 9.4 . Professional cyber-athletes have a standard deviation of \u03c3= 73 .4\nactions per minute. If we want to estimate the average actions per minute of the\npopulation, how many samples are needed to obtain a margin of error <20 at 90%\nconfidence?\n552", "568": "9.1. CONFIDENCE INTERVAL\nSolution . With a 90% confidence level, the z\u03b1-value is\nz\u03b1= \u03a6\u22121\u0010\n1\u2212\u03b1\n2\u0011\n= \u03a6\u22121(0.95) = 1 .645.\nThe margin of error is 20. So we have\nz\u03b1\u03c3\u221a\nN= 20.\nMoving around the terms gives us\nN\u2265\u0010\nz\u03b1\u03c3\n20\u00112\n= 36.45.\nTherefore, we need at least N= 37 samples to ensure a margin of error of <20 at a\n90% confidence level.\nFigure 9.5: Relationships between the standard error se, the z\u03b1value, and the margin of error. The\nconfidence level \u03b1is the area under the curve for the tails of each PDF.\nThe concepts of standard error se, thez\u03b1value, and the margin of error are summarized\ninFigure 9.5 . The left-hand side is the PDF of bZ. It is the normalized random variable,\nwhich is also the standard Gaussian. The right-hand side is the PDF of b\u0398, the unnormalized\nrandom variable. The z\u03b1value is located in the bZ-space. It defines the range of bZin the\nPDF within which we are confident about the true parameter. The corresponding value\nin the b\u0398-space is the margin of error . This is found by multiplying z\u03b1with the standard\ndeviation of b\u0398, known as the standard error . Correspondingly, in the bZ-space the standard\ndeviation is the unity.\nTwo further points about the confidence interval should be mentioned:\n\u0088Number of Samples N. The confidence interval is a function of N. As we increase the\nnumber of samples, the distribution of the estimator b\u0398 becomes narrower. Specifically,\nifb\u0398 follows a Gaussian distribution\nb\u0398\u223cGaussian\u0012\n\u03b8,\u03c32\nN\u0013\n,\nthenb\u0398p\u2192\u03b8asN\u2192 \u221e .Figure 9.6 illustrates a few examples of b\u0398 as Ngrows. In the\nlimit when N\u2192 \u221e , we observe that the interval becomes\n\u0014\nb\u0398\u22121.96\u03c3\u221a\nN,b\u0398 + 1 .96\u03c3\u221a\nN\u0015\n\u2212\u2192h\nb\u0398,b\u0398i\n=b\u0398.\n553", "569": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nIn this case, the statement \u03b8\u2208h\nb\u0398\u22121.96\u03c3\u221a\nN,b\u0398 + 1 .96\u03c3\u221a\nNi\nbecomes \u03b8=b\u0398. That\nmeans the estimator b\u0398 returns the correct true parameter \u03b8. Of course, it is possible\nthatE[b\u0398]\u0338=\u03b8, i.e., the estimator is biased. In that case, having more samples will\napproach another estimate that is not \u03b8.\n-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 100.511.522.533.54\nN = 10\nN = 25\nN = 100\nFigure 9.6: The PDF of b\u0398as the number of samples Ngrows. Here, we assume that Xnare i.i.d.\nGaussian random variables with mean \u03b8= 0and variance \u03c32= 1.\n\u0088Distribution of bZ. When defining the confidence interval we constructed an interme-\ndiate variable\nbZ=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN.\nSince Xn\u2019s are i.i.d. Gaussian, it follows that Zis also Gaussian. This gives us a way\nto calculate the probability using the standard Gaussian table. What happens when\nXn\u2019s are notGaussian? The good news is that even if Xn\u2019s are not Gaussian, for\nsufficiently large N, the random variable b\u0398 is more or less Gaussian, because of the\nCentral Limit Theorem. Therefore, even if Xn\u2019s are not Gaussian we can still use the\nGaussian probability table to construct \u03b1and\u03f5.\n9.1.5 Student\u2019s t-distribution\nIn the discussions above, we estimate the population mean \u03b8using the estimator b\u0398. The\nassumption was that the variance \u03c32was known a priori and hence is fixed. In practice,\nhowever, there are many situations where \u03c32is not known. Thus we not only need to use\nthe mean estimator b\u0398 but also the variance estimator bS, which can be defined as\nbS2def=1\nN\u22121NX\nn=1(Xn\u2212b\u0398)2,\nwhere b\u0398 is the estimator of the mean. What is the confidence interval for b\u0398?\nFor a confidence interval to be valid, we expect it to take the form of\nI=\"\nb\u0398\u2212z\u03b1bS\u221a\nN,b\u0398 +z\u03b1bS\u221a\nN#\n,\n554", "570": "9.1. CONFIDENCE INTERVAL\nwhich is essentially the confidence interval we have just derived but with \u03c3replaced by bS.\nHowever, there is a problem with this. When we derive the confidence interval assuming a\nknown \u03c3, the z\u03b1value is determined by checking the standard Gaussian\nbZ=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN,\nwhich gives us z\u03b1= \u03a6\u22121(1\u2212\u03b1/2). The whole derivation is based on the fact that bZis a\nstandard Gaussian. Now that we have replaced \u03c3bybS, the new random variable\nTdef=b\u0398\u2212\u03b8\nbS/\u221a\nN(9.10)\nisnota standard Gaussian.\nIt turns out that the distribution of Tis Student\u2019s t-distribution with N\u22121 degrees\nof freedom. The PDF of Student\u2019s t-distribution is given as follows.\nDefinition 9.1. IfXis a random variable following Student\u2019s t-distribution of\u03bd\ndegrees of freedom, then the PDF of Xis\nfX(x) =\u0393\u0000\u03bd+1\n2\u0001\n\u221a\u03bd\u03c0\u0393\u0000\u03bd\n2\u0001\u0012\n1 +x2\n\u03bd\u0013\u2212\u03bd+1\n2\n. (9.11)\nWe may compare Student\u2019s t-distribution with the Gaussian distribution. Figure 9.7 shows\nthe standard Gaussian and several tdistributions with \u03bd=N\u22121 degrees of freedom. Note\nthat Student\u2019s t-distribution has a similar shape to the Gaussian but it has a heavier tail.\n-5 -4 -3 -2 -1 0 1 2 3 4 500.050.10.150.20.250.30.350.4\nGaussian(0,1)\nt-dist, N = 11\nt-dist, N = 3\nt-dist, N = 2\nFigure 9.7: The PDF of Student\u2019s t-distribution with \u03bd=N\u22121degrees of freedom.\nSince T=b\u0398\u2212\u03b8\nbS/\u221a\nNis at-random variable, to determine the z\u03b1value we can follow the\nsame procedure by considering the CDF of T. Let the CDF of the Student\u2019s t-distribution\nwith \u03bddegrees of freedom be\n\u03a8\u03bd(z) = CDF of Xatz.\n555", "571": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nIf we want P[|T| \u2264z\u03b1] = 1\u2212\u03b1, it follows that\nz\u03b1= \u03a8\u22121\n\u03bd\u0010\n1\u2212\u03b1\n2\u0011\n. (9.12)\nTherefore, the new confidence interval, assuming an unknown bS, is\nI=\"\nb\u0398\u2212z\u03b1bS\u221a\nN,b\u0398 +z\u03b1bS\u221a\nN#\n,\nwith z\u03b1defined in Equation (9.12), using \u03bd=N\u22121.\nPractice Exercise 9.2 . A survey asked N= 14 people for their rating of a movie. As-\nsume that the mean estimator is b\u0398 and the variance estimator is bS. Find the confidence\ninterval.\nSolution . If we use Student\u2019s t-distribution, it follows that\nz\u03b1= \u03a8\u22121\n13\u0010\n1\u2212\u03b1\n2\u0011\n= 2.16,\nwhere the degrees of freedom are \u03bd= 14\u22121 = 13. Thus the confidence interval is\nI=\"\nb\u0398\u22122.16bS\u221a\nN,b\u0398 + 2 .16bS\u221a\nN#\n.\nThe MATLAB and Python codes to report the z\u03b1value of a Student\u2019s t-distribution\nare shown below. They are both called through the inverse CDF function. In MATLAB it\nisicdf, and in Python it is stats.t.ppf .\n% MATLAB code to compute the z_alpha value of t distribution\nalpha = 0.05;\nnu = 13;\nz = icdf(\u2019norm\u2019,1-alpha/2,nu)\n# Python code to compute the z_alpha value of t distribution\nimport scipy.stats as stats\nalph = 0.05\nnu = 13\nz = stats.t.ppf(1-alph/2, nu)\nprint(z)\nExample 9.5 . A class of 10 students took a midterm exam. Their scores are given in\nthe following table.\nStudent 1 2 3 4 5 6 7 8 9 10\nScore 72 69 75 58 67 70 60 71 59 65\nFind the 95% confidence interval.\n556", "572": "9.1. CONFIDENCE INTERVAL\nSolution . The mean and standard deviation of the datasets are respectively b\u0398 = 66 .6\nandbS= 5.61. The critical z\u03b1value is determined by Student\u2019s t-distribution:\nz\u03b1= \u03a8\u22121\n9\u0010\n1\u2212\u03b1\n2\u0011\n= 2.26.\nThe confidence interval is\n\"\nb\u0398\u2212z\u03b1bS\u221a\nN,b\u0398 +z\u03b1bS\u221a\nN#\n= [62 .59,70.61].\nTherefore, with 95% probability, the interval [62 .59,70.61] will include the true popu-\nlation mean.\nRemark 1 . Make sure you understand the meaning of \u201cpopulation mean\u201d in this\nexample. Since we have ten students, isn\u2019t the population mean just the average of the\nten scores? This is incorrect. In statistics, we assume that these ten students are the\nrealizations of some underlying (unknown) random variable Xwith some PDF fX(x).\nThe population mean \u03b8is therefore the expectation E[X], where the expectation is\ntaken w.r.t. fX. The sample average b\u0398, which is the average of the ten numbers, is an\nestimator of the population mean \u03b8.\nRemark 2 . You may be wondering why we are using Student\u2019s t-distribution here\nwhen we do not even know the PDF of X. The answer is that it is an approximation.\nWhen Xis Gaussian, the sample average b\u0398 is a Student\u2019s t-distribution, assuming\nthat the variance is approximated by the sample variance bS. This result is attributed\nto the original paper of William Gosset, who developed Student\u2019s t-distribution.\nThe above example can be solved computationally. An implementation through Python\nis given below, and the MATLAB implementation is straightforward if you translate it from\nthe Python.\n# Python code to generate a confidence interval\nimport numpy as np\nimport scipy.stats as stats\nx = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65])\nTheta_hat = np.mean(x) # Sample mean\nS_hat = np.std(x) # Sample standard deviation\nnu = x.size-1 # degrees of freedom\nalpha = 0.05 # confidence level\nz = stats.t.ppf(1-alph/2, nu)\nCI_L = Theta_hat-z*S_hat/np.sqrt(N)\nCI_U = Theta_hat+z*S_hat/np.sqrt(N)\nprint(CI_L, CI_U)\nWhat is Student\u2019s t-distribution?\n\u0088It was developed by William Gosset in 1908. When he published the paper he\nused the pseudonym Student.\n557", "573": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\n\u0088We use Student\u2019s t-distribution to model the estimator b\u0398\u2019s PDF when the vari-\nance \u03c32is replaced by the sample variance bS2.\n\u0088Student\u2019s t-distribution has a heavier tail than a Gaussian.\n9.1.6 Comparing Student\u2019s t-distribution and Gaussian\nWe now discuss an important theoretical result regarding the relationship between a Stu-\ndent\u2019s t-distribution and Gaussian distribution. The main result is that the standard Gaus-\nsian is a limiting distribution of the tdistribution as the degrees of freedom \u03bd\u2192 \u221e .\nTheorem 9.1. As\u03bd\u2192 \u221e , the Student\u2019s t-distribution approaches the standard Gaus-\nsian distribution:\nlim\n\u03bd\u2192\u221e(\n\u0393\u0000\u03bd+1\n2\u0001\n\u221a\u03bd\u03c0\u0393\u0000\u03bd\n2\u0001\u0012\n1 +y2\n\u03bd\u0013\u2212\u03bd+1\n2)\n=1\u221a\n2\u03c0e\u2212t2\n2. (9.13)\nThe proof of the theorem requires Stirling\u2019s approximation, which is not essential for this\nbook. Feel free to skip it if needed.\nProof . There are two results we need to use:\n\u0088Stirling\u2019s approximation:2\u0393(z)\u2248q\n2\u03c0\nz\u0000z\ne\u0001z.\n\u0088Exponential approximation: (1 +x\nk)\u2212k\u2192e\u2212x, ask\u2192 \u221e .\nWe have that\n\u0393\u0000\u03bd+1\n2\u0001\n\u221a\u03bd\u03c0\u0393\u0000\u03bd\n2\u0001\u2248q\n2\u03c0\n\u03bd+1\n2\u0000\u03bd+1\n2e\u0001\u03bd+1\n2\n\u221a\u03bd\u03c0q\n2\u03c0\n\u03bd\n2\u0000\u03bd\n2e\u0001\u03bd\n2\n=1\u221a\u03bd\u03c0r\u03bd\n\u03bd+ 11\u221ae\u0012\u03bd+ 1\n\u03bd\u0013\u03bd\n2\u221a\u03bd+ 1\u221a\u03bd\n=1\u221a\u03bd\u03c0\u221a\u03bd\u221a\n2e\u0012\u03bd+ 1\n\u03bd\u0013\u03bd\n2\n=1\u221a\n2\u03c0e\u0012\n1 +1\n\u03bd\u0013\u03bd\n2\n.\nPutting a limit of \u03bd\u2192 \u221e , we have that\nlim\n\u03bd\u2192\u221e1\u221a\n2\u03c0e\u0012\n1 +1\n\u03bd\u0013\u03bd\n2\n=1\u221a\n2\u03c0ee1\n2=1\u221a\n2\u03c0.\n2K. G. Binmore, Mathematical analysis: A straightforward approach . Cambridge University Press, 1977.\nSection 17.7.2.\n558", "574": "9.2. BOOTSTRAPPING\nThe other limit follows from the fact that\nlim\n\u03bd\u2192\u221e\u0012\n1 +t2\n\u03bd\u0013\u2212\u03bd+1\n2\n=e\u2212t2\n2.\nCombining the two limits proves the theorem. \u25a1\nEnd of the proof. Please join us again.\nThis theorem has several implications:\n\u0088When Nis large, S2\u2192\u03c32. The Gaussian approximation kicks in, and so Student\u2019s\nt-distribution is more or less the same as the Gaussian.\n\u0088Student\u2019s t-distribution is better for small N, usually N\u226430. If N\u226530, using the\nGaussian approximation suffices.\n\u0088IfXis Gaussian, Student\u2019s t-distribution is an excellent model. If Xis not Gaussian,\nStudent\u2019s t-distribution will have some issues unless Nincreases.\n9.2 Bootstrapping\nWhen estimating the confidence interval, we focus exclusively on the sample average b\u0398 =\n(1/N)PN\nn=1Xn. There are, however, many estimators that are not sample averages. For\nexample, we might be interested in an estimator that estimates the sample median: b\u0398 =\nmedian {X1, . . . , X N}. In such cases, the Gaussian-based analysis or the Student\u2019s t-based\nanalysis we just derived would not work.\nStepping back a little further, it is important to understand the hierarchy of estimation.\nFigure 9.8 illustrates a rough breakdown of the various techniques. On the left-hand side\nof the tree, we have three point estimation methods: MLE, MAP, and MMSE. They are\nso-called point estimation methods because they are reporting a point \u2014 a single value.\nThis stands in contrast to the right-hand side of the tree, in which we report the confidence\ninterval . Note that point estimates and confidence intervals do not conflict with each other.\nThe point estimates are used for the actual engineering solution and the confidence intervals\nare used to report the confidence about the point estimates. Under the branch of confidence\nintervals we discussed sample average. However, if we want to study an estimator that is\nnot the sample average, we need the technique known as the bootstrapping \u2014 a method\nfor estimating the confidence interval. Notably, it does not give you a better point estimate.\nAs we have frequently emphasized, since b\u0398 is a random variable , it has its own PDF,\nCDF, mean, variance, etc. The confidence interval introduced in the previous section pro-\nvides one way to quantify the randomness of b\u0398. Throughout the derivation of the confidence\ninterval we need to estimate the variance Var( b\u0398). For simple problems such as the sample\naverage, analyzing Var( b\u0398) is not difficult. However, if b\u0398 is a more complicated statistic, e.g.,\nthemedian , analyzing Var( b\u0398) may not be as straightforward. Bootstrapping is a technique\nthat is suitable for this purpose.\n559", "575": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nFigure 9.8: Hierarchy of estimation. Bootstrapping belongs to the category of confidence interval. It is\nused to report the confidence intervals for estimators that are not the sample averages.\nWhy is it difficult to provide a confidence interval for estimators such as the median?\nA couple of difficulties arise:\n\u0088Many estimators do not have a simple expression for the variance. For simple esti-\nmators such as the sample average b\u0398 = (1 /N)PN\nn=1Xn, the variance is \u03c32/N. If the\nestimator is the median b\u0398 = median {X1, . . . , X N}, the variance of b\u0398 will depend on\nthe underlying distribution of the Xn\u2019s. If the estimator is something beyond the sam-\nple median, the variance of b\u0398 can be even more complicated to determine. Therefore,\ntechniques such as Central Limit Theorem do not apply here.\n\u0088We typically have only oneset of data points. We cannot re-collect more i.i.d. samples\nto estimate the variance of the estimator. Therefore, our only option is to squeeze the\ninformation from the data we have been given.\nWhen do we use bootstrapping?\n\u0088Bootstrapping is a technique to estimate the confidence interval.\n\u0088We use bootstrapping when the estimator does not have a simple expression for\nthe variance.\n\u0088Bootstrapping allows us to estimate the variance without re-collecting more data.\n\u0088Bootstrapping does not improve your point estimates.\n9.2.1 A brute force approach\nBefore we discuss the idea of bootstrapping, we need to elaborate on the difficulty of esti-\nmating the variance using repeated measurements. Suppose that we somehow have access to\nthe population distribution. Let us denote the CDF of this population distribution by FX,\n560", "576": "9.2. BOOTSTRAPPING\nand the PDF by fX. By having access to the population distribution we can synthetically\ngenerate as many samples Xn\u2019s as we want. This is certainly hypothetical, but let\u2019s assume\nthat it is possible for now.\nIf we have full access to the population distribution, then we are able to draw K\nreplicate datasets X1, . . . ,XKfrom FX:\nX(1)={X(1)\n1, . . . , X(1)\nN} \u223cFX,\nX(2)={X(2)\n1, . . . , X(2)\nN} \u223cFX, (9.14)\n...\nX(K)={X(K)\n1, . . . , X(K)\nN} \u223cFX.\nEach dataset X(K)contains Ndata points, and by virtue of i.i.d. all the samples have the\nsame underlying distribution FX.\nFor each dataset we construct an estimator b\u0398 = g(\u00b7) for some function g(\u00b7). The\nestimator takes the data points of the dataset Xand returns a value. Since we have K\ndatasets, correspondingly we will have Kestimators:\nb\u0398(1)=g(X(1)) =g(X(1)\n1, . . . , X(1)\nN),\nb\u0398(2)=g(X(2)) =g(X(2)\n1, . . . , X(2)\nN), (9.15)\n...\nb\u0398(K)=g(X(K)) =g(X(K)\n1, . . . , X(K)\nN).\nNote that these estimators g(\u00b7) can be anything. It can be the sample average or it can be\nthe sample median. There is no restriction.\nSince we are interested in constructing the confidence interval for b\u0398, we need to analyze\nthe mean and variance of b\u0398. The true mean and the estimated mean of b\u0398 are\nE[b\u0398] = true mean of b\u0398, (9.16)\nM(b\u0398) = estimated mean based on b\u0398(1), . . . ,b\u0398(K)\ndef=1\nKKX\nk=1b\u0398(k)=1\nKKX\nk=1g(X(k)), (9.17)\nrespectively. Similarly, the true variance and the estimated variance of b\u0398 are\nVar[b\u0398] = true variance of b\u0398, (9.18)\nV(b\u0398) = estimated variance based on b\u0398(1), . . . ,b\u0398(K)\ndef=1\nKKX\nk=1\u0010\nb\u0398(k)\u2212M(b\u0398)\u00112\n=1\nKKX\nk=1\u0010\ng(X(k))\u2212M(b\u0398)\u00112\n. (9.19)\n561", "577": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nThese two equations should be familiar: Since b\u0398 is a random variable, and {b\u0398(k)}are i.i.d.\ncopies of b\u0398, we can compute the average of b\u0398(1), . . . ,b\u0398(K)and the corresponding variance.\nAs the number of repeated trials Kapproaches \u221e, the estimated variance V(b\u0398) will converge\nto Var( b\u0398) according to the law of large numbers.\nWe can summarize the procedure we have just outlined. To produce an estimate of the\nvariance, we run the algorithm below.\nAlgorithm 1: Brute force method to generate an estimated variance\n\u0088Assume: We have access to FX.\n\u0088Step 1: Generate datasets X(1), . . . ,X(K)from FX.\n\u0088Step 2: Compute M(b\u0398) and V(b\u0398) based on the samples.\n\u0088Output: The estimated variance is V(b\u0398).\nThe problem, however, is that we only have onedataset X(1). We do not have access to\nX(2), . . . ,X(K), and we do not have access to FX. Therefore, we are not able to approxi-\nmate the variance using the above brute force simulation. Bootstrapping is a computational\ntechnique to mimic the above simulation process by using the available data in X(1).\n9.2.2 Bootstrapping\nThe idea of bootstrapping is illustrated in Figure 9.9 . Imagine that we have a population\nCDF FXand PDF fX. The dataset we have in hand, X, is a collection of the random realiza-\ntions of the random variable X. This dataset Xcontains Ndata points X={X1, . . . , X N}.\nFigure 9.9: A conceptual illustration of bootstrapping. Given the observed dataset X, we synthetically\nconstruct Kbootstrapped datasets (colored in yellow) by sampling with replacement from X. We\nthen compute the estimators, e.g., computing the median, for every bootstrapped dataset. Finally, we\nconstruct the estimator\u2019s histogram (in blue) to compute the bootstrapped mean and variance.\nIn bootstrapping, we synthesize Kbootstrapped datasets Y(1), . . . ,Y(K), where each\nbootstrapped dataset Y(k)consists of Nsamples redrawn from X. Essentially, we draw with\n562", "578": "9.2. BOOTSTRAPPING\nreplacement Nsamples from the observed dataset X:\nY(1)={Y(1)\n1, . . . , Y(1)\nN}=Nrandom samples from X,\n...\nY(K)={Y(K)\n1, . . . , Y(K)\nN}=Nrandom samples from X.\nAfterward, we construct our estimator b\u0398 according to our desired function g(\u00b7). For example,\nifg(\u00b7) = median, we have\nb\u0398(1)\nboot=g(Y(1)) = median( Y(1)),\n...\nb\u0398(K)\nboot=g(Y(K)) = median( Y(K)).\nThen, we define the bootstrapped mean and the bootstrapped variance as\nMboot(b\u0398) =1\nKKX\nk=1b\u0398(k)\nboot, (9.20)\nVboot(b\u0398) =1\nKKX\nk=1\u0010\nb\u0398(k)\nboot\u2212Mboot(b\u0398)\u00112\n. (9.21)\nThe procedure we have just outlined can be summarized as follows.\nAlgorithm 2: Bootstrapping to generate an estimated variance\n\u0088Assume: We do NOT have access to FX, but we have one dataset X.\n\u0088Step 1: Generate datasets Y(1), . . . ,Y(K)fromX, by sampling with replacement from\nX.\n\u0088Step 2: Compute Mboot(b\u0398) and Vboot(b\u0398) based on the samples.\n\u0088Output: The bootstrapped variance is Vboot(b\u0398).\nThe only difference between this algorithm and the previous one is that we are not synthe-\nsizing data from the population but rather from the observed dataset X.\nWhat makes bootstrapping work? The basic principle of bootstrapping is based on\nthree approximations:\nVarF(b\u0398)(a)\u2248Vfull(b\u0398)(b)\u2248\nVarbF(b\u0398)(c)\u2248Vboot(b\u0398)\nIn this set of equations, the ultimate quantity we want to know is Var F(b\u0398), which is the\nvariance of b\u0398 under F. (By \u201cunder F\u201d we mean that the variance was found by integrating\nwith respect to the distribution FX.) However, since we do not have access to F, we have\n563", "579": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nto approximate Var F(b\u0398) by Vfull(b\u0398).Vfull(b\u0398) is the sample variance computed from the\nKhypothetical datasets X(1), . . . ,X(K). We call it \u201cfull\u201d because we can generate as many\nhypothetical datasets as we want. It is marked as the approximation ( a) above.\nIn the bootstrapping world, we approximate the underlying distribution Fby some\nother distribution bF. For example, if Fis the CDF of a Gaussian distribution, we can\nchoose bFto be the finite-sample staircase function approximating F. In our case, we use the\nobserved dataset Xto serve as a proxy bFtoF. This is the second approximation, marked by\n(b). Normally, if you have a reasonably large X, it is safe to assume that this finite-sample\ndataset Xhas a CDF bFthat is close to the true CDF F.\nThe third approximation is to find a numerical estimate Var bF(b\u0398) via the simulation\nprocedure we have just outlined. This is essentially the same line of argument for ( a) but\nnow applied to the bootstrapping world. We mark this approximation by ( c). Its goal is to\napproximate Var bF(b\u0398) via Vboot(b\u0398).\nThe three approximations have their respective influence on the accuracy of the boot-\nstrapped variance:\nHow does bootstrapping work?\n\u0088It is based on three approximations:\n\u0088(a): A hypothetical approximation. The best we can do is that we have access\ntoF. It is practically impossible to achieve, but it gives us intuition.\n\u0088(b): Approximate FbybF, where bFis the empirical CDF of the observed data.\nThis is usually the source of error. The approximation error reduces when you\nuse more samples to approximate F.\n\u0088(c): Approximate the theoretical bootstrapped variance by a finite approxima-\ntion. This approximation error is usually small because you can generate as many\nbootstrapped datasets as you want.\nOne \u201cmysterious\u201d property of bootstrapping is the sampling with replacement scheme\nused to synthesize the bootstrapped samples. The typical questions are:\n\u0088(1)Why does sampling from the observed dataset Xlead to meaningful boot-\nstrapped datasets Y(1), . . . ,Y(K)? To answer this question we consider the following\ntoy example. Suppose we have a dataset Xcontaining N= 20 samples, as shown\nbelow.\nX = [0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\nThis dataset is generated from a random variable Xwith a PDF bfhaving three states:\n0 (30%), 1 (20%), 2 (50%). As we draw samples from X, the percentage of the states\nwill determine the likelihood of one state being drawn. For example, if we randomly\npick a sample YnfromX, we have a 30% chance of having Ynto be 0, 20% chance\nof having it to be 1, and 50% chance of having it to be 2. Therefore, the PDF of Yn\n(the randomly drawn sample from X) will be 0 (30%), 1 (20%), 2 (50%), the same\nas the original PDF. If you think about this problem more deeply, by \u201csampling with\nreplacement\u201d we essentially assign each Xnwith an equal probability of 1 /N. If one\nof the states is more popular, the individual probabilities will add to form a higher\nprobability mass.\n564", "580": "9.2. BOOTSTRAPPING\n\u0088(2)Why can\u2019t we do sampling without replacement, aka permutation ? We need to\nunderstand that sampling without replacement is the same as permuting the data in X.\nBy permuting the data in X, the simple probability assignments such as P[X= 0] =6\n20,\nP[X= 1] =4\n20andP[X= 2] =10\n20will be destroyed. Moreover, permuting the data\ndoes not change the mean and variance of the data because we are only shuffling the\norder. As far as constructing the confidence interval is concerned, shuffling the order\nis not useful.\nOn computers it is easy to generate the bootstrapped dataset, along with their mean\nand variance. In MATLAB the key step is to call a forloop. Inside the forloop, we draw\nNrandom indices randi from 1 to Nand pick the samples. The estimator Thetahat is then\nconstructed by calling your target estimator function g(\u00b7). In this example the estimator is\nthe median. After the forloop, we compute the mean and variance of b\u0398. These are the\nbootstrapped mean and variance, respectively.\n% MATLAB code to estimate a bootstrapped variance\nX = [72, 69, 75, 58, 67, 70, 60, 71, 59, 65];\nN = size(X,2);\nK = 1000;\nThetahat = zeros(1,K);\nfor i=1:K % repeat K times\nidx = randi(N,[1, N]); % sampling w/ replacement\nY = X(idx);\nThetahat(i) = median(Y); % estimator\nend\nM = mean(Thetahat) % bootstrapped mean\nV = var(Thetahat) % bootstrapped variance\nThe Python commands are similar. We call np.random.randint to generate random\nintegers and we pick samples according to Y = X[idx] . After generating the bootstrapped\ndataset, we compute the bootstrap estimators Thetahat .\n# Python code to estimate a bootstrapped variance\nimport numpy as np\nX = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65])\nN = X.size\nK = 1000\nThetahat = np.zeros(K)\nfor i in range(K):\nidx = np.random.randint(N, size=N)\nY = X[idx]\nThetahat[i] = np.median(Y)\nM = np.mean(Thetahat)\nV = np.var(Thetahat)\nAfter we have constructed the bootstrapped variance, we can define the bootstrapped\nstandard error as\nbseboot=q\nVboot(b\u0398). (9.22)\n565", "581": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nAccordingly we define the bootstrapped confidence interval as\nI=\u0002b\u0398\u2212z\u03b1bseboot,b\u0398 +z\u03b1bseboot\u0003\n, (9.23)\nwhere z\u03b1is the critical value of the Gaussian.\nThe validity of the confidence intervals constructed by bootstrapping is subject to\nthe validity of z\u03b1. Ifb\u0398 is roughly a Gaussian, the bootstrapped confidence interval will be\nreasonably good. If b\u0398 is not Gaussian, there are advanced methods to replace z\u03b1with better\nestimates. This topic is beyond the scope of this book; we refer interested readers to Larry\nWasserman, All of Statistics , Springer 2003, Chapter 8.\n9.3 Hypothesis Testing\nImagine that you are a vaccine company developing COVID-19 vaccines. You gave the\nvaccine to 934 patients, and 928 patients have developed antigens. How confident can you\nbe that your vaccine is effective? Questions like this are becoming more common nowadays\nin situations in which we need to make statistically informed choices between YES and NO.\nThe subject of this section is hypothesis testing \u2014 a principled statistical procedure used\nto evaluate statements that should be accepted or rejected.\n9.3.1 What is a hypothesis?\nA hypothesis is a statement that requires testing by observation to determine whether it is\ntrue or false. A few examples:\n\u0088The coin is unbiased.\n\u0088Students entering the graduate program have GPA \u22653.\n\u0088More people like orange juice than lemonade.\n\u0088Algorithm A performs better than Algorithm B.\nAs you can see from these examples, a hypothesis is something we can test based on the\ndata. Therefore, being \u201ccorrect\u201d or \u201cwrong\u201d depends on the statistics we have and the cutoff\nthreshold. Accepting or rejecting a hypothesis does not mean that the statement is correct\nor wrong, since the truth is unknown. If we accept a hypothesis, we have made a better\ndecision solely based on the statistical evidence. It is possible that tomorrow when you have\ncollected more data we may reject a previously accepted hypothesis.\nThe procedure for testing whether a hypothesis should be accepted or rejected is known\nashypothesis testing . In hypothesis testing, we often have two opposite hypotheses:\n\u0088H0: Null hypothesis. It is the \u201cstatus quo\u201d, or the current status.\n\u0088H1: Alternative hypothesis. It is the alternative to the null hypothesis.\nTo better understand hypothesis testing, consider a courthouse. By default, any person\nbeing prosecuted is assumed to be innocent. The police need to show sufficient evidence in\norder to prove the person guilty. The null hypothesis is the default assumption. Hypothesis\ntesting asks whether we have strong enough evidence to reject the null hypothesis. If our\nevidence is not strong enough, we must assume that the null hypothesis is possibly true.\n566", "582": "9.3. HYPOTHESIS TESTING\nExample 9.6 . Suggest a null hypothesis and an alternative hypothesis regarding\nwhether a coin is unbiased.\nSolution : Let \u03b8be the probability of getting a head.\n\u0088H0:\u03b8= 0.5, and H1:\u03b8 >0.5. This is a one-sided alternative.\n\u0088H0:\u03b8= 0.5, and H1:\u03b8 <0.5. This is another one-sided alternative.\n\u0088H0:\u03b8= 0.5, and H1:\u03b8\u0338= 0.5. This is a two-sided alternative.\nPractice Exercise 9.3 . Suggest a null and an alternative hypothesis regarding whether\nmore than 62% of people in the United States use Microsoft Windows.\nSolution : Let \u03b8be the proportion of people using Microsoft Windows in United States.\n\u0088H0:\u03b8\u22650.62, and H1:\u03b8 <0.62. This is a one-sided alternative.\nPractice Exercise 9.4 . Suggest a null and an alternative hypothesis regarding whether\nself-checkout at Walmart is faster than using a cashier.\nSolution : Let \u03b8be the proportion of people that check out faster with self-checkout..\n\u0088H0:\u03b8\u22650.5, and H1:\u03b8 <0.5. This is a one-sided alternative.\n9.3.2 Critical-value test\nIn hypothesis testing, there are two major approaches: the critical-value test , and the\np-value test . The two tests are more or less equivalent. If you reject the null hypothesis using\nthe critical-value test, you will reject the hypothesis using the p-value. In this subsection,\nwe will discuss the critical-value test. Let us consider a toy problem:\nSuppose that we have a 4-sided die and our goal is to test whether the die is unbiased.\nTo do so, we define the null and the alternative hypotheses as\n\u0088H0:\u03b8= 0.25, which is our default belief.\n\u0088H1:\u03b8 >0.25, which is a one-sided alternative.\nThere is no particular reason for considering the one-sided alternative other than the fact\nthat the calculation is slightly easier. You are welcome to consider the two-sided alternative.\nWe must obtain data prior to conducting any hypothesis testing. Let\u2019s assume that we\nhave thrown the die N= 1000 times. We find that \u201c3\u201d appears 290 times (we could just as\nwell have chosen 1, 2, or 4). We let X1, . . . , X 1000be the N= 1000 binary random variables\nrepresenting whether we have obtained a \u201c3\u201d or not. If the true probability is \u03b8= 0.25, then\nwe will have P[Xn= 3] = \u03b8= 0.25 and P[Xn\u0338= 3] = 1 \u2212\u03b8= 0.75. We know that we cannot\naccess the true probability, so we can only construct an estimator of the probability:\nb\u0398 =1\nNNX\nn=1Xn.\n567", "583": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nIn this experiment, we can show that b\u0398 = 290 /1000 = 0 .29.\nTo make our problem slightly easier, we pretend that we know the variance Var[ Xn].\nIn practice, we certainly do not know Var[ Xn], and so we need to estimate the variance. If\nwe knew the variance, it should be Var[ Xn] =\u03b8(1\u2212\u03b8) = 0 .25(1\u22120.25) = 0 .1875, because\nXnis a Bernoulli random variable with a mean \u03b8.\nThe question asked by hypothesis testing is: How far is \u201c b\u0398 = 0 .29\u201d from \u201c \u03b8= 0.25\u201d?\nIf the statistic generated by our data, b\u0398 = 0 .29, is \u201cfar\u201d from the hypothesized \u03b8= 0.25,\nthen we need to reject H0because H0says that \u03b8= 0.25. However, if there is no strong\nevidence that \u03b8 >0.25, we will need to assume that H0may possibly be true. So the key\nquestion is what is meant by \u201cfar\u201d.\nFor many problems like this one, it is possible to analyze the PDF of b\u0398. Since b\u0398 is the\nsample average of a sequence of Bernoulli random variables, it follows that b\u0398 is a binomial\n(with a scaling constant 1 /N). IfNis large enough, e.g., N\u226530, the Central Limit Theorem\ntells us that b\u0398 is also very close to a Gaussian. Therefore, we can more or less claim that\nb\u0398\u223cGaussian\u0012\n\u03b8,\u03c32\nN\u0013\n.\nWith a simple translation and scaling, we can normalize b\u0398 to obtain bZ:\nbZ=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN\u223cGaussian (0 ,1).\nFigure 9.10 illustrates the range of values for this problem. There are two axes: the b\u0398-\naxis (which is the estimator) and the bZ-axis (which is the normalized variable). The values\ncorresponding to each axis are shown in the figure. For example. b\u0398 = 0 .29 is equivalent\ntobZ= 2.92, and b\u0398 = 0 .25 is equivalent to bZ= 0, etc. Therefore, when we ask how far\n\u201cb\u0398 = 0 .29\u201d is from \u201c \u03b8= 0.25\u201d, we can map this question from the b\u0398-axis to the bZ-axis,\nand ask the relative position of bZfrom the origin.\nFigure 9.10: The mapping between b\u0398andbZ. To decide whether we want to reject or keep H0, the\ncritical-value approach compares bZrelative to the critical value z\u03b1.\nOn a computer, obtaining these values is quite straightforward. Using MATLAB, find-\ningbZcan be done by calling the following commands. The Python code is analogous.\n568", "584": "9.3. HYPOTHESIS TESTING\n% MATLAB command to estimate the Z_hat value.\nTheta_hat = 0.29; % Your estimate\ntheta = 0.25; % Your hypothesis\nsigma = sqrt(theta*(1-theta)); % Known standard deviation\nN = 1000; % Number of samples\nZ_hat = (Theta_hat - theta)/(sigma/sqrt(N));\n# Python command to estimate the Z_hat value\nimport numpy as np\nTheta_hat = 0.29 # Your estimate\ntheta = 0.25 # Your hypothesis\nN = 1000 # Number of samples\nsigma = np.sqrt(theta*(1-theta)) # Known standard deviation\nZ_hat = (Theta_hat - theta)/(sigma / np.sqrt(N))\nprint(Z_hat)\nOne essential element of hypothesis testing is the cutoff threshold, which is defined\nthrough the critical level \u03b1. It is the area under the curve of the PDF of bZ. Typically,\n\u03b1is chosen to be a small value, such as \u03b1= 0.05 (corresponding to a 5% margin). The\ncorresponding cutoff is known as the critical value . It is defined as\nz\u03b1= cutoff location where area under the curve is \u03b1.\nIfbZis Gaussian(0,1) and if we are looking at the right-hand tail, it follows that\nz\u03b1= \u03a6\u22121(1\u2212\u03b1). (9.24)\nIn our example, we find that z0.05= 1.65, which is marked in Figure 9.10 .\nOn computers, determining the critical value z\u03b1is straightforward. In MATLAB the\ncommand is icdf, and in Python the command is stats.norm.ppf .\n% MATLAB code to compute the critical value\nalpha = 0.05;\nz_alpha = icdf(\u2019norm\u2019, 1-alpha, 0, 1);\n# Python code to compute the critical value\nimport scipy.stats as stats\nalpha = 0.05\nz_alpha = stats.norm.ppf(1-alpha, 0, 1)\nDo we have enough evidence to reject H0in this example? Of course! The estimated\nvalueb\u0398 = 0 .29 is equivalent to bZ= 2.92, which is much too far from the cutoff z\u03b1= 1.65.\nIn other words, we conclude that at a 5% critical level we have strong evidence to believe\nthat the die is biased. Therefore, we need to reject H0.\nThis conclusion makes a lot of sense if you think about it carefully. The estimator\nb\u0398 = 0 .29 is obtained from N= 1000 independent experiments. If we were only conducting\nN= 20 experiments, it might be consistent with the null hypothesis to have b\u0398 = 0 .29.\n569", "585": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nHowever, if we have N= 1000 experiments, having b\u0398 = 0 .29 does not seem likely when\nthere is no systematic bias. If there is no systematic bias, the estimator b\u0398 should slightly\njitter around b\u0398 = 0 .25, but it is quite unlikely to vary wildly to b\u0398 = 0 .29. Thus, based on\nthe available statistics, we decide to reject the null hypothesis.\nThe decision based on comparing the critical value is known as the critical-value test .\nThe idea (for testing a right-hand tail of a Gaussian random variable) is summarized in\nthree steps:\nHow to conduct a critical-value test\n\u0088Set a critical value z\u03b1. Compute bZ= (b\u0398\u2212\u03b8)/(\u03c3/\u221a\nN).\n\u0088IfbZ\u2265z\u03b1, then reject H0.\n\u0088IfbZ < z \u03b1, then keep H0.\nIf you are testing a left-hand tail , you can switch the order of the inequalities.\nThe critical-value test belongs to a larger family of testing procedures based on deci-\nsion theory. To give you a preview of the general theory of hypothesis testing, we define a\ndecision rule , a function that maps a realization of the estimator to a binary decision space.\nIn our problem the estimator is bZ(or equivalently b\u0398). We denote its realization by bz. The\nbinary decision space is {H0, H1}, corresponding to whether we want to claim H0orH1.\nClaiming H0is equivalent to keeping H0, and claiming H1is equivalent to rejecting H0.\nFor the critical-value test, the decision rule \u03b4(\u00b7) :R\u2192 {0,1}is given by the equation (for\ntesting a right-hand tail):\n\u03b4(bz) =(\n1,ifbz\u2265z\u03b1, (claim H1),\n0,ifbz < z \u03b1, (claim H0).(9.25)\nExample 9.7 . It was found that only 35% of the children in a kindergarten eat\nbroccoli. The teachers conducted a campaign to get more kids to eat broccoli, after\nwhich it was found that 390 kids out of 1009 kids reported that they had eaten broccoli.\nHas the campaign successfully increased the number of kids eating broccoli? Assume\nthat the standard deviation is known.\nSolution . We setup the null and the alternative hypothesis.\nH0:\u03b8= 0.35, H 1:\u03b8 >0.35.\nWe construct an estimator b\u0398 = (1 /N)PN\nn=1Xn, where Xnis Bernoulli with proba-\nbility \u03b8. Based on \u03b8,\u03c32=\u03b8(1\u2212\u03b8) = 0 .227. (Again, in practice we do not know the\ntrue variance, but in this problem we pretend that we know it.)\nBy the Central Limit Theorem, b\u0398 is roughly a Gaussian. We compute the test\nstatistics b\u0398 =390\n1009= 0.387. Standardization gives bZ=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN= 2.432. At a 5%\ncritical level, we have that z\u03b1= 1.65. So bZ= 2.432>1.65 = z\u03b1, and hence we need\nto reject the null hypothesis. Even if we choose a 1% critical level so that z\u03b1= 2.32,\nour estimator bZ= 2.432>2.32 = z\u03b1will still reject the null hypothesis.\n570", "586": "9.3. HYPOTHESIS TESTING\nA graphical illustration of this problem is shown in Figure 9.11 . It can be seen\nthatb\u0398 = 0 .387 is actually quite far away from the cutoff 1.65. Thus, we need to reject\nthe null hypothesis.\nFigure 9.11: Example of a critical-value test. In this example, the test statistic b\u0398 = 0 .387is\nequivalent to bZ= 2.432, which is significantly larger than the cutoff z\u03b1= 1.65. Therefore, we\nhave strong evidence to reject the null hypothesis, because the probability of obtaining b\u0398 = 0 .387\nis very low if H0is true.\n9.3.3 p-value test\nAn alternative to the critical-value test is the p-value test. Instead of looking at the cutoff\nvalue z\u03b1, we inspect the probability of obtaining our observation if H0is true. To understand\nhow the p-value test works, we consider another toy problem.\nSuppose that we have two hypotheses about flipping a coin:\n\u0088H0:\u03b8= 0.9, which is our default belief.\n\u0088H1:\u03b8 <0.9, which is a one-sided alternative.\nIt was found that with N= 150 coin flips, the coin landed on heads 128 times. Thus the\nestimator is b\u0398 =128\n150= 0.853. Then, by following our previous procedures, we have that\nbZ=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN=0.853\u22120.9q\n0.9(1\u22120.9)\n150=\u22121.92.\nAt this point we can follow the previous subsection by computing the critical value z\u03b1\nand make the decision. However, let\u2019s take a different route. We want to know what is the\nprobability under the curve if we integrate the PDF of bZfrom\u2212\u221eto\u22121.92. This is easy.\nSincebZis Gaussian(0 ,1), it follows from the CDF of a Gaussian that\nP[bZ\u2264 \u22121.92]|{z}\np-value= 0.0274.\nReferring to Figure 9.12 , the value 0.0274 is the pink area under the curve, which is the\nPDF of bZ. Since the area under the curve is less than the critical level \u03b1(say 5%), we reject\nthe null hypothesis.\nOn computers, computing the p-value is done using the CDF commands.\n571", "587": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nFigure 9.12: Thep-value test asks us to look at the probability of bZ\u2264bz. If this probability (the p-value)\nis less than the critical level \u03b1, we have significant evidence to reject the null hypothesis.\n% MATLAB code to compute the p-value\np = cdf(\u2019norm\u2019, -1.92, 0, 1);\n# Python code to compute the p-value\nimport scipy.stats as stats\np = stats.norm.cdf(-1.92,0,1)\nIn this example, the probability P[bZ\u2264 \u22121.92] is known as the p-value . It is the\nprobability of bZ\u2264z, under the distribution mandated by the null hypothesis, where z\nis the (normalized) estimated value based on data. Using our example, zis\u22121.92. By\n\u201cdistribution mandated by the null hypothesis\u201d we mean that the PDF of bZis the PDF that\nthe null hypothesis wants. In the above example the PDF is Gaussian(0 ,1), corresponding\nto Gaussian( \u03b8, \u03c3/\u221a\nN) forb\u0398.\nMore formally, the p-value for a left-hand tail test is defined as\np-value( bz) =P[bZ\u2264bz],\nwhere bzis the random realization of bZestimated from the data. The decision rule based\non the p-value is (for the left-hand tail):\n\u03b4(bz) =(\n1,P[bZ\u2264bz]< \u03b1 (claim H1),\n0,P[bZ\u2264bz]\u2265\u03b1 (claim H0).(9.26)\nIf the alternative hypothesis is right-handed, then the probability becomes P[bZ\u2265bz] instead.\nRelationship between critical-value and p-value tests . There is a one-to-one corre-\nspondence between the p-value and the critical value. In the p-value test, if bZis Gaussian,\nit follows that\np-value = P[bZ\u2264bz] = \u03a6(bz),\n572", "588": "9.3. HYPOTHESIS TESTING\nwhere \u03a6 is CDF of the standard Gaussian. Taking the inverse, the corresponding bzis\nbz= \u03a6\u22121(p-value) .\nIn practice, we do not need to take any inverse of the p-value to obtain bzbecause it is\ndirectly available from the data.\nTo test the p-value, we compare it with the critical level \u03b1by checking\np-value < \u03b1.\nTaking the inverse of both sides, it follows that the decision rule is equivalent to\n\u03a6\u22121(p-value)|{z}\nbz<\u03a6\u22121(\u03b1)|{z}\nz\u03b1,\nwhere the quantity on the right-hand side is the critical value z\u03b1. Therefore, if the test\nstatistic fails in the p-value test it will also fail in the critical-value test, and vice versa.\nWhat is the difference between the critical-value test and p-value test?\n\u0088Critical-value test: Compare w.r.t. critical value, which is the cutoff on the Z-\naxis.\n\u0088p-value test: Compare w.r.t. \u03b1, which is the probability.\n\u0088Both will give you the same statistical conclusion. So it does not matter which\none you use.\nExample 9.8 . We flip a coin for N= 150 times and find that 128 are heads. Consider\ntwo hypotheses\n\u0088H0:\u03b8= 0.9, which is our default belief.\n\u0088H1:\u03b8\u0338= 0.9, which is a two-sided alternative.\nFor a critical level of \u03b1= 0.05, shall we keep or reject H0?\nSolution . We know that b\u0398 = 128 /150 = 0 .853. The normalized statistic is\nbZ=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN=0.853\u22120.9q\n0.9(1\u22120.9)\n150=\u22121.92.\nTo compute the p-value, we observe that the two-sided test means that we consider\nthe two tails. Thus, we have\np-value = P[|bZ|>1.92]\n= 2\u00d7P[bZ >1.92]\n= 2\u00d70.0274 = 0 .055.\n573", "589": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nFor a critical level of \u03b1= 0.05, the p-value is larger. This means that the probability\nof obtaining |Z|>1.92 is not extreme enough. Therefore, we do not have sufficient\nevidence to reject the null hypothesis.\nIf we take the critical-value test, we will reach the same conclusion. The critical\nvalue for \u03b1= 0.05 is determined by taking the inverse CDF at 1 \u22120.025, giving\nz\u03b1= \u03a6\u22121\u0010\n1\u2212\u03b1\n2\u0011\n= 1.96.\nSincebZ= 1.92 has not passed this threshold, we conclude that there is not enough\nevidence to reject the null hypothesis.\nFigure 9.13: Example of a two-sided test using the p-value and the z\u03b1-value.\n9.3.4 Z-test and T-test\nThe critical-value test and the p-value tests are generic tools for hypothesis testing. In this\nsubsection we introduce the Z-test and the T-test. It is important to understand that the\nZ-test and the T-test refer to the distributional assumptions we make about the variance.\nThey define the distribution we use to conduct the test but not the tools. In fact, both the\nZ-test and the T-test can be implemented using the critical-value test or the p-value test.\nFigure 9.14 illustrates the hierarchy of the tests.\nFigure 9.14: When conducting a hypothesis testing of the sample average, we may or may not know\nthe variance. If we know the variance, we use the Gaussian distribution to conduct either a p-value test\nor a critical-value test. If we do not know the variance, we use Student\u2019s t-distribution.\nThe difference between the Gaussian distribution and the Tdistribution is mainly\n574", "590": "9.3. HYPOTHESIS TESTING\nattributable to the knowledge about the population variance. If the variance is known,\nthe distribution of the estimator (which in our case is the sample average) is Gaussian. If\nthe variance is estimated from the sample, the distribution of the estimator will follow a\nStudent\u2019s t-distribution.\nTo introduce the Z-test and the T-test we consider the following two examples. The\nfirst example is a Z-test.\nExample 9.9 (Z-test). Suppose we have a Gaussian random variable with unknown\nmean \u03b8and a known variance \u03c3= 11.6. We draw N= 25 samples and construct an\nestimator b\u0398 = 80 .94. We propose two hypotheses:\n\u0088H0:\u03b8= 85, which is our default belief.\n\u0088H1:\u03b8 <85, which is a one-sided alternative.\nFor a critical level of \u03b1= 0.05, shall we keep or reject the null hypothesis?\nSolution . The test statistic is\nbZ=b\u0398\u2212\u03b8\n\u03c3/\u221a\nN=\u22121.75.\nSince the individual samples are assumed to follow a Gaussian, the sample average b\u0398\nis also a Gaussian. Hence, bZis distributed according to Gaussian(0 ,1).\nFigure 9.15: A one-sided Z-test using the p-value and the z\u03b1-value.\nFor a critical level of 0 .05, a one-sided critical value is\nz\u03b1= \u03a6\u22121(1\u2212\u03b1) =\u22121.645.\nSincebZ=\u22121.75, which is more extreme than the critical value, we conclude that we\nneed to reject H0.\nIf we use the p-value test, we have that the p-value is\nP[bZ\u2264 \u22121.75] = \u03a6( \u22121.75) = 0 .0401.\n575", "591": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nSince the p-value is smaller than the critical level \u03b1= 0.05, it implies that bZ=\u22121.75\nis more extreme. Hence, we reject H0.\nThe following example is a T-test. In a T-test we do not know the population variance\nbut only know the sample variance bS. Thus the test statistic we use is a Trandom variable.\nExample 9.10 (T-test). Suppose we have a Gaussian random variable with unknown\nmean \u03b8and an unknown variance \u03c3. We draw N= 100 samples and construct an\nestimator b\u0398 = 130 .1, with a sample variance bS= 21.21. We propose two hypotheses:\n\u0088H0:\u03b8= 120, which is our default belief.\n\u0088H1:\u03b8\u0338= 120, which is a two-sided alternative.\nFor a critical level of \u03b1= 0.05, shall we keep or reject the null hypothesis?\nSolution . The test statistic is\nbT=b\u0398\u2212\u03b8\nbS/\u221a\nN= 4.762.\nNote that while the sample average b\u0398 is a Gaussian, the test statistic bTis distributed\naccording to a Tdistribution with N\u22121 degrees of freedom. For a critical level of\n0.05, a two-sided critical value is\nt\u03b1= \u03a8\u22121\n99\u0010\n1\u2212\u03b1\n2\u0011\n= 1.984.\nSincebT= 4.762, which is more extreme than the critical value, we conclude that we\nneed to reject H0.\nIf we use the p-value test, we have that the p-value is\nP[|bT| \u22654.762] = 2 \u00d7P[bT\u22654.762] = 3 .28\u00d710\u22126.\nSince the p-value is (much) smaller than the critical level \u03b1= 0.05, it implies that\n|bT| \u22654.762 is quite extreme. Hence, we reject H0.\nFigure 9.16: A two-sided T-test using the p-value and the z\u03b1-value.\nFor this example, the MATLAB and Python commands to compute t\u03b1and the p-value are\n576", "592": "9.4. NEYMAN-PEARSON TEST\n% MATLAB code to compute critical-value and p-value\nt_alpha = icdf(\u2019t\u2019, 1-0.025, 99);\np = 1-cdf(\u2019t\u2019, 4.762, 99);\n# Python code to compute critical value and p-value\nimport scipy.stats as stats\nt_alpha = stats.t.ppf(1-0.025,99)\np = 1-stats.t.cdf(4.762,99)\nWhat are the Z-test and the T-test?\n\u0088Both are hypothesis testings for the sample averages.\n\u0088Z-test: Assume known variance. Hence, use the Gaussian distribution.\n\u0088T-test: Assume unknown variance. Hence, use the Student\u2019s t-distribution.\nRemark . We are exclusively analyzing the sample average in this section. There are other\ntypes of estimators we can analyze. For example, we can discuss the difference between the\ntwo means, the ratio of two random variables, etc. If you need tools for these more advanced\nproblems, please refer to the reference section at the end of this chapter.\n9.4 Neyman-Pearson Test\nThe hypothesis testing procedures we discussed in the previous section are elementary in\nthe sense that we have not discussed much theory. This section aims to fill the gap so that\nyou can understand hypothesis testing from a broader perspective. This generalization will\nalso help to bridge statistics to other disciplines such as classification in machine learning\nand detection in signal processing. We call this theoretical analysis the Neyman-Pearson\nframework .\n9.4.1 Null and alternative distributions\nWhen we discussed hypothesis testing in the previous section, we focused exclusively on the\nnull hypothesis H0. Regardless of whether we are studying the Z-test or the T-test, using\nthe critical value or the p-value, all the distributions are associated with the distribution\nunder H0.\nWhat do we mean by \u201cdistribution under H0\u201d? Using b\u0398 as an example, the PDF of\nb\u0398 is assumed to be Gaussian( \u03b8, \u03c32/N). This Gaussian, centered at \u03b8, is the distribution\nassumed under H0. As we decide whether to keep or reject H0, we look at the critical value\nand the p-value of the test statistic under Gaussian( \u03b8, \u03c32/N).\nImportantly, the analysis of hypothesis testing is not just about H0\u2014 it is also about\nthe alternative hypothesis H1, which uses a different PDF. For example, H1could use\n577", "593": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nGaussian( \u03b8\u2032, \u03c32/N) for\u03b8\u2032> \u03b8. Therefore, for the same testing statistic b\u0398, we can check how\nclose it is to H1.\nTo capture both distributions, we define\nf0(y) =fY(y|H0),\nf1(y) =fY(y|H1).\nThe first PDF defines the distribution when the true model is H0. The second PDF is the\ndistribution when the true model is H1.\nExample 9.11 . Consider an estimator Y\u223cGaussian( \u03b8, \u03c32/N). Define two hypotheses\nH0:\u03b8= 120 and H1:\u03b8 >120. The two PDFs are then\nf0(y) =fY(y|H0) = Gaussian(120 , \u03c32/N),\nf1(y) =fY(y|H1) = Gaussian( \u03b8\u2032, \u03c32/N), \u03b8\u2032>120.\nA graph of the two distributions is shown in Figure 9.17 . In this figure we plot the\nPDF under the null hypothesis and the PDF under an alternative hypothesis. The decision\nis based on the null, where we marked the critical value.\nFigure 9.17: The PDF of the estimator under hypotheses H0andH1. The yellow region defines the\nrejection zone R\u03b1. If the estimator has a realization Y=ythat falls into the rejection zone R\u03b1, we\nneed to reject H0.\nStudents are frequently confused about the exact equation of the PDF under H1. If\nthe alternative hypothesis is defined as \u03b8 >120, shall we define the PDF as a Gaussian\ncentered at 130 or 151.4? They are both valid alternative hypotheses. The answer is that\nwe are going to express all equations based on \u03b8\u2032. For example, if we want to analyze the\nprediction error (this term will be explained later), the prediction error will be a function\nof\u03b8\u2032. If\u03b8\u2032is close to \u03b8, we will expect a larger prediction error. However, if \u03b8\u2032is far away\nfrom \u03b8, the prediction error may be small.\nWhenever we discuss hypothesis testing, a decision rule is always implied. A decision\nrule is a mapping \u03b4(\u00b7) from sample space Yof the test statistic Y(orb\u0398 if you prefer) to the\n578", "594": "9.4. NEYMAN-PEARSON TEST\nbinary space of {0,1}:\n\u03b4(y) =(\n1, ify\u2208R\u03b1,(we will reject H0),\n0, ify\u0338\u2208R\u03b1,(we will keep H0).(9.27)\nHere R\u03b1is the rejection zone . For example, in a one-sided testing at a critical level \u03b1, the\nrejection zone is R\u03b1={y\u2265\u03a6\u22121(1\u2212\u03b1)}. Therefore, as long as y\u2265\u03a6\u22121(1\u2212\u03b1), we will\nreject the null hypothesis. Otherwise, we will keep the null hypothesis. A rejection zone can\nbe one-sided, two-sided, or even more complicated.\nExample 9.12 . Consider H0:\u03b8= 0.35 and H1:\u03b8 >0.35. It was found that the\nsample average over 1009 samples is b\u0398 = 0 .387, with \u03c32= 0.227. The normalized test\nstatistic is bZ=\u221a\nN(b\u0398\u2212\u03b8)/\u03c3= 2.432. At a 5% critical level, define the decision rule\nbased on the critical-value approach.\nSolution . If\u03b1= 0.05, it follows that z\u03b1= \u03a6\u22121(1\u22120.05) = 1 .65. Therefore, the\ndecision rule is\n\u03b4(bz) =(\n1, ifbz\u22651.65,(we will reject H0),\n0, ifbz <1.65,(we will keep H0),\nwhere bzis the realization of bZ. In this particular problem, we have bz= 2.432. Thus,\naccording to the decision rule, we need to reject H0.\nA decision rule is something youcreate. You do not need to follow the critical-value\nor the p-value procedure \u2014 you can create your own decision rule. For example, you can\nsay \u201creject H0when|y|>0.000001\u201d. There is nothing wrong with this decision rule except\nthat you will almost always reject the null hypothesis (so it is a bad decision rule). See\nFigure 9.18 for a graph of a similar example. If you follow the critical-value or the p-value\nprocedures, it turns out that the resulting decision rule is equivalent to some form of optimal\ndecision rule. This concept is the Neyman-Pearson framework, which we will explain shortly.\n9.4.2 Type 1 and type 2 errors\nSince hypothesis testing is about applying a decision rule to the test statistics, and since\nno decision rule is perfect, it is natural to ask about the error expected from a particular\ndecision rule. In this subsection we define the decision error. However, the terminology varies\nfrom discipline to discipline. We will explain the decision error first through the statistics\nperspective and then through the signal processing perspective.\nTwo tables of the cases that can be generated by a binary decision-making process are\nshown in Figure 9.19 . The columns of the tables are the true statements, i.e., whether the\ntest statistic has a population distribution under H0orH1. The rows of the tables are the\nstatements predicted by the decision rule, i.e., whether we should declare the statistics are\nfrom H0orH1. Each combination of the truth and prediction has a label:\n\u0088True positive: The truth is H1, and you declare H1.\n\u0088True negative: The truth is H0, and you declare H0.\n\u0088False positive: The truth is H0, and you declare H1.\n579", "595": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nFigure 9.18: Two possible decision rules \u03b41(y)and\u03b42(y). In this example, \u03b41(y)is designed according\nto the critical-value approach at \u03b1= 0.025, whereas \u03b42(y)is arbitrarily designed. Both are valid decision\nrules, although \u03b42should not be used because it tends to reject the null hypothesis more often than\ndesired.\n\u0088False negative: The truth is H1, and you declare H0.\nDifferent communities have different ways of labeling these quantities. In the statistics com-\nmunity the false negative rate (i.e., the number of false negative cases divided by the total\nnumber of cases) is called the type 2 error , and the false positive rate is called the type 1\nerror. The true positive rate is called the power of the decision rule.\nIn the engineering community (e.g., radar engineering and signal processing) the ob-\njective is to detect whether a target (e.g., a missile or an enemy aircraft) is present. In this\ncontext, the false positive rate is known as the probability of false alarm , since personnel\nwill be alerted when no target is present. The false negative rate is known as the probability\nofmiss because you miss a target. If the truth is H1and the prediction is also H1, we call\nthis the probability of detection .\nFigure 9.19: Terminologies used in labeling the prediction error. The terms \u201cType 1 error\u201d and \u201cType\n2 error\u201d are commonly used by the statistics community, whereas the terms \u201cfalse alarm\u201d, \u201cmiss\u201d and\n\u201cdetection\u201d are more often used in the engineering community.\nThe diagram in Figure 9.20 will help to clarify these definitions. Given two hypotheses\nH0andH1, there exists the corresponding distributions f0(y) and f1(y), which are the PDFs\n580", "596": "9.4. NEYMAN-PEARSON TEST\nof the test statistics Y(orb\u0398 if you prefer). Supposing that our decision rule is to declare\nH1when Y\u2265\u03b7for some \u03b7, for example, \u03b7= 1.65 for a 5% critical level, there are two areas\nunder the curve that we need to consider.\n\u0088Type 1 / False alarm . The blue region under the curve represents the probability of\ndeclaring H1(i.e., we choose to reject the null) while the truth is actually H0(i.e., we\nshould have not rejected the null). Mathematically, this probability is\npF=P[Y\u2265\u03b7|H0] =Z\ny\u2265\u03b7f0(y)dy. (9.28)\n\u0088Type 2 / Miss . The pink region under the curve represents the probability of declaring\nH0(i.e., we choose to keep the null) while the truth is actually H1(i.e., we should\nhave rejected the null). Mathematically, this probability is\npM=P[Y < \u03b7 |H1] =Z\ny<\u03b7f1(y)dy. (9.29)\nFigure 9.20: Definition of type 1 and type 2 errors.\nThepower of the decision rule is also known as the detection. It is defined as\npD=P[Y\u2265\u03b7|H1]. (9.30)\nA plot illustrating the power of the decision rule is shown in Figure 9.21 . Since pDis the\nconditional probability of Y\u2265\u03b7given H1, it is the complement of pM, and so we have the\nidentity\npD= 1\u2212pM.\nSome communities refer to the above quantities in terms of the counts instead of the\nprobabilities . The difference is that the probabilities are normalized to [0 ,1] whereas the\ncounts are just the raw integers obtained from running an experiment. We prefer to use the\nprobabilities because they are the theoretical values . If you tell us the distributions f0and\nf1, we can report the probabilities. The counts, by contrast, are just another form of sample\nstatistics . The number of counts today may be different from the number of counts tomorrow\n581", "597": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nFigure 9.21: The power of the decision rule is the area under the curve of f1, integrated for yinside\nthe rejection zone.\nbecause they are obtained from the experiments. The difference between probabilities and\ncounts is analogous to the difference between PMFs and histograms.\nSince the probability of errors changes as the decision rule changes, it is necessary to\ndefine pF,pDandpMas functions of \u03b4. In addition, hypothesis testing is not limited to one-\nsided tests. We can define the rejection zone as R\u03b1={y|reject H0using a critical level \u03b1}.\nThe probabilities pFandpMare defined as\npF(\u03b4) =Z\n\u03b4(y)f0(y)dy=Z\ny\u2208R\u03b1f0(y)dy, (9.31)\npM(\u03b4) =Z\n\u03b4(y)f1(y)dy=Z\ny\u0338\u2208R\u03b1f1(y)dy. (9.32)\nUsing the property that pD= 1\u2212pM, we have that\npD(\u03b4) = 1\u2212pM(\u03b4) =Z\ny\u2208R\u03b1f1(y)dy. (9.33)\nNote that the rejection zone does not need to depend on \u03b1. You can arbitrarily define the\nrejection zone, and the probabilities pF,pM, and pDcan still be defined.\nExample 9.13 . Find pF(\u03b41) and pF(\u03b42) for the decision rule in Figure 9.18 .\nSolution . Since f0is a Gaussian with zero mean and unit variance, it follows that\npF(\u03b41) =Z\u221e\n1.961\u221a\n2\u03c0e\u2212y2\n2dy= 1\u2212\u03a6(1.92) = 0 .025,\npF(\u03b42) =Z\u221e\n0.51\u221a\n2\u03c0e\u2212y2\n2dy= 1\u2212\u03a6(0.5) = 0 .3085.\n9.4.3 Neyman-Pearson decision\nAt this point you have probably observed something about the critical-value test and the\np-value test. Among the four types of decision combinations, we are looking at the false\n582", "598": "9.4. NEYMAN-PEARSON TEST\npositive rate, or the probability of false alarm pF(\u03b4). The critical-value test requires us to\nfind\u03b4such that pF(\u03b4) is equal to \u03b1. That is, if you tell us the critical level \u03b1(e.g., \u03b1= 0.05),\nwe will find a decision rule (by telling you the cutoff) such that the false alarm rate is \u03b1.\nConsider an example:\nExample 9.14 . Let \u03b1= 0.05. Assume that f0is a Gaussian with zero-mean and\nunit-variance. Let us do a one-sided test for H0:\u03b8= 0 versus H1:\u03b8 >0. Find \u03b4such\nthatpF(\u03b4) =\u03b1.\nSolution . Let the decision rule \u03b4be\n\u03b4(y) =(\n1, y\u2265\u03b7,\n0, y < \u03b7.\nOur goal is to find \u03b7. The probability of false alarm is\npF(\u03b4) =Z\u221e\n\u03b71\u221a\n2\u03c0e\u2212y2\n2dy= 1\u2212\u03a6(\u03b7).\nEquating this to \u03b1, it follows that 1 \u2212\u03a6(\u03b7) =\u03b1implies \u03b7= \u03a6\u22121(1\u2212\u03b1) = 1 .65. So the\ndecision rule becomes\n\u03b4(y) =(\n1, y \u22651.65,\n0, y < 1.65.\nIf you apply this decision rule, you are guaranteed that the false alarm rate is \u03b1= 0.05.\nBut why should we aim for pF(\u03b4)equal to \u03b1? Isn\u2019t a lower false alarm rate better?\nIndeed, we would not mind having a lower false alarm, so we are happy to have any \u03b4\nthat satisfies pF(\u03b4)\u2264\u03b1. However, changing the equality to an inequality means that we\nnow have a set of \u03b4instead of a unique \u03b4. More important, we need to pay attention to\nthe trade-off between pF(\u03b4) and pD(\u03b4). The smaller the pF(\u03b4) a decision rule \u03b4provides,\nthe smaller the pD(\u03b4) you can achieve. This is immediately apparent from Figure 9.20 and\nFigure 9.21 . (If you move the cutoff to the right, the gray area and the blue area will both\nshrink.) Therefore, the desired optimization should be formulated as: From all the decision\nrules \u03b4that have a false alarm rate of no larger than \u03b1, we pick the one that maximizes the\ndetection rate. The resulting decision rule is known as the Neyman-Pearson decision rule .\nDefinition 9.2. TheNeyman-Pearson decision rule is defined as the solution to the\noptimization\n\u03b4\u2217=argmax\n\u03b4pD(\u03b4),\nsubject to pF(\u03b4)\u2264\u03b1. (9.34)\nFigure 9.22 illustrates two decision rules \u03b4\u2217(y) and \u03b4(y). The first decision rule \u03b4\u2217(y) is\nobtained according to the critical-value approach, with \u03b1= 0.025. As we will prove shortly,\nthis is also the optimal Neyman-Pearson decision rule for a one-sided hypothesis testing at\n\u03b1= 0.025. The second decision rule \u03b4(y) has a harsher cutoff, meaning that you need an\nextreme test statistic to reject the null hypothesis. Clearly, the p-value obtained by \u03b4(y) is\n583", "599": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nless than \u03b1= 0.025. Thus, \u03b4(y) is a valid decision rule according to the Neyman-Pearson\nformulation. However, \u03b4(y) is not optimal because the detection rate is not maximized.\nFigure 9.22: Two decision rules \u03b4(y)and\u03b4\u2217(y). Assume that \u03b1= 0.025. Then \u03b4(y)is one of the many\nfeasible choices in the Neyman-Pearson optimization, but \u03b4\u2217(y)is the optimal solution.\nBecause of the complementary behavior of pFandpD, it follows that pDis maximized\nwhen pFhits the upper bound. If we want to maximize the detection rate we need to stretch\nthe false alarm rate as much as possible. As a result, the Neyman-Pearson solution occurs\nwhen pF(\u03b4) =\u03b1, i.e., when the equality is met.\nThe Neyman-Pearson framework is a general framework for all distributions f0andf1,\nas opposed to the critical-value and p-value examples, which are either Gaussian or Student\u2019s\nt-distribution. The solution to the Neyman-Pearson optimization is a decision rule known\nas the likelihood ratio test . The likelihood ratio is defined as follows.\nDefinition 9.3. Thelikelihood ratio for two distributions f1(y)andf0(y)is\nL(y) =f1(y)\nf0(y). (9.35)\nIt turns out that the solution to the Neyman-Pearson optimization takes the form of the\nlikelihood ratio.\nTheorem 9.2. The solution to the Neyman-Pearson optimization is a decision rule\nthat checks the likelihood ratio\n\u03b4\u2217(y) =(\n1, L (y)\u2265\u03b7,\n0, L (y)< \u03b7,(9.36)\nfor some decision boundary \u03b7which is a function of the critical level \u03b1.\n584", "600": "9.4. NEYMAN-PEARSON TEST\nWhat is so special about Neyman-Pearson decision rule?\n\u0088It is the optimal decision. Its optimality is defined w.r.t. maximizing the detection\nrate while keeping a reasonable false alarm rate:\n\u03b4\u2217= argmax\n\u03b4pD(\u03b4),\nsubject to pF(\u03b4)\u2264\u03b1.\n\u0088If your goal is to maximize the detection rate while maintaining the false alarm\nrate, you cannot do better than Neyman-Pearson.\n\u0088Its solution is the likelihood ratio test:\n\u03b4\u2217(y) =(\n1, L (y)\u2265\u03b7,\n0, L (y)< \u03b7,\nwhere L(y) =f1(y)/f0(y) is the likelihood ratio.\n\u0088The critical-value test and the p-value test are special cases of the Neyman-\nPearson test.\nDeriving the solution to the Neyman-Pearson optimization can be skipped if this is your\nfirst time reading the book.\nProof . Given \u03b1, choose \u03b4\u2217such that the false alarm rate is maximized: pF(\u03b4\u2217) =\u03b1. Then,\nby substituting the definition of \u03b4\u2217into the false alarm rate,\n\u03b1=pF(\u03b4\u2217) =Z\u221e\n\u2212\u221e\u03b4\u2217(y)f0(y)dy\n=Z\nL(y)\u2265\u03b71\u00b7f0(y)dy+Z\nL(y)<\u03b70\u00b7f0(y)dy. (9.37)\nNow, consider another decision rule \u03b4that is not optimal but is feasible. That means that\n\u03b4satisfies pF(\u03b4)\u2264\u03b1. Therefore,\n\u03b1\u2265pF(\u03b4) =Z\u221e\n\u2212\u221e\u03b4(y)f0(y)dy\n=Z\nL(y)\u2265\u03b7\u03b4(y)\u00b7f0(y)dy+Z\nL(y)<\u03b7\u03b4(y)\u00b7f0(y)dy. (9.38)\nOur goal is to show that pD(\u03b4\u2217)\u2265pD(\u03b4), because by proving this result we can claim that\n\u03b4\u2217maximizes the detection rate.\nBy combining Equation (9.37) and Equation (9.38), we have\n0\u2264pF(\u03b4\u2217)\u2212pF(\u03b4)\n=Z\nL(x)\u2265\u03b7(1\u2212\u03b4(y))f0(y)dy\u2212Z\nL(y)<\u03b7\u03b4(y)f0(y)dy. (9.39)\n585", "601": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nDefine L(y) =f1(y)\nf0(y). Then L(y)\u2265\u03b7if and only if f1(y)\u2265\u03b7f0(y). So,\npD(\u03b4\u2217)\u2212pD(\u03b4) =Z\nL(y)\u2265\u03b7(1\u2212\u03b4(y))f1(y)dy\u2212Z\nL(y)<\u03b7\u03b4(y)f1(y)dy\n=Z\nL(y)\u2265\u03b7(1\u2212\u03b4(y))\u03b7f0(y)dy\u2212Z\nL(y)<\u03b7\u03b4(y)\u03b7f0(y)dy\n=\u03b7\"Z\nL(y)\u2265\u03b7(1\u2212\u03b4(y))f0(y)dy\u2212Z\nL(y)<\u03b7\u03b4(y)f0(y)dy#\n\u22650,\nwhere the last inequality holds because of Equation (9.39). Therefore, we conclude that \u03b4\u2217\nmaximizes pD. \u25a1\nEnd of the proof. Please join us again.\nAt this point, you may object that the likelihood ratio test (i.e., the Neyman-Pearson\ndecision rule) is very different from the hypothesis testing examples we have seen in the\nprevious chapter because now we need to handle the likelihood ratio L(y). Rest assured\nthat they are the same, as illustrated by the following example.\nExample 9.15 . Consider two hypotheses: H0:Y\u223cGaussian(0 , \u03c32), and H1:Y\u223c\nGaussian( \u00b5, \u03c32), with \u00b5 > 0. Construct the Neyman-Pearson decision rule (i.e., the\nlikelihood ratio test).\nSolution . Let us first define the likelihood functions. It is clear from the description\nthat\nf0(y) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212y2\n2\u03c32\u001b\nand f1(y) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(y\u2212\u00b5)2\n2\u03c32\u001b\n.\nTherefore, the likelihood ratio is\nL(y) =f1(y)\nf0(y)= exp\u001a\n\u22121\n2\u03c32(\u00b52\u22122\u00b5y)\u001b\n.\nThe likelihood ratio test states that the decision rule is\n\u03b4\u2217(y) =(\n1, L (y)\u2265\u03b7,\n0, L (y)< \u03b7.\nSo it remains to simplify the condition L(y)\u22db\u03b7. To this end, we observe that\nL(y)\u2265\u03b7 \u21d0\u21d2 \u22121\n2\u03c32(\u00b52\u22122\u00b5y)\u2265log\u03b7\n\u21d0\u21d2 y\u2265\u00b5\n2\u2212\u03c32\n\u00b5log\u03b7\n|{z}\ndef=\u03c4.\n586", "602": "9.4. NEYMAN-PEARSON TEST\nTherefore, instead of determining \u03b7, we just need to define \u03c4because the decision rules\nbased on \u03b7and\u03c4are equivalent.\nTo determine \u03c4, Neyman-Pearson states that pF(\u03b4)\u2264\u03b1(and at the optimal point\nthe equality has to hold). Substituting this criterion into the decision rule,\n\u03b1=pF(\u03b4) =Z\nL(y)\u2265\u03b7f0(y)dy\n=Z\ny\u2265\u03c4f0(y)dy\n=Z\ny\u2265\u03c41\u221a\n2\u03c0\u03c32e\u2212y2\n2\u03c32dy\n= 1\u2212\u03a6\u0010\u03c4\n\u03c3\u0011\n.\nTaking the inverse of the CDF, we obtain \u03c4:\n\u03c4=\u03c3\u03a6\u22121(1\u2212\u03b1).\nPutting everything together, the final decision rule is\n\u03b4\u2217(y) =(\n1, y \u2265\u03c3\u03a6\u22121(1\u2212\u03b1),\n0, y < \u03c3 \u03a6\u22121(1\u2212\u03b1).\nSo if \u03b1= 0.05 we will reject H0when y\u22651.65\u03c3. We can also replace \u03c3by\u03c3/\u221a\nNif\nthe estimator is constructed from multiple measurements.\nThe above example tells us that even though the likelihood ratio test may appear\ncomplicated at first glance, the decision is the same as the good old hypothesis testing rules\nwe have derived. The flexibility we have gained with the likelihood ratio test is the variety\nof distributions we can handle. Instead of restricting ourselves to Gaussians or Student\u2019s\nt-distribution (which exclusively focuses on the sample averages), the likelihood ratio test\nallows us to consider any distributions. The exact decision rule could be less obvious, but\nthe method is generalizable to a broad range of problems.\nPractice Exercise 9.5 . In a telephone system, the waiting time is defined as the\ninter-arrival time between two consecutive calls. However, it is known that sometimes\nthe waiting time can be mistakenly recorded as the time between three consecutive\ncalls (i.e., by skipping the second one). Since the interarrival time of an independent\nPoisson process is either an exponential random variable or an Erlang random variable,\ndepending on how many occurrences we are counting, we define the hypotheses\nf0(y) =(\ne\u2212y, y \u22650,\n0, y < 0,and f1(y) =(\nye\u2212y, y \u22650,\n0, y < 0.\nSuppose we are given one measurement Y=y. Find the Neyman-Pearson decision\nrule for \u03b1= 0.05.\n587", "603": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nSolution . The likelihood ratio is\nL(y) =f1(y)\nf0(y)=ye\u2212y\ne\u2212y=y, y \u22650.\nSubstituting this into the decision rule, we have\n\u03b4\u2217(y) =(\n1, L (y)\u2265\u03b7\u21d0\u21d2y\u2265\u03b7,\n0, L (y)< \u03b7\u21d0\u21d2y < \u03b7.\nIt remains to determine \u03b7. Inspecting pF(\u03b4), we have that\n\u03b1=pF(\u03b4\u2217) =Z\nL(y)\u2265\u03b7f0(y)dy\n=Z\ny\u2265\u03b7e\u2212ydy=e\u2212\u03b7.\nSetting e\u2212\u03b7=\u03b1, we have that \u03b1=\u2212log\u03b1. Hence, the decision rule is\n\u03b4\u2217(y) =(\n1, L (y)\u2265\u03b7\u21d0\u21d2y\u2265 \u2212log\u03b1,\n0, L (y)< \u03b7\u21d0\u21d2y <\u2212log\u03b1.\nFor\u03b1= 0.05, we reject the null hypothesis when y\u22652.9957. Figure 9.23 illustrates\nthe hypothesis testing rule.\nFigure 9.23: Neyman-Pearson decision rule at \u03b1= 0.05.\nRemark . This example is instructive in that we have only one measurement Y=y.\nIf we have repeated measurements and take the average, then the Central Limit The-\norem will kick in. In that case, we can resort to our favorite Gaussian distribution\nor Student\u2019s t-distribution instead of dealing with the exponential and the Erlang\ndistributions. However, the example demonstrates the usefulness of Neyman-Pearson,\nespecially when the distributions are complicated.\n588", "604": "9.5. ROC AND PRECISION-RECALL CURVE\n9.5 ROC and Precision-Recall Curve\nBeing a binary decision rule, the hypothesis testing procedure shares many similarities with\na two-class classification algorithm.3Given a testing statistic or a testing sample, both\nthe hypothesis testing and a classification algorithm will report YES or NO. Therefore,\nany performance evaluation metric developed for hypothesis testing is equally applicable to\nclassification and vice versa.\nThe topic we study in this section is the receiver operating characteristic (ROC) curve\nand the precision-recall (PR) curve. The ROC curve and the PR curve are arguably the\nmost popular metrics in modern machine learning, in particular for classification, detection,\nand segmentation tasks in computer vision. There are many unresolved questions about\nthese two curves and there are many debates about how to use them. Our goal is not to add\nanother voice to the debate; rather, we would like to fill in the gap between the hypothesis\ntesting theory (particularly the Neyman-Pearson framework) and these two sets of curves.\nWe will establish the equivalence between the two curves and leave the open-ended debates\nto you.\n9.5.1 Receiver Operating Characteristic (ROC)\nOur approach to understanding the ROC curve and the PR curve is based on the Neyman-\nPearson framework. Under this framework, we know that the optimal decision rule w.r.t to\nthe Neyman-Pearson criterion is the solution to the optimization\n\u03b4\u2217(\u03b1) = argmax\n\u03b4pD(\u03b4)\nsubject to pF(\u03b4)\u2264\u03b1.\nAs a result of this optimization, the decision rule \u03b4\u2217will achieve a certain false alarm rate\npF(\u03b4\u2217) and detection rate pD(\u03b4\u2217). Clearly, the decision rule \u03b4\u2217changes as we change the\ncritical level \u03b1. Accordingly we write \u03b4\u2217as\u03b4\u2217(\u03b1) to reflect this dependency.\nWhat this observation implies is that as we sweep through the range of \u03b1\u2019s, we construct\ndifferent decision rules, each one with a different pFandpD. If we denote the decision rules\nby\u03b41, \u03b42, . . . , \u03b4 M, we have Mpairs of false alarm rate pFand detection rate pD:\n\u0088Decision rule \u03b41: False alarm rate pF(\u03b41) and detection rate pD(\u03b41).\n\u0088Decision rule \u03b42: False alarm rate pF(\u03b42) and detection rate pD(\u03b42).\n\u0088...\n\u0088Decision rule \u03b4M: False alarm rate pF(\u03b4M) and detection rate pD(\u03b4M).\n3In a classification algorithm, the goal is to look at the testing sample yand compute certain thresholding\ncriteria. For example, a typical decision rule of a classification algorithm is \u03b4(y) =(\n1,wT\u03d5(y)\u2265\u03c4\n0,wT\u03d5(y)< \u03c4. Here,\nyou can think of the vector was the regression coefficient, and \u03d5(\u00b7) is some kind of feature transform. The\nequation says that class 1 will be reported if the inner product is larger than a threshold \u03c4, and class 0\nwill be reported otherwise. Therefore, a binary classification, when written in this form, is the same as a\nhypothesis testing procedure.\n589", "605": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nIf we plot pD(\u03b4) on the y-axis as a function of pF(\u03b4) on the x-axis, we obtain a curve shown\ninFigure 9.24 (see the example below for the problem setting). The black curve shown on\nthe right is known as the receiver operating characteristic (ROC) curve.\nFigure 9.24: An example of an ROC curve, where we consider two hypotheses: H0:Y\u223cGaussian (0,2),\nandH1:Y\u223cGaussian (3,2). We construct the Neyman-Pearson decision rule for a range of critical\nlevels \u03b1. For each \u03b1we compute the theoretical pF(\u03b1)andpD(\u03b1), shown on the left-hand side of the\nfigure. The pair of (pD, pF)is then plotted as the right-hand side curve by sweeping the \u03b1\u2019s.\nThe setup of the figure follows the example below.\nExample 9.16 . We consider two hypotheses: H0:Y\u223cGaussian(0 ,2), and H1:Y\u223c\nGaussian(3 ,2). Derive the Neyman-Pearson decision rule and plot the ROC curve.\nSolution . We construct a Neyman-Pearson decision rule:\n\u03b4\u2217(y) =(\n1, y \u2265\u03c3\u03a6\u22121(1\u2212\u03b1),\n0, y < \u03c3 \u03a6\u22121(1\u2212\u03b1).\nwhere \u03c4is a tunable threshold. For example, if \u03b1= 0.05, then \u03c3\u03a6\u22121(1\u22120.05) = 3 .2897,\nand if \u03b1= 0.1, then \u03c3\u03a6\u22121(1\u22120.1) = 2 .5631. Therefore, the false alarm rate and the\ndetection rate are functions of the critical level \u03b1.\nFor this particular example, we have the false alarm rate and detection rate in\nclosed form, as functions of \u03b1:\npF(\u03b1) =Z\u221e\n\u03c3\u03a6\u22121(1\u2212\u03b1)1\u221a\n2\u03c0\u03c32e\u2212y2\n2\u03c32dy\n= 1\u2212\u03a6\u0012\u03c3\u03a6\u22121(1\u2212\u03b1)\n\u03c3\u0013\n=\u03b1,\n590", "606": "9.5. ROC AND PRECISION-RECALL CURVE\npD(\u03b1) =Z\u221e\n\u03c3\u03a6\u22121(1\u2212\u03b1)1\u221a\n2\u03c0\u03c32e\u2212(y\u2212\u00b5)2\n2\u03c32dy\n= 1\u2212\u03a6\u0010\n\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0011\n.\nThese give us the two curves on the left-hand side of Figure 9.24 .\nWhat is an ROC curve?\n\u0088It is a plot showing pDon the y-axis and pFon the x-axis.\n\u0088pD= detection rate (also known as the power of the test).\n\u0088pF= false alarm rate (also known as the type 1 error of the test).\nThe ROC curve tells us the behavior of the decision rule as we change the threshold \u03b1.\nA graphical illustration is shown in Figure 9.25 . There are a few key observations we need\nto pay attention to:\nFigure 9.25: Interpreting the ROC curve.\n\u0088The ROC curve must go through (0 ,0). This happens when you always keep the\nnull hypothesis or always declare class 0, no matter what observations. If you always\nkeep H0, certainly you will not make any false positive (or false alarm), because you\nwill never say H0is wrong. Therefore, the detection rate (or the power of the test) is\nalso 0. This is a useless decision rule for both classification and hypothesis testing.\n\u0088The ROC curve must go through (1 ,1). This happens when you always reject the null\nhypothesis, no matter what observations we have. If you always reject H0, you will\nalways say that \u201cthere is a target\u201d. As far as detection is concerned, you are perfect\n591", "607": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nbecause you have not missed any targets. However, the false positive rate is also high\nbecause you will falsely declare a target when there is nothing. Therefore, this is also\na useless decision rule.\n\u0088The ROC curve tells us the operating point of the decision rule as we change the thresh-\nold. A threshold is a universal concept for both hypothesis testing and classification.\nIn hypothesis testing, we have the critical level \u03b1, say 0.05 or 0.1. In classification, we\nalso have a threshold for judging whether a sample should be classified as class 1 or\nclass 0. Often in classification, the intermediate estimates are probabilities or distances\nto decision boundaries. These real numbers need to be binarized to generate a binary\ndecision. The ROC curve tells us that if you pick a threshold, your decision rule will\nhave a certain pFandpDas predicted by the curve. If you want to tolerate a higher\npF, you can move along the curve to find your operating point.\n\u0088The ideal operating point on a ROC curve is when pF= 0 and pD= 1. However, this\nis a hypothetical situation that does not happen in any real decision rule.\n9.5.2 Comparing ROC curves\nBecause of how the ROC curves are constructed, every binary decision rule has its own ROC\ncurve. Typically, when one tries to compare classification algorithms, the area under the\ncurve (AUC ) occupied by the ROC curve is compared. A decision rule having a larger AUC\nis often a \u201cbetter\u201d decision rule.\nTo illustrate the idea of comparing estimators, we consider a trivial decision rule based\non a blind guess.\nExample 9.17 . (A blind guess decision) Consider a decision rule that we reject H0\nwith probability \u03b1and keep H0with probability 1 \u2212\u03b1. We call this a blind guess, since\nthe decision rule ignores observation y. Mathematically, this trivial decision rule is\n\u03b4(y) =(\n1, with probability \u03b1,\n0, with probability 1 \u2212\u03b1.\nFind pF,pD, and AUC.\nSolution . For this decision rule we compute its false positive rate (or false alarm rate)\nand its true positive rate (or detection rate). However, since \u03b4(y) is now random, we\nneed to take the expectation over the two random states that \u03b4(y) can take. This gives\nus\npF(\u03b1) =E\u0014Z\n\u03b4(y)f0(y)dy\u0015\n=Z\n1\u00b7f0(y)dyP[\u03b4(y) = 1] +Z\n0\u00b7f0(y)dyP[\u03b4(y) = 0]\n=\u03b1Z\nf0(y)dy=\u03b1.\n592", "608": "9.5. ROC AND PRECISION-RECALL CURVE\nSimilarly, the detection rate is\npD(\u03b1) =E\u0014Z\n\u03b4(y)f1(y)dy\u0015\n=\u03b1Z\nf1(y)dy=\u03b1.\nIf we plot pDas a function of pF, we notice that the function is a straight line going\nfrom (0 ,0) to (1 ,1). This decision rule is useless. Comparing this with the Neyman-\nPearson decision rule, it is clear that Neyman-Pearson has a larger AUC. The AUC\nfor this trivial decision rule is the area of the triangle, which is 0.5.\nFigure 9.26: The ROC curve of the blind guess decision rule is a straight line. The AUC is 0.5.\nIf you set \u03b1= 0.5, then the decision rule becomes\n\u03b4(y) =(\n1, with probability1\n2,\n0, with probability1\n2.\nThis is equivalent to flipping a fair coin with probability 1 /2 of declaring H0and 1 /2\ndeclaring H1. Its operating point is the yellow circle.\nComputing the AUC can be done by calling special library functions. However, to\nspell out the details we demonstrate something more elementary. The program below is a\npiece of MATLAB code plotting two ROC curves corresponding to two different decision\nrules. The first decision rule is the trivial decision rule, where we have just shown that\npF(\u03b1) =pD(\u03b1) =\u03b1. The second decision rule is the Neyman-Pearson decision rule, for\nwhich we showed in Figure 9.24 that pF(\u03b1) =\u03b1andpD(\u03b1) = 1 \u2212\u03a6\u0000\n\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0001\n.\nUsing the MATLAB code below, we can plot the two ROC curves shown in Figure 9.26 .\n% MATLAB code to plot ROC curve\nsigma = 2; mu = 3;\nalphaset = linspace(0,1,1000);\nPF1 = zeros(1,1000); PD1 = zeros(1,1000);\nPF2 = zeros(1,1000); PD2 = zeros(1,1000);\nfor i=1:1000\nalpha = alphaset(i);\n593", "609": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nPF1(i) = alpha;\nPD1(i) = alpha;\nPF2(i) = alpha;\nPD2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma);\nend\nfigure;\nplot(PF1, PD1,\u2019LineWidth\u2019, 4, \u2019Color\u2019, [0.8, 0, 0]); hold on;\nplot(PF2, PD2,\u2019LineWidth\u2019, 4, \u2019Color\u2019, [0, 0, 0]); hold off;\nTo compute the AUC we perform a numerical integration:\nAUC =Z\npD(\u03b1)\u00b7dpF(\u03b1)\u2248X\nipD(\u03b1i)\u00b7\u2206pF(\u03b1i)\n=X\nipD(\u03b1i)\u00b7\u0002\npF(\u03b1i)\u2212pF(\u03b1i\u22121)\u0003\n,\nwhere \u03b1iis the ith critical level we use to plot the ROC curve. (We assume that the \u03b1\u2019s are\nsorted in ascending order.) In MATLAB, the commands are\nauc1 = sum(PD1.*[0 diff(PF1)])\nauc2 = sum(PD2.*[0 diff(PF2)])\nThe AUC of the two decision rules computed by MATLAB are 0.8561 and 0.5005, respec-\ntively. The small slack of 0.0005 is caused by the numerical approximation at the tail, which\ncan be ignored as long as you are consistent for all the ROC curves.\nThe commands for Python are analogous to the commands for MATLAB.\n# Python code to plot ROC curve\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nsigma = 2; mu = 3;\nalphaset = np.linspace(0,1,1000)\nPF1 = np.zeros(1000); PD1 = np.zeros(1000)\nPF2 = np.zeros(1000); PD2 = np.zeros(1000)\nfor i in range(1000):\nalpha = alphaset[i]\nPF1[i] = alpha\nPD1[i] = alpha\nPF2[i] = alpha\nPD2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma)\nplt.plot(PF1,PD1)\nplt.plot(PF2,PD2)\nTo compute the AUC, the Python code is (continuing from the previous code):\n594", "610": "9.5. ROC AND PRECISION-RECALL CURVE\nauc1 = np.sum(PD1 * np.append(0, np.diff(PF1)))\nauc2 = np.sum(PD2 * np.append(0, np.diff(PF2)))\nIt is possible to get a decision rule that is worse than a blind guess. The following\nexample illustrates a trivial setup.\nPractice Exercise 9.6 . (Flipped Neyman-Pearson). Consider two hypotheses\nH0= Gaussian(0 , \u03c32),\nH1= Gaussian( \u00b5, \u03c32), \u00b5 > 0.\nLet\u03b1be the critical level. The Neyman-Pearson decision rule is\n\u03b4\u2217(y) =(\n1, y \u2265\u03c3\u03a6\u22121(1\u2212\u03b1),\n0, y < \u03c3 \u03a6\u22121(1\u2212\u03b1).\nNow, consider a flipped Neyman-Pearson decision rule\n\u03b4+(y) =(\n1, y < \u03c3 \u03a6\u22121(1\u2212\u03b1),\n0, y \u2265\u03c3\u03a6\u22121(1\u2212\u03b1).\nFind pF,pD, and AUC for the new decision rule \u03b4+.\nSolution . Since we flip the rejection zone, the probability of false alarm is\npF(\u03b1) =Z\n\u03b4+(y)f0(y)dy\n=Z\u03c3\u03a6\u22121(1\u2212\u03b1)\n\u2212\u221e1\u221a\n2\u03c0\u03c32e\u2212y2\n2\u03c32dy\n= \u03a6\u0012\u03c3\u03a6\u22121(1\u2212\u03b1)\n\u03c3\u0013\n= 1\u2212\u03b1.\nSimilarly, the probability of detection is\npD(\u03b1) =Z\n\u03b4+(y)f1(y)dy\n=Z\u03c3\u03a6\u22121(1\u2212\u03b1)\n\u2212\u221e1\u221a\n2\u03c0\u03c32e\u2212(y\u2212\u00b5)2\n2\u03c32dy\n= \u03a6\u0012\u03c3\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0013\n= \u03a6\u0010\n\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0011\n.\n595", "611": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nIf you plot pDas a function of pF, you will obtain a curve shown in Figure 9.27 .\nThe AUC for this flipped decision rule is 0.1439, whereas that for Neyman-Pearson is\n0.8561. The two numbers are complements of each other, meaning that their sum is\nunity.\nFigure 9.27: The ROC curve of a flipped Neyman-Pearson decision rule.\nWhat if we arbitrarily construct a decision rule that is neither Neyman-Pearson nor\nthe blind guess? The following example demonstrates one possible choice.\nPractice Exercise 9.7 . Consider two hypotheses\nH0= Gaussian(0 , \u03c32),\nH1= Gaussian( \u00b5, \u03c32), \u00b5 > 0.\nLet\u03b1be the critical level. Consider the following decision rule:\n\u03b4\u2663(y) =(\n1,|y| \u2265\u03c3\u03a6\u22121(1\u2212\u03b1),\n0,|y|< \u03c3\u03a6\u22121(1\u2212\u03b1).\nFind pF,pD, and AUC for the new decision rule \u03b4\u2663.\nSolution . The probability of false alarm is\npF(\u03b1) =Z\n\u03b4\u2663(y)f0(y)dy\n= 1\u2212Z\u03c3\u03a6\u22121(1\u2212\u03b1)\n\u2212\u03c3\u03a6\u22121(1\u2212\u03b1)1\u221a\n2\u03c0\u03c32e\u2212y2\n2\u03c32dy\n= 1\u2212\u03a6\u0000\n\u03a6\u22121(1\u2212\u03b1)\u0001\n+ \u03a6\u0000\n\u2212\u03a6\u22121(1\u2212\u03b1)\u0001\n= 2\u03b1.\n596", "612": "9.5. ROC AND PRECISION-RECALL CURVE\nSimilarly, the probability of detection is\npD(\u03b1) =Z\n\u03b4\u2663(y)f1(y)dy\n= 1\u2212Z\u03c3\u03a6\u22121(1\u2212\u03b1)\n\u2212\u03c3\u03a6\u22121(1\u2212\u03b1)1\u221a\n2\u03c0\u03c32e\u2212(y\u2212\u00b5)2\n2\u03c32dy\n= 1\u2212\u03a6\u0012\u03c3\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0013\n+ \u03a6\u0012\u2212\u03c3\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0013\n= 1\u2212\u03a6\u0010\n\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0011\n+ \u03a6\u0010\n\u2212\u03a6\u22121(1\u2212\u03b1)\u2212\u00b5\n\u03c3\u0011\n.\nIf you plot pDas a function of pF, you will obtain a curve shown in Figure 9.28 .\nThe AUC for this proposed decision rule is 0.7534, whereas that of Neyman-Pearson\nis 0.8561. Therefore, the Neyman-Pearson decision rule is better.\nFigure 9.28: The ROC curve of a proposed decision rule.\nThe MATLAB code we used to generate Figure 9.28 is shown below. Note that we\nneed to separate the calculations of the two curves, because the proposed curve can only\ntake 0 < \u03b1 < 0.5. The Python code is implemented analogously.\n% MATLAB code to generate the ROC curve.\nsigma = 2; mu = 3;\nPF1 = zeros(1,1000); PD1 = zeros(1,1000);\nPF2 = zeros(1,1000); PD2 = zeros(1,1000);\nalphaset = linspace(0,0.5,1000);\nfor i=1:1000\nalpha = alphaset(i);\nPF1(i) = 2*alpha;\nPD1(i) = 1-(normcdf(norminv(1-alpha)-mu/sigma)-...\nnormcdf(-norminv(1-alpha)-mu/sigma));\nend\nalphaset = linspace(0,1,1000);\n597", "613": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nfor i=1:1000\nalpha = alphaset(i);\nPF2(i) = alpha;\nPD2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma);\nend\nfigure;\nplot(PF1, PD1,\u2019LineWidth\u2019, 4, \u2019Color\u2019, [0.8, 0, 0]); hold on;\nplot(PF2, PD2,\u2019LineWidth\u2019, 4, \u2019Color\u2019, [0, 0, 0]); hold off;\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nsigma = 2; mu = 3;\nPF1 = np.zeros(1000); PD1 = np.zeros(1000)\nPF2 = np.zeros(1000); PD2 = np.zeros(1000)\nalphaset = np.linspace(0,0.5,1000)\nfor i in range(1000):\nalpha = alphaset[i]\nPF1[i] = 2*alpha\nPD1[i] = 1-(stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) \\\n-stats.norm.cdf(-stats.norm.ppf(1-alpha)-mu/sigma))\nalphaset = np.linspace(0,1,1000)\nfor i in range(1000):\nalpha = alphaset[i]\nPF2[i] = alpha\nPD2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma)\nplt.plot(PF1, PD1)\nplt.plot(PF2, PD2)\n9.5.3 The ROC curve in practice\nIf the Neyman-Pearson decision rule is the optimal rule, why don\u2019t we always use it? The\nproblem is that in practice we may not have access to the distributions. For example, if we\nclassify images, how do we know that the data follows a Gaussian distribution or a mixture\nof distributions? Consequently, the ROC curves we discussed in the subsections above are\nthetheoretical ROC curves. In practice, we plot the empirical ROC curves.\nPlotting an empirical ROC curve for a binary classification method (and hypothesis\ntesting) is intuitive. The ingredients we need are a set of scores and a set of labels .\nThe scores are the probability values determining the likelihood of a sample belonging to\none class. Generally speaking, for empirical data this requires looking at the training data,\nbuilding a model, and computing the likelihood. We will not go into the details of how a\nbinary classifier is built. Instead, we assume that you have already built a binary classifier\nand have obtained the scores. Our goal is to show you how to plot the ROC curve.\n598", "614": "9.5. ROC AND PRECISION-RECALL CURVE\nThe following MATLAB code uses a dataset fisheriris . The code builds a binary\nclassifier and returns the scores.\n% MATLAB code to train a classification algorithm.\n% Do not worry if you cannot understand this code.\n% It is not the focus on this book.\nload fisheriris\npred = meas(51:end,1:2);\nresp = (1:100)\u2019>50;\nmdl = fitglm(pred,resp,\u2019Distribution\u2019,\u2019binomial\u2019,\u2019Link\u2019,\u2019logit\u2019);\nscores = mdl.Fitted.Probability;\nlabels = [ones(1,50), zeros(1,50)];\nsave(\u2019ch9_ROC_example_data\u2019,\u2019scores\u2019,\u2019labels\u2019);\nTo give you an idea of how the scores of the classifier look, we plot the histogram of\nthe scores in Figure 9.29 . As you can see, there is no clear division between the two classes.\nNo matter what threshold \u03c4we use, some cases will be misclassified.\nFigure 9.29: The distribution of probability scores obtained from a binary classifier for the dataset\nfisheriris . The green vertical lines represent the threshold for turning the scores into binary decisions.\nAny score greater than \u03c4will be classified as Class 1, and any score that is less than \u03c4will be classified\nas Class 0. These predicted labels would then be compared to the true labels to plot the ROC curve.\nRecall that the ROC curve is a function of pDversus pF. Using terminology from\nstatistics, pDis the true positive rate and pFis the false positive rate. By sweeping a range\nof decision thresholds (over the scores), we can compute the corresponding pF\u2019s and pD\u2019s.\nOn a computer this can be done by setting up two columns of labels: the true label labels\nand the predicted labels prediction . For any threshold \u03c4, we binarize the scores to turn\nthem into a decision vector. Then we count the number of true positives, true negatives,\nfalse positives, and false negatives. The total of these numbers will give us pFandpD.\nIn MATLAB, the above description can be easily implemented by sweeping through\nthe range of \u03c4.\n% MATLAB code to generate an empirical ROC curve\nload ch9_ROC_example_data\n599", "615": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\ntau = linspace(0,1,1000);\nfor i=1:1000\nidx = (scores <= tau(i));\npredict = zeros(1,100);\npredict(idx) = 1;\ntrue_positive = 0; true_negative = 0;\nfalse_positive = 0; false_negative = 0;\nfor j=1:100\nif (predict(j)==1) && (labels(j)==1)\ntrue_positive = true_positive + 1; end\nif (predict(j)==1) && (labels(j)==0)\nfalse_positive = false_positive + 1; end\nif (predict(j)==0) && (labels(j)==1)\nfalse_negative = false_negative + 1; end\nif (predict(j)==0) && (labels(j)==0)\ntrue_negative = true_negative + 1; end\nend\nPF(i) = false_positive/50;\nPD(i) = true_positive/50;\nend\nplot(PF, PD, \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0, 0, 0]);\nThe Python codes of this problem are similar. We give them here for completeness.\n# Python code to generate an empirical ROC curve\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nscores = np.loadtxt(\u2019ch9_ROC_example_data.txt\u2019)\nlabels = np.append(np.ones(50), np.zeros(50))\ntau = np.linspace(0,1,1000)\nPF = np.zeros(1000)\nPD = np.zeros(1000)\nfor i in range(1000):\nidx = scores<= tau[i]\npredict = np.zeros(100)\npredict[idx] = 1\ntrue_positive = 0; true_negative = 0\nfalse_positive = 0; false_negative = 0\nfor j in range(100):\nif (predict[j]==1) and (labels[j]==1): true_positive += 1\nif (predict[j]==1) and (labels[j]==0): false_positive += 1\nif (predict[j]==0) and (labels[j]==1): false_negative += 1\nif (predict[j]==0) and (labels[j]==0): true_negative += 1\nPF[i] = false_positive/50\nPD[i] = true_positive/50\nplt.plot(PF, PD)\n600", "616": "9.5. ROC AND PRECISION-RECALL CURVE\nThe empirical ROC curve for this problem is shown in Figure 9.30 . Each point on the\ncurve is a coordinate ( pF, pD), evaluated at a particular threshold \u03c4. Mathematically, the\ndecision rule we used was\n\u03b4(y) =(\n1, score (y)\u2265\u03c4,\n0, score (y)< \u03c4.\nFor every \u03c4, we have a false alarm rate and a detection rate. Since this is an empirical\ndataset with only 100 samples, there are many occasions where pFdoes not change but\npDincreases, or pDstays constant but pFincreases. For this particular example, we can\ncompute the AUC, which is 0.7948.\nFigure 9.30: The empirical ROC curve for the dataset fisheriris , using a classifier based on the\nlogistic regression.\nNote that the empirical ROC is rough. It does not have the smooth concave shape of\nthe theoretical ROC curve. One can prove that if the decision rule is Neyman-Pearson, i.e.,\nif we conduct a likelihood ratio test, then the resulting ROC curve is concave. Otherwise,\nyou can still obtain an empirical ROC curve for real datasets and classifiers. However, the\nshape is not necessarily concave.\n9.5.4 The Precision-Recall (PR) curve\nIn modern data science, an alternative performance metric to the ROC curve is the precision-\nrecall (PR) curve. The precision and recall are defined as follows.\nDefinition 9.4. Let TP = true positive, FP = false positive, FN = false negative.\nTheprecision is defined as\nprecision =TP\nTP+FP=pD\npD+pF, (9.40)\nand the recall is defined as\nrecall =TP\nTP+FN=pD\npD+pM=pD. (9.41)\n601", "617": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nIn this definition, TP, FP, and FN are the numbers of samples that are classified as true\npositive, false positive, and false negative, respectively. However, both precision and recall are\ndefined as ratios of numbers. The ratios can be equivalently defined through the rates. Using\nour terminology, this gives us the definitions in terms of pD,pFandpM. Since pD= 1\u2212pM,\nit also holds that the recall is pD.\nLet us take a moment to consider the meanings of precision and recall. Precision is\ndefined as\nprecision =TP\nTP + FP=# true positives\ntotal # positives you claim. (9.42)\nThe numerator of the precision is the number of true positive samples and the denominator\nis the total number of positives that you claim. This includes the true positives and the\nfalse positives. Therefore, precision measures how trustworthy your claim is. There are two\nscenarios to consider:\n\u0088High precision : This means that among all the positives you claim, many of them are\nthe true positives. Therefore, whatever you claim is trustworthy. One possibility for\nobtaining a high precision is that the critical level \u03b1of the Neyman-Pearson decision\nrule approaches 1. In other words, you are very accepting of the null hypotheses. Thus,\nwhenever you reject, it will be a reliable reject.\n\u0088Low precision : This means that you are overclaiming the positives, and so there are\nmany false positives. Thus, even though you claim many positives, not all are trust-\nworthy. One reason why low precision occurs is that you are too eager to reject the\nnull. Thus you tend to overkill the unnecessary cases.\nA similar analysis can be applied to the recall. The recall is defined as\nrecall =TP\nTP + FN=# true positives\ntotal # positives in the distribution. (9.43)\nThe difference between the recall and the precision is the denominator. For recall, the\ndenominator is the total number of positives in the distribution . We are not interested\nin knowing what you have claimed but in knowing how many of them are there in the\ndistribution. If you examine the definition using pD, you can see that recall is the probability\nof detection \u2014 how successfully you can detect a target. A high recall and a low recall can\noccur in two situations:\n\u0088High recall : This means that you are very good at detecting the target or rejecting\nthe null appropriately. A high recall can happen when the critical level \u03b1is low so that\nyou never miss a target. However, if the critical level \u03b1is low, you will suffer from a\nlow precision.\n\u0088Low recall : This means that you are too accepting of the null hypotheses, and so you\nnever claim that there is a target. As a result the number of successful detections is\nlow. However, having a low recall can buy you high precision because you do not reject\nthe null unless it has extreme evidence (hence there is no false alarm.)\nAs you can see from the discussions above, the precision-recall has a trade-off, just as\nthe ROC curve does. Since the PR curve and ROC curve are derived from pFandpD, there\nis a one-to-one correspondence. This can be proved by rearranging the terms in the previous\ntheorem.\n602", "618": "9.5. ROC AND PRECISION-RECALL CURVE\nTheorem 9.3. Thefalse alarm rate pFand the detection rate pDcan be expressed\nin terms of the precision and recall as\npF=recall(1 \u2212precision)\nprecision, (9.44)\npD= recall .\nThis result implies that whenever we have an ROC curve we can convert it to a PR curve.\nMoreover, whenever we have a PR curve we can convert it to an ROC curve. Therefore,\nthere is no additional information one can squeeze out by converting the curves. What we\ncan claim, at most, is that the two curves offer different ways of interpreting the decision\nrule.\nTo illustrate the equivalence between an ROC curve and a PR curve, we plot two\ndifferent decision rules in Figure 9.31 . Any point on the ROC curve will have a corresponding\npoint on the PR curve, and vice versa.\nFigure 9.31: There is a one-to-one correspondence between the ROC curve and the PR curve.\nThe MATLAB and Python codes for generating the PR curve are straightforward.\nAssuming that we have run the code used to generate Figure 9.28 , we plot the PR curve as\nfollows (this will give us Figure 9.31 ).\n% MATLAB code to generate a PR curve\nprecision1 = PD1./(PD1+PF1);\nprecision2 = PD2./(PD2+PF2);\nrecall1 = PD1;\nrecall2 = PD2;\nplot(recall1, precision1, \u2019LineWidth\u2019, 4); hold on;\nplot(recall2, precision2, \u2019LineWidth\u2019, 4); hold off;\n603", "619": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nPractice Exercise 9.8 . Suppose that the decision rule is a blind guess:\n\u03b4(y) =(\n1, with probability \u03b1,\n0, with probability 1 \u2212\u03b1,\nPlot the ROC curve and the PR curve.\nSolution : As we have shown earlier, pF(\u03b1) and pD(\u03b1) for this decision rule are pF(\u03b1) =\n\u03b1andpD(\u03b1) =\u03b1. Therefore,\nprecision =pD\npD+pF=\u03b1\n\u03b1+\u03b1=1\n2,and recall = pD=\u03b1.\nThus the PR curve is a straight line with a level of 0.5.\nFigure 9.32: The PR curve of a blind-guess decision rule is a straight line.\nPractice Exercise 9.9 . Convert the ROC curve in Figure 9.30 to a PR curve.\nSolution : The conversion is done by first computing pFandpD. Defining the precision\nand recall in terms of pFandpD, we plot the PR curves below.\nFigure 9.33: The PR curve of a real dataset.\n604", "620": "9.6. SUMMARY\nAs you can see from the figure, the PR curve behaves very differently from the\nROC curve. It is sometimes argued that the two curves can be interpreted differently,\neven though they describe the same decision rule for the same dataset.\n9.6 Summary\nIn this chapter, we have discussed five principles for quantifying the confidence of an esti-\nmator and making statistical decisions. To summarize the chapter, we clarify a few common\nmisconceptions about these topics.\n\u0088Confidence interval . Students frequently become confused about the meaning of a\nconfidence interval. It is not the interval that 95% of the samples will fall inside. It\nis also not the interval within which the estimator has a 95% chance to show up.\nA confidence interval is a random interval that has a 95% chance of including the\npopulation parameter. A better way to think about a confidence interval is to think of\nit as an alternative to a point estimate. A point estimate only gives a point, whereas\na confidence interval extends the point to an interval. All the randomness of the point\nestimate is also there in the confidence interval. However, if the confidence interval is\nnarrow, there is a good chance for the point estimate to be accurate.\n\u0088Bootstrapping . The most common misconception about bootstrapping is that it can\ncreate something from nothing. Another misconception is that bootstrapping can make\nyour estimates better. Both beliefs are wrong. Bootstrapping is a technique for esti-\nmating the estimator\u2019s variance, and consequently it provides a confidence interval.\nBootstrapping does not improve the point estimate, no matter how many bootstrap-\nping samples you synthesize. Bootstrapping works because the sampling with the re-\nplacement step is equivalent to drawing samples from the empirical distribution. The\nwhole process relies on the proximity between the empirical distribution and the true\npopulation. If you do not have enough samples and the empirical distribution does not\napproximate the population, bootstrapping will not work. Therefore, bootstrapping\ndoes not create something from nothing; it uses whatever you have and tells you how\nreliable the estimate is.\n\u0088Hypothesis testing . Students are often overwhelmed at first by the great number of\ntests one can use for hypothesis testing, e.g., p-value, critical value, Z-test, T-test, \u03c72\ntest, F-test, etc. Our advice is to forget about them and remember that hypothesis\ntesting is a court trial. Your job is to decide whether you have enough evidence to\ndeclare that the defendant is guilty. To reach a guilty verdict, you need to make sure\nthat the test statistic is unlikely to happen. Therefore, the best practice is to draw\nthe distributions of the test statistic and ask yourself how likely is it that the test\nstatistic has such a value. When you draw the pictures of the distributions, you will\nknow whether you should use a Gaussian Z, a Student\u2019s t, a\u03c72, aF-statistic, etc.\nWhen you examine the likelihood of the test statistic, you will know whether you want\nto use the p-value or the critical value. If you follow this principle, you will never be\nconfused by the oceans of tests you find in the textbooks.\n605", "621": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\n\u0088Neyman-Pearson . Beginners often find Neyman-Pearson abstract and do not under-\nstand why it is useful. In this chapter, however, we have explained why we need to\nunderstand Neyman-Pearson. It is a very general framework for many kinds of hy-\npothesis testing problems. All it says is that if we want to maximize the detection rate\nwhile maintaining the false alarm rate, then the optimal testing procedure boils down\nto the critical-value test and the p-value test. This gives us a certificate that our usual\nhypothesis testing is optimal according to the Neyman-Pearson framework.\n\u0088ROC and PR curves . On the internet nowadays there is a huge quantity of articles,\nblogs, and tutorials about howto plot the ROC curve and the PR curve. Often these\ncurves are explained through programming examples such as Python, R, or MATLAB.\nOur advice for studying the ROC curve and the PR curve is to go back to the Neyman-\nPearson framework. These two curves do not come out of the blue. The ROC curve is\nthe natural figure explaining the objective and the constraint in the Neyman-Pearson\nframework. By changing the coordinates, we obtain the PR curve. Therefore, the two\ncurves are the same in terms of the amount of information, but they offer different\ninterpretations.\n9.7 Reference\nConfidence Interval\n9-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 9.1.\n9-2 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics , W. H. Freeman,\n2nd Edition, 2009. Chapter 6.3.\n9-3 Robert V. Hogg, Joseph W. McKean, and Allen T. Craig, Introduction to Mathematical\nStatistics , Pearson, 7th Edition, 2013. Chapter 4.2.\n9-4 Larry Wasserman, All of Statistics , Springer 2003. Chapter 6.\n9-5 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 8.4.\nBootstrapping\n9-6 Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Elements of Statistical Learn-\ning, Springer, 2nd Edition. Chapter 8.2.\n9-7 Larry Wasserman, All of Statistics , Springer 2003. Chapter 8.\n9-8 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics , W. H. Freeman,\n2nd Edition, 2009. Chapter 6.4.\n9-9 Robert V. Hogg, Joseph W. McKean, and Allen T. Craig, Introduction to Mathematical\nStatistics , Pearson, 7th Edition, 2013. Chapter 4.9.\n606", "622": "9.8. PROBLEMS\nHypothesis Testing\n9-10 Robert V. Hogg, Joseph W. McKean, and Allen T. Craig, Introduction to Mathematical\nStatistics , Pearson, 7th Edition, 2013. Chapter 4.5.\n9-11 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and\nStochastic Processes , McGraw-Hill, 4th Edition, 2001. Chapter 8.\n9-12 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En-\ngineering , Prentice Hall, 3rd Edition, 2008. Chapter 8.5.\n9-13 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008. Chapter 9.\n9-14 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics , W. H. Freeman,\n2nd Edition, 2009. Chapter 6.3.\n9-15 Larry Wasserman, All of Statistics , Springer 2003. Chapter 10.\n9-16 Laura Simon, Introduction to Mathematical Statistics , Penn State University STAT\n415 Textbook, Online materials. Accessed 12/2020. https://online.stat.psu.edu/\nstat415/\nNeyman-Pearson and ROC curves\n9-17 Robert V. Hogg, Joseph W. McKean, and Allen T. Craig, Introduction to Mathematical\nStatistics , Pearson, 7th Edition, 2013. Chapter 8.\n9-18 H. Vincent Poor, An Introduction Signal Detection and Estimation , Springer, 1998.\n9-19 Bernard C. Levy, Principles of Signal Detection and Parameter Estimation , Springer,\n2008.\n9-20 Steven M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory ,\nPrentice-Hall, 1993.\n9-21 Steven M. Kay, Fundamentals of Statistical Signal Processing: Detection Theory , Prentice-\nHall, 1998.\n9.8 Problems\nExercise 1.\nConsider i.i.d. Gaussian random variables X1, . . . , X Nwith an unknown mean \u03b8and a known\nvariance \u03c32= 1. Suppose N= 30. Find the confidence level 1 \u2212\u03b1for the confidence intervals\nof the mean b\u0398:\n(a)I= [b\u0398\u22122.14\u03c3\u221a\nN,b\u0398 +2.14\u03c3\u221a\nN]\n(b)I= [b\u0398\u22121.85\u03c3\u221a\nN,b\u0398 +1.85\u03c3\u221a\nN]\n607", "623": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\nExercise 2.\nSuppose that we have conducted an experiment with N= 100 samples. A 95% confidence\ninterval of the mean was 0 .45\u2264\u00b5\u22640.82.\n(a) Would a 99% confidence interval calculated from the sample data be wider or narrower?\n(b) Is it correct to interpret the confidence interval as saying that there is a 95% chance\nthat\u00b5is between 0.49 and 0.82? You may answer yes, no, or partially correct. Explain.\n(c) Is it correct to say that if we conduct the experiment 1000 times, there will be 950\nconfidence intervals that will contain \u00b5? You may answer yes, no, or partially correct.\nExplain.\nExercise 3.\nSuppose that we have conducted an experiment. We know that \u03c3= 25. We obtained N= 20\nsamples and found that the sample mean is b\u0398 = 1014.\n(a) Construct a 95% two-sided confidence interval of b\u0398.\n(b) Construct a 95% one-sided confidence interval (the lower tail) of b\u0398.\nExercise 4.\nLetX1, . . . , X Nbe i.i.d. Gaussian with Xn\u223cGaussian(0 ,1). Let Yn=eXn, and suppose\nwe have N= 100 samples. We want to compute a 95% confidence interval for skewness.\n(a) Randomly subsample the dataset with B= 30 samples. Repeat the exercise 5 times.\nPlot the resulting histograms using MATLAB or Python.\n(b) Repeat (a) for M= 500 times and compute the 95% bootstrapped confidence interval\nof the skewness.\n(c) Try using a larger B= 70 and a smaller B= 10. Report the 95% bootstrapped\nconfidence interval of the skewness.\nExercise 5.\nLetX1, . . . , X Nbe i.i.d. uniform with Xn\u223cUniform(0 , \u03b8). Let b\u0398 = max {X1, . . . , X N}.\nGenerate a dataset of N= 50 with \u03b8= 1.\n(a) Find the distribution of the estimator b\u0398.\n(b) Show that P[b\u0398 =\u03b8] = 1\u2212(1\u2212(1/n))N. Thus, as N\u2192 \u221e , we have P[b\u0398 =\u03b8] = 0.\n(c) Use Python or MATLAB to generate the histogram of b\u0398 from bootstrapping. How\ndoes the bootstrapped histogram look as Ngrows? Why?\nExercise 6.\nLetXbe a Gaussian random variable with unknown mean and unknown variance. It was\nfound that with N= 15,\nNX\nn=1Xn= 250 ,NX\nn=1X2\nn= 10000 .\n608", "624": "9.8. PROBLEMS\nFind a 95% confidence interval of the mean of X.\nExercise 7.\nLetb\u0398 be the sample mean of a dataset containing Nsamples. It is known that the samples\nare drawn from Gaussian( \u03b8,32). Find Nsuch that\nP[b\u0398\u22121\u2264\u03b8\u2264b\u0398 + 1] = 0 .95.\nExercise 8.\nWhich of the following statements are valid hypothesis testing problems?\n(a)H0:\u00b5= 25 and H1:\u00b5\u0338= 25.\n(b)H0:\u03c3 >10 and H1:\u03c3= 10.\n(c)H0:X= 50 and H1:X\u0338= 50.\n(d)H0:p-value = 0.1, H1:p-value = 0.5.\nExercise 9.\nIt is claimed that the mean is \u03b8= 12 with a standard deviation 0.5. Consider H0:\u03b8= 12\nandH1:\u03b8 <12. Ten samples are obtained, and it is found that b\u0398 = 13 .5. With a 95%\nconfidence level, should we accept or reject the null hypothesis?\nExercise 10.\nConsider a hypothesis testing problem: H0:\u03b8= 175 versus an alternative hypothesis H1:\n\u03b8 >175. Assume N= 10 and \u03c3= 20.\n(a) Find the type 1 error if the critical region is b\u0398>185.\n(b) Find the type 2 error if the true mean is 195.\nExercise 11.\nConsider H0:\u03b8= 30000 versus an alternative hypothesis H1:\u03b8 >30000. Suppose N= 16,\nand let \u03c3= 1500.\n(a) If we want \u03b1= 0.01, what is z\u03b1?\n(b) What is the type 2 error when \u03b8= 31000?\nExercise 12.\nLetWn\u223cGaussian(0 , \u03c32), and consider two hypotheses:\nH0: Xn=\u03b80+Wn, n = 1, . . . , N,\nH1: Xn=\u03b81+Wn, n = 1, . . . , N.\nLetX= (1/N)PN\nn=1Xn.\n609", "625": "CHAPTER 9. CONFIDENCE AND HYPOTHESIS\n(a) Show that the likelihood of observing X1, . . . , X Ngiven H0is\nfX(x|H0) =1\n(2\u03c0\u03c32)N/2exp(\n\u22121\n2\u03c32NX\nn=1(Xn\u2212\u03b80)2)\n.\n(b) Find the likelihood fX(x|H1) of observing X1, . . . , X Ngiven H1.\n(c) The likelihood ratio test states that\nfX(x|H1)\nfX(x|H0)\u2277H1\nH0\u03c4.\nShow that the likelihood ratio test is given by\nX\u2277H1\nH0\u03b80+\u03b81\n2+\u03c32log\u03c4\nN(\u03b81\u2212\u03b80).\n610", "626": "Chapter 10\nRandom Processes\nIn modern data science, many problems involve time. The stock market changes every\nminute; a speech signal changes every millisecond; a car changes its steering angle constantly;\nthe examples are endless. A common theme among all these examples is randomness. We\ndo not know whether a stock will go up or down tomorrow, although we may be able to\nmake some predictions based on previous observations. We do not know the next word of a\nsentence, but we can guess based on the context. Random processes are tools that can be\napplied to these situations. We treat a random process as an infinitely long vector of random\nvariables where the correlations between the individual variables define the statistical prop-\nerties of the process. If we can determine these correlations, we will be able to summarize\nthe past and predict the future.\nThe objective of this chapter is to introduce the basic concepts of random processes .\nGiven the breadth of the subject, we can only cover the most elementary results, but they\nare sufficient for many engineering and data science problems. However, there are complex\nsituations for which these elementary results will be insufficient. The references at the end\nof this chapter contain more in-depth discussions of random processes.\nPlan of this chapter\nWe begin by outlining the definition of random processes and ways to characterize their\nrandomness in Section 10.1. In Section 10.2 we discuss the mean function, the autocorrelation\nfunction, and the autocovariance function of a random process. In Section 10.3 we look at\na special subclass of random processes known as the wide-sense stationary processes. Wide-\nsense stationary processes allow us to use tools in the Fourier domain to make statistical\nstatements. Based on wide-sense stationary processes, we discuss power spectral density in\nSection 10.4. With this concept, we can ask what will happen to the random process when we\npass it through a linear transformation. In Section 10.5 we discuss such interactions between\nthe random process and a linear time-invariant system. Finally, we discuss a practical usage\nof random processes in the subject of optimal linear filters in Section 10.6.\n611", "627": "CHAPTER 10. RANDOM PROCESSES\n10.1 Basic Concepts\n10.1.1 Everything you need to know about a random process\nHere is the single most important thing you need to remember about random processes:\nWhat is a random process?\nA random process is a function indexed by a random key .\nThat\u2019s it. Now you may be wondering what exactly a \u201cfunction indexed by a random key \u201d\nmeans. To help you see the picture, we consider two examples.\nExample 10.1 . We consider a set of straight lines. We define two random variables a\nandbthat are uniformly distributed in a certain range. We then define a function:\nf(t) =at+b, \u22122\u2264t\u22642. (10.1)\nClearly, f(t) is a function of time t. But since aandbare random, f(t) is also random.\nThe randomness is caused by aandb. To emphasize this dependency, we write f(t) as\nf(t, \u03be) =a(\u03be)t+b(\u03be),\u22122\u2264t\u22642,\nwhere \u03be\u2208\u2126 denotes the random index of the constants ( a, b) and \u2126 is the sample\nspace of \u03be. Therefore, by picking a different pair of constants ( a(\u03be), b(\u03be)), we will have\na different function f(t, \u03be), which in our case is a straight line of different slope and\ny-intercept.\n-2 -1 0 1 2\nt-1-0.500.51f(t)\nFigure 10.1: The set of straight lines f(x) =ax+bwhere a, b\u2208R.\nAs a special case of the example, suppose that the sample space contains only\ntwo pairs of constants: ( a, b) = (1 .2,0.6) and ( a, b) = (\u22120.75,1.8). The probability of\n612", "628": "10.1. BASIC CONCEPTS\ngetting either pair is1\n2. Then the function f(t, \u03be) will take two forms:\nf(t, \u03be) =(\n1.2t+ 0.6, with probability1\n2,\n\u22120.75t+ 1.8, with probability1\n2.\nEvery time you pick a sample you pick one of the two functions, either f(t, \u03be1) or\nf(t, \u03be2). So we say that f(t, \u03be) is a random process because it is a function f(t) indexed\nby a random key \u03be.\nExample 10.2 . This example studies the function\nf(t) = cos( \u03c90t+ \u0398),\u22121\u2264t\u22641,\nwhere \u0398 is a random phase distributed uniformly over the range [0 ,2\u03c0]. Depending on\nthe randomness of \u0398, the function f(t) will take a different phase offset. To emphasize\nthis dependency, we write\nf(t, \u03be) = cos( \u03c90t+ \u0398(\u03be)),\u22121\u2264t\u22641. (10.2)\n-1 -0.5 0 0.5 1\nt-2-1012f(t)\nFigure 10.2: The set of phase-shifted cosines f(t) = cos( \u03c90t+\u03b8)where \u03b8\u2208[0,2\u03c0].\nAgain, \u03bedenotes the index of the random variable \u0398. Since \u0398 is drawn uniformly\nfrom the interval [0 ,2\u03c0], the following functions are two possible realizations:\nf(t, \u03be1) = cos\u0012\n\u03c90t+3\u03c0\n4\u0013\n,\u22121\u2264t\u22641,\nf(t, \u03be2) = cos\u0012\n\u03c90t\u22127\u03c0\n3\u0013\n,\u22121\u2264t\u22641.\nJust as with the previous example, f(t) is a function indexed by a random key \u03be.\nThese two examples should give you a feeling for what to expect from a random process.\nA random process is quite similar to a random variable because they are both contained\nin a certain sample space. For (discrete) random variables, the sample space is a collection\nof outcomes {\u03be1, \u03be2, . . . , \u03be N}. The random variable X:F \u2192Ris a mapping that maps\n\u03bentoX(\u03ben), where X(\u03ben) is a number. For random processes, the sample space is also\n613", "629": "CHAPTER 10. RANDOM PROCESSES\n{\u03be1, \u03be2, . . . , \u03be N}. However, the mapping Xdoes not map \u03bento anumber X(\u03ben) but to a\nfunction X(t, \u03ben). A function has the time index t, which is absent in the number. Therefore,\nfor the same \u03ben,X(t1, \u03ben) can take one value and X(t2, \u03ben) can take another value.\nFigure 10.3: The sample space of a random process X(t, \u03be)contains many functions. Therefore, each\nrandom realization is afunction .\nFigure 10.3 shows the sample space of a random process. Each outcome in the sample\nspace is a function. The probability of getting a function is specified by the probability\nmass or the probability density of the associated random key \u03be. If you put your hand into\nthe sample space, the sample you pick will be a function that will change with time and is\nindexed by the random key. From our discussions of joint random variables in Chapter 5,\nyou can think of the function as a vector. When you pull a sample from the sample space,\nyou pull the entire vector and not just an element.\n10.1.2 Statistical and temporal perspectives\nSince a random process is a function indexed by a random key, it is a two-dimensional object.\nIt is a function both of time tand of the random key \u03be. That\u2019s why we use the notation\nX(t, \u03be) to denote a random process. These two axes play different roles, as illustrated in\nFigure 10.4 .\nTemporal perspective : Let us fix the random key at \u03be=\u03be0. This gives us a function\nX(t, \u03be0). Since \u03beis already fixed at \u03be0, we are looking at a particular realization drawn\nfrom the sample space. This realization is expressed as a function X(t, \u03be0), which is just\na deterministic function that evolves over time. There is no randomness associated with\nit. This is analogous to a random variable. While Xitself is a random variable, by fixing\nthe random key \u03be=\u03be0,X(\u03be0) is just a real number. For random processes, X(t, \u03be0) now\nbecomes a function.\nSince X(t, \u03be0) is a function that evolves over time, we view it along the horizontal axis.\nFor example, we can study the sequence\nX(t1, \u03be0), X(t2, \u03be0), . . . , X (tK, \u03be0),\nwhere t1, . . . , t Kare the time indices of the function. This sequence is deterministic and is\njust a sequence of numbers, although the numbers evolve as tchanges.\nStatistical perspective : The other perspective, which could be slightly more abstract,\nis the statistical perspective. Let us fix the time at t=t0. The random key \u03becan take any\n614", "630": "10.1. BASIC CONCEPTS\n(a) Temporal perspective (b) Statistical perspective\nFigure 10.4: Temporal and statistical perspectives of a random process. For the temporal perspective\n(which we call the horizontal perspective), we fix the random key \u03beand look at the function in time.\nFor the statistical perspective (which we call the vertical perspective), we fix the time and look at the\nfunction at different random keys .\nstate defined in the sample space. So if the sample space contains {\u03be1, . . . , \u03be N}, the sequence\n{X(t0, \u03be1), . . . , X (t0, \u03beN)}is a sequence of random variables, because the \u03be\u2019s can go from\none state to another state.\nA good way to visualize the statistical perspective is the vertical perspective in which\nwe write the sequence as a vertical column of random variables:\nX(t0, \u03be1)\nX(t0, \u03be2)\n...\nX(t0, \u03beN)\nThat is, if you fix the time at t=t0, you are getting a sequence of random variables. The\nprobability of getting a particular value X(t0) depends on which random state you land on.\nWhy do we bother to differentiate the temporal perspective and the statistical per-\nspective? The reason is that the operations associated with the two are different, even if\nsometimes they give you the same result. For example, if we take the temporal average of\nthe random process, we get a number:\nX(\u03be) =1\nTZT\n0X(t, \u03be)dt. (10.3)\nWe call this the \u201ctemporal average\u201d because we have integrated the function over time. The\nresulting value will not change with time. However, X(\u03be) depends on the random key you\nprovide. If you pick a different random realization, X(\u03be) will take a different value. So the\ntemporal average is a random variable .\nOn the other hand, if we take the statistical average of the random process, we get\nE[X(t)] =Z\n\u2126X(t, \u03be)p(\u03be)d\u03be, (10.4)\n615", "631": "CHAPTER 10. RANDOM PROCESSES\nwhere p(\u03be) is the PDF of the random key \u03be. We call this the statistical average because we\nhave taken the expectation over all possible random keys. The resulting object E[X(t)] is\ndeterministic but a function of time.\nNo matter how you look at the temporal average or the statistical average, they are\ndifferent with the following exception: that X(\u03be) = const and E[X(t)] = const, for example,\nX(\u03be) =E[X(t)] = 0. This happens only for some special (and useful) random processes\nknown as ergodic processes that allow us to approximate the statistical average using the\ntemporal average, with some guarantees derived from the law of large numbers. We will\nreturn to this point later.\nExample 10.3 . Let A\u223cUniform[0 ,1]. Define X(t, \u03be) =A(\u03be) cos(2 \u03c0t).\nIn this example, the magnitude A(\u03be) is a random variable depending on the\nrandom key \u03be. For example if we draw \u03be1, perhaps we will get a value A(\u03be1) = 0 .5.\nThen X(t, \u03be1) = 0 .5 cos(2 \u03c0t). To take another example, if we draw \u03be2, we may get\nA(\u03be2) = 1. Then X(t, \u03be2) = 1 cos(2 \u03c0t).Figure 10.5 shows a few random realizations\nof the cosines. We can look at X(t, \u03be) from the statistical and the temporal views.\n-2 -1 0 1 2-1-0.500.51\nX1(t)\nX2(t)\nX3(t)\nX4(t)\nX5(t)\nFigure 10.5: Five different realizations of the random process X(t) =Acos(2 \u03c0t).\n\u0088Statistical View : Fix t(for example t= 10). In this case, we have\nX(t, \u03be) =A(\u03be) cos(2 \u03c0(10)) = A(\u03be) cos(20 \u03c0),\nwhich is a random variable because cos(20 \u03c0) is a constant. The randomness of\nXcomes from the fact that A(\u03be)\u223cUniform[0 ,1].\n\u0088Temporal View : Fix \u03be(for example A(\u03be) = 0 .7). In this case, we have\nX(t, \u03be) = 0 .7 cos(2 \u03c0t),\nwhich is a deterministic function of t.\nExample 10.4 . Let Abe a discrete random variable with a PMF\nP(A= +1) =1\n2andP(A=\u22121) =1\n2.\n616", "632": "10.1. BASIC CONCEPTS\nWe define the function X[n, \u03be] =A(\u03be)(\u22121)n. In this example, Acan only take two\nstates. If A= +1, then X[n, \u03be] = (\u22121)n. IfA=\u22121, then X[n, \u03be] = (\u22121)n+1.\n-1.5-1-0.500.511.5\n0 1 2 3 4 5X1(n)\n-1.5-1-0.500.511.5\n0 1 2 3 4 5X2(n)\nFigure 10.6: Realizations of the random process X[n] =A(\u22121)n.\nThe graphical illustration of this example is shown in Figure 10.6 . Again, we can\nlook at X[n, \u03be] from two views.\n\u0088Statistical View : Fix n, say n= 10. Then,\nX(\u03be) =(\n(\u22121)10= 1, with prob 1 /2,\n(\u22121)11=\u22121, with prob 1 /2,\nwhich is a Bernoulli random variable.\n\u0088Temporal View : Fix \u03be. Then,\nX[n] =(\n(\u22121)n, ifA= +1 ,\n(\u22121)n+1, ifA=\u22121,\nwhich is a time series.\nIn this example, we see that the sample space of X(n, \u03be) consists of only two functions\nwith probabilities\nP(X[n] = (\u22121)n) =1\n2,\nP(X[n] = (\u22121)n+1) =1\n2,\nTherefore, if there is a sequence outside the sample space, e.g.,\nP\u0000\nX[n] =\u00021 1 1 \u22121 1 \u22121\u00b7\u00b7\u00b7\u0003\u0001\n= 0\nthen the probability of obtaining that sequence is 0.\nWhat do we mean by statistical average and temporal average?\n\u0088Statistical average: Take the expectation of X(t, \u03be) over \u03be. This is the vertical\n617", "633": "CHAPTER 10. RANDOM PROCESSES\naverage.\n\u0088Temporal average: Take the expectation of X(t, \u03be) over t. This is the horizontal\naverage.\n\u0088In general, statistical average \u0338= temporal average.\n10.2 Mean and Correlation Functions\nGiven a random variable, we often want to know the expectation and variance, and often\nwe also want to know the expectation and variance for the random processes. Nevertheless,\nwe need to consider the time axis. In this section, we discuss the mean function and the\nautocorrelation function .\n10.2.1 Mean function\nDefinition 10.1. Themean function \u00b5X(t)of a random process X(t)is\n\u00b5X(t) =E[X(t)]. (10.5)\nLet\u2019s consider the \u201cexpectation\u201d of X(t). Recall that a random process is actually X(t, \u03be)\nwhere \u03beis the random key. Therefore, the expectation is taken with respect to \u03be, or to state\nit more explicitly,\n\u00b5X(t) =E[X(t)] =Z\n\u2126X(t, \u03be)p(\u03be)d\u03be,\nwhere p(\u03be) is the PDF of the random key. This is an abstract definition, but it is not difficult\nto understand if you follow the example below.\nExample 10.5 . Let A\u223cUniform[0 ,1], and let X(t) =Acos(2 \u03c0t). Find \u00b5X(t).\nSolution . The solution to this problem is actually very simple:\n\u00b5X(t) =E[X(t)] =E[Acos(2 \u03c0t)]\n= cos(2 \u03c0t)E[A] =1\n2cos(2 \u03c0t).\nSo the answer is \u00b5X(t) =1\n2cos(2 \u03c0t).\nWe can link the equations to the definition more explicitly. To do so, we rewrite\nX(t) as\nX(t, \u03be) =A(\u03be) cos(2 \u03c0t).\n618", "634": "10.2. MEAN AND CORRELATION FUNCTIONS\nThen we take the expectation over A:\n\u00b5X(t) =Z\n\u2126X(t, a)pA(a)da=Z1\n0acos(2 \u03c0t)\u00b71da\n= cos(2 \u03c0t)\u0014a2\n2\u00151\n0=1\n2cos(2 \u03c0t).\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51\nFigure 10.7: The mean function of X(t) =Acos(2 \u03c0t).\nAn illustration is provided in Figure 10.7 , in which we observe many random\nrealizations of the random process X(t, \u03be). On top of these, we also see the mean\nfunction. The way to visualize the mean function is to use the statistical perspective .\nThat is, fix a time tand look at all the possible values that the function can take. For\nexample, if we fix t=t0, then we will have a set of realizations of one random variable:\n\u001a\n0.71 cos(2 \u03c0t0),0.58 cos(2 \u03c0t0), . . . , 0.93 cos(2 \u03c0t0)\u001b\n\u2192take expectation\nTherefore, when we take the expectation, it is that of the underlying random variable.\nIf we move to another timestamp t=t1, we will have a different expectation because\ncos(2 \u03c0t0) now becomes cos(2 \u03c0t1).\nThe MATLAB/Python codes used to generate Figure 10.7 are shown below. You can\nalso replace the line 0.5*cos(2*pi*t) by the mean function mean(X) (in MATLAB).\n% MATLAB code for Example 10.5\nx = zeros(1000,20);\nt = linspace(-2,2,1000);\nfor i=1:20\nX(:,i) = rand(1)*cos(2*pi*t);\nend\nplot(t, X, \u2019LineWidth\u2019, 2, \u2019Color\u2019, [0.8 0.8 0.8]); hold on;\nplot(t, 0.5*cos(2*pi*t), \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0.6 0 0]);\n# Python code for Example 10.5\nx = np.zeros((1000,20))\n619", "635": "CHAPTER 10. RANDOM PROCESSES\nt = np.linspace(-2,2,1000)\nfor i in range(20):\nx[:,i] = np.random.rand(1)*np.cos(2*np.pi*t)\nplt.plot(t,x,color=\u2019gray\u2019)\nplt.plot(t,0.5*np.cos(2*np.pi*t),color=\u2019red\u2019)\nplt.show()\nExample 10.6 . Let \u0398 \u223cUniform[ \u2212\u03c0, \u03c0], and let X(t) = cos( \u03c9t+ \u0398). Find \u00b5X(t).\nSolution .\n\u00b5X(t) =E[cos(\u03c9t+ \u0398)] =Z\u03c0\n\u2212\u03c0cos(\u03c9t+\u03b8)\u00b71\n2\u03c0d\u03b8= 0.\nAgain, as in the previous example, we can try to map this simple calculation with the\ndefinition. Write X(t) as\nX(t, \u03be) = cos( \u03c9t+ \u0398(\u03be)).\nThen the expectation is\n\u00b5X(t) =Z\n\u2126cos(\u03c9t+\u03b8)p\u0398(\u03b8)d\u03b8\n=Z\u03c0\n\u2212\u03c0cos(\u03c9t+\u03b8)\u00b71\n2\u03c0d\u03b8= 0.\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51\nFigure 10.8: The mean function of X(t) = cos( \u03c9t+ \u0398) .\nFigure 10.8 illustrates the random realizations for X(t) = cos( \u03c9t+ \u0398) and the\nmean function. The zero mean should not be a surprise because if we take the statistical\naverage (the vertical average) across all the possible values at any time instant, the\npositive and negative values of the realizations will make the mean zero.\nWe should emphasize that the statistical average is not the same as the temporal\naverage, even if they give you the same value. Why do we say that? If we calculate\nthetemporal average of the function cos( \u03c9t+\u03b80) for a specific value \u0398 = \u03b80, then we\n620", "636": "10.2. MEAN AND CORRELATION FUNCTIONS\nhave\nX=1\nTZT\n0cos(\u03c9t+\u03b80)dt= 0,\nassuming that Tis a multiple of the cosine period. This implies that the temporal\naverage is zero, which is the same as the statistical average. This gives us an example\nin which the statistical average and the temporal average have the same value, although\nwe know they are two completely different things.\nThe MATLAB/Python codes used to generate Figure 10.8 are shown below.\n% MATLAB code for Example 10.6\nx = zeros(1000,20);\nt = linspace(-2,2,1000);\nfor i=1:20\nX(:,i) = cos(2*pi*t+2*pi*rand(1));\nend\nplot(t, X, \u2019LineWidth\u2019, 2, \u2019Color\u2019, [0.8 0.8 0.8]); hold on;\nplot(t, 0*cos(2*pi*t), \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0.6 0 0]);\n# Python code for Example 10.6\nx = np.zeros((1000,20))\nt = np.linspace(-2,2,1000)\nfor i in range(20):\nTheta = 2*np.pi*(np.random.rand(1))\nx[:,i] = np.cos(2*np.pi*t+Theta)\nplt.plot(t,x,color=\u2019gray\u2019)\nplt.plot(t,np.zeros((1000,1)),color=\u2019red\u2019)\nplt.show()\nExample 10.7 . Let us consider a discrete-time random process. Let X[n] =Sn, where\nS\u223cUniform[0 ,1]. Find \u00b5X[n].\n\u00b5X[n] =E[sn] =Z1\n0snds=1\nn+ 1.\nIn this example the randomness goes with the constant s. Thus, if we write X[n] as\nX[n, \u03be] = [S(\u03be)]n,\nthe expectation is\nE[X[n]] =Z\n\u2126snpS(s)ds=Z1\n0sn\u00b71ds=1\nn+ 1.\nThe graphical illustration is provided in Figure 10.9 .\n621", "637": "CHAPTER 10. RANDOM PROCESSES\n-0.200.20.40.60.811.2\n0 5 10 15 20\nFigure 10.9: The mean function of X[n] =Sn, where S\u223cUniform [0,1].\nThe MATLAB code used to generate Figure 10.9 is shown below. We skip the Python\nimplementation because it is straightforward.\n% MATLAB code for Example 10.7\nt = 0:20;\nfor i=1:20\nX(:,i) = rand(1).^t;\nend\nstem(t, X, \u2019LineWidth\u2019, 2, \u2019Color\u2019, [0.8 0.8 0.8]); hold on;\nstem(t, 1./(t+1), \u2019LineWidth\u2019, 2, \u2019MarkerSize\u2019, 8);\n10.2.2 Autocorrelation function\nIn random processes, the notions of \u201cvariance\u201d and \u201ccovariance\u201d are trickier than for random\nvariables. Let us first define the concept of an autocorrelation function .\nDefinition 10.2. Theautocorrelation function of a random process X(t)is\nRX(t1, t2) =E[X(t1)X(t2)]. (10.6)\nRX(t1, t2) is not difficult to calculate \u2014 just integrate X(t1)X(t2) using the appropriate\nPDFs.\nExample 10.8 . Let A\u223cUniform[0 ,1],X(t) =Acos(2 \u03c0t). Find RX(t1, t2).\nSolution .\nRX(t1, t2) =E[Acos(2 \u03c0t1)Acos(2 \u03c0t2)]\n=E[A2] cos(2 \u03c0t1) cos(2 \u03c0t2) =1\n3cos(2 \u03c0t1) cos(2 \u03c0t2).\n622", "638": "10.2. MEAN AND CORRELATION FUNCTIONS\nExample 10.9 . Let \u0398 \u223cUniform[ \u2212\u03c0, \u03c0],X(t) = cos( \u03c9t+ \u0398). Find RX(t1, t2).\nSolution .\nRX(t1, t2) =E[cos(\u03c9t1+ \u0398) cos( \u03c9t2+ \u0398)]\n=1\n2\u03c0Z\u03c0\n\u2212\u03c0cos(\u03c9t1+\u03b8) cos( \u03c9t2+\u03b8)d\u03b8\n(a)=1\n2\u03c0Z\u03c0\n\u2212\u03c01\n2\u0014\ncos(\u03c9(t1+t2) + 2\u03b8) + cos( \u03c9(t1\u2212t2))\u0015\nd\u03b8\n=1\n2cos\u0010\n\u03c9(t1\u2212t2)\u0011\n,\nwhere in (a) we applied the trigonometric formula:\ncosAcosB=1\n2[cos(A+B) + cos( A\u2212B)],\nAs you can see, the calculations are not difficult. The tricky thing is the interpretation\nofRX(t1, t2).\nHow do we understand the meaning of E[X(t1)X(t2)]?\nE[X(t1)X(t2)] is analogous to the correlation E[XY] between two random variables\nXandY.\nThe autocorrelation function E[X(t1)X(t2)] is analogous to the correlation E[XY] in rela-\ntion to a pair of random variables. In our discussions of E[XY], we mentioned that E[XY]\ncould be regarded as the inner product of two vectors, and so it is a measure of the closeness\nbetween XandY. Now, if we substitute XandYwith X(t1) and X(t2) respectively, then\nwe are effectively asking about the closeness between X(t1) and X(t2). So, in a nutshell, the\nautocorrelation function tells us the correlation between the function at two different time\nstamps.\nWhat do we mean by the correlation between two timestamps? Remember that X(t1)\nandX(t2) are two random variables. Consider the following example.\nExample 10.10 . LetX(t) =Acos(2 \u03c0t), where A\u223cUniform[0 ,1]. Find E[X(0)X(0.5)].\nSolution . IfX(t) =Acos(2 \u03c0t), then\nX(0) = Acos(0) = A,\nX(0.5) = Acos(\u03c0) =\u2212A.\nWhen you have two random variables, you consider their correlations. Using this ex-\n623", "639": "CHAPTER 10. RANDOM PROCESSES\nample, we have that\nE[X(0)X(0.5)] =\u2212E[A\u00b7A]\n=\u2212E[A2] =\u22121\n3.\nA picture will reveal what is happening. Figure 10.10 presents the realizations of the\nrandom process X(t) =Acos(2 \u03c0t). If we consider X(0) and X(0.5), each of them is a\nrandom variable, and thus we can ask about their PDFs. It is obvious from the illustration\nthat the random variable X(0) has a PDF that is a uniform distribution from 0 to 1,\nwhereas the random variable X(0.5) has a PDF that is a uniform distribution from \u22121 to 0.\nMathematically, the PDFs are\nfX(0)(x) =(\n1,0\u2264x\u22641,\n0,otherwiseand fX(0.5)(x) =(\n1,\u22121\u2264x\u22640,\n0,otherwise .\nSince X(0) and X(0.5) have their own PDFs, we can calculate their correlation. This will\ngive us E[X(0)X(0.5)] which after some calculations is E[X(0)X(0.5)] =\u22121\n3.\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51\nFigure 10.10: The autocorrelation between X(0)andX(0.5)should be regarded as the correlation\nbetween two random variables. Each random variable has its own PDF.\nWe can now consider the autocorrelation for any t1andt2. When you are evaluating\nthe autocorrelation function, you are not just evaluating at t= 0 and t= 0.5, you are\nalso evaluating the correlation for all pairs of t1andt2. Now you want to know what the\ncorrelation is between t= 0 and t= 0.5,t= 2 and t= 3.1, etc. Of course, there are\ninfinitely many pairs of time instants. The point of the autocorrelation function is to tell\nyou the correlation of allthe pairs. In other words, if we tell you RX(t1, t2), you will be able\nto plug in a value of t1and a value of t2and tell us the correlation at ( t1, t2). How is this\npossible? To find out, let\u2019s consider the following example.\nExample 10.11 . Let A\u223cUniform[0 ,1],X(t) =Acos(2 \u03c0t). Find RX(0,0.5), and draw\nRX(t1, t2).\n624", "640": "10.2. MEAN AND CORRELATION FUNCTIONS\nSolution . From the previous example, we know that\nRX(t1, t2) =1\n3cos(2 \u03c0t1) cos(2 \u03c0t2).\nTherefore, RX(0,0.5) =1\n3cos(2 \u03c00) cos(2 \u03c00.5) =\u22121\n3, which is the same as if we had\ncomputed it from the first principle.\nThe autocorrelation function tells you how one point of a time series is correlated\nwith another point of the time series. If RX(t1, t2) gives a high value, then it means the\nrandom variables at t1andt2have a strong correlation. To understand this, suppose\nwe let t1= 0, and let us vary t2. Then\nRX(0, t2) =1\n3cos(2 \u03c00) cos(2 \u03c0t2) =1\n3cos(2 \u03c0t2).\nThis is a periodic function that cycles through itself whenever t2is an integer. As\nwe recall from Figure 10.10 , ift2= 0.5, the random variable X(t2) will take only\nthe negative values, but otherwise it is correlated with X(0). On the other hand, if\nt2= 0.25, then Figure 10.10 says that the random variable X(t2) is a constant 0, and\nso the correlation with X(0) is zero.\nClearly, RX(t1, t2) is a 2-dimensional function of t1andt2. You need to tell RX\nwhich of the two time instants you want to compare, and then RXwill tell you the\ncorrelation. So no matter what happens, you must specify two time instants. Because\nRX(t1, t2) is a 2-dimensional function, we can visualize it by calculating all the possible\nvalues it takes. For example, if RX(t1, t2) =1\n3cos(2 \u03c0t1) cos(2 \u03c0t2), we can plot RXas\na function of t1andt2.Figure 10.11 shows the plot.\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51\n-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1\nt1-1\n-0.75\n-0.5\n-0.25\n0\n0.25\n0.5\n0.75\n1t2\nFigure 10.11: The autocorrelation function RX(t1, t2) =1\n3cos(2 \u03c0t1) cos(2 \u03c0t2).\nThe MATLAB/Python code for Figure 10.11 is shown below.\n% MATLAB code for Example 10.11\nt = linspace(-1,1,1000);\nR = (1/3)*cos(2*pi*t(:)).*cos(2*pi*t);\nimagesc(t,t,R);\n# Python code for Example 10.11\nimport numpy as np\n625", "641": "CHAPTER 10. RANDOM PROCESSES\nimport matplotlib.pyplot as plt\nt = np.linspace(-1,1,1000)\nR = (1/3)*np.outer(np.cos(2*np.pi*t), np.cos(2*np.pi*t))\nplt.imshow(R, extent=[-1, 1, -1, 1])\nplt.show()\nTo understand the 2D function shown on the right hand side of Figure 10.11 , we can\ntake a closer look by drawing Figure 10.12 . For any two time instants t1andt2, we have\ntwo random variables X(t1) and X(t2). The joint expectation E[X(t1)X(t2)] will return us\nsome value, and this is a point in the 2D plot RX(t1, t2). The value tells us the correlation\nbetween X(t1) and X(t2). In the example in which t1= 0 and t2= 0.5, the correlation is\n\u22121\n3. Interestingly, if we pick another pair of time instants t1=\u22120.5 and t2= 0, the joint\nexpectation is E[X(\u22120.5)X(0)] = \u22121\n3, which is the same value. However, this \u22121\n3is located\nat a different valley than E[X(0)X(0.5)] in the 2D plot.\nFigure 10.12: To understand the autocorrelation function, pick two time instants t1andt2, and then\nevaluate the joint expectation E[X(t1)X(t2)].\nThe above example shows a periodic autocorrelation function. The fact that it is peri-\nodic is coincidental because the random process X(t) is a periodic function. In general, an\narbitrary random process can have an arbitrary autocorrelation function that is not periodic.\nThere are, of course, various properties of the autocorrelation functions and special types\nof autocorrelation functions. We will study one of them, called the wide-sense stationary\nprocesses , later.\nExample 10.12 . Let \u0398 \u223cUniform[ \u2212\u03c0, \u03c0],X(t) = cos( \u03c9t+ \u0398). Draw the autocorrela-\ntion function RX(t1, t2).\nSolution . From the previous example we know that\nRX(t1, t2) =1\n2cos\u0010\n\u03c9(t1\u2212t2)\u0011\n.\nFigure 10.13 shows the realizations, and the mean and autocorrelation functions.\n626", "642": "10.2. MEAN AND CORRELATION FUNCTIONS\nNote that the autocorrelation function has a structure: Every row is a shifted\nversion of the previous row. We call this a Toeplitz structure. An autocorrelation with\na Toeplitz structure is specified once we know any of the rows. A Toeplitz structure also\nimplies that the autocorrelation function does not depend on the pair ( t1, t2) but only\non the difference t1\u2212t2. In other words, RX(0,1) is the same as RX(11.6,12.6), and so\nknowing RX(0,1) is enough to know all RX(t0, t0+t). Not all random processes have\na Toeplitz autocorrelation function. Random processes with a Toeplitz autocorrelation\nfunction are \u201cnice\u201d processes that we will study in detail later.\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1-0.500.51\n-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1\nt1-1\n-0.75\n-0.5\n-0.25\n0\n0.25\n0.5\n0.75\n1t2\nFigure 10.13: The autocorrelation function RX(t1, t2) =1\n2cos\u0010\n\u03c9(t1\u2212t2)\u0011\n.\nThe MATLAB code used to generate Figure 10.13 is shown below.\n% MATLAB code for Example 10.12\nt = linspace(-1,1,1000);\nR = Toeplitz(0.5*cos(2*pi*t(:)));\nimagesc(t,t,R);\ngrid on;\nxticks(-1:0.25:1);\nyticks(-1:0.25:1);\nPractice Exercise 10.1 . Let \u0398 \u223cUniform[0 ,2\u03c0],X(t) = cos( \u03c9t+ \u0398). Find the PDF\nofX(0).\nSolution . Let Z=X(0) = cos \u0398. Then the CDF of Zis\nFZ(z) =P[Z\u2264z]\n=P[cos \u0398 \u2264z]\n=P[cos\u22121z\u2264\u0398\u22642\u03c0\u2212cos\u22121z]\n= 1\u2212cos\u22121z\n\u03c0.\n627", "643": "CHAPTER 10. RANDOM PROCESSES\nThen by the fundamental theorem of calculus,\nfZ(z) =1\n\u03c0\u221a\n1\u2212z2.\nA similar concept to the autocorrelation function is the autocovariance function. The\nidea is to remove the mean before computing the correlation. This is analogous to the\ncovariance Cov( X, Y) =E[(X\u2212\u00b5X)(Y\u2212\u00b5Y)] as opposed to the correlation E[XY] in the\nrandom variable case.\nDefinition 10.3. Theautocovariance function of a random process X(t)is\nCX(t1, t2) =E[(X(t1)\u2212\u00b5X(t1)) (X(t2)\u2212\u00b5X(t2))]. (10.7)\nAs one might expect, the autocovariance function is closely related to the autocorrelation\nfunction.\nTheorem 10.1.\nCX(t1, t2) =RX(t1, t2)\u2212\u00b5X(t1)\u00b5X(t2). (10.8)\nProof . Plugging in the definition, we have that\nCX(t1, t2) =E[(X(t1)\u2212\u00b5X(t1)) (X(t2)\u2212\u00b5X(t2))]\n=E[X(t1)X(t2)\u2212X(t1)\u00b5X(t2)\u2212X(t2)\u00b5X(t1) +\u00b5X(t1)\u00b5X(t2)]\n=RX(t1, t2)\u2212\u00b5X(t1)\u00b5X(t2)\u2212\u00b5X(t1)\u00b5X(t2) +\u00b5X(t1)\u00b5X(t2)\n=RX(t1, t2)\u2212\u00b5X(t1)\u00b5X(t2).\n\u25a1\nPractice Exercise 10.2 . Suppose X(t) =Acos(2 \u03c0t) for A\u223cUniform[0 ,1]. Find\nCX(t1, t2).\nSolution .\nCX(t1, t2) =RX(t1, t2)\u2212\u00b5X(t1)\u00b5X(t2)\n=1\n3cos(2 \u03c0t1) cos(2 \u03c0t2)\u22121\n2cos(2 \u03c0t1)\u00b71\n2cos(2 \u03c0t2)\n=1\n12cos(2 \u03c0t1) cos(2 \u03c0t2).\nPractice Exercise 10.3 . Suppose X(t) = cos( \u03c9t+ \u0398) for \u0398 \u223cUniform[ \u2212\u03c0, \u03c0]. Find\nCX(t1, t2).\n628", "644": "10.2. MEAN AND CORRELATION FUNCTIONS\nSolution .\nCX(t1, t2) =RX(t1, t2)\u2212\u00b5X(t1)\u00b5X(t2)\n=1\n2cos\u0012\n\u03c9(t1\u2212t2)\u0013\n\u22120\u00b70\n=1\n2cos\u0012\n\u03c9(t1\u2212t2)\u0013\n.\nIn some problems we are interested in modeling the correlation between two random\nprocesses X(t) and Y(t). This gives us the cross-correlation and the cross-covariance func-\ntions.\nDefinition 10.4. Thecross-correlation function ofX(t)andY(t)is\nRX,Y(t1, t2) =E[X(t1)Y(t2)]. (10.9)\nDefinition 10.5. Thecross-covariance function ofX(t)andY(t)is\nCX,Y(t1, t2) =E[(X(t1)\u2212\u00b5X(t1)) (Y(t2)\u2212\u00b5Y(t2))]. (10.10)\nRemark . If\u00b5X(t1) =\u00b5Y(t2) = 0, then CX,Y(t1, t2) =RX,Y(t1, t2) =E[X(t1)Y(t2)].\n10.2.3 Independent processes\nHow do we establish independence for two random processes? We know that for two random\nvariables to be independent, the joint PDF can be written as a product of two PDFs:\nfX,Y(x, y) =fX(x)fY(y). (10.11)\nIf we extrapolate this idea to random processes, a natural formulation would be\nfX(t),Y(t)(x, y) =fX(t)(x)fY(t)(y). (10.12)\nBut this definition has a problem because X(t) and Y(t) are functions. It is not enough to\njust look at one time index, say t=t0. The way to think about this situation is to consider\na pair of random vectors XandY. When you say XandYare independent, you require\nfX,Y(x,y) =fX(x)fY(y). The PDF fX(x) itself is a joint distribution, i.e., fX(x) =\nfX1,...,X N(x1, . . . , x N). Therefore, for random processes, we need something similar.\nDefinition 10.6. Two random processes X(t)andY(t)areindependent if for any\nt1, . . . , t N,\nfX(t1),...,X (tN),Y(t1),...,Y (tN)(x1, . . . , x N, y1, . . . , y N)\n=fX(t1),...,X (tN)(x1, . . . , x N)\u00d7fY(t1),...,Y (tN)(y1, . . . , y N).\n629", "645": "CHAPTER 10. RANDOM PROCESSES\nThis definition is reminiscent of fX,Y(x,y) =fX(x)fY(y). The requirement here is that\nthe factorization holds for anyN, including very small Nand very large N, because X(t)\nandY(t) are infinitely long.\nIndependence means that the behavior of one process will not influence the behavior\nof the other process. We define uncorrelated as follows.\nDefinition 10.7. Two random processes are X(t)andY(t)uncorrelated if\nE[X(t1)Y(t2)] =E[X(t1)]E[Y(t2)], (10.13)\nIndependence implies uncorrelation, as we can see from the following. If X(t) and Y(t) are\nindependent, it follows that\nE[X(t1)Y(t2)] =Z\nX(t1, \u03be)Y(t2, \u03b6)fX,Y(\u03be, \u03b6)d\u03be d\u03b6\n=Z\nX(t1, \u03be)Y(t2, \u03b6)fX(\u03be)fY(\u03b6)d\u03be d\u03b6, independence\n=Z\nX(t1, \u03be)fX(\u03be)d\u03beZ\nY(t2, \u03b6)fY(\u03b6)d\u03b6=E[X(t1)]E[Y(t2)].\nIf two random processes are uncorrelated, they are not necessarily independent.\nIndependent X and Y\u21d2\n\u21cduncorrelated X and Y\nExample 10.13 . Let Y(t) =X(t) +N(t), where X(t) and N(t) are independent.\nThen\nRX,Y(t1, t2) =E[X(t1)Y(t2)] =E[X(t1) (X(t2) +N(t2))]\n=RX(t1, t2) +\u00b5X(t1)\u00b5N(t2).\n10.3 Wide-Sense Stationary Processes\nAs we have seen in the previous sections, some random processes have a \u201cnice\u201d autocor-\nrelation function, in the sense that the 2D function RX(t1, t2) has a Toeplitz structure.\nRandom processes with this property are known as wide-sense stationary (WSS) processes.\nWSS processes belong to a very small subset in the entire universe of random processes,\nbut they are practically the most useful ones. Before we discuss how to use them, we first\npresent a formal definition of a WSS process.1\n1Many textbooks introduce strictly stationary processes before discussing a wide-sense stationary process.\nWe skip the former because, throughout our book, we only use WSS processes. Readers interested in strictly\nstationary processes can consult the references listed at the end of this chapter.\n630", "646": "10.3. WIDE-SENSE STATIONARY PROCESSES\n10.3.1 Definition of a WSS process\nDefinition 10.8. A random process X(t)iswide-sense stationary if:\n1.\u00b5X(t) =constant ,for all t, and\n2.RX(t1, t2) =RX(t1\u2212t2)for all t1, t2.\nThere are two criteria that define a WSS process. The first criterion is that the mean is a\nconstant. That is, the mean function does not change with time. The second criterion is that\nthe autocorrelation function only depends on the difference t1\u2212t2and not on the absolute\nstarting point. For example, RX(0.1,1.1) needs to be the same as RX(6.3,7.3), because the\nintervals are both 1.\nHow can these two criteria be mapped to the Toeplitz structure we discussed in the\nprevious examples? Figure 10.14 shows the autocorrelation function RX(t1, t2), which is a\n2D function. We take three cross sections corresponding to t2=\u22120.13,t2= 0 and t2= 0.3.\nAs you can see from the figure, each RX(t1, t2) is a shifted version of another one. To obtain\nany value RX(t1, t2) on the function, there is no need to probe to the 2D map; you only\nneed to probe to the red curve and locate the position marked as t1\u2212t2, and you will be\nable to obtain the value RX(t1, t2).\n-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1\nt1-1\n-0.75\n-0.5\n-0.25\n0\n0.25\n0.5\n0.75\n1t2\n-1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1\nt1-1-0.500.51\nt2 = -0.13\nt2 = 0\nt2 = 0.3\nFigure 10.14: Cross sections of the autocorrelation function RX(t1, t2) =1\n2cos\u0010\n\u03c9(t1\u2212t2)\u0011\n.\nNot all random processes have a Toeplitz autocorrelation function. For example, the\nrandom process X(t) =Acos(2 \u03c0t) is not a WSS process, because the autocorrelation func-\ntion is\nRX(t1, t2) =1\n3cos(2 \u03c0t1) cos(2 \u03c0t2),\nwhich cannot be written as the difference t1\u2212t2.\nRemark 1 . WSS processes can also be defined using the autocovariance function instead\nof the autocorrelation function, because if a process is WSS, then the mean function is a\nconstant. If the mean function is a constant, then CX(t1, t2) =RX(t1, t2)\u2212\u00b52. So any\ngeometric structure that RXpossesses will be translated to CX, as the constant \u00b52will not\n631", "647": "CHAPTER 10. RANDOM PROCESSES\ninfluence the geometry. Therefore, it is equally valid to say that a WSS process has\nCX(t1, t2) =CX(t1\u2212t2).\nRemark 2 . Because a WSS is completely characterized by the difference t1\u2212t2, there is\nno need to keep track of the absolute indices t1andt2. We can rewrite the autocorrelation\nfunction as\nRX(\u03c4) =E[X(t+\u03c4)X(t)]. (10.14)\nThere is nothing new in this equation: It only says that instead of writing RX(t+\u03c4, t), we\ncan write RX(\u03c4) because the time index tplays no role in terms of RX. Thus from now on,\nfor any WSS processes we will write the autocorrelation function as RX(\u03c4).\n10.3.2 Properties of RX(\u03c4)\nWhen X(t) is WSS, RX(\u03c4) has several important properties.\nCorollary 10.1. RX(0) = average power of X(t).\nProof . Since\nRX(0) =E[X(t+ 0)X(t)] =E[X(t)2],\nand since E[X(t)2] is the average power, RX(0) is the average power of X(t). \u25a1\nCorollary 10.2. RX(\u03c4)is symmetric. That is, RX(\u03c4) =RX(\u2212\u03c4).\nProof . Note that RX(\u03c4) =E[X(t+\u03c4)X(t)]. By switching the order of multiplication in the\nexpectation, we have\nE[X(t+\u03c4)X(t)] =E[X(t)X(t+\u03c4)] =RX(\u2212\u03c4).\n\u25a1\nCorollary 10.3.\nP(|X(t+\u03c4)\u2212X(\u03c4)|> \u03f5)\u22642(RX(0)\u2212RX(\u03c4))\n\u03f52.\nThis result says that if RX(\u03c4) is slowly decaying from RX(0), the probability of having a\nlarge deviation |X(t+\u03c4)\u2212X(\u03c4)|is small.\nProof .\nP(|X(t+\u03c4)\u2212X(\u03c4)|> \u03f5)\u2264E[(X(t+\u03c4)\u2212X(\u03c4))2]/\u03f52\n=\u0010\nE[X(t+\u03c4)2]\u22122E[X(t+\u03c4)X(t)] +E[X(t)2]\u0011\n/\u03f52\n=\u0010\n2E[X(t)2]\u22122E[X(t+\u03c4)X(t)]\u0011\n/\u03f52\n= 2\u0010\nRX(0)\u2212RX(\u03c4)\u0011\n/\u03f52.\n\u25a1\n632", "648": "10.3. WIDE-SENSE STATIONARY PROCESSES\nCorollary 10.4. |RX(\u03c4)| \u2264RX(0), for all \u03c4.\nProof . By Cauchy\u2019s inequality E[XY]2\u2264E[X2]E[Y2], we can show that\nRX(\u03c4)2=E[X(t)X(t+\u03c4)]2\n\u2264E[X(t)2]E[X(t+\u03c4)2]\n=E[X(t)2]2=RX(0)2.\n\u25a1\n10.3.3 Physical interpretation of RX(\u03c4)\nHow should we understand the autocorrelation function RX(\u03c4) for WSS processes? Cer-\ntainly, by definition, RX(\u03c4) =E[X(t+\u03c4)X(t)] means that we can analyze RX(\u03c4) from the\nstatistical perspective. But in this section we want to take a slightly different approach by\nanswering the question from a computational perspective.\nConsider the following function:\nbRX(\u03c4)def=1\n2TZT\n\u2212TX(t+\u03c4)X(t)dt. (10.15)\nThis function is the temporal average ofX(t+\u03c4)X(t), as opposed to the statistical average.\nWhy do we want to consider this temporal average? We first show the main result, that\nE[bRX(\u03c4)] =RX(\u03c4).\nLemma 10.1. LetbRX(\u03c4)def=1\n2TRT\n\u2212TX(t+\u03c4)X(t)dt. Then\nEh\nbRX(\u03c4)i\n=RX(\u03c4). (10.16)\nProof .\nEh\nbRX(\u03c4)i\n=1\n2TZT\n\u2212TE[X(t+\u03c4)X(t)]dt\n=1\n2TZT\n\u2212TRX(\u03c4)dt=RX(\u03c4)1\n2TZT\n\u2212Tdt=RX(\u03c4).\n\u25a1\nThis lemma implies that if the signal X(t) is long enough, we can approximate RX(\u03c4)\nbybRX(\u03c4). The approximation is asymptotically consistent, in the sense that E[bRX(\u03c4)] =\nRX(\u03c4). Now, the more interesting question is the interpretation of bRX(\u03c4). What is it?\nHow should we understand bRX(\u03c4)?\nbRX(\u03c4) is the \u201cunflipped convolution\u201d, or correlation , ofX(\u03c4) and X(t+\u03c4).\n633", "649": "CHAPTER 10. RANDOM PROCESSES\nCorrelation is analogous to convolution. For convolution , the definition is\nY(\u03c4) =ZT\n\u2212TX(t\u2212\u03c4)X(t)dt, (10.17)\nwhereas for correlation , the definition is\nY(\u03c4) =ZT\n\u2212TX(t+\u03c4)X(t)dt. (10.18)\nClearly, bRX(\u03c4) is the latter. A graphical illustration of the difference between convolution\nand correlation is provided in Figure 10.15 . The only difference between the two is that the\ncorrelation does not flip the function, whereas the convolution does flip the function.\n-0.100.10.20.30.40.50.6\n-10 -5 0 5 10\n-0.100.10.20.30.40.50.6\n-10 -5 0 5 10\n(a) Convolution (b) Correlation\nFigure 10.15: The difference between convolution and correlation. In convolution, the function X(t)is\nflipped before we compute the result. For correlation, the function is not flipped.\nThe temporal correlation is easy to visualize. Starting with the function X(t+\u03c4), if you\nmake \u03c4larger or smaller, then effectively you are shifting X(t) left or right. The integrationRT\n\u2212TX(t+\u03c4)X(t)dtcalculates the energy accumulated. If the integral is large, there is a\nstrong correlation between X(t) and X(t+\u03c4). Otherwise the correlation is small. Here is\nan extreme example:\nExample 10.14 . Consider a random process X(t) such that for every t,X(t) is an\ni.i.d. Gaussian random variable with zero mean and unit variance. Then\nRX(\u03c4) =E[X(t+\u03c4)X(t)] =(\nE[X2(t)], \u03c4 = 0,\nE[X(t+\u03c4)]E[X(t)], \u03c4 \u0338= 0.\nUsing the fact that X(t) is i.i.d. Gaussian for all t, we can show that E[X2(t)] = 1 for\nanyt, andE[X(t+\u03c4)]E[X(t)] = 0. Therefore, we have\nRX(\u03c4) =(\n1, \u03c4 = 0,\n0. \u03c4 \u0338= 0.\n634", "650": "10.3. WIDE-SENSE STATIONARY PROCESSES\nThe equation says that since the random process is i.i.d. Gaussian, shifting and in-\ntegrating will give maximum correlation at the origin. As soon as the shift is not at\nthe origin, the correlation is zero. This makes sense because the samples are just i.i.d.\nGaussian. One pixel offset is enough to destroy any correlation.\nNow let\u2019s calculate the temporal correlation. We know that\nbRX(\u03c4) =1\n2TZT\n\u2212TX(t+\u03c4)X(t)d\u03c4.\nThis equation says that we shift X(t) to the left and right and then integrate. If \u03c4\nis not zero, the product X(t+\u03c4)X(t) will sometimes be positive and sometimes be\nnegative. After integrating the entire period, we cancel out most of the terms. Let\u2019s\nplot the functions and see if all these steps make sense. In Figure 10.16 (a), we show\ntwo random realizations of the random process X(t). They are just i.i.d. Gaussian\nsamples.\nInFigure 10.16 (b) we plot the temporal autocorrelation function bRX(\u03c4). Since\nbRX(\u03c4) itself is a random process, it has different realizations. We plot two random\nrealizations, which are computed based on shifting and integrating X(t). In the same\nplot, we also show the statistical expectation RX(\u03c4). As we can see from the plot,\nthe temporal correlation and the statistical correlation match reasonably well except\nfor the fluctuation in bRX(\u03c4), which is expected because it is computed from a finite\nnumber of samples.\n0 200 400 600 800 1000-4-3-2-101234\n0 500 1000 1500 2000-0.200.20.40.60.811.2\ncorrelation of sample 1\ncorrelation of sample 2\nauto-correlation function\n(a)X(t) (b) bRX(\u03c4)\nFigure 10.16: (a) A random process X(t)with two different realizations. (b) As we calculate the\ntemporal correlation of each of the two realizations, we obtain a noisy function that is nearly an\nimpulse. If we take the average of many of these realizations, we obtain a pure delta function.\nOn a computer, the commands to do the autocorrelation function are xcorr in MAT-\nLAB and np.correlate in Python. Below are the codes used to generate Figure 10.16 .\n% MATLAB code to demonstrate autocorrelation\nN = 1000; % number of sample paths\nT = 1000; % number of time stamps\nX = 1*randn(N,T);\nxc = zeros(N,2*T-1);\nfor i=1:N\n635", "651": "CHAPTER 10. RANDOM PROCESSES\nxc(i,:) = xcorr(X(i,:))/T;\nend\nplot(xc(1,:),\u2019b:\u2019, \u2019LineWidth\u2019, 2); hold on;\nplot(xc(2,:),\u2019k:\u2019, \u2019LineWidth\u2019, 2);\n# Python code to demonstrate autocorrelation\nN = 1000\nT = 1000\nX = np.random.randn(N,T)\nxc= np.zeros((N,2*T-1))\nfor i in range(N):\nxc[i,:] = np.correlate(X[i,:],X[i,:],mode=\u2019full\u2019)/T\nplt.plot(xc[0,:],\u2019b:\u2019)\nplt.plot(xc[1,:],\u2019k:\u2019)\nplt.show()\nUnder what conditions will bRX(\u03c4)\u2192RX(\u03c4) asT\u2192 \u221e ? The answer to this question\nis provided by an important theorem called Mean-Square Ergodic Theorem , which can be\nthought of as the random process version of the weak law of large numbers. We leave the\ndiscussion of the mean ergodic theorem to the Appendix.\nEverything you need to know about a WSS process\n\u0088The mean of a WSS process is a constant (does not need to be zero)\n\u0088The correlation function only depends on the difference, so RX(t1, t2) is Toeplitz.\n\u0088You can write RX(t1, t2) asRX(\u03c4), where \u03c4=t1\u2212t2.\n\u0088RX(\u03c4) tells you how much correlation you have with someone located at a time\ninstant \u03c4from you.\n10.4 Power Spectral Density\nBeginning with this section we are going to focus on WSS processes. By WSS, we mean that\nthe autocorrelation function RX(t1, t2) has a Toeplitz structure. Putting it in other words,\nwe assume RX(t1, t2) can be simplified to RX(\u03c4), where \u03c4=t1\u2212t2. We call this property\ntime invariance .\n10.4.1 Basic concepts\nAssuming that RX(\u03c4) is square integrable, i.e.,R\u221e\n\u2212\u221eRX(\u03c4)2d\u03c4 <\u221e, we can now define the\nFourier transform of RX(\u03c4) which is called the power spectral density .\n636", "652": "10.4. POWER SPECTRAL DENSITY\nTheorem 10.2 (Einstein-Wiener-Khinchin Theorem ).The power spectral density\nSX(\u03c9)of a WSS process is\nSX(\u03c9) =Z\u221e\n\u2212\u221eRX(\u03c4)e\u2212j\u03c9\u03c4d\u03c4=F(RX(\u03c4)),\nassuming thatR\u221e\n\u2212\u221eRX(\u03c4)2d\u03c4 <\u221eso that the Fourier transform of RX(\u03c4)exists.\nPractice Exercise 10.4 . Let RX(\u03c4) =e\u22122\u03b1|\u03c4|. Find SX(\u03c9).\nSolution . Using the Fourier transform table,\nSX(\u03c9) =F {RX(\u03c4)}=4\u03b1\n4\u03b12+\u03c92.\nFigure 10.17 shows the autocorrelation function and the power spectral density.\n-2 -1 0 1 200.20.40.60.81\nRX()\n-10 -5 0 5 1000.20.40.60.81\nSX()\nFigure 10.17: Example for RX(\u03c4) =e\u22122\u03b1|\u03c4|, with \u03b1= 1.\nWhy is Theorem 10.2 a theorem rather than a definition ? This is because power spectral\ndensity has its definition. There is no way that you can get any \u201cpower\u201d information merely\nby looking at the Fourier transform of RX(\u03c4). We will discuss the origin of the power spectral\ndensity later, but for now, we only need to know that SX(\u03c9) is the Fourier transform of\nRX(\u03c4).\nRemark . The power spectral density is defined for WSS processes. If the process is not\nWSS, then RXwill be a 2D function instead of a 1D function of \u03c4, so we cannot take the\nFourier transform in \u03c4. We will discuss this in detail shortly.\nPractice Exercise 10.5 . Let X(t) =acos(\u03c90t+\u0398),\u0398\u223cUniform[0 ,2\u03c0]. Find SX(\u03c9).\nSolution . We know that the autocorrelation function is\nRX(\u03c4) =a2\n2cos(\u03c90\u03c4)\n=a2\n2\u0012ej\u03c90\u03c4+e\u2212j\u03c90\u03c4\n2\u0013\n.\n637", "653": "CHAPTER 10. RANDOM PROCESSES\nBy taking the Fourier transform of both sides, we have\nSX(\u03c9) =a2\n2\u00142\u03c0\u03b4(\u03c9\u2212\u03c90) + 2\u03c0\u03b4(\u03c9+\u03c90)\n2\u0015\n=\u03c0a2\n2[\u03b4(\u03c9\u2212\u03c90) +\u03b4(\u03c9+\u03c90)].\nThe result is shown in Figure 10.18 .\n-2 -1 0 1 2-0.500.5\nRX()\n00.511.52\n-10 -5 0 5 10SX()\nFigure 10.18: Example for RX(\u03c4) =a2\n2cos(\u03c90\u03c4), with a= 1and\u03c90= 2\u03c0.\nPractice Exercise 10.6 . Let SX(\u03c9) =N0\n2rect(\u03c9\n2W). Find RX(\u03c4).\nSolution . Since SX(\u03c9) =F(RX(\u03c4)), the inverse holds:\nRX(\u03c4) =N0\n2W\n\u03c0sinc(W\u03c4).\nThis example shows what we call the bandlimited white noise . The power spectral\ndensity SX(\u03c9) is uniform, meaning that it covers all frequencies (or wavelengths in\noptics). It is called \u201cwhite noise\u201d because white light is essentially a mixture of all\nwavelengths.\nThe bandwidth of the power spectral density Wdefines the zero crossings of\nRX(\u03c4). It is easy to show that when W\u2192 \u221e ,RX(\u03c4) converges to a delta function.\nThis happens when X(t) is i.i.d. Gaussian. Therefore, the pure Gaussian noise random\nprocess is also known as the white noise process . Reshaping the i.i.d. Gaussian noise\nto an arbitrary power spectral density can be done by passing it through a linear filter,\nas we will explain later.\n-2 -1 0 1 2-0.500.511.52\nRX()\n-10 -5 0 5 10-0.500.511.5\nSX()\nFigure 10.19: Example for SX(\u03c9) =N0\n2rect(\u03c9\n2W), with N0= 2andW= 5.\n638", "654": "10.4. POWER SPECTRAL DENSITY\nFinding SX(\u03c9) from RX(\u03c4) is straightforward, at least in principle. The more inter-\nesting questions to ask are: (1) Why do we need to learn about power spectral density? (2)\nWhy do we need WSS to define power spectral density?\nHow is power spectral density useful?\n\u0088Power spectral densities are useful when we pass a random process through some\nlinear operations, e.g., convolution, running average, or running difference.\n\u0088Power spectral densities are the Fourier transforms of the autocorrelation func-\ntions. Fourier transforms are useful for speeding up computation and drawing\nrandom samples from a given power spectral density.\nA random process itself is not interesting until we process it; there are many ways to do\nthis. The most basic operation is to send the random process through a linear time-invariant\nsystem, e.g., a convolution. Convolution is equivalent to filtering the random process. For\nexample, if the input process contains noise, we can design a linear time-invariant filter to\ndenoise the random process. The power spectral density, which is the Fourier transform\nof the autocorrelation function, makes the filtering easier because everything can be done\nin the spectral (Fourier) domain. Moreover, we can analyze the performance and quantify\nthe limit using standard results in Fourier analysis. For some specialized problems such as\nimaging through atmospheric turbulence, the distortions happen in the phase domain. This\ncan be simulated by drawing samples from the power spectral density, e.g., the Kolmogorov\nspectrum or the von K\u00b4 arm\u00b4 an spectrum. Power spectral densities have many important\nengineering applications.\nWhy does the power spectral density require wide-sense stationarity?\n\u0088If a process is WSS, then RXwill have a Toeplitz structure.\n\u0088A Toeplitz matrix is important. If you do eigendecomposition to a Toeplitz ma-\ntrix, the eigenvectors are the Fourier bases.\n\u0088So if RXis Toeplitz, then you can diagonalize it using the Fourier transform.\n\u0088Therefore, the power spectral density can be defined.\nWhy does power spectral density require WSS? This has to do with the Toeplitz\nstructure of the autocorrelation function. To make our discussion easier let us discretize\nthe autocorrelation function RX(t1, t2) by considering RX[m, n]. (You can do a mental\ncalculation by converting t1to integer indices m, and t2ton. See any textbook on signals\nand systems if you need help. This is called the \u201cdiscrete time signal\u201d.) Following the range\noft1andt2,RX[m, n] can be expressed as:\nR=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0RX[0] RX[1] \u00b7\u00b7\u00b7RX[N\u22121]\nRX[1] RX[0] \u00b7\u00b7\u00b7RX[N\u22122]\n............\nRX[N\u22121]RX[N\u22121]\u00b7\u00b7\u00b7 RX[0]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nwhere we used the fact that RX[m, n] =RX[m\u2212n] for WSS processes and RX[k] =RX[\u2212k].\nWe call the resulting matrix Rtheautocorrelation matrix , which is a discretized version\n639", "655": "CHAPTER 10. RANDOM PROCESSES\nof the autocorrelation function RX(t1, t2). Looking at R, we again observe the Toeplitz\nstructure. For example, Figure 10.20 shows one Toeplitz structure and one non-Toeplitz\nstructure.\nFigure 10.20: We show two autocorrelation functions RX[m, n]on the left-hand side. The first autocor-\nrelation function comes from a WSS process that has a Toeplitz structure. The second autocorrelation\nfunction does not have Toeplitz structure. For the Toeplitz matrix, we can diagonalize it using the\nFourier transform. The eigenvalues are the power spectral density.\nAny Toeplitz matrix Rcan be diagonalized using the Fourier transforms. That is, we\ncan write Ras\nR=FH\u039bF,\nwhere Fis the (discrete) Fourier transform matrix and\u039bis a diagonal matrix. This can be\nunderstood as the eigendecomposition of R. The important point here is that only Toeplitz\nmatrices can be eigendecomposed using the Fourier transforms; an arbitrary symmetric\nmatrix cannot. Figure 10.20 illustrates this point. If your matrix is Toeplitz, you can diago-\nnalize it, and hence you can define the power spectral density, just as in the first example. If\nyour matrix is not Toeplitz, then the power spectral density is undefined. To get the Toeplitz\nmatrix, you must start with a WSS process.\nBefore moving on, we define cross power spectral densities , which will be useful in\nsome applications.\nDefinition 10.9. Thecross power spectral density between two random processes\nX(t)andY(t)is\nSX,Y(\u03c9) =F(RX,Y(\u03c4))where RX,Y(\u03c4) =E[X(t+\u03c4)Y(t)],\nSY,X(\u03c9) =F(RY,X(\u03c4))where RY,X(\u03c4) =E[Y(t+\u03c4)X(t)].(10.19)\nRemark . In general, SX,Y(\u03c9)\u0338=SY,X(\u03c9). Rather, since RX,Y(\u03c4) =RY,X(\u2212\u03c4), we have\nSX,Y(\u03c9) =SY,X(\u03c9).\n10.4.2 Origin of the power spectral density\nTo understand the power spectral density, it is crucial to understand where it comes from\nand why it is the Fourier transform of the autocorrelation function.\n640", "656": "10.4. POWER SPECTRAL DENSITY\nWe begin by assuming that X(t) is a WSS random process with mean \u00b5Xand auto-\ncorrelation RX(\u03c4). We now consider the notion of power . Consider a random process X(t).\nThe power within a period [ \u2212T, T] is\nbPX=1\n2TZT\n\u2212T|X(t)|2dt.\nbPXdefines the power because the integration alone is the energy, and the normalization by\n1/2Tgives us the power. However, there are two problems. First, since X(t) is random, the\npower bPXis also random. Is there a way we can eliminate the randomness? Second, Tis\na finite period of time. It does not capture the entire process, and so we do not know the\npower of the entire process.\nA natural solution to these two problems is to consider\nPXdef=E\"\nlim\nT\u2192\u221e1\n2TZT\n\u2212T|X(t)|2dt#\n. (10.20)\nHere, we take the limit of Tto infinity so that we can compute the power of the entire\nprocess. We also take the expectation to eliminate the randomness. Therefore, PXcan be\nregarded as the average power of the complete random process X(t).\nNext, we need one definition and one lemma. The definition defines SX(\u03c9), and the\nlemma will link SX(\u03c9) with the power PX.\nDefinition 10.10. Thepower spectral density (PSD) of a WSS process is defined as\nSX(\u03c9) = lim\nT\u2192\u221eEh\n|eXT(\u03c9)|2i\n2T, (10.21)\nwhere\neXT(\u03c9) =ZT\n\u2212TX(t)e\u2212j\u03c9tdt (10.22)\nis the Fourier transform of X(t)limited to [\u2212T, T].\nThis definition is abstract, but in a nutshell, it simply considers everything in the Fourier\ndomain. The ratio |eXT(\u03c9)|2/2Tis the power, but in the frequency domain. The reason is\nthat if X(t) is Fourier transformable, then Parseval\u2019s theorem will hold. Parseval\u2019s theorem\nstates that energy in the original space is conserved in the Fourier space. Since the ratio\n|eXT(\u03c9)|2/2Tis the energy divided by time, it is the power. However, this is still not enough\nto help us understand power spectral density: We need a lemma.\nLemma 10.2. Define\nPXdef=E\"\nlim\nT\u2192\u221e1\n2TZT\n\u2212T|X(t)|2dt#\n.\n641", "657": "CHAPTER 10. RANDOM PROCESSES\nThen\nPX=1\n2\u03c0Z\u221e\n\u2212\u221eSX(\u03c9)d\u03c9. (10.23)\nThe lemma has to be read together with the previous definition. If we can prove the lemma,\nwe know that by integrating SX(\u03c9) we will obtain the power. Therefore, SX(\u03c9) can be\nviewed as a density function , specifically the density function of the power. SX(\u03c9) is called\nthe power spectral density because everything is defined in the Fourier domain. Putting this\nall together gives us \u201cpower spectral density\u201d.\nProof . First, we recall that PXis the expectation of the average power of X(t). Let\nXT(t) =\u001aX(t)\u2212T\u2264t\u2264T,\n0 otherwise .\nIt follows that integrating over \u2212\u221eto\u221eis equivalent to\nZ\u221e\n\u2212\u221e|XT(t)|2dt=ZT\n\u2212T|X(t)|2dt.\nBy Parseval\u2019s theorem, energy is conserved in both the time and the frequency domain:\nZ\u221e\n\u2212\u221e|XT(t)|2dt=1\n2\u03c0Z\u221e\n\u2212\u221e|eXT(\u03c9)|2d\u03c9.\nTherefore, PXsatisfies\nPX=E\"\nlim\nT\u2192\u221e1\n2TZT\n\u2212T|X(t)|2dt#\n=E\u0014\nlim\nT\u2192\u221e1\n2\u03c01\n2TZ\u221e\n\u2212\u221e|eXT(\u03c9)|2d\u03c9\u0015\n=1\n2\u03c0Z\u221e\n\u2212\u221elim\nT\u2192\u221e1\n2TEh\n|eXT(\u03c9)|2i\n| {z }\ndef=SX(\u03c9)d\u03c9.\n\u25a1\nThe power spectral densities are functions whose integrations give us the power. If we\nwant to determine the power of a random process, the Einstein-Wiener-Khinchin theorem\n(Theorem 10.2) says that SX(\u03c9) is just the Fourier transform of RX(\u03c4):\nSX(\u03c9) =Z\u221e\n\u2212\u221eRX(\u03c4)e\u2212j\u03c9\u03c4d\u03c4=F(RX(\u03c4)).\nThe proof of the Einstein-Wiener-Khinchin theorem is quite intricate, so we defer\nthe proof to the Appendix. The significance of the theorem is that it turns an abstract\nquantity, the power spectral density, into a very easily computable quantity, namely the\nFourier transform of the autocorrelation function. For now, we will happily use this theorem\nbecause it saves us a great deal of trouble when we want to determine the power spectral\ndensity from the first principles.\n642", "658": "10.5. WSS PROCESS THROUGH LTI SYSTEMS\n10.5 WSS Process through LTI Systems\nRandom processes have limited usefulness until we can apply operations to them. In this\nsection we discuss how WSS processes respond to a linear time-invariant (LTI) system. This\ntechnique is most useful in signal processing, communication, speech analysis, and imaging.\nWe will be brief here since you can find most of this information in any standard textbook\non signals and systems.\n10.5.1 Review of linear time-invariant systems\nWhen we say a \u201csystem\u201d, we mean that there exists an input-output relationship as shown\ninFigure 10.21 .\nFigure 10.21: A system can be viewed as a black box that takes an input X(t)and turns it into an\noutput Y(t).\nLinear time-invariant (LTI) systems are the simplest systems we use in engineering\nproblems. An LTI system has two properties.\n\u0088Linearity . Linearity means that when two input random processes are added and\nscaled , the output random processes will also be added and scaled in exactly the\nsame way. Mathematically, linearity says that if X1(t)\u2192Y1(t) and X2(t)\u2192\nY2(t), then\naX1(t) +bX2(t)\u2192aY1(t) +bY2(t).\n\u0088Time-invariant : Time invariance means that if we shift the input random process\nby a certain time period, the output will be shifted in the same way. Mathemat-\nically, time invariance means that if X(t)\u2192Y(t), then\nX(t+\u03c4)\u2192Y(t+\u03c4).\nIf a system is linear time-invariant, the input-to-output relation is given by convolution :\nTheconvolution between two functions X(t) and h(t) is defined as\nY(t) =h(t)\u2217X(t) =Z\u221e\n\u2212\u221eh(\u03c4)X(t\u2212\u03c4)d\u03c4,\n643", "659": "CHAPTER 10. RANDOM PROCESSES\nin which we call h(t) the system response or impulse response .\nThe function h(t) is called the impulse response because if X(t) =\u03b4(t), then according to\nthe convolution equation we have\nY(t) =Z\u221e\n\u2212\u221eh(\u03c4)\u03b4(t\u2212\u03c4)d\u03c4=h(t).\nTherefore, if we send an impulse to the system, the output will be h(t).\nConvolution is commutative, meaning that h(t)\u2217X(t) =X(t)\u2217h(t). Written as inte-\ngrations, we have\nZ\u221e\n\u2212\u221eh(\u03c4)X(t\u2212\u03c4)d\u03c4=Z\u221e\n\u2212\u221eh(t\u2212\u03c4)X(\u03c4)d\u03c4. (10.24)\nFor LTI systems, Y(t) can be determined through the Fourier transforms.\nTheFourier transform of a (squared-integrable) function X(t) is\nX(\u03c9) =F{X(t)}=Z\u221e\n\u2212\u221eX(\u03c4)e\u2212j\u03c9\u03c4d\u03c4. (10.25)\nA basic property of convolution is that convolution in the time domain is equivalent to\nmultiplication in the Fourier domain . Therefore\nY(\u03c9) =H(\u03c9)X(\u03c9), (10.26)\nwhere H(\u03c9) =F{h(t)}is the Fourier transform of h(t), and Y(\u03c9) =F(Y(t)) is the Fourier\ntransform of Y(t).\nIn the rest of this section we study the pair of input and output random processes that\nare defined as follows\n\u0088X(t) = input. It is a WSS random process.\n\u0088Y(t) = output. It is constructed by sending X(t) through an LTI system with\nimpulse response h(t). Therefore, Y(t) =h(t)\u2217X(t).\n10.5.2 Mean and autocorrelation through LTI Systems\nSince X(t) is WSS, the mean function of X(t) stays constant, i.e., \u00b5X(t) =\u00b5X. The following\ntheorem gives the mean function of the output.\nTheorem 10.3. IfX(t)passes through an LTI system to yield Y(t), the mean function\nofY(t)is\nE[Y(t)] =\u00b5XZ\u221e\n\u2212\u221eh(\u03c4)d\u03c4. (10.27)\n644", "660": "10.5. WSS PROCESS THROUGH LTI SYSTEMS\nProof . Suppose that Y(t) =h(t)\u2217X(t). Then,\n\u00b5Y(t) =E[Y(t)] =E\u0014Z\u221e\n\u2212\u221eh(\u03c4)X(t\u2212\u03c4)d\u03c4\u0015\n=Z\u221e\n\u2212\u221eh(\u03c4)E[X(t\u2212\u03c4)]d\u03c4\n=Z\u221e\n\u2212\u221eh(\u03c4)\u00b5Xd\u03c4=\u00b5XZ\u221e\n\u2212\u221eh(\u03c4)d\u03c4,\nwhere the second to last equality is valid because X(t) is WSS, so that E[X(t\u2212\u03c4)] =\u00b5X.\n\u25a1\nThe theorem suggests that if the input X(t) has a constant mean, the output Y(t)\nshould also have a constant mean. This should not be a surprise because if the system is\nlinear, a constant input will give a constant output.\nExample 10.15 . Consider a WSS random process X(t) such that each sample is an\ni.i.d. Gaussian random variable with zero mean and unit variance. We send this process\nthrough an LTI system with impulse response h(t), where\nh(t) =(\n10(1\u2212 |t|),\u22121\u2264t\u22641,\n0, otherwise .\nThe mean function of X(t) is\u00b5X(t) = 0, and that of Y(t) is\u00b5Y(t) = 0. Figure 10.22\nillustrates a numerical example, in which we see that the random processes X(t) and\nY(t) have different shapes but the mean functions remain constant.\n-10 -5 0 5 10-4-2024\nX(t)\nX(t)\nY(t)\nY(t)\n-2 -1 0 1 2-0.0500.050.10.150.2\nRX(t)\nRY(t)\n(a)\u00b5X(t) and \u00b5Y(t) (b) RX(t) and RY(t)\nFigure 10.22: When sending a WSS random process through an LTI system, the mean and the\nautocorrelation functions are changed.\nNext, we derive the autocorrelation function of a random process when sent through\nan LTI system.\nTheorem 10.4. IfX(t)passes through an LTI system to yield Y(t), the autocorre-\n645", "661": "CHAPTER 10. RANDOM PROCESSES\nlation function ofY(t)is\nRY(\u03c4) =Z\u221e\n\u2212\u221eZ\u221e\n\u2212\u221eh(s)h(r)RX(\u03c4+s\u2212r)ds dr. (10.28)\nProof . We start with the definition of Y(t):\nRY(\u03c4) =E[Y(t)Y(t+\u03c4)]\n=E\u0014Z\u221e\n\u2212\u221eh(s)X(t\u2212s)dsZ\u221e\n\u2212\u221eh(r)X(t+\u03c4\u2212r)dr\u0015\n(a)=Z\u221e\n\u2212\u221eZ\u221e\n\u2212\u221eh(s)h(r)E[X(t\u2212s)X(t+\u03c4\u2212r)ds dr ]\n=Z\u221e\n\u2212\u221eZ\u221e\n\u2212\u221eh(s)h(r)RX(\u03c4+s\u2212r)ds dr,\nwhere in (a) we assume that integration and expectation are interchangeable.\n\u25a1\nA shorthand notation of the above formula is RY(t) = [h\u229b(h\u2217RX)](t), where \u2217denotes\nthe convolution and \u229bdenotes the correlation. Figure 10.22 (b) shows the autocorrelation\nfunctions RXandRY. In this example RXis a delta function because for i.i.d. Gaussian\nnoise the power spectral density is a constant. After convolving with the system response,\nthe autocorrelation RYhas a different shape.\n10.5.3 Power spectral density through LTI systems\nDenoting the Fourier transform of the impulse response by H(\u03c9) =F(h(t)), we derive the\npower spectral density of the output.\nTheorem 10.5. IfX(t)passes through an LTI system to yield Y(t), the power spec-\ntral density ofY(t)is\nSY(\u03c9) =|H(\u03c9)|2SX(\u03c9). (10.29)\nProof . By definition, the power spectral density SY(\u03c9) is the Fourier transform of the\nautocorrelation function RY(\u03c9). Therefore,\nSY(\u03c9) =Z\u221e\n\u2212\u221eRY(\u03c4)e\u2212j\u03c9\u03c4d\u03c4\n=Z\u221e\n\u2212\u221eZ\u221e\n\u2212\u221eZ\u221e\n\u2212\u221eh(s)h(r)RX(\u03c4+s\u2212r)ds dre\u2212j\u03c9\u03c4d\u03c4.\nLetting u=\u03c4+s\u2212r, we have\nSY(\u03c9) =Z\u221e\n\u2212\u221eZ\u221e\n\u2212\u221eZ\u221e\n\u2212\u221eh(s)h(r)RX(u)e\u2212j\u03c9(u\u2212s+r)ds dr du\n=Z\u221e\n\u2212\u221eh(s)ej\u03c9sdsZ\u221e\n\u2212\u221eh(r)e\u2212j\u03c9rdrZ\u221e\n\u2212\u221eRX(u)e\u2212j\u03c9udu\n=H(\u03c9)H(\u03c9)SX(\u03c9),\n646", "662": "10.5. WSS PROCESS THROUGH LTI SYSTEMS\nwhere H(\u03c9) is the complex conjugate of H(\u03c9).\n\u25a1\nIt is tempting to think that since Y(t) =h(t)\u2217X(t), the power spectral density should\nalso be SY(\u03c9) =H(\u03c9)X(\u03c9), but this is not true. The above result shows that we need an\nadditional complex conjugate H(\u03c9) because SY(\u03c9) is the power , which means the square\nof the signal. Note that RXis \u201csquared\u201d because we have convolved it with itself, and RY\nis also squared. Therefore, to match RXandRY, the impulse response halso needs to be\nsquared in the Fourier domain.\nExample 10.16 . A WSS process X(t) has a correlation function\nRX(\u03c4) = sinc( \u03c0\u03c4).\nSuppose that X(t) passes through an LTI system with input/output relationship\n2d2\ndt2Y(t) + 2d\ndtY(t) + 4Y(t) = 3d2\ndt2X(t)\u22123d\ndtX(t) + 6X(t).\nFind RY(\u03c4).\nSolution : The sinc function has a Fourier transform given by\nsinc(Wt)\u2190\u2192\nF\u03c0\nWrect\u0010\u03c9\n2W\u0011\n.\nTherefore, the autocorrelation function is\nRX(\u03c4) = sinc( \u03c0\u03c4)\u2190\u2192\nF\u03c0\n\u03c0rect\u0010\u03c9\n2\u03c0\u0011\n.\nBy taking the Fourier transform on both sides, we have\nSX(\u03c9) =(\n1,\u2212\u03c0\u2264\u03c9\u2264\u03c0,\n0, elsewhere .\nThe system response is found from the differential equation:\nH(\u03c9) =3(j\u03c9)2\u22123(j\u03c9) + 6\n2(j\u03c9)2+ 2(j\u03c9) + 4\n=3\u0002\n(2\u2212\u03c92)\u2212j\u03c9\u0003\n2 [(2\u2212\u03c92) +j\u03c9].\nTaking the magnitude square yields\n|H(\u03c9)|2=3\u0002\n(2\u2212\u03c92)\u2212j\u03c9\u0003\n2 [(2\u2212\u03c92) +j\u03c9]3\u0002\n(2\u2212\u03c92) +j\u03c9\u0003\n2 [(2\u2212\u03c92)\u2212j\u03c9]\n=9\n4(2\u2212\u03c92)2+\u03c92\n(2\u2212\u03c92)2+\u03c92=9\n4.\n647", "663": "CHAPTER 10. RANDOM PROCESSES\nTherefore, the output power spectral density is\nSY(\u03c9) =|H(\u03c9)|2SX(\u03c9) =9\n4SX(\u03c9).\nTaking the inverse Fourier transform, we have\nRY(\u03c4) =9\n4sinc(\u03c0\u03c4).\nExample 10.17 . A random process X(t) has zero mean and RX(t, s) = min( t, s).\nConsider a new process Y(t) =etX(e\u22122t).\n1. Is Y(t) WSS?\n2. Suppose Y(t) passes through a LTI system to yield an output Z(t) according to\nd\ndtZ(t) + 2Z(t) =d\ndtY(t) +Y(t).\nFind RZ(\u03c4).\nSolution :\n1. In order to verify whether Y(t) is WSS, we need to check the mean function and\nthe autocorrelation function. The mean function is\nE[Y(t)] =E\u0002\netX(e\u22122t)\u0003\n=etE\u0002\nX(e\u22122t)\u0003\n.\nSince X(t) has zero mean, E[X(t)] = 0 for all t. This implies that if u=e\u22122t,\nthenE[X(u)] = 0 because uis just another time instant. Thus E[X(e\u22122t)] = 0,\nand hence E[Y(t)] = 0.\nThe autocorrelation is\nE[Y(t+\u03c4)Y(t)] =Eh\net+\u03c4X(e\u22122(t+\u03c4))etX(e\u22122t)i\n=e2t+\u03c4Eh\nX(e\u22122(t+\u03c4))X(e\u22122t)i\n=e2t+\u03c4RX(e\u22122(t+\u03c4), e\u22122t).\nSubstituting RX(t, s) = min( t, s), we have that\ne2t+\u03c4RX(e\u22122(t+\u03c4), e\u22122t) =e2t+\u03c4min(e\u22122(t+\u03c4), e\u22122t)\n=e2t+\u03c4\u001a\ne\u22122(t+\u03c4), \u03c4\u22650\ne\u22122t, \u03c4 < 0\n=\u001ae\u2212\u03c4, \u03c4\u22650\ne\u03c4, \u03c4 < 0\n=e\u2212|\u03c4|.\n648", "664": "10.5. WSS PROCESS THROUGH LTI SYSTEMS\nSoRY(\u03c4) =e\u2212|\u03c4|. Since RY(\u03c4) is a function of \u03c4,Y(t) is WSS.\n2. The system response is given by\nH(\u03c9) =1 +j\u03c9\n2 +j\u03c9.\nThe magnitude is therefore\n|H(\u03c9)|2=1 +\u03c92\n4 +\u03c92.\nHence, the output autocorrelation function is\nRY(\u03c4) =e\u2212|\u03c4|\u2190\u2192SY(\u03c9) =2\n1 +\u03c92,\nand\nSZ(\u03c9) =|H(\u03c9)|2SY(\u03c9)\n=1 +\u03c92\n4 +\u03c922\n1 +\u03c92=2\n4 +\u03c92.\nTherefore\nRZ(\u03c4) =1\n2e\u22122|\u03c4|.\n10.5.4 Cross-correlation through LTI Systems\nThe above analyses are developed for the autocorrelation function. If we consider the cross-\ncorrelation between two random processes, say X(t) and Y(t), then the above results do not\nhold. In this section, we discuss the cross-correlation through LTI systems.\nTo begin with, we need to define WSS for a pair of random processes.\nDefinition 10.11. Two random processes X(t)andY(t)arejointly WSS if\n1.X(t)is WSS and Y(t)is WSS, and\n2.RX,Y(t1, t2) =E[X(t1)Y(t2)]is a function of t1\u2212t2.\nIfX(t) and Y(t) are jointly WSS, we write\nRX,Y(t1, t2) =RX,Y(\u03c4)def=E[X(t+\u03c4)Y(\u03c4)].\nThe definition of \u201cjointly WSS\u201d is necessary here because RX,Yis defined by XandY. Just\nknowing that X(t) and Y(t) are WSS does not allow one to say that RX,Y(t1, t2) can be\nwritten as the time difference.\nIf we flip the order of XandYto consider RY,X(\u03c4) and not RX,Y(\u03c4), then we need\nto flip the argument. The following lemma explains why.\n649", "665": "CHAPTER 10. RANDOM PROCESSES\nLemma 10.3. For any random processes X(t)andY(t), thecross-correlation RX,Y(\u03c4)\nis related to RY,X(\u03c4)as\nRX,Y(\u03c4) =RY,X(\u2212\u03c4). (10.30)\nProof . Recall the definition of RY,X(\u2212\u03c4) =E[Y(t\u2212\u03c4)X(t)]. This can be simplified as\nfollows:\nRY,X(\u2212\u03c4) =E[Y(t\u2212\u03c4)X(t)]\n=E[X(t)Y(t\u2212\u03c4)]\n=E[X(t\u2032+\u03c4)Y(t\u2032)]\n=RX,Y(\u03c4),\nwhere we substituted t\u2032=t\u2212\u03c4.\n\u25a1\nExample 10.18 . Let X(t) and N(t) be two independent WSS random processes with\nexpectations E[X(t)] =\u00b5XandE[N(t)] = 0, respectively. Let Y(t) =X(t) +N(t). We\nwant to show that X(t) and Y(t) are jointly WSS, and we want to find RX,Y(\u03c4).\nSolution . Before we show the joint WSS property of X(t) and Y(t), we first show\nthatY(t) is WSS:\nE[Y(t)] =E[X(t) +N(t)] =\u00b5X.\nRY(t1, t2) =E[(X(t1) +N(t1))(X(t2) +N(t2))]\n=E[(X(t1)X(t2)] +E[(N(t1)N(t2)]\n=RX(t1\u2212t2) +RN(t1\u2212t2).\nThus, Y(t) is WSS.\nTo show that X(t) and Y(t) are jointly WSS, we need to check the cross-\ncorrelation function:\nRX,Y(t1, t2) =E[X(t1)Y(t2)]\n=E[X(t1)(X(t2) +N(t2))]\n=E[X(t1)(X(t2)] +E[X(t1)N(t2)]\n=RX(t1, t2) +E[X(t1)]E[N(t2)]\n=RX(t1, t2).\nSince RX,Y(t1, t2) is a function of t1\u2212t2, and since X(t) and Y(t) are WSS, X(t) and\nY(t) must be jointly WSS.\nFinally, to find RX,Y(\u03c4), we substitute \u03c4=t1\u2212t2and obtain RX,Y(\u03c4) =RX(\u03c4).\nKnowing the definition of jointly WSS, we consider the cross-correlation between X(t)\nandY(t). Note that here we are asking about the cross-correlation between the input and\nthe output of the same LTI system, as illustrated in Figure 10.23 . The pair X(t) and\nY(t) =h(t)\u2217X(t) are special because Y(t) is the convolved version of X(t).\n650", "666": "10.5. WSS PROCESS THROUGH LTI SYSTEMS\nFigure 10.23: The source of the signals when defining RX(\u03c4),RX,Y(\u03c4),RY,X(\u03c4)andRY(\u03c4).\nTheorem 10.6. LetX(t)andY(t)be jointly WSS processes, and let Y(t) =h(t)\u2217\nX(t). Then the cross-correlation RY,X(\u03c4)is\nRY,X(\u03c4) =h(\u03c4)\u2217RX(\u03c4). (10.31)\nProof . Recalling the definition of cross-correlation, we have\nRY,X(\u03c4) =E[Y(t+\u03c4)X(t)]\n=E\u0014\nX(t)Z\u221e\n\u2212\u221eX(t+\u03c4\u2212r)h(r)dr\u0015\n=Z\u221e\n\u2212\u221eE[X(t)X(t+\u03c4\u2212r)]h(r)dr=Z\u221e\n\u2212\u221eRX(\u03c4\u2212r)h(r)dr,\nwhich is the convolution RY,X(\u03c4) =h(\u03c4)\u2217RX(\u03c4).\n\u25a1\nWe next define the cross power spectral density of two jointly WSS processes as the\nFourier transform of the cross-correlation function.\nDefinition 10.12. Thecross power spectral density of two jointly WSS processes\nX(t)andY(t)is defined as\nSX,Y(\u03c9) =F[RX,Y(\u03c4)],\nSY,X(\u03c9) =F[RY,X(\u03c4)].\nThe relationship between SX,YandSY,Xcan be seen from the following theorem.\nTheorem 10.7. For two jointly WSS random processes X(t)andY(t), the cross\npower spectral density satisfies the property that\nSX,Y(\u03c9) =SY,X(\u03c9), (10.32)\nwhere (\u00b7)denotes the complex conjugate.\n651", "667": "CHAPTER 10. RANDOM PROCESSES\nProof . Since SX,Y(\u03c9) =F[RX,Y(\u03c4)] by definition, it follows that\nF[RX,Y(\u03c4)] =Z\u221e\n\u2212\u221eRX,Y(\u03c4)e\u2212j\u03c9\u03c4d\u03c4\n=Z\u221e\n\u2212\u221eRY,X(\u2212\u03c4)e\u2212j\u03c9\u03c4d\u03c4=Z\u221e\n\u2212\u221eRX,Y(\u03c4\u2032)ej\u03c9\u03c4\u2032d\u03c4\u2032,\nwhich is exactly the conjugate SY,X(\u03c9).\n\u25a1\nWhen sending the random process through an LTI system, the cross-correlation power\nspectral density is given by the theorem below.\nTheorem 10.8. IfX(t)passes through an LTI system to yield Y(t), then the cross\npower spectral density is\nSY,X(\u03c9) =H(\u03c9)SX(\u03c9),\nSX,Y(\u03c9) =H(\u03c9)SX(\u03c9).\nProof . By taking the Fourier transform on RY,X(\u03c4) we have that SY,X(\u03c9) =H(\u03c9)SX(\u03c9).\nSince RX,Y(\u03c4) =RY,X(\u2212\u03c4), it holds that SX,Y(\u03c9) =H(\u03c9)SX(\u03c9).\n\u25a1\nExample 10.19 . Let X(t) be a WSS random process with\nRX(\u03c4) =e\u2212\u03c42/2, H (\u03c9) =e\u2212\u03c92/2.\nFind SX,Y(\u03c9),RX,Y(\u03c4),SY(\u03c9) and RY(\u03c4).\nSolution . First, by the Fourier transform table we know that\nSX(\u03c9) =\u221a\n2\u03c0e\u2212\u03c92/2.\nSince H(\u03c9) =e\u2212\u03c92/2, we have\nSX,Y(\u03c9) =H(\u03c9)SX(\u03c9)\n=\u221a\n2\u03c0e\u2212\u03c92.\nThe cross-correlation function is\nRX,Y(\u03c9) =F\u22121h\u221a\n2\u03c0e\u2212\u03c92i\n=1\u221a\n2e\u2212\u03c42\n4.\n652", "668": "10.6. OPTIMAL LINEAR FILTER\nThe power spectral density of Y(t) is\nSY(\u03c9) =|H(\u03c9)|2SX(\u03c9)\n=\u221a\n2\u03c0e\u22123\u03c92\n2.\nTherefore, the autocorrelation function of Y(t) is\nRY(\u03c4) =F\u22121h\u221a\n2\u03c0e\u22123\u03c92\n2i\n=1\u221a\n3e\u2212\u03c42/6.\n10.6 Optimal Linear Filter\nIn the previous sections, we have built many tools to analyze random processes. Our next\ngoal is to apply these techniques. To that end, we will discuss optimal linear filter design ,\nwhich is a set of estimation techniques for predicting and recovering information from a time\nseries.\n10.6.1 Discrete-time random processes\nWe begin by introducing some notations. In the previous sections, we have been using\ncontinuous-time random processes to study statistics. In this section, we mainly focus on\ndiscrete-time random processes. The shift from continuous-time to discrete-time is straight-\nforward as far as the theories are concerned \u2014 we switch the continuous-time index tto\na discrete-time index n. However, shifting to discrete-time random processes can simplify\nmany difficult problems because many discrete-time problems can be solved by matrices and\nvectors. This will make the computations and implementations much easier. To make this\ntransition easier, we provide a few definitions and results without proof.\nNotations for discrete-time random processes\n\u0088We denote the discrete-time indices by mandn, corresponding to the continuous-\ntime indices t1andt2, respectively.\n\u0088A discrete-time random process is denoted by X[n].\n\u0088Its mean function and the autocorrelation function are\n\u00b5X[n] =E[X[n]],\nRX[m, n] =E[X[m]X[n]].\n\u0088We say that X[n] is WSS if \u00b5X[n] = constant, and RX[m, n] is a function of\nm\u2212n.\n653", "669": "CHAPTER 10. RANDOM PROCESSES\n\u0088IfX[n] is WSS, we write RX[m, n] as\nRX[m, n] =RX[m\u2212n] =RX[k],\nwhere k=m\u2212nis the interval.\n\u0088IfX[n] is WSS, we define the power spectral density as\nSX(ej\u03c9) =F{RX[k]},\nwhere SX(ej\u03c9) denotes the discrete-time Fourier transform.\nWhen a random process X[n] is sent through an LTI system with an impulse response\nh[n], the output is\nY[n] =h[n]\u2217X[n] =\u221eX\nk=\u2212\u221eh[k]X[n\u2212k]. (10.33)\nWhen a WSS process X[n] passes through an LTI system h[n] to yield an output Y[n],\nthe auto- and cross-correlation function and power spectral densities are\n\u0088RY[k] =E[Y[n+k]Y[n]],SY(ej\u03c9) =F{RY[k]}=|H(ej\u03c9)|2SX(ej\u03c9).\n\u0088RXY[k] =E[X[n+k]Y[n]],SXY(ej\u03c9) =F{RXY[k]}=H(ej\u03c9)SX(ej\u03c9).\n\u0088RY X[k] =E[Y[n+k]X[n]],SY X(ej\u03c9) =F{RY X[k]}=H(ej\u03c9)SX(ej\u03c9).\n10.6.2 Problem formulation\nThe problem we study here is known as the optimal linear filter design . Suppose that there\nis a WSS process X[n] that we want to process. For example, if X[n] is a corrupted version\nof some clean time-series, we may want to remove the noise by filtering (also known as\naveraging) X[n]. Conceptualizing the denoising process as a linear time-invariant system\nwith an impulse response h[n], our goal is to determine the optimal h[n] such that the\nestimated time series bY[n] is as close to the true time series Y[n] as possible.\nReferring to Figure 10.24 , we refer to X[n] as the input function and to Y[n] as the\ntarget function .X[n] and Y[n] are related according to the equation\nY[n] =K\u22121X\nk=0h[k]X[n\u2212k]\n| {z }\nbY[n]+E[n], (10.34)\nwhere E[n] is a noise random process to model the error. The linear part of the equation\nis known as the prediction and is constructed by sending X[n] through the system. For\nsimplicity we assume that X[n] is WSS. Thus, it follows that Y[n] is also WSS. We may\nalso assume that we can estimate RX[k],RY X[k],RXY[k] and RY[k].\n654", "670": "10.6. OPTIMAL LINEAR FILTER\nFigure 10.24: A schematic diagram illustrating the optimal linear filter problem: Given an input function\nX[n], we want to design a filter h[n]such that the prediction bY[n]is close to the target function Y[n].\nExample 10.20 . If we let K= 3, Equation (10.34) gives us\nY[n] =h[0]X[n] +h[1]X[n\u22121] +h[2]X[n\u22122] +E[n].\nThat is, the current sample Y[n] is a linear combination of the previous samples X[n],\nX[n\u22121] and X[n\u22122].\nGiven X[n] and Y[n], what would be the best guess of the impulse response h[n] so\nthat the prediction is as close to the true values as possible? From our discussions of linear\nregression, we know that this is equivalent to solving the optimization problem\nminimize\n{h[k]}K\u22121\nk=0 \nY[n]\u2212K\u22121X\nk=0h[k]X[n\u2212k]!2\n. (10.35)\nThe choice of the squared error is more or less arbitrary, depending on how we want to\nmodel E[n]. By using the square norm, we implicitly assume that the error is Gaussian.\nThis may not be true, but it is commonly used because the squared norm is differentiable .\nWe will follow this tradition.\nThe challenge associated with the minimization is that in most of the practical set-\ntings the random processes X[n] and Y[n] are changing rapidly because they are random\nprocesses. Therefore, even if we solve the optimization problem, the estimates h[k] will be\nrandom variables since we are solving a random equation. To eliminate this randomness, we\ntake the expectation over all the possible choices of X[n] and Y[n], yielding\nminimize\n{h[k]}K\u22121\nk=0 \nY[n]\u2212K\u22121X\nk=0h[k]X[n\u2212k]!2\n,\n\u21d3\nminimize\n{h[k]}K\u22121\nk=0EX,Y\uf8ee\n\uf8f0 \nY[n]\u2212K\u22121X\nk=0h[k]X[n\u2212k]!2\uf8f9\n\uf8fb.\nThe resulting impulse responses h[k], derived by solving the above minimization, is\nknown as the optimal linear filter . It is the best linear model for describing the input-\noutput relationships between X[n] and Y[n].\n655", "671": "CHAPTER 10. RANDOM PROCESSES\nWhat is the optimal linear filter?\nThe optimal linear filter is the solution to the optimization problem\nminimize\n{h[k]}K\u22121\nk=0EX,Y\uf8ee\n\uf8f0 \nY[n]\u2212K\u22121X\nk=0h[k]X[n\u2212k]!2\uf8f9\n\uf8fb. (10.36)\n10.6.3 Yule-Walker equation\nTo solve the optimal linear filter problem, we first perform some (slightly tedious) algebra\nto obtain the following results:\nLemma 10.4. LetbY[n] =PK\u22121\nk=0h[k]X[n\u2212k]be the prediction of Y[n]. The squared-\nnorm error can be written as\nEX,Y\u0014\u0010\nY[n]\u2212bY[n]\u00112\u0015\n=RY[0]\u22122K\u22121X\nk=0h[k]RY X[k] +K\u22121X\nk=0K\u22121X\nj=0h[k]h[j]RX[j\u2212k]. (10.37)\nThus we can express the error in terms of RY X[k],RX[k]andRY[k].\nProof . We expand the error as follows:\nEX,Y\u0014\u0010\nY[n]\u2212bY[n]\u00112\u0015\n=EY\u0002\n(Y[n])2\u0003\n\u22122EX,Yh\nY[n]bY[n]i\n+EXh\n(bY[n])2i\n.\nThe first term is the autocorrelation of Y[n]:\nEY\u0002\n(Y[n])2\u0003\n=E[Y[n+ 0]Y[n]] =RY[0]. (10.38)\nThe second term is\nEX,Yh\nY[n]bY[n]i\n=EX,Y\"\nY[n]K\u22121X\nk=0h[k]X[n\u2212k]#\n=K\u22121X\nk=0h[k]EX,Y[Y[n]X[n\u2212k]]\n=K\u22121X\nk=0h[k]RY X[k]. (10.39)\n656", "672": "10.6. OPTIMAL LINEAR FILTER\nThe third term is\nEXh\n(bY[n])2i\n=EX\uf8ee\n\uf8f0 K\u22121X\nk=0h[k]X[n\u2212k]!\uf8eb\n\uf8edK\u22121X\nj=0h[j]X[n\u2212j]\uf8f6\n\uf8f8\uf8f9\n\uf8fb\n=EX\uf8ee\n\uf8f0K\u22121X\nk=0K\u22121X\nj=0h[k]h[j]X[n\u2212k]X[n\u2212j]\uf8f9\n\uf8fb\n=K\u22121X\nk=0K\u22121X\nj=0h[k]h[j]EX[X[n\u2212k]X[n\u2212j]]\n=K\u22121X\nk=0K\u22121X\nj=0h[k]h[j]RX[j\u2212k]. (10.40)\nThis completes the proof.\n\u25a1\nThe significance of this theorem is that it allows us to write the error in terms of RY X[k],\nRX[k] and RY[k]. As we have mentioned, while we can solve the randomized optimization\nEquation (10.35), the resulting solution will be a random vector depending on the particular\nrealizations X[n] and Y[n]. Switching from Equation (10.35) to Equation (10.36) eliminates\nthe randomness because we have taken the expectation. The resulting optimization according\nto the theorem is also convenient. Instead of seeking individual realizations, we only need\nto know the overall statistical description of the data through RY X[k],RX[k] and RY[k].\nThese can be estimated through modeling or pseudorandom signals.\nThe solution to the optimal linear filter problem is summarized by the Yule-Walker\nequation :\nTheorem 10.9. The solution {h[0], . . . , h [K\u22121]}to the optimal linear filter problem\nminimize\n{h[k]}K\u22121\nk=0EX,Y\uf8ee\n\uf8f0 \nY[n]\u2212K\u22121X\nk=0h[k]X[n\u2212k]!2\uf8f9\n\uf8fb (10.41)\nis given by the following matrix equation:\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edRY X[0]\nRY X[1]\n...\nRY X[K\u22121]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8=\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edRX[0] RX[1] \u00b7\u00b7\u00b7 RX[K\u22121]\nRX[1] RX[0] \u00b7\u00b7\u00b7...\n............\nRX[K\u22121]RX[k\u22122]\u00b7\u00b7\u00b7 RX[0]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edh[0]\nh[1]\n...\nh[K\u22121]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8,(10.42)\nwhich is known as the Yule-Walker equation.\nTherefore, by solving the simple linear problem given by the Yule-Walker equation, we will\nfind the optimal linear filter solution.\nProof . Since the error is a squared norm, the optimal solution is obtained by taking the\n657", "673": "CHAPTER 10. RANDOM PROCESSES\nderivative:\nd\ndh[i]EX,Y\u0014\u0010\nY[n]\u2212bY[n]\u00112\u0015\n=d\ndh[i]\uf8f1\n\uf8f2\n\uf8f3RY[0]\u22122K\u22121X\nk=0h[k]RY X[k] +K\u22121X\nk=0K\u22121X\nj=0h[k]h[j]RX[j\u2212k]\uf8fc\n\uf8fd\n\uf8fe\n= 0\u22122RY X[i] + 2K\u22121X\nk=0h[k]RX[i\u2212k],\nin which the derivative of the last term is computed by noting that\nd\ndh[i]K\u22121X\nk=0K\u22121X\nj=0h[k]h[j]RX[j\u2212k]\n=d\ndh[i]K\u22121X\nj=0h[j]2RX[0] +d\ndh[i]K\u22121X\nk=0X\nj\u0338=kh[k]h[j]RX[j\u2212k]\n= 2K\u22121X\nk=0h[k]RX[i\u2212k].\nEquating the derivative to zero yields\nRY X[i] =K\u22121X\nk=0h[k]RX[i\u2212k], i = 0, . . . , K \u22121,\nand putting the above equations into the matrix-vector form we complete the proof.\n\u25a1\nThe matrix in the Yule-Walker equation is a Toeplitz matrix, in which each row is\na shifted version of the preceding row. This matrix structure is a consequence of a WSS\nprocess so that the autocorrelation function is determined by the time difference kand not\nby the starting and end times.\nRemark . If we take the derivative of the loss w.r.t. h[i], we have that\n0 =d\ndh[i]EX,Y\u0014\u0010\nY[n]\u2212bY[n]\u00112\u0015\n=\u22122Eh\u0010\nY[n]\u2212bY[n]\u0011\nX[n\u2212i]i\n.\nThis condition is known as the orthogonality condition , as it says that the error Y[n]\u2212bY[n]\nis orthogonal to the signal X[n\u2212i].\n10.6.4 Linear prediction\nWe now demonstrate how to use the Yule-Walker equation in modeling an autoregressive\nprocess . The procedure in this simple example can be used in speech processing and time-\nseries forecasting.\nSuppose that we have a WSS random process Y[n]. We would like to predict the future\nsamples by using the most recent Ksamples through an autoregressive model. Since the\n658", "674": "10.6. OPTIMAL LINEAR FILTER\nmodel is linear, we can write\nbY[n] =KX\nk=1h[k]Y[n\u2212k] +E[n]. (10.43)\nIn this model, we say that the predicted value bY[n] is a linear combination of the past K\nsamples, albeit to approximation error E[n].\nThe problem we need to solve is\nminimize\nh[k]E\u0014\u0010\nY[n]\u2212bY[n]\u00112\u0015\n.\nSincebY[n] is written in terms of the past samples of Y[n] in this problem, in the Yule-Walker\nequation we can replace Xwith Y. Consequently, we can write the matrix equation from\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edRY X[0]\nRY X[1]\n...\nRY X[K\u22121]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8=\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edRX[0] RX[1] \u00b7\u00b7\u00b7 RX[K\u22121]\nRX[1] RX[0] \u00b7\u00b7\u00b7 RX[K\u22122]\n............\nRX[K\u22121]RX[k\u22122]\u00b7\u00b7\u00b7 RX[0]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edh[0]\nh[1]\n...\nh[K\u22121]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8,\nto\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edRY[1]\nRY[2]\n...\nRY[K]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8\n|{z}\nr=\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edRY[0] RY[1] \u00b7\u00b7\u00b7 RY[K\u22121]\nRY[1] RY[0] \u00b7\u00b7\u00b7 RY[K\u22122]\n............\nRY[K\u22121]RY[k\u22122]\u00b7\u00b7\u00b7 RY[0]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8\n| {z }\nR\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edh[0]\nh[1]\n...\nh[K\u22121]\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8. (10.44)\nOn a computer, solving the Yule-Walker equation requires a few steps. First, we need\nto estimate the correlation\nRY[k] =E[Y[n+k]Y[n]]\u22481\nNNX\nn=1Y[n+k]Y[n].\nThe averaging on the right-hand side is often done using xcorr in MATLAB or np.correlate\nin Python. A graphical illustration of the input and the autocorrelation function is shown\ninFigure 10.25 .\nAfter we have found RY[n], we need to construct the Yule-Walker equation. For this\nlinear prediction problem, the left-hand side of the Yule-Walker equation is the vector r,\ndefined according to Equation (10.44). The Yule-Walker equation also requires the matrix R.\nThisRcan be constructed via the Toeplitz matrix as\nR= Toeplitz\u001a\nRY[0], RY[1], . . . , R Y[K\u22121]\u001b\n.\nIn MATLAB, we can call Toeplitz to construct the matrix. In Python, the command is\nlin.Toeplitz .\nTo solve the Yule-Walker equation, we need to invert the matrix R. There are built-in\ncommands for such an operation. In MATLAB, the command is \\(the backslash), whereas\nin Python the command is np.linalg.lstsq .\n659", "675": "CHAPTER 10. RANDOM PROCESSES\n0 50 100 150 200 250 300-0.2-0.100.10.2\nY[n]\n-300 -200 -100 0 100 200 300-0.2-0.100.10.20.30.4\nRY[k]\n(a)Y[n] (b) RY[k]\nFigure 10.25: An example time-series and its autocorrelation function.\n% MATLAB code to solve the Yule Walker Equation\ny = load(\u2019data_ch10.txt\u2019);\nK = 10;\nN = 320;\ny_corr = xcorr(y);\nR = Toeplitz(y_corr(N+[0:K-1]));\nlhs = y_corr(N+[1:K]);\nh = R\\lhs;\n# Python code to solve the Yule Walker Equation\ny = np.loadtxt(\u2019./data_ch10.txt\u2019)\nK = 10\nN = 320\ny_corr = np.correlate(y,y,mode=\u2019full\u2019)\nR = lin.Toeplitz(y_corr[N-1:N+K-1]) #call scipy.linalg\nlhs = y_corr[N:N+K]\nh = np.linalg.lstsq(R,lhs,rcond = None)[0]\nNote that in both the MATLAB and Python codes the Toeplitz matrix Rstarts with\nthe index N. This is because, as you can see from Figure 10.25 , the origin of the autocor-\nrelation function is the middle index of the computed autocorrelation function. For r, the\nstarting index is N+ 1 because the vector starts with RY[1].\nTo predict the future samples, we recall the autoregressive model for this problem:\nbY[n] =K\u22121X\nk=0h[k]Y[n\u2212k].\nTherefore, given Y[n\u22121], Y[n\u22122], . . . , Y [n\u2212K], we can predict bY[n]. Then we insert this\npredicted bY[n] into the sequence and increment the estimation problem to the next time\nindex. By repeating the process, we will be able to predict the future samples of Y[n].\n660", "676": "10.6. OPTIMAL LINEAR FILTER\nFigure 10.26 illustrates the prediction results of the Yule-Walker equation. As you can\nsee, the predictions are reasonably meaningful since the patterns follow the trend.\n0 50 100 150 200 250 300 350-0.3-0.2-0.100.10.2\nPrediction\nInput\nFigure 10.26: An example of the predictions made by the autoregressive model.\nThe MATLAB and Python codes are shown below.\n% MATLAB code to predict the samples\nz = y(311:320);\nyhat = zeros(340,1);\nyhat(1:320) = y;\nfor t = 1:20\npredict = z\u2019*h;\nz = [z(2:10); predict];\nyhat(320+t) = predict;\nend\nplot(yhat, \u2019r\u2019, \u2019LineWidth\u2019, 3); hold on;\nplot(y, \u2019k\u2019, \u2019LineWidth\u2019, 4);\n# Python code to predict the samples\nz = y[310:320]\nyhat = np.zeros((340,1))\nyhat[0:320,0] = y\nfor t in range(20):\npredict = np.inner(np.reshape(z,(1,10)),h)\nz = np.concatenate((z[1:10], predict))\nyhat[320+t,0] = predict\nplt.plot(yhat,\u2019r\u2019)\nplt.plot(y,\u2019k\u2019)\nplt.show()\n661", "677": "CHAPTER 10. RANDOM PROCESSES\n10.6.5 Wiener filter\nIn the previous formulation, we notice that the impulse response has a finite length. There\nare, however, problems in which the impulse response is infinite. For example, a recur-\nsive filter h[n] will be infinitely long. The extension from finite length to infinite length is\nstraightforward. We can model the problem as\nY[n] =\u221eX\nk=\u2212\u221eh[k]X[n\u2212k] +E[n].\nHowever, when h[n] is infinitely long the Yule-Walker equation does not hold because the\nmatrix Rwill be infinitely large. Nevertheless, the building block equation for Yule-Walker\nis still valid:\nRY X[i] =\u221eX\nk=\u2212\u221eh[k]RX[i\u2212k]. (10.45)\nTo maintain the spirit of the Yule-Walker equation while enabling computation, we\nrecognize that the infinite sum on the right-hand side is, in fact, a convolution . Thus we\ncan take the (discrete-time) Fourier transform of both sides to obtain\nSY X(ej\u03c9) =H(ej\u03c9)SX(ej\u03c9). (10.46)\nTherefore, the corresponding optimal linear filter (in the Fourier domain) is\nH(ej\u03c9) =SY X(ej\u03c9)\nSX(ej\u03c9), (10.47)\nand\nh[n] =F\u22121\u001aSY X(e\u2212j\u03c9)\nSX(e\u2212j\u03c9)\u001b\n.\nThe filter obtained in this way is known as the Wiener filter .\nExample 10.21 . (Denoising ) Suppose X[n] =Y[n] +W[n], where W[n] is the noise\nterm that is independent of Y[n], as shown in Figure 10.27 .\nFigure 10.27: Design of a Wiener filter that takes an input function X[n]and outputs an estimate\nbY[n]that is close to the true function Y[n].\nNow, given the input function X[n], can we construct the Wiener filter h[n] such\nthat the predicted function bY[n] is as close to Y[n] as possible? The Wiener filter for\nthis problem is also the optimal denoising filter.\n662", "678": "10.6. OPTIMAL LINEAR FILTER\nSolution . The following correlation functions can easily be seen:\nRX[k] =E[X[n+k]X[n]]\n=E[(Y[n+k] +W[n+k])(Y[n] +W[n])]\n=E[Y[n+k]Y[n]] +E[Y[n+k]W[n]]\n+E[W[n+k]Y[n]] +E[W[n+k]W[n]]\n=E[Y[n+k]Y[n]] + 0 + 0 + E[W[n+k]W[n]]\n=RY[k] +RW[k].\nSimilarly, we have\nRY X[k] =E[Y[n+k]X[n]]\n=E[Y[n](Y[n+k] +W[n+k])] = RY[k].\nConsequently, the optimal linear filter is\nH(ej\u03c9) =SY X(ej\u03c9)\nSX(ej\u03c9)\n=F{RY X[k]}\nF{RX[k]}\n=SY(ej\u03c9)\nSY(ej\u03c9) +SW(ej\u03c9).\nWhat is the Wiener filter for a denoising problem?\n\u0088Suppose the corrupted function X[n] is related to the clean function Y[n] through\nX[n] =Y[n] +W[n], for some noise function W[n].\n\u0088The Wiener filter is\nH(ej\u03c9) =SY(ej\u03c9)\nSY(ej\u03c9) +SW(ej\u03c9). (10.48)\n\u0088To perform the filtering, the denoised function bY[n] is\nbY[n] =F\u22121\b\nH(ej\u03c9)X(ej\u03c9)\t\n.\nFigure 10.28 shows an example of applying the Wiener filter to a noise removal prob-\nlem. In this example we let W[n] be an i.i.d. Gaussian process with standard deviation\n\u03c3= 0.05 and mean \u00b5= 0. The noisy samples of random process X[n] are defined as\nX[n] =Y[n]+W[n], where Y[n] is the clean function. As you can see from Figure 10.28 (a),\nthe Wiener filter is able to denoise the function reasonably well.\nThe optimal linear filter used for this denoising task is infinitely long. This can be seen\ninFigure 10.28 (b), where the filter length is the same as the length of the observed time\nseries X[n]. IfX[n] is longer, the filter h[n] will also become longer. Therefore, finite-length\napproaches such as the Yule-Walker equation do not apply here.\n663", "679": "CHAPTER 10. RANDOM PROCESSES\n0 50 100 150 200 250 300-0.2-0.100.10.2\nNoisy Input X[n]\nWiener Filtered Yhat[n]\nGround Truth Y[n]\n-300 -200 -100 0 100 200 300-0.0500.050.10.150.20.25 h[n]\n(a) Noise removal by Wiener filtering (b) Wiener filter\nFigure 10.28: (a) Applying a Wiener filter to denoise a function. (b) The Wiener filter used for the\ndenoising task.\nThe MATLAB / Python codes used to generate Figure 10.28 (a) are shown below.\nThe main commands here are scipy.fft andscipy.ifft , which are available in the scipy\nlibrary. The commands Yhat = H.*fft(x, 639) in MATLAB execute the Wiener filtering\nstep. Here, we resample the function xto 639 samples so that it matches with the Wiener\nfilter H. Similar commands in Python are H * fft(x, 639) .\n% MATLAB code for Wiener filtering\nw = 0.05*randn(320,1);\nx = y + w;\nRy = xcorr(y);\nRw = xcorr(w);\nSy = fft(Ry);\nSw = fft(Rw);\nH = Sy./(Sy + Sw);\nYhat = H.*fft(x, 639);\nyhat = real(ifft(Yhat));\nplot(x, \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0.7, 0.7, 0.7]); hold on;\nplot(yhat(1:320), \u2019r\u2019, \u2019LineWidth\u2019, 2);\nplot(y, \u2019k:\u2019, \u2019LineWidth\u2019, 2);\n# Python code for Wiener filtering\nfrom scipy.fft import fft, ifft\nw = 0.05*np.random.randn(320)\nx = y + w\nRy = np.correlate(y,y,mode=\u2019full\u2019)\nRw = np.correlate(w,w,mode=\u2019full\u2019)\nSy = fft(Ry)\nSw = fft(Rw)\nH = Sy / (Sy+Sw)\nYhat = H * fft(x, 639)\n664", "680": "10.6. OPTIMAL LINEAR FILTER\nyhat = np.real(ifft(Yhat))\nplt.plot(x,color=\u2019gray\u2019)\nplt.plot(yhat[0:320],\u2019r\u2019)\nplt.plot(y,\u2019k:\u2019)\nExample 10.22 . (Deconvolution ) Suppose that the corrupted function is generated\naccording to a linear process given by\nX[n] =\u221eX\n\u2113=\u2212\u221eg[\u2113]Y[n\u2212\u2113] +W[n],\nwhere g[n] is the impulse response of some kind of degradation process and W[n] is\nthe Gaussian noise term, as shown in Figure 10.29 . Find the optimal linear filter (i.e.,\nthe Wiener filter) to estimate bY[n].\nFigure 10.29: Design of a Wiener filter that takes an input function X[n]and outputs an estimate\nbY[n]that is close to the true function Y[n].\nSolution . To construct the Wiener filter, we first determine the cross-correlation func-\ntion:\nRY X[k] =E[Y[n+k]X[n]] =E\"\nY[n+k]\u221eX\n\u2113=\u2212\u221eg[\u2113]Y[n\u2212\u2113] +W[n]#\n.\nUsing algebra, it follows that\nE\"\nY[n+k]\u221eX\n\u2113=\u2212\u221eg[\u2113]Y[n\u2212\u2113] +W[n]#\n=\u221eX\n\u2113=\u2212\u221eg[\u2113]E[Y[n+k]Y[n\u2212\u2113]] +E[Y[n+k]W[n]]\n=\u221eX\n\u2113=\u2212\u221eg[\u2113]RY[k+\u2113] + 0 = ( g\u229bRY)[k],\nwhich is the correlation between gandRY. Therefore, the cross power spectral density\nSY X(ej\u03c9) is\nSY X(ej\u03c9) =G(ej\u03c9)SY(ej\u03c9).\n665", "681": "CHAPTER 10. RANDOM PROCESSES\nThe autocorrelation of this problem is\nRX[k] =E[X[n+k]X[n]]\n=E[((g\u2217Y)[n+k] +W[n+k])((g\u2217Y)[n] +W[n])]\n=E[(g\u2217Y)[n+k](g\u2217Y)[n]] +E[W[n+k]W[n]]\n= (g\u229b(g\u2217RY))[k] +RW[k],\nwhere, according to the previous section, the first part is the correlation \u229bfollowed by\na convolution \u2217. Therefore, the power spectral density of Xis\nSX(ej\u03c9) =|G(ej\u03c9)|2SY(ej\u03c9) +SW(ej\u03c9).\nCombining the results, the Wiener filter is\nH(ej\u03c9) =SY X(ej\u03c9)\nSX(ej\u03c9)=G(ej\u03c9)SY(ej\u03c9)\n|G(ej\u03c9)|2SY(ej\u03c9) +SW(ej\u03c9).\nWhat is the Wiener filter for a deconvolution problem?\n\u0088Suppose that the corrupted function X[n] is related to the clean function Y[n]\nthrough X[n] = (g\u2217Y)[n] +W[n], for some degradation g[n] and noise W[n].\n\u0088The Wiener filter is\nH(ej\u03c9) =G(ej\u03c9)SY(ej\u03c9)\n|G(ej\u03c9)|2SY(ej\u03c9) +SW(ej\u03c9). (10.49)\n\u0088To perform the filtering, the estimated function bY[n] is\nbY[n] =F\u22121\b\nH(ej\u03c9)X(ej\u03c9)\t\n.\nAs an example of the deconvolution problem, we show a WSS function Y[n] inFig-\nure 10.30 . This clean function Y[n] is constructed by passing an i.i.d. noise process through\nan arbitrary LTI system so that the WSS property is guaranteed. Given this Y[n], we con-\nstruct a degradation process in which the impulse response is given by g[n]. In this example,\nwe assume that g[n] is a uniform function. We then add noise W[n] to the time series to\nobtain the corrupted observation X[n]. The reconstruction by the Wiener filter is shown in\nFigure 10.30 .\nThe MATLAB and Python codes used to generate Figure 10.30 are shown below.\n% MATLAB code to solve the Wiener deconvolution problem\nload(\u2019ch10_wiener_deblur_data\u2019);\ng = ones(32,1)/32;\nw = 0.02*randn(320,1);\nx = conv(y,g,\u2019same\u2019) + w;\nRy = xcorr(y);\n666", "682": "10.6. OPTIMAL LINEAR FILTER\n50 100 150 200 250 300-0.8-0.6-0.4-0.200.20.40.6\nNoisy Input X[n]\nWiener Filtered Yhat[n]\nGround Truth Y[n]\nFigure 10.30: Reconstructing time series from degraded observations using a Wiener filter.\nRw = xcorr(w);\nSy = fft(Ry);\nSw = fft(Rw);\nG = fft(g,639);\nH = (conj(G).*Sy)./(abs(G).^2.*Sy + Sw);\nYhat = H.*fft(x, 639);\nyhat = real(ifft(Yhat));\nfigure;\nplot(x, \u2019LineWidth\u2019, 4, \u2019Color\u2019, [0.5, 0.5, 0.5]); hold on;\nplot(16:320+15, yhat(1:320), \u2019r\u2019, \u2019LineWidth\u2019, 2);\nplot(1:320, y, \u2019k:\u2019, \u2019LineWidth\u2019, 2);\n# Python code to solve the Wiener deconvolution problem\ny = np.loadtxt(\u2019./ch10_wiener_deblur_data.txt\u2019)\ng = np.ones(64)/64\nw = 0.02*np.random.randn(320)\nx = np.convolve(y,g,mode=\u2019same\u2019) + w\nRy = np.correlate(y,y,mode=\u2019full\u2019)\nRw = np.correlate(w,w,mode=\u2019full\u2019)\nSy = fft(Ry)\nSw = fft(Rw)\nG = fft(g,639)\nH = (np.conj(G)*Sy)/( np.power(np.abs(G),2)*Sy + Sw )\nYhat = H * fft(x, 639)\nyhat = np.real(ifft(Yhat))\nplt.plot(x,color=\u2019gray\u2019)\n667", "683": "CHAPTER 10. RANDOM PROCESSES\nplt.plot(np.arange(32,320+32),yhat[0:320],\u2019r\u2019)\nplt.plot(y,\u2019k:\u2019)\nCaveat to Wiener filtering . In practice, the above Wiener filter needs to be modified\nbecause SY(ej\u03c9) and SW(ej\u03c9) cannot be estimated from the data via the temporal corre-\nlation (as we did in the MATLAB/Python programs). The reason is that we never have\naccess to Y[n] and W[n]. In this case, one has to guess the power spectral densities SY(ej\u03c9)\nandSW(ej\u03c9). The noise power SW(ej\u03c9) is usually not difficult to estimate. For example,\nin the program we showed above, the noise power spectral density is Sw = 0.02^2*320\n(MATLAB), which is the noise standard deviation times the number of samples.\nThe signal SY(ej\u03c9) is often the hard part. In the absence of any knowledge about the\nground truth\u2019s power spectral density, the Wiener filter does not work. However, for certain\nproblems in which SY(ej\u03c9) can be predetermined by prior knowledge, the Wiener filter is\nguaranteed to be optimal \u2014 optimal in the mean-squared-error sense over the entire time\naxis.\nWiener filter versus ridge regression . The Wiener filter equation can be interpreted\nas a ridge regression. Denoting the forward observation model by\nx=Gy+w,\nthe corresponding ridge regression minimization is\nby= argmin\ny\u2225x\u2212Gy\u22252+\u03bb\u2225y\u22252\n= (GTG+\u03bbI)\u22121GTx.\nIfGis a convolutional matrix, the above solution can be written in the Fourier domain (by\nusing the Fourier transform as the eigenvectors):\nbY(ej\u03c9) =\"\nG(ej\u03c9)\n|G(ej\u03c9)|2+\u03bb#\n| {z }\nH(ej\u03c9)X(ej\u03c9).\nComparing this \u201coptimal linear filter\u201d with the Wiener filter, we observe that the Wiener\nfilter has slightly more generality:\nbY(ej\u03c9) =\"\nG(ej\u03c9)SY(ej\u03c9)\n|G(ej\u03c9)|2SY(ej\u03c9) +SW(ej\u03c9)#\nX(ej\u03c9).\nTherefore, in the absence of SY(ej\u03c9) and assuming that SW(ej\u03c9) is a constant (e.g., for\nGaussian noise), the Wiener filter is exactly a ridge regression.\n668", "684": "10.7. SUMMARY\n10.7 Summary\nRandom processes are very useful tools for analyzing random variables over time. In this\nchapter, we have introduced some of the most basic mechanisms:\n\u0088Statistical versus temporal analysis : The statistical analysis of a random process\nlooks at the random process vertically . It treats X(t) as a random variable and studies\nthe randomness across different realizations. The temporal analysis is the horizontal\nperspective. It treats X(t) as a function in time with a fixed random index. In general,\nstatistical average \u0338= temporal average.\n\u0088Mean function \u00b5X(t): The mean function is the expectation of the random process.\nAt every time t, we take the expectation to obtain the expected value E[X(t)].\n\u0088Autocorrelation function RX(t1, t2). This is the joint expectation of the random pro-\ncess at two different time instants t1andt2. The corresponding values X(t1) and X(t2)\nare two random variables, and so the joint expectation measures how correlated these\ntwo variables are.\n\u0088Wide-sense stationary (WSS) : This is a special class of random processes in which\n\u00b5X(t) is a constant and RX(t1, t2) is a function of t1\u2212t2. When this happens, the auto-\ncorrelation function (which is originally a 2D function) will have a Toeplitz structure.\nWe write RX(t1, t2) asRX(\u03c4), where \u03c4=t1\u2212t2.\n\u0088Power spectral density (PSD) : This is the Fourier transform of the autocorrelation\nfunction RX(\u03c4), according to the Einstein-Wiener-Khinchin theorem. It is called the\npower spectral density because we can integrate it in the Fourier space to retrieve the\npower. This provides us with some convenient computational tools for analyzing data.\n\u0088Random process through a linear time-invariant (LTI) system : This tells us how a\nrandom process behaves after going through an LTI system. The analysis can be done\nat the realization level, where we look at each random process, or at the statistical\nlevel, where we look at the autocorrelation function and the PSD.\n\u0088Optimal linear filter : A set of techniques that can be used to retrieve signals by using\nthe statistical information of the data and the system. We introduced two specific\napproaches: the Yule-Walker equation for a finite-length filter and the Wiener filter\nfor an infinite-length filter. We demonstrated how these techniques could be applied\nto forecast a time series and recover a time series from corrupted measurements.\nWhile we have covered some of the most basic ideas in random processes, there are\nalso several topics we have not discussed. These include, but are not limited to: strictly\nstationary process, a more restrictive class of random process than WSS; Poisson process,\na useful model for arrival analysis; Markov chain, a discrete-time random process where\nthe current state only depends on the previous state. Readers interested in these materials\nshould consult the references listed at the end of this chapter.\n669", "685": "CHAPTER 10. RANDOM PROCESSES\n10.8 Appendix\nThe Einstein-Wiener-Khinchin theorem\nThe Einstein-Wiener-Khinchin theorem is a fundamental result. It states that for any wide-\nsense stationary process, the power spectral density SX(\u03c9) is the Fourier transform of the\nautocorrelation function.\nTheorem 10.10 (The Einstein-Wiener-Khinchin theorem ).For a WSS random pro-\ncessX(t),\nSX(\u03c9) =F {RX(\u03c4)}, (10.50)\nwhenever the Fourier transform of RX(\u03c4)exists.\nProof . First, let\u2019s recall the definition of SX(\u03c9):\nSX(\u03c9)def= lim\nT\u2192\u221e1\n2TEh\n|eXT(\u03c9)|2i\n. (10.51)\nBy expanding the expectation, we have\nE[|eXT(\u03c9)|2] =E\" ZT\n\u2212TX(t)e\u2212j\u03c9tdt! ZT\n\u2212TX(\u03b8)e\u2212j\u03c9\u03b8d\u03b8!\u2217#\n=ZT\n\u2212TZT\n\u2212TE[X(t)X(\u03b8)]e\u2212j\u03c9(t\u2212\u03b8)dt d\u03b8 =ZT\n\u2212TZT\n\u2212TRX(t\u2212\u03b8)e\u2212j\u03c9(t\u2212\u03b8)dt d\u03b8.\n(10.52)\nOur next step is to analyze RX(t\u2212\u03b8). Define\nQX(v) =F {RX(\u03c4)}. (10.53)\nThen, by inverse Fourier transform\nRX(\u03c4) =1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)ejv\u03c4dv,\nand therefore\nRX(t\u2212\u03b8) =1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)ejv(t\u2212\u03b8)dv.\nSubstituting this into Equation (10.52) yields\nE[|eXT(\u03c9)|2=ZT\n\u2212TZT\n\u2212T\u00121\n2\u03c0Z\u221e\n\u2212\u221eQX(v)ejv(t\u2212\u03b8)dv\u0013\ne\u2212j\u03c9(t\u2212\u03b8)dt d\u03b8\n=1\n2\u03c0Z\u221e\n\u2212\u221eQX(v) ZT\n\u2212Tejt(v\u2212\u03c9)dt! ZT\n\u2212Tej\u03b8(\u03c9\u2212v)d\u03b8!\ndv.\n670", "686": "10.8. APPENDIX\nWe now need to simplify the two inner integrals. Recall by Fourier pair that\nrect\u0012t\nT\u0013\nF\n\u2190\u2192Tsinc\u0012\u03c9T\n2\u0013\n.\nThis implies that\nZT\n\u2212Tejt(v\u2212\u03c9)dt=ZT\n\u2212Te\u2212j(\u03c9\u2212v)tdt\n=Z\u221e\n\u2212\u221erect(t\n2T)e\u2212j(\u03c9\u2212v)tdt= 2Tsinc(( \u03c9\u2212v)T) = 2 Tsin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T.\nHence, we have\nEh\n|eXT(\u03c9)|2i\n=1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)\u0012\n2Tsin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\ndv. (10.54)\nand so\n1\n2TE[|eXT(\u03c9)|2=2T\n2\u03c0Z\u221e\n\u2212\u221eQX(v)\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\ndv. (10.55)\nAsT\u2192 \u221e (see Lemma 10.5 below), we have\n2T\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\n\u2212\u2192 2\u03c0\u03b4(\u03c9\u2212v).\nTherefore,\nlim\nT\u2192\u221e1\n2TEh\n|eXT(\u03c9)|2i\n=1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)\"\nlim\nT\u2192\u221e2T\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132#\ndv\n=Z\u221e\n\u2212\u221eQX(v)\u03b4(\u03c9\u2212v)dv=QX(\u03c9).\nSince QX(\u03c9) =F[RX(\u03c4)], we conclude that\nSX(\u03c9) = lim\nT\u2192\u221e1\n2TE[|eXT(\u03c9)|2] =QX(\u03c9) =F[RX(\u03c4)].\nLemma 10.5.\nlim\nT\u2192\u221e1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)2T\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\ndv=QX(\u03c9). (10.56)\nTo prove this lemma, we first define \u03b4T(\u03c9) = 2 T(sin(\u03c9T)\n\u03c9T)2. It is sufficient to show that\n\f\f\f\flim\nT\u2192\u221e1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)2T\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\ndv\u2212QX(\u03c9)\f\f\f\f\u21920 as T\u2192 \u221e .(10.57)\nWe will proceed by demonstrating the following three facts about \u03b4T(\u03c9):\n671", "687": "CHAPTER 10. RANDOM PROCESSES\n1.\n1\n2\u03c0Z\u221e\n\u2212\u221e\u03b4T(\u03c9)d\u03c9= 1\n.\n2. For any \u25b3>0, Z\n{\u03c9:|\u03c9|>\u25b3}\u03b4T(\u03c9)d\u03c9\u21920 as T\u2192 \u221e\n.\n3. For any |\u03c9| \u2265 \u25b3 >0, we have |\u03b4T(\u03c9)| \u22642\nT\u25b32.\nProof of Fact 1 .\n1\n2\u03c0Z\u221e\n\u2212\u221e\u03b4T(\u03c9)d\u03c9=1\n2\u03c0Z\u221e\n\u2212\u221e2T\u0012sin(\u03c9T)\n\u03c9T\u00132\n|{z}\nsinc2(\u03c9T)d\u03c9.\nNote that\n\u039b\u0012t\n4T\u0013\n\u2190\u21922Tsinc2(\u03c9T).\nTherefore,\n1\n2\u03c0Z\u221e\n\u2212\u221e2Tsinc2(\u03c9T)d\u03c9=1\n2\u03c0Z\u221e\n\u2212\u221e2Tsinc2(\u03c9T)ej\u03c90d\u03c9\n= \u039b\u00120\n4T\u0013\n= 1.\nProof of Fact 2 .\u03b4T(\u03c9) is symmetric, so, it is sufficient to check only one side:\nZ\u221e\n\u25b3\u03b4T(\u03c9)d\u03c9=Z\u221e\n\u25b32T\u0012sin(\u03c9t)\n\u03c9T\u00132\nd\u03c9\n=2T\nT2Z\u221e\n\u25b3sin2(\u03c9t)\n\u03c92d\u03c9\n\u22642\nTZ\u221e\n\u25b31\n\u03c92d\u03c9 |sin(.)|2\u22641\n=2\nT\u0014\n\u22121\n\u03c9\u0015\u221e\n\u25b3=2\nT\u25b3\u21920 as T\u2192 \u221e .\nProof of Fact 3 .\n|\u03b4T(\u03c9)|= 2T\u0012sin(\u03c9T)\n\u03c9T\u00132\n\u22642T\u00121\n(\u03c9T)2\u0013\n=2\n\u03c92T\u22642\nT\u25b32.\nProof of Lemma . Consider QX(\u03c9). By Property 1,\nQX(\u03c9) =QX(\u03c9).1\n2\u03c0Z\u221e\n\u2212\u221e\u03b4T(\u03c9\u2212v)dv=1\n2\u03c0Z\u221e\n\u2212\u221eQX(\u03c9)\u03b4T(\u03c9\u2212v)dv.\n672", "688": "10.8. APPENDIX\nTherefore,\n\f\f\f\f1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)\u03b4T(\u03c9\u2212v)dv\u2212QX(\u03c9)\f\f\f\f\n=\f\f\f\f1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)\u03b4T(\u03c9\u2212v)dv\u22121\n2\u03c0Z\u221e\n\u2212\u221eQX(\u03c9)\u03b4T(\u03c9\u2212v)dv\f\f\f\f\n=1\n2\u03c0\f\f\f\fZ\u221e\n\u2212\u221e(QX(v)\u2212QX(\u03c9))\u03b4T(\u03c9\u2212v)dv\f\f\f\f\u22641\n2\u03c0Z\u221e\n\u2212\u221e\f\fQX(v)\u2212QX(\u03c9)\f\f\u03b4T(\u03c9\u2212v)dv.\nFor any \u03f5 >0, let\u25b3be a constant such that\n|\u03c9\u2212v|<\u25b3 whenever |QX(v)\u2212QX(\u03c9)|< \u03f5.\nThen we can partition the above integral into\n1\n2\u03c0Z\u221e\n\u2212\u221e\f\fQX(\u03c9)\u2212QX(v)\f\f\u03b4T(\u03c9\u2212v)dv=1\n2\u03c0Z\u03c9+\u25b3\n\u03c9\u2212\u25b3\f\fQX(\u03c9)\u2212QX(v)\f\f\u03b4T(\u03c9\u2212v)dv(1)\n+1\n2\u03c0Z\u221e\n\u03c9+\u25b3\f\fQX(\u03c9)\u2212QX(v)\f\f\u03b4T(\u03c9\u2212v)dv (2)\n+1\n2\u03c0Z\u03c9+\u25b3\n\u2212\u221e\f\fQX(\u03c9)\u2212QX(v)\f\f\u03b4T(\u03c9\u2212v)dv. (3)\nPartition (1) above can be evaluated as follows:\n1\n2\u03c0Z\u03c9+\u25b3\n\u03c9\u2212\u25b3\f\fQX(\u03c9)\u2212QX(v)\f\f\u03b4T(\u03c9\u2212v)dv\n\u22641\n2\u03c0Z\u03c9+\u25b3\n\u03c9\u2212\u25b3\u03f5\u03b4T(\u03c9\u2212v)dv\n=\u03f5\n2\u03c0Z\u03c9+\u25b3\n\u03c9\u2212\u25b3\u03b4T(\u03c9\u2212v)dv\n\u2264\u03f5\n2\u03c0Z\u221e\n\u2212\u221e\u03b4T(\u03c9\u2212v)dv=\u03f5,\nwhere the last inequality holds because \u03b4T(\u03c9\u2212v)\u22650. Since \u03f5can be arbitrarily small, the\nonly possibility for\n1\n2\u03c0Z\u03c9+\u25b3\n\u03c9\u2212\u25b3\f\fQX(\u03c9)\u2212QX(v)\f\f\u03b4T(\u03c9\u2212v)dv\nfor all \u03f5is that the integral is 0.\nPartition (2) above can be evaluated as follows:\n1\n2\u03c0Z\u221e\n\u03c9+\u25b3\f\fQX(\u03c9)\u2212QX(v)\f\f\u03b4T(\u03c9\u2212v)dv\n\u22641\n2\u03c0Z\u221e\n\u03c9+\u25b3\u0000\f\fQX(\u03c9)\f\f+\f\fQX(v)\f\f\u0001\n\u03b4T(\u03c9\u2212v)dv\n=QX(\u03c9)1\n2\u03c0Z\u221e\n\u03c9+\u25b3\u03b4T(\u03c9\u2212v)dv+1\n2\u03c0Z\u221e\n\u03c9+\u25b3QX(v)\u03b4T(\u03c9\u2212v)dv.\n673", "689": "CHAPTER 10. RANDOM PROCESSES\nBy Property 2,1\n2\u03c0R\u221e\n\u03c9+\u25b3\u03b4T(\u03c9\u2212v)dv\u21920 asT\u2192 \u221e . By Property 3,\n1\n2\u03c0Z\u221e\n\u03c9+\u25b3QX(v)\u03b4T(\u03c9\u2212v)dv\u22641\n2\u03c02\nT\u25b32Z\u221e\n\u03c9+\u25b3QX(v)dv\n| {z }\n<\u221ebecause QX(v)=F[RX(\u03c4)]\u21920.\nTherefore, we conclude that\n1\n2\u03c0Z\u221e\n\u03c9+\u25b3QX(v)\u03b4T(\u03c9\u2212v)dv\u21920 as T\u2192 \u221e .\nand hence (1), (2) and (3) all \u21920 asT\u2192 \u221e . So we have\n\f\f\f\flim\nT\u2192\u221e1\n2\u03c0Z\u221e\n\u2212\u221eQX(v)2T\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\ndv\u2212QX(\u03c9)\f\f\f\f\u21920 as T\u2192 \u221e ,\nwhich completes the proof.\n10.8.1 The Mean-Square Ergodic Theorem\nThe mean-square ergodic theorem states that for any WSS random process, the statistical\naverage is the same as the temporal average . This provides an important tool in practice\nbecause finding the statistical average is typically very difficult. With the mean ergodic\ntheorem, one can easily estimate the statistical average using the temporal average.\nTheorem 10.11 (Mean-Square Ergodic Theorem ).LetY(t)be a WSS process,\nwith mean E[Y(t)] =mand autocorrelation function RY(\u03c4). Assume that the Fourier\ntransform of RY(\u03c4)exists. Define\nMTdef=1\n2TZT\n\u2212TY(t)dt. (10.58)\nThenEh\f\fMT\u2212m\f\f2i\n\u21920asT\u2192 \u221e .\nProof of Mean Ergodic Theorem . Let X(t) =Y(t)\u2212m. It follows that\nMT\u2212m=1\n2TZT\n\u2212TY(t)dt\u2212m=1\n2TZT\n\u2212TX(t)dt.\nWe define the finite-window approximation of X(t):\nXT(t) =\u001aX(t),\u2212T\u2264t\u2264T,\n0, elsewhere .\nThen the difference MT\u2212mcan be computed as\nMT\u2212m=1\n2TZT\n\u2212TX(t)dt=1\n2TZ\u221e\n\u2212\u221eX(t)e\u2212j0tdt=1\n2TeXT(\u03c9)\f\f\n\u03c9=0=eXT(0)\n2T.\n674", "690": "10.9. REFERENCES\nTaking the expectation of the squares yields\nE\u0002\n|MT\u2212m|2\u0003\n=Eh\f\feXT(0)\f\f2i\n4T2.\nRecall from the Einstein-Wiener-Khinchin theorem,\n1\n2TEh\f\feXT(\u03c9)\f\f2i\n=1\n2\u03c0Z\u221e\n\u2212\u221eSX(v)2T\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\ndv.\nPutting the limit T\u2192 \u221e , if we have that\nlim\nT\u2192\u221e1\n2\u03c0Z\u221e\n\u2212\u221eSX(v)2T\u0012sin((\u03c9\u2212v)T)\n(\u03c9\u2212v)T\u00132\ndv=SX(\u03c9),\nthen we will have\n1\n2TEh\f\feXT(\u03c9)\f\f2i\n\u2192SX(\u03c9) and1\n2TEh\f\feXT(0)\f\f2i\n\u2192SX(0).\nHence,\nlim\nT\u2192\u221eEh\f\fMT\u2212m\f\f2i\n= lim\nT\u2192\u221e1\n2TEh\f\feXT(0)\f\f2i\n= lim\nT\u2192\u221e1\n2TSX(0) = 0 .\nThis completes the proof.\n10.9 References\nBasic texts\nThe following textbooks are basic texts about random processes. They offer many comple-\nmentary materials to our book. For example, we omitted the topics of straightly stationary\nprocesses and memoryless properties. We have also omitted a few classical examples, such\nas the random telegraph signal, the incremental independence of Poisson processes, and\nMarkov chains. These materials can be found in the texts below.\n10-1 John A. Gubner, Probability and Random Processes for Electrical and Computer En-\ngineers , Cambridge University Press, Illustrated edition, 2006.\n10-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes For Electrical En-\ngineering , Pearson, 3rd Edition, 2007.\n10-3 Athanasios Papoulis, S. Unnikrishna Pillai, Probability, Random Variables and Stochas-\ntic Processes , McGraw-Hill, 4th Edition, 2012.\n10-4 Henry Stark and John Woods, Probability and Random Processes With Applications\nto Signal Processing , Prentice Hall, 3rd Edition, 2001.\n675", "691": "CHAPTER 10. RANDOM PROCESSES\n10-5 Eugene Wong and Bruce Hajek, Stochastic Processes in Engineering Systems , Springer-\nVerlag, 1985.\n10-6 Bruce Hajek, Random Processes for Engineers , Cambridge University Press, 2015.\n10-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability , Athena Sci-\nentific, 2nd Edition, 2008.\n10-8 Robert G. Gallager, Stochastic Processes: Theory for Applications , Cambridge Uni-\nversity Press, 1st Edition, 2014.\nSignal and systems / Fourier transforms\nThe following references are classic references on signal and systems.\n10-9 Alan Oppenheim and Ronald Schafer, Discrete-Time Signal Processing , 2nd Edition,\nPrentice Hall 1999.\n10-10 Alan Oppenheim and Alan Willsky, Signals and Systems , Pearson, 2nd Edition, 1996.\n10-11 Martin Vetterli, Jelena Kovacevic, and Vivek K. Goyal, Foundations of Signal Pro-\ncessing , Cambridge University Press, 3rd Edition, 2014.\n10-12 Todd K. Moon and Wynn C. Stirling, Mathematical Methods and Algorithms for Signal\nProcessing , Prentice-Hall, 2000.\nEngineering applications\n10-13 John G. Proakis and Masoud Salehi, Communication Systems Engineering , Pearson,\n2nd Edition, 2001.\n10-14 Rodger E. Ziemer, William H. Tranter, Principles of Communications , Wiley, 7th\nEdition, 2014.\n10-15 Joseph W. Goodman, Statistical Optics , Wiley, 2015.\n10.10 Problems\nExercise 1. (Video Solution)\nConsider the random process\nX(t) = 2 Acos(t) + (B\u22121) sin( t),\nwhere AandBare two independent random variables with E[A] =E[B] = 0, and E[A2] =\nE[B2] = 1.\n(a) Find \u00b5X(t).\n(b) Find RX(t1, t2).\n676", "692": "10.10. PROBLEMS\n(c) Find CX(t1, t2).\nExercise 2. (Video Solution)\nLetX[n] be a discrete-time random process with mean function mX[n] =E{X[n]}and\ncorrelation function RX[n, m] =E{X[n]X[m]}. Suppose that\nY[n] =\u221eX\ni=\u2212\u221eh[n\u2212i]X[i]. (10.59)\n(a) Find \u00b5Y[n].\n(b) Find RXY[n, m].\nExercise 3. (Video Solution)\nLetY(t) =X(t)\u2212X(t\u2212d).\n(a) Find RX,Y(\u03c4) and SX,Y(\u03c9).\n(b) Find RY(\u03c4).\n(c) Find SY(\u03c9).\nExercise 4. (Video Solution)\nLetX(t) be a zero-mean WSS process with autocorrelation function RX(\u03c4). Let Y(t) =\nX(t) cos( \u03c9t+ \u0398), where \u0398 \u223cuniform( \u2212\u03c0, \u03c0) and \u0398 is independent of the process X(t).\n(a) Find the autocorrelation function RY(\u03c4).\n(b) Find the cross-correlation function of X(t) and Y(t).\n(c) Is Y(t) WSS? Why or why not?\nExercise 5. (Video Solution)\nA WSS process X(t) with autocorrelation function\nRX(\u03c4) = 1 /(1 +\u03c42)\nis passed through an LTI system with impulse response\nh(t) = 3 sin( \u03c0t)/(\u03c0t).\nLetY(t) be the system output. Find SY(\u03c9) and sketch SY(\u03c9).\nExercise 6. (Video Solution)\nA white noise X(t) with power spectral density SX(\u03c9) =N0/2 is applied to a lowpass filter\nh(t) with impulse response\nh(t) =1\nRCe\u2212t/RC, t > 0. (10.60)\nFind the followings.\n677", "693": "CHAPTER 10. RANDOM PROCESSES\n(a)SXY(\u03c9).\n(b)RXY(\u03c4).\n(c)SY(\u03c9).\n(d)RY(\u03c4).\nExercise 7. (Video Solution)\nConsider a WSS process X(t) with autocorrelation function\nRX(\u03c4) = sinc( \u03c0\u03c4).\nThe process is sent to an LTI system with input-output relationship\n2d2\ndt2Y(t) + 2d\ndtY(t) + 4Y(t) = 3d2\ndt2X(t)\u22123d\ndtX(t) + 6X(t).\nFind the autocorrelation function RY(\u03c4).\nExercise 8. (Video Solution)\nGiven the functions a(t),b(t) and c(t), let\ng(t,1) = a(t),\ng(t,2) = b(t),\ng(t,3) = c(t).\nLetX(t) = g(t, Z), where Zis a discrete random variable with PMF P[Z= 1] = p1,\nP[Z= 2] = p2andP[Z= 3] = p3. Find, in terms of the p1,p2,p3,a(t),b(t) and c(t),\n(a)\u00b5X(t).\n(b)RX(t1, t2).\nExercise 9.\nIn the previous problem, let a(t) =e\u2212\u03bb|t|,b(t) = sin( \u03c0t) and c(t) =\u22121.\n(a) Choose p1,p2,p3so that X(t) is WSS.\n(b) Choose p1,p2,p3so that X(t) is not WSS.\nExercise 10. (Video Solution)\nFind the autocorrelation function RX(\u03c4) corresponding to each of the following power spec-\ntral densities:\n(a)\u03b4(\u03c9\u2212\u03c90) +\u03b4(\u03c9+\u03c90).\n(b)e\u2212\u03c92/2.\n(c)e\u2212|\u03c9|.\n678", "694": "10.10. PROBLEMS\nExercise 11. (Video Solution)\nA WSS process X(t) with autocorrelation function RX(\u03c4) =e\u2212\u03c42/(2\u03c32\nT)is passed through\nan LTI system with transfer function H(\u03c9) =e\u2212\u03c92/(2\u03c32\nH). Denote the system output by\nY(t). Find the followings.\n(a)SXY(\u03c9).\n(b)RXY(\u03c4).\n(c)SY(\u03c9).\n(d)RY(\u03c4).\nExercise 12. (Video Solution)\nA white noise X(t) with power spectral density SX(\u03c9) =N0/2 is applied to a lowpass filter\nh(t) with\nH(\u03c9) =(\n1\u2212\u03c92,if|\u03c9| \u2264\u03c0,\n0, otherwise .\nFindE[|Y(t)|2], where Y(t) is the output of the filter.\nExercise 13. (Video Solution)\nLetX(t) be a WSS process with correlation function\nRX(\u03c4) =(\n1\u2212 |\u03c4|,if\u22121\u2264\u03c4\u22641,\n0, otherwise .(10.61)\nIt is known that when X(t) is input to a system with transfer function H(\u03c9), the system\noutput Y(t) has a correlation function\nRY(\u03c4) =sin\u03c0\u03c4\n\u03c0\u03c4. (10.62)\nFind the transfer function H(\u03c9).\nExercise 14.\nConsider the system\nY(t) =e\u2212tZt\n\u2212\u221ee\u03c4X(\u03c4)d\u03c4.\nAssume that X(t) is zero-mean white noise with power spectral density SX(\u03c9) =N0/2.\nFind the followings:\n(a)SXY(\u03c9).\n(b)RXY(\u03c4).\n(c)SY(\u03c9).\n(d)RY(\u03c4).\n679", "695": "CHAPTER 10. RANDOM PROCESSES\n680", "696": "Chapter A\nAppendix\nUseful Identities\n1.\u221eP\nk=0rk= 1 + r+r2+\u00b7\u00b7\u00b7=1\n1\u2212r\n2.nP\nk=1k= 1 + 2 + 3 + \u00b7\u00b7\u00b7+n=n(n+1)\n2\n3.ex=\u221eP\nk=0xk\nk!= 1 +x\n1!+x2\n2!+\u00b7\u00b7\u00b7\n4.\u221eP\nk=1krk\u22121= 1 + 2 r+ 3r2+\u00b7\u00b7\u00b7=1\n(1\u2212r)2\n5.nP\nk=1k2= 12+ 22+ 33+\u00b7\u00b7\u00b7+n2=n3\n3+n2\n2+n\n6\n6. (a+b)n=nP\nk=0\u0000n\nk\u0001\nakbn\u2212k\nCommon Distributions\nDistribution PMF / PDF E[X] Var[ X] MX(s)\nBernoulli pX(1) = pandpX(0) = 1 \u2212p p p (1\u2212p) 1 \u2212p+pes\nBinomial pX(k) =\u0000n\nk\u0001\npk(1\u2212p)n\u2212knp np (1\u2212p) (1 \u2212p+pes)n\nGeometric pX(k) =p(1\u2212p)k\u221211\np1\u2212p\np2pes\n1\u2212(1\u2212p)es\nPoisson pX(k) =\u03bbke\u2212\u03bb\nk!\u03bb \u03bb e\u03bb(es\u22121)\nGaussian fX(x) =1\u221a\n2\u03c0\u03c32exp\u001a\n\u2212(x\u2212\u00b5)2\n2\u03c32\u001b\n\u00b5 \u03c32exp\u001a\n\u00b5s+\u03c32s2\n2\u001b\nExponential fX(x) =\u03bbexp{\u2212\u03bbx}1\n\u03bb1\n\u03bb2\u03bb\n\u03bb\u2212s\nUniform fX(x) =1\nb\u2212aa+b\n2(b\u2212a)2\n12esb\u2212esa\ns(b\u2212a)\n681", "697": "CHAPTER A. APPENDIX\nSum of Two Random Variables\nX1 X2 Sum X1+X2\nBernoulli( p) Bernoulli( p) Binomial(2 , p)\nBinomial( n, p) Binomial( m, p) Binomial( m+n, p)\nPoisson( \u03bb1) Poisson( \u03bb2) Poisson( \u03bb1+\u03bb2)\nExponential( \u03bb) Exponential( \u03bb) Erlang(2 , \u03bb)\nGaussian( \u00b51, \u03c32\n1) Gaussian( \u00b52, \u03c32\n2) Gaussian( \u00b51+\u00b52, \u03c32\n1+\u03c32\n2)\nFourier Transform Table\nF(\u03c9) =Z\u221e\n\u2212\u221ef(t)e\u2212j\u03c9tdt.\nf(t)\u2190\u2192F(\u03c9) f(t)\u2190\u2192F(\u03c9)\n1. e\u2212atu(t)\u2190\u21921\na+j\u03c9,a >0 10. sinc2\u0012Wt\n2\u0013\n\u2190\u21922\u03c0\nW\u2206\u0010\u03c9\n2W\u0011\n2. eatu(\u2212t)\u2190\u21921\na\u2212j\u03c9,a >0 11. e\u2212atsin(\u03c90t)u(t)\u2190\u2192\u03c90\n(a+j\u03c9)2+\u03c92\n0,a >0\n3. e\u2212a|t|\u2190\u21922a\na2+\u03c92,a >0 12. e\u2212atcos(\u03c90t)u(t)\u2190\u2192a+j\u03c9\n(a+j\u03c9)2+\u03c92\n0,a >0\n4.a2\na2+t2\u2190\u2192\u03c0ae\u2212a|\u03c9|,a >0 13. expn\n\u2212t2\n2\u03c32o\n\u2190\u2192\u221a\n2\u03c0\u03c3expn\n\u2212\u03c32\u03c92\n2o\n5. te\u2212atu(t)\u2190\u21921\n(a+j\u03c9)2,a >0 14. \u03b4(t)\u2190\u21921\n6.tne\u2212atu(t)\u2190\u2192n!\n(a+j\u03c9)n+1,a >0 15. 1 \u2190\u21922\u03c0\u03b4(\u03c9)\n7. rect\u0012t\n\u03c4\u0013\n\u2190\u2192\u03c4sinc\u0010\u03c9\u03c4\n2\u0011\n16. \u03b4(t\u2212t0)\u2190\u2192e\u2212jwt0\n8. sinc( Wt)\u2190\u2192\u03c0\nWrect\u0010\u03c9\n2W\u0011\n17. ej\u03c90t\u2190\u21922\u03c0\u03b4(\u03c9\u2212\u03c90)\n9. \u2206\u0012t\n\u03c4\u0013\n\u2190\u2192\u03c4\n2sinc2\u0010\u03c9\u03c4\n4\u0011\n18. f(t)ej\u03c90t\u2190\u2192F(\u03c9\u2212\u03c90)\nSome definitions:\nsinc(t) =sin(t)\nt\nrect(t) =(\n1,\u22120.5\u2264t\u22640.5,\n0, otherwise .\n\u2206(t) =(\n1\u22122|t|,\u22120.5\u2264t\u22640.5,\n0, otherwise .\n682", "698": "Basic Trigonometric Identities\nej\u03b8= cos \u03b8+jsin\u03b8\nsin 2\u03b8= 2 sin \u03b8cos\u03b8\ncos 2\u03b8= 2 cos2\u03b8\u22121\ncosAcosB=1\n2(cos(A+B) + cos( A\u2212B))\nsinAsinB=\u22121\n2(cos(A+B)\u2212cos(A\u2212B))\nsinAcosB=1\n2(sin(A+B) + sin( A\u2212B))\ncosAsinB=1\n2(sin(A+B)\u2212sin(A\u2212B))\ncos(A+B) = cos AcosB\u2212sinAsinB\ncos(A\u2212B) = cos AcosB+ sin AsinB\nsin(A+B) = sin AcosB+ cos AsinB\nsin(A\u2212B) = sin AcosB\u2212cosAsinB\n683", "699": "Index\nabsolutely integrable, 183\nalmost sure convergence, 362\nautocorrelation function\n2D visualization, 622\ninterpretation, 623, 633\nLTI system, 644\nproperties, 632\ntemporal average, 635\ndefinition, 618\nMATLAB and Python, 625\nautocovariance function\ndefinition, 618\nrelation to autocorrelation function, 628\nautoregressive model, 406, 658\nlinear prediction, 658\nMATLAB and Python, 407, 659\nprediction, 660\nToeplitz, 659\nYule-Walker equation, 658\nBasel problem, 5\nbasis functions, 405\nBayes\u2019 theorem, 89\nconditional probability, 81\nlaw of total probability, 90\nBayesian, 43\nBernoulli random variable\ndefinition, 137\nMATLAB and Python, 137\nmaximum variance, 140\nproperties, 138\nbias-variance\naverage predictor, 433\nMATLAB and Python, 434\nnoise-free case, 430\nnoisy case, 433\ntrade off, 429\nbinomial random variable\nalternative definition, 148definition, 143\nMATLAB and Python, 144\nproperties, 146\nbinomial series, 6\nbinomial theorem, 6\nproof, 9\nbirthday paradox, 31, 321\nbootstrapping, 559\nbootstrapped distribution, 562\nconfidence interval, 559\ndefinition, 559\ndistribution of samples, 560\ninterpretation, 564\nMATLAB and Python, 565\nprocedure, 562\nstandard error, 565\nwhen to use, 560\nCauchy distribution, 331, 360\nCauchy-Schwarz inequality, 261, 335\nCentral Limit Theorem, 323, 366, 372, 381\nBerry-Esseen Theorem, 375\nexamples, 376\ninterpretation, 375\nlimitations, 378\nproof, 374\ncharacteristic function, 329\nalternative definition, 329\nFourier transform, 330\nChebyshev\u2019s inequality, 341\nproof, 342\nChernoff\u2019s bound, 343\ncompare with Chebyshev, 344\nChernoff, Herman, 343\ncombination, 34\nconcave function, 336\nconditional distribution\nconditional expectation, 275\nconditional PDF, 271\n684", "700": "INDEX\nconditional PMF, 267\nconditional probability, 81\nBayes\u2019 theorem, 89\ndefinition, 81\nindependence, 85\nproperties, 84\nratio, 81\nconfidence interval, 541\nbootstrapping, 559\ncritical value, 552\ndefinition, 546\ndistribution of estimator, 544\nestimator, 543\nexamples, 547\nhow to construct, 548\ninterpretation, 545\nmargin of error, 552\nMATLAB and Python, 550\nnumber of samples, 553\nproperties, 551\nstandard error, 551\nStudent\u2019s t-distribution, 554\nconjugate prior, 513\nconvergence in distribution, 367\nconvergence in probability, 356\nconvex function, 336\nconvex optimization\nCVXPY, 451\nconvolution, 220, 639\ncorrelation, 639\nfiltering, 639\ncorrelation, 633\nautocorrelation function, 618\nautocovariance function, 618\ncross-correlation function, 649\nconvolution, 639\ncorrelation coefficient\nMATLAB and Python, 265\nproperties, 263\ndefinition, 263\ncosine angle, 26\ncovariance, 261\ncovariance matrix, 289\nindependent, 289\ncross power spectral density, 651\ncross-correlation function\ncross-covariance function, 629\ndefinition, 629examples, 650\nthrough LTI systems, 649\ncross-covariance function, 629\ncross-correlation function, 629\ncumulative distribution function\ncontinuous, 186\ndiscrete, 121\nleft- and right-continuous, 190\nMATLAB and Python, 186\nproperties, 188\ndelta function, 178\ndiscrete cosine transform (DCT), 23\neigenvalues and eigenvectors, 295\nGaussian, 296\nMATLAB and Python, 296\nErd\u02dd os-R\u00b4 enyi graph, 140\nMATLAB and Python, 480\neven functions, 15\nevent, 61\nevent space, 61\nexpectation, 104\ncontinuous, 180\nproperties, 130, 182\ntransformation, 182\ncenter of mass, 127\ndiscrete, 125\nexistence, 130, 183\nexponential random variables\ndefinition, 205\nMATLAB and Python, 205\norigin, 207, 209\nproperties, 206\nexponential series, 12\nfield, 64\n\u03c3-field, 65\nBorel \u03c3-field, 65\nFourier transform, 644\ntable, 330\ncharacteristic function, 330\nfrequentist, 43\nFundamental Theorem of Calculus, 17\nchain rule, 19\nproof, 18\nGaussian random variables\nCDF, 214\n685", "701": "INDEX\ndefinition, 211\nMATLAB and Python, 212\norigin, 220\nproperties, 212\nstandard Gaussian, 213\ngeometric random variable\ndefinition, 149\nMATLAB and Python, 150\nproperties, 151\ngeometric sequence\nfinite, 4\ninfinite, 4\ngeometric series, 3\nfinite, 4\ninfinite, 4\nharmonic series, 5\nhistogram, 2, 113\nHoeffding\u2019s inequality, 348\nHoeffding lemma, 348\nproof, 348\nhypothesis testing\np-value test, 567, 571\nT-test, 574\nZ-test, 574\nalternative hypothesis, 566\ncritical level, 569\ncritical-value test, 567\ndefinition, 566\nMATLAB and Python, 568\nnull hypothesis, 566\nimpulse response, 643\nindependence, 85\nconditional probability, 88\nversus disjoint, 86\nindependent\nrandom variables, 251\nindependent and identically distributed (i.i.d.),\n253\nindicator function, 182\ninner product, 24\nMATLAB and Python, 24\nJensen\u2019s inequality, 336\nproof, 338\njoint distribution\ndefinition, 241\njoint CDF, 255joint PDF, 247\njoint PMF, 245\njoint expectation, 257\ncosine angle, 258\nkurtosis, 216\nMATLAB and Python, 217\nLaplace transform, 324\nlaw of large numbers, 323, 351, 381\nstrong law of large numbers, 360\nweak law of large numbers, 354\nlearning curve, 427\nMATLAB and Python, 427\nLegendre polynomial, 403\nMATLAB and Python, 404\nlikelihood, 466, 468, 503\nlog-likelihood, 470\nlinear algebra\nbasis vector, 23\nrepresentation, 23\nspan, 22\nstandard basis vector, 22\nlinear combination, 21\nlinear model, 21\nlinear prediction, 658\nlinear programming, 414\nlinear regression\nMATLAB and Python, 30\nlinear time-invariant (LTI)\nconvolution, 639\ndefinition, 643\nsystem, 643\nmarginal distribution, 250\nMarkov\u2019s inequality, 339\nproof, 339\ntight, 341\nmatrix calculus, 28\nmaximum-a-posteriori (MAP), 502\nchoosing prior, 505\nconjugate prior, 513\nMAP versus LASSO, 519\nMAP versus ML, 504\nMAP versus regression, 517\nMAP versus ridge, 518\nposterior, 503, 511\nprior, 503\nsolution, 506\n686", "702": "INDEX\nmaximum-likelihood\n1D Gaussian, 484\nconsistent estimator, 494\nestimation, 468\nestimator, 491\nhigh-dimensional Gaussian, 486\nimage reconstruction, 481\nindependent observations, 469\ninvariance principle, 500\nMATLAB and Python, 472\nnumber of training samples, 475\nPoisson, 485\nregression versus ML, 487\nsocial networks, 478\nunbiased estimator, 492\nvisualization, 471\nmean, 199\nmean function\nLTI system, 644\ndefinition, 618\nMATLAB and Python, 621\nmean squared error (MSE), 520, 522\nmeasure, 68\nalmost surely, 73\nfinite sets, 68\nintervals, 68\nLebesgue integration, 71\nmeasure zero sets, 71\ndefinition, 72\nexamples, 72\nregions, 68\nsize, 69\nmedian, 196\nminimum mean-square estimation (MMSE),\n520\nconditional expectation, 523\nGaussian, 529\nminimum-norm least squares, 411\nmode, 198\nmodel selection, 165\nmoment, 133\ncontinuous case, 184\nmoment-generating function, 322, 324\ncommon distributions, 326\nderivative, 325\nexistence, 331\nsum of random variables, 327\nmultidimensional Gaussian, 290MATLAB and Python, 291\ncovariance, 293\ntransformation, 293\nwhitening, 299\nNeyman-Pearson test, 577\ndecision rule, 582\nlikelihood ratio, 584\nrejection zone, 578\nlikelihood ratio test, 578\nnorm, 24, 26\n\u21131, 27\n\u2113\u221e, 27\nMATLAB and Python, 26\nweighted, 27\nnormalization property, 112\nodd functions, 15\nopen and closed intervals, 45\noptimal linear filter, 653\ndeconvolution, 665\ndenoising, 662\northogonality condition, 658\nWiener filter, 661\nYule-Walker equation, 656\ninput function, 654\nprediction, 654\ntarget function, 654\northogonality condition, 658\noverdetermined system, 409\noverfitting, 418\nfactors, 420\nLASSO, 454\nlinear analysis, 425\nsource, 429\nparameter estimation, 165, 465\nPascal triangle, 8\nPascal\u2019s identity, 7\nperformance guarantee\naverage case, 321\nworst case, 321\npermutation, 33\nPoisson random variable\napplications, 154\ndefinition, 152\norigin, 157\nphoton arrivals, 161\nPoisson approximation of binomial, 159\n687", "703": "INDEX\nproperties, 155\nMATLAB and Python, 152\npositive semi-definite, 297\nposterior, 466, 503\npower spectral density, 636\nEinstein-Wiener-Khinchin Theorem, 636\nthrough LTI systems, 646\ncross power spectral density, 640, 651\neigendecomposition, 639\nFourier transform, 640\norigin, 640\nwide-sense stationary, 639\nPR (precision-recall) curve\ndefinition, 601\nMATLAB and Python, 603\nprecision, 601\nrecall, 601\nprincipal-component analysis, 303\nlimitations, 311\nmain idea, 303\nMATLAB and Python, 306\nprior, 466, 503\nprobability, 43, 45\nmeasure of a set, 43\nprobability axioms, 74\nadditivity, 75\ncorollaries, 77\ncountable additivity, 75\nmeasure, 76\nnon-negativity, 75\nnormalization, 75\nprobability density function, 172\ndefinition, 175\ndiscrete cases, 178\nproperties, 174\nintuition, 172\nper unit length, 173\nprobability inequality, 323, 333\nprobability law, 66\ndefinition, 66\nexamples, 66\nmeasure, 67\nprobability mass function, 104, 110\nprobability space\n(\u2126,F,P), 58\nRademacher random variable, 140\nrandom number generator, 229random process\ndiscrete time, 653\ndefinition, 612\nexample\nrandom amplitude, 612\nrandom phase, 613\nfunction, 612\nindependent, 629\nindex, 612\nsample space, 614\nstatistical average, 614\ntemporal average, 614\nuncorrelated, 630\nrandom variable, 104, 105\nfunction of, 223\ntransformation of, 223\nrandom vector, 286\nexpectation, 288\nindependent, 286\nregression, 391, 394\nloss, 394\nMATLAB and Python, 400\noutliers, 412\nprediction model, 394\nsolution, 397\nlinear model, 395\noutliers, 417\nsquared error, 396\nregularization, 440\nLASSO, 449\nMATLAB and Python, 442\nparameter, 445\nridge, 440\nsparse solution, 449\nrobust linear regression, 412\nMATLAB and Python, 416\nlinear programming, 414\nROC\ncomparing performance, 597\ncomputation, 592\ndefinition, 589\nMATLAB and Python, 593\nproperties, 591\nReceiver operating characteristic, 589\nsample average, 320, 351\nsample space, 59\ncontinuous outcomes, 59\n688", "704": "INDEX\ncounterexamples, 61\ndiscrete outcomes, 59\nexamples, 59\nexclusive, 61\nexhaustive, 61\nfunctions, 59\nset, 45\nassociative, 56\ncommutative, 56\ncomplement, 52\ncountable, 45\nDe Morgan\u2019s Law, 57\ndifference, 53\ndisjoint, 54\ndistributive, 56\nempty set, 48\nfinite, 45\nimproper subset, 47\ninfinite, 45\nintersection, 50\nfinite, 50\ninfinite, 51\nof functions, 46\npartition, 55\nproper subset, 47\nsubset, 47\nuncountable, 45\nunion, 48\nfinite, 48\ninfinite, 49\nuniversal set, 48\nsimplex method, 414\nskewness, 216\nMATLAB and Python, 217\nstatistic, 320\nStudent\u2019s t-distribution\ndefinition, 554\ndegrees of freedom, 555\nMATLAB and Python, 556\nrelation to Gaussian, 555\nsum of random variables, 280\nBernoulli, 327\nbinomial, 328\nGaussian, 283, 329\nPoisson, 328\ncommon distributions, 282\nconvolution, 281\nsymmetric matrices, 296Taylor approximation, 11\nfirst-order, 11\nsecond-order, 11\nexponential, 12\nlogarithmic, 13\ntesting error, 420\nanalysis, 424\ntesting set, 420\nThree Prisoners problem, 92\nToeplitz, 407, 630\ntraining error, 420\nanalysis, 421\ntraining set, 420\ntype 1 error\ndefinition, 579\nfalse alarm, 580\nfalse positive, 579\npower of test, 581\ntype 2 error\ndefinition, 579\nfalse negative, 579\nmiss, 580\nunderdetermined system, 409\nuniform random variables, 202\nMATLAB and Python, 203\nunion bound, 333\nvalidation, 165\nvariance, 134\nproperties, 135\ncontinuous case, 184\nwhite noise, 638\nwide-sense stationary, 630\njointly, 649\nWiener filter, 661\ndeconvolution, 665\ndefinition, 661\ndenoising, 662\nMATLAB and Python, 661\npower spectral density, 662\nrecursive filter, 661\nYule-Walker equation, 656\nMATLAB and Python, 659\n689", "705": "", "706": "", "707": "", "708": ""}}