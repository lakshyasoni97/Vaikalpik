{"1": {"Title": "Introduction", "Sub Topics": 
	{"1.1": {"Title": "Machine learning: what and why?", 
		"Sub Topics": 
		{"1.1.1": {"Title": "Types of machine learning"}}}, 
	"1.2": {"Title": "Supervised learning", "Sub Topics": 
		{"1.2.1": {"Title": "Classi\ufb01cation"}, 
		"1.2.2": {"Title": "Regression"}}}, 
	"1.3": {"Title": "Unsupervised learning", 
		"Sub Topics": 
		{"1.3.1": {"Title": "Discovering clusters"}, 
		"1.3.2": {"Title": "Discovering latent factors"},
		"1.3.3": {"Title": "Discovering graph structure"}, 
		"1.3.4": {"Title": "Matrix completion"}}}, 
	"1.4": {"Title": "Some basic concepts in machine learning", 
		"Sub Topics": 
		{"1.4.1": {"Title": "Parametric vs non-parametric models"},
		"1.4.2": {"Title": "A simple non-parametric classifier: K-nearest neighbors"}, 
		"1.4.3": {"Title": "The curse of dimensionality"},
		"1.4.4": {"Title": "Parametric models for classification and regression"}, 
		"1.4.5": {"Title": "Linear regression"},
		"1.4.6": {"Title": "Logistic regression"},
		"1.4.7": {"Title": "Overfitting"},
		"1.4.8": {"Title": "Model selection"},
		"1.4.9": {"Title": "No free lunch theorem"}}}}}, 
		"2": {"Title": "Probability", 
			"Sub Topics": 
			{"2.1": {"Title": "Introduction", "Sub Topics": {}}, 
			"2.2": {"Title": "A brief review of probability theory", 
				"Sub Topics": 
				{"2.2.1": {"Title": "Discrete random variables"}, 
				"2.2.2": {"Title": "Fundamental rules"},
				"2.2.3": {"Title": "Bayes rule"},
				"2.2.4": {"Title": "Independence and conditional independence"},
				"2.2.5": {"Title": "Continuous random variables"}, 
				"2.2.6": {"Title": "Quantiles"}, 
				"2.2.7": {"Title": "Mean and variance"}}}, 
			"2.3": {"Title": "Some common discrete distributions", 
				"Sub Topics": 
				{"2.3.1": {"Title": "The binomial and Bernoulli distributions"}, 
				"2.3.2": {"Title": "The multinomial and multinoulli distributions"},
				"2.3.3": {"Title": "The Poisson distribution"},
				"2.3.4": {"Title": "The empirical distribution"}}},
				"2.4": {"Title": "Some common continuous distributions", 
				"Sub Topics": 
				{"2.4.1": {"Title": "Gaussian (normal) distribution"},
				 "2.4.2": {"Title": "Degenerate pdf"}, 
				"2.4.3": {"Title": "The Laplace distribution"},
				"2.4.4": {"Title": "The gamma distribution"}, 
				"2.4.5": {"Title": "The beta distribution"},
				"2.4.6": {"Title": "Pareto distribution"},}}, 
			"2.5": {"Title": "Joint probability distributions", 
				"Sub Topics": {
					"2.5.1": {"Title": "Covariance and correlation"}, 
					"2.5.2": {"Title": "The multivariate Gaussian"},
					"2.5.3": {"Title": "Multivariate Student t distribution"},
					"2.5.4": {"Title": "Dirichlet distribution"}}}, 
			"2.6": {"Title": "Transformations of random variables", 
				"Sub Topics": 
				{"2.6.1": {"Title": "Linear transformations"},
				"2.6.2": {"Title": "General transformations"},
				"2.6.3": {"Title": "Central limit theorem"},}}, 
			"2.7": {"Title": "Monte Carlo approximation", 
				"Sub Topics": 
				{"2.7.1": {"Title": "Example: change of variables, the MC way"}, 
				"2.7.2": {"Title": "Example: estimating Ï€ by Monte Carlo integration"},
				"2.7.3": {"Title": "Accuracy of Monte Carlo approximation"}}}, 
			"2.8": {"Title": "Information theory", 
				"Sub Topics": 
				{"2.8.1": {"Title": "Entropy"},
				 "2.8.2": {"Title": "KL divergence 57283 Mutual information"},
				 "2.8.3": {"Title": "Mutual information"},
				}}}}, 
		"3": {"Title": "Generative models for discrete data", 
			"Sub Topics": 
			{"3.1": {"Title": "Introduction", "Sub Topics": {}}, 
			"3.2": {"Title": "Bayesian concept learning", 
				"Sub Topics": 
				{"3.2.1": {"Title": "Likelihood"},
				 "3.2.2": {"Title": "Prior"},
				 "3.2.3": {"Title": "Posterior"},
				 "3.2.4": {"Title": "Posterior predictive distribution"},
				 "3.2.5": {"Title": "A more complex prior"},
				}}, 
			"3.3": {"Title": "The beta-binomial model", 
			"Sub Topics": 
			{"3.3.1": {"Title": "Likelihood"},
			"3.3.2": {"Title": "Prior"},
			"3.3.3": {"Title": "Posterior"}, 
			"3.3.4": {"Title": "Posterior predictive distribution"}}}, 
			"3.4": {"Title": "The Dirichlet-multinomial model", 
				"Sub Topics": 
				{"3.4.1": {"Title": "Likelihood 79342 Prior 79343 Posterior 79344 Posterior predictive"}}}, 
			"3.5": {"Title": "Naive Bayes classi\ufb01ers", 
				"Sub Topics": 
				{"3.5.1": {"Title": "Model fitting"},
				"3.5.2": {"Title": "Using the model for prediction"},
				"3.5.3": {"Title": "The log-sum-exp trick"},
				"3.5.4": {"Title": "Feature selection using mutual information"},
				"3.5.5": {"Title": "Classifying documents using bag of words"},}}}}, 
				"4": {"Title": "Gaussian models", 
					"Sub Topics": 
					{"4.1": {"Title": "Introduction", 
					"Sub Topics": 
					{"4.1.1": {"Title": "Notation"},
					"4.1.2": {"Title": "Basics"},
					"4.1.3": {"Title": "MLE for an MVN"},
					"4.1.4": {"Title": "Maximum entropy derivation of the Gaussian *"}}}, 
					"4.2": {"Title": "Gaussian discriminant analysis", 
					"Sub Topics": 
					{"4.2.1": {"Title": "Quadratic discriminant analysis (QDA) 102422 Linear discriminant analysis (LDA) 103423 Two-class LDA 104424 MLE for discriminant analysis 106425 Strategies for preventing over\ufb01tting 106426 Regularized LDA * 107427 Diagonal LDA"}, 
					"4.2.8": {"Title": "Nearest shrunken centroids classi\ufb01er *"}}}, "4.3": {"Title": "Inference in jointly Gaussian distributions", "Sub Topics": {"4.3.1": {"Title": "Statement of the result 111432 Examples 111433 Information form 115434 Proof of the result *"}}}, "4.4": {"Title": "Linear Gaussian systems", "Sub Topics": {"4.4.1": {"Title": "Statement of the result 119442 Examples"}, "4.4.3": {"Title": "Proof of the result *"}}}, "4.5": {"Title": "Digression: The Wishart distribution *", "Sub Topics": {"4.5.1": {"Title": "Inverse Wishart distribution 126452 Visualizing the Wishart distribution *"}}}, "4.6": {"Title": "Inferring the parameters of an MVN", "Sub Topics": {"4.6.2": {"Title": "Posterior distribution of \u03a3*"}, "4.6.3": {"Title": "Posterior distribution of \u03bcand\u03a3*"}, "4.6.4": {"Title": "Sensor fusion with unknown precisions *"}}}}}, "5": {"Title": "Bayesian statistics", "Sub Topics": {"5.1": {"Title": "Introduction", "Sub Topics": {}}, "5.2": {"Title": "Summarizing posterior distributions", "Sub Topics": {"5.2.1": {"Title": "MAP estimation 149522 Credible intervals"}, "5.2.3": {"Title": "Inference for a difference in proportions"}}}, "5.3": {"Title": "Bayesian model selection", "Sub Topics": {"5.3.1": {"Title": "Bayesian Occam\u2019s razor 156532 Computing the marginal likelihood (evidence) 158533 Bayes factors 163534 Jeffreys-Lindley paradox *"}}}, "5.4": {"Title": "Priors", "Sub Topics": {"5.4.1": {"Title": "Uninformative priors 165542 Jeffreys priors * 166543 Robust priors 168544 Mixtures of conjugate priors"}}}, "5.5": {"Title": "Hierarchical Bayes", "Sub Topics": {"5.5.1": {"Title": "Example: modeling related cancer rates"}}}, "5.6": {"Title": "Empirical Bayes", "Sub Topics": {"5.6.1": {"Title": "Example: beta-binomial model 173562 Example: Gaussian-Gaussian model"}}}, "5.7": {"Title": "Bayesian decision theory", "Sub Topics": {"5.7.1": {"Title": "Bayes estimators for common loss functions 177572 The false positive vs false negative tradeoff"}, "5.7.3": {"Title": "Other topics *"}}}}}, "6": {"Title": "Frequentist statistics", "Sub Topics": {"6.1": {"Title": "Introduction", "Sub Topics": {}}, "6.2": {"Title": "Sampling distribution of an estimator", "Sub Topics": {"6.2.1": {"Title": "Bootstrap"}, "6.2.2": {"Title": "Large sample theory for the MLE *"}}}, "6.3": {"Title": "Frequentist decision theory", "Sub Topics": {"6.3.1": {"Title": "Bayes risk 195632 Minimax risk 196633 Admissible estimators"}}}, "6.4": {"Title": "Desirable properties of estimators", "Sub Topics": {"6.4.1": {"Title": "Consistent estimators 200642 Unbiased estimators 200643 Minimum variance estimators"}, "6.4.4": {"Title": "The bias-variance tradeoff"}}}, "6.5": {"Title": "Empirical risk minimization", "Sub Topics": {"6.5.1": {"Title": "Regularized risk minimization 205652 Structural risk minimization"}, "6.5.3": {"Title": "Estimating the risk using cross validation"}, "6.5.4": {"Title": "Upper bounding the risk using statistical learning theory *"}, "6.5.5": {"Title": "Surrogate loss functions"}}}, "6.6": {"Title": "Pathologies of frequentist statistics *", "Sub Topics": {"6.6.1": {"Title": "Counter-intuitive behavior of con\ufb01dence intervals 212662 p-values considered harmful 213663 The likelihood principle 214664 Why isn\u2019t everyone a Bayesian?"}}}}}, "7": {"Title": "Linear regression", "Sub Topics": {"7.1": {"Title": "Introduction", "Sub Topics": {}}, "7.2": {"Title": "Model speci\ufb01cation 21773 Maximum likelihood estimation (least squares)", "Sub Topics": {}}, "7.4": {"Title": "Robust linear regression * 22375 Ridge regression", "Sub Topics": {}}, "7.6": {"Title": "Bayesian linear regression", "Sub Topics": {"7.6.1": {"Title": "Computing the posterior 232762 Computing the posterior predictive"}, "7.6.4": {"Title": "EB for linear regression (evidence procedure)"}}}}}, "8": {"Title": "Logistic regression", "Sub Topics": {"8.1": {"Title": "Introduction", "Sub Topics": {}}, "8.2": {"Title": "Model speci\ufb01cation 24583 Model \ufb01tting", "Sub Topics": {}}, "8.4": {"Title": "Bayesian logistic regression", "Sub Topics": {"8.4.1": {"Title": "Laplace approximation 255842 Derivation of the BIC"}, "8.4.3": {"Title": "Gaussian approximation for logistic regression 256844 Approximating the posterior predictive"}, "8.4.5": {"Title": "Residual analysis (outlier detection) *"}}}, "8.5": {"Title": "Online learning and stochastic optimization", "Sub Topics": {"8.5.1": {"Title": "Online learning and regret minimization"}, "8.5.2": {"Title": "Stochastic optimization and risk minimization"}, "8.5.3": {"Title": "The LMS algorithm 264854 The perceptron algorithm 265855 A Bayesian view"}}}, "8.6": {"Title": "Generative vs discriminative classi\ufb01ers", "Sub Topics": {"8.6.1": {"Title": "Pros and cons of each approach"}, "8.6.2": {"Title": "Dealing with missing data 269863 Fisher\u2019s linear discriminant analysis (FLDA) *"}}}}}, "9": {"Title": "Generalized linear models and the exponential family", "Sub Topics": {"9.1": {"Title": "Introduction", "Sub Topics": {}}, "9.2": {"Title": "The exponential family", "Sub Topics": {"9.2.1": {"Title": "De\ufb01nition 282922 Examples 282923 Log partition function 284924 MLE for the exponential family 286925 Bayes for the exponential family *"}, "9.2.6": {"Title": "Maximum entropy derivation of the exponential family *"}}}, "9.3": {"Title": "Generalized linear models (GLMs)", "Sub Topics": {"9.3.1": {"Title": "Basics 290932 ML and MAP estimation 292933 Bayesian inference"}}}, "9.4": {"Title": "Probit regression", "Sub Topics": {"9.4.1": {"Title": "ML/MAP estimation using gradient-based optimization 294942 Latent variable interpretation 294943 Ordinal probit regression * 295944 Multinomial probit models *"}}}, "9.5": {"Title": "Multi-task learning", "Sub Topics": {"9.5.1": {"Title": "Hierarchical Bayes for multi-task learning 296952 Application to personalized email spam \ufb01ltering 296953 Application to domain adaptation 297954 Other kinds of prior"}}}, "9.6": {"Title": "Generalized linear mixed models *", "Sub Topics": {"9.6.1": {"Title": "Example: semi-parametric GLMMs for medical data"}, "9.6.2": {"Title": "Computational issues"}}}, "9.7": {"Title": "Learning to rank *", "Sub Topics": {"9.7.1": {"Title": "The pointwise approach 301972 The pairwise approach 301973 The listwise approach 302974 Loss functions for ranking"}}}}}, "10": {"Title": "Directed graphical models (Bayes nets)", "Sub Topics": {"10.1": {"Title": "Introduction", "Sub Topics": {"10.1.1": {"Title": "Chain rule 3071012 Conditional independence"}, "10.1.3": {"Title": "Graphical models"}, "10.1.4": {"Title": "Graph terminology 3091015 Directed graphical models"}}}, "10.2": {"Title": "Examples", "Sub Topics": {"10.2.1": {"Title": "Naive Bayes classi\ufb01ers 3111022 Markov and hidden Markov models"}, "10.2.3": {"Title": "Medical diagnosis 3131024 Genetic linkage analysis *"}, "10.2.5": {"Title": "Directed Gaussian graphical models *"}}}, "10.3": {"Title": "Inference 319104 Learning", "Sub Topics": {}}, "10.5": {"Title": "Conditional independence properties of DGMs", "Sub Topics": {"10.5.2": {"Title": "Other Markov properties of DGMs 3271053 Markov blanket and full conditionals"}}}, "10.6": {"Title": "In\ufb02uence (decision) diagrams *", "Sub Topics": {}}}}, "11": {"Title": "Mixture models and the EM algorithm", "Sub Topics": {"11.1": {"Title": "Latent variable models", "Sub Topics": {}}, "11.2": {"Title": "Mixture models", "Sub Topics": {"11.2.1": {"Title": "Mixtures of Gaussians 3391122 Mixture of multinoullis 3401123 Using mixture models for clustering 3401124 Mixtures of experts"}}}, "11.3": {"Title": "Parameter estimation for mixture models", "Sub Topics": {"11.3.1": {"Title": "Unidenti\ufb01ability 3461132 Computing a MAP estimate is non-convex"}}}, "11.4": {"Title": "The EM algorithm", "Sub Topics": {"11.4.1": {"Title": "Basic idea 3491142 EM for GMMs 3501143 EM for mixture of experts 3571144 EM for DGMs with hidden variables 3581145 EM for the Student distribution * 3591146 EM for probit regression * 3621147 Theoretical basis for EM * 3631148 Online EM 3651149 Other EM variants *"}}}, "11.5": {"Title": "Model selection for latent variable models", "Sub Topics": {"11.5.1": {"Title": "Model selection for probabilistic models"}, "11.5.2": {"Title": "Model selection for non-probabilistic methods"}}}, "11.6": {"Title": "Fitting models with missing data", "Sub Topics": {"11.6.1": {"Title": "EM for the MLE of an MVN with missing data"}}}}}, "12": {"Title": "Latent linear models", "Sub Topics": {"12.1": {"Title": "Factor analysis", "Sub Topics": {"12.1.1": {"Title": "FA is a low rank parameterization of an MVN 3811212 Inference of the latent factors"}, "12.1.3": {"Title": "Unidenti\ufb01ability"}, "12.1.4": {"Title": "Mixtures of factor analysers"}, "12.1.5": {"Title": "EM for factor analysis models 3861216 Fitting FA models with missing data"}}}, "12.2": {"Title": "Principal components analysis (PCA)", "Sub Topics": {"12.2.1": {"Title": "Classical PCA: statement of the theorem 3871222 Proof * 3891223 Singular value decomposition (SVD) 3921224 Probabilistic PCA 3951225 EM algorithm for PCA"}}}, "12.3": {"Title": "Choosing the number of latent dimensions", "Sub Topics": {"12.3.1": {"Title": "Model selection for FA/PPCA"}, "12.3.2": {"Title": "Model selection for PCA"}}}, "12.4": {"Title": "PCA for categorical data 402125 PCA for paired and multi-view data", "Sub Topics": {}}, "12.6": {"Title": "Independent Component Analysis (ICA)", "Sub Topics": {"12.6.1": {"Title": "Maximum likelihood estimation 4101262 The FastICA algorithm 4111263 Using EM 4141264 Other estimation principles *"}}}}}, "13": {"Title": "Sparse linear models", "Sub Topics": {"13.1": {"Title": "Introduction", "Sub Topics": {}}, "13.2": {"Title": "Bayesian variable selection", "Sub Topics": {"13.2.1": {"Title": "The spike and slab model"}, "13.2.3": {"Title": "Algorithms"}}}, "13.6": {"Title": "Non-convex regularizers", "Sub Topics": {"13.6.1": {"Title": "Bridge regression 4581362 Hierarchical adaptive lasso 4581363 Other hierarchical priors"}}}, "13.7": {"Title": "Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)", "Sub Topics": {"13.7.1": {"Title": "ARD for linear regression 4631372 Whence sparsity? 4651373 Connection to MAP estimation"}, "13.7.4": {"Title": "Algorithms for ARD * 4661375 ARD for logistic regression"}}}, "13.8": {"Title": "Sparse coding *", "Sub Topics": {"13.8.1": {"Title": "Learning a sparse coding dictionary 4691382 Results of dictionary learning from image patches"}, "13.8.3": {"Title": "Compressed sensing 4721384 Image inpainting and denoising"}}}}}, "14": {"Title": "Kernels", "Sub Topics": {"14.1": {"Title": "Introduction", "Sub Topics": {}}, "14.2": {"Title": "Kernel functions", "Sub Topics": {"14.2.1": {"Title": "RBF kernels 4801422 Kernels for comparing documents 4801423 Mercer (positive de\ufb01nite) kernels 4811424 Linear kernels 4821425 Matern kernels 4821426 String kernels"}, "14.2.7": {"Title": "Pyramid match kernels"}, "14.2.8": {"Title": "Kernels derived from probabilistic generative models"}}}, "14.3": {"Title": "Using kernels inside GLMs", "Sub Topics": {"14.3.1": {"Title": "Kernel machines 4861432 L1VMs, RVMs, and other sparse vector machines"}}}, "14.4": {"Title": "The kernel trick", "Sub Topics": {"14.4.1": {"Title": "Kernelized nearest neighbor classi\ufb01cation 4891442 Kernelized K-medoids clustering"}, "14.4.3": {"Title": "Kernelized ridge regression"}, "14.4.4": {"Title": "Kernel PCA"}}}, "14.5": {"Title": "Support vector machines (SVMs)", "Sub Topics": {"14.5.1": {"Title": "SVMs for regression 4971452 SVMs for classi\ufb01cation"}, "14.5.4": {"Title": "Summary of key points 5041455 A probabilistic interpretation of SVMs"}}}, "14.6": {"Title": "Comparison of discriminative kernel methods 505147 Kernels for building generative models", "Sub Topics": {}}}}, "15": {"Title": "Gaussian processes", "Sub Topics": {"15.1": {"Title": "Introduction", "Sub Topics": {}}, "15.2": {"Title": "GPs for regression", "Sub Topics": {"15.2.1": {"Title": "Predictions using noise-free observations 5171522 Predictions using noisy observations 5181523 Effect of the kernel parameters 5191524 Estimating the kernel parameters 5211525 Computational and numerical issues * 5241526 Semi-parametric GPs *"}}}, "15.3": {"Title": "GPs meet GLMs", "Sub Topics": {"15.3.1": {"Title": "Binary classi\ufb01cation 5251532 Multi-class classi\ufb01cation 5281533 GPs for Poisson regression"}}}, "15.4": {"Title": "Connection with other methods", "Sub Topics": {"15.4.1": {"Title": "Linear models compared to GPs"}, "15.4.2": {"Title": "Linear smoothers compared to GPs 5331543 SVMs compared to GPs"}, "15.4.4": {"Title": "L1VM and RVMs compared to GPs 5341545 Neural networks compared to GPs 5351546 Smoothing splines compared to GPs * 5361547 RKHS methods compared to GPs *"}}}, "15.5": {"Title": "GP latent variable model 540156 Approximation methods for large datasets", "Sub Topics": {}}}}, "16": {"Title": "Adaptive basis function models", "Sub Topics": {"16.1": {"Title": "Introduction", "Sub Topics": {}}, "16.2": {"Title": "Classi\ufb01cation and regression trees (CART)", "Sub Topics": {"16.2.1": {"Title": "Basics 5441622 Growing a tree 5451623 Pruning a tree 5491624 Pros and cons of trees 5501625 Random forests 5501626 CART compared to hierarchical mixture of experts *"}}}, "16.3": {"Title": "Generalized additive models", "Sub Topics": {"16.3.1": {"Title": "Back\ufb01tting"}, "16.3.2": {"Title": "Computational efficiency 5531633 Multivariate adaptive regression splines (MARS)"}}}, "16.4": {"Title": "Boosting", "Sub Topics": {"16.4.1": {"Title": "Forward stagewise additive modeling 5551642 L2boosting"}, "16.4.3": {"Title": "AdaBoost 5581644 LogitBoost"}, "16.4.5": {"Title": "Boosting as functional gradient descent 5601646 Sparse boosting 5611647 Multivariate adaptive regression trees (MART) 5621648 Why does boosting work so well?"}, "16.4.9": {"Title": "A Bayesian view"}}}, "16.5": {"Title": "Feedforward neural networks (multilayer perceptrons)", "Sub Topics": {"16.5.1": {"Title": "Convolutional neural networks 5641652 Other kinds of neural networks 5681653 A brief history of the \ufb01eld 5681654 The backpropagation algorithm"}, "16.5.5": {"Title": "Identi\ufb01ability 5721656 Regularization"}, "16.5.7": {"Title": "Bayesian inference *"}}}, "16.6": {"Title": "Ensemble learning", "Sub Topics": {"16.6.1": {"Title": "Stacking 5801662 Error-correcting output codes"}, "16.6.3": {"Title": "Ensemble learning is not equivalent to Bayes model averaging"}}}, "16.7": {"Title": "Experimental comparison", "Sub Topics": {"16.7.1": {"Title": "Low-dimensional features 5821672 High-dimensional features"}}}, "16.8": {"Title": "Interpreting black-box models", "Sub Topics": {}}}}, "17": {"Title": "Markov and hidden Markov models", "Sub Topics": {"17.1": {"Title": "Introduction", "Sub Topics": {}}, "17.2": {"Title": "Markov models", "Sub Topics": {"17.2.1": {"Title": "Transition matrix"}, "17.2.2": {"Title": "Application: Language modeling"}, "17.2.3": {"Title": "Stationary distribution of a Markov chain * 5961724 Application: Google\u2019s PageRank algorithm for web page ranking *"}}}, "17.3": {"Title": "Hidden Markov models", "Sub Topics": {"17.3.1": {"Title": "Applications of HMMs"}}}, "17.4": {"Title": "Inference in HMMs", "Sub Topics": {"17.4.1": {"Title": "Types of inference problems for temporal models 6061742 The forwards algorithm 6091743 The forwards-backwards algorithm 6101744 The Viterbi algorithm"}, "17.4.5": {"Title": "Forwards \ufb01ltering, backwards sampling"}}}, "17.5": {"Title": "Learning for HMMs", "Sub Topics": {"17.5.1": {"Title": "Training with fully observed data 6171752 EM for HMMs (the Baum-Welch algorithm) 6181753 Bayesian methods for \u201c\ufb01tting\u201d HMMs * 6201754 Discriminative training"}, "17.5.5": {"Title": "Model selection"}}}, "17.6": {"Title": "Generalizations of HMMs", "Sub Topics": {"17.6.1": {"Title": "Variable duration (semi-Markov) HMMs 6221762 Hierarchical HMMs 6241763 Input-output HMMs 6251764 Auto-regressive and buried HMMs 6261765 Factorial HMM 6271766 Coupled HMM and the in\ufb02uence model 6281767 Dynamic Bayesian networks (DBNs)"}}}}}, "18": {"Title": "State space models", "Sub Topics": {"18.1": {"Title": "Introduction", "Sub Topics": {}}, "18.2": {"Title": "Applications of SSMs", "Sub Topics": {"18.2.1": {"Title": "SSMs for object tracking 6321822 Robotic SLAM 6331823 Online parameter learning using recursive least squares 6361824 SSM for time series forecasting *"}}}, "18.3": {"Title": "Inference in LG-SSM", "Sub Topics": {"18.3.1": {"Title": "The Kalman \ufb01ltering algorithm 6401832 The Kalman smoothing algorithm"}}}, "18.4": {"Title": "Learning for LG-SSM", "Sub Topics": {"18.4.1": {"Title": "Identi\ufb01ability and numerical stability 6461842 Training with fully observed data"}, "18.4.3": {"Title": "EM for LG-SSM 6471844 Subspace methods 6471845 Bayesian methods for \u201c\ufb01tting\u201d LG-SSMs"}}}, "18.5": {"Title": "Approximate online inference for non-linear, non-Gaussian SSMs", "Sub Topics": {"18.5.1": {"Title": "Extended Kalman \ufb01lter (EKF) 6481852 Unscented Kalman \ufb01lter (UKF) 6501853 Assumed density \ufb01ltering (ADF)"}}}, "18.6": {"Title": "Hybrid discrete/continuous SSMs", "Sub Topics": {"18.6.1": {"Title": "Inference 6561862 Application: data association and multi-target tracking 6581863 Application: fault diagnosis 6591864 Application: econometric forecasting"}}}}}, "19": {"Title": "Undirected graphical models (Markov random \ufb01elds)", "Sub Topics": {"19.1": {"Title": "Introduction", "Sub Topics": {}}, "19.2": {"Title": "Conditional independence properties of UGMs", "Sub Topics": {"19.2.1": {"Title": "Key properties"}, "19.2.2": {"Title": "An undirected alternative to d-separation"}, "19.2.3": {"Title": "Comparing directed and undirected graphical models"}}}, "19.3": {"Title": "Parameterization of MRFs", "Sub Topics": {"19.3.1": {"Title": "The Hammersley-Clifford theorem 6651932 Representing potential functions"}}}, "19.4": {"Title": "Examples of MRFs", "Sub Topics": {"19.4.1": {"Title": "Ising model"}, "19.4.2": {"Title": "Hop\ufb01eld networks 6691943 Potts model 6711944 Gaussian MRFs 6721945 Markov logic networks *"}}}, "19.5": {"Title": "Learning", "Sub Topics": {"19.5.1": {"Title": "Training maxent models using gradient methods 6761952 Training partially observed maxent models 6771953 Approximate methods for computing the MLEs of MRFs 6781954 Pseudo likelihood"}, "19.5.5": {"Title": "Stochastic maximum likelihood 6791956 Feature induction for maxent models *"}, "19.5.7": {"Title": "Iterative proportional \ufb01tting (IPF) *"}}}, "19.6": {"Title": "Conditional random \ufb01elds (CRFs)", "Sub Topics": {"19.6.1": {"Title": "Chain-structured CRFs, MEMMs and the label-bias problem 6841962 Applications of CRFs 6861963 CRF training"}}}, "19.7": {"Title": "Structural SVMs", "Sub Topics": {"19.7.1": {"Title": "SSVMs: a probabilistic view"}, "19.7.2": {"Title": "SSVMs: a non-probabilistic view 6951973 Cutting plane methods for \ufb01tting SSVMs 6981974 Online algorithms for \ufb01tting SSVMs"}, "19.7.5": {"Title": "Latent structural SVMs"}}}}}, "20": {"Title": "Exact inference for graphical models", "Sub Topics": {"20.1": {"Title": "Introduction", "Sub Topics": {}}, "20.2": {"Title": "Belief propagation for trees", "Sub Topics": {"20.2.1": {"Title": "Serial protocol"}, "20.2.2": {"Title": "Parallel protocol"}, "20.2.3": {"Title": "Gaussian BP * 7102024 Other BP variants *"}}}, "20.3": {"Title": "The variable elimination algorithm", "Sub Topics": {"20.3.1": {"Title": "The generalized distributive law * 7172032 Computational complexity of VE 7172033 A weakness of VE"}}}, "20.4": {"Title": "The junction tree algorithm *", "Sub Topics": {"20.4.1": {"Title": "Creating a junction tree"}, "20.4.2": {"Title": "Message passing on a junction tree 7222043 Computational complexity of JTA"}, "20.4.4": {"Title": "JTA generalizations *"}}}, "20.5": {"Title": "Computational intractability of exact inference in the worst case", "Sub Topics": {"20.5.1": {"Title": "Approximate inference"}}}}}, "21": {"Title": "Variational inference", "Sub Topics": {"21.1": {"Title": "Introduction", "Sub Topics": {}}, "21.2": {"Title": "Variational inference", "Sub Topics": {"21.2.1": {"Title": "Alternative interpretations of the variational objective"}, "21.2.2": {"Title": "Forward or reverse KL? *"}}}, "21.3": {"Title": "The mean \ufb01eld method", "Sub Topics": {"21.3.1": {"Title": "Derivation of the mean \ufb01eld update equations 7362132 Example: mean \ufb01eld for the Ising model"}}}, "21.4": {"Title": "Structured mean \ufb01eld *", "Sub Topics": {"21.4.1": {"Title": "Example: factorial HMM"}}}, "21.5": {"Title": "Variational Bayes", "Sub Topics": {"21.5.1": {"Title": "Example: VB for a univariate Gaussian 7422152 Example: VB for linear regression"}}}, "21.6": {"Title": "Variational Bayes EM", "Sub Topics": {"21.6.1": {"Title": "Example: VBEM for mixtures of Gaussians *"}}}, "21.7": {"Title": "Variational message passing and VIBES 756218 Local variational bounds *", "Sub Topics": {}}}}, "22": {"Title": "More variational inference", "Sub Topics": {"22.1": {"Title": "Introduction", "Sub Topics": {}}, "22.2": {"Title": "Loopy belief propagation: algorithmic issues", "Sub Topics": {"22.2.1": {"Title": "A brief history 7672222 LBP on pairwise models 7682223 LBP on a factor graph"}, "22.2.4": {"Title": "Convergence 7712225 Accuracy of LBP"}, "22.2.6": {"Title": "Other speedup tricks for LBP *"}}}, "22.3": {"Title": "Loopy belief propagation: theoretical issues *", "Sub Topics": {"22.3.1": {"Title": "UGMs represented in exponential family form 7762232 The marginal polytope 7772233 Exact inference as a variational optimization problem 7782234 Mean \ufb01eld as a variational optimization problem 7792235 LBP as a variational optimization problem 7792236 Loopy BP vs mean \ufb01eld"}}}, "22.4": {"Title": "Extensions of belief propagation *", "Sub Topics": {"22.4.1": {"Title": "Generalized belief propagation"}, "22.4.2": {"Title": "Convex belief propagation"}}}, "22.5": {"Title": "Expectation propagation", "Sub Topics": {"22.5.1": {"Title": "EP as a variational inference problem 7882252 Optimizing the EP objective using moment matching 7892253 EP for the clutter problem 7912254 LBP is a special case of EP 7922255 Ranking players using TrueSkill 7932256 Other applications of EP"}}}, "22.6": {"Title": "MAP state estimation", "Sub Topics": {"22.6.1": {"Title": "Linear programming relaxation"}, "22.6.2": {"Title": "Max-product belief propagation"}, "22.6.3": {"Title": "Graphcuts"}, "22.6.4": {"Title": "Experimental comparison of graphcuts and BP 8042265 Dual decomposition"}}}}}, "23": {"Title": "Monte Carlo inference", "Sub Topics": {"23.1": {"Title": "Introduction", "Sub Topics": {}}, "23.2": {"Title": "Sampling from standard distributions", "Sub Topics": {"23.2.1": {"Title": "Using the cdf 8152322 Sampling from a Gaussian (Box-Muller method)"}}}, "23.3": {"Title": "Rejection sampling", "Sub Topics": {"23.3.1": {"Title": "Basic idea 8172332 Example 8182333 Application to Bayesian statistics"}, "23.3.4": {"Title": "Adaptive rejection sampling 8192335 Rejection sampling in high dimensions"}}}, "23.4": {"Title": "Importance sampling", "Sub Topics": {"23.4.1": {"Title": "Basic idea 8202342 Handling unnormalized distributions 8212343 Importance sampling for a DGM: likelihood weighting 8222344 Sampling importance resampling (SIR)"}}}, "23.5": {"Title": "Particle \ufb01ltering", "Sub Topics": {"23.5.1": {"Title": "Sequential importance sampling 8242352 The degeneracy problem 8252353 The resampling step 8252354 The proposal distribution 8272355 Application: robot localization 8282356 Application: visual object tracking 8282357 Application: time series forecasting"}}}, "23.6": {"Title": "Rao-Blackwellised particle \ufb01ltering (RBPF)", "Sub Topics": {"23.6.1": {"Title": "RBPF for switching LG-SSMs 8312362 Application: tracking a maneuvering target 8322363 Application: Fast SLAM"}}}}}, "24": {"Title": "Markov chain Monte Carlo (MCMC) inference", "Sub Topics": {"24.1": {"Title": "Introduction", "Sub Topics": {}}, "24.2": {"Title": "Gibbs sampling", "Sub Topics": {"24.2.1": {"Title": "Basic idea 8382422 Example: Gibbs sampling for the Ising model 8382423 Example: Gibbs sampling for inferring the parameters of a GMM 8402424 Collapsed Gibbs sampling * 8412425 Gibbs sampling for hierarchical GLMs 8442426 BUGS and JAGS 8462427 The Imputation Posterior (IP) algorithm 8472428 Blocking Gibbs sampling"}}}, "24.3": {"Title": "Metropolis Hastings algorithm", "Sub Topics": {"24.3.1": {"Title": "Basic idea 8482432 Gibbs sampling is a special case of MH 8492433 Proposal distributions 8502434 Adaptive MCMC 8532435 Initialization and mode hopping 8542436 Why MH works * 8542437 Reversible jump (trans-dimensional) MCMC *"}}}, "24.4": {"Title": "Speed and accuracy of MCMC", "Sub Topics": {"24.4.1": {"Title": "The burn-in phase"}, "24.4.2": {"Title": "Mixing rates of Markov chains * 8572443 Practical convergence diagnostics 8582444 Accuracy of MCMC 8602445 How many chains?"}}}, "24.5": {"Title": "Auxiliary variable MCMC *", "Sub Topics": {"24.5.1": {"Title": "Auxiliary variable sampling for logistic regression 8632452 Slice sampling 8642453 Swendsen Wang 8662454 Hybrid/Hamiltonian MCMC *"}}}, "24.6": {"Title": "Annealing methods", "Sub Topics": {"24.6.1": {"Title": "Simulated annealing 8692462 Annealed importance sampling 8712463 Parallel tempering"}}}, "24.7": {"Title": "Approximating the marginal likelihood", "Sub Topics": {"24.7.1": {"Title": "The candidate method 8722472 Harmonic mean estimate 8722473 Annealed importance sampling"}}}}}, "25": {"Title": "Clustering", "Sub Topics": {"25.1": {"Title": "Introduction", "Sub Topics": {"25.1.1": {"Title": "Measuring (dis)similarity 8752512 Evaluating the output of clustering methods *"}}}, "25.2": {"Title": "Dirichlet process mixture models", "Sub Topics": {"25.2.1": {"Title": "From \ufb01nite to in\ufb01nite mixture models"}, "25.2.2": {"Title": "The Dirichlet process"}, "25.2.3": {"Title": "Applying Dirichlet processes to mixture modeling"}, "25.2.4": {"Title": "Fitting a DP mixture model"}}}, "25.3": {"Title": "Affinity propagation 887254 Spectral clustering", "Sub Topics": {}}, "25.5": {"Title": "Hierarchical clustering", "Sub Topics": {"25.5.1": {"Title": "Agglomerative clustering 8952552 Divisive clustering"}, "25.5.3": {"Title": "Choosing the number of clusters 8992554 Bayesian hierarchical clustering"}}}, "25.6": {"Title": "Clustering datapoints and features", "Sub Topics": {"25.6.1": {"Title": "Biclustering 9032562 Multi-view clustering"}}}}}, "26": {"Title": "Graphical model structure learning", "Sub Topics": {"26.1": {"Title": "Introduction", "Sub Topics": {}}, "26.2": {"Title": "Structure learning for knowledge discovery", "Sub Topics": {"26.2.1": {"Title": "Relevance networks 9082622 Dependency networks"}}}, "26.3": {"Title": "Learning tree structures", "Sub Topics": {"26.3.1": {"Title": "Directed or undirected tree? 9112632 Chow-Liu algorithm for \ufb01nding the ML tree structure 9122633 Finding the MAP forest"}, "26.3.4": {"Title": "Mixtures of trees"}}}, "26.4": {"Title": "Learning DAG structures", "Sub Topics": {"26.4.1": {"Title": "Markov equivalence 9142642 Exact structural inference 9162643 Scaling up to larger graphs"}}}, "26.5": {"Title": "Learning DAG structure with latent variables", "Sub Topics": {"26.5.1": {"Title": "Approximating the marginal likelihood when we have missing data 9222652 Structural EM 9252653 Discovering hidden variables"}, "26.5.4": {"Title": "Case study: Google\u2019s Rephil"}, "26.5.5": {"Title": "Structural equation models *"}}}, "26.6": {"Title": "Learning causal DAGs", "Sub Topics": {"26.6.1": {"Title": "Causal interpretation of DAGs 9312662 Using causal DAGs to resolve Simpson\u2019s paradox 9332663 Learning causal DAG structures"}}}, "26.7": {"Title": "Learning undirected Gaussian graphical models", "Sub Topics": {"26.7.1": {"Title": "MLE for a GGM 9382672 Graphical lasso 9392673 Bayesian inference for GGM structure * 9412674 Handling non-Gaussian data using copulas *"}}}, "26.8": {"Title": "Learning undirected discrete graphical models", "Sub Topics": {"26.8.1": {"Title": "Graphical lasso for MRFs/CRFs 9422682 Thin junction trees"}}}}}, "27": {"Title": "Latent variable models for discrete data", "Sub Topics": {"27.1": {"Title": "Introduction", "Sub Topics": {}}, "27.2": {"Title": "Distributed state LVMs for discrete data", "Sub Topics": {"27.2.1": {"Title": "Mixture models"}, "27.2.2": {"Title": "Exponential family PCA"}, "27.2.3": {"Title": "LDA and mPCA 9482724 GaP model and non-negative matrix factorization"}}}, "27.3": {"Title": "Latent Dirichlet allocation (LDA)", "Sub Topics": {"27.3.1": {"Title": "Basics 9502732 Unsupervised discovery of topics 9532733 Quantitatively evaluating LDA as a language model 9532734 Fitting using (collapsed) Gibbs sampling 9552735 Example 9562736 Fitting using batch variational inference 9572737 Fitting using online variational inference 9592738 Determining the number of topics"}}}, "27.4": {"Title": "Extensions of LDA", "Sub Topics": {"27.4.1": {"Title": "Correlated topic model 9612742 Dynamic topic model 9622743 LDA-HMM 9632744 Supervised LDA"}}}, "27.5": {"Title": "LVMs for graph-structured data", "Sub Topics": {"27.5.1": {"Title": "Stochastic block model 9712752 Mixed membership stochastic block model"}, "27.5.3": {"Title": "Relational topic model"}}}, "27.6": {"Title": "LVMs for relational data", "Sub Topics": {"27.6.1": {"Title": "In\ufb01nite relational model 9762762 Probabilistic matrix factorization for collaborative \ufb01ltering"}}}, "27.7": {"Title": "Restricted Boltzmann machines (RBMs)", "Sub Topics": {"27.7.1": {"Title": "Varieties of RBMs"}, "27.7.2": {"Title": "Learning RBMs 9872773 Applications of RBMs"}}}}}, "28": {"Title": "Deep learning", "Sub Topics": {"28.1": {"Title": "Introduction", "Sub Topics": {}}, "28.2": {"Title": "Deep generative models", "Sub Topics": {"28.2.1": {"Title": "Deep directed networks 9962822 Deep Boltzmann machines 9962823 Deep belief networks 9972824 Greedy layer-wise learning of DBNs"}}}, "28.3": {"Title": "Deep neural networks", "Sub Topics": {"28.3.1": {"Title": "Deep multi-layer perceptrons"}, "28.3.2": {"Title": "Deep auto-encoders 10002833 Stacked denoising auto-encoders"}}}, "28.4": {"Title": "Applications of deep networks", "Sub Topics": {"28.4.1": {"Title": "Handwritten digit classi\ufb01cation using DBNs 10012842 Data visualization and feature discovery using deep auto-encoders"}, "28.4.3": {"Title": "Information retrieval using deep auto-encoders (semantic hashing) 10032844 Learning audio features using 1d convolutional DBNs"}, "28.4.5": {"Title": "Learning image features using 2d convolutional DBNs"}}}, "28.5": {"Title": "Discussion", "Sub Topics": {}}}}}