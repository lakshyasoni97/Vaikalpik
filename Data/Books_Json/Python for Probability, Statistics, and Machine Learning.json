{"Name": "Python for Probability, Statistics, and Machine Learning.pdf", "Pages": {"0": "Python for \nProbability, \nStatistics, and \nMachine Learning\nJos\u00e9 Unpingco\nSecond Edition", "1": "Python for Probability, Statistics, and Machine\nLearning", "2": "Jos\u00e9Unpingco\nPython for Probability,\nStatistics, and Machine\nLearning\nSecond Edition\n123", "3": "Jos\u00e9Unpingco\nSan Diego, CA, USA\nISBN 978-3-030-18544-2 ISBN 978-3-030-18545-9 (eBook)\nhttps://doi.org/10.1007/978-3-030-18545-9\n1stedition: \u00a9Springer International Publishing Switzerland 2016\n2ndedition: \u00a9Springer Nature Switzerland AG 2019\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or partof the material is concerned, speci \ufb01cally the rights of translation, reprinting, reuse of illustrations,\nrecitation, broadcasting, reproduction on micro \ufb01lms or in any other physical way, and transmission\nor information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar\nmethodology now known or hereafter developed.The use of general descriptive names, registered names, trademarks, service marks, etc. in thispublication does not imply, even in the absence of a speci \ufb01c statement, that such names are exempt from\nthe relevant protective laws and regulations and therefore free for general use.The publisher, the authors and the editors are safe to assume that the advice and information in this\nbook are believed to be true and accurate at the date of publication. Neither the publisher nor the\nauthors or the editors give a warranty, expressed or implied, with respect to the material containedherein or for any errors or omissions that may have been made. The publisher remains neutral with regardto jurisdictional claims in published maps and institutional af \ufb01liations.\nThis Springer imprint is published by the registered company Springer Nature Switzerland AG\nThe registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland", "4": "To Irene, Nicholas, and Daniella, for all\ntheir patient support.", "5": "Preface to the Second Edition\nThis second edition is updated for Python version 3.6+. Furthermore, many existing\nsections have been revised for clarity based on feedback from the \ufb01rst version. The\nbook is now over thirty percent larger than the original with new material aboutimportant probability distributions, including key derivations and illustrative codesamples. Additional important statistical tests are included in the statistics chapterincluding the Fisher Exact test and the Mann \u2013Whitney \u2013Wilcoxon Test. A new\nsection on survival analysis has been included. The most signi \ufb01cant addition is the\nsection on deep learning for image processing with a detailed discussion of gradientdescent methods that underpin all deep learning work. There is also substan-tial discussion regarding generalized linear models. As before, there are moreProgramming Tips that illustrate effective Python modules and methods for scienti \ufb01c\nprogramming and machine learning. There are 445 run-able code blocks that havebeen tested for accuracy so you can try these out for yourself in your own codes.Over 158 graphical visualizations (almost all generated using Python) illustrate the\nconcepts that are developed both in code and in mathematics. We also discuss and use\nkey Python modules such as NumPy, Scikit-learn, SymPy, SciPy, lifelines, CVXPY,Theano, Matplotlib, Pandas, TensorFlow, StatsModels, and Keras.\nAs with the \ufb01rst edition, all of the key concepts are developed mathematically\nand are reproducible in Python, to provide the reader with multiple perspectives onthe material. As before, this book is not designed to be exhaustive and re \ufb02ects the\nauthor \u2019s eclectic industrial background. The focus remains on concepts and fun-\ndamentals for day-to-day work using Python in the most expressive way possible.\nvii", "6": "Acknowledgements\nI would like to acknowledge the help of Brian Granger and Fernando Perez, two\nof the originators of the Jupyter Notebook, for all their great work, as well as the\nPython community as a whole, for all their contributions that made this book pos-\nsible. Hans Petter Langtangen is the author of the Doconce [1] document preparationsystem that was used to write this text. Thanks to Geoffrey Poore [2] for his workwith PythonTeX and L\nATEX, both key technologies used to produce this book.\nSan Diego, CA, USA\nFebruary 2019Jos\u00e9Unpingco\nReferences\n1. H.P. Langtangen, DocOnce markup language, https://github.com/hplgit/doconce\n2. G.M. Poore, Pythontex: reproducible documents with latex, python, and more. Comput. Sci.\nDiscov. 8(1), 014010 (2015)viii Preface to the Second Edition", "7": "Preface to the First Edition\nThis book will teach you the fundamental concepts that underpin probability and\nstatistics and illustrate how they relate to machine learning via the Python languageand its powerful extensions. This is not a good \ufb01rstbook in any of these topics\nbecause we assume that you already had a decent undergraduate-level introductionto probability and statistics. Furthermore, we also assume that you have a goodgrasp of the basic mechanics of the Python language itself. Having said that, this\nbook is appropriate if you have this basic background and want to learn how to use\nthe scienti \ufb01c Python toolchain to investigate these topics. On the other hand, if you\nare comfortable with Python, perhaps through working in another scienti \ufb01c\ufb01eld,\nthen this book will teach you the fundamentals of probability and statistics and howto use these ideas to interpret machine learning methods. Likewise, if you are apracticing engineer using a commercial package (e.g., MATLAB, IDL), then youwill learn how to effectively use the scienti \ufb01c Python toolchain by reviewing\nconcepts you are already familiar with.\nThe most important feature of this book is that everything in it is reproducible\nusing Python. Speci \ufb01cally, all of the code, all of the \ufb01gures, and (most of) the text is\navailable in the downloadable supplementary materials that correspond to this bookas IPython Notebooks. IPython Notebooks are liveinteractive documents that allow\nyou to change parameters, recompute plots, and generally tinker with all of theideas and code in this book. I urge you to download these IPython Notebooks and\nfollow along with the text to experiment with the topics covered. I guarantee doingthis will boost your understanding because the IPython Notebooks allow for\ninteractive widgets, animations, and other intuition-building features that help make\nmany of these abstract ideas concrete. As an open-source project, the entire sci-enti\ufb01c Python toolchain, including the IPython Notebook, is freely available.\nHaving taught this material for many years, I am convinced that the only way tolearn is to experiment as you go. The text provides instructions on how to getstarted installing and con \ufb01guring your scienti \ufb01c Python environment.\nix", "8": "This book is not designed to be exhaustive and re \ufb02ects the author \u2019s eclectic\nbackground in industry. The focus is on fundamentals and intuitions for day-to-daywork, especially when you must explain the results of your methods to a non-technical audience. We have tried to use the Python language in the most expressive\nway possible while encouraging good Python-coding practices.\nAcknowledgements\nI would like to acknowledge the help of Brian Granger and Fernando Perez, two\nof the originators of the Jupyter/IPython Notebook, for all their great work, as wellas the Python community as a whole, for all their contributions that made this book\npossible. Additionally, I would also like to thank Juan Carlos Chavez for his\nthoughtful review. Hans Petter Langtangen is the author of the Doconce [14]document preparation system that was used to write this text. Thanks to GeoffreyPoore [25] for his work with PythonTeX and L\nATEX.\nSan Diego, CA, USA Jos \u00e9Unpingco\nFebruary 2016x Preface to the First Edition", "9": "Contents\n1 Getting Started with Scienti \ufb01c Python ....................... 1\n1.1 Installation and Setup ................................ 2\n1.2 Numpy ........................................... 4\n1.2.1 Numpy Arrays and Memory ..................... 6\n1.2.2 Numpy Matrices ............................. 9\n1.2.3 Numpy Broadcasting .......................... 10\n1.2.4 Numpy Masked Arrays ......................... 13\n1.2.5 Floating-Point Numbers ........................ 13\n1.2.6 Numpy Optimizations and Prospectus .............. 17\n1.3 Matplotlib ........................................ 17\n1.3.1 Alternatives to Matplotlib ....................... 19\n1.3.2 Extensions to Matplotlib ........................ 20\n1.4 IPython .......................................... 20\n1.5 Jupyter Notebook ................................... 22\n1.6 Scipy ............................................ 24\n1.7 Pandas ........................................... 25\n1.7.1 Series ..................................... 25\n1.7.2 Dataframe .................................. 27\n1.8 Sympy ........................................... 30\n1.9 Interfacing with Compiled Libraries ...................... 32\n1.10 Integrated Development Environments .................... 33\n1.11 Quick Guide to Performance and Parallel Programming ....... 34\n1.12 Other Resources .................................... 37\nReferences ............................................. 38\n2 Probability ............................................ 39\n2.1 Introduction ....................................... 39\n2.1.1 Understanding Probability Density ................ 40\n2.1.2 Random Variables ............................ 41\n2.1.3 Continuous Random Variables ................... 46\nxi", "10": "2.1.4 Transformation of Variables Beyond Calculus ........ 49\n2.1.5 Independent Random Variables ................... 51\n2.1.6 Classic Broken Rod Example .................... 53\n2.2 Projection Methods .................................. 55\n2.2.1 Weighted Distance ............................ 57\n2.3 Conditional Expectation as Projection .................... 58\n2.3.1 Appendix ................................... 64\n2.4 Conditional Expectation and Mean Squared Error ............ 65\n2.5 Worked Examples of Conditional Expectation and Mean Square\nError Optimization .................................. 68\n2.5.1 Example ................................... 69\n2.5.2 Example ................................... 72\n2.5.3 Example ................................... 75\n2.5.4 Example ................................... 78\n2.5.5 Example ................................... 79\n2.5.6 Example ................................... 82\n2.6 Useful Distributions ................................. 83\n2.6.1 Normal Distribution ........................... 83\n2.6.2 Multinomial Distribution ........................ 84\n2.6.3 Chi-square Distribution ......................... 86\n2.6.4 Poisson and Exponential Distributions .............. 89\n2.6.5 Gamma Distribution ........................... 90\n2.6.6 Beta Distribution ............................. 91\n2.6.7 Dirichlet-Multinomial Distribution ................. 93\n2.7 Information Entropy ................................. 95\n2.7.1 Information Theory Concepts .................... 96\n2.7.2 Properties of Information Entropy ................. 98\n2.7.3 Kullback \u2013Leibler Divergence .................... 99\n2.7.4 Cross-Entropy as Maximum Likelihood ............. 100\n2.8 Moment Generating Functions .......................... 101\n2.9 Monte Carlo Sampling Methods ........................ 104\n2.9.1 Inverse CDF Method for Discrete Variables .......... 105\n2.9.2 Inverse CDF Method for Continuous Variables ....... 107\n2.9.3 Rejection Method ............................. 108\n2.10 Sampling Importance Resampling ....................... 113\n2.11 Useful Inequalities .................................. 115\n2.11.1 Markov \u2019s Inequality ........................... 115\n2.11.2 Chebyshev \u2019s Inequality ......................... 116\n2.11.3 Hoeffding \u2019s Inequality ......................... 118\nReferences ............................................. 120xii Contents", "11": "3 Statistics .............................................. 123\n3.1 Introduction ....................................... 123\n3.2 Python Modules for Statistics .......................... 124\n3.2.1 Scipy Statistics Module ........................ 124\n3.2.2 Sympy Statistics Module ....................... 125\n3.2.3 Other Python Modules for Statistics ............... 126\n3.3 Types of Convergence ............................... 126\n3.3.1 Almost Sure Convergence ...................... 126\n3.3.2 Convergence in Probability ...................... 129\n3.3.3 Convergence in Distribution ..................... 131\n3.3.4 Limit Theorems .............................. 132\n3.4 Estimation Using Maximum Likelihood ................... 133\n3.4.1 Setting Up the Coin-Flipping Experiment ........... 135\n3.4.2 Delta Method ................................ 145\n3.5 Hypothesis Testing and P-Values ........................ 147\n3.5.1 Back to the Coin-Flipping Example ................ 149\n3.5.2 Receiver Operating Characteristic ................. 152\n3.5.3 P-Values ................................... 154\n3.5.4 Test Statistics ................................ 155\n3.5.5 Testing Multiple Hypotheses ..................... 163\n3.5.6 Fisher Exact Test ............................. 163\n3.6 Con \ufb01dence Intervals ................................. 166\n3.7 Linear Regression ................................... 169\n3.7.1 Extensions to Multiple Covariates ................. 178\n3.8 Maximum A-Posteriori ............................... 183\n3.9 Robust Statistics .................................... 188\n3.10 Bootstrapping ...................................... 195\n3.10.1 Parametric Bootstrap .......................... 200\n3.11 Gauss \u2013Markov ..................................... 201\n3.12 Nonparametric Methods .............................. 205\n3.12.1 Kernel Density Estimation ...................... 205\n3.12.2 Kernel Smoothing ............................ 207\n3.12.3 Nonparametric Regression Estimators .............. 213\n3.12.4 Nearest Neighbors Regression .................... 214\n3.12.5 Kernel Regression ............................ 218\n3.12.6 Curse of Dimensionality ........................ 219\n3.12.7 Nonparametric Tests ........................... 221\n3.13 Survival Analysis ................................... 228\n3.13.1 Example ................................... 231\nReferences ............................................. 236Contents xiii", "12": "4 Machine Learning ....................................... 237\n4.1 Introduction ....................................... 237\n4.2 Python Machine Learning Modules ...................... 237\n4.3 Theory of Learning .................................. 241\n4.3.1 Introduction to Theory of Machine Learning ......... 244\n4.3.2 Theory of Generalization ....................... 249\n4.3.3 Worked Example for Generalization/Approximation\nComplexity ................................. 250\n4.3.4 Cross-Validation ............................. 256\n4.3.5 Bias and Variance ............................ 260\n4.3.6 Learning Noise .............................. 265\n4.4 Decision Trees ..................................... 268\n4.4.1 Random Forests .............................. 275\n4.4.2 Boosting Trees ............................... 277\n4.5 Boosting Trees ..................................... 281\n4.5.1 Boosting Trees ............................... 281\n4.6 Logistic Regression .................................. 285\n4.7 Generalized Linear Models ............................ 295\n4.8 Regularization ..................................... 300\n4.8.1 Ridge Regression ............................. 304\n4.8.2 Lasso Regression ............................. 309\n4.9 Support Vector Machines ............................. 311\n4.9.1 Kernel Tricks ................................ 315\n4.10 Dimensionality Reduction ............................. 317\n4.10.1 Independent Component Analysis ................. 321\n4.11 Clustering ........................................ 325\n4.12 Ensemble Methods .................................. 329\n4.12.1 Bagging .................................... 329\n4.12.2 Boosting ................................... 331\n4.13 Deep Learning ..................................... 334\n4.13.1 Introduction to Tensor \ufb02ow...................... 343\n4.13.2 Understanding Gradient Descent .................. 350\n4.13.3 Image Processing Using Convolutional Neural\nNetworks ................................... 363\nReferences ............................................. 379\nNotation .................................................... 381\nIndex ...................................................... 383xiv Contents", "13": "Chapter 1\nGetting Started with Scienti\ufb01c Python\nPython is fundamental to data science and machine learning, as well as an ever-\nexpanding list of areas including cyber-security, and web programming. The funda-mental reason for Python\u2019s widespread use is that it provides the software glue that\npermits easy exchange of methods and data across core routines typically written in\nFortran or C.\nPython is a language geared toward scientists and engineers who may not have\nformal software development training. It is used to prototype, design, simulate, and\ntest without getting in the way because Python provides an inherently easy and\nincremental development cycle, interoperability with existing codes, access to a large\nbase of reliable open-source codes, and a hierarchical compartmentalized design\nphilosophy. Python is known for enhancing user productivity because it reduces thedevelopment time (i.e., time spent programming) and thereby increases program\nrun-time.\nPython is an interpreted language. This means that Python codes run on a Python\nvirtual machine that provides a layer of abstraction between the code and the plat-\nform it runs on, thus making codes portable across different platforms. For example,\nthe same script that runs on a Windows laptop can also run on a Linux-based super-\ncomputer or on a mobile phone. This makes programming easier because the virtual\nmachine handles the low-level details of implementing the business logic of the scripton the underlying platform.\nPython is a dynamically typed language, which means that the interpreter itself\n\ufb01gures out the representative types (e.g., \ufb02oats, integers) interactively or at run-time.This is in contrast to a language like Fortran that has compilers that study the code\nfrom beginning to end, perform many compiler-level optimizations, link intimately\nwith the existing libraries on a speci\ufb01c platform, and then create an executable that ishenceforth liberated from the compiler. As you may guess, the compiler\u2019s access to\nthe details of the underlying platform means that it can utilize optimizations that ex-\nploit chip-speci\ufb01c features and cache memory. Because the virtual machine abstractsaway these details, it means that the Python language does not have programmable\naccess to these kinds of optimizations. So, where is the balance between the ease\n\u00a9 Springer Nature Switzerland AG 2019\nJ. Unpingco, Python for Probability, Statistics, and Machine Learning ,\nhttps://doi.org/10.1007/978-3-030-18545-9_11", "14": "2 1 Getting Started with Scienti\ufb01c Python\nof programming the virtual machine and these key numerical optimizations that are\ncrucial for scienti\ufb01c work?\nThe balance comes from Python\u2019s native ability to bind to compiled Fortran and C\nlibraries. This means that you can send intensive computations to compiled libraries\ndirectly from the interpreter. This approach has two primary advantages. First, it gives\nyou the fun of programming in Python, with its expressive syntax and lack of visualclutter. This is a particular boon to scientists who typically want to usesoftware as\na tool as opposed to developing software as a product. The second advantage is that\nyou can mix-and-match different compiled libraries from diverse research areas thatwere not otherwise designed to work together. This works because Python makes\nit easy to allocate and \ufb01ll memory in the interpreter, pass it as input to compiled\nlibraries, and then recover the output back in the interpreter.\nMoreover, Python provides a multiplatform solution for scienti\ufb01c codes. As an\nopen-source project, Python itself is available anywhere you can build it, even though\nit typically comes standard nowadays, as part of many operating systems. This meansthat once you have written your code in Python, you can just transfer the script to\nanother platform and run it, as long as the third-party compiled libraries are also\navailable there. What if the compiled libraries are absent? Building and con\ufb01guring\ncompiled libraries across multiple systems used to be a painstaking job, but as scien-\nti\ufb01c Python has matured, a wide range of libraries have now become available acrossall of the major platforms (i.e., Windows, MacOS, Linux, Unix) as prepackaged\ndistributions.\nFinally, scienti\ufb01c Python facilitates maintainability of scienti\ufb01c codes because\nPython syntax is clean, free of semi-colon litter and other visual distractions that\nmakes code hard to read and easy to obfuscate. Python has many built-in testing,\ndocumentation, and development tools that ease maintenance. Scienti\ufb01c codes areusually written by scientists unschooled in software development, so having solid\nsoftware development tools built into the language itself is a particular boon.\n1.1 Installation and Setup\nThe easiest way to get started is to download the freely available Anaconda distribu-\ntion provided by Anaconda ( anaconda.com ), which is available for all of the major\nplatforms. On Linux, even though most of the toolchain is available via the built-in\nLinux package manager, it is still better to install the Anaconda distribution because\nit provides its own powerful package manager (i.e., conda ) that can keep track of\nchanges in the software dependencies of the packages that it supports. Note that if\nyou do not have administrator privileges, there is also a corresponding Miniconda\ndistribution that does not require these privileges. Regardless of your platform, we\nrecommend Python version 3.6 or better.\nYou may have encountered other Python variants on the web, such as IronPython\n(Python implemented in C#) and Jython (Python implemented in Java ). In this text,\nwe focus on the C implementation of Python (i.e., known as CPython ), which is, by", "15": "1.1 Installation and Setup 3\nfar, the most popular implementation. These other Python variants permit specialized,\nnative interaction with libraries in C#orJava (respectively), which is still possible\n(but clunky) using CPython. Even more Python variants exist that implement the low-level machinery of Python differently for various reasons, beyond interacting with\nnative libraries in other languages. Most notable of these is Pypy that implements a\njust-in-time compiler (JIT) and other powerful optimizations that can substantiallyspeed up pure Python codes. The downside of Pypy is that its coverage of some\npopular scienti\ufb01c modules (e.g., Matplotlib, Scipy) is limited or nonexistent which\nmeans that you cannot use those modules in code meant for Pypy .\nIf you want to install a Python module that is not available via the conda manager,\nthepip installer is available. This installer is the main one used outside of the\nscienti\ufb01c computing community. The key difference between the two installer is thatconda implements a satis\ufb01ability solver that checks for con\ufb02icts in versions among\nand between installed packages. This can result in conda decreasing versions of\ncertain packages to accommodate proposed package installation. The pipinstaller\ndoes not check for such con\ufb02icts checks only if the proposed package already has its\ndependencies installed and will install them if not or remove existing incompatible\nmodules. The following command line uses pipto install the given Python module,\nTerminal> pip install package_name\nThepipinstaller will download the package you want and its dependencies and\ninstall them in the existing directory tree. This works beautifully in the case where\nthe package in question is pure-Python, without any system-speci\ufb01c dependencies.\nOtherwise, this can be a real nightmare, especially on Windows, which lacks freelyavailable Fortran compilers. If the module in question is a C library, one way to cope\nis to install the freely available Visual Studio Community Edition, which usually\nhas enough to compile many C-codes. This platform dependency is the problem\nthatconda was designed to solve by making the binary dependencies of the various\nplatforms available instead of attempting to compile them. On a Windows system,if you installed Anaconda and registered it as the default Python installation (it asks\nduring the install process), then you can use the high-quality Python wheel \ufb01les on\nChristoph Gohlke\u2019s laboratory site at the University of California, Irvine where hekindly makes a long list of scienti\ufb01c modules available.\n1Failing this, you can try\ntheconda-forge site, which is a community-powered repository of modules that\nconda is capable of installing, but which are not formally supported by Anaconda.\nNote that conda-forge allows you to share scienti\ufb01c Python con\ufb01gurations with\nyour remote colleagues using authentication so that you can be sure that you are\ndownloading and running code from users you trust.\nAgain, if you are on Windows, and none of the above works, then you may wan-\nt to consider installing a full virtual machine solution, as provided by VMWare\u2019s\nPlayer or Oracle\u2019s VirtualBox (both freely available under liberal terms), or with\n1Wheel \ufb01les are a Python distribution format that you download and install using pipas in pip\ninstall file.whl . Christoph names \ufb01les according to Python version (e.g., cp27 means Python\n2.7) and chipset (e.g., amd32 vs. Intel win32 ).", "16": "4 1 Getting Started with Scienti\ufb01c Python\nthe Windows subsystem for Linux (WSL) that is built into Windows 10. Using ei-\nther of these, you can set up a Linux machine running on top of Windows, which\nshould cure these problems entirely! The great part of this approach is that youcan share directories between the virtual machine and the Windows system so that\nyou don\u2019t have to maintain duplicate data \ufb01les. Anaconda Linux images are also\navailable on the cloud by Platform as a Service (PaaS) providers like Amazon WebServices and Microsoft Azure. Note that for the vast majority of users, especially\nnewcomers to Python, the Anaconda distribution should be more than enough on\nany platform. It is just worth highlighting the Windows-speci\ufb01c issues and associat-ed workarounds early on. Note that there are other well-maintained scienti\ufb01c Python\nWindows installers like WinPython andPythonXY . These provide the spyder in-\ntegrated development environment, which is very MATLAB-like environment fortransitioning MATLAB users.\n1.2 Numpy\nAs we touched upon earlier, to use a compiled scienti\ufb01c library, the memory allocatedin the Python interpreter must somehow reach this library as input. Furthermore, theoutput from these libraries must likewise return to the Python interpreter. This two-\nway exchange of memory is essentially the core function of the Numpy (numerical\narrays in Python) module. Numpy is the de facto standard for numerical arrays inPython. It arose as an effort by Travis Oliphant and others to unify the preexisting\nnumerical arrays in Python. In this section, we provide an overview and some tips\nfor using Numpy effectively, but for much more detail, Travis\u2019 freely available book[1] is a great place to start.\nNumpy provides speci\ufb01cation of byte-sized arrays in Python. For example, below\nwe create an array of three numbers, each of 4 bytes long (32-bits at 8-bits per byte)a ss h o w nb yt h e itemsize property. The \ufb01rst line imports Numpy as np, which is\nthe recommended convention. The next line creates an array of 32-bit \ufb02oating-point\nnumbers. The itemize property shows the number of bytes per item.\n>>> import numpy asnp# recommended convention\n>>> x=np.array([ 1,2,3],dtype =np.float32)\n>>> x\narray([1., 2., 3.], dtype=float32)>>> x.itemsize\n4\nIn addition to providing uniform containers for numbers, Numpy provides a com-\nprehensive set of universal functions (i.e., ufuncs ) that process arrays element-wise\nwithout additional looping semantics. Below, we show how to compute the element-\nwise sine using Numpy,", "17": "1.2 Numpy 5\n>>> np.sin(np .array([ 1,2,3],dtype =np.float32) )\narray([0.84147096, 0.9092974 , 0.14112 ], dtype=float32)\nThis computes the sine of the input array [1,2,3] , using Numpy\u2019s unary function,\nnp.sin . There is another sine function in the built-in math module, but the Numpy\nversion is faster because it does not require explicit looping (i.e., using a forloop)\nover each of the elements in the array. That looping happens in the compiled np.sin\nfunction itself. Otherwise, we would have to do looping explicitly as in the following:\n>>> from math import sin\n>>> [sin(i) for iin[1,2,3]]# list comprehension\n[0.8414709848078965, 0.9092974268256817, 0.1411200080598672]\nNumpy uses common-sense casting rules to resolve the output types. For example,\nif the inputs had been an integer-type, the output would still have been a \ufb02oating-point type. In this example, we provided a Numpy array as input to the sine function.\nWe could have also used a plain Python list instead and Numpy would have built the\nintermediate Numpy array (e.g., np.sin([1,1,1]) ). The Numpy documentation\nprovides a comprehensive (and very long) list of available ufuncs .\nNumpy arrays come in many dimensions. For example, the following shows a\ntwo-dimensional 2x3array constructed from two conforming Python lists.\n>>> x=np.array([ [ 1,2,3],[4,5,6]] )\n>>> x.shape\n(2, 3)\nNote that Numpy is limited to 32 dimensions unless you build it for more.\n2Numpy\narrays follow the usual Python slicing rules in multiple dimensions as shown below\nwhere the :colon character selects all elements along a particular axis.\n>>> x=np.array([ [ 1,2,3],[4,5,6]] )\n>>> x[:, 0]# 0th column\narray([1, 4])>>> x[:, 1]# 1st column\narray([2, 5])\n>>> x[0,:] # 0th row\narray([1, 2, 3])\n>>> x[1,:] # 1st row\narray([4, 5, 6])\nYou can also select subsections of arrays by using slicing as shown below\n>>> x=np.array([ [ 1,2,3],[4,5,6]] )\n>>> x\narray([[1, 2, 3],\n[4, 5, 6]])\n2Seearrayobject.h in the Numpy source code.", "18": "6 1 Getting Started with Scienti\ufb01c Python\n>>> x[:, 1:]# all rows, 1st thru last column\narray([[2, 3],\n[5, 6]])\n>>> x[:,:: 2]# all rows, every other column\narray([[1, 3],\n[4, 6]])\n>>> x[:,:: -1]# reverse order of columns\narray([[3, 2, 1],\n[6, 5, 4]])\n1.2.1 Numpy Arrays and Memory\nSome interpreted languages implicitly allocate memory. For example, in MATLAB,you can extend a matrix by simply tacking on another dimension as in the followingMATLAB session:\n>> x=ones(3,3)\nx=\n111\n111\n111\n>> x(:,4)=ones(3,1) % tack on extra dimension\nx=\n1111\n11111111\n>> size(x)\nans =\n34\nThis works because MATLAB arrays use pass-by-value semantics so that slice oper-\nations actually copy parts of the array as needed. By contrast, Numpy uses pass-by-\nreference semantics so that slice operations are views into the array without implicit\ncopying. This is particularly helpful with large arrays that already strain available\nmemory. In Numpy terminology, slicing creates views (no copying) and advanced\nindexing creates copies. Let\u2019s start with advanced indexing.\nIf the indexing object (i.e., the item between the brackets) is a non-tuple sequence\nobject, another Numpy array (of type integer or boolean), or a tuple with at least\none sequence object or Numpy array, then indexing creates copies. For the aboveexample, to accomplish the same array extension in Numpy, you have to do something\nlike the following:\n>>> x=np.ones(( 3,3))\n>>> x\narray([[1., 1., 1.],\n[1., 1., 1.],", "19": "1.2 Numpy 7\n[1., 1., 1.]])\n>>> x[:,[ 0,1,2,2]]# notice duplicated last dimension\narray([[1., 1., 1., 1.],\n[1., 1., 1., 1.],\n[1., 1., 1., 1.]])\n>>> y=x[:,[ 0,1,2,2]]# same as above, but do assign it to y\nBecause of advanced indexing, the variable yhas its own memory because the rele-\nvant parts of xwere copied. To prove it, we assign a new element to xand see that\nyis not updated.\n>>> x[0,0]=999 # change element in x\n>>> x # changed\narray([[999., 1., 1.],\n[ 1., 1., 1.],\n[ 1., 1., 1.]])\n>>> y # not changed!\narray([[1., 1., 1., 1.],\n[1., 1., 1., 1.],[1., 1., 1., 1.]])\nHowever, if we start over and construct yby slicing (which makes it a view) as shown\nbelow, then the change we made does affect ybecause a view is just a window into\nthe same memory.\n>>> x=np.ones(( 3,3))\n>>> y=x[:2,:2]# view of upper left piece\n>>> x[0,0]= 999 # change value\n>>> x\narray([[999., 1., 1.],\n[ 1., 1., 1.],\n[ 1., 1., 1.]])\n>>> y\narray([[999., 1.],\n[ 1., 1.]])\nNote that if you want to explicitly force a copy without any indexing tricks, you\ncan do y=x.copy() . The code below works through another example of advanced\nindexing versus slicing.\n>>> x=np.arange( 5)# create array\n>>> x\narray([0, 1, 2, 3, 4])\n>>> y=x[[0,1,2]]# index by integer list to force copy\n>>> y\narray([0, 1, 2])\n>>> z=x[:3] # slice creates view", "20": "8 1 Getting Started with Scienti\ufb01c Python\n>>> z # note y and z have same entries\narray([0, 1, 2])\n>>> x[0]=999 # change element of x\n>>> x\narray([999, 1, 2, 3, 4])\n>>> y # note y is unaffected,\narray([0, 1, 2])\n>>> z # but z is (it's a view).\narray([999, 1, 2])\nIn this example, yis a copy, not a view, because it was created using advanced\nindexing whereas zwas created using slicing. Thus, even though yandzhave the\nsame entries, only zis affected by changes to x. Note that the flags property of\nNumpy arrays can help sort this out until you get used to it.\nManipulating memory using views is particularly powerful for signal and image\nprocessing algorithms that require overlapping fragments of memory. The following\nis an example of how to use advanced Numpy to create overlapping blocks that do\nnot actually consume additional memory,\n>>> from numpy.lib.stride_tricks import as_strided\n>>> x=np.arange( 16,dtype =np.int64)\n>>> y=as_strided(x,( 7,4),(16,8))# overlapped entries\n>>> y\narray([[ 0, 1, 2, 3],\n[ 2 ,3 ,4 ,5 ] ,[ 4 ,5 ,6 ,7 ] ,\n[ 6 ,7 ,8 ,9 ] ,\n[ 8, 9, 10, 11],\n[10, 11, 12, 13],\n[12, 13, 14, 15]])\nThe above code creates a range of integers and then overlaps the entries to create a\n7x4Numpy array. The \ufb01nal argument in the as_strided function are the strides,\nwhich are the steps in bytes to move in the row and column dimensions, respectively.Thus, the resulting array steps eight bytes in the column dimension and sixteen bytes\nin the row dimension. Because the integer elements in the Numpy array are eight\nbytes, this is equivalent to moving by one element in the column dimension and bytwo elements in the row dimension. The second row in the Numpy array starts at\nsixteen bytes (two elements) from the \ufb01rst entry (i.e., 2) and then proceeds by eight\nbytes (by one element) in the column dimension (i.e., 2,3,4,5 ). The important\npart is that memory is re-used in the resulting 7x4Numpy array. The code below\ndemonstrates this by reassigning elements in the original xarray. The changes show\nup in the yarray because they point at the same allocated memory.\n>>> x[:: 2]=99 # assign every other value\n>>> x\narray([99, 1, 99, 3, 99, 5, 99, 7, 99, 9, 99, 11, 99, 13, 99, 15])", "21": "1.2 Numpy 9\n>>> y# the changes appear because y is a view\narray([[99, 1, 99, 3],\n[99, 3, 99, 5],\n[99, 5, 99, 7],\n[99, 7, 99, 9],[99, 9, 99, 11],\n[99, 11, 99, 13],\n[99, 13, 99, 15]])\nBear in mind that as_strided does not check that you stay within memory block\nbounds. So, if the size of the target matrix is not \ufb01lled by the available data, the\nremaining elements will come from whatever bytes are at that memory location. Inother words, there is no default \ufb01lling by zeros or other strategy that defends memory\nblock bounds. One defense is to explicitly control the dimensions as in the following\ncode:\n>>> n=8 # number of elements\n>>> x=np.arange(n) # create array\n>>> k=5 # desired number of rows\n>>> y=as_strided(x,(k,n -k+1),(x .itemsize,) *2)\n>>> y\narray([[0, 1, 2, 3],\n[1, 2, 3, 4],[2, 3, 4, 5],\n[3, 4, 5, 6],\n[4, 5, 6, 7]])\n1.2.2 Numpy Matrices\nMatrices in Numpy are similar to Numpy arrays but they can only have two dimen-sions. They implement row\u2013column matrix multiplication as opposed to element-wise multiplication. If you have two matrices you want to multiply, you can either\ncreate them directly or convert them from Numpy arrays. For example, the following\nshows how to create two matrices and multiply them.\n>>> import numpy asnp\n>>> A=np.matrix([[ 1,2,3],[4,5,6],[7,8,9]])\n>>> x=np.matrix([[ 1],[0],[0]])\n>>> A*x\nmatrix([[1],\n[4],\n[7]])\nThis can also be done using arrays as shown below", "22": "10 1 Getting Started with Scienti\ufb01c Python\n>>> A=np.array([[ 1,2,3],[4,5,6],[7,8,9]])\n>>> x=np.array([[ 1],[0],[0]])\n>>> A.dot(x)\narray([[1],\n[4],\n[7]])\nNumpy arrays support element-wise multiplication, not row\u2013column multiplication.\nYou must use Numpy matrices for this kind of multiplication unless use the innerproduct np.dot , which also works in multiple dimensions (see np.tensordot for\nmore general dot products). Note that Python 3.x has a new @notation for matrix\nmultiplication so we can re-do the last calculation as follows:\n>>> A\n@x\narray([[1],\n[4],\n[7]])\nIt is unnecessary to cast all multiplicands to matrices for multiplication. In the\nnext example, everything until last line is a Numpy array and thereafter we cast the\narray as a matrix with np.matrix which then uses row\u2013column multiplication. Note\nthat it is unnecessary to cast the xvariable as a matrix because the left-to-right order\nof the evaluation takes care of that automatically. If we need to use Aas a matrix\nelsewhere in the code then we should bind it to another variable instead of re-casting\nit every time. If you \ufb01nd yourself casting back and forth for large arrays, passing thecopy=False \ufb02ag to matrix avoids the expense of making a copy.\n>>> A=np.ones(( 3,3))\n>>> type (A) # array not matrix\n<class 'numpy.ndarray'>\n>>> x=np.ones(( 3,1))# array not matrix\n>>> A*x\narray([[1., 1., 1.],\n[1., 1., 1.],\n[1., 1., 1.]])\n>>> np.matrix(A) *x# row-column multiplication\nmatrix([[3.],\n[3.],\n[3.]])\n1.2.3 Numpy Broadcasting\nNumpy broadcasting is a powerful way to make implicit multidimensional grids forexpressions. It is probably the single most powerful feature of Numpy and the mostdif\ufb01cult to grasp. Proceeding by example, consider the vertices of a two-dimensional\nunit square as shown below", "23": "1.2 Numpy 11\n>>> X,Y=np.meshgrid(np .arange( 2),np .arange( 2))\n>>> X\narray([[0, 1],\n[0, 1]])\n>>> Y\narray([[0, 0],\n[1, 1]])\nNumpy\u2019s meshgrid creates two-dimensional grids. The XandYarrays have cor-\nresponding entries match the coordinates of the vertices of the unit square (e.g.,\n(0,0), (0,1), (1,0), (1,1)). To add the x and y-coordinates, we could use XandY\nas in X+Yshown below, The output is the sum of the vertex coordinates of the unit\nsquare.\n>>> X+Y\narray([[0, 1],\n[1, 2]])\nBecause the two arrays have compatible shapes, they can be added together element-\nwise. It turns out we can skip a step here and not bother with meshgrid to implicitly\nobtain the vertex coordinates by using broadcasting as shown below\n>>> x=np.array([ 0,1])\n>>> y=np.array([ 0,1])\n>>> x\narray([0, 1])\n>>> y\narray([0, 1])\n>>> x+y[:, None ]# add broadcast dimension\narray([[0, 1],\n[1, 2]])\n>>> X+Y\narray([[0, 1],\n[1, 2]])\nOn line 7 the None Python singleton tells Numpy to make copies of yalong this\ndimension to create a conformable calculation. Note that np.newaxis can be used\ninstead of None to be more explicit. The following lines show that we obtain the\nsame output as when we used the X+YNumpy arrays. Note that without broadcasting\nx+y=array([0, 2]) which is not what we are trying to compute. Let\u2019s continue\nwith a more complicated example where we have differing array shapes.\n>>> x=np.array([ 0,1])\n>>> y=np.array([ 0,1,2])\n>>> X,Y =np.meshgrid(x,y)\n>>> X\narray([[0, 1],", "24": "12 1 Getting Started with Scienti\ufb01c Python\n[0, 1],\n[0, 1]])\n>>> Y\narray([[0, 0],\n[1, 1],\n[2, 2]])\n>>> X+Y\narray([[0, 1],\n[1, 2],[2, 3]])\n>>> x+y[:, None ]# same as with meshgrid\narray([[0, 1],\n[1, 2],\n[2, 3]])\nIn this example, the array shapes are different, so the addition of xandyis\nnot possible without Numpy broadcasting. The last line shows that broadcasting\ngenerates the same output as using the compatible array generated by meshgrid .\nThis shows that broadcasting works with different array shapes. For the sake ofcomparison, on line 3, meshgrid creates two conformable arrays, XandY.O nt h e\nlast line, x+y[:,None] produces the same output as X+Ywithout the meshgrid .W e\ncan also put the None dimension on the xarray as x[:,None]+y which would give\nthe transpose of the result.\nBroadcasting works in multiple dimensions also. The output shown has shape\n(4,3,2) . On the last line, the x+y[:,None] produces a two-dimensional array\nwhich is then broadcast against z[:,None,None] , which duplicates itself along the\ntwoadded dimensions to accommodate the two-dimensional result on its left (i.e., x\n+y[:,None] ). The caveat about broadcasting is that it can potentially create large,\nmemory-consuming, intermediate arrays. There are methods for controlling this by\nre-using previously allocated memory but that is beyond our scope here. Formulasin physics that evaluate functions on the vertices of high dimensional grids are great\nuse-cases for broadcasting.\n>>> x=np.array([ 0,1])\n>>> y=np.array([ 0,1,2])\n>>> z=np.array([ 0,\n1,2,3])\n>>> x+y[:, None ]+z[:, None ,None ]\narray([[[0, 1],\n[1, 2],\n[2, 3]],\n[[1, 2],\n[2, 3],[3, 4]],", "25": "1.2 Numpy 13\n[[2, 3],\n[3, 4],\n[4, 5]],\n[[3, 4],\n[4, 5],[5, 6]]])\n1.2.4 Numpy Masked Arrays\nNumpy provides a powerful method to temporarily hide array elements without\nchanging the shape of the array itself,\n>>> from numpy import ma# import masked arrays\n>>> x=np.arange( 10)\n>>> y=ma.masked_array(x, x <5)\n>>> print (y)\n[-- -- -- -- -- 5 6 7 8 9]\n>>> print (y.shape)\n(10,)\nNote that the elements in the array for which the logical condition ( x<5)i st r u ea r e\nmasked, but the size of the array remains the same. This is particularly useful in\nplotting categorical data, where you may only want those values that correspond to\na given category for part of the plot. Another common use is for image processing,wherein parts of the image may need to be excluded from subsequent processing.\nNote that creating a masked array does not force an implicit copy operation unless\ncopy=True argument is used. For example, changing an element in xdoes change\nthe corresponding element in y, even though yi sam a s k e da r r a y ,\n>>> x[-1]=9 9 # change this\n>>> print (x)\n[ 012345678 9 9 ]>>> print (y)# masked array changed!\n[-- -- -- -- -- 5 6 7 8 99]\n1.2.5 Floating-Point Numbers\nThere are precision limitations when representing \ufb02oating-point numbers on a com-\nputer with \ufb01nite memory. For example, the following shows these limitations when\nadding two simple numbers,\n>>> 0.1 + 0.2\n0.30000000000000004", "26": "14 1 Getting Started with Scienti\ufb01c Python\nSo, then, why is the output not 0.3? The issue is the \ufb02oating-point representation of\nthe two numbers and the algorithm that adds them. To represent an integer in binary,\nwe just write it out in powers of 2. For example, 230 =(11100110 )2. Python can\ndo this conversion using string formatting,\n>>> print ('{0:b}' .format( 230))\n11100110\nTo add integers, we just add up the corresponding bits and \ufb01t them into the allowable\nnumber of bits. Unless there is an over\ufb02ow (the results cannot be represented withthat number of bits), then there is no problem. Representing \ufb02oating point is trickier\nbecause we have to represent these numbers as binary fractions. The IEEE 754\nstandard requires that \ufb02oating-point numbers be represented as \u00b1C\u00d72\nEwhere Cis\nthe signi\ufb01cand ( mantissa ) and Eis the exponent.\nTo represent a regular decimal fraction as binary fraction, we need to compute\nthe expansion of the fraction in the following form a1/2+a2/22+a3/23...In other\nwords, we need to \ufb01nd the aicoef\ufb01cients. We can do this using the same process we\nwould use for a decimal fraction: just keep dividing by the fractional powers of 1 /2\nand keep track of the whole and fractional parts. Python\u2019s divmod function can do\nmost of the work for this. For example, to represent 0.125 as a binary fraction,\n>>> a= 0.125\n>>> divmod (a*2,1)\n(0.0, 0.25)\nThe \ufb01rst item in the tuple is the quotient and the other is the remainder. If the quotient\nwas greater than 1, then the corresponding aiterm is one and is zero otherwise. For\nthis example, we have a1=0. To get the next term in the expansion, we just keep\nmultiplying by 2which moves us rightward along the expansion to ai+1and so on.\nThen,\n>>> a= 0.125\n>>> q,a =divmod (a*2,1)\n>>> print (q,a)\n0.0 0.25\n>>> q,a =divmod (a*2,1)\n>>> print (q,a)\n0.0 0.5\n>>> q,a =divmod (a*2,1)\n>>> print (q,a)\n1.0 0.0\nThe algorithm stops when the remainder term is zero. Thus, we have that 0 .125=\n(0.001 )2. The speci\ufb01cation requires that the leading term in the expansion be one.\nT h u s ,w eh a v e0 .125= (1.000 )\u00d72\u22123. This means the signi\ufb01cand is 1and the\nexponent is -3.\nNow, let\u2019s get back to our main problem 0.1+0.2 by developing the representation\n0.1by coding up the individual steps above.", "27": "1.2 Numpy 15\n>>> a= 0.1\n>>> bits =[]\n>>> while a>0:\n... q,a =divmod (a*2,1)\n... bits .append(q)\n...>>> print (''.join([ '%d'%ifor iinbits]))\n0001100110011001100110011001100110011001100110011001101\nNote that the representation has an in\ufb01nitely repeating pattern. This means that we\nhave (1.\n1001 )2\u00d72\u22124. The IEEE standard does not have a way to represent in\ufb01nitely\nrepeating sequences. Nonetheless, we can compute this,\n\u221e/summationdisplay\nn=11\n24n\u22123+1\n24n=3\n5\nThus, 0 .1\u22481.6\u00d72\u22124. Per the IEEE 754 standard, for float type, we have\n24-bits for the signi\ufb01cand and 23-bits for the fractional part. Because we can-\nnot represent the in\ufb01nitely repeating sequence, we have to round off at 23-bits,10011001100110011001101 . Thus, whereas the signi\ufb01cand\u2019s representation used\nto be 1.6, with this rounding, it is Now\n>>> b='10011001100110011001101'\n>>> 1+sum([int(i)/(2**n)for n,i inenumerate (b,1)])\n1.600000023841858\nThus, we now have 0 .1\u22481.600000023841858 \u00d72\n\u22124=0.10000000149011612. For\nthe0.2expansion, we have the same repeating sequence with a different exponent,\nso that we have 0 .2\u22481.600000023841858 \u00d72\u22123=0.20000000298023224. To\nadd0.1+0.2 in binary, we must adjust the exponents until they match the higher of\nthe two. Thus,\n0.11001100110011001100110\n+1.10011001100110011001101\n--------------------------10.01100110011001100110011\nNow, the sum has to be scaled back to \ufb01t into the signi\ufb01cand\u2019s available bits so the\nresult is 1.00110011001100110011010 with exponent -2. Computing this in the\nusual way as shown below gives the result.\n>>> k='00110011001100110011010'\n>>> print ('%0.12f '%((1+sum([int(i)/(2**n)\n... for n,i inenumerate (k,1)])) /2**2 ))\n0.300000011921\nwhich matches what we get with numpy", "28": "16 1 Getting Started with Scienti\ufb01c Python\n>>> import numpy asnp\n>>> print ('%0.12f '%(np.float32( 0.1)+np.float32( 0.2)))\n0.300000011921\nThe entire process proceeds the same for 64-bit \ufb02oats. Python has a fractions\nanddecimal modules that allow more exact number representations. The decimal\nmodule is particularly important for certain \ufb01nancial computations.\nRound-off Error . Let\u2019s consider the example of adding 100,000,000 and10in\n32-bit \ufb02oating point.\n>>> print ('{0:b}' .format( 100000000 ))\n101111101011110000100000000\nThis means that 100 ,000 ,000=(1.01111101011110000100000000 )2\u00d7226.L i k e -\nwise, 10 =(1.010 )2\u00d723. To add these we have to make the exponents match as in\nthe following,\n1.01111101011110000100000000\n+0.00000000000000000000001010\n-------------------------------\n1.01111101011110000100001010\nNow, we have to round off because we only have 23 bits to the right of the decimal\npoint and obtain 1.0111110101111000010000 , thus losing the trailing 10bits.\nThis effectively makes the decimal 10 =(1010 )2we started out with become 8 =\n(1000 )2. Thus, using Numpy again,\n>>> print (format(np .float32( 100000000 )+np.float32( 10),'10.3f' ))\n100000008.000\nThe problem here is that the order of magnitude between the two numbers was so\ngreat that it resulted in loss in the signi\ufb01cand\u2019s bits as the smaller number was right-\nshifted. When summing numbers like these, the Kahan summation algorithm (see\nmath.fsum() ) can effectively manage these round-off errors.\n>>> import math\n>>> math .fsum([np .float32( 100000000 ),np .float32( 10)])\n100000010.0\nCancelation Error . Cancelation error (loss of signi\ufb01cance) results when two nearly\nequal \ufb02oating-point numbers are subtracted. Let\u2019s consider subtracting 0.1111112\nand0.1111111 . As binary fractions, we have the following,\n1.11000111000111001000101 E-4\n-1.11000111000111000110111 E-4\n---------------------------\n0.00000000000000000011100", "29": "1.2 Numpy 17\nAs a binary fraction, this is 1.11 with exponent -23 or (1.75)10\u00d72\u221223\u2248\n0.00000010430812836. In Numpy, this loss of precision is shown in the following:\n>>> print (format(np .float32( 0.1111112 )-np.float32( 0.1111111 ),'1.17f' ))\n0.00000010430812836\nTo sum up, when using \ufb02oating point, you must check for approximate equality us-\ning something like Numpy allclose instead of the usual Python equality (i.e., ==)\nsign. This enforces error bounds instead of strict equality. Whenever practicable, use\n\ufb01xed scaling to employ integer values instead of decimal fractions. Double preci-sion 64-bit \ufb02oating-point numbers are much better than single precision and, while\nnot eliminating these problems, effectively kicks the can down the road for all but\nthe strictest precision requirements. The Kahan algorithm is effective for summing\ufb02oating point numbers across very large data without accruing round-off errors. To\nminimize cancelation errors, re-factor the calculation to avoid subtracting two nearly\nequal numbers.\n1.2.6 Numpy Optimizations and Prospectus\nThe scienti\ufb01c Python community continues to push the frontier of scienti\ufb01c com-puting. Several important extensions to Numpy are under active development. First,\nNumba is a compiler that generates optimized machine code from pure-Python code\nusing the LLVM compiler infrastructure. LLVM started as a research project at the U-niversity of Illinois to provide a target-independent compilation strategy for arbitrary\nprogramming languages and is now a well-established technology. The combination\nof LLVM and Python via Numba means that accelerating a block of Python code canbe as easy as putting a @numba.jit decorator above the function de\ufb01nition, but this\ndoesn\u2019t work for all situations. Numba can target general graphics processing units\n(GPGPUs) also.\nThe Dask project contains dask.array extensions for manipulating very large\ndatasets that are too big to \ufb01t in a single computer\u2019s RAM (i.e., out of core) usingNumpy semantics. Furthermore, dask includes extensions for Pandas dataframes\n(see Sect. 1.7). Roughly speaking, this means that dask understands how to un-\npack Python expressions and translate them for a variety of distributed backend dataservices upon which the computing takes place. This means that dask separates\nthe expression of the computation from the particular implementation on a given\nbackend.\n1.3 Matplotlib\nMatplotlib is the primary visualization tool for scienti\ufb01c graphics in Python. Like allgreat open-source projects, it originated to satisfy a personal need. At the time of its\ninception, John Hunter primarily used MATLAB for scienti\ufb01c visualization, but as\nhe began to integrate data from disparate sources using Python, he realized he needed", "30": "18 1 Getting Started with Scienti\ufb01c Python\na Python solution for visualization, so he single-handedly wrote Matplotlib. Since\nthose early years, Matplotlib has displaced the other competing methods for two-\ndimensional scienti\ufb01c visualization and today is a very actively maintained project,even without John Hunter, who sadly passed away in 2012.\nJohn had a few basic requirements for Matplotlib:\n\u2022Plots should look publication quality with beautiful text.\n\u2022Plots should output Postscript for inclusion within L\nATEX documents and publica-\ntion quality printing.\n\u2022Plots should be embeddable in a graphical user interface (GUI) for application\ndevelopment.\n\u2022The code should be mostly Python to allow for users to become developers.\n\u2022Plots should be easy to make with just a few lines of code for simple graphs.\nEach of these requirements has been completely satis\ufb01ed and Matplotlib\u2019s capabili-\nties have grown far beyond these requirements. In the beginning, to ease the transition\nfrom MATLAB to Python, many of the Matplotlib functions were closely named af-\nter the corresponding MATLAB commands. The community has moved away fromthis style and, even though you may still \ufb01nd the old MATLAB-esque style used in\nthe online Matplotlib documentation.\nThe following shows the quickest way to draw a plot using Matplotlib and the\nplain Python interpreter. Later, we\u2019ll see how to do this even faster using IPython. The\n\ufb01rst line imports the requisite module as pltwhich is the recommended convention.\nThe next line plots a sequence of numbers generated using Python\u2019s range object.\nNote the output list contains a Line2D object. This is an artist in Matplotlib parlance.\nFinally, the plt.show() function draws the plot in a GUI \ufb01gure window.\nimport matplotlib.pyplot asplt\nplt.plot( range (10))\nplt.show() # unnecessary in IPython (discussed later)\nIf you try this in your own plain Python interpreter (and you should!), you will\nsee that you cannot type in anything further in the interpreter until the \ufb01gure window(i.e., something like Fig. 1.1) is closed. This is because the plt.show() function\npreoccupies the interpreter with the controls in the GUI and blocks further interaction.\nAs we discuss below, IPython provides ways to get around this blocking so you cansimultaneously interact with the interpreter and the \ufb01gure window.\n3\nAs shown in Fig. 1.1,t h e plot function returns a list containing the Line2D ob-\nject. More complicated plots yield larger lists \ufb01lled with artists . The terminology is\nthat artists draw on the canvas contained in the Matplotlib \ufb01gure. The \ufb01nal line is the\nplt.show function that provokes the embedded artists to render on the Matplotlib\ncanvas. The reason this is a separate function is that plots may have dozens of com-plicated artists and rendering may be a time-consuming task to only be undertaken at\n3You can also do this in the plain Python interpreter by doing import matplotlib;matplotlib.\ninteractive(True) .", "31": "1.3 Matplotlib 19\nFig. 1.1 The Matplotlib \ufb01gure window. The icons on the bottom allow some limited plot-editing\ntools\nthe end, when all the artists have been mustered. Matplotlib supports plotting images,\ncontours, and many others that we cover in detail in the following chapters.\nEven though this is the quickest way to draw a plot in Matplotlib, it is not recom-\nmended because there are no handles to the intermediate products of the plot suchas the plot\u2019s axis. While this is okay for a simple plot like this, later on we will see\nhow to construct complicated plots using the recommended method.\nOne of the best ways to get started with Matplotlib is to browse the extensive online\ngallery of plots on the main Matplotlib site. Each plot comes with corresponding\nsource code that you can use as a starting point for your own plots. In Sect. 1.4,w e\ndiscuss special magic commands that make this particularly easy. The annual John\nHunter: Excellence in Plotting Contest provides fantastic, compelling examples of\nscienti\ufb01c visualizations that are possible using Matplotlib.\n1.3.1 Alternatives to Matplotlib\nEven though Matplotlib is the most complete option for script-based plotting, there\nare some alternatives for specialized scienti\ufb01c graphics that may be of interest.", "32": "20 1 Getting Started with Scienti\ufb01c Python\nIf you require real-time data display and tools for volumetric data rendering and\ncomplicated 3D meshes with isosurfaces, then PyQtGraph is an option. PyQtGraph\nis a pure-Python graphics and GUI library that depends on Python bindings for theQt GUI library (i.e., PySide orPyQt4 ) and Numpy. This means that the PyQtGraph\nrelies on these other libraries (especially Qt\u2019s GraphicsView framework) for the\nheavy-duty number crunching and rendering. This package is actively maintained,with solid documentation. You also need to grasp a few Qt-GUI development con-\ncepts to use this effectively.\nAn alternative that comes from the Rcommunity is ggplot which is a Python\nport of the ggplot2 package that is fundamental to statistical graphics in R.F r o m\nthe Python standpoint, the main advantage of ggplot is the tight integration with\nthe Pandas dataframe, which makes it easy to draw beautifully formatted statisticalgraphs. The downside of this package is that it applies un-Pythonic semantics based\non the Grammar of Graphics [2], which is nonetheless a well-thought-out method\nfor articulating complicated graphs. Of course, because there are two-way bridgesbetween Python and Rvia the R2Py module (among others), it is workable to send\nNumpy arrays to Rfor native ggplot2 rendering and then retrieve the so-computed\ngraphic back into Python. This is a work\ufb02ow that is lubricated by the Jupyter Note-\nbook (see below) via the rmagic extension. Thus, it is quite possible to get the best\nof both worlds via the Jupyter Notebook and this kind of multi-language work\ufb02owis quite common in data analysis communities.\n1.3.2 Extensions to Matplotlib\nInitially, to encourage adoption of Matplotlib from MATLAB, many of the graphical\nsensibilities were adopted from MATLAB to preserve the look and feel for tran-\nsitioning users. Modern sensibilities and prettier default plots are possible becauseMatplotlib provides the ability to drill down and tweak every element on the canvas.\nHowever, this can be tedious to do and several alternatives offer relief. For statistical\nplots, the \ufb01rst place to look is the seaborn module that includes a vast array of\nbeautifully formatted plots including violin plots, kernel density plots, and bivariate\nhistograms. The seaborn gallery includes samples of available plots and the corre-\nsponding code that generates them. Note that importing seaborn hijacks the default\nsettings for all plots, so you have to coordinate this if you only want to use seaborn\nfor some (not all) of your visualizations in a given session. Note that you can \ufb01nd\nthe defaults for Matplotlib in the matplotlib.rcParams dictionary.\n1.4 IPython\nIPython [ 3] originated as a way to enhance Python\u2019s basic interpreter for smooth\ninteractive scienti\ufb01c development. In the early days, the most important enhancement", "33": "1.4 IPython 21\nwas tab completion for dynamic introspection of workspace variables. For example,\nyou can start IPython at the commandline by typing ipython and then you should\nsee something like the following in your terminal:\nPython 2.7.11 |Continuum Analytics, Inc.| (default, Dec 7 2015, 14:00\nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 4.0.0 -- An enhanced Interactive Python.\n? -> Introduction and overview of IPython's features.%%quickref -> Quick reference.help -> Python's own help system.object? -> Details about 'object', use 'object??' for extra details.\nIn [1]:\nNext, creating a string as shown and hitting the TABkey after the dotcharacter\ninitiates the introspection, showing all the functions and attributes of the string\nobject in x.\nIn [1]: x = 'this is a string'\nIn [2]: x.<TAB>\nx.capitalize x.format x.isupper x.rindex x.strip\nx.center x.index x.join x.rjust x.swapcase\nx.count x.isalnum x.ljust x.rpartition x.title\nx.decode x.isalpha x.lower x.rsplit x.translatex.encode x.isdigit x.lstrip x.rstrip x.upper\nx.endswith x.islower x.partition x.split x.zfill\nx.expandtabs x.isspace x.replace x.splitlines\nx.find x.istitle x.rfind x.startswith\nTo get help about any of these, you simply add the ?character at the end as shown\nbelow\nIn [2]: x.center?\nType: builtin_function_or_methodString Form:<built-in method center of str object at 0x03193390>\nDocstring:\nS.center(width[, fillchar]) -> string\nReturn S centered in a string of length width. Padding is\ndone using the specified fill character (default is a space)\nand IPython provides the built-in help documentation. Note that you can also get this\ndocumentation with help(x.center) which works in the plain Python interpreter\nas well.\nThe combination of dynamic tab-based introspection and quick interactive help\naccelerates development because you can keep your eyes and \ufb01ngers in one place asyou work. This was the original IPython experience, but IPython has since grown\ninto a complete framework for delivering a rich scienti\ufb01c computing work\ufb02ow that\nretains and enhances these fundamental features.", "34": "22 1 Getting Started with Scienti\ufb01c Python\n1.5 Jupyter Notebook\nAs you may have noticed investigating Python on the web, most Python users are\nweb developers, not scienti\ufb01c programmers, meaning that the Python stack is very\nwell developed for web technologies. The genius of the IPython development team\nwas to leverage these technologies for scienti\ufb01c computing by embedding IPython\nin modern web browsers. In fact, this strategy has been so successful that IPythonhas moved into other languages beyond Python such as Julia and R as the Jupyter\nproject. You can start the Jupyter Notebook with the following commandline:\nTerminal> jupyter notebook\nAfter starting the notebook, you should see something like the following in the\nterminal:\n[I 16:08:21.213 NotebookApp] Serving notebooks from local directory: /home/user\n[I 16:08:21.214 NotebookApp] The Jupyter Notebook is running at:[I 16:08:21.214 NotebookApp] http://localhost:8888/?token=80281f0c324924d34a4e[I 16:08:21.214 NotebookApp] Use Control-C to stop this server and shut down\nThe \ufb01rst line reveals where Jupyter looks for default settings. The next line shows\nwhere it looks for documents in the Jupyter Notebook format. The third line showsthat the Jupyter Notebook started a web server on the local machine (i.e., 127.0.0.1 )\non port number 8888 . This is the address your browser needs to connect to the\nJupyter session although your default browser should have opened automatically to\nthis address. The port number and other con\ufb01guration options are available either\non the commandline or in the pro\ufb01le \ufb01le shown in the \ufb01rst line. If you are on aWindows platform and you do not get this far, then the Window\u2019s \ufb01rewall is prob-\nably blocking the port. For additional con\ufb01guration help, see the main Jupyter site\n(www.jupyter.org ).\nWhen Jupyter starts, it initiates several Python processes that use the blazing-\nfastZeroMQ message passing framework for interprocess communication, along\nwith the web-sockets protocol for back-and-forth communication with the brows-er. To start Jupyter and get around your default browser, you can use the ad-\nditonal --no-browser \ufb02ag and then manually type in the local host address\nhttp://127.0.0.1:8888 into your favorite browser to get started. Once all that is\nsettled, you should see something like the following Fig. 1.2,\nYou can create a new document by clicking the New Notebook button shown\nin Fig. 1.2. Then, you should see something like Fig. 1.3. To start using the Jupyter\nNotebook, you just start typing code in the shaded textbox and then hit SHIFT+ENTER\nto execute the code in that Jupyter cell. Figure 1.4shows the dynamic introspection\nin the pulldown menu when you type the TABkey after the x.. Context-based help is\nalso available as before by using the ?suf\ufb01x which opens a help panel at the bottom\nof the browser window. There are many amazing features including the ability toshare notebooks between different users and to run Jupyter Notebooks in the Amazon\ncloud, but these features go beyond our scope here. Check the jupyter.org website\nor peek at the mailing list for the latest work on these fronts.", "35": "1.5 Jupyter Notebook 23\nFig. 1.2 The Jupyter Notebook dashboard\nFig. 1.3 A new Jupyter Notebook\nThe Jupyter Notebook supports high-quality mathematical typesetting using\nMathJaX, which is a JavaScript implementation of most of L ATEX, as well as video\nand other rich content. The concept of consolidating mathematical algorithm de-scriptions and the code that implements those algorithms into a shareable document\nis more important than all of these amazing features. There is no understating the\nimportance of this in practice because the algorithm documentation (if it exists) isusually in one format and completely separate from the code that implements it.\nThis common practice leads to un-synchronized documentation and code that ren-\nders one or the other useless. The Jupyter Notebook solves this problem by puttingeverything into a living shareable document based upon open standards and freely", "36": "24 1 Getting Started with Scienti\ufb01c Python\nFig. 1.4 Jupyter Notebook pulldown completion menu\navailable software. Jupyter Notebooks can even be saved as static HTML documents\nfor those without Python!\nFinally, Jupyter provides a large set of magic commands for creating macros,\npro\ufb01ling, debugging, and viewing codes. A full list of these can be found by typingin%lsmagic in Jupyter. Help on any of these is available using the ?character suf\ufb01x.\nSome frequently used commands include the %cdcommand that changes the current\nworking directory, the %lscommand that lists the \ufb01les in the current directory, and the\n%hist command that shows the history of previous commands (including optional\nsearching). The most important of these for new users is probably the %loadpy\ncommand that can load scripts from the local disk or from the web. Using this toexplore the Matplotlib gallery is a great way to experiment with and re-use the plots\nthere.\n1.6 Scipy\nScipy was the \ufb01rst consolidated module for a wide range of compiled libraries, al-\nl based on Numpy arrays. Scipy includes numerous special functions (e.g., Airy,\nBessel, elliptical) as well as powerful numerical quadrature routines via the QUAD-\nPACK Fortran library (see scipy.integrate ), where you will also \ufb01nd other\nquadrature methods. Note that some of the same functions appear in multiple places", "37": "1.6 Scipy 25\nwithin Scipy itself as well as in Numpy. Additionally, Scipy provides access to the\nODEPACK library for solving differential equations. Lots of statistical functions,\nincluding random number generators, and a wide variety of probability distributionsare included in the scipy.stats module. Interfaces to the Fortran MINPACK op-\ntimization library are provided via scipy.optimize . These include methods for\nroot-\ufb01nding, minimization and maximization problems, with and without higher or-der derivatives. Methods for interpolation are provided in the scipy.interpolate\nmodule via the FITPACK Fortran package. Note that some of the modules are so\nbig that you do not get all of them with import scipy because that would take too\nlong to load. You may have to load some of these packages individually as import\nscipy.interpolate , for example.\nAs we discussed, the Scipy module is already packed with an extensive list of\nscienti\ufb01c codes. For that reason, the scikits modules were originally established\nas a way to stage candidates that could eventually make it into the already stuffed\nScipy module, but it turns out that many of these modules became so successful ontheir own that they will never be integrated into Scipy proper. Some examples include\nsklearn for machine learning and scikit-image for image processing.\n1.7 Pandas\nPandas [ 4] is a powerful module that is optimized on top of Numpy and provides\na set of data structures particularly suited to time series and spreadsheet-style data\nanalysis (think of pivot tables in Excel). If you are familiar with the Rstatistical\npackage, then you can think of Pandas as providing a Numpy-powered dataframe forPython.\n1.7.1 Series\nThere are two primary data structures in Pandas. The \ufb01rst is the Series object which\ncombines an index and corresponding data values.\n>>> import pandas aspd# recommended convention\n>>> x=pd.Series(index =range (5),data =[1,3,9,11,12])\n>>> x\n01\n1329\n31 1\n41 2\ndtype: int64", "38": "26 1 Getting Started with Scienti\ufb01c Python\nThe main thing to keep in mind with Pandas is that these data structures were o-\nriginally designed to work with time-series data. In that case, the index in the data\nstructures corresponds to a sequence of ordered time stamps. In the general case, theindex must be a sort-able array-like entity. For example,\n>>> x=pd.Series(index =['a','b','d','z','z'],data =[1,3,9,11,12])\n>>> x\na1b3d9z1 1z1 2\ndtype: int64\nNote the duplicated zentries in the index . We can get at the entries in the Series\nin a number of ways. First, we can used the dotnotation to select as in the following:\n>>> x.a\n1>>> x.z\nz1 1\nz1 2dtype: int64\nWe can also use the indexed position of the entries with iloc as in the following:\n>>> x.iloc[: 3]\na1\nb3d9\ndtype: int64\nwhich uses the same slicing syntax as Numpy arrays. You can also slice across the\nindex , even if it is not numeric with locas in the following:\n>>> x.loc[ 'a':'d']\na1\nb3\nd9dtype: int64\nwhich you can get directly from the usual slicing notation:>>> x['a':'d']\na1\nb3\nd9\ndtype: int64", "39": "1.7 Pandas 27\nNote that, unlike Python, slicing this way includes the endpoints. While that is\nvery interesting, the main power of Pandas comes from its power to aggregate and\ngroup data. In the following, we build a more interesting Series object:\n>>> x=pd.Series( range (5),[1,2,11,9,10])\nand then group it in the following:\n>>> grp=x.groupby( lambda i:i%2)# odd or even\n>>> grp.get_group( 0)# even group\n21\n10 4\ndtype: int64>>> grp.get_group( 1)# odd group\n10\n11 293\ndtype: int64\nThe \ufb01rst line groups the elements of the Series object by whether or not the index\nis even or odd. The lambda function returns 0or1depending on whether or not the\ncorresponding index is even or odd, respectively. The next line shows the 0(even)\ngroup and then the one after shows the 1(odd) group. Now, that we have separate\ngroups, we can perform a wide variety of summarizations on the group. You can think\nof these as reducing each group into a single value. For example, in the following,we get the maximum value of each group:\n>>> grp.max() # max in each group\n0413\ndtype: int64\nNote that the operation above returns another Series object with an index corre-\nsponding to the [0,1] elements.\n1.7.2 Dataframe\nThe Pandas DataFrame is an encapsulation of the Series that extends to two di-\nmensions. One way to create a DataFrame is with dictionaries as in the following:\n>>> df=pd.DataFrame({ 'col1' :[1,3,11,2],'col2' :[9,23,0,2]})\nNote that the keys in the input dictionary are now the column headings (labels) of\ntheDataFrame , with each corresponding column matching the list of corresponding\nvalues from the dictionary. Like the Series object, the DataFrame also has in\nindex , which is the [0,1,2,3] column on the far-left. We can extract elements\nfrom each column using the iloc method as discussed earlier as shown below", "40": "28 1 Getting Started with Scienti\ufb01c Python\n>>> df.iloc[: 2,:2]# get section\ncol1 col2\n01913 2 3\nor by directly slicing or by using the dotnotation as shown below\n>>> df['col1' ]# indexing\n01\n13\n21 132\nName: col1, dtype: int64\n>>> df.col1 # use dot notation\n01\n13\n21 132\nName: col1, dtype: int64\nSubsequent operations on the DataFrame preserve its column-wise structure as in\nthe following:\n>>> df.sum()\ncol1 17col2 34\ndtype: int64\nwhere each column was totaled. Grouping and aggregating with the dataframe is\neven more powerful than with Series. Let\u2019s construct the following dataframe:\n>>> df=pd.DataFrame({ 'col1' :[1,1,0,0],'col2' :[1,2,3,4]})\nIn the above dataframe, note that the col1 column has only two entries. We can\ngroup the data using this column as in the following:\n>>> grp=df.groupby( 'col1' )\n>>> grp.get_group( 0)\ncol1 col2\n203304\n>>> grp.get_group( 1)\ncol1 col2\n011\n112\nNote that each group corresponds to entries for which col1\nwas either of its two\nvalues. Now that we have grouped on col1 , as with the Series object, we can also\nfunctionally summarize each of the groups as in the following:", "41": "1.7 Pandas 29\n>>> grp.sum()\ncol2\ncol107\n13\nwhere the sumis applied across each of the Dataframes present in each group. Note\nthat the index of the output above is each of the values in the original col1 .\nThe Dataframe can compute new columns based on existing columns using the\neval method as shown below\n>>> df['sum_col' ]=df.eval( 'col1+col2' )\n>>> df\ncol1 col2 sum_col\n011 2\n112 3203 3\n304 4\nNote that you can assign the output to a new column to the Dataframe as shown.\n4\nWe can group by multiple columns as shown below>>> grp=df.groupby([ 'sum_col' ,'col1' ])\nDoing the sumoperation on each group gives the following:\n>>> res=grp.sum()\n>>> res\ncol2\nsum_col col1\n21 130 3\n12\n40 4\nThis output is much more complicated than anything we have seen so far, so let\u2019s\ncarefully walk through it. Below the headers, the \ufb01rst row 211 indicates that for\nsum_col=2 and for all values of col1 (namely, just the value 1), the value of col2\nis1. For the next row, the same pattern applies except that for sum_col=3 , there are\nnow two values for col1 , namely 0and1, which each have their corresponding two\nvalues for the sumoperation in col2 . This layered display is one way to look at the\nresult. Note that the layers above are not uniform. Alternatively, we can unstack\nthis result to obtain the following tabular view of the previous result:\n4Note this kind of on-the-\ufb02y memory extension is not possible in regular Numpy. For example,\nx = np.array([1,2]); x[3]=3 generates an error.", "42": "30 1 Getting Started with Scienti\ufb01c Python\n>>> res.unstack()\ncol2\ncol1 0 1sum_col\n2 NaN 1.0\n3 3.0 2.04 4.0 NaN\nTheNaNvalues indicate positions in the table where there is no entry. For example, for\nthe pair (sum_col=2,col2=0) , there is no corresponding value in the Dataframe,\nas you may verify by looking at the penultimate code block. There is also no entrycorresponding to the (sum_col=4,col2=1) pair. Thus, this shows that the original\npresentation in the penultimate code block is the same as this one, just without the\nabovementioned missing entries indicated by NaN.\nWe have barely scratched the surface of what Pandas is capable of and we have\ncompletely ignored its powerful features for managing dates and times. The text by\nMckinney [ 4] is a very complete and happily readable introduction to Pandas. The\nonline documentation and tutorials at the main Pandas site are also great for diving\ndeeper into Pandas.\n1.8 Sympy\nSympy [ 5] is the main computer algebra module in Python. It is a pure-Python\npackage with no platform dependencies. With the help of multiple Google Summer\nof Code sponsorships, it has grown into a powerful computer algebra system with\nmany collateral projects that make it faster and integrate it tighter with Numpy andJupyter. Sympy\u2019s online tutorial is excellent and allows interacting with its embedded\ncode samples in the browser by running the code on the Google App Engine behind\nthe scenes. This provides an excellent way to interact and experiment with Sympy.\nIf you \ufb01nd Sympy too slow or need algorithms that it does not implement, then\nSAGE is your next stop. The SAGE project is a consolidation of over 70 of the\nbest open-source packages for computer algebra and related computation. AlthoughSympy and SAGE share code freely between them, SAGE is a specialized build of\nthe Python kernel to facilitate deep integration with the underlying libraries. Thus,\nit is not a pure-Python solution for computer algebra (i.e., not as portable) and it is aproper superset of Python with its own extended syntax. The choice between SAGE\nand Sympy really depends on whether or not you intend primarily work in SAGE or\njust need occasional computer algebra support in your existing Python code.\nAn important new development regarding SAGE is the freely available SAGE\nCloud ( https://cloud.sagemath.com/ ), sponsored by University of Washington that\nallows you to use SAGE entirely in the browser with no additional setup. Both\nSAGE and Sympy offer tight integration with the Jupyter Notebook for mathematical\ntypesetting in the browser using MathJaX.", "43": "1.8 Sympy 31\nTo get started with Sympy, you must import the module as usual,\n>>> import sympy asS# might take awhile\nwhich may take a bit because it is a big package. The next step is to create a Sympy\nvariable as in the following:\n>>> x=S.symbols( 'x')\nNow we can manipulate this using Sympy functions and Python logic as shown\nbelow\n>>> p=sum(x**ifor iinrange (3))# 2nd order polynomial\n>>> p\nx**2 +x+1\nNow, we can \ufb01nd the roots of this polynomial using Sympy functions,>>> S.solve(p) # solves p == 0\n[-1/2 - sqrt(3)*I/2, -1/2 + sqrt(3)*I/2]\nThere is also a sympy.roots function that provides the same output but as a dictio-\nnary.\n>>> S.roots(p)\n{-1/2 - sqrt(3)*I/2: 1, -1/2 + sqrt(3)*I/2: 1}\nWe can also have more than one symbolic element in any expression as in the fol-\nlowing:\n>>> from sympy.abc import a,b,c # quick way to get common symbols\n>>> p=a*x**2 + b*x+c\n>>> S.solve(p,x) # specific solving for x-variable\n[(-b + sqrt(-4*a*c + b**2))/(2*a), -(b + sqrt(-4*a*c + b**2))/(2*a)]\nwhich is the usual quadratic formula for roots. Sympy also provides many mathe-\nmatical functions designed to work with Sympy variables. For example,\n>>> S.exp(S .I*a)#using Sympy exponential\nexp(I*a)\nWe can expand this using expand_complex to obtain the following:\n>>> S.expand_complex(S .exp(S .I*a))\nI*exp(-im(a))*sin(re(a)) + exp(-im(a))*cos(re(a))\nwhich gives us Euler\u2019s formula for the complex exponential. Note that Sympy does\nnot know whether or not ais itself a complex number. We can \ufb01x this by making\nthat fact part of the construction of aas in the following:", "44": "32 1 Getting Started with Scienti\ufb01c Python\n>>> a=S.symbols( 'a',real =True )\n>>> S.expand_complex(S .exp(S .I*a))\nI*sin(a) + cos(a)\nNote the much simpler output this time because we have forced the additional con-\ndition on a.\nA powerful way to use Sympy is to construct complicated expressions that you\ncan later evaluate using Numpy via the lambdify method. For example,\n>>> y=S.tan(x) *x+x**2\n>>> yf=S.lambdify(x,y, 'numpy' )\n>>> y.subs(x, .1)# evaluated using Sympy\n0.0200334672085451\n>>> yf(.1)# evaluated using Numpy\n0.020033467208545055\nAfter creating the Numpy function with lambdify , you can use Numpy arrays as\ninput as shown\n>>> yf(np .arange( 3))# input is Numpy array\narray([ 0. , 2.55740772, -0.37007973])>>> [y.subs(x,i) .evalf() for iinrange (3)] # need extra work for Sympy\n[0, 2.55740772465490, -0.370079726523038]\nWe can get the same output using Sympy, but that requires the extra programming\nlogic shown to do the vectorizing that Numpy performs natively.\nOnce again, we have merely scratched the surface of what Sympy is capable of\nand the online interactive tutorial is the best place to learn more. Sympy also allowsautomatic mathematical typesetting within the Jupyter Notebook using L\nATEXs ot h e\nso-constructed notebooks look almost publication-ready (see sympy.latex ) and\ncan be made so with the jupyter nbconvert command. This makes it easier to\njump the cognitive gap between the Python code and the symbology of traditional\nmathematics.\n1.9 Interfacing with Compiled Libraries\nAs we have discussed, Python for scienti\ufb01c computing really consists of gluing\ntogether different scienti\ufb01c libraries written in a compiled language like C or Fortran.\nUltimately, you may want to use libraries not available with existing Python bindings.\nThere are many, many options for doing this. The most direct way is to use the built-inctypes module which provides tools for providing input/output pointers to the\nlibrary\u2019s functions just as if you were calling them from a compiled language. This\nmeans that you have to know the function signatures in the library exactly \u2014how\nmany bytes for each input and how many bytes for the output. You are responsible\nfor building the inputs exactly the way the library expects and collecting the resulting", "45": "1.9 Interfacing with Compiled Libraries 33\noutputs. Even though this seems tedious, Python bindings for vast libraries have been\nbuilt this way.\nIf you want an easier way, then SWIG is an automatic wrapper generating tool\nthat can provide bindings to a long list of languages, not just Python; so if you need\nbindings for multiple languages, then this is your best and only option. Using SWIG\nconsists of writing an interface \ufb01le so that the compiled Python dynamically linkedlibrary ( .pyd \ufb01les) can be readily imported into the Python interpreter. Huge and\ncomplex libraries like Trilinos (Sandia National Labs) have been interfaced to Python\nusing SWIG , so it is a well-tested option. SWIG also supports Numpy arrays.\nHowever, the SWIG model assumes that you want to continue developing primarily\nin C/Fortran and you are hooking into Python for usability or other reasons. On the\nother hand, if you start developing algorithms in Python and then want to speed themup, then Cython is an excellent option because it provides a mixed language that\nallows you to have both C language and Python code intermixed. Like SWIG , you\nhave to write additional \ufb01les in this hybrid Python/C dialect to have Cython generate\nthe C-code that you will ultimately compile. The best part of Cython is the pro\ufb01ler\nthat can generate an HTML report showing where the code is slow and could bene\ufb01t\nfrom translation to Cython . The Jupyter Notebook integrates nicely with Cython\nvia its %cython magic command. This means you can write Cython code in a cell in\nJupyter Notebook and the notebook will handle all of the tedious details like settingup the intermediate \ufb01les to actually compile the Cython extension. Cython also\nsupports Numpy arrays.\nCython andSWIG are just two of the ways to create Python bindings for your\nfavorite compiled libraries. Other notable (but less popular) options include FWrap ,\nf2py ,CFFI , and weave . It is also possible to use Python\u2019s own API directly, but\nthis is a tedious undertaking that is hard to justify given the existence of so manywell-developed alternatives.\n1.10 Integrated Development Environments\nFor those who prefer integrated development environments (IDEs), there is a lot to\nchoose from. The most comprehensive is Enthought Canopy, which includes a rich,\nsyntax-highlighted editor, integrated help, debugger, and even integrated training. Ifyou are already familiar with Eclipse from other projects, or do mixed-language pro-\ngramming, then there is a Python plug-in called PyDev that contains all usual features\nfrom Eclipse with a Python debugger. Wingware provides an affordable professional-\nlevel IDE with multi-project management support and unusually clairvoyant code\ncompletion that works even in debug mode. Another favorite is PyCharm, which alsosupports multiple languages and is particularly popular among Python web develop-\ners because it provides powerful templates for popular web frameworks like Django.\nVisual Studio Code has quickly developed a strong following among Python new-comers because of its beautiful interface and plug-in ecosystem. If you are a VIM\nuser, then the Jedi plug-in provides excellent code completion that works well with", "46": "34 1 Getting Started with Scienti\ufb01c Python\npylint , which provides static code analysis (i.e., identi\ufb01es missing modules and\ntypos). Naturally, emacs has many related plug-ins for developing in Python. Note\nthat are many other options, but I have tried to emphasize those most suitable forPython beginners.\n1.11 Quick Guide to Performance and Parallel\nProgramming\nThere are many options available to improve the performance of your Python codes.\nThe \ufb01rst thing to determine is what is limiting your computation. It could be CPU\nspeed (unlikely), memory limitations (out-of-core computing), or it could be data\ntransfer speed (waiting on data to arrive for processing). If your code is pure-Python,then you can try running it with Pypy , which is is an alternative Python implementa-\ntion that employs a just-in-time compiler. If your code does not experience a massive\nspeedup with Pypy , then there is probably something external to the code that is\nslowing it down (e.g., disk access or network access). If Pypy doesn\u2019t make any\nsense because you are using many compiled modules that Pypy does not support,\nthen there are many diagnostic tools available.\nPython has its own built-in pro\ufb01ler cProfile you can invoke from the command\nline as in the following:\n>>> python -m cProfile -o program .prof my_program .py\nThe output of the pro\ufb01ler is saved to the program.prof \ufb01le. This \ufb01le can be visual-\nized in runsnakerun to get a nice graphical picture of where the code is spending\nthe most time. The task manager on your operating system can also provide clues as\nyour program runs to see how it is consuming resources. The line_profiler by\nRobert Kern provides an excellent way to see how the code is spending its time by\nannotating each line of the code by its timings. In combination with runsnakerun ,\nthis narrows down problems to the line level from the function level.\nThe most common situation is that your program is waiting on data from disk\nor from some busy network resource. This is a common situation in web program-ming and there are lots of well-established tools to deal with this. Python has a\nmultiprocessing module that is part of the standard library. This makes it easy\nto spawn child worker processes that can break off and individually process smallparts of a big job. However, it is still your responsibility as the programmer to \ufb01gure\nout how to distribute the data for your algorithm. Using this module means that the\nindividual processes are to be managed by the operating system, which will be incharge of balancing the load.\nThe basic template for using multiprocessing is the following:\n# filename multiprocessing_demo.py\nimport multiprocessing\nimport time\ndef worker(k):", "47": "1.11 Quick Guide to Performance and Parallel Programming 35\n'worker function'\nprint('am starting process %d' % (k))\ntime.sleep(10) # wait ten seconds\nprint('am done waiting!')\nreturn\nif __name__ == '__main__':\nfor i in range(10):\np = multiprocessing.Process(target=worker, args=(i,))\np.start()\nThen, you run this program at the terminal as in the following:\nTerminal> python multiprocessing_demo.py\nIt is crucially important that you run the program from the terminal this way. It is not\npossible to do this interactively from within Jupyter, say. If you look at the process\nmanager on the operating system, you should see a number of new Python processes\nloitering for ten seconds. You should also see the output of the print statements\nabove. Naturally, in a real application, you would be assigning some meaningfulwork for each of the workers and \ufb01guring out how to send partially \ufb01nished pieces\nbetween individual workers. Doing this is complex and easy to get wrong, so Python\n3 has the helpful concurrent.futures .\n# filename: concurrent_demo.py\nfrom concurrent import futures\nimport time\ndef worker(k):\n'worker function'print ('am starting process %d' % (k))\ntime.sleep(10) # wait ten seconds\nprint ('am done waiting!')\nreturn\ndef main():\nwith futures.ProcessPoolExecutor(max_workers=3) as executor:\nlist(executor.map(worker,range(10)))\nif __name__ == '__main__':\nmain()\nTerminal> python concurrent_demo.py\nYou should see something like the following in the terminal. Note that we explicitly\nrestricted the number of processes to three.\nam starting process 0\nam starting process 1\nam starting process 2\nam done waiting!\nam done waiting!\n...\nThefutures module is built on top of multiprocessing and makes it easier\nto use for this kind of simple task. Note that there are also versions of both that use\nthreads instead of processes while maintaining the same usage pattern. The main", "48": "36 1 Getting Started with Scienti\ufb01c Python\ndifference between threads and processes is that processes have their own compart-\nmentalized resources. The C language Python (i.e., CPython) implementation uses\na global interpreter lock (GIL) that prevents threads from locking up on internaldata structures. This is a course-grained locking mechanism where one thread may\nindividually run faster because it does not have to keep track of all the bookkeep-\ning involved in running multiple threads simultaneously. The downside is that youcannot run multiple threads simultaneously to speed up certain tasks.\nThere is no corresponding locking problem with processes but these are somewhat\nslower to start up because each process has to create its own private workspacefor data structures that may be transferred between them. However, each process\ncan certainly run independently and simultaneously once all that is set up. Note\nthat certain alternative implementations of Python like IronPython use a \ufb01ner-grainthreading design rather than a GIL approach. As a \ufb01nal comment, on modern systems\nwith multiple cores, it could be that multiple threads actually slow things down\nbecause the operating system may have to switch threads between different cores.This creates additional overheads in the thread switching mechanism that ultimately\nslow things down.\nJupyter itself has a parallel programming framework built ( ipyparallel ) that is\nboth powerful and easy to use. The \ufb01rst step is to \ufb01re up separate Jupyter engines at\nthe terminal as in the following:\nTerminal> ipcluster start --n=4\nThen, in an Jupyter window, you can get the client,\nIn [ 1]:from ipyparallel import Client\n...:r c =Client()\nThe client has a connection to each of the processes we started before using\nipcluster . To use all of the engines, we assign the DirectView object from the\nclient as in the following:\nIn [ 2]: dview =rc[:]\nNow, we can apply functions for each of the engines. For example, we can get the\nprocess identi\ufb01ers using the os.getpid function,\nIn [ 3]:import os\nIn [ 4]: dview .apply_sync(os .getpid)\nOut[ 4]: [ 6824 ,4752 ,8836 ,3124 ]\nOnce the engines are up and running, data can be distributed to them using scatter ,\nIn [ 5]: dview .scatter( 'a',range (10))\nOut[ 5]:<AsyncResult: finished >\nIn [ 6]: dview .execute( 'print(a)' ).display_outputs()\n[stdout: 0][0,1,2]\n[stdout: 1][3,4,5]\n[stdout: 2][6,7]\n[stdout: 3][8,9]", "49": "1.11 Quick Guide to Performance and Parallel Programming 37\nNote that the execute method evaluates the given string in each engine. Now that\nthe data have been sprinkled among the active engines, we can do further computing\non them,\nIn [ 7]: dview .execute( 'b=sum(a)' )\nOut[ 7]:<AsyncResult: finished >\nIn [ 8]: dview .execute( 'print(b)' ).display_outputs()\n[stdout: 0]3\n[stdout: 1]12\n[stdout: 2]13\n[stdout: 3]17\nIn this example, we added up the individual asub-lists available on each of the\nengines. We can gather up the individual results into a single list as in the following:\nIn [ 9]: dview .gather( 'b').result\nOut[ 9]: [ 3,12,13,17]\nThis is one of the simplest mechanisms for distributing work to the individual en-\ngines and collecting the results. Unlike the other methods we discussed, you can do\nthis iteratively, which makes it easy to experiment with how you want to distributeand compute with the data. The Jupyter documentation has many more examples\nof parallel programming styles that include running the engines on cloud resources,\nsupercomputer clusters, and across disparate networked computing resources. Al-though there are many other specialized parallel programming packages, Jupyter\nprovides the best trade-off for generality against complexity across all of the major\nplatforms.\n1.12 Other Resources\nThe Python community is \ufb01lled with super-smart and amazingly helpful people. One\nof the best places to get help with scienti\ufb01c Python is the www.stackoverflow.com\nsite which hosts a competitive Q&A forum that is particularly welcoming for Pythonnewbies. Several of the key Python developers regularly participate there and the\nquality of the answers is very high. The mailing lists for any of the key tools (e.g.,\nNumpy, Jupyter, Matplotlib) are also great for keeping up with the newest develop-ments. Anything written by Hans Petter Langtangen [ 6] is excellent, especially if\nyou have a physics background. The Scienti\ufb01c Python conference held annually in\nAustin is also a great place to see your favorite developers in person, ask questions,and participate in the many interesting subgroups organized around niche topics.\nThePyData workshop is a semi-annual meeting focused on Python for large-scale\ndata-intensive processing.", "50": "38 1 Getting Started with Scienti\ufb01c Python\nReferences\n1. T.E. Oliphant, A Guide to NumPy (Trelgol Publishing, 2006)\n2. L. Wilkinson, D. Wills, D. Rope, A. Norton, R. Dubbs, The Grammar of Graphics . Statistics\nand Computing (Springer, Berlin, 2006)\n3. F. Perez, B.E. Granger et al., IPython software package for interactive scienti\ufb01c computing.\nhttp://ipython.org/\n4. W. McKinney, Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython\n(O\u2019Reilly, 2012)\n5. O. Certik et al., SymPy: python library for symbolic mathematics. http://sympy.org/\n6. H.P. Langtangen, Python Scripting for Computational Science , vol. 3, 3rd edn. Texts in Com-\nputational Science and Engineering (Springer, Berlin, 2009)", "51": "Chapter 2\nProbability\n2.1 Introduction\nThis chapter takes a geometric view of probability theory and relates it to famil-\niar concepts in linear algebra and geometry. This approach connects your natural\ngeometric intuition to the key abstractions in probability that can help guide yourreasoning. This is particularly important in probability because it is easy to be misled.\nWe need a bit of rigor and some intuition to guide us.\nIn grade school, you were introduced to the natural numbers (i.e., 1,2,3,.. )\nand you learned how to manipulate them by operations like addition, subtraction,\nand multiplication. Later, you were introduced to positive and negative numbers andwere again taught how to manipulate them. Ultimately, you were introduced to the\ncalculus of the real line, and learned how to differentiate, take limits, and so on. This\nprogression provided more abstractions, but also widened the \ufb01eld of problems youcould successfully tackle. The same is true of probability. One way to think about\nprobability is as a new number concept that allows you to tackle problems that have\na special kind of uncertainty built into them. Thus, the key idea is that there is some\nnumber, say x, with a traveling companion, say, f(x), and this companion represents\nthe uncertainties about the value of xas if looking at the number xthrough a frosted\nwindow. The degree of opacity of the window is represented by f(x). If we want to\nmanipulate x, then we have to \ufb01gure out what to do with f(x). For example if we\nwant y=2x, then we have to understand how f(x)generates f(y).\nWhere is the random part? To conceptualize this, we need still another analogy:\nthink about a beehive with the swarm around it representing f(x), and the hive itself,\nwhich you can barely see through the swarm, as x. The random piece is you don\u2019t\nknow which bee in particular is going to sting you! Once this happens the uncertainty\nevaporates. Up until that happens, all we have is a concept of a swarm (i.e., density of\nbees) which represents a potentiality of which bee will ultimately sting. In summary,\none way to think about probability is as a way of carrying through mathematical\nreasoning (e.g., adding, subtracting, taking limits) with a notion of potentiality that\nis so-transformed by these operations.\n\u00a9 Springer Nature Switzerland AG 2019\nJ. Unpingco, Python for Probability, Statistics, and Machine Learning ,\nhttps://doi.org/10.1007/978-3-030-18545-9_239", "52": "40 2 Probability\n2.1.1 Understanding Probability Density\nIn order to understand the heart of modern probability, which is built on the Lebesgue\ntheory of integration, we need to extend the concept of integration from basic calculus.\nTo begin, let us consider the following piecewise function\nf(x)=\u23a7\n\u23aa\u23a8\n\u23aa\u23a91i f 0 <x\u22641\n2i f 1 <x\u22642\n0 otherwise\nas shown in Fig. 2.1. In calculus, you learned Riemann integration, which you can\napply here as/integraldisplay2\n0f(x)dx=1+2=3\nwhich has the usual interpretation as the area of the two rectangles that make up\nf(x). So far, so good.\nWith Lesbesgue integration, the idea is very similar except that we focus on the\ny-axis instead of moving along the x-axis. The question is given f(x)=1, what\nis the set of xvalues for which this is true? For our example, this is true whenever\nx\u2208(0,1]. So now we have a correspondence between the values of the function\n(namely, 1and2) and the sets of xvalues for which this is true, namely, {(0,1]}and\n{(1,2]}, respectively. To compute the integral, we simply take the function values\n(i.e.,1,2 ) and some way of measuring the size of the corresponding interval (i.e.,\n\u03bc) as in the following:\nFig. 2.1 Simple piecewise-\nconstant function", "53": "2.1 Introduction 41\n/integraldisplay2\n0fd\u03bc=1\u03bc({(0,1]})+2\u03bc({(1,2]})\nWe have suppressed some of the notation above to emphasize generality. Note\nthat we obtain the same value of the integral as in the Riemann case when\n\u03bc((0,1])=\u03bc((1,2])=1. By introducing the \u03bcfunction as a way of measuring\nthe intervals above, we have introduced another degree of freedom in our integra-tion. This accommodates many weird functions that are not tractable using the usual\nRiemann theory, but we refer you to a proper introduction to Lesbesgue integration\nfor further study [ 1]. Nonetheless, the key step in the above discussion is the intro-\nduction of the \u03bcfunction, which we will encounter again as the so-called probability\ndensity function.\n2.1.2 Random V ariables\nMost introductions to probability jump straight into random variables and then\nexplain how to compute complicated integrals. The problem with this approach is\nthat it skips over some of the important subtleties that we will now consider. Unfortu-\nnately, the term random variable is not very descriptive. A better term is measurable\nfunction . To understand why this is a better term, we have to dive into the formal\nconstructions of probability by way of a simple example.\nConsider tossing a fair six-sided die. There are only six outcomes possible,\n\u03a9={1,2,3,4,5,6}\nAs we know, if the die is fair, then the probability of each outcome is 1/6. To say\nthis formally, the measure of each set (i.e., {1},{2},...,{6})i s\u03bc({1})=\u03bc({2})...=\n\u03bc({6})=1/6. In this case, the \u03bcfunction we discussed earlier is the usual probability\nmass function, denoted by P. The measurable function maps a set into a number on\nthe real line. For example, {1}/mapsto\u21921 is one such function.\nNow, here\u2019s where things get interesting. Suppose you were asked to construct\na fair coin from the fair die. In other words, we want to throw the die and then\nrecord the outcomes as if we had just tossed a fair coin. How could we do this?One way would be to de\ufb01ne a measurable function that says if the die comes up\n3or less, then we declare heads and otherwise declare tails . This has some strong\nintuition behind it, but let\u2019s articulate it in terms of formal theory. This strategycreates two different non-overlapping sets {1,2,3}and{4,5,\n6}. Each set has the\nsame probability measure ,\nP({1,2,3})=1/2\nP({4,5,6})=1/2", "54": "42 2 Probability\nAnd the problem is solved. Everytime the die comes up {1,2,3}, we record heads\nand record tails otherwise.\nIs this the only way to construct a fair coin experiment from a fair die? Alterna-\ntively, we can de\ufb01ne the sets as {1},{2},{3,4,5,6}. If we de\ufb01ne the corresponding\nmeasure for each set as the following\nP({1})=1/2\nP({2})=1/2\nP({3,4,5,6})=0\nthen, we have another solution to the fair coin problem. To implement this, all we do\nis ignore every time the die shows 3,4,5,6 and throw again. This is wasteful, but\nit solves the problem. Nonetheless, we hope you can see how the interlocking piecesof the theory provide a framework for carrying the notion of uncertainty/potentiality\nfrom one problem to the next (e.g., from the fair die to the fair coin).\nLet\u2019s consider a slightly more interesting problem where we toss two dice. We\nassume that each throw is independent , meaning that the outcome of one does not\nin\ufb02uence the other. What are the sets in this case? They are all pairs of possible\noutcomes from two throws as shown below,\n\u03a9={(1,1),(1,2),...,( 5,6),(6,6)}\nWhat are the measures of each of these sets? By virtue of the independence claim,\nthe measure of each is the product of the respective measures of each element. For\ninstance,\nP\n((1,2))=P({1})P({2})=1\n62\nWith all that established, we can ask the following question: what is the probability\nthat the sum of the dice equals seven? As before, the \ufb01rst thing to do is characterizethe measurable function for this as X:(a,b)/mapsto\u2192(a+b). Next, we associate all of\nthe(a,b)pairs with their sum. We can create a Python dictionary for this as shown,\n>>> d={(i,j):i +jfor iinrange (1,7)for jinrange (1,7)}\nThe next step is to collect all of the (a,b)pairs that sum to each of the possible values\nfrom two to twelve.\n>>> from collections import defaultdict\n>>> dinv =defaultdict( list )\n>>> for i,j ind.items():\n... dinv[j] .append(i)\n...", "55": "2.1 Introduction 43\nProgramming Tip\nThedefaultdict object from the built-in collections module creates dictio-\nnaries with default values when it encounters a new key. Otherwise, we wouldhave had to create default values manually for a regular dictionary.\nFor example, dinv[7] contains the following list of pairs that sum to seven,\n>>> dinv[ 7]\n[(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)]\nThe next step is to compute the probability measured for each of these items.\nUsing the independence assumption, this means we have to compute the sum of theproducts of the individual item probabilities in dinv . Because we know that each\noutcome is equally likely, the probability of every term in the sum equals 1/36. Thus,\nall we have to do is count the number of items in the corresponding list for each key indinv and divide by 36. For example, dinv[11] contains [(5, 6), (6, 5)] .\nThe probability of 5+6=6+5=11 is the probability of this set which is composed\nof the sum of the probabilities of the individual elements (5,6),(6,5) .I nt h i s\ncase, we have P(11)=P({(5,6)})+P({(6,5)})=1/36+1/36=2/36. Repeating\nthis procedure for all the elements, we derive the probability mass function as shown\nbelow,\n>>> X={i:len(j)/36. for i,j indinv .items()}\n>>> print (X)\n{2: 0.027777777777777776,\n3: 0.05555555555555555,4: 0.08333333333333333,5: 0.1111111111111111,\n6: 0.1388888888888889,\n7: 0.16666666666666666,8: 0.1388888888888889,9: 0.1111111111111111,10: 0.08333333333333333,11: 0.05555555555555555,12: 0.027777777777777776}\nProgramming Tip\nIn the preceding code note that 36. is written with the trailing decimal mark.\nThis is a good habit to get into because the default division operation changed\nbetween Python 2.x and Python 3.x. In Python 2.x division is integer divisionby default, and it is \ufb02oating-point division in Python 3.x.\nThe above example exposes the elements of probability theory that are in play\nfor this simple problem while deliberately suppressing some of the gory technical", "56": "44 2 Probability\ndetails. With this framework, we can ask other questions like what is the probability\nthat half the product of three dice will exceed the their sum? We can solve this using\nthe same method as in the following. First, let\u2019s create the \ufb01rst mapping,\n>>> d={(i,j,k):((i *j*k)/2>i+j+k)for iinrange (1,7)\n... for jinrange (1,7)\n... for kinrange (1,7)}\nThe keys of this dictionary are the triples and the values are the logical values of\nwhether or not half the product of three dice exceeds their sum. Now, we do the\ninverse mapping to collect the corresponding lists,\n>>> dinv =defaultdict( list )\n>>> for i,j ind.items():\n... dinv[j] .append(i)\n...\nNote that dinv contains only two keys, True andFalse . Again, because the dice\nare independent, the probability of any triple is 1 /63. Finally, we collect this for each\noutcome as in the following,\n>>> X={i:len(j)/6.0**3 for i,j indinv .items()}\n>>> print (X)\n{False: 0.37037037037037035, True: 0.6296296296296297}\nThus, the probability of half the product of three dice exceeding their sum is\n136/6.0**3) = 0.63 . The set that is induced by the random variable has only\ntwo elements in it, True andFalse , with P(True)=136/216 and P(False)=\n1\u2212136/216.\nAs a \ufb01nal example to exercise another layer of generality, let is consider the \ufb01rst\nproblem with the two dice where we want the probability of a seven, but this timeone of the dice is no longer fair. The distribution for the unfair die is the following:\nP({1})=P({2})=P({3})=1\n9\nP({4})=P({5})=P({6})=2\n9\nFrom our earlier work, we know the elements corresponding to the sum of seven\nare the following:\n{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)}\nBecause we still have the independence assumption, all we need to change is the\nprobability computation of each of elements. For example, given that the \ufb01rst die isthe unfair one, we have\nP((1,6))=P(1)P(6)=1\n9\u00d71\n6", "57": "2.1 Introduction 45\nand likewise for (2,5)we have the following:\nP((2,5))=P(2)P(5)=1\n9\u00d71\n6\nand so forth. Summing all of these gives the following:\nPX(7)=1\n9\u00d71\n6+1\n9\u00d71\n6+1\n9\u00d71\n6+2\n9\u00d71\n6+2\n9\u00d71\n6+2\n9\u00d71\n6=1\n6\nLet\u2019s try computing this using Pandas instead of Python dictionaries. First, we con-\nstruct a DataFrame object with an index of tuples consisting of all pairs of possible\ndice outcomes.\n>>> from pandas import DataFrame\n>>> d=DataFrame(index =[(i,j) for iinrange (1,7)for jinrange (1,7)],\n... columns =[\u2019sm\u2019 ,\u2019d1\u2019 ,\u2019d2\u2019 ,\u2019pd1\u2019 ,\u2019pd2\u2019 ,\u2019p\u2019])\nNow, we can populate the columns that we set up above where the outcome of the\n\ufb01rst die is the d1column and the outcome of the second die is d2,\n>>> d.d1=[i[0]for iind.index]\n>>> d.d2=[i[1]for iind.index]\nNext, we compute the sum of the dices in the smcolumn,\n>>> d.sm=list (map(sum,d.index))\nWith that established, the DataFrame now looks like the following:\n>>> d.head( 5)# show first five lines\nsm d1 d2 pd1 pd2 p\n(1, 1) 2 1 1 NaN NaN NaN(1, 2) 3 1 2 NaN NaN NaN(1, 3) 4 1 3 NaN NaN NaN(1, 4) 5 1 4 NaN NaN NaN(1, 5) 6 1 5 NaN NaN NaN\nNext, we \ufb01ll out the probabilities for each face of the unfair die ( d1) and the fair die\n(d2),\n>>> d.loc[d .d1<=3,\u2019pd1\u2019 ]=1/9.\n>>> d.loc[d .d1>3,\u2019pd1\u2019 ]=2/9.\n>>> d.pd2=1/6.\n>>> d.head( 10)\nsm d1 d2 pd1 pd2 p\n(1, 1) 2 1 1 0.111111 0.166667 NaN(1, 2) 3 1 2 0.111111 0.166667 NaN(1, 3) 4 1 3 0.111111 0.166667 NaN(1, 4) 5 1 4 0.111111 0.166667 NaN(1, 5) 6 1 5 0.111111 0.166667 NaN(1, 6) 7 1 6 0.111111 0.166667 NaN(2, 1) 3 2 1 0.111111 0.166667 NaN", "58": "46 2 Probability\n(2, 2) 4 2 2 0.111111 0.166667 NaN\n(2, 3) 5 2 3 0.111111 0.166667 NaN(2, 4) 6 2 4 0.111111 0.166667 NaN\nFinally, we can compute the joint probabilities for the sum of the shown faces as the\nfollowing:\n>>> d.p=d.pd1 *d.pd2\n>>> d.head( 5)\nsm d1 d2 pd1 pd2 p\n(1, 1) 2 1 1 0.111111 0.166667 0.0185185(1, 2) 3 1 2 0.111111 0.166667 0.0185185(1, 3) 4 1 3 0.111111 0.166667 0.0185185(1, 4) 5 1 4 0.111111 0.166667 0.0185185\n(1, 5) 6 1 5 0.111111 0.166667 0.0185185\nWith all that established, we can compute the density of all the dice outcomes by\nusing groupby as in the following,\n>>> d.groupby( \u2019sm\u2019 )[\u2019p\u2019].sum()\nsm2 0.0185193 0.0370374 0.0555565 0.0925936 0.1296307 0.1666678 0.1481489 0.12963010 0.11111111 0.07407412 0.037037Name: p, dtype: float64\nThese examples have shown how the theory of probability breaks down sets and\nmeasurements of those sets and how these can be combined to develop the probability\nmass functions for new random variables.\n2.1.3 Continuous Random V ariables\nThe same ideas work with continuous variables but managing the sets becomes\ntrickier because the real line, unlike discrete sets, has many limiting properties already\nbuilt into it that have to be handled carefully. Nonetheless, let\u2019s start with an example\nthat should illustrate the analogous ideas. Suppose a random variable Xis uniformly\ndistributed on the unit interval. What is the probability that the variable takes on\nvalues less than 1/2?\nIn order to build intuition onto the discrete case, let\u2019s go back to our dice-throwing\nexperiment with the fair dice. The sum of the values of the dice is a measurable\nfunction,", "59": "2.1 Introduction 47\nY:{1,2,..., 6}2/mapsto\u2192{ 2,3,..., 12}\nThat is, Yis a mapping of the cartesian product of sets to a discrete set of outcomes.\nIn order to compute probabilities of the set of outcomes, we need to derive the\nprobability measure for Y,PY, from the corresponding probability measures for\neach die. Our previous discussion went through the mechanics of that. This meansthat\nP\nY:{2,3,..., 12}/mapsto\u2192[ 0,1]\nNote there is a separation between the function de\ufb01nition and where the target items\nof the function are measured in probability. More bluntly,\nY:A/mapsto\u2192B\nwith,\nPY:B/mapsto\u2192[ 0,1]\nThus, to compute PY, which is derived from other random variables, we have to\nexpress the equivalence classes in Bin terms of their progenitor Asets.\nThe situation for continuous variables follows the same pattern, but with many\nmore deep technicalities that we are going to skip. For the continuous case, the\nrandom variable is now,\nX:R/mapsto\u2192R\nwith corresponding probability measure,\nPX:R/mapsto\u2192[ 0,1]\nBut where are the corresponding sets here? Technically, these are the Borel sets,\nbut we can just think of them as intervals. Returning to our question, what is the\nprobability that a uniformly distributed random variable on the unit interval takes\nvalues less than 1/2? Rephrasing this question according to the framework, we havethe following:\nX:[0,1]/mapsto\u2192[ 0,1]\nwith corresponding,\nP\nX:[0,1]/mapsto\u2192[ 0,1]\nTo answer the question, by the de\ufb01nition of the uniform random variable on the unit\ninterval, we compute the following integral,\nPX([0,1/2])=PX(0<X<1/2)=/integraldisplay1/2\n0dx=1/2", "60": "48 2 Probability\nwhere the above integral\u2019s dxsweeps through intervals of the B-type. The measure of\nanydxinterval (i.e., A-type set) is equal to dx, by de\ufb01nition of the uniform random\nvariable. To get all the moving parts into one notationally rich integral, we can alsowrite this as,\nP\nX(0<X<1/2)=/integraldisplay1/2\n0dPX(dx)=1/2\nNow, let\u2019s consider a slightly more complicated and interesting example. As\nbefore, suppose we have a uniform random variable, Xand let us introduce another\nrandom variable de\ufb01ned,\nY=2X\nNow, what is the probability that 0 <Y<1\n2? To express this in our framework, we\nwrite,\nY:[0,1]/mapsto\u2192[ 0,2]\nwith corresponding,\nPY:[0,2]/mapsto\u2192[ 0,1]\nTo answer the question, we need to measure the set [0,1/2], with the probability\nmeasure for Y,PY([0,1/2]). How can we do this? Because Yis derived from the X\nrandom variable, as with the fair-dice throwing experiment, we have to create a set\nof equivalences in the target space (i.e., B-type sets) that re\ufb02ect back on the input\nspace (i.e., A-type sets). That is, what is the interval [0,1/2] equivalent to in terms\nof the Xrandom variable? Because, functionally, Y=2X, then the B-type interval\n[0,1/2] corresponds to the A-type interval [0,1/4]. From the probability measure of\nX, we compute this with the integral,\nPY([0,1/2])=PX([0,1/4])=/integraldisplay1/4\n0dx=1/4\nNow, let\u2019s up the ante and consider the following random variable,\nY=X2\nwhere now Xis still uniformly distributed, but now over the interval [\u22121/2,1/2].\nWe can express this in our framework as,\nY:[ \u2212 1/2,1/2]/mapsto\u2192[ 0,1/4]\nwith corresponding,\nPY:[0,1/4]/mapsto\u2192[ 0,1]\nWhat is the PY(Y<1/8)? In other words, what is the measure of the set BY=\n[0,1/8]? As before, because Xis derived from our uniformly distributed random", "61": "2.1 Introduction 49\nvariable, we have to re\ufb02ect the BYset onto sets of the A-type. The thing to recognize\nis that because X2is symmetric about zero, all BYsets re\ufb02ect back into two sets.\nThis means that for any set BY, we have the correspondence BY=A+\nX\u222aA\u2212\nX. So,\nwe have,\nBY=/braceleftBig\n0<Y<1\n8/bracerightBig\n=/braceleftBig\n0<X<1\u221a\n8/bracerightBig/uniondisplay/braceleftBig\n\u22121\u221a\n8<X<0/bracerightBig\nFrom this perspective, we have the following solution,\nPY(BY)=P(A+\nX)/2+P(A\u2212\nX)/2\nwhere the1\n2comes from normalizing the PYto one. Also,\nA+\nX=/braceleftBig\n0<X<1\u221a\n8/bracerightBig\nA\u2212\nX=/braceleftBig\n\u22121\u221a\n8<X<0/bracerightBig\nTherefore,\nPY(BY)=1\n2\u221a\n8+1\n2\u221a\n8\nbecause P(A+\nX)=P(A\u2212\nX)=1/\u221a\n8. Let\u2019s see if this comes out using the usual\ntransformation of variables method from calculus. Using this method, the density\nfY(y)=fX(\u221ay)/(2\u221ay)=1\n2\u221ay. Then, we obtain,\n/integraldisplay 1\n8\n01\n2\u221aydy=1\u221a\n8\nwhich is what we got using the sets method. Note that you would favor the calculus\nmethod in practice, but it is important to understand the deeper mechanics, becausesometimes the usual calculus method fails, as the next problem shows.\n2.1.4 Transformation of V ariables Beyond Calculus\nSuppose XandYare uniformly distributed in the unit interval and we de\ufb01ne Zas\nZ=X\nY\u2212X", "62": "50 2 Probability\nWhat is the fZ(z)? If you try this using the usual calculus method, you will fail (try\nit!). The problem is one of the technical prerequisites for the calculus method is not\nin force.\nThe key observation is that Z/\u2208(\u22121,0]. If this were possible, the Xand Y\nwould have different signs, which cannot happen, given that XandYare uniformly\ndistributed over (0,1]. Now, let\u2019s consider when Z>0. In this case, Y>Xbecause\nZcannot be positive otherwise. For the density function, we are interested in the set\n{0<Z<z}. We want to compute\nP(Z<z)=/integraldisplay/integraldisplay\nB1dXdY\nwith,\nB1={0<Z<z}\nNow, we have to translate that interval into an interval relevant to Xand Y.F o r\n0<Z,w eh a v e Y>X.F o r Z<z,w eh a v e Y>X(1/z+1). Putting this together\ngives\nA1={max(X,X(1/z+1)) < Y<1}\nIntegrating this over Yas follows,\n/integraldisplay1\n0{max(X,X(1/z+1)) < Y<1}dY=z\u2212X\u2212Xz\nzwhere z>X\n1\u2212X\nand integrating this one more time over Xgives\n/integraldisplay z\n1+z\n0\u2212X+z\u2212Xz\nzdX=z\n2(z+1)where z>0\nNote that this is the computation for the probability itself, not the probability density\nfunction. To get that, all we have to do is differentiate the last expression to obtain\nfZ(z)=1\n(z+1)2where z>0\nNow we need to compute this density using the same process for when z<\u22121.\nWe want the interval Z<zfor when z<\u22121. For a \ufb01xed z, this is equivalent to\nX(1+1/z)< Y. Because zis negative, this also means that Y<X. Under these\nterms, we have the following integral,\n/integraldisplay1\n0{X(1/z+1)<Y<X}dY=\u2212X\nzwhere z<\u22121", "63": "2.1 Introduction 51\nand integrating this one more time over Xgives the following\n\u22121\n2zwhere z<\u22121\nTo get the density for z<\u22121, we differentiate this with respect to zto obtain the\nfollowing,\nfZ(z)=1\n2z2where z<\u22121\nPutting this all together, we obtain,\nfZ(z)=\u23a7\n\u23aa\u23a8\n\u23aa\u23a91\n(z+1)2ifz>0\n1\n2z2 ifz<\u22121\n0 otherwise\nWe will leave it as an exercise to show that this integrates out to one.\n2.1.5 Independent Random V ariables\nIndependence is a standard assumption. Mathematically, the necessary and suf\ufb01cient\ncondition for independence between two random variables XandYis the following:\nP(X,Y)=P(X)P(Y)\nTwo random variables XandYareuncorrelated if,\nE(X\u2212X)E(Y\u2212Y)=0\nwhere X=E(X)Note that uncorrelated random variables are sometimes called\northogonal random variables. Uncorrelatedness is a weaker property than indepen-\ndence, however. For example, consider the discrete random variables XandYuni-\nformly distributed over the set {1,2,3}where\nX=\u23a7\n\u23aa\u23a8\n\u23aa\u23a91i f \u03c9=1\n0i f \u03c9=2\n\u22121i f\u03c9=3\nand also,\nY=\u23a7\n\u23aa\u23a8\n\u23aa\u23a90i f\u03c9=1\n1i f\u03c9=2\n0i f\u03c9=3", "64": "52 2 Probability\nThus, E(X)=0 and E(XY)=0, so XandYare uncorrelated. However, we have\nP(X=1,Y=1)=0/negationslash=P(X=1)P(Y=1)=1\n9\nSo, these two random variables are notindependent. Thus, uncorrelatedness does not\nimply independence, generally, but there is the important case of Gaussian random\nvariables for which it does. To see this, consider the probability density function fortwo zero-mean, unit-variance Gaussian random variables XandY,\nf\nX,Y(x,y)=ex2\u22122\u03c1xy+y2\n2(\u03c12\u22121)\n2\u03c0/radicalbig\n1\u2212\u03c12\nwhere \u03c1:=E(XY)is the correlation coef\ufb01cient. In the uncorrelated case where\n\u03c1=0, the probability density function factors into the following,\nfX,Y(x,y)=e\u22121\n2/parenleftbig\nx2+y2/parenrightbig\n2\u03c0=e\u2212x2\n2\n\u221a\n2\u03c0e\u2212y2\n2\n\u221a\n2\u03c0=fX(x)fY(y)\nwhich means that XandYare independent.\nIndependence and conditional independence are closely related, as in the follow-\ning:\nP(X,Y|Z)=P(X|Z)P(Y|Z)\nwhich says that Xand Yand independent conditioned on Z. Conditioning inde-\npendent random variables can break their independence. For example, consider two\nindependent Bernoulli-distributed random variables, X1,X2\u2208{0,1}. We de\ufb01ne\nZ=X1+X2. Note that Z\u2208{0,1,2}. In the case where Z=1, we have,\nP(X1|Z=1)> 0\nP(X2|Z=1)> 0\nEven though X1,X2are independent, after conditioning on Z, we have the following,\nP(X1=1,X2=1|Z=1)=0/negationslash=P(X1=1|Z=1)P(X2=1|Z=1)\nThus, conditioning on Zbreaks the independence of X1,X2. This also works in the\nopposite direction\u2014conditioning can make dependent random variables indepen-dent. De\ufb01ne Z\nn=/summationtextn\niXiwith Xiindependent, integer-valued random variables.\nThe Znvariables are dependent because they stack the same telescoping set of Xi\nvariables. Consider the following,", "65": "2.1 Introduction 53\nP(Z1=i,Z3=j|Z2=k)=P(Z1=i,Z2=k,Z3=j)\nP(Z2=k)(2.1.5.1)\n=P(X1=i)P(X2=k\u2212i)P(X3=j\u2212k)\nP(Z2=k)(2.1.5.2)\nwhere the factorization comes from the independence of the Xivariables. Using the\nde\ufb01nition of conditional probability,\nP(Z1=i|Z2)=P(Z1=i,Z2=k)\nP(Z2=k)\nWe can continue to expand Eq. 2.1.5.1 ,\nP(Z1=i,Z3=j|Z2=k)=P(Z1=i|Z2)P(X3=j\u2212k)P(Z2=k)\nP(Z2=k)\n=P(Z1=i|Z2)P(Z3=j|Z2)\nwhere P(X3=j\u2212k)P(Z2=k)=P(Z3=j,Z2). Thus, we see that depen-\ndence between random variables can be broken by conditioning to create condition-ally independent random variables. As we have just witnessed, understanding how\nconditioning in\ufb02uences independence is important and is the main topic of study\nin Probabilistic Graphical Models, a \ufb01eld with many algorithms and concepts toextract these notions of conditional independence from graph-based representations\nof random variables.\n2.1.6 Classic Broken Rod Example\nLet\u2019s do one last example to exercise \ufb02uency in our methods by considering the\nfollowing classic problem: given a rod of unit-length, broken independently andrandomly at two places, what is the probability that you can assemble the three\nremaining pieces into a triangle? The \ufb01rst task is to \ufb01nd a representation of a triangle\nas an easy-to-apply constraint. What we want is something like the following:\nP(triangle exists )=/integraldisplay\n1\n0/integraldisplay1\n0{triangle exists }dXdY\nwhere XandYare independent and uniformly distributed in the unit-interval. Heron\u2019s\nformula for the area of the triangle,\narea=/radicalbig\n(s\u2212a)(s\u2212b)(s\u2212c)s", "66": "54 2 Probability\nwhere s=(a+b+c)/2 is what we need. The idea is that this yields a valid area\nonly when each of the terms under the square root is greater than or equal to zero.\nThus, suppose that we have\na=X\nb=Y\u2212X\nc=1\u2212Y\nassuming that Y>X. Thus, the criterion for a valid triangle boils down to\n{(s>a)\u2227(s>b)\u2227(s>c)\u2227(X<Y)}\nAfter a bit of manipulation, this consolidates into:\n/braceleftBig1\n2<Y<1/logicalanddisplay1\n2(2Y\u22121)< X<1\n2/bracerightBig\nwhich we integrate out by dX \ufb01rst to obtain\nP(triangle exists )=/integraldisplay1\n0/integraldisplay1\n0/braceleftBig1\n2<Y<1/logicalanddisplay1\n2(2Y\u22121)< X<1\n2/bracerightBig\ndXdY\nP(triangle exists )=/integraldisplay1\n1\n2(1\u2212Y)dY\nand then by dYto obtain \ufb01nally,\nP(triangle exists )=1\n8\nwhen Y>X. By symmetry, we get the same result for X>Y. Thus, the \ufb01nal result\nis the following:\nP(triangle exists )=1\n8+1\n8=1\n4\nWe can quickly check using this result using Python for the case Y>Xusing\nthe following code:\n>>> import numpy asnp\n>>> x,y =np.random .rand( 2,1000 )# uniform rv\n>>> a,b,c =x,(y -x),1-y# 3 sides\n>>> s=(a+b+c)/2\n>>> np.mean((s >a)&(s>b) &(s>c)&(y>x)) # approx 1/8=0.125\n0.137", "67": "2.1 Introduction 55\nProgramming Tip\nThe chained logical &symbols above tell Numpy that the logical operation\nshould be considered element-wise.\n2.2 Projection Methods\nThe concept of projection is key to developing an intuition about conditional proba-\nbility. We already have a natural intuition of projection from looking at the shadows\nof objects on a sunny day. As we will see, this simple idea consolidates many abstract\nideas in optimization and mathematics. Consider Fig. 2.2where we want to \ufb01nd a\npoint along the blue line (namely, x) that is closest to the black square (namely, y).\nIn other words, we want to in\ufb02ate the gray circle until it just touches the black line.\nRecall that the circle boundary is the set of points for which\n/radicalBig\n(y\u2212x)T(y\u2212x)=/bardbly\u2212x/bardbl=/epsilon1\nfor some value of /epsilon1. So we want a point xalong the line that satis\ufb01es this for the\nsmallest /epsilon1. Then, that point will be the closest point on the black line to the black\nsquare. It may be obvious from the diagram, but the closest point on the line occurswhere the line segment from the black square to the black line is perpendicular to\nthe line. At this point, the gray circle just touches the black line. This is illustrated\nbelow in Fig. 2.3.\nFig. 2.2 Given the point y\n(black square) we want to \ufb01ndthexalong the line that is\nclosest to it. The gray circleis the locus of points within a\ufb01xed distance from y\n", "68": "56 2 Probability\nFig. 2.3 The closest point\non the line occurs when theline is tangent to the circle.When this happens, the blackline and the line (minimumdistance) are perpendicular\nProgramming Tip\nFigure 2.2uses the matplotlib.patches module. This module contains\nprimitive shapes like circles, ellipses, and rectangles that can be assembled into\ncomplex graphics. After importing a particular shape, you can apply that shape\nto an existing axis using the add_patch method. The patches themselves can\nby styled using the usual formatting keywords like color andalpha .\nNow that we can see what\u2019s going on, we can construct the the solution analytically.\nWe can represent an arbitrary point along the black line as:\nx=\u03b1v\nwhere \u03b1\u2208Rslides the point up and down the line with\nv=[1,1]T\nFormally, vis the subspace onto which we want to project y. At the closest point,\nthe vector between yandx(the error vector above) is perpendicular to the line. This\nmeans that\n(y\u2212x)Tv=0\nand by substituting and working out the terms, we obtain\n\u03b1=yTv\n/bardblv/bardbl2", "69": "2.2 Projection Methods 57\nThe error is the distance between \u03b1vandy. This is a right triangle, and we can use\nthe Pythagorean theorem to compute the squared length of this error as\n/epsilon12=/bardbl(y\u2212x)/bardbl2=/bardbly/bardbl2\u2212\u03b12/bardblv/bardbl2=/bardbly/bardbl2\u2212/bardblyTv/bardbl2\n/bardblv/bardbl2\nwhere /bardblv/bardbl2=vTv. Note that since /epsilon12\u22650, this also shows that\n/bardblyTv/bardbl\u2264/bardbl y/bardbl/bardblv/bardbl\nwhich is the famous and useful Cauchy\u2013Schwarz inequality which we will exploit\nlater. Finally, we can assemble all of this into the projection operator\nPv=1\n/bardblv/bardbl2vvT\nWith this operator, we can take any yand \ufb01nd the closest point on vby doing\nPvy=v/parenleftbiggvTy\n/bardblv/bardbl2/parenrightbigg\nwhere we recognize the term in parenthesis as the \u03b1we computed earlier. It\u2019s called\nanoperator because it takes a vector ( y) and produces another vector ( \u03b1v). Thus,\nprojection uni\ufb01es geometry and optimization.\n2.2.1 Weighted Distance\nWe can easily extend this projection operator to cases where the measure of distancebetween yand the subspace vis weighted. We can accommodate these weighted\ndistances by re-writing the projection operator as\nP\nv=vvTQT\nvTQv(2.2.1.1)\nwhere Qis positive de\ufb01nite matrix. In the previous case, we started with a point\nyand in\ufb02ated a circle centered at yuntil it just touched the line de\ufb01ned by vand\nthis point was closest point on the line to y. The same thing happens in the general\ncase with a weighted distance except now we in\ufb02ate an ellipse, not a circle, until the\nellipse touches the line.\nNote that the error vector ( y\u2212\u03b1v)i nF i g . 2.4is still perpendicular to the line\n(subspace v), but in the space of the weighted distance. The difference between the\n\ufb01rst projection (with the uniform circular distance) and the general case (with the\nelliptical weighted distance) is the inner product between the two cases. For example,", "70": "58 2 Probability\nFig. 2.4 In the weighted case,\nthe closest point on the line istangent to the ellipse and isstill perpendicular in the senseof the weighted distance\nin the \ufb01rst case we have yTvand in the weighted case we have yTQTv. To move\nfrom the uniform circular case to the weighted ellipsoidal case, all we had to do was\nchange all of the vector inner products. Before we \ufb01nish, we need a formal propertyof projections:\nP\nvPv=Pv\nknown as the idempotent property which basically says that once we have projected\nonto a subspace, subsequent projections leave us in the same subspace. Y ou can\nverify this by computing Eq. 2.2.1.1 .\nThus, projection ties a minimization problem (closest point to a line) to an alge-\nbraic concept (inner product). It turns out that these same geometric ideas from linear\nalgebra [ 2] can be translated to the conditional expectation. How this works is the\nsubject of our next section.\n2.3 Conditional Expectation as Projection\nNow that we understand projection methods geometrically, we can apply them toconditional probability. This is the key concept that ties probability to geometry,\noptimization, and linear algebra.\nInner Product for Random Variables . From our previous work on projection for\nvectors in R\nn, we have a good geometric grasp on how projection is related to\nMinimum Mean Squared Error (MMSE). By one abstract step, we can carry all of\nour geometric interpretations to the space of random variables. For example, wepreviously noted that at the point of projection, we had the following orthogonal\n(i.e., perpendicular vectors) condition,", "71": "2.3 Conditional Expectation as Projection 59\n(y\u2212vopt)Tv=0\nwhich by noting the inner product slightly more abstractly as /angbracketleftx,y/angbracketright=xTy, we can\nexpress as\n/angbracketlefty\u2212vopt,v/angbracketright= 0\nand by de\ufb01ning the inner product for the random variables XandYas\n/angbracketleftX,Y/angbracketright=E(XY)\nwe have the same relationship:\n/angbracketleftX\u2212hopt(Y),Y/angbracketright= 0\nwhich holds not for vectors in Rn, but for random variables XandYand functions of\nthose random variables. Exactly why this is true is technical, but it turns out that one\ncan build up the entire theory of probability this way [ 3], by using the expectation as\nan inner product.\nFurthermore, by abstracting out the inner product concept, we have connected\nminimum-mean-squared-error (MMSE) optimization problems, geometry, and ran-dom variables. That\u2019s a lot of mileage to get a out of an abstraction and it enables\nus to shift between these interpretations to address real problems. Soon, we\u2019ll do\nthis with some examples, but \ufb01rst we collect the most important result that \ufb02owsnaturally from this abstraction.\nConditional Expectation as Projection . The conditional expectation is the mini-\nmum mean squared error (MMSE) solution to the following problem\n1:\nmin\nh/integraldisplay\nR(x\u2212h(y))2dx\nwith the minimizing hopt(Y)as\nhopt(Y)=E(X|Y)\nwhich is another way of saying that among all possible functions h(Y), the one\nthat minimizes the MSE is E(X|Y). From our previous discussion on projection, we\nnoted that these MMSE solutions can be thought of as projections onto a subspace\nthat characterizes Y. For example, we previously noted that at the point of projection,\nwe have perpendicular terms,\n/angbracketleftX\u2212hopt(Y),Y/angbracketright= 0 (2.3.0.1)\n1See appendix for proof using the Cauchy\u2013Schwarz inequality.", "72": "60 2 Probability\nbut since we know that the MMSE solution\nhopt(Y)=E(X|Y)\nwe have by direct substitution,\nE(X\u2212E(X|Y),Y)=0 (2.3.0.2)\nThat last step seems pretty innocuous, but it ties MMSE to conditional expectation to\nthe inner project abstraction, and in so doing, reveals the conditional expectation to be\na projection operator for random variables. Before we develop this further, let\u2019s grab\nsome quick dividends. From the previous equation, by linearity of the expectation,we obtain,\nE(XY)=E(YE(X|Y))\nwhich is the so-called tower property of the expectation. Note that we could have\nfound this by using the formal de\ufb01nition of conditional expectation,\nE(X|Y)=/integraldisplay\nR2xfX,Y(x,y)\nfY(y)dxdy\nand brute-force direct integration,\nE(YE(X|Y))=/integraldisplay\nRy/integraldisplay\nRxfX,Y(x,y)\nfY(y)fY(y)dxdy\n=/integraldisplay\nR2xyf X,Y(x,y)dxdy\n=E(XY)\nwhich is not very geometrically intuitive. This lack of geometric intuition makes it\nhard to apply these concepts and keep track of these relationships.\nWe can keep pursuing this analogy and obtain the length of the error term from\nthe orthogonality property of the MMSE solution as,\n/angbracketleftX\u2212hopt(Y),X\u2212hopt(Y)/angbracketright=/angbracketleft X,X/angbracketright\u2212/angbracketleft hopt(Y),hopt(Y)/angbracketright\nand then by substituting all the notation we obtain\nE(X\u2212E(X|Y))2=E(X)2\u2212E(E(X|Y))2\nwhich would be tough to compute by direct integration.\nTo formally establish that E(X|Y)isin fact a projection operator we need to\nshow idempotency. Recall that idempotency means that once we project something", "73": "2.3 Conditional Expectation as Projection 61\nonto a subspace, further projections do nothing. In the space of random variables,\nE(X|\u00b7) is the idempotent projection as we can show by noting that\nhopt=E(X|Y)\nis purely a function of Y, so that\nE(hopt(Y)|Y)=hopt(Y)\nbecause Yis \ufb01xed, this veri\ufb01es idempotency. Thus, conditional expectation is the\ncorresponding projection operator for random variables. We can continue to carry\nover our geometric interpretations of projections for vectors ( v) into random vari-\nables ( X). With this important result, let\u2019s consider some examples of conditional\nexpectations obtained by using brute force to \ufb01nd the optimal MMSE function hopt\nas well as by using our new perspective on conditional expectation.\nExample . Suppose we have a random variable, X, then what constant is closest to\nXin the sense of the mean-squared-error (MSE)? In other words, which c\u2208R\nminimizes the following mean squared error:\nMSE=E(X\u2212c)2\nwe can work this out many ways. First, using calculus-based optimization,\nE(X\u2212c)2=E(c2\u22122cX+X2)=c2\u22122cE(X)+E(X2)\nand then take the \ufb01rst derivative with respect to cand solve:\ncopt=E(X)\nRemember that Xmay potentially take on many values, but this says that the closest\nnumber to Xin the MSE sense is E(X). This is intuitively pleasing. Coming at this\nsame problem using our inner product, from Eq. 2.3.0.2 we know that at the point of\nprojection\nE((X\u2212copt)1)=0\nwhere the 1 represents the space of constants we are projecting onto. By linearity of\nthe expectation, gives\ncopt=E(X)\nUsing the projection approach, because E(X|Y)is the projection operator, with\nY=\u03a9(the entire underlying probability space), we have, using the de\ufb01nition of\nconditional expectation:\nE(X|Y=\u03a9)=E(X)", "74": "62 2 Probability\nThis is because of the subtle fact that a random variable over the entire \u03a9space can\nonly be a constant. Thus, we just worked the same problem three ways (optimization,\northogonal inner products, projection).\nExample . Let\u2019s consider the following example with probability density fX,Y=\nx+ywhere (x,y)\u2208[0,1]2and compute the conditional expectation straight from\nthe de\ufb01nition:\nE(X|Y)=/integraldisplay1\n0xfX,Y(x,y)\nfY(y)dx=/integraldisplay1\n0xx+y\ny+1/2dx=3y+2\n6y+3\nThat was pretty easy because the density function was so simple. Now, let\u2019s do it the\nhard way by going directly for the MMSE solution h(Y). Then,\nMSE =min\nh/integraldisplay1\n0/integraldisplay1\n0(x\u2212h(y))2fX,Y(x,y)dxdy\n=min\nh/integraldisplay1\n0yh2(y)\u2212yh(y)+1\n3y+1\n2h2(y)\u22122\n3h(y)+1\n4dy\nN o ww eh a v et o\ufb01 n daf u n c t i o n hthat is going to minimize this. Solving for a function,\nas opposed to solving for a number, is generally very, very hard, but because we\nare integrating over a \ufb01nite interval, we can use the Euler\u2013Lagrange method fromvariational calculus to take the derivative of the integrand with respect to the function\nh(y)and set it to zero. Using Euler\u2013Lagrange methods, we obtain the following result,\n2yh(y)\u2212y+h(y)\u22122\n3=0\nSolving this gives\nhopt(y)=3y+2\n6y+3\nwhich is what we obtained before. Finally, we can solve this using our inner product\nin Eq. 2.3.0.1 as\nE((X\u2212h(Y))Y)=0\nWriting this out gives,\n/integraldisplay1\n0/integraldisplay1\n0(x\u2212h(y))y(x+y)dxdy =/integraldisplay1\n01\n6y(\u22123(2y+1)h(y)+3y+2)dy=0\nand the integrand must be zero,\n2y+3y2\u22123yh(y)\u22126y2h(y)=0", "75": "2.3 Conditional Expectation as Projection 63\nand solving this for h(y)gives the same solution:\nhopt(y)=3y+2\n6y+3\nThus, doing it by the brute force integration from the de\ufb01nition, optimization, or\ninner product gives us the same answer; but, in general, no method is necessarilyeasiest because they both involve potentially dif\ufb01cult or impossible integration, opti-\nmization, or functional equation solving. The point is that now that we have a deep\ntoolbox, we can pick and choose which tools we want to apply for different problems.\nBefore we leave this example, let\u2019s use Sympy to verify the length of the error\nfunction we found earlier for this example:\nE(X\u2212E(X|Y))\n2=E(X)2\u2212E(E(X|Y))2\nthat is based on the Pythagorean theorem. First, we need to compute the marginal\ndensities,\n>>> from sympy.abc import y,x\n>>> from sympy import integrate, simplify\n>>> fxy =x+y # joint density\n>>> fy=integrate(fxy,(x, 0,1))# marginal density\n>>> fx=integrate(fxy,(y, 0,1))# marginal density\nThen, we need to write out the conditional expectation,\n>>> EXY =(3*y+2)/(6*y+3)# conditional expectation\nNext, we can compute the left side, E(X\u2212E(X|Y))2, as the following,\n>>> # from the definition\n>>> LHS=integrate((x -EXY) **2* fxy,(x, 0,1),(y, 0,1))\n>>> LHS # left-hand-side\n-log(3)/144 + 1/12\nWe can similarly compute the right side, E(X)2\u2212E(E(X|Y))2, as the following,\n>>> # using Pythagorean theorem\n>>> RHS=integrate((x) **2* fx,(x, 0,1))-integrate((EXY) **2* fy,(y, 0,1))\n>>> RHS # right-hand-side\n-log(3)/144 + 1/12\nFinally, we can verify that the left and right sides match,\n>>> print (simplify(LHS -RHS) ==0)\nTrue\nIn this section, we have pulled together all the projection and least-squares opti-\nmization ideas from the previous sections to connect geometric notions of projection\nfrom vectors in Rnto random variables. This resulted in the remarkable realization\nthat the conditional expectation is in fact a projection operator for random variables.", "76": "64 2 Probability\nKnowing this allows to approach dif\ufb01cult problems in multiple ways, depending on\nwhich way is more intuitive or tractable in a particular situation. Indeed, \ufb01nding the\nright problem to solve is the hardest part, so having many ways of looking at thesame concepts is crucial.\nFor much more detailed development, the book by Mikosch [ 4] has some excel-\nlent sections covering much of this material with a similar geometric interpretation.Kobayashi et al. [ 5] does too. Nelson [ 3] also has a similar presentation based on\nhyper-real numbers.\n2.3.1 Appendix\nWe want to prove that we the conditional expectation is the minimum mean squared\nerror minimizer of the following:\nJ=min\nh/integraldisplay\nR2|X\u2212h(Y)|2fX,Y(x,y)dxdy\nWe can expand this as follows,\nJ=min\nh/integraldisplay\nR2|X|2fX,Y(x,y)dxdy+/integraldisplay\nR2|h(Y)|2fX,Y(x,y)dxdy\n\u2212/integraldisplay\nR22Xh(Y)fX,Y(x,y)dxdy\nTo minimize this, we have to maximize the following:\nA=max\nh/integraldisplay\nR2Xh(Y)fX,Y(x,y)dxdy\nBreaking up the integral using the de\ufb01nition of conditional expectation\nA=max\nh/integraldisplay\nR/parenleftbigg/integraldisplay\nRXfX|Y(x|y)dx/parenrightbigg\nh(Y)fY(y)dy (2.3.1.1)\n=max\nh/integraldisplay\nRE(X|Y)h(Y)fY(Y)dy (2.3.1.2)\nFrom properties of the Cauchy\u2013Schwarz inequality, we know that the maximum\nhappens when hopt(Y)=E(X|Y), so we have found the optimal h(Y)function as:\nhopt(Y)=E(X|Y)\nwhich shows that the optimal function is the conditional expectation.", "77": "2.4 Conditional Expectation and Mean Squared Error 65\n2.4 Conditional Expectation and Mean Squared Error\nIn this section, we work through a detailed example using conditional expectation\nand optimization methods. Suppose we have two fair six-sided dice ( XandY) and\nwe want to measure the sum of the two variables as Z=X+Y. Further, let\u2019s suppose\nthat given Z, we want the best estimate of Xin the mean-squared-sense. Thus, we\nwant to minimize the following:\nJ(\u03b1)=/summationdisplay\n(x\u2212\u03b1z)2P(x,z)\nwhere Pis the probability mass function for this problem. The idea is that when\nwe have solved this problem, we will have a function of Zthat is going to be the\nminimum MSE estimate of X. We can substitute in for ZinJand get:\nJ(\u03b1)=/summationdisplay\n(x\u2212\u03b1(x+y))2P(x,y)\nLet\u2019s work out the steps in Sympy in the following:\n>>> import sympy asS\n>>> from sympy.stats import density, E, Die\n>>> x=Die( \u2019D1\u2019 ,6) # 1st six sided die\n>>> y=Die( \u2019D2\u2019 ,6) # 2nd six sides die\n>>> a=S.symbols( \u2019a\u2019)\n>>> z=x+y # sum of 1st and 2nd die\n>>> J=E((x -a*(x+y))**2)# expectation\n>>> print (S.simplify(J))\n329*a**2/6 - 329*a/6 + 91/6\nWith all that setup we can now use basic calculus to minimize the objective function\nJ,\n>>> sol, =S.solve(S .diff(J,a),a) # using calculus to minimize\n>>> print (sol) # solution is 1/2\n1/2\nProgramming Tip\nSympy has a stats module that can do some basic work with expressions\ninvolving probability densities and expectations. The above code uses its E\nfunction to compute the expectation.\nThis says that z/2 is the MSE estimate of Xgiven Zwhich means geometrically\n(interpreting the MSE as a squared distance weighted by the probability mass func-\ntion) that z/2i sa s close toxas we are going to get for a given z.", "78": "66 2 Probability\nFig. 2.5 The values of Zare\nin yellow with the correspond-ing values for XandYon the\naxes. The gray scale colorsindicate the underlying jointprobability density\nLet\u2019s look at the same problem using the conditional expectation operator E(\u00b7|z)\nand apply it to our de\ufb01nition of Z. Then\nE(z|z)=E(x+y|z)=E(x|z)+E(y|z)=z\nusing the linearity of the expectation. Now, since by the symmetry of the problem\n(i.e., two identical die), we have\nE(x|z)=E(y|z)\nwe can plug this in and solve\n2E(x|z)=z\nwhich once again gives,\nE(x|z)=z\n2\nwhich is equal to the estimate we just found by minimizing the MSE. Let\u2019s explore\nthis further with Fig. 2.5. Figure 2.5 shows the values of Zin yellow with the\ncorresponding values for XandYon the axes. Suppose z=2, then the closest Xto\nthis is X=1, which is what E(x|z)=z/2=1 gives. What happens when Z=7?\nIn this case, this value is spread out diagonally along the Xaxis so if X=1, then Z\nis 6 units away, if X=2, then Zis 5 units away and so on.\nNow, back to the original question, if we had Z=7 and we wanted to get as close\nas we could to this using X, then why not choose X=6 which is only one unit away\nfrom Z? The problem with doing that is X=6 only occurs 1/6 of the time, so we\nare not likely to get it right the other 5/6 of the time. So, 1/6 of the time we are one\nunit away but 5/6 of the time we are much more than one unit away. This means that", "79": "2.4 Conditional Expectation and Mean Squared Error 67\nthe MSE score is going to be worse. Since each value of Xfrom 1 to 6 is equally\nlikely, to play it safe, we choose 7/2 as the estimate, which is what the conditional\nexpectation suggests.\nWe can check this claim with samples using Sympy below:\n>>> import numpy asnp\n>>> from sympy import stats\n>>> # Eq constrains Z\n>>> samples_z7 =lambda : stats .sample(x, S .Eq(z, 7))\n>>> #using 6 as an estimate\n>>> mn=np.mean([( 6-samples_z7()) **2 for iinrange (100)])\n>>> #7/2 is the MSE estimate\n>>> mn0=np.mean([( 7/2.- samples_z7()) **2 for iinrange (100)])\n>>> print (\u2019MSE= %3.2f using 6 vs MSE= %3.2f using 7/2 \u2019 %(mn,mn0))\nMSE=9.20 using 6 vs MSE=2.99 using 7/2\nProgramming Tip\nThestats.sample(x, S.Eq(z,7)) function call samples the xvariable\nsubject to a condition on the zvariable. In other words, it generates random\nsamples of xdie, given that the sum of the outcomes of that die and the ydie\nadd up to z==7 .\nPlease run the above code repeatedly until you are convinced that the E(x|z)gives\nthe lower MSE every time. To push this reasoning, let\u2019s consider the case where the\ndie is so biased so that the outcome of 6is ten times more probable than any of the\nother outcomes. That is,\nP(6)=2/3\nwhereas P(1)=P(2)=...=P(5)=1/15. We can explore this using Sympy as\nin the following:\n>>> # here 6 is ten times more probable than any other outcome\n>>> x=stats .FiniteRV( \u2019D3\u2019 ,{1:1/15. ,2:1/15. ,\n... 3:1/15. ,4:1/15. ,\n... 5:1/15. ,6:2/3. })\nAs before, we construct the sum of the two dice, and plot the corresponding proba-\nbility mass function in Fig. 2.6. As compared with Fig. 2.5, the probability mass has\nbeen shifted away from the smaller numbers.\nLet\u2019s see what the conditional expectation says about how we can estimate X\nfrom Z.\n>>> E(x, S .Eq(z, 7))# conditional expectation E(x|z=7)\n5.00000000000000\nNow that we have E(x|z=7)=5, we can generate samples as before and see if\nthis gives the minimum MSE.", "80": "68 2 Probability\nFig. 2.6 The values of Zare\nin yellow with the correspond-ing values for XandYon the\naxes\n>>> samples_z7 =lambda : stats .sample(x, S .Eq(z, 7))\n>>> #using 6 as an estimate\n>>> mn=np.mean([( 6-samples_z7()) **2 for iinrange (100)])\n>>> #5 is the MSE estimate\n>>> mn0=np.mean([( 5-samples_z7()) **2 for iinrange (100)])\n>>> print (\u2019MSE= %3.2f using 6 vs MSE= %3.2f using 5 \u2019 %(mn,mn0))\nMSE=3.19 using 6 vs MSE=2.86 using 5\nUsing a simple example, we have emphasized the connection between minimum\nmean squared error problems and conditional expectation. Hopefully, the last two \ufb01g-\nures helped expose the role of the probability density. Next, we\u2019ll continue revealing\nthe true power of the conditional expectation as we continue to develop correspondinggeometric intuition.\n2.5 Worked Examples of Conditional Expectation\nand Mean Square Error Optimization\nBrzezniak [ 6] is a great book because it approaches conditional expectation through\na sequence of exercises, which is what we are trying to do here. The main differ-\nence is that Brzezniak takes a more abstract measure-theoretic approach to the same\nproblems. Note that you doneed to grasp measure theory for advanced areas in\nprobability, but for what we have covered so far, working the same problems in his\ntext using our methods is illuminating. It always helps to have more than one way\nto solve any problem. I have numbered the examples corresponding to the book and\ntried to follow its notation.", "81": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 69\n2.5.1 Example\nThis is Example 2.1 from Brzezniak. Three coins, 10, 20 and 50p are tossed. The\nvalues of the coins that land heads up are totaled. What is the expected total giventhat two coins have landed heads up? In this case we have we want to compute E(\u03be|\u03b7)\nwhere\n\u03be:=10X\n10+20X20+50X50\nwhere Xi\u2208{0,1}and where X10is the Bernoulli-distributed random variable cor-\nresponding to the 10p coin (and so on). Thus, \u03berepresents the total value of the\nheads-up coins. The \u03b7represents the condition that only two of the three coins are\nheads-up,\n\u03b7:=X10X20(1\u2212X50)+(1\u2212X10)X20X50+X10(1\u2212X20)X50\nand is a function that is non-zero only when two of the three coins lands heads-up.\nEach triple term catches each of these three possibilities. For example, the \ufb01rst term\nequals one when the 10 and 20p are heads up and the 50p is heads down. The theremaining terms are zero.\nTo compute the conditional expectation, we want to \ufb01nd a function hof\u03b7that\nminimizes the mean-squared-error (MSE),\nMSE=/summationdisplay\nX\u2208{0,1}31\n23(\u03be\u2212h(\u03b7))2\nwhere the sum is taken over all possible triples of outcomes for {X10,X20,X50}\nbecause each of the three coins has a1\n2chance of coming up heads.\nNow, the question boils down to how can we characterize the function h(\u03b7)?N o t e\nthat\u03b7/mapsto\u2192{ 0,1}sohtakes on only two values. So, the orthogonal inner product\ncondition is the following:\n/angbracketleft\u03be\u2212h(\u03b7),\u03b7/angbracketright= 0\nBut, because are only interested in \u03b7=1, this simpli\ufb01es to\n/angbracketleft\u03be\u2212h(1),1/angbracketright= 0\n/angbracketleft\u03be,1/angbracketright=/angbracketleft h(1),1/angbracketright\nThis doesn\u2019t look so hard to evaluate but we have to compute the integral over the\nset where \u03b7=1. In other words, we need the set of triples {X10,X20,X50}where\n\u03b7=1. That is, we can compute\n/integraldisplay\n{\u03b7=1}\u03bedX=h(1)/integraldisplay\n{\u03b7=1}dX", "82": "70 2 Probability\nwhich is what Brzezniak does. Instead, we can de\ufb01ne h(\u03b7)=\u03b1\u03b7and then \ufb01nd \u03b1.\nRe-writing the orthogonal condition gives\n/angbracketleft\u03be\u2212\u03b7,\u03b1\u03b7/angbracketright= 0\n/angbracketleft\u03be,\u03b7/angbracketright=\u03b1/angbracketleft\u03b7,\u03b7/angbracketright\n\u03b1=/angbracketleft\u03be,\u03b7/angbracketright\n/angbracketleft\u03b7,\u03b7/angbracketright\nwhere\n/angbracketleft\u03be,\u03b7/angbracketright=/summationdisplay\nX\u2208{0,1}31\n23(\u03be\u03b7)\nNote that we can just sweep over all triples {X10,X20,X50}because the de\ufb01nition\nofh(\u03b7)zeros out when \u03b7=0 anyway. All we have to do is plug everything in and\nsolve. This tedious job is perfect for Sympy.\n>>> import sympy asS\n>>> X10,X20,X50 =S.symbols( \u2019X10,X20,X50\u2019 ,real =True )\n>>> xi = 10* X10+20* X20+50* X50\n>>> eta =X10*X20*(1-X50) +X10*(1-X20) *(X50) +(1-X10) *X20*(X50)\n>>> num=S.summation(xi *eta,(X10, 0,1),(X20, 0,1),(X50, 0,1))\n>>> den=S.summation(eta *eta,(X10, 0,1),(X20, 0,1),(X50, 0,1))\n>>> alpha =num/den\n>>> print (alpha) # alpha=160/3\n160/3\nThis means that\nE(\u03be|\u03b7)=160\n3\u03b7\nwhich we can check with a quick simulation\n>>> import pandas aspd\n>>> d=pd.DataFrame(columns =[\u2019X10\u2019 ,\u2019X20\u2019 ,\u2019X50\u2019 ])\n>>> d.X10 =np.random .randint( 0,2,1000 )\n>>> d.X10 =np.random .randint( 0,2,1000 )\n>>> d.X20 =np.random .randint( 0,2,1000 )\n>>> d.X50 =np.random .randint( 0,2,1000 )\nProgramming Tip\nThe code above creates an empty Pandas data frame with the named columns.\nThe next four lines assigns values to each of the columns.\nThe code above simulates \ufb02ipping the three coins 1000 times. Each column of the\ndataframe is either 0or1corresponding to heads-down or heads-up, respectively.\nThe condition is that two of the three coins have landed heads-up. Next, we can group", "83": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 71\nthe columns according to their sums. Note that the sum can only be in {0,1,2,3}\ncorresponding to 0heads-up, 1heads-up, and so on.\n>>> grp=d.groupby(d .eval( \u2019X10+X20+X50\u2019 ))\nProgramming Tip\nTheeval function of the Pandas data frame takes the named columns and\nevaluates the given formula. At the time of this writing, only simple formulas\ninvolving primitive operations are possible.\nNext, we can get the 2group, which corresponds to exactly two coins having landed\nheads-up, and then evaluate the sum of the values of the coins. Finally, we can take\nthe mean of these sums.\n>>> grp.get_group( 2).eval( \u201910*X10+20*X20+50*X50\u2019 ).mean()\n52.60162601626016\nThe result is close to 160/3=53.33 which supports the analytic result. The fol-\nlowing code shows that we can accomplish the same simulation using pure Numpy.\n>>> import numpy asnp\n>>> from numpy import array\n>>> x=np.random .randint( 0,2,(3,1000 ))\n>>> print (np.dot(x[:,x .sum(axis =0)==2].T,array([ 10,20,50])).mean())\n52.860759493670884\nIn this case, we used the Numpy dot product to compute the value of the heads-\nup coins. The sum(axis=0)==2 part selects the columns that correspond to two\nheads-up coins.\nStill another way to get at the same problem is to forego the random sampling\npart and just consider all possibilities exhaustively using the itertools module\nin Python\u2019s standard library.\n>>> import itertools asit\n>>> list (it.product(( 0,1),(0,1),(0,1)))\n[(0, 0, 0),\n(0, 0, 1),(0, 1, 0),(0, 1, 1),(1, 0, 0),(1, 0, 1),(1, 1, 0),(1, 1, 1)]\nNote that we need to call list above in order to trigger the iteration in\nit.product . This is because the itertools module is generator-based so does\nnot actually dothe iteration until it is iterated over (by list in this case). This\nshows all possible triples (X10,X20,X50)where 0and1indicate heads-down and", "84": "72 2 Probability\nheads-up, respectively. The next step is to \ufb01lter out the cases that correspond to two\nheads-up coins.\n>>> list (filter (lambda i:sum(i)==2,it.product(( 0,1),(0,1),(0,1))))\n[(0, 1, 1), (1, 0, 1), (1, 1, 0)]\nNext, we need to compute the sum of the coins and combine the prior code.\n>>> list (map(lambda k:10*k[0]+20* k[1]+50* k[2],\n... filter (lambda i:sum(i)==2,\n... it.product(( 0,1),(0,1),(0,1)))))\n[70, 60, 30]\nThe mean of the output is 53.33 , which is yet another way to get the same result.\nFor this example, we demonstrated the full spectrum of approaches made possibleusing Sympy, Numpy, and Pandas. It is always valuable to have multiple ways of\napproaching the same problem and cross-checking the result.\n2.5.2 Example\nThis is Example 2.2 from Brzezniak. Three coins, 10, 20 and 50p are tossed as\nbefore. What is the conditional expectation of the total amount shown by the three\ncoins given the total amount shown by the 10 and 20p coins only? For this problem,\n\u03be:=10X10+20X20+50X50\n\u03b7:=30X10X20+20(1\u2212X10)X20+10X10(1\u2212X20)\nwhich takes on four values \u03b7/mapsto\u2192{ 0,10,20,30}and only considers the 10p and 20p\ncoins. In contrast to the last problem, here we are interested in h(\u03b7)for all of the\nvalues of \u03b7. Naturally, there are only four values for h(\u03b7)corresponding to each of\nthese four values. Let\u2019s \ufb01rst consider \u03b7=10. The orthogonal condition is then\n/angbracketleft\u03be\u2212h(10),10/angbracketright= 0\nThe domain for \u03b7=10 is{X10=1,X20=0,X50}which we can integrate out of\nthe expectation below,\nE{X10=1,X20=0,X50}(\u03be\u2212h(10))10=0\nE{X50}(10\u2212h(10)+50X50)=0\n10\u2212h(10)+25=0\nwhich gives h(10)=35. Repeating the same process for \u03b7\u2208{20,30}gives h(20)=\n45 and h(30)=55, respectively. This is the approach Brzezniak takes. On the other\nhand, we can just look at af\ufb01ne functions, h(\u03b7)=a\u03b7+band use brute-force calculus.", "85": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 73\n>>> from sympy.abc import a,b\n>>> h=a*eta +b\n>>> eta =X10*X20*30 + X10*(1-X20) *(10)+(1-X10) *X20*(20)\n>>> MSE=S.summation((xi -h)**2* S.Rational( 1,8),(X10, 0,1),\n... (X20, 0,1),\n... (X50, 0,1))\n>>> sol=S.solve([S .diff(MSE,a),S .diff(MSE,b)],(a,b))\n>>> print (sol)\n{a: 64/3, b: 32}\nProgramming Tip\nTheRational function from Sympy code expresses a rational number that\nSympy is able to manipulate as such. This is different that specifying a fraction\nlike1/8. , which Python would automatically compute as a \ufb02oating point\nnumber (i.e., 0.125 ). The advantage of using Rational is that Sympy can\nlater produce rational numbers as output, which are sometimes easier to make\nsense of.\nThis means that\nE(\u03be|\u03b7)=25+\u03b7 (2.5.2.1)\nsince\u03b7takes on only four values, {0,10,20,30}, we can write this out explicitly as\nE(\u03be|\u03b7)=\u23a7\n\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23a925 for \u03b7=0\n35 for \u03b7=10\n45 for \u03b7=20\n55 for \u03b7=30(2.5.2.2)\nAlternatively, we can use orthogonal inner products to write out the following con-\nditions for the postulated af\ufb01ne function:\n/angbracketleft\u03be\u2212h(\u03b7),\u03b7/angbracketright= 0 (2.5.2.3)\n/angbracketleft\u03be\u2212h(\u03b7),1/angbracketright= 0 (2.5.2.4)\nWriting these out and solving for aand bis tedious and a perfect job for Sympy.\nStarting with Eq. 2.5.2.3 ,\n>>> expr =S.expand((xi -h)*eta)\n>>> print (expr)\n30*X10**2*X20*X50*a - 10*X10**2*X20*a - 10*X10**2*X50*a + 100*X10**2+ 60*X10*X20**2*X50*a - 20*X10*X20**2*a - 30*X10*X20*X50*a+ 400*X10*X20 + 500*X10*X50 - 10*X10*b - 20*X20**2*X50*a + 400*X20**2+ 1000*X20*X50 - 20*X20*b", "86": "74 2 Probability\nand then because E(X2\ni)=1/2=E(Xi), we make the following substitutions\n>>> expr .xreplace({X10 **2:0.5, X20 **2:0.5,X10: 0.5,X20: 0.5,X50: 0.5})\n-7.5*a - 15.0*b + 725.0\nWe can do this for the other orthogonal inner product in Eq. 2.5.2.4 as follows,\nProgramming Tip\nBecause Sympy symbols are hashable, they can be used as keys in Python\ndictionaries as in the xreplace function above.\n>>> S.expand((xi -h)*1).xreplace({X10 **2:0.5,\n... X20**2:0.5,\n... X10: 0.5,\n... X20: 0.5,\n... X50: 0.5})\n-0.375*a - b + 40.0\nThen, combining this result with the previous one and solving for aandbgives,\n>>> S.solve([ -350.0* a-15.0* b+725.0 ,-15.0* a-b+40.0 ])\n{a: 1.00000000000000, b: 25.0000000000000}\nwhich again gives us the \ufb01nal solution,\nE(\u03be|\u03b7)=25+\u03b7\nThe following is a quick simulation to demonstrate this. We can build on the Pandas\ndataframe we used for the last example and create a new column for the sum of the\n10p and 20p coins, as shown below.\n>>> d[\u2019sm\u2019 ]=d.eval( \u2019X10*10+X20*20\u2019 )\nWe can group this by the values of this sum,\n>>> d.groupby( \u2019sm\u2019 ).mean()\nX10 X20 X50\nsm0 0.0 0.0 0.50202410 1.0 0.0 0.53164620 0.0 1.0 0.45783130 1.0 1.0 0.516854\nBut we want the expectation of the value of the coins\n>>> d.groupby( \u2019sm\u2019 ).mean() .eval( \u201910*X10+20*X20+50*X50\u2019 )\nsm0 25.10121510 36.58227820 42.89156630 55.842697dtype: float64\nwhich is very close to our analytical result in Eq. 2.5.2.2 .", "87": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 75\n2.5.3 Example\nThis is Example 2.3 paraphrased from Brzezniak. Given Xuniformly distributed on\n[0,1], \ufb01ndE(\u03be|\u03b7)where\n\u03be(x)=2x2\n\u03b7(x)=\u23a7\n\u23aa\u23a8\n\u23aa\u23a91i f x\u2208[0,1/3]\n2i f x\u2208(1/3,2/3)\n0i f x\u2208(2/3,1]\nNote that this problem is different from the previous two because the sets that char-\nacterize \u03b7are intervals instead of discrete points. Nonetheless, we will eventually\nhave three values for h(\u03b7)because \u03b7/mapsto\u2192{ 0,1,2}.F o r\u03b7=1, we have the orthogonal\nconditions,\n/angbracketleft\u03be\u2212h(1),1/angbracketright= 0\nwhich boils down to\nE{x\u2208[0,1/3]}(\u03be\u2212h(1))=0\n/integraldisplay 1\n3\n0(2x2\u2212h(1))dx=0\nand then by solving this for h(1)gives h(1)=2/24. This is the way Brzezniak\nworks this problem. Alternatively, we can use h(\u03b7)=a+b\u03b7+c\u03b72and brute force\ncalculus.\n>>> x,c,b,a =S.symbols( \u2019x,c,b,a\u2019 )\n>>> xi=2 * x**2\n>>> eta=S.Piecewise(( 1,S.And(S .Gt(x, 0),\n... S.Lt(x,S .Rational( 1,3)))), # 0<x<1 / 3\n... (2,S.And(S .Gt(x,S .Rational( 1,3)),\n... S.Lt(x,S .Rational( 2,3)))), #1 / 3<x<2 / 3 ,\n... (0,S.And(S .Gt(x,S .Rational( 2,3)),\n... S.Lt(x, 1)))) #1 / 3<x<2 / 3\n>>> h=a+b*eta +c*eta**2\n>>> J=S.integrate((xi -h)**2,(x, 0,1))\n>>> sol=S.solve([S .diff(J,a),\n... S.diff(J,b),\n... S.diff(J,c),\n... ],\n... (a,b,c))\n>>> print (sol)\n{a: 38/27, b: -20/9, c: 8/9}>>> print (S.piecewise_fold(h .subs(sol)))\nPiecewise((2/27, (x > 0) & (x < 1/3)),\n(14/27, (x > 1/3) & (x < 2/3)),(38/27, (x > 2/3) & (x < 1)))", "88": "76 2 Probability\nThus, collecting this result gives:\nE(\u03be|\u03b7)=38\n27\u221220\n9\u03b7+8\n9\u03b72\nwhich can be re-written as a piecewise function of x,\nE(\u03be|\u03b7(x))=\u23a7\n\u23aa\u23a8\n\u23aa\u23a92\n27for 0<x<1\n3\n14\n27for1\n3<x<2\n3\n38\n27for2\n3<x<1(2.5.3.1)\nAlternatively, we can use the orthogonal inner product conditions directly by\nchoosing h(\u03b7)=c+\u03b7b+\u03b72a,\n/angbracketleft\u03be\u2212h(\u03b7),1/angbracketright= 0\n/angbracketleft\u03be\u2212h(\u03b7),\u03b7/angbracketright= 0\n/angbracketleft\u03be\u2212h(\u03b7),\u03b72/angbracketright= 0\nand then solving for a,b, and c.\n>>> x,a,b,c,eta =S.symbols( \u2019x,a,b,c,eta\u2019 ,real =True )\n>>> xi =2 * x**2\n>>> eta=S.Piecewise(( 1,S.And(S .Gt(x, 0),\n... S.Lt(x,S .Rational( 1,3)))), # 0 < x < 1/3\n... (2,S.And(S .Gt(x,S .Rational( 1,3)),\n... S.Lt(x,S .Rational( 2,3)))), # 1/3 < x < 2/3,\n... (0,S.And(S .Gt(x,S .Rational( 2,3)),\n... S.Lt(x, 1)))) # 1/3 < x < 2/3\n>>> h=c+b*eta+a*eta**2\nThen, the orthogonal conditions become,\n>>> S.integrate((xi -h)*1,(x, 0,1))\n-5*a/3 - b - c + 2/3>>> S.integrate((xi -h)*eta,(x, 0,1))\n-3*a - 5*b/3 - c + 10/27>>> S.integrate((xi -h)*eta**2,(x, 0,1))\n-17*a/3 - 3*b - 5*c/3 + 58/81\nNow, we just combine the three equations and solve for the parameters,\n>>> eqs=[-5*a/3 - b-c+ 2/3 ,\n... -3*a-5 * b/3 - c+ 10/27 ,\n... -17* a/3 - 3* b-5 * c/3 + 58/81 ]\n>>> sol=S.solve(eqs)\n>>> print (sol)\n{a: 0.888888888888889, b: -2.22222222222222, c: 1.40740740740741}", "89": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 77\nWe can assemble the \ufb01nal result by substituting in the solution,\n>>> print (S.piecewise_fold(h .subs(sol)))\nPiecewise((0.074074074074074, (x > 0) & (x < 1/3)),\n(0.518518518518518, (x > 1/3) & (x < 2/3)),\n(1.40740740740741, (x > 2/3) & (x < 1)))\nwhich is the same as our analytic result in Eq. 2.5.3.1 , just in decimal format.\nProgramming Tip\nThe de\ufb01nition of Sympy\u2019s piecewise function is verbose because of the way\nPython parses inequality statements. As of this writing, this has not been rec-onciled in Sympy, so we have to use the verbose declaration.\nTo reinforce our result, let\u2019s do a quick simulation using Pandas.\n>>> d=pd.DataFrame(columns =[\u2019x\u2019,\u2019eta\u2019 ,\u2019xi\u2019 ])\n>>> d.x=np.random .rand( 1000 )\n>>> d.xi=2 * d.x**2\n>>> d.xi.head()\n0 0.6492011 1.213763\n2 1.225751\n3 0.0052034 0.216274Name: xi, dtype: float64\nNow, we can use the pd.cut function to group the xvalues in the following,\n>>> pd.cut(d .x,[0,1/3,2/3,1]).head()\n0 (0.333, 0.667]\n1 (0.667, 1.0]\n2 (0.667, 1.0]3 (0.0, 0.333]4 (0.0, 0.333]\nName: x, dtype: category\nCategories (3, interval[float64]): [(0.0, 0.333] < (0.333, 0.667]\n< (0.667, 1.0]]\nNote that the head() call above is only to limit the printout shown. The categories\nlisted are each of the intervals for eta that we speci\ufb01ed using the [0,1/3,2/3,1]\nlist. Now that we know how to use pd.cut , we can just compute the mean on each\ngroup as shown below,\n>>> d.groupby(pd .cut(d .x,[0,1/3,2/3,1])).mean()[ \u2019xi\u2019 ]\nx(0.0, 0.333] 0.073048(0.333, 0.667] 0.524023(0.667, 1.0] 1.397096Name: xi, dtype: float64", "90": "78 2 Probability\nwhich is pretty close to our analytic result in Eq. 2.5.3.1 . Alternatively, sympy.stats\nhas some limited tools for the same calculation.\n>>> from sympy.stats import E, Uniform\n>>> x=Uniform( \u2019x\u2019,0,1)\n>>> E(2*x**2,S.And(x <S.Rational( 1,3), x >0))\n2/27>>> E(2*x**2,S.And(x <S.Rational( 2,3), x >S.Rational( 1,3)))\n14/27>>> E(2*x**2,S.And(x <1,x >S.Rational( 2,3)))\n38/27\nwhich again gives the same result still another way.\n2.5.4 Example\nThis is Example 2.4 from Brzezniak. Find E(\u03be|\u03b7)for\n\u03be(x)=2x2\n\u03b7=/braceleftBigg\n2i f 0 \u2264x<1\n2\nxif1\n2<x\u22641\nOnce again, Xis uniformly distributed on the unit interval. Note that \u03b7is no longer\ndiscrete for every domain. For the domain 0 <x<1/2,h(2)takes on only one\nvalue, say, h0. For this domain, the orthogonal condition becomes,\nE{\u03b7=2}((\u03be(x)\u2212h0)2)=0\nwhich simpli\ufb01es to,\n/integraldisplay1/2\n02x2\u2212h0dx=0\n/integraldisplay1/2\n02x2dx=/integraldisplay1/2\n0h0dx\nh0=2/integraldisplay1/2\n02x2dx\nh0=1\n6\nFor the other domain where {\u03b7=x}in Eq. 2.5.4 , we again use the orthogonal\ncondition,", "91": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 79\nE{\u03b7=x}((\u03be(x)\u2212h(x))x)=0\n/integraldisplay1\n1/2(2x2\u2212h(x))xdx=0\nh(x)=2x2\nAssembling the solution gives,\nE(\u03be|\u03b7(x))=/braceleftBigg\n1\n6for 0\u2264x<1\n2\n2x2for1\n2<x\u22641\nalthough this result is not explicitly written as a function of \u03b7.\n2.5.5 Example\nThis is Exercise 2.6 in Brzezniak. Find E(\u03be|\u03b7)where\n\u03be(x)=2x2\n\u03b7(x)=1\u2212|2x\u22121|\nandXis uniformly distributed in the unit interval. We can write this out as a piecewise\nfunction in the following,\n\u03b7=/braceleftBigg\n2x for 0\u2264x<1\n2\n2\u22122xfor1\n2<x\u22641\nThe discontinuity is at x=1/2. Let\u2019s start with the {\u03b7=2x}domain.\nE{\u03b7=2x}((2x2\u2212h(2x))2x)=0\n/integraldisplay1/2\n0(2x2\u2212h(2x))2xdx=0\nWe can make this explicitly a function of \u03b7by a change of variables ( \u03b7=2x) which\ngives/integraldisplay1\n0(\u03b72/2\u2212h(\u03b7))\u03b7\n2d\u03b7=0\nThus, for this domain, h(\u03b7)=\u03b72/2. Note that due to the change of variables, h(\u03b7)\nis valid de\ufb01ned over \u03b7\u2208[0,1].", "92": "80 2 Probability\nFor the other domain where {\u03b7=2\u22122x},w eh a v e\nE{\u03b7=2\u22122x}((2x2\u2212h(2\u22122x))(2\u22122x))=0\n/integraldisplay1\n1/2(2x2\u2212h(2\u22122x))(2\u22122x)dx=0\nOnce again, a change of variables makes the \u03b7dependency explicit using \u03b7=2\u22122x\nwhich gives\n/integraldisplay1\n0((2\u2212\u03b7)2/2\u2212h(\u03b7))\u03b7\n2d\u03b7=0\nh(\u03b7)=(2\u2212\u03b7)2/2\nOnce again, the change of variables means this solution is valid over \u03b7\u2208[0,1].\nThus, because both pieces are valid over the same domain ( \u03b7\u2208[0,1]), we can just\nadd them to get the \ufb01nal solution,\nh(\u03b7)=\u03b72\u22122\u03b7+2\nA quick simulation can help bear this out.\n>>> from pandas import DataFrame\n>>> import numpy asnp\n>>> d=DataFrame(columns =[\u2019xi\u2019 ,\u2019eta\u2019 ,\u2019x\u2019,\u2019h\u2019,\u2019h1\u2019 ,\u2019h2\u2019 ])\n>>> # 100 random samples\n>>> d.x=np.random .rand( 100)\n>>> d.xi=d.eval( \u20192*x**2\u2019 )\n>>> d.eta =1-abs(2*d.x-1)\n>>> d.h1=d[(d .x<0.5 )].eval( \u2019eta**2/2\u2019 )\n>>> d.h2=d[(d .x>=0.5 )].eval( \u2019(2-eta)**2/2\u2019 )\n>>> d.fillna( 0,inplace =True )\n>>> d.h=d.h1+d.h2\n>>> d.head()\nxi eta x h h1 h2\n0 1.102459 0.515104 0.742448 1.102459 0.000000 1.1024591 0.239610 0.692257 0.346128 0.239610 0.239610 0.0000002 1.811868 0.096389 0.951806 1.811868 0.000000 1.8118683 0.000271 0.023268 0.011634 0.000271 0.000271 0.0000004 0.284240 0.753977 0.376988 0.284240 0.284240 0.000000\nNote that we have to be careful where we apply the individual solutions using the\nslice(d.x<0.5) index. The fillna part ensures that the default NaN that \ufb01lls out\nthe empty row-etries is replaced with zero before combining the individual solutions.\nOtherwise, the NaN values would circulate through the rest of the computation. The\nfollowing is the essential code that draws Fig. 2.7.", "93": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 81\nFig. 2.7 The diagonal line\nshows where the conditionalexpectation equals the \u03be\nfunction\nfrom matplotlib.pyplot import subplots\nfig,ax =subplots()\nax.plot(d .xi,d .eta, \u2019.\u2019,alpha =.3,label =\u2019$\\eta$\u2019 )\nax.plot(d .xi,d .h,\u2019k.\u2019 ,label =\u2019$h(\\eta)$\u2019 )\nax.legend(loc =0,fontsize =18)\nax.set_xlabel( \u2019$2 x\u02c62$\u2019 ,fontsize =18)\nax.set_ylabel( \u2019$h(\\eta)$\u2019 ,fontsize =18)\nProgramming Tip\nBasic L ATEX formatting works for the labels in Fig. 2.7.T h e loc=0 in the\nlegend function is the code for the best placement for the labels in the leg-\nend. The individual labels should be speci\ufb01ed when the elements are drawn\nindividually, otherwise they will be hard to separate out later. This is accom-plished using the label keyword in the plot commands.\nFigure 2.7 shows the \u03bedata plotted against \u03b7and h(\u03b7)=E(\u03be|\u03b7). Points on\nthe diagonal are points where \u03beandE(\u03be|\u03b7)match. As shown by the dots, there\nis no agreement between the raw \u03b7data and \u03be. Thus, one way to think about the\nconditional expectation is as a functional transform that bends the curve onto the\ndiagonal line. The black dots plot \u03beversus E(\u03be|\u03b7)and the two match everywhere\nalong the diagonal line. This is to be expected because the conditional expectationis the MSE best estimate for \u03beamong all functions of \u03b7.", "94": "82 2 Probability\n2.5.6 Example\nThis is Exercise 2.14 from Brzezniak. Find E(\u03be|\u03b7)where\n\u03be(x)=2x2\n\u03b7=/braceleftBigg\n2x if 0\u2264x<1\n2\n2x\u22121i f1\n2<x\u22641\nandXis uniformly distributed in the unit interval. This is the same as the last example\nand the only difference here is that \u03b7is not continuous at x=1\n2, as before. The \ufb01rst\npart is exactly the same as the \ufb01rst part of the prior example so we will skip it here.The second part follows the same reasoning as the last example, so we will just write\nthe answer for the {\u03b7=2x\u22121}case as the following\nh(\u03b7)=(1+\u03b7)\n2\n2,\u2200\u03b7\u2208[0,1]\nand then adding these up as before gives the full solution:\nh(\u03b7)=1\n2+\u03b7+\u03b72\nThe interesting part about this example is shown in Fig. 2.8. The dots show where\n\u03b7is discontinuous and yet the h(\u03b7)=E(\u03be|\u03b7)solution is equal to \u03be(i.e., matches\nthe diagonal). This illustrates the power of the orthogonal inner product technique,\nFig. 2.8 The diagonal line\nshows where the conditionalexpectation equals the \u03be\nfunction\n", "95": "2.5 Worked Examples of Conditional Expectation and Mean Square Error Optimization 83\nwhich does not need continuity or complex set-theoretic arguments to calculate solu-\ntions. By contrast, I urge you to consider Brzezniak\u2019s solution to this problem which\nrequires such methods.\nExtending projection methods to random variables provides multiple ways for\ncalculating solutions to conditional expectation problems. In this section, we also\nworked out corresponding simulations using a variety of Python modules. It is alwaysadvisable to have more than one technique at hand to cross-check potential solutions.\nWe worked out some of the examples in Brzezniak\u2019s book using our methods as a way\nto show multiple ways to solve the same problem. Comparing Brzezniak\u2019s measure-theoretic methods to our less abstract techniques is a great way to get a handle on\nboth concepts, which are important for advanced study in stochastic process.\n2.6 Useful Distributions\n2.6.1 Normal Distribution\nWithout a doubt, the normal (Gaussian) distribution is the most important and foun-\ndational probability distribution. The one-dimensional form is the following:\nf(x)=e\u2212(x\u2212\u03bc)2\n2\u03c32\n\u221a\n2\u03c0\u03c32\nwhere E(x)=\u03bcandV(x)=\u03c32. The multidimension al version for x\u2208Rnis the\nfollowing,\nf(x)=1\ndet(2\u03c0R)1\n2e\u22121\n2(x\u2212\u03bc)TR\u22121(x\u2212\u03bc)\nwhere Ris the covariance matrix with entries\nRi,j=E/bracketleftbig\n(xi\u2212\u00afxi)(xj\u2212\u00afxj)/bracketrightbig\nA key property of the normal distribution is that it is completely speci\ufb01ed by its \ufb01rst\ntwo moments. Another key property is that the normal distribution is preserved under\nlinear transformations. For example,\ny=Ax\nmeans y\u223cN(Ax,AR xAT). This means that it is easy to do linear algebra and\nmatrix operations with normal distributed random variables. There are many intuitive\ngeometric relationships that are preserved with normal distributed random variables,\nas discussed in the Gauss-Markov chapter.", "96": "84 2 Probability\n2.6.2 Multinomial Distribution\nThe Multinomial distribution generalized the Binomial distribution. Recall that the\nBinomial distribution characterizes the number of heads obtained in ntrials.\nConsider the problem of nballs to be divided among ravailable bins where each\nbin may accommodate more than one ball. For example, suppose n=10 and and\nr=3, then one possible valid con\ufb01guration is N10=[ 3,3,4]. The probability\nthat a ball lands in the ithbin is pi, where/summationtextpi=1. The Multinomial distribution\ncharacterizes the probability distribution of Nn. The Binomial distribution is a special\ncase of the Multinomial distribution with n=2. The Multinomial distribution is\nimplemented in the scipy.stats module as shown below,\n>>> from scipy.stats import multinomial\n>>> rv=multinomial( 10,[1/3]*3)\n>>> rv.rvs( 4)\narray([[2, 2, 6],\n[4, 2, 4],[2, 4, 4],\n[2, 6, 2]])\nNote that the sum across the columns is always n\n>>> rv.rvs( 10).sum(axis =1)\narray([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\nTo derive the probability mass function, we de\ufb01ne the occupancy vector ,ei\u2208Rr\nwhich is a binary vector with exactly one non-zero component (i.e., a unit vector).\nThen, the Nnvector can be written as the sum of nvectors X, each drawn from the\nset{ej}r\nj=1,\nNn=n/summationdisplay\ni=1Xi\nwhere the probability P(X=ej)=pj. Thus, Nnhas a discrete distribution over the\nset of vectors with non-negative components that sum to n. Because the Xvectors\nare independent and identically distributed, the probability of any particular Nn=\n[x1,x2,..., xr]/latticetop=xis\nP(Nn=x)=Cnpx1\n1px2\n2\u00b7\u00b7\u00b7pxrr\nwhere Cnis a combinatorial factor that accounts for all the ways a component can\nsum to xj. Consider that there are/parenleftbign\nx1/parenrightbig\nways that the \ufb01rst component can be chosen.\nThis leaves n\u2212x1balls left for the rest of the vector components. Thus, the second\ncomponent has/parenleftbign\u2212x1\nx2/parenrightbig\nways to pick a ball. Following the same pattern, the third\ncomponent has/parenleftbign\u2212x1\u2212x2\nx3/parenrightbig\nways and so forth,\nCn=/parenleftbiggn\nx1/parenrightbigg/parenleftbiggn\u2212x1\nx2/parenrightbigg/parenleftbiggn\u2212x1\u2212x2\nx3/parenrightbigg\n\u00b7\u00b7\u00b7/parenleftbiggn\u2212x1\u2212x2\u2212\u00b7\u00b7\u00b7\u2212 xr\u22121\nxr/parenrightbigg", "97": "2.6 Useful Distributions 85\nsimpli\ufb01es to the following,\nCn=n!\nx1!\u00b7\u00b7\u00b7 xr!\nThus, the probability mass function for the Multinomial distribution is the following,\nP(Nn=x)=n!\nx1!\u00b7\u00b7\u00b7 xr!px1\n1px2\n2\u00b7\u00b7\u00b7pxrr\nThe expectation of this distribution is the following,\nE(Nn)=n/summationdisplay\ni=1E(Xi)\nby the linearity of the expectation. Then,\nE(Xi)=r/summationdisplay\nj=1pjej=Ip=p\nwhere pjare the components of the vector pandIis the identity matrix. Then,\nbecause this is the same for any Xi,w eh a v e\nE(Nn)=np\nFor the covariance of Nn, we need to compute the following,\nCov(Nn)=E/parenleftBig\nNnN/latticetop\nn/parenrightBig\n\u2212E(Nn)E(Nn)/latticetop\nFor the \ufb01rst term on the right, we have\nE/parenleftBig\nNnN/latticetop\nn/parenrightBig\n=E\u239b\n\u239d(n/summationdisplay\ni=1Xi)(n/summationdisplay\nj=1X/latticetop\nj)\u239e\n\u23a0\nand for i=j,w eh a v e\nE(XiX/latticetop\ni)=diag(p)\nand for i/negationslash=j,w eh a v e\nE(XiX/latticetop\nj)=pp/latticetop\nNote that this term has elements on the diagonal. Then, combining the above two\nequations gives the following,\nE(NnN/latticetop\nn)=ndiag(p)+(n2\u2212n)pp/latticetop", "98": "86 2 Probability\nNow, we can assemble the covariance matrix,\nCov(Nn)=ndiag(p)+(n2\u2212n)pp/latticetop\u2212n2pp/latticetop=ndiag(p)\u2212npp/latticetop\nSpeci\ufb01cally, the off-diagonal terms are npipjand the diagonal terms are npi(1\u2212pi).\n2.6.3 Chi-square Distribution\nThe\u03c72distribution appears in many different contexts so it\u2019s worth understanding.\nSuppose we have nindependent random variables Xisuch that Xi\u223cN(0,1).W e\nare interested in the following random variable R=/radicalBig/summationtext\niX2\ni. The joint probability\ndensity of Xiis the following,\nfX(X)=e\u22121\n2/summationtext\niX2\ni\n(2\u03c0)n\n2\nwhere the Xrepresents a vector of Xirandom variables. Y ou can think of Ras the\nradius of an n-dimensional sphere. The volume of this sphere is given by the the\nfollowing formula,\nVn(R)=\u03c0n\n2\n\u0393(n\n2+1)Rn\nTo reduce the amount of notation we de\ufb01ne,\nA:=\u03c0n\n2\n\u0393(n\n2+1)\nThe differential of this volume is the following,\ndVn(R)=nARn\u22121dR\nIn term of the Xicoordinates, the probability (as always) integrates out to one.\n/integraldisplay\nfX(X)dVn(X)=1\nIn terms of R, the change of variable provides,\n/integraldisplay\nfX(R)nARn\u22121dR", "99": "2.6 Useful Distributions 87\nThus,\nfR(R):=fX(R)=nARn\u22121e\u22121\n2R2\n(2\u03c0)n\n2\nBut we are interested in the distribution Y=R2. Using the same technique again,\n/integraldisplay\nfR(R)dR=/integraldisplay\nfR(\u221a\nY)dY\n2\u221a\nY\nFinally,\nfY(Y):=nA Yn\u22121\n2e\u22121\n2Y\n(2\u03c0)n\n21\n2\u221a\nY\nThen, \ufb01nally substituting back in Agives the \u03c72distribution with ndegrees of\nfreedom,\nfY(Y)=n\u03c0n\n2\n\u0393(n\n2+1)Yn/2\u22121e\u22121\n2Y\n(2\u03c0)n\n21\n2=2\u2212n\n2\u22121n\n\u0393/parenleftbign\n2+1/parenrightbige\u2212Y/2Yn\n2\u22121\nExample : Hypothesis testing is a common application of the \u03c72distribution. Con-\nsider Table 2.1 which tabulates the infection status of a certain population. The\nhypothesis is that these data are distributed according to the multinomial distribution\nwith the following rates for each group, p1=1/4 (mild infection), p2=1/4 (strong\ninfection), and p3=1/2 (no infection). Suppose niis the count of persons in the ith\ncolumn and/summationtext\nini=n=684. Let kdenote the number of columns. Then, in order\nto apply the Central Limit Theorem, we want to sum the nirandom variables, but\nthese all sum to n, a constant, which prohibits using the theorem. Instead, suppose\nwe sum the nivariables up to k\u22121 terms. Then,\nz=k\u22121/summationdisplay\ni=1ni\nis asymptotically normally distributed by the theorem with mean E(z)=/summationtextk\u22121\ni=1npi.\nUsing our previous results and notation for multinomial random variables, we can\nwrite this as\nz=[1/latticetop\nk\u22121,0]Nn\nTable 2.1 Diagnosis table\nMild infection Strong infection No infection Total\n128 136 420 684", "100": "88 2 Probability\nwhere 1k\u22121is a vector of all ones of length k\u22121 and Nn\u2208Rk. With this notation,\nwe have\nE(z)=n[1/latticetop\nk\u22121,0]p=k\u22121/summationdisplay\ni=1npi=n(1\u2212pk)\nWe can get the variance of zusing the same method,\nV(z)=[1/latticetop\nk\u22121,0]Cov(Nn)[1/latticetop\nk\u22121,0]/latticetop\nwhich gives,\nV(z)=[1/latticetop\nk\u22121,0](ndiag(p)\u2212npp/latticetop)[1/latticetop\nk\u22121,0]/latticetop\nThe variance is then,\nV(z)=n(1\u2212pk)pk\nWith the mean and variance established we can subtract the hypothesize mean for\neach column under the hypothesis and create the transformed variable,\nz/prime=k\u22121/summationdisplay\ni=1ni\u2212npi\u221an(1\u2212pk)pk\u223cN(0,1)\nby the Central Limit Theorem. Likewise,\nk\u22121/summationdisplay\ni=1(ni\u2212npi)2\nn(1\u2212pk)pk\u223c\u03c72\nk\u22121\nWith all that established, we can test the hypothesis that the data in the table follow\nthe hypothesized multinomial distribution.\n>>> from scipy import stats\n>>> n= 684\n>>> p1=p2= 1/4\n>>> p3= 1/2\n>>> v=n*p3*(1-p3)\n>>> z=(128- n*p1)**2/ v+(136- n*p2)**2/ v\n>>> 1-stats .chi2( 2).cdf(z)\n0.00012486166748693073\nThis value is very low and suggests that the hypothesized multinomial distribution\nis not a good one for this data. Note that this approximation only works when nis\nlarge in comparison to the number of columns in the table.", "101": "2.6 Useful Distributions 89\n2.6.4 Poisson and Exponential Distributions\nThe Poisson distribution for a random variable Xrepresents a number of outcomes\noccurring in a given time interval ( t).\np(x;\u03bbt)=e\u2212\u03bbt(\u03bbt)x\nx!\nThe Poisson distribution is closely related to the binomial distribution, b(k;n,p)\nwhere pis small and nis large. That is, when there is a low-probability event but\nmany trials, n. Recall that the binomial distribution is the following,\nb(k;n,p)=/parenleftbiggn\nk/parenrightbigg\npk(1\u2212p)n\u2212k\nfork=0 and taking the logarithm of both sides, we obtain\nlogb(0;n,p)=(1\u2212p)n=/parenleftbigg\n1\u2212\u03bb\nn/parenrightbiggn\nThen, the Taylor expansion of this gives the following,\nlogb(0;n,p)\u2248\u2212\u03bb\u2212\u03bb2\n2n\u2212\u00b7\u00b7\u00b7\nFor large n, this results in,\nb(0;n,p)\u2248e\u2212\u03bb\nA similar argument for kleads to the Poisson distribution. Conveniently, we have\nE(X)=V(X)=\u03bb. For example, suppose that the average number of vehicles\npassing under a toll-gate per hour is 3. Then, the probability that 6 vehicles passunder the gate in a given hour is p(x=6;\u03bbt=3)=\n81\n30e3\u22480.05.\nThe Poisson distribution is available from the scipy.stats module. The fol-\nlowing code computes the last result,\n>>> from scipy.stats import poisson\n>>> x=poisson( 3)\n>>> print (x.pmf( 6))\n0.05040940672246224\nThe Poisson distribution is important for applications involving reliability and\nqueueing. The Poisson distribution is used to compute the probability of speci\ufb01c\nnumbers of events during a particular time period. In many cases the time period ( X)\nitself is the random variable. For example, we might be interested in understanding\nthe time Xbetween arrivals of vehicles at a checkpoint. With the Poisson distribution,\nthe probability of noevents occurring in the span of time up to time tis given by the", "102": "90 2 Probability\nfollowing,\np(0;\u03bbt)=e\u2212\u03bbt\nNow, suppose Xis the time to the \ufb01rst event. The probability that the length of time\nuntil the \ufb01rst event will exceed xis given by the following,\nP(X>x)=e\u2212\u03bbx\nThen, the cumulative distribution function is given by the following,\nP(0\u2264X\u2264x)=FX(x)=1\u2212e\u2212\u03bbx\nTaking the derivative gives the exponential distribution,\nfX(x)=\u03bbe\u2212\u03bbx\nwhere E(X)=1/\u03bbandV(X)=1\n\u03bb2. For example, suppose we want to know\nthe probability of a certain component lasting beyond T=10 years where Tis\nmodeled as a an exponential random variable with 1 /\u03bb=5 years. Then, we have\n1\u2212FX(10)=e\u22122\u22480.135.\nThe exponential distribution is available in the scipy.stats module. The\nfollowing code computes the result of the example above. Note that the parameters\nare described in slightly different terms as above, as described in the correspondingdocumentation for expon .\n>>> from scipy.stats import expon\n>>> x=expon( 0,5)# create random variable object\n>>> print (1- x.cdf( 10))\n0.1353352832366127\n2.6.5 Gamma Distribution\nWe have previously discussed how the exponential distribution can be created from\nthe Poisson events. The exponential distribution has the memoryless property, namely,\nP(T>t0+t|T>t0)=P(T>t)\nFor example, given Tas the random variable representing the time until failure,\nthis means that a component that has survived up through t0has the same failure\nprobability of lasting tunits beyond that point. To derive this result, it is easier to\ncompute the complementary event,\nP(t0<T<t0+t|T>t0)=P(t0<T<t0+t)=e\u2212\u03bbt/parenleftBig\ne\u03bbt\u22121/parenrightBig", "103": "2.6 Useful Distributions 91\nThen, one minus this result shows the memoryless property, which, unrealistically,\ndoes not account for wear over the \ufb01rst thours. The gamma distribution can remedy\nthis.\nRecall that the exponential distribution describes the time until the occurrence\nof a Poisson event, the random variable Xfor the time until a speci\ufb01ed number of\nPoisson events ( \u03b1) is described by the gamma distribution. Thus, the exponential\ndistribution is a special case of the gamma distribution when \u03b1=1 and \u03b2=1/\u03bb.\nForx>0, the gamma distribution is the following,\nf(x;\u03b1,\u03b2)=\u03b2\u2212\u03b1x\u03b1\u22121e\u2212x\n\u03b2\n\u0393(\u03b1)\nand f(x;\u03b1,\u03b2)=0 when x\u22640 and\u0393is the gamma function. For example, suppose\nthat vehicles passing under a gate follows a Poisson process, with an average of 5vehicles passing per hour, what is the probability that at most an hour will have\npassed before 2 vehicles pass the gate? If Xis time in hours that transpires before\nthe 2 vehicles pass, then we have \u03b2=1/5 and \u03b1=2. The required probability\nP(X<1)\u22480.96. The gamma distribution has E(X)=\u03b1\u03b2andV(X)=\u03b1\u03b2\n2\nThe following code computes the result of the example above. Note that the\nparameters are described in slightly different terms as above, as described in thecorresponding documentation for gamma .\n>>> from scipy.stats import gamma\n>>> x=gamma( 2,scale =1/5 )# create random variable object\n>>> print (x.cdf( 1))\n0.9595723180054873\n2.6.6 Beta Distribution\nThe uniform distribution assigns a single constant value over the unit interval. The\nBeta distribution generalizes this to a function over the unit interval. The probabilitydensity function of the Beta distribution is the following,\nf(x)=1\n\u03b2(a,b)xa\u22121(1\u2212x)b\u22121\nwhere\n\u03b2(a,b)=/integraldisplay1\n0xa\u22121(1\u2212x)b\u22121dx\nNote that a=b=1 yields the uniform distribution. In the special case for integers\nwhere 0 \u2264k\u2264n,w eh a v e", "104": "92 2 Probability\n/integraldisplay1\n0/parenleftbiggn\nk/parenrightbigg\nxk(1\u2212x)n\u2212kdx=1\nn+1\nTo get this result without calculus, we can use an experiment by Thomas Bayes.\nStart with nwhite balls and one gray ball. Uniformly at random, toss them onto\nthe unit interval. Let Xbe the number of white balls to the left of the gray ball.\nThus, X\u2208{0,1,..., n}. To compute P(X=k), we condition on the probability of\nthe position Bof the gray ball, which is uniformly distributed over the unit interval\n(f(p)=1). Thus, we have\nP(X=k)=/integraldisplay1\n0P(X=k|B=p)f(p)dp=/integraldisplay1\n0/parenleftbiggn\nk/parenrightbigg\npk(1\u2212p)n\u2212kdp\nNow, consider a slight variation on the experiment where we start with n+1 white\nballs and again toss them onto the unit interval and then later choose one ball at\nrandom to color gray. Using the same Xas before, by symmetry, because any one of\nthen+1 balls is equally likely to be chosen, we have\nP(X=k)=1\nn+1\nfork\u2208{0,1,..., n}. Both situations describe the same problem because it does\nnot matter whether we paint the ball before or after we throw it. Setting the last twoequations equal gives the desired result without using calculus.\n/integraldisplay\n1\n0/parenleftbiggn\nk/parenrightbigg\npk(1\u2212p)n\u2212kdp=1\nn+1\nThe following code shows where to get the Beta distribution from the scipy\nmodule.\n>>> from scipy.stats import beta\n>>> x=beta( 1,1)# create random variable object\n>>> print (x.cdf( 1))\n1.0\nGiven this experiment, it is not too surprising that there is an intimate relationship\nbetween the Beta distribution and binomial random variables. Suppose we want toestimate the probability of heads for coin-tosses using Bayesian inference. Using\nthis approach, all unknown quantities are treated as random variables. In this case,\nthe probability of heads ( p) is the unknown quantity that requires a prior distribu-\ntion. Let us choose the Beta distribution as the prior distribution, Beta(a,b). Then,\nconditioning on p,w eh a v e\nX|p\u223cbinom (n,p)", "105": "2.6 Useful Distributions 93\nwhich says that Xis conditionally distributed as a binomial. To get the posterior\nprobability, f(p|X=k), we have the following Bayes rule,\nf(p|X=k)=P(X=k|p)f(p)\nP(X=k)\nwith the corresponding denominator,\nP(X=k)=/integraldisplay1\n0/parenleftbiggn\nk/parenrightbigg\npk(1\u2212p)n\u2212kf(p)dp\nNote that unlike with our experiment before, f(p)is not constant. Without substitut-\ning in all of the distributions, we observe that the posterior is a function of pwhich\nmeans that everything else that is not a function of pis a constant. This gives,\nf(p|X=k)\u221dpa+k\u22121(1\u2212p)b+n\u2212k\u22121\nwhich is another Beta distribution with parameters a+k,b+n\u2212k. This special\nrelationship in which the beta prior probability distribution on pon data that are\nconditionally binomial distributed yields the posterior that is also binomial distributed\nis known as conjugacy . We say that the Beta distribution is the conjugate prior of the\nbinomial distribution.\n2.6.7 Dirichlet-Multinomial Distribution\nThe Dirichlet-multinomial distribution is a discrete multivariate distribution alsoknown as the multivariate Polya distribution. The Dirichlet-multinomial distributionarises in situations where the usual multinomial distribution is inadequate. For exam-\nple, if a multinomial distribution is used to model the number of balls that land in a\nset of bins and the multinomial parameter vector (i.e., probabilities of balls landingin particular bins) varies from trial to trial, then the Dirichlet distribution can be used\nto include variation in those probabilities because the Dirichlet distribution is de\ufb01ned\nover a simplex that describes the multinomial parameter vector.\nSpeci\ufb01cally, suppose we have Krival events, each with probability \u03bc\nk. Then, the\nprobability of the vector \u03bcgiven that each event has been observed \u03b1ktimes is the\nfollowing,\nP(\u03bc|\u03b1)\u221dK/productdisplay\nk=1\u03bc\u03b1k\u22121\nk\nwhere 0 \u2264\u03bck\u22641 and/summationtext\u03bck=1. Note that this last sum is a constraint that makes\nthe distribution K\u22121 dimensional. The normalizing constant for this distribution is\nthe multinomial Beta function,", "106": "94 2 Probability\nFig. 2.9 One thousand sam-\nples from a Dirichlet distribu-tion with \u03b1=[1,1,1]\nBeta(\u03b1)=/producttextK\nk=1\u0393(\u03b1k)\n\u0393(/summationtextK\nk=1\u03b1k)\nThe elements of the \u03b1vector are also called concentration parameters. As before,\nthe Dirichlet distribution can be found in the scipy.stats module,\n>>> from scipy.stats import dirichlet\n>>> d=dirichlet([ 1,1,1])\n>>> d.rvs( 3)# get samples from distribution\narray([[0.33938968, 0.62186914, 0.03874119],\n[0.21593733, 0.54123298, 0.24282969],[0.37483713, 0.07830673, 0.54685613]])\nNote that each of the rows sums to one. This is because of the/summationtext\u03bck=1 constraint.\nWe can generate more samples and plot this using Axes3D in Matplotlib in Fig. 2.9.\nNotice that the generated samples lie on the triangular simplex shown. The corners\nof the triangle correspond to each of the components in the \u03bc. Using, a non-uniform\n\u03b1=[2,3,4]vector, we can visualize the probability density function using the pdf\nmethod on the dirichlet object as shown in Fig. 2.10 . By choosing the \u03b1\u2208R3,\nthe peak of the density function can be moved within the corresponding triangularsimplex.\nWe have seen that the Beta distribution generalizes the uniform distribution over\nthe unit interval. Likewise, the Dirichlet distribution generalizes the Beta distribution\nover a vector with components in the unit interval. Recall that binomial distribution\nand the Beta distribution form a conjugate pair for Bayesian inference because with\np\u223cBeta,\nX|p\u223cBinomial (n,p)\nThat is, the data conditioned on p, is binomial distributed. Analogously, the multino-\nmial distribution and the Dirichlet distribution also form such a conjugate pair with", "107": "2.6 Useful Distributions 95\nFig. 2.10 Probability density\nfunction for the Dirichletdistribution with \u03b1=[2,3,4]\nmultinomial parameter p\u223cDirichlet,\nX|p\u223cmultinomial (n,p)\nFor this reason, the Dirichlet-multinomial distribution is popular in machine learning\ntext processing because non-zero probabilities can be assigned to words not speci\ufb01-\ncally contained in speci\ufb01c documents, which helps generalization performance.\n2.7 Information Entropy\nWe are in a position to discuss information entropy. This will give us a powerful per-\nspective on how information passes between experiments, and will prove important\nin certain machine learning algorithms.\nThere used to be a TV game show where the host would hide a prize behind one of\nthree doors and the contestant would have to pick one of the doors. However, before\nopening the door of the contestant\u2019s choice, the host would open one of the other\ndoors and ask the contestant if she wanted to change her selection. This is the classicMonty Hall problem. The question is should the contestant stay with her original\nchoice or switch after seeing what the host has revealed? From the information\ntheory perspective, does the information environment change when the host revealswhat is behind one of the doors? The important detail here is that the host never opens\nthe door with the prize behind it, regardless of the contestant\u2019s choice. That is, the\nhost knows where the prize is, but he does not reveal that information directly to the\ncontestant. This is the fundamental problem information theory addresses \u2014 how to\naggregate and reason about partial information. We need a concept of information\nthat can accommodate this kind of question.", "108": "96 2 Probability\n2.7.1 Information Theory Concepts\nThe Shannon information content of an outcome xis de\ufb01ned as,\nh(x)=log21\nP(x)\nwhere P(x)is the probability of x.T h e entropy of the ensemble Xis de\ufb01ned to be\nthe Shannon information content of\nH(X)=/summationdisplay\nxP(x)log21\nP(x)\nIt is no accident that the entropy has this functional form as the expectation of h(x).\nIt leads to a deep and powerful theory of information.\nTo get some intuition about what information entropy means, consider a sequence\nof three-bit numbers where each individual bit is equally likely. Thus, the individualinformation content of a single bit is h(x)=log\n2(2)=1. The units of entropy\narebits so this says that information content of a single bit is one bit. Because the\nthree-bit number has elements that are mutually independent and equally likely, theinformation entropy of the three-bit number is h(X)=2\n3\u00d7log2(23)/8=3. Thus,\nthe basic idea of information content at least makes sense at this level.\nA better way to interpret this question is as how much information would I have\nto provide in order to uniquely encode an arbitrary three-bit number? In this case,\nyou would have to answer three questions: Is the \ufb01rst bit zero or one? Is the second\nbit zero or one? Is the third bit zero or one? Answering these questions uniquely\nspeci\ufb01es the unknown three-bit number. Because the bits are mutually independent,\nknowing the state of any of the bits does not inform the remainder.\nNext, let\u2019s consider a situation that lacks this mutual independence. Suppose in a\ngroup of nine otherwise identical balls there is a heavier one. Furthermore, we also\nhave a measuring scale that indicates whether one side is heavier, lighter, or equal\nto the other. How could we identify the heavier ball? At the outset, the information\ncontent, which measures the uncertainty of the situation is log2(9)because one of the\nnine balls is heavier. Figure 2.11 shows one strategy. We could arbitrarily select out\none of the balls (shown by the square), leaving the remaining eight to be balanced.\nThe thick, black horizontal line indicates the scale. The items below and above this\nline indicate the counterbalanced sides of the scale.\nIf we get lucky, the scale will report that the group of four walls on either side\nof the balance are equal in weight. This means that the ball that was omitted is the\nheavier one. This is indicated by the hashed left-pointing arrow. In this case, all theuncertainty has evaporated, and the informational value of that one weighing is equal\nto log\n2(9). In other words, the scale has reduced the uncertainty to zero (i.e., found\nthe heavy ball). On the other hand, the scale could report that the upper group of fourballs is heavier (black, upward-pointing arrow) or lighter (gray, downward-pointing", "109": "2.7 Information Entropy 97\nFig. 2.11 One heavy ball is hidden among eight identical balls. By weighing groups sequentially,\nwe can determine the heavy ball\nFig. 2.12 For this strategy, the balls are broken up into three groups of equal size and subsequently\nweighed\narrow). In this case, we cannot isolate the heavier ball until we perform all of the\nindicated weighings, moving from left-to-right. Speci\ufb01cally, the four balls on the\nheavier side have to be split by a subsequent weighing into two balls and then to oneball before the heavy ball can be identi\ufb01ed. Thus, this process takes three weighings.\nThe \ufb01rst one has information content log\n2(9/8), the next has log2(4), and the \ufb01nal\none has log2(2). Adding all these up sums to log2(9). Thus, whether or not the heavier\nball is isolated in the \ufb01rst weighing, the strategy consumes log2(9)bits, as it must,\nto \ufb01nd the heavy ball.\nHowever, this is not the only strategy. Figure 2.12 shows another. In this approach,\nthe nine balls are split up into three groups of three balls apiece. Two groups are\nweighed. If they are of equal weight, then this means the heavier ball is in the group", "110": "98 2 Probability\nthat was left out (dashed arrow). Then, this group is split into two groups, with\none element left out. If the two balls on the scale weigh the same, then it means the\nexcluded one is the heavy one. Otherwise, it is one of the balls on the scale. The sameprocess follows if one of the initially weighed groups is heavier (black upward-facing\narrow) or lighter (gray lower-facing arrow). As before the information content of the\nsituation is log\n2(9). The \ufb01rst weighing reduces the uncertainty of the situation by\nlog2(3)and the subsequent weighing reduces it by another log2(3). As before, these\nsum to log2(9), but here we only need two weighings whereas the \ufb01rst strategy in\nFig. 2.11 takes an average of 1 /9+3\u22178/9\u22482.78 weighings, which is more than\ntwo from the second strategy in Fig. 2.12 .\nWhy does the second strategy use fewer weighings? To reduce weighings, we need\neach weighing to adjudicate equally probable situations as many times as possible.Choosing one of the nine balls at the outset (i.e, \ufb01rst strategy in Fig. 2.11 ) does not\ndo this because the probability of selecting the correct ball is 1/9. This does not\ncreate a equiprobable situation in the process. The second strategy leaves an equallyprobable situation at every stage (see Fig. 2.12 ), so it extracts the most information\nout of each weighing as possible. Thus, the information content tells us how many bits\nof information have to be resolved using any strategy (i.e., log\n2(9)in this example).\nIt also illuminates how to ef\ufb01ciently remove uncertainty; namely, by adjudicating\nequiprobable situations as many times as possible.\n2.7.2 Properties of Information Entropy\nNow that we have the \ufb02avor of the concepts, consider the following properties of the\ninformation entropy,\nH(X)\u22650\nwith equality if and only if P(x)=1 for exactly one x. Intuitively, this means that\nwhen just one of the items in the ensemble is known absolutely (i.e., with P(x)=\n1), the uncertainty collapses to zero. Also note that entropy is maximized when\nPis uniformly distributed across the elements of the ensemble. This is illustrated\nin Fig. 2.13 for the case of two outcomes. In other words, information entropy is\nmaximized when the two con\ufb02icting alternatives are equally probable. This is the\nmathematical reason why using the scale in the last example to adjudicate equally\nprobable situations was so useful for abbreviating the weighing process.\nMost importantly, the concept of entropy extends jointly as follows,\nH(X,Y)=/summationdisplay\nx,yP(x,y)log21\nP(x,y)\nIf and only if XandYare independent, entropy becomes additive,\nH(X,Y)=H(X)+H(Y)", "111": "2.7 Information Entropy 99\nFig. 2.13 The information\nentropy is maximized when\np=1/2\n2.7.3 Kullback\u2013Leibler Divergence\nNotions of information entropy lead to notions of distance between probability dis-\ntributions that will become important for machine learning methods. The Kullback\u2013\nLeibler divergence between two probability distributions Pand Qthat are de\ufb01ned\nover the same set is de\ufb01ned as,\nDKL(P,Q)=/summationdisplay\nxP(x)log2P(x)\nQ(x)\nNote that DKL(P,Q)\u22650 with equality if and only if P=Q. Sometimes the\nKullback\u2013Leibler divergence is called the Kullback\u2013Leibler distance, but it is not\nformally a distance metric because it is asymmetrical in Pand Q. The Kullback\u2013\nLeibler divergence de\ufb01nes a relative entropy as the loss of information if Pis modeled\nin terms of Q. There is an intuitive way to interpret the Kullback\u2013Leibler divergence\nand understand its lack of symmetry. Suppose we have a set of messages to transmit,each with a corresponding probability {(x\n1,P(x1)),(x2,P(x2)),...,( xn,P(xn))}.\nBased on what we know about information entropy, it makes sense to encode the\nlength of the message by log21\np(x)bits. This parsimonious strategy means that more\nfrequent messages are encoded with fewer bits. Thus, we can rewrite the entropy of\nthe situation as before,\nH(X)=/summationdisplay\nkP(xk)log21\nP(xk)\nNow, suppose we want to transmit the same set of messages, but with a different set of\nprobability weights, {(x1,Q(x1)),(x2,Q(x2)),...,( xn,Q(xn))}. In this situation,\nwe can de\ufb01ne the cross-entropy as\nHq(X)=/summationdisplay\nkP(xk)log21\nQ(xk)", "112": "100 2 Probability\nNote that only the purported length of the encoded message has changed, not the\nprobability of that message. The difference between these two is the Kullback\u2013Leibler\ndivergence,\nDKL(P,Q)=Hq(X)\u2212H(X)=/summationdisplay\nxP(x)log2P(x)\nQ(x)\nIn this light, the Kullback\u2013Leibler divergence is the average difference in the encoded\nlengths of the same set of messages under two different probability regimes. This\nshould help explain the lack of symmetry of the Kullback\u2013Leibler divergence \u2014 left\nto themselves, Pand Qwould provide the optimal-length encodings separately, but\nthere can be no necessary symmetry in how each regime would rate the informational\nvalue of each message ( Q(xi)versus P(xi)). Given that each encoding is optimal-\nlength in its own regime means that it must therefore be at least sub-optimal in\nanother, thus giving rise to the Kullback\u2013Leibler divergence. In the case where the\nencoding length of all messages remains the same for the two regimes, then theKullback\u2013Leibler divergence is zero.\n2\n2.7.4 Cross-Entropy as Maximum Likelihood\nReconsidering maximum likelihood from our statistics chapter in more general terms,\nwe have\n\u03b8ML=arg max\n\u03b8n/summationdisplay\ni=1logpmodel(xi;\u03b8)\nwhere pmodel is the assumed underlying probability density function parameterized\nby\u03b8for the xidata elements. Dividing the above summation by ndoes not change\nthe derived optimal values, but it allows us to rewrite this using the empirical density\nfunction for xas the following,\n\u03b8ML=arg max\n\u03b8Ex\u223c\u02c6pdata(logpmodel(xi;\u03b8))\nNote that we have the distinction between pdata and\u02c6pdata where the former is the\nunknown distribution of the data and the latter is the estimated distribution of the\ndata we have on hand.\nThe cross-entropy can be written as the following,\nDKL(P,Q)=EX\u223cP(logP(x))\u2212EX\u223cP(logQ(x))\n2The best, easy-to-understand presentation of this material is chapter four of Mackay\u2019s text [ 7].\nAnother good reference is chapter four of [ 8].", "113": "2.7 Information Entropy 101\nwhere X\u223cPmeans the random variable Xhas distribution P.T h u s ,w eh a v e\n\u03b8ML=arg max\n\u03b8DKL(\u02c6pdata,pmodel)\nThat is, we can interpret maximum likelihood as the cross-entropy between the pmodel\nand the \u02c6pdata distributions. The \ufb01rst term has nothing to do with the estimated \u03b8so\nmaximizing this is the same as minimizing the following,\nEx\u223c\u02c6pdata(logpmodel(xi;\u03b8))\nbecause information entropy is always non-negative. The important interpretation is\nthat maximum likelihood is an attempt to choose \u03b8model parameters that make the\nempirical distribution of the data match the model distribution.\n2.8 Moment Generating Functions\nGenerating moments usually involves integrals that are extremely dif\ufb01cult to com-\npute. Moment generating functions make this much, much easier. The moment gen-\nerating function is de\ufb01ned as,\nM(t)=E(exp(tX))\nThe \ufb01rst moment is the mean, which we can easily compute from M(t)as,\ndM(t)\ndt=d\ndtE(exp(tX))=Ed\ndt(exp(tX))\n=E(Xexp(tX))\nNow, we have to set t=0 and we have the mean,\nM(1)(0)=E(X)\ncontinuing this derivative process again, we obtain the second moment as,\nM(2)(t)=E(X2exp(tX))\nM(2)(0)=E(X2)\nWith this in hand, we can easily compute the variance as,\nV(X)=E(X2)\u2212E(X)2=M(2)(0)\u2212M(1)(0)2", "114": "102 2 Probability\nExample . Returning to our favorite binomial distribution, let\u2019s compute some\nmoments using Sympy.\n>>> import sympy asS\n>>> from sympy import stats\n>>> p,t =S.symbols( \u2019p t\u2019 ,positive =True )\n>>> x=stats .Binomial( \u2019x\u2019,10,p)\n>>> mgf =stats .E(S.exp(t *x))\nNow, let\u2019s compute the \ufb01rst moment (aka, mean) using the usual integration method\nand using moment generating functions,\n>>> print (S.simplify(stats .E(x)))\n10*p>>> print (S.simplify(S .diff(mgf,t) .subs(t, 0)))\n10*p\nOtherwise, we can compute this directly as follows,\n>>> print (S.simplify(stats .moment(x, 1))) # mean\n10*p\n>>> print (S.simplify(stats .moment(x, 2))) # 2nd moment\n10*p*(9*p + 1)\nIn general, the moment generating function for the binomial distribution is the fol-\nlowing,\nMX(t)=/parenleftbig\np/parenleftbig\net\u22121/parenrightbig\n+1/parenrightbign\nA key aspect of moment generating functions is that they are unique identi\ufb01ers\nof probability distributions. By the Uniqueness theorem, given two random vari-\nables Xand Y, if their respective moment generating functions are equal, then the\ncorresponding probability distribution functions are equal.\nExample . Let\u2019s use the uniqueness theorem to consider the following problem. Sup-\npose we know that the probability distribution of Xgiven U=pis binomial with\nparameters nand p. For example, suppose Xrepresents the number of heads in n\ncoin \ufb02ips, given the probability of heads is p. We want to \ufb01nd the unconditional\ndistribution of X. Writing out the moment generating function as the following,\nE(etX|U=p)=(pet+1\u2212p)n\nBecause Uis uniform over the unit interval, we can integrate this part out\nE(etX)=/integraldisplay1\n0(pet+1\u2212p)ndp\n=1\nn+1et(n+1)\u22121\net\u22121\n=1\nn+1(1+et+e2t+e3t+\u00b7\u00b7\u00b7+ ent)", "115": "2.8 Moment Generating Functions 103\nThus, the moment generating function of Xcorresponds to that of a random variable\nthat is equally likely to be any of the values 0 ,1,..., n. This is another way of saying\nthat the distribution of Xis discrete uniform over {0,1,..., n}. Concretely, suppose\nwe have a box of coins whose individual probability of heads is unknown and that\nwe dump the box on the \ufb02oor, spilling all of the coins. If we then count the number\nof coins facing heads-up, that distribution is uniform.\nMoment generating functions are useful for deriving distributions of sums of\nindependent random variables. Suppose X1and X2are independent and Y=X1+\nX2. Then, the moment generating function of Yfollows from the properties of the\nexpectation,\nMY(t)=E(etY)=E(etX1+tX2)\n=E(etX1etX2)=E(etX1)E(etX2)\n=MX1(t)MX2(t)\nExample . Suppose we have two normally distributed random variables, X1\u223c\nN(\u03bc1,\u03c31)and X2\u223cN(\u03bc2,\u03c32)with Y=X1+X2. We can save some tedium\nby exploring this in Sympy,\n>>> S.var( \u2019x:2\u2019 ,real =True )\n(x0, x1)>>> S.var( \u2019mu:2\u2019 ,real =True )\n(mu0, mu1)\n>>> S.var( \u2019sigma:2\u2019 ,positive =True )\n(sigma0, sigma1)>>> S.var( \u2019t\u2019,positive =True )\nt>>> x0=stats .Normal(x0,mu0,sigma0)\n>>> x1=stats .Normal(x1,mu1,sigma1)\nProgramming Tip\nTheS.var function de\ufb01nes the variable and injects it into the global names-\npace. This is sheer laziness. It is more expressive to de\ufb01ne variables explicitlyas inx = S.symbols(\u2019x\u2019) . Also notice that we used the Greek names for\nthemuandsigma variables. This will come in handy later when we want to\nrender the equations in the Jupyter notebook which understands how to type-set these symbols in L\nATEX. The var(\u2019x:2\u2019) creates two symbols, x0and\nx1. Using the colon this way makes it easy to generate array-like sequences of\nsymbols.\nIn the next block we compute the moment generating functions\n>>> mgf0 =S.simplify(stats .E(S.exp(t *x0)))\n>>> mgf1 =S.simplify(stats .E(S.exp(t *x1)))\n>>> mgfY =S.simplify(mgf0 *mgf1)", "116": "104 2 Probability\nThe moment generating functions an individual normally distributed random variable\nis the following,\ne\u03bc0t+\u03c32\n0t2\n2\nNote the coef\ufb01cients of t. To show that Yis normally distributed, we want to match\nthe moment generating function of Yto this format. The following is the form of the\nmoment generating function of Y,\nMY(t)=et\n2/parenleftbig\n2\u03bc0+2\u03bc1+\u03c32\n0t+\u03c32\n1t/parenrightbig\nWe can extract the exponent using Sympy and collect on the tvariable using the\nfollowing code,\n>>> S.collect(S .expand(S .log(mgfY)),t)\nt**2*(sigma0**2/2 + sigma1**2/2) + t*(mu0 + mu1)\nThus, by the Uniqueness theorem, Yis normally distributed with \u03bcY=\u03bc0+\u03bc1and\n\u03c32\nY=\u03c32\n0+\u03c32\n1.\nProgramming Tip\nWhen using the Jupyter notebook, you can do S.init_printing to\nget the mathematical typesetting to work in the browser. Otherwise, if you\nwant to keep the raw expression and to selectively render to L ATEX,\nthen you can from IPython.display import Math , and then use\nMath(S.latex(expr)) to see the typeset version of the expression.\n2.9 Monte Carlo Sampling Methods\nSo far, we have studied analytical ways to transform random variables and how to\naugment these methods using Python. In spite of all this, we frequently must resort to\npurely numerical methods to solve real-world problems. Hopefully, now that we haveseen the deeper theory, these numerical methods will feel more concrete. Suppose\nwe want to generate samples of a given density, f(x), given we already can generate\nsamples from a uniform distribution, U[0,1]. How do we know a random sample v\ncomes from the f(x)distribution? One approach is to look at how a histogram of\nsamples of vapproximates f(x). Speci\ufb01cally,\nP(v\u2208N\n\u0394(x))=f(x)\u0394x (2.9.0.1)", "117": "2.9 Monte Carlo Sampling Methods 105\nwhich says that the probability that a sample is in some N\u0394neighborhood of xis\napproximately f(x)\u0394x. Figure 2.14 shows the target probability density function\nf(x)and a histogram that approximates it. The histogram is generated from samples\nv. The hatched rectangle in the center illustrates Eq. 2.9.0.1 . The area of this rectangle\nis approximately f(x)\u0394xwhere x=0, in this case. The width of the rectangle is\nN\u0394(x)The quality of the approximation may be clear visually, but to know that v\nsamples are characterized by f(x), we need the statement of Eq. 2.9.0.1 , which says\nthat the proportion of samples vthat \ufb01ll the hatched rectangle is approximately equal\ntof(x)\u0394x.\nNow that we know how to evaluate samples vthat are characterized by the density\nf(x), let\u2019s consider how to create these samples for both discrete and continuous\nrandom variables.\n2.9.1 Inverse CDF Method for Discrete V ariables\nSuppose we want to generate samples from a fair six-sided die. Our workhouse\nuniform random variable is de\ufb01ned continuously over the unit interval and the fair\nsix-sided die is discrete. We must \ufb01rst create a mapping between the continuousrandom variable uand the discrete outcomes of the die. This mapping is shown in\nFig. 2.15 where the unit interval is broken up into segments, each of length 1/6.\nEach individual segment is assigned to one of the die outcomes. For example, ifu\u2208[1/6,2/6), then the outcome for the die is 2. Because the die is fair, all segments\non the unit interval are the same length. Thus, our new random variable vis derived\nfrom uby this assignment.\nFig. 2.14 The histogram\napproximates the target prob-ability density\nFig. 2.15 A uniform distri-\nbution random variable on theunit interval is assigned tothe six outcomes of a fair dieusing these segments\n", "118": "106 2 Probability\nFor example, for v=2, we have,\nP(v=2)=P(u\u2208[1/6,2/6))=1/6\nwhere, in the language of the Eq. 2.9.0.1 ,f(x)=1 (uniform distribution), \u0394x=1/6,\nand N\u0394(2)=[1/6,2/6). Naturally, this pattern holds for all the other die outcomes\nin{1,2,3,..., 6}. Let\u2019s consider a quick simulation to make this concrete. The\nfollowing code generates uniform random samples and stacks them in a Pandas\ndataframe.\n>>> import pandas aspd\n>>> import numpy asnp\n>>> from pandas import DataFrame\n>>> u=np.random .rand( 100)\n>>> df=DataFrame(data =u,columns =[\u2019u\u2019])\nThe next block uses pd.cut to map the individual samples to the set {1,2,..., 6}\nlabeled v.\n>>> labels =[1,2,3,4,5,6]\n>>> df[\u2019v\u2019]=pd.cut(df .u,np .linspace( 0,1,7),\n... include_lowest =True ,labels =labels)\nThis is what the dataframe contains. The vcolumn contains the samples drawn from\nthe fair die.\n>>> df.head()\nuv\n0 0.356225 31 0.466557 32 0.776817 53 0.836790 64 0.037928 1\nThe following is a count of the number of samples in each group. There should be\nroughly the same number of samples in each group because the die is fair.\n>>> df.groupby( \u2019v\u2019).count()\nu\nv11 721 5\n31 8\n42 051 461 6\nSo far, so good. We now have a way to simulate a fair die from a uniformly distributed\nrandom variable.\nTo extend this to unfair die, we need only make some small adjustments to this\ncode. For example, suppose that we want an unfair die so that P(1)=P(2)=P(3)=\n1/12 and P(4)=P(5)=P(6)=1/4. The only change we have to make is with\npd.cut as follows,", "119": "2.9 Monte Carlo Sampling Methods 107\n>>> df[\u2019v\u2019]=pd.cut(df .u,[0,1/12 ,2/12 ,3/12 ,2/4,3/4,1],\n... include_lowest =True ,labels =labels)\n>>> df.groupby( \u2019v\u2019).count() /df.shape[ 0]\nu\nv1 0.102 0.073 0.054 0.285 0.296 0.21\nwhere now these are the individual probabilities of each digit. Y ou can take more\nthan100 samples to get a clearer view of the individual probabilities but the mech-\nanism for generating them is the same. The method is called the inverse CDF3\nmethod because the CDF (namely, [0,1/12,2/12,3/12,2/4,3/4,1] )i nt h e\nlast example has been inverted (using the pd.cut method) to generate the samples.\nThe inversion is easier to see for continuous variables, which we consider next.\n2.9.2 Inverse CDF Method for Continuous V ariables\nThe method above applies to continuous random variables, but now we have tosqueeze the intervals down to individual points. In the example above, our inversefunction was a piecewise function that operated on uniform random samples. In this\ncase, the piecewise function collapses to a continuous inverse function. We want to\ngenerate random samples for a CDF that is invertible. As before, the criterion forgenerating an appropriate sample vis the following,\nP(F(x)<v< F(x+\u0394x))=F(x+\u0394x)\u2212F(x)=/integraldisplay\nx+\u0394x\nxf(u)du\u2248f(x)\u0394x\nwhich says that the probability that the sample vis contained in a \u0394xinterval is\napproximately equal to f(x)\u0394x, at that point. Once again, the trick is to use a\nuniform random sample uand an invertible CDF F(x)to construct these samples.\nNote that for a uniform random variable u\u223cU[0,1],w eh a v e ,\nP(x<F\u22121(u)< x+\u0394x)=P(F(x)<u<F(x+\u0394x))\n=F(x+\u0394x)\u2212F(x)\n=/integraldisplayx+\u0394x\nxf(p)dp\u2248f(x)\u0394x\nThis means that v=F\u22121(u)is distributed according to f(x), which is what we\nwant.\n3Cumulative density function. Namely, F(x)=P(X<x).", "120": "108 2 Probability\nLet\u2019s try this to generate samples from the exponential distribution,\nf\u03b1(x)=\u03b1e\u2212\u03b1x\nwhich has the following CDF,\nF(x)=1\u2212e\u2212\u03b1x\nand corresponding inverse,\nF\u22121(u)=1\n\u03b1ln1\n(1\u2212u)\nNow, all we have to do is generate some uniformly distributed random samples and\nthen feed them into F\u22121.\n>>> from numpy import array, log\n>>> import scipy.stats\n>>> alpha =1 . # distribution parameter\n>>> nsamp = 1000 # num of samples\n>>> # define uniform random variable\n>>> u=scipy .stats .uniform( 0,1)\n>>> # define inverse function\n>>> Finv =lambda u:1/alpha *log( 1/(1-u))\n>>> # apply inverse function to samples\n>>> v=array( list (map(Finv,u .rvs(nsamp))))\nNow, we have the samples from the exponential distribution, but how do we\nknow the method is correct with samples distributed accordingly? Fortunately,scipy.stats already has a exponential distribution, so we can check our work\nagainst the reference using a probability plot (i.e., also known as a quantile-quantile\nplot). The following code sets up the probability plot from scipy.stats .\nfig,ax =subplots()\nscipy .stats .probplot(v,( 1,),dist =\u2019expon\u2019 ,plot =ax)\nNote that we have to supply an axes object ( ax) for it to draw on. The result is\nFig. 2.16 . The more the samples line match the diagonal line, the more they match\nthe reference distribution (i.e., exponential distribution in this case). Y ou may alsowant to try dist=norm in the code above To see what happens when the normal\ndistribution is the reference distribution.\n2.9.3 Rejection Method\nIn some cases, inverting the CDF may be impossible. The rejection method can handle\nthis situation. The idea is to pick two uniform random variables u1,u2\u223cU[a,b]so\nthat", "121": "2.9 Monte Carlo Sampling Methods 109\nFig. 2.16 The samples created using the inverse cdf method match the exponential reference\ndistribution\nP/parenleftbigg\nu1\u2208N\u0394(x)/logicalanddisplay\nu2<f(u1)\nM/parenrightbigg\n\u2248\u0394x\nb\u2212af(u1)\nM\nwhere we take x=u1and f(x)< M. This is a two-step process. First, draw u1\nuniformly from the interval [a,b]. Second, feed it into f(x)and if u2<f(u1)/M,\nthen you have a valid sample for f(x). Thus, u1is the proposed sample from fthat\nmay or may not be rejected depending on u2. The only job of the Mconstant is to\nscale down the f(x)so that the u2variable can span the range. The ef\ufb01ciency of\nthis method is the probability of accepting u1which comes from integrating out the\nabove approximation,\n/integraldisplayf(x)\nM(b\u2212a)dx=1\nM(b\u2212a)/integraldisplay\nf(x)dx=1\nM(b\u2212a)\nThis means that we don\u2019t want an necessarily large Mbecause that makes it more\nlikely that samples will be discarded.\nLet\u2019s try this method for a density that does not have a continuous inverse.4\nf(x)=exp/parenleftbigg\n\u2212(x\u22121)2\n2x/parenrightbigg\n(x+1)/12\nwhere x>0. The following code implements the rejection plan.\n4Note that this example density does not exactly integrate out to one like a probability density\nfunction should, but the normalization constant for this is distracting for our purposes here.", "122": "110 2 Probability\nFig. 2.17 The rejection method generate samples in the histogram that nicely match the target\ndistribution. Unfortunately, the ef\ufb01ciency is not so good\n>>> import numpy asnp\n>>> x=np.linspace( 0.001 ,15,100)\n>>> f=lambda x: np .exp( -(x-1)**2/2./ x)*(x+1)/12.\n>>> fx=f(x)\n>>> M=0.3 # scale factor\n>>> u1=np.random .rand( 10000 )*15 # uniform random samples scaled out\n>>> u2=np.random .rand( 10000 ) # uniform random samples\n>>> idx, =np.where(u2 <=f(u1) /M) # rejection criterion\n>>> v=u1[idx]\nFigure 2.17 shows a histogram of the so-generated samples that nicely \ufb01ts the prob-\nability density function. The title in the \ufb01gure shows the ef\ufb01ciency (the number ofrejected samples), which is poor. It means that we threw away most of the proposed\nsamples. Thus, even though there is nothing conceptually wrong with this result,\nthe low ef\ufb01ciency must be \ufb01xed, as a practical matter. Figure 2.18 shows where\nthe proposed samples were rejected. Samples under the curve were retained (i.e.,\nu\n2<f(u1)\nM) but the vast majority of the samples are outside this umbrella.\nThe rejection method uses u1to select along the domain of f(x)and the other\nu2uniform random variable decides whether to accept or not. One idea would be to\nchoose u1so that xvalues are coincidentally those that are near the peak of f(x),\ninstead of uniformly anywhere in the domain, especially near the tails, which are low\nprobability anyway. Now, the trick is to \ufb01nd a new density function g(x)to sample\nfrom that has a similiar concentration of probability density. One way it to familiarize\noneself with the probability density functions that have adjustable parameters and\nfast random sample generators already. There are lots of places to look and, chancesare, there is likely already such a generator for your problem. Otherwise, the family\nof\u03b2densities is a good place to start.\nTo be explicit, what we want is u\n1\u223cg(x)so that, returning to our earlier argument,\nP/parenleftbigg\nu1\u2208N\u0394(x)/logicalanddisplay\nu2<f(u1)\nM/parenrightbigg\n\u2248g(x)\u0394xf(u1)\nM", "123": "2.9 Monte Carlo Sampling Methods 111\nFig. 2.18 The proposed samples under the curve were accepted and the others were not. This shows\nthe majority of samples were rejected\nbut this is notwhat we need here. The problem is with the second part of the logical/logicalandtext\nconjunction. We need to put something there that will give us something proportional\ntof(x). Let us de\ufb01ne the following,\nh(x)=f(x)\ng(x)(2.9.3.1)\nwith corresponding maximum on the domain as hmaxand then go back and construct\nthe second part of the clause as\nP/parenleftbigg\nu1\u2208N\u0394(x)/logicalanddisplay\nu2<h(u1)\nhmax/parenrightbigg\n\u2248g(x)\u0394xh(u1)\nhmax=f(x)/hmax\nRecall that satisfying this criterion means that u1=x. As before, we can estimate\nthe probability of acceptance of the u1as 1/hmax.\nNow, how to construct the g(x)function in the denominator of Eq. 2.9.3.1 ? Here\u2019s\nwhere familiarity with some standard probability densities pays off. For this case, wechoose the \u03c7\n2distribution. The following plots the g(x)and f(x)(left plot) and the\ncorresponding h(x)=f(x)/g(x)(right plot). Note that g(x)and f(x)have peaks\nthat almost coincide, which is what we are looking for (Fig. 2.19 ).\n>>> ch=scipy .stats .chi2( 4)# chi-squared\n>>> h=lambda x: f(x) /ch.pdf(x) # h-function\nNow, let\u2019s generate some samples from this \u03c72distribution with the rejection method.\n>>> hmax =h(x) .max()\n>>> u1=ch.rvs( 5000 ) # samples from chi-square distribution\n>>> u2=np.random .rand( 5000 )# uniform random samples\n>>> idx =(u2 <=h(u1) /hmax) # rejection criterion\n>>> v=u1[idx] # keep these only", "124": "112 2 Probability\nFig. 2.19 The plot on the right shows h(x)=f(x)/g(x)and the one on the left shows f(x)and\ng(x)separately\nFig. 2.20 Using the updated method, the histogram matches the target probability density function\nwith high ef\ufb01ciency\nFig. 2.21 Fewer proposed points were rejected in this case, which means better ef\ufb01ciency\nUsing the \u03c72distribution with the rejection method results in throwing away\nless than 10% of the generated samples compared with our prior example where wethrew out at least 80%. This is dramatically more ef\ufb01cient! Figure 2.20 shows that the\nhistogram and the probability density function match. For completeness, Fig. 2.21\nshows the samples with the corresponding threshold h(x)/h\nmax that was used to\nselect them.", "125": "2.10 Sampling Importance Resampling 113\n2.10 Sampling Importance Resampling\nAn alternative to the Rejection Method that does not involve rejecting samples or\ncoming up with Mbounds or bounding functions is the Sampling Importance Resam-\npling (SIR) method. Choose a tractable gprobability density function and draw a n\nsamples from it, {xi}n\ni=1. Our objective is to derive samples f. Next, compute the\nfollowing,\nqi=wi/summationtextwi\nwhere\nwi=f(xi)\ng(xi)\nThe qide\ufb01ne a probability mass function whose samples approximate samples from\nf. To see this, consider,\nP(X\u2264a)=n/summationdisplay\ni=1qiI(\u2212\u221e,a](xi)\n=/summationtextn\ni=1wiI(\u2212\u221e,a](xi)/summationtextn\ni=1wi\n=1\nn/summationtextn\ni=1f(xi)\ng(xi)I(\u2212\u221e,a](xi)\n1\nn/summationtextn\ni=1f(xi)\ng(xi)\nBecause the samples are generated from the gprobability distribution, the numerator\nis approximately,\nEg/parenleftbiggf(x)\ng(x)/parenrightbigg\n=/integraldisplaya\n\u2212\u221ef(x)dx\nwhich gives\nP(X\u2264a)=/integraldisplaya\n\u2212\u221ef(x)dx\nwhich shows that the samples generated this way are f-distributed. Note more sam-\nples have to be generated from this probability mass function the further away gis\nfrom the desired function f. Further, because there is no rejection step, we no longer\nhave the issue of ef\ufb01ciency.\nFor example, let us choose a beta distribution for g, as in the following code,\n>>> g=scipy .stats .beta( 2,3)\nThis distribution does not bear a strong resemblance to our desired ffunction from\nlast section. as shown in the Fig. 2.22 . Note that we scaled the domain of the beta\ndistribution to get it close to the support of f.", "126": "114 2 Probability\nFig. 2.22 Histogram of samples generated using SIR comparted to target probability density func-\ntion\nFig. 2.23 Histogram and probability density function using SIR\nIn the next block, we sample from the gdistribution and compute the weights as\ndescribed above. The \ufb01nal step is to sample from this new probability mass function.\nThe resulting normalized histogram is shown compared to the target fprobability\ndensity function in Fig. 2.23 .\n>>> xi=g.rvs( 500)\n>>> w=np.array([f(i *15)/g.pdf(i) for iinxi])\n>>> fsamples =np.random .choice(xi *15,5000 ,p=w/w.sum())\nIn this section, we investigated how to generate random samples from a given\ndistribution, beit discrete or continuous. For the continuous case, the key issue was\nwhether or not the cumulative density function had a continuous inverse. If not,\nwe had to turn to the rejection method, and \ufb01nd an appropriate related density thatwe could easily sample from to use as part of a rejection threshold. Finding such a", "127": "2.10 Sampling Importance Resampling 115\nfunction is an art, but many families of probability densities have been studied over\nthe years that already have fast random number generators.\nThe rejection method has many complicated extensions that involve careful par-\ntitioning of the domains and lots of special methods for corner cases. Nonetheless,\nall of these advanced techniques are still variations on the same fundamental theme\nwe illustrated here [ 9,10].\n2.11 Useful Inequalities\nIn practice, few quantities can be analytically calculated. Some knowledge of bound-\ning inequalities helps \ufb01nd the ballpark for potential solutions. This sections discusses\nthree key inequalities that are important for probability, statistics, and machine learn-\ning.\n2.11.1 Markov\u2019s Inequality\nLetXbe a non-negative random variable and suppose that E(X)<\u221e. Then, for\nanyt>0,\nP(X>t)\u2264E(X)\nt\nThis is a foundational inequality that is used as a stepping stone to other inequalities.\nIt is easy to prove. Because X>0, we have the following,\nE(X)=/integraldisplay\u221e\n0xfx(x)dx=/integraldisplayt\n0xfx(x)dx\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nomit this+/integraldisplay\u221e\ntxfx(x)dx\n\u2265/integraldisplay\u221e\ntxfx(x)dx\u2265t/integraldisplay\u221e\ntfx(x)dx=tP(X>t)\nThe step that establishes the inequality is the part where the/integraltextt\n0xfx(x)dxis omitted.\nFor a particular fx(x)that may be concentrated around the [0,t]interval, this could\nbe a lot to throw out. For that reason, the Markov Inequality is considered a loose\ninequality, meaning that there is a substantial gap between both sides of the inequality.\nFor example, as shown in Fig. 2.24 ,t h e\u03c72distribution has a lot of its mass on the left,\nwhich would be omitted in the Markov Inequality. Figure 2.25 shows the two curves\nestablished by the Markov Inequality. The gray shaded region is the gap between the\ntwo terms and indicates that looseness of the bound (fatter shaded region) for thiscase.", "128": "116 2 Probability\nFig. 2.24 The\u03c72\n1density has\nmuch of its weight on theleft, which is excluded in theestablishment of the MarkovInequality\nFig. 2.25 The shaded area\nshows the region between thecurves on either side of theMarkov Inequality\n2.11.2 Chebyshev\u2019s Inequality\nChebyshev\u2019s Inequality drops out directly from the Markov Inequality. Let \u03bc=E(X)\nand\u03c32=V(X). Then, we have\nP(|X\u2212\u03bc|\u2265t)\u2264\u03c32\nt2\nNote that if we normalize so that Z=(X\u2212\u03bc)/\u03c3,w eh a v e P(|Z|\u2265k)\u22641/k2.I n\nparticular, P(|Z|\u2265 2)\u22641/4. We can illustrate this inequality using Sympy statistics\nmodule,", "129": "2.11 Useful Inequalities 117\n>>> import sympy\n>>> import sympy.stats asss\n>>> t=sympy .symbols( \u2019t\u2019,real =True )\n>>> x=ss.ChiSquared( \u2019x\u2019,1)\nTo get the left side of the Chebyshev inequality, we have to write this out as the\nfollowing conditional probability,\n>>> r=ss.P((x -1)>t,x>1)+ss.P(-(x-1)>t,x<1)\nWe could take the above expression, which is a function of tand attempt to compute\nthe integral, but that would take a very long time (the expression is very long andcomplicated, which is why we did not print it out above). In this situation, it\u2019s\nbetter to use the built-in cumulative density function as in the following (after some\nrearrangement of the terms),\n>>> w=(1-ss.cdf(x)(t +1))+ss.cdf(x)( 1-t)\nTo plot this, we can evaluated at a variety of tvalues by using the .subs substitution\nmethod, but it is more convenient to use the lambdify method to convert the\nexpression to a function.\n>>> fw=sympy .lambdify(t,w)\nThen, we can evaluate this function using something like\n>>> [fw(i) for iin[0,1,2,3,4,5]]\n[1.0,0.157299207050285,(0.08326451666355039+0j),(0.04550026389635842+0j),(0.0253473186774682+0j),(0.014305878435429631+0j)]\nto produce the following Fig. 2.26 .\nFig. 2.26 The shaded area\nshows the region between the\ncurves on either side of the\nChebyshev Inequality\n", "130": "118 2 Probability\nProgramming Tip\nNote that we cannot use vectorized inputs for the lambdify function because\nit contains embedded functions that are only available in Sympy. Otherwise,we could have used lambdify(t,fw,numpy) to specify the corresponding\nfunctions in Numpy to use for the expression.\n2.11.3 Hoeffding\u2019s Inequality\nHoeffding\u2019s Inequality is similar, but less loose, than Markov\u2019s Inequality. Let\nX1,..., Xnbe iid observations such that E(Xi)=\u03bcand a\u2264Xi\u2264b. Then,\nfor any /epsilon1>0, we have\nP(|Xn\u2212\u03bc|\u2265/epsilon1)\u22642e x p(\u22122n/epsilon12/(b\u2212a)2)\nwhere Xn=1\nn/summationtextn\niXi. Note that we further assume that the individual random\nvariables are bounded.\nCorollary .I fX1,..., Xnare independent with P(a\u2264Xi\u2264b)=1 and all with\nE(Xi)=\u03bc. Then, we have\n|Xn\u2212\u03bc|\u2264/radicalbigg\nc\n2nlog2\n\u03b4\nwhere c=(b\u2212a)2. We will see this inequality again in the machine learning chapter.\nFigure 2.27 shows the Markov and Hoeffding bounds for the case of ten identically\nand uniformly distributed random variables, Xi\u223cU[0,1]. The solid line shows\nP(|Xn\u22121/2|>/epsilon1). Note that the Hoeffding Inequality is tighter than the Markov\nInequality and that both of them merge when /epsilon1gets big enough.\nProof of Hoeffding\u2019s Inequality . We will need the following lemma to prove Hoeffd-\ning\u2019s inequality.\nLemma .L e t Xbe a random variable with E(X)=0 and a\u2264X\u2264b. Then, for any\ns>0, we have the following,\nE(esX)\u2264es2(b\u2212a)2/8(2.11.3.1)\nBecause Xis contained in the closed interval [a,b], we can write it as a convex\ncombination of the endpoints of the interval.\nX=\u03b11a+\u03b12b", "131": "2.11 Useful Inequalities 119\nFig. 2.27 This shows the\nMarkov and Hoeffding boundsfor the case of ten identicallyand uniformly distributedrandom variables\nwhere \u03b11+\u03b12=1. Solving for the \u03b1iterms, we have\n\u03b11=x\u2212a\nb\u2212a\n\u03b12=b\u2212x\nb\u2212a\nFrom Jensen\u2019s inequality, for a convex functions f, we know that\nf/parenleftBig/summationdisplay\n\u03b1ixi/parenrightBig\n\u2264/summationdisplay\n\u03b1if(xi)\nGiven the convexity of eX, we therefore have,\nesX\u2264\u03b11esa+\u03b12esb\nWithE(X)=0, we can write the expectation of both sides\nE(esX)\u2264E(\u03b11)esa+E(\u03b12)esb\nwithE(\u03b11)=b\nb\u2212aandE(\u03b12)=\u2212a\nb\u2212a.T h u s ,w eh a v e\nE(esX)\u2264b\nb\u2212aesa\u2212a\nb\u2212aesb\nUsing p:=\u2212a\nb\u2212a, we can rewrite the following,\nb\nb\u2212aesa\u2212a\nb\u2212aesb=(1\u2212p)esa+pesb=:e\u03c6(u)", "132": "120 2 Probability\nwhere\n\u03c6(u)=\u2212 pu+log(1\u2212p+peu)\nand u=s(b\u2212a). Note that \u03c6(0)=\u03c6/prime(0)=0. Also, \u03c6/prime/prime(0)=p(1\u2212p)\u22641/4.\nThus, the Taylor expansion of \u03c6(u)\u2248u2\n2\u03c6/prime/prime(t)\u2264u2\n8fort\u2208[0,u]. /squaresolid\nTo prove Hoeffding\u2019s inequality, we start with Markov\u2019s inequality,\nP(X\u2265/epsilon1)\u2264E(X)\n/epsilon1\nThen, given s>0, we have the following,\nP(X\u2265/epsilon1)=P(esX\u2265es/epsilon1)\u2264E(esX)\nes/epsilon1\nWe can write the one-sided Hoeffding inequality as the following,\nP(Xn\u2212\u03bc\u2265/epsilon1)\u2264e\u2212s/epsilon1E(exp(s\nnn/summationdisplay\ni=1(Xi\u2212E(Xi))))\n=e\u2212s/epsilon1n/productdisplay\ni=1E(es\nn(Xi\u2212E(Xi)))\n\u2264e\u2212s/epsilon1n/productdisplay\ni=1es2\nn2(b\u2212a)2/8\n=e\u2212s/epsilon1es2\nn(b\u2212a)2/8\nNow, we want to pick s>0 to minimize this upper bound. Then, with s=4n/epsilon1\n(b\u2212a)2\nP(Xn\u2212\u03bc\u2265/epsilon1)\u2264e\u22122n/epsilon12\n(b\u2212a)2\nThe other side of the inequality follows similarly to obtain Hoeffding\u2019s\ninequality. /squaresolid\nReferences\n1. F. Jones, Lebesgue Integration on Euclidean Space . Jones and Bartlett Books in Mathematics.\n(Jones and Bartlett, London, 2001)\n2. G. Strang, Linear Algebra and Its Applications (Thomson, Brooks/Cole, 2006)\n3. N. Edward, Radically Elementary Probability Theory . Annals of Mathematics Studies (Prince-\nton University Press, Princeton, 1987)", "133": "References 121\n4. T. Mikosch, Elementary Stochastic Calculus with Finance in View . Advanced Series on Statis-\ntical Science & Applied Probability (World Scienti\ufb01c, Singapore, 1998)\n5. H. Kobayashi, B.L. Mark, W. Turin, Probability, Random Processes, and Statistical Analy-\nsis: Applications to Communications, Signal Processing, Queueing Theory and MathematicalFinance . EngineeringPro Collection (Cambridge University Press, Cambridge, 2011)\n6. Z. Brzezniak, T. Zastawniak, Basic Stochastic Processes: A Course Through Exercises . Springer\nUndergraduate Mathematics Series (Springer, London, 1999)\n7. D.J.C. MacKay, Information Theory, Inference and Learning Algorithms (Cambridge Univer-\nsity Press, Cambridge, 2003)\n8. T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning: Data Mining,\nInference, and Prediction . Springer Series in Statistics (Springer, New Y ork, 2013)\n9. W.L. Dunn, J.K. Shultis, Exploring Monte Carlo Methods (Elsevier Science, Boston, 2011)\n10. N.L. Johnson, S. Kotz, N. Balakrishnan, Continuous Univariate Distributions .W i l e yS e r i e s\nin Probability and Mathematical Statistics: Applied Probability and Statistics, vol. 2. (Wiley,New Y ork, 1995)", "134": "Chapter 3\nStatistics\n3.1 Introduction\nTo get started thinking about statistics, consider the three famous problems\n\u2022Suppose you have a bag \ufb01lled with colored marbles. Y ou close your eyes and reach\ninto it and pull out a handful of marbles, what can you say about what is in the\nbag?\n\u2022Y ou arrive in a strange town and you need a taxicab. Y ou look out the window,\nand in the dark, you can just barely make out the number on the roof of one of\nthe cabs. In this town, you know they label the cabs sequentially. How many cabs\ndoes the town have?\n\u2022Y ou have already taken the entrance exam twice and you want to know if it\u2019s worth\nit to take it a third time in the hopes that your score will improve. Because only\nthe last score is reported, you are worried that you may do worse the third time.How do you decide whether or not to take the test again?\nStatistics provides a structured way to approach each of these problems. This is\nimportant because it is easy to be fooled by your biases and intuitions. Unfortunately,\nthe \ufb01eld does not provide a single way to do this, which explains the many library\nshelves that groan under the weight of statistics texts. This means that although manystatistical quantities are easy to compute , these are not so easy to justify, explain,\nor even understand. Fundamentally, when we start with just the data, we lack the\nunderlying probability density that we discussed in the last chapter. This removeskey structures that we have to compensate for in; however, we choose to process the\ndata. In the following, we consider some of the most powerful statistical tools in the\nPython arsenal and suggest ways to think through them.\n\u00a9 Springer Nature Switzerland AG 2019\nJ. Unpingco, Python for Probability, Statistics, and Machine Learning ,\nhttps://doi.org/10.1007/978-3-030-18545-9_3123", "135": "124 3 Statistics\n3.2 Python Modules for Statistics\n3.2.1 Scipy Statistics Module\nAlthough there are some basic statistical functions in Numpy (e.g., mean ,std,\nmedian ), the real repository for statistical functions is in scipy.stats . There are\nover eighty continuous probability distributions implemented in scipy.stats and\nan additional set of more than ten discrete distributions, along with many other sup-\nplementary statistical functions.\nTo get started with scipy.stats , you have to load the module and create an\nobject that has the distribution you\u2019re interested in. For example,\n>>> import scipy.stats # might take awhile\n>>>n=scipy .stats .norm( 0,10)# create normal distrib\nThenvariable is an object that represents a normally distributed random variable\nwith mean zero and standard deviation, \u03c3=10. Note that the more general term\nfor these two parameters is location andscale , respectively. Now that we have this\nde\ufb01ned, we can compute mean , as in the following:\n>>>n.mean() # we already know this from its definition!\n0.0\nWe can also compute higher order moments as>>>n.moment( 4)\n30000.0\nThe main public methods for continuous random variables are\n\u2022rvs: random variates\n\u2022pdf: probability density function\n\u2022cdf: cumulative distribution function\n\u2022sf: survival Function (1-CDF)\n\u2022ppf: percent point function (Inverse of CDF)\n\u2022isf: inverse survival function (Inverse of SF)\n\u2022stats : mean, variance, (Fisher\u2019s) skew, or (Fisher\u2019s) kurtosis\n\u2022moment : non-central moments of the distribution\nFor example, we can compute the value of the pdf at a speci\ufb01c point.\n>>>n.pdf(0)\n0.03989422804014327\nor, the cdf for the same random variable.\n>>>n\n.cdf(0)\n0.5", "136": "3.2 Python Modules for Statistics 125\nY ou can also create samples from this distribution as in the following:\n>>>n.rvs( 10)\narray([15.3244518 , -9.4087413 , 6.94760096, 0.61627683, -3.92073633,\n6.9753351 , 7.95314387, -3.18127815, 5.69087949, 0.84197674])\nMany common statistical tests are already built-in. For example, Shapiro\u2013Wilks tests\nthe null hypothesis that the data were drawn from a normal distribution,1as in the\nfollowing:\n>>>scipy .stats .shapiro(n .rvs(100))\n(0.9749656915664673, 0.05362436920404434)\nThe second value in the tuple is the p-value (discussed below).\n3.2.2 Sympy Statistics Module\nSympy has its own much smaller, but still extremely useful statistics module that\nenables symbolic manipulation of statistical quantities. For example,\n>>> from sympy import stats, sqrt, exp, pi\n>>>X=stats .Normal( 'x',0,10)# create normal random variable\nWe can obtain the probability density function as\n>>> from sympy.abc import x\n>>>stats .density(X)(x)\nsqrt(2)*exp(-x**2/200)/(20*sqrt(pi))\n>>>sqrt( 2)*exp(-x**2/200 )/(20*sqrt(pi))\nsqrt(2)*exp(-x**2/200)/(20*sqrt(pi))\nand we can evaluate the cumulative density function as in the following:>>>stats .cdf(X)( 0)\n1/2\nNote that you can evaluate this numerically by using the evalf() method on the\noutput. Sympy provides intuitive ways to consider standard probability questions by\nusing the stats.P function, as in the following:\n>>>stats .P(X>0)# prob X >0?\n1/2\nThere is also a corresponding expectation function, stats.E you can use to com-\npute complicated expectations using all of Sympy\u2019s powerful built-in integrationmachinery. For example we can compute, E(\u221a\n|X|)in the following:\n>>>stats .E(abs(X)**(1/2)).evalf()\n2.59995815363879\nUnfortunately, there is very limited support for multivariate distributions at the time\nof this writing.\n1We will explain null hypothesis and the rest of it later.", "137": "126 3 Statistics\n3.2.3 Other Python Modules for Statistics\nThere are many other important Python modules for statistical work. Two important\nmodules are Seaborn and Statsmodels. As we discussed earlier, Seaborn is librarybuilt on top of Matplotlib for very detailed and expressive statistical visualizations,\nideally suited for exploratory data analysis. Statsmodels is designed to complement\nScipy with descriptive statistics, estimation, and inference for a large variety ofstatistical models. Statsmodels includes (among many others) generalized linear\nmodels, robust linear models, and methods for time-series analysis, with an emphasis\non econometric data and problems. Both these modules are well supported and verywell documented and designed to integrate tightly into Matplotlib, Numpy, Scipy,\nand the rest of the scienti\ufb01c Python stack. Because the focus of this text is more\nconceptual as opposed to domain speci\ufb01c, I have chosen not to emphasize either of\nthese, notwithstanding how powerful each is.\n3.3 Types of Convergence\nThe absence of the probability density for the raw data means that we have to argue\nabout sequences of random variables in a structured way. From basic calculus, recallthe following convergence notation:\nx\nn\u2192xo\nfor the real number sequence xn. This means that for any given /epsilon1>0, no matter how\nsmall, we can exhibit a msuch that for any n>m,w eh a v e\n|xn\u2212xo|</epsilon1\nIntuitively, this means that once we get past min the sequence, we get as to within\n/epsilon1ofxo. This means that nothing surprising happens in the sequence on the long\nmarch to in\ufb01nity, which gives a sense of uniformity to the convergence process.\nWhen we argue about convergence for statistics, we want to same look-and-feel as\nwe have here, but because we are now talking about random variables, we needother concepts. There are two moving parts for random variables. Recall from our\nprobability chapter that random variables are really functions that map sets into the\nreal line: X:\u03a9/mapsto\u2192 R. Thus, one part is the behavior of the subsets of \u03a9in terms\nof convergence. The other part is how the sequences of real values of the random\nvariable behave in convergence.\n3.3.1 Almost Sure Convergence\nThe most straightforward extension into statistics of this convergence concept is\nalmost sure convergence , which is also known as convergence with probability one ,", "138": "3.3 Types of Convergence 127\nP{for each /epsilon1>0 there is n/epsilon1>0 such that for all n>n/epsilon1,|Xn\u2212X|</epsilon1}= 1\n(3.3.1.1)\nNote the similarity to the prior notion of convergence for real numbers. When this\nhappens, we write this as Xnas\u2192X. In this context, almost sure convergence means\nthat if we take any particular \u03c9\u2208\u03a9and then look at the sequence of real numbers\nthat are produced by each of the random variables,\n(X1(\u03c9),X2(\u03c9),X3(\u03c9),... , Xn(\u03c9))\nthen this sequence is just a real-valued sequence in the sense of our convergence\non the real line and converges in the same way. If we collect all of the \u03c9for which\nthis is true and the measure of that collection equals one, then we have almost sure\nconvergence of the random variable. Notice how the convergence idea applies to both\nsides of the random variable: the (domain) \u03a9side and the (co-domain) real-valued\nside.\nAn equivalent and more compact way of writing this is the following:\nP/parenleftBig\n\u03c9\u2208\u03a9:limn\u2192\u221eXn(\u03c9)=X(\u03c9)/parenrightBig\n=1\nExample. To get some feel for the mechanics of this kind of convergence consider the\nfollowing sequence of uniformly distributed random variables on the unit interval,\nXn\u223cU[0,1]. Now, consider taking the maximum of the set of nsuch variables as\nthe following:\nX(n)=max{X1,..., Xn}\nIn other words, we scan through a list of nuniformly distributed random variables\nand pick out the maximum over the set. Intuitively, we should expect that X(n)should\nsomehow converge to one. Let\u2019s see if we can make this happen almost surely. Wewant to exhibit mso that the following is true,\nP(|1\u2212X\n(n)|)</epsilon1when n>m\nBecause X(n)<1, we can simplify this as the following:\n1\u2212P(X(n)</epsilon1)=1\u2212(1\u2212/epsilon1)m\u2212\u2192m\u2192\u221e1\nThus, this sequence converges almost surely. We can work this example out in Python\nusing Scipy to make it concrete with the following code:\n>>> from scipy import stats\n>>>u=stats .uniform()\n>>>xn=lambda i: u .rvs(i) .max()\n>>>xn(5)\n0.966717838482003", "139": "128 3 Statistics\nThus, the xnvariable is the same as the X(n)random variable in our example.\nFigure 3.1shows a plot of these random variables for different values of nand mul-\ntiple realizations of each random variable (multiple gray lines). The dark horizontalline is at the 0.95 level. For this example, suppose we are interested in the conver-\ngence of the random variable to within 0.05 of one so we are interested in the region\nbetween one and 0.95 . Thus, in our Eq. 3.3.1.1 ,/epsilon1=0.05. Now, we have to \ufb01nd n\n/epsilon1\nto get the almost sure convergence. From Fig. 3.1, as soon as we get past n>60, we\ncan see that all the realizations start to \ufb01t in the region above the 0.95 horizontal line.\nHowever, there are still some cases where a particular realization will skip belowthis line. To get the probability guarantee of the de\ufb01nition satis\ufb01ed, we have to make\nsure that for whatever n\n/epsilon1we settle on, the probability of this kind of noncompliant\nbehavior should be extremely small, say, less than 1%. Now, we can compute thefollowing to estimate this probability for n=60 over 1000 realizations:\n>>> import numpy as np\n>>>np.mean([xn( 60)> 0.95 foriinrange (1000)])\n0.961\nSo, the probability of having a noncompliant case beyond n>60 is pretty good, but\nnot still what we are after ( 0.99 ). We can solve for the min our analytic proof of\nconvergence by plugging in our factors for /epsilon1and our desired probability constraint,\n>>> print (np.log(1-.99 )/np.log(.95))\n89.78113496070968\nNow, rounding this up and re-visiting the same estimate as above,>>> import numpy as np\n>>>np.mean([xn( 90)> 0.95 foriinrange (1000)])\n0.995\nwhich is the result we were looking for. The important thing to understand from\nthis example is that we had to choose convergence criteria for both the values of\nFig. 3.1 Almost sure conver-\ngence example for multiplerealizations of the limitingsequence\n", "140": "3.3 Types of Convergence 129\nthe random variable ( 0.95 ) and for the probability of achieving that level ( 0.99 )i n\norder to compute the m. Informally speaking, almost sure convergence means that\nnot only will any particular Xnbe close to Xfor large n, but whole sequence of\nvalues will remain close to Xwith high probability.\n3.3.2 Convergence in Probability\nA weaker kind of convergence is convergence in probability which means the fol-\nlowing:\nP(|Xn\u2212X|>/epsilon1)\u21920\nasn\u2192\u221e for each /epsilon1>0.\nThis is notationally shown as XnP\u2192X. For example, let\u2019s consider the following\nsequence of random variables where Xn=1/2nwith probability pnand where\nXn=cwith probability 1 \u2212pn. Then, we have XnP\u21920a s pn\u21921. This is\nallowable under this notion of convergence because a diminishing amount of non-\nconverging behavior (namely, when Xn=c) is possible. Note that we have said\nnothing about how p n\u21921.\nExample . To get some sense of the mechanics of this kind of convergence, let\n{X1,X2,X3,...}be the indicators of the corresponding intervals,\n(0,1],(0,1\n2],(1\n2,1],(0,1\n3],(1\n3,2\n3],(2\n3,1]\nIn other words, just keep splitting the unit interval into equal chunks and enumerate\nthose chunks with Xi. Because each Xiis an indicator function, it takes only two\nvalues: zero and one. For example, for X2=1i f0 <x\u22641/2 and zero other-\nwise. Note that x\u223cU(0,1). This means that P(X2=1)=1/2. Now, we want\nto compute the sequence of P(Xn>/epsilon1)for each nfor some /epsilon1\u2208(0,1).F o r X1,\nwe have P(X1>/epsilon1)=1 because we already chose /epsilon1in the interval covered by\nX1.F o r X2,w eh a v e P(X2>/epsilon1)=1/2, for X3,w eh a v e P(X3>/epsilon1)=1/3, and\nso on. This produces the following sequence: (1,1\n2,1\n2,1\n3,1\n3,...) . The limit of the\nsequence is zero so that XnP\u21920. However, for every x\u2208(0,1), the sequence of\nfunction values of Xn(x)consists of in\ufb01nitely many zeros and ones (remember that\nindicator functions can evaluate to either zero or one). Thus, the set of xfor which the\nsequence Xn(x)converges is empty because the sequence bounces between zero and\none. This means that almost sure convergence fails here even though we have conver-\ngence in probability. The key distinction is that convergence in probability considersthe convergence of a sequence of probabilities whereas almost sure convergence is\nconcerned about the sequence of values of the random variables over sets of events\nthat \ufb01ll out the underlying probability space entirely (i.e., with probability one).\nThis is a good example so let\u2019s see if we can make it concrete with some Python.\nThe following is a function to compute the different subintervals:", "141": "130 3 Statistics\n>>>make_interval =lambda n: np .array( list(zip(range (n+1),\n... range (1,n+1))))/n\nNow, we can use this function to create a Numpy array of intervals, as in the example,\n>>> intervals =np.vstack([make_interval(i) foriinrange (1,5)])\n>>> print (intervals)\n[[0. 1. ]\n[0. 0.5 ][0.5 1. ]\n[0. 0.33333333]\n[0.33333333 0.66666667][0.66666667 1. ][0. 0.25 ][0.25 0.5 ][0.5 0.75 ][0.75 1. ]]\nThe following function computes the bit string in our example, {X1,X2,..., Xn}:\n>>>bits =lambda u:((intervals[:, 0]<u)&(u<=intervals[:, 1])).astype( int)\n>>>bits(u .rvs())\narray([1, 0, 1, 0, 0, 1, 0, 0, 0, 1])\nNow that we have the individual bit strings, to show convergence we want to show\nthat the probability of each entry goes to a limit. For example, using ten realizations,\n>>> print (np.vstack([bits(u .rvs()) for iinrange (10)]))\n[ [ 1101000100 ]\n[ 1101000100 ]\n[ 1100100100 ][ 1010010010 ]\n[ 1010010010 ]\n[ 1100100100 ][ 1101001000 ]\n[ 1100100100 ]\n[ 1100100100 ][ 1101001000 ] ]\nWe want the limiting probability of a one in each column to convert to a limit. We\ncan estimate this over 1000 realizations using the following code:\n>>>np.vstack([bits(u .rvs()) foriinrange (1000)]).mean(axis =0)\narray([1. , 0.493, 0.507, 0.325, 0.34 , 0.335, 0.253, 0.24 , 0.248,\n0.259])\nNote that these entries should approach the (1,1\n2,1\n2,1\n3,1\n3,...) sequence we found\nearlier. Figure 3.2shows the convergence of these probabilities for a large number\nof intervals. Eventually, the probability shown on this graph will decrease to zero", "142": "3.3 Types of Convergence 131\nFig. 3.2 Convergence in\nprobability for the randomvariable sequence\nwith large enough n. Again, note that the individual sequences of zeros and ones\ndo not converge, but the probabilities of these sequences converge. This is the key\ndifference between almost sure convergence and convergence in probability. Thus,\nconvergence in probability does not imply almost sure convergence. Conversely,\nalmost sure convergence does imply convergence in probability.\nThe following notation should help emphasize the difference between almost sure\nconvergence and convergence in probability, respectively,\nP/parenleftBig\nlimn\u2192\u221e|Xn\u2212X|</epsilon1/parenrightBig\n=1(almost sure convergence )\nlimn\u2192\u221eP(|Xn\u2212X|</epsilon1)=1(convergence in probability )\n3.3.3 Convergence in Distribution\nSo far, we have been discussing convergence in terms of sequences of probabilities\nor sequences of values taken by the random variable. By contrast, the next major\nkind of convergence is convergence in distribution where\nlimn\u2192\u221eFn(t)=F(t)\nfor all tfor which Fis continuous and Fis the cumulative density function. For this\ncase, convergence is only concerned with the cumulative density function, written\nasXnd\u2192X.\nExample . To develop some intuition about this kind of convergence, consider a\nsequence of XnBernoulli random variables. Furthermore, suppose these are all really\njust the same random variable X. Trivially, Xnd\u2192X. Now, suppose we de\ufb01ne\nY=1\u2212X, which means that Yhas the same distribution as X. Thus, Xnd\u2192Y.B y", "143": "132 3 Statistics\ncontrast, because |Xn\u2212Y|= 1 for all n, we can never have almost sure convergence\nor convergence in probability. Thus, convergence in distribution is the weakest of\nthe three forms of convergence in the sense that it is implied by the other two, butimplies neither of the two.\nAs another striking example, we could have Y\nnd\u2192Zwhere Z\u223cN(0,1),b u t\nwe could also have Ynd\u2192\u2212 Z. That is, Yncould converge in distribution to either Z\nor\u2212Z. This may seem ambiguous, but this kind of convergence is practically very\nuseful because it allows for complicated distributions to be approximated by simpler\ndistributions.\n3.3.4 Limit Theorems\nNow that we have all of these notions of convergence, we can apply them to different\nsituations and see what kinds of claims we can construct from them.\nWeak Law of Large Numbers .L e t{X1,X2,..., Xn}be an iid (independent, iden-\ntically distributed) set of random variables with \ufb01nite mean E(Xk)=\u03bcand \ufb01nite\nvariance. Let Xn=1\nn/summationtext\nkXk. Then, we have XnP\u2192\u03bc. This result is important\nbecause we frequently estimate parameters using an averaging process of some kind.This basically justi\ufb01es this in terms of convergence in probability. Informally, this\nmeans that the distribution of\nXnbecomes concentrated around \u03bcasn\u2192\u221e .\nStrong Law of Large Numbers .L e t{X1,X2,...,}be an iid set of random vari-\nables. Suppose that \u03bc=E|Xi|<\u221e, then Xnas\u2192\u03bc. The reason this is called the\nstrong law is that it implies the weak law because almost sure convergence impliesconvergence in probability. The so-called Komogorov criterion gives the convergence\nof the following:\n/summationdisplay\nk\u03c32\nk\nk2\nas a suf\ufb01cient condition for concluding that the Strong Law applies to the sequence\n{Xk}with corresponding {\u03c32\nk}.\nAs an example, consider an in\ufb01nite sequence of Bernoulli trials with Xi=1i f\ntheithtrial is successful. Then Xnis the relative frequency of successes in ntrials\nand E(Xi)is the probability pof success on the ithtrial. With all that established,\nthe Weak Law says only that if we consider a suf\ufb01ciently large and \ufb01xed n,t h e\nprobability that the relative frequency will converge to pis guaranteed. The Strong\nLaw states that if we regard the observation of all the in\ufb01nite {Xi}as one performance\nof the experiment, the relative frequency of successes will almost surely converge to\np. The difference between the Strong Law and the Weak Law of large numbers is\nsubtle and rarely arises in practical applications of probability theory.", "144": "3.3 Types of Convergence 133\nCentral Limit Theorem . Although the Weak Law of Large Numbers tells us that\nthe distribution of Xnbecomes concentrated around \u03bc, it does not tell us what that\ndistribution is. The central limit theorem (CLT) says that Xnhas a distribution that\nis approximately Normal with mean \u03bcand variance \u03c32/n. Amazingly, nothing is\nassumed about the distribution of Xi, except the existence of the mean and variance.\nThe following is the Central Limit Theorem: Let {X1,X2,..., Xn}be iid with mean\n\u03bcand variance \u03c32. Then,\nZn=\u221an(Xn\u2212\u03bc)\n\u03c3P\u2212\u2192 Z\u223cN(0,1)\nThe loose interpretation of the Central Limit Theorem is that Xncan be legitimately\napproximated by a Normal distribution. Because we are talking about convergence\nin probability here, claims about probability are legitimized, not claims about the\nrandom variable itself. Intuitively, this shows that normality arises from sums ofsmall, independent disturbances of \ufb01nite variance. Technically, the \ufb01nite variance\nassumption is essential for normality. Although the Central Limit Theorem provides\na powerful, general approximation, the quality of the approximation for a particularsituation still depends on the original (usually unknown) distribution.\n3.4 Estimation Using Maximum Likelihood\nThe estimation problem starts with the desire to infer something meaningful from\ndata. For parametric estimation, the strategy is to postulate a model for the data and\nthen use the data to \ufb01t model parameters. This leads to two fundamental questions:where to get the model and how to estimate the parameters? The \ufb01rst question is\nbest answered by the maxim: all models are wrong, some are useful . In other words,\nchoosing a model depends as much on the application as on the model itself. Think\nabout models as building different telescopes to view the sky. No one would ever\nclaim that the telescope generates the sky! It is same with data models. Models giveus multiple perspectives on the data that themselves are proxies for some deeper\nunderlying phenomenon.\nSome categories of data may be more commonly studied using certain types of\nmodels, but this is usually very domain speci\ufb01c and ultimately depends on the aims\nof the analysis. In some cases, there may be strong physical reasons behind choosing\na model. For example, one could postulate that the model is linear with some noiseas in the following:\nY=aX+/epsilon1\nwhich basically says that you, as the experimenter, dial in some value for Xand\nthen read off something directly proportional to Xas the measurement, Y,p l u ss o m e\nadditive noise that you attribute to jitter in the apparatus. Then, the next step is toestimate the parameter ain the model, given some postulated claim about the nature", "145": "134 3 Statistics\nof/epsilon1. How to compute the model parameters depends on the particular methodology.\nThe two broad rubrics are parametric and nonparametric estimation. In the former, we\nassume we know the density function of the data and then try to derive the embeddedparameters for it. In the latter, we claim only to know that the density function is a\nmember of a broad class of density functions and then use the data to characterize\na member of that class. Broadly speaking, the former consumes less data than thelatter, because there are fewer unknowns to compute from the data.\nLet\u2019s concentrate on parametric estimation for now. The tradition is to denote\nthe unknown parameter to be estimated as \u03b8which is a member of a large space of\nalternates, \u0398. To judge between potential \u03b8values, we need an objective function,\nknown as a risk function, L(\u03b8,\u02c6\u03b8), where \u02c6\u03b8(x)is an estimate for the unknown \u03b8that\nis derived from the available data x. The most common and useful risk function is\nthe squared error loss,\nL(\u03b8,\u02c6\u03b8)=(\u03b8\u2212\u02c6\u03b8)\n2\nAlthough neat, this is not practical because we need to know the unknown \u03b8to\ncompute it. The other problem is because \u02c6\u03b8is a function of the observed data, it is\nalso a random variable with its own probability density function. This leads to the\nnotion of the expected risk function,\nR(\u03b8,\u02c6\u03b8)=E\u03b8(L(\u03b8,\u02c6\u03b8))=/integraldisplay\nL(\u03b8,\u02c6\u03b8(x))f(x;\u03b8)dx\nIn other words, given a \ufb01xed \u03b8, integrate over the probability density function of the\ndata, f(x), to compute the risk. Plugging in for the squared error loss, we compute\nthe mean squared error,\nE\u03b8(\u03b8\u2212\u02c6\u03b8)2=/integraldisplay\n(\u03b8\u2212\u02c6\u03b8)2f(x;\u03b8)dx\nThis has the important factorization into the bias ,\nbias=E\u03b8(\u02c6\u03b8)\u2212\u03b8\nwith the corresponding variance, V\u03b8(\u02c6\u03b8)as in the following mean squared error\n(MSE):\nE\u03b8(\u03b8\u2212\u02c6\u03b8)2=bias2+V\u03b8(\u02c6\u03b8)\nThis is an important trade-off that we will return to repeatedly. The idea is the bias is\nnonzero when the estimator \u02c6\u03b8, integrated over all possible data, f(x), does not equal\nthe underlying target parameter \u03b8. In some sense, the estimator misses the target, no\nmatter how much data is used. When the bias equals zero, the estimated is unbiased .\nFor \ufb01xed MSE, low bias implies high variance and vice versa. This trade-off was\nonce not emphasized and instead much attention was paid to the smallest variance\nof unbiased estimators (see Cramer\u2013Rao bounds). In practice, understanding and", "146": "3.4 Estimation Using Maximum Likelihood 135\nexploiting the trade-off between bias and variance and reducing the MSE is more\nimportant.\nWith all this setup, we can now ask how bad can bad get by examining minimax\nrisk,\nRmmx=inf\n\u02c6\u03b8sup\n\u03b8R(\u03b8,\u02c6\u03b8)\nwhere the inf is take over all estimators. Intuitively, this means if we found the\nworst possible \u03b8and swept over all possible parameter estimators \u02c6\u03b8, and then took\nthe smallest possible risk we could \ufb01nd, we would have the minimax risk. Thus, an\nestimator, \u02c6\u03b8mmx ,i sa minimax estimator if it achieves this feat,\nsup\n\u03b8R(\u03b8,\u02c6\u03b8mmx)=inf\n\u02c6\u03b8sup\n\u03b8R(\u03b8,\u02c6\u03b8)\nIn other words, even in the face of the worst \u03b8(i.e., the sup\u03b8),\u02c6\u03b8mmx still achieves the\nminimax risk. There is a greater theory that revolves around minimax estimators ofvarious kinds, but this is far beyond our scope here. The main thing to focus on is\nthat under certain technical but easily satis\ufb01able conditions, the maximum likelihood\nestimator is approximately minimax. Maximum likelihood is the subject of the nextsection. Let\u2019s get started with the simplest application: coin-\ufb02ipping.\n3.4.1 Setting Up the Coin-Flipping Experiment\nSuppose we have coin and want to estimate the probability of heads ( p) for it. We\nmodel the distribution of heads and tails as a Bernoulli distribution with the following\nprobability mass function:\n\u03c6(x)=px(1\u2212p)(1\u2212x)\nwhere xis the outcome, 1for heads and 0for tails. Note that maximum likelihood is\na parametric method that requires the speci\ufb01cation of a particular model for whichwe will compute embedded parameters. For nindependent \ufb02ips, we have the joint\ndensity as the product of nof these functions as in,\n\u03c6(x)= n/productdisplay\ni=1px\ni(1\u2212p)(1\u2212xi)\nThe following is the likelihood function :\nL(p;x)=n/productdisplay\ni=1pxi(1\u2212p)1\u2212xi", "147": "136 3 Statistics\nThis is basically notation. We have just renamed the previous equation to emphasize\nthepparameter, which is what we want to estimate.\nThe principle of maximum likelihood is to maximize the likelihood as the function\nofpafter plugging in all of the xidata. We then call this maximizer \u02c6pwhich\nis a function of the observed xidata, and as such, is a random variable with its\nown distribution. This method therefore ingests data and an assumed model for theprobability density, and produces a function that estimates the embedded parameter in\nthe assumed probability density. Thus, maximum likelihood generates the functions\nof data that we need in order to get at the underlying parameters of the model.Note that there is no limit to the ways we can functionally manipulate the data we\nhave collected. The maximum likelihood principle gives us a systematic method for\nconstructing these functions subject to the assumed model. This is a point worthemphasizing: the maximum likelihood principle yields functions as solutions the\nsame way solving differential equations yields functions as solutions. It is very, very\nmuch harder to produce a function than to produce a value as a solution, even withthe assumption of a convenient probability density. Thus, the power of the principle\nis that you can construct such functions subject to the model assumptions.\nSimulating the Experiment . We need the following code to simulate coin-\ufb02ipping:\n>>> from scipy.stats import bernoulli\n>>>p_true =1/2.0 # estimate this!\n>>>fp=bernoulli(p_true) # create bernoulli random variate\n>>>xs=fp.rvs(100) # generate some samples\n>>> print (xs[: 30]) # see first 30 samples\n[ 010110011101110110110100110101 ]\nNow, we can write out the likelihood function using Sympy. Note that we give the\nSympy variables the positive=True attribute upon construction because this eases\nSympy\u2019s internal simpli\ufb01cation algorithms.\n>>> import sympy\n>>>x,p,z =sympy .symbols( 'x p z' , positive =True)\n>>>phi=p**x*(1-p)**(1-x)# distribution function\n>>>L=np.prod([phi .subs(x,i) foriinxs]) # likelihood function\n>>> print (L) # approx 0.5?\np**57*(-p + 1)**43\nNote that, once we plug in the data, the likelihood function is solely a function of\nthe unknown parameter ( pin this case). The following code uses calculus to \ufb01nd\nthe extrema of the likelihood function. Note that taking the log ofLmakes the\nmaximization problem tractable but doesn\u2019t change the extrema.\n>>>logL=sympy .expand_log(sympy .log(L))\n>>>sol,=sympy .solve(sympy .diff(logL,p),p)\n>>> print (sol)\n57/100", "148": "3.4 Estimation Using Maximum Likelihood 137\nProgramming Tip\nNote that sol,=sympy.solve statement includes a comma after the sol vari-\nable. This is because the solve function returns a list containing a single ele-\nment. Using this assignment unpacks that single element into the sol variable\ndirectly. This is another one of the many small elegancies of Python.\nThe following code generates Fig. 3.3.\nfig,ax =subplots()\nx=np.linspace( 0,1,100)\nax.plot(x, map(sympy .lambdify(p,logJ, 'numpy' ),x), 'k-',lw=3)\nax.plot(sol,logJ .subs(p,sol), 'o',\ncolor ='gray' ,ms=15,label ='Estimated' )\nax.plot(p_true,logJ .subs(p,p_true), 's',\ncolor ='k',ms=15,label ='Actual' )\nax.set_xlabel( '$p$' ,fontsize =18)\nax.set_ylabel( 'Likelihood' ,fontsize =18)\nax.set_title( 'Estimate not equal to true value' ,fontsize =18)\nax.legend(loc =0)\nProgramming Tip\nIn the prior code, we use the lambdify function in lambdify(p,logJ,\n\u2019numpy\u2019) to take a Sympy expression and convert it into a Numpy version\nthat is easier to compute. The lambdify function has an extra argument where\nyou can specify the function space that it should use to convert the expression.In the above this is set to Numpy.\nFigure 3.3 shows that our estimator \u02c6p(circle) is not equal to the true value of\np(square), despite being the maximum of the likelihood function. This may sound\ndisturbing, but keep in mind this estimate is a function of the random data; and sincethat data can change, the ultimate estimate can likewise change. Remember that the\nestimator is a function of the data and is thus also a random variable , just like the\ndata is. This means it has its own probability distribution with corresponding meanand variance. So, what we are observing is a consequence of that variance.\nFigure 3.4 shows what happens when you run many thousands of coin experi-\nments and compute the maximum likelihood estimate for each experiment, given aparticular number of samples per experiment. This simulation gives us a histogram\nof the maximum likelihood estimates, which is an approximation of the probability\ndistribution of the \u02c6pestimator itself. This \ufb01gure shows that the sample mean of the", "149": "138 3 Statistics\nFig. 3.3 Maximum likelihood estimate versus true parameter. Note that the estimate is slightly off\nfrom the true value. This is a consequence of the fact that the estimator is a function of the data andlacks knowledge of the true underlying value\nFig. 3.4 Histogram of maximum likelihood estimates. The title shows the estimated mean and\nstandard deviation of the samples\nestimator ( \u03bc=1\nn/summationtext\u02c6pi) is pretty close to the true value, but looks can be deceiving.\nThe only way to know for sure is to check if the estimator is unbiased, namely, if\nE(\u02c6p)=p\nBecause this problem is simple, we can solve for this in general noting that the terms\nabove are either p,i fxi=1o r1 \u2212pifxi=0. This means that we can write\nL(p|x)=p/summationtextn\ni=1xi(1\u2212p)n\u2212/summationtextn\ni=1xi", "150": "3.4 Estimation Using Maximum Likelihood 139\nwith corresponding logarithm as\nJ=log(L(p|x))=log(p)n/summationdisplay\ni=1xi+log(1\u2212p)/parenleftBigg\nn\u2212n/summationdisplay\ni=1xi/parenrightBigg\nTaking the derivative of this gives\ndJ\ndp=1\npn/summationdisplay\ni=1xi+(n\u2212/summationtextn\ni=1xi)\np\u22121\nand solving this for pleads to\n\u02c6p=1\nnn/summationdisplay\ni=1xi\nThis is our estimator forp. Up until now, we have been using Sympy to solve for\nthis based on the data xibut now that we have it analytically we don\u2019t have to solve\nfor it each time. To check if this estimator is biased, we compute its expectation:\nE/parenleftbig\n\u02c6p/parenrightbig\n=1\nnn/summationdisplay\niE(xi)=1\nnnE(xi)\nby linearity of the expectation and where\nE(xi)=p\nTherefore,\nE/parenleftbig\n\u02c6p/parenrightbig\n=p\nThis means that the estimator is unbiased . Similarly,\nE/parenleftBig\n\u02c6p2/parenrightBig\n=1\nn2E\u23a1\n\u23a3/parenleftBiggn/summationdisplay\ni=1xi/parenrightBigg2\u23a4\n\u23a6\nand where\nE/parenleftBig\nx2\ni/parenrightBig\n=p\nand by the independence assumption,\nE/parenleftbig\nxixj/parenrightbig\n=E(xi)E(xj)=p2", "151": "140 3 Statistics\nThus,\nE/parenleftBig\n\u02c6p2/parenrightBig\n=/parenleftbigg1\nn2/parenrightbigg\nn/bracketleftBig\np+(n\u22121)p2/bracketrightBig\nSo, the variance of the estimator, \u02c6p, is the following:\nV(\u02c6p)=E/parenleftBig\n\u02c6p2/parenrightBig\n\u2212E/parenleftbig\n\u02c6p/parenrightbig2=p(1\u2212p)\nn\nNote that the nin the denominator means that the variance asymptotically goes to zero\nasnincreases (i.e., we consider more and more samples). This is good news because\nit means that more and more coin-\ufb02ips lead to a better estimate of the underlying p.\nUnfortunately, this formula for the variance is practically useless because we\nneed pto compute it and pis the parameter we are trying to estimate in the \ufb01rst\nplace! However, this is where the plug-in principle2saves the day. It turns out in this\nsituation, you can simply substitute the maximum likelihood estimator, \u02c6p,f o rt h e p\nin the above equation to obtain the asymptotic variance for V(\u02c6p). The fact that this\nwork is guaranteed by the asymptotic theory of maximum likelihood estimators.\nNevertheless, looking at V(\u02c6p)2, we can immediately notice that if p=0, then\nthere is no estimator variance because the outcomes are guaranteed to be tails. Also,\nfor any n, the maximum of this variance happens at p=1/2. This is our worst-case\nscenario and the only way to compensate is with larger n.\nAll we have computed is the mean and variance of the estimator. In general, this\nis insuf\ufb01cient to characterize the underlying probability density of \u02c6p, except if we\nsomehow knew that \u02c6pwere normally distributed. This is where the powerful Central\nLimit Theorem we discussed in Sect. 3.3.4 comes in. The form of the estimator, which\nis just a sample mean, implies that we can apply this theorem and conclude that \u02c6pis\nasymptotically normally distributed. However, it doesn\u2019t quantify how many samples\nnwe need. In our simulation this is no problem because we can generate as much\ndata as we like, but in the real world, with a costly experiment, each sample may beprecious.\n3In the following, we won\u2019t apply the Central Limit Theorem and instead\nproceed analytically.\nProbability Density for the Estimator . To write out the full density for \u02c6p, we \ufb01rst\nhave to ask what is the probability that the estimator will equal a speci\ufb01c value and\nthe tally up all the ways that could happen with their corresponding probabilities.For example, what is the probability that\n2This is also known as the invariance property of maximum likelihood estimators. It basically states\nthat the maximum likelihood estimator of any function, say, h(\u03b8), is the same hwith the maximum\nlikelihood estimator for \u03b8substituted in for \u03b8; namely, h(\u03b8ML).\n3It turns out that the central limit theorem augmented with an Edgeworth expansion tells us that\nconvergence is regulated by the skewness of the distribution [ 1]. In other words, the more symmetric\nthe distribution, the faster it converges to the normal distribution according to the central limittheorem.", "152": "3.4 Estimation Using Maximum Likelihood 141\n\u02c6p=1\nnn/summationdisplay\ni=1xi=0\nThis can only happen one way: when xi=0\u2200i. The probability of this happening\ncan be computed from the density\nf(x,p)=n/productdisplay\ni=1/parenleftBig\npxi(1\u2212p)1\u2212xi/parenrightBig\nf/parenleftBiggn/summationdisplay\ni=1xi=0,p/parenrightBigg\n=(1\u2212p)n\nLikewise, if {xi}has only one nonzero element, then\nf/parenleftBiggn/summationdisplay\ni=1xi=1,p/parenrightBigg\n=npn\u22121/productdisplay\ni=1(1\u2212p)\nwhere the ncomes from the nways to pick one element from the nelements xi.\nContinuing this way, we can construct the entire density as\nf/parenleftBiggn/summationdisplay\ni=1xi=k,p/parenrightBigg\n=/parenleftbiggn\nk/parenrightbigg\npk(1\u2212p)n\u2212k\nwhere the \ufb01rst term on the right is the binomial coef\ufb01cient of nthings taken kat a\ntime. This is the binomial distribution and it\u2019s not the density for \u02c6p, but rather for\nn\u02c6p. We\u2019ll leave this as-is because it\u2019s easier to work with below. We just have to\nremember to keep track of the nfactor.\nCon\ufb01dence Intervals . Now that we have the full density for \u02c6p, we are ready to ask\nsome meaningful questions. For example, what is the probability the estimator is\nwithin /epsilon1fraction of the true value of p?\nP/parenleftbig\n|\u02c6p\u2212p|\u2264/epsilon1p/parenrightbig\nMore concretely, we want to know how often the estimated \u02c6pis trapped within /epsilon1of\nthe actual value. That is, suppose we ran the experiment 1000 times to generate 1000\ndifferent estimates of \u02c6p. What percentage of the 1000 so-computed values are trapped\nwithin /epsilon1of the underlying value. Rewriting the above equation as the following:\nP/parenleftbig\np\u2212/epsilon1p<\u02c6p<p+/epsilon1p/parenrightbig\n=P/parenleftBigg\nnp\u2212n/epsilon1p<n/summationdisplay\ni=1xi<np+n/epsilon1p/parenrightBigg", "153": "142 3 Statistics\nLet\u2019s plug in some live numbers here for our worst-case scenario (i.e., highest variance\nscenario) where p=1/2. Then, if /epsilon1=1/100, we have\nP/parenleftBigg\n99n\n100<n/summationdisplay\ni=1xi<101n\n100/parenrightBigg\nSince the sum in integer valued, we need n>100 to even compute this. Thus, if\nn=101 we have,\nP/parenleftBigg\n9999\n200<101/summationdisplay\ni=1xi<10201\n200/parenrightBigg\n=f/parenleftBigg101/summationdisplay\ni=1xi=50,p/parenrightBigg\n...\n=/parenleftbigg101\n50/parenrightbigg\n(1/2)50(1\u22121/2)101\u221250=0.079\nThis means that in the worst-case scenario for p=1/2, given n=101 trials, we\nwill only get within 1% of the actual p=1/2 about 8% of the time. If you feel\ndisappointed, it is because you\u2019ve been paying attention. What if the coin was really\nheavy and it was hard work to repeat this 101 times?\nLet\u2019s come at this another way: given I could only \ufb02ip the coin 100 times, how\nclose could I come to the true underlying value with high probability (say, 95%)? In\nthis case, instead of picking a value for /epsilon1, we are solving for /epsilon1. Plugging in gives\nP/parenleftBigg\n50\u221250/epsilon1<100/summationdisplay\ni=1xi<50+50/epsilon1/parenrightBigg\n=0.95\nwhich we have to solve for /epsilon1. Fortunately, all the tools we need to solve for this are\nalready in Scipy\n>>> from scipy.stats import binom\n>>> # n=100, p = 0.5, distribution of the estimator phat\n>>>b=binom( 100,.5)\n>>> # symmetric sum the probability around the mean\n>>>g=lambda i:b.pmf(np .arange( -i,i)+50).sum()\n>>> print (g(10))# approx 0.95\n0.9539559330706295\nThe two vertical lines in Fig. 3.5show how far out from the mean we have to go to\naccumulate 95% of the probability. Now, we can solve this as\n50+50/epsilon1=60", "154": "3.4 Estimation Using Maximum Likelihood 143\nFig. 3.5 Probability mass\nfunction for \u02c6p. The two verti-\ncal lines form the con\ufb01denceinterval\nwhich makes /epsilon1=1/5 or 20%. So, \ufb02ipping 100 times means I can only get within\n20% of the real p95% of the time in the worst-case scenario (i.e., p=1/2). The\nfollowing code veri\ufb01es the situation:\n>>> from scipy.stats import bernoulli\n>>>b=bernoulli( 0.5)# coin distribution\n>>>xs=b.rvs(100)# flip it 100 times\n>>>phat =np.mean(xs) # estimated p\n>>> print (abs(phat -0.5)< 0.5*0.20 )# make it w/in interval?\nTrue\nLet\u2019s keep doing this and see if we can get within this interval 95% of the time.\n>>>out=[]\n>>>b=bernoulli( 0.5)# coin distribution\n>>> for iinrange (500): # number of tries\n... xs=b.rvs(100) # flip it 100 times\n... phat =np.mean(xs) # estimated p\n... out.append( abs(phat -0.5)< 0.5*0.20 )# within 20% ?\n...>>> # percentage of tries w/in 20% interval\n>>> print (100*np.mean(out))\n97.39999999999999\nWell, that seems to work! Now we have a way to get at the quality of the estimator,\n\u02c6p.\nMaximum Likelihood Estimator Without Calculus . The prior example showed\nhow we can use calculus to compute the maximum likelihood estimator. It\u2019s important\nto emphasize that the maximum likelihood principle does not depend on calculus\nand extends to more general situations where calculus is impossible. For example,\nletXbe uniformly distributed in the interval [0,\u03b8].G i v e n nmeasurements of X,t h e\nlikelihood function is the following:\nL(\u03b8)= n/productdisplay\ni=11\n\u03b8=1\n\u03b8n", "155": "144 3 Statistics\nwhere each xi\u2208[0,\u03b8]. Note that the slope of this function is not zero anywhere\nso the usual calculus approach is not going to work here. Because the likelihood is\nthe product of the individual uniform densities, if any of the xivalues were outside\nof the proposed [0,\u03b8]interval, then the likelihood would go to zero, because the\nuniform density is zero outside of the [0,\u03b8]. This is no good for maximization. Thus,\nobserving that the likelihood function is strictly decreasing with increasing \u03b8,w e\nconclude that the value for \u03b8that maximizes the likelihood is the maximum of the\nxivalues. To summarize, the maximum likelihood estimator is the following:\n\u03b8ML=max\nixi\nAs always, we want the distribution of this estimator to judge its performance. In\nthis case, this is pretty straightforward. The cumulative density function for the max\nfunction is the following:\nP/parenleftBig\n\u02c6\u03b8ML<v/parenrightBig\n=P(x0\u2264v\u2227x1\u2264v...\u2227xn\u2264v)\nand since all the xiare uniformly distributed in [0,\u03b8],w eh a v e\nP/parenleftBig\n\u02c6\u03b8ML<v/parenrightBig\n=/parenleftBigv\n\u03b8/parenrightBign\nSo, the probability density function is then,\nf\u02c6\u03b8ML(\u03b8ML)=n\u03b8n\u22121\nML\u03b8\u2212n\nThen, we can compute the E(\u03b8ML)=(\u03b8n)/(n+1)with corresponding variance as\nV(\u03b8ML)=(\u03b82n)/(n+1)2/(n+2).\nFor a quick sanity check, we can write the following simulation for \u03b8=1a si n\nthe following:\n>>> from scipy import stats\n>>>rv=stats .uniform( 0,1)# define uniform random variable\n>>>mle=rv.rvs(( 100,500)).max(0)# max along row-dimension\n>>> print (mean(mle)) # approx n/(n+1) = 100/101 \u02dc= 0.99\n0.989942138048\n>>> print (var(mle)) #approx n/(n+1)**2/(n+2) \u02dc= 9.61E-5\n9.95762009884e-05\nProgramming Tip\nThemax(0) suf\ufb01x on for the mle computation takes the maximum of the so-\ncomputed array along the row ( axis=0 ) dimension.", "156": "3.4 Estimation Using Maximum Likelihood 145\nY ou can also plot hist(mle) to see the histogram of the simulated maximum like-\nlihood estimates and match it up against the probability density function we derived\nabove.\nIn this section, we explored the concept of maximum likelihood estimation using a\ncoin-\ufb02ipping experiment both analytically and numerically with the scienti\ufb01c Python\nstack. We also explored the case when calculus is not workable for maximum likeli-hood estimation. There are two key points to remember. First, maximum likelihood\nestimation produces a function of the data that is itself a random variable, with its\nown probability distribution. We can get at the quality of the so-derived estimatorsby examining the con\ufb01dence intervals around the estimated values using the prob-\nability distributions associated with the estimators themselves. Second, maximum\nlikelihood estimation applies even in situations where using basic calculus is notapplicable [ 2].\n3.4.2 Delta Method\nSometimes we want to characterize the distribution of a function of a random variable.\nIn order to extend and generalize the Central Limit Theorem in this way, we need theTaylor series expansion. Recall that the Taylor series expansion is an approximation\nof a function of the following form:\nT\nr(x)=r/summationdisplay\ni=0g(i)(a)\ni!(x\u2212a)i\nthis basically says that a function gcan be adequately approximated about a point\nausing a polynomial based on its derivatives evaluated at a. Before we state the\ngeneral theorem, let\u2019s examine an example to understand how the mechanics work.\nExample . Suppose that Xis a random variable with E(X)=\u03bc/negationslash=0. Furthermore,\nsupposedly have a suitable function gand we want the distribution of g(X). Applying\nthe Taylor series expansion, we obtain the following:\ng(X)\u2248g(\u03bc)+g/prime(\u03bc)(X\u2212\u03bc)\nIf we use g(X)as an estimator for g(\u03bc), then we can say that we approximately have\nthe following:\nE(g(X))=g(\u03bc)\nV(g(X))=(g/prime(\u03bc))2V(X)\nConcretely, suppose we want to estimate the odds,p\n1\u2212p. For example, if p=2/3,\nthen we say that the odds is 2:1 meaning that the odds of the one outcome are twice", "157": "146 3 Statistics\nas likely as the odds of the other outcome. Thus, we have g(p)=p\n1\u2212pand we want\nto \ufb01nd V(g(\u02c6p)). In our coin-\ufb02ipping problem, we have the estimator \u02c6p=1\nn/summationtextXk\nfrom the Bernoulli-distributed data Xkindividual coin-\ufb02ips. Thus,\nE(\u02c6p)=p\nV(\u02c6p)=p(1\u2212p)\nn\nNow,g/prime(p)=1/(1\u2212p)2,s ow eh a v e ,\nV(g(\u02c6p))=(g/prime(p))2V(\u02c6p)\n=/parenleftbigg1\n(1\u2212p)2/parenrightbigg2p(1\u2212p)\nn\n=p\nn(1\u2212p)3\nwhich is an approximation of the variance of the estimator g(\u02c6p). Let\u2019s simulate this\nand see how it agrees.\n>>> from scipy import stats\n>>> # compute MLE estimates\n>>>d=stats .bernoulli( 0.1).rvs(( 10,5000)).mean( 0)\n>>> # avoid divide-by-zero\n>>>d=d[np.logical_not(np .isclose(d, 1))]\n>>> # compute odds ratio\n>>>odds =d/(1-d)\n>>> print ('odds ratio=' ,np.mean(odds), 'var=' ,np.var(odds))\nodds ratio= 0.12289206349206351 var= 0.01797950092214664\nThe \ufb01rst number above is the mean of the simulated odds ratio and the second is\nthe variance of the estimate. According to the variance estimate above, we have\nV(g(1/10))\u22480.0137, which is not too bad for this approximation. Recall we want\nto estimate the odds from \u02c6p. The code above takes 5000 estimates of the \u02c6pto estimate\nV(g). The odds ratio for p=1/10 is 1 /9\u22480.111.\nProgramming Tip\nThe code above uses the np.isclose function to identify the ones from the\nsimulation and the np.logical_not removes these elements from the data\nbecause the odds ratio has a zero in the denominator for these values.\nLet\u2019s try this again with a probability of heads of 0.5 instead of 0.3.", "158": "3.4 Estimation Using Maximum Likelihood 147\nFig. 3.6 The odds ratio is\nclose to linear for small valuesbut becomes unbounded as\npapproaches one. The delta\nmethod is more effective forsmall underlying values of p,\nwhere the linear approxima-tion is better\n>>> from scipy import stats\n>>>d=stats .bernoulli( .5).rvs(( 10,5000)).mean( 0)\n>>>d=d[np.logical_not(np .isclose(d, 1))]\n>>> print ('odds ratio=' ,np.mean(d), 'var=' ,np.var(d))\nodds ratio= 0.499379627776666 var= 0.024512322762879256\nThe odds ratio in this case is equal to one, which is not close to what was reported.\nAccording to our approximation, we should have V(g)=0.4, which does not look\nlike what our simulation just reported. This is because the approximation is best\nwhen the odds ratio is nearly linear and worse otherwise (see Fig. 3.6).\n3.5 Hypothesis Testing and P-Values\nIt is sometimes very dif\ufb01cult to unequivocally attribute outcomes to causal factors.\nFor example, did your experiment generate the outcome you were hoping for or not?\nMaybe something did happen, but the effect is not pronounced enough to separate itfrom inescapable measurement errors or other factors in the ambient environment?\nHypothesis testing is a powerful statistical method to address these questions. Let\u2019s\nbegin by again considering our coin-tossing experiment with unknown parameter\np. Recall that the individual coin-\ufb02ips are Bernoulli distributed. The \ufb01rst step is to\nestablish separate hypotheses. First, H\n0is the so-called null hypothesis. In our case\nthis can be\nH0:\u03b8<1\n2\nand the alternative hypothesis is then\nH1:\u03b8\u22651\n2", "159": "148 3 Statistics\nWith this setup, the question now boils down to \ufb01guring out which hypothesis the\ndata is most consistent with. To choose between these, we need a statistical test\nthat is a function, G, of the sample set Xn={Xi}ninto the real line, where Xiis\nthe heads or tails outcome ( Xi\u2208{0,1}). In other words, we compute G(Xn)and\ncheck if it exceeds a threshold c. If not, then we declare H0(otherwise, declare H1).\nNotationally, this is the following:\nG(Xn)<c\u21d2H0\nG(Xn)\u2265c\u21d2H1\nIn summary, we have the observed data Xnand a function Gthat maps that data\nonto the real line. Then, using the constant cas a threshold, the inequality effectively\ndivides the real line into two parts, one corresponding to each of the hypotheses.\nWhatever this test Gis, it will make mistakes of two types\u2014false negatives and\nfalse positives. The false positives arise from the case where we declare H0when\nthe test says we should declare H1. This is summarized in the Table 3.1.\nFor this example, here are the false positives (aka false alarms):\nPFA=P/parenleftbigg\nG(Xn)>c|\u03b8\u22641\n2/parenrightbigg\nOr, equivalently,\nPFA=P(G(Xn)>c|H0)\nLikewise, the other error is a false negative, which we can write analogously as\nPFN=P(G(Xn)<c|H1)\nBy choosing some acceptable values for either of these errors, we can solve for\nthe other one. The practice is usually to pick a value of PFAand then \ufb01nd the\ncorresponding value of PFN. Note that it is traditional in engineering to speak about\ndetection probability , which is de\ufb01ned as\nPD=1\u2212PFN=P(G(Xn)>c|H1)\nIn other words, this is the probability of declaring H1when the test exceeds the\nthreshold. This is otherwise known as the probability of a true detection ortrue-\ndetect .\nTable 3.1 Truth table for hypotheses testing\nDeclare H0 Declare H1\nH0True Correct False positive (Type I error)\nH1True False negative (Type II error) Correct (true-detect)", "160": "3.5 Hypothesis Testing and P-V alues 149\n3.5.1 Back to the Coin-Flipping Example\nIn our previous maximum likelihood discussion, we wanted to derive an estimator for\nthevalue of the probability of heads for the coin-\ufb02ipping experiment. For hypothesis\ntesting, we want to ask a softer question: is the probability of heads greater or less\nthan 1/2? As we just established, this leads to the two hypotheses:\nH0:\u03b8<1\n2\nversus,\nH1:\u03b8>1\n2\nLet\u2019s assume we have \ufb01ve observations. Now we need the Gfunction and a threshold\ncto help pick between the two hypotheses. Let\u2019s count the number of heads observed\nin \ufb01ve observations as our criterion. Thus, we have\nG(X5):=5/summationdisplay\ni=1Xi\nand suppose further that we pick H1only if exactly \ufb01ve out of \ufb01ve observations are\nheads. We\u2019ll call this the all-heads test.\nNow, because all of the Xiare random variables, so is Gand we must \ufb01nd the\ncorresponding probability mass function for G. Assuming the individual coin tosses\nare independent, the probability of \ufb01ve heads is \u03b85. This means that the probability\nof rejecting the H0hypothesis (and choosing H1, because there are only two choices\nhere) based on the unknown underlying probability is \u03b85. In the parlance, this is\nknown and the power function as in denoted by \u03b2as in\n\u03b2(\u03b8)=\u03b85\nLet\u2019s get a quick plot this in Fig. 3.7.\nNow, we have the following false alarm probability:\nPFA=P(G(Xn)=5|H0)=P(\u03b85|H0)\nNotice that this is a function of \u03b8, which means there are many false alarm probability\nvalues that correspond to this test. To be on the conservative side, we\u2019ll pick the\nsupremum (i.e., maximum) of this function, which is known as the size of the test,\ntraditionally denoted by \u03b1,\n\u03b1=sup\n\u03b8\u2208\u03980\u03b2(\u03b8)", "161": "150 3 Statistics\nFig. 3.7 Power function for the all-heads test. The dark circle indicates the value of the function\nindicating \u03b1\nwith domain \u03980={\u03b8<1/2}which in our case is\n\u03b1=sup\n\u03b8<1\n2\u03b85=/parenleftbigg1\n2/parenrightbigg5\n=0.03125\nLikewise, for the detection probability,\nPD(\u03b8)=P(\u03b85|H1)\nwhich is again a function of the parameter \u03b8. The problem with this test is that the\nPDis pretty low for most of the domain of \u03b8. For instance, values in the nineties for\nPDonly happen when \u03b8>0.98. In other words, if the coin produces heads 98 times\nout of 100, then we can detect H1reliably. Ideally, we want a test that is zero for the\ndomain corresponding to H0(i.e.,\u03980) and equal to one otherwise. Unfortunately,\neven if we increase the length of the observed sequence, we cannot escape this effect\nwith this test. Y ou can try plotting \u03b8nfor larger and larger values of nto see this.\nMajority Vote Test . Due to the problems with the detection probability in the all-\nheads test, maybe we can think of another test that will have the performance wewant? Suppose we reject H\n0if the majority of the observations are heads. Then,\nusing the same reasoning as above, we have\n\u03b2(\u03b8)=5/summationdisplay\nk=3/parenleftbigg5\nk/parenrightbigg\n\u03b8k(1\u2212\u03b8)5\u2212k\nFigure 3.8shows the power function for both the majority vote and the all-heads\ntests.", "162": "3.5 Hypothesis Testing and P-V alues 151\nFig. 3.8 Compares the power function for the all-heads test with that of the majority vote test\nIn this case, the new test has size\n\u03b1=sup\n\u03b8<1\n2\u03b85+5\u03b84(\u2212\u03b8+1)+10\u03b83(\u2212\u03b8+1)2=1\n2\nAs before we only get to upward of 90% for detection probability only when the\nunderlying parameter \u03b8>0.75. Let\u2019s see what happens when we consider more\nthan \ufb01ve samples. For example, let\u2019s suppose that we have n=100 samples and we\nwant to vary the threshold for the majority vote test. For example, let\u2019s have a newtest where we declare H\n1when k=60 out of the 100 trials turns out to be heads.\nWhat is the \u03b2function in this case?\n\u03b2(\u03b8)=100/summationdisplay\nk=60/parenleftbigg100\nk/parenrightbigg\n\u03b8k(1\u2212\u03b8)100\u2212k\nThis is too complicated to write by hand, but the statistics module in Sympy has all\nthe tools we need to compute this.\n>>> from sympy.stats import P, Binomial\n>>>theta =S.symbols( 'theta' ,real =True)\n>>>X=Binomial( 'x',100,theta)\n>>>beta_function =P(X>60)\n>>> print (beta_function .subs(theta, 0.5))# alpha\n0.0176001001088524\n>>> print (beta_function .subs(theta, 0.70))\n0.979011423996075\nThese results are much better than before because the \u03b2function is much steeper.\nIf we declare H1when we observe 60 out of 100 trials are heads, then we wrongly\ndeclare heads approximately 1.8% of the time. Otherwise, if it happens that the truevalue for p>0.7, we will conclude correctly approximately 97% of the time. A\nquick simulation can sanity check these results as shown below:", "163": "152 3 Statistics\n>>> from scipy import stats\n>>>rv=stats .bernoulli( 0.5)# true p = 0.5\n>>> # number of false alarms \u02dc 0.018\n>>> print (sum(rv.rvs(( 1000,100)).sum(axis =1)>60)/1000. )\n0.025\nThe above code is pretty dense so let\u2019s unpack it. In the \ufb01rst line, we use the\nscipy.stats module to de\ufb01ne the Bernoulli random variable for the coin-\ufb02ip.\nThen, we use the rvs method of the variable to generate 1000 trials of the experi-\nment where each trial consists of 100 coin-\ufb02ips. This generates a 1000 \u00d7100 matrix\nwhere the rows are the individual trials and the columns are the outcomes of eachrespective set of 100 coin-\ufb02ips. The sum(axis=1) part computes the sum across the\ncolumns. Because the values of the embedded matrix are only 1or0this gives us\nthe count of \ufb02ips that are heads per row. The next >60 part computes the boolean\n1000-long vector of values that are bigger than 60. The \ufb01nal sum adds these up.\nAgain, because the entries in the array are True orFalse thesum computes the\ncount of times the number of heads has exceeded 60 per 100 coin-\ufb02ips in each of1000 trials. Then, dividing this number by 1000 gives a quick approximation of false\nalarm probability we computed above for this case where the true value of p=0.5.\n3.5.2 Receiver Operating Characteristic\nBecause the majority vote test is a binary test, we can compute the receiver operating\ncharacteristic (ROC) which is the graph of the (PFA,PD). The term comes from\nradar systems but is a very general method for consolidating all of these issues into a\nsingle graph. Let\u2019s consider a typical signal processing example with two hypotheses.InH\n0, there is noise but no signal present at the receiver,\nH0:X=/epsilon1\nwhere /epsilon1\u223cN(0,\u03c32)represents additive noise. In the alternative hypothesis, there is\na deterministic signal at the receiver,\nH1:X=\u03bc+/epsilon1\nAgain, the problem is to choose between these two hypotheses. For H0,w eh a v e\nX\u223cN(0,\u03c32)and for H1,w eh a v e X\u223cN(\u03bc,\u03c32). Recall that we only observe\nvalues for xand must pick either H0orH1from these observations. Thus, we need\na threshold, c, to compare xagainst in order to distinguish the two hypotheses.\nFigure 3.9 shows the probability density functions under each of the hypotheses.\nThe dark vertical line is the threshold c. The gray shaded area is the probability of\ndetection, PDand the shaded area is the probability of false alarm, PFA.T h et e s t\nevaluates every observation of xand concludes H0ifx<cand H1otherwise.", "164": "3.5 Hypothesis Testing and P-V alues 153\nFig. 3.9 The two density functions for the H0and H1hypotheses. The shaded gray area is the\ndetection probability and the shaded dark gray area is the probability of false alarm. The verticalline is the decision threshold\nProgramming Tip\nThe shading shown in Fig. 3.9comes from Matplotlib\u2019s fill_between func-\ntion. This function has a where keyword argument to specify which part of the\nplot to apply shading with speci\ufb01ed color keyword argument. Note there is\nalso a fill_betweenx function that \ufb01lls horizontally. The text function can\nplace formatted text anywhere in the plot and can utilize basic L ATEX formatting.\nAs we slide the threshold left and right along the horizontal axis, we naturally\nchange the corresponding areas under each of the curves shown in Fig. 3.9and thereby\nchange the values of PDand PFA. The contour that emerges from sweeping the\nthreshold this way is the ROC as shown in Fig. 3.10 . This \ufb01gure also shows the\ndiagonal line which corresponds to making decisions based on the \ufb02ip of a fair coin.\nAny meaningful test must do better than coin-\ufb02ipping so the more the ROC bows up\nto the top left corner of the graph, the better. Sometimes ROCs are quanti\ufb01ed intoa single number called the area under the curve (AUC), which varies from 0.5 to\n1.0 as shown. In our example, what separates the two probability density functions\nis the value of \u03bc. In a real situation, this would be determined by signal processing\nmethods that include many complicated trade-offs. The key idea is that whatever\nthose trade-offs are, the test itself boils down to the separation between these twodensity functions\u2014good tests separate the two density functions and bad tests do\nnot. Indeed, when there is no separation, we arrive at the diagonal-line coin-\ufb02ipping\nsituation we just discussed.\nWhat values for P\nDand PFAare considered acceptable depends on the appli-\ncation. For example, suppose you are testing for a fatal disease. It could be that\nyou are willing to except a relatively high PFAvalue if that corresponds to a good", "165": "154 3 Statistics\nFig. 3.10 The receiver oper-\nating characteristic (ROC)corresponding to Fig. 3.9\nPDbecause the test is relatively cheap to administer compared to the alternative of\nmissing a detection. On the other hand, may be a false alarm triggers an expensive\nresponse, so that minimizing these alarms is more important than potentially missinga detection. These trade-offs can only be determined by the application and design\nfactors.\n3.5.3 P-V alues\nThere are a lot of moving parts in hypothesis testing. What we need is a way toconsolidate the \ufb01ndings. The idea is that we want to \ufb01nd the minimum level at\nwhich the test rejects H\n0. Thus, the p-value is the probability, under H0, that the test\nstatistic is at least as extreme as what was actually observed. Informally, this means\nthat smaller values imply that H0should be rejected, although this doesn\u2019t mean that\nlarge values imply that H0should be retained. This is because a large p-value can\narise from either H0being true or the test having low statistical power.\nIfH0is true, the p-value is uniformly distributed in the interval (0,1).I fH1is\ntrue, the distribution of the p-value will concentrate closer to zero. For continuousdistributions, this can be proven rigorously and implies that if we reject H\n0when\nthe corresponding p-value is less than \u03b1, then the probability of a false alarm is \u03b1.\nPerhaps it helps to formalize this a bit before computing it. Suppose \u03c4(X)is a test\nstatistic that rejects H0as it gets bigger. Then, for each sample x, corresponding to\nthe data we actually have on-hand, we de\ufb01ne\np(x)=sup\n\u03b8\u2208\u03980P\u03b8(\u03c4(X)>\u03c4(x))", "166": "3.5 Hypothesis Testing and P-V alues 155\nThis equation states that the supremum (i.e., maximum) probability that the test\nstatistic, \u03c4(X), exceeds the value for the test statistic on this particular data ( \u03c4(x))\nover the domain \u03980is de\ufb01ned as the p-value. Thus, this embodies a worst-case\nscenario over all values of \u03b8.\nHere\u2019s one way to think about this. Suppose you rejected H0, and someone says\nthat you just got lucky and somehow just drew data that happened to correspond to\na rejection of H0. What p-values provide is a way to address this by capturing the\nodds of just a favorable data-draw. Thus, suppose that your p-value is 0.05. Then,\nwhat you are showing is that the odds of just drawing that data sample, given H0is\nin force, is just 5%. This means that there\u2019s a 5% chance that you somehow lucked\nout and got a favorable draw of data.\nLet\u2019s make this concrete with an example. Given, the majority vote rule above,\nsuppose we actually do observe three of \ufb01ve heads. Given the H0, the probability of\nobserving this event is the following:\np(x)=sup\n\u03b8\u2208\u039805/summationdisplay\nk=3/parenleftbigg5\nk/parenrightbigg\n\u03b8k(1\u2212\u03b8)5\u2212k=1\n2\nFor the all-heads test, the corresponding computation is the following:\np(x)=sup\n\u03b8\u2208\u03980\u03b85=1\n25=0.03125\nFrom just looking at these p-values, you might get the feeling that the second test\nis better, but we still have the same detection probability issues we discussed above;so, p-values help in summarizing some aspects of our hypothesis testing, but they do\nnotsummarize all the salient aspects of the entire situation.\n3.5.4 T est Statistics\nAs we have seen, it is dif\ufb01cult to derive good test statistics for hypothesis testing\nwithout a systematic process. The Neyman\u2013Pearson Test is derived from \ufb01xing afalse alarm value ( \u03b1) and then maximizing the detection probability. This results in\nthe Neyman\u2013Pearson Test,\nL(x)=f\nX|H1(x)\nfX|H0(x)H1\u2277\nH0\u03b3\nwhere Lis the likelihood ratio and where the threshold \u03b3is chosen such that\n/integraldisplay\nx:L(x)>\u03b3fX|H0(x)dx=\u03b1", "167": "156 3 Statistics\nThe Neyman\u2013Pearson Test is one of a family of tests that use the likelihood ratio.\nExample . Suppose we have a receiver and we want to distinguish whether just\nnoise ( H0) or signal pulse noise ( H1) is received. For the noise-only case, we have\nx\u223cN(0,1)and for the signal pulse noise case we have x\u223cN(1,1). In other\nwords, the mean of the distribution shifts in the presence of the signal. This is a very\ncommon problem in signal processing and communications. The Neyman\u2013PearsonTest then boils down to the following:\nL(x)=e\n\u22121\n2+xH1\u2277\nH0\u03b3\nNow we have to \ufb01nd the threshold \u03b3that solves the maximization problem that char-\nacterizes the Neyman\u2013Pearson Test. Taking the natural logarithm and re-arranging\ngives\nxH1\u2277\nH01\n2+log\u03b3\nThe next step is \ufb01nd \u03b3corresponding to the desired \u03b1by computing it from the\nfollowing:/integraldisplay\u221e\n1/2+log\u03b3fX|H0(x)dx=\u03b1\nFor example, taking \u03b1=1/100, gives \u03b3\u22486.21. To summarize the test in this case,\nwe have,\nxH1\u2277\nH02.32\nThus, if we measure Xand see that its value exceeds the threshold above, we declare\nH1and otherwise declare H0. The following code shows how to solve this example\nusing Sympy and Scipy. First, we set up the likelihood ratio,\n>>> import sympy as S\n>>> from sympy import stats\n>>>s=stats .Normal( 's',1,1)# signal+noise\n>>>n=stats .Normal( 'n',0,1)# noise\n>>>x=S.symbols( 'x',real =True)\n>>>L=stats .density(s)(x) /stats .density(n)(x)\nNext, to \ufb01nd the \u03b3value,\n>>>g=S.symbols( 'g',positive =True)# define gamma\n>>>v=S.integrate(stats .density(n)(x),\n... (x,S.Rational( 1,2)+S.log(g),S .oo))", "168": "3.5 Hypothesis Testing and P-V alues 157\nProgramming Tip\nProviding additional information regarding the Sympy variable by using the\nkeyword argument positive=True helps the internal simpli\ufb01cation algorithms\nwork faster and better. This is especially useful when dealing with complicated\nintegrals that involve special functions. Furthermore, note that we used the\nRational function to de\ufb01ne the 1/2 fraction, which is another way of provid-\ning hints to Sympy. Otherwise, it\u2019s possible that the \ufb02oating-point representa-\ntion of the fraction could disguise the simple fraction and thereby miss internal\nsimpli\ufb01cation opportunities.\nWe want to solve for gin the above expression. Sympy has some built-in numerical\nsolvers as in the following:\n>>> print (S.nsolve(v -0.01 ,3.0))# approx 6.21\n6.21116124253284\nNote that in this situation it is better to use the numerical solvers because Sympy\nsolve may grind along for a long time to resolve this.\nGeneralized Likelihood Ratio Test . The likelihood ratio test can be generalized\nusing the following statistic:\n\u039b(x)=sup\u03b8\u2208\u03980L(\u03b8)\nsup\u03b8\u2208\u0398L(\u03b8)=L(\u02c6\u03b80)\nL(\u02c6\u03b8)\nwhere \u02c6\u03b80maximizes L(\u03b8)subject to \u03b8\u2208\u03980and\u02c6\u03b8is the maximum likelihood\nestimator. The intuition behind this generalization of the Likelihood Ratio Test is\nthat the denominator is the usual maximum likelihood estimator and the numerator\nis the maximum likelihood estimator, but over a restricted domain ( \u03980). This means\nthat the ratio is always less than unity because the maximum likelihood estimator\nover the entire space will always be at least as maximal as that over the more restricted\nspace. When this \u039bratio gets small enough, it means that the maximum likelihood\nestimator over the entire domain ( \u0398) is larger which means that it is safe to reject the\nnull hypothesis H0. The tricky part is that the statistical distribution of \u039bis usually\neye-wateringly dif\ufb01cult. Fortunately, Wilks Theorem says that with suf\ufb01ciently large\nn, the distribution of \u22122l o g\u039bis approximately chi-square with r\u2212r0degrees of\nfreedom, where ris the number of free parameters for \u0398and r0is the number of\nfree parameters in \u03980. With this result, if we want an approximate test at level \u03b1,\nwe can reject H0when \u22122l o g\u039b\u2265\u03c72\nr\u2212r0(\u03b1)where \u03c72\nr\u2212r0(\u03b1)denotes the 1 \u2212\u03b1\nquantile of the \u03c72\nr\u2212r0chi-square distribution. However, the problem with this result\nis that there is no de\ufb01nite way of knowing how big nshould be. The advantage of this\ngeneralized likelihood ratio test is that it can test multiple hypotheses simultaneously,\nas illustrated in the following example.", "169": "158 3 Statistics\nExample . Let\u2019s return to our coin-\ufb02ipping example, except now we have three dif-\nferent coins. The likelihood function is then,\nL(p1,p2,p3)=binom (k1;n1,p1)binom (k2;n2,p2)binom (k3;n3,p3)\nwhere binom is the binomial distribution with the given parameters. For example,\nbinom (k;n,p)=n/summationdisplay\nk=0/parenleftbiggn\nk/parenrightbigg\npk(1\u2212p)n\u2212k\nThe null hypothesis is that all three coins have the same probability of heads, H0:p=\np1=p2=p3. The alternative hypothesis is that at least one of these probabilities is\ndifferent. Let\u2019s consider the numerator of the \u039b\ufb01rst, which will give us the maximum\nlikelihood estimator of p. Because the null hypothesis is that all the pvalues are\nequal, we can just treat this as one big binomial distribution with n=n1+n2+n3\nandk=k1+k2+k3is the total number of heads observed for any coin. Thus, under\nthe null hypothesis, the distribution of kis binomial with parameters nand p.N o w ,\nwhat is the maximum likelihood estimator for this distribution? We have worked thisproblem before and have the following:\n\u02c6p\n0=k\nn\nIn other words, the maximum likelihood estimator under the null hypothesis is the\nproportion of ones observed in the sequence of ntrials total. Now, we have to sub-\nstitute this in for the likelihood under the null hypothesis to \ufb01nish the numerator of\n\u039b,\nL(\u02c6p0,\u02c6p0,\u02c6p0)=binom (k1;n1,\u02c6p0)binom (k2;n2,\u02c6p0)binom (k3;n3,\u02c6p0)\nFor the denominator of \u039b, which represents the case of maximizing over the entire\nspace, the maximum likelihood estimator for each separate binomial distribution is\nlikewise,\n\u02c6pi=ki\nni\nwhich makes the likelihood in the denominator the following:\nL(\u02c6p1,\u02c6p2,\u02c6p3)=binom (k1;n1,\u02c6p1)binom (k2;n2,\u02c6p2)binom (k3;n3,\u02c6p3)\nfor each of the i\u2208{1,2,3}binomial distributions. Then, the \u039bstatistic is then the\nfollowing:\n\u039b(k1,k2,k3)=L(\u02c6p0,\u02c6p0,\u02c6p0)\nL(\u02c6p1,\u02c6p2,\u02c6p3)", "170": "3.5 Hypothesis Testing and P-V alues 159\nWilks theorems state that \u22122l o g\u039bis chi-square distributed. We can compute this\nexample with the statistics tools in Sympy and Scipy.\n>>> from scipy.stats import binom, chi2\n>>> import numpy as np\n>>> # some sample parameters\n>>>p0,p1,p2 = 0.3 ,0.4,0.5\n>>>n0,n1,n2 =5 0,180,200\n>>>brvs=[ binom(i,j) for i,j inzip((n0,n1,n2),(p0,p1,p2))]\n>>> def gen_sample (n=1):\n... 'generate samples from separate binomial distributions'\n... ifn==1:\n... return [i.rvs() for iinbrvs]\n... else:\n... return [gen_sample() forkinrange (n)]\n...\nProgramming Tip\nNote the recursion in the de\ufb01nition of the gen_sample function where a con-\nditional clause of the function calls itself. This is a quick way to reusing code\nand generating vectorized output. Using np.vectorize is another way, but the\ncode is simple enough in this case to use the conditional clause. In Python, it isgenerally bad for performance to have code with nested recursion because of\nhow the stack frames are managed. However, here we are only recursing once\nso this is not an issue.\nNext, we compute the logarithm of the numerator of the \u039bstatistic,\n>>>k0,k1,k2 =gen_sample()\n>>> print (k0,k1,k2)\n12 68 103\n>>>pH0 =sum((k0,k1,k2)) /sum((n0,n1,n2))\n>>>numer =np.sum([np .log(binom(ni,pH0) .pmf(ki))\n... for ni,ki in\n... zip((n0,n1,n2),(k0,k1,k2))])\n>>> print (numer)\n-15.545863836567879\nNote that we used the null hypothesis estimate for the \u02c6p0. Likewise, for the logarithm\nof the denominator we have the following:\n>>>denom =np.sum([np .log(binom(ni,pi) .pmf(ki))\n... forni,ki,pi in\n... zip((n0,n1,n2),(k0,k1,k2),(p0,p1,p2))])", "171": "160 3 Statistics\n>>> print (denom)\n-8.424106480792402\nNow, we can compute the logarithm of the \u039bstatistic as follows and see what the\ncorresponding value is according to Wilks theorem,\n>>>chsq=chi2( 2)\n>>>logLambda =-2*(numer -denom)\n>>> print (logLambda)\n14.243514711550954\n>>> print (1-chsq.cdf(logLambda))\n0.0008073467083287156\nBecause the value reported above is less than the 5% signi\ufb01cance level, we reject\nthe null hypothesis that all the coins have the same probability of heads. Note that\nthere are two degrees of freedom because the difference in the number of parameters\nbetween the null hypothesis ( p) and the alternative ( p1,p2,p3) is two. We can build\na quick Monte Carlo simulation to check the probability of detection for this example\nusing the following code, which is just a combination of the last few code blocks,\n>>>c=chsq .isf( .05)# 5% significance level\n>>>out =[]\n>>> for k0,k1,k2 ingen_sample( 100):\n... pH0 =sum((k0,k1,k2)) /sum((n0,n1,n2))\n... numer =np.sum([np .log(binom(ni,pH0) .pmf(ki))\n... for ni,ki in\n... zip((n0,n1,n2),(k0,k1,k2))])\n... denom =np.sum([np .log(binom(ni,pi) .pmf(ki))\n... for ni,ki,pi in\n... zip((n0,n1,n2),(k0,k1,k2),(p0,p1,p2))])\n... out.append( -2*(numer -denom) >c)\n...>>> print (np.mean(out)) # estimated probability of detection\n0.59\nThe above simulation shows the estimated probability of detection, for this set of\nexample parameters. This relative low probability of detection means that while\nthe test is unlikely (i.e., at the 5% signi\ufb01cance level) to mistakenly pick the nullhypothesis, it is likewise missing many of the H\n1cases (i.e., low probability of\ndetection). The trade-off between which is more important is up to the particular\ncontext of the problem. In some situations, we may prefer additional false alarms in\nexchange for missing fewer H1cases.\nPermutation Test . The Permutation Test is good way to test whether or not samples\ncome from the same distribution. For example, suppose that\nX1,X2,..., Xm\u223cF", "172": "3.5 Hypothesis Testing and P-V alues 161\nand also,\nY1,Y2,..., Yn\u223cG\nThat is, Yiand Xicome from different distributions. Suppose we have some test\nstatistic, for example\nT(X1,..., Xm,Y1,..., Yn)=|X\u2212Y|\nUnder the null hypothesis for which F=G,a n yo ft h e (n+m)!permutations are\nequally likely. Thus, suppose for each of the (n+m)!permutations, we have the\ncomputed statistic,\n{T1,T2,..., T(n+m)!}\nThen, under the null hypothesis, each of these values is equally likely. The distribution\nofTunder the null hypothesis is the permutation distribution that puts weight 1 /(n+\nm)!on each T-value. Suppose tois the observed value of the test statistic and assume\nthat large Trejects the null hypothesis, then the p-value for the permutation test is\nthe following:\nP(T>to)=1\n(n+m)!(n+m)!/summationdisplay\nj=1I(Tj>to)\nwhere I()is the indicator function. For large (n+m)!, we can sample randomly\nfrom the set of all permutations to estimate this p-value.\nExample . Let\u2019s return to our coin-\ufb02ipping example from last time, but now we have\nonly two coins. The hypothesis is that both coins have the same probability of heads.We can use the built-in function in Numpy to compute the random permutations.\n>>>x=binom( 10,0.3).rvs(5)# p=0.3\n>>>y=binom( 10,0.5).rvs(3)# p=0.5\n>>>z=np.hstack([x,y]) # combine into one array\n>>>t_o =abs(x.mean() -y.mean())\n>>>out =[]# output container\n>>> for kinrange (1000):\n... perm =np.random .permutation(z)\n... T=abs(perm[: len(x)].mean() -perm[ len(x):] .mean())\n... out.append((T >\nt_o))\n...\n>>> print ('p-value = ' ,n p .mean(out))\np-value = 0.0\nNote that the size of total permutation space is 8 != 40320 so we are taking relatively\nfew (i.e., 100) random permutations from this space.\nWald Test . The Wald Test is an asymptotic test. Suppose we have H0:\u03b8=\u03b80and\notherwise H1:\u03b8/negationslash=\u03b80, the corresponding statistic is de\ufb01ned as the following:", "173": "162 3 Statistics\nW=\u02c6\u03b8n\u2212\u03b80\nse\nwhere \u02c6\u03b8is the maximum likelihood estimator and seis the standard error,\nse=/radicalBig\nV(\u02c6\u03b8n)\nUnder general conditions, Wd\u2192 N(0,1). Thus, an asymptotic test at level \u03b1rejects\nwhen|W|>z\u03b1/2where z\u03b1/2corresponds to P(|Z|>z\u03b1/2)=\u03b1with Z\u223cN(0,1).\nFor our favorite coin-\ufb02ipping example, if H0:\u03b8=\u03b80, then\nW=\u02c6\u03b8\u2212\u03b80/radicalBig\n\u02c6\u03b8(1\u2212\u02c6\u03b8)/n\nWe can simulate this using the following code at the usual 5% signi\ufb01cance level,\n>>> from scipy import stats\n>>> theta0 = 0.5 #H 0\n>>> k=np.random .binomial( 1000,0.3)\n>>> theta_hat =k/1000. # MLE\n>>> W=(theta_hat -theta0) /np.sqrt(theta_hat *(1-theta_hat) /1000 )\n>>> c=stats .norm() .isf(0.05/2 )# z_{alpha/2}\n>>> print (abs(W)>c)# if true, reject H0\nTrue\nThis rejects H0because the true \u03b8=0.3 and the null hypothesis is that \u03b8=0.5.\nNote that n=1000 in this case which puts us well inside the asymptotic range of\nthe result. We can re-do this example to estimate the detection probability for thisexample as in the following code:\n>>> theta0 = 0.5 #H 0\n>>> c=stats .norm() .isf( 0.05/2. )# z_{alpha/2}\n>>> out =[]\n>>> for iinrange (100):\n... k=np.random .binomial( 1000 ,0.3)\n... theta_hat =k/1000. # MLE\n... W=(theta_hat -theta0) /np.sqrt(theta_hat *(1-theta_hat) /1000. )\n... out.append( abs(W)>c)# if true, reject H0\n...\n>>> print (np.mean(out)) # detection probability\n1.0", "174": "3.5 Hypothesis Testing and P-V alues 163\n3.5.5 T esting Multiple Hypotheses\nThus far, we have focused primarily on two competing hypotheses. Now, we con-\nsider multiple comparisons. The general situation is the following. We test the nullhypothesis against a sequence of ncompeting hypotheses H\nk. We obtain p-values\nfor each hypothesis so now we have multiple p-values to consider {pk}. To boil this\nsequence down to a single criterion, we can make the following argument. Givennindependent hypotheses that are all untrue, the probability of getting at least one\nfalse alarm is the following:\nP\nFA=1\u2212(1\u2212p0)n\nwhere p0is the individual p-value threshold (say, 0.05). The problem here is that\nPFA\u21921a s n\u2192\u221e . If we want to make many comparisons at once and control the\noverall false alarm rate the overall p-value should be computed under the assumptionthat none of the competing hypotheses is valid. The most common way to address\nthis is with the Bonferroni correction which says that the individual signi\ufb01cance level\nshould be reduced to p/n. Obviously, this makes it much harder to declare signif-\nicance for any particular hypothesis. The natural consequence of this conservative\nrestriction is to reduce the statistical power of the experiment, thus making it more\nlikely the true effects will be missed.\nIn 1995, Benjamini and Hochberg devised a simple method that tells which\np-values are statistically signi\ufb01cant. The procedure is to sort the list of p-values\nin ascending order, choose a false-discovery rate (say, q), and then \ufb01nd the largest\np-value in the sorted list such that p\nk\u2264kq/n, where kis the p-value\u2019s position in the\nsorted list. Finally, declare that pkvalue and all the others less than it statistically sig-\nni\ufb01cant. This procedure guarantees that the proportion of false positives is less than\nq(on average). The Benjamini\u2013Hochberg procedure (and its derivatives) is fast and\neffective and is widely used for testing hundreds of primarily false hypotheses when\nstudying genetics or diseases. Additionally, this procedure provides better statistical\npower than the Bonferroni correction.\n3.5.6 Fisher Exact T est\nContingency tables represent the partitioning of a sample population of two cate-\ngories between two different classi\ufb01cations as shown in the following Table 3.2.T h e\nTable 3.2 Example contingency table\nInfection No infection Total\nMale 13 11 24\nFemale 12 1 13\nTotal 25 12 37", "175": "164 3 Statistics\nquestion is whether or not the observed table corresponds to a random partition of\nthe sample population, constrained by the marginal sums. Note that because this is\na two-by-two table, a change in any of the table entries automatically affects all ofthe other terms because of the row and column sum constraints. This means that\nequivalent questions like \u201cUnder a random partition, what is the probability that a\nparticular table entry is at least as large as a given value?\u201d can be meaningfully posed.\nThe Fisher Exact Test addresses this question. The idea is to compute the probabil-\nity of a particular entry of the table, conditioned upon the marginal row and column\nsums,\nP(X\ni,j|r1,r2,c1,c2)\nwhere Xi,jis(i,j)table entry, r1represents the sum of the \ufb01rst row, r2represents the\nsum of the second row, c1represents the sum of the \ufb01rst column, and c2is the sum\nof the second column. This probability is given by the hypergeometric distribution .\nRecall that the hypergeometric distribution gives the probability of sampling (withoutreplacement) kitems from a population of Nitems consisting of exactly two different\nkinds of items,\nP(X=k)=/parenleftbig\nK\nk/parenrightbig/parenleftbigN\u2212K\nn\u2212k/parenrightbig\n/parenleftbigN\nn/parenrightbig\nwhere Nis the population size, Kis the total number of possible favorable draws, n\nis the number of draws, and kis the number of observed favorable draws. With the\ncorresponding identi\ufb01cation of variables, the hypergeometric distribution gives the\ndesired conditional probability: K=r1,k=x,n=c1,N=r1+r2.\nIn the example of the Table 3.2, the probability for x=13 male infections among\na population of r1=24 males in a total population of c1=25 infected persons,\nincluding r2=13 females. The scipy.stats module has the Fisher Exact Test\nimplemented as shown below:\n>>> import scipy.stats\n>>>table =[[13,11],[12,1]]\n>>>odds_ratio, p_value =scipy .stats .fisher_exact(table)\n>>> print (p_value)\n0.02718387758955712\nThe default for scipy.stats.fisher_exact is the two-sided test. The following\nresult is for the less option,\n>>> import scipy.stats\n>>>odds_ratio, p_value =scipy .stats .fisher_exact(table,alternative ='less' )\n>>> print (p_value)\n0.018976707519532877\nThis means that the p-value is computed by summing over the probabilities of contin-\ngency tables that are less extreme than the given table. To understand what this means,", "176": "3.5 Hypothesis Testing and P-V alues 165\nwe can use the scipy.stats.hypergeom function to compute the probabilities of\nthese with the number of infected men is less than or equal to 13.\n>>>hg=scipy .stats .hypergeom( 37,24,25)\n>>>probs =[(hg.pmf(i)) foriinrange (14)]\n>>> print (probs)\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n0.0014597467322717626, 0.017516960787261115]>>> print (sum(probs))\n0.018976707519532877\nThis is the same as the prior p-value result we obtained from scipy.stats.\nfisher_exact . Another option is greater which derives from the following anal-\nogous summation:\n>>>odds_ratio, p_value =scipy .stats .fisher_exact(table,alternative ='greater' )\n>>>probs =[hg.pmf(i) for iinrange (13,25)]\n>>> print (probs)\n[0.017516960787261115, 0.08257995799708828, 0.2018621195484381,0.28386860561499044, 0.24045340710916852, 0.12467954442697629,0.039372487713781906, 0.00738234144633414, 0.0007812001530512284,4.261091743915799e-05, 1.0105355914424832e-06, 7.017608273906114e-09]>>> print (p_value)\n0.9985402532677288>>> print (sum(probs))\n0.9985402532677288\nFinally, the two-sided version excludes those individual table probabilities that are\nless that of the given table\n>>>_,p_value =scipy .stats .fisher_exact(table)\n>>>probs =[h g.pmf(i) for iinrange (25)]\n>>> print (sum(i for iinprobs ifi<=hg.pmf(13)))\n0.027183877589557117>>> print (p_value)\n0.02718387758955712\nThus, for this particular contingency table, we could reasonably conclude that 13\ninfected males in this total population is statistically signi\ufb01cant with a p-value lessthan \ufb01ve percent.\nPerforming this kind of analysis for tables larger than 2x2 easily becomes compu-\ntationally challenging due to the nature of the underlying combinatorics and usually\nrequires specialized approximations.\nIn this section, we discussed the structure of statistical hypothesis testing and\nde\ufb01ned the various terms that are commonly used for this process, along with the\nillustrations of what they mean in our running coin-\ufb02ipping example. From an engi-\nneering standpoint, hypothesis testing is not as common as con\ufb01dence intervals andpoint estimates. On the other hand, hypothesis testing is very common in social and\nmedical science, where one must deal with practical constraints that may limit the", "177": "166 3 Statistics\nsample size or other aspects of the hypothesis testing rubric. In engineering, we can\nusually have much more control over the samples and models we employ because\nthey are typically inanimate objects that can be measured repeatedly and consistently.This is obviously not so with human studies, which generally have other ethical and\nlegal considerations.\n3.6 Con\ufb01dence Intervals\nIn a previous coin-\ufb02ipping discussion, we discussed estimation of the underlying\nprobability of getting a heads. There, we derived the estimator as\n\u02c6pn=1\nnn/summationdisplay\ni=1Xi\nwhere Xi\u2208{0,1}. Con\ufb01dence intervals allow us to estimate how close we can get\nto the true value that we are estimating. Logically, that seems strange, doesn\u2019t it? Wereally don\u2019t know the exact value of what we are estimating (otherwise, why estimate\nit?), and yet, somehow we know how close we can get to something we admit we\ndon\u2019t know? Ultimately, we want to make statements like the probability of the value\nin a certain interval is 90% . Unfortunately, that is something we will not be able to\nsay using our methods. Note that Bayesian estimation gets closer to this statement\nby using credible intervals , but that is a story for another day. In our situation, the\nbest we can do is say roughly the following: if we ran the experiment multiple times,\nthen the con\ufb01dence interval would trap the true parameter 90% of the time .\nLet\u2019s return to our coin-\ufb02ipping example and see this in action. One way to get at\na con\ufb01dence interval is to use Hoeffding\u2019s inequality from Sect. 2.11.3 specialized\nto our Bernoulli variables as\nP(|\u02c6p\nn\u2212p|>/epsilon1)\u22642e x p(\u22122n/epsilon12)\nNow, we can form the interval I=[ \u02c6 pn\u2212/epsilon1n,\u02c6pn+/epsilon1n], where /epsilon1nis carefully\nconstructed as\n/epsilon1n=/radicalbigg\n1\n2nlog2\n\u03b1\nwhich makes the right side of the Hoeffding inequality equal to \u03b1. Thus, we \ufb01nally\nhave\nP(p/\u2208I)=P/parenleftbig\n|\u02c6pn\u2212p|>/epsilon1n/parenrightbig\n\u2264\u03b1\nThus, P(p\u2208I)\u22651\u2212\u03b1. As a numerical example, let\u2019s take n=100,\u03b1=0.05,\nthen plugging into everything we have given /epsilon1n=0.136. So, the 95% con\ufb01dence\ninterval here is therefore", "178": "3.6 Con\ufb01dence Intervals 167\nI=[\u02c6pn\u2212/epsilon1n,\u02c6pn+/epsilon1n]=[\u02c6 pn\u22120.136,\u02c6pn+0.136]\nThe following code sample is a simulation to see if we can really trap the under-\nlying parameter in our con\ufb01dence interval.\n>>> from scipy import stats\n>>> import numpy as np\n>>>b=stats .bernoulli( .5)# fair coin distribution\n>>>nsamples = 100\n>>> # flip it nsamples times for 200 estimates\n>>>xs=b.rvs(nsamples *200).reshape(nsamples, -1)\n>>>phat =np.mean(xs,axis =0)# estimated p\n>>> # edge of 95% confidence interval\n>>>epsilon_n =np.sqrt(np .log(2/0.05 )/2/nsamples)\n>>>pct=np.logical_and(phat -epsilon_n <=0.5 ,\n... 0.5 <= (epsilon_n +phat)\n... ).mean() *100\n>>> print ('Interval trapped correct value ' , pct, '%of the time' )\nInterval trapped correct value 99.5 % of the time\nThe result shows that the estimator and the corresponding interval was able to trap the\ntrue value at least 95% of the time. This is how to interpret the action of con\ufb01dence\nintervals.\nHowever, the usual practice is to not use Hoeffding\u2019s inequality and instead use\narguments around asymptotic normality. The de\ufb01nition of the standard error is thefollowing:\nse=/radicalBig\nV(\u02c6\u03b8n)\nwhere \u02c6\u03b8nis the point estimator for the parameter \u03b8,g i v e n nsamples of data Xn, and\nV(\u02c6\u03b8n)is the variance of \u02c6\u03b8n. Likewise, the estimated standard error is /hatwidese. For example,\nin our coin-\ufb02ipping example, the estimator was \u02c6p=/summationtextXi/nwith corresponding\nvariance V(\u02c6pn)=p(1\u2212p)/n. Plugging in the point estimate gives us the estimated\nstandard error: /hatwidese=/radicalbig\n\u02c6p(1\u2212\u02c6p)/n. Because maximum likelihood estimators are\nasymptotically normal,4we know that \u02c6pn\u223cN(p,/hatwidese2). Thus, if we want a 1 \u2212\u03b1\ncon\ufb01dence interval, we can compute\nP(|\u02c6pn\u2212p|<\u03be)> 1\u2212\u03b1\nbut since we know that (\u02c6pn\u2212p)is asymptotically normal, N(0,/hatwidese2), we can instead\ncompute/integraldisplay\u03be\n\u2212\u03beN(0,/hatwidese2)dx>1\u2212\u03b1\n4Certain technical regularity conditions must hold for this property of maximum likelihood estimator\nto work. See [ 2] for more details.", "179": "168 3 Statistics\nFig. 3.11 The gray circles\nare the point estimates thatare bounded above and belowby both asymptotic con\ufb01-dence intervals and Hoeffdingintervals. The asymptoticintervals are tighter becausethe underpinning asymptoticassumptions are valid for theseestimates\nThis looks ugly to compute because we need to \ufb01nd \u03be, but Scipy has everything we\nneed for this.\n>>> # compute estimated se for all trials\n>>>se=np.sqrt(phat *(1-phat) /xs.shape[ 0])\n>>> # generate random variable for trial 0\n>>>rv=stats .norm( 0, se[ 0])\n>>> # compute 95% confidence interval for that trial 0\n>>>np.array(rv .interval( 0.95))+phat[ 0]\narray([0.42208023, 0.61791977])>>> defcompute_CI (i):\n... return stats .norm.interval( 0.95,loc=i,\n... scale =np.sqrt(i *(1-i)/xs.shape[ 0]))\n...\n>>>lower,upper =compute_CI(phat)\nFigure 3.11 shows the asymptotic con\ufb01dence intervals and the Hoeffding-derived\ncon\ufb01dence intervals. As shown, the Hoeffding intervals are a bit more generous\nthan the asymptotic estimates. However, this is only true so long as the asymptotic\napproximation is valid. In other words, there exists some number of nsamples for\nwhich the asymptotic intervals may not work. So, even though they may be a bit\nmore generous, the Hoeffding intervals do not require arguments about asymptotic\nconvergence. In practice, nonetheless, asymptotic convergence is always in play\n(even if not explicitly stated).\nCon\ufb01dence Intervals and Hypothesis Testing . It turns out that there is a close\ndual relationship between hypothesis testing and con\ufb01dence intervals. To see this in\naction, consider the following hypothesis test for a normal distribution, H0:\u03bc=\u03bc0\nversus H1:\u03bc/negationslash=\u03bc0. A reasonable test has the following rejection region:\n/braceleftbigg\nx:| \u00afx\u2212\u03bc0|>z\u03b1/2\u03c3\u221an/bracerightbigg", "180": "3.6 Con\ufb01dence Intervals 169\nwhere P(Z>z\u03b1/2)=\u03b1/2 and P(\u2212z\u03b1/2<Z<z\u03b1/2)=1\u2212\u03b1and where\nZ\u223cN(0,1). This is the same thing as saying that the region corresponding to\nacceptance of H0is then,\n\u00afx\u2212z\u03b1/2\u03c3\u221an\u2264\u03bc0\u2264\u00afx+z\u03b1/2\u03c3\u221an(3.6.0.1)\nBecause the test has size \u03b1, the false alarm probability, P(H0rejected |\u03bc=\n\u03bc0)=\u03b1. Likewise, the P(H0accepted |\u03bc=\u03bc0)=1\u2212\u03b1. Putting this all\ntogether with interval de\ufb01ned above means that\nP/parenleftbigg\n\u00afx\u2212z\u03b1/2\u03c3\u221an\u2264\u03bc0\u2264\u00afx+z\u03b1/2\u03c3\u221an/vextendsingle/vextendsingle/vextendsingleH\n0/parenrightbigg\n=1\u2212\u03b1\nBecause this is valid for any \u03bc0, we can drop the H0condition and say the following:\nP/parenleftbigg\n\u00afx\u2212z\u03b1/2\u03c3\u221an\u2264\u03bc0\u2264\u00afx+z\u03b1/2\u03c3\u221an/parenrightbigg\n=1\u2212\u03b1\nAs may be obvious by now, the interval in Eq. 3.6.0.1 above isthe 1\u2212\u03b1con\ufb01-\ndence interval! Thus, we have just obtained the con\ufb01dence interval by inverting theacceptance region of the level \u03b1test. The hypothesis test \ufb01xes the parameter and then\nasks what sample values (i.e., the acceptance region) are consistent with that \ufb01xed\nvalue. Alternatively, the con\ufb01dence interval \ufb01xes the sample value and then askswhat parameter values (i.e., the con\ufb01dence interval) make this sample value most\nplausible. Note that sometimes this inversion method results in disjoint intervals\n(known as con\ufb01dence sets ).\n3.7 Linear Regression\nLinear regression gets to the heart of statistics: Given a set of data points, what is\nthe relationship of the data in hand to data yet seen? How should information from\none dataset propagate to other data? Linear regression offers the following model toaddress this question:\nE(Y|X=x)\u2248ax+b\nThat is, given speci\ufb01c values for X, assume that the conditional expectation is a linear\nfunction of those speci\ufb01c values. However, because the observed values are not the\nexpectations themselves, the model accommodates this with an additive noise term.In other words, the observed variable (a.k.a. response, target, dependent variable) is\nmodeled as\nE(Y|X=x\ni)+/epsilon1i\u2248ax+b+/epsilon1i=y", "181": "170 3 Statistics\nwhere E(/epsilon1i)=0 and the /epsilon1iare iid and where the distribution function of /epsilon1idepends\non the problem, even though it is often assumed Gaussian. The X=xvalues are\nknown as independent variables, covariates, or regressors.\nLet\u2019s see if we can use all of the methods we have developed so far to understand\nthis form of regression. The \ufb01rst task is to determine how to estimate the unknown\nlinear parameters, aandb. To make this concrete, let\u2019s assume that /epsilon1\u223cN(0,\u03c32).\nBear in mind that E(Y|X=x)is a deterministic function of x. In other words, the\nvariable xchanges with each draw, but after the data have been collected these are no\nlonger random quantities. Thus, for \ufb01xed x,yis a random variable generated by /epsilon1.\nPerhaps we should denote /epsilon1as/epsilon1xto emphasize this, but because /epsilon1is an independent,\nidentically distributed (iid) random variable at each \ufb01xed x, this would be excessive.\nBecause of Gaussian additive noise, the distribution of yis completely characterized\nby its mean and variance.\nE(y)=ax+b\nV(y)=\u03c32\nUsing the maximum likelihood procedure, we write out the log-likelihood function\nas\nL(a,b)=n/summationdisplay\ni=1log N(axi+b,\u03c32)\u221d1\n2\u03c32n/summationdisplay\ni=1(yi\u2212axi\u2212b)2\nNote that we suppressed the terms that are irrelevent to the maximum \ufb01nding. Taking\nthe derivative of this with respect to agives the following equation:\n\u2202L(a,b)\n\u2202a=2n/summationdisplay\ni=1xi(b+axi\u2212yi)=0\nLikewise, we do the same for the bparameter\n\u2202L(a,b)\n\u2202b=2n/summationdisplay\ni=1(b+axi\u2212yi)=0\nThe following code simulates some data and uses Numpy tools to compute the\nparameters as shown:\n>>> import numpy as np\n>>>a=6;b=1 # parameters to estimate\n>>>x=np.linspace( 0,1,100)\n>>>y=a*x+np.random .randn( len(x))+b\n>>>p,var_ =np.polyfit(x,y, 1,cov=True)# fit data to line\n>>>y_=np.polyval(p,x) # estimated by linear regression", "182": "3.7 Linear Regression 171\nFig. 3.12 The panel on the left shows the data and regression line. The panel on the right shows a\nhistogram of the regression errors\nThe graph on the left of Fig. 3.12 shows the regression line plotted against the data.\nThe estimated parameters are noted in the title. The histogram on the right of Fig. 3.12\nshows the residual errors in the model. It is always a good idea to inspect the residualsof any regression for normality. These are the differences between the \ufb01tted line for\neach x\nivalue and the corresponding yivalue in the data. Note that the xterm does\nnot have to be uniformly monotone.\nTo decouple the deterministic variation from the random variation, we can \ufb01x the\nindex and write separate problems of the form\nyi=axi+b+/epsilon1i\nwhere /epsilon1i\u223cN(0,\u03c32). What could we do with just this one component of the prob-\nlem? In other words, suppose we had m-samples of this component as in {yi,k}m\nk=1.\nFollowing the usual procedure, we could obtain estimates of the mean of yias\n\u02c6yi=1\nmm/summationdisplay\nk=1yi,k\nHowever, this tells us nothing about the individual parameters aandbbecause they\nare not separable in the terms that are computed, namely, we may have\nE(yi)=axi+b\nbut we still only have one equation and the two unknowns, aandb. How about if we\nconsider and \ufb01x another component jas in\nyj=axj+b+/epsilon1i\nThen, we have\nE(yj)=axj+b", "183": "172 3 Statistics\nFig. 3.13 The \ufb01tted and true lines are plotted with the data values. The squares at either end of the\nsolid line show the mean value for each of the data groups shown\nso at least now we have two equations and two unknowns and we know how to\nestimate the left-hand sides of these equations from the data using the estimators \u02c6yi\nand\u02c6yj. Let\u2019s see how this works in the code sample below (Fig. 3.13 ):\n>>>x0, xn =x[0],x[80]\n>>> # generate synthetic data\n>>>y_0 =a*x0+np.random .randn( 20)+b\n>>>y_1 =a*xn+np.random .randn( 20)+b\n>>> # mean along sample dimension\n>>>yhat =np.array([y_0,y_1]) .mean(axis =1)\n>>>a_,b_ =np.linalg .solve(np .array([[x0, 1],\n... [xn,1]]),yhat)\nProgramming Tip\nThe prior code uses the solve function in the Numpy linalg module, which\ncontains the core linear algebra codes in Numpy that incorporate the battle-tested LAPACK library.\nWe can write out the solution for the estimated parameters for this case where x0=0\n\u02c6a=\u02c6yi\u2212\u02c6y0\nxi\n\u02c6b=\u02c6y0", "184": "3.7 Linear Regression 173\nThe expectations and variances of these estimators are the following:\nE(\u02c6a)=axi\nxi=a\nE(\u02c6b)=b\nV(\u02c6a)=2\u03c32\nx2\ni\nV(\u02c6b)=\u03c32\nThe expectations show that the estimators are unbiased. The estimator \u02c6ahas a variance\nthat decreases as larger points xiare selected. That is, it is better to have samples\nfurther out along the horizontal axis for \ufb01tting the line. This variance quanti\ufb01es the\nleverage of those distant points.\nRegression From Projection Methods . Let\u2019s see if we can apply our knowledge of\nprojection methods to the general case. In vector notation, we can write the following:\ny=ax+b1+/epsilon1\nwhere 1is the vector of all ones. Let\u2019s use the inner product notation,\n/angbracketleftx,y/angbracketright= E(xTy)\nThen, by taking the inner product with some x1\u22081\u22a5we obtain,5\n/angbracketlefty,x1/angbracketright=a/angbracketleftx,x1/angbracketright\nRecall that E(/epsilon1)=0. We can \ufb01nally solve for aas\n\u02c6a=/angbracketlefty,x1/angbracketright\n/angbracketleftx,x1/angbracketright(3.7.0.1)\nThat was pretty neat but now we have the mysterious x1vector. Where does this\ncome from? If we project xonto the 1\u22a5, then we get the MMSE approximation to x\nin the 1\u22a5space. Thus, we take\nx1=P1\u22a5(x)\nRemember that P1\u22a5is a projection matrix so the length of x1is at most x. This means\nthat the denominator in the \u02c6aequation above is really just the length of the xvector\nin the coordinate system of P1\u22a5. Because the projection is orthogonal (namely, of\nminimum length), the Pythagorean theorem gives this length as the following:\n/angbracketleftx,x1/angbracketright2=/angbracketleftx,x/angbracketright\u2212/angbracketleft 1,x/angbracketright2\n5The space of all vectors, asuch that /angbracketlefta,1/angbracketright= 0 is denoted 1\u22a5.", "185": "174 3 Statistics\nThe \ufb01rst term on the right is the length of the xvector and last term is the length of xin\nthe coordinate system orthogonal to P1\u22a5, namely, that of 1. We can use this geometric\ninterpretation to understand what is going on in typical linear regression in much moredetail. The fact that the denominator is the orthogonal projection of xtells us that the\nchoice of x\n1has the strongest effect (i.e., largest value) on reducing the variance of \u02c6a.\nThat is, the more xis aligned with 1, the worse the variance of \u02c6a. This makes intuitive\nsense because the closer xis to 1, the more constant it is, and we have already seen\nfrom our one-dimensional example that distance between the xterms pays off in\nreduced variance. We already know that \u02c6ais an unbiased estimator, and, because we\nchose x1deliberately as a projection, we know that it is also of minimum variance.\nSuch estimators are known as minimum-variance unbiased estimators (MVUE).\nIn the same spirit, let\u2019s examine the numerator of \u02c6ain Eq. 3.7.0.1 . We can write\nx1as the following:\nx1=x\u2212P1x\nwhere P1is projection matrix of xonto the 1vector. Using this, the numerator of \u02c6a\nbecomes\n/angbracketlefty,x1/angbracketright=/angbracketleft y,x/angbracketright\u2212/angbracketleft y,P1x/angbracketright\nNote that,\nP1=11T1\nn\nso that writing this out explicitly gives\n/angbracketlefty,P1x/angbracketright=/parenleftBig\nyT1/parenrightBig/parenleftBig\n1Tx/parenrightBig\n/n=/parenleftBig/summationdisplay\nyi/parenrightBig/parenleftBig/summationdisplay\nxi/parenrightBig\n/n\nand similarly, we have the following for the denominator:\n/angbracketleftx,P1x/angbracketright=/parenleftBig\nxT1/parenrightBig/parenleftBig\n1Tx/parenrightBig\n/n=/parenleftBig/summationdisplay\nxi/parenrightBig/parenleftBig/summationdisplay\nxi/parenrightBig\n/n\nSo, plugging all of this together gives the following:\n\u02c6a=xTy\u2212(/summationtextxi)(/summationtextyi)/n\nxTx\u2212(/summationtextxi)2/n\nwith corresponding variance,\nV(\u02c6a)=\u03c32/bardblx1/bardbl2\n/angbracketleftx,x1/angbracketright2\n=\u03c32\n/bardblx/bardbl2\u2212n(x2)", "186": "3.7 Linear Regression 175\nUsing the same approach with \u02c6bgives\n\u02c6b=/angbracketlefty,x\u22a5/angbracketright\n/angbracketleft1,x\u22a5/angbracketright(3.7.0.2)\n=/angbracketlefty,1\u2212Px(1)/angbracketright\n/angbracketleft1,1\u2212Px(1)/angbracketright(3.7.0.3)\n=xTx(/summationtextyi)/n\u2212xTy(/summationtextxi)/n\nxTx\u2212(/summationtextxi)2/n(3.7.0.4)\nwhere\nPx=xxT\n/bardblx/bardbl2\nwith variance\nV(\u02c6b)=\u03c32/angbracketleft1\u2212Px(1),1\u2212Px(1)/angbracketright\n/angbracketleft1,1\u2212Px(1)/angbracketright2\n=\u03c32\nn\u2212(nx)2\n/bardblx/bardbl2\nQualifying the Estimates . Our formulas for the variance above include the unknown\n\u03c32, which we must estimate from the data itself using our plug-in estimates. We can\nform the residual sum of squares as\nRSS=/summationdisplay\ni(\u02c6axi+\u02c6b\u2212yi)2\nThus, the estimate of \u03c32can be expressed as\n\u02c6\u03c32=RSS\nn\u22122\nwhere nis the number of samples. This is also known as the residual mean square .T h e\nn\u22122 represents the degrees of freedom (df). Because we estimated two parameters\nfrom the same data we have n\u22122 instead of n. Thus, in general, df=n\u2212p,\nwhere pis the number of estimated parameters. Under the assumption that the noise\nis Gaussian, the RSS/\u03c32is chi-squared distributed with n\u22122 degrees of freedom.\nAnother important term is the sum of squares about the mean , (a.k.a corrected sum\nof squares),\nSYY=/summationdisplay\n(yi\u2212\u00afy)2\nTheSYY captures the idea of not using the xidata and just using the mean of the yi\ndata to estimate y. These two terms lead to the R2term,", "187": "176 3 Statistics\nR2=1\u2212RSS\nSYY\nNote that for perfect regression, R2=1. That is, if the regression gets each yidata\npoint exactly right, then RSS=0 this term equals one. Thus, this term is used to\nmeasure of goodness-of-\ufb01t. The stats module in scipy computes many of these\nterms automatically,\nfrom scipy import stats\nslope,intercept,r_value,p_value,stderr =stats .linregress(x,y)\nwhere the square of the r_value variable is the R2above. The computed p-value is\nthe two-sided hypothesis test with a null hypothesis that the slope of the line is zero.\nIn other words, this tests whether or not the linear regression makes sense for the data\nfor that hypothesis. The Statsmodels module provides a powerful extension to Scipy\u2019sstats module by making it easy to do regression and keeps track of these parameters.\nLet\u2019s reformulate our problem using the Statsmodels framework by creating a Pandas\ndataframe for the data,\nimport statsmodels.formula.api as smf\nfrom pandas import DataFrame\nimport numpy as np\nd=DataFrame({ 'x':np.linspace( 0,1,10)}) # create data\nd['y']=a*d.x+b+np.random .randn( *d.x.shape)\nNow that we have the input data in the above Pandas dataframe, we can perform the\nregression as in the following:\nresults =smf.ols('y \u02dc x' , data =d).fit()\nThe\u223csymbol is notation for y=ax+b+/epsilon1, where the constant bis implicit in\nthis usage of Statsmodels. The names in the string are taken from the columns in thedataframe. This makes it very easy to build models with complicated interactions\nbetween the named columns in the dataframe. We can examine a report of the model\n\ufb01t by looking at the summary,\nprint (results .summary2())\nResults: Ordinary least squares\n=================================================================Model: OLS Adj .R-squared: 0.808\nDependent Variable: y AIC: 28.1821\nDate: 0000-00-00 00 :00BIC: 00.0000\nNo.Observations: 10 Log-Likelihood: -12.091\nDf Model: 1 F-statistic: 38.86\nDf Residuals: 8 Prob (F -statistic): 0.000250\nR-squared: 0.829 Scale: 0.82158\n-------------------------------------------------------------------\nCoef . Std.Err. tP >|t| [0.025 0.975 ]\n-------------------------------------------------------------------Intercept 1.5352 0.5327 2.8817 0.0205 0.3067 2.7637\nx 5.5990 0.8981 6.2340 0.0003 3.5279 7.6701", "188": "3.7 Linear Regression 177\nThere is a lot more here than we have discussed so far, but the Statsmodels doc-\numentation is the best place to go for complete information about this report. The\nF-statistic attempts to capture the contrast between including the slope parameter orleaving it off. That is, consider two hypotheses:\nH\n0:E(Y|X=x)=b\nH1:E(Y|X=x)=b+ax\nIn order to quantify how much better adding the slope term is for the regression, we\ncompute the following:\nF=SYY\u2212RSS\n\u02c6\u03c32\nThe numerator computes the difference in the residual squared errors between includ-\ning the slope in the regression or just using the mean of the yivalues. Once again, if we\nassume (or can claim asymptotically) that the /epsilon1noise term is Gaussian, /epsilon1\u223cN(0,\u03c32),\nthen the H0hypothesis will follow an F-distribution6with degrees of freedom from\nthe numerator and denominator. In this case, F\u223cF(1,n\u22122). The value of this statis-\ntic is reported by Statsmodels above. The corresponding reported probability showsthe chance of Fexceeding its computed value if H\n0were true. So, the take-home\nmessage from all this is that including the slope leads to a much smaller reduction\nin squared error than could be expected from a favorable draw of npoints of this\ndata, under the Gaussian additive noise assumption. This is evidence that including\nthe slope is meaningful for this data.\nThe Statsmodels report also shows the adjusted R2term. This is a correction to\ntheR2calculation that accounts for the number of parameters pthat the regression\nis \ufb01tting and the sample size n,\nAdjusted R2=1\u2212RSS/(n\u2212p)\nSYY/(n\u22121)\nThis is always lower than R2except when p=1 (i.e., estimating only b). This\nbecomes a better way to compare regressions when one is attempting to \ufb01t many\nparameters with comparatively small n.\nLinear Prediction . Using linear regression for prediction introduces some other\nissues. Recall the following expectation:\nE(Y|X=x)\u2248\u02c6ax+\u02c6b\nwhere we have determined \u02c6aand\u02c6bfrom the data. Given a new point of interest, xp,\nwe would certainly compute\n\u02c6yp=\u02c6axp+\u02c6b\n6The F(m,n)F-distribution has two integer degree-of-freedom parameters, mandn.", "189": "178 3 Statistics\nas the predicted value for \u02c6yp. This is the same as saying that our best prediction\nforybased on xpis the above conditional expectation. The variance for this is the\nfollowing:\nV(yp)=x2\npV(\u02c6a)+V(\u02c6b)+2xpcov(\u02c6a\u02c6b)\nNote that we have the covariance above because \u02c6aand\u02c6bare derived from the same\ndata. We can work this out below using our previous notation from Eq. 3.7.0.1 ,\ncov(\u02c6a\u02c6b)=xT\n1V{yyT}x\u22a5\n(xT\n1x)(1Tx\u22a5)=xT\n1\u03c32Ix\u22a5\n(xT\n1x)(1Tx\u22a5)\n=\u03c32xT\n1x\u22a5\n(xT\n1x)(1Tx\u22a5)=\u03c32(x\u2212P1x)Tx\u22a5\n(xT\n1x)(1Tx\u22a5)\n=\u03c32\u2212xTPT\n1x\u22a5\n(xT\n1x)(1Tx\u22a5)=\u03c32\u2212xT1\nn11Tx\u22a5\n(xT\n1x)(1Tx\u22a5)\n=\u03c32\u2212xT1\nn1\n(xT\n1x)=\u2212\u03c32x/summationtextn\ni=1(x2\ni\u2212x2)\nAfter plugging all this in, we obtain the following:\nV(yp)=\u03c32x2\np\u22122xpx+/bardblx/bardbl2/n\n/bardblx/bardbl2\u2212nx2\nwhere, in practice, we use the plug-in estimate for the \u03c32.\nThere is an important consequence for the con\ufb01dence interval for yp. We cannot\nsimply use the square root of V(yp)to form the con\ufb01dence interval because the\nmodel includes the extra /epsilon1noise term. In particular, the parameters were computed\nusing a set of statistics from the data, but now must include different realizations for\nthe noise term for the prediction part. This means we have to compute\n\u03b72=V(yp)+\u03c32\nThen, the 95% con\ufb01dence interval yp\u2208(yp\u22122\u02c6\u03b7,yp+2\u02c6\u03b7)is the following:\nP(yp\u22122\u02c6\u03b7<yp<yp+2\u02c6\u03b7)\u2248P(\u22122<N(0,1)< 2)\u22480.95\nwhere \u02c6\u03b7comes from substituting the plug-in estimate for \u03c3.\n3.7.1 Extensions to Multiple Covariates\nWith all the machinery we have, it is a short notational hop to consider multiple\nregressors as in the following:", "190": "3.7 Linear Regression 179\nY=X\u03b2+/epsilon1\nwith the usual E(/epsilon1)=0and V(/epsilon1)=\u03c32I. Thus, Xis a n\u00d7pfull rank matrix of\nregressors and Yis the n-vector of observations. Note that the constant term has been\nincorporated into Xas a column of ones. The corresponding estimated solution for\n\u03b2is the following:\n\u02c6\u03b2=(XTX)\u22121XTY\nwith corresponding variance,\nV(\u02c6\u03b2)=\u03c32(XTX)\u22121\nand with the assumption of Gaussian errors, we have\n\u02c6\u03b2\u223cN(\u03b2,\u03c32(XTX)\u22121)\nThe unbiased estimate of \u03c32is the following:\n\u02c6\u03c32=1\nn\u2212p/summationdisplay\n\u02c6/epsilon12\ni\nwhere \u02c6/epsilon1=X\u02c6\u03b2\u2212Yis the vector of residuals. Tukey christened the following matrix\nas the hatmatrix (a.k.a. in\ufb02uence matrix):\nV=X(XTX)\u22121XT\nbecause it maps Yinto\u02c6Y,\n\u02c6Y=VY\nAs an exercise you can check that Vis a projection matrix. Note that that matrix is\nsolely a function of X. The diagonal elements of Vare called the leverage values and\nare contained in the closed interval [1/n,1]. These terms measure distance between\nthe values of xiand the mean values over the nobservations. Thus, the leverage terms\ndepend only on X. This is the generalization of our initial discussion of leverage where\nwe had multiple samples at only two xipoints. Using the hat matrix, we can compute\nthe variance of each residual, ei=\u02c6y\u2212yias\nV(ei)=\u03c32(1\u2212vi)\nwhere vi=Vi,i. Given the abovementioned bounds on vi, these are always less than\n\u03c32.\nDegeneracy in the columns of Xcan become a problem. This is when two or\nmore of the columns become co-linear. We have already seen this with our singleregressor example wherein xclose to 1was bad news. To compensate for this effect", "191": "180 3 Statistics\nwe can load the diagonal elements and solve for the unknown parameters as in the\nfollowing:\n\u02c6\u03b2=(XTX+\u03b1I)\u22121XTY\nwhere \u03b1>0 is a tunable hyper-parameter. This method is known as ridge regression\nand was proposed in 1970 by Hoerl and Kenndard. It can be shown that this is theequivalent to minimizing the following objective:\n/bardblY\u2212X\u03b2/bardbl\n2+\u03b1/bardbl\u03b2/bardbl2\nIn other words, the length of the estimated \u03b2is penalized with larger \u03b1. This has the\neffect of stabilizing the subsequent inverse calculation and also providing a means\nto trade bias and variance, which we will discuss at length in Sect. 4.8.\nInterpreting Residuals . Our model assumes an additive Gaussian noise term. We\ncan check the voracity of this assumption by examining the residuals after \ufb01tting.\nThe residuals are the difference between the \ufb01tted values and the original data\n\u02c6/epsilon1i=\u02c6axi+\u02c6b\u2212yi\nWhile the p-value and the F-ratio provide some indication of whether or not comput-\ning the slope of the regression makes sense, we can get directly at the key assumption\nof additive Gaussian noise.\nFor suf\ufb01ciently small dimensions, the scipy.stats.probplot we discussed in\nthe last chapter provides quick visual evidence one way or another by plotting the\nstandardized residuals,\nri=ei\n\u02c6\u03c3\u221a1\u2212vi\nThe other part of the iid assumption implies homoscedasticity (all rihave equal\nvariances). Under the additive Gaussian noise assumption, the eishould also be\ndistributed according to N(0,\u03c32(1\u2212vi)). The normalized residuals rishould then\nbe distributed according to N(0,1). Thus, the presence of any ri/\u2208[ \u2212 1.96,1.96]\nshould not be common at the 5% signi\ufb01cance level and is thereby breeds suspicionregarding the homoscedasticity assumption.\nThe Levene test in scipy.stats.leven tests the null hypothesis that all the\nvariances are equal. This basically checks whether or not the standardized residuals\nvary across x\nimore than expected. Under the homoscedasticity assumption, the\nvariance should be independent of xi. If not, then this is a clue that there is a missing\nvariable in the analysis or that the variables themselves should be transformed (e.g.,\nusing the log function) into another format that can reduce this effect. Also, we can\nuse weighted least-squares instead of ordinary least-squares.\nVariable Scaling . It is tempting to conclude in a multiple regression that small\ncoef\ufb01cients in any of the \u03b2terms implies that those terms are not important. However,\nsimple unit conversions can cause this effect. For example, if one of the regressors", "192": "3.7 Linear Regression 181\nFig. 3.14 The point on the\nright has outsized in\ufb02uencein this data because it is theonly one used to determine theslope of the \ufb01tted line\nis in units of kilometers and the others are in meters, then just the scale factor can\ngive the impression of outsized or under-sized effects. The common way to accountfor this is to scale the regressors so that\nx\n/prime=x\u2212\u00afx\n\u03c3x\nThis has the side effect of converting the slope parameters into correlation coef\ufb01-\ncients, which is bounded by \u00b11.\nIn\ufb02uential Data . We have already discussed the idea of leverage. The concept\nofin\ufb02uence combines leverage with outliers. To understand in\ufb02uence, consider\nFig. 3.14 .\nThe point on the right in Fig. 3.14 is the only one that contributes to the calculation\nof the slope for the \ufb01tted line. Thus, it is very in\ufb02uential in this sense. Cook\u2019s distance\nis a good way to get at this concept numerically. To compute this, we have to computethej\nthcomponent of the estimated target variable with the ithpoint deleted. We call\nthis\u02c6yj(i). Then, we compute the following:\nDi=/summationtext\nj(\u02c6yj\u2212\u02c6yj(i))2\np/n/summationtext\nj(\u02c6yj\u2212yj)2\nwhere, as before, pis the number of estimated terms (e.g., p=2 in the bivariate\ncase). This calculation emphasizes the effect of the outlier by predicting the target\nvariable with and without each point. In the case of Fig. 3.14 , losing any of the points\non the left cannot change the estimated target variable much, but losing the single\npoint on the right surely does. The point on the right does not seem to be an outlier\n(itison the \ufb01tted line), but this is because it is in\ufb02uential enough to rotate the line\nto align with it. Cook\u2019s distance helps capture this effect by leaving each sample\nout and re-\ufb01tting the remainder as shown in the last equation. Figure 3.15 shows the", "193": "182 3 Statistics\nFig. 3.15 The calculated\nCook\u2019s distance for the data inFig. 3.14\ncalculated Cook\u2019s distance for the data in Fig. 3.14 , showing that the data point on the\nright (sample index 5) has outsized in\ufb02uence on the \ufb01tted line. As a rule of thumb,\nCook\u2019s distance values that are larger than one are suspect.\nAs another illustration of in\ufb02uence, consider Fig. 3.16 which shows some data\nthat nicely line up, but with one outlier (\ufb01lled black circle) in the upper panel. The\nlower panel shows so-computed Cook\u2019s distance for this data and emphasizes the\npresence of the outlier. Because the calculation involves leaving a single sample outand re-calculating the rest, it can be a time-consuming operation suitable to relatively\nsmall datasets. There is always the temptation to downplay the importance of outliers\nbecause they con\ufb02ict with a favored model, but outliers must be carefully examined\nto understand why the model is unable to capture them. It could be something as\nsimple as faulty data collection, or it could be an indication of deeper issues that havebeen overlooked. The following code shows how Cook\u2019s distance was compute for\nFigs. 3.15 and 3.16 .\n>>>fit =lambda i,x,y: np .polyval(np .polyfit(x,y, 1),i)\n>>>omit =lambda i,x: ([k for j,k inenumerate (x) ifj!=i])\n>>> def cook_d (k):\n... num =sum((fit(j,omit(k,x),omit(k,y)) -fit(j,x,y)) **2 for jinx)\n... den =sum((y-np.polyval(np .polyfit(x,y, 1),x)) **2/ len(x)*2)\n... return num/den\n...\nProgramming Tip\nThe function omit sweeps through the data and excludes the ithdata element.\nThe embedded enumerate function associates every element in the iterable\nwith its corresponding index.", "194": "3.8 Maximum A-Posteriori 183\nFig. 3.16 The upper panel\nshows data that \ufb01t on a lineand an outlier point (\ufb01lledblack circle). The lower panelshows the calculated Cook\u2019sdistance for the data in upperpanel and shows that the tenthpoint (i.e., the outlier) hasdisproportionate in\ufb02uence\n3.8 Maximum A-Posteriori\nWe saw with maximum likelihood estimation how we could use the principle of\nmaximum likelihood to derive a formula of the data that would estimate the underly-\ning parameters (say, \u03b8). Under that method, the parameter was \ufb01xed, but unknown.\nIf we change our perspective slightly and consider the underlying parameter as arandom variable in its own right, this leads to additional \ufb02exibility in estimation.\nThis method is the simplest of the family of Bayesian statistical methods and is most\nclosely related to maximum likelihood estimation. It is very popular in communi-cations and signal processing and is the backbone of many important algorithms in\nthose areas.\nGiven that the parameter \u03b8is also a random variable, it has a joint distribution\nwith the other random variables, say, f(x,\u03b8). Bayes\u2019 theorem gives the following:\nP(\u03b8|x)=P(x|\u03b8)P(\u03b8)\nP(x)\nThe P(x|\u03b8)term is the usual likelihood term we have seen before. The term in the\ndenominator is prior probability of the data xand it explicitly makes a very powerful\nclaim: even before collecting or processing any data, we know what the probability\nof that data is. The P(\u03b8)is the prior probability of the parameter. In other words,\nregardless of the data that is collected, this is the probability of the parameter itself.\nIn a particular application, whether or not you feel justi\ufb01ed making these claims is\nsomething that you have to reconcile for yourself and the problem at hand. There are\nmany persuasive philosophical arguments one way or the other, but the main thingto keep in mind when applying any method is whether or not the assumptions are\nreasonable for the problem at hand.\nHowever, for now, let\u2019s just assume that we somehow have P(\u03b8)and the next step\nis the maximizing of this expression over the \u03b8. Whatever results from that maximiza-", "195": "184 3 Statistics\ntion is the maximum a-posteriori (MAP) estimator for \u03b8. Because the maximization\ntakes place with respect to \u03b8and not x, we can ignore the P(x)part. To make things\nconcrete, let us return to our original coin-\ufb02ipping problem. From our earlier analysis,we know that the likelihood function for this problem is the following:\n/lscript(\u03b8):=\u03b8\nk(1\u2212\u03b8)(n\u2212k)\nwhere the probability of the coin coming up heads is \u03b8. The next step is the prior\nprobability, P(\u03b8). For this example, we will choose the \u03b2(6,6)distribution (shown in\nthe top left panel of Fig. 3.17 ). The \u03b2family of distributions is a gold mine because\nit allows for a wide variety of distributions using few input parameters. Now that\nwe have all the ingredients, we turn to maximizing the posterior function, P(\u03b8|x).\nBecause the logarithm is convex, we can use it to make the maximization process\neasier by converting the product to a sum without changing the extrema that we are\nlooking for. Thus, we prefer to work with the logarithm of P(\u03b8|x)as in the following:\nL:=log P(\u03b8|x)=log/lscript(\u03b8)+log P(\u03b8)\u2212log P(x)\nThis is tedious to do by hand and therefore an excellent job for Sympy.\n>>> import sympy\n>>> from sympy import stats asst\n>>> from sympy.abc import p,k,n\n# setup objective function using sympy.log\n>>>obj=sympy .expand_log(sympy .log(p **k*(1-p)**(n-k)*\nst.density(st .Beta( 'p',6,6))(p)))\n# use calculus to maximize objective\n>>>sol=sympy .solve(sympy .simplify(sympy .diff(obj,p)),p)[ 0]\n>>>sol\n(k+5)/(n+1 0 )\nwhich means that our MAP estimator of \u03b8is the following:\n\u02c6\u03b8MAP=k+5\nn+10\nwhere kis the number of heads in the sample. This is obviously a biased estimator\nof\u03b8,\nE(\u02c6\u03b8MAP)=5+n\u03b8\n10+n/negationslash=\u03b8\nBut is this bias bad? Why would anyone want a biased estimator? Remember that\nwe constructed this entire estimator using the idea of the prior probability of P(\u03b8)\nwhich favors (biases!) the estimate according to the prior. For example, if \u03b8=1/2,\nthe MAP estimator evaluates to \u02c6\u03b8MAP=1/2. No bias there! This is because the\npeak of the prior probability is at \u03b8=1/2.", "196": "3.8 Maximum A-Posteriori 185\nTo compute the corresponding variance for this estimator, we need this interme-\ndiate result,\nE(\u02c6\u03b82\nMAP)=25+10n\u03b8+n\u03b8((n\u22121)p+1)\n(10+n)2\nwhich gives the following variance:\nV(\u02c6\u03b8MAP)=n(1\u2212\u03b8)\u03b8\n(n+10)2\nLet\u2019s pause and compare this to our previous maximum likelihood (ML) estimator\nshown below:\n\u02c6\u03b8ML=1\nnn/summationdisplay\ni=1Xi=k\nn\nAs we discussed before, the ML-estimator is unbiased with the following variance:\nV(\u02c6\u03b8ML)=\u03b8(1\u2212\u03b8)\nn\nHow does this variance compare to that of the MAP? The ratio of the two is the\nfollowing:\nV(\u02c6\u03b8MAP)\nV(\u02c6\u03b8ML)=n2\n(n+10)2\nThis ratio shows that the variance for the MAP estimator is smaller than that of\nthe ML-estimator. This is payoff for having a biased MAP estimator\u2014it requires\nfewer samples to estimate if the underlying parameter is consistent with the priorprobability. If not, then it will take more samples to pull the estimator away from the\nbias. In the limit as n\u2192\u221e , the ratio goes to one. This means that the bene\ufb01t of the\nreduced variance vanishes with enough samples.\nThe above discussion admits a level of arbitrariness via the prior distribution. We\ndon\u2019t have to choose just one prior, however. The following shows how we can use\nthe previous posterior distribution as the prior for the next posterior distribution:\nP(\u03b8|x\nk+1)=P(xk+1|\u03b8)P(\u03b8|xk)\nP(xk+1)\nThis is a very different strategy because we are using every data sample xkas a\nparameter for the posterior distribution instead of lumping all the samples togetherin a summation (this is where we got the kterm in the prior case). This case is much\nharder to analyze because now every incremental posterior distribution is itself a\nrandom function because of the injection of the xrandom variable. On the other\nhand, this is more in line with more general Bayesian methods because it is clear\nthat the output of this estimation process is a posterior distribution function, not just\na single parameter estimate.", "197": "186 3 Statistics\nFig. 3.17 The prior probability is the \u03b2(6,6)distribution shown in the top left panel. The dots near\nthe peaks of each of the subgraphs indicate the MAP estimate at that frame\nFigure 3.17 illustrates this method. The graph in the top row, far left shows the prior\nprobability ( \u03b2(6,6)) and the dot on the top shows the most recent MAP estimate for \u03b8.\nThus, before we obtain any data, the peak of the prior probability is the estimate. The\nnext graph to right shows the effect of x0=0 on the incremental prior probability.\nNote that the estimate has barely moved to the left. This is because the in\ufb02uence of\nthe data has not caused the prior probability to drift away from the original \u03b2(6,6)-\ndistribution. The \ufb01rst two rows of the \ufb01gure all have xk=0 just to illustrate how far\nleft the original prior probability can be moved by those data. The dots on the tops\nof the subgraphs show how the MAP estimate changes frame-by-frame as more data\nis incorporated. The remaining graphs, proceeding top-down and left-to-right, showthe incremental change in the prior probability for x\nk=1. Again, this shows how\nfar to the right the estimate can be pulled from where it started. For this example,\nthere are an equal number of xk=0 and xk=1 data, which correspond to \u03b8=1/2.", "198": "3.8 Maximum A-Posteriori 187\nProgramming Tip\nThe following is a quick paraphrase of how Fig. 3.17 was constructed. The \ufb01rst\nstep is to recursively create the posteriors from the data. Note the example datais sorted to make the progression easy to see as a sequence.\nfrom sympy.abc import p,x\nfrom scipy.stats import density, Beta, Bernoulli\nprior =density(Beta( 'p',6,6))(p)\nlikelihood =density(Bernoulli( 'x',p))(x)\ndata =(0,0,0,0,0,0,0,1,1,1,1,1,1,1,1)\nposteriors =[prior]\nforiindata:\nposteriors .append(posteriors[ -1]*likelihood .subs(x,i))\nWith the posteriors in hand, the next step is to compute the peak values at each\nframe using the fminbound function from Scipy\u2019s optimize module.\npvals =linspace( 0,1,100)\nmxvals =[]\nfori,j inzip(ax.flat,posteriors):\ni\n.plot(pvals,sympy .lambdify(p,j)(pvals),color ='k')\nmxval =fminbound(sympy .lambdify(p, -j),0,1)\nmxvals .append(mxval)\nh=i.axis()[ -1]\ni.axis(ymax =h*1.3)\ni.plot(mxvals[ -1],h*1.2,'ok')\ni.plot(mxvals[: -1],[h*1.2]*len(mxvals[: -1]),'o')\nFigure 3.18 is the same as Fig. 3.17 except that the initial prior probability is\nthe\u03b2(1.3,1.3)-distribution, which has a wider lobe that the \u03b2(6,6)-distribution. As\nshown in the \ufb01gure, this prior has the ability to be swayed more violently one way\nor the other based on the xkdata that is incorporated. This means that it can more\nquickly adapt to data that is not so consistent with the initial prior and thus does not\nrequire a large amount of data in order to unlearn the prior probability. Depending\non the application, the ability to unlearn the prior probability or stick with it is adesign problem for the analyst. In this example, because the data are representative\nof a\u03b8=1/2 parameter, both priors eventually settle on an estimated posterior that\nis about the same. However, if this had not been the case ( \u03b8/negationslash=1/2), then the second\nprior would have produced a better estimate for the same amount of data.\nBecause we have the entire posterior density available, we can compute something\nthat is closely related to the con\ufb01dence interval we discussed earlier, except in this\nsituation, given the Bayesian interpretation, it is called a credible interval orcredible\nset. The idea is that we want to \ufb01nd a symmetric interval around the peak that accounts", "199": "188 3 Statistics\nFig. 3.18 For this example, the prior probability is the \u03b2(1.3,1.3)distribution, which has a wider\nmain lobe than the \u03b2(6,6)distribution. The dots near the peaks of each of the subgraphs indicate\nthe MAP estimate at that frame\nfor 95% (say) of the posterior density. This means that we can then say the probability\nthat the estimated parameter is within the credible interval is 95%. The computationrequires signi\ufb01cant numerical processing because even though we have the posterior\ndensity in hand, it is hard to integrate analytically and requires numerical quadrature\n(see Scipy\u2019s integrate module). Figure 3.19 shows extent of the interval and the\nshaded region under the posterior density that accounts for 95%.\n3.9 Robust Statistics\nWe considered maximum likelihood estimation (MLE) and maximum a-posteriori\n(MAP) estimation and in each case we started out with a probability density function", "200": "3.9 Robust Statistics 189\nFig. 3.19 The credible inter-\nval in Bayesian maximum\na-posteriori is the intervalcorresponding to the shadedregion in the posterior density\nof some kind and we further assumed that the samples were identically distributed and\nindependent (iid). The idea behind robust statistics [ 3] is to construct estimators that\ncan survive the weakening of either or both of these assumptions. More concretely,\nsuppose you have a model that works great except for a few outliers. The temptation\nis to just ignore the outliers and proceed. Robust estimation methods provide adisciplined way to handle outliers without cherry-picking data that works for your\nfavored model.\nThe Notion of Location . The \ufb01rst notion we need is location , which is a generaliza-\ntion of the idea of central value . Typically, we just use an estimate of the mean for\nthis, but we will see later why this could be a bad idea. The general idea of location\nsatis\ufb01es the following requirements. Let Xbe a random variable with distribution F,\nand let \u03b8(X)be some descriptive measure of F. Then \u03b8(X)is said to be a measure\noflocation if for any constants aandb, we have the following:\n\u03b8(X+b)=\u03b8(X)+b (3.9.0.1)\n\u03b8(\u2212X)=\u2212\u03b8(X) (3.9.0.2)\nX\u22650\u21d2\u03b8(X)\u22650 (3.9.0.3)\n\u03b8(aX)=a\u03b8(X) (3.9.0.4)\nThe \ufb01rst condition is called location equivariance (orshift-invariance in signal pro-\ncessing lingo). The fourth condition is called scale equivariance , which means that\nthe units that Xis measured in should not effect the value of the location estimator.\nThese requirements capture the intuition of centrality of a distribution, or where most\nof the probability mass is located.\nFor example, the sample mean estimator is \u02c6\u03bc=1\nn/summationtextXi. The \ufb01rst requirement is\nobviously satis\ufb01ed as \u02c6\u03bc=1\nn/summationtext(Xi+b)=b+1\nn/summationtextXi=b+\u02c6\u03bc. Let us consider the\nsecond requirement :\u02c6\u03bc=1\nn/summationtext\u2212Xi=\u2212\u02c6\u03bc. Finally, the last requirement is satis\ufb01ed\nwith\u02c6\u03bc=1\nn/summationtextaXi=a\u02c6\u03bc.", "201": "190 3 Statistics\nRobust Estimation and Contamination . Now that we have the generalized location\nof centrality embodied in the location parameter, what can we do with it? Previously,\nwe assumed that our samples were all identically distributed. The key idea is that thesamples might be actually coming from a single distribution that is contaminated by\nanother nearby distribution, as in the following:\nF(X)=/epsilon1G(X)+(1\u2212/epsilon1)H(X)\nwhere /epsilon1randomly toggles between zero and one. This means that our data samples\n{X\ni}actually derived from two separate distributions, G(X)and H(X).W ej u s t\ndon\u2019t know how they are mixed together. What we really want is an estimator that\ncaptures the location of G(X)in the face of random intermittent contamination by\nH(X). For example, it may be that this contamination is responsible for the outliers\nin a model that otherwise works well with the dominant Fdistribution. It can get\neven worse than that because we don\u2019t know that there is only one contaminating\nH(X)distribution out there. There may be a whole family of distributions that are\ncontaminating G(X). This means that whatever estimators we construct have to be\nderived from a more generalized family of distributions instead of from a single\ndistribution, as the maximum likelihood method assumes. This is what makes robust\nestimation so dif\ufb01cult\u2014it has to deal with spaces of function distributions instead\nof parameters from a particular probability distribution.\nGeneralized Maximum Likelihood Estimators . M-estimators are generalized\nmaximum likelihood estimators. Recall that for maximum likelihood, we want tomaximize the likelihood function as in the following:\nL\n\u03bc(xi)=/productdisplay\nf0(xi\u2212\u03bc)\nand then to \ufb01nd the estimator \u02c6\u03bcso that\n\u02c6\u03bc=arg max\u03bcL\u03bc(xi)\nSo far, everything is the same as our usual maximum likelihood derivation except\nfor the fact that we don\u2019t assume a speci\ufb01c f0as the distribution of the {Xi}. Making\nthe de\ufb01nition of\n\u03c1=\u2212 logf0\nwe obtain the more convenient form of the likelihood product and the optimal \u02c6\u03bcas\n\u02c6\u03bc=arg min\u03bc/summationdisplay\n\u03c1(xi\u2212\u03bc)\nIf\u03c1is differentiable, then differentiating this with respect to \u03bcgives\n/summationdisplay\n\u03c8(xi\u2212\u02c6\u03bc)=0 (3.9.0.5)", "202": "3.9 Robust Statistics 191\nwith\u03c8=\u03c1/prime, the \ufb01rst derivative of \u03c1, and for technical reasons we will assume that \u03c8\nis increasing. So far, it looks like we just pushed some de\ufb01nitions around, but the key\nidea is we want to consider general \u03c1functions that may not be maximum likelihood\nestimators for any distribution. Thus, our focus is now on uncovering the nature of\n\u02c6\u03bc.\nDistribution of M-Estimates . For a given distribution F, we de\ufb01ne \u03bc0=\u03bc(F)as\nthe solution to the following:\nEF(\u03c8(x\u2212\u03bc0))=0\nIt is technical to show, but it turns out that \u02c6\u03bc\u223cN(\u03bc0,v\nn)with\nv=EF(\u03c8(x\u2212\u03bc0)2)\n(EF(\u03c8/prime(x\u2212\u03bc0)))2\nThus, we can say that \u02c6\u03bcis asymptotically normal with asymptotic value \u03bc0and\nasymptotic variance v. This leads to the ef\ufb01ciency ratio which is de\ufb01ned as the\nfollowing:\nEff(\u02c6\u03bc)=v0\nv\nwhere v0is the asymptotic variance of the MLE and measures how near \u02c6\u03bcis to the\noptimum. In other words, this provides a sense of how much outlier contamination\ncosts in terms of samples. For example, if for two estimates with asymptotic variancesv\n1andv2,w eh a v e v1=3v2, then \ufb01rst estimate requires three times as many\nobservations to obtain the same variance as the second. Furthermore, for the sample\nmean (i.e., \u02c6\u03bc=1\nn/summationtextXi) with F=N,w eh a v e \u03c1=x2/2 and\u03c8=xand also \u03c8/prime=1.\nThus, we have v=V(x). Alternatively, using the sample median as the estimator\nfor the location, we have v=1/(4f(\u03bc0)2). Thus, if we have F=N(0,1),f o rt h e\nsample median, we obtain v=2\u03c0/4\u22481.571. This means that the sample median\ntakes approximately 1.6 times as many samples to obtain the same variance for the\nlocation as the sample mean. The sample median is far more immune to the effects\nof outliers than the sample mean, so this gives a sense of how much this robustness\ncosts in samples.\nM-Estimates as Weighted Means . One way to think about M-estimates is a weighted\nmeans. Operationally, this means that we want weight functions that can circumscribe\nthe in\ufb02uence of the individual data points, but, when taken as a whole, still provide\ngood estimated parameters. Most of the time, we have \u03c8(0)=0 and\u03c8/prime(0)exists so\nthat\u03c8is approximately linear at the origin. Using the following de\ufb01nition:\nW(x)=/braceleftBigg\n\u03c8(x)/xifx/negationslash=0\n\u03c8/prime(x) ifx=0", "203": "192 3 Statistics\nWe can write our Eq. 3.9.0.5 as follows:\n/summationdisplay\nW(xi\u2212\u02c6\u03bc)(xi\u2212\u02c6\u03bc)=0 (3.9.0.6)\nSolving this for \u02c6\u03bcyields the following,\n\u02c6\u03bc=/summationtextwixi/summationtextwi\nwhere wi=W(xi\u2212\u02c6\u03bc). This is not practically useful because the wicontains \u02c6\u03bc,\nwhich is what we are trying to solve for. The question that remains is how to pick\nthe\u03c8functions. This is still an open question, but the Huber functions are a well-\nstudied choice.\nHuber Functions . The family of Huber functions is de\ufb01ned by the following:\n\u03c1k(x)=/braceleftBigg\nx2if|x|\u2264k\n2k|x|\u2212k2if|x|>k\nwith corresponding derivatives 2 \u03c8k(x)with\n\u03c8k(x)=/braceleftBigg\nx if|x|\u2264k\nsgn(x)kif|x|>k\nwhere the limiting cases k\u2192\u221e and k\u21920 correspond to the mean and median,\nrespectively. To see this, take \u03c8\u221e=xand therefore W(x)=1 and thus the de\ufb01ning\nEq.3.9.0.6 results in\nn/summationdisplay\ni=1(xi\u2212\u02c6\u03bc)=0\nand then solving this leads to \u02c6\u03bc=1\nn/summationtextxi. Note that choosing k=0 leads to the\nsample median, but that is not so straightforward to solve for. Nonetheless, Huber\nfunctions provide a way to move between two extremes of estimators for location\n(namely, the mean vs. the median) with a tunable parameter k.T h e Wfunction\ncorresponding to Huber\u2019s \u03c8is the following:\nWk(x)=min/braceleftBig\n1,k\n|x|/bracerightBig\nFigure 3.20 shows the Huber weight function for k=2 with some sample points. The\nidea is that the computed location, \u02c6\u03bcis computed from Eq. 3.9.0.6 to lie somewhere\nin the middle of the weight function so that those terms (i.e., insiders ) have their\nvalues fully re\ufb02ected in the location estimate. The black circles are the outliers that", "204": "3.9 Robust Statistics 193\nFig. 3.20 This shows the\nHuber weight function, W2(x)\nand some cartoon data pointsthat are insiders or outsidersas far as the robust locationestimate is concerned\nhave their values attenuated by the weight function so that only a fraction of their\npresence is represented in the location estimate.\nBreakdown Point . So far, our discussion of robustness has been very abstract. A\nmore concrete concept of robustness comes from the breakdown point. In the simplest\nterms, the breakdown point describes what happens when a single data point in anestimator is changed in the most damaging way possible. For example, suppose we\nhave the sample mean, \u02c6\u03bc=/summationtextx\ni/n, and we take one of the xipoints to be in\ufb01nite.\nWhat happens to this estimator? It also goes in\ufb01nite. This means that the breakdownpoint of the estimator is 0%. On the other hand, the median has a breakdown point\nof 50%, meaning that half of the data for computing the median could go in\ufb01nite\nwithout affecting the median value. The median is a rank statistic that cares more\nabout the relative ranking of the data than the values of the data, which explains its\nrobustness.\nThe simplest but still formal way to express the breakdown point is to take ndata\npoints, D={(x\ni,yi)}. Suppose Tis a regression estimator that yields a vector of\nregression coef\ufb01cients, \u03b8,\nT(D)=\u03b8\nLikewise, consider all possible corrupted samples of the data D/prime. The maximum bias\ncaused by this contamination is the following:\nbias m=sup\nD/prime/bardblT(D/prime)\u2212T(D)/bardbl\nwhere the sup sweeps over all possible sets of m-contaminated samples. Using this,\nthe breakdown point is de\ufb01ned as the following:\n/epsilon1m=min/braceleftBigm\nn:bias m\u2192\u221e/bracerightBig", "205": "194 3 Statistics\nFor example, in our least-squares regression, even one point at in\ufb01nity causes an\nin\ufb01nite T. Thus, for least-squares regression, /epsilon1m=1/n. In the limit n\u2192\u221e ,w e\nhave/epsilon1m\u21920.\nEstimating Scale . In robust statistics, the concept of scale refers to a measure of\nthe dispersion of the data. Usually, we use the estimated standard deviation for this,\nbut this has a terrible breakdown point. Even more troubling, in order to get a goodestimate of location, we have to either somehow know the scale ahead of time,\nor jointly estimate it. None of these methods have easy-to-compute closed-form\nsolutions and must be computed numerically.\nThe most popular method for estimating scale is the median absolute deviation\nMAD=Med(|x\u2212Med(x)|)\nIn words, take the median of the data xand then subtract that median from the data\nitself, and then take the median of the absolute value of the result. Another good\ndispersion estimate is the interquartile range ,\nIQR=x\n(n\u2212m+1)\u2212x(n)\nwhere m=[n/4].T h e x(n)notation means the nthdata element after the data have\nbeen sorted. Thus, in this notation, max(x)=x(n). In the case where x\u223cN(\u03bc,\u03c32),\nthenMAD andIQR are constant multiples of \u03c3such that the normalized MAD is the\nfollowing:\nMADN(x)=MAD\n0.675\nThe number comes from the inverse CDF of the normal distribution corresponding\nto the 0 .75 level. Given the complexity of the calculations, jointly estimating both\nlocation and scale is a purely numerical matter. Fortunately, the Statsmodels module\nhas many of these ready to use. Let\u2019s create some contaminated data in the followingcode:\nimport statsmodels.api as sm\nfrom scipy import stats\ndata=np.hstack([stats .norm( 10,1).rvs(10),\nstats .norm( 0,1).rvs(100)])\nThese data correspond to our model of contamination that we started this section\nwith. As shown in the histogram in Fig. 3.21 , there are two normal distributions, one\ncentered neatly at zero, representing the majority of the samples, and another comingless regularly from the normal distribution on the right. Notice that the group of infre-\nquent samples on the right separates the mean and median estimates (vertical dotted\nand dashed lines). In the absence of the contaminating distribution on the right, thestandard deviation for this data should be close to one. However, the usual non-robust\nestimate for standard deviation ( np.std ) comes out to approximately three. Using", "206": "3.9 Robust Statistics 195\nFig. 3.21 Histogram of sample data. Notice that the group of infrequent samples on the right\nseparates the mean and median estimates indicated by the vertical lines\ntheMADN estimator ( sm.robust.scale.mad(data) ) we obtain approximately 1.25.\nThus, the robust estimate of dispersion is less moved by the presence of the contam-\ninating distribution.\nThe generalized maximum likelihood M-estimation extends to joint scale and\nlocation estimation using Huber functions. For example,\nhuber =sm.robust .scale .Huber()\nloc,scl =huber(data)\nwhich implements Huber\u2019s proposal two method of joint estimation of location and\nscale. This kind of estimation is the key ingredient to robust regression methods, manyof which are implemented in Statsmodels in statsmodels.formula.api.rlm .T h e\ncorresponding documentation has more information.\n3.10 Bootstrapping\nAs we have seen, it can be very dif\ufb01cult or impossible to determine the probability\ndensity distribution of the estimator of some quantity. The idea behind the bootstrap isthat we can use computation to approximate these functions which would otherwise\nbe impossible to solve for analytically.\nLet\u2019s start with a simple example. Suppose we have the following set of random\nvariables, {X\n1,X2,..., Xn}where each Xk\u223cF. In other words the samples are\nall drawn from the same unknown distribution F. Having run the experiment, we\nthereby obtain the following sample set:\n{x1,x2,..., xn}", "207": "196 3 Statistics\nThe sample mean is computed from this set as\n\u00afx=1\nnn/summationdisplay\ni=1xi\nThe next question is how close is the sample mean to the true mean, \u03b8=EF(X).\nNote that the second central moment of Xis as follows:\n\u03bc2(F):= EF(X2)\u2212(EF(X))2\nThe standard deviation of the sample mean, \u00afx,g i v e n nsamples from an underlying\ndistribution F, is the following:\n\u03c3(F)=(\u03bc2(F)/n)1/2\nUnfortunately, because we have only the set of samples {x1,x2,..., xn}and not F\nitself, we cannot compute this and instead must use the estimated standard error,\n\u00af\u03c3=(\u00af\u03bc2/n)1/2\nwhere \u00af\u03bc2=/summationtext(xi\u2212\u00afx)2/(n\u22121), which is the unbiased estimate of \u03bc2(F). However,\nthis is not the only way to proceed. Instead, we could replace Fby some estimate,\n\u02c6Fobtained as a piece-wise function of {x1,x2,..., xn}by placing probability mass\n1/non each xi. With that in place, we can compute the estimated standard error as\nthe following:\n\u02c6\u03c3B=(\u03bc2(\u02c6F)/n)1/2\nwhich is called the bootstrap estimate of the standard error. Unfortunately, the story\neffectively ends here. In even a slightly more general setting, there is no clean formula\u03c3(F)within which Fcan be swapped for \u02c6F.\nThis is where the computer saves the day. We actually do not need to know the\nformula \u03c3(F)because we can compute it using a resampling method. The key idea\nis to sample with replacement from {x\n1,x2,..., xn}. The new set of nindependent\ndraws (with replacement) from this set is the bootstrap sample ,\ny\u2217={x\u2217\n1,x\u2217\n2,..., x\u2217\nn}\nThe Monte Carlo algorithm proceeds by \ufb01rst by selecting a large number of\nbootstrap samples, {y\u2217\nk}, then computing the statistic on each of these samples, and\nthen computing the sample standard deviation of the results in the usual way. Thus,\nthe bootstrap estimate of the statistic \u03b8is the following:\n\u02c6\u03b8\u2217\nB=1\nB/summationdisplay\nk\u02c6\u03b8\u2217(k)", "208": "3.10 Bootstrapping 197\nFig. 3.22 The\u03b2(3,2)distribution and the histogram that approximates it\nwith the corresponding square of the sample standard deviation as\n\u02c6\u03c32\nB=1\nB\u22121/summationdisplay\nk(\u02c6\u03b8\u2217(k)\u2212\u02c6\u03b8\u2217\nB)2\nThe process is much simpler than the notation implies. Let\u2019s explore this with a\nsimple example using Python. The next block of code sets up some samples from a\u03b2(3,2)distribution,\n>>> import numpy as np\n>>> from scipy import stats\n>>>rv=stats .beta( 3,2)\n>>>xsamples =rv.rvs(50)\nBecause this is simulation data, we already know that the mean is \u03bc\n1=3/5 and the\nstandard deviation of the sample mean for n=50 is\u00af\u03c3=\u221a\n2/50, which we will\nverify later.\nFigure 3.22 shows the \u03b2(3,2)distribution and the corresponding histogram of the\nsamples. The histogram represents \u02c6Fand is the distribution we sample from to obtain\nthe bootstrap samples. As shown, the \u02c6Fis a pretty crude estimate for the Fdensity\n(smooth solid line), but that\u2019s not a serious problem insofar as the following bootstrap\nestimates are concerned. In fact, the approximation \u02c6Fhas a natural tendency to pull\ntoward the bulk of probability mass. This is a feature, not a bug; and is the underlyingmechanism that explains bootstrapping, but the formal proofs that exploit this basic\nidea are far out of our scope here. The next block generates the bootstrap samples\n>>>yboot =np.random .choice(xsamples,( 100,50))\n>>>yboot_mn =yboot .mean()", "209": "198 3 Statistics\nFig. 3.23 For each bootstrap draw, we compute the sample mean. This is the histogram of those\nsample means that will be used to compute the bootstrap estimate of the standard deviation\nand the bootstrap estimate is therefore,\n>>>np.std(yboot .mean(axis =1))# approx sqrt(1/1250)\n0.025598763883825818\nFigure 3.23 shows the distribution of computed sample means from the bootstrap\nsamples. As promised, the next block shows how to use sympy.stats to compute\nthe\u03b2(3,2)parameters we quoted earlier.\n>>> import sympy as S\n>>> import sympy.stats\n>>> for iinrange (50):# 50 samples\n... # load sympy.stats Beta random variables\n... # into global namespace using exec\n... execstring =\"x%d= S.stats.Beta('x'+str( %d),3,2)\" %(i,i)\n... exec (execstring)\n...>>> # populate xlist with the sympy.stats random variables\n>>> # from above\n>>>xlist =[eval ('x%d'%(i)) for iinrange (50)]\n>>> # compute sample mean\n>>>sample_mean =sum(xlist) /len(xlist)\n>>> # compute expectation of sample mean\n>>>sample_mean_1 =S.stats .E(sample_mean)\n>>> # compute 2nd moment of sample mean\n>>>sample_mean_2 =S.stats .E(S.expand(sample_mean **2))\n>>> # standard deviation of sample mean\n>>> # use sympy sqrt function\n>>>sigma_smn =S.sqrt(sample_mean_2 -\nsample_mean_1 **2)# sqrt(2)/50\n>>> print (sigma_smn)\nsqrt(-9*hyper((4,), (6,), 0)**2/25 + hyper((5,), (7,), 0)/125 + 49/(20000*beta(3, 2)**2))", "210": "3.10 Bootstrapping 199\nProgramming Tip\nUsing the exec function enables the creation of a sequence of Sympy ran-\ndom variables. Sympy has the var function which can automatically create\na sequence of Sympy symbols, but there is no corresponding function in the\nstatistics module to do this for random variables.\nExample . Recall the delta method from Sect. 3.4.2 . Suppose we have a set of\nBernoulli coin-\ufb02ips ( Xi) with probability of head p. Our maximum likelihood esti-\nmator of pis\u02c6p=/summationtextXi/nforn\ufb02ips. We know this estimator is unbiased with\nE(\u02c6p)=pand V(\u02c6p)=p(1\u2212p)/n. Suppose we want to use the data to estimate the\nvariance of the Bernoulli trials ( V(X)=p(1\u2212p)). By the notation the delta method,\ng(x)=x(1\u2212x). By the plug-in principle, our maximum likelihood estimator of this\nvariance is then \u02c6p(1\u2212\u02c6p). We want the variance of this quantity. Using the results\nof the delta method, we have\nV(g(\u02c6p))=(1\u22122\u02c6p)2V(\u02c6p)\nV(g(\u02c6p))=(1\u22122\u02c6p)2\u02c6p(1\u2212\u02c6p)\nn\nLet\u2019s see how useful this is with a short simulation.\n>>> from scipy import stats\n>>> import numpy as np\n>>>p= 0.25 # true head-up probability\n>>>x=stats .bernoulli(p) .rvs(10)\n>>> print (x)\n[ 0000001000 ]\nThe maximum likelihood estimator of pis\u02c6p=/summationtextXi/n,\n>>>phat =x.mean()\n>>> print (phat)\n0.1\nThen, plugging this into the delta method approximant above,>>> print ((1-2*phat) **2*(phat) **2/10 )\n0.0006400000000000003\nNow, let\u2019s try this using the bootstrap estimate of the variance>>>phat_b =np.random .choice(x,( 50,10)).mean( 1)\n>>> print (np.var(phat_b *(1-phat_b)))\n0.0050490000000000005", "211": "200 3 Statistics\nThis shows that the delta method\u2019s estimated variance is different from the bootstrap\nmethod, but which one is better? For this situation we can solve for this directly using\nSympy\n>>> import sympy as S\n>>> from sympy.stats import E, Bernoulli\n>>>xdata =[Bernoulli(i,p) foriinS.symbols( 'x:10' )]\n>>>ph=sum(xdata) /float (len(xdata))\n>>>g=ph*(1-ph)\nProgramming Tip\nThe argument in the S.symbols(\u2019x:10\u2019) function returns a sequence of\nSympy symbols named x1,x2 and so on. This is shorthand for creating and\nnaming each symbol sequentially.\nNote that gis theg(\u02c6p)=\u02c6p(1\u2212\u02c6p)whose variance we are trying to estimate. Then,\nwe can plug in for the estimated \u02c6pand get the correct value for the variance,\n>>> print (E(g**2)-E(g)**2)\n0.00442968750000000\nThis case is generally representative\u2014the delta method tends to underestimate the\nvariance and the bootstrap estimate is better here.\n3.10.1 Parametric Bootstrap\nIn the previous example, we used the {x1,x2,..., xn}samples themselves as the\nbasis for \u02c6Fby weighting each with 1 /n. An alternative is to assume that the samples\ncome from a particular distribution, estimate the parameters of that distribution from\nthe sample set, and then use the bootstrap mechanism to draw samples from the\nassumed distribution, using the so-derived parameters. For example, the next codeblock does this for a normal distribution.\n>>>rv=stats .norm( 0,2)\n>>>xsamples =rv.rvs(45)\n>>> # estimate mean and var from xsamples\n>>>mn_ =np.mean(xsamples)\n>>>std_ =np.std(xsamples)\n>>> # bootstrap from assumed normal distribution with\n>>> # mn_,std_ as parameters\n>>>rvb =stats .norm(mn_,std_) #plug-in distribution\n>>>yboot =rvb.rvs(1000)", "212": "3.10 Bootstrapping 201\nRecall the sample variance estimator is the following:\nS2=1\nn\u22121/summationdisplay\n(Xi\u2212\u00afX)2\nAssuming that the samples are normally distributed, this means that (n\u22121)S2/\u03c32\nhas a chi-squared distribution with n\u22121 degrees of freedom. Thus, the variance,\nV(S2)=2\u03c34/(n\u22121). Likewise, the MLE plug-in estimate for this is V(S2)=\n2\u02c6\u03c34/(n\u22121)The following code computes the variance of the sample variance, S2\nusing the MLE and bootstrap methods:\n>>> # MLE-Plugin Variance of the sample mean\n>>> print (2*(std_ **2)**2/9. ) # MLE plugin\n2.22670148617726>>> # Bootstrap variance of the sample mean\n>>> print (yboot .var())\n3.2946788568183387>>> # True variance of sample mean\n>>> print (2*(2**2)**2/9. )\n3.5555555555555554\nThis shows that the bootstrap estimate is better here than the MLE plug-in estimate.\nNote that this technique becomes even more powerful with multivariate distri-\nbutions with many parameters because all the mechanics are the same. Thus, the\nbootstrap is a great all-purpose method for computing standard errors, but, in thelimit, is it converging to the correct value? This is the question of consistency . Unfor-\ntunately, to answer this question requires more and deeper mathematics than we can\nget into here. The short answer is that for estimating standard errors, the bootstrapis a consistent estimator in a wide range of cases and so it de\ufb01nitely belongs in your\ntoolkit.\n3.11 Gauss\u2013Markov\nIn this section, we consider the famous Gauss\u2013Markov problem which will give\nus an opportunity to use all the material we have so far developed. The Gauss\u2013\nMarkov model is the fundamental model for noisy parameter estimation because it\nestimates unobservable parameters given a noisy indirect measurement. Incarnationsof the same model appear in all studies of Gaussian models. This case is an excellent\nopportunity to use everything we have so far learned about projection and conditional\nexpectation.", "213": "202 3 Statistics\nFollowing Luenberger [ 4] let\u2019s consider the following problem:\ny=W\u03b2+/epsilon1\nwhere Wis a n\u00d7mmatrix, and yis a n\u00d71 vector. Also, /epsilon1is a n-dimensional\nnormally distributed random vector with zero mean and covariance,\nE(/epsilon1/epsilon1T)=Q\nNote that engineering systems usually provide a calibration mode where you can\nestimate Qso it\u2019s not fantastical to assume you have some knowledge of the noise\nstatistics. The problem is to \ufb01nd a matrix Kso that \u02c6\u03b2=KTyapproximates \u03b2.N o t e\nthat we only have knowledge of \u03b2viayso we can\u2019t measure it directly. Further, note\nthatKis a matrix, not a vector, so there are m\u00d7nentries to compute.\nWe can approach this problem the usual way by trying to solve the MMSE prob-\nlem:\nmin\nKE(/bardbl\u02c6\u03b2\u2212\u03b2/bardbl2)\nwhich we can write out as\nmin\nKE(/bardbl\u02c6\u03b2\u2212\u03b2/bardbl2)=min\nKE(/bardblKTy\u2212\u03b2/bardbl2)=min\nKE(/bardblKTW\u03b2+KT/epsilon1\u2212\u03b2/bardbl2)\nand since /epsilon1is the only random variable here, this simpli\ufb01es to\nmin\nK/bardblKTW\u03b2\u2212\u03b2/bardbl2+E(/bardblKT/epsilon1/bardbl2)\nThe next step is to compute\nE(/bardblKT/epsilon1/bardbl2)=TrE(KT/epsilon1/epsilon1TK)=Tr(KTQK)\nusing the properties of the trace of a matrix. We can assemble everything as\nmin\nK/bardblKTW\u03b2\u2212\u03b2/bardbl2+Tr(KTQK)\nNow, if we were to solve this for K, it would be a function of \u03b2, which is the same\nthing as saying that the estimator, \u02c6\u03b2is a function of what we are trying to estimate, \u03b2,\nwhich makes no sense. However, writing this out tells us that if we had KTW=I,\nthen the \ufb01rst term vanishes and the problem simpli\ufb01es to\nmin\nKTr(KTQK)\nwith the constraint,\nKTW=I", "214": "3.11 Gauss\u2013Markov 203\nFig. 3.24 The red circles\nshow the points to be estimatedin the xy-plane by the black\npoints\nThis requirement is the same as asserting that the estimator is unbiased,\nE(\u02c6\u03b2)=KTW\u03b2=\u03b2\nTo line this problem up with our earlier work, let\u2019s consider the ithcolumn of K,ki.\nNow, we can re-write the problem as\nmin\nk(kT\niQk i)\nwith\nWTki=ei\nand we know how to solve this from our previous work on contrained optimization,\nki=Q\u22121W(WTQ\u22121W)\u22121ei\nNow all we have to do is stack these together for the general solution:\nK=Q\u22121W(WTQ\u22121W)\u22121\nIt\u2019s easy when you have all of the concepts lined up! For completeness, the covariance\nof the error is\nE(\u02c6\u03b2\u2212\u03b2)(\u02c6\u03b2\u2212\u03b2)T=KTQK=(WTQ\u22121W)\u22121", "215": "204 3 Statistics\nFig. 3.25 Focusing on the xy-\nplane in Fig. 3.24 , the dashed\nline shows the true valuefor\u03b2versus the mean of the\nestimated values /hatwide\u03b2\nm\nFigure 3.24 shows the simulated ydata as red circles. The black dots show the\ncorresponding estimates, \u02c6\u03b2for each sample. The black lines show the true value of \u03b2\nversus the average of the estimated \u03b2-values, /hatwider\u03b2m. The matrix Kmaps the red circles\nin the corresponding dots. Note there are many possible ways to map the red circlesto the plane, but the Kis the one that minimizes the MSE for \u03b2.\nProgramming Tip\nThe following snippets provide a quick code walkthrough. To simulate the target\ndata, we de\ufb01ne the relevant matrices below:\nQ=np.eye(3)*0.1 # error covariance matrix\n# this is what we are trying estimate\nbeta =matrix(ones(( 2,1)))\nW=matrix([[ 1,2],\n[2,3],\n[1,1]])\nThen, we generate the noise terms and create the simulated data, y,\nntrials =5 0\nepsilon =np.random .multivariate_normal(( 0,0,0),Q,ntrials) .T\ny=W*beta+epsilon\nFigure 3.25 shows more detail in the horizontal xy-plane of Fig. 3.24 . Figure 3.25\nshows the dots, which are individual estimates of \u02c6\u03b2from the corresponding simulated\nydata. The dashed line is the true value for \u03b2and the \ufb01lled line ( /hatwide\u03b2m) is the average\nof all the dots. The gray ellipse provides an error ellipse for the covariance of the\nestimated \u03b2values.", "216": "3.11 Gauss\u2013Markov 205\nProgramming Tip\nThe following snippets provide a quick walkthrough of the construction of\nFig. 3.25 . To draw the ellipse, we need to import the patch primitive,\nfrom matplotlib.patches import Ellipse\nTo compute the parameters of the error ellipse based on the covariance matrix\nof the individual estimates of \u03b2in the bm_cov variable below,\nU,S,V =linalg .svd(bm_cov)\nerr=np.sqrt((matrix(bm)) *(bm_cov) *(matrix(bm) .T))\ntheta =np.arccos(U[ 0,1])/np.pi*180\nThen, we draw the add the scaled ellipse in the following,\nax.add_patch(Ellipse(bm,err *2/np.sqrt(S[ 0]),\nerr*2/np.sqrt(S[ 1]),\nangle =theta,color ='gray' ))\n3.12 Nonparametric Methods\nSo far, we have considered parametric methods that reduce inference or prediction to\nparameter-\ufb01tting. However, for these to work, we had to assume a speci\ufb01c functional\nform for the unknown probability distribution of the data. Nonparametric methods\neliminate the need to assume a speci\ufb01c functional form by generalizing to classes offunctions.\n3.12.1 Kernel Density Estimation\nWe have already made heavy use of this method with the histogram, which is aspecial case of kernel density estimation. The histogram can be considered the crudestand most useful nonparametric method that estimates the underlying probability\ndistribution of the data.\nTo be formal and place the histogram on the same footing as our earlier estimations,\nsuppose that X=[0,1]\ndis the d-dimensional unit cube and that his the bandwidth\nor size of a bin or sub-cube. Then, there are N\u2248(1/h)dsuch bins, each with\nvolume hd,{B1,B2,..., BN}. With all this in place, we can write the histogram has\na probability density estimator of the form,", "217": "206 3 Statistics\n\u02c6ph(x)=N/summationdisplay\nk=1\u02c6\u03b8k\nhI(x\u2208Bk)\nwhere\n\u02c6\u03b8k=1\nnn/summationdisplay\nj=1I(Xj\u2208Bk)\nis the fraction of data points ( Xk) in each bin, Bk. We want to bound the bias and\nvariance of \u02c6ph(x). Keep in mind that we are trying to estimate a function of x,b u t\nthe set of all possible probability distribution functions is extremely large and hard to\nmanage. Thus, we need to restrict our attention to the following class of probabilitydistribution of so-called Lipschitz functions,\nP(L)={p:|p(x)\u2212p(y)|\u2264L/bardblx\u2212y/bardbl,\u2200x,y}\nRoughly speaking, these are the density functions whose slopes (i.e., growth rates)\nare bounded by L. It turns out that the bias of the histogram estimator is bounded in\nthe following way:/integraldisplay\n|p(x)\u2212E(\u02c6p\nh(x))|dx\u2264Lh\u221a\nd\nSimilarly, the variance is bounded by the following:\nV(\u02c6ph(x))\u2264C\nnhd\nfor some constant C. Putting these two facts together means that the risk is bounded\nby\nR(p,\u02c6p)=/integraldisplay\nE(p(x)\u2212\u02c6ph(x))2dx\u2264L2h2d+C\nnhd\nThis upper bound is minimized by choosing\nh=/parenleftbiggC\nL2nd/parenrightbigg 1\nd+2\nIn particular, this means that\nsup\np\u2208P(L)R(p,\u02c6p)\u2264C0/parenleftbigg1\nn/parenrightbigg 2\nd+2\nwhere the constant C0is a function of L. There is a theorem [ 2] that shows this bound\nin tight, which basically means that the histogram is a really powerful probability", "218": "3.12 Nonparametric Methods 207\ndensity estimator for Lipschitz functions with risk that goes as/parenleftbig1\nn/parenrightbig2\nd+2. Note that this\nclass of functions is not necessarily smooth because the Lipschitz condition admits\nnon-smooth functions. While this is a reassuring result, we typically do not knowwhich function class (Lipschitz or not) a particular probability belongs to ahead of\ntime. Nonetheless, the rate at which the risk changes with both dimension dand\nnsamples would be hard to understand without this result. Figure 3.26 shows the\nprobability distribution function of the \u03b2(2,2)distribution compared to computed\nhistograms for different values of n. The box plots on each of the points show\nhow the variation in each bin of the histogram reduces with increasing n. The risk\nfunction R(p,\u02c6p)above is based upon integrating the squared difference between the\nhistogram (as a piece-wise function of x) and the probability distribution function.\nProgramming Tip\nThe following snippet is the main element of the code for Fig. 3.26 .\ndefgenerate_samples (n,ntrials =500):\nphat =np.zeros((nbins,ntrials))\nfor kinrange (ntrials):\nd=rv.rvs(n)\nphat[:,k],_ =histogram(d,bins,density =True)\nreturn phat\nThe code uses the histogram function from Numpy. To be consistent with\nthe risk function R(p,\u02c6p), we have to make sure the bins keyword argument\nis formatted correctly using a sequence of bin-edges instead of just a single\ninteger. Also, the density=True keyword argument normalizes the histogram\nappropriately so that the comparison between it and the probability distributionfunction of the simulated beta distribution is correctly scaled.\n3.12.2 Kernel Smoothing\nWe can extend our methods to other function classes using kernel functions. A one-\ndimensional smoothing kernel is a smooth function Kwith the following properties:\n/integraldisplay\nK(x)dx=1\n/integraldisplay\nxK(x)dx=0\n0</integraldisplay\nx2K(x)dx<\u221e", "219": "208 3 Statistics\nFig. 3.26 The box plots on each of the points show how the variation in each bin of the histogram\nreduces with increasing n\nFor example, K(x)=I(x)/2 is the boxcar kernel, where I(x)=1 when |x|\u2264 1\nand zero otherwise. The kernel density estimator is very similar to the histogram,except now we put a kernel function on every point as in the following:\n\u02c6p(x)=1\nnn/summationdisplay\ni=11\nhdK/parenleftbigg/bardblx\u2212Xi/bardbl\nh/parenrightbigg\nwhere X\u2208Rd. Figure 3.27 shows an example of a kernel density estimate using a\nGaussian kernel function, K(x)=e\u2212x2/2/\u221a\n2\u03c0. There are \ufb01ve data points shown\nby the vertical lines in the upper panel. The dotted lines show the individual K(x)\nfunction at each of the data points. The lower panel shows the overall kernel density\nestimate, which is the scaled sum of the upper panel.\nThere is an important technical result in [ 2] that states that kernel density estima-\ntors are minimax in the sense we discussed in the maximum likelihood Sect. 3.4.I n\nbroad strokes, this means that the analogous risk for the kernel density estimator isapproximately bounded by the following factor:\nR(p,\u02c6p)/lessorsimilarn\n\u22122m\n2m+d", "220": "3.12 Nonparametric Methods 209\nFig. 3.27 The upper panel shows the individual kernel functions placed at each of the data points.\nThe lower panel shows the composite kernel density estimate which is the sum of the individual\nfunctions in the upper panel\nfor some constant Cwhere mis a factor related to bounding the derivatives of the\nprobability density function. For example, if the second derivative of the density\nfunction is bounded, then m=2. This means that the convergence rate for this\nestimator decreases with increasing dimension d.\nCross-Validation . As a practical matter, the tricky part of the kernel density esti-\nmator (which includes the histogram as a special case) is that we need to somehow\ncompute the bandwidth hterm using data. There are several rule-of-thumb methods\nthat for some common kernels, including Silverman\u2019s rule and Scott\u2019s rule for Gaus-sian kernels. For example, Scott\u2019s factor is to simply compute h=n\n\u22121/(d+4)and\nSilverman\u2019s is h=(n(d+2)/4)(\u22121/(d+4)). Rules of this kind are derived by assum-\ning the underlying probability density function is of a certain family (e.g., Gaussian),and then deriving the best hfor a certain type of kernel density estimator, usually\nequipped with extra functional properties (say, continuous derivatives of a certain\norder). In practice, these rules seem to work pretty well, especially for uni-modalprobability density functions. Avoiding these kinds of assumptions means computing\nthe bandwidth from data directly and that is where cross-validation comes in.\nCross-validation is a method to estimate the bandwidth from the data itself. The\nidea is to write out the following integrated squared error (ISE):\nISE(\u02c6p\nh,p)=/integraldisplay\n(p(x)\u2212\u02c6ph(x))2dx\n=/integraldisplay\n\u02c6ph(x)2dx\u22122/integraldisplay\np(x)\u02c6phdx+/integraldisplay\np(x)2dx", "221": "210 3 Statistics\nThe problem with this expression is the middle term,7\n/integraldisplay\np(x)\u02c6phdx\nwhere p(x)is what we are trying to estimate with \u02c6ph. The form of the last expression\nlooks like an expectation of \u02c6phover the density of p(x),E(\u02c6ph). The approach is to\napproximate this with the mean,\nE(\u02c6ph)\u22481\nnn/summationdisplay\ni=1\u02c6ph(Xi)\nThe problem with this approach is that \u02c6phis computed using the same data that the\napproximation utilizes. The way to get around this is to split the data into two equally\nsized chunks D1,D2; and then compute \u02c6phfor a sequence of different hvalues over\ntheD1set. Then, when we apply the above approximation for the data ( Zi)i nt h e\nD2set,\nE(\u02c6ph)\u22481\n|D2|/summationdisplay\nZi\u2208D2\u02c6ph(Zi)\nPlugging this approximation back into the integrated squared error provides the\nobjective function,\nISE\u2248/integraldisplay\n\u02c6ph(x)2dx\u22122\n|D2|/summationdisplay\nZi\u2208D2\u02c6ph(Zi)\nSome code will make these steps concrete. We will need some tools from Scikit-learn.\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.neighbors.kde import KernelDensity\nThetrain_test_split function makes it easy to split and keep track of the D1and\nD2sets we need for cross-validation. Scikit-learn already has a powerful and \ufb02exible\nimplementation of kernel density estimators. To compute the objective function, weneed some basic numerical integration tools from Scipy. For this example, we will\ngenerate samples from a \u03b2(2,2)distribution, which is implemented in the stats\nsubmodule in Scipy.\n>>> from scipy.integrate import quad\n>>> from scipy import stats\n>>>rv=stats .beta( 2,2)\n>>>n=100 # number of samples to generate\n>>>d=rv.rvs(n)[:, None]# generate samples as column-vector\n7The last term is of no interest because we are only interested in relative changes in the ISE.", "222": "3.12 Nonparametric Methods 211\nProgramming Tip\nThe use of the [:,None] in the last line formats the Numpy array returned by\nthervs function into a Numpy vector with a column dimension of one. This is\nrequired by the KernelDensity constructor because the column dimension is\nused for different features (in general) for Scikit-learn. Thus, even though weonly have one feature, we still need to comply with the structured input that\nScikit-learn relies upon. There are many ways to inject the additional dimension\nother than using None . For example, the more cryptic, np.c_ , or the less cryptic\n[:,np.newaxis] can do the same, as can the np.reshape function.\nThe next step is to split the data into two halves and loop over each of the hiband-\nwidths to create a separate kernel density estimator based on the D1data,\n>>>train,test,_,_ =train_test_split(d,d,test_size =0.5)\n>>>kdes=[KernelDensity(bandwidth =i).fit(train)\n... foriin[.05,0.1,0.2,0.3]]\nProgramming Tip\nNote that the single underscore symbol in Python refers to the last evaluated\nresult. The above code unpacks the tuple returned by train_test_split into\nfour elements. Because we are only interested in the \ufb01rst two, we assign the\nlast two to the underscore symbol. This is a stylistic usage to make it clear to\nthe reader that the last two elements of the tuple are unused. Alternatively, wecould assign the last two elements to a pair of dummy variables that we do not\nuse later, but then the reader skimming the code may think that those dummy\nvariables are relevant.\nThe last step is to loop over the so-created kernel density estimators and compute\nthe objective function.\n>>> import numpy as np\n>>> for iinkdes:\n... f=lambda x: np .exp(i .score_samples(x))\n... f2=lambda x: f([[x]]) **2\n... print ('h= %3.2f \\t %3.4f '%(i.bandwidth,quad(f2, 0,1)[0]\n... -2*np.mean(f(test))))\n...h=0.05-1.1323\nh=0.10-1.1336\nh=0.20-1.1330\nh=0.30-1.0810", "223": "212 3 Statistics\nFig. 3.28 Each line above is\na different kernel density esti-mator for the given bandwidthas an approximation to thetrue density function. A plainhistogram is imprinted on thebottom for reference\nProgramming Tip\nThe lambda functions de\ufb01ned in the last block are necessary because Scikit-\nlearn implements the return value of the kernel density estimator as a logarithm\nvia the score_samples function. The numerical quadrature function quad\nfrom Scipy computes the/integraltext\n\u02c6ph(x)2dxpart of the objective function.\nScikit-learn has many more advanced tools to automate this kind of hyper-\nparameter (i.e., kernel density bandwidth) search. To utilize these advanced tools,\nwe need to format the current problem slightly differently by de\ufb01ning the followingwrapper class (Fig. 3.28 ):\n>>> class KernelDensityWrapper (KernelDensity):\n... def predict (self,x):\n... return np.exp(self.score_samples(x))\n... def score (self,test):\n... f=lambda x:self.predict(x)\n... f2=lambda x: f([[x]]) **2\n... return -(quad(f2, 0,1)[0]-2*np.mean(f(test)))\n...\nThis is tantamount to reorganizing the above previous code into functions that Scikit-\nlearn requires. Next, we create the dictionary of parameters we want to search over(params ) below and then start the grid search with the fit function,\n>>> from sklearn.model_selection import GridSearchCV\n>>> params ={'bandwidth' :np.linspace( 0.01 ,0.5,10)}\n>>> clf =GridSearchCV(KernelDensityWrapper(), param_grid =params,cv =2)\n>>> clf.fit(d)", "224": "3.12 Nonparametric Methods 213\nGridSearchCV(cv=2,error_score='raise-deprecating',\nestimator=KernelDensityWrapper(algorithm='auto',atol=0,bandwidth=1.0,\nbreadth_first=True,kernel='gaussian',leaf_size=40,\nmetric='euclidean',metric_params=None,rtol=0),fit_params=None,iid='warn',n_jobs=None, param_grid={'bandwidth':\narray([0.01,0.06 444,0.11889,0.17333,0.22778,0.28222,0.33667, 0.39111,\n0.44556,0.5])},pre_dispatch='2*n_jobs',refit=True,return_train_score='warn',\nscoring=None,verbose=0)\n>>> print (clf .best_params_)\n{'bandwidth': 0.17333333333333334}\nThe grid search iterates over all the elements in the params dictionary and reports the\nbest bandwidth over that list of parameter values. The cvkeyword argument above\nspeci\ufb01es that we want to split the data into two equally sized sets for training andtesting. We can also examine the values of the objective function for each point on\nthe grid as follows:\n>>>clf.cv_results_[ 'mean_test_score' ]\narray([0.60758058,1.06324954,1.11858734,1.13187097,1.12006532,\n1.09186225,1.05391076,1.01126161,0.96717292,0.92354959])\nKeep in mind that the grid search examines multiple folds for cross-validation\nto compute the above means and standard deviations. Note that there is also a\nRandomizedSearchCV in case you would rather specify a distribution of param-\neters instead of a list. This is particularly useful for searching very large parameter\nspaces where an exhaustive grid search would be too computationally expensive.\nAlthough kernel density estimators are easy to understand and have many attractiveanalytical properties, they become practically prohibitive for large, high-dimensional\ndatasets.\n3.12.3 Nonparametric Regression Estimators\nBeyond estimating the underlying probability density, we can use nonparametric\nmethods to compute estimators of the underlying function that is generating thedata. Nonparametric regression estimators of the following form are known as linear\nsmoothers:\n\u02c6y(x)= n/summationdisplay\ni=1/lscripti(x)yi\nTo understand the performance of these smoothers, we can de\ufb01ne the risk as the\nfollowing:\nR(\u02c6y,y)=E/parenleftBigg\n1\nnn/summationdisplay\ni=1(\u02c6y(xi)\u2212y(xi))2/parenrightBigg", "225": "214 3 Statistics\nand \ufb01nd the best \u02c6ythat minimizes this. The problem with this metric is that we do\nnot know y(x), which is why we are trying to approximate it with \u02c6y(x). We could\nconstruct an estimation by using the data at hand as in the following:\n\u02c6R(\u02c6y,y)=1\nnn/summationdisplay\ni=1(\u02c6y(xi)\u2212Yi)2\nwhere we have substituted the data Yifor the unknown function value, y(xi).T h e\nproblem with this approach is that we are using the data to estimate the function and\nthen using the same data to evaluate the risk of doing so. This kind of double-dippingleads to overly optimistic estimators. One way out of this conundrum is to use leave-\none-out cross-validation, wherein the \u02c6yfunction is estimated using all but one of the\ndata pairs, (X\ni,Yi). Then, this missing data element is used to estimate the above\nrisk. Notationally, this is written as the following:\n\u02c6R(\u02c6y,y)=1\nnn/summationdisplay\ni=1(\u02c6y(\u2212i)(xi)\u2212Yi)2\nwhere \u02c6y(\u2212i)denotes computing the estimator without using the ithdata pair. Unfortu-\nnately, for anything other than relatively small datasets, it quickly becomes computa-\ntionally prohibitive to use leave-one-out cross-validation in practice. We\u2019ll get back\nto this issue shortly, but let\u2019s consider a concrete example of such a nonparametricsmoother.\n3.12.4 Nearest Neighbors Regression\nThe simplest possible nonparametric regression method is the k-nearest neighbors\nregression. This is easier to explain in words than to write out in math. Given an input\nx, \ufb01nd the closest one of the kclusters that contains it and then return the mean of\nthe data values in that cluster. As a univariate example, let\u2019s consider the following\nchirp waveform:\ny(x)=cos/parenleftbigg\n2\u03c0/parenleftbigg\nfox+BWx2\n2\u03c4/parenrightbigg/parenrightbigg\nThis waveform is important in high-resolution radar applications. The fois the start\nfrequency and BW/\u03c4is the frequency slope of the signal. For our example, the fact\nthat it is nonuniform over its domain is important. We can easily create some data\nby sampling the chirp as in the following:\n>>> import numpy as np\n>>> from numpy import cos, pi\n>>>xi=np.linspace( 0,1,100)[:,None]", "226": "3.12 Nonparametric Methods 215\nFig. 3.29 The dotted line shows the chirp signal and the solid line shows the nearest neighbor\nestimate. The gray circles are the sample points that we used to \ufb01t the nearest neighbor estimator.The shaded area shows the gaps between the estimator and the unsampled chirp\n>>>xin =np.linspace( 0,1,12)[:,None]\n>>>f0=1 # init frequency\n>>>BW=5\n>>>y=np.cos(2*pi*(f0*xin+(BW/2.0)*xin**2))\nWe can use this data to construct a simple nearest neighbor estimator using Scikit-\nlearn,\n>>> from sklearn.neighbors import KNeighborsRegressor\n>>>knr=KNeighborsRegressor( 2)\n>>>knr.fit(xin,y)\nKNeighborsRegressor(algorithm='auto',leaf_size=30,metric='minkowski',metric_params=None,n_jobs=None,n_neighbors=2,p=2, weights='uniform')\nProgramming Tip\nScikit-learn has a fantastically consistent interface. The fit function above \ufb01ts\nthe model parameters to the data. The corresponding predict function returns\nthe output of the model given an arbitrary input. We will spend a lot moretime on Scikit-learn in the machine learning chapter. The [:,None] part at the\nend is just injecting a column dimension into the array in order to satisfy the\ndimensional requirements of Scikit-learn.\nFigure 3.29 shows the sampled signal (gray circles) against the values generated by\nthe nearest neighbor estimator (solid line). The dotted line is the full unsampled chirp\nsignal, which increases in frequency with x. This is important for our example because\nit adds a nonstationary aspect to this problem in that the function gets progressively", "227": "216 3 Statistics\nFig. 3.30 This is the same as Fig. 3.29 except that here there are three nearest neighbors used to\nbuild the estimator\nwigglier with increasing x. The area between the estimated curve and the signal\nis shaded in gray. Because the nearest neighbor estimator uses only two nearest\nneighbors, for each new x, it \ufb01nds the two adjacent Xithat bracket the xin the\ntraining data and then averages the corresponding Yivalues to compute the estimated\nvalue. That is, if you take every adjacent pair of sequential gray circles in the \ufb01gure,\nyou \ufb01nd that the horizontal solid line splits the pair on the vertical axis. We can adjustthe number of nearest neighbors by changing the constructor,\n>>>knr=KNeighborsRegressor( 3)\n>>>knr.fit(xin,y)\nKNeighborsRegressor(algorithm='auto',leaf_size=30,metric='minkowski',metric_params=None,n_jobs=None,n_neighbors=3,p=2, weights='uniform')\nwhich produces the following corresponding Fig. 3.30 .\nFor this example, Fig. 3.30 shows that with more nearest neighbors the \ufb01t performs\npoorly, especially toward the end of the signal, where there is increasing variation,\nbecause the chirp is not uniformly continuous.\nScikit-learn provides many tools for cross-validation. The following code sets up\nthe tools for leave-one-out cross-validation:\n>>> from sklearn.model_selection import LeaveOneOut\n>>>loo=LeaveOneOut()\nTheLeaveOneOut object is an iterable that produces a set of disjoint indices of the\ndata\u2014one for \ufb01tting the model (training set) and one for evaluating the model (testing\nset). The next block loops over the disjoint sets of training and test indices iteratesprovided by the loo variable to evaluate the estimated risk, which is accumulated in\ntheout list.", "228": "3.12 Nonparametric Methods 217\n>>>out=[]\n>>> fortrain_index, test_index inloo.split(xin):\n... _=knr.fit(xin[train_index],y[train_index])\n... out.append((knr .predict(xi[test_index]) -y[test_index]) **2)\n...\n>>> print ('Leave-one-out Estimated Risk: ' ,np.mean(out),)\nLeave-one-out Estimated Risk: 1.0351713662681845\nThe last line in the code above reports leave-one-out\u2019s estimated risk.\nLinear smoothers of this type can be rewritten in using the following matrix:\nS=/bracketleftbig\n/lscripti(xj)/bracketrightbig\ni,j\nso that\n\u02c6y=Sy\nwhere y=[Y1,Y2,..., Yn]\u2208Rnand\u02c6y=/bracketleftbig\n\u02c6y(x1),\u02c6y(x2) ,..., \u02c6y(xn)/bracketrightbig\n\u2208Rn.T h i s\nleads to a quick way to approximate leave-one-out cross-validation as the following:\n\u02c6R=1\nnn/summationdisplay\ni=1/parenleftbiggyi\u2212\u02c6y(xi)\n1\u2212Si,i/parenrightbigg2\nHowever, this does not reproduce the approach in the code above because it assumes\nthat each \u02c6y(\u2212i)(xi)is consuming one fewer nearest neighbor than \u02c6y(x).\nWe can get this Smatrix from the knr object as in the following:\n>>>_=knr.fit(xin,y) # fit on all data\n>>>S=(knr.kneighbors_graph(xin)) .todense() /float (knr.n_neighbors)\nThetodense part reformats the sparse matrix that is returned into a regular Numpy\nmatrix . The following shows a subsection of this Smatrix:\n>>> print (S[:5,:5])\n[[0.33333333 0.33333333 0.33333333 0. 0. ]\n[0.33333333 0.33333333 0.33333333 0. 0. ]\n[0. 0.33333333 0.33333333 0.33333333 0. ]\n[0. 0. 0.33333333 0.33333333 0.33333333][0. 0. 0. 0.33333333 0.33333333]]\nThe sub-blocks show the windows of the the ydata that are being processed by the\nnearest neighbor estimator. For example,\n>>> print (np.hstack([knr .predict(xin[: 5]),(S *y)[: 5]])) #columns match\n[[ 0.55781314 0.55781314]\n[ 0.55781314 0.55781314][-0.09768138 -0.09768138][-0.46686876 -0.46686876][-0.10877633 -0.10877633]]", "229": "218 3 Statistics\nOr, more concisely checking all entries for approximate equality,\n>>>np.allclose(knr .predict(xin),S *y)\nTrue\nwhich shows that the results from the nearest neighbor object and the matrix multiply\nmatch.\nProgramming Tip\nNote that because we formatted the returned Sas a Numpy matrix, we auto-\nmatically get the matrix multiplication instead of default element-wise multi-\nplication in the S*y term.\n3.12.5 Kernel Regression\nFor estimating the probability density, we started with the histogram and moved tothe more general kernel density estimate. Likewise, we can also extend regression\nfrom nearest neighbors to kernel-based regression using the Nadaraya\u2013Watson kernel\nregression estimator. Given a bandwidth h>0, the kernel regression estimator is\nde\ufb01ned as the following:\n\u02c6y(x)=/summationtext\nn\ni=1K/parenleftbigx\u2212xi\nh/parenrightbig\nYi/summationtextn\ni=1K/parenleftbigx\u2212xi\nh/parenrightbig\nUnfortunately, Scikit-learn does not implement this regression estimator; however,\nJan Hendrik Metzen makes a compatible version available on github.com .\n>>> from kernel_regression import KernelRegression\nThis code makes it possible to internally optimize over the bandwidth parameter\nusing leave-one-out cross-validation by specifying a grid of potential bandwidth\nvalues ( gamma ), as in the following:\n>>>kr=KernelRegression(gamma =np.linspace( 6e3,7e3,500))\n>>>kr.fit(xin,y)\nKernelRegression(gamma=6000.0,kernel='rbf')\nFigure 3.31 shows the kernel estimator (heavy black line) using the Gaussian kernel\ncompared to the nearest neighbor estimator (solid light black line). As before, the\ndata points are shown as circles. Figure 3.31 shows that the kernel estimator can pick\nout the sharp peaks that are missed by the nearest neighbor estimator.", "230": "3.12 Nonparametric Methods 219\nFig. 3.31 The heavy black line is the Gaussian kernel estimator. The light black line is the nearest\nneighbor estimator. The data points are shown as gray circles. Note that unlike the nearest neighborestimator, the Gaussian kernel estimator is able to pick out the sharp peaks in the training data\nThus, the difference between nearest neighbor and kernel estimation is that the\nlatter provides a smooth moving averaging of points whereas the former provides adiscontinuous averaging. Note that kernel estimates suffer near the boundaries where\nthere is mismatch between the edges and the kernel function. This problem gets\nworse in higher dimensions because the data naturally drift toward the boundaries(this is a consequence of the curse of dimensionality ). Indeed, it is not possible to\nsimultaneously maintain local accuracy (i.e., low bias) and a generous neighborhood\n(i.e., low variance). One way to address this problem is to create a local polynomialregression using the kernel function as a window to localize a region of interest. For\nexample,\n\u02c6y(x)=\nn/summationdisplay\ni=1K/parenleftbiggx\u2212xi\nh/parenrightbigg\n(Yi\u2212\u03b1\u2212\u03b2xi)2\nand now we have to optimize over the two linear parameters \u03b1and\u03b2. This method\nis known as local linear regression [5,6]. Naturally, this can be extended to higher\norder polynomials. Note that these methods are not yet implemented in Scikit-learn.\n3.12.6 Curse of Dimensionality\nThe so-called curse of dimensionality occurs as we move into higher and higher\ndimensions. The term was coined by Bellman in 1961 while he was studying adaptive\ncontrol processes. Nowadays, the term vaguely refers to anything that becomes more\ncomplicated as the number of dimensions increases substantially. Nevertheless, theconcept is useful for recognizing and characterizing the practical dif\ufb01culties of high-\ndimensional analysis and estimation.\nConsider the volume of an d-dimensional sphere of radius r,", "231": "220 3 Statistics\nVs(d,r)=\u03c0d/2rd\n\u0393/parenleftbigd\n2+1/parenrightbig\nFurther, consider the sphere Vs(d,1/2)enclosed by an d-dimensional unit cube. The\nvolume of the cube is always equal to one, but lim d\u2192\u221e Vs(d,1/2)=0. What does\nthis mean? It means that the volume of the cube is pushed away from its center,\nwhere the embedded hypersphere lives. Speci\ufb01cally, the distance from the center of\nthe cube to its vertices in ddimensions is\u221a\nd/2, whereas the distance from the center\nof the inscribing sphere is 1 /2. This diagonal distance goes to in\ufb01nity as ddoes. For\na\ufb01 x e d d, the tiny spherical region at the center of the cube has many long spines\nattached to it, like a hyper-dimensional sea urchin or porcupine.\nAnother way to think about this is to consider the /epsilon1>0 thick peel of the hyper-\nsphere,\nP/epsilon1=Vs(d,r)\u2212Vs(d,r\u2212/epsilon1)\nThen, we consider the following limit:\nlim\nd\u2192\u221eP/epsilon1=lim\nd\u2192\u221eVs(d,r)/parenleftbigg\n1\u2212Vs(d,r\u2212/epsilon1)\nVs(d,r)/parenrightbigg\n(3.12.6.1)\n=lim\nd\u2192\u221eVs(d,r)/parenleftBigg\n1\u2212lim\nd\u2192\u221e/parenleftbiggr\u2212/epsilon1\nr/parenrightbiggd/parenrightBigg\n(3.12.6.2)\n=lim\nd\u2192\u221eVs(d,r) (3.12.6.3)\nSo, in the limit, the volume of the /epsilon1-thick peel consumes the volume of the hyper-\nsphere.\nWhat are the consequences of this? For methods that rely on nearest neighbors,\nexploiting locality to lower bias becomes intractable. For example, suppose we havead-dimensional space and a point near the origin we want to localize around. To\nestimate behavior around this point, we need to average the unknown function about\nthis point, but in a high-dimensional space, the chances of \ufb01nding neighbors toaverage are slim. Looked at from the opposing point of view, suppose we have a\nbinary variable, as in the coin-\ufb02ipping problem. If we have 1000 trials, then, based\non our earlier work, we can be con\ufb01dent about estimating the probability of heads.Now, suppose we have 10 binary variables. Now we have 2\n10=1024 vertices to\nestimate. If we had the same 1000 points, then at least 24 vertices would not get any\ndata. To keep the same resolution, we would need 1000 samples at each vertex fora grand total of 1000 \u00d71024\u224810\n6data points. So, for a tenfold increase in the\nnumber of variables, we now have about 1000 more data points to collect to maintain\nthe same statistical resolution. This is the curse of dimensionality.\nPerhaps some code will clarify this. The following code generates samples in\ntwo dimensions that are plotted as points in Fig. 3.32 with the inscribed circle in\ntwo dimensions. Note that for d=2 dimensions, most of the points are con-\ntained in the circle.", "232": "3.12 Nonparametric Methods 221\nFig. 3.32 Two-dimensional\nscatter of points randomly\nand independently uniformly\ndistributed in the unit square.\nNote that most of the points\nare contained in the circle.\nCounter to intuition, this does\nnot persist as the number of\ndimensions increases\n>>> import numpy as np\n>>>v=np.random .rand( 1000,2)-1/2.\nThe next code block describes the core computation in Fig. 3.33 . For each of the\ndimensions, we create a set of uniformly distributed random variates along each\ndimension and then compute how close each d-dimensional vector is to the origin.\nThose that measure one half are those contained in the hypersphere. The histogram\nof each measurement is shown in the corresponding panel in the Fig. 3.33 . The dark\nvertical line shows the threshold value. V alues to the left of this indicate the population\nthat are contained in the hypersphere. Thus, Fig. 3.33 shows that as dincreases, fewer\npoints are contained in the inscribed hypersphere. The following code paraphrases\nthe content of Fig. 3.33 .\nfig,ax =subplots()\nfordin[2,3,5,10,20,50]:\nv=np.random .rand( 5000,d)-1/2.\nax.hist([np .linalg .norm(i) for iinv])\n3.12.7 Nonparametric T ests\nDetermining whether or not two sets of observations derive from the same underlying\nprobability distribution is an important problem. The most popular way to do this is\nwith a standard t-test, but that requires assumptions about normality that may be hard", "233": "222 3 Statistics\nFig. 3.33 Each panel shows the histogram of lengths of uniformly distributed d-dimensional ran-\ndom vectors. The population to the left of the dark vertical line are those that are contained in the\ninscribed hypersphere. This shows that fewer points are contained in the hypersphere with increasing\ndimension\nFig. 3.34 The black line den-\nsity function is stochastically\nlarger than the gray one\nto justify, which leads to nonparametric methods can get at these questions without\nsuch assumptions.\nLetVand Wbe continuous random variables. The variable Visstochastically\nlarger than Wif,\nP(V\u2265x)\u2265P(W\u2265x)\nfor all x\u2208Rwith strict inequality for at least one x.T h et e r m stochastically smaller\nmeans the obverse of this. For example, the black line density function shown in\nFig. 3.34 is stochastically larger than the gray one.\nThe Mann\u2013Whitney\u2013Wilcoxon Test . The Mann\u2013Whitney\u2013Wilcoxon Test appro-\naches the following alternative hypotheses:", "234": "3.12 Nonparametric Methods 223\n\u2022H0:F(x)=G(x)for all xversus\n\u2022Ha:F(x)\u2265G(x),Fstochastically greater than G.\nSuppose we have two datasets XandYand we want to know if they are drawn from\nthe same underlying probability distribution or if one is stochastically greater than\nthe other. There are nxelements in Xandnyelements in Y. If we combine these two\ndatasets and rank them, then, under the null hypothesis, any data element should be\nas likely as any other to be assigned any particular rank. that is, the combined set Z,\nZ={X1,..., Xnx,Y1,..., Yny}\ncontains n=nx+nyelements. Thus, any assignment of nyranks from the integers\n{1,..., n}to{Y1,..., Yny}should be equally likely (i.e., P=/parenleftbign\nny/parenrightbig\u22121). Importantly,\nthis property is independent of the Fdistribution.\nThat is, we can de\ufb01ne the Ustatistic as the following:\nUX=nx/summationdisplay\ni=1ny/summationdisplay\nj=1I(Xi\u2265Yj)\nwhere I(\u00b7)is the usual indicator function. For an interpretation, this counts the number\nof times that elements of Youtrank elements of X. For example, let us suppose that\nX={1,3,4,5,6}and Y={2,7,8,10,11}. We can get a this in one move using\nNumpy broadcasting,\n>>> import numpy as np\n>>>x=np.array([ 1,3,4,5,6])\n>>>y=np.array([ 2,7,8,10,11])\n>>>U_X =(y<=x[:,None]).sum()\n>>>U_Y =(x<=y[:,None]).sum()\n>>> print (U_X, U_Y)\n42 1\nNote that\nUX+UY=nx/summationdisplay\ni=1ny/summationdisplay\nj=1I(Yi\u2265Xj)+I(Xi\u2265Yj)=nxny\nbecause I(Yi\u2265Xj)+I(Xi\u2265Yj)=1. We can verify this in Python,\n>>> print ((U_X +U_Y) ==len(x)*len(y))\nTrue\nNow that we can compute the UXstatistic, we have to characterize it. Let us consider\nUX.I fH0is true, then XandYare identically distributed random variables. Thus all/parenleftbignx+ny\nnx/parenrightbig\nallocations of the X-variables in the ordered combined sample are equally\nlikely. Among these, there are/parenleftbignx+ny\u22121\nnx/parenrightbig\nallocations have a Yvariable as the largest", "235": "224 3 Statistics\nobservation in the combined sample. For these, omitting this largest observation does\nnot affect UXbecause it would not have been counted anyway. The other/parenleftbignx+ny\u22121\nnx\u22121/parenrightbig\nallocations have an element of Xas the largest observation. Omitting this observation\nreduces UXbyny.\nWith all that, suppose Nnx,ny(u)be the number of allocations of XandYelements\nthat result in UX=u. Under H0situation of equally likely outcomes, we have\npnx,ny(u)=P(UX=u)=Nnx,ny(u)/parenleftbignx+ny\nnx/parenrightbig\nFrom our previous discussion, we have the recursive relationship,\nNnx,ny(u)=Nnx,ny\u22121(u)+Nnx\u22121,ny(u\u2212ny)\nAfter dividing all of this by/parenleftbignx+ny\nnx/parenrightbig\nand using the pnx,ny(u)notation above, we obtain\nthe following:\npnx,ny(u)=ny\nnx+nypnx,ny\u22121(u)+nx\nnx+nypnx\u22121,ny(u\u2212ny)\nwhere 0 \u2264u\u2264nxny. To start this recursion, we need the following initial conditions:\np0,ny(ux=0)=1\np0,ny(ux>0)=0\npnx,0(ux=0)=1\npnx,0(ux>0)=0\nTo see how this works in Python,\n>>> def prob(n,m,u):\n... ifu<0:return 0\n... ifn==0 orm==0:\n... return int(u==0)\n... else:\n... f=m/float (m+n)\n... return (f*prob(n,m -1,u) +\n... (1-f)*prob(n -1,m,u-m))\n...\nT h e s ea r es h o w ni nF i g . 3.35 and approach a normal distribution for large nx,ny,\nwith the following mean and variance:", "236": "3.12 Nonparametric Methods 225\nFig. 3.35 The normal approximation to the distribution improves with increasing nx,ny\nE(U)=nxny\n2(3.12.7.1)\nV(U)=nxny(nx+ny+1)\n12(3.12.7.2)\nThe variance becomes more complicated when there are ties.\nExample . We are trying to determine whether or not one network con\ufb01guration is\nfaster than another. We obtain the following round-trip times for each of the networks:\n>>> X=np.array([ 50.6 ,31.9 ,40.5 ,38.1 ,39.4 ,35.1 ,33.1 ,36.5 ,38.7 ,42.3 ])\n>>> Y=np.array([ 28.8 ,30.1 ,18.2 ,38.5 ,44.2 ,28.2 ,32.9 ,48.8 ,39.5 ,30.7 ])\nBecause there are too few elements to use the scipy.stats.mannwhitneyu func-\ntion (which internally uses the normal approximation to the U-statistic), we can\nuse our custom function above, but \ufb01rst we need to compute the UXstatistic using\nNumpy,\n>>>U_X =(Y<=X[:,None]).sum()\nFor the p-value, we want to compute the probability that the observed UXstatistic at\nleast as great as what was observed,", "237": "226 3 Statistics\n>>> print (sum(prob( 10,10,i) for iinrange (U_X, 101)))\n0.08274697438784127\nThis is close to the usual \ufb01ve percent p-value threshold so it is possible at a slightly\nhigher threshold to conclude that the two sets of samples do not originate from the\nsame underlying distribution. Keep in mind that the usual \ufb01ve percent threshold is\njust a guideline. Ultimately, it is up to the analyst to make the call.\nProving Mean and Variance for U-Statistic . To prove Eq. 3.12.7.1 , we assume\nthere are no ties. One way to get at the result E(U)=nxny/2,\nE(UY)=/summationdisplay\nj/summationdisplay\niP(Xi\u2264Yj)\nbecause E(I(Xi\u2264Yj))=P(Xi\u2264Yj). Further, because all the subscripted Xand\nYvariables are drawn independently from the same distribution, we have\nE(UY)=nxnyP(X\u2264Y)\nand also,\nP(X\u2264Y)+P(X\u2265Y)=1\nbecause those are the two mutually exclusive conditions. Because the Xvariables and\nYvariables are drawn from the same distribution, we have P(X\u2264Y)=P(X\u2265Y),\nwhich means P(X\u2264Y)=1/2 and therefore E(UY)=nxny/2. Another way to get\nthe same result, is to note that, as we showed earlier, UX+UY=nxny. Then, taking\nthe expectation of both sides noting that E(UX)=E(UY)=E(U),g i v e s\n2E(U)=nxny\nwhich gives E(U)=nxny/2.\nGetting the variance is trickier. To start, we compute the following:\nE(UXUY)=/summationdisplay\ni/summationdisplay\nj/summationdisplay\nk/summationdisplay\nlP(Xi\u2265Yj\u2227Xk\u2264Yl)\nOf these terms, we have P(Yj\u2264Xi\u2264Yj)=0 because these are continuous random\nvariables. Let\u2019s consider the terms of the following type, P(Yi\u2264Xk\u2264Yl). To reduce\nthe notational noise, let\u2019s re-write this as P(Z\u2264X\u2264Y). Writing this out gives\nP(Z\u2264X\u2264Y)=/integraldisplay\nR/integraldisplay\u221e\nZ(F(Y)\u2212F(Z))f(y)f(z)dydz\nwhere Fis the cumulative density function and fis the probability density function\n(dF(x)/dx=f(x)). Let\u2019s break this up term by term. Using some calculus for the\nterm,", "238": "3.12 Nonparametric Methods 227\n/integraldisplay\u221e\nZF(Y)f(y)dy=/integraldisplay1\nF(Z)FdF =1\n2(1\u2212F(Z))\nThen, integrating out the Zvariable from this result, we obtain the following:\n/integraldisplay\nR1\n2/parenleftbigg\n1\u2212F(Z)2\n2/parenrightbigg\nf(z)dz=1\n3\nNext, we compute,\n/integraldisplay\nRF(Z)/integraldisplay\u221e\nZf(y)dyf(z)dz=/integraldisplay\nR(1\u2212F(Z))F(Z)f(z)dz\n=/integraldisplay\nR(1\u2212F)FdF=1\n6\nFinally, assembling the result, we have\nP(Z\u2264X\u2264Y)=1\n3\u22121\n6=1\n6\nAlso, terms like P(Xk\u2265Yi\u2227Xm\u2264Yi)=P(Xm\u2264Yi\u2264Xk)=1/6b yt h es a m e\nreasoning. That leaves the terms like P(Xk\u2265Yi\u2227Xm\u2264Yl)=1/4 because of\nmutual independence and P(Xk\u2265Yi)=1/2. Now that we have all the terms, we\nhave to assemble the combinatorics to get the \ufb01nal answer.\nThere are ny(ny\u22121)nx+nx(nx\u22121)nyterms of type P(Yi\u2264Xk\u2264Yl). There\nareny(ny\u22121)nx(nx\u22121)terms like P(Xk\u2265Yi\u2227Xm\u2264Yl). Putting this all together,\nthis means that\nE(UXUY)=nxny(nx+ny\u22122)\n6+nxny(nx\u22121)(ny\u22121)\n4\nTo assemble the E(U2)result, we need to appeal to our earlier result,\nUX+UY=nxny\nSquaring both sides of this and taking the expectation gives\nE(U2\nX)+2E(UXUY)+E(U2\nY)=n2\nxn2\ny\nBecause E(U2\nX)=E(U2\nX)=E(U), we can simplify this as the following:\nE(U2)=n2\nxn2\ny\u22122E(UXUY)\n2\nE(U2)=nxny(1+nx+ny+3nxny)\n12", "239": "228 3 Statistics\nThen, since V(U)=E(U2)\u2212E(U)2, we \ufb01nally have\nV(U)=nxny(1+nx+ny)\n12\n3.13 Survival Analysis\nSurvival Curves . The problem is to estimate the length of time units (e.g., subjects,\nindividuals, components) exist in a cohort over time. For example, consider the\nfollowing data. The rows are the days in a 30-day period and the columns are the\nindividual units. For example, these could be \ufb01ve patients who all receive a particulartreatment on day 0 and then survive (indicated by 1) the next 30 days on not (indicated\nby0)\n>>>d=pd.DataFrame(index =range (1,8),\n... columns =['A','B','C','D','E' ],\n... data=1)\n>>>d.loc[3:,'A']=0\n>>>d.loc[6:,'B']=0\n>>>d.loc[5:,'C']=0\n>>>d.loc[4:,'D']=0\n>>>d.index .name='day'\n>>>d\nABCDE\nday1 11111\n2 11111\n3 011114 01101\n5 01001\n6 000017 00001\nImportantly, survival is a one-way street\u2014once a subject is dead , then that subject\ncannot return to the experiment. This is important because survival analysis is also\napplied to component failure or other topics where this fact is not so obvious. Thefollowing chart shows the survival status of each of the subjects for all seven days.\nThe blue circles indicate that the subject is alive and the red squares indicate death\nof the subject (Figs. 3.36 and 3.37 ).\nThere is another important recursive perspective on this calculation. Imagine there\nis a life raft containing [A,B,C,D,E]. Everyone survives until day two when Adies.\nThis leaves four in the life raft [B,C,D,E]. Thus, from the perspective of day one,\nthe survival probability is the probability of surviving just up until day two and then\nsurviving day two, PS(t\u22652)=P(t/\u2208[0,2)|t<2)PS(t=2)=(1)(4/5)=4/5.", "240": "3.13 Survival Analysis 229\nFig. 3.36 The red squares\nindicate a dead subject, andt h eb l u eal i v i n gs u b j e c t\nFig. 3.37 The survival prob-\nability decreases by day\nIn words, this means that surviving past the second day is the product of surviving\nthe second day itself and not having a death up to that point (i.e., surviving up tothat point). Using this recursive approach, the survival probability for the third day is\nP\nS(t\u22653)=PS(t>3)PS(t=3)=(4/5)(3/4)=3/5. Recall that just before the\nthird day, the life raft contains [B,C,D,E]and on the third day we have [B,C,E].\nThus, from the perspective of just before the third day there are four survivors in the\nraft and on the third day there are three 3 /4. Using this recursive argument generate\nthe same plot and come in handy with censoring.\nCensoring and Truncation . Censoring occurs when a subject leaves (right cen-\nsoring) or enters (left censoring) the study. There are two general types of rightcensoring. The so-called Type I right censoring is when a subject randomly drops", "241": "230 3 Statistics\nout of the study. This random drop out is another statistical effect that has to be\naccounted for in estimating survival. Type II right censoring occurs when the study\nis terminated when enough speci\ufb01c random events occur.\nLikewise, left censoring occurs when a subject enters the study prior to a certain\ndate, but exactly when this happened is unknown. This happens in study designs\ninvolving two separate studies stages. For example, a subject might enroll in the \ufb01rstselection process but be ineligible for the second process. Speci\ufb01cally, suppose a\nstudy concerns drug use and certain subjects have used the drug before the study but\nare unable to report exactly when. These subjects are left censored. Left truncation(a.k.a. staggered entry, delayed entry) is similar except the date of entry is known.\nFor example, a subject that starts taking a drug after being initially left out of the\nstudy.\nRight censoring is the most common so let\u2019s consider an example. Let\u2019s estimate\nthe survival function given in the following survival times in days:\n{1,2,3\n+,4,5,6+,7,8}\nwhere the censored survival times are indicated by the plus symbol. As before, the\nsurvival time at the 0thday is 8 /8=1, the \ufb01rst day is 7 /8, the second day =\n(7/8)(6/7). Now, we come to the \ufb01rst right censored entry. The survival time for the\nthird day is (7/8)(6/7)(5/5)=(7/8)(6/7). Thus, the subject who dropped out is not\nconsidered dead and cannot be counted as such but is considered just absent as far\nas the functional estimation of the probabilities goes. Continuing for the fourth day,we have (7/8)(6/7)(5/5)(4/5), the \ufb01fth day, (7/8)(6/7)(5/5)(4/5)(3/4), the sixth\n(right censored) day (7/8)(6/7)(5/5\n)(4/5)(3/4)(2/2), and so on. We can summarize\nthis in the following table.\nHazard Functions and Their Properties . Generally, the survival function is a con-\ntinuous function of time S(t)=P(T>t)where Tis the event time (e.g., time of\ndeath). Note that the cumulative density function, F(t)=P(T\u2264t)=1\u2212S(t)and\nf(t)=dF(t)\ndtis the usual probability density function. The so-called hazard function\nis the instantaneous rate of failure at time t,\nh(t)=f(t)\nS(t)=lim\n\u0394t\u21920P(T\u2208(t,t+\u0394t]|T\u2265t)\n\u0394t\nNote that is a continuous-limit version of the calculation we performed above.\nIn words, it says given the event time T\u2265t(subject has survived up to t), what is\nthe probability of the event occurring in the differential interval \u0394tfor a vanishingly\nsmall\u0394t. Note that this is not the usual derivative slope from calculus because there\nis no difference term in the numerator. The hazard function is also called the force\nof mortality ,intensity rate ,o rt h e instantaneous risk . Informally, you can think of\nthe hazard function as encapsulating the two issues we are most concerned about:deaths and the population at risk for those deaths. Loosely speaking, the probability\ndensity function in the numerator represents the probability of a death occurring in a", "242": "3.13 Survival Analysis 231\nsmall differential interval. However, we are not particularly interested in unquali\ufb01ed\ndeaths, but only deaths that can happen to a speci\ufb01c at-risk population. Returning to\nour lifeboat analogy, suppose there are 1000 people in the lifeboat and the probabilityof anybody falling off the lifeboat is 1/1000. Two things are happening here: (1) the\nprobability of the bad event is small and (2) there are a lot of subjects over which\nto spread the probability of that bad event. This means that the hazard rate for anyparticular individual is small. On the other hand, if there are only two subjects in the\nlife raft and the probability of falling off is 3/4, then the hazard rate is high because\nnot only is the unfortunate event probable, the risk of that unfortunate event is sharedby only two subjects.\nIt is a mathematical fact that,\nh(t)=\u2212dlogS(t)\ndt\nThis leads to the following interpretation:\nS(t)=exp/parenleftbigg\n\u2212/integraldisplayt\n0h(u)du/parenrightbigg\n:=exp(\u2212H(t))\nwhere H(t)is the cumulative hazard function . Note that H(t)=\u2212 logS(t). Consider\na subject whose survival time is 5 years. For this subject to have died at the \ufb01fth year,it had to be alive during the fourth year. Thus, the hazard at 5 years is the failure\nrate per year, conditioned on the fact that the subject survived until the fourth year.\nNote that this is not the same as the unconditional failure rate per year at the \ufb01fth\nyear, because the unconditional rate applies to all units at time zero and does not\nuse information about survival up to that point gleaned from the other units. Thus,\nthehazard function can be thought of as the point-wise unconditional probability of\nexperiencing the event, scaled by the fraction of survivors up to that point.\n3.13.1 Example\nTo get a sense of this, let\u2019s consider the example where the probability densityfunction is exponential with parameter \u03bb,f(t)=\u03bbexp(\u2212t\u03bb),\u2200t>0. This makes\nS(t)=1\u2212F(t)=exp(\u2212t\u03bb)and then the hazard function becomes h(t)=\u03bb,\nnamely a constant. To see this, recall that the exponential distribution is the only\ncontinuous distribution that has no memory:\nP(X\u2264u+t|X>u)=1\u2212exp(\u2212\u03bbt)=P(X\u2264t)\nThis means no matter how long we have been waiting for a death to occur, the\nprobability of a death from that point onward is the same\u2014thus the hazard function\nis a constant.", "243": "232 3 Statistics\nExpectations . Given all these de\ufb01nitions, it is an exercise in integration by parts to\nshow that the expected life remaining is the following:\nE(T)=/integraldisplay\u221e\n0S(u)du\nThis is equivalent to the following:\nE(T/vextendsingle/vextendsinglet=0)=/integraldisplay\u221e\n0S(u)du\nand we can likewise express the expected remaining life at tas the following:\nE(T/vextendsingle/vextendsingleT\u2265t)=/integraltext\u221e\ntS(u)du\nS(t)\nParametric Regression Models . Because we are interested in how study parameters\naffect survival, we need a model that can accommodate regression in exogenous\n(independent) variables ( x).\nh(t|x)=ho(t)exp(xT\u03b2)\nwhere \u03b2are the regression coef\ufb01cients and ho(t)is the baseline instantaneous haz-\nard function. Because the hazard function is always nonnegative, the effects of the\ncovariates enter through the exponential function. These kinds of models are calledproportional hazard rate models . If the baseline function is a constant ( \u03bb), then this\nreduces to the exponential regression model given by the following:\nh(t|x)=\u03bbexp(x\nT\u03b2)\nCox Proportional Hazards Model . The tricky part about the above proportional\nhazard rate model is the speci\ufb01cation of the baseline instantaneous hazard func-\ntion. In many cases, we are not so interested in the absolute hazard function (or itscorrectness), but rather a comparison of such hazard functions between two study\npopulations. The Cox model emphasizes this comparison by using a maximum like-\nlihood algorithm for a partial likelihood function. There is a lot to keep track of inthis model, so let\u2019s try the mechanics \ufb01rst to get a feel for what is going on.\nLetjdenote the j\nthfailure time, assuming that failure times are sorted in increas-\ning order. The hazard function for subject iat failure time jishi(tj).U s i n gt h e\ngeneral proportional hazards model, we have\nhi(tj)=h0(tj)exp(zi\u03b2):=h0(tj)\u03c8i\nTo keep it simple, we have zi\u2208{0,1}that indicates membership in the experi-\nmental group ( zi=1) or the control group ( zi=0). Consider the \ufb01rst failure time,", "244": "3.13 Survival Analysis 233\nt1for subject ifailing is the hazard function hi(t1)=h0(t1)\u03c8i. From the de\ufb01nitions,\nthe probability that subject iis the one who fails is the following:\np1=hi(t1)/summationtexthk(t1)=h0(t1)\u03c8i/summationtexth0(t1)\u03c8k\nwhere the summation is over all surviving units up to that point. Note that the baseline\nhazard cancels out and gives the following:\np1=\u03c8i/summationtext\nk\u03c8k\nWe can keep computing this for the other failure times to obtain {p1,p2,... pD}.\nThe product of all of these is the partial likelihood ,L(\u03c8)=p1\u00b7p2\u00b7\u00b7\u00b7pD. The next\nstep is to maximize this partial likelihood (usually logarithm of the partial likelihood)\nover\u03b2. There are a lot of numerical issues to keep track of here. Fortunately, the\nPython lifelines module can keep this all straight for us.\nLet\u2019s see how this works using the Rossi dataset that is available in lifelines.\n>>> from lifelines.datasets import load_rossi\n>>> from lifelines import CoxPHFitter, KaplanMeierFitter\n>>>rossi_dataset =load_rossi()\nThe Rossi dataset concerns prison recidivism. The fin variable indicates whether\nor not the subjects received \ufb01nancial assistance upon discharge from prison.\n\u2022week : week of \ufb01rst arrest after release, or censoring time.\n\u2022arrest : the event indicator, equal to 1 for those arrested during the period of the\nstudy and 0 for those who were not arrested.\n\u2022fin: a factor, with levels yes if the individual received \ufb01nancial aid after release\nfrom prison, and no if he did not; \ufb01nancial aid was a randomly assigned factor\nmanipulated by the researchers.\n\u2022age: in years at the time of release.\n\u2022race : a factor with levels black and other.\n\u2022wexp : a factor with levels yes if the individual had full-time work experience prior\nto incarceration and no if he did not.\n\u2022mar: a factor with levels married if the individual was married at the time of release\nand not married if he was not.\n\u2022paro : a factor coded yes if the individual was released on parole and no if he was\nnot.\n\u2022prio : number of prior convictions.\n\u2022educ : education, a categorical variable coded numerically, with codes 2 (grade 6\nor less), 3 (grades 6 through 9), 4 (grades 10 and 11), 5 (grade 12), or 6 (somepost-secondary).\n\u2022emp1 \u2013emp52 : factors coded yes if the individual was employed in the correspond-\ning week of the study and no otherwise.", "245": "234 3 Statistics\n>>>rossi_dataset .head()\nweek arrest fin age race wexp mar paro prio\n0 2 0 1 0 2 710 0131 1 7 1 0 1 810 018\n2 2 5 1 0 1 901 01 1 3\n3 5 2 0 1 2 311 1114 5 2 0 0 1 901 013\nNow, we just have to set up the calculation in lifelines ,u s i n gt h e scikit-\nlearn style. The lifelines module handles the censoring issues.\n>>>cph =CoxPHFitter()\n>>>cph.fit(rossi_dataset,\n... duration_col ='week' ,\n... event_col ='arrest' )\n<lifelines.CoxPHFitter: fitted with 432 observations, 318 censored>>>>cph.print_summary() # access the results using cph.summary\n<lifelines.CoxPHFitter: fitted with 432 observations, 318 censored>\nduration col = 'week'\nevent col = 'arrest'\nnumber of subjects = 432\nnumber of events = 114\nlog-likelihood = -658.75\ntime fit was run = 2019-03-12 13:54:12 UTC\n---\ncoef exp(coef) se(coef) z p -log2(p) lower 0.95 upper 0.95\nfin -0.38 0.68 0.19 -1.98 0.05 4.40 -0.75 -0.00age -0.06 0.94 0.02 -2.61 0.01 6.79 -0.10 -0.01race 0.31 1.37 0.31 1.02 0.31 1.70 -0.29 0.92wexp -0.15 0.86 0.21 -0.71 0.48 1.06 -0.57 0.27mar -0.43 0.65 0.38 -1.14 0.26 1.97 -1.18 0.31paro -0.08 0.92 0.20 -0.43 0.66 0.59 -0.47 0.30prio 0.09 1.10 0.03 3.19 <0.005 9.48 0.04 0.15---Concordance = 0.64Likelihood ratio test = 33.27 on 7 df, -log2(p)=15.37\nThe values in the summary are plotted in Fig. 3.38 .\nThe Cox proportional hazards model object from lifelines allows us to pre-\ndict the survival function for an individual with given covariates, assuming that the\nindividual just entered the study. For example, for the \ufb01rst individual (i.e., row) intherossi_dataset , we can use the model to predict the survival function for that\nindividual.\n>>>cph.predict_survival_function(rossi_dataset .iloc[ 0,:]).head()\n0\nevent_at\n0.0 1.0000001.0 0.997616\n2.0 0.995230", "246": "3.13 Survival Analysis 235\nFig. 3.38 This shows the \ufb01tted coef\ufb01cients from the summary table for each covariate\nFig. 3.39 The Cox propor-\ntional hazards model canpredict the survival probabil-ity for an individual based ontheir covariates\n3.0 0.992848\n4.0 0.990468\nThis result is plotted in Fig. 3.39 .", "247": "236 3 Statistics\nReferences\n1. W. Feller, An Introduction to Probability Theory and Its Applications ,v o l .1( W i l e y ,N e wY o r k ,\n1950)\n2. L. Wasserman, All of Statistics: A Concise Course in Statistical Inference (Springer, Berlin,\n2004)\n3. R.A. Maronna, D.R. Martin, V .J. Y ohai, Robust Statistics: Theory and Methods . Wiley Series in\nProbability and Statistics (Wiley, New Y ork, 2006)\n4. D.G. Luenberger, Optimization by Vector Space Methods . Professional Series (Wiley, New Y ork,\n1968)\n5. C. Loader, Local Regression and Likelihood (Springer, Berlin, 2006)\n6. T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning: Data Mining, Infer-\nence, and Prediction . Springer Series in Statistics (Springer, New Y ork, 2013)", "248": "Chapter 4\nMachine Learning\n4.1 Introduction\nMachine Learning is a huge and growing area. In this chapter, we cannot possibly\neven survey this area, but we can provide some context and some connections to\nprobability and statistics that should make it easier to think about machine learningand how to apply these methods to real-world problems. The fundamental problem\nof statistics is basically the same as machine learning: given some data, how to\nmake it actionable? For statistics, the answer is to construct analytic estimators using\npowerful theory. For machine learning, the answer is algorithmic prediction. Given\na dataset, what forward-looking inferences can we draw? There is a subtle bit in thisdescription: how can we know the future if all we have is data about the past? This\nis the crux of the matter for machine learning, as we will explore in the chapter.\n4.2 Python Machine Learning Modules\nPython provides many bindings for machine learning libraries, some specialized for\ntechnologies such as neural networks, and others geared toward novice users. For ourdiscussion, we focus on the powerful and popular Scikit-learn module. Scikit-learn\nis distinguished by its consistent and sensible API, its wealth of machine learning\nalgorithms, its clear documentation, and its readily available datasets that make iteasy to follow along with the online documentation. Like Pandas, Scikit-learn relies\non Numpy for numerical arrays. Since its release in 2007, Scikit-learn has become\nthe most widely used, general-purpose, open-source machine learning modules that\nis popular in both industry and academia. As with all of the Python modules we use,\nScikit-learn is available on all the major platforms.\nTo get started, let\u2019s revisit the familiar ground of linear regression using Scikit-\nlearn. First, let\u2019s create some data.\n\u00a9 Springer Nature Switzerland AG 2019\nJ. Unpingco, Python for Probability, Statistics, and Machine Learning ,\nhttps://doi.org/10.1007/978-3-030-18545-9_4237", "249": "238 4 Machine Learning\n>>> import numpy as np\n>>> from matplotlib.pylab import subplots\n>>> from sklearn.linear_model import LinearRegression\n>>>X=np.arange( 10) # create some data\n>>>Y=X+np.random .randn( 10)# linear with noise\nWe next import and create an instance of the LinearRegression class from Scikit-\nlearn.\n>>> from sklearn.linear_model import LinearRegression\n>>>lr=LinearRegression() # create model\nScikit-learn has a wonderfully consistent API. All Scikit-learn objects use the fit\nmethod to compute model parameters and the predict method to evaluate the model.\nFor the LinearRegression instance, the fit method computes the coef\ufb01cients of\nthe linear \ufb01t. This method requires a matrix of inputs where the rows are the samplesand the columns are the features. The target of the regression are the Yvalues, which\nmust be correspondingly shaped, as in the following:\n>>>X,Y =X.reshape(( -1,1)), Y .reshape(( -1,1))\n>>>lr.fit(X,Y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\nnormalize=False)\n>>>lr.coef_\narray([[0.94211853]])\nProgramming Tip\nThe negative one in the reshape((-1,1)) call above is for the truly lazy.\nUsing a negative one tells Numpy to \ufb01gure out what that dimension should begiven the other dimension and number of array elements.\nthecoef_ property of the linear regression object shows the estimated parameters for\nthe \ufb01t. The convention is to denote estimated parameters with a trailing underscore.\nThe model has a score method that computes the R2value for the regression. Recall\nfrom our statistics chapter (Sect. 3.7) that the R2value is an indicator of the quality\nof the \ufb01t and varies between zero (bad \ufb01t) and one (perfect \ufb01t).\n>>>lr.score(X,Y)\n0.9059042979442372\nNow, that we have this \ufb01tted, we can evaluate the \ufb01t using the predict method,\n>>>xi=np.linspace( 0,10,15)# more points to draw\n>>>xi=xi.reshape(( -1,1))# reshape as columns\n>>>yp=lr.predict(xi)", "250": "4.2 Python Machine Learning Modules 239\nFig. 4.1 The Scikit-learn\nmodule can easily perform\nbasic linear regression. The\ncircles show the training data\nand the \ufb01tted line is shown in\nblack\nThe resulting \ufb01t is shown in Fig. 4.1.\nMultilinear Regression . The Scikit-learn module easily extends linear regression\nto multiple dimensions. For example, for multilinear regression,\ny=\u03b10+\u03b11x1+\u03b12x2+...+\u03b1nxn\nThe problem is to \ufb01nd all of the \u03b1terms given the training set {x1,x2,..., xn,y}.\nWe can create another example dataset and see how this works,\n>>>X=np.random .randint( 20,size =(10,2))\n>>>Y=X.dot([ 1,3])+1 + np.random .randn(X .shape[ 0])*20\nFigure 4.2shows the two-dimensional regression example, where the size of the\ncircles is proportional to the targetted Yvalue. Note that we salted the output with\nrandom noise just to keep things interesting. Nonetheless, the interface with Scikit-\nlearn is the same,\n>>>lr=LinearRegression()\n>>>lr.fit(X,Y)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\nnormalize=False)\n>>> print (lr.coef_)\n[0.35171694 4.04064287]\nThecoef_ variable now has two terms in it, corresponding to the two input dimen-\nsions. Note that the constant offset is already built-in and is an option on the\nLinearRegression constructor. Figure 4.3shows how the regression performs.\nPolynomial Regression . We can extend this to include polynomial regression by\nusing the PolynomialFeatures in the preprocessing sub-module. To keep it\nsimple, let\u2019s go back to our one-dimensional example. First, let\u2019s create some syn-\nthetic data,", "251": "240 4 Machine Learning\nFig. 4.2 Scikit-learn can easily perform multilinear regression. The size of the circles indicate the\nvalue of the two-dimensional function of (X1,X2)\nFig. 4.3 The predicted data is plotted in black. It overlays the training data, indicating a good \ufb01t\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>>X=np.arange( 10).reshape( -1,1)# create some data\n>>>Y=X+X**2+X**3+ np.random .randn( *X.shape) *80\nNext, we have to create a transformation from Xto a polynomial of X,", "252": "4.2 Python Machine Learning Modules 241\n>>>qfit =PolynomialFeatures(degree =2)# quadratic\n>>>Xq=qfit.fit_transform(X)\n>>> print (Xq)\n[[ 1. 0. 0.]\n[ 1. 1. 1.]\n[ 1. 2. 4.][ 1. 3. 9.]\n[ 1. 4. 16.]\n[ 1. 5. 25.][ 1. 6. 36.]\n[ 1. 7. 49.]\n[ 1. 8. 64.][ 1. 9. 81.]]\nNote there is an automatic constant term in the output 0\nthcolumn where fit_\ntransform has mapped the single-column input into a set of columns representing\nthe individual polynomial terms. The middle column has the linear term, and the\nlast has the quadratic term. With these polynomial features stacked as columns of\nXq, all we have to do is fit andpredict again. The following draws a comparison\nbetween the linear regression and the quadratic repression (see Fig. 4.4),\n>>>lr=LinearRegression() # create linear model\n>>>qr=LinearRegression() # create quadratic model\n>>>lr.fit(X,Y) # fit linear model\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\nnormalize=False)\n>>>qr.fit(Xq,Y) # fit quadratic model\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\nnormalize=False)\n>>>lp=lr.predict(xi)\n>>>qp=qr.predict(qfit .fit_transform(xi))\nThis just scratches the surface of Scikit-learn. We will go through many more\nexamples later, but the main thing is to concentrate on the usage (i.e., fit,predict )\nwhich is standardized across all of the machine learning methods in Scikit-learn.\n4.3 Theory of Learning\nThere is nothing so practical as a good theory. In this section, we establish the formal\nframework for thinking about machine learning. This framework will help us think\nbeyond particular methods for machine learning so we can integrate new methodsor combine existing methods intelligently.\nBoth machine learning and statistics strive to develop understanding from data.\nSome historical perspective helps. Most of the methods in statistics were derivedtoward the start of the 20th century when data were hard to come by. Society was pre-\noccupied with the potential dangers of human overpopulation and work was focused", "253": "242 4 Machine Learning\nFig. 4.4 The title shows the R2score for the linear and quadratic regressions\non studying agriculture and crop yields. At this time, even a dozen data points was\nconsidered plenty. Around the same time, the deep foundations of probability were\nbeing established by Kolmogorov. Thus, the lack of data meant that the conclusionshad to be buttressed by strong assumptions and solid mathematics provided by the\nemerging theory of probability. Furthermore, inexpensive powerful computers were\nnot yet widely available. The situation today is much different: there are lots ofdata collected and powerful and easily programmable computers are available. The\nimportant problems no longer revolve around a dozen data points on a farm acre,\nbut rather millions of points on a square millimeter of a DNA microarray. Does thismean that statistics will be superseded by machine learning?\nIn contrast to classical statistics, which is concerned with developing models that\ncharacterize, explain, and describe phenomena, machine learning is overwhelminglyconcerned with prediction. Areas like exploratory statistics are very closely related to\nmachine learning, but still not as focused on prediction. In some sense, this is unavoid-\nable due to the size of the data machine learning can reduce. In other words, machine\nlearning can help distill a table of a million columns into one hundred columns, but\ncan we still interpret one hundred columns meaningfully? In classical statistics, thiswas never an issue because data were of a much smaller scale. Whereas mathemat-\nical models, usually normal distributions, \ufb01tted with observations are common in\nstatistics, machine learning uses data to construct models that sit on complicateddata structures and exploit nonlinear optimizations that lack closed-form solutions.\nA common maxim is that statistics is data plus analytical theory and machine learn-\ning is data plus computable structures. This makes it seem like machine learning iscompletely ad hoc and devoid of underlying theory, but this is not the case, and both", "254": "4.3 Theory of Learning 243\nFig. 4.5 In the classical\nstatistics problem, we observea sample and model what theurn contains\nFig. 4.6 In the machine\nlearning problem, we wantthe function that colors themarbles\nmachine learning and statistics share many important theoretical results. By way of\ncontrast, let us consider a concrete problem.\nLet\u2019s consider the classic balls in urns problem (see Fig. 4.5): we have an urn\ncontaining red and blue balls and we draw \ufb01ve balls from the urn, note the colorof each ball, and then try to determine the proportion of red and blue balls in the\nurn. We have already studied many statistical methods for dealing with this problem.\nNow, let\u2019s generalize the problem slightly. Suppose the urn is \ufb01lled with white ballsand there is some target unknown function fthat paints each selected ball either red\nor blue (see Fig. 4.6). The machine learning problem is how to \ufb01nd the ffunction,\ngiven only the observed red/blue balls. So far, this doesn\u2019t sound much different fromthe statistical problem. However, now we want to take our estimated ffunction, say,\n\u02c6f, and use it to predict the next handful of balls from another urn. Now, here\u2019s where\nthe story takes a sharp turn. Suppose the next urn already has some red and blue balls\nin it? Then, applying the function fmay result in purple balls which were not seen\nin the training d a t a( s e eF i g . 4.7). What can we do? We have just scraped the surface\nof the issues machine learning must confront using methods that are not part of the\nstatistics canon.", "255": "244 4 Machine Learning\nFig. 4.7 The problem is\nfurther complicated becausewe may see colored marblesthat were not present in theoriginal problem\n4.3.1 Introduction to Theory of Machine Learning\nSome formality and an example can get us going. We de\ufb01ne the unknown target\nfunction, f:X/mapsto\u2192 Y. The training set is {(x,y)}which means that we only see the\nfunction\u2019s inputs/outputs. The hypothesis set His the set of all possible guesses at f.\nThis is the set from which we will ultimately draw our \ufb01nal estimate, \u02c6f. The machine\nlearning problem is how to derive the best element from the hypothesis set by using\nthe training set. Let\u2019s consider a concrete example in the code below. Suppose X\nconsists of all three-bit vectors (i.e., X={000,001,..., 111}) as in the code below,\n>>> import pandas as pd\n>>> import numpy as np\n>>> from pandas import DataFrame\n>>>df=DataFrame(index =pd.Index([ '{0:04b}' .format(i)\n... for iinrange (2**4)],\n... dtype ='str' ,\n... name='x'),columns =['f'])\nProgramming Tip\nThe string speci\ufb01cation above uses Python\u2019s advanced string formatting mini-\nlanguage. In this case, the speci\ufb01cation says to convert the integer into a \ufb01xed-\nwidth, four-character ( 04b) binary representation.\nNext, we de\ufb01ne the target function fbelow which just checks if the number of zeros\nin the binary representation exceeds the number of ones. If so, then the function\noutputs 1and0otherwise (i.e., Y={0,1}).\n>>>df.f=np.array(df .index .map( lambda i:i.count( '0'))\n... >df.index .map( lambda i:i.count( '1')),dtype =int)\n>>>df.head( 8)# show top half only", "256": "4.3 Theory of Learning 245\nf\nx0000 10001 10010 10011 00100 10101 00110 00111 0\nThe hypothesis set for this problem is the set of allpossible functions of X.T h e\nsetDrepresents all possible input/output pairs. The corresponding hypothesis set H\nhas 216elements, one of which matches f. There are 216elements in the hypothesis\nset because for each of sixteen input elements, there are two possible corresponding\nvalues (zero or one) for each input. Thus, the size of the hypothesis set is 2 \u00d72\u00d7...\u00d7\n2=216. Now, presented with a training set consisting of the \ufb01rst eight input/output\npairs, our goal is to minimize errors over the training set ( Ein(\u02c6f)). There are 28\nelements from the hypothesis set that exactly match fover the training set. But how\nto pick among these 28elements? It seems that we are stuck here. We need another\nelement from the problem in order to proceed. The extra piece we need is to assume\nthat the training set represents a random sampling ( in-sample data) from a greater\npopulation ( out-of-sample data) that would be consistent with the population that \u02c6f\nwould ultimately predict upon. In other words, we are assuming a stable probabilitystructure for both the in-sample and out-of-sample data. This is a major assumption!\nThere is a subtle consequence of this assumption\u2014whatever the machine learning\nmethod does once deployed, in order for it to continue to work, it cannot disturb thedata environment that it was trained on. Said differently, if the method is not to be\ntrained continuously, then it cannot break this assumption by altering the genera-\ntive environment that produced the data it was trained on. For example, suppose wedevelop a model that predicts hospital readmissions based on seasonal weather and\npatient health. Because the model is so effective, in the next six months, the hos-\npital forestalls readmissions by delivering interventions that improve patient health.Clearly using the model cannot change seasonal weather, but because the hospital\nused the model to change patient health, the training data used to build the model is\nno longer consistent with the forward-looking health of the patients. Thus, there islittle reason to think that the model will continue to work as well going forward.\nReturning to our example, let\u2019s suppose that the \ufb01rst eight elements from Xare\ntwice as likely as the last eight. The following code is a function that generateselements from Xaccording to this distribution.\n>>>np.random .seed( 12)\n>>> def get_sample (n=1):\n... ifn==1:\n... return '{0:04b}' .format(np .random .choice( list (range (8))*2\n... +list (range (8,16))))\n... else :\n... return [get_sample( 1)for _inrange (n)]\n...", "257": "246 4 Machine Learning\nProgramming Tip\nThe function that returns random samples uses the np.random.choice func-\ntion from Numpy which takes samples (with replacement) from the given iter-able. Because we want the \ufb01rst eight numbers to be twice as frequent as the\nrest, we simply repeat them in the iterable using range(8)*2 . Recall that mul-\ntiplying a Python list by an integer duplicates the entire list by that integer. Itdoes not do element-wise multiplication as with Numpy arrays. If we wanted\nthe \ufb01rst eight to be 10 times more frequent, then we would use range(8)*10 ,\nfor example. This is a simple but powerful technique that requires very littlecode. Note that the pkeyword argument in np.random.choice also provides\nan explicit way to specify more complicated distributions.\nThe next block applies the function de\ufb01nition fto the sampled data to generate the\ntraining set consisting of eight elements.\n>>>train =df.loc[get_sample( 8),'f']# 8-element training set\n>>>train .index .unique() .shape # how many unique elements?\n(6,)\nNotice that even though there are eight elements, there is redundancy because these\nare drawn according to an underlying probability. Otherwise, if we just got all sixteen\ndifferent elements, we would have a training set consisting of the complete speci\ufb01-cation of fand then we would therefore know what h\u2208Hto pick! However, this\neffect gives us a clue as to how this will ultimately work. Given the elements in the\ntraining set, consider the set of elements from the hypothesis set that exactly match.How to choose among these? The answer is it does not matter! Why? Because under\nthe assumption that the prediction will be used in an environment that is determined\nby the same probability, getting something outside of the training set is just as likelyas getting something inside the training set. The size of the training set is key here\u2014\nthe bigger the training set, the less likely that there will be real-world data that fall\noutside of it and the better \u02c6fwill perform.\n1The following code shows the elements\nof the training set in the context of all possible data.\n>>>df['fhat' ]=df.loc[train .index .unique(), 'f']\n>>>df.fhat\nx0000 NaN\n0001 NaN\n0010 1.00011 0.0\n0100 1.0\n0101 NaN\n1This assumes that the hypothesis set is big enough to capture the entire training set (which it is for\nthis example). We will discuss this trade-off in greater generality shortly.", "258": "4.3 Theory of Learning 247\n0110 0.0\n0111 NaN\n1000 1.01001 0.0\n1010 NaN\n1011 NaN1100 NaN\n1101 NaN\n1110 NaN1111 NaN\nName: fhat, dtype: float64\nNote that there are NaNsymbols where the training set had no values. For de\ufb01niteness,\nwe \ufb01ll these in with zeros, although we can \ufb01ll them with anything we want so longas whatever we do is not determined by the training set.\n>>>df.fhat .fillna( 0,inplace =True )#final specification of fhat\nNow, let\u2019s pretend we have deployed this and generate some test data.\n>>>test=df.loc[get_sample( 50),'f']\n>>>(df.loc[test .index, 'fhat' ]!=test) .mean()\n0.18\nThe result shows the error rate, given the probability mechanism that generates the\ndata. The following Pandas-fu compares the overlap between the training set and the\ntest set in the context of all possible data. The NaN values show the rows where the\ntest data had items absent in the training data. Recall that the method returns zero\nfor these items. As shown, sometimes this works in its favor, and sometimes not.\n>>>pd.concat([test .groupby(level =0).mean(),\n... train .groupby(level =0).mean()],\n... axis=1,\n... keys=['test' ,'train' ])\ntest train\n0000 1 NaN\n0001 1 NaN\n0010 1 1.00011 0 0.0\n0100 1 1.0\n0101 0 NaN\n0110 0 0.0\n0111 0 NaN1000 1 1.0\n1001 0 0.0\n1010 0 NaN1011 0 NaN", "259": "248 4 Machine Learning\n1100 0 NaN\n1101 0 NaN\n1110 0 NaN1111 0 NaN\nNote that where the test data and training data shared elements, the prediction\nmatched; but when the test set produced an unseen element, the prediction mayor may not have matched.\nProgramming Tip\nThepd.concat function concatenates the two Series objects in the list. The\naxis=1 means join the two objects along the columns where each newly created\ncolumn is named according to the given keys .T h e level=0 in the groupby\nfor each of the Series objects means group along the index. Because the index\ncorresponds to the 4-bit elements, this accounts for repetition in the elements.\nThemean aggregation function computes the values of the function for each 4-\nbit element. Because all functions in each respective group have the same value,\nthemean just picks out that value because the average of a list of constants is\nthat constant.\nNow, we are in position to ask how big the training set should be to achieve a\nlevel of performance. For example, on average, how many in-samples do we need\nfor a given error rate? For this problem, we can ask how large (on average) must the\ntraining set be in order to capture allof the possibilities and achieve perfect out-of-\nsample error rates? For this problem, this turns out to be sixty-three.2Let\u2019s start over\nand retrain with these many in-samples.\n>>>train =df.loc[get_sample( 63),'f']\n>>> del df['fhat' ]\n>>>df['fhat' ]=df.loc[train .index .unique(), 'f']\n>>>df.fhat .fillna( 0,inplace =True )#final specification of fhat\n>>>test =df.loc[get_sample( 50),'f']\n>>># error rate\n>>>(df.loc[test .index, 'fhat' ]!=df.loc[test .index, 'f']).mean()\n0.0\nNotice that this bigger training set has a better error rate because it is able to identify\nthe best element from the hypothesis set because the training set captured more ofthe complexity of the unknown f. This example shows the trade-offs between the\nsize of the training set, the complexity of the target function, the probability structure\nof the data, and the size of the hypothesis set. Note that upon exposure to the data,the so-called learning method did nothing besides memorize the data and give any\nunknown, newly encountered data the zero output. This means that the hypothesis set\ncontains the single hypothesis function that memorizes and defaults to zero output.\n2This is a slight generalization of the classic coupon collector problem.", "260": "4.3 Theory of Learning 249\nIf the method attempted to change the default zero output based on the particular\ndata, then we could say that meaningful learning took place. What we lack here is\ngeneralization , which is the topic of the next section.\n4.3.2 Theory of Generalization\nWhat we really want to know is how our method will perform once deployed. It\nwould be nice to have some kind of performance guarantee. In other words, weworked hard to minimize the errors in the training set, but what errors can we expect\nat deployment? In training, we minimized the in-sample error, E\nin(\u02c6f), but that\u2019s not\ngood enough. We want guarantees about the out-of-sample error, Eout(\u02c6f).T h i si s\nwhat generalization means in machine learning. The mathematical statement of this\nis the following:\nP/parenleftBig\n|Eout(\u02c6f)\u2212Ein(\u02c6f)|>/epsilon1/parenrightBig\n<\u03b4\nfor/epsilon1and\u03b4. Informally, this says that the probability of the respective errors differ-\ning by more than a given /epsilon1is less than some quantity, \u03b4. This means that whatever\nthe performance on the training set, it should probably be pretty close to the corre-\nsponding performance once deployed. Note that this does not say that the in-sampleerrors ( E\nin) are any good in an absolute sense. It just says that we would not expect\nmuch different after deployment. Thus, good generalization means no surprises after\ndeployment, not necessarily good performance. There are two main ways to get atthis: cross-validation and probability inequalities. Let\u2019s consider the latter \ufb01rst. There\nare two entangled issues: the complexity of the hypothesis set and the probability\nof the data. It turns out we can separate these two by deriving a separate notion ofcomplexity free from any particular data probability.\nVC Dimension . We \ufb01rst need a way to quantify model complexity. Following\nWasserman [ 1], let Abe a class of sets and F={x\n1,x2,..., xn},as e to f ndata\npoints. Then, we de\ufb01ne\nNA(F)=#{F\u2229A:A\u2208A}\nThis counts the number of subsets of Fthat can be extracted by the sets of A.T h e\nnumber of items in the set (i.e., cardinality) is noted by the # symbol. For example,\nsuppose F={1}and A={(x\u2264a)}. In other words, Aconsists of all intervals\nclosed on the right and parameterized by a. In this case we have NA(F)=1 because\nall elements can be extracted from Fusing A. Speci\ufb01cally, any a>1 means that A\ncontains F.\nThe shatter coef\ufb01cient is de\ufb01ned as,\ns(A,n)=max\nF\u2208FnNA(F)", "261": "250 4 Machine Learning\nwhere Fconsists of all \ufb01nite sets of size n. Note that this sweeps over all \ufb01nite\nsets so we don\u2019t need to worry about any particular dataset of \ufb01nitely many points.\nThe de\ufb01nition is concerned with Aand how its sets can pick off elements from the\ndataset. A set Fisshattered byAif it can pick out every element in it. This provides\na sense of how the complexity in Aconsumes data. In our last example, the set of\nhalf-closed intervals shattered every singleton set {x1}.\nNow, we come to the main de\ufb01nition of the Vapnik\u2013Chervonenkis [ 2] dimension\ndVCwhich de\ufb01ned as the largest kfor which s(A,n)=2k, except in the case where\ns(A,n)=2nfor which it is de\ufb01ned as in\ufb01nity. For our example where F={x1},\nwe already saw that Ashatters F. How about when F={x1,x2}?N o w ,w eh a v e\ntwo points and we have to consider whether all subsets can be extracted by A.I nt h i s\ncase, there are four subsets, {\u2205,{x1},{x2},{x1,x2}}. Note that \u2205denotes the empty\nset. The empty set is easily extracted\u2014pick aso that it is smaller than both x1and\nx2. Assuming that x1<x2, we can get the next set by choosing x1<a<x2.T h e\nlast set is likewise doable by choosing x2<a. The problem is that we cannot capture\nthe third set, {x2}, without capturing x1as well. This means that we cannot shatter\nany \ufb01nite set with n=2u s i n g A. Thus, dVC=1.\nHere is the climatic result\nEout(\u02c6f)\u2264Ein(\u02c6f)+/radicalBigg\n8\nnln/parenleftbigg4((2n)dVC+1)\n\u03b4/parenrightbigg\nwith probability at least 1 \u2212\u03b4. This basically says that the expected out-of-sample\nerror can be no worse than the in-sample error plus a penalty due to the complexityof the hypothesis set. The expected in-sample error comes from the training set but\nthe complexity penalty comes from just the hypothesis set, thus disentangling these\ntwo issues.\nA general result like this, for which we do not worry about the probability of the\ndata, is certain to be pretty generous, but nonetheless, it tells us how the complexity\npenalty enters into the out-of-sample error. In other words, the bound on E\nout(\u02c6f)\ngets worse for a more complex hypothesis set. Thus, this generalization bound is a\nuseful guideline but not very practical if we want to get a good estimate of Eout(\u02c6f).\n4.3.3 Worked Example for Generalization/Approximation\nComplexity\nThe stylized curves in Fig. 4.8illustrate the idea that there is some optimal point of\ncomplexity that represents the best generalization given the training set.\nTo get a \ufb01rm handle on these curves, let\u2019s develop a simple one-dimensional\nmachine learning method and go through the steps to create this graph. Let\u2019s sup-\npose we have a training set consisting of x-y pairs {(xi,yi)}. Our method groups\nthe x-data into intervals and then averages the y-data in those intervals. Predict-", "262": "4.3 Theory of Learning 251\nFig. 4.8 In the ideal situation,\nthere is a best model that\nrepresents the optimal trade-\noff between complexity and\nerror. This is shown by the\nvertical line\ning for new x-data means simply identifying the interval containing the new data\nthen reporting the corresponding value. In other words, we are building a simple\none-dimensional, nearest neighbor classi\ufb01er. For example, suppose the training set\nx-data is the following:\n>>> train =DataFrame(columns =['x','y'])\n>>> train[ 'x']=np.sort(np .random .choice( range (2**10 ),size =90))\n>>> train .x.head( 10)# first ten elements\n01 5\n13 0\n24 5\n36 5\n47 6\n58 2\n6 115\n7 145\n8 147\n9 158\nName: x, dtype: int64\nIn this example, we took a random set of 10-bit integers. To group these into, say,\nten intervals, we simply use Numpy reshape as in the following:\n>>>train .x.values .reshape( 10,-1)\narray([[ 15, 30, 45, 65, 76, 82, 115, 145, 147],\n[158, 165, 174, 175, 181, 209, 215, 217, 232],\n[233, 261, 271, 276, 284, 296, 318, 350, 376],\n[384, 407, 410, 413, 452, 464, 472, 511, 522],\n[525, 527, 531, 534, 544, 545, 548, 567, 567],\n[584, 588, 610, 610, 641, 645, 648, 659, 667],\n[676, 683, 684, 697, 701, 703, 733, 736, 750],\n[754, 755, 772, 776, 790, 794, 798, 804, 830],\n[831, 834, 861, 883, 910, 910, 911, 911, 937],\n[943, 946, 947, 955, 962, 962, 984, 989, 998]])", "263": "252 4 Machine Learning\nwhere every row is one of the groups. Note that the range of each group (i.e., length\nof the interval) is not preassigned, and is learned from the training data. For this\nexample, the y-values correspond to the number of ones in the bit representation ofthe x-values. The following code de\ufb01nes this target function,\n>>>f_target =np.vectorize( lambda i:i.count( '1'))\nProgramming Tip\nThe above function uses np.vectorize which is a convenience method in\nNumpy that converts plain Python functions into Numpy versions. This basi-cally saves additional looping semantics and makes it easier to use with other\nNumpy arrays and functions.\nNext, we create the bit representations of all of the x-data below and then complete\ntraining set y-values,\n>>>train[ 'xb']=train .x.map('{0:010b}' .format)\n>>>train .y=train .xb.map(f_target)\n>>>train .head( 5)\nxy x b\n0 15 4 0000001111\n1 30 4 0000011110\n2 45 4 00001011013 65 2 0001000001\n4 76 3 0001001100\nTo train on this data, we just group by the speci\ufb01ed amount and then average the\ny-data over each group.\n>>> train .y.values .reshape( 10,-1).mean(axis =1)\narray([3.55555556, 4.88888889, 4.44444444, 4.88888889, 4.11111111,\n4. , 6. , 5.11111111, 6.44444444, 6.66666667])\nNote that the axis=1 keyword argument just means average across the columns. So\nfar, this de\ufb01nes the training. To predict using this method, we have to extract the\nedges from each of the groups and then \ufb01ll in with the group-wise mean we justcomputed for y. The following code extracts the edges of each group.\n>>>le,re =train .x.values .reshape( 10,-1)[:,[ 0,-1]].T\n>>> print (le) # left edge of group\n[ 15 158 233 384 525 584 676 754 831 943]>>> print (re) # right edge of group\n[147 232 376 522 567 667 750 830 937 998]", "264": "4.3 Theory of Learning 253\nNext, we compute the group-wise means and assign them to their respective edges.\n>>> val =train .y.values .reshape( 10,-1).mean(axis =1).round()\n>>> func =pd.Series(index =range (1024 ))\n>>> func[le] =val # assign value to left edge\n>>> func[re] =val # assign value to right edge\n>>> func .iloc[ 0]=0# default 0 if no data\n>>> func .iloc[ -1]=0# default 0 if no data\n>>> func .head()\n0 0.01 NaN\n2 NaN\n3 NaN4 NaN\ndtype: float64\nNote that the Pandas Series object automatically \ufb01lls in unassigned values with\nNaN. We have thus far only \ufb01lled in values at the edges of the groups. Now, we need\nto \ufb01ll in the intermediate values.\n>>>fi=func.interpolate( 'nearest' )\n>>>fi.head()\n0 0.0\n1 0.0\n2 0.03 0.0\n4 0.0\ndtype: float64\nTheinterpolate method of the Series object can apply a wide variety of powerful\ninterpolation methods, but we only need the simple nearest neighbor method to createour piece-wise approximant. Figure 4.9shows how this looks for the training data\nwe have created.\nNow, with all that established, we can now draw the curves for this machine\nlearning method. Instead of partitioning the training data for cross-validation (which\nwe\u2019ll discuss later), we can simulate test data using the same mechanism as for thetraining data, as shown next,\n>>>test=pd.DataFrame(columns =['x','xb','y'])\n>>>test[ 'x']=np.random .choice( range (2**10 ),size =500)\n>>>test.xb=test.x.map('{0:010b}' .format)\n>>>test.y=test.xb.map(f_target)\n>>>test.sort_values( 'x',inplace =True)\nThe curves are the respective errors for the training data and the testing data. For our\nerror measure, we use the mean squared error,\nE\nout=1\nnn/summationdisplay\ni=1(\u02c6f(xi)\u2212yi)2", "265": "254 4 Machine Learning\nFig. 4.9 The vertical lines show the training data and the thick black line is the approximant we\nhave learned from the training data\nwhere {(xi,yi)}n\ni=1come from the test data. The in-sample error ( Ein) is de\ufb01ned\nthe same except for the in-sample data. In this example, the size of each group isproportional to d\nVC, so the more groups we choose, the more complexity in the \ufb01tting.\nNow, we have all the ingredients to understand the trade-offs of complexity versus\nerror.\nFigure 4.10 shows the curves for our one-dimensional clustering method. The\ndotted line shows the mean squared error on the training set and the other line showsthe same for the test data. The shaded region is the complexity penalty of this method.\nNote that with enough complexity, the method can exactly memorize the testing\ndata, but that only penalizes the testing error ( E\nout). This effect is exactly what the\nVapnik\u2013Chervonenkis theory expresses. The horizontal axis is proportional to the\nVC dimension. In this case, complexity boils down to the number of intervals used\nin the sectioning. At the far right, we have as many intervals as there are elements inthe dataset, meaning that every element is wrapped in its own interval. The average\nvalue of the data in that interval is therefore just the corresponding yvalue because\nthere are no other elements to average over.\nBefore we leave this problem, there is another way to visualize the performance of\nour learning method. This problem can be thought of as a multi-class identi\ufb01cation\nproblem. Given a 10-bit integer, the number of ones in its binary representation is inone of the classes {0,1,..., 10}. The output of the model tries to put each integer\nin its respective class. How well this was done can be visualized using a confusion\nmatrix as shown in the next code block,\n>>> from sklearn.metrics import confusion_matrix\n>>>cmx=confusion_matrix(test .y.values,fi[test .x].values)\n>>> print (cmx)", "266": "4.3 Theory of Learning 255\nFig. 4.10 The dotted line shows the mean squared error on the training set and the other line shows\nthe same for the test data. The shaded region is the complexity penalty of this method. Note that\nas the complexity of the model increases, the training error decreases, and the method essentiallymemorizes the data. However, this improvement in training error comes at the cost of larger testingerror\n[ [ 1000000000 ]\n[ 1010110000 ]\n[ 0039740005 ][ 1 0 3 23 19 6 6 0 2 0]\n[ 0 0 1 26 27 14 27 2 2 0]\n[ 0 0 3 15 31 28 30 8 1 0][0 0 1 81 82 02 52 3 2 2 ]\n[1 0 11 0 51 3 71 9 3 6 ]\n[ 4012022743 ][ 2000010000 ] ]\nThe rows of this 10 \u00d710 matrix show the true class and the columns indicate the\nclass that the model predicted. The numbers in the matrix indicate the number of\ntimes that association was made. For example, the \ufb01rst row shows that there was oneentry in the test set with no ones in its binary representation (i.e, namely the number\nzero) and it was correctly classi\ufb01ed (namely, it is in the \ufb01rst row, \ufb01rst column of the\nmatrix). The second row shows there were four entries total in the test set with abinary representation containing exactly a single one. This was incorrectly classi\ufb01ed\nas the 0-class (i.e, \ufb01rst column) once, the 2-class (third column) once, the 4-class (\ufb01fth\ncolumn) once, and the 5-class (sixth column) once. It was never classi\ufb01ed correctlybecause the second column is zero for this row. In other words, the diagonal entries\nshow the number of times it was correctly classi\ufb01ed.", "267": "256 4 Machine Learning\nUsing this matrix, we can easily estimate the true-detection probability that we\ncovered earlier in our hypothesis testing section,\n>>> print (cmx.diagonal() /cmx.sum(axis =1))\n[1. 0. 0.10714286 0.38333333 0.27272727 0.24137931\n0.25252525 0.29230769 0.16 0. ]\nIn other words, the \ufb01rst element is the probability of detecting 0when 0is in force,\nthe second element is the probability of detecting 1when 1is in force, and so on. We\ncan likewise compute the false-alarm rate for each of the classes in the following:\n>>> print ((cmx .sum(axis =0)-cmx.diagonal()) /(cmx .sum() -cmx.sum(axis =1)))\n[0.01803607 0. 0.02330508 0.15909091 0.20199501 0.15885417\n0.17955112 0.09195402 0.02105263 0.03219316]\nProgramming Tip\nThe Numpy sum function can sum across a particular axis or, if the axis is\nunspeci\ufb01ed, will sum all entries of the array.\nIn this case, the \ufb01rst element is the probability that 0is declared when another category\nis in force, the next element is the probability that 1is declared when another category\nis in force, and so on. For a decent classi\ufb01er, we want a true-detection probability\nto be greater than the corresponding false-alarm rate, otherwise the classi\ufb01er is nobetter than a coin-\ufb02ip.\nThe missing feature of this problem, from the learning algorithm standpoint, is\nthat we did not supply the bit representation of every element which was used toderive the target variable, y. Instead, we just used the integer value of each of the 10-\nbit numbers, which essentially concealed the mechanism for creating the yvalues.\nIn other words, there was an unknown transformation from the input space Xto\nYthat the learning algorithm had to overcome, but that it could not overcome, at\nleast not without memorizing the training data. This lack of knowledge is a key issue\nin all machine learning problems, although we have made it explicit here with thisstylized example. This means that there may be one or more transformations from\nX\u2192 X\n/primethat can help the learning algorithm get traction on the so-transformed\nspace while providing a better trade-off between generalization and approximationthan could have been achieved otherwise. Finding such transformations is called\nfeature engineering .\n4.3.4 Cross-V alidation\nIn the last section, we explored a stylized machine learning example to understand\nthe issues of complexity in machine learning. However, to get an estimate of out-of-\nsample errors, we simply generated more synthetic data. In practice, this is not an", "268": "4.3 Theory of Learning 257\noption, so we need to estimate these errors from the training set itself. This is what\ncross-validation does. The simplest form of cross-validation is k-fold validation. For\nexample, if K=3, then the training data is divided into three sections wherein each\nof the three sections is used for testing and the remaining two are used for training.\nThis is implemented in Scikit-learn as in the following:\n>>> import numpy as np\n>>> from sklearn.model_selection import KFold\n>>> data =np.array([ 'a',]*3+['b',]*3+['c',]*3)# example\n>>> print (data)\n['a' 'a' 'a' 'b' 'b' 'b' 'c' 'c' 'c']\n>>> kf=KFold( 3)\n>>> for train_idx,test_idx inkf.split(data):\n... print (train_idx,test_idx)\n...[ 345678 ][ 012 ]\n[ 012678 ][ 345 ]\n[ 012345 ][ 678 ]\nIn the code above, we construct a sample data array and then see how KFold splits it up\ninto indices for training and testing, respectively. Notice that there are no duplicatedelements in each row between training and testing indices. To examine the elements\nof the dataset in each category, we simply use each of the indices as in the following:\n>>> for train_idx,test_idx inkf.split(data):\n... print ('training' , data[ train_idx ])\n... print ('testing' , data[ test_idx ])\n...\ntraining ['b' 'b' 'b' 'c' 'c' 'c']testing ['a' 'a' 'a']\ntraining ['a' 'a' 'a' 'c' 'c' 'c']\ntesting ['b' 'b' 'b']\ntraining ['a' 'a' 'a' 'b' 'b' 'b']\ntesting ['c' 'c' 'c']\nThis shows how each group is used in turn for training/testing. There is no random\nshuf\ufb02ing of the data unless the shuffle keyword argument is given. The error over\nthe test set is the cross-validation error . The idea is to postulate models of differing\ncomplexity and then pick the one with the best cross-validation error. For example,suppose we had the following sine wave data,\n>>>xi=np.linspace( 0,1,30)\n>>>yi=np.sin(2*np.pi*xi)\nand we want to \ufb01t this with polynomials of increasing order.\nFigure 4.11 shows the individual folds in each panel. The circles represent the\ntraining data. The diagonal line is the \ufb01tted polynomial. The gray shaded areasindicate the regions of errors between the \ufb01tted polynomial and the held-out testing", "269": "258 4 Machine Learning\nFig. 4.11 This shows the folds and errors for the linear model. The shaded areas show the errors\nin each respective test set (i.e., cross-validation scores ) for the linear model\ndata. The larger the gray area, the bigger the cross-validation errors, as are reported\nin the title of each frame.\nAfter reviewing the last four \ufb01gures and averaging the cross-validation errors,\nthe one with the least average error is declared the winner. Thus, cross-validation\nprovides a method of using a single dataset to make claims about unseen out-of-sample data insofar as the model with the best complexity can be determined. The\nentire process to generate the above \ufb01gures can be captured using cross_val_score\nas shown for the linear regression (compare the output with the values in the titles ineach panel of Fig. 4.11),\n>>> from sklearn.metrics import make_scorer, mean_squared_error\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.linear_model import LinearRegression\n>>>Xi=xi.reshape( -1,1)# refit column-wise\n>>>Yi=yi.reshape( -1,1)\n>>>lf=LinearRegression()\n>>>scores =cross_val_score(lf,Xi,Yi,cv =4,\n... scoring =make_scorer(mean_squared_error))\n>>> print (scores)\n[0.3554451 0.33131438 0.50454257 0.45905672]", "270": "4.3 Theory of Learning 259\nProgramming Tip\nThemake_scorer function is a wrapper that enables cross_val_score to\ncompute scores from the given estimator\u2019s output.\nThe process can be further automated by using a pipeline as in the following:\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> polyfitter =Pipeline([( 'poly' , PolynomialFeatures(degree =3)),\n... ('linear' , LinearRegression())])\n>>> polyfitter .get_params()\n{'memory': None, 'steps': [('poly', PolynomialFeatures(degree=3,\ninclude_bias=True, interaction_only=False)),\n('linear', LinearRegression(copy_X=True,fit_intercept=True, n_jobs=None,\nnormalize=False))], 'poly': PolynomialFeatures(degree=3,\ninclude_bias=True, interaction_only=False), 'linear':LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\nnormalize=False), 'poly__degree': 3, 'poly__include_bias':\nTrue, 'poly__interaction_only': False, 'linear__copy_X':True, 'linear__fit_intercept': True, 'linear__n_jobs': None,\n'linear__normalize': False}\nThePipeline object is a way of stacking standard steps into one big estima-\ntor, while respecting the usual fit andpredict interfaces. The output of the\nget_params function contains the polynomial degrees we previously looped over\nto create Fig. 4.11, etc. We will use these named parameters in the next code block.\nTo do this automatically using this polyfitter estimator, we need the Grid Search\nCross Validation object, GridSearchCV . The next step is to use this to create the\ngrid of parameters we want to loop over as in the following:\n>>> from sklearn.model_selection import GridSearchCV\n>>>gs=GridSearchCV(polyfitter,{ 'poly__degree' :[1,2,3]},\n... cv=4,return_train_score =True)\nThegsobject will loop over the polynomial degrees up to cubic using fourfold\ncross-validation cv=4 , like we did manually earlier. The poly__degree item comes\nfrom the previous get_params call. Now, we just apply the usual fit method on\nthe training data,\n>>>_=gs.fit(Xi,Yi)\n>>>gs.cv_results_\n{'mean_fit_time': array([0.00041956, 0.00041848, 0.00043315]),'std_fit_time': array([3.02347168e-05, 5.91589236e-06, 6.70625100e-06]),'mean_score_time': array([0.00027096, 0.00027257, 0.00032073]),'std_score_time': array([9.02611933e-06, 1.20837301e-06, 5.49008608e-05]),'param_poly__degree': masked_array(data=[1, 2, 3],\nmask=[False, False, False],\nfill_value='?',\ndtype=object), 'params':", "271": "260 4 Machine Learning\n[{'poly__degree': 1}, {'poly__degree': 2}, {'poly__degree': 3}],\n'split0_test_score': array([ -2.03118491, -68.54947351, -1.64899934]),'split1_test_score': array([-1.38557769, -3.20386236, 0.81372823]),'split2_test_score': array([ -7.82417707, -11.8740862 , 0.47246476]),'split3_test_score': array([ -3.21714294, -60.70054797, 0.14328163]),'mean_test_score': array([ -3.4874447 , -36.06830421, -0.07906481]),'std_test_score': array([ 2.47972092, 29.1121604 , 0.975868 ]),'rank_test_score': array([2, 3, 1], dtype=int32),'split0_train_score': array([0.52652515, 0.93434227, 0.99177894]),'split1_train_score': array([0.5494882 , 0.60357784, 0.99154288]),'split2_train_score': array([0.54132528, 0.59737218, 0.99046089]),'split3_train_score': array([0.57837263, 0.91061274, 0.99144127]),'mean_train_score': array([0.54892781, 0.76147626, 0.99130599]),'std_train_score': array([0.01888775, 0.16123462, 0.00050307])}\nthe scores shown correspond to the cross-validation scores for each of the parame-\nters (e.g., polynomial degrees) using fourfold cross-validation. Note that the higherscores are better here and the cubic polynomial is best, as we observed earlier.\nThe default R\n2metric is used for the scoring in this case as opposed to mean\nsquared error. The validation results of this pipeline for the quadratic \ufb01t are shownin Fig. 4.12, and for the cubic \ufb01t, in Fig. 4.13. This can be changed by pass-\ning the scoring=make_scorer(mean_squared_error) keyword argument to\nGridSearchCV . There is also RandomizedSearchCV that does not necessarily eval-\nuate every point on the grid and instead randomly samples the grid according to\nan input probability distribution. This is very useful for a large number of hyper-\nparameters.\n4.3.5 Bias and V ariance\nConsidering average error in terms of in-samples and out-samples depends on aparticular training data set. What we want is a concept that captures the performance\nof the estimator for allpossible training data. For example, our ultimate estimator,\n\u02c6fis derived from a particular set of training data ( D) and is thus denoted, \u02c6f\nD.T h i s\nmakes the out-of-sample error explicitly, Eout(\u02c6fD). To eliminate the dependence on\na particular set of training dataset, we have to compute the expectation across all\ntraining datasets,\nEDEout(\u02c6fD)=bias+var\nwhere\nbias(x)=(\u02c6f(x)\u2212f(x))2\nand\nvar(x)=ED(\u02c6fD(x)\u2212\u02c6f(x))2", "272": "4.3 Theory of Learning 261\nand where \u02c6fis the mean of all estimators for all datasets. There is nothing to say that\nsuch a mean is an estimator that could have arisen from any particular training data,\nhowever. It just implies that for any particular point x, the mean of the values of all the\nestimators is \u02c6f(x). Therefore, bias captures the sense that, even if all possible data\nwere presented to the learning method, it would still differ from the target function\nby this amount. On the other hand, var shows the variation in the \ufb01nal hypothesis,\ndepending on the training data set, notwithstanding the target function. Thus, the\ntension between approximation and generalization is captured by these two terms.\nFor example, suppose there is only one hypothesis. Then, var=0 because there\ncan be no variation due to a particular set of training data because no matter what\nthat training data is, the learning method always selects the one and only hypothesis.\nIn this case, the bias could be very large, because there is no opportunity for thelearning method to alter the hypothesis due to the training data, and the method can\nonly ever pick the single hypothesis!\nLet\u2019s construct an example to make this concrete. Suppose we have a hypothesis set\nconsisting of all linear regressions without an intercept term, h(x)=ax. The training\ndata consists of only two points {(x\ni,sin(\u03c0xi))}2\ni=1where xiis drawn uniformly from\nthe interval [\u22121,1]. From Sect. 3.7on linear regression, we know that the solution\nforais the following:\na=xTy\nxTx(4.3.5.1)\nFig. 4.12 This shows the folds and errors as in Figs. 4.10 and4.11. The shaded areas show the\nerrors in each respective test set for the quadratic model", "273": "262 4 Machine Learning\nFig. 4.13 This shows the folds and errors. The shaded areas show the errors in each respective test\nset for the cubic model\nwhere x=[x1,x2]andy=[y1,y2].T h e \u02c6f(x)represents the solution over all\npossible sets of training data for a \ufb01xed x. The following code shows how to construct\nthe training data,\n>>> from scipy import stats\n>>> def gen_sindata (n=2):\n... x=stats .uniform( -1,2)# define random variable\n... v=x.rvs((n, 1)) # generate sample\n... y=np.sin(np .pi*v) # use sample for sine\n... return (v,y)\n...\nAgain, using Scikit-learn\u2019s LinearRegression object, we can compute the aparam-\neter. Note that we have to set fit_intercept=False keyword to suppress the\ndefault automatic \ufb01tting of the intercept.\n>>>lr=LinearRegression(fit_intercept =False )\n>>>lr.fit( *gen_sindata( 2))\nLinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\nnormalize=False)\n>>>lr.coef_\narray([[0.24974914]])", "274": "4.3 Theory of Learning 263\nFig. 4.14 For a two-element\ntraining set consisting of thepoints shown, the line is thebest \ufb01t over the hypothesis set,\nh(x)=ax\nProgramming Tip\nNote that we designed gen_sindata to return a tuple to use the automatic\nunpacking feature of Python functions in lr.fit(*gen_sindata()) . In other\nwords, using the asterisk notation means we don\u2019t have to separately assign the\noutputs of gen_sindata before using them for lr.fit .\nIn this case, \u02c6f(x)=ax, where athe expected value of the parameter over allpossible\ntraining datasets. Using our knowledge of probability, we can write this out explicitlyas the following (Fig. 4.14):\na=E/parenleftBigg\nx1sin(\u03c0x1)+x2sin(\u03c0x2)\nx2\n1+x2\n2/parenrightBigg\nwhere x=[x1,x2]andy=[sin(\u03c0x1),sin(\u03c0x2)]in Eq. ( 4.3.5.1 ). However, comput-\ning this expectation analytically is hard, but for this speci\ufb01c situation, a\u22481.43. To\nget this value using simulation, we just loop over the process, collect the outputs,\nand the average them as in the following:\n>>>a_out =[]# output container\n>>> for iinrange (100):\n... _=lr.fit(*gen_sindata( 2))\n... a_out .append(lr .coef_[ 0,0])\n...>>>np.mean(a_out) # approx 1.43\n1.5476180748170179", "275": "264 4 Machine Learning\nFig. 4.15 These curves\ndecompose the mean squared\nerror into its constituent bias\nand variance for this example\nNote that you may have to loop over many more iterations to get close to the purported\nvalue. The var requires the variance of a,\nvar(x)=E((a\u2212a)x)2=x2E(a\u2212a)2\u22480.71x2\nThebias is the following:\nbias(x)=(sin(\u03c0x)\u2212ax)2\nFigure 4.15 shows the bias ,var, and mean squared error for this problem. Notice\nthat there is zero bias and zero variance when x=0. This is because the learning\nmethod cannot help but get that correct because all the hypotheses happen to match\nthe value of the target function at that point! Likewise, the var is zero because all\npossible pairs, which constitute the training data, are \ufb01tted through zero because\nh(x)=axhas no choice but to go through zero. The errors are worse at the end\npoints. As we discussed in our statistics chapter, those points have the most leverage\nagainst the hypothesized models and result in the worst errors. Notice that reducing\nthe edge-errors depends on getting exactly those points near the edges as training\ndata. The sensitivity to a particular dataset is re\ufb02ected in this behavior.\nWhat if we had more than two points in the training data? What would happen\ntovar andbias ? Certainly, the var would decrease because it would be harder and\nharder to generate training datasets that would be substantially different from each\nother. The bias would also decrease because more points in the training data means\nbetter approximation of the sine function over the interval. What would happen if\nwe changed the hypothesis set to include more complex polynomials? As we have\nalready seen with our polynomial regression earlier in this chapter, we would see the\nsame overall effect as here, but with relatively smaller absolute errors and the same\nedge effects we noted earlier.", "276": "4.3 Theory of Learning 265\n4.3.6 Learning Noise\nWe have thus far not considered the effect of noise in our analysis of learning. The\nfollowing example should help resolve this. Let\u2019s suppose we have the followingscalar target function,\ny(x)=w\nT\nox+\u03b7\nwhere \u03b7\u223cN(0,\u03c32)is an additive noise term and w,x\u2208Rd. Furthermore, we have\nnmeasurements of y. This means the training set consists of {(xi,yi)}n\ni=1. Stacking\nthe measurements together into a vector format,\ny=Xw o+\u03b7\nwith y,\u03b7\u2208Rn,wo\u2208RdandXcontains xias columns. The hypothesis set consists\nof all linear models,\nh(w,x)=wTx\nWe need to learn the correct wfrom the hypothesis set given the training data. So\nfar, this is the usual setup for the problem, but how does the noise factor play tothis? In our usual situation, the training set consists of randomly chosen elements\nfrom a larger space. In this case, that would be the same as getting random sets of\nx\nivectors. That still happens in this case, but the problem is that even if the same\nxiappears twice, it will not be associated with the same yvalue due to the additive\nnoise coming from \u03b7. To keep this simple, we assume that there is a \ufb01xed set of xi\nvectors and that we get all of them in the training set. For every speci\ufb01c training set,\nwe know how to solve for the MMSE from our earlier statistics work,\nw=(XTX)\u22121XTy\nGiven this setup, what is the in-sample mean squared error? Because this is the\nMMSE solution, we know from our study of the associated orthogonality of such\nsystems that we have,\nEin=/bardbly/bardbl2\u2212/bardblXw/bardbl2(4.3.6.1)\nwhere our best hypothesis, h=Xw. Now, we want to compute the expectation of\nthis over the distribution of \u03b7. For instance, for the \ufb01rst term, we want to compute,\nE|y|2=1\nnE(yTy)=1\nnTr E(yyT)", "277": "266 4 Machine Learning\nwhere Tr is the matrix trace operator (i.e., sum of the diagonal elements). Because\neach\u03b7is independent, we have\nTr E(yyT)=TrXw owT\noXT+\u03c32TrI=TrXw owT\noXT+n\u03c32(4.3.6.2)\nwhere Iis the n\u00d7nidentity matrix. For the second term in Eq. ( 4.3.6.1 ), we have\n|Xw|2=TrXwwTXT=TrX(XTX)\u22121XTyyTX(XTX)\u22121XT\nThe expectation of this is the following:\nE|Xw|2=TrX(XTX)\u22121XTE(yyT)X(XTX)\u22121XT(4.3.6.3)\nwhich, after substituting in Eq. ( 4.3.6.2 ), yields,\nE|Xw|2=TrXw owT\noXT+\u03c32d (4.3.6.4)\nNext, assembling Eq. ( 4.3.6.1 ) from this and Eq. ( 4.3.6.2 )g i v e s\nE(Ein)=1\nnEin=\u03c32/parenleftbigg\n1\u2212d\nn/parenrightbigg\n(4.3.6.5)\nwhich provides an explicit relationship between the noise power, \u03c32, the complexity\nof the method ( d) and the number of training samples ( n). This is very illustrative\nbecause it reveals the ratio d/n, which is a statement of the trade-off between model\ncomplexity and in-sample data size. From our analysis of the VC dimension, we\nalready know that there is a complicated bound that represents the penalty for com-\nplexity, but this problem is unusual in that we can actually derive an expression forthis without resorting to bounding arguments. Furthermore, this result shows, that\nwith a very large number of training examples ( n\u2192\u221e ), the expected in-sample\nerror approaches \u03c3\n2. Informally, this means that the learning method cannot general-\nizefrom noise and thus can only reduce the expected in-sample error by memorizing\nthe data (i.e., d\u2248n).\nThe corresponding analysis for the expected out-of-sample error is similar, but\nmore complicated because we don\u2019t have the orthogonality condition. Also, the out-\nof-sample data has different noise from that used to derive the weights, w. This results\nin extra cross-terms,\nEout=Tr/parenleftbigg\nXw owT\noXT+\u03be\u03beT+XwwTXT\u2212XwwT\noXT\n\u2212Xw owTXT/parenrightbigg\n(4.3.6.6)", "278": "4.3 Theory of Learning 267\nwhere we are using the \u03benotation for the noise in the out-of-sample case, which is\ndifferent from that in the in-sample case. Simplifying this leads to the following:\nE(Eout)=Tr\u03c32I+\u03c32X(XTX)\u22121XT(4.3.6.7)\nThen, assembling all of this gives\nE(Eout)=\u03c32/parenleftbigg\n1+d\nn/parenrightbigg\n(4.3.6.8)\nwhich shows that even in the limit of large n, the expected out-of-sample error also\napproaches the noise power limit, \u03c32. This shows that memorizing the in-sample data\n(i.e., d/n\u22481) imposes a proportionate penalty on the out-of-sample performance\n(i.e., EEout\u22482\u03c32when EEin\u22480).\nThe following code simulates this important example:\n>>> def est_errors (d=3,n=10,niter =100):\n... assert n>d\n... wo=np.matrix(arange(d)) .T\n... Ein =list()\n... Eout =list()\n... # choose any set of vectors\n... X=np.matrix(np .random .rand(n,d))\n... for ni inrange (niter):\n... y=X*wo+np.random .randn(X .shape[ 0],1)\n... # training weights\n... w=np.linalg .inv(X .T*X)*X.T*y\n... h=X*w\n... Ein.append(np .linalg .norm(h -y)**2)\n... # out of sample error\n... yp=X*wo+np.random .randn(X .shape[ 0],1)\n... Eout.append(np .linalg .norm(h -yp)**2)\n... return (np.mean(Ein) /n,np.mean(Eout) /n)\n...\nProgramming Tip\nPython has an assert statement to make sure that certain entry conditions for\nthe variables in the function are satis\ufb01ed. It is a good practice to use reasonable\nassertions at entry and exit to improve the quality of code.\nThe following runs the simulation for the given value of d.", "279": "268 4 Machine Learning\nFig. 4.16 The dots show the learning curves estimated from the simulation and the solid lines\nshow the corresponding terms for our analytical result. The horizontal line shows the variance ofthe additive noise ( \u03c3\n2=1 in this case). Both the expected in-sample and out-of-sample errors\nasymptotically approach this line\n>>>d=10\n>>>xi=arange(d *2,d*10,d//2)\n>>>ei,eo =np.array([est_errors(d =d,n=n,niter =100 )for ninxi]) .T\nwhich results in Fig. 4.16. This \ufb01gure shows the estimated expected in-sample and\nout-of-sample errors from our simulation compared with our corresponding analyti-cal result. The heavy horizontal line shows the variance of the additive noise \u03c3\n2=1.\nBoth these curves approach this asymptote because the noise is the ultimate learning\nlimit for this problem. For a given dimension d, even with an in\ufb01nite amount of\ntraining data, the learning method cannot generalize beyond the limit of the noise\npower. Thus, the expected generalization error is E(Eout)\u2212E(Ein)=2\u03c32d\nn.\n4.4 Decision Trees\nA decision tree is the easiest classi\ufb01er to understand, interpret, and explain. A decision\ntree is constructed by recursively splitting the dataset into a sequence of subsets basedon if-then questions. The training set consists of pairs (x,y)where x\u2208R\ndwhere\ndis the number of features available and where yis the corresponding label. The\nlearning method splits the training set into groups based on xwhile attempting to\nkeep the assignments in each group as uniform as possible. In order to do this, the\nlearning method must pick a feature and an associated threshold for that feature uponwhich to divide the data. This is tricky to explain in words, but easy to see with an\nexample. First, let\u2019s set up the Scikit-learn classi\ufb01er,", "280": "4.4 Decision Trees 269\n>>> from sklearn import tree\n>>>clf =tree.DecisionTreeClassifier()\nLet\u2019s also create some example data,\n>>> import numpy as np\n>>>M=np.fromfunction( lambda i,j:j >=2,(4,4)).astype( int)\n>>> print (M)\n[[0 0 1 1]\n[0 0 1 1]\n[0 0 1 1]\n[0 0 1 1]]\nProgramming Tip\nThefromfunction creates Numpy arrays using the indices as inputs to a\nfunction whose value is the corresponding array entry.\nWe want to classify the elements of the matrix based on their respective positions inthe matrix. By just looking at the matrix, the classi\ufb01cation is pretty simple\u2014classifyas0for any positions in the \ufb01rst two columns of the matrix, and classify 1otherwise.\nLet\u2019s walk through this formally and see if this solution emerges from the decision\ntree. The values of the array are the labels for the training set and the indices ofthose values are the elements of x. Speci\ufb01cally, the training set has X={(i,j)}and\nY={0,1}Now, let\u2019s extract those elements and construct the training set.\n>>>i,j =np.where(M ==0)\n>>>x=np.vstack([i,j]) .T# build nsamp by nfeatures\n>>>y=j.reshape( -1,1)*0# 0 elements\n>>> print (x)\n[[0 0]\n[0 1][1 0]\n[1 1]\n[2 0][2 1]\n[3 0]\n[3 1]]\n>>> print (y)\n[[0]\n[0]\n[0]\n[0][0]\n[0]", "281": "270 4 Machine Learning\n[0]\n[0]]\nThus, the elements of xare the two-dimensional indices of the values of y.F o r\nexample, M[x[0,0],x[0,1]]=y[0,0] . Likewise, to complete the training set, we\njust need to stack the rest of the data to cover all the cases,\n>>>i,j =np.where(M ==1)\n>>>x=np.vstack([np .vstack([i,j]) .T,x ]) # build nsamp x nfeatures\n>>>y=np.vstack([j .reshape( -1,1)*0+1 ,y]) # 1 elements\nWith all that established, all we have to do is train the classi\ufb01er:\n>>>clf.fit(x,y)\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\nmax_features=None, max_leaf_nodes=None,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0, presort=False, random_state=None,splitter='best')\nTo evaluate how the classi\ufb01er performed, we can report the score,\n>>>clf.score(x,y)\n1.0\nFor this classi\ufb01er, the score is the accuracy, which is de\ufb01ned as the ratio of the sum of\nthe true-positive ( TP) and true-negatives ( TN) divided by the sum of all the terms,\nincluding the false terms,\naccuracy =TP+TN\nTP+TN+FN+FP\nIn this case, the classi\ufb01er gets every point correctly, so FN=FP=0. On a related\nnote, two other common names from information retrieval theory are recall (a.k.a.\nsensitivity) and precision (a.k.a. positive predictive value, TP/(TP+FP)). We can\nvisualize this tree in Fig. 4.17. The Gini coef\ufb01cients (a.k.a. categorical variance) in\nthe \ufb01gure are a measure of the purity of each so-determined class. This coef\ufb01cient\nis de\ufb01ned as,\nGini m=/summationdisplay\nkpm,k(1\u2212pm,k)\nwhere\npm,k=1\nNm/summationdisplay\nxi\u2208RmI(yi=k)\nwhich is the proportion of observations labeled kin the mthnode and I(\u00b7)is the\nusual indicator function. Note that the maximum value of the Gini coef\ufb01cient is", "282": "4.4 Decision Trees 271\nFig. 4.17 Example decision\ntree. The Gini coef\ufb01cient\nin each branch measures thepurity of the partition in eachnode. The samples item in\nthe box shows the number ofitems in the correspondingnode in the decision tree\nmaxGini m=1\u22121/m. For our simple example, half of the sixteen samples are in\ncategory 0and the other half are in the 1category. Using the notation above, the top\nbox corresponds to the 0thnode, so p0,0=1/2=p0,1. Then, Gini 0=0.5. The\nnext layer of nodes in Fig. 4.17 is determined by whether or not the second dimension\nof the xdata is greater than 1.5. The Gini coef\ufb01cients for each of these child nodes\nis zero because after the prior split, each subsequent category is pure. The value list\nin each of the nodes shows the distribution of elements in each category at each node.\nTo make this example more interesting, we can contaminate the data slightly,\n>>>M[1,0]=1# put in different class\n>>> print (M) # now contaminated\n[[0 0 1 1]\n[1 0 1 1][0 0 1 1]\n[0 0 1 1]]\nNow we have a 1entry in the previously pure \ufb01rst column\u2019s second row. Let\u2019s re-do\nthe analysis as in the following:\n>>>i,j =np.where(M ==0)\n>>>x=np.vstack([i,j]) .T\n>>>y=j.reshape( -1,1)*0\n>>>i,j =np.where(M ==1)\n>>>x=np.vstack([np .vstack([i,j]) .T,x])\n>>>y=np.vstack([j .reshape( -1,1)*0+1 ,y])\n>>>clf.fit(x,y)\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\nmax_features=None, max_leaf_nodes=None,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0, presort=False, random_state=None,splitter='best')\nThe result is shown in Fig. 4.18. Note the tree has grown signi\ufb01cantly due to this one\nchange! The 0thnode has the following parameters, p0,0=7/16 and p0,1=9/16.\nThis makes the Gini coef\ufb01cient for the 0thnode equal to7\n16/parenleftbig\n1\u22127\n16/parenrightbig\n+9\n16(1\u22129\n16)=\n0.492. As before, the root node splits on X[1]\u22641.5. Let\u2019s see if we can reconstruct\nthe succeeding layer of nodes manually, as in the following:", "283": "272 4 Machine Learning\n>>>y[x[:, 1]>1.5]# first node on the right\narray([[1],\n[1],[1],\n[1],\n[1],[1],\n[1],\n[1]])\nThis obviously has a zero Gini coef\ufb01cient. Likewise, the node on the left contains\nthe following:\n>>>y[x[:, 1]<=1.5 ]# first node on the left\narray([[1],\n[0],[0],\n[0],\n[0],[0],\n[0],\n[0]])\nThe Gini coef\ufb01cient in this case is computed as (1/8)*(1-1/8)+(7/8)*\n(1-7/8)=0.21875 . This node splits based on X[1]<0.5 . The child node to the\nright derives from the following equivalent logic,\n>>>np.logical_and(x[:, 1]<=1.5 ,x[:, 1]>0.5 )\narray([False, False, False, False, False, False, False, False, False,\nFalse, True, True, False, True, False, True])\nwith corresponding classes,\n>>>y[np.logical_and(x[:, 1]<=1.5 ,x[:, 1]>0.5)]\narray([[0],\n[0],\n[0],[0]])\nProgramming Tip\nThelogical_and in Numpy provides element-wise logical conjunction. It\nis not possible to accomplish this with something like 0.5< x[:,1] <=1.5\nbecause of the way Python parses this syntax.", "284": "4.4 Decision Trees 273\nFig. 4.18 Decision tree for\ncontaminated data. Note thatjust one change in the trainingdata caused the tree to grow\ufb01ve times as large as before!\nNotice that for this example as well as for the previous one, the decision tree\nwas exactly able to memorize (over\ufb01t) the data with perfect accuracy. From ourdiscussion of machine learning theory, this is an indication of potential problems in\ngeneralization.\nThe key step in building the decision tree is to come up with the initial split. There\nare a number of algorithms that can build decision trees based on different criteria,\nbut the general idea is to control the information entropy as the tree is developed.\nIn practical terms, this means that the algorithms attempt to build trees that are notexcessively deep. It is well established that this is a very hard problem to solve\ncompletely and there are many approaches to it. This is because the algorithms must\nmake global decisions at each node of the tree using the local data available up tothat point.\nFor this example, the decision tree partitions the Xspace into different regions\ncorresponding to different Ylabels as shown in Fig. 4.19. The root node at the top\nof Fig. 4.18 splits the input data based on X[1]\u2264 1.5. This corresponds to the top\nleft panel in Fig. 4.19 (i.e.,node 0 ) where the vertical line divides the training data\nshown into two regions, corresponding to the two subsequent child nodes. The next\nsplit happens with X[1]\u2264 0.5 as shown in the next panel of Fig. 4.19 titled node\n1. This continues until the last panel on the lower right, where the contaminated\nelement we injected has been isolated into its own sub-region. Thus, the last panel\nis a representation of Fig. 4.18, where the horizontal/vertical lines correspond to\nsuccessive splits in the decision tree.\nFigure 4.20 shows another example, but now using a simple triangular matrix.\nAs shown by the number of vertical and horizontal partitioning lines, the decision", "285": "274 4 Machine Learning\nFig. 4.19 The decision tree divides the training set into regions by splitting successively along each\ndimension until each region is as pure as possible\nFig. 4.20 The decision tree\n\ufb01tted to this triangular matrixis very complex, as shownby the number of horizontaland vertical partitions. Thus,even though the pattern inthe training data is visuallyclear, the decision tree cannotautomatically uncover it\ntree that corresponds to this \ufb01gure is tall and complex. Notice that if we apply\na simple rotational transform to the training data, we can obtain Fig. 4.21, which\nrequires a trivial decision tree to \ufb01t. Thus, there may be transformations of the training\ndata that simplify the decision tree, but these are very dif\ufb01cult to derive in general.\nNonetheless, this highlights a key weakness of decision trees wherein they may beeasy to understand, to train, and to deploy, but may be completely blind to such\ntime-saving and complexity-saving transformations. Indeed, in higher dimensions,", "286": "4.4 Decision Trees 275\nFig. 4.21 Using a simple\nrotation on the training data inFig.4.20, the decision tree can\nnow easily \ufb01t the training datawith a single partition\nit may be impossible to even visualize the potential of such latent transformations.\nThus, the advantages of decision trees can be easily outmatched by other methods\nthat we will study later that dohave the ability to uncover useful transformations,\nbut which will necessarily be harder to train. Another disadvantage is that becauseof how decision trees are built, even a single misplaced data point can cause the tree\nto grow very differently. This is a symptom of high variance.\nIn all of our examples, the decision tree was able to memorize the training data\nexactly, as we discussed earlier, this is a sign of potentially high generalization errors.\nThere are pruning algorithms that strategically remove some of the deepest nodes. but\nthese are not yet fully implemented in Scikit-learn, as of this writing. Alternatively,\nrestricting the maximum depth of the decision tree can have a similar effect. The\nDecisionTreeClassifier andDecisionTreeRegressor in Scikit-learn both\nhave keyword arguments that specify maximum depth.\n4.4.1 Random Forests\nIt is possible to combine a set of decision trees into a larger composite tree that\nhas better performance than its individual components by using ensemble learning.\nThis is implemented in Scikit-learn as RandomForestClassifier . The composite\ntree helps mitigate the primary weakness of decision trees\u2014high variance. Random\nforest classi\ufb01ers help by averaging out the predictions of many constituent trees to\nminimize this variance by randomly selecting subsets of the training set to train theembedded trees. On the other hand, this randomization can increase bias because\nthere may be a subset of the training set that yields an excellent decision tree, but\nthe averaging effect over randomized training samples washes this out in the same\naveraging that reduces the variance. This is a key trade-off. The following code\nimplements a simple random forest classi\ufb01er from our last example.", "287": "276 4 Machine Learning\nFig. 4.22 The constituent\ndecision trees of the randomforest and how they partitionedthe training set are shownin these four panels. Therandom forest classi\ufb01er usesthe individual outputs of eachof the constituent trees toproduce a collaborative \ufb01nalestimate\n>>> from sklearn.ensemble import RandomForestClassifier\n>>>rfc =RandomForestClassifier(n_estimators =4,max_depth =2)\n>>>rfc.fit(X_train,y_train .flat)\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\nmax_depth=2, max_features='auto', max_leaf_nodes=None,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0, n_estimators=4, n_jobs=None,oob_score=False, random_state=None, verbose=0,warm_start=False)\nNote that we have constrained the maximum depth max_depth=2 to help with gen-\neralization. To keep things simple we have only set up a forest with four individual\nclassi\ufb01ers.3Figure 4.22 shows the individual classi\ufb01ers in the forest that have been\ntrained above. Even though all the constituent decision trees share the same trainingdata, the random forest algorithm randomly picks feature subsets (with replacement)\nupon which to train individual trees. This helps avoid the tendency of decision trees\nto become too deep and lopsided, which hurts both performance and generalization.At the prediction step, the individual outputs of each of the constituent decision\ntrees are put to a majority vote for the \ufb01nal classi\ufb01cation. To estimate generalization\nerrors without using cross-validation, the training elements notused for a particular\nconstituent tree can be used to test that tree and form a collaborative estimate of\ngeneralization errors. This is called the out-of-bag estimate.\nThe main advantage of random forest classi\ufb01ers is that they require very little\ntuning and provide a way to trade-off bias and variance via averaging and random-\n3We have also set the random seed to a \ufb01xed value to make the \ufb01gures reproducible in the Jupyter\nNotebook corresponding to this section.", "288": "4.4 Decision Trees 277\nization. Furthermore, they are fast and easy to train in parallel (see the n_jobs\nkeyword argument) and fast to predict. On the downside, they are less interpretable\nthan simple decision trees. There are many other powerful tree methods in Scikit-learn like ExtraTrees and Gradient Boosted Regression Trees GradientBoosting\nRegressor which are discussed in the online documentation.\n4.4.2 Boosting Trees\nTo understand additive modeling using trees, recall the Gram\u2013Schmidt orthogonal-\nization procedure for vectors. The purpose of this orthogonalization procedure is tocreate an orthogonal set of vectors starting with a given vector u\n1. We have already\ndiscussed the projection operator in Sect. 2.2. The Gram\u2013Schmidt orthogonalization\nprocedure starts with a vector v1, which we de\ufb01ne as the following:\nu1=v1\nwith the corresponding projection operator proj u1. The next step in the procedure is\nto remove the residual of u1from v2, as in the following:\nu2=v2\u2212proj u1(v2)\nThis procedure continues for v3as in the following:\nu3=v3\u2212proj u1(v3)\u2212proj u2(v3)\nand so on. The important aspect of this procedure is that new incoming vectors\n(i.e., vk) are stripped of any preexisting components already present in the set of\n{u1,u2,..., uM}.\nNote that this procedure is sequential. That is, the order of the incoming\nvimatters.4Thus, any new vector can be expressed using the so-constructed\n{u1,u2,..., uM}basis set, as in the following:\nx=/summationdisplay\n\u03b1iui\nThe idea behind additive trees is to reproduce this procedure for trees instead of\nvectors. There are many natural topological and algebraic properties that we lack\nfor the general problem, however. For example, we already have well-established\nmethods for measuring distances between vectors for the Gram\u2013Schmidt procedureoutlined above (namely, the L\n2distance), which we lack here. Thus, we need the\nconcept of loss function , which is a way of measuring how well the process is working\nout at each sequential step. This loss function is parameterized by the training data and\n4At least up to a rotation of the resulting orthonormal basis.", "289": "278 4 Machine Learning\nby the classi\ufb01cation function under consideration: Ly(f(x)). For example, if we want\na classi\ufb01er ( f) that selects the label yibased upon the input data xi(f:xi\u2192yi),\nthen the squared error loss function would be the following:\nLy(f(x))=/summationdisplay\ni(yi\u2212f(xi))2\nWe represent the classi\ufb01er in terms of a set of basis trees:\nf(x)=/summationdisplay\nk\u03b1kux(\u03b8k)\nThe general algorithm for forward stage-wise additive modeling is the following:\n\u2022Initialize f(x)=0\n\u2022Form=1t o m=M, compute the following:\n(\u03b2m,\u03b3m)=arg min\n\u03b2,\u03b3/summationdisplay\niL(yi,fm\u22121(xi)+\u03b2b(xi;\u03b3))\n\u2022Set fm(x)=fm\u22121(x)+\u03b2mb(x;\u03b3m)\nThe key point is that the residuals from the prior step are used to \ufb01t the basis function\nfor the subsequent iteration. That is, the following equation is being sequentially\napproximated.\nfm(x)\u2212fm\u22121(x)=\u03b2mb(xi;\u03b3m)\nLet\u2019s see how this works for decision trees and the exponential loss function.\nL(x,f(x))=exp(\u2212yf(x))\nRecall that for the classi\ufb01cation problem, y\u2208{ \u2212 1,1}. For AdaBoost, the basis func-\ntions are the individual classi\ufb01ers, Gm(x)/mapsto\u2192{ \u2212 1,1}The key step in the algorithm\nis the minimization step for the objective function\nJ(\u03b2,G)=/summationdisplay\niexp(yi(fm\u22121(xi)+\u03b2G(xi)))\n(\u03b2m,Gm)=arg min\n\u03b2,G/summationdisplay\niexp(yi(fm\u22121(xi)+\u03b2G(xi)))\nNow, because of the exponential, we can factor out the following:\nw(m)\ni=exp(yifm\u22121(xi))", "290": "4.4 Decision Trees 279\nas a weight on each data element and re-write the objective function as the following:\nJ(\u03b2,G)=/summationdisplay\niw(m)\niexp(yi\u03b2G(xi))\nThe important observation here is that yiG(xi)/mapsto\u21921 if the tree classi\ufb01es xicorrectly\nand yiG(xi)/mapsto\u2192\u2212 1 otherwise. Thus, the above sum has terms like the following:\nJ(\u03b2,G)=/summationdisplay\nyi/negationslash=G(xi)w(m)\niexp(\u2212\u03b2)+/summationdisplay\nyi=G(xi)w(m)\niexp(\u03b2)\nFor\u03b2>0, this means that the best G(x)is the one that incorrectly classi\ufb01es for the\nlargest weights. Thus, the minimizer is the following:\nGm=arg min\nG/summationdisplay\niw(m)\niI(yi/negationslash=G(xi))\nwhere Iis the indicator function (i.e., I(True)=1,I(False)=0).\nFor\u03b2>0, we can re-write the objective function as the following:\nJ=(exp(\u03b2)\u2212exp(\u2212\u03b2))/summationdisplay\niw(m)\niI(yi/negationslash=G(xi))+exp(\u2212\u03b2)/summationdisplay\niw(m)\ni\nand substitute \u03b8=exp(\u2212\u03b2)so that\nJ\n/summationtext\niw(m)\ni=/parenleftbigg1\n\u03b8\u2212\u03b8/parenrightbigg\n/epsilon1m+\u03b8 (4.4.2.1)\nwhere\n/epsilon1m=/summationtext\niw(m)\niI(yi/negationslash=G(xi))\n/summationtext\niw(m)\ni\nis the error rate of the classi\ufb01er with 0 \u2264/epsilon1m\u22641. Now, \ufb01nding \u03b2is a straightforward\ncalculus minimization exercise on the right side of Eq. ( 4.5.1.1 ), which gives the\nfollowing:\n\u03b2m=1\n2log1\u2212/epsilon1m\n/epsilon1m\nImportantly, \u03b2mcan become negative if /epsilon1m<1\n2, which would violate our assumptions\non\u03b2. This is captured in the requirement that the base learner be better than just\nrandom guessing, which would correspond to /epsilon1m>1\n2. Practically speaking, this", "291": "280 4 Machine Learning\nmeans that boosting cannot \ufb01x a base learner that is no better than a random guess.\nFormally speaking, this is known as the empirical weak learning assumption [3].\nNow we can move to the iterative weight update. Recall that\nw(m+1)\ni=exp(yifm(xi))=w(m)\niexp(yi\u03b2mGm(xi))\nwhich we can re-write as the following:\nw(m+1)\ni=w(m)\niexp(\u03b2m)exp(\u22122\u03b2mI(Gm(xi)=yi))\nThis means that the data elements that are incorrectly classi\ufb01ed have their corre-\nsponding weights increased by exp (\u03b2m)and those that are correctly classi\ufb01ed have\ntheir corresponding weights reduced by exp (\u2212\u03b2m). The reason for the choice of the\nexponential loss function comes from the following:\nf\u2217(x)=arg min\nf(x)EY|x(exp(\u2212Yf(x)))=1\n2logP(Y=1|x)\nP(Y=\u20131|x)\nThis means that boosting is approximating a f(x)that is actually half the log-odds\nof the conditional class probabilities. This can be rearranged as the following\nP(Y=1|x)=1\n1+exp(\u20132f\u2217(x))\nThe important bene\ufb01t of this general formulation for boosting, as a sequence of\nadditive approximations, is that it opens the door to other choices of loss function,\nespecially loss functions that are based on robust statistics that can account for errors\nin the training data (c.f. Hastie).\nGradient Boosting . Given a differentiable loss function, the optimization process\ncan be formulated using numerical gradients. The fundamental idea is to treat the\nf(xi)as a scalar parameter to be optimized over. Generally speaking, we can think\nof the following loss function,\nL(f)=N/summationdisplay\ni=1L(yi,f(xi))\nas a vectorized quantity\nf={f(x1),f(x2),... , f(xN)}\nso that the optimization is over this vector\n\u02c6f=arg min\nfL(f)", "292": "4.4 Decision Trees 281\nWith this general formulation we can use numerical optimization methods to solve\nfor the optimal fas a sum of component vectors as in the following:\nfM=M/summationdisplay\nm=0hm\nNote that this leaves aside the prior assumption that fis parameterized as a sum of\nindividual decision trees.\ngi,m=/bracketleftbigg\u2202L(yi,f(xi))\n\u2202f(xi)/bracketrightbigg\nf(xi)=fm\u22121(xi)\n4.5 Boosting Trees\n4.5.1 Boosting Trees\nTo understand additive modeling using trees, recall the Gram\u2013Schmidt orthogonal-\nization procedure for vectors. The purpose of this orthogonalization procedure is to\ncreate an orthogonal set of vectors starting with a given vector u1. We have already\ndiscussed the projection operator in Sect. 2.2. The Gram\u2013Schmidt orthogonalization\nprocedure starts with a vector v1, which we de\ufb01ne as the following:\nu1=v1\nwith the corresponding projection operator proj u1. The next step in the procedure is\nto remove the residual of u1from v2, as in the following:\nu2=v2\u2212proj u1(v2)\nThis procedure continues for v3as in the following:\nu3=v3\u2212proj u1(v3)\u2212proj u2(v3)\nand so on. The important aspect of this procedure is that new incoming vectors\n(i.e., vk) are stripped of any preexisting components already present in the set of\n{u1,u2,..., uM}.\nNote that this procedure is sequential. That is, the order of the incoming\nvimatters.5Thus, any new vector can be expressed using the so-constructed\n{u1,u2,..., uM}basis set, as in the following:\n5At least up to a rotation of the resulting orthonormal basis.", "293": "282 4 Machine Learning\nx=/summationdisplay\n\u03b1iui\nThe idea behind additive trees is to reproduce this procedure for trees instead of\nvectors. There are many natural topological and algebraic properties that we lack\nfor the general problem, however. For example, we already have well-establishedmethods for measuring distances between vectors for the Gram\u2013Schmidt procedure\noutlined above (namely, the L\n2distance), which we lack here. Thus, we need the\nconcept of loss function , which is a way of measuring how well the process is working\nout at each sequential step. This loss function is parameterized by the training data and\nby the classi\ufb01cation function under consideration: Ly(f(x)). For example, if we want\na classi\ufb01er ( f) that selects the label yibased upon the input data xi(f:xi\u2192yi),\nthen the squared error loss function would be the following:\nLy(f(x))=/summationdisplay\ni(yi\u2212f(xi))2\nWe represent the classi\ufb01er in terms of a set of basis trees:\nf(x)=/summationdisplay\nk\u03b1kux(\u03b8k)\nThe general algorithm for forward stage-wise additive modeling is the following:\n\u2022Initialize f(x)=0\n\u2022Form=1t o m=M, compute the following:\n(\u03b2m,\u03b3m)=arg min\n\u03b2,\u03b3/summationdisplay\niL(yi,fm\u22121(xi)+\u03b2b(xi;\u03b3))\n\u2022Set fm(x)=fm\u22121(x)+\u03b2mb(x;\u03b3m)\nThe key point is that the residuals from the prior step are used to \ufb01t the basis function\nfor the subsequent iteration. That is, the following equation is being sequentially\napproximated.\nfm(x)\u2212fm\u22121(x)=\u03b2mb(xi;\u03b3m)\nLet\u2019s see how this works for decision trees and the exponential loss function.\nL(x,f(x))=exp(\u2212yf(x))\nRecall that for the classi\ufb01cation problem, y\u2208{ \u2212 1,1}. For AdaBoost, the basis func-\ntions are the individual classi\ufb01ers, Gm(x)/mapsto\u2192{ \u2212 1,1}The key step in the algorithm\nis the minimization step for the objective function", "294": "4.5 Boosting Trees 283\nJ(\u03b2,G)=/summationdisplay\niexp(yi(fm\u22121(xi)+\u03b2G(xi)))\n(\u03b2m,Gm)=arg min\n\u03b2,G/summationdisplay\niexp(yi(fm\u22121(xi)+\u03b2G(xi)))\nNow, because of the exponential, we can factor out the following:\nw(m)\ni=exp(yifm\u22121(xi))\nas a weight on each data element and re-write the objective function as the following:\nJ(\u03b2,G)=/summationdisplay\niw(m)\niexp(yi\u03b2G(xi))\nThe important observation here is that yiG(xi)/mapsto\u21921 if the tree classi\ufb01es xicorrectly\nand yiG(xi)/mapsto\u2192\u2212 1 otherwise. Thus, the above sum has terms like the following:\nJ(\u03b2,G)=/summationdisplay\nyi/negationslash=G(xi)w(m)\niexp(\u2212\u03b2)+/summationdisplay\nyi=G(xi)w(m)\niexp(\u03b2)\nFor\u03b2>0, this means that the best G(x)is the one that incorrectly classi\ufb01es for the\nlargest weights. Thus, the minimizer is the following:\nGm=arg min\nG/summationdisplay\niw(m)\niI(yi/negationslash=G(xi))\nwhere Iis the indicator function (i.e., I(True)=1,I(False)=0).\nFor\u03b2>0, we can re-write the objective function as the following:\nJ=(exp(\u03b2)\u2212exp(\u2212\u03b2))/summationdisplay\niw(m)\niI(yi/negationslash=G(xi))+exp(\u2212\u03b2)/summationdisplay\niw(m)\ni\nand substitute \u03b8=exp(\u2212\u03b2)so that\nJ\n/summationtext\niw(m)\ni=/parenleftbigg1\n\u03b8\u2212\u03b8/parenrightbigg\n/epsilon1m+\u03b8 (4.5.1.1)\nwhere\n/epsilon1m=/summationtext\niw(m)\niI(yi/negationslash=G(xi))\n/summationtext\niw(m)\ni", "295": "284 4 Machine Learning\nis the error rate of the classi\ufb01er with 0 \u2264/epsilon1m\u22641. Now, \ufb01nding \u03b2is a straightforward\ncalculus minimization exercise on the right side of Eq. ( 4.5.1.1 ), which gives the\nfollowing:\n\u03b2m=1\n2log1\u2212/epsilon1m\n/epsilon1m\nImportantly, \u03b2mcan become negative if /epsilon1m<1\n2, which would violate our assumptions\non\u03b2. This is captured in the requirement that the base learner be better than just\nrandom guessing, which would correspond to /epsilon1m>1\n2. Practically speaking, this\nmeans that boosting cannot \ufb01x a base learner that is no better than a random guess.\nFormally speaking, this is known as the empirical weak learning assumption [3].\nNow we can move to the iterative weight update. Recall that\nw(m+1)\ni=exp(yifm(xi))=w(m)\niexp(yi\u03b2mGm(xi))\nwhich we can re-write as the following:\nw(m+1)\ni=w(m)\niexp(\u03b2m)exp(\u22122\u03b2mI(Gm(xi)=yi))\nThis means that the data elements that are incorrectly classi\ufb01ed have their corre-\nsponding weights increased by exp (\u03b2m)and those that are correctly classi\ufb01ed have\ntheir corresponding weights reduced by exp (\u2212\u03b2m). The reason for the choice of the\nexponential loss function comes from the following:\nf\u2217(x)=arg min\nf(x)EY|x(exp(\u2212Yf(x)))=1\n2logP(Y=1|x)\nP(Y=\u20131|x)\nThis means that boosting is approximating a f(x)that is actually half the log-odds\nof the conditional class probabilities. This can be rearranged as the following\nP(Y=1|x)=1\n1+exp(\u20132f\u2217(x))\nThe important bene\ufb01t of this general formulation for boosting, as a sequence of\nadditive approximations, is that it opens the door to other choices of loss function,\nespecially loss functions that are based on robust statistics that can account for errorsin the training data (c.f. Hastie).\nGradient Boosting . Given a differentiable loss function, the optimization process\ncan be formulated using numerical gradients. The fundamental idea is to treat the\nf(x\ni)as a scalar parameter to be optimized over. Generally speaking, we can think\nof the following loss function,\nL(f)=N/summationdisplay\ni=1L(yi,f(xi))", "296": "4.5 Boosting Trees 285\nas a vectorized quantity\nf={f(x1),f(x2),... , f(xN)}\nso that the optimization is over this vector\n\u02c6f=arg min\nfL(f)\nWith this general formulation we can use numerical optimization methods to solve\nfor the optimal fas a sum of component vectors as in the following:\nfM=M/summationdisplay\nm=0hm\nNote that this leaves aside the prior assumption that fis parameterized as a sum of\nindividual decision trees.\ngi,m=/bracketleftbigg\u2202L(yi,f(xi))\n\u2202f(xi)/bracketrightbigg\nf(xi)=fm\u22121(xi)\n4.6 Logistic Regression\nThe Bernoulli distribution we studied earlier answers the question of which of two\noutcomes ( Y\u2208{0,1}) would be selected with probability, p.\nP(Y)=pY(1\u2212p)1\u2212Y\nWe also know how to solve the corresponding likelihood function for the maximum\nlikelihood estimate of pgiven observations of the output, {Yi}n\ni=1. However, now\nwe want to include other factors in our estimate of p. For example, suppose we\nobserve not just the outcomes, but a corresponding continuous variable, x. That is,\nthe observed data is now {(xi,Yi)}n\ni=1How can we incorporate xinto our estimation\nofp?\nThe most straightforward idea is to model p=ax+bwhere a,bare parameters\nof a \ufb01tted line. However, because pis a probability with value bounded between\nzero and one, we need to wrap this estimate in another function that can map the\nentire real line into the [0,1]interval. The logistic (a.k.a. sigmoid) function has this\nproperty,\n\u03b8(s)=es\n1+es", "297": "286 4 Machine Learning\nThus, the new parameterized estimate for pis the following:\n\u02c6p=\u03b8(ax+b)=eax+b\n1+eax+b(4.6.0.1)\nThe logit function is de\ufb01ned as the following:\nlogit (t)=logt\n1\u2212t\nIt has the important property of extracting the regression components from the prob-\nability estimator,\nlogit (p)=b+ax\nMore continuous variables can be accommodated easily as\nlogit (p)=b+/summationdisplay\nkakxk\nThis can be further extended beyond the binary case to multiple target labels. The\nmaximum likelihood estimate of this uses numerical optimization methods that are\nimplemented in Scikit-learn.\nLet\u2019s construct some data to see how this works. In the following: we assign class\nlabels to a set of randomly scattered points in the two-dimensional plane,\n>>> import numpy as np\n>>> from matplotlib.pylab import subplots\n>>>v= 0.9\n>>>@np.vectorize\n... def gen_y (x):\n... ifx<5:return np.random .choice([ 0,1],p=[v,1-v])\n... else: return np.random .choice([ 0,1],p=[1-v,v])\n...>>>xi=np.sort(np .random .rand( 500)*10)\n>>>yi=gen_y(xi)\nProgramming Tip\nThenp.vectorize decorator used in the code above makes it easy to avoid\nlooping in code that uses Numpy arrays by embedding the looping semantics\ninside of the so-decorated function. Note, however, that this does not necessarily\naccelerate the wrapped function. It\u2019s mainly for convenience.", "298": "4.6 Logistic Regression 287\nFig. 4.23 This scatterplot\nshows the binary Yvariables\nand the corresponding xdata\nfor each category\nFigure 4.23 shows a scatter plot of the data we constructed in the above code,\n{(xi,Yi)}. As constructed, it is more likely that large values of xcorrespond to Y=1.\nOn the other hand, values of x\u2208[4,6]of either category are heavily overlapped. This\nmeans that xis not a particularly strong indicator of Yin this region. Figure 4.24\nshows the \ufb01tted logistic regression curve against the same data. The points along\nthe curve are the probabilities that each point lies in either of the two categories.\nFor large values of xthe curve is near one, meaning that the probability that the\nassociated Yvalue is equal to one. On the other extreme, small values of xmean\nthat this probability is close to zero. Because there are only two possible categories,\nthis means that the probability of Y=0 is thereby higher. The region in the middle\ncorresponding to the middle probabilities re\ufb02ect the ambiguity between the two\ncategories because of the overlap in the data for this region. Thus, logistic regression\ncannot make a strong case for one category here. The following code \ufb01ts the logisticregression model,\n>>> from sklearn.linear_model import LogisticRegression\n>>>lr=LogisticRegression()\n>>>lr.fit(np .c_[xi],yi)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\nintercept_scaling=1, max_iter=100, multi_class='warn',n_jobs=None, penalty='l2', random_state=None, solver='warn',tol=0.0001, verbose=0, warm_start=False)\nFor a deeper understanding of logistic regression, we need to alter our notation\nslightly and once again use our projection methods. More generally we can re-writeEq. ( 4.6.0.1 ) as the following:\np(x)=1\n1+exp(\u2212\u03b2Tx)(4.6.0.2)", "299": "288 4 Machine Learning\nFig. 4.24 This shows the\n\ufb01tted logistic regression onthe data shown in Fig. 4.23.\nThe points along the curveare the probabilities that eachpoint lies in either of the twocategories\nwhere \u03b2,x\u2208Rn. From our prior work on projection we know that the signed per-\npendicular distance between xand the linear boundary described by \u03b2is\u03b2Tx//bardbl\u03b2/bardbl.\nThis means that the probability that is assigned to any point in Rnis a function of\nhow close that point is to the linear boundary described by the following equation,\n\u03b2Tx=0\nBut there is something subtle hiding here. Note that for any \u03b1\u2208R,\n\u03b1\u03b2Tx=0\ndescribes the same hyperplane. This means that we can multiply \u03b2by an arbi-\ntrary scalar and still get the same geometry. However, because of exp (\u2212\u03b1\u03b2Tx)\nin Eq. ( 4.6.0.2 ), this scaling determines the intensity of the probability attributed\ntox. This is illustrated in Fig. 4.25. The panel on the left shows two categories\n(squares/circles) split by the dotted line that is determined by \u03b2Tx=0. The back-\nground colors show the probabilities assigned to points in the plane. The right panel\nshows that by scaling with \u03b1, we can increase the probabilities of class membership\nfor the given points, given the exact same geometry. The points near the boundaryhave lower probabilities because they could easily be on the opposite side. However,\nby scaling by \u03b1, we can raise those probabilities to any desired level at the cost of\ndriving the points further from the boundary closer to one. Why is this a problem?By driving the probabilities arbitrarily using \u03b1, we can overemphasize the training\nset at the cost of out-of-sample data. That is, we may wind up insisting on emphatic\nclass membership of yet unseen points that are close to the boundary that otherwisewould have more equivocal probabilities (say, near 1 /2). Once again, this is another\nmanifestation of bias/variance trade-off.\nRegularization is a method that controls this effect by penalizing the size of \u03b2as\npart of its solution. Algorithmically, logistic regression works by iteratively solving", "300": "4.6 Logistic Regression 289\nFig. 4.25 Scaling can arbitrarily increase the probabilities of points near the decision boundary\na sequence of weighted least-squares problems. Regression adds a /bardbl\u03b2/bardbl/Cterm to\nthe least-squares error. To see this in action, let\u2019s create some data from a logisticregression and see if we can recover it using Scikit-learn. Let\u2019s start with a scatter of\npoints in the two-dimensional plane,\n>>>x0,x1 =np.random .rand( 2,20)*6-3\n>>>X=np.c_[x0,x1,x1 *0+1]# stack as columns\nNote that Xhas a third column of all ones. This is a trick to allow the corresponding\nline to be offset from the origin in the two-dimensional plane. Next, we create a linearboundary and assign the class probabilities according to proximity to the boundary.\n>>> beta =np.array([ 1,-1,1])# last coordinate for affine offset\n>>> prd =X.dot(beta)\n>>> probs =1 / (1+np.exp( -prd/np.linalg .norm(beta)))\n>>> c=(prd >0)# boolean array class labels\nThis establishes the training data. The next block creates the logistic regression object\nand \ufb01ts the data.\n>>>lr=LogisticRegression()\n>>>_=lr.fit(X[:,: -1],c)\nNote that we have to omit the third dimension because of how Scikit-learn inter-\nnally breaks down the components of the boundary. The resulting code extracts thecorresponding \u03b2from the LogisticRegression object.\n>>>betah =np.r_[lr .coef_ .flat,lr .intercept_]", "301": "290 4 Machine Learning\nFig. 4.26 The left panel shows the resulting boundary (dashed line) with C=1 as the regularization\nparameter. The right panel is for C=1000. The gray line is the boundary used to assign the class\nmembership for the synthetic data. The dark circle is the point that logistic regression categorizesincorrectly\nProgramming Tip\nThe Numpy np.r_ object provides a quick way to stack Numpy arrays hori-\nzontally instead of using np.hstack .\nThe resulting boundary is shown in the left panel in Fig. 4.26. The crosses and\ntriangles represent the two classes we created above, along with the separating grayline. The logistic regression \ufb01t produces the dotted black line. The dark circle is the\npoint that logistic regression categorizes incorrectly. The regularization parameter is\nC=1 by default. Next, we can change the strength of the regularization parameter\nas in the following:\n>>>lr=LogisticRegression(C =1000 )\nand the re-\ufb01t the data to produce the right panel in Fig. 4.26. By increasing the\nregularization parameter, we essentially nudged the \ufb01tting algorithm to believe the\ndata more than the general model. That is, by doing this we accepted more variancein exchange for better bias.\nMaximum Likelihood Estimation for Logistic Regression . Let us again consider\nthe binary classi\ufb01cation problem. We de\ufb01ne y\nk=P(C1|xk), the conditional prob-\nability of the data as a member of given class. Our construction of this problem\nprovides\nyk=\u03b8([w,w0]\u00b7[xk,1])", "302": "4.6 Logistic Regression 291\nwhere \u03b8is the logistic function. Recall that there are only two classes for this problem.\nThe dataset looks like the following:\n{(x0,r0),... ,( xk,rk) ,...,( xn\u22121,rn\u22121)}\nwhere rk\u2208{0,1}. For example, we could have the following sequence of observed\nclasses,\n{C0,C1,C1,C0,C1}\nFor this case the likelihood is then the following:\n/lscript=P(C0|x0)P(C1|x1)P(C1|x1)P(C0|x0)P(C1|x1)\nwhich we can re-write as the following:\n/lscript(w,w0)=(1\u2212y0)y1y2(1\u2212y3)y4\nRecall that there are two mutually exhaustive classes. More generally, this can be\nwritten as the following:\n/lscript(w|X)=n/productdisplay\nkyrk\nk(1\u2212yk)1\u2212rk\nNaturally, we want to compute the logarithm of this as the cross-entropy,\nE=\u2212/summationdisplay\nkrklog(yk)+(1\u2212rk)log(1\u2212yk)\nand then minimize this to \ufb01nd wandw0. This is dif\ufb01cult to do with calculus because\nthe derivatives have nonlinear terms in them that are hard to solve for.\nMulti-class Logistic Regression Using Softmax . The logistic regression problem\nprovides a solution for the probability between exactly two alternative classes. Toextend to the multi-class problem, we need the softmax function. Consider the like-\nlihood ratio between the i\nthclass and the reference class, Ck,\nlogp(x|Ci)\np(x|Ck)=wT\nix\nTaking the exponential of this and normalizing across all the classes gives the softmax\nfunction,\nyi=p(Ci|x)=exp/parenleftbig\nwT\nix/parenrightbig\n/summationtext\nkexp/parenleftbig\nwT\nkx/parenrightbig", "303": "292 4 Machine Learning\nNote that/summationtext\niyi=1. If the wT\nixterm is larger than the others, after the exponentiation\nand normalization, it automatically suppresses the other yj\u2200j/negationslash=i, which acts like the\nmaximum function, except this function is differentiable, hence soft,a si n softmax .\nWhile that is all straightforward, the trick is deriving the wivectors from the training\ndata{xi,yi}.\nOnce again, the launching point is the likelihood function. As with the two-class\nlogistic regression problem, we have the likelihood as the following:\n/lscript=/productdisplay\nk/productdisplay\ni(yk\ni)rk\ni\nThe log-likelihood of this is the same as the cross-entropy,\nE=\u2212/summationdisplay\nk/summationdisplay\nirk\nilogyk\ni\nThis is the error function we want to minimize. The computation works as before\nwith logistic regression, except there are more derivatives to keep track of in this case.\nUnderstanding Logistic Regression . To generalize this technique beyond logistic\nregression, we need to re-think the problem more abstractly as the dataset {xi,yi}.\nWe have the yi\u2208{0,1}data modeled as Bernoulli random variables. We also have\nthexidata associated with each yi, but it is not clear how to exploit this association.\nWhat we would like is to construct E(Y|X)which we already know (see Sect. 2.1)\nis the best MSE estimator. For this problem, we have\nE(Y|X)=P(Y|X)\nbecause only Y=1 is nonzero in the summation. Regardless, we don\u2019t have the\nconditional probabilities anyway. One way to look at logistic regression is as a wayto build in the functional relationship between y\niandxi. The simplest thing we could\ndo is approximate,\nE(Y|X)\u2248\u03b20+\u03b21x:=\u03b7(x)\nIf this is the model, then the target would be the yidata. We can force the output\nof this linear regression into the interval [0,1]by composing it with a sigmoidal\nfunction,\n\u03b8(x)=1\n1+exp(\u2212x)\nThen we have a new function \u03b8(\u03b7(x))to match against yiusing\nJ(\u03b20,\u03b21)=/summationdisplay\ni(\u03b8(\u03b7(xi))\u2212yi)2", "304": "4.6 Logistic Regression 293\nThis is a nice setup for an optimization problem. We could certainly solve this\nnumerically using scipy.optimize . Unfortunately, this would take us into the\nblack box of the optimization algorithm where we would lose all of our intuitionsand experience with linear regression. We can take the opposite approach. Instead of\ntrying to squash the output of the linear estimator into the desired domain, we can\nmap the y\nidata into the unbounded space of the linear estimator. Thus, we de\ufb01ne\nthe inverse of the above \u03b8function as the link function.\ng(y)=log/parenleftbiggy\n1\u2212y/parenrightbigg\nThis means that our approximation to the unknown conditional expectation is the\nfollowing:\ng(E(Y|X))\u2248\u03b20+\u03b21x:=\u03b7(x)\nWe cannot apply this directly to the yi, so we compute the Taylor series expansion\ncentered on E(Y|X), up to the linear term, to obtain the following:\ng(Y)\u2248 g(E(Y|X))+(Y\u2212E(Y|X))g/prime(E(Y|X))\n\u2248 \u03b7(x)+(Y\u2212\u03b8(\u03b7(x)))g/prime(\u03b8(\u03b7(x))):=z\nBecause we do not know the conditional expectation, we replaced these terms with\nour earlier \u03b8(\u03b7(x))function. This new approximation de\ufb01nes our transformed data\nthat we will use to feed the linear model. Note that the \u03b2parameters are embedded\nin this transformation. The (Y\u2212\u03b8(\u03b7(x)))term acts as the usual additive noise term.\nAlso,\ng/prime(x)=1\nx(1\u2212x)\nThe following code applies this transformation to the xi,yi data\n>>>b0, b1 =- 2 ,0.5\n>>>g=lambda x: np .log(x /(1-x))\n>>>theta =lambda x:1/(1+np.exp(-x))\n>>>eta =lambda x: b0 +b1*x\n>>>theta_ =theta(eta(xi))\n>>>z=eta(xi) +(yi-theta_) /(theta_ *(1-theta_))\nNote the two vertical scales shown in Fig. 4.27. The red scale on the right is\nthe{0,1}domain of the yidata (red dots) and the left scale is transformed zidata\n(black dots). Note that the transformed data is more linear where the original data\nis less equivocal at the extremes. Also, this transformation used a speci\ufb01c pair of", "305": "294 4 Machine Learning\nFig. 4.27 The transformation\nunderlying logistic regression\n\u03b2iparameters. The idea is to iterate over this transformation and derive new \u03b2i\nparameters. With this approach, we have\nV(Z|X)=(g/prime)2V(Y|X)\nRecall that, for this binary variable, we have\nP(Y|X)=\u03b8(\u03b7(x)))\nThus,\nV(Y|X)=\u03b8(\u03b7(x))(1\u2212\u03b8(\u03b7(x)))\nfrom which we obtain\nV(Z|X)=[\u03b8(\u03b7(x))(1\u2212\u03b8(\u03b7(x)))]\u22121\nThe important fact here is the variance is a function of the X(i.e., heteroskedastic).\nAs we discussed with Gauss\u2013Markov, the appropriate linear regression is weighted\nleast-squares where the weights at each data point are inversely proportional to the\nvariance. This ensures that the regression process accounts for this heteroskedasticity.\nNumpy has a weighted least-squares implemented in the polyfit function,\n>>>w=(theta_ *(1-theta_))\n>>>p=np.polyfit(xi,z, 1,w=np.sqrt(w))\nThe output of this \ufb01t is shown in Fig. 4.28, along with the raw data and V(Z|X)for\nthis particular \ufb01tted \u03b2i. Iterating a few more times re\ufb01nes the estimated line but it\ndoes not take many such iterations to converge. As indicated by the variance line,\nthe \ufb01tted line favors the data at either extreme.", "306": "4.7 Generalized Linear Models 295\n4.7 Generalized Linear Models\nLogistic regression is one example of a wider class of Generalized Linear Models\n(GLMs). These GLMs have the following three key features\n\u2022A target Yvariable distributed according to one of the exponential family of\ndistributions (e.g., Normal, binomial, Poisson)\n\u2022An equation that links the expected value of Ywith a linear combination of the\nobserved variables (i.e., {x1,x2,..., xn}).\n\u2022A smooth invertible link function g(x)such that g(E(Y))=/summationtext\nk\u03b2kxk\nExponential Family . Here is the one-parameter exponential family,\nf(y;\u03bb)=e\u03bby\u2212\u03b3(\u03bb)\nThe natural parameter is\u03bband yis the suf\ufb01cient statistic. For example, for logistic\nregression, we have \u03b3(\u03bb)=\u2212 log(1+e\u03bb)and\u03bb=logp\n1\u2212p.\nAn important property of this exponential family is that\nE\u03bb(y)=d\u03b3(\u03bb)\nd\u03bb=\u03b3/prime(\u03bb) (4.7.0.1)\nTo see this, we compute the following:\n1=/integraldisplay\nf(y;\u03bb)dy=/integraldisplay\ne\u03bby\u2212\u03b3(\u03bb)dy\n0=/integraldisplaydf(y;\u03bb)\nd\u03bbdy=/integraldisplay\ne\u03bby\u2212\u03b3(\u03bb)/parenleftbig\ny\u2212\u03b3/prime(\u03bb)/parenrightbig\ndy\n/integraldisplay\nye\u03bby\u2212\u03b3(\u03bb)dy=E\u03bb(y)=\u03b3/prime(\u03bb)\nUsing the same technique, we also have,\nV\u03bb(Y)=\u03b3/prime/prime(\u03bb)\nwhich explains the usefulness of this generalized notation for the exponential family.\nDeviance . The scaled Kullback\u2013Leibler divergence is called the deviance as de\ufb01ned\nbelow,\nD(f1,f2)=2/integraldisplay\nf1(y)logf1(y)\nf2(y)dy", "307": "296 4 Machine Learning\nHoeffding\u2019s Lemma\nUsing our exponential family notation, we can write out the deviance as the\nfollowing:\n1\n2D(f(y;\u03bb1),f(y;\u03bb2))=/integraldisplay\nf(y;\u03bb1)logf(y;\u03bb1)\nf(y;\u03bb2)dy\n=/integraldisplay\nf(y;\u03bb1)((\u03bb1\u2212\u03bb2)y\u2212(\u03b3(\u03bb1)\u2212\u03b3(\u03bb2)))dy\n=E\u03bb1[(\u03bb1\u2212\u03bb2)y\u2212(\u03b3(\u03bb1)\u2212\u03b3(\u03bb2))]\n=(\u03bb1\u2212\u03bb2)E\u03bb1(y)\u2212(\u03b3(\u03bb1)\u2212\u03b3(\u03bb2))\n=(\u03bb1\u2212\u03bb2)\u03bc1\u2212(\u03b3(\u03bb1)\u2212\u03b3(\u03bb2))\nwhere \u03bc1:= E\u03bb1(y). For the maximum likelihood estimate \u02c6\u03bb1,w eh a v e \u03bc1=y.\nPlugging this into the above equation gives the following:\n1\n2D(f(y;\u02c6\u03bb1),f(y;\u03bb2))=(\u02c6\u03bb1\u2212\u03bb2)y\u2212(\u03b3(\u02c6\u03bb1)\u2212\u03b3(\u03bb2))\n=logf(y;\u02c6\u03bb1)\u2212logf(y;\u03bb2)\n=logf(y;\u02c6\u03bb1)\nf(y;\u03bb2)\nTaking the negative exponential of both sides gives\nf(y;\u03bb2)=f(y;\u02c6\u03bb1)e\u22121\n2D(f(y;\u02c6\u03bb1),f(y;\u03bb2))\nBecause Dis always nonnegative, the likelihood is maximized when the\ndeviance is zero. In particular, for the scalar case, it means that yitself is\nthe best maximum likelihood estimate for the mean. Also, f(y;\u02c6\u03bb1)is called\nthesaturated model. We write Hoeffding\u2019s Lemma as the following:\nf(y;\u03bc)=f(y;y)e\u22121\n2D(f(y;y),f(y;\u03bc))(4.7.0.2)\nto emphasize that f(y;y)is the likelihood function when the mean is replaced\nby the sample itself and f(y;\u03bc)is the likelihood function when the mean is\nreplaced by \u03bc.\nVectorizing Equation ( 4.7.0.2 ) using mutual independence gives the following:\nf(y;\u03bc)=e\u2212/summationtext\niD(yi,\u03bci)/productdisplay\nf(yi;yi)\nThe idea now is to minimize the deviance by deriving,", "308": "4.7 Generalized Linear Models 297\nFig. 4.28 The output of the\nweighted least-squares \ufb01t isshown, along with the rawdata and V(Z|X)\n\u03bc(\u03b2)=g\u22121(MT\u03b2)\nThis means the MLE \u02c6\u03b2is the best p\u00d71 vector \u03b2that minimizes the total deviance\nwhere gis the linkfunction and Mis the p\u00d7n structure matrix. This is the key step\nwith GLM estimation because it reduces the number of parameters from ntop.T h e\nstructure matrix is where the associated xidata enters into the problem. Thus, GLM\nmaximum likelihood \ufb01tting minimizes the total deviance like plain linear regressionminimizes the sum of squares.\nWith the following:\n\u03bb=M\nT\u03b2\nwith 2 \u00d7n-dimensional M. The corresponding joint density function is the following:\nf(y;\u03b2)=e\u03b2T\u03be\u2212\u03c8(\u03b2)f0(y)\nwhere\n\u03be=My\nand\n\u03c8(\u03b2)=/summationdisplay\n\u03b3(mT\ni\u03b2)\nwhere now the suf\ufb01cient statistic is \u03beand the parameter vector is \u03b2, which \ufb01ts into\nour exponential family format, and miis the ithcolumn of M.", "309": "298 4 Machine Learning\nGiven this joint density, we can compute the log-likelihood as the following:\n/lscript=\u03b2T\u03be\u2212\u03c8(\u03b2)\nTo maximize this likelihood, we take the derivative of this with respect to \u03b2to obtain\nthe following:\nd/lscript\nd\u03b2=My\u2212M\u03bc(MT\u03b2)\nsince\u03b3/prime(mT\ni\u03b2)=mT\ni\u03bci(\u03b2)and (c.f. Eq. ( 4.7.0.1 )),\u03b3/prime=\u03bc\u03bb. Setting this derivative\nequal to zero gives the conditions for the maximum likelihood solution,\nM(y\u2212\u03bc(MT\u03b2))=0 (4.7.0.3)\nwhere \u03bcis the element-wise inverse of the link function. This leads us to exactly the\nsame place we started: trying to regress yagainst \u03bc(MT\u03b2).\nExample . The structure matrix Mis where the xidata associated with the corre-\nsponding yienters the problem. If we choose\nMT=[1,x]\nwhere 1is an n-length vector and\n\u03b2=[\u03b20,\u03b21]T\nwith\u03bc(x)=1/(1+e\u2212x), we have the original logistic regression problem.\nGenerally, \u03bc(\u03b2)is a nonlinear function and thus we regress against our transformed\nvariable z\nz=MT\u03b2+diag(g/prime(\u03bc))(y\u2212\u03bc(MT\u03b2))\nThis \ufb01ts the format of the Gauss Markov (see Sect. 3.11) problem and has the fol-\nlowing solution,\n\u02c6\u03b2=(MR\u22121\nzMT)\u22121MR\u22121\nzz (4.7.0.4)\nwhere\nRz:= V(z)=diag(g/prime(\u03bc))2R=v(\u03bc)diag(g/prime(\u03bc))2I\nwhere gis the link function and vis the variance function on the designated distri-\nbution of the yi. Thus, \u02c6\u03b2has the following covariance matrix,", "310": "4.7 Generalized Linear Models 299\nFig. 4.29 Some data for\nPoisson example\nV(\u02c6\u03b2)=(MR\u22121\nzMT)\u22121\nThese results allow inferences about the estimated parameters \u02c6\u03b2. We can easily write\nEq. ( 4.7.0.4 ) as an iteration as follow,\n\u02c6\u03b2k+1=(MR\u22121\nzkMT)\u22121MR\u22121\nzkzk\nExample . Consider the data shown in Fig. 4.29. Note that the variance of the data\nincreases for each xand the data increases as a power of xalong x. This makes this\ndata a good candidate for a Poisson GLM with g(\u03bc)=log(\u03bc).\nWe can use our iterative matrix-based approach. The following code initializes\nthe iteration.\n>>> M =np.c_[x *0+1 ,x].T\n>>> gi =np.exp # inverse g link function\n>>> bk =np.array([ .9,0.5])# initial point\n>>> muk =gi(M .T@bk).flatten()\n>>> Rz =np.diag( 1/muk)\n>>> zk =M.T@bk+Rz@(y-muk)\nand this next block establishes the main iteration\n>>> while abs(sum(M@(y-muk))) > .01 :# orthogonality condition as threshold\n... Rzi =np.linalg .inv(Rz)\n... bk=(np.linalg .inv(M @Rzi @M.T)) @M@Rzi @zk\n... muk =gi(M .T@bk).flatten()\n... Rz=np.diag( 1/muk)\n... zk=M.T@bk+Rz@(y-muk)\n...\nwith corresponding \ufb01nal \u03b2computed as the following:", "311": "300 4 Machine Learning\n>>> print (bk)\n[0.71264653 0.48934384]\nwith corresponding estimated V(\u02c6\u03b2)as\n>>> print (np.linalg .inv(M @Rzi @M.T))\n[[ 0.01867659 -0.00359408]\n[-0.00359408 0.00073501]]\nThe orthogonality condition Eq. ( 4.7.0.3 ) is the following:\n>>> print (M@(y-muk))\n[-5.88442660e-05 -3.12199976e-04]\nFor comparison, the statsmodels module provides the Poisson GLM object.\nNote that the reported standard error is the square root of the diagonal elements of\nV(\u02c6\u03b2). A plot of the data and the \ufb01tted model is shown below in Fig. 4.30.\n>>>pm=sm.GLM(y, sm .tools .add_constant(x),\n... family =sm.families .Poisson())\n>>>pm_results =pm.fit()\n>>>pm_results .summary()\n<class 'statsmodels.iolib.summary.Summary'>\"\"\"\nGeneralized Linear Model Regression Results\n==============================================================================Dep. Variable: y No. Observations: 50Model: GLM Df Residuals: 48\nModel Family: Poisson Df Model: 1\nLink Function: log Scale: 1.0000Method: IRLS Log-Likelihood: -134.00Date: Tue, 12 Mar 2019 Deviance: 44.230\nTime: 06:54:16 Pearson chi2: 43.1\nNo. Iterations: 5 Covariance Type: nonrobust==============================================================================\ncoef std err z P>|z| [0.025 0.975]\n------------------------------------------------------------------------------\nconst 0.7126 0.137 5.214 0.000 0.445 0.981x1 0.4893 0.027 18.047 0.000 0.436 0.542==============================================================================\n\"\"\"\n4.8 Regularization\nWe have referred to regularization in Sect. 4.6, but we want to develop this impor-\ntant idea more fully. Regularization is the mechanism by which we navigate the\nbias/variance trade-off. To get started, let\u2019s consider a classic constrained least-squares problem,", "312": "4.8 Regularization 301\nFig. 4.30 Fitted using the\nPoisson GLM\nFig. 4.31 The solution of the\nconstrained L2minimization\nproblem is at the point wherethe constraint (dark line)intersects the L\n2ball (gray\ncircle) centered at the origin.The point of intersection is\nindicated by the dark circle.\nThe two neighboring squaresindicate points on the line thatare close to the solution\nminimizex/bardblx/bardbl2\n2\nsubject to: x0+2x1=1\nwhere /bardblx/bardbl2=/radicalBig\nx2\n0+x2\n1is the L2norm. Without the constraint, it would be easy to\nminimize the objective function\u2014just take x=0. Otherwise, suppose we somehow\nknow that /bardblx/bardbl2<c, then the locus of points de\ufb01ned by this inequality is the circle\nin Fig. 4.31. The constraint is the line in the same \ufb01gure. Because every value of c\nde\ufb01nes a circle, the constraint is satis\ufb01ed when the circle touches the line. The circle\ncan touch the line at many different points, but we are only interested in the smallest\nsuch circle because this is a minimization problem. Intuitively, this means that wein\ufb02ate aL\n2ball at the origin and stop when it just touches the constraint. The point\nof contact is our L2minimization solution.", "313": "302 4 Machine Learning\nWe can obtain the same result using the method of Lagrange multipliers. We\ncan re-write the entire L2minimization problem as one objective function using the\nLagrange multiplier, \u03bb,\nJ(x0,x1,\u03bb)=x2\n0+x2\n1+\u03bb(1\u2212x0\u2212x1)\nand solve this as an ordinary function using calculus. Let\u2019s do this using Sympy.\n>>> import sympy as S\n>>>S.var('x:2 l' ,real =True)\n(x0, x1, l)>>>J=S.Matrix([x0,x1]) .norm() **2 + l*(1-x0-2*x1)\n>>>sol=S.solve( map(J.diff,[x0,x1,l]))\n>>> print (sol)\n{l: 2/5, x0: 1/5, x1: 2/5}\nProgramming Tip\nUsing the Matrix object is overkill for this problem but it does demonstrate how\nSympy\u2019s matrix machinery works. In this case, we are using the norm method\nto compute the L2norm of the given elements. Using S.var de\ufb01nes Sympy\nvariables and injects them into the global namespace. It is more Pythonic to\ndo something like x0 = S.symbols(\u2019x0\u2019,real=True) instead but the other\nway is quicker, especially for variables with many dimensions.\nThe solution de\ufb01nes the exact point where the line is tangent to the circle in Fig. 4.31.\nThe Lagrange multiplier has incorporated the constraint into the objective function.\nThere is something subtle and very important about the nature of the solution,\nhowever. Notice that there are other points very close to the solution on the circle,\nindicated by the squares in Fig. 4.31. This closeness could be a good thing, in case it\nhelps us actually \ufb01nd a solution in the \ufb01rst place, but it may be unhelpful in so far asit creates ambiguity. Let\u2019s hold that thought and try the same problem using the L\n1\nnorm instead of the L2norm. Recall that\n/bardblx/bardbl1=d/summationdisplay\ni=1|xi|\nwhere dis the dimension of the vector x. Thus, we can reformulate the same problem\nin the L1norm as in the following:\nminimizex/bardblx/bardbl1\nsubject to: x1+2x2=1", "314": "4.8 Regularization 303\nIt turns out that this problem is somewhat harder to solve using Sympy, but we have\nconvex optimization modules in Python that can help.\n>>> from cvxpy import Variable, Problem, Minimize, norm1, norm\n>>>x=Variable(( 2,1),name ='x')\n>>>constr =[np.matrix([[ 1,2]])*x==1]\n>>>obj=Minimize(norm1(x))\n>>>p=Problem(obj,constr)\n>>>p.solve()\n0.49999999996804073>>> print (x.value)\n[[6.2034426e-10]\n[5.0000000e-01]]\nProgramming Tip\nThecvxy module provides a uni\ufb01ed and accessible interface to the power-\nfulcvxopt convex optimization package, as well as other open-source solver\npackages.\nAs shown in Fig. 4.32, the constant-norm contour in the L1norm is shaped like\na diamond instead of a circle. Furthermore, the solutions found in each case are\ndifferent. Geometrically, this is because in\ufb02ating the circular L2reaches out in all\ndirections whereas the L1ball creeps out along the principal axes. This effect is much\nmore pronounced in higher dimensional spaces where L1-balls get more spikey.6Like\ntheL2case, there are also neighboring points on the constraint line, but notice that\nthese are not close to the boundary of the corresponding L1ball, as they were in the\nL2case. This means that these would be harder to confuse with the optimal solution\nbecause they correspond to a substantially different L1ball.\nTo double-check our earlier L2result, we can also use the cvxpy module to \ufb01nd\ntheL2solution as in the following code,\n>>>constr =[np.matrix([[ 1,2]])*x==1]\n>>>obj=Minimize(norm(x, 2))#L2 norm\n>>>p=Problem(obj,constr)\n>>>p.solve()\n0.4473666974719267\n>>> print (x.value)\n[[0.1999737 ]\n[0.40004849]]\nThe only change to the code is the L2norm and we get the same solution as before.\nLet\u2019s see what happens in higher dimensions for both L2andL1as we move from\ntwo dimensions to four dimensions.\n6We discussed the geometry of high-dimensional space when we covered the curse of dimensionality\nin the statistics chapter.", "315": "304 4 Machine Learning\n>>>x=Variable(( 4,1),name ='x')\n>>>constr =[np.matrix([[ 1,2,3,4]])*x==1]\n>>>obj=Minimize(norm1(x))\n>>>p=Problem(obj,constr)\n>>>p.solve()\n0.2499999991355072>>> print (x.value)\n[[3.88487210e-10]\n[8.33295420e-10][7.97158511e-10]\n[2.49999999e-01]]\nA n da l s oi nt h e L\n2case with the following code,\n>>>constr =[np.matrix([[ 1,2,3,4]])*x==1]\n>>>obj=Minimize(norm(x, 2))\n>>>p=Problem(obj,constr)\n>>>p.solve()\n0.1824824789618193>>> print (x.value)\n[[0.03332451]\n[0.0666562 ][0.09999604]\n[0.13333046]]\nNote that the L\n1solution has selected out only one dimension for the solution, as the\nother components are effectively zero. This is not so with the L2solution, which has\nmeaningful elements in multiple coordinates. This is because the L1problem has\nmany pointy corners in the four-dimensional space that poke at the hyperplane that\nis de\ufb01ned by the constraint. This essentially means the subsets (namely, the points atthe corners) are found as solutions because these touch the hyperplane. This effect\nbecomes more pronounced in higher dimensions, which is the main bene\ufb01t of using\ntheL\n1norm as we will see in the next section.\n4.8.1 Ridge Regression\nNow that we have a sense of the geometry of the situation, let\u2019s revisit our classic\nlinear regression problem. To recap, we want to solve the following problem,\nmin\n\u03b2\u2208Rp/bardbly\u2212X\u03b2/bardbl\nwhere X=/bracketleftbig\nx1,x2,..., xp/bracketrightbig\nandxi\u2208Rn. Furthermore, we assume that the pcolumn\nvectors are linearly independent (i.e., rank(X)=p). Linear regression produces the", "316": "4.8 Regularization 305\nFig. 4.32 The diamond is\ntheL1ball in two dimensions\nand the line is the constraint.The point of intersection isthe solution to the optimiza-tion problem. Note that for\nL\n1optimization, the two\nnearby points on the con-straint (squares) do not touchtheL\n1ball. Compare this with\nFig.4.31\n\u03b2that minimizes the mean squared error above. In the case where p=n, there is\na unique solution to this problem. However, when p<n, then there are in\ufb01nitely\nmany solutions.\nTo make this concrete, let\u2019s work this out using Sympy. First, let\u2019s de\ufb01ne an\nexample Xandymatrix,\n>>> import sympy as S\n>>> from sympy import Matrix\n>>>X=Matrix([[ 1,2,3],\n... [3,4,5]])\n>>>y=Matrix([[ 1,2]]).T\nNow, we can de\ufb01ne our coef\ufb01cient vector \u03b2using the following code,\n>>>b0,b1,b2 =S.symbols( 'b:3' ,real =True)\n>>>beta =Matrix([[b0,b1,b2]]) .T# transpose\nNext, we de\ufb01ne the objective function we are trying to minimize\n>>>obj=(X*beta -y).norm( ord=2)**2\nProgramming Tip\nThe Sympy Matrix class has useful methods like the norm function used above\nto de\ufb01ne the objective function. The ord=2 means we want to use the L2norm.\nThe expression in parenthesis evaluates to a Matrix object.\nNote that it is helpful to de\ufb01ne real variables using the keyword argument whenever\napplicable because it relieves Sympy\u2019s internal machinery of dealing with complex", "317": "306 4 Machine Learning\nnumbers. Finally, we can use calculus to solve this by setting the derivatives of the\nobjective function to zero.\n>>>sol=S.solve([obj .diff(i) for iinbeta])\n>>>beta.subs(sol)\nMatrix([\n[ b2],\n[-2*b2 + 1/2],[ b2]])\nNotice that the solution does not uniquely specify all the components of the beta\nvariable. This is a consequence of the p<nnature of this problem where p=2\nandn=3. While the existence of this ambiguity does not alter the solution,\n>>>obj.subs(sol)\n0\nBut it does change the length of the solution vector beta ,\n>>>beta.subs(sol) .norm( 2)\nsqrt(2*b2**2 + (2*b2 - 1/2)**2)\nIf we want to minimize this length we can easily use the same calculus as before,>>>S.solve((beta .subs(sol) .norm() **2).diff())\n[1/6]\nThis provides the solution of minimum length in the L\n2sense,\n>>>betaL2 =beta.subs(sol) .subs(b2,S .Rational( 1,6))\n>>>betaL2\nMatrix([\n[1/6],\n[1/6],[1/6]])\nBut what is so special about solutions of minimum length? For machine learning,\ndriving the objective function to zero is symptomatic of over\ufb01tting the data. Usually,\nat the zero bound, the machine learning method has essentially memorized the train-\ning data, which is bad for generalization. Thus, we can effectively stall this problem\nby de\ufb01ning a region for the solution that is away from the zero bound.\nminimize\n\u03b2/bardbly\u2212X\u03b2/bardbl2\n2\nsubject to: /bardbl\u03b2/bardbl2<c\nwhere cis the tuning parameter. Using the same process as before, we can re-write\nthis as the following:", "318": "4.8 Regularization 307\nmin\n\u03b2\u2208Rp/bardbly\u2212X\u03b2/bardbl2\n2+\u03b1/bardbl\u03b2/bardbl2\n2\nwhere \u03b1is the tuning parameter. These are the penalized or Lagrange forms of\nthese problems derived from the constrained versions. The objective function is\npenalized by the /bardbl\u03b2/bardbl2term. For L2penalization, this is called ridge regression. This\nis implemented in Scikit-learn as Ridge . The following code sets this up for our\nexample,\n>>> from sklearn.linear_model import Ridge\n>>> clf =Ridge(alpha =100.0 ,fit_intercept =False )\n>>> clf.fit(np .array(X) .astype( float ),np .array(y) .astype( float ))\nRidge(alpha=100.0, copy_X=True, fit_intercept=False, max_iter=None,\nnormalize=False, random_state=None, solver='auto', tol=0.001)\nNote that the alpha scales of the penalty for the /bardbl\u03b2/bardbl2.W es e tt h e\nfit_intercept=False argument to omit the extra offset term from our example.\nThe corresponding solution is the following:\n>>> print (clf.coef_)\n[[0.0428641 0.06113005 0.07939601]]\nTo double-check the solution, we can use some optimization tools from Scipy and\nour previous Sympy analysis, as in the following:\n>>> from scipy.optimize import minimize\n>>>f =S.lambdify((b0,b1,b2),obj +beta.norm() **2*100. )\n>>>g =lambda x:f(x[ 0],x[1],x[2])\n>>>out =minimize(g,[ .1,.2,.3])# initial guess\n>>>out.x\narray([0.0428641 , 0.06113005, 0.07939601])\nProgramming Tip\nWe had to de\ufb01ne the additional gfunction from the lambda function we created\nfrom the Sympy expression in fbecause the minimize function expects a single\nobject vector as input instead of a three separate arguments.\nwhich produces the same answer as the Ridge object. To better understand the\nmeaning of this result, we can re-compute the mean squared error solution to this\nproblem in one step using matrix algebra instead of calculus,\n>>>betaLS =X.T*(X*X.T).inv() *y\n>>>betaLS\nMatrix([\n[1/6],[1/6],\n[1/6]])", "319": "308 4 Machine Learning\nNotice that this solves the posited problem exactly,\n>>>X*betaLS -y\nMatrix([\n[0],\n[0]])\nThis means that the \ufb01rst term in the objective function goes to zero,\n/bardbly\u2212X\u03b2LS/bardbl= 0\nBut, let\u2019s examine the L2length of this solution versus the ridge regression solution,\n>>> print (betaLS .norm() .evalf(), np .linalg .norm(clf .coef_))\n0.288675134594813 0.10898596412575512\nThus, the ridge regression solution is shorter in the L2sense, but the \ufb01rst term in the\nobjective function is not zero for ridge regression,\n>>> print ((y-X*clf.coef_ .T).norm() **2)\n1.86870864136429\nRidge regression solution trades \ufb01tting error ( /bardbly\u2212X\u03b2/bardbl2) for solution length ( /bardbl\u03b2/bardbl2).\nLet\u2019s see this in action with a familiar example from Sect. 3.12.4 . Consider\nFig.4.33. For this example, we created our usual chirp signal and attempted to \ufb01t it\nwith a high-dimensional polynomial, as we did in Sect. 4.3.4 . The lower panel is the\nsame except with ridge regression. The shaded gray area is the space between thetrue signal and the approximant in both cases. The horizontal hash marks indicate\nthe subset of x\nivalues that each regressor was trained on. Thus, the training set\nrepresents a nonuniform sample of the underlying chirp waveform. The top panelshows the usual polynomial regression. Note that the regressor \ufb01ts the given points\nextremely well, but fails at the endpoint. The ridge regressor misses many of the\npoints in the middle, as indicated by the gray area, but does not overshoot at theends as much as the plain polynomial regression. This is the basic trade-off for ridge\nregression. The Jupyter notebook corresponding to this section has the code for this\ngraph, but the main steps are shown in the following:\n# create chirp signal\nxi=np.linspace( 0,1,100)[:, None ]\n# sample chirp randomlyxin=np.sort(np .random .choice(xi .flatten(), 20,replace =False ))[:, None ]\n# create sampled waveform\ny=np.cos( 2*pi*(xin +xin**2))\n# create full waveform for referenceyi=np.cos( 2*pi*(xi+xi**2))\n# create polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\nqfit =PolynomialFeatures(degree =8)# quadratic", "320": "4.8 Regularization 309\nFig. 4.33 The top \ufb01gure\nshows polynomial regressionand the lower panel showspolynomial ridge regression.The ridge regression doesnot match as well throughoutmost of the domain, but it doesnot \ufb02are as violently at theends. This is because the ridgeconstraint holds the coef\ufb01cientvector down at the expense ofpoorer performance along themiddle of the domain\nXq=qfit .fit_transform(xin)\n# reformat input as polynomialXiq =qfit .fit_transform(xi)\nfrom sklearn.linear_model import LinearRegression\nlr=LinearRegression() # create linear model\nlr.fit(Xq,y) # fit linear model\n# create ridge regression model and fit\nclf =Ridge(alpha =1e-9 ,fit_intercept =False )\nclf.fit(Xq,y)\n4.8.2 Lasso Regression\nLasso regression follows the same basic pattern as ridge regression, except with the\nL1norm in the objective function.\nmin\n\u03b2\u2208Rp/bardbly\u2212X\u03b2/bardbl2+\u03b1/bardbl\u03b2/bardbl1\nThe interface in Scikit-learn is likewise the same. The following is the same problem\nas before using lasso instead of ridge regression,", "321": "310 4 Machine Learning\n>>>X=np.matrix([[ 1,2,3],\n... [3,4,5]])\n>>>y=np.matrix([[ 1,2]]).T\n>>> from sklearn.linear_model import Lasso\n>>>lr=Lasso(alpha =1.0,fit_intercept =False )\n>>>_=lr.fit(X,y)\n>>> print (lr.coef_)\n[0. 0. 0.32352941]\nAs before, we can use the optimization tools in Scipy to solve this also,\n>>> from scipy.optimize import fmin\n>>>obj = 1/4.* (X*beta-y).norm( 2)**2 + beta.norm( 1)*l\n>>>f=S.lambdify((b0,b1,b2),obj .subs(l, 1.0))\n>>>g=lambda x:f(x[ 0],x[1],x[2])\n>>>fmin(g,[ 0.1,0.2,0.3])\nOptimization terminated successfully.\nCurrent function value: 0.360297\nIterations: 121Function evaluations: 221\narray([2.27469304e-06, 4.02831864e-06, 3.23134859e-01])\nProgramming Tip\nThefmin function from Scipy\u2019s optimization module uses an algorithm that\ndoes not depend upon derivatives. This is useful because, unlike the L2norm,\ntheL1norm has sharp corners that make it harder to estimate derivatives.\nThis result matches the previous one from the Scikit-learn Lasso object. Solving it\nusing Scipy is motivating and provides a good sanity check, but specialized algo-rithms are required in practice. The following code block re-runs the lasso with\nvarying \u03b1and plots the coef\ufb01cients in Fig. 4.34. Notice that as \u03b1increases, all but\none of the coef\ufb01cients is driven to zero. Increasing \u03b1makes the trade-off between\n\ufb01tting the data in the L\n1sense and wanting to reduce the number of nonzero coef\ufb01-\ncients (equivalently, the number of features used) in the model. For a given problem,\nit may be more practical to focus on reducing the number of features in the model\n(i.e., large \u03b1) than the quality of the data \ufb01t in the training data. The lasso provides\na clean way to navigate this trade-off.\nThe following code loops over a set of \u03b1values and collects the corresponding\nlasso coef\ufb01cients to be plotted in Fig. 4.34\n>>>o=[]\n>>>alphas =np.logspace( -3,0,10)\n>>> for ainalphas:", "322": "4.8 Regularization 311\nFig. 4.34 As\u03b1increases, more of the model coef\ufb01cients are driven to zero for lasso regression\n... clf =Lasso(alpha =a,fit_intercept =False )\n... _=clf.fit(X,y)\n... o.append(clf .coef_)\n...\n4.9 Support Vector Machines\nSupport vector machines (SVM) originated from the statistical learning theory devel-oped by Vapnik\u2013Chervonenkis. As such, it represents a deep application of statistical\ntheory that incorporates the VC dimension concepts we discussed in the \ufb01rst section.\nLet\u2019s start by looking at some pictures. Consider the two-dimensional classi\ufb01cationproblem shown in Fig. 4.35. Figure 4.35 shows two classes (gray and white circles)\nthat can be separated by any of the lines shown. Speci\ufb01cally, any such separating line\ncan be written as the locus of points ( x) in the two-dimensional plane that satisfy the\nfollowing:\n\u03b2\n0+\u03b2Tx=0\nTo classify an arbitrary xusing this line, we just compute the sign of \u03b20+\u03b2Tx\nand assign one class to the positive sign and the other class to the negative sign.\nTo uniquely specify such a separating line (or, hyperplane in a higher dimensional\nspace) we need additional criteria.\nFigure 4.36 shows the data with two bordering parallel lines that form a margin\naround the central separating line. The maximal margin algorithm \ufb01nds the widest", "323": "312 4 Machine Learning\nFig. 4.35 In the two-\ndimensional plane, the twoclasses (gray and white cir-cles) are easily separated byany one of the lines shown\nFig. 4.36 The maximal mar-\ngin algorithm \ufb01nds the sepa-rating line that maximizes themargin shown. The elementsthat touch the margins are thesupport elements. The dottedelements are not relevant tothe solution\nmargin and the unique separating line. As a consequence, the algorithm uncovers\nthe elements in the data that touch the margins. These are the support elements. The\nother elements away from the border are not relevant to the solution. This reduces\nmodel variance because the solution is insensitive to the removal of elements other\nthan these supporting elements (usually a small minority).\nTo see how this works for linearly separable classes, consider a training set con-\nsisting of {(x,y)}where y\u2208{ \u2212 1,1}. For any point xi, we compute the functional\nmargin as \u02c6\u03b3i=yi(\u03b20+\u03b2Txi). Thus, \u02c6\u03b3i>0 when xiis correctly classi\ufb01ed. The\ngeometrical margin is \u03b3=\u02c6\u03b3//bardbl\u03b2/bardbl. When xiis correctly classi\ufb01ed, the geometrical\nmargin is equal to the perpendicular distance from xito the line. Let\u2019s look see how\nthe maximal margin algorithm works.\nLetMbe the width of the margin. The maximal margin algorithm is can be for-\nmulated as a quadratic programming problem. We want to simultaneously maximize\nthe margin Mwhile ensuring that all of the data points are correctly classi\ufb01ed.\nmaximize\n\u03b20,\u03b2,/bardbl\u03b2/bardbl=1M\nsubject to: yi(\u03b20+\u03b2Txi)\u2265M,i=1,..., N.", "324": "4.9 Support Vector Machines 313\nThe \ufb01rst line says we want to generate a maximum value for Mby adjusting \u03b20and\n\u03b2while keeping /bardbl\u03b2/bardbl= 1. The functional margins for each ithdata element are\nthe constraints to the problem and must be satis\ufb01ed for every proposed solution. Inwords, the constraints enforce that the elements have to be correctly classi\ufb01ed and\noutside of the margin around the separating line. With some reformulation, it turns\nout that M=1//bardbl\u03b2/bardbland this can be put into the following standard format,\nminimize\n\u03b20,\u03b2/bardbl\u03b2/bardbl\nsubject to: yi(\u03b20+\u03b2Txi)\u22651,i=1,..., N.\nThis is a convex optimization problem and can be solved using powerful methods in\nthat area.\nThe situation becomes more complex when the two classes are not separable and\nwe have to allow some unavoidable mixing between the two classes in the solution.\nThis means that the constraints have to be modi\ufb01ed as in the following:\nyi(\u03b20+\u03b2Txi)\u2265M(1\u2212\u03bei)\nwhere \u03beiare the slack variables and represent the proportional amount that the pre-\ndiction is on the wrong side of the margin. Thus, elements are misclassi\ufb01ed when\n\u03bei>1. With these additional variables, we have a more general formulation of the\nconvex optimization problem,\nminimize\n\u03b20,\u03b2/bardbl\u03b2/bardbl\nsubject to: yi(\u03b20+\u03b2Txi)\u22651\u2212\u03bei,\n\u03bei\u22650,/summationdisplay\n\u03bei\u2264constant ,i=1,..., N.\nwhich can be rewritten in the following equivalent form,\nminimize\n\u03b20,\u03b21\n2/bardbl\u03b2/bardbl+C/summationdisplay\n\u03bei\nsubject to: yi(\u03b20+\u03b2Txi)\u22651\u2212\u03bei,\u03bei\u22650i=1,..., N.(4.9.0.1)\nBecause the \u03beiterms are all positive, the objective is to maximize the margin (i.e.,\nminimize /bardbl\u03b2/bardbl) while minimizing the proportional drift of the predictions to the\nwrong side of the margin (i.e., C/summationtext\u03bei). Thus, large values of Cshunt algorithmic\nfocus toward the correctly classi\ufb01ed points near the decision boundary and small\nvalues focus on further data. The value Cis a hyper-parameter for the SVM.\nThe good news is that all of these complicated pieces are handled neatly inside of\nScikit-learn. The following sets up the linear kernel for the SVM (more on kernels\nsoon),", "325": "314 4 Machine Learning\nFig. 4.37 The two class\nshown (white and gray circles)are linearly separable. Themaximal margin solution isshown by the dark black linein the middle. The dotted linesshow the extent of the margin.The large circles indicatethe support vectors for themaximal margin solution\n>>> from sklearn.datasets import make_blobs\n>>> from sklearn.svm import SVC\n>>>sv=SVC(kernel ='linear' )\nWe can create some synthetic data using make_blobs and then \ufb01t it to the SVM,\n>>>X,y=make_blobs(n_samples =200 , centers =2, n_features =2,\n... random_state =0,cluster_std =.5)\n>>>sv.fit(X,y)\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\ndecision_function_shape='ovr', degree=3, gamma='auto_deprecated',kernel='linear', max_iter=-1, probability=False, random_state=None,shrinking=True, tol=0.001, verbose=False)\nAfter \ufb01tting, the SVM now has the estimated support vectors and the coef\ufb01cients\nof the \u03b2in the sv.support_vectors_ andsv.coef_ attributes, respectively.\nFigure 4.37 shows the two sample classes (white and gray circles) and the line\nseparating them that was found by the maximal margin algorithm. The two parallel\ndotted lines show the margin. The large circles enclose the support vectors, which\nare the data elements that are relevant to the solution. Notice that only these elementscan touch the edges of the margins.\nFigure 4.38 shows what happens when the value of Cchanges. Increasing this\nvalue emphasizes the \u03bepart of the objective function in Eq. ( 4.9.0.1 ). As shown in the\ntop left panel, a small value for Cmeans that the algorithm is willing to accept many\nsupport vectors at the expense of maximizing the margin. That is, the proportional\namount that predictions are on the wrong side of the margin is more acceptable withsmaller C.A st h ev a l u eo f Cincreases, there are fewer support vectors because the\noptimization process prefers to eliminate support vectors that are far away from the\nmargins and accept fewer of these that encroach into the margin. Note that as thevalue of Cprogresses through this \ufb01gure, the separating line tilts slightly.", "326": "4.9 Support Vector Machines 315\nFig. 4.38 The maximal mar-\ngin algorithm \ufb01nds the sepa-rating line that maximizes themargin shown. The elementsthat touch the margins are thesupport elements. The dottedelements are not relevant tothe solution\n4.9.1 Kernel Tricks\nSupport Vector Machines provide a powerful method to deal with linear separations,\nbut they can also apply to nonlinear boundaries by exploiting the so-called kernel\ntrick . The convex optimization formulation of the SVM includes a dual formulation\nthat leads to a solution that requires only the inner products of the features. The kernel\ntrick is to substitute inner products by nonlinear kernel functions. This can be thoughtof as mapping the original features onto a possibly in\ufb01nite-dimensional space of new\nfeatures. That is, if the data are not linearly separable in two-dimensional space (for\nexample) maybe they are separable in three-dimensional space (or higher)?\nTo make this concrete, suppose the original input space is R\nnand we want to use\na nonlinear mapping \u03c8:x/mapsto\u2192 Fwhere Fis an inner product space of higher dimen-\nsion. The kernel trick is to calculate the inner product in Fusing a kernel function,\nK(xi,xj)=/angbracketleft\u03c8(xi),\u03c8(xj)/angbracketright. The long way to compute this is to \ufb01rst compute \u03c8(x)\nand then do the inner product. The kernel-trick way to do it is to use the kernel func-tion and avoid computing \u03c8. In other words, the kernel function returns what the inner\nproduct in Fwould have returned if \u03c8had been applied. For example, to achieve an\nn\nthpolynomial mapping of the input space, we can use \u03ba(xi,xj)=(xT\nixj+\u03b8)n.\nFor example, suppose the input space is R2and F=R4and we have the following\nmapping,\n\u03c8(x):(x0,x1)/mapsto\u2192(x2\n0,x2\n1,x0x1,x1x0)\nThe inner product in Fis then,\n/angbracketleft\u03c8(x),\u03c8(y)/angbracketright=/angbracketleft x,y/angbracketright2", "327": "316 4 Machine Learning\nIn other words, the kernel is the square of the inner product in input space. The\nadvantage of using the kernel instead of simply enlarging the feature space is com-\nputational because you only need to compute the kernel on all distinct pairs of theinput space. The following example should help make this concrete. First we create\nsome Sympy variables,\n>>> import sympy as S\n>>>x0,x1 =S.symbols( 'x:2' ,real =True)\n>>>y0,y1 =S.symbols( 'y:2' ,real =True)\nNext, we create the \u03c8function that maps into R\n4and the corresponding kernel\nfunction,\n>>>psi =lambda x,y: (x **2,y**2,x*y,x*y)\n>>>kern =lambda x,y: S .Matrix(x) .dot(y) **2\nNotice that the inner product in R4is equal to the kernel function, which only uses\nthe R2variables.\n>>> print (S.Matrix(psi(x0,x1)) .dot(psi(y0,y1)))\nx0**2*y0**2 + 2*x0*x1*y0*y1 + x1**2*y1**2\n>>> print (S.expand(kern((x0,x1),(y0,y1)))) # same as above\nx0**2*y0**2 + 2*x0*x1*y0*y1 + x1**2*y1**2\nPolynomial Regression Using Kernels . Recall our favorite linear regression prob-\nlem from the regularization chapter,\nmin\n\u03b2/bardbly\u2212X\u03b2/bardbl2\nwhere Xis an\u00d7mmatrix with m>n. As we discussed, there are multiple solutions\nto this problem. The least-squares solution is the following:\n\u03b2LS=XT(XXT)\u22121y\nGiven a new feature vector x, the corresponding estimator for yis the following:\n\u02c6y=xT\u03b2LS=xTXT(XXT)\u22121y\nUsing the kernel trick, the solution can be written more generally as the following:\n\u02c6y=k(x)TK\u22121y\nwhere the n\u00d7nkernel matrix Kreplaces XXTand where k(x)is a n-vector of\ncomponents k(x)=[\u03ba(xi,x)]and where Ki,j=\u03ba(xi,xj)for the kernel function\n\u03ba. With this more general setup, we can substitute \u03ba(xi,xj)=(xT\nixj+\u03b8)nfornth-\norder polynomial regression [ 4]. Note that ridge regression can also be incorporated", "328": "4.9 Support Vector Machines 317\nby inverting (K+\u03b1I), which can help stabilize poorly conditioned Kmatrices with\na tunable \u03b1hyper-parameter [ 4].\nFor some kernels, the enlarged Fspace is in\ufb01nite-dimensional. Mercer\u2019s condi-\ntions provide technical restrictions on the kernel functions. Powerful and well-studied\nkernels have been implemented in Scikit-learn. The advantage of kernel functions\nmay evaporate for when n\u2192min which case using the \u03c8functions instead can be\nmore practicable.\n4.10 Dimensionality Reduction\nThe features from a particular dataset that will ultimately prove important for machine\nlearning can be dif\ufb01cult to know ahead of time. This is especially true for problems\nthat do not have a strong physical underpinning. The row dimension of the inputmatrix ( X) for \ufb01tting data in Scikit-learn is the number of samples and the column\ndimension is the number of features. There may be a large number of column dimen-\nsions in this matrix, and the purpose of dimensionality reduction is to somehow\nreduce these to only those columns that are important for the machine learning task.\nFortunately, Scikit-learn provides some powerful tools to help uncover the most\nrelevant features. Principal component analysis (PCA) consists of taking the input\nXmatrix and (1) subtracting the mean, (2) computing the covariance matrix, and (3)\ncomputing the eigenvalue decomposition of the covariance matrix. For example, if X\nhas more columns than is practicable for a particular learning method, then PCA can\nreduce the number of columns to a more manageable number. PCA is widely used\nin statistics and other areas beyond machine learning, so it is worth examining whatit does in some detail. First, we need the decomposition module from Scikit-learn.\n>>> from sklearn import decomposition\n>>> import numpy as np\n>>>pca =decomposition .PCA()\nLet\u2019s create some very simple data and apply PCA.\n>>>x=np.linspace( -1,1,30)\n>>>X=np.c_[x,x +1,x+2]# stack as columns\n>>>pca.fit(X)\nPCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\nsvd_solver='auto', tol=0.0, whiten=False)\n>>> print (pca .explained_variance_ratio_)\n[1.00000000e+00 2.73605815e-32 8.35833807e-34]\nProgramming Tip\nThenp.c_ is a shortcut method for creating stacked column-wise arrays.", "329": "318 4 Machine Learning\nFig. 4.39 The top panel\nshows the columns of thefeature matrix and the bottompanel shows the dominantcomponent that PCA hasextracted\nIn this example, the columns are just constant offsets of the \ufb01rst column. The\nexplained variance ratio is the percentage of the variance attributable to the trans-\nformed columns of X. You can think of this as the information that is relatively\nconcentrated in each column of the transformed matrix X. Figure 4.39 shows the\ngraph of this dominant transformed column in the bottom panel. Note that a constant\noffset in each of the columns does not change its respective variance and thus, as far\nas PCA is concerned, the three columns are identical from an information standpoint.\nTo make this more interesting, let\u2019s change the slope of each of the columns as in\nthe following:\n>>>X=np.c_[x, 2*x+1,3*x+2,x] # change slopes of columns\n>>>pca.fit(X)\nPCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\nsvd_solver='auto', tol=0.0, whiten=False)\n>>> print (pca .explained_variance_ratio_)\n[1.00000000e+00 3.26962032e-33 3.78960782e-34 2.55413064e-35]\nHowever, changing the slope did not impact the explained variance ratio. Again,\nthere is still only one dominant column. This means that PCA is invariant to both\nconstant offsets and scale changes. This works for functions as well as simple lines,\n>>>x=np.linspace( -1,1,30)\n>>>X=np.c_[np .sin( 2*np.pi*x),\n... 2*np.sin( 2*np.pi*x)+1,\n... 3*np.sin( 2*np.pi*x)+2]\n>>>pca.fit(X)\nPCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\nsvd_solver='auto', tol=0.0, whiten=False)\n>>> print (pca .explained_variance_ratio_)\n[1.00000000e+00 3.70493694e-32 2.51542007e-33]\nOnce again, there is only one dominant column, which is shown in the bottom\npanel of Fig. 4.40. The top panel shows the individual columns of the feature matrix.", "330": "4.10 Dimensionality Reduction 319\nFig. 4.40 The top panel\nshows the columns of thefeature matrix and the bottompanel shows the dominantcomponent that PCA hascomputed\nTo sum up, PCA is able to identify and eliminate features that are merely linear\ntransformations of existing features. This also works when there is additive noise inthe features, although more samples are needed to separate the uncorrelated noise\nfrom between features.\nTo see how PCA can simplify machine learning tasks, consider Fig. 4.41 wherein\nthe two classes are separated along the diagonal. After PCA, the transformed data lie\nalong a single axis where the two classes can be split using a one-dimensional interval,which greatly simpli\ufb01es the classi\ufb01cation task. The class identities are preserved\nunder PCA because the principal component is along the same direction that the\nclasses are separated. On the other hand, if the classes are separated along the directionorthogonal to the principal component, then the two classes become mixed under\nPCA and the classi\ufb01cation task becomes much harder. Note that in both cases, the\nexplained_variance_ratio_ is the same because the explained variance ratio\ndoes not account for class membership.\nPCA works by decomposing the covariance matrix of the data using the Singular\nValue Decomposition (SVD). This decomposition exists for all matrices and returnsthe following factorization for an arbitrary matrix A(Fig. 4.42),\nA=USV\nT\nBecause of the symmetry of the covariance matrix, U=V. The elements of the\ndiagonal matrix Sare the singular values of Awhose squares are the eigenvalues of\nATA. The eigenvector matrix Uis orthogonal: UTU=I. The singular values are in\ndecreasing order so that the \ufb01rst column of Uis the axis corresponding to the largest\nsingular value. This is the \ufb01rst dominant column that PCA identi\ufb01es. The entries of the\ncovariance matrix are of the form E(xixj)where xiandxjare different features.7This\nmeans that the covariance matrix is \ufb01lled with entries that attempt to uncover mutually\n7Note that these entries are constructed from the data using an estimator of the covariance matrix\nbecause we do not have the full probability densities at hand.", "331": "320 4 Machine Learning\nFig. 4.41 The left panel\nshows the original two-dimensional data space oftwo easily distinguishableclasses and the right panelshows the reduced the dataspace transformed using PCA.Because the two classes areseparated along the princi-pal component discovered byPCA, the classes are preservedunder the transformation\nFig. 4.42 As compared with\nFig.4.41, the two classes\ndiffer along the coordinatedirection that is orthogonal tothe principal component. As aresult, the two classes are nolonger distinguishable aftertransformation\ncorrelated relationships between all pairs of columns of the feature matrix. Once\nthese have been tabulated in the covariance matrix, the SVD \ufb01nds optimal orthogonaltransformations to align the components along the directions most strongly associated\nwith these correlated relationships. Simultaneously, because orthogonal matrices\nhave columns of unit-length, the SVD collects the absolute squared lengths of thesecomponents into the Smatrix. In our example above in Fig. 4.41, the two feature\nvectors were obviously correlated along the diagonal, meaning that PCA selected\nthat diagonal direction as the principal component.\nWe have seen that PCA is a powerful dimension reduction method that is invariant\nto linear transformations of the original feature space. However, this method performspoorly with transformations that are nonlinear. In that case, there are a wide range of", "332": "4.10 Dimensionality Reduction 321\nextensions to PCA, such as Kernel PCA, that are available in Scikit-learn, which allow\nfor embedding parameterized nonlinearities into the PCA at the risk of over\ufb01tting.\n4.10.1 Independent Component Analysis\nIndependent Component Analysis (ICA) via the FastICA algorithm is also available\nin Scikit-learn. This method is fundamentally different from PCA in that it is thesmall differences between components that are emphasized, not the large principal\ncomponents. This method is adopted from signal processing. Consider a matrix of\nsignals ( X) where the rows are the samples and the columns are the different signals.\nFor example, these could be EKG signals from multiple leads on a single patient.\nThe analysis starts with the following model,\nX=SA\nT(4.10.1.1)\nIn other words, the observed signal matrix is an unknown mixture ( A)o fs o m es e t\nof conformable, independent random sources S,\nS=[s1(t),s2(t),... , sn(t)]\nThe distribution on the random sources is otherwise unknown, except there can be\nat most one Gaussian source, otherwise, the mixing matrix Acannot be identi\ufb01ed\nbecause of technical reasons. The problem in ICA is to \ufb01nd Ain Eq. ( 4.10.1.1 ) and\nthereby un-mix the si(t)signals, but this cannot be solved without a strategy to reduce\nthe inherent arbitrariness in this formulation.\nTo make this concrete, let us simulate the situation with the following code,\n>>> from numpy import matrix, c_, sin, cos, pi\n>>>t=np.linspace( 0,1,250)\n>>>s1=sin(2*pi*t*6)\n>>>s2=np.maximum(cos( 2*pi*t*3),0.3)\n>>>s2=s2-s2.mean()\n>>>s3=np.random .randn( len(t))*.1\n>>># normalize columns\n>>>s1=s1/np.linalg .norm(s1)\n>>>s2=s2/np.linalg .norm(s2)\n>>>s3=s3/np.linalg .norm(s3)\n>>>S=c_[s1,s2,s3] # stack as columns\n>>># mixing matrix\n>>>A=matrix([[ 1,1,1],\n... [0.5,-1,3],\n... [0.1,-2,8]])\n>>>X=S*A.T# do mixing", "333": "322 4 Machine Learning\nFig. 4.43 The left column shows the original signals and the right column shows the mixed signals.\nThe object of ICA is to recover the left column from the right\nThe individual signals ( si(t)) and their mixtures ( Xi(t)) are shown in Fig. 4.43.T o\nrecover the individual signals using ICA, we use the FastICA object and \ufb01t the\nparameters on the Xmatrix,\n>>> from sklearn.decomposition import FastICA\n>>>ica =FastICA()\n>>># estimate unknown S matrix\n>>>S_=ica.fit_transform(X)\nThe results of this estimation are shown in Fig. 4.44, showing that ICA is able to\nrecover the original signals from the observed mixture. Note that ICA is unableto distinguish the signs of the recovered signals or preserve the order of the input\nsignals.\nTo develop some intuition as to how ICA accomplishes this feat, consider the\nfollowing two-dimensional situation with two uniformly distributed independent\nvariables, u\nx,uy\u223cU[0,1]. Suppose we apply the following orthogonal rotation\nmatrix to these variables,\n/bracketleftbiggu/prime\nx\nu/prime\ny/bracketrightbigg\n=/bracketleftbiggcos(\u03c6)\u2212sin(\u03c6)\nsin(\u03c6)cos(\u03c6)/bracketrightbigg/bracketleftbiggux\nuy/bracketrightbigg", "334": "4.10 Dimensionality Reduction 323\nFig. 4.44 The left column shows the original signals and the right column shows the signals that\nICA was able to recover. They match exactly, outside of a possible sign change\nFig. 4.45 The left panel shows two classes labeled on the ux,uyuniformly independent random\nvariables. The right panel shows these random variables after a rotation, which removes their mutual\nindependence and makes it hard to separate the two classes along the coordinate directions\nThe so-rotated variables u/prime\nx,u/prime\nyare no longer independent, as shown in Fig. 4.45.\nThus, one way to think about ICA is as a search through orthogonal matrices so\nthat the independence is restored. This is where the prohibition against Gaussian", "335": "324 4 Machine Learning\ndistributions arises. The two-dimensional Gaussian distribution of independent vari-\nables is proportional the following:\nf(x)\u221dexp(\u22121\n2xTx)\nNow, if we similarly rotated the xvector as,\ny=Qx\nthe resulting density for yis obtained by plugging in the following:\nx=QTy\nbecause the inverse of an orthogonal matrix is its transpose, we obtain\nf(y)\u221dexp(\u22121\n2yTQQTy)=exp(\u22121\n2yTy)\nIn other words, the transformation is lost on the yvariable. This means that ICA\ncannot search over orthogonal transformations if it is blind to them, which explains\nthe restriction of Gaussian random variables. Thus, ICA is a method that seeks to\nmaximize the non-Gaussian-ness of the transformed random variables. There aremany methods to doing this, some of which involve cumulants and others that use\nthenegentropy ,\nJ(Y)=H(Z)\u2212H(Y)\nwhere H(Z)is the information entropy of the Gaussian random variable Zthat has\nthe same variance as Y. Further details would take us beyond our scope, but that is\nthe outline of how the FastICA algorithm works.\nThe implementation of this method in Scikit-learn includes two different ways of\nextracting more than one independent source component. The de\ufb02ation method iter-\natively extracts one component at a time using a incremental normalization step. The\nparallel method also uses the single-component method but carries out normalization\nof all the components simultaneously, instead of for just the newly computed com-ponent. Because ICA extracts independent components, a whitening step is used\nbeforehand to balance the correlated components from the data matrix. Whereas\nPCA returns uncorrelated components along dimensions optimal for Gaussian ran-dom variables, ICA returns components that are as far from the Gaussian density as\npossible.\nThe left panel on Fig. 4.45 shows the original uniform random sources. The white\nand black colors distinguish between two classes. The right panel shows the mixture\nof these sources, which is what we observe as input features. The top row of Fig. 4.46\nshows the PCA (left) and ICA (right) transformed data spaces. Notice that ICA is\nable to un-mix the two random sources whereas PCA transforms along the dominant", "336": "4.10 Dimensionality Reduction 325\nFig. 4.46 The panel on the top left shows two classes in a plane after a rotation. The bottom left\npanel shows the result of dimensionality reduction using PCA, which causes mixing between thetwo classes. The top right panel shows the ICA transformed output and the lower right panel showsthat, because ICA was able to un-rotate the data, the lower dimensional data maintains the separationbetween the classes\ndiagonal. Because ICA is able to preserve the class membership, the data space can\nbe reduced to two nonoverlapping sections, as shown. However, PCA cannot achievea similar separation because the classes are mixed along the dominant diagonal that\nPCA favors as the main component in the decomposition.\nFor a good principal component analysis treatment, see [ 5\u20138]. Independent Com-\nponent Analysis is discussed in more detail in [ 9].\n4.11 Clustering\nClustering is the simplest member of a family of machine learning methods that do\nnot require supervision to learn from data. Unsupervised methods have training setsthat do not have a target variable. These unsupervised learning methods rely upon a\nmeaningful metric to group data into clusters. This makes it an excellent exploratory", "337": "326 4 Machine Learning\nFig. 4.47 The four clusters\nare pretty easy to see in thisexample and we want clus-tering methods to determinethe extent and number of suchclusters automatically\ndata analysis method because there are very few assumptions built into the method\nitself. In this section, we focus on the popular K-means clustering method that is\navailable in Scikit-learn.\nLet\u2019s manufacture some data to get going with make_blobs from Scikit-learn.\nFigure 4.47 shows some example clusters in two dimensions. Clustering methods\nwork by minimizing the following objective function,\nJ=/summationdisplay\nk/summationdisplay\ni/bardblxi\u2212\u03bck/bardbl2\nThe distortion for the kthcluster is the summand,\n/summationdisplay\ni/bardblxi\u2212\u03bck/bardbl2\nThus, clustering algorithms work to minimize this by adjusting the centers of the\nindividual clusters, \u03bck. Intuitively, each \u03bckis the center of mass of the points in the\ncloud. The Euclidean distance is the typical metric used for this,\n/bardblx/bardbl2=/summationdisplay\nx2\ni\nThere are many clever algorithms that can solve this problem for the best \u03bckcluster-\ncenters. The K-means algorithm starts with a user-speci\ufb01ed number of Kclusters\nto optimize over. This is implemented in Scikit-learn with the KMeans object that\nfollows the usual \ufb01tting conventions in Scikit-learn,\n>>> from sklearn.cluster import KMeans\n>>>kmeans =KMeans(n_clusters =4)\n>>>kmeans .fit(X)\nKMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,", "338": "4.11 Clustering 327\nn_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\nrandom_state=None, tol=0.0001, verbose=0)\nwhere we have chosen K=4. How do we choose the value of K? This is the\neternal question of generalization versus approximation\u2014too many clusters provide\ngreat approximation but bad generalization. One way to approach this problem is tocompute the mean distortion for increasingly larger values of Kuntil it no longer\nmakes sense. To do this, we want to take every data point and compare it to the\ncenters of all the clusters. Then, take the smallest value of this across all clustersand average those. This gives us an idea of the overall mean performance for the K\nclusters. The following code computes this explicitly.\nProgramming Tip\nThecdist function from Scipy computes all the pairwise differences between\nthe two input collections according to the speci\ufb01ed metric.\n>>> from scipy.spatial.distance import cdist\n>>>m_distortions =[]\n>>> for kinrange (1,7):\n... kmeans =KMeans(n_clusters =k)\n... _=kmeans .fit(X)\n... tmp=cdist(X,kmeans .cluster_centers_, 'euclidean' )\n... m_distortions .append( sum(np.min(tmp,axis =1))/X.shape[ 0])\n...\nNote that the code above uses the cluster_centers_ , which are estimated from\nK-means algorithm. The resulting Fig. 4.48 shows the point of diminishing returns\nfor added additional clusters.\nAnother \ufb01gure-of-merit is the silhouette coef\ufb01cient, which measures how compact\nand separated the individual clusters are. To compute the silhouette coef\ufb01cient, we\nneed to compute the mean intra-cluster distance for each sample ( ai) and the mean\nFig. 4.48 The mean distor-\ntion shows that there is adiminishing value in usingmore clusters\n", "339": "328 4 Machine Learning\nFig. 4.49 The shows how the\nsilhouette coef\ufb01cient varies asthe clusters move closer andbecome more compact\ndistance to the next nearest cluster ( bi). Then, the silhouette coef\ufb01cient for the ith\nsample is (Fig. 4.49)\nsci=bi\u2212ai\nmax(ai,bi)\nThe mean silhouette coef\ufb01cient is just the mean of all these values over all the\nsamples. The best value is one and the worst is negative one, with values near zeroindicating overlapping clusters and negative values showing that samples have been\nincorrectly assigned to the wrong cluster. This \ufb01gure-of-merit is implemented in\nScikit-learn as in the following:\n>>> from sklearn.metrics import silhouette_score\nFigure 4.50 shows how the silhouette coef\ufb01cient varies as the clusters become more\ndispersed and/or closer together.\nK-means is easy to understand and to implement, but can be sensitive to the\ninitial choice of cluster-centers. The default initialization method in Scikit-learn\nuses a very effective and clever randomization to come up with the initial cluster-centers. Nonetheless, to see why initialization can cause instability with K-means,\nconsider the following Fig. 4.50.I nF i g . 4.50, there are two large clusters on the left\nand a very sparse cluster on the far right. The large circles at the centers are thecluster-centers that K-means found. Given K=2, how should the cluster-centers\nbe chosen? Intuitively, the \ufb01rst two clusters should have their own cluster-center\nsomewhere between them and the sparse cluster on the right should have its owncluster-center.\n8Why isn\u2019t this happening?\n8Note that we are using the init=random keyword argument for this example in order to illustrate\nthis.", "340": "4.11 Clustering 329\nFig. 4.50 The large cir-\ncles indicate the cluster-centers found by the K-meansalgorithm\nThe problem is that the objective function for K-means is trading the distance of the\nfar-off sparse cluster with its small size. If we keep increasing the number of samplesin the sparse cluster on the right, then K-means will move the cluster-centers out to\nmeet them, as shown in Fig. 4.50. That is, if one of the initial cluster-centers was right\nin the middle of the sparse cluster, the algorithm would have immediately captured itand then moved the next cluster-center to the middle of the other two clusters (bottom\npanel of Fig. 4.50). Without some thoughtful initialization, this may not happen and\nthe sparse cluster would have been merged into the middle cluster (top panel ofFig.4.50). Furthermore, such problems are hard to visualize with high-dimensional\nclusters. Nonetheless, K-means is generally very fast, easy to interpret, and easy to\nunderstand. It is straightforward to parallelize using the n_jobs keyword argument\nso that many initial cluster-centers can be easily evaluated. Many extensions of K-\nmeans use different metrics beyond Euclidean and incorporate adaptive weighting\nof features. This enables the clusters to have ellipsoidal instead of spherical shapes.\n4.12 Ensemble Methods\nWith the exception of the random forest, we have so far considered machine learn-\ning models as stand-alone entities. Combinations of models that jointly produce a\nclassi\ufb01cation are known as ensembles . There are two main methodologies that create\nensembles: bagging andboosting .\n4.12.1 Bagging\nBagging refers to bootstrap aggregating, where bootstrap here is the same as we\ndiscussed in Sect. 3.10. Basically, we resample the data with replacement and then\ntrain a classi\ufb01er on the newly sampled data. Then, we combine the outputs of eachof the individual classi\ufb01ers using a 9 (for discrete outputs) or a weighted average\n(for continuous outputs). This combination is particularly effective for models that", "341": "330 4 Machine Learning\nFig. 4.51 Two regions in\nthe plane are separated bya nonlinear boundary. Thetraining data is sampled fromthis plane. The objective isto correctly classify the so-sampled data\nare easily in\ufb02uenced by a single data element. The resampling process means that\nthese elements cannot appear in every bootstrapped training set so that some of\nthe models will not suffer these effects. This makes the so-computed combination ofoutputs less volatile. Thus, bagging helps reduce the collective variance of individual\nhigh-variance models.\nTo get a sense of bagging, let\u2019s suppose we have a two-dimensional plane that\nis partitioned into two regions with the following boundary: y=\u2212 x+x\n2.P a i r s\nof(xi,yi)points above this boundary are labeled one and points below are labeled\nzero. Figure 4.51 shows the two regions with the nonlinear separating boundary as\nthe black curved line.\nThe problem is to take samples from each of these regions and classify them\ncorrectly using a perceptron (see Sect. 4.13). A perceptron is the simplest possible\nlinear classi\ufb01er that \ufb01nds a line in the plane to separate two purported categories.\nBecause the separating boundary is nonlinear, there is no way that the perceptron cancompletely solve this problem. The following code sets up the perceptron available\nin Scikit-learn.\n>>> from sklearn.linear_model import Perceptron\n>>>p=Perceptron()\n>>>p\nPerceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\nfit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=None,validation_fraction=0.1, verbose=0, warm_start=False)\nThe training data and the resulting perceptron separating boundary are shown in\nFig.4.52. The circles and crosses are the sampled training data and the gray sepa-\nrating line is the perceptron\u2019s separating boundary between the two categories. The\nblack squares are those elements in the training data that the perceptron misclas-si\ufb01ed. Because the perceptron can only produce linear separating boundaries, and\nthe boundary in this case is nonlinear, the perceptron makes mistakes near where\nthe boundary curves. The next step is to see how bagging can improve upon this byusing multiple perceptrons.", "342": "4.12 Ensemble Methods 331\nFig. 4.52 The perceptron\n\ufb01nds the best linear boundarybetween the two classes\nThe following code sets up the bagging classi\ufb01er in Scikit-learn. Here we select\nonly three perceptrons. Figure 4.53 shows each of the three individual classi\ufb01ers and\nthe \ufb01nal bagged classi\ufb01er in the panel on the bottom right. As before, the black circles\nindicate misclassi\ufb01cations in the training data. Joint classi\ufb01cations are determined\nby majority voting.\n>>> from sklearn.ensemble import BaggingClassifier\n>>>bp=BaggingClassifier(Perceptron(),max_samples =0.50 ,n_estimators =3)\n>>>bp\nBaggingClassifier(base_estimator=Perceptron(alpha=0.0001, class_weight=None,early_stopping=False, eta0=1.0,\nfit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=None,validation_fraction=0.1, verbose=0, warm_start=False),\nbootstrap=True, bootstrap_features=False, max_features=1.0,max_samples=0.5, n_estimators=3, n_jobs=None, oob_score=False,random_state=None, verbose=0, warm_start=False)\nTheBaggingClassifier can estimate its own out-of-sample error if passed\ntheoob_score=True \ufb02ag upon construction. This keeps track of which samples\nwere used for training and which were not, and then estimates the out-of-sample\nerror using those samples that were unused in training. The max_samples keyword\nargument speci\ufb01es the number of items from the training set to use for the baseclassi\ufb01er. The smaller the max_samples used in the bagging classi\ufb01er, the better\nthe out-of-sample error estimate, but at the cost of worse in-sample performance. Of\ncourse, this depends on the overall number of samples and the degrees-of-freedomin each individual classi\ufb01er. The VC dimension surfaces again!\n4.12.2 Boosting\nAs we discussed, bagging is particularly effective for individual high-variance classi-\n\ufb01ers because the \ufb01nal majority-vote tends to smooth out the individual classi\ufb01ers and", "343": "332 4 Machine Learning\nFig. 4.53 Each panel with the single gray line is one of the perceptrons used for the ensemble\nbagging classi\ufb01er on the lower right\nproduce a more stable collaborative solution. On the other hand, boosting is particu-\nlarly effective for high-bias classi\ufb01ers that are slow to adjust to new data. On the one\nhand, boosting is similar to bagging in that it uses a majority-voting (or averaging for\nnumeric prediction) process at the end; and it also combines individual classi\ufb01ers ofthe same type. On the other hand, boosting is serially iterative, whereas the individual\nclassi\ufb01ers in bagging can be trained in parallel. Boosting uses the misclassi\ufb01cations\nof prior iterations to in\ufb02uence the training of the next iterative classi\ufb01er by weightingthose misclassi\ufb01cations more heavily in subsequent steps. This means that, at every\nstep, boosting focuses more and more on speci\ufb01c misclassi\ufb01cations up to that point,\nletting the prior classi\ufb01cations be carried by earlier iterations.\nThe primary implementation for boosting in Scikit-learn is the Adaptive Boost-\ning ( AdaBoost ) algorithm, which does classi\ufb01cation ( AdaBoostClassifier ) and\nregression ( AdaBoostRegressor ). The \ufb01rst step in the basic AdaBoost algorithm\nis to initialize the weights over each of the training set indices, D\n0(i)=1/nwhere\nthere are nelements in the training set. Note that this creates a discrete uniform dis-\ntribution over the indices , not over the training data {(xi,yi)}itself. In other words,\nif there are repeated elements in the training data, then each gets its own weight. The\nnext step is to train the base classi\ufb01er hkand record the classi\ufb01cation error at the kth\niteration, /epsilon1k. Two factors can next be calculated using /epsilon1k,", "344": "4.12 Ensemble Methods 333\n\u03b1k=1\n2log1\u2212/epsilon1k\n/epsilon1k\nand the normalization factor,\nZk=2/radicalbig\n/epsilon1k(1\u2212/epsilon1k)\nFor the next step, the weights over the training data are updated as in the following:\nDk+1(i)=1\nZkDk(i)exp(\u2212\u03b1kyihk(xi))\nThe \ufb01nal classi\ufb01cation result is assembled using the \u03b1kfactors, g=sgn(/summationtext\nk\u03b1khk).\nTo re-do the problem above using boosting with perceptrons, we set up the\nAdaBoost classi\ufb01er in the following:\n>>> from sklearn.ensemble import AdaBoostClassifier\n>>>clf=AdaBoostClassifier(Perceptron(),n_estimators =3,\n... algorithm ='SAMME' ,\n... learning_rate =0.5 )\n>>>clf\nAdaBoostClassifier(algorithm='SAMME',\nbase_estimator=Perceptron(alpha=0.0001, class_weight=None,\nearly_stopping=False, eta0=1.0,\nfit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,\nn_jobs=None, penalty=None, random_state=0, shuffle=True, tol=None,validation_fraction=0.1, verbose=0, warm_start=False),\nlearning_rate=0.5, n_estimators=3, random_state=None)\nThelearning_rate above controls how aggressively the weights are updated.\nThe resulting classi\ufb01cation boundaries for the embedded perceptrons are shown\nin Fig. 4.54. Compare this to the lower right panel in Fig. 4.53. The performance for\nboth cases is about the same.\nFig. 4.54 The individual per-\nceptron classi\ufb01ers embeddedin the AdaBoost classi\ufb01er areshown along with the mis-classi\ufb01ed points (in black).Compare this to the lowerright panel of Fig. 4.53\n", "345": "334 4 Machine Learning\n4.13 Deep Learning\nNeural networks have a long history going back to the 1960s, but the recent availabil-\nity of large-scale, high-quality data and new parallel computing infrastructures havereinvigorated neural networks in terms of size and complexity. This new reinvigo-\nration, with many new and complex topologies, is called deep learning . There have\nbeen exciting developments in image and video processing, speech recognition, andautomated video captioning based on deep learning systems. However, this is still a\nvery active area of research. Fortunately, big companies with major investments in\nthis area have made much of their research software open source (e.g., Tensor\ufb02ow,PyTorch), with corresponding Python bindings. To build up our understanding of\nneural networks, we begin with Rosenblatt\u2019s 1960 Perceptron.\nPerceptron Learning . The perceptron is the primary ancestor of the most popular\ndeep learning technologies (i.e., multilayer perceptron) and it is the best place to\nstart as it will reveal the basic mechanics and themes of more complicated neuralnetworks. The job of the perceptron is to create a linear classi\ufb01er that can separate\npoints in R\nnbetween two classes. The basic idea is that given a set of associations:\n{(x0,y0),... ,( xm,ym)}\nwhere each x\u2208Rn\u22121is augmented with a unit-entry to account for an offset term,\nand a set of weights w\u2208Rn, compute the following as an estimate of the label\ny\u2208{ \u2212 1,1}.\n\u02c6y=wTx\nConcisely, this means that we want wsuch that\nwTxiC2\u2277\nC10\nwhere xiis in class C2ifxT\niw>0 and class C1otherwise. To determine these\nweights, we apply the following learning rule:\nw(k+1)=w(k)\u2212(y\u2212\u02c6y)xi\nThe output of the perceptron can be summarized as\n\u02c6y=sgn(xT\niw)\nT h es i g ni st h e activation function of the perceptron. With this setup, we can write\nout the perceptron\u2019s output as the following:", "346": "4.13 Deep Learning 335\n>>> import numpy as np\n>>> def yhat(x,w):\n... return np.sign(np .dot(x,w))\n...\nLet us create some fake data to play with:\n>>> npts = 100\n>>> X=np.random .rand(npts, 2)*6-3 # random scatter in 2-d plane\n>>> labels =np.ones(X .shape[ 0],dtype =np.int) # labels are 0 or 1\n>>> labels[(X[:, 1]<X[:, 0])]=-1\n>>> X=np.c_[X,np .ones(X .shape[ 0])]# augment with offset term\nNote that we added a column of ones to account for the offset term. Certainly, by our\nconstruction, this problem is linearly separable, so let us see if the perceptron can\n\ufb01nd the boundary between the two classes. Let us start by initializing the weights,\n>>>winit =np.random .randn( 3)\nand then apply the learning rule,>>>w=winit\n>>> for i,j inzip(X,labels):\n... w=w-(yhat(i,w) -j)*i\n...\nNote that we are taking a single ordered pass through the data. In practice, we would\nhave randomly shuf\ufb02ed the input data to ensure that there is no incidental structure inthe order of the data that would in\ufb02uence training. Now, let us examine the accuracy\nof the perceptron,\n>>> from sklearn.metrics import accuracy_score\n>>> print (accuracy_score(labels,[yhat(i,w) for iinX]))\n0.96\nWe can re-run the training rule over the data to try to improve the accuracy. A pass\nthrough the data is called an epoch .\n>>> for i,j inzip(X,labels):\n... w=w-(yhat(i,w) -j)*i\n...>>> print (accuracy_score(labels,[yhat(i,w) for iinX]))\n0.98\nNote that our initial weight for this epoch is the last weight from the previous pass.\nIt is common to randomly shuf\ufb02e the data between epochs. More epochs will resultin better accuracy in this case.", "347": "336 4 Machine Learning\nFig. 4.55 Thesoftsign\nfunction is a smooth approxi-mation to the sign function.\nThis makes it easier to differ-entiate for backpropagation\nWe can re-do this entire example with keras . First, we de\ufb01ne the model,\n>>> from keras.models import Sequential\n>>> from keras.layers import Dense\n>>> from keras.optimizers import SGD\n>>> model =Sequential()\n>>> model .add(Dense( 1, input_shape =(2,), activation ='softsign' ))\n>>> model .compile(SGD(), 'hinge' )\nNote that we use the softsign activation instead of the sgn that we used earlier\nbecause we need a differentiable activation function. Given the form of the weightupdate in perceptron learning, it is equivalent to the hinge loss function. Stochastic\ngradient descent ( SGD) is chosen for updating the weights. The softsign function\nis de\ufb01ned as the following:\ns(t)=x\n1+|x|\nWe can pull it out from the tensorflow backend that keras uses as in the following:\nplotted in Fig. 4.55\n>>> import tensorflow as tf\n>>>x=tf.placeholder( 'float' )\n>>>xi=np.linspace( -10,10,100)\n>>> with tf.Session() ass:\n... y_=(s.run(tf .nn.softsign(x),feed_dict ={x:xi}))\n...\nNext, all we have to do is fit the model on data,\n>>>h=model .fit(X[:,: 2], labels, epochs =300, verbose =0)\nThehvariable is the history that contains the internal metrics and parameters involved\nin the fit training phase. We can extract the trajectory of the loss function from this\nhistory and draw the loss in Fig. 4.56.", "348": "4.13 Deep Learning 337\nFig. 4.56 Trajectory of the\nloss function\nFig. 4.57 The basic multi-\nlayer perceptron has a singlehidden layer between inputand output. Each of the arrowshas a multiplicative weightassociated with it\nMultilayer Perceptron . The multilayer perceptron (MLP) generalizes the percep-\ntron by stacking them as fully connected individual layers. The basic topology is\ns h o w ni nF i g . 4.57. In the previous section we saw that the basic perceptron could\ngenerate a linear boundary for data that is linearly separable. The MLP can cre-\nate more complex nonlinear boundaries. Let us examine the moons dataset from\nscikit-learn,\n>>> from sklearn.datasets import make_moons\n>>> X, y =make_moons(n_samples =1000 , noise =0.1 , random_state =1234 )\nThe purpose of the noise term is to make data for each of the categories harder to\ndisambiguate. These data are shown in Fig. 4.58.\nThe challenge for the MLP is to derive a nonlinear boundary between these two\nclasses. We construct our MLP using keras ,\n>>> from keras.optimizers import Adam\n>>> model =Sequential()\n>>> model .add(Dense( 4,input_shape =(2,),activation ='sigmoid' ))\n>>> model .add(Dense( 2,activation ='sigmoid' ))\n>>> model .add(Dense( 1,activation ='sigmoid' ))\n>>> model .compile(Adam(lr =0.05 ),'binary_crossentropy' )", "349": "338 4 Machine Learning\nFig. 4.58 Data from\nmake_moons\nThis MLP has three layers. The input layer has four units and the next layer has\ntwo units and the output layer has one unit to distinguish between the two availableclasses. Instead of plain stochastic gradient descent, we use the more advanced Adam\noptimizer. A quick summary of the model elements and parameters comes from the\nmodel.summary() method,\n>>>model .summary()\n_________________________________________________________________Layer (type) Output Shape Param #\n=================================================================\ndense_2 (Dense) (None, 4) 12_________________________________________________________________dense_3 (Dense) (None, 2) 10\n_________________________________________________________________\ndense_4 (Dense) (None, 1) 3=================================================================\nTotal params: 25\nTrainable params: 25Non-trainable params: 0_________________________________________________________________\nAs usual, we split the input data into train and test sets,\n>>> from sklearn.model_selection import train_test_split\n>>> X_train,X_test,y_train,y_test =train_test_split(X,y,\n... test_size =0.3 ,\n... random_state =1234 )\nThus, we reserve 30% of the data for testing. Next, we train the MLP,\n>>>h=model .fit(X_train, y_train, epochs =100, verbose =0)\nTo compute the accuracy metric using the test set, we need to compute the model\nprediction on the this set.\n>>>y_train_ =model .predict_classes(X_train,verbose =0)\n>>>y_test_ =model .predict_classes(X_test,verbose =0)", "350": "4.13 Deep Learning 339\nFig. 4.59 The derived bound-\nary separates the two classes\n>>> print (accuracy_score(y_train,y_train_))\n1.0>>> print (accuracy_score(y_test,y_test_))\n0.9966666666666667\nTo visualize the so-derived boundary between these two classes, we use the contourf\nfunction from matplotlib which generates a \ufb01lled contour plot shown in Fig. 4.59.\nInstead of computing the accuracy separately, we can assign it as a metric for\nkeras to track by supplying it on the compile step, as in the following:\n>>>model .compile(Adam(lr =0.05 ),\n... 'binary_crossentropy' ,\n... metrics =['accuracy' ])\nThen, we can train again,>>>h=model .fit(X_train, y_train, epochs =100, verbose =0)\nNow, we can evaluate the model on the test data,>>>loss,acc =model .evaluate(X_test,y_test,verbose =0)\n>>> print (acc)\n0.9966666666666667\nwhere loss is the loss function and acc is the corresponding accuracy. The docu-\nmentation has other metrics that can be speci\ufb01ed during the compile step.\nBackpropagation . We have seen that the MLP can generate complicated nonlinear\nboundaries for classi\ufb01cation problems. The key algorithm underpinning MLP is\nbackpropagation. The idea is that when we stack layers into the MLP, we are applying\nfunction composition, which basically means we take the output of one function andthen feed it into the input of another.", "351": "340 4 Machine Learning\nh=(f\u25e6g)(x)=f(g(x))\nFor example, for the simple perceptron, we have g(x)=wTxand f(x)=sgn(x).\nThe key property of this composition is that derivatives use the chain rule from\ncalculus.\nh/prime(x)=f/prime(g(x))g/prime(x)\nNotice this has turned the differentiation operation into a multiplication operation.\nExplaining backpropagation in general is a notational nightmare, so let us see if we\ncan get the main idea from a speci\ufb01c example. Consider the following two-layer\nMLP with one input and one output.\nThere is only one input ( x1). The output of the \ufb01rst layer is\nz1=f(x1w1+b1)=f(p1)\nwhere fis the sigmoid function and b1is the bias term. The output of the second\nlayer is\nz2=f(z1w2+b2)=f(p2)\nTo keep it simple, let us suppose that the loss function for this MLP is the squared\nerror,\nJ=1\n2(z2\u2212y)2\nwhere yis the target label. Backpropagation has two phases. The forward phase\ncomputes the MLP loss function given the values of the inputs and corresponding\nweights. The backward phase applies the incremental weight updates to each weightbased on the forward phase. To implement gradient descent, we have to calculate the\nderivative of the loss function with respect to each of the weights.\n\u2202J\n\u2202w2=\u2202J\n\u2202z2\u2202z2\n\u2202p2\u2202p2\n\u2202w2\nThe \ufb01rst term is the following:\n\u2202J\n\u2202z2=z2\u2212y\nThe second term is the following:\n\u2202z2\n\u2202p2=f/prime(p2)=f(p2)(1\u2212f(p2))", "352": "4.13 Deep Learning 341\nNote that by property of the sigmoid function, we have f/prime(x)=(1\u2212f(x))f(x).\nThe third term is the following:\n\u2202p2\n\u2202w2=z1\nThus, the update for w2is the following:\n\u0394w 2\u221d(z2\u2212y)z1(1\u2212z2)z2\nThe corresponding analysis fo b2gives the following:\n\u0394b2=(z2\u2212y)z2(1\u2212z2)\nLet\u2019s keep going backward to w1,\n\u2202J\n\u2202w1=\u2202J\n\u2202z2\u2202z2\n\u2202p2\u2202p2\n\u2202z1\u2202z1\n\u2202p1\u2202p1\n\u2202w1\nThe \ufb01rst new term is the following:\n\u2202p2\n\u2202z1=w2\nand then the next two terms,\n\u2202z1\n\u2202p1=f(p1)(1\u2212f(p1))=z1(1\u2212z1)\n\u2202p1\n\u2202w1=x1\nThis makes the update for w1,\n\u0394w 1\u221d(z2\u2212y)z2(1\u2212z2)w2z1(1\u2212z1)x1\nTo understand why this is called backpropagation, we can de\ufb01ne\n\u03b42:=(z2\u2212y)z2(1\u2212z2)\nThis makes the weight update for w2,\n\u0394w 2\u221d\u03b42z1", "353": "342 4 Machine Learning\nThis means that the weight update for w2is proportional to the output of the prior\nlayer ( z1) and a factor that accounts steepness of the activation function. Likewise,\nthe weight update for w1can be written as the following:\n\u0394w 1\u221d\u03b41x1\nwhere\n\u03b41:=\u03b42w2z1(1\u2212z1)\nNote that this weight update is proportional to the input (prior layer\u2019s output) just as\nthe weight update for w2was proportional to the prior layer output z1. Also, the \u03b4fac-\ntors propagate recursively backward to the input layer. These characteristics permit\nef\ufb01cient numerical implementations for large networks because the subsequent com-\nputations are based on prior calculations. This also means that each individual unit\u2019scalculations are localized upon the output of the prior layer. This helps segregate the\nindividual processing behavior of each unit within each layer.\nFunctional Deep Learning . Keras has an alternative API that makes it possible to\nunderstand the performance of neural networks using the composition of functions\nideas we discussed. The key objects for this functional interpretation are the Input\nobject and the Model object.\n>>> from keras.layers import Input\n>>> from keras.models import Model\n>>> import keras.backend as K\nWe can re-create the data from our earlier classi\ufb01cation example\n>>> from sklearn.datasets import make_moons\n>>> X, y =make_moons(n_samples =1000 , noise =0.1 , random_state =1234 )\nThe \ufb01rst step is to construct a placeholder for the input using the Input object,\n>>>inputs =Input(shape =(2,))\nNext, we can stack the Dense layers as before but now tie their inputs to the previous\nlayer\u2019s outputs by calling Dense as a function.\n>>> l1=Dense( 3,input_shape =(2,),activation ='sigmoid' )(inputs)\n>>> l2=Dense( 2,input_shape =(3,),activation ='sigmoid' )(l1)\n>>> outputs =Dense( 1,input_shape =(3,),activation ='sigmoid' )(l1)\nThis means that output =(/lscript2\u25e6/lscript1)(input)where /lscript1and/lscript2are the middle layers.\nWith that established, we collect the individual pieces in the Model object and then\nfit andtrain as usual.\n>>>model =Model(inputs =inputs,outputs =outputs)\n>>>model .compile(Adam(lr =0.05 ),", "354": "4.13 Deep Learning 343\nFig. 4.60 The embedded\nrepresentation of the input justbefore the \ufb01nal output thatshows the internal divergenceof the two target classes\n... 'binary_crossentropy' ,\n... metrics =['accuracy' ])\n>>>h=model .fit(X_train, y_train, epochs =500, verbose =0)\nThis gives the same result as before. The advantage of the functional perspective is\nthat now we can think of the individual layers as mappings between multidimensional\nRnspaces. For example, /lscript1:R2/mapsto\u2192 R3and/lscript2:R3/mapsto\u2192 R2. Now, we can investigate\nthe performance of the network from the inputs just up until the \ufb01nal mapping to R\nat the output by de\ufb01ning the functional mapping (/lscript2\u25e6/lscript1)(inputs):R2/mapsto\u2192 R2,a s\nshown in Fig. 4.60.\nTo get this result, we have to de\ufb01ne a keras function using the inputs .\n>>>l2_function =K.function([inputs], [l2])\n>>># functional mapping just before output layer\n>>>l2o=l2_function([X_train])\nthel2o list contains the output of the l2layer that is shown in Fig. 4.60.\n4.13.1 Introduction to Tensor\ufb02ow\nTensor\ufb02ow is the leading deep learning framework. It is written in C++ with Python\nbindings. Although we will primarily use the brilliant Keras abstraction layer tocompose our neural networks with Tensor\ufb02ow providing the backed computing, it\nis helpful to see how Tensor\ufb02ow itself works and how to interact with it, especially\nfor later debugging. To get started, import Tensor\ufb02ow using the recommended con-vention.\n>>> import tensorflow as tf", "355": "344 4 Machine Learning\nFig. 4.61 Flow diagram for\nadder\nTensor\ufb02ow is graph-based. We have to assemble a computational graph. To get\nstarted, let\u2019s de\ufb01ne some constants,\n>>># declare constants\n>>>a=tf.constant( 2)\n>>>b=tf.constant( 3)\nThe context manager (i.e., the with statement) is the recommended way to create a\nsession variable, which is a realization of the computational graph that is composed\nof operations and tensor data objects. In this context, a tensor is another word for a\nmultidimensional matrix.\n>>># default graph using the context manager\n>>> with tf.Session() assess:\n... print ('a= ' ,a.eval())\n... print ('b= ' ,b.eval())\n... print (\"a+b\" ,sess .run(a +b))\n...\na= 2\nb= 3a+b 5\nThus, we can do some basic arithmetic on the declared variables. We can abstract\nthe graph using placeholders. For example, to implement the computational graph\nshown in Fig. 4.61, we can de\ufb01ne the following:\n>>>a=tf.placeholder(tf .int16)\n>>>b=tf.placeholder(tf .int16)\nNext, we de\ufb01ne the addition operation in the graph,>>># declare operation\n>>>adder =tf.add(a,b)\nThen, we compose and execute the graph using the context manager,>>># default graph using context manager\n>>> with tf.\nSession() assess:\n... print (sess .run(adder, feed_dict ={a: 2,b : 3}))\n...\n5", "356": "4.13 Deep Learning 345\nFig. 4.62 Flow diagram for\nmultiplier\nThis works with matrices also, with few changes (Fig. 4.62)\n>>> import numpy as np\n>>>a=tf.placeholder( 'float' ,[3,5])\n>>>b=tf.placeholder( 'float' ,[3,5])\n>>>adder =tf.add(a,b)\n>>> with tf.Session() assess:\n... b_=np.arange( 15).reshape(( 3,5))\n... print (sess .run(adder,feed_dict ={a:np .ones(( 3,5)),\n... b:b_}))\n...[[ 1. 2. 3. 4. 5.]\n[ 6. 7. 8. 9. 10.]\n[11. 12. 13. 14. 15.]]\nMatrix operations like multiplication are also implemented,\n>>># the None dimension leaves it variable\n>>>b=tf.placeholder( 'float' ,[5,None])\n>>>multiplier =tf.matmul(a,b)\n>>> with tf.Session() assess:\n... b_=np.arange( 20).reshape(( 5,4))\n... print (sess .run(multiplier,feed_dict ={a:np .ones(( 3,5)),\n... b:b_}))\n...\n[[40. 45. 50. 55.]\n[40. 45. 50. 55.]\n[40. 45. 50. 55.]]\nThe individual computational graphs can be stacked as shown in Fig. 4.63.\n>>>b=tf.placeholder( 'float' ,[3,5])\n>>>c=tf.placeholder( 'float' ,[5,None])\n>>>adder =tf.add(a,b)\n>>>multiplier =tf.matmul(adder,c)\n>>> with tf.Session() assess:\n... b_=np.arange( 15).reshape(( 3,-1))\n... c_=np.arange( 20).reshape(( 5,4))\n... print (sess .run(multiplier,feed_dict ={a:np .ones(( 3,5)),\n... b:b_,", "357": "346 4 Machine Learning\nFig. 4.63 Flow diagram for\nadder and multiplier\n... c:c_}))\n...\n[[160. 175. 190. 205.]\n[360. 400. 440. 480.][560. 625. 690. 755.]]\nOptimizers . To compute the parameters of complicated neural networks, a wide\nvariety of optimization algorithms are also implemented in Tensor\ufb02ow. Consider the\nclassic least-squares problem: Find xthat minimizes\nmin\nx/bardblAx\u2212b/bardbl2\nFirst, we have to de\ufb01ne a variable that we want the optimizer to solve for,\n>>>x=tf.Variable(tf .zeros(( 3,1)))\nNext, we create sample matrices Aandb,\n>>>A=tf.constant([ 6,6,4,\n... 3,4,0,\n... 7,2,2,\n... 0,2,1,\n... 1,6,3],'float' ,shape =(5,3))\n>>>b=tf.constant([ 1,2,3,4,5],'float' ,shape =(5,1))\nIn neural network terminology, the output of the model ( Ax) is called the activation ,\n>>>activation =tf.matmul(A,x)\nThe job of the optimizer is to minimize the squared distance between the activation\nand the bvector. Tensor\ufb02ow implements primitives like reduce_sum to compute\nthe square difference as a cost variable.\n>>>cost =tf.reduce_sum(tf .pow(activation -b,2))\nWith all that de\ufb01ned, we can construct the speci\ufb01c Tensor\ufb02ow optimizer we want,\n>>>learning_rate = 0.001\n>>>optimizer =tf.train .GradientDescentOptimizer(learning_rate) .minimize(cost)", "358": "4.13 Deep Learning 347\nFig. 4.64 Iterative costs as gradient descent algorithm computes solution. Note that we are showing\nonly a slice of all the values computed\nThe learning_rate is an embedded parameter for the GradientDescent\nOptimizer gradient descent algorithm. Next, we have to initialize all the variables\n(Fig. 4.64),\n>>>init=tf.global_variables_initializer()\nand create the session, without the context manager, just to show that the context\nmanager is not a requirement,\n>>>sess =tf.Session()\n>>>sess.run(init)\n>>>costs =[]\n>>> for iinrange (500):\n... costs .append(sess .run(cost))\n... sess.run(optimizer)\n...\nNote that we have to iterate over the optimizer to get it to step-wise work through\nthe gradient descent algorithm. As an illustration, we can plot the change in the costfunction as it iterates.\nThe \ufb01nal answer after all the iterations is the following:\n>>> print (x.eval(session =sess))\n[[-0.08000698]\n[ 0.6133011 ]\n[ 0.09500197]]\nBecause this is a classic problem, we know how to solve it analytically as in the\nfollowing:", "359": "348 4 Machine Learning\n>>># least squares solution\n>>>A_=np.matrix(A .eval(session =sess))\n>>> print (np.linalg .inv(A_ .T*A_)*(A_.T)*b.eval(session =sess))\n[[-0.07974136]\n[ 0.6141343 ]\n[ 0.09303147]]\nwhich is pretty close to what we found by iterating.\nLogistic Regression with Tensor\ufb02ow . As an example, let us revisit the logistic\nregression problem using Tensor\ufb02ow.\n>>> import numpy as np\n>>> from matplotlib.pylab import subplots\n>>>v= 0.9\n>>>@np.vectorize\n... def gen_y (x):\n... ifx<5:return np.random .choice([ 0,1],p=[v,1-v])\n... else: return np.random .choice([ 0,1],p=[1-v,v])\n...\n>>>xi=np.sort(np .random .rand( 500)*10)\n>>>yi=gen_y(xi)\nThe simplest multilayer perceptron has a single hidden layer. Given the training\nset{xi,yi}The input vector xiis component-wise multiplied by the weight vector,\nwand then fed into the nonlinear sigmoidal function. The output of the sigmoidal\nfunction is then compared to the training output, yi, corresponding to the weight\nvector, to form the error. The key step after error-formation is the backpropagation\nstep. This applies the chain rule from calculus to transmit the differential error backto the weight vector.\nLet\u2019s see if we can reproduce the logistic regression solution shown in Fig. 4.24\nusing Tensor\ufb02ow. The \ufb01rst step is to import the Tensor\ufb02ow module,\n>>> import tensorflow as tf\nWe need to reformat the training set slightly,>>>yi[yi ==0]=-1 # use 1/-1 mapping\nThen, we create the computational graph by creating variables and placeholders for\nthe individual terms,\n>>>w=tf.Variable([ 0.1])\n>>>b=tf.Variable([ 0.1])\n>>># the training set items fill these\n>>>x=tf.placeholder( \"float\" ,[None])\n>>>y=tf.placeholder( \"float\" ,[None])", "360": "4.13 Deep Learning 349\nThe output of the neural network is sometimes called the activation ,\n>>>activation =tf.exp(w *x+b)/(1+tf.exp(w *x+b))\nThe optimization problem is to reduce the following objective function, which\nincludes the one-dimensional regularization term w2,\n>>># objective\n>>>obj=tf.reduce_sum(tf .log(1+tf.exp(-y*(b+w*x)))) +tf.pow(w, 2)\nGiven the objective function, we choose the GradientDescentOptimizer as the\noptimization algorithm with the embedded learning rate,\n>>>optimizer =tf.train .GradientDescentOptimizer( 0.001/5. ).minimize(obj)\nNow, we are just about ready to start the session. But, \ufb01rst we need to initialize all\nthe variables,\n>>>init=tf.global_variables_initializer()\nWe\u2019ll use an interactive session for convenience and then step through the opti-\nmization algorithm in the following loop,\n>>>s=tf.InteractiveSession()\n>>>s.run(init)\n>>> for iinrange (1000):\n... s.run(optimizer,feed_dict ={x:xi,y:yi})\n...\nThe result of this is shown in Fig. 4.65 which says that logistic regression and this\nsimple single-layer perceptron both come up with the same answer.\nFig. 4.65 This shows the\nresult from logistic regression\nas compared to the corre-\nsponding result from simplesingle-layer perceptron\n", "361": "350 4 Machine Learning\n4.13.2 Understanding Gradient Descent\nConsider a smooth function fover Rnsuppose we want to \ufb01nd the minimum value\noff(x)over this domain, as in the following:\nx\u2217=arg min\nxf(x)\nThe idea with gradient descent is to choose an initial point x(0)\u2208Rn\nx(k+1)=x(k)\u2212\u03b1\u2207f(x(k))\nwhere \u03b1is the step size (learning rate). The intuition here is that \u2207fis the direction\nof increase and so that moving in the opposite direction scaled by \u03b1moves toward\na lower function value. This approach turns out to be very fast for well-conditioned,strongly convex fbut in general there are practical issues.\nFigure 4.66 shows the function f(x)=2\u22123x\n3+x4and its \ufb01rst-order Taylor\nseries approximation at selected points along the curve for a given width parameter.That is, the Taylor approximation approximates the function at a speci\ufb01c point with\na corresponding interval around that point for which the approximation is assumed\nvalid. The size of this width is determined by the \u03b1step parameter. Crucially, the\nquality of the approximation varies along the curve. In particular, there are sections\nwhere two nearby approximations overlap given the width, as indicated by the darkshaded regions. This is key because gradient descent works by using such \ufb01rst-order\napproximations to estimate the next step in the minimization. That is, the gradient\ndescent algorithm never actually sees f (x), but rather only the given \ufb01rst-order\napproximant. It judges the direction of the next iterative step by sliding down the\nslope of the approximant to the edge of a region (determined by \u03b1) and then using\nthat next point for the next calculated approximant. As shown by the shaded regions,it is possible that the algorithm will overshoot the minimum because the step size\n(\u03b1) is too large. This can cause oscillations as shown in Fig. 4.67.\nLet us consider the following Python implementation of gradient descent, using\nSympy.\n>>>x=sm.var('x')\n>>>fx=2-3 * x**3 + x**4\n>>>df=fx.diff(x) # compute derivative\n>>>x0=.1# initial guess\n>>>xlist =[(x0,fx .subs(x,x0))]\n>>>alpha = 0.1 # step size\n>>> for iinrange (20):\n... x0=x0-alpha *df.subs(x,x0)\n... xlist .append((x0,fx .subs(x,x0)))\n...", "362": "4.13 Deep Learning 351\nFig. 4.66 The piece-wise linear approximant to f(x)\nFig. 4.67 The step size may cause oscillations\nFigure 4.67 shows the sequential steps. Note that the algorithm oscillates at the end\nbecause the step size is too large. Practically speaking, it is not possible to know theoptimal step size for general functions without strong assumptions on f(x).\nFigure 4.68 shows how the algorithm moves along the function as well as the\napproximant ( \u02c6f(x)) that the algorithm sees along the way. Note that initial steps are\ncrowded around the initial point because the corresponding gradient is small there.\nToward the middle, the algorithm makes a big jump because the gradient is steep,before \ufb01nally oscillating toward the end. Sections of the underlying function that are\nrelatively \ufb02at can cause the algorithm to converge very slowly. Furthermore, if there\nare multiple local minima, then the algorithm cannot guarantee \ufb01nding the globalminimum.", "363": "352 4 Machine Learning\nFig. 4.68 Gradient descent produces a sequence of approximants\nAs we have seen, the step size is key to both performance and convergence. Indeed,\na step size that is too big can cause divergence and one that is too small can take a\nvery long time to converge.\nNewton\u2019s Method . Consider the following second-order Taylor series expansion\nJ(x)=f(x0)+\u2207 f(x0)T(x\u2212x0)+1\n2(x\u2212x0)T\u22072f(x0)(x\u2212x0)\nwhere H(x):= \u22072f(x)is the Hessian matrix of second derivatives. The (i,j)th\nentry of this matrix is the following:\n\u22022f(x)\n\u2202xi\u2202xj\nWe can use basic matrix calculus to \ufb01nd the minimum by computing:\n\u2207xJ(x)=\u2207 f(x0)+H(x)(x\u2212x0)=0\nSolving this for xgives the following:\nx=x0\u2212H(x)\u22121\u2207f(x0)\nThus, after renaming some terms, the descent algorithm works by the following\nupdate equation:\nx(k+1)=x(k)\u2212H(x(k))\u22121\u2207f(x(k))\nThere are a number of practical problems with this update equation. First, it requires\ncomputing the Hessian matrix at every step. For a signi\ufb01cant problem, this meansmanaging a potentially very large matrix. For example, given 1000 dimensions the", "364": "4.13 Deep Learning 353\ncorresponding Hessian has 1000 \u00d71000 elements. Some other issues are that the\nHessian may not be numerically stable enough to invert, the functional form of the\npartial derivatives may have to be separately approximated, and the initial guess hasto be in a region where the convexity of the function matches the derived assumption.\nOtherwise, just based on these equations, the algorithm will converge on the local\nmaximum and not the local minimum . Consider a slight change of the previous code\nto implement Newton\u2019s method:\n>>>x0=2.# init guess is near to solution\n>>>xlist =[(x0,fx .subs(x,x0))]\n>>>df2 =fx.diff(x, 2)# 2nd derivative\n>>> for iinrange (5):\n... x0=x0-df.subs(x,x0) /df2.subs(x,x0)\n... xlist .append((x0,fx .subs(x,x0)))\n...\n>>>xlist =np.array(xlist) .astype( float )\n>>> print (xlist)\n[[ 2. -6. ]\n[ 2.33333333 -6.4691358 ][ 2.25555556 -6.54265522]\n[ 2.25002723 -6.54296874]\n[ 2.25 -6.54296875][ 2.25 -6.54296875]]\nNote that it took very few iterations to get to the minimum (as compared to our\nprior method), but if the initial guess is too far away from the actual minimum, the\nalgorithm may not \ufb01nd the local minimum at all and instead \ufb01nd the local maximum.Naturally, there are many extensions to this method to account for these effects, but\nthe main thrust of this section is to illustrate how higher order derivatives (when\navailable) in a computationally feasible context can greatly accelerate convergence\nof descent algorithms.\nManaging Step Size . The problem of determining a good step size (learning rate)\ncan be approached with an exact line search . That is, along the ray that extends along\nx+q\u2207f(x), \ufb01nd\nq\nmin=arg min\nq\u22650f(x+q\u2207f(x))\nIn words, this means that given a direction from a point xalong the direction \u2207f(x),\n\ufb01nd the minimum for this one-dimensional problem. Thus, the minimization pro-\ncedure alternates at each iteration between moving to a new xposition in Rnand\n\ufb01nding a new step size by solving the one-dimensional minimization.\nWhile this is conceptually clean, the problem is that solving the one-dimensional\nline search at every step means evaluating the objective function f(x)at many points", "365": "354 4 Machine Learning\nalong the one-dimensional slice. This can be very time consuming for an objective\nfunction that is computationally expensive to evaluate. With Newton\u2019s method, we\nhave seen that higher order derivatives can accelerate convergence and we can applythose ideas to the one-dimensional line search, as with the backtracking algorithm.\n\u2022Fix parameters \u03b2\u2208[0,1)an\u03b1>0.\n\u2022Iff(x\u2212\u03b1\u2207f(x)) > f(x)\u2212\u03b1/bardbl\u2207f(x)/bardbl\n2\n2then reduce \u03b1\u2192\u03b2\u03b1.O t h e r w i s e ,d o\nthe usual gradient descent update: x(k+1)=x(k)\u2212\u03b1\u2207f(x(k)).\nTo gain some intuition about how this works, return to our second-order Taylor series\nexpansion of the function fabout x0,\nf(x0)+\u2207 f(x0)T(x\u2212x0)+1\n2(x\u2212x0)T\u22072f(x0)(x\u2212x0)\nWe have already discussed the numerical issues with the Hessian term, so one\napproach is to simply replace that term with an n\u00d7nidentity matrix Ito obtain\nthe following:\nh\u03b1(x)=f(x0)+\u2207 f(x0)T(x\u2212x0)+1\n2\u03b1/bardblx\u2212x0/bardbl2\nThis is our more tractable surrogate function. But what is the relationship between\nthis surrogate and what we are actually trying to minimize? The key difference is that\nthe curvature information that is contained in the Hessian term has now been reduced\nt oas i n g l e1 /\u03b1factor. Intuitively, this means that local complicated curvature of f\nabout a given point x0has been replaced with a uniform bowl-shaped structure, the\nsteepness of which is determined by scaling 1 /\u03b1. Given a speci\ufb01c \u03b1, we already\nknow how to step directly to the minimum of h\u03b1(x); namely, using the following\ngradient descent update equation:\nx(k+1)=x(k)\u2212\u03b1\u2207f(x(k))\nThat is the immediate solution to the surrogate problem, but it does not directly\nsupply the next iteration for the function we really want: f. Let us suppose that\nour minimization of the surrogate has taken us to a new point x(k)that satis\ufb01es the\nfollowing inequality,\nf(x(k+1))\u2264h\u03b1(x(k+1))\nor, more explicitly,\nf(x(k+1))\u2264f(x(k))+\u2207 f(x(k))T(x(k+1)\u2212x(k))+1\n2\u03b1/bardblx(k+1)\u2212x(k)/bardbl2", "366": "4.13 Deep Learning 355\nWe can substitute the update equation into this and simplify as,\nf(x(k+1))\u2264f(x(k))\u2212\u03b1\u2207f(x(k))T(\u2207f(x(k)))+\u03b1\n2/bardbl\u2207f(x(k))/bardbl2\nwhich ultimately boils down to the following:\nf(x(k+1))\u2264f(x(k))\u2212\u03b1\n2/bardbl\u2207f(x(k))/bardbl2(4.13.2.1)\nThe important observation here is that if we have not reached the minimum of f,\nthen the last term is always positive and we have moved downward,\nf(x(k+1))< f(x(k))\nwhich is what we were after. Conversely, if the inequality in Eq. ( 4.13.2.1 ) holds\nfor some \u03b1>0, then we know that h\u03b1>f. This is the key observation behind\nthe backtracking algorithm. That is, we can test a sequence of values for \u03b1until we\n\ufb01nd one that satis\ufb01es Eq. ( 4.13.2.1 ). For example, we can start with some initial \u03b1\nand then scale it up or down until the inequality is satis\ufb01ed which means that we\nhave found the correct step size and then can proceed with the descent step. This is\nwhat the backtracking algorithm is doing as shown in Fig. 4.69. The dotted line is\ntheh\u03b1(x)and the gray line is f(x). The algorithm hops to the quadratic minimum\nof the h\u03b1(x)function which is close to the actual minimum of f(x).\nThe basic implementation of backtracking is shown below:\n>>>x0=1\n>>>alpha = 0.5\n>>>xnew =x0-alpha *df.subs(x,x0)\n>>> while fx.subs(x,xnew) >(fx.subs(x,x0) -(alpha /2.)*(fx.subs(x,x0)) **2):\n... alpha =alpha * 0.8\n... xnew =x0-alpha *df.subs(x,x0)\n...\n>>> print (alpha,xnew)\n0.32000000000000006 2.60000000000000\nStochastic Gradient Descent . A common variation on gradient descent is to alter\nhow the weights are updated. Speci\ufb01cally, suppose we want to minimize an objective\nfunction of the following form:\nminxm/summationdisplay\ni=1fi(x)\nwhere iindexes the ithdata element for an error function. Equivalently, each sum-\nmand is parameterized by a data element.\nFor the usual gradient descent algorithm, we would compute the incremental\nweights, component-wise as in the following", "367": "356 4 Machine Learning\nFig. 4.69 The approximation h\u03b1(x)(dotted line) moves the next iteration from x=1t ot h e\nindicated point that is near the minimum of f(x)by \ufb01nding an appropriate step size ( \u03b1)\nx(k+1)=x(k)\u2212\u03b1km/summationdisplay\ni=1\u2202fi(x(k))\nby summing over all of the data. The key idea for stochastic gradient descent is to\nnotsum over all of the data but rather to update the weights for each randomized ith\ndata element:\nx(k+1)=x(k)\u2212\u03b1k\u2202fi(x(k))\nA compromise between batch and this jump-every-time stochastic gradient descent\nismini-batch gradient descent in which a randomized subset ( \u03c3r,|\u03c3r|=Mb)o ft h e\ndata is summed over at each step as in the following:\nx(k+1)=x(k)\u2212\u03b1k/summationdisplay\ni\u2208\u03c3r\u2202fi(x(k))\nEach step update for the standard gradient descent algorithm processes mdata points\nfor each of the pdimensions, O(mp), whereas for stochastic gradient descent, we\nhave O(p). Mini-batch gradient descent is somewhere in-between these estimates.\nFor very large, high-dimensional data, the computational costs of gradient descentcan become prohibitive thus favoring stochastic gradient descent. Outside of the\ncomputational advantages, stochastic gradient descent has other favorable attributes.\nFor example, the noisy jumping around helps the algorithm avoid getting stalled inlocal minima and this helps the algorithm when the starting point is far away from\nthe actual minimum. The obverse is that stochastic gradient descent can struggle\nto clinch the minimum when it is close to it. Another advantage is robustness to a\nminority of baddata elements. Because only random subsets of the data are actually\nused in the update, the few individual outlier data points (perhaps due to poor dataintegrity) do not necessarily contaminate every step update (Fig. 4.70).", "368": "4.13 Deep Learning 357\nFig. 4.70 The approximation\nh\u03b1(x)(dotted line) moves the\nnext iteration from x=1\nto the indicated point that isnear the minimum of f(x)\nby \ufb01nding an appropriate stepsize (\u03b1)\nMomentum . The gradient descent algorithm can be considered as a particle mov-\ning along a high-dimensional landscape in search of a minimum. Using a physicalanalogy, we can add the concept of momentum to the particle\u2019s motion. Consider the\nposition of the particle ( x\n(k)) at any time kunder a net force proportional to \u2212\u2207J.\nThis setup induces an estimated velocity term for the particle motion proportionalto\u03b7(x\n(k+1)\u2212x(k)). That is, the particle\u2019s velocity is estimated proportional to the\ndifference in two successive positions. The simplest version of stochastic gradient\ndescent update that incorporates this momentum is the following:\nx(k+1)=x(k)\u2212\u03b1\u2207f(x(k))+\u03b7(x(k+1)\u2212x(k))\nMomentum is particularly useful when gradient descent sloshes up and down a steep\nravine in the error surface instead of pursuing the descending ravine directly to the\nlocal minimum. This oscillatory behavior can cause slow convergence. There are\nmany extensions to this basic idea such as Nesterov momentum.\nAdvanced Stochastic Gradient Descent . Methods that aggregate histories of the\nstep updates can provide superior performance to the basic stochastic gradient descent\nalgorithm. For example, Adam (Adaptive Moment Estimator) implements an adap-\ntive step size for each parameter. It also keeps track of an exponentially decayingmean and variance of past gradients using the exponentially weighted moving aver-\nage (EWMA). This smoothing technique computes the following recursion,\ny\nn=axn+(1\u2212a)yn\u22121\nwith y0=x0as the initial condition. The 0 <a<1 factor controls the amount\nof mixing between the previous moving average and the new data point at n.F o r\nexample, if a=0.9, the EWMA favors the new data xnover the prior value yn\u22121\n(1\u2212a=0.1) of the EWMA. This calculation is common in a wide variety of", "369": "358 4 Machine Learning\nFig. 4.71 Different variations\nof gradient descent\ntime series applications (i.e., signal processing, quantitative \ufb01nance). The impulse\nresponse of the EWMA ( x=\u03b4n)i s(1\u2212a)n. You can think of this as the weighted\nwindow function that is applied to xn. As opposed to the standard moving average\nthat considers a \ufb01xed window of data to average over, this exponential window retains\nprior memory of the entire sequence, albeit weighted by powers of (1\u2212a).T os e e\nthis, we can generate the response to an impulse data series using pandas ,\n>>> import pandas as pd\n>>>x=pd.Series([ 1]+[0]*20)\n>>>ma=x.rolling(window =3, center =False ).mean()\n>>>ewma =x.ewm(1).mean()\nAs shown by Fig. 4.71, the single nonzero data point thereafter in\ufb02uences the EWMA\nwhereas for the \ufb01xed-width moving window average, the effect terminates after the\nwindow passes. Note that mini-batch smoothes out data at each iteration by averaging\nover training data and EWMA smoothes out the descent motion across iterations ofthe algorithm.\nAdvanced stochastic gradient descent algorithms are themselves an area of intense\ninterest and development. Each method has its strengths and weaknesses pursuant tothe data at hand (i.e., sparse vs. dense data) and there is no clear favorite appropriate to\nall circumstances. As a practical matter, some variants have parallel implementations\nthat accelerate performance (i.e., Nui\u2019s Hogwild update scheme).\nPython Example Using Sympy . Each of these methods will make more sense with\nsome Python. We emphasize that this implementation is strictly expository and wouldnot be suitable for a large-scale application. Let us reconsider the classi\ufb01cation\nproblem in the section on logistic regression with the target y\ni\u2208{0,1}. The logistic\nregression seeks to minimize the cross-entropy:\nJ(\u03b2)=m/summationdisplay\nilog(1+exp(xiT\u03b2))\u2212yixiT\u03b2", "370": "4.13 Deep Learning 359\nwith the corresponding gradient,\n\u2207\u03b2J(\u03b2)=m/summationdisplay\ni1\n1+exp(\u2212xT\ni\u03b2)xi\u2212yixi\nTo get started let\u2019s create some sample data for logistic regression\n>>> import numpy as np\n>>> import sympy as sm\n>>> npts = 100\n>>> X=np.random .rand(npts, 2)*6-3 # random scatter in 2-d plane\n>>> labels =np.ones(X .shape[ 0],dtype =np.int) # labels are 0 or 1\n>>> labels[(X[:, 1]<X[:, 0])]=0\nThis provides the data in the XNumpy array and the target labels in the labels array.\nNext, we want to develop the objective function with Sympy,\n>>> x0,x1 =sm.symbols( 'x:2' ,real =True )# data placeholders\n>>> b0,b1 =sm.symbols( 'b:2' ,real =True )# parameters\n>>> bias =sm.symbols( 'bias' ,real =True )# bias term\n>>> y=sm.symbols( 'y',real =True )# label placeholders\n>>> summand =sm.log( 1+sm.exp(x0 *b0+x1*b1+bias)) -y*(x0*b0+x1*b1+bias)\n>>> J=sum([summand .subs({x0:i,x1:j,y:y_i})\n... for (i,j),y_i inzip(X,labels)])\nWe can use Sympy to compute the gradient as in the following:\n>>> from sympy.tensor.array import derive_by_array\n>>>grad =derive_by_array(summand,(b0,b1,bias))\nUsing the sm.latex function renders grad as the following:\n/bracketleftBig\n\u2212x0y+x0eb0x0+b1x1+bias\neb0x0+b1x1+bias+1\u2212x1y+x1eb0x0+b1x1+bias\neb0x0+b1x1+bias+1\u2212y+eb0x0+b1x1+bias\neb0x0+b1x1+bias+1/bracketrightBig\nwhich matches our previous computation of the gradient. For standard gradient\ndescent, the gradient is computed by summing over all of the data,\n>>>grads =np.array([grad .subs({x0:i,x1:j,y:y_i})\n... for(i,j),y_i inzip(X,labels)]) .sum(axis =0)\nNow, to implement gradient descent, we set up the following loop:\n>>># convert expression into function\n>>>Jf=sm.lambdify((b0,b1,bias),J)\n>>>gradsf =sm.lambdify((b0,b1,bias),grads)\n>>>niter = 200\n>>>winit =np.random .randn( 3)*20\n>>>alpha = 0.1 # learning rate (step-size)", "371": "360 4 Machine Learning\n>>>WK=winit # initialize\n>>>Jout=[]# container for output\n>>> for iinrange (niter):\n... WK=WK-alpha *np.array(gradsf( *WK))\n... Jout.append(Jf( *WK))\n...\nFor stochastic gradient descent, the above code changes to the following:\n>>> import random\n>>>sgdWK =winit # initialize\n>>>Jout=[]# container for output\n>>># don't sum along all data as before\n>>>grads =np.array([grad .subs({x0:i,x1:j,y:y_i})\n... for (i,j),y_i inzip(X,labels)])\n>>> for iinrange (niter):\n... gradsf =sm.lambdify((b0,b1,bias),random .choice(grads))\n... sgdWK =sgdWK -alpha *np.array(gradsf( *sgdWK))\n... Jout.append(Jf( *sgdWK))\n...\nThe main difference here is that the gradient calculation no longer sums across\nall of the input data (i.e., grads list) and is instead randomly chosen by the\nrandom.choice function the above body of the loop. The extension to batch gra-\ndient descent from this code just requires averaging over a sub-selection of the data\nfor the gradients in the batch variable.\n>>>mbsgdWK =winit # initialize\n>>>Jout=[]# container for output\n>>>mb=1 0 # number of elements in batch\n>>> for iinrange (niter):\n... batch =np.vstack([random .choice(grads)\n... for iinrange (mb)]) .mean(axis =0)\n... gradsf =sm.lambdify((b0,b1,bias),batch)\n... mbsgdWK =mbsgdWK -alpha *np.array(gradsf( *mbsgdWK))\n... Jout.append(Jf( *mbsgdWK))\n...\nIt is straightforward to incorporate momentum into this loop using a Python deque ,\nas in the following:\n>>> from collections import deque\n>>>momentum =deque([winit,winit], 2)\n>>>mbsgdWK =winit # initialize\n>>>Jout =[]# container for output\n>>>mb=1 0 # number of elements in batch\n>>> for iinrange (niter):\n... batch =np.vstack([random .choice(grads)\n... for iinrange (mb)]) .mean(axis =0)", "372": "4.13 Deep Learning 361\n... gradsf =sm.lambdify((b0,b1,bias),batch)\n... mbsgdWK =mbsgdWK -alpha *np.array(gradsf( *mbsgdWK)) +0.5* (momentum[ 1]-momentum[ 0])\n... Jout .append(Jf( *mbsgdWK))\n...\nFigure 4.71 shows the three variants of the gradient descent algorithm. Notice that\nthe stochastic gradient descent algorithm is the most erratic, as it is characterized\nby taking a new direction for every randomly selected data element. Mini-batchgradient descent smoothes these out by averaging across multiple data elements. The\nmomentum variant is somewhere in-between the to as the effect of the momentum\nterm is not pronounced in this example.\nPython Example Using Theano . The code shown makes each step of the gradient\ndescent algorithms explicit using Sympy, but this implementation is far too slow.Thetheano module provides thoughtful and powerful high-level abstractions for\nalgorithm implementation that relies upon underlying C/C++ and GPU execution\nmodels. This means that calculations that are prototyped with theano can be executed\ndownstream outside of the Python interpreter which makes them much faster. The\ndownside of this approach is that calculations can become much harder to debug\nbecause of the multiple levels of abstraction. Nonetheless, theano is a powerful tool\nfor algorithm development and execution.\nTo get started we need some basics from theano .\n>>> import theano\n>>> import theano.tensor as T\n>>> from theano import function, shared\nthe next step is to de\ufb01ne variables, which are essentially placeholders for values that\nwill be computed downstream later. The next block de\ufb01nes two named variables\nas a double-sized \ufb02oat matrix and vector. Note that we did not have to specify thedimensions of each at this point.\n>>>x=T.dmatrix( \"x\")# double matrix\n>>>y=T.dvector( \"y\")# double vector\nThe parameters of our implementation of gradient descent come next, as the follow-\ning:\n>>>w=shared(np .random .randn( 2), name =\"w\")# parameters to fit\n>>>b=shared( 0.0, name =\"b\")# bias term\nvariables that are shared are ones whose values can be set separately via other\ncomputations or directly via the set_value() method. These values can also be\nretrieved using the get_value() method. Now, we need to de\ufb01ne the probability\nof obtaining a 1from the given data as p. The cross-entropy function and the T.dot\nfunction are already present (along with a wide range of other related functions)intheano . The conformability of the constituent arguments is the responsibility\nof the user.", "373": "362 4 Machine Learning\n>>>p=1/(1+T.exp(-T.dot(x,w) -b)) # probability of 1\n>>>error =T.nnet.binary_crossentropy(p,y)\n>>>loss =error .mean()\n>>>gw, gb =T.grad(loss, [w, b])\nTheerror variable is TensorVariable type which has many built-in methods such\nasmean . The so-derived loss function is therefore also a TensorVariable .T h e\nlastT.grad line is the best part of Theano because it can compute these gradients\nautomatically.\n>>>train =function(inputs =[x,y],\n... outputs =[error],\n... updates =((w, w -alpha *gw),\n... (b, b -alpha *gb)))\nThe last step is to set up training by de\ufb01ning the training function in theano .T h e\nuser will supply the previously de\ufb01ned and named input variables ( xandy) and\ntheano will return the previously de\ufb01ned error variable. Recall that the wandb\nvariables were de\ufb01ned as shared variables. This means that the function train can\nupdate their values between calls using the update formula speci\ufb01ed in the updates\nkeyword variable. In this case, the update is just plain gradient descent with the\npreviously de\ufb01ned alpha step size variable.\nWe can execute the training plan using the train function in the following loop:\n>>>training_steps =1000\n>>> for iinrange (training_steps):\n... error =train(X, labels)\n...\nThetrain(X,labels) call is where the Xandlabels arrays we de\ufb01ned earlier\nreplace the placeholder variables. The update step refreshes all of the shared vari-\nables at each iterative step. At the end of the iteration, the so-computed parametersa r ei nt h e wandbvariables with values available via get_value() . The implemen-\ntation for stochastic gradient descent requires just a little modi\ufb01cation to this loop,\nas in the following:\n>>> for iinrange (training_steps):\n... idx =np.random .randint( 0,X.shape[ 0])\n... error =train([X[idx,:]], [labels[idx]])\n...\nwhere the idx variable selects a random data element from the set and uses that for\nthe update step at every iteration. Likewise, batch stochastic gradient descent followswith the following modi\ufb01cation,\n>>>batch_size =5 0\n>>>indices =np.arange(X .shape[ 0])\n>>> for iinrange (training_steps):\n... idx =np.random .permutation(indices)[:batch_size]", "374": "4.13 Deep Learning 363\n... error =train(X[idx,:], labels[idx])\n...\n>>> print (w.get_value())\n[-4.84350587 5.013989 ]\n>>> print (b.get_value()) # bias term\n0.5736726430208784\nHere, we set up an indices variable that is used for randomly selecting subsets in\ntheidx variable that are passed to the train function. All of these implementations\nparallel the corresponding previous implementations in Sympy, but these are many\norders of magnitude faster due to theano .\n4.13.3 Image Processing Using Convolutional Neural\nNetworks\nIn this section, we develop the convolutional neural network (CNN) which is the\nfundamental deep learning image processing application. We deconstruct every layer\nof this network to develop insight into the purpose of the individual operations. CNNs\ntake image as inputs and images can be represented as Numpy arrays, which makesthem fast and easy to use with any of the scienti\ufb01c Python tools. The individual\nentries of the Numpy array are the pixels and the row/column dimensions are the\nheight/width of the image, respectively. The array values are between 0through\n255 and correspond to the intensity of the pixel at that location. Three-dimensional\nimages have a third depth-dimension as the color channel (e.g., red, green, blue).\nTwo-dimensional image arrays are grayscale.\nProgramming Tip\nMatplotlib makes it easy to draw images using the underlying Numpy arrays.\nFor instance, we can draw Fig. 4.72 using the following MNIST image from\nsklearn.datasets , which represents grayscale hand-drawn digits (the num-\nber zero in this case).\n>>> from matplotlib.pylab import subplots, cm\n>>> from sklearn import datasets\n>>>mnist =datasets .load_digits()\n>>>fig, ax =subplots()\n>>>ax.imshow(mnist .images[ 0],\n... interpolation ='nearest' ,\n... cmap=cm.gray)\n<matplotlib.image.AxesImage object at 0x7f98d4212f98>\nThecmap keyword argument speci\ufb01es the colormap as gray. The interpolation\nkeyword means that the resulting image from imshow does not try to visually\nsmooth out the data, which can be confusing when working at the pixel level.\nThe other hand-drawn digits are shown below in Fig. 4.73.", "375": "364 4 Machine Learning\nFig. 4.72 Image of a hand-drawn number zero from the MNIST dataset\nFig. 4.73 Samples of the other hand-drawn digits from MNIST\nConvolution . Convolution is an intensive calculation and it is the core of convolu-\ntional neural networks. The purpose of convolution is to create alternative representa-\ntions of the input image that emphasize or deemphasize certain features represented\nby the kernel . The convolution operation consists of a kernel and an input matrix .\nThe convolution operation is a way of aligning and comparing image data with the\ncorresponding data in an image kernel. You can think of an image kernel as a tem-\nplate for a canonical feature that the convolution operation will uncover. To keep itsimple suppose we have the following 3x3 kernel matrix,\n>>> import numpy as np\n>>>kern =np.eye(3,dtype =np.int)\n>>>kern", "376": "4.13 Deep Learning 365\narray([[1, 0, 0],\n[0, 1, 0],\n[0, 0, 1]])\nUsing this kernel, we want to \ufb01nd anything in an input image that looks like a diagonal\nline. Let\u2019s suppose we have the following input Numpy image\n>>>tmp =np.hstack([kern,kern *0])\n>>>x=np.vstack([tmp,tmp])\n>>>x\narray([[1, 0, 0, 0, 0, 0],\n[0, 1, 0, 0, 0, 0],\n[0, 0, 1, 0, 0, 0],\n[1, 0, 0, 0, 0, 0],[0, 1, 0, 0, 0, 0],\n[0, 0, 1, 0, 0, 0]])\nNote that this image is just the kernel stacked into a larger Numpy array. We want to\nsee if the convolution can pull out the kernel that is embedded in the image. Of course,\nin a real application we would not know whether or not the kernel is present in the\nimage, but this example helps us understand the convolution operation step-by-step.\nThere is a convolution function available in the scipy module.\n>>> from scipy.ndimage.filters import convolve\n>>>res =convolve(x,kern,mode ='constant' ,cval =0)\n>>>res\narray([[2, 0, 0, 0, 0, 0],\n[0, 3, 0, 0, 0, 0],\n[0, 0, 2, 0, 0, 0],\n[2, 0, 0, 1, 0, 0],[0, 3, 0, 0, 0, 0],\n[0, 0, 2, 0, 0, 0]])\nEach step of the convolution operation is represented in Fig. 4.74.T h e kern matrix\n(light blue square) is overlaid upon the xmatrix and the element-wise product is\ncomputed and summed. Thus, the 0,0 array output corresponds to this operation\napplied to the top left 3x3 slice of the input, which results in 3. The convolution\noperation is sensitive to boundary conditions. For this example, we have chosen\nmode=constant andcval=0 which means that the input image is bordered by zeros\nwhen the kernel sweeps outside of the input image boundary. This is the simplest\noption for managing the edge conditions and scipy.ndimage.filters.convolve\nprovides other practical alternatives. It is also common to normalize the output of theconvolution operation by dividing by the number of pixels in the kernel (i.e., 3in this\nexample). Another way to think about the convolution operation is as a matched \ufb01lter\nthat peaks when it \ufb01nds a compatible sub-feature. The \ufb01nal output of the convolutionoperation is shown in Fig. 4.75. The values of the individual pixels are shown in color.\nNotice where the maximum values of the output image are located on the diagonals.", "377": "366 4 Machine Learning\nFig. 4.74 The convolution process that produces the resarray. As shown in the sequence, the light\nbluekern array is slid around, overlaid, multiplied, and summed upon the xarray to generate the\nvalues of shown in the title. The output of the convolution is shown in Fig. 4.75\nFig. 4.75 Theres array\noutput of the convolutionis shown in Fig. 4.74.T h e\nvalues (in red) shown arethe individual outputs of theconvolution operation. The\ngrayscale indicates the relative\nmagnitude of the shown values(darker is greater)\n", "378": "4.13 Deep Learning 367\nFig. 4.76 The input array is a forward-slash diagonal. This sequence shows the step-by-step con-\nvolution operation. The output of this convolution is shown in Fig. 4.77\nHowever, the convolution operation is not a perfect detector and results in nonzero\nvalues for other cases. For example, suppose the input image is a forward-slash\ndiagonal line. The step-by-step convolution with the kernel is shown in Fig. 4.76\nwith corresponding output in Fig. 4.77 that looks nothing like the kernel or the input\nimage.\nWe can use multiple kernels to explore an input image. For example, suppose we\nhave the input image shown on the left in Fig. 4.78. The two kernels are shown in\nthe upper row, with corresponding outputs on the bottom row. Each kernel is able to\nemphasize its particular feature but extraneous features appear in both outputs. Wecan have as many outputs as we have kernels but because each output image is as\nlarge as the input image, we need a way to reduce the size of this data.", "379": "368 4 Machine Learning\nFig. 4.77 The output of the convolution operation shown in Fig. 4.76. Note that the output has\nnonzero elements where there is no match between the input image and the kernel\nFig. 4.78 Given two kernels (upper row) and the input image on the left, the output images are\nshown on the bottom row. Note that each kernel is able to emphasize its feature on the inputcomposite image but other extraneous features appear in the outputs\nMaximum Pooling . To reduce the size of the output images, we can apply maximum\npooling to replace a tiled subset of the image with the maximum pixel value in that\nparticular subset. The following Python code illustrates maximum pooling,\n>>> def max_pool (res,width =2,height =2):\n... m,n =res.shape\n... xi=[slice (i,i+width) for iinrange (0,m,width)]\n... yi=[slice (i,i+height) foriinrange (0,n,height)]\n... out =np.zeros(( len(xi), len(yi)),dtype =res.dtype)\n... for ni,i inenumerate (xi):", "380": "4.13 Deep Learning 369\nFig. 4.79 Themax_pool\nfunction reduces the sizeof the output images (leftcolumn) to the images on theright column. Note that thepool size is 2x2 so that the\nresulting pooled images arehalf the size of the originalimages in each dimension\n... fornj,j inenumerate (yi):\n... out[ni,nj] =res[i,j] .max()\n... return out\n...\nProgramming Tip\nTheslice object provides programmatic array slicing. For example, x[0,3]=x\n[slice(0,3)] . This means you can separate the slice from the array, which\nmakes it easier to manage.\nPooling reduces the dimensionality of the output of the convolution and makes stack-ing convolutions together computationally feasible. Figure 4.79 shows the output of\nthemax_pool function on the indicated input images.\nRecti\ufb01ed Linear Activation . Recti\ufb01ed Linear Activation Units (ReLUs) are neural\nnetwork units that implement the following activation function,\nr(x)=/braceleftBigg\nxifx>0\n0 otherwise", "381": "370 4 Machine Learning\nFig. 4.80 The training dataset for our convolutional neural network. The forward-slash images are\nlabeled category 0and the backward slash images are category 1\nTo use this activation properly, the kernels in the convolutional layer must be scaled\nto the {\u22121,1}range. We can implement our own recti\ufb01ed linear activation function\nusing the following code,\n>>> def relu(x):\n... 'rectified linear activation function'\n... out =np.zeros(x .shape,dtype =x.dtype)\n... idx =x>=0\n... out[idx] =x[idx]\n... return out\n...\nNow that we understand the basic building blocks, let us investigate how the\noperations \ufb01t together. To create some training image data, we use the following\nfunction to create some random backward and forward slashed images as shown inFig.4.80. As before, we have the scaled kernels shown in Fig. 4.81. We are going to", "382": "4.13 Deep Learning 371\nFig. 4.81 The two scaled\nfeature kernels for the convo-lutional neural network\napply the convolution, max-pooling, and recti\ufb01ed linear activation function sequence\nstep-by-step and observe the outputs at each step.\n>>> def gen_rand_slash (m=6,n=6,direction ='back' ):\n... '''generate random forward/backslash images.\n... Must have at least two pixels'''\n... assert direction in('back' ,'forward' )\n... assert n>=2 and m>=2\n... import numpy as np\n... import random\n... out =-np.ones((m,n),dtype =float )\n... i=random .randint( 2,min(m,n))\n... j=random .randint( -i,max(m,n) -1)\n... t=np.diag([ 1,]*i,j)\n... ifdirection =='forward' :\n... t=np.flipud(t)\n... try:\n... assert t.sum() .sum() >=2\n... out[np .where(t)] =1\n... return out\n... except :\n... return gen_rand_slash(m =m,n=n,direction =direction)\n...\n>>> # create slash-images training data with classification id 1 or 0\n>>> training =[(gen_rand_slash(), 1)for iinrange (10)]+\\\n... [(gen_rand_slash(direction ='forward' ),0)for iinrange (10)]\nFigure 4.82 shows the output of convolving the training data in Fig. 4.80 with\nkern1 , as shown on the left panel of Fig. 4.81. Note that the following code de\ufb01nes\neach of these kernels,\n>>>kern1 =(np.eye(3,dtype =np.int)*2-1)/9. # scale\n>>>kern2 =np.flipud(kern1)", "383": "372 4 Machine Learning\nFig. 4.82 The output of convolving the training data in Fig. 4.80 withkern1 , as shown on the left\npanel of Fig. 4.81\nThe next operation is the activation function for the recti\ufb01ed linear unit with output\nshown in Fig. 4.83. Note that all of the negative terms have been replaced by zeros.\nThe next step is the maximum pooling operation as shown in Fig. 4.84. Notice that\nthe number of total pixels in the training data has reduced from thirty-six per image\nto nine per image. With these processed images, we have the inputs we need for the\n\ufb01nal classi\ufb01cation step.\nConvolutional Neural Network Using Keras . Now that we have experimented\nwith the individual operations using our own Python code, we can construct the\nconvolutional neural network using Keras. In particular, we use the Keras functional\ninterface to de\ufb01ne this neural network because that makes it easy to unpack theoperations at the individual layers.\n>>> from keras import metrics\n>>> from keras.models import Model\n>>> from keras.layers.core import Dense, Activation, Flatten\n>>> from keras.layers import Input\n>>> from keras.layers.convolutional import Conv2D", "384": "4.13 Deep Learning 373\nFig. 4.83 The output of the recti\ufb01ed linear unit activation function with the input shown in Fig. 4.82\n>>> from keras.layers.pooling import MaxPooling2D\n>>> from keras.optimizers import SGD\n>>> from keras import backend asK\n>>> from keras.utils import to_categorical\nNote that the names of the modules are consistent with their operations. We also\nneed to tell Keras how to manage the input images,\n>>>K.set_image_data_format( 'channels_first' )# image data format\n>>>inputs =Input(shape =(1,6,6))# input data shape\nNow we can build the individual convolutional layers. Note the speci\ufb01cation of the\nactivations at each layer and placement of the inputs .\n>>>clayer =Conv2D( 2,(3,3),padding ='same' ,\n... input_shape =(1,6,6),name ='conv' ,\n... use_bias =False ,\n... trainable =False )(inputs)", "385": "374 4 Machine Learning\nFig. 4.84 The output of maximum pooling operation with the input shown in Fig. 4.83 for \ufb01xed\nimage kernel kern1\n>>>relu_layer =Activation( 'relu' )(clayer)\n>>>maxpooling =MaxPooling2D(pool_size =(2,2),\n... name='maxpool' )(relu_layer)\n>>>flatten =Flatten()(maxpooling)\n>>>softmax_layer =Dense( 2,\n... activation ='softmax' ,\n... name='softmax' )(flatten)\n>>>model =Model(inputs =inputs, outputs =softmax_layer)\n>>># inject fixed kernels into convolutional layer\n>>>fixed_kernels =[np.dstack([kern1,kern2]) .reshape( 3,3,1,2)]\n>>>model .layers[ 1].set_weights(fixed_kernels)\nObserve that the functional interface means that each layer is explicitly a function of\nthe previous one. Note that trainable=False for the convolutional layer because\nwe want to inject our \ufb01xed kernels into it at the end. The flatten layer reshapes the\ndata so that the entire processed image at the point is fed into the softmax_layer ,\nwhose output is proportional to the probability that the image belongs to either\nclass. The set_weights() function is where we inject our \ufb01xed kernels. These", "386": "4.13 Deep Learning 375\nare not going to be updated by the optimization algorithm because of the prior\ntrainable=False option. With the topology of the neural network de\ufb01ned, we\nnow have to choose the optimization algorithm and pack all of this con\ufb01gurationinto the model with the compile step.\n>>>lr= 0.01 # learning rate\n>>>sgd =SGD(lr =lr, decay =1e-6 , momentum =0.9, nesterov =True)\n>>>model .compile(loss ='categorical_crossentropy' ,\n... optimizer =sgd,\n... metrics =['accuracy' ,\n... metrics .categorical_crossentropy])\nThemetrics speci\ufb01cation means that we want to training process to keep track of\nthose named items. Next, we generate some training data using our gen_rand_slash\nfunction with the associated class of each image ( 1or0). Most of this code is just\nshaping the tensors for Keras. The \ufb01nal model.fit() step is where the internal\nweights of the neural network are adjusted according to the given inputs.\n>>># generate some training data\n>>>ntrain =len(training)\n>>>t=np.dstack([training[i][ 0].T\n... for iinrange (ntrain)]) .T.reshape(ntrain, 1,6,6)\n>>>y_binary =to_categorical(np .hstack([np .ones(ntrain //2),\n... np.zeros(ntrain //2)]))\n>>># fit the configured model\n>>>h=model .fit(t,y_binary,epochs =500,verbose =0)\nWith that completed, we can investigate the functional mapping of each layer\nwithK.function . The following creates a mapping between the input layer and the\nconvolutional layer.\n>>>convFunction =K.function([inputs],[clayer])\nNow, we can feed the training data into this function as see the output of just the\nconvolutional layer, which is shown.\nWe can do this again for the pooling layer by creating another Keras function,\n>>>maxPoolingFunction =K.function([inputs],[maxpooling])\nwhose output is shown in Fig. 4.86. We can examine the \ufb01nal output of this network\nusing the predict function (Fig. 4.85),\n>>>fixed_kernels =model .predict(t)\n>>>fixed_kernels\narray([[0.0960771 , 0.9039229 ],\n[0.12564187, 0.8743582 ],\n[0.14237107, 0.857629 ],[0.4294672 , 0.57053274],\n[0.13607137, 0.8639286 ],", "387": "376 4 Machine Learning\nFig. 4.85 Compare this to Fig. 4.82. This shows our hand-tooled convolution is the same as that\nimplemented by Keras\n[0.7519819 , 0.24801806],\n[0.16871268, 0.83128726],\n[0.0960771 , 0.9039229 ],\n[0.4294672 , 0.57053274],[0.3497647 , 0.65023535],\n[0.8890644 , 0.11093564],\n[0.7882034 , 0.21179655],[0.6911642 , 0.30883583],\n[0.7882034 , 0.21179655],\n[0.5335865 , 0.46641356],[0.6458056 , 0.35419443],\n[0.8880452 , 0.11195483],\n[0.7702401 , 0.22975995],[0.7702401 , 0.2297599 ],\n[0.6458056 , 0.35419443]], dtype=float32)\nand we can see the weights given to each of the classes. Taking the maximum of\nthese across the columns gives the following:", "388": "4.13 Deep Learning 377\nFig. 4.86 Output of max-pooling layer for \ufb01xed kernel kern1 . Compare this to Fig. 4.84.T h i s\nshows our hand-tooled implementation is equivalent to that by Keras\n>>>np.argmax(fixed_kernels,axis =1)\narray([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\nwhich means that our convolutional neural network with the \ufb01xed kernels did well\npredicting the classes of each of our input images. Recall that our model con\ufb01gurationprevented our \ufb01xed kernels from updating in the training process. Thus, the main\nwork of model training was changing the weights of the \ufb01nal output layer. We can\nre-do this exercise by removing this constraint and see how the network performs ifit is able to adaptively re-weight the kernel terms as part of training by changing the\ntrainable keyword argument and then re-build and train the model, as shown next.\n>>>clayer =Conv2D( 2,(3,3),padding ='same' ,\n... input_shape =(1,6,6),name ='conv' ,\n... use_bias =False )(inputs)\n>>>relu_layer =Activation( 'relu' )(clayer)\n>>>maxpooling =MaxPooling2D(pool_size =(2,2),\n... name='maxpool' )(relu_layer)\n>>>flatten =Flatten()(maxpooling)", "389": "378 4 Machine Learning\n>>>softmax_layer =Dense( 2,\n... activation ='softmax' ,\n... name='softmax' )(flatten)\n>>>model =Model(inputs =inputs, outputs =softmax_layer)\n>>>model .compile(loss ='categorical_crossentropy' ,\n... optimizer =sgd)\n>>>h=model .fit(t,y_binary,epochs =500,verbose =0)\n>>>new_kernels =model .predict(t)\n>>>new_kernels\narray([[1.4370615e-03, 9.9856299e-01],\n[3.6707439e-03, 9.9632925e-01],\n[1.0132928e-04, 9.9989867e-01],[4.6108435e-03, 9.9538910e-01],\n[2.5441888e-05, 9.9997461e-01],\n[7.4225911e-03, 9.9257737e-01],[1.3943247e-03, 9.9860567e-01],\n[1.4370615e-03, 9.9856299e-01],\n[4.6108435e-03, 9.9538910e-01],\n[3.4720991e-03, 9.9652785e-01],\n[9.9974054e-01, 2.5950689e-04],[9.9987161e-01, 1.2833292e-04],\n[9.9983239e-01, 1.6753815e-04],\n[9.9987161e-01, 1.2833292e-04],[9.8536682e-01, 1.4633193e-02],\n[9.9561429e-01, 4.3856688e-03],\n[9.9778903e-01, 2.2109088e-03],[9.9855381e-01, 1.4462060e-03],\n[9.9855381e-01, 1.4462066e-03],\n[9.9561429e-01, 4.3856665e-03]], dtype=float32)\nwith corresponding max output,\n>>>np.argmax(new_kernels,axis =1)\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\nThe newly updated kernels are shown in Fig. 4.87. Note how different these are from\nthe original \ufb01xed kernels. We can see the change in the respective predictions inFig.4.88. Thus, the bene\ufb01t of updating the kernels in the training process is to improve\nthe accuracy overall, but at the cost of interpretability of the kernels themselves.\nNonetheless, it is seldom the case that the kernels are known ahead of time, as in\nour arti\ufb01cial example here, so in practice, there may be nothing to really interpret\nanyway. Nonetheless, for other problems where there is a target feature in the datafor which good a priori exemplars exist that could serve a kernels, then priming these\nkernels early in training may help to tune into those target features, especially if they\nare rare in the training data.", "390": "References 379\nFig. 4.87 Kernels updated\nduring the training process.Compare to Fig. 4.81\nFig. 4.88 Recall that the sec-\nond half of the training setwas classi\ufb01ed as category 1.\nThe updated kernels provide a\nwider margin for classi\ufb01cation\nthan our \ufb01xed kernels, eventhough the ultimate perfor-mance is very similar betweenthem\nReferences\n1. L. Wasserman, All of Statistics: A Concise Course in Statistical Inference (Springer, Berlin,\n2004)\n2. V . Vapnik, The Nature of Statistical Learning Theory . Information Science and Statistics\n(Springer, Berlin, 2000)\n3. R.E. Schapire, Y . Freund, Boosting Foundations and Algorithms . Adaptive Computation and\nMachine Learning (MIT Press, Cambridge, 2012)\n4. C. Bauckhage, Numpy/Scipy recipes for data science: Kernel least squares optimization (1)\n(2015). researchgate.net\n5. W. Richert, Building Machine Learning Systems with Python (Packt Publishing Ltd., Birming-\nham, 2013)\n6. E. Alpaydin, Introduction to Machine Learning (Wiley Press, New York, 2014)\n7. H. Cuesta, Practical Data Analysis (Packt Publishing Ltd., Birmingham, 2013)\n8. A.J. Izenman, Modern Multivariate Statistical Techniques , vol. 1 (Springer, Berlin, 2008)\n9. A. Hyv\u00e4rinen, J. Karhunen, E. Oja, Independent Component Analysis , vol. 46 (Wiley, New York,\n2004)", "391": "Notation\nSymbol Meaning\n\u03c3 standard deviation\n\u03bc mean\nV variance\nE expectation\nf(x) function of x\nx\u2192 y mapping from xtoy\n(a,b) open interval\n[a,b] closed interval\n(a,b] half-open interval\n\u0394 differential of\n\u03a0 product operator\n\u03a3 summation of\n|x| absolute value of x\n/bardblx/bardbl norm of x\n#A number of elements in A\nA\u2229B intersection of sets A, B\nA\u222aB union of sets A, B\nA\u00d7B cartesian product of sets A, B\n\u2208 element of\n\u2227 logical conjunction\n\u00ac logical negation\n{} set delimiters\nP(X|Y) probability of Xgiven Y\n\u2200 for all\n\u2203 there exists\nA\u2286BA is a subset of B\nA\u2282BA is a proper subset of B\nfX(x) probability density function of random variable X\nFX(x) cumulative density function of random variable X\n\u00a9 Springer Nature Switzerland AG 2019\nJ. Unpingco, Python for Probability, Statistics, and Machine Learning ,\nhttps://doi.org/10.1007/978-3-030-18545-9381", "392": "382 Notation\nSymbol Meaning\n\u223c distributed according to\n\u221d proportional to\n/defines equal by de\ufb01nition\n:= equal by de\ufb01nition\n\u22a5 perpendicular to\n\u2234 therefore\n\u21d2 implies\n\u2261 equivalent to\nX matrix X\nx vector x\nsgn(x) sign of x\nR real line\nRnn-dimensional vector space\nRm\u00d7nm\u00d7n-dimensional matrix space\nU(a,b) uniform distribution on the interval (a,b)\nN(\u03bc,\u03c32) normal distribution with mean \u03bcand variance \u03c32\nas\u2192 converges almost surely\nd\u2192 converges in distribution\nP\u2192 converges in probability\nTr sum of the diagonal of a matrixdiag matrix diagonal", "393": "Index\nA\nAdaBoost, 332\nAlmost sure convergence, 126\nB\nBackpropagation, 339\nBacktracking algorithm, 354\nBagging, 329\nBeta distribution, 91\nBias/variance trade-off, 260\nBoosting, 331\nBoosting trees, 277,281\nC\nCauchy\u2013Schwarz inequality, 57,64\nCentral limit theorem, 133\nChebyshev inequality, 116\nChi-squared distribution, 86\nCluster distortion, 326\nComplexity penalty, 254\nConda package manager, 2\nConditional expectation projection, 58\nCon\ufb01dence intervals, 141,168\nCon\ufb01dence sets, 169\nConfusion matrix, 254\nConvergence in distribution, 131\nConvergence in probability, 129\nConvolution, 364\nConvolutional Neural Network using Keras,\n372\nCross-validation, 256\nCtypes, 32\nCython, 33D\nDelta method, 145\nDirichlet distribution, 93\nE\nExact line search, 353\nExplained variance ratio, 318\nExponential family, 295\nExponentially Weighted Moving Average,\n357\nF\nFalse-discovery rate, 163\nFastICA, 322\nFeature engineering, 256\nFisher Exact test, 164\nFunctional Deep Learning, 342\nG\nGamma distribution, 90\nGeneralized likelihood ratio test, 157\nGeneralized linear models, 295\nGradient Boosting, 280,284\nH\nHoeffding inequality, 118\nI\nIdempotent property, 58\nIndependent Component Analysis, 321\nInformation entropy, 95,96,273\nInner product, 58\n\u00a9 Springer Nature Switzerland AG 2019\nJ. Unpingco, Python for Probability, Statistics, and Machine Learning ,\nhttps://doi.org/10.1007/978-3-030-18545-9383", "394": "384 Index\nInverse CDF method, 105,107\nIpcluster, 36\nJ\nJupyter notebook, 22\nK\nKeras, 336\nKernel trick, 315\nKullback\u2013Leibler divergence, 99\nL\nLagrange multipliers, 302\nLasso regression, 309\nLesbesgue integration, 40\nM\nMann\u2013Whitney\u2013Wilcoxon test, 222\nMarkov inequality, 115\nMaximal margin algorithm, 311\nMaximum A-Posteriori Estimation, 183\nMaximum pooling, 368\nMeasurable function, 41\nMeasure, 41\nMini-batch gradient descent, 356\nMinimax risk, 135\nMMSE, 58\nMoment generating functions, 101\nMomentum, 357\nMonte Carlo sampling methods, 104\nMultilayer perceptron, 337\nMultilinear regression, 239\nMultinomial distribution, 84\nMultiprocessing, 34\nN\nNewton\u2019s method, 352\nNeyman\u2013Pearson test, 155\nNormal distribution, 83\nO\nOut-of-sample data, 245\nP\nPandas, 25\ndataframe, 27\nseries, 25Perceptron, 330,334\nPermutation test, 160\nPlug-in principle, 140\nPoisson distribution, 89\nPolynomial regression, 239\nProjection operator, 57\nP-values, 154\nPypy, 34\nR\nRandom forests, 275\nReceiver Operating Characteristic, 152\nRecti\ufb01ed linear activation, 369\nRegression, 304\nRejection method, 108\nRunsnakerun, 34\nS\nSAGE, 30\nScipy, 24\nSeaborn, 126\nShatter coef\ufb01cient, 249\nSilhouette coef\ufb01cient, 327\nStochastic gradient descent, 355\nStrong law of large numbers, 132\nSWIG, 33\nSympy, 30\nlambdify, 137\nstatistics module, 125\nT\nTensor\ufb02ow, 343\noptimizers, 346\nsession, 344\nTheano, 361\nTower property of expectation, 60\nU\nUniqueness theorem, 102\nUniversal functions, 4\nV\nV apnik\u2013Chervonenkis dimension, 249\nW\nWald test, 161\nWeak law of large numbers, 132"}}