{"Name": "Database_System_Concepts_by_Abraham_Silberschatz.pdf", "Pages": {"0": "Database System ConceptsSEVENTH EDITION\nAbraham Silberschatz \nHenry F. Korth \nS. Sudarshan\n", "1": "DATABASE\nSYSTEM CONCEPTS\nSIXTH EDITION\nAbraham Silberschatz\nYale University\nHenry F. Korth\nLehigh University\nS. Sudarshan\nIndian Institute of Technology, Bombay\nTMsilberschatz6e_fm_i-ii.indd Page i  12/3/09  2:51:50 PM user /Users/user/Desktop/Temp Work/00November_2009/24:11:09/VYN/silberschatz\nSEVENTH EDITION\n", "2": "DATABASE SYSTEM CONCEPTS, SEVENTH EDITION \nPublished by McGraw-Hill Education, 2 Penn Plaza, New York, NY 10121. Copyright \u00a9 2020 by \nMcGraw-Hill Education. All rights reserved. Printed in the United States of America. Previous \neditions \u00a9 2011, 2006, and 2002. No part of this publication may be reproduced or distributed in \nany form or by any means, or stored in a database or retrieval system, without the prior written \nconsent of McGraw-Hill Education, including, but not limited to, in any network or other \nelectronic storage or transmission, or broadcast for distance learning.\nSome ancillaries, including electronic and print components, may not be available to customers \noutside the United States.\nThis book is printed on acid-free paper. \n1 2 3 4 5 6 7 8 9 LCR 21 20 19 \nISBN 978-0-07-802215-9 (bound edition)\nMHID 0-07-802215-0 (bound edition)\nISBN 978-1-260-51504-6 (loose-leaf edition)\nMHID 1-260-51504-4 (loose-leaf edition)\nPortfolio Manager: Thomas Scaife Ph.D.\nProduct Developers: Tina Bower & Megan Platt\nMarketing Manager: Shannon O\u2019Donnell\nContent Project Managers: Laura Bies & Sandra Schnee\nBuyer: Susan K. Culbertson\nDesign: Egzon Shaqiri\nContent Licensing Specialists: Shawntel Schmitt & Lorraine Buczek\nCover Image: \u00a9 Pavel Nesvadba/Shutterstock\nCompositor: Aptara\u00ae, Inc.\nAll credits appearing on page or at the end of the book are considered to be an extension  \nof the copyright page.\nLibrary of Congress Cataloging-in-Publication Data\nNames: Silberschatz, Abraham, author. | Korth, Henry F., author. | Sudarshan, S., author.\nTitle: Database system concepts/Abraham Silberschatz, Yale University, Henry F. Korth,  \n Lehigh University, S. Sudarshan, Indian Institute of Technology, Bombay.\nDescription: Seventh edition. | New York, NY: McGraw-Hill, [2020] | Includes bibliographical  \n references.\nIdentifiers: LCCN 2018060474 | ISBN 9780078022159 (alk. paper) | ISBN 0078022150 (alk. paper)\nSubjects:  LCSH: Database management.\nClassification: LCC QA76.9.D3 S5637 2020 | DDC 005.74\u2014dc23 LC record available at  \n https://lccn.loc.gov/2018060474\nThe Internet addresses listed in the text were accurate at the time of publication. The inclusion of \na website does not indicate an endorsement by the authors or McGraw-Hill Education, and \nMcGraw-Hill Education does not guarantee the accuracy of the information presented at these sites.\nmheducation.com/highered\n", "3": "To meine schatzi, Valerie\nher parents and my dear friends, Steve and Mary Anne\nand in memory of my parents, Joseph and Vera\nAvi Silberschatz\nTo my wife, Joan\nmy children, Abigail and Joseph\nmy mother, Frances\na n di nm e m o r yo fm yf a t h e r ,H e n r y\nHank Korth\nTo my wife, Sita\nmy children, Madhur and Advaith\nand my mother, Indira\nS. Sudarshan\n", "4": "", "5": "About the Authors\nAbraham (Avi) Silberschatz is the Sidney J. Weinberg Professor o f Computer Science\nat Yale University. Prior to coming to Yale in 2003, he was the vi ce president of the\nInformation Sciences Research Center at Bell Labs. He previously held a n endowed\nprofessorship at the University of Texas at Austin, where he tau ght until 1993. Silber-\nschatz is a fellow of the ACM , a fellow of the IEEE , and a member of the Connecticut\nAcademy of Science and Engineering. He received the 2002 IEEE Taylor L. Booth Ed-\nucation Award, the 1998 ACM Karl V. Karlstrom Outstanding Educator Award, and\nthe 1997 ACM SIGMOD Contribution Award. Silberschatz was awarded the Bell Lab-\noratories President\u2019s Award three times, in 1998, 1999 and 2004. H is writings have\nappeared in numerous journals, conferences, workshops, and book cha pters. He has\nobtained over 48 patents and over 24 grants. He is an author of the t extbook Operating\nSystem Concepts .\nHenry F. (Hank) Korth is a Professor of Computer Science and Engineerin g and co-\ndirector of the Computer Science and Business program at Lehigh Univer sity. Prior to\njoining Lehigh, he was director of Database Principles Research at B ell Labs, a vice\npresident of Panasonic Technologies, an associate professor at th e University of Texas\nat Austin, and a research sta\ufb00 member at IBM Research. Korth is a fellow of the ACM\nand of the IEEE and a winner of the 10-Year Award at the VLDB Conference. His numer-\nous research publications span a wide range of aspects of database systems, including\ntransaction management in parallel and distributed systems, real- time systems, query\nprocessing, and the in\ufb02uence on these areas from modern computing arc hitectures.\nMost recently, his research has addressed issues in the application of blockchains in\nenterprise databases.\nS. Sudarshan is currently the Subrao M. Nilekani Chair Professor at the Indian Insti-\ntute of Technology, Bombay. He received his Ph.D. at the University of Wisconsin in\n1992, and he was a member of the technical sta\ufb00 at Bell Labs before joi ning IITBom-\nbay. Sudarshan is a fellow of the ACM . His research spans several areas of database\nsystems, with a focus on query processing and query optimization. H is paper on key-\nword search in databases published in 2002 won the IEEE ICDE Most In\ufb02uential Paper\nAward in 2012, and his work on main-memory databases received the Bel l Laborato-\nries President\u2019s Award in 1999. His current research areas include test ing and grading\nof SQL queries, optimization of database applications by rewri ting of imperative code,\nand query optimization for parallel databases. He has published over 100 papers and\nobtained 15 patents.\n", "6": "", "7": "Contents\nChapter 1 Introduction\n1.1 Database-System Applications 1\n1.2 Purpose of Database Systems 5\n1.3 View of Data 8\n1.4 Database Languages 13\n1.5 Database Design 17\n1.6 Database Engine 181.7 Database and Application Architecture 21\n1.8 Database Users and Administrators 24\n1.9 History of Database Systems 25\n1.10 Summary 29\nExercises 31\nFurther Reading 33\nPART ONE\n RELATIONAL LANGUAGES\nChapter 2 Introduction to the Relational Model\n2.1 Structure of Relational Databases 37\n2.2 Database Schema 41\n2.3 Keys 43\n2.4 Schema Diagrams 46\n2.5 Relational Query Languages 472.6 The Relational Algebra 48\n2.7 Summary 58\nExercises 60\nFurther Reading 63\nChapter 3 Introduction to SQL\n3.1 Overview of the SQL Query Language 65\n3.2 SQL Data De\ufb01nition 66\n3.3 Basic Structure of SQL Queries 71\n3.4 Additional Basic Operations 79\n3.5 Set Operations 85\n3.6 Null Values 893.7 Aggregate Functions 91\n3.8 Nested Subqueries 98\n3.9 Modi\ufb01cation of the Database 108\n3.10 Summary 114\nExercises 115\nFurther Reading 124\nvii\n", "8": "viii Contents\nChapter 4 Intermediate SQL\n4.1 Join Expressions 125\n4.2 Views 137\n4.3 Transactions 143\n4.4 Integrity Constraints 145\n4.5 SQL Data Types and Schemas 1534.6 Index De\ufb01nition in SQL 164\n4.7 Authorization 165\n4.8 Summary 173\nExercises 176\nFurther Reading 180\nChapter 5 Advanced SQL\n5.1 Accessing SQL from a Programming\nLanguage 183\n5.2 Functions and Procedures 198\n5.3 Triggers 206\n5.4 Recursive Queries 2135.5 Advanced Aggregation Features 219\n5.6 Summary 231\nExercises 232\nFurther Reading 238\nPART TWO DATABASE DESIGN\nChapter 6 Database Design Using the E-R Model\n6.1 Overview of the Design Process 241\n6.2 The Entity-Relationship Model 244\n6.3 Complex Attributes 249\n6.4 Mapping Cardinalities 252\n6.5 Primary Key 256\n6.6 Removing Redundant Attributes in Entity\nSets 261\n6.7 Reducing E-R Diagrams to Relational\nSchemas 2646.8 Extended E-R Features 271\n6.9 Entity-Relationship Design Issues 279\n6.10 Alternative Notations for Modeling\nData 285\n6.11 Other Aspects of Database Design 291\n6.12 Summary 292\nExercises 294\nFurther Reading 300\nChapter 7 Relational Database Design\n7.1 Features of Good Relational Designs 303\n7.2 Decomposition Using Functional\nDependencies 308\n7.3 Normal Forms 313\n7.4 Functional-Dependency Theory 320\n7.5 Algorithms for Decomposition Using\nFunctional Dependencies 330\n7.6 Decomposition Using Multivalued\nDependencies 3367.7 More Normal Forms 341\n7.8 Atomic Domains and First Normal\nForm 342\n7.9 Database-Design Process 343\n7.10 Modeling Temporal Data 347\n7.11 Summary 351\nExercises 353\nFurther Reading 360\n", "9": "Contents ix\nPART THREE\n APPLICATION DESIGN AND\nDEVELOPMENT\nChapter 8 Complex Data Types\n8.1 Semi-structured Data 365\n8.2 Object Orientation 376\n8.3 Textual Data 382\n8.4 Spatial Data 3878.5 Summary 394\nExercises 397\nFurther Reading 401\nChapter 9 Application Development\n9.1 Application Programs and User\nInterfaces 403\n9.2 Web Fundamentals 405\n9.3 Servlets 411\n9.4 Alternative Server-Side Frameworks 416\n9.5 Client-Side Code and Web Services 421\n9.6 Application Architectures 4299.7 Application Performance 434\n9.8 Application Security 437\n9.9 Encryption and Its Applications 447\n9.10 Summary 453\nExercises 455\nFurther Reading 462\nPART FOUR\n BIG DATA ANALYTICS\nChapter 10 Big Data\n10.1 Motivation 467\n1 0 . 2 B i gD a t aS t o r a g eS y s t e m s 4 7 2\n10.3 The MapReduce Paradigm 483\n10.4 Beyond MapReduce: Algebraic\nOperations 49410.5 Streaming Data 500\n10.6 Graph Databases 508\n10.7 Summary 511\nExercises 513\nFurther Reading 516\nChapter 11 Data Analytics\n11.1 Overview of Analytics 519\n11.2 Data Warehousing 521\n11.3 Online Analytical Processing 527\n11.4 Data Mining 54011.5 Summary 550\nExercises 552\nFurther Reading 555\n", "10": "x Contents\nPART FIVE\n STORAGE MANAGEMENT AND\nINDEXING\nChapter 12 Physical Storage Systems\n12.1 Overview of Physical Storage Media 559\n12.2 Storage Interfaces 562\n12.3 Magnetic Disks 563\n12.4 Flash Memory 567\n12.5 RAID 57012.6 Disk-Block Access 577\n12.7 Summary 580\nExercises 582\nFurther Reading 584\nChapter 13 Data Storage Structures\n13.1 Database Storage Architecture 587\n13.2 File Organization 588\n13.3 Organization of Records in Files 595\n13.4 Data-Dictionary Storage 602\n13.5 Database Bu\ufb00er 604\n13.6 Column-Oriented Storage 61113.7 Storage Organization in Main-Memory\nDatabases 615\n13.8 Summary 617\nExercises 619\nFurther Reading 621\nChapter 14 Indexing\n14.1 Basic Concepts 623\n14.2 Ordered Indices 625\n14.3 B+-Tree Index Files 634\n14.4 B+-Tree Extensions 650\n14.5 Hash Indices 658\n14.6 Multiple-Key Access 661\n14.7 Creation of Indices 66414.8 Write-Optimized Index Structures 665\n14.9 Bitmap Indices 670\n14.10 Indexing of Spatial and Temporal Data 672\n14.11 Summary 677\nExercises 679\nFurther Reading 683\nPART SIX\n QUERY PROCESSING AND\nOPTIMIZATION\nChapter 15 Query Processing\n15.1 Overview 689\n15.2 Measures of Query Cost 692\n15.3 Selection Operation 695\n15.4 Sorting 701\n15.5 Join Operation 704\n15.6 Other Operations 71915.7 Evaluation of Expressions 724\n15.8 Query Processing in Memory 731\n15.9 Summary 734\nExercises 736\nFurther Reading 740\n", "11": "Contents xi\nChapter 16 Query Optimization\n16.1 Overview 743\n16.2 Transformation of Relational\nExpressions 747\n16.3 Estimating Statistics of Expression\nResults 757\n16.4 Choice of Evaluation Plans 76616.5 Materialized Views 778\n16.6 Advanced Topics in Query\nOptimization 783\n16.7 Summary 787\nExercises 789\nFurther Reading 794\nPART SEVEN\n TRANSACTION MANAGEMENT\nChapter 17 Transactions\n17.1 Transaction Concept 799\n1 7 . 2 AS i m p l eT r a n s a c t i o nM o d e l 8 0 1\n17.3 Storage Structure 804\n17.4 Transaction Atomicity and Durability 805\n17.5 Transaction Isolation 807\n17.6 Serializability 812\n17.7 Transaction Isolation and Atomicity 81917.8 Transaction Isolation Levels 821\n17.9 Implementation of Isolation Levels 823\n17.10 Transactions as SQL Statements 826\n17.11 Summary 828\nExercises 831\nFurther Reading 834\nChapter 18 Concurrency Control\n18.1 Lock-Based Protocols 835\n18.2 Deadlock Handling 849\n18.3 Multiple Granularity 853\n18.4 Insert Operations, Delete Operations, and\nPredicate Reads 857\n18.5 Timestamp-Based Protocols 861\n18.6 Validation-Based Protocols 866\n18.7 Multiversion Schemes 86918.8 Snapshot Isolation 872\n18.9 Weak Levels of Consistency in\nPractice 880\n18.10 Advanced Topics in Concurrency\nControl 883\n18.11 Summary 894\nExercises 899\nFurther Reading 904\nChapter 19 Recovery System\n19.1 Failure Classi\ufb01cation 907\n19.2 Storage 908\n19.3 Recovery and Atomicity 912\n19.4 Recovery Algorithm 922\n19.5 Bu\ufb00er Management 926\n19.6 Failure with Loss of Non-Volatile\nStorage 930\n19.7 High Availability Using Remote Backup\nSystems 93119.8 Early Lock Release and Logical Undo\nOperations 935\n19.9 ARIES 941\n19.10 Recovery in Main-Memory Databases 947\n19.11 Summary 948\nExercises 952\nFurther Reading 956\n", "12": "xii Contents\nPART EIGHT\n PARALLEL AND DISTRIBUTED\nDATABASES\nChapter 20 Database-System Architectures\n20.1 Overview 961\n20.2 Centralized Database Systems 962\n20.3 Server System Architectures 963\n20.4 Parallel Systems 970\n20.5 Distributed Systems 98620.6 Transaction Processing in Parallel and\nDistributed Systems 989\n20.7 Cloud-Based Services 990\n20.8 Summary 995\nExercises 998\nFurther Reading 1001\nChapter 21 Parallel and Distributed Storage\n21.1 Overview 1003\n21.2 Data Partitioning 1004\n21.3 Dealing with Skew in Partitioning 1007\n21.4 Replication 1013\n21.5 Parallel Indexing 101721.6 Distributed File Systems 1019\n21.7 Parallel Key-Value Stores 1023\n21.8 Summary 1032\nExercises 1033\nFurther Reading 1036\nChapter 22 Parallel and Distributed Query Processing\n22.1 Overview 1039\n22.2 Parallel Sort 1041\n22.3 Parallel Join 1043\n22.4 Other Operations 1048\n22.5 Parallel Evaluation of Query Plans 1052\n22.6 Query Processing on Shared-Memory\nArchitectures 106122.7 Query Optimization for Parallel\nExecution 1064\n22.8 Parallel Processing of Streaming Data 1070\n22.9 Distributed Query Processing 1076\n22.10 Summary 1086\nExercises 1089\nFurther Reading 1093\nChapter 23 Parallel and Distributed Transaction Processing\n23.1 Distributed Transactions 1098\n23.2 Commit Protocols 1100\n23.3 Concurrency Control in Distributed\nDatabases 1111\n23.4 Replication 1121\n23.5 Extended Concurrency Control\nProtocols 112923.6 Replication with Weak Degrees of\nConsistency 1133\n23.7 Coordinator Selection 1146\n23.8 Consensus in Distributed Systems 1150\n23.9 Summary 1162\nExercises 1165\nFurther Reading 1168\n", "13": "Contents xiii\nPART NINE ADVANCED TOPICS\nChapter 24 Advanced Indexing Techniques\n24.1 Bloom Filter 1175\n24.2 Log-Structured Merge Tree and\nVariants 1176\n24.3 Bitmap Indices 1182\n24.4 Indexing of Spatial Data 118624.5 Hash Indices 1190\n24.6 Summary 1203\nExercises 1205\nFurther Reading 1206\nChapter 25 Advanced Application Development\n25.1 Performance Tuning 1210\n25.2 Performance Benchmarks 1230\n25.3 Other Issues in Application\nDevelopment 1234\n25.4 Standardization 123725.5 Distributed Directory Systems 1240\n25.6 Summary 1243\nExercises 1245\nFurther Reading 1248\nChapter 26 Blockchain Databases\n26.1 Overview 1252\n26.2 Blockchain Properties 1254\n26.3 Achieving Blockchain Properties via\nCryptographic Hash Functions 1259\n26.4 Consensus 1263\n26.5 Data Management in a Blockchain 126726.6 Smart Contracts 1269\n26.7 Performance Enhancement 1274\n26.8 Emerging Applications 1276\n26.9 Summary 1279\nExercises 1280\nFurther Reading 1282\nPART TEN APPENDIX A\nAppendix A Detailed University Schema 1287\nIndex 1299\nPART ELEVEN ONLINE CHAPTERS\nChapter 27 Formal Relational Query Languages\nChapter 28 Advanced Relational Database Design\nChapter 29 Object-Based Databases\nChapter 30 XML\nChapter 31 Information Retrieval\nChapter 32 PostgreSQL\n", "14": "", "15": "Preface\nDatabase management has evolved from a specialized computer application to a cen-\ntral component of virtually all enterprises, and, as a result, knowledge about database\nsystems has become an essential part of an education in computer science. In this text,\nwe present the fundamental concepts of database management. These concepts include\naspects of database design, database languages, and database-system implementation.\nThis text is intended for a \ufb01rst course in databases at the junior or senior under-\ngraduate, or \ufb01rst-year graduate, level. In addition to basic material for a \ufb01rst course,\nthe text contains advanced material that can be used for course supplements, or as\nintroductory material for an advanced course.\nWe assume only a familiarity with basic data structures, computer organization,\nand a high-level programming language such as Java, C, C++, or Python. We present\nconcepts as intuitive descriptions, many of which are based on our running example of\na university. Important theoretical results are covered, but formal proofs are omitted.\nIn place of proofs, \ufb01gures and examples are used to suggest why a result is true. Formal\ndescriptions and proofs of theoretical results may be found in research papers and\nadvanced texts that are referenced in the bibliographical notes.\nThe fundamental concepts and algorithms covered in the book are often based\non those used in existing commercial or experimental database systems. Our aim is\nto present these concepts and algorithms in a general setting that is not tied to one\nparticular database system, though we do provide references to speci\ufb01c systems where\nappropriate.\nIn this, the seventh edition of Database System Concepts , we have retained the over-\nall style of the prior editions while evolving the content and organization to re\ufb02ect the\nchanges that are occurring in the way databases are designed, managed, and used. One\nsuch major change is the extensive use of \u201cBig Data\u201d systems. We have also taken into\naccount trends in the teaching of database concepts and made adaptations to facilitate\nthese trends where appropriate.\nxv\n", "16": "xvi Preface\nAmong the notable changes in this edition are:\n\u2022Extensive coverage of Big Data systems, from the user perspective (Chapter 10),\nas well as from an internal perspective (Chapter 20 through Chapter 23), with\nextensive additions and modi\ufb01cations compared to the sixth edition.\n\u2022A new chapter entitled \u201cBlockchain Databases\u201d (Chapter 26) that introduces\nblockchain technology and its growing role in enterprise applications. An im-\nportant focus in this chapter is the interaction between blockchain systems and\ndatabase systems.\n\u2022Updates to all chapters covering database internals (Chapter 12 through Chap-\nter 19) to re\ufb02ect current-generation technology, such as solid-state disks, main-\nmemory databases, multi-core systems, and column-stores.\n\u2022Enhanced coverage of semi-structured data management using JSON ,RDF,a n d\nSPARQL (Section 8.1).\n\u2022Updated coverage of temporal data (in Section 7.10), data analytics (Chapter 11),\nand advanced indexing techniques such as write-optimized indices (Section 14.8\nand Section 24.2).\n\u2022Reorganization and update of chapters to better support courses with a signi\ufb01cant\nhands-on component (which we strongly recommend for any database course),\nincluding use of current-generation application development tools and Big Data\nsystems such as Apache Hadoop and Spark.\nThese and other updates have arisen from the many comments and suggestions we\nhave received from readers of the sixth editi on, our students at Yale University, Lehigh\nUniversity, and IITBombay, and our own observations and analyses of developments\nin database technology.\nContent of This Book\nThe text is organized in eleven major parts.\n\u2022Overview (Chapter 1). Chapter 1 provides a general overview of the nature and pur-\npose of database systems. We explain how the concept of a database system has\ndeveloped, what the common features of database systems are, what a database\nsystem does for the user, and how a database system interfaces with operating\nsystems. We also introduce an example database application: a university organi-\nzation consisting of multiple departments, instructors, students, and courses. This\napplication is used as a running example throughout the book. This chapter is\nmotivational, historical, and explanatory in nature.\n", "17": "Preface xvii\n\u2022Part 1: Relational Model and SQL (Chapter 2 through Chapter 5). Chapter 2 in-\ntroduces the relational model of data, covering basic concepts such as the struc-\nture of relational databases, database schemas, keys, schema diagrams, relational\nquery languages, relational operations, and the relational algebra. Chapter 3, Chap-\nter 4, and Chapter 5 focus on the most in\ufb02uential of the user-oriented relational\nlanguages: SQL. The chapters in this part describe data manipulation: queries,\nupdates, insertions, and deletions, assuming a schema design has been provided.\nAlthough data-de\ufb01nition syntax is covered in detail here, schema design issues are\ndeferred to Part 2.\n\u2022Part 2: Database Design (Chapter 6 and Chapter 7). Chapter 6 provides an\noverview of the database-design process and a detailed description of the entity-\nrelationship data model. The entity-relationship data model provides a high-level\nview of the issues in database design and of the problems encountered in capturing\nthe semantics of realistic applications within the constraints of a data model. UML\nclass-diagram notation is also covered in this chapter. Chapter 7 introduces rela-\ntional database design. The theory of functional dependencies and normalization\nis covered, with emphasis on the motivation and intuitive understanding of each\nnormal form. This chapter begins with an overview of relational design and relies\non an intuitive understanding of logical implication of functional dependencies.\nThis allows the concept of normalization to be introduced prior to full coverage of\nfunctional-dependency theory, which is presented later in the chapter. Instructors\nmay choose to use only this initial coverage without loss of continuity. Instructors\ncovering the entire chapter will bene\ufb01t from students having a good understand-\ning of normalization concepts to motivate them to learn some of the challenging\nconcepts of functional-dependency theory. The chapter ends with a section on\nmodeling of temporal data.\n\u2022Part 3: Application Design and Development (Chapter 8 and Chapter 9). Chapter\n8 discusses several complex data types that are particularly important for appli-\ncation design and development, including semi-structured data, object-based data,\ntextual data, and spatial data. Although the popularity of XML in a database con-\ntext has been diminishing, we retain an introduction to XML , while adding coverage\nofJSON ,RDF,a n d SPARQL . Chapter 9 discusses tools and technologies that are\nused to build interactive web-based and m obile database applications. This chap-\nter includes detailed coverage on both the server side and the client side. Among\nthe topics covered are servlets, JSP, Django, JavaScript, and web services. Also\ndiscussed are application architecture, object-relational mapping systems includ-\ning Hibernate and Django, performance (including caching using memcached and\nRedis), and the unique challenges in ensuring web-application security.\n\u2022Part 4: Big Data Analytics (Chapter 10 and Chapter 11). Chapter 10 provides\nan overview of large-scale data-analytic applications, with a focus on how those\napplications place distinct demands on data management compared with the de-\n", "18": "xviii Preface\nmands of traditional database applications. The chapter then discusses how those\ndemands are addressed. Among the topics covered are Big Data storage systems\nincluding distributed \ufb01le systems, key-value stores and No SQL systems, MapRe-\nduce, Apache Spark, streaming data, and graph databases. The connection of these\nsystems and concepts with database concepts introduced earlier is emphasized.\nChapter 11 discusses the structure and use of systems designed for large-scale data\nanalysis. After \ufb01rst explaining the concepts of data analytics, business intelligence,\nand decision support, the chapter discusses the structure of a data warehouse and\nthe process of gathering data into a warehouse. The chapter next covers usage of\nwarehouse data in OLAP applications followed by a survey of data-mining algo-\nrithms and techniques.\n\u2022Part 5: Storage Management and Indexing (Chapter 12 through Chapter 14). Chap-\nter 12 deals with storage devices and how the properties of those devices in\ufb02u-\nence database physical organization and performance. Chapter 13 deals with data-\nstorage structures, including \ufb01le organization and bu\ufb00er management. A variety of\ndata-access techniques are presented in Chapter 14. Multilevel index-based access\nis described, culminating in detailed coverage of B+-trees. The chapter then covers\nindex structures for applications where the B+-tree structure is less appropriate, in-\ncluding write-optimized indices such as LSM trees and bu\ufb00er trees, bitmap indices,\nand the indexing of spatial data using k-d trees, quadtrees and R-trees.\n\u2022Part 6: Query Processing and Optimization (Chapter 15 and Chapter 16). Chap-\nter 15 and Chapter 16 address query-evaluation algorithms and query optimiza-\ntion. Chapter 15 focuses on algorithms for the implementation of database opera-\ntions, particularly the wide range of join algorithms, which are designed to work on\nvery large data that may not \ufb01t in main-memory. Query processing techniques for\nmain-memory databases are also covered in this chapter. Chapter 16 covers query\noptimization, starting by showing how query plans can be transformed to other\nequivalent plans by using transformation rules. The chapter then describes how\nto generate estimates of query execution costs, and how to e\ufb03ciently \ufb01nd query\nexecution plans with the lowest cost.\n\u2022Part 7: Transaction Management (Chapter 17 through Chapter 19). Chapter 17\nfocuses on the fundamentals of a transaction-processing system: atomicity, con-\nsistency, isolation, and durability. It provides an overview of the methods used\nto ensure these properties, including log-based recovery and concurrency control\nusing locking, timestamp-based techniques, and snapshot isolation. Courses re-\nquiring only a survey of the transaction concept can use Chapter 17 on its own\nwithout the other chapters in this part; those chapters provide signi\ufb01cantly greater\ndepth. Chapter 18 focuses on concurrency control and presents several techniques\nfor ensuring serializability, including locking, timestamping, and optimistic (vali-\ndation) techniques. Multiversion concurrency control techniques, including the\nwidely used snapshot isolation technique, and an extension of the technique that\n", "19": "Preface xix\nguarantees serializability, are also covered. This chapter also includes discussion\nof weak levels of consistency, concurrency on index structures, concurrency in\nmain-memory database systems, long-duration transactions, operation-level con-\ncurrency, and real-time transaction processing. Chapter 19 covers the primary\ntechniques for ensuring correct transaction execution despite system crashes and\nstorage failures. These techniques include logs, checkpoints, and database dumps,\nas well as high availability using remote backup systems. Recovery with early lock\nrelease, and the widely used ARIES algorithm are also presented. This chapter in-\ncludes discussion of recovery in main-memory database systems and the use of\nNVRAM .\n\u2022Part 8: Parallel and Distributed Databases (Chapter 20 through Chapter 23).\nChapter 20 covers computer-system architecture, and describes the in\ufb02uence of\nthe underlying computer system on the database system. We discuss centralized\nsystems, client\u2013server systems, parallel and distributed architectures, and cloud-\nbased systems in this chapter. The remaining three chapters in this part address\ndistinct aspects of parallel and distributed databases, with Chapter 21 covering\nstorage and indexing, Chapter 22 covering query processing, and Chapter 23 cov-\nering transaction management. Chapter 21 includes discussion of partitioning and\ndata skew, replication, parallel indexing, distributed \ufb01le systems (including the\nHadoop \ufb01le system), and parallel key-value stores. Chapter 22 includes discussion\nof parallelism both among multiple queries and within a single query. It covers par-\nallel and distributed sort and join, MapReduce, pipelining, the Volcano exchange-\noperator model, thread-level parallelism, streaming data, and the optimization of\ngeographically distributed queries. Chapt er 23 includes discussion of traditional\ndistributed consensus such as two-phase commit and more sophisticated solutions\nincluding Paxos and Raft. It covers a variety of algorithms for distributed concur-\nrency control, including replica management and weaker degrees of consistency.\nThe trade-o\ufb00s implied by the CAP theorem are discussed along with the means of\ndetecting inconsistency using version vectors and Merkle trees.\n\u2022Part 9: Advanced Topics (Chapter 24 through Chapter 26). Chapter 24 expands\nupon the coverage of indexing in Chapter 14 with detailed coverage of the LSM\ntree and its variants, bitmap indices, spatial indexing, and dynamic hashing tech-\nniques. Chapter 25 expands upon the coverage of Chapter 9 with a discussion of\nperformance tuning, benchmarking, testing, and migration from legacy systems,\nstandardization, and distributed directory systems. Chapter 26 looks at blockchain\ntechnology from a database perspective. It describes blockchain data structures\nand the use of cryptographic hash functions and public-key encryption to ensure\nthe blockchain properties of anonymity, irrefutability, and tamper resistance. It\ndescribes and compares the distributed consensus algorithms used to ensure de-\ncentralization, including proof-of-work, proof-of-stake, and Byzantine consensus.\nMuch of the chapter focuses on the features that make blockchain an important\ndatabase concept, including the role of permisssioned blockchains, the encoding\n", "20": "xx Preface\nof business logic and agreements in smart contracts, and interoperability across\nblockchains. Techniques for achieving database-scale transaction-processing per-\nformance are discussed. A \ufb01nal section surveys current and contemplated enter-\nprise blockchain applications.\n\u2022Part 10: Appendix . Appendix A presents details of our university schema, including\nthe full schema, DDL , and all the tables.\n\u2022Part 11: Online Chapters (Chapter 27 through Chapter 32) available online at\ndb-book.com . We provide six chapters that cover material that is of historical\nnature or is advanced; these chapters are available only online. Chapter 27 cov-\ners \u201cpure\u201d query languages: the tuple and domain relational calculus and Data-\nlog, which has a syntax modeled after the Prolog language. Chapter 28 covers\nadvanced topics in relational database design, including the theory of multivalued\ndependencies and fourth normal form, as well as higher normal forms. Chapter\n29 covers object-based databases and more complex data types such as array, and\nmultiset types, as well as tables that are not in 1NF. Chapter 30 expands on the cov-\nerage in Chapter 8 of XML . Chapter 31 covers information retrieval, which deals\nwith querying of unstructured textual data. Chapter 32 provides an overview of the\nPostgre SQL database system, and is targeted at courses focusing on database inter-\nnals. The chapter is likely to be particularly useful for supporting student projects\nthat work with the open-source code base of the Postgre SQL database.\nAt the end of each chapter we provide references in a section titled Further Reading .\nThis section is intentionally abbreviated and provides references that allow students\nto continue their study of the material covered in the chapter or to learn about new\ndevelopments in the area covered by the chapter. On occasion, the further reading\nsection includes original source papers that have become classics of which everyone\nshould be aware. Detailed bibliographical notes for each chapter are available online,\nand provide references for readers who wish to go into further depth on any of the\ntopics covered in the chapter.\nThe Seventh Edition\nThe production of this seventh edition has been guided by the many comments and\nsuggestions we received concerning the earlier editions, by our own observations while\nteaching at Yale University, Lehigh University, and IITBombay, and by our analysis of\nthe directions in which database technology is evolving.\nWe provided a list of the major new features of this edition earlier in this preface;\nthese include coverage of extensive coverage of Big Data, updates to all chapters to\nre\ufb02ect current generation hardware technology, semi-structured data management, ad-\nvanced indexing techniques, and a new chapter on blockchain databases. Beyond these\nmajor changes, we revised the material in each chapter, bringing the older material\n", "21": "Preface xxi\nup-to-date, adding discussions on recent developments in database technology, and im-\nproving descriptions of topics that students found di\ufb03cult to understand. We have also\nadded new exercises and updated references.\nFor instructors who previously used the si xth edition, we list the more signi\ufb01cant\nchanges below:\n\u2022Relational algebra has been moved into Chapter 2, to help students better under-\nstand relational operations that form the basis of query languages such as SQL.\nDeeper coverage of relational algebra also aids in understanding the algebraic op-\nerators needed for discussion later of query processing and optimization. The two\nvariants of the relational calculus are now in an online chapter, since we believe\nthey are now of value only to more theoretic ally oriented courses, and can be omit-\nted by most database courses.\n\u2022The SQL chapters now include more details of database-system speci\ufb01c SQL vari-\nations, to aid students carrying out practical assignments. Connections between\nSQL and the multiset relational algebra are also covered in more detail. Chapter\n4 now covers all the material concerning joins, whereas previously natural join\nwas in the preceding chapter. Coverage of sequences used to generate unique key\nvalues, and coverage of row-level security have also been added to this chapter.\nRecent extensions to the JDBC API that are particularly useful are now covered in\nChapter 5; coverage of OLAP has been moved from this chapter to Chapter 11.\n\u2022Chapter 6 has been modi\ufb01ed to cover E-Rdiagrams along with E-Rconcepts, in-\nstead of \ufb01rst covering the concepts and then introducing E-Rdiagrams as was done\nin earlier editions. We believe this will help students better comprehend the E-R\nmodel.\n\u2022Chapter 7 now has improved coverage of temporal data modeling, including\nSQL:2011 temporal database features.\n\u2022Chapter 8 is a new chapter that covers complex data types, including semi-\nstructured data, such as XML ,JSON ,RDF,a n d SPARQL , object-based data, textual\ndata, and spatial data. Object-based databases, XML , and information retrieval on\ntextual data were covered in detail in the sixth edition; these topics have been ab-\nbreviated and covered in Chapter 8, while the original chapters from the sixth\nedition have now been made available online.\n\u2022Chapter 9 has been signi\ufb01cantly updated to re\ufb02ect modern application devel-\nopment tools and techniques, including extended coverage of JavaScript and\nJavaScript libraries for building dynamic web interfaces, application development\nin Python using the Django framework, coverage of web services, and disconnec-\ntion operations using HTML 5. Object-relation mapping using Django has been\nadded, as also discussion of techniques for developing high-performance applica-\ntions that can handle large transaction loads.\n", "22": "xxii Preface\n\u2022Chapter 10 is a new chapter on Big Data, covering Big Data concepts and tools\nfrom a user perspective. Big Data storage systems, the MapReduce paradigm,\nApache Hadoop and Apache Spark, and streaming and graph databases are cov-\nered in this chapter. The goal is to enable readers to use Big Data systems, with\nonly a summary coverage of what happens behind the scenes. Big Data internals\nare covered in detail in later chapters.\n\u2022The chapter on storage and \ufb01le structure has been split into two chapters. Chap-\nter 12 which covers storage has been updated with new technology, including ex-\npanded coverage of \ufb02ash memory, column-oriented storage, and storage organiza-\ntion in main-memory databases. Chapter 13, which covers data storage structures\nhas been expanded, and now covers details such as free-space maps, partitioning,\nand most importantly column-oriented storage.\n\u2022Chapter 14 on indexing now covers write-optimized index structures including the\nLSM tree and its variants, and the bu\ufb00er tree, which are seeing increasing usage.\nSpatial indices are now covered brie\ufb02y in this chapter. More detailed coverage of\nLSM trees and spatial indices is provided in Chapter 24, which covers advanced\nindexing techniques. Bitmap indices are now covered in brief in Chapter 14, while\nmore detailed coverage has been moved to Chapter 24. Dynamic hashing tech-\nniques have been moved into Chapter 24, since they are of limited practical im-\nportance today.\n\u2022Chapter 15 on query processing has signi\ufb01cantly expanded coverage of pipelining\nin query processing, new material on query processing in main-memory, including\nquery compilation, as well as brief coverage of spatial joins. Chapter 16 on query\noptimization has more examples of equivalence rules for operators such as outer\njoins and aggregates, has updated material on statistics for cost estimation, an\nimproved presentation of the join-order optimization algorithm. Techniques for\ndecorrelating nested subqueries using semijoin and antijoin operations have also\nbeen added.\n\u2022Chapter 18 on concurrency control has new material on concurrency control in\nmain-memory. Chapter 19 on recovery now gives more importance to high avail-\nability using remote backup systems.\n\u2022Our coverage of parallel and distributed databases has been completely revamped.\nBecause of the evolution of these two areas into a continuum from low-level paral-\nlelism to geographically distributed systems, we now present these topics together.\n\u00b0Chapter 20 on database architectures has been signi\ufb01cantly updated from the\nearlier edition, including new material on practical interconnection networks\nlike the tree-like (or fat-tree) architecture, and signi\ufb01cantly expanded and up-\ndated material on shared-memory architectures and cache coherency. There is\nan entirely new section on cloud-based services, covering virtual machines and\ncontainers, platform-as-a-service, software-as-a-service, and elasticity.\n", "23": "Preface xxiii\n\u00b0Chapter 21 covers parallel and distributed storage; while a few parts of this\nchapter were present in the sixth edition, such as partitioning techniques, ev-\nerything else in this chapter is new.\n\u00b0Chapter 22 covers parallel and distributed query processing. Again only a few\nsections of this chapter, such as parallel algorithms for sorting, join, and a few\nother relational operations, were present in the sixth edition, almost everything\nelse in this chapter is new.\n\u00b0Chapter 23 covers parallel and distributed transaction processing. A few parts\nof this chapter, such as the sections on 2PC, persistent messaging, and concur-\nrency control in distributed databases, are new but almost everything else in\nthis chapter is new.\nAs in the sixth edition, we facilitate the following of our running example by listing\nthe database schema and the sample relation instances for our university database to-\ngether in Appendix A as well as where they are used in the various regular chapters. In\naddition, we provide, on our web site db-book.com ,SQL data-de\ufb01nition statements for\nthe entire example, along with SQL statements to create our example relation instances.\nThis encourages students to run example queries directly on a database system and to\nexperiment with modifying those queries. All topics not listed above are updated from\nthe sixth edition, though their overall organization is relatively unchanged.\nEnd of Chapter Material\nEach chapter has a list of review terms, in addition to a summary, which can help\nreaders review key topics covered in the chapter.\nAs in the sixth edition, the exercises are divided into two sets: practice exercises\nandexercises . The solutions for the practice exercises are publicly available on the web\nsite of the book. Students are encouraged to solve the practice exercises on their own\nand later use the solutions on the web site to check their own solutions. Solutions to\nthe other exercises are available only to instructors (see \u201cInstructor\u2019s Note,\u201d below, for\ninformation on how to get the solutions).\nMany chapters have a tools section at the end of the chapter that provides infor-\nmation on software tools related to the topic of the chapter; some of these tools can\nbe used for laboratory exercises. SQL DDL and sample data for the university database\nand other relations used in the exercises are available on the web site of the book and\ncan be used for laboratory exercises.\nInstructor\u2019s Note\nIt is possible to design courses by using various subsets of the chapters. Some of the\nchapters can also be covered in an order di\ufb00erent from their order in the book. We\noutline some of the possibilities here:\n", "24": "xxiv Preface\n\u2022Chapter 5 (Advanced SQL). This chapter can be skipped or deferred to later with-\nout loss of continuity. We expect most courses will cover at least Section 5.1.1 early,\nasJDBC is likely to be a useful tool in student projects.\n\u2022Chapter 6 ( E-RModel). This chapter can be covered ahead of Chapter 3, Chapter\n4, and Chapter 5 if you so desire, since Chapter 6 does not have any dependency\nonSQL. However, for courses with a programming emphasis, a richer variety of\nlaboratory exercises is possible after studying SQL, and we recommend that SQL\nbe covered before database design for such courses.\n\u2022Chapter 15 (Query Processing) and Chapter 16 (Query Optimization). These\nchapters can be omitted from an introductory course without a\ufb00ecting coverage\nof any other chapter.\n\u2022Part 7 (Transaction Management). Our coverage consists of an overview (Chapter\n17) followed by chapters with details. You might choose to use Chapter 17 while\nomitting Chapter 18 and Chapter 19, if you defer these latter chapters to an ad-\nvanced course.\n\u2022Part 8 (Parallel and Distributed Databases). Our coverage consists of an overview\n(Chapter 20), followed by chapters on the topics of storage, query processing,\nand transactions. You might choose to use Chapter 20 while omitting Chapter 21\nthrough Chapter 23 if you defer these latter chapters to an advanced course.\n\u2022Part 11 (Online chapters). Chapter 27 (Formal-Relational Query Languages). This\nchapter can be covered immediately after Chapter 2, ahead of SQL. Alternatively,\nthis chapter may be omitted from an introductory course. The \ufb01ve other online\nchapters (Advanced Relational Database Design, Object-Based Databases, XML ,\nInformation Retrieval, and Postgre SQL) can be used as self-study material or omit-\nted from an introductory course.\nModel course syllabi, based on the text, can be found on the web site of the book.\nWeb Site and Teaching Supplements\nA web site for the book is available at the URL:db-book.com . The web site contains:\n\u2022Slides covering all the chapters of the book.\n\u2022Answers to the practice exercises.\n\u2022The six online chapters.\n\u2022Laboratory material, including SQL DDL and sample data for the university\nschema and other relations used in exercises, and instructions for setting up and\nusing various database systems and tools.\n\u2022An up-to-date errata list.\n", "25": "Preface xxv\nThe following additional material is available only to facult y:\n\u2022An instructor\u2019s manual containing solutions to all exercises in t he book.\n\u2022A question bank containing extra exercises.\nFor more information about how to get a copy of the instructor\u2019s manual and the\nquestion bank, please send an email message to sem@mheducation.com. I n the\nUnited States, you may call 800-338-3987. The McGraw-Hill web sit e for this book\niswww.mhhe.com/silberschatz .\nContacting Us\nWe have endeavored to eliminate typos, bugs, and the like from the text. But, as in new\nreleases of software, bugs almost surely remain; an up-to-date erra ta list is accessible\nfrom the book\u2019s web site. We would appreciate it if you would notif y us of any errors\nor omissions in the book that are not on the current list of errata .\nWe would be glad to receive suggestions on improvements to the book. We also\nwelcome any contributions to the book web site that could be of use t o other read-\ners, such as programming exercises, project suggestions, online la bs and tutorials, and\nteaching tips.\nEmail should be addressed to db-book-authors@cs.yale.edu . Any other corre-\nspondence should be sent to Avi Silberschatz, Department of Comput er Science, Yale\nUniversity, 51 Prospect Street, P.O. Box 208285, New Haven, CT 065 20-8285 USA.\nAcknowledgments\nMany people have helped us with this seventh edition, as well as with the previous six\neditions from which it is derived, and we are indebted to all of them.\nSeventh Edition\n\u2022Ioannis Alagiannis and Renata Borovica-Gajic for writing Cha pter 32 on the\nPostgre SQL database, which is available online. The chapter is a complete re write\nof the Postgre SQL chapter in the 6th edition, which was authored by Anastasia\nAilamaki, Sailesh Krishnamurthy, Spiros Papadimitriou, Bia nca Schroeder, Karl\nSchnaitter, and Gavin Sherry.\n\u2022Judi Paige for her help in generating \ufb01gures, presentation slides, an d with handling\nthe copy-editing material.\n\u2022Mark Wogahn for making sure that the software to produce the bo ok, including\nLaTeX macros and fonts, worked properly.\n", "26": "xxvi Preface\n\u2022Sriram Srinivasan for discussions and feedback that have immensely bene\ufb01ted the\nchapters on parallel and distributed databases.\n\u2022N. L. Sarda for his insightful feedback on the sixth edition, and on some sections\nof the seventh edition.\n\u2022Bikash Chandra and Venkatesh Emani for their help with updates to the applica-\ntion development chapter, including creation of sample code.\n\u2022Students at IIT Bombay, particularly Ashish Mithole, for their feedback on draft\nversions of the chapters on parallel and distributed databases.\n\u2022Students at Yale, Lehigh, and IITBombay, for their comments on the sixth edition.\n\u2022Je\ufb00rey Anthony, partner and CTO, Synaptic; and Lehigh students Corey Ca-\nplan (now co-founder, Leavitt Innovations); Gregory Cheng; Timothy LaRowe;\nand Aaron Rotem for comments and suggestions that have bene\ufb01ted the new\nblockchain chapter.\nPrevious Editions\n\u2022Hakan Jakobsson (Oracle), for writing the chapter on the Oracle database sys-\nt e mi nt h es i x t he d i t i o n ;S r i r a mP a d m a n a b h a n( IBM), for writing the chapter de-\nscribing the IBM DB2 database system in the sixth edition; and Sameet Agarwal,\nJos\u00b4e A. Blakeley, Thierry D\u2019Hers, Gerald Hinson, Dirk Myers, Vaqar Pirzada, Bill\nRamos, Balaji Rathakrishnan, Michael Rys, Florian Waas, and Michael Zwilling\nfor writing the chapter describing the Microsoft SQL S erver database system in\nthe sixth edition; and in particular Jos \u00b4e Blakeley, who sadly is no longer amongst\nus, for coordinating and editing the chapter; and C \u00b4esar Galindo-Legaria, Goetz\nGraefe, Kalen Delaney, and Thomas Casey for their contributions to the previous\nedition of the Microsoft SQL S erver chapter. These chapters, however, are not part\nof the seventh edition.\n\u2022Anastasia Ailamaki, Sailesh Krishnamurthy, Spiros Papadimitriou, Bianca\nSchroeder, Karl Schnaitter, and Gavin Sherry for writing the chapter on\nPostgre SQL in the sixth edition.\n\u2022Daniel Abadi for reviewing the table of contents of the \ufb01fth edition and helping\nwith the new organization.\n\u2022Steve Dolins, University of Florida; Rolando Fernanez, George Washington Uni-\nversity; Frantisek Franek, McMaster University; Latifur Khan, University of Texas\nat Dallas; Sanjay Madria, Missouri University of Science and Technology; Aris\nOuksel, University of Illinois; and Richard Snodgrass, University of Waterloo; who\nserved as reviewers of the book and whose comments helped us greatly in formu-\nlating the sixth edition.\n", "27": "Preface xxvii\n\u2022Judi Paige for her help in generating \ufb01gures and presentation slides.\n\u2022Mark Wogahn for making sure that the software to produce the book, including\nLaTeX macros and fonts, worked properly.\n\u2022N. L. Sarda for feedback that helped us improve several chapters. Vikram Pudi for\nmotivating us to replace the earlier bank schema; and Shetal Shah for feedback on\nseveral chapters.\n\u2022Students at Yale, Lehigh, and IITBombay, for their comments on the \ufb01fth edition,\nas well as on preprints of the sixth edition.\n\u2022Chen Li and Sharad Mehrotra for providing material on JDBC and security for the\n\ufb01fth edition.\n\u2022Marilyn Turnamian and Nandprasad Joshi provided secretarial assistance for the\n\ufb01fth edition, and Marilyn also prepared an early draft of the cover design for the\n\ufb01fth edition.\n\u2022Lyn Dupr \u00b4e copyedited the third edition and Sara Strandtman edited the text of the\nthird edition.\n\u2022Nilesh Dalvi, Sumit Sanghai, Gaurav Bhalotia, Arvind Hulgeri K. V. Raghavan,\nPrateek Kapadia, Sara Strandtman, Greg Speegle, and Dawn Bezviner helped to\nprepare the instructor\u2019s manual for earlier editions.\n\u2022The idea of using ships as part of the cover concept was originally suggested to us\nby Bruce Stephan.\n\u2022The following people o\ufb00ered suggestions and comments for the \ufb01fth and earlier\neditions of the book. R. B. Abhyankar, Hani Abu-Salem, Jamel R. Alsabbagh,\nRaj Ashar, Don Batory, Phil Bernhard, Christian Breimann, Gavin M. Bierman,\nJanek Bogucki, Haran Boral, Paul Bourgeois, Phil Bohannon, Robert Brazile, Yuri\nBreitbart, Ramzi Bualuan, Michael Carey, Soumen Chakrabarti, Tom Chappell,\nZhengxin Chen, Y. C. Chin, Jan Chomicki, Laurens Damen, Prasanna Dhan-\ndapani, Qin Ding, Valentin Dinu, J. Edwards, Christos Faloutsos, Homma Far-\nian, Alan Fekete, Frantisek Franek, Shashi Gadia, Hector Garcia-Molina, Goetz\nGraefe, Jim Gray, Le Gruenwald, Eitan M. Gurari, William Hankley, Bruce\nHillyer, Ron Hitchens, Chad Hogg, Arvind Hulgeri, Yannis Ioannidis, Zheng Ji-\naping, Randy M. Kaplan, Graham J. L. Kemp, Rami Khouri, Hyoung-Joo Kim,\nWon Kim, Henry Korth (father of Henry F.), Carol Kroll, Hae Choon Lee, Sang-\nWon Lee, Irwin Levinstein, Mark Llewellyn, Gary Lindstrom, Ling Liu, Dave\nMaier, Keith Marzullo, Marty Maskarinec, Fletcher Mattox, Sharad Mehrotra, Jim\nMelton, Alberto Mendelzon, Ami Motro, Bhagirath Narahari, Yiu-Kai Dennis Ng,\nThanh-Duy Nguyen, Anil Nigam, Cyril Orji, Meral Ozsoyoglu, D. B. Phatak, Juan\nAltmayer Pizzorno, Bruce Porter, Sunil Prabhakar, Jim Peterson, K. V. Raghavan,\nNahid Rahman, Rajarshi Rakshit, Krithi Ramamritham, Mike Reiter, Greg Ric-\n", "28": "xxviii Preface\ncardi, Odinaldo Rodriguez, Mark Roth, Marek Rusinkiewicz, Michael Rys, Sunita\nSarawagi, N. L. Sarda, Patrick Schmid, Nikhil Sethi, S. Seshadri, Stewart Shen,\nShashi Shekhar, Amit Sheth, Max Smolens, Nandit Soparkar, Greg Speegle, Je\ufb00\nStorey, Dilys Thomas, Prem Thomas, Tim Wahls, Anita Whitehall, Christopher\nWilson, Marianne Winslett, Weining Zhang, and Liu Zhenming.\nPersonal Notes\nSudarshan would like to acknowledge his wife, Sita, for her love, patience, and support,\nand children Madhur and Advaith for their love and joie de vivre. Hank would like to\nacknowledge his wife, Joan, and his children, Abby and Joe, for their love and under-\nstanding. Avi would like to acknowledge Valerie for her love, patience, and support\nduring the revision of this book.\nA. S.\nH. F. K.\nS. S.\n", "29": "CHAPTER1\nIntroduction\nAdatabase-management system (DBMS ) is a collection of interrelated data and a set\nof programs to access those data. The collection of data, usually referred to as the\ndatabase , contains information relevant to an enterprise. The primary goal of a DBMS\nis to provide a way to store and retrieve database information that is both convenient\nande\ufb03cient .\nDatabase systems are designed to manage large bodies of information. Manage-\nment of data involves both de\ufb01ning structures for storage of information and provid-\ning mechanisms for the manipulation of information. In addition, the database system\nmust ensure the safety of the information stored, despite system crashes or attempts\nat unauthorized access. If data are to be shared among several users, the system must\navoid possible anomalous results.\nBecause information is so important in most organizations, computer scientists\nhave developed a large body of concepts and techniques for managing data. These\nconcepts and techniques form the focus of this book. This chapter brie\ufb02y introduces\nthe principles of database systems.\n1.1 Database-System Applications\nThe earliest database systems arose in the 1960s in response to the computerized man-\nagement of commercial data. Those earlier applications were relatively simple com-\npared to modern database applications. Modern applications include highly sophisti-\ncated, worldwide enterprises.\nAll database applications, old and new, share important common elements. The\ncentral aspect of the application is not a program performing some calculation, but\nrather the data themselves. Today, some of the most valuable corporations are valuable\nnot because of their physical assets, but rather because of the information they own.\nImagine a bank without its data on accounts and customers or a social-network site\nthat loses the connections among its users. Such companies\u2019 value would be almost\ntotally lost under such circumstances.\n1\n", "30": "2 Chapter 1 Introduction\nDatabase systems are used to manage collections of data that:\n\u2022are highly valuable,\n\u2022are relatively large, and\n\u2022are accessed by multiple users and applications, often at the same time.\nThe \ufb01rst database applications had only simple, precisely formatted, structured\ndata. Today, database applications may include data with complex relationships and a\nmore variable structure. As an example of an application with structured data, consider\na university\u2019s records regarding courses, stu dents, and course registration. The univer-\nsity keeps the same type of information about each course: course-identi\ufb01er, title, de-\npartment, course number, etc., and similarly for students: student-identi\ufb01er, name, ad-\ndress, phone, etc. Course registration is a collection of pairs: one course identi\ufb01er and\none student identi\ufb01er. Information of this sort has a standard, repeating structure and\nis representative of the type of database applications that go back to the 1960s. Con-\ntrast this simple university database application with a social-networking site. Users of\nthe site post varying types of information about themselves ranging from simple items\nsuch as name or date of birth, to complex posts consisting of text, images, videos, and\nlinks to other users. There is only a limited amount of common structure among these\ndata. Both of these applications, however, share the basic features of a database.\nModern database systems exploit commonalities in the structure of data to gain\ne\ufb03ciency but also allow for weakly structured data and for data whose formats are\nhighly variable. As a result, a database system is a large, complex software system whose\ntask is to manage a large, complex collection of data.\nManaging complexity is challenging, not only in the management of data but in\nany domain. Key to the management of complexity is the concept of abstraction .A b -\nstraction allows a person to use a complex device or system without having to know the\ndetails of how that device or system is constructed. A person is able, for example, to\ndrive a car by knowing how to operate its controls. However, the driver does not need\nto know how the motor was built nor how it operates. All the driver needs to know is an\nabstraction of what the motor does. Similarly, for a large, complex collection of data,\na database system provides a simpler, abstract view of the information so that users\nand application programmers do not need to be aware of the underlying details of how\ndata are stored and organized. By providing a high level of abstraction, a database sys-\ntem makes it possible for an enterprise to combine data of various types into a uni\ufb01ed\nrepository of the information needed to run the enterprise.\nHere are some representative applications:\n\u2022Enterprise Information\n\u00b0Sales: For customer, product, and purchase information.\n", "31": "1.1 Database-System Applications 3\n\u00b0Accounting: For payments, receipts, account balances, assets, and other ac-\ncounting information.\n\u00b0Human resources: For information about employees, salaries, payroll taxes, and\nbene\ufb01ts, and for generation of paychecks.\n\u2022Manufacturing: For management of the supply chain and for tracking production\nof items in factories, inventories of items in warehouses and stores, and orders for\nitems.\n\u2022Banking and Finance\n\u00b0Banking: For customer information, accounts, loans, and banking transactions.\n\u00b0Credit card transactions: For purchases on credit cards and generation of\nmonthly statements.\n\u00b0Finance: For storing information about holdings, sales, and purchases of \ufb01nan-\ncial instruments such as stocks and bonds; also for storing real-time market\ndata to enable online trading by customers and automated trading by the \ufb01rm.\n\u2022Universities: For student information, course registrations, and grades (in addition\nto standard enterprise information such as human resources and accounting).\n\u2022Airlines: For reservations and schedule information. Airlines were among the \ufb01rst\nto use databases in a geographically distributed manner.\n\u2022Telecommunication: For keeping records of calls, texts, and data usage, generating\nmonthly bills, maintaining balances on prepaid calling cards, and storing informa-\ntion about the communication networks.\n\u2022Web-based services\n\u00b0Social-media: For keeping records of users, connections between users (such as\nfriend/follows information), posts made by users, rating/like information about\nposts, etc.\n\u00b0Online retailers: For keeping records of sales data and orders as for any retailer,\nbut also for tracking a user\u2019s product views, search terms, etc., for the purpose\nof identifying the best items to recommend to that user.\n\u00b0Online advertisements: For keeping records of click history to enable targeted\nadvertisements, product suggestions, news articles, etc. People access such\ndatabases every time they do a web search, make an online purchase, or ac-\ncess a social-networking site.\n\u2022Document databases: For maintaining collections of new articles, patents, pub-\nlished research papers, etc.\n\u2022Navigation systems: For maintaining the locations of varies places of interest along\nwith the exact routes of roads, train systems, buses, etc.\n", "32": "4 Chapter 1 Introduction\nAs this list illustrates, databases form an essential part not only of every enterprise but\nalso of a large part of a person\u2019s daily activities.\nThe ways in which people interact with databases has changed over time. Early\ndatabases were maintained as back-o\ufb03ce systems with which users interacted via\nprinted reports and paper forms for input. As database systems became more sophisti-\ncated, better languages were developed for programmers to use in interacting with the\ndata, along with user interfaces that allowed end users within the enterprise to query\nand update data.\nAs the support for programmer interaction with databases improved, and computer\nhardware performance increased even as hardware costs decreased, more sophisticated\napplications emerged that brought database data into more direct contact not only with\nend users within an enterprise but also with the general public. Whereas once bank\ncustomers had to interact with a teller for ever y transaction, automated-teller machines\n(ATM s) allowed direct customer interaction. Today, virtually every enterprise employs\nweb applications or mobile applications to a llow its customers to interact directly with\nthe enterprise\u2019s database, and, thus, with the enterprise itself.\nThe user, or customer, can focus on the product or service without being aware\nof the details of the large database that makes the interaction possible. For instance,\nwhen you read a social-media post, or access an online bookstore and browse a book or\nmusic collection, you are accessing data stored in a database. When you enter an order\nonline, your order is stored in a database. When you access a bank web site and retrieve\nyour bank balance and transaction information, the information is retrieved from the\nbank\u2019s database system. When you access a web site, information about you may be\nretrieved from a database to select which advertisements you should see. Almost every\ninteraction with a smartphone results in some sort of database access. Furthermore,\ndata about your web accesses may be stored in a database.\nThus, although user interfaces hide details of access to a database, and most people\nare not even aware they are dealing with a database, accessing databases forms an\nessential part of almost everyone\u2019s life today.\nBroadly speaking, there are two modes in which databases are used.\n\u2022T h e\ufb01 r s tm o d ei st os u p p o r t online transaction processing , where a large number\nof users use the database, with each user retrieving relatively small amounts of\ndata, and performing small updates. This is the primary mode of use for the vast\nmajority of users of database applications such as those that we outlined earlier.\n\u2022The second mode is to support data analytics , that is, the processing of data to\ndraw conclusions, and infer rules or decision procedures, which are then used to\ndrive business decisions.\nFor example, banks need to decide whether to give a loan to a loan applicant,\nonline advertisers need to decide which advertisement to show to a particular user.\nThese tasks are addressed in two steps. First, data-analysis techniques attempt to\nautomatically discover rules and patterns from data and create predictive models .\nThese models take as input attributes (\u201cfeatures\u201d) of individuals, and output pre-\n", "33": "1.2 Purpose of Database Systems 5\ndictions such as likelihood of paying back a loan, or clicking on an advertisement,\nwhich are then used to make the business decision.\nAs another example, manufacturers and retailers need to make decisions on\nwhat items to manufacture or order in what quantities; these decisions are driven\nsigni\ufb01cantly by techniques for analyzing past data, and predicting trends. The cost\nof making wrong decisions can be very high, and organizations are therefore willing\nto invest a lot of money to gather or purchase required data, and build systems that\ncan use the data to make accurate predictions.\nThe \ufb01eld of data mining combines knowledge-discovery techniques invented by\narti\ufb01cial intelligence researchers and statistical analysts with e\ufb03cient implemen-\ntation techniques that enable them to be used on extremely large databases.\n1.2 Purpose of Database Systems\nTo understand the purpose of database systems, consider part of a university organiza-\ntion that, among other data, keeps information about all instructors, students, depart-\nments, and course o\ufb00erings. One way to keep the information on a computer is to store\nit in operating-system \ufb01les. To allow users to manipulate the information, the system\nhas a number of application programs that manipulate the \ufb01les, including programs to:\n\u2022Add new students, instructors, and courses.\n\u2022Register students for courses and generate class rosters.\n\u2022Assign grades to students, compute grade point averages ( GPA), and generate tran-\nscripts.\nProgrammers develop these application programs to meet the needs of the university.\nNew application programs are added to the system as the need arises. For exam-\nple, suppose that a university decides to create a new major. As a result, the university\ncreates a new department and creates new permanent \ufb01les (or adds information to\nexisting \ufb01les) to record information about all the instructors in the department, stu-\ndents in that major, course o\ufb00erings, degree requirements, and so on. The university\nmay have to write new application programs to deal with rules speci\ufb01c to the new ma-\njor. New application programs may also have to be written to handle new rules in the\nuniversity. Thus, as time goes by, the system acquires more \ufb01les and more application\nprograms.\nThis typical \ufb01le-processing system is supported by a conventional operating system.\nThe system stores permanent records in various \ufb01les, and it needs di\ufb00erent application\nprograms to extract records from, and add records to, the appropriate \ufb01les.\nKeeping organizational information in a \ufb01le-processing system has a number of\nmajor disadvantages:\n", "34": "6 Chapter 1 Introduction\n\u2022Data redundancy and inconsistency . Since di\ufb00erent programmers create the \ufb01les\nand application programs over a long period, the various \ufb01les are likely to have\ndi\ufb00erent structures, and the programs may be written in several programming lan-\nguages. Moreover, the same information may be duplicated in several places (\ufb01les).\nFor example, if a student has a double major (say, music and mathematics), the\naddress and telephone number of that student may appear in a \ufb01le that consists of\nstudent records of students in the Music department and in a \ufb01le that consists of\nstudent records of students in the Mathematics department. This redundancy leads\nto higher storage and access cost. In addition, it may lead to data inconsistency ;\nthat is, the various copies of the same data may no longer agree. For example, a\nchanged student address may be re\ufb02ected in the Music department records but\nnot elsewhere in the system.\n\u2022Di\ufb03culty in accessing data . Suppose that one of the university clerks needs to\n\ufb01nd out the names of all students who live within a particular postal-code area.\nThe clerk asks the data-processing department to generate such a list. Because\nthe designers of the original system did not anticipate this request, there is no\napplication program on hand to meet it. There is, however, an application program\nto generate the list of allstudents. The university clerk now has two choices: either\nobtain the list of all students and extract the needed information manually or ask\na programmer to write the necessary application program. Both alternatives are\nobviously unsatisfactory. Suppose that such a program is written and that, several\ndays later, the same clerk needs to trim that list to include only those students who\nhave taken at least 60 credit hours. As expected, a program to generate such a list\ndoes not exist. Again, the clerk has the preceding two options, neither of which is\nsatisfactory.\nThe point here is that conventional \ufb01le-processing environments do not allow\nneeded data to be retrieved in a convenient and e\ufb03cient manner. More responsive\ndata-retrieval systems are required for general use.\n\u2022Data isolation . Because data are scattered in various \ufb01les, and \ufb01les may be in dif-\nferent formats, writing new application programs to retrieve the appropriate data\nis di\ufb03cult.\n\u2022Integrity problems . The data values stored in the database must satisfy certain types\nofconsistency constraints . Suppose the university maintains an account for each\ndepartment, and records the balance amount in each account. Suppose also that\nthe university requires that the account balance of a department may never fall\nbelow zero. Developers enforce these constraints in the system by adding appro-\npriate code in the various application programs. However, when new constraints\nare added, it is di\ufb03cult to change the programs to enforce them. The problem is\ncompounded when constraints involve several data items from di\ufb00erent \ufb01les.\n\u2022Atomicity problems . A computer system, like any other device, is subject to failure.\nIn many applications, it is crucial that, if a failure occurs, the data be restored to the\n", "35": "1.2 Purpose of Database Systems 7\nconsistent state that existed prior to the failure. Consider a banking system with a\nprogram to transfer $500 from account Ato account B. If a system failure occurs\nduring the execution of the program, it is possible that the $500 was removed\nfrom the balance of account Abut was not credited to the balance of account\nB, resulting in an inconsistent database state. Clearly, it is essential to database\nconsistency that either both the credit an d debit occur, or that neither occur. That\nis, the funds transfer must be atomic \u2014 i tm u s th a p p e ni ni t se n t i r e t yo rn o ta ta l l .I t\nis di\ufb03cult to ensure atomicity in a conventional \ufb01le-processing system.\n\u2022Concurrent-access anomalies . For the sake of overall performance of the system\nand faster response, many systems allow multiple users to update the data simulta-\nneously. Indeed, today, the largest internet retailers may have millions of accesses\nper day to their data by shoppers. In such an environment, interaction of concur-\nrent updates is possible and may result in inconsistent data. Consider account A,\nwith a balance of $10,000. If two bank clerks debit the account balance (by say\n$500 and $100, respectively) of account Aat almost exactly the same time, the re-\nsult of the concurrent executions may leave the account balance in an incorrect (or\ninconsistent) state. Suppose that the programs executing on behalf of each with-\ndrawal read the old balance, reduce that value by the amount being withdrawn, and\nwrite the result back. If the two programs run concurrently, they may both read\nthe value $10,000, and write back $9500 an d $9900, respectively. Depending on\nwhich one writes the value last, the balance of account Amay contain either $9500\nor $9900, rather than the correct value of $9400. To guard against this possibility,\nthe system must maintain some form of supervision. But supervision is di\ufb03cult\nto provide because data may be accessed by many di\ufb00erent application programs\nthat have not been coordinated previously.\nAs another example, suppose a registration program maintains a count of\nstudents registered for a course in order to enforce limits on the number of students\nregistered. When a student registers, the program reads the current count for the\ncourses, veri\ufb01es that the count is not already at the limit, adds one to the count, and\nstores the count back in the database. Suppose two students register concurrently,\nwith the count at 39. The two program executions may both read the value 39, and\nboth would then write back 40, leading to an incorrect increase of only 1, even\nthough two students successfully registered for the course and the count should\nbe 41. Furthermore, suppose the course registration limit was 40; in the above\ncase both students would be able to register, leading to a violation of the limit of\n40 students.\n\u2022Security problems . Not every user of the database system should be able to access\nall the data. For example, in a university, payroll personnel need to see only that\npart of the database that has \ufb01nancial information. They do not need access to\ninformation about academic records. But since application programs are added to\nthe \ufb01le-processing system in an ad hoc manner, enforcing such security constraints\nis di\ufb03cult.\n", "36": "8 Chapter 1 Introduction\nThese di\ufb03culties, among others, prompted both the initial development of\ndatabase systems and the transition of \ufb01le-based applications to database systems, back\nin the 1960s and 1970s.\nIn what follows, we shall see the concepts and algorithms that enable database\nsystems to solve the problems with \ufb01le-processing systems. In most of this book, we use\na university organization as a running example of a typical data-processing application.\n1.3 View of Data\nA database system is a collection of interrelated data and a set of programs that allow\nusers to access and modify these data. A major purpose of a database system is to\nprovide users with an abstract view of the data. That is, the system hides certain details\nof how the data are stored and maintained.\n1.3.1 Data Models\nUnderlying the structure of a database is the data model : a collection of conceptual tools\nfor describing data, data relationships, data semantics, and consistency constraints.\nThere are a number of di\ufb00erent data models that we shall cover in the text. The\ndata models can be classi\ufb01ed into four di\ufb00erent categories:\n\u2022Relational Model . The relational model uses a collection of tables to represent both\ndata and the relationships among those data. Each table has multiple columns, and\neach column has a unique name. Tables are also known as relations .T h er e l a t i o n a l\nmodel is an example of a record-based model. Record-based models are so named\nbecause the database is structured in \ufb01xed-format records of several types. Each\ntable contains records of a particular type. Each record type de\ufb01nes a \ufb01xed number\nof \ufb01elds, or attributes. The columns of the table correspond to the attributes of the\nrecord type. The relational data model is the most widely used data model, and\na vast majority of current database systems are based on the relational model.\nChapter 2 and Chapter 7 cover the relational model in detail.\n\u2022Entity-Relationship Model . The entity-relationship ( E-R) data model uses a collec-\ntion of basic objects, called entities ,a n d relationships among these objects. An en-\ntity is a \u201cthing\u201d or \u201cobject\u201d in the real world that is distinguishable from other\nobjects. The entity-relationship model is widely used in database design. Chapter\n6 explores it in detail.\n\u2022Semi-structured Data Model . The semi-structured data model permits the speci\ufb01-\ncation of data where individual data items of the same type may have di\ufb00erent\nsets of attributes. This is in contrast to the data models mentioned earlier, where\nevery data item of a particular type must have the same set of attributes. JSON and\nExtensible Markup Language (XML) are widely used semi-structured data represen-\ntations. Semi-structured data models are explored in detail in Chapter 8.\n", "37": "1.3 View of Data 9\n\u2022Object-Based Data Model . Object-oriented programming (especially in Java, C++,\nor C#) has become the dominant software-development methodology. This led\ninitially to the development of a distinct object-oriented data model, but today the\nconcept of objects is well integrated into relational databases. Standards exist to\nstore objects in relational tables. Database systems allow procedures to be stored\nin the database system and executed by the database system. This can be seen as\nextending the relational model with notions of encapsulation, methods, and object\nidentity. Object-based data models are summarized in Chapter 8.\nA large portion of this text is focused on the relational model because it serves as\nthe foundation for most database applications.\n1.3.2 Relational Data Model\nIn the relational model, data are represented in the form of tables. Each table has mul-\ntiple columns, and each column has a unique name. Each row of the table represents\none piece of information. Figure 1.1 presents a sample relational database comprising\ntwo tables: one shows details of university instructors and the other shows details of\nthe various university departments.\nThe \ufb01rst table, the instructor table, shows, for example, that an instructor named\nEinstein with ID22222 is a member of the Physics department and has an annual\nsalary of $95,000. The second table, department , shows, for example, that the Biology\ndepartment is located in the Watson building and has a budget of $90,000. Of course,\na real-world university would have many more departments and instructors. We use\nsmall tables in the text to illustrate concepts. A larger example for the same schema is\navailable online.\n1.3.3 Data Abstraction\nFor the system to be usable, it must retrieve data e\ufb03ciently. The need for e\ufb03ciency has\nled database system developers to use complex data structures to represent data in the\ndatabase. Since many database-system users are not computer trained, developers hide\nthe complexity from users through several levels of data abstraction , to simplify users\u2019\ninteractions with the system:\n\u2022Physical level . The lowest level of abstraction describes howthe data are actually\nstored. The physical level describes complex low-level data structures in detail.\n\u2022Logical level . The next-higher level of abstraction describes what data are stored\nin the database, and what relationships exist among those data. The logical level\nthus describes the entire database in terms of a small number of relatively simple\nstructures. Although implementation of the simple structures at the logical level\nmay involve complex physical-level structures, the user of the logical level does not\nneed to be aware of this complexity. This is referred to as physical data indepen-\n", "38": "10 Chapter 1 Introduction\nID\n name\n dept\nname\n salary\n22222\n Einstein\n Physics\n 95000\n12121\n Wu\n Finance\n 90000\n32343\n El Said\n History\n 60000\n45565\n Katz\n Comp. Sci.\n 75000\n98345\n Kim\n Elec. Eng.\n 80000\n76766\n Crick\n Biology\n 72000\n10101\n Srinivasan\n Comp. Sci.\n 65000\n58583\n Cali\ufb01eri\n History\n 62000\n83821\n Brandt\n Comp. Sci.\n 92000\n15151\n Mozart\n Music\n 40000\n33456\n Gold\n Physics\n 87000\n76543\n Singh\n Finance\n 80000\n(a) The instructor table\ndept\nname\n building\n budget\nComp. Sci.\n Taylor\n 100000\nBiology\n Watson\n 90000\nElec. Eng.\n Taylor\n 85000\nMusic\n Packard\n 80000\nFinance\n Painter\n 120000\nHistory\n Painter\n 50000\nPhysics\n Watson\n 70000\n(b) The department table\nFigure 1.1 A sample relational database.\ndence . Database administrators, who must decide what information to keep in the\ndatabase, use the logical level of abstraction.\n\u2022View level . The highest level of abstraction describes only part of the entire\ndatabase. Even though the logical level uses simpler structures, complexity remains\nbecause of the variety of information stored in a large database. Many users of the\ndatabase system do not need all this information; instead, they need to access only\na part of the database. The view level of abstraction exists to simplify their interac-\nt i o nw i t ht h es y s t e m .T h es y s t e mm a yp r o v i d em a n yv i e w sf o rt h es a m ed a t a b a s e .\nFigure 1.2 shows the relationship among the three levels of abstraction.\nAn important feature of data models, such as the relational model, is that they\nhide such low-level implementation details from not just database users, but even from\n", "39": "1.3 View of Data 11\nview 1 view 2\nlogical\nlevel\nphysical\nlevelview n \u2026view level\nFigure 1.2 The three levels of data abstraction.\ndatabase-application developers. The datab ase system allows application developers\nto store and retrieve data using the abstractions of the data model, and converts the\nabstract operations into operations on the low-level implementation.\nAn analogy to the concept of data types in programming languages may clarify\nthe distinction among levels of abstraction. Many high-level programming languages\ns u p p o r tt h en o t i o no fas t r u c t u r e dt y p e .W em a yd e s c r i b et h et y p eo far e c o r da b s t r a c t l y\nas follows:1\ntypeinstructor =record\nID:char (5);\nname :char (20);\ndept\nname :char (20);\nsalary :numeric (8,2);\nend;\nThis code de\ufb01nes a new record type called instructor with four \ufb01elds. Each \ufb01eld has a\nname and a type associated with it. For example, char(20) speci\ufb01es a string with 20\ncharacters, while numeric (8,2) speci\ufb01es a number with 8 digits, two of which are to\nthe right of the decimal point. A university organization may have several such record\ntypes, including:\n\u2022department ,w i t h\ufb01 e l d s dept\nname ,building ,a n d budget .\n\u2022course ,w i t h\ufb01 e l d s course\n id,title,dept\nname ,a n d credits .\n\u2022student ,w i t h\ufb01 e l d s ID,name ,dept\nname ,a n d tot\ncred.\n1The actual type declaration depends on the language being used. C and C++ use struct declarations. Java does not\nhave such a declaration, but a simple class can be de\ufb01ned to the same e\ufb00ect.\n", "40": "12 Chapter 1 Introduction\nAt the physical level, an instructor ,department ,o rstudent record can be described\nas a block of consecutive bytes. The compiler hides this level of detail from program-\nmers. Similarly, the database system hides many of the lowest-level storage details from\ndatabase programmers. Database administrators, on the other hand, may be aware of\ncertain details of the physical organization of the data. For example, there are many\npossible ways to store tables in \ufb01les. One way is to store a table as a sequence of records\nin a \ufb01le, with a special character (such as a comma) used to delimit the di\ufb00erent at-\ntributes of a record, and another special character (such as a new-line character) may\nbe used to delimit records. If all attributes have \ufb01xed length, the lengths of attributes\nmay be stored separately, and delimiters may be omitted from the \ufb01le. Variable length\nattributes could be handled by storing the length, followed by the data. Databases use\na type of data structure called an index to support e\ufb03cient retrieval of records; these\ntoo form part of the physical level.\nAt the logical level, each such record is described by a type de\ufb01nition, as in the\nprevious code segment. The interrelationship of these record types is also de\ufb01ned at\nthe logical level; a requirement that the dept\n name value of an instructor record must\nappear in the department table is an example of such an interrelationship. Programmers\nusing a programming language work at this level of abstraction. Similarly, database\nadministrators usually work at this level of abstraction.\nFinally, at the view level, computer users see a set of application programs that hide\ndetails of the data types. At the view level, several views of the database are de\ufb01ned, and\na database user sees some or all of these views. In addition to hiding details of the logical\nlevel of the database, the views also provide a security mechanism to prevent users from\naccessing certain parts of the database. For example, clerks in the university registrar\no\ufb03ce can see only that part of the database that has information about students; they\ncannot access information about salaries of instructors.\n1.3.4 Instances and Schemas\nDatabases change over time as information is inserted and deleted. The collection of\ninformation stored in the database at a particular moment is called an instance of the\ndatabase. The overall design of the database is called the database schema . The con-\ncept of database schemas and instances can be understood by analogy to a program\nwritten in a programming language. A database schema corresponds to the variable\ndeclarations (along with associated type de\ufb01nitions) in a program. Each variable has\na particular value at a given instant. The values of the variables in a program at a point\nin time correspond to an instance of a database schema.\nDatabase systems have several schemas, partitioned according to the levels of ab-\nstraction. The physical schema describes the database design at the physical level, while\nthelogical schema describes the database design at the logical level. A database may\nalso have several schemas at the view level, sometimes called subschemas ,t h a td e s c r i b e\ndi\ufb00erent views of the database.\nOf these, the logical schema is by far the most important in terms of its e\ufb00ect on\napplication programs, since programmers construct applications by using the logical\n", "41": "1.4 Database Languages 13\nschema. The physical schema is hidden beneath the logical schema and can usually be\nchanged easily without a\ufb00ecting application programs. Application programs are said\nto exhibit physical data independence if they do not depend on the physical schema\nand thus need not be rewritten if the physical schema changes.\nWe also note that it is possible to create schemas that have problems, such as\nunnecessarily duplicated information. For example, suppose we store the department\nbudget as an attribute of the instructor record. Then, whenever the value of the budget\nfor a department (say the Physics department) changes, that change must be re\ufb02ected\nin the records of all instructors associated with the department. In Chapter 7, we shall\nstudy how to distinguish good schema designs from bad schema designs.\nTraditionally, logical schemas were changed infrequently, if at all. Many newer\ndatabase applications, however, require more \ufb02exible logical schemas where, for ex-\nample, di\ufb00erent records in a single relation may have di\ufb00erent attributes.\n1.4 Database Languages\nA database system provides a data-de\ufb01nition language (DDL) to specify the database\nschema and a data-manipulation language (DML ) to express database queries and up-\ndates. In practice, the data-de\ufb01nition and data-manipulation languages are not two sep-\narate languages; instead they simply form parts of a single database language, such as\ntheSQL language. Almost all relational database systems employ the SQL language,\nwhich we cover in great detail in Chapter 3, Chapter 4, and Chapter 5.\n1.4.1 Data-Definition Language\nWe specify a database schema by a set of de\ufb01nitions expressed by a special language\ncalled a data-de\ufb01nition language ( DDL ). The DDL is also used to specify additional\nproperties of the data.\nWe specify the storage structure and access methods used by the database system\nby a set of statements in a special type of DDL called a data storage and de\ufb01nition\nlanguage. These statements de\ufb01ne the implementation details of the database schemas,\nwhich are usually hidden from the users.\nThe data values stored in the database must satisfy certain consistency constraints.\nFor example, suppose the university requires that the account balance of a department\nmust never be negative. The DDL provides facilities to specify such constraints. The\ndatabase system checks these constraints every time the database is updated. In general,\na constraint can be an arbitrary predicate pertaining to the database. However, arbitrary\npredicates may be costly to test. Thus, database systems implement only those integrity\nconstraints that can be tested with minimal overhead:\n\u2022Domain Constraints . A domain of possible values must be associated with every\nattribute (for example, integer types, character types, date/time types). Declaring\nan attribute to be of a particular domain acts as a constraint on the values that it\n", "42": "14 Chapter 1 Introduction\ncan take. Domain constraints are the most elementary form of integrity constraint.\nThey are tested easily by the system whenever a new data item is entered into the\ndatabase.\n\u2022Referential Integrity . There are cases where we wish to ensure that a value that\nappears in one relation for a given set of attributes also appears in a certain set\nof attributes in another relation (referential integrity). For example, the depart-\nment listed for each course must be one that actually exists in the university. More\nprecisely, the dept\nname value in a course record must appear in the dept\nname\nattribute of some record of the department relation. Database modi\ufb01cations can\ncause violations of referential integrity. When a referential-integrity constraint is\nviolated, the normal procedure is to reject the action that caused the violation.\n\u2022Authorization . We may want to di\ufb00erentiate among the users as far as the type of\naccess they are permitted on various data values in the database. These di\ufb00erentia-\ntions are expressed in terms of authorization , the most common being: read autho-\nrization , which allows reading, but not modi\ufb01cation, of data; insert authorization ,\nwhich allows insertion of new data, but not modi\ufb01cation of existing data; update\nauthorization , which allows modi\ufb01cation, but not deletion, of data; and delete au-\nthorization , which allows deletion of data. We may assign the user all, none, or a\ncombination of these types of authorization.\nThe processing of DDL statements, just like those of any other programming lan-\nguage, generates some output. The output of the DDL is placed in the data dictionary ,\nwhich contains metadata \u2014that is, data about data. The data dictionary is considered\nto be a special type of table that can be accessed and updated only by the database sys-\ntem itself (not a regular user). The database system consults the data dictionary before\nreading or modifying actual data.\n1.4.2 The SQLData-Definition Language\nSQL provides a rich DDL that allows one to de\ufb01ne tables with data types and integrity\nconstraints.\nFor instance, the following SQL DDL statement de\ufb01nes the department table:\ncreate table department\n(dept\nname char (20),\nbuilding char (15),\nbudget numeric (12,2));\nExecution of the preceding DDL statement creates the department table with three\ncolumns: dept\nname ,building ,a n d budget , each of which has a speci\ufb01c data type asso-\nciated with it. We discuss data types in more detail in Chapter 3.\nThe SQL DDL also supports a number of types of integrity constraints. For exam-\nple, one can specify that the dept\nname attribute value is a primary key , ensuring that no\n", "43": "1.4 Database Languages 15\ntwo departments can have the same department name. As another example, one can\nspecify that the dept\nname attribute value appearing in any instructor record must also\nappear in the dept\nname attribute of some record of the department table. We discuss\nSQL support for integrity constraints and authorizations in Chapter 3 and Chapter 4.\n1.4.3 Data-Manipulation Language\nAdata-manipulation language (DML ) is a language that enables users to access or ma-\nnipulate data as organized by the appropriate data model. The types of access are:\n\u2022Retrieval of information stored in the database.\n\u2022Insertion of new information into the database.\n\u2022Deletion of information from the database.\n\u2022Modi\ufb01cation of information stored in the database.\nThere are basically two types of data-manipulation language:\n\u2022Procedural DML srequire a user to specify what data are needed and howto get\nthose data.\n\u2022Declarative DML s(also referred to as nonprocedural DML s)r e q u i r eau s e rt os p e c -\nifywhat data are needed without specifying how to get those data.\nDeclarative DML s are usually easier to learn and use than are procedural DML s.\nHowever, since a user does not have to specify how to get the data, the database system\nhas to \ufb01gure out an e\ufb03cient means of accessing data.\nAquery is a statement requesting the retrieval of information. The portion of a\nDML that involves information retrieval is called a query language . Although technically\nincorrect, it is common practice to use the terms query language anddata-manipulation\nlanguage synonymously.\nThere are a number of database query languages in use, either commercially or\ne x p e r i m e n t a l l y .W es t u d yt h em o s tw i d e l yu s e dq u e r yl a n g u a g e , SQL,i nC h a p t e r3\nthrough Chapter 5.\nThe levels of abstraction that we discu ssed in Section 1.3 apply not only to de\ufb01ning\nor structuring data, but also to manipulating data. At the physical level, we must de\ufb01ne\nalgorithms that allow e\ufb03cient access to data. At higher levels of abstraction, we em-\nphasize ease of use. The goal is to allow humans to interact e\ufb03ciently with the system.\nThe query processor component of the database system (which we study in Chapter\n15 and Chapter 16) translates DML queries into sequences of actions at the physical\nlevel of the database system. In Chapter 22, we study the processing of queries in the\nincreasingly common parallel and distributed settings.\n", "44": "16 Chapter 1 Introduction\n1.4.4 The SQLData-Manipulation Language\nThe SQL query language is nonprocedural. A query takes as input several tables (pos-\nsibly only one) and always returns a single table. Here is an example of an SQL query\nthat \ufb01nds the names of all instructors in the History department:\nselect instructor .name\nfrom instructor\nwhere instructor .dept\nname='History';\nThe query speci\ufb01es that those rows from the table instructor where the dept\nname is\nHistory must be retrieved, and the name attribute of these rows must be displayed. The\nresult of executing this query is a table with a single column labeled name and a set of\nrows, each of which contains the name of an instructor whose dept\nname is History. If\nthe query is run on the table in Figure 1.1, the result consists of two rows, one with the\nname El Said and the other with the name Cali\ufb01eri.\nQueries may involve information from more than one table. For instance, the fol-\nlowing query \ufb01nds the instructor IDand department name of all instructors associated\nwith a department with a budget of more than $95,000.\nselect instructor .ID,department .dept\nname\nfrom instructor ,department\nwhere instructor .dept\nname=department .dept\nname and\ndepartment .budget >95000;\nIf the preceding query were run on the tables in Figure 1.1, the system would \ufb01nd that\nthere are two departments with a budget of greater than $95,000\u2014Computer Science\nand Finance; there are \ufb01ve instructors in these departments. Thus, the result consists of\nat a b l ew i t ht w oc o l u m n s( ID,dept\nname ) and \ufb01ve rows: (12121, Finance), (45565, Com-\nputer Science), (10101, Computer Science ), (83821, Computer Science), and (76543,\nFinance).\n1.4.5 Database Access from Application Programs\nNon-procedural query languages such as SQL are not as powerful as a universal Turing\nmachine; that is, there are some computation s that are possible using a general-purpose\nprogramming language but are not possible using SQL.SQL also does not support ac-\ntions such as input from users, output to displays, or communication over the network.\nSuch computations and actions must be written in a host language, such as C/C++,\nJava, or Python, with embedded SQL queries that access the data in the database.\nApplication programs are programs that are used to interact with the database in this\nfashion. Examples in a university system are programs that allow students to register\nfor courses, generate class rosters, calculate student GPA, generate payroll checks, and\nperform other tasks.\n", "45": "1.5 Database Design 17\nTo access the database, DML statements need to be sent from the host to the\ndatabase where they will be executed. This is most commonly done by using an\napplication-program interface (set of procedures) that can be used to send DML and\nDDL statements to the database and retrieve the results. The Open Database Con-\nnectivity ( ODBC ) standard de\ufb01nes application program interfaces for use with C and\nseveral other languages. The Java Database Connectivity ( JDBC ) standard de\ufb01nes a\ncorresponding interface for the Java language.\n1.5 Database Design\nDatabase systems are designed to manage large bodies of information. These large\nbodies of information do not exist in isolation. They are part of the operation of some\nenterprise whose end product may be information from the database or may be some\ndevice or service for which the database plays only a supporting role.\nDatabase design mainly involves the design of the database schema. The design of\na complete database application environment that meets the needs of the enterprise\nbeing modeled requires attention to a broader set of issues. In this text, we focus on the\nwriting of database queries and the design of database schemas, but discuss application\ndesign later, in Chapter 9.\nA high-level data model provides the database designer with a conceptual frame-\nwork in which to specify the data requirements of the database users and how the\ndatabase will be structured to ful\ufb01ll these requirements. The initial phase of database\ndesign, then, is to characterize fully the data needs of the prospective database users.\nThe database designer needs to interact extensively with domain experts and users to\ncarry out this task. The outcome of this phase is a speci\ufb01cation of user requirements.\nNext, the designer chooses a data model, and by applying the concepts of the cho-\nsen data model, translates these requirements into a conceptual schema of the database.\nThe schema developed at this conceptual-design phase provides a detailed overview of\nthe enterprise. The designer reviews the schema to con\ufb01rm that all data requirements\nare indeed satis\ufb01ed and are not in con\ufb02ict with one another. The designer can also\nexamine the design to remove any redundant features. The focus at this point is on\ndescribing the data and their relationships, rather than on specifying physical storage\ndetails.\nIn terms of the relational model, the conceptual-design process involves decisions\nonwhat attributes we want to capture in the database and how to group these attributes\nto form the various tables. The \u201cwhat\u201d part is basically a business decision, and we\nshall not discuss it further in this text. The \u201chow\u201d part is mainly a computer-science\nproblem. There are principally two ways to tackle the problem. The \ufb01rst one is to use\nthe entity-relationship model (Chapter 6); the other is to employ a set of algorithms\n(collectively known as normalization that takes as input the set of all attributes and\ngenerates a set of tables (Chapter 7).\nA fully developed conceptual schema indicates the functional requirements of the\nenterprise. In a speci\ufb01cation of functional requirements , users describe the kinds of oper-\n", "46": "18 Chapter 1 Introduction\nations (or transactions) that will be performed on the data. Example operations include\nmodifying or updating data, searching for and retrieving speci\ufb01c data, and deleting\ndata. At this stage of conceptual design, the designer can review the schema to ensure\nit meets functional requirements.\nThe process of moving from an abstract data model to the implementation of\nthe database proceeds in two \ufb01nal design phases. In the logical-design phase ,t h ed e -\nsigner maps the high-level conceptual schema onto the implementation data model of\nthe database system that will be used. The designer uses the resulting system-speci\ufb01c\ndatabase schema in the subsequent physical-design phase ,i nw h i c ht h ep h y s i c a lf e a t u r e s\nof the database are speci\ufb01ed. These features include the form of \ufb01le organization and\nthe internal storage structures; they are discussed in Chapter 13.\n1.6 Database Engine\nA database system is partitioned into modules that deal with each of the responsibilities\nof the overall system. The functional components of a database system can be broadly\ndivided into the storage manager, the query processor components, and the transaction\nmanagement component.\nThe storage manager is important because databases typically require a large\namount of storage space. Corporate databases commonly range in size from hundreds\nof gigabytes to terabytes of data. A gigabyte is approximately 1 billion bytes, or 1000\nmegabytes (more precisely, 1024 megabytes), while a terabyte is approximately 1 tril-\nlion bytes or 1 million megabytes (more precisely, 1024 gigabytes). The largest enter-\nprises have databases that reach into the multi-petabyte range (a petabyte is 1024 ter-\nabytes). Since the main memory of computers cannot store this much information, and\nsince the contents of main memory are lost in a system crash, the information is stored\non disks. Data are moved between disk storage and main memory as needed. Since the\nmovement of data to and from disk is slow relative to the speed of the central process-\ning unit, it is imperative that the database system structure the data so as to minimize\nthe need to move data between disk and main memory. Increasingly, solid-state disks\n(SSDs ) are being used for database storage. SSDs are faster than traditional disks but\nalso more costly.\nThe query processor is important because it helps the database system to simplify\nand facilitate access to data. The query processor allows database users to obtain good\nperformance while being able to work at the view level and not be burdened with un-\nderstanding the physical-level details of the implementation of the system. It is the job\nof the database system to translate updates and queries written in a nonprocedural\nlanguage, at the logical level, into an e\ufb03cient sequence of operations at the physical\nlevel.\nThe transaction manager is important because it allows application developers to\ntreat a sequence of database accesses as if they were a single unit that either happens in\nits entirety or not at all. This permits application developers to think at a higher level of\n", "47": "1.6 Database Engine 19\nabstraction about the application without needing to be concerned with the lower-level\ndetails of managing the e\ufb00ects of concurrent access to the data and of system failures.\nWhile database engines were traditionally centralized computer systems, today\nparallel processing is key for handling very large amounts of data e\ufb03ciently. Modern\ndatabase engines pay a lot of attention to parallel data storage and parallel query pro-\ncessing.\n1.6.1 Storage Manager\nThestorage manager is the component of a database system that provides the interface\nbetween the low-level data stored in the database and the application programs and\nqueries submitted to the system. The storage m anager is responsible for the interaction\nwith the \ufb01le manager. The raw data are stored on the disk using the \ufb01le system provided\nby the operating system. The storage manager translates the various DML statements\ninto low-level \ufb01le-system commands. Thus, the storage manager is responsible for stor-\ning, retrieving, and updating data in the database.\nThe storage manager components include:\n\u2022Authorization and integrity manager , which tests for the satisfaction of integrity\nconstraints and checks the authority of users to access data.\n\u2022Transaction manager , which ensures that the database remains in a consistent (cor-\nrect) state despite system failures, and that concurrent transaction executions pro-\nceed without con\ufb02icts.\n\u2022File manager , which manages the allocation of space on disk storage and the data\nstructures used to represent information stored on disk.\n\u2022Bu\ufb00er manager , which is responsible for fetching data from disk storage into main\nmemory, and deciding what data to cache in main memory. The bu\ufb00er manager is\na critical part of the database system, since it enables the database to handle data\nsizes that are much larger than the size of main memory.\nThe storage manager implements several data structures as part of the physical\nsystem implementation:\n\u2022Data \ufb01les , which store the database itself.\n\u2022Data dictionary , which stores metadata about the structure of the database, in\nparticular the schema of the database.\n\u2022Indices , which can provide fast access to data items. Like the index in this textbook,\na database index provides pointers to those data items that hold a particular value.\nFor example, we could use an index to \ufb01nd the instructor record with a particular\nID,o ra l l instructor records with a particular name .\n", "48": "20 Chapter 1 Introduction\nWe discuss storage media, \ufb01le structures, and bu\ufb00er management in Chapter 12 and\nChapter 13. Methods of accessing data e\ufb03ciently are discussed in Chapter 14.\n1.6.2 The Query Processor\nThe query processor components include:\n\u2022DDL interpreter ,w h i c hi n t e r p r e t s DDL statements and records the de\ufb01nitions in\nthe data dictionary.\n\u2022DML compiler , which translates DML statements in a query language into an eval-\nuation plan consisting of low-level instructions that the query-evaluation engine\nunderstands.\nA query can usually be translated into any of a number of alternative evalua-\ntion plans that all give the same result. The DML compiler also performs query\noptimization ; that is, it picks the lowest cost evaluation plan from among the alter-\nnatives.\n\u2022Query evaluation engine , which executes low-level instructions generated by the\nDML compiler.\nQuery evaluation is covered in Chapter 15, while the methods by which the query opti-\nmizer chooses from among the possible evaluation strategies are discussed in Chapter\n16.\n1.6.3 Transaction Management\nOften, several operations on the database form a single logical unit of work. An exam-\nple is a funds transfer, as in Section 1.2, in which one account Ais debited and another\naccount Bis credited. Clearly, it is essential that either both the credit and debit occur,\nor that neither occur. That is, the funds transfer must happen in its entirety or not at\nall. This all-or-none requirement is called atomicity . In addition, it is essential that the\nexecution of the funds transfer preserves the consistency of the database. That is, the\nvalue of the sum of the balances of AandBmust be preserved. This correctness require-\nment is called consistency . Finally, after the successful execution of a funds transfer,\nthe new values of the balances of accounts AandBmust persist, despite the possibility\nof system failure. This persistence requirement is called durability .\nAtransaction is a collection of operations that performs a single logical function\nin a database application. Each transaction is a unit of both atomicity and consistency.\nThus, we require that transactions do not violate any database-consistency constraints.\nThat is, if the database was consistent when a transaction started, the database must\nbe consistent when the transaction successfully terminates. However, during the exe-\ncution of a transaction, it may be necessary temporarily to allow inconsistency, since\n", "49": "1.7 Database and Application Architecture 21\neither the debit of Aor the credit of Bmust be done before the other. This temporary\ninconsistency, although necessary, may lead to di\ufb03culty if a failure occurs.\nIt is the programmer\u2019s responsibility to properly de\ufb01ne the various transactions so\nthat each preserves the consistency of the database. For example, the transaction to\ntransfer funds from account Ato account Bc o u l db ed e \ufb01 n e dt ob ec o m p o s e do ft w o\nseparate programs: one that debits account Aand another that credits account B.T h e\nexecution of these two programs one after the other will indeed preserve consistency.\nHowever, each program by itself does not transform the database from a consistent\nstate to a new consistent state. Thus, those programs are not transactions.\nEnsuring the atomicity and durability properties is the responsibility of the\ndatabase system itself\u2014speci\ufb01cally, of the recovery manager . In the absence of failures,\nall transactions complete successfully, and atomicity is achieved easily. However, be-\ncause of various types of failure, a transaction may not always complete its execution\nsuccessfully. If we are to ensure the atomici ty property, a failed transaction must have\nno e\ufb00ect on the state of the database. Thus, the database must be restored to the state\nin which it was before the transaction in question started executing. The database sys-\ntem must therefore perform failure recovery , that is, it must detect system failures and\nrestore the database to the state that existed prior to the occurrence of the failure.\nFinally, when several transactions update the database concurrently, the consis-\ntency of data may no longer be preserved, even though each individual transaction is\ncorrect. It is the responsibility of the concurrency-control manager to control the inter-\naction among the concurrent transactions, to ensure the consistency of the database.\nThetransaction manager consists of the concurrency-control manager and the recovery\nmanager.\nThe basic concepts of transaction processing are covered in Chapter 17. The man-\nagement of concurrent transactions is covered in Chapter 18. Chapter 19 covers failure\nrecovery in detail.\nThe concept of a transaction has been applied broadly in database systems and\napplications. While the initial use of tra nsactions was in \ufb01nancial applications, the\nconcept is now used in real-time applications in telecommunication, as well as in the\nmanagement of long-duration activities such as product design or administrative work-\n\ufb02ows.\n1.7 Database and Application Architecture\nWe are now in a position to provide a single picture of the various components of a\ndatabase system and the connections among them. Figure 1.3 shows the architecture\nof a database system that runs on a centralized server machine. The \ufb01gure summarizes\nhow di\ufb00erent types of users interact with a database, and how the di\ufb00erent components\nof a database engine are connected to each other.\nThe centralized architecture shown in Figure 1.3 is applicable to shared-memory\nserver architectures, which have multiple CPUs and exploit parallel processing, but all\n", "50": "22 Chapter 1 Introduction\nnaive users\n(tellers, agents,\nweb users)\nquery processor\nstorage manager\ndisk storage\nindices\nstatistical data datadata dictionaryapplication\nprogrammers\napplication\ninterfaces\napplication\nprogram\nobject codecompiler and\nlinker\nbu\ufb00er manager \ufb01le manager authorization\nand integrity\nmanagertransaction\nmanagerDML compiler\nand organizer\nquery evaluation\nengineDML queries DDL interpreterapplication\nprogramsquery\ntoolsadministration\ntoolssophisticated\nusers\n(analysts)database\nadministrators\nuse write use use\nFigure 1.3 System structure.\ntheCPUs access a common shared memory. To scale up to even larger data volumes\nand even higher processing speeds, parallel databases are designed to run on a cluster\nconsisting of multiple machines. Further, distributed databases allow data storage and\nquery processing across multiple ge ographically separated machines.\n", "51": "1.7 Database and Application Architecture 23\nIn Chapter 20, we cover the general structure of modern computer systems, with a\nfocus on parallel system architectures. Chapter 21 and Chapter 22 describe how query\nprocessing can be implemented to exploit parallel and distributed processing. Chapter\n23 presents a number of issues that arise in processing transactions in a parallel or a\ndistributed database and describes how to deal with each issue. The issues include how\nto store data, how to ensure atomicity of transactions that execute at multiple sites, how\nto perform concurrency control, and how to provide high availability in the presence\nof failures.\nWe now consider the architecture of applications that use databases as their back-\nend. Database applications can be partitioned into two or three parts, as shown in\nFigure 1.4. Earlier-generation database applications used a two-tier architecture ,w h e r e\nthe application resides at the client machine, and invokes database system functionality\nat the server machine through query language statements.\nIn contrast, modern database applications use a three-tier architecture ,w h e r et h e\nclient machine acts as merely a front end and does not contain any direct database calls;\nweb browsers and mobile applications are t he most commonly used application clients\ntoday. The front end communicates with an application server . The application server,\nin turn, communicates with a database system to access data. The business logic of the\napplication, which says what actions to carry out under what conditions, is embedded\nin the application server, instead of being distributed across multiple clients. Three-\ntier applications provide better security as well as better performance than two-tier\napplications.\nuser\napplication\ndatabase systemnetwork\n(a) Two-tier architectureclient\nserveruser\napplication client\ndatabase systemnetwork\napplication server\n(b) Three-tier architecture\nFigure 1.4 Two-tier and three-tier architectures.\n", "52": "24 Chapter 1 Introduction\n1.8 Database Users and Administrators\nA primary goal of a database system is to retrieve information from and store new\ninformation in the database. People who work with a database can be categorized as\ndatabase users or database administrators.\n1.8.1 Database Users and User Interfaces\nThere are four di\ufb00erent types of database-system users, di\ufb00erentiated by the way they\nexpect to interact with the system. Di\ufb00erent types of user interfaces have been designed\nfor the di\ufb00erent types of users.\n\u2022Na\u00a8\u0131ve users are unsophisticated users who interact with the system by using prede-\n\ufb01ned user interfaces, such as web or mobile applications. The typical user interface\nfor na \u00a8\u0131ve users is a forms interface, where the user can \ufb01ll in appropriate \ufb01elds of\nthe form. Na \u00a8\u0131ve users may also view read reports generated from the database.\nAs an example, consider a student, who during class registration period, wishes\nto register for a class by using a web interface. Such a user connects to a web\napplication program that runs at a web server. The application \ufb01rst veri\ufb01es the\nidentity of the user and then allows her to access a form where she enters the\ndesired information. The form information is sent back to the web application\nat the server, which then determines if there is room in the class (by retrieving\ninformation from the database) and if so adds the student information to the class\nroster in the database.\n\u2022Application programmers are computer professionals who write application pro-\ngrams. Application programmers can choose from many tools to develop user in-\nterfaces.\n\u2022Sophisticated users interact with the system without writing programs. Instead,\nthey form their requests either using a database query language or by using tools\nsuch as data analysis software. Analysts who submit queries to explore data in the\ndatabase fall in this category.\n1.8.2 Database Administrator\nOne of the main reasons for using DBMS s is to have central control of both the data\nand the programs that access those data. A person who has such central control over\nthe system is called a database administrator (DBA). The functions of a DBA include:\n\u2022Schema de\ufb01nition .T h e DBA creates the original database schema by executing a\nset of data de\ufb01nition statements in the DDL .\n\u2022Storage structure and access-method de\ufb01nition .T h e DBA may specify some param-\neters pertaining to the physical organization of the data and the indices to be cre-\nated.\n", "53": "1.9 History of Database Systems 25\n\u2022Schema and physical-organization modi\ufb01cation .T h e DBA carries out changes to the\nschema and physical organization to re\ufb02ect the changing needs of the organiza-\ntion, or to alter the physical organization to improve performance.\n\u2022Granting of authorization for data access . By granting di\ufb00erent types of authoriza-\ntion, the database administrator can regulate which parts of the database various\nusers can access. The authorization information is kept in a special system struc-\nture that the database system consults whenever a user tries to access the data in\nthe system.\n\u2022Routine maintenance . Examples of the database administrator\u2019s routine mainte-\nnance activities are:\n\u00b0Periodically backing up the database on to remote servers, to prevent loss of\ndata in case of disasters such as \ufb02ooding.\n\u00b0Ensuring that enough free disk space is available for normal operations, and\nupgrading disk space as required.\n\u00b0Monitoring jobs running on the database and ensuring that performance is not\ndegraded by very expensive tasks submitted by some users.\n1.9 History of Database Systems\nInformation processing drives the growth of computers, as it has from the earliest days\nof commercial computers. In fact, automation of data processing tasks predates com-\nputers. Punched cards, invented by Herman Hollerith, were used at the very beginning\nof the twentieth century to record U.S. census data, and mechanical systems were used\nto process the cards and tabulate results. Punched cards were later widely used as a\nmeans of entering data into computers.\nTechniques for data storage and processing have evolved over the years:\n\u20221950s and early 1960s : Magnetic tapes were developed for data storage. Data-\nprocessing tasks such as payroll were automated, with data stored on tapes. Pro-\ncessing of data consisted of reading data from one or more tapes and writing data\nto a new tape. Data could also be input from punched card decks and output\nto printers. For example, salary raises were processed by entering the raises on\npunched cards and reading the punched card deck in synchronization with a tape\ncontaining the master salary details. The records had to be in the same sorted or-\nder. The salary raises would be added to the salary read from the master tape and\nwritten to a new tape; the new tape would become the new master tape.\nTapes (and card decks) could be read only sequentially, and data sizes were\nmuch larger than main memory; thus, data-processing programs were forced to\n", "54": "26 Chapter 1 Introduction\nprocess data in a particular order by reading and merging data from tapes and\ncard decks.\n\u2022Late 1960s and early 1970s : Widespread use of hard disks in the late 1960s changed\nthe scenario for data processing greatly, since hard disks allowed direct access to\ndata. The position of data on disk was immaterial, since any location on disk could\nbe accessed in just tens of milliseconds. Data were thus freed from the tyranny of\nsequentiality. With the advent of disks, the network and hierarchical data models\nwere developed, which allowed data structures such as lists and trees to be stored\non disk. Programmers could construct and manipulate these data structures.\nA landmark paper by Edgar Codd in 1970 de\ufb01ned the relational model and non-\nprocedural ways of querying data in the relational model, and relational databases\nwere born. The simplicity of the relational model and the possibility of hiding im-\nplementation details completely from the programmer were enticing indeed. Codd\nlater won the prestigious Association of Computing Machinery Turing Award for\nhis work.\n\u2022Late 1970s and 1980s : Although academically interesting, the relational model was\nnot used in practice initially because of its perceived performance disadvantages;\nrelational databases could not match the performance of existing network and\nhierarchical databases. That changed with System R, a groundbreaking project\nat IBM Research that developed techniques for the construction of an e\ufb03cient\nrelational database system. The fully functional System R prototype led to IBM\u2019s\n\ufb01rst relational database product, SQL/DS . At the same time, the Ingres system was\nbeing developed at the University of California at Berkeley. It led to a commercial\nproduct of the same name. Also around this time, the \ufb01rst version of Oracle was\nreleased. Initial commercial relational database systems, such as IBM DB2 ,O r a c l e ,\nIngres, and DEC Rdb, played a major role in advancing techniques for e\ufb03cient\nprocessing of declarative queries.\nBy the early 1980s, relational databases had become competitive with network\nand hierarchical database systems even in the area of performance. Relational\ndatabases were so easy to use that they eventually replaced network and hierar-\nchical databases. Programmers using those older models were forced to deal with\nmany low-level implementation details, and they had to code their queries in a\nprocedural fashion. Most importantly, they had to keep e\ufb03ciency in mind when\ndesigning their programs, which involved a lot of e\ufb00ort. In contrast, in a rela-\ntional database, almost all these low-level tasks are carried out automatically by the\ndatabase system, leaving the programmer free to work at a logical level. Since at-\ntaining dominance in the 1980s, the relational model has reigned supreme among\ndata models.\nThe 1980s also saw much research on parallel and distributed databases, as\nwell as initial work on object-oriented databases.\n", "55": "1.9 History of Database Systems 27\n\u20221990s :T h e SQL language was designed primarily for decision support applica-\ntions, which are query-intensive, yet the mainstay of databases in the 1980s was\ntransaction-processing applications, which are update-intensive.\nIn the early 1990s, decision support and querying re-emerged as a major ap-\nplication area for databases. Tools for analyzing large amounts of data saw a large\ngrowth in usage. Many database vendors introduced parallel database products in\nthis period. Database vendors also began to add object-relational support to their\ndatabases.\nThe major event of the 1990s was the explosive growth of the World Wide\nWeb. Databases were deployed much more extensively than ever before. Database\nsystems now had to support very high transaction-processing rates, as well as very\nhigh reliability and 24 \u00d77 availability (availability 24 hours a day, 7 days a week,\nmeaning no downtime for scheduled maintenance activities). Database systems\nalso had to support web interfaces to data.\n\u20222000s : The types of data stored in database systems evolved rapidly during this\nperiod. Semi-structured data became increasingly important. XML emerged as a\ndata-exchange standard. JSON , a more compact data-exchange format well suited\nfor storing objects from JavaScript or other programming languages subsequently\ngrew increasingly important. Increasingly, such data were stored in relational\ndatabase systems as support for the XML and JSON formats was added to the\nmajor commercial systems. Spatial data (that is, data that include geographic in-\nformation) saw widespread use in navigation systems and advanced applications.\nDatabase systems added support for such data.\nOpen-source database systems, notably Postgre SQL andMySQL saw increased\nuse. \u201cAuto-admin\u201d features were added to database systems in order to allow au-\ntomatic recon\ufb01guration to adapt to changing workloads. This helped reduce the\nhuman workload in administering a database.\nSocial network platforms grew at a rapid pace, creating a need to manage data\nabout connections between people and their posted data, that did not \ufb01t well into\na tabular row-and-column format. This led to the development of graph databases.\nIn the latter part of the decade, the use of data analytics and data mining in\nenterprises became ubiquitous. Database systems were developed speci\ufb01cally to\nserve this market. These systems featured physical data organizations suitable for\nanalytic processing, such as \u201ccolumn-stores,\u201d in which tables are stored by column\nrather than the traditional row-oriented storage of the major commercial database\nsystems.\nThe huge volumes of data, as well as the fact that much of the data used for\nanalytics was textual or semi-structured, led to the development of programming\nframeworks, such as map-reduce , to facilitate application programmers\u2019 use of par-\nallelism in analyzing data. In time, support for these features migrated into tradi-\ntional database systems. Even in the late 2010s, debate continued in the database\n", "56": "28 Chapter 1 Introduction\nresearch community over the relative merits of a single database system serving\nboth traditional transaction processing applications and the newer data-analysis\napplications versus maintaining separate systems for these roles.\nThe variety of new data-intensive applications and the need for rapid devel-\nopment, particularly by startup \ufb01rms, led to \u201cNoSQL\u201d systems that provide a\nlightweight form of data management. The name was derived from those systems\u2019\nlack of support for the ubiquitous database query language SQL, though the name\nis now often viewed as meaning \u201cnot only SQL.\u201d The lack of a high-level query lan-\nguage based on the relational model gave programmers greater \ufb02exibility to work\nwith new types of data. The lack of traditional database systems\u2019 support for strict\ndata consistency provided more \ufb02exibility in an application\u2019s use of distributed\ndata stores. The NoSQL model of \u201ceventual consistency\u201d allowed for distributed\ncopies of data to be inconsistent as long they would eventually converge in the\nabsence of further updates.\n\u20222010s : The limitations of NoSQL systems, such as lack of support for consistency,\nand lack of support for declarative querying, were found acceptable by many ap-\nplications (e.g., social networks), in return for the bene\ufb01ts they provided such as\nscalability and availability. However, by the early 2010s it was clear that the lim-\nitations made life signi\ufb01cantly more complicated for programmers and database\nadministrators. As a result, these systems evolved to provide features to support\nstricter notions of consistency, while continuing to support high scalability and\navailability. Additionally, these systems increasingly support higher levels of ab-\nstraction to avoid the need for programmers to have to reimplement features that\nare standard in a traditional database system.\nEnterprises are increasingly outsourcing the storage and management of their\ndata. Rather than maintaining in-house systems and expertise, enterprises may\nstore their data in \u201ccloud\u201d services that host data for various clients in multiple,\nwidely distributed server farms. Data are delivered to users via web-based services.\nOther enterprises are outsourcing not only the storage of their data but also whole\napplications. In such cases, termed \u201csoftware as a service,\u201d the vendor not only\nstores the data for an enterprise but also runs (and maintains) the application\nsoftware. These trends result in signi\ufb01cant savings in costs, but they create new\nissues not only in responsibility for security breaches, but also in data ownership,\nparticularly in cases where a government requests access to data.\nThe huge in\ufb02uence of data and data analytics in daily life has made the man-\nagement of data a frequent aspect of the news. There is an unresolved tradeo\ufb00\nbetween an individual\u2019s right of privacy and society\u2019s need to know. Various na-\ntional governments have put regulations on privacy in place. High-pro\ufb01le security\nbreaches have created a public awareness of the challenges in cybersecurity and\nthe risks of storing data.\n", "57": "1.10 Summary 29\n1.10 Summary\n\u2022A database-management system ( DBMS ) consists of a collection of interrelated\ndata and a collection of programs to access those data. The data describe one\nparticular enterprise.\n\u2022The primary goal of a DBMS is to provide an environment that is both convenient\nand e\ufb03cient for people to use in retrieving and storing information.\n\u2022Database systems are ubiquitous today, and most people interact, either directly\nor indirectly, with databases many times every day.\n\u2022Database systems are designed to store large bodies of information. The manage-\nment of data involves both the de\ufb01nition of structures for the storage of infor-\nmation and the provision of mechanisms for the manipulation of information. In\naddition, the database system must provide for the safety of the information stored\nin the face of system crashes or attempts at unauthorized access. If data are to be\nshared among several users, the system must avoid possible anomalous results.\n\u2022A major purpose of a database system is to provide users with an abstract view of\nthe data. That is, the system hides certain details of how the data are stored and\nmaintained.\n\u2022Underlying the structure of a database is the data model: a collection of conceptual\ntools for describing data, data relationships, data semantics, and data constraints.\n\u2022The relational data model is the most widely deployed model for storing data in\ndatabases. Other data models are the object-oriented model, the object-relational\nmodel, and semi-structured data models.\n\u2022A data-manipulation language ( DML ) is a language that enables users to access or\nmanipulate data. Nonprocedural DML s, which require a user to specify only what\ndata are needed, without specifying exactly how to get those data, are widely used\ntoday.\n\u2022A data-de\ufb01nition language ( DDL ) is a language for specifying the database schema\nand other properties of the data.\n\u2022Database design mainly involves the design of the database schema. The entity-\nrelationship ( E-R) data model is a widely used model for database design. It pro-\nvides a convenient graphical representation to view data, relationships, and con-\nstraints.\n\u2022A database system has several subsystems.\n\u00b0The storage manager subsystem provides the interface between the low-level\ndata stored in the database and the application programs and queries submitted\nto the system.\n", "58": "30 Chapter 1 Introduction\n\u00b0The query processor subsystem compiles and executes DDL and DML state-\nments.\n\u2022Transaction management ensures that the database remains in a consistent (cor-\nrect) state despite system failures. The transaction manager ensures that concur-\nrent transaction executions proceed without con\ufb02icts.\n\u2022The architecture of a database system is greatly in\ufb02uenced by the underlying com-\nputer system on which the database system runs. Database systems can be central-\nized, or parallel, involving multiple machines. Distributed databases span multiple\ngeographically separated machines.\n\u2022Database applications are typically broken up into a front-end part that runs at\nclient machines and a part that runs at the backend. In two-tier architectures, the\nfront end directly communicates with a database running at the back end. In three-\ntier architectures, the back end part is itself broken up into an application server\nand a database server.\n\u2022There are four di\ufb00erent types of database-system users, di\ufb00erentiated by the way\nthey expect to interact with the system. Di\ufb00erent types of user interfaces have been\ndesigned for the di\ufb00erent types of users.\n\u2022Data-analysis techniques attempt to automatically discover rules and patterns from\ndata. The \ufb01eld of data mining combines knowledge-discovery techniques invented\nby arti\ufb01cial intelligence researchers and statistical analysts with e\ufb03cient imple-\nmentation techniques that enable them to be used on extremely large databases.\nReview Terms\n\u2022Database-management system\n(DBMS )\n\u2022Database-system applications\n\u2022Online transaction processing\n\u2022Data analytics\n\u2022File-processing systems\n\u2022Data inconsistency\n\u2022Consistency constraints\n\u2022Data abstraction\n\u00b0Physical level\n\u00b0Logical level\n\u00b0View level\u2022Instance\n\u2022Schema\n\u00b0Physical schema\n\u00b0Logical schema\n\u00b0Subschema\n\u2022Physical data independence\n\u2022Data models\n\u00b0Entity-relationship model\n\u00b0Relational data model\n\u00b0Semi-structured data model\n\u00b0Object-based data model\n", "59": "Practice Exercises 31\n\u2022Database languages\n\u00b0Data-de\ufb01nition language\n\u00b0Data-manipulation language\n\u22c4Procedural DML\n\u22c4Declarative DML\n\u22c4nonprocedural DML\n\u00b0Query language\n\u2022Data-de\ufb01nition language\n\u00b0Domain Constraints\n\u00b0Referential Integrity\n\u00b0Authorization\n\u22c4Read authorization\n\u22c4Insert authorization\n\u22c4Update authorization\n\u22c4Delete authorization\n\u2022Metadata\n\u2022Application program\n\u2022Database design\n\u00b0Conceptual design\n\u00b0Normalization\n\u00b0Speci\ufb01cation of functional re-\nquirements\n\u00b0Physical-design phase\n\u2022Database Engine\n\u00b0Storage manager\n\u22c4Authorization and integrity\nmanager\u22c4Transaction manager\n\u22c4File manager\n\u22c4Bu\ufb00er manager\n\u22c4Data \ufb01les\n\u22c4Data dictionary\n\u22c4Indices\n\u00b0Query processor\n\u22c4DDL interpreter\n\u22c4DML compiler\n\u22c4Query optimization\n\u22c4Query evaluation engine\n\u00b0Transactions\n\u22c4Atomicity\n\u22c4Consistency\n\u22c4Durability\n\u22c4Recovery manager\n\u22c4Failure recovery\n\u22c4Concurrency-control manager\n\u2022Database Architecture\n\u00b0Centralized\n\u00b0Parallel\n\u00b0Distributed\n\u2022Database Application Architecture\n\u00b0Two-tier\n\u00b0Three-tier\n\u00b0Application server\n\u2022Database administrator ( DBA)\nPractice Exercises\n1.1 This chapter has described several major advantages of a database system. What\nare two disadvantages?\n1.2 List \ufb01ve ways in which the type declaration system of a language such as Java\nor C++ di\ufb00ers from the data de\ufb01nition language used in a database.\n", "60": "32 Chapter 1 Introduction\n1.3 List six major steps that you would take in setting up a database for a particular\nenterprise.\n1.4 Suppose you want to build a video site similar to YouTube. Consider each of the\npoints listed in Section 1.2 as disadvantages of keeping data in a \ufb01le-processing\nsystem. Discuss the relevance of each of these points to the storage of actual\nvideo data, and to metadata about the video, such as title, the user who uploaded\nit, tags, and which users viewed it.\n1.5 Keyword queries used in web search are quite di\ufb00erent from database queries.\nList key di\ufb00erences between the two, in terms of the way the queries are speci\ufb01ed\nand in terms of what is the result of a query.\nExercises\n1.6 List four applications you have used that most likely employed a database system\nto store persistent data.\n1.7 List four signi\ufb01cant di\ufb00erences between a \ufb01le-processing system and a DBMS .\n1.8 Explain the concept of physical data independence and its importance in\ndatabase systems.\n1.9 List \ufb01ve responsibilities of a database-management system. For each responsi-\nbility, explain the problems that would arise if the responsibility were not dis-\ncharged.\n1.10 List at least two reasons why database systems support data manipulation using\na declarative query language such as SQL, instead of just providing a library of\nC or C++ functions to carry out data manipulation.\n1.11 Assume that two students are trying to register for a course in which there is only\none open seat. What component of a database system prevents both students\nfrom being given that last seat?\n1.12 Explain the di\ufb00erence between two-tier and three-tier application architectures.\nWhich is better suited for web applications? Why?\n1.13 List two features developed in the 2000s and that help database systems handle\ndata-analytics workloads.\n1.14 Explain why NoSQL systems emerged in the 2000s, and brie\ufb02y contrast their\nfeatures with traditional database systems.\n1.15 Describe at least three tables that might be used to store information in a social-\nnetworking system such as Facebook.\n", "61": "Further Reading 33\nTools\nThere are a large number of commercial database systems in use today.\nThe major ones include: IBM DB2 (www.ibm.com/software/data/db2 ), Ora-\ncle ( www.oracle.com ), Microsoft SQL Server ( www.microsoft.com/sql ),IBM In-\nformix ( www.ibm.com/software/data/informix ),SAP Adaptive Server Enterprise\n(formerly Sybase) ( www.sap.com/products/sybase-ase.html ), and SAP HANA\n(www.sap.com/products/hana.html ) .S o m eo ft h e s es y s t e m sa r ea v a i l a b l ef r e ef o r\npersonal or non-commercial use, or for development, but are not free for actual deploy-\nment.\nThere are also a number of free/public domain database systems; widely used ones\ninclude MySQL ( www.mysql.com ), PostgreSQL ( www.postgresql.org ), and the em-\nbedded database SQLite (www.sqlite.org ).\nA more complete list of links to vendor web sites and other information is available\nfrom the home page of this book, at db-book.com .\nFurther Reading\n[Codd (1970)] is the landmark paper that introduced the relational model. Textbook\ncoverage of database systems is provided by [O\u2019Neil and O\u2019Neil (2000)], [Ramakrish-\nnan and Gehrke (2002)], [Date (2003)], [Kifer et al. (2005)], [Garcia-Molina et al.\n(2008)], and [Elmasri and Navathe (2016)], in addition to this textbook,\nA review of accomplishments in database management and an assessment of future\nresearch challenges appears in [Abadi et al. (2016)]. The home page of the ACM Special\nInterest Group on Management of Data ( www.acm.org/sigmod )p r o v i d e saw e a l t ho f\ninformation about database research. Database vendor web sites (see the Tools section\nabove) provide details about their respective products.\nBibliography\n[Abadi et al. (2016)] D. Abadi, R. Agrawal, A. Ailamaki, M. Balazinska, P. A. Bernstein,\nM. J. Carey, S. Chaudhuri, J. Dean, A. Doan, M. J. Franklin, J. Gehrke, L. M. Haas, A. Y.\nHalevy, J. M. Hellerstein, Y. E. Ioannidis, H. J agadish, D. Kossmann, S. Madden, S. Mehro-\nt r a ,T .M i l o ,J .F .N a u g h t o n ,R .R a m a k r i s h n a n ,V .M a r k l ,C .O l s t o n ,B .C .O o i ,C .R \u00c2 \u00b4 e ,\nD. Suciu, M. Stonebraker, T. Walter, and J. Widom, \u201cThe Beckman Report on Database\nResearch\u201d, Communications of the ACM , Volume 59, Number 2 (2016), pages 92\u201399.\n[Codd (1970)] E. F. Codd, \u201cA Relational Model for Large Shared Data Banks\u201d, Communi-\ncations of the ACM , Volume 13, Number 6 (1970), pages 377\u2013387.\n[Date (2003)] C. J. Date, An Introduction to Database Systems , 8th edition, Addison Wesley\n(2003).\n", "62": "34 Chapter 1 Introduction\n[Elmasri and Navathe (2016)] R. Elmasri and S. B. Navathe, Fundamentals of Database Sys-\ntems, 7th edition, Addison Wesley (2016).\n[Garcia-Molina et al. (2008)] H. Garcia-Molina, J. D. Ullman, and J. D. Widom, Database\nSystems: The Complete Book , 2nd edition, Prentice Hall (2008).\n[Kifer et al. (2005)] M. Kifer, A. Bernstein, and P. Lewis, Database Systems: An Application\nOriented Approach, Complete Version , 2nd edition, Addison Wesley (2005).\n[O\u2019Neil and O\u2019Neil (2000)] P. O\u2019Neil and E. O\u2019Neil, Database: Principles, Programming, Per-\nformance , 2nd edition, Morgan Kaufmann (2000).\n[Ramakrishnan and Gehrke (2002)] R. Ramakrishnan and J. Gehrke, Database Management\nSystems , 3rd edition, McGraw Hill (2002).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "63": "PART1\nRELATIONAL LANGUAGES\nA data model is a collection of conceptual tools for describing data, data relationships,\ndata semantics, and consistency constraints. The relational model uses a collection of\ntables to represent both data and the relationships among those data. Its conceptual\nsimplicity has led to its widespread adoption; today a vast majority of database products\nare based on the relational model. The relational model describes data at the logical\nand view levels, abstracting away low-level details of data storage.\nTo make data from a relational database available to users, we have to address how\nusers specify requests for retrieving and updating data. Several query languages have\nbeen developed for this task, which are covered in this part.\nChapter 2 introduces the basic concepts underlying relational databases, including\nthe coverage of relational algebra\u2014a formal query language that forms the basis for\nSQL. The language SQL is the most widely used relational query language today and is\ncovered in great detail in this part.\nChapter 3 provides an overview of the SQL query language, including the SQL\ndata de\ufb01nition, the basic structure of SQL que ries, set operations, aggregate functions,\nnested subqueries, and modi\ufb01cation of the database.\nChapter 4 provides further details of SQL, including join expressions, views, trans-\nactions, integrity constraints that are enforced by the database, and authorization\nmechanisms that control what access and update actions can be carried out by a user.\nChapter 5 covers advanced topics related to SQL including access to SQL from pro-\ngramming languages, functions, procedures, triggers, recursive queries, and advanced\naggregation features.\n35\n", "64": "", "65": "CHAPTER2\nIntroduction to the Relational\nModel\nThe relational model remains the primary data model for commercial data-processing\napplications. It attained its primary positio n because of its simplicity, which eases the\njob of the programmer, compared to earlier data models such as the network model\nor the hierarchical model. It has retained this position by incorporating various new\nfeatures and capabilities over its half-century of existence. Among those additions are\nobject-relational features such as complex data types and stored procedures, support for\nXML data, and various tools to support semi-structured data. The relational model\u2019s\nindependence from any speci\ufb01c underlying low-level data structures has allowed it to\npersist despite the advent of new approaches to data storage, including modern column-\nstores that are designed for large-scale data mining.\nIn this chapter, we \ufb01rst study the fundamentals of the relational model. A substan-\ntial theory exists for relational databases. In Chapter 6 and Chapter 7, we shall examine\naspects of database theory that help in the design of relational database schemas, while\nin Chapter 15 and Chapter 16 we discuss aspects of the theory dealing with e\ufb03cient\nprocessing of queries. In Chapter 27, we study aspects of formal relational languages\nbeyond our basic coverage in this chapter.\n2.1 Structure of Relational Databases\nA relational database consists of a collection of tables , each of which is assigned a\nunique name. For example, consider the instructor table of Figure 2.1, which stores\ninformation about instructors. The table has four column headers: ID,name ,dept\n name ,\nandsalary . Each row of this table records information about an instructor, consisting of\nthe instructor\u2019s ID,name ,dept\n name ,a n d salary . Similarly, the course t a b l eo fF i g u r e2 . 2\nstores information about courses, consisting of a course\n id,title,dept\n name ,a n d credits ,\nfor each course. Note that each instructor is identi\ufb01ed by the value of the column ID,\nwhile each course is identi\ufb01ed by the value of the column course\n id.\n37\n", "66": "38 Chapter 2 Introduction to the Relational Model\nID\n name\n dept\n name\n salary\n10101\n Srinivasan\n Comp. Sci.\n 65000\n12121\n Wu\n Finance\n 90000\n15151\n Mozart\n Music\n 40000\n22222\n Einstein\n Physics\n 95000\n32343\n El Said\n History\n 60000\n33456\n Gold\n Physics\n 87000\n45565\n Katz\n Comp. Sci.\n 75000\n58583\n Cali\ufb01eri\n History\n 62000\n76543\n Singh\n Finance\n 80000\n76766\n Crick\n Biology\n 72000\n83821\n Brandt\n Comp. Sci.\n 92000\n98345\n Kim\n Elec. Eng.\n 80000\nFigure 2.1 The instructor relation.\nFigure 2.3 shows a third table, prereq , which stores the prerequisite courses for each\ncourse. The table has two columns, course\n idandprereq\n id. Each row consists of a pair\nof course identi\ufb01ers such that the second cou rse is a prerequisite for the \ufb01rst course.\nThus, a row in the prereq table indicates that two courses are related in the sense\nthat one course is a prerequisite for the other. As another example, when we consider\nthe table instructor , a row in the table can be thought of as representing the relationship\ncourse\n id\n title\n dept\n name\n credits\nBIO-101\n Intro. to Biology\n Biology\n 4\nBIO-301\n Genetics\n Biology\n 4\nBIO-399\n Computational Biology\n Biology\n 3\nCS-101\n Intro. to Computer Science\n Comp. Sci.\n 4\nCS-190\n Game Design\n Comp. Sci.\n 4\nCS-315\n Robotics\n Comp. Sci.\n 3\nCS-319\n Image Processing\n Comp. Sci.\n 3\nCS-347\n Database System Concepts\n Comp. Sci.\n 3\nEE-181\n Intro. to Digital Systems\n Elec. Eng.\n 3\nFIN-201\n Investment Banking\n Finance\n 3\nHIS-351\n World History\n History\n 3\nMU-199\n Music Video Production\n Music\n 3\nPHY-101\n Physical Principles\n Physics\n 4\nFigure 2.2 The course relation.\n", "67": "2.1 Structure of Relational Databases 39\ncourse\n id\n prereq\n id\nBIO-301\n BIO-101\nBIO-399\n BIO-101\nCS-190\n CS-101\nCS-315\n CS-101\nCS-319\n CS-101\nCS-347\n CS-101\nEE-181\n PHY-101\nFigure 2.3 The prereq relation.\nbetween a speci\ufb01ed IDand the corresponding values for name ,dept\n name ,a n d salary\nvalues.\nIn general, a row in a table represents a relationship among a set of values. Since a\ntable is a collection of such relationships, there is a close correspondence between the\nconcept of table and the mathematical concept of relation , from which the relational\ndata model takes its name. In mathematical terminology, a tuple is simply a sequence\n(or list) of values. A relationship between nvalues is represented mathematically by an\nn-tuple of values, that is, a tuple with nvalues, which corresponds to a row in a table.\nThus, in the relational model the term relation is used to refer to a table, while the\nterm tuple is used to refer to a row. Similarly, the term attribute refers to a column of a\ntable.\nExamining Figure 2.1, we can see that the relation instructor has four attributes:\nID,name ,dept\n name ,a n d salary .\nWe use the term relation instance to refer to a speci\ufb01c instance of a relation, that\nis, containing a speci\ufb01c set of rows. The instance of instructor shown in Figure 2.1 has\n12 tuples, corresponding to 12 instructors.\nIn this chapter, we shall be using a number of di\ufb00erent relations to illustrate the\nvarious concepts underlying the relational data model. These relations represent part\nof a university. To simplify our presentation, we exclude much of the data an actual\nuniversity database would contain. We shall discuss criteria for the appropriateness of\nrelational structures in great detail in Chapter 6 and Chapter 7.\nThe order in which tuples appear in a relation is irrelevant, since a relation is a set\nof tuples. Thus, whether the tuples of a relation are listed in sorted order, as in Figure\n2.1, or are unsorted, as in Figure 2.4, does not matter; the relations in the two \ufb01gures\nare the same, since both contain the same set of tuples. For ease of exposition, we\ngenerally show the relations sorted by their \ufb01rst attribute.\nFor each attribute of a relation, there is a set of permitted values, called the domain\nof that attribute. Thus, the domain of the salary attribute of the instructor relation is\nthe set of all possible salary values, while the domain of the name attribute is the set of\nall possible instructor names.\n", "68": "40 Chapter 2 Introduction to the Relational Model\nID\n name\n dept\n name\n salary\n22222\n Einstein\n Physics\n 95000\n12121\n Wu\n Finance\n 90000\n32343\n El Said\n History\n 60000\n45565\n Katz\n Comp. Sci.\n 75000\n98345\n Kim\n Elec. Eng.\n 80000\n76766\n Crick\n Biology\n 72000\n10101\n Srinivasan\n Comp. Sci.\n 65000\n58583\n Cali\ufb01eri\n History\n 62000\n83821\n Brandt\n Comp. Sci.\n 92000\n15151\n Mozart\n Music\n 40000\n33456\n Gold\n Physics\n 87000\n76543\n Singh\n Finance\n 80000\nFigure 2.4 Unsorted display of the instructor relation.\nWe require that, for all relations r, the domains of all attributes of rbe atomic.\nAd o m a i ni s atomic if elements of the domain are considered to be indivisible units.\nFor example, suppose the table instructor had an attribute phone\n number ,w h i c hc a n\nstore a set of phone numbers corresponding to the instructor. Then the domain of\nphone\n number would not be atomic, since an element of the domain is a set of phone\nnumbers, and it has subparts, namely, the individual phone numbers in the set.\nThe important issue is not what the domain itself is, but rather how we use domain\nelements in our database. Suppose now that the phone\n number attribute stores a single\nphone number. Even then, if we split the value from the phone number attribute into a\ncountry code, an area code, and a local number, we would be treating it as a non-atomic\nvalue. If we treat each phone number as a single indivisible unit, then the attribute phone\nnumber would have an atomic domain.\nThenull value is a special value that signi\ufb01es that the value is unknown or does not\nexist. For example, suppose as before that we include the attribute phone\n number in the\ninstructor relation. It may be that an instructor does not have a phone number at all,\nor that the telephone number is unlisted. We would then have to use the null value to\nsignify that the value is unknown or does not exist. We shall see later that null values\ncause a number of di\ufb03culties when we access or update the database, and thus they\nshould be eliminated if at all possible. We shall assume null values are absent initially,\nand in Section 3.6 we describe the e\ufb00ect of nulls on di\ufb00erent operations.\nThe relatively strict structure of relations results in several important practical ad-\nvantages in the storage and processing of data, as we shall see. That strict structure\nis suitable for well-de\ufb01ned and relatively stat ic applications, but it is less suitable for\napplications where not only data but also the types and structure of those data change\nover time. A modern enterprise needs to \ufb01nd a good balance between the e\ufb03ciencies\nof structured data and those situations where a predetermined structure is limiting.\n", "69": "2.2 Database Schema 41\n2.2 Database Schema\nWhen we talk about a database, we must di\ufb00erentiate between the database schema ,\nwhich is the logical design of the database, and the database instance , which is a snap-\nshot of the data in the database at a given instant in time.\nThe concept of a relation corresponds to the programming-language notion of\na variable, while the concept of a relation schema corresponds to the programming-\nlanguage notion of type de\ufb01nition.\nIn general, a relation schema consists of a list of attributes and their corresponding\ndomains. We shall not be concerned about the precise de\ufb01nition of the domain of each\nattribute until we discuss the SQL language in Chapter 3.\nThe concept of a relation instance corresponds to the programming-language no-\ntion of a value of a variable. The value of a given variable may change with time; simi-\nlarly the contents of a relation instance may change with time as the relation is updated.\nIn contrast, the schema of a relation does not generally change.\nAlthough it is important to know the di\ufb00erence between a relation schema and a\nrelation instance, we often use the same name, such as instructor , to refer to both the\nschema and the instance. Where required, we explicitly refer to the schema or to the\ninstance, for example \u201cthe instructor schema,\u201d or \u201can instance of the instructor relation.\u201d\nHowever, where it is clear whether we mean the schema or the instance, we simply use\nthe relation name.\nConsider the department relation of Figure 2.5. The schema for that relation is:\ndepartment (dept\n name ,building ,budget )\nNote that the attribute dept\n name appears in both the instructor schema and the\ndepartment schema. This duplication is not a coincidence. Rather, using common at-\ntributes in relation schemas is one way of relating tuples of distinct relations. For ex-\nample, suppose we wish to \ufb01nd the information about all the instructors who work in\nthe Watson building. We look \ufb01rst at the department relation to \ufb01nd the dept\n name of\nall the departments housed in Watson. Then, for each such department, we look in\ndept\n name\n building\n budget\nBiology\n Watson\n 90000\nComp. Sci.\n Taylor\n 100000\nElec. Eng.\n Taylor\n 85000\nFinance\n Painter\n 120000\nHistory\n Painter\n 50000\nMusic\n Packard\n 80000\nPhysics\n Watson\n 70000\nFigure 2.5 The department relation.\n", "70": "42 Chapter 2 Introduction to the Relational Model\ncourse\n id\n sec\nid\n semester\n year\n building\n room\n number\n time\n slot\nid\nBIO-101\n 1\n Summer\n 2017\n Painter\n 514\n B\nBIO-301\n 1\n Summer\n 2018\n Painter\n 514\n A\nCS-101\n 1\n Fall\n 2017\n Packard\n 101\n H\nCS-101\n 1\n Spring\n 2018\n Packard\n 101\n F\nCS-190\n 1\n Spring\n 2017\n Taylor\n 3128\n E\nCS-190\n 2\n Spring\n 2017\n Taylor\n 3128\n A\nCS-315\n 1\n Spring\n 2018\n Watson\n 120\n D\nCS-319\n 1\n Spring\n 2018\n Watson\n 100\n B\nCS-319\n 2\n Spring\n 2018\n Taylor\n 3128\n C\nCS-347\n 1\n Fall\n 2017\n Taylor\n 3128\n A\nEE-181\n 1\n Spring\n 2017\n Taylor\n 3128\n C\nFIN-201\n 1\n Spring\n 2018\n Packard\n 101\n B\nHIS-351\n 1\n Spring\n 2018\n Painter\n 514\n C\nMU-199\n 1\n Spring\n 2018\n Packard\n 101\n D\nPHY-101\n 1\n Fall\n 2017\n Watson\n 100\n A\nFigure 2.6 The section relation.\ntheinstructor relation to \ufb01nd the information about the instructor associated with the\ncorresponding dept\n name .\nEach course in a university may be o\ufb00ered multiple times, across di\ufb00erent\nsemesters, or even within a semester. We need a relation to describe each individual\no\ufb00ering, or section, of the class. The schema is:\nsection (course\n id,sec\nid,semester ,year,building ,room\n number ,time\n slot\nid)\nFigure 2.6 shows a sample instance of the section relation.\nWe need a relation to describe the association between instructors and the class\nsections that they teach. The relation schema to describe this association is:\nteaches (ID,course\n id,sec\nid,semester ,year)\nFigure 2.7 shows a sample instance of the teaches relation.\nAs you can imagine, there are many more relations maintained in a real university\ndatabase. In addition to those relations we have listed already, instructor ,department ,\ncourse ,section ,prereq ,a n d teaches , we use the following relations in this text:\n\u2022student (ID,name ,dept\n name ,tot\ncred)\n\u2022advisor (s\nid,i\nid)\n\u2022takes (ID,course\n id,sec\nid,semester ,year,grade )\n", "71": "2.3 Keys 43\nID\n course\n id\n sec\nid\n semester\n year\n10101\n CS-101\n 1\n Fall\n 2017\n10101\n CS-315\n 1\n Spring\n 2018\n10101\n CS-347\n 1\n Fall\n 2017\n12121\n FIN-201\n 1\n Spring\n 2018\n15151\n MU-199\n 1\n Spring\n 2018\n22222\n PHY-101\n 1\n Fall\n 2017\n32343\n HIS-351\n 1\n Spring\n 2018\n45565\n CS-101\n 1\n Spring\n 2018\n45565\n CS-319\n 1\n Spring\n 2018\n76766\n BIO-101\n 1\n Summer\n 2017\n76766\n BIO-301\n 1\n Summer\n 2018\n83821\n CS-190\n 1\n Spring\n 2017\n83821\n CS-190\n 2\n Spring\n 2017\n83821\n CS-319\n 2\n Spring\n 2018\n98345\n EE-181\n 1\n Spring\n 2017\nFigure 2.7 The teaches relation.\n\u2022classroom (building ,room\n number ,capacity )\n\u2022time\n slot(time\n slot\nid,day,start\n time,end\ntime)\n2.3 Keys\nWe must have a way to specify how tuples within a given relation are distinguished.\nThis is expressed in terms of their attributes. That is, the values of the attribute values\nof a tuple must be such that they can uniquely identify the tuple. In other words, no two\ntuples in a relation are allowed to have exactly the same value for all attributes.1\nAsuperkey is a set of one or more attributes that, taken collectively, allow us to\nidentify uniquely a tuple in the relation. For example, the IDattribute of the relation\ninstructor is su\ufb03cient to distinguish one instructor tuple from another. Thus, IDis a\nsuperkey. The name attribute of instructor , on the other hand, is not a superkey, because\nseveral instructors might have the same name.\nFormally, let Rdenote the set of attributes in the schema of relation r.I fw es a y\nthat a subset KofRis asuperkey forr, we are restricting consideration to instances of\nrelations rin which no two distinct tuples have the same values on all attributes in K.\nThat is, if t1and t2are in randt1\u2260t2,t h e n t1.K\u2260t2.K.\n1Commercial database systems relax the requirement that a r elation is a set and instead allow duplicate tuples. This is\ndiscussed further in Chapter 3.\n", "72": "44 Chapter 2 Introduction to the Relational Model\nA superkey may contain extraneous attributes. For example, the combination of\nIDand name is a superkey for the relation instructor .I fKis a superkey, then so is any\nsuperset of K. We are often interested in superkeys for which no proper subset is a\nsuperkey. Such minimal superkeys are called candidate keys .\nIt is possible that several distinct sets of attributes could serve as a candidate key.\nSuppose that a combination of name and dept\n name is su\ufb03cient to distinguish among\nmembers of the instructor relation. Then, both {ID}and {name ,dept\n name }are candidate\nkeys. Although the attributes IDand name together can distinguish instructor tuples,\ntheir combination, {ID,name }, does not form a candidate key, since the attribute ID\nalone is a candidate key.\nWe shall use the term primary key to denote a candidate key that is chosen by the\ndatabase designer as the principal means of identifying tuples within a relation. A key\n(whether primary, candidate, or super) is a property of the entire relation, rather than\nof the individual tuples. Any two individual tuples in the relation are prohibited from\nhaving the same value on the key attributes at the same time. The designation of a key\nrepresents a constraint in the real-world enterprise being modeled. Thus, primary keys\nare also referred to as primary key constraints .\nIt is customary to list the primary key attributes of a relation schema before the\nother attributes; for example, the dept\n name attribute of department is listed \ufb01rst, since\nit is the primary key. Primary key attributes are also underlined.\nConsider the classroom relation:\nclassroom (building\n ,room\n number\n ,capacity )\nHere the primary key consists of two attributes, building and room\n number ,w h i c ha r e\nunderlined to indicate they are part of the primary key. Neither attribute by itself can\nuniquely identify a classroom, although tog ether they uniquely identify a classroom.\nAlso consider the time\n slotrelation:\ntime\n slot(time\n slot\nid\n,day\n,start\n time\n,end\ntime)\nEach section has an associated time\n slot\nid.T h e time\n slotrelation provides information\non which days of the week, and at what times, a particular time\n slot\nidmeets. For ex-\nample, time\n slot\nid'A' may meet from 8.00 AMto 8.50 AMon Mondays, Wednesdays,\nand Fridays. It is possible for a time slot to have multiple sessions within a single day, at\ndi\ufb00erent times, so the time\n slot\nidand daytogether do not uniquely identify the tuple.\nThe primary key of the time\n slotrelation thus consists of the attributes time\n slot\nid,day,\nand start\n time, since these three attributes together uniquely identify a time slot for a\ncourse.\nPrimary keys must be chosen with care. As we noted, the name of a person is insu\ufb03-\ncient, because there may be many people with the same name. In the United States, the\nsocial security number attribute of a person would be a candidate key. Since non-U.S.\nresidents usually do not have social security numbers, international enterprises must\n", "73": "2.3 Keys 45\ngenerate their own unique identi\ufb01ers. An alternative is to use some unique combination\nof other attributes as a key.\nThe primary key should be chosen such that its attribute values are never, or are\nvery rarely, changed. For instance, the address \ufb01eld of a person should not be part of\nthe primary key, since it is likely to change. Social security numbers, on the other hand,\nare guaranteed never to change. Unique identi\ufb01ers generated by enterprises generally\ndo not change, except if two enterprises merge; in such a case the same identi\ufb01er may\nhave been issued by both enterprises, and a reallocation of identi\ufb01ers may be required\nto make sure they are unique.\nFigure 2.8 shows the complete set of relati ons that we use in our sample university\nschema, with primary-key attributes underlined.\nNext, we consider another type of constraint on the contents of relations, called\nforeign-key constraints. Consider the attribute dept\n name of the instructor relation. It\nwould not make sense for a tuple in instructor to have a value for dept\n name that does not\ncorrespond to a department in the department relation. Thus, in any database instance,\ngiven any tuple, say ta,f r o mt h e instructor relation, there must be some tuple, say tb,i n\nthedepartment relation such that the value of the dept\n name attribute of tais the same\nas the value of the primary key, dept\n name ,o ftb.\nAforeign-key constraint from attribute(s) Aof relation r1to the primary-key Bof\nrelation r2states that on any database instance, the value of Afor each tuple in r1must\nalso be the value of Bfor some tuple in r2.A t t r i b u t es e t Ais called a foreign key from r1,\nreferencing r2.T h er e l a t i o n r1is also called the referencing relation of the foreign-key\nconstraint, and r2is called the referenced relation .\nFor example, the attribute dept\n name ininstructor is a foreign key from instructor ,\nreferencing department ;n o t et h a t dept\n name is the primary key of department . Similarly,\nclassroom (building\n ,room\n number\n ,capacity )\ndepartment (dept\n name\n ,building ,budget )\ncourse (course\n id\n,title,dept\n name ,credits )\ninstructor (ID\n,name ,dept\n name ,salary )\nsection (course\n id\n,sec\nid\n,semester\n ,year\n,building ,room\n number ,time\n slot\nid)\nteaches (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n)\nstudent (ID\n,name ,dept\n name ,tot\ncred)\ntakes (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n,grade )\nadvisor (s\nID\n,i\nID)\ntime\n slot(time\n slot\nid\n,day\n,start\n time\n,end\ntime)\nprereq (course\n id\n,prereq\n id\n)\nFigure 2.8 Schema of the university database.\n", "74": "46 Chapter 2 Introduction to the Relational Model\nthe attributes building androom\n number of the section relation together form a foreign\nkey referencing the classroom relation.\nNote that in a foreign-key constraint, the referenced attribute(s) must be the pri-\nmary key of the referenced relation. The more general case, a referential-integrity con-\nstraint, relaxes the requirement that the referenced attributes form the primary key of\nthe referenced relation.\nAs an example, consider the values in the time\n slot\nidattribute of the section re-\nlation. We require that these values must exist in the time\n slot\nidattribute of the time\nslotrelation. Such a requirement is an example of a referential integrity constraint. In\ngeneral, a referential integrity constraint requires that the values appearing in speci\ufb01ed\nattributes of any tuple in the referencing relation also appear in speci\ufb01ed attributes of\nat least one tuple in the referenced relation.\nNote that time\n slotdoes not form a primary key of the time\n slotrelation, although it\nis a part of the primary key; thus, we cannot use a foreign-key constraint to enforce the\nabove constraint. In fact, foreign-key constraints are a special case of referential integrity\nconstraints, where the referenced attributes form the primary key of the referenced\nrelation. Database systems today typically support foreign-key constraints, but they\ndo not support referential integrity constraints where the referenced attribute is not a\nprimary key.\n2.4 Schema Diagrams\nA database schema, along with primary key and foreign-key constraints, can be de-\npicted by schema diagrams . Figure 2.9 shows the schema diagram for our university\norganization. Each relation appears as a box, with the relation name at the top in blue\nand the attributes listed inside the box.\nPrimary-key attributes are shown underlined. Foreign-key constraints appear as\narrows from the foreign-key attributes of the referencing relation to the primary key of\nthe referenced relation. We use a two-headed arrow, instead of a single-headed arrow,\nto indicate a referential integrity constraint that is not a foreign-key constraints. In\nFigure 2.9, the line with a two-headed arrow from time\n slot\nidin the section relation to\ntime\n slot\nidin the time\n slotrelation represents the referential integrity constraint from\nsection .time\n slot\nidtotime\n slot.time\n slot\nid.\nMany database systems provide design tools with a graphical user interface for\ncreating schema diagrams.2We shall discuss a di\ufb00erent diagrammatic representation\nof schemas, called the entity-relationship diagram, at length in Chapter 6; although\nthere are some similarities in appearance, these two notations are quite di\ufb00erent, and\nshould not be confused for one another.\n2The two-headed arrow notation to represent referential integrity constraints has been introduced by us and is not\nsupported by any tool as far as we know; the notations for primary and foreign keys, however, are widely used.\n", "75": "2.5 Relational Query Languages 47\nID\ncourse_id\nsec_id\nsemester\nyear\ngradeID\nname\ndept_name\ntot_cred\nbuilding\nroom_number\ncapacitys_id\ni_id\nID\ncourse_id\nsec_id\nsemester\nyeartakes\nsection\nclassroom\nteachesprereq\ncourse_id\nprereq_idcourse_id\ntitle\ndept_name\ncreditscoursestudent\ndept_name\nbuilding\nbudgetdepartment\ninstructor\nID\nname\ndept_name\nsalaryadvisor\ntime_slot\ntime_slot_id\nday\nstart_time\nend_timecourse_id\nsec_id\nsemester\nyear\nbuilding\nroom_number\ntime_slot_id\nFigure 2.9 Schema diagram for the university database.\n2.5 Relational Query Languages\nAquery language is a language in which a user requests information from the database.\nThese languages are usually on a level higher than that of a standard programming\nlanguage. Query languages can be categorized as imperative, functional, or declarative.\nIn an imperative query language , the user instructs the system to perform a speci\ufb01c\nsequence of operations on the database to compute the desired result; such languages\nusually have a notion of state variables, which are updated in the course of the compu-\ntation.\nIn a functional query language , the computation is expressed as the evaluation of\nfunctions that may operate on data in the database or on the results of other functions;\nfunctions are side-e\ufb00ect free, and they do not update the program state.3In adeclara-\ntive query language , the user describes the desired information without giving a speci\ufb01c\nsequence of steps or function calls for obtaining that information; the desired informa-\ntion is typically described using some form of mathematical logic. It is the job of the\ndatabase system to \ufb01gure out how to obtain the desired information.\n3The term procedural language has been used in earlier editions of the book to refer to languages based on procedure\ninvocations, which include functional languages; however, t he term is also widely used to refer to imperative languages.\nTo avoid confusion we no longer use the term.\n", "76": "48 Chapter 2 Introduction to the Relational Model\nThere are a number of \u201cpure\u201d query languages.\n\u2022The relational algebra , which we describe in Section 2.6, is a functional query\nlanguage.4The relational algebra forms the theoretical basis of the SQL query lan-\nguage.\n\u2022The tuple relational calculus and domain relational calculus, which we describe in\nChapter 27 (available online) are declarative.\nThese query languages are terse and formal, lacking the \u201csyntactic sugar\u201d of commercial\nlanguages, but they illustrate the fundamental techniques for extracting data from the\ndatabase.\nQuery languages used in practice, such as the SQL query language, include ele-\nments of the imperative, functional, and declarative approaches. We study the very\nwidely used query language SQL in Chapter 3 through Chapter 5.\n2.6 The Relational Algebra\nThe relational algebra consists of a set of operations that take one or two relations as\ninput and produce a new relation as their result.\nSome of these operations, such as the select, project, and rename operations, are\ncalled unary operations because they operate on one relation. The other operations,\nsuch as union, Cartesian product, and set di\ufb00erence, operate on pairs of relations and\nare, therefore, called binary operations.\nAlthough the relational algebra operations form the basis for the widely used SQL\nquery language, database systems do not allow users to write queries in relational alge-\nbra. However, there are implementations of relational algebra that have been built for\nstudents to practice relational alge bra queries. The website of our book, db-book.com ,\nunder the link titled Laboratory Material, provides pointers to a few such implementa-\ntions.\nIt is worth recalling at this point that since a relation is a set of tuples, relations\ncannot contain duplicate tuples. In practice, however, tables in database systems are\npermitted to contain duplicates unless a speci\ufb01c constraint prohibits it. But, in dis-\ncussing the formal relational algebra, we require that duplicates be eliminated, as is\nrequired by the mathematical de\ufb01nition of a set. In Chapter 3 we discuss how rela-\ntional algebra can be extended to work on multisets, which are sets that can contain\nduplicates.\n4Unlike modern functional languages, relational algebra supports only a small number of prede\ufb01ned functions, which\nde\ufb01ne an algebra on relations.\n", "77": "2.6 The Relational Algebra 49\nID\n name\n dept\n name\n salary\n22222\n Einstein\n Physics\n 95000\n33456\n Gold\n Physics\n 87000\nFigure 2.10 Result of \u03c3dept\n name=\u201cPhysics\u201d (instructor ).\n2.6.1 The Select Operation\nTheselect operation selects tuples that satisfy a given predicate. We use the lowercase\nGreek letter sigma ( \u03c3) to denote selection. The predicate appears as a subscript to \u03c3.\nThe argument relation is in parentheses after the \u03c3. Thus, to select those tuples of the\ninstructor relation where the instructor is in the \u201cPhysics\u201d department, we write:\n\u03c3dept\n name=\u201cPhysics\u201d (instructor )\nIf the instructor relation is as shown in Figure 2.1, then the relation that results\nfrom the preceding query is as shown in Figure 2.10.\nWe can \ufb01nd all instructors with salary greater than $90,000 by writing:\n\u03c3salary>90000 (instructor )\nIn general, we allow comparisons using =,\u2260,<,\u2264,>,a n d\u2265in the selection pred-\nicate. Furthermore, we can combine several predicates into a larger predicate by using\nthe connectives and(\u2227),or(\u2228), and not(\u00ac). Thus, to \ufb01nd the instructors in Physics\nwith a salary greater than $90,000, we write:\n\u03c3dept\n name=\u201cPhysics\u201d \u2227salary>90000 (instructor )\nThe selection predicate may include comparisons between two attributes. To illus-\ntrate, consider the relation department . To \ufb01nd all departments whose name is the same\nas their building name, we can write:\n\u03c3dept\n name=building (department )\n2.6.2 The Project Operation\nSuppose we want to list all instructors\u2019 ID,name ,a n d salary , but we do not care about\nthedept\n name .T h e project operation allows us to produce this relation. The project\noperation is a unary operation that returns its argument relation, with certain attributes\nleft out. Since a relation is a set, any duplicate rows are eliminated. Projection is denoted\nby the uppercase Greek letter pi ( \u03a0). We list those attributes that we wish to appear in\nthe result as a subscript to \u03a0. The argument relation follows in parentheses. We write\nthe query to produce such a list as:\n\u03a0ID,name ,salary(instructor )\nFigure 2.11 shows the relation that results from this query.\n", "78": "50 Chapter 2 Introduction to the Relational Model\nID\n name\n salary\n10101\n Srinivasan\n 65000\n12121\n Wu\n 90000\n15151\n Mozart\n 40000\n22222\n Einstein\n 95000\n32343\n El Said\n 60000\n33456\n Gold\n 87000\n45565\n Katz\n 75000\n58583\n Cali\ufb01eri\n 62000\n76543\n Singh\n 80000\n76766\n Crick\n 72000\n83821\n Brandt\n 92000\n98345\n Kim\n 80000\nFigure 2.11 Result of \u03a0ID,name ,salary (instructor ).\nThe basic version of the project operator \u03a0L(E) allows only attribute names to be\npresent in the list L. A generalized version of the operator allows expressions involving\nattributes to appear in the list L.F o re x a m p l e ,w ec o u l du s e :\n\u03a0ID,name ,salary\u221512(instructor )\nto get the monthly salary of each instructor.\n2.6.3 Composition of Relational Operations\nThe fact that the result of a relational operation is itself a relation is important. Con-\nsider the more complicated query \u201cFind the names of all instructors in the Physics\ndepartment.\u201d We write:\n\u03a0name(\u03c3dept\n name=\u201cPhysics\u201d (instructor ))\nNotice that, instead of giving the name of a relation as the argument of the projection\noperation, we give an expression that evaluates to a relation.\nIn general, since the result of a relational-algebra operation is of the same type\n(relation) as its inputs, relational-algebra operations can be composed together into a\nrelational-algebra expression . Composing relational-algebra operations into relational-\nalgebra expressions is just like composing arithmetic operations (such as +,\u2212,\u2217,a n d\n\u00f7) into arithmetic expressions.\n2.6.4 The Cartesian-Product Operation\nTheCartesian-product operation, denoted by a cross ( \u00d7), allows us to combine infor-\nmation from any two relations. We write the Cartesian product of relations r1and r2\nasr1\u00d7r2.\n", "79": "2.6 The Relational Algebra 51\ninstructor. ID\nname\n dept\n name\n salary\n teaches .ID\ncourse\n id\nsec\nid\nsemester\n year\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-101\n 1\n Fall\n 2017\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-315\n 1\n Spring\n 2018\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-347\n 1\n Fall\n 2017\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 12121\n FIN-201\n 1\n Spring\n 2018\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 15151\n MU-199\n 1\n Spring\n 2018\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n12121\n Wu\n Finance\n 90000\n 10101\n CS-101\n 1\n Fall\n 2017\n12121\n Wu\n Finance\n 90000\n 10101\n CS-315\n 1\n Spring\n 2018\n12121\n Wu\n Finance\n 90000\n 10101\n CS-347\n 1\n Fall\n 2017\n12121\n Wu\n Finance\n 90000\n 12121\n FIN-201\n 1\n Spring\n 2018\n12121\n Wu\n Finance\n 90000\n 15151\n MU-199\n 1\n Spring\n 2018\n12121\n Wu\n Finance\n 90000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n15151\n Mozart\n Music\n 40000\n 10101\n CS-101\n 1\n Fall\n 2017\n15151\n Mozart\n Music\n 40000\n 10101\n CS-315\n 1\n Spring\n 2018\n15151\n Mozart\n Music\n 40000\n 10101\n CS-347\n 1\n Fall\n 2017\n15151\n Mozart\n Music\n 40000\n 12121\n FIN-201\n 1\n Spring\n 2018\n15151\n Mozart\n Music\n 40000\n 15151\n MU-199\n 1\n Spring\n 2018\n15151\n Mozart\n Music\n 40000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n22222\n Einstein\n Physics\n 95000\n 10101\n CS-101\n 1\n Fall\n 2017\n22222\n Einstein\n Physics\n 95000\n 10101\n CS-315\n 1\n Spring\n 2018\n22222\n Einstein\n Physics\n 95000\n 10101\n CS-347\n 1\n Fall\n 2017\n22222\n Einstein\n Physics\n 95000\n 12121\n FIN-201\n 1\n Spring\n 2018\n22222\n Einstein\n Physics\n 95000\n 15151\n MU-199\n 1\n Spring\n 2018\n22222\n Einstein\n Physics\n 95000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\nFigure 2.12 Result of the Cartesian product instructor \u00d7teaches .\nA Cartesian product of database relations di\ufb00ers in its de\ufb01nition slightly from the\nmathematical de\ufb01nition of a Cartesian product of sets. Instead of r1\u00d7r2producing\npairs ( t1,t2)o ft u p l e sf r o m r1and r2, the relational algebra concatenates t1and t2into\na single tuple, as shown in Figure 2.12.\nSince the same attribute name may appear in the schemas of both r1and r2,w e\nneed to devise a naming schema to distinguish between these attributes. We do so here\nby attaching to an attribute the name of the relation from which the attribute originally\ncame. For example, the relation schema for r=instructor \u00d7teaches is:\n(instructor .ID,instructor .name ,instructor .dept\n name ,instructor .salary ,\nteaches .ID,teaches .course\n id,teaches .sec\nid,teaches .semester ,teaches .year)\n", "80": "52 Chapter 2 Introduction to the Relational Model\nWith this schema, we can distinguish instructor .IDfrom teaches .ID. For those attributes\nthat appear in only one of the two schemas, we shall usually drop the relation-name\npre\ufb01x. This simpli\ufb01cation does not lead to any ambiguity. We can then write the relation\nschema for ras:\n(instructor .ID,name ,dept\n name ,salary ,\nteaches .ID,course\n id,sec\nid,semester ,year)\nThis naming convention requires that the relations that are the arguments of the\nCartesian-product operation have distinct names. This requirement causes problems\nin some cases, such as when the Cartesian product of a relation with itself is desired. A\nsimilar problem arises if we use the result of a relational-algebra expression in a Carte-\nsian product, since we shall need a name for the relation so that we can refer to the\nrelation\u2019s attributes. In Section 2.6.8, we see how to avoid these problems by using the\nrename operation.\nNow that we know the relation schema for r=instructor \u00d7teaches ,w h a tt u p l e s\nappear in r? As you may suspect, we construct a tuple of rout of each possible pair of\ntuples: one from the instructor relation (Figure 2.1) and one from the teaches relation\n(Figure 2.7). Thus, ris a large relation, as you can see from Figure 2.12, which includes\nonly a portion of the tuples that make up r.\nAssume that we have n1tuples in instructor and n2tuples in teaches . Then, there\naren1\u2217n2ways of choosing a pair of tuples\u2014one tuple from each relation; so there\naren1\u2217n2tuples in r. In particular for our example, for some tuples tinr,i tm a yb e\nthat the two IDvalues, instructor .IDand teaches .ID,a r ed i \ufb00 e r e n t .\nIn general, if we have relations r1(R1)a n d r2(R2), then r1\u00d7r2is a relation r(R)\nwhose schema Ris the concatenation of the schemas R1andR2.R e l a t i o n rcontains all\ntuples tfor which there is a tuple t1inr1and a tuple t2inr2for which tandt1have the\nsame value on the attributes in R1andtandt2have the same value on the attributes in\nR2.\n2.6.5 The Join Operation\nSuppose we want to \ufb01nd the information about all instructors together with the course\nidof all courses they have taught. We need the information in both the instructor\nrelation and the teaches relation to compute the required result. The Cartesian product\nofinstructor andteaches does bring together information from both these relations, but\nunfortunately the Cartesian product associates every instructor with every course that\nwas taught, regardless of whether that instructor taught that course.\nSince the Cartesian-product operation associates every tuple of instructor with every\ntuple of teaches , we know that if an instructor has taught a course (as recorded in the\nteaches relation), then there is some tuple in instructor \u00d7teaches that contains her\nname and satis\ufb01es instructor .ID=teaches .ID. So, if we write:\n\u03c3instructor.ID=teaches.ID(instructor \u00d7teaches )\nwe get only those tuples of instructor \u00d7teaches that pertain to instructors and the\ncourses that they taught.\n", "81": "2.6 The Relational Algebra 53\ninstructor. ID\nname\n dept\n name\n salary\n teaches .ID\ncourse\n id\nsec\nid\nsemester\n year\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-101\n 1\n Fall\n 2017\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-315\n 1\n Spring\n 2018\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-347\n 1\n Fall\n 2017\n12121\n Wu\n Finance\n 90000\n 12121\n FIN-201\n 1\n Spring\n 2018\n15151\n Mozart\n Music\n 40000\n 15151\n MU-199\n 1\n Spring\n 2018\n22222\n Einstein\n Physics\n 95000\n 22222\n PHY-101\n 1\n Fall\n 2017\n32343\n El Said\n History\n 60000\n 32343\n HIS-351\n 1\n Spring\n 2018\n45565\n Katz\n Comp. Sci.\n 75000\n 45565\n CS-101\n 1\n Spring\n 2018\n45565\n Katz\n Comp. Sci.\n 75000\n 45565\n CS-319\n 1\n Spring\n 2018\n76766\n Crick\n Biology\n 72000\n 76766\n BIO-101\n 1\n Summer\n 2017\n76766\n Crick\n Biology\n 72000\n 76766\n BIO-301\n 1\n Summer\n 2018\n83821\n Brandt\n Comp. Sci.\n 92000\n 83821\n CS-190\n 1\n Spring\n 2017\n83821\n Brandt\n Comp. Sci.\n 92000\n 83821\n CS-190\n 2\n Spring\n 2017\n83821\n Brandt\n Comp. Sci.\n 92000\n 83821\n CS-319\n 2\n Spring\n 2018\n98345\n Kim\n Elec. Eng.\n 80000\n 98345\n EE-181\n 1\n Spring\n 2017\nFigure 2.13 Result of \u03c3instructor .ID=teaches.ID(instructor \u00d7teaches ).\nThe result of this expression is shown in Figure 2.13. Observe that instructors Gold,\nCali\ufb01eri, and Singh do not teach any course (as recorded in the teaches relation), and\ntherefore do not appear in the result.\nNote that this expression results in the duplication of the instructor\u2019s ID.T h i sc a n\nbe easily handled by adding a projection to eliminate the column teaches.ID.\nThe joinoperation allows us to combine a selection and a Cartesian product into\nas i n g l eo p e r a t i o n .\nConsider relations r(R)a n d s(S), and let \u03b8be a predicate on attributes in the\nschema R\u222aS.T h e joinoperation r\u22c8\u03b8sis de\ufb01ned as follows:\nr\u22c8\u03b8s=\u03c3\u03b8(r\u00d7s)\nThus,\u03c3instructor.ID=teaches.ID(instructor \u00d7teaches ) can equivalently be written as\ninstructor \u22c8instructor.ID=teaches.IDteaches .\n2.6.6 Set Operations\nConsider a query to \ufb01nd the set of all courses taught in the Fall 2017 semester, the\nSpring 2018 semester, or both. The information is contained in the section relation\n(Figure 2.6). To \ufb01nd the set of all courses taught in the Fall 2017 semester, we write:\n\u03a0course\n id(\u03c3semester=\u201cFall\u201d\u2227year=2017(section ))\nT o\ufb01 n dt h es e to fa l lc o u r s e st a u g h ti nt h eS p r i n g2 0 1 8s e m e s t e r ,w ew r i t e :\n\u03a0course\n id(\u03c3semester=\u201cSpring\u201d\u2227year=2018(section ))\n", "82": "54 Chapter 2 Introduction to the Relational Model\nTo answer the query, we need the union of these two sets; that is, we need all course\nids that appear in either or both of the two relations. We \ufb01nd these data by the binary\noperation union, denoted, as in set theory, by \u222a. So the expression needed is:\n\u03a0course\n id(\u03c3semester=\u201cFall\u201d\u2227year=2017(section ))\u222a\n\u03a0course\n id(\u03c3semester=\u201cSpring\u201d\u2227year=2018(section ))\nThe result relation for this query appears in Figure 2.14. Notice that there are eight\ntuples in the result, even though there are three distinct courses o\ufb00ered in the Fall\n2017 semester and six distinct courses o\ufb00ered in the Spring 2018 semester. Since rela-\ntions are sets, duplicate values such as CS-101, which is o\ufb00ered in both semesters, are\nreplaced by a single occurrence.\nObserve that, in our example, we took the union of two sets, both of which con-\nsisted of course\n idvalues. In general, for a union operation to make sense:\n1.We must ensure that the input relations to the union operation have the same\nnumber of attributes; the number of attributes of a relation is referred to as its\narity.\n2.When the attributes have associated types, the types of the ith attributes of both\ninput relations must be the same, for each i.\nSuch relations are referred to as compatible relations.\nFor example, it would not make sense to take the union of the instructor andsection\nrelations, since they have di\ufb00erent numbers of attributes. And even though the instruc-\ntorand the student relations both have arity 4, their 4th attributes, namely, salary and\ntot\ncred, are of two di\ufb00erent types. The union of these two attributes would not make\nsense in most situations.\nTheintersection operation, denoted by \u2229, allows us to \ufb01nd tuples that are in both\nthe input relations. The expression r\u2229sproduces a relation containing those tuples in\ncourse\n id\nCS-101\nCS-315\nCS-319\nCS-347\nFIN-201\nHIS-351\nMU-199\nPHY-101\nFigure 2.14 Courses offered in either Fall 2017, Spring 2018, or both semesters.\n", "83": "2.6 The Relational Algebra 55\ncourse\n id\nCS-101\nFigure 2.15 C o u r s e so f f e r e di nb o t ht h eF a l l2 0 1 7a n dS p r i n g2 0 1 8s e m e s t e r s .\nras well as in s. As with the union operation, we must ensure that intersection is done\nbetween compatible relations.\nSuppose that we wish to \ufb01nd the set of all courses taught in both the Fall 2017 and\nthe Spring 2018 semesters. Using set intersection, we can write\n\u03a0course\n id(\u03c3semester=\u201cFall\u201d\u2227year=2017(section ))\u2229\n\u03a0course\n id(\u03c3semester=\u201cSpring\u201d\u2227year=2018(section ))\nThe result relation for this query appears in Figure 2.15.\nTheset-di\ufb00erence operation, denoted by \u2212, allows us to \ufb01nd tuples that are in one\nrelation but are not in another. The expression r\u2212sproduces a relation containing\nthose tuples in rbut not in s.\nWe can \ufb01nd all the courses taught in the Fall 2017 semester but not in Spring 2018\nsemester by writing:\n\u03a0course\n id(\u03c3semester=\u201cFall\u201d\u2227year=2017(section ))\u2212\n\u03a0course\n id(\u03c3semester=\u201cSpring\u201d\u2227year=2018(section ))\nThe result relation for this query appears in Figure 2.16.\nAs with the union operation, we must ensure that set di\ufb00erences are taken between\ncompatible relations.\n2.6.7 The Assignment Operation\nIt is convenient at times to write a relational-algebra expression by assigning parts of it\nto temporary relation variables. The assignment operation, denoted by \u2190, works like\nassignment in a programming language. To illustrate this operation, consider the query\nto \ufb01nd courses that run in Fall 2017 as well as Spring 2018, which we saw earlier. We\ncould write it as:\ncourse\n id\nCS-347\nPHY-101\nFigure 2.16 Courses offered in the Fall 2017 semester but not in Spring 2018\nsemester.\n", "84": "56 Chapter 2 Introduction to the Relational Model\ncourses\n fall\n2017\u2190\u03a0course\n id(\u03c3semester=\u201cFall\u201d\u2227year=2017(section ))\ncourses\n spring\n 2018\u2190\u03a0course\n id(\u03c3semester=\u201cSpring\u201d\u2227year=2018(section ))\ncourses\n fall\n2017\u2229courses\n spring\n 2018\nThe \ufb01nal line above displays the query result. The preceding two lines assign the query\nresult to a temporary relation. The evaluation of an assignment does not result in any\nrelation being displayed to the user. Rather, the result of the expression to the right of\nthe\u2190is assigned to the relation variable on the left of the \u2190. This relation variable\nmay be used in subsequent expressions.\nWith the assignment operation, a query can be written as a sequential program con-\nsisting of a series of assignments followed by an expression whose value is displayed\nas the result of the query. For relational-algebra queries, assignment must always be\nmade to a temporary relation variable. Assignments to permanent relations constitute\na database modi\ufb01cation. Note that the assignment operation does not provide any addi-\ntional power to the algebra. It is, however, a convenient way to express complex queries.\n2.6.8 The Rename Operation\nUnlike relations in the database, the results of relational-algebra expressions do not\nhave a name that we can use to refer to them. It is useful in some cases to give them\nnames; the rename operator, denoted by the lowercase Greek letter rho ( \u03c1), lets us do\nthis. Given a relational-algebra expression E, the expression\n\u03c1x(E)\nreturns the result of expression Eunder the name x.\nAr e l a t i o n rby itself is considered a (trivial) relational-algebra expression. Thus,\nwe can also apply the rename operation to a relation rto get the same relation under a\nnew name. Some queries require the same relation to be used more than once in the\nquery; in such cases, the rename operation can be used to give unique names to the\ndi\ufb00erent occurrences of the same relation.\nA second form of the rename operation is as follows: Assume that a relational-\nalgebra expression Ehas arity n. Then, the expression\n\u03c1x(A1,A2,\u2026,An)(E)\nreturns the result of expression Eunder the name x, and with the attributes renamed\ntoA1,A2,\u2026,An. This form of the rename operation can be used to give names to\nattributes in the results of relational algebra operations that involve expressions on\nattributes.\nTo illustrate renaming a relation, we consider the query \u201cFind the IDand name of\nthose instructors who earn more than the instructor whose IDis 12121.\u201d (That\u2019s the\ninstructor Wu in the example table in Figure 2.1.)\nThere are several strategies for writing this query, but to illustrate the rename op-\neration, our strategy is to compare the salary of each instructor with the salary of the\n", "85": "2.6 The Relational Algebra 57\nNote 2.1 OTHER RELATIONAL OPERATIONS\nIn addition to the relational algebra operations we have seen so far, there are a\nnumber of other operations that are commonly used. We summarize them below\nand describe them in detail later, along with equivalent SQL constructs.\nThe aggregation operation allows a function to be computed over the set of\nvalues returned by a query. These functions include average, sum, min, and max,\namong others. The operation allows also for these aggregations to be performed\nafter splitting the set of values into groups, for example, by computing the average\nsalary in each department. We study the aggregation operation in more detail in\nSection 3.7 (Note 3.2 on page 97).\nThe natural join operation replaces the predicate \u03b8in\u22c8\u03b8with an implicit pred-\nicate that requires equality over those attributes that appear in the schemas of\nboth the left and right relations. This is notationally convenient but poses risks for\nqueries that are reused and thus might be used after a relation\u2019s schema is changed.\nIt is covered in Section 4.1.1.\nRecall that when we computed the join of instructor and teaches ,i n s t r u c t o r s\nw h oh a v en o tt a u g h ta n yc o u r s ed on o ta p p e a ri nt h ej o i nr e s u l t .T h e outer join\noperation allows for the retention of such tuples in the result by inserting nulls for\nthe missing values. It is covered in Section 4.1.3 (Note 4.1 on page 136).\ninstructor with ID1 2 1 2 1 .T h ed i \ufb03 c u l t yh e r ei st h a tw en e e dt or e f e r e n c et h e instructor\nrelation once to get the salary of each instructor and then a second time to get the\nsalary of instructor 12121; and we want to do all this in one expression. The rename\noperator allows us to do this using di\ufb00erent names for each referencing of the instructor\nrelation. In this example, we shall use the name ito refer to our scan of the instructor\nrelation in which we are seeking those that will be part of the answer, and wto refer to\nthe scan of the instructor relation to obtain the salary of instructor 12121:\n\u03a0i.ID,i.name((\u03c3i.salary>w.salary(\u03c1i(instructor )\u00d7\u03c3w.id=12121(\u03c1w(instructor )))))\nThe rename operation is not strictly required, since it is possible to use a positional\nnotation for attributes. We can name attributes of a relation implicitly by using a posi-\ntional notation, where $1, $2, \u2026refer to the \ufb01rst attribute, the second attribute, and\nso on. The positional notation can also be used to refer to attributes of the results of\nrelational-algebra operations. However, the positional notation is inconvenient for hu-\nmans, since the position of the attribute is a number, rather than an easy-to-remember\nattribute name. Hence, we do not use the positional notation in this textbook.\n", "86": "58 Chapter 2 Introduction to the Relational Model\n2.6.9 Equivalent Queries\nNote that there is often more than one way to write a query in relational algebra. Con-\nsider the following query, which \ufb01nds information about courses taught by instructors\nin the Physics department:\n\u03c3dept\n name=\u201cPhysics\u201d (instructor \u22c8instructor.ID=teaches.IDteaches )\nNow consider an alternative query:\n(\u03c3dept\n name=\u201cPhysics\u201d (instructor ))\u22c8instructor.ID=teaches.IDteaches\nNote the subtle di\ufb00erence between the two queries: in the \ufb01rst query, the selection\nthat restricts dept\n name to Physics is applied after the join of instructor andteaches has\nbeen computed, whereas in the second query, the selection that restricts dept\n name to\nPhysics is applied to instructor , and the join operation is applied subsequently.\nAlthough the two queries are not identical, they are in fact equivalent ;t h a ti s ,t h e y\ngive the same result on any database.\nQuery optimizers in database systems typically look at what result an expression\ncomputes and \ufb01nd an e\ufb03cient way of computing that result, rather than following the\nexact sequence of steps speci\ufb01ed in the query. The algebraic structure of relational\nalgebra makes it easy to \ufb01nd e\ufb03cient but equivalent alternative expressions, as we will\nsee in Chapter 16.\n2.7 Summary\n\u2022The relational data model is based on a collection of tables. The user of the\ndatabase system may query these tables, insert new tuples, delete tuples, and up-\ndate (modify) tuples. There are several languages for expressing these operations.\n\u2022The schema of a relation refers to its logical design, while an instance of the re-\nlation refers to its contents at a point in time. The schema of a database and an\ninstance of a database are similarly de\ufb01ned. The schema of a relation includes its\nattributes, and optionally the types of the attributes and constraints on the relation\nsuch as primary and foreign-key constraints.\n\u2022A superkey of a relation is a set of one or more attributes whose values are guar-\nanteed to identify tuples in the relation uniquely. A candidate key is a minimal\nsuperkey, that is, a set of attributes that forms a superkey, but none of whose sub-\nsets is a superkey. One of the candidate keys of a relation is chosen as its primary\nkey.\n\u2022A foreign-key constraint from attribute(s) Aof relation r1to the primary-key Bof\nrelation r2states that the value of Afor each tuple in r1must also be the value of\nBfor some tuple in r2.T h er e l a t i o n r1is called the referencing relation, and r2is\ncalled the referenced relation.\n", "87": "Practice Exercises 59\n\u2022A schema diagram is a pictorial depiction of the schema of a database that shows\nthe relations in the database, their attributes, and primary keys and foreign keys.\n\u2022The relational query languages de\ufb01ne a set of operations that operate on tables and\noutput tables as their results. These operations can be combined to get expressions\nthat express desired queries.\n\u2022The relational algebra provides a set of operations that take one or more relations\nas input and return a relation as an output. Practical query languages such as SQL\nare based on the relational algebra, but they add a number of useful syntactic\nfeatures.\n\u2022The relational algebra de\ufb01nes a set of algebraic operations that operate on tables,\nand output tables as their results. These operations can be combined to get expres-\nsions that express desired queries. The al gebra de\ufb01nes the basic operations used\nwithin relational query languages like SQL.\nReview Terms\n\u2022Table\n\u2022Relation\n\u2022Tuple\n\u2022Attribute\n\u2022Relation instance\n\u2022Domain\n\u2022Atomic domain\n\u2022Null value\n\u2022Database schema\n\u2022Database instance\n\u2022Relation schema\n\u2022Keys\n\u00b0Superkey\n\u00b0Candidate key\n\u00b0Primary key\n\u00b0Primary key constraints\n\u2022Foreign-key constraint\n\u00b0Referencing relation\n\u00b0Referenced relation\u2022Referential integrity constraint\n\u2022Schema diagram\n\u2022Query language types\n\u00b0Imperative\n\u00b0Functional\n\u00b0Declarative\n\u2022Relational algebra\n\u2022Relational-algebra expression\n\u2022Relational-algebra operations\n\u00b0Select\u03c3\n\u00b0Project\u03a0\n\u00b0Cartesian product \u00d7\n\u00b0Join\u22c8\n\u00b0Union\u222a\n\u00b0Set di\ufb00erence \u2212\n\u00b0Set intersection \u2229\n\u00b0Assignment \u2190\n\u00b0Rename\u03c1\n", "88": "60 Chapter 2 Introduction to the Relational Model\nemployee (person\n name ,street ,city)\nworks (person\n name ,company\n name ,salary )\ncompany (company\n name ,city)\nFigure 2.17 Employee database.\nPractice Exercises\n2.1 Consider the employee database of Figure 2.17. What are the appropriate pri-\nmary keys?\n2.2 Consider the foreign-key constraint from the dept\n name attribute of instructor to\nthedepartment relation. Give examples of inserts and deletes to these relations\nthat can cause a violation of the foreign-key constraint.\n2.3 Consider the time\n slotrelation. Given that a particular time slot can meet more\nthan once in a week, explain why dayandstart\n time are part of the primary key\nof this relation, while end\ntime is not.\n2.4 In the instance of instructor shown in Figure 2.1, no two instructors have the\nsame name. From this, can we conclude that name can be used as a superkey\n(or primary key) of instructor ?\n2.5 What is the result of \ufb01rst performing the Cartesian product of student andadvi-\nsor, and then performing a selection operation on the result with the predicate\ns\nid=ID? (Using the symbolic notation of relational algebra, this query can be\nwritten as \u03c3s\nid=ID(student\u00d7advisor ).)\n2.6 Consider the employee database of Figure 2.17. Give an expression in the rela-\ntional algebra to express each of the following queries:\na. Find the name of each employee who lives in city \u201cMiami\u201d.\nb. Find the name of each employee whose salary is greater than $100000.\nc. Find the name of each employee who lives in \u201cMiami\u201d and whose salary\nis greater than $100000.\n2.7 Consider the bank database of Figure 2.18. Give an expression in the relational\nalgebra for each of the following queries:\na. Find the name of each branch located in \u201cChicago\u201d.\nb. Find the IDof each borrower who has a loan in branch \u201cDowntown\u201d.\n", "89": "Practice Exercises 61\nbranch (branch\n name ,branch\n city, assets )\ncustomer (ID,customer\n name ,customer\n street, customer\n city)\nloan (loan\n number ,branch\n name, amount )\nborrower (ID,loan\n number )\naccount (account\n number ,branch\n name, balance )\ndepositor (ID,account\n number )\nFigure 2.18 Bank database.\n2.8 Consider the employee database of Figure 2.17. Give an expression in the rela-\ntional algebra to express each of the following queries:\na. Find the IDand name of each employee who does not work for \u201cBigBank\u201d.\nb. Find the IDand name of each employee who earns at least as much as\nevery employee in the database.\n2.9 Thedivision operator of relational algebra, \u201c \u00f7\u201d, is de\ufb01ned as follows. Let r(R)\nand s(S) be relations, and let S\u2286R; that is, every attribute of schema Sis\nalso in schema R. Given a tuple t,l e t t[S] denote the projection of tuple ton\nthe attributes in S.T h e n r\u00f7sis a relation on schema R\u2212S(that is, on the\nschema containing all attributes of schema Rthat are not in schema S). A tuple\ntis in r\u00f7sif and only if both of two conditions hold:\n\u2022tis in\u03a0R\u2212S(r)\n\u2022For every tuple tsins, there is a tuple trinrsatisfying both of the following:\na.tr[S]=ts[S]\nb.tr[R\u2212S]=t\nGiven the above de\ufb01nition:\na. Write a relational algebra expressi on using the division operator to \ufb01nd\ntheIDs of all students who have taken all Comp. Sci. courses. (Hint:\nproject takes to just IDand course\n id, and generate the set of all Comp.\nSci.course\n ids using a select expression, before doing the division.)\nb. Show how to write the above query in relational algebra, without using\ndivision. (By doing so, you would have shown how to de\ufb01ne the division\noperation using the other relational algebra operations.)\n", "90": "62 Chapter 2 Introduction to the Relational Model\nExercises\n2.10 Describe the di\ufb00erences in meaning between the terms relation and relation\nschema .\n2.11 Consider the advisor relation shown in the schema diagram in Figure 2.9, with\ns\nidas the primary key of advisor . Suppose a student can have more than one\nadvisor. Then, would s\nidstill be a primary key of the advisor relation? If not,\nwhat should the primary key of advisor be?\n2.12 Consider the bank database of Figure 2.18. Assume that branch names and cus-\ntomer names uniquely identify branches and customers, but loans and accounts\ncan be associated with more than one customer.\na. What are the appropriate primary keys?\nb. Given your choice of primary keys, identify appropriate foreign keys.\n2.13 Construct a schema diagram for the bank database of Figure 2.18.\n2.14 Consider the employee database of Figure 2.17. Give an expression in the rela-\ntional algebra to express each of the following queries:\na. Find the ID and name of each employee who works for \u201cBigBank\u201d.\nb. Find the ID, name, and city of residence of each employee who works for\n\u201cBigBank\u201d.\nc. Find the ID, name, street address, and city of residence of each employee\nwho works for \u201cBigBank\u201d and earns more than $10000.\nd. Find the ID and name of each employee in this database who lives in the\nsame city as the company for which she or he works.\n2.15 Consider the bank database of Figure 2.18. Give an expression in the relational\nalgebra for each of the following queries:\na. Find each loan number with a loan amount greater than $10000.\nb. Find the ID of each depositor who has an account with a balance greater\nthan $6000.\nc. Find the ID of each depositor who has an account with a balance greater\nthan $6000 at the \u201cUptown\u201d branch.\n2.16 List two reasons why null values might be introduced into a database.\n2.17 Discuss the relative merits of imperative, functional, and declarative languages.\n2.18 Write the following queries in relational algebra, using the university schema.\na. Find the ID and name of each instructor in the Physics department.\n", "91": "Further Reading 63\nb. Find the ID and name of each instructor in a department located in the\nbuilding \u201cWatson\u201d.\nc. Find the ID and name of each student who has taken at least one course\nin the \u201cComp. Sci.\u201d department.\nd. Find the ID and name of each student who has taken at least one course\nsection in the year 2018.\ne. Find the ID and name of each student who has not taken any course\nsection in the year 2018.\nFurther Reading\nE. F. Codd of the IBM San Jose Research Laboratory proposed the relational model\nin the late 1960s ([Codd (1970)]). In that paper, Codd also introduced the original\nde\ufb01nition of relational algebra. This work led to the prestigious ACM Turing Award to\nCodd in 1981 ([Codd (1982)]).\nAfter E. F. Codd introduced the relational model, an expansive theory developed\naround the relational model pertaining to schema design and the expressive power of\nvarious relational languages. Several classic texts cover relational database theory, in-\ncluding [Maier (1983)] (which is available free, online), and [Abiteboul et al. (1995)].\nCodd\u2019s original paper inspired several research projects that were formed in the\nmid to late 1970s with the goal of constructing practical relational database systems,\nincluding System R at the IBM San Jose Research Laboratory, Ingres at the University\nof California at Berkeley, and Query-by-Example at the IBM T. J. Watson Research\nCenter. The Oracle database was developed commercially at the same time.\nMany relational database products are now commercially available. These include\nIBM\u2019sDB2 and Informix, Oracle, Microsoft SQL Server, and Sybase and HANA from\nSAP. Popular open-source relational database systems include MySQL andPostgre SQL.\nHive and Spark are widely used systems that support parallel execution of queries\nacross large numbers of computers.\nBibliography\n[Abiteboul et al. (1995)] S. Abiteboul, R. Hull, and V. Vianu, Foundations of Databases ,A d -\ndison Wesley (1995).\n[Codd (1970)] E. F. Codd, \u201cA Relational Model for Large Shared Data Banks\u201d, Communi-\ncations of the ACM , Volume 13, Number 6 (1970), pages 377\u2013387.\n[Codd (1982)] E. F. Codd, \u201cThe 1981 ACM Turing Award Lecture: Relational Database: A\nPractical Foundation for Productivity\u201d, Communications of the ACM , Volume 25, Number 2\n(1982), pages 109\u2013117.\n", "92": "64 Chapter 2 Introduction to the Relational Model\n[Maier (1983)] D. Maier, The Theory of Relational Databases , Computer Science Press\n(1983).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "93": "CHAPTER3\nIntroduction to SQL\nIn this chapter, as well as in Chapter 4 and Chapter 5, we study the most widely used\ndatabase query language, SQL.\nAlthough we refer to the SQL language as a \u201cquery language,\u201d it can do much more\nthan just query a database. It can de\ufb01ne the structure of the data, modify data in the\ndatabase, and specify security constraints.\nIt is not our intention to provide a complete users\u2019 guide for SQL. Rather, we present\nSQL\u2019s fundamental constructs and concepts. Individual implementations of SQL may\ndi\ufb00er in details or may support only a subset of the full language.\nWe strongly encourage you to try out the SQL queries that we describe here on an actual\ndatabase. See the Tools section at the end of this chapter for tips on what database sys-\ntems you could use, and how to create the schema, populate sample data, and execute\nyour queries.\n3.1 Overview of the SQL Query Language\nIBM developed the original version of SQL, originally called Sequel, as part of the\nSystem R project in the early 1970s. The Sequel language has evolved since then, and its\nname has changed to SQL (Structured Query Language). Many products now support\ntheSQL language. SQL has clearly established itself as thestandard relational database\nlanguage.\nIn 1986, the American National Standards Institute ( ANSI ) and the International\nOrganization for Standardization ( ISO) published an SQL standard, called SQL-86 .\nANSI p u b l i s h e da ne x t e n d e ds t a n d a r df o r SQL,SQL-89 , in 1989. The next version of the\nstandard was SQL-92 standard, followed by SQL:1999 ,SQL:2003 ,SQL:2006 ,SQL:2008 ,\nSQL:2011 , and most recently SQL:2016 .\nThe SQL language has several parts:\n\u2022Data-de\ufb01nition language (DDL). The SQL DDL provides commands for de\ufb01ning\nrelation schemas, deleting relations, and modifying relation schemas.\n65\n", "94": "66 Chapter 3 Introduction to SQL\n\u2022Data-manipulation language (DML ). The SQL DML provides the ability to query\ninformation from the database and to insert tuples into, delete tuples from, and\nmodify tuples in the database.\n\u2022Integrity .T h e SQL DDL includes commands for specifying integrity constraints\nthat the data stored in the database must satisfy. Updates that violate integrity\nconstraints are disallowed.\n\u2022View de\ufb01nition .T h e SQL DDL includes commands for de\ufb01ning views.\n\u2022Transaction control .SQL includes commands for specifying the beginning and end\npoints of transactions.\n\u2022Embedded SQL and dynamic SQL. Embedded and dynamic SQL de\ufb01ne how SQL\nstatements can be embedded within general-purpose programming languages, such\nas C, C++, and Java.\n\u2022Authorization .T h e SQL DDL includes commands for specifying access rights to\nrelations and views.\nIn this chapter, we present a survey of basic DML and the DDL features of SQL.\nFeatures described here have been part of the SQL standard since SQL-92 .\nIn Chapter 4, we provide a more detailed coverage of the SQL query language,\nincluding (a) various join expressions, (b) views, (c) transactions, (d) integrity con-\nstraints, (e) type system, and (f) authorization.\nIn Chapter 5, we cover more advanced features of the SQL language, including (a)\nmechanisms to allow accessing SQL from a programming language, (b) SQL functions\nand procedures, (c) triggers, (d) recursive queries, (e) advanced aggregation features,\nand (f) several features designed for data analysis.\nAlthough most SQL implementations support the standard features we describe\nhere, there are di\ufb00erences between implementations. Most implementations support\nsome nonstandard features while omitting support for some of the more advanced and\nmore recent features. In case you \ufb01nd that some language features described here do not\nwork on the database system that you use, consult the user manuals for your database\nsystem to \ufb01nd exactly what features it supports.\n3.2 SQL Data Definition\nThe set of relations in a database are speci\ufb01ed using a data-de\ufb01nition language ( DDL ).\nThe SQL DDL allows speci\ufb01cation of not only a set of relations, but also information\nabout each relation, including:\n\u2022The schema for each relation.\n\u2022The types of values associated with each attribute.\n", "95": "3.2 SQL Data De\ufb01nition 67\n\u2022The integrity constraints.\n\u2022The set of indices to be maintained for each relation.\n\u2022The security and authorization information for each relation.\n\u2022The physical storage structure of each relation on disk.\nWe discuss here basic schema de\ufb01nition and basic types; we defer discussion of the\nother SQL DDL features to Chapter 4 and Chapter 5.\n3.2.1 Basic Types\nThe SQL standard supports a variety of built-in types, including:\n\u2022char(n): A \ufb01xed-length character string with user-speci\ufb01ed length n. The full form,\ncharacter , can be used instead.\n\u2022varchar (n): A variable-length character string with user-speci\ufb01ed maximum length\nn.T h ef u l lf o r m , character varying ,i se q u i v a l e n t .\n\u2022int: An integer (a \ufb01nite subset of the integers that is machine dependent). The full\nform, integer , is equivalent.\n\u2022smallint : A small integer (a machine-dependent subset of the integer type).\n\u2022numeric (p,d): A \ufb01xed-point number with user-speci\ufb01ed precision. The number\nconsists of pdigits (plus a sign), and dof the pdigits are to the right of the decimal\npoint. Thus, numeric (3,1) allows 44 .5 to be stored exactly, but neither 444 .5n o r\n0.32 can be stored exactly in a \ufb01eld of this type.\n\u2022real, double precision : Floating-point and double-precision \ufb02oating-point numbers\nwith machine-dependent precision.\n\u2022\ufb02oat(n): A \ufb02oating-point number with precision of at least ndigits.\nAdditional types are covered in Section 4.5.\nEach type may include a special value called the nullvalue. A null value indicates\nan absent value that may exist but be unknown or that may not exist at all. In certain\ncases, we may wish to prohibit null values from being entered, as we shall see shortly.\nThe char data type stores \ufb01xed-length strings. Consider, for example, an attribute\nAof type char(10). If we stored a string \u201cAvi\u201d in this attribute, seven spaces are ap-\npended to the string to make it 10 characters long. In contrast, if attribute Bwere of\ntype varchar (10), and we stored \u201cAvi\u201d in attribute B, no spaces would be added. When\ncomparing two values of type char, if they are of di\ufb00erent lengths, extra spaces are au-\ntomatically attached to the shorter one to make them the same size before comparison.\nWhen comparing a char type with a varchar type, one may expect extra spaces to\nbe added to the varchar type to make the lengths equal, before comparison; however,\nthis may or may not be done, depending on the database system. As a result, even if\n", "96": "68 Chapter 3 Introduction to SQL\nthe same value \u201cAvi\u201d is stored in the attributes AandBabove, a comparison A=Bmay\nreturn false. We recommend you always use the varchar type instead of the char type\nto avoid these problems.\nSQL also provides the nvarchar type to store multilingual data using the Unicode\nrepresentation. However, many databases allow Unicode (in the UTF-8 representation)\nto be stored even in varchar types.\n3.2.2 Basic Schema Definition\nWe de\ufb01ne an SQL relation by using the create table command. The following command\ncreates a relation department in the database:\ncreate table department\n(dept\n name varchar (20),\nbuilding varchar (15),\nbudget numeric (12,2),\nprimary key (dept\n name ));\nThe relation created above has three attributes, dept\n name , which is a character string\nof maximum length 20, building , which is a character string of maximum length 15,\nandbudget , which is a number with 12 digits in total, two of which are after the deci-\nmal point. The create table command also speci\ufb01es that the dept\n name attribute is the\nprimary key of the department relation.\nThe general form of the create table command is:\ncreate table r\n(A1D1,\nA2D2,\n...,\nAnDn,\n\u27e8integrity-constraint1\u27e9,\n\u2026,\n\u27e8integrity-constraintk\u27e9);\nwhere ri st h en a m eo ft h er e l a t i o n ,e a c h Aii st h en a m eo fa na t t r i b u t ei nt h es c h e m ao f\nrelation r,a n d Diis the domain of attribute Ai;t h a ti s , Dispeci\ufb01es the type of attribute\nAialong with optional constraints that restrict the set of allowed values for Ai.\nThe semicolon shown at the end of the create table statements, as well as at the end\nof other SQL statements later in this chapter, is optional in many SQL implementations.\nSQL supports a number of di\ufb00erent integrity constraints. In this section, we discuss\nonly a few of them:\n\u2022primary key (Aj1,Aj2,\u2026,Ajm): The primary-key speci\ufb01cation says that attributes\nAj1,Aj2,\u2026,Ajmform the primary key for the relation. The primary-key attributes\n", "97": "3.2 SQL Data De\ufb01nition 69\nare required to be nonnull and unique ; that is, no tuple can have a null value for\na primary-key attribute, and no two tuples in the relation can be equal on all the\nprimary-key attributes. Although the primary-key speci\ufb01cation is optional, it is gen-\nerally a good idea to specify a primary key for each relation.\n\u2022foreign key (Ak1,Ak2,\u2026,Akn)references s:T h e foreign key speci\ufb01cation says that\nthe values of attributes ( Ak1,Ak2,\u2026,Akn) for any tuple in the relation must corre-\nspond to values of the primary key attributes of some tuple in relation s.\nFigure 3.1 presents a partial SQL DDL de\ufb01nition of the university database we\nuse in the text. The de\ufb01nition of the course table has a declaration \u201c foreign key\n(dept\n name )references department \u201d. This foreign-key declaration speci\ufb01es that for\neach course tuple, the department name speci\ufb01ed in the tuple must exist in the pri-\nmary key attribute ( dept\n name )o ft h e department relation. Without this constraint,\nit is possible for a course to specify a nonexistent department name. Figure 3.1\nalso shows foreign-key constraints on tables section ,instructor and teaches .S o m e\ndatabase systems, including MySQL, require an alternative syntax, \u201c foreign key\n(dept\n name )references department (dept\n name )\u201d, where the referenced attributes\nin the referenced table are listed explicitly.\n\u2022not null :T h e not null constraint on an attribute speci\ufb01es that the null value is not\nallowed for that attribute; in other words, the constraint excludes the null value\nfrom the domain of that attribute. For example, in Figure 3.1, the not null con-\nstraint on the name attribute of the instructor relation ensures that the name of an\ninstructor cannot be null.\nMore details on the foreign-key constraint, as well as on other integrity constraints that\nthecreate table command may include, are provided later, in Section 4.4.\nSQL prevents any update to the database that violates an integrity constraint. For\nexample, if a newly inserted or modi\ufb01ed tuple in a relation has null values for any\nprimary-key attribute, or if the tuple has the same value on the primary-key attributes\nas does another tuple in the relation, SQL \ufb02ags an error and prevents the update. Sim-\nilarly, an insertion of a course tuple with a dept\n name value that does not appear in\nthedepartment relation would violate the foreign-key constraint on course ,a n d SQL\nprevents such an insertion from taking place.\nA newly created relation is empty initially. Inserting tuples into a relation, updating\nthem, and deleting them are done by data manipulation statements insert ,update ,a n d\ndelete , which are covered in Section 3.9.\nTo remove a relation from an SQL database, we use the drop table command.\nThe drop table command deletes all information about the dropped relation from the\ndatabase. The command\ndrop table r;\nis a more drastic action than\n", "98": "70 Chapter 3 Introduction to SQL\ncreate table department\n(dept\n name varchar (20),\nbuilding varchar (15),\nbudget numeric (12,2),\nprimary key (dept\n name ));\ncreate table course\n(course\n id varchar (7),\ntitle varchar (50),\ndept\n name varchar (20),\ncredits numeric (2,0),\nprimary key (course\n id),\nforeign key (dept\n name )references department );\ncreate table instructor\n(ID varchar (5),\nname varchar (20) not null ,\ndept\n name varchar (20),\nsalary numeric (8,2),\nprimary key (ID),\nforeign key (dept\n name )references department );\ncreate table section\n(course\n id varchar (8),\nsec\nid varchar (8),\nsemester varchar (6),\nyear numeric (4,0),\nbuilding varchar (15),\nroom\n number varchar (7),\ntime\n slot\nid varchar (4),\nprimary key (course\n id,sec\nid,semester ,year),\nforeign key (course\n id)references course );\ncreate table teaches\n(ID varchar (5),\ncourse\n id varchar (8),\nsec\nid varchar (8),\nsemester varchar (6),\nyear numeric (4,0),\nprimary key (ID,course\n id,sec\nid,semester ,year),\nforeign key (course\n id,sec\nid,semester ,year)references section ,\nforeign key (ID)references instructor );\nFigure 3.1 SQL data definition for part of the university database.\n", "99": "3.3 Basic Structure of SQL Queries 71\ndelete from r;\nThe latter retains relation r, but deletes all tuples in r.T h ef o r m e rd e l e t e sn o to n l ya l l\ntuples of r,b u ta l s ot h es c h e m af o r r.A f t e r ris dropped, no tuples can be inserted into\nrunless it is re-created with the create table command.\nWe use the alter table command to add attributes to an existing relation. All tuples\nin the relation are assigned nullas the value for the new attribute. The form of the alter\ntable command is\nalter table raddAD;\nwhere ris the name of an existing relation, Ais the name of the attribute to be added,\nandDis the type of the added attribute. We can drop attributes from a relation by the\ncommand\nalter table rdrop A;\nwhere ris the name of an existing relation, and Ais the name of an attribute of the\nrelation. Many database systems do not support dropping of attributes, although they\nwill allow an entire table to be dropped.\n3.3 Basic Structure of SQL Queries\nThe basic structure of an SQL query consists of three clauses: select ,from,a n d where .\nA query takes as its input the relations listed in the from clause, operates on them as\nspeci\ufb01ed in the where and select clauses, and then produces a relation as the result. We\nintroduce the SQL syntax through examples, and we describe the general structure of\nSQL queries later.\n3.3.1 Queries on a Single Relation\nLet us consider a simple query using our university example, \u201cFind the names of all in-\nstructors.\u201d Instructor names are found in the instructor relation, so we put that relation\nin the from clause. The instructor\u2019s name appears in the name attribute, so we put that\nin the select clause.\nselect name\nfrom instructor ;\nThe result is a relation consisting of a single attribute with the heading name .I ft h e\ninstructor relation is as shown in Figure 2.1, then the relation that results from the\npreceding query is shown in Figure 3.2.\n", "100": "72 Chapter 3 Introduction to SQL\nname\nSrinivasan\nWu\nMozart\nEinstein\nEl Said\nGold\nKatz\nCali\ufb01eri\nSingh\nCrick\nBrandt\nKim\nFigure 3.2 Result of \u201c select name from instructor \u201d.\nNow consider another query, \u201cFind the department names of all instructors,\u201d\nwhich can be written as:\nselect dept\n name\nfrom instructor ;\nSince more than one instructor can belong to a department, a department name could\nappear more than once in the instructor relation. The result of the above query is a\nrelation containing the department names, shown in Figure 3.3.\nIn the formal, mathematical de\ufb01nition of the relational model, a relation is a set.\nThus, duplicate tuples would never appear in relations. In practice, duplicate elimina-\ntion is time-consuming. Therefore, SQL allows duplicates in database relations as well\nas in the results of SQL expressions.1Thus, the preceding SQL query lists each depart-\nment name once for every tuple in which it appears in the instructor relation.\nIn those cases where we want to force the elimination of duplicates, we insert the\nkeyword distinct after select . We can rewrite the preceding query as:\nselect distinct dept\n name\nfrom instructor ;\nif we want duplicates removed. The result of the above query would contain each de-\npartment name at most once.\n1Any database relation whose schema includes a primary-ke y declaration cannot contain duplicate tuples, since they\nwould violate the primary-key constraint.\n", "101": "3.3 Basic Structure of SQL Queries 73\ndept\n name\nComp. Sci.\nFinance\nMusic\nPhysics\nHistory\nPhysics\nComp. Sci.\nHistory\nFinance\nBiology\nComp. Sci.\nElec. Eng.\nFigure 3.3 Result of \u201c select dept\n name from instructor \u201d.\nSQL allows us to use the keyword allto specify explicitly that duplicates are not\nremoved:\nselect all dept\n name\nfrom instructor ;\nSince duplicate retention is the default, we shall not use allin our examples. To ensure\nthe elimination of duplicates in the results of our example queries, we shall use distinct\nwhenever it is necessary.\nThe select clause may also contain arithmetic expressions involving the operators\n+,\u2212,\u2217, and / operating on constants or attributes of tuples. For example, the query:\nselect ID,name ,dept\n name ,salary *1 . 1\nfrom instructor ;\nreturns a relation that is the same as the instructor relation, except that the attribute\nsalary is multiplied by 1.1. This shows what would result if we gave a 10% raise to each\ninstructor; note, however, that it does not result in any change to the instructor relation.\nSQL also provides special data types, such as various forms of the date type, and\nallows several arithmetic functions to operate on these types. We discuss this further\nin Section 4.5.1.\nThe where clause allows us to select only those rows in the result relation of the from\nclause that satisfy a speci\ufb01ed predicate. Consider the query \u201cFind the names of all in-\nstructors in the Computer Science department who have salary greater than $70,000.\u201d\nThis query can be written in SQL as:\n", "102": "74 Chapter 3 Introduction to SQL\nname\nKatz\nBrandt\nFigure 3.4 Result of \u201cFind the names of all instructors in the Computer Science\ndepartment who have salary greater than $70,000.\u201d\nselect name\nfrom instructor\nwhere dept\n name='Comp. Sci.' andsalary >70000;\nIf the instructor relation is as shown in Figure 2.1, then the relation that results from\nthe preceding query is shown in Figure 3.4.\nSQL allows the use of the logical connectives and,or,a n d notin the where clause.\nThe operands of the logical connectives can be expressions involving the comparison\noperators <,<=,>,>=,=,a n d <>.SQL allows us to use the comparison operators\nto compare strings and arithmetic expressions, as well as special types, such as date\ntypes.\nWe shall explore other features of where clause predicates later in this chapter.\n3.3.2 Queries on Multiple Relations\nSo far our example queries were on a single relation. Queries often need to access\ninformation from multiple relations. We now study how to write such queries.\nAs an example, suppose we want to answer the query \u201cRetrieve the names of all\ninstructors, along with their department names and department building name.\u201d\nLooking at the schema of the relation instructor ,w er e a l i z et h a tw ec a ng e tt h e\ndepartment name from the attribute dept\n name , but the department building name is\npresent in the attribute building of the relation department . To answer the query, each\ntuple in the instructor relation must be matched with the tuple in the department relation\nwhose dept\n name value matches the dept\n name value of the instructor tuple.\nInSQL, to answer the above query, we list the relations that need to be accessed\nin the from clause and specify the matching condition in the where clause. The above\nquery can be written in SQL as\nselect name ,instructor .dept\n name ,building\nfrom instructor ,department\nwhere instructor .dept\n name=department .dept\n name ;\nIf the instructor and department relations are as shown in Figure 2.1 and Figure 2.5\nrespectively, then the result of this query is shown in Figure 3.5.\nNote that the attribute dept\n name occurs in both the relations instructor and de-\npartment , and the relation name is used as a pre\ufb01x (in instructor .dept\n name ,a n d de-\n", "103": "3.3 Basic Structure of SQL Queries 75\nname\n dept\n name\n building\nSrinivasan\n Comp. Sci.\n Taylor\nWu\n Finance\n Painter\nMozart\n Music\n Packard\nEinstein\n Physics\n Watson\nEl Said\n History\n Painter\nGold\n Physics\n Watson\nKatz\n Comp. Sci.\n Taylor\nCali\ufb01eri\n History\n Painter\nSingh\n Finance\n Painter\nCrick\n Biology\n Watson\nBrandt\n Comp. Sci.\n Taylor\nKim\n Elec. Eng.\n Taylor\nFigure 3.5 The result of \u201cRetrieve the names of all instructors, along with their\ndepartment names and department building name.\u201d\npartment .dept\n name ) to make clear to which attribute we are referring. In contrast, the\nattributes name andbuilding appear in only one of the relations and therefore do not\nneed to be pre\ufb01xed by the relation name.\nThis naming convention requires that the relations that are present in the from\nclause have distinct names. This requirement causes problems in some cases, such as\nwhen information from two di\ufb00erent tuples in the same relation needs to be combined.\nIn Section 3.4.1, we see how to avoid these problems by using the rename operation.\nWe now consider the general case of SQL queries involving multiple relations. As\nwe have seen earlier, an SQL query can contain three types of clauses, the select clause,\nthefrom clause, and the where clause. The role of each clause is as follows:\n\u2022The select clause is used to list the attributes desired in the result of a query.\n\u2022The from clause is a list of the relations to be accessed in the evaluation of the\nquery.\n\u2022The where clause is a predicate involving attributes of the relation in the from\nclause.\nAt y p i c a l SQL query has the form:\nselect A1,A2,\u2026,An\nfrom r1,r2,\u2026,rm\nwhere P;\n", "104": "76 Chapter 3 Introduction to SQL\nEach Airepresents an attribute, and each riar e l a t i o n . Pis a predicate. If the where\nclause is omitted, the predicate Pistrue.\nAlthough the clauses must be written in the order select ,from,where , the easiest\nway to understand the operations speci\ufb01ed by the query is to consider the clauses in\noperational order: \ufb01rst from,t h e n where ,a n dt h e n select .2\nThe from clause by itself de\ufb01nes a Cartesian product of the relations listed in the\nclause. It is de\ufb01ned formally in terms of relational algebra, but it can also be understood\nas an iterative process that generates tuples for the result relation of the from clause.\nfor each tuple t1inrelation r1\nfor each tuple t2inrelation r2\n\u2026\nfor each tuple tminrelation rm\nConcatenate t1,t2,\u2026,tminto a single tuple t\nAdd tinto the result relation\nThe result relation has all attributes from all the relations in the from clause. Since the\nsame attribute name may appear in both riandrj, as we saw earlier, we pre\ufb01x the name\nof the relation from which the attribute originally came, before the attribute name.\nFor example, the relation schema for the Cartesian product of relations instructor\nandteaches is:\n(instructor .ID,instructor .name ,instructor .dept\n name ,instructor .salary ,\nteaches .ID,teaches .course\n id,teaches .sec\nid,teaches .semester ,teaches .year)\nWith this schema, we can distinguish instructor .IDfrom teaches .ID. For those attributes\nthat appear in only one of the two schemas, we shall usually drop the relation-name\npre\ufb01x. This simpli\ufb01cation does not lead to any ambiguity. We can then write the relation\nschema as:\n(instructor .ID,name ,dept\n name ,salary ,teaches .ID,course\n id,sec\nid,semester ,year)\nTo illustrate, consider the instructor relation in Figure 2.1 and the teaches relation\nin Figure 2.7. Their Cartesian product is s hown in Figure 3.6, which includes only a\nportion of the tuples that make up the Cartesian product result.\nThe Cartesian product by itself combines tuples from instructor and teaches that\nare unrelated to each other. Each tuple in instructor is combined with every tuple in\nteaches , even those that refer to a di\ufb00erent instructor. The result can be an extremely\nlarge relation, and it rarely makes sense to create such a Cartesian product.\n2In practice, SQLmay convert the expression into an equivalent form that can be processed more e\ufb03ciently. However,\nwe shall defer concerns about e\ufb03cie ncy to Chapter 15 and Chapter 16.\n", "105": "3.3 Basic Structure of SQL Queries 77\ninstructor .ID\n name\n dept\n name\n salary\n teaches .ID\n course\n id\n sec\nid\n semester\n year\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-101\n 1\n Fall\n 2017\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-315\n 1\n Spring\n 2018\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 10101\n CS-347\n 1\n Fall\n 2017\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 12121\n FIN-201\n 1\n Spring\n 2018\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 15151\n MU-199\n 1\n Spring\n 2018\n10101\n Srinivasan\n Comp. Sci.\n 65000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n12121\n Wu\n Finance\n 90000\n 10101\n CS-101\n 1\n Fall\n 2017\n12121\n Wu\n Finance\n 90000\n 10101\n CS-315\n 1\n Spring\n 2018\n12121\n Wu\n Finance\n 90000\n 10101\n CS-347\n 1\n Fall\n 2017\n12121\n Wu\n Finance\n 90000\n 12121\n FIN-201\n 1\n Spring\n 2018\n12121\n Wu\n Finance\n 90000\n 15151\n MU-199\n 1\n Spring\n 2018\n12121\n Wu\n Finance\n 90000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n15151\n Mozart\n Music\n 40000\n 10101\n CS-101\n 1\n Fall\n 2017\n15151\n Mozart\n Music\n 40000\n 10101\n CS-315\n 1\n Spring\n 2018\n15151\n Mozart\n Music\n 40000\n 10101\n CS-347\n 1\n Fall\n 2017\n15151\n Mozart\n Music\n 40000\n 12121\n FIN-201\n 1\n Spring\n 2018\n15151\n Mozart\n Music\n 40000\n 15151\n MU-199\n 1\n Spring\n 2018\n15151\n Mozart\n Music\n 40000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n22222\n Einstein\n Physics\n 95000\n 10101\n CS-101\n 1\n Fall\n 2017\n22222\n Einstein\n Physics\n 95000\n 10101\n CS-315\n 1\n Spring\n 2018\n22222\n Einstein\n Physics\n 95000\n 10101\n CS-347\n 1\n Fall\n 2017\n22222\n Einstein\n Physics\n 95000\n 12121\n FIN-201\n 1\n Spring\n 2018\n22222\n Einstein\n Physics\n 95000\n 15151\n MU-199\n 1\n Spring\n 2018\n22222\n Einstein\n Physics\n 95000\n 22222\n PHY-101\n 1\n Fall\n 2017\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\n ...\nFigure 3.6 The Cartesian product of the instructor relation with the teaches relation.\nInstead, the predicate in the where clause is used to restrict the combinations cre-\nated by the Cartesian product to those that are meaningful for the desired answer. We\nwould likely want a query involving instructor and teaches to combine a particular tu-\npletininstructor with only those tuples in teaches that refer to the same instructor to\nwhich trefers. That is, we wish only to match teaches tuples with instructor tuples that\nhave the same IDvalue. The following SQL query ensures this condition and outputs\nthe instructor name and course identi\ufb01ers from such matching tuples.\nselect name ,course\n id\nfrom instructor ,teaches\nwhere instructor .ID=teaches .ID;\n", "106": "78 Chapter 3 Introduction to SQL\nname\n course\n id\nSrinivasan\n CS-101\nSrinivasan\n CS-315\nSrinivasan\n CS-347\nWu\n FIN-201\nMozart\n MU-199\nEinstein\n PHY-101\nEl Said\n HIS-351\nKatz\n CS-101\nKatz\n CS-319\nCrick\n BIO-101\nCrick\n BIO-301\nBrandt\n CS-190\nBrandt\n CS-190\nBrandt\n CS-319\nKim\n EE-181\nFigure 3.7 Result of \u201cFor all instructors in the university who have taught some\ncourse, find their names and the course IDof all courses they taught.\u201d\nNote that the preceding query outputs only instructors who have taught some course.\nInstructors who have not taught any course are not output; if we wish to output such\ntuples, we could use an operation called the outer join ,w h i c hi sd e s c r i b e di nS e c t i o n\n4.1.3.\nIf the instructor relation is as shown in Figure 2.1 and the teaches relation is as\nshown in Figure 2.7, then the relation that results from the preceding query is shown\nin Figure 3.7. Observe that instructors Gold, Cali\ufb01eri, and Singh, who have not taught\nany course, do not appear in Figure 3.7.\nIf we wished to \ufb01nd only instructor names and course identi\ufb01ers for instructors\nin the Computer Science department, we could add an extra predicate to the where\nclause, as shown below.\nselect name ,course\n id\nfrom instructor ,teaches\nwhere instructor .ID=teaches .IDandinstructor .dept\n name='Comp. Sci.';\nNote that since the dept\n name attribute occurs only in the instructor relation, we could\nhave used just dept\n name ,i n s t e a do f instructor .dept\n name in the above query.\nIn general, the meaning of an SQL query can be understood as follows:\n1.Generate a Cartesian product of the relations listed in the from clause.\n2.Apply the predicates speci\ufb01ed in the where clause on the result of Step 1.\n", "107": "3.4 Additional Basic Operations 79\n3.For each tuple in the result of Step 2, output the attributes (or results of expres-\nsions) speci\ufb01ed in the select clause.\nThis sequence of steps helps make clear what the result of an SQL query should be, not\nhow it should be executed. A real implementation of SQL would not execute the query\nin this fashion; it would instead optimize evaluation by generating (as far as possible)\nonly elements of the Cartesian product that satisfy the where clause predicates. We\nstudy such implementation techniques in Chapter 15 and Chapter 16.\nWhen writing queries, you should be careful to include appropriate where clause\nconditions. If you omit the where clause condition in the preceding SQL query, it will\noutput the Cartesian product, which could be a huge relation. For the example instruc-\ntorrelation in Figure 2.1 and the example teaches relation in Figure 2.7, their Cartesian\nproduct has 12 \u221713=156 tuples\u2014more than we can show in the text! To make mat-\nters worse, suppose we have a more realistic number of instructors than we show in\nour sample relations in the \ufb01gures, say 200 instructors. Let\u2019s assume each instructor\nteaches three courses, so we have 600 tuples in the teaches relation. Then the preceding\niterative process generates 200 \u2217600=120,000 tuples in the result.\n3.4 Additional Basic Operations\nA number of additional basic operations are supported in SQL.\n3.4.1 The Rename Operation\nConsider again the query that we used earlier:\nselect name ,course\n id\nfrom instructor ,teaches\nwhere instructor .ID=teaches .ID;\nThe result of this query is a relation with the following attributes:\nname ,course\n id\nThe names of the attributes in the result are derived from the names of the attributes\nin the relations in the from clause.\nWe cannot, however, always derive names in this way, for several reasons: First, two\nrelations in the from clause may have attributes with the same name, in which case an\nattribute name is duplicated in the result. Second, if we use an arithmetic expression in\ntheselect clause, the resultant attribute does not have a name. Third, even if an attribute\nname can be derived from the base relations as in the preceding example, we may want\nto change the attribute name in the result. Hence, SQL provides a way of renaming the\nattributes of a result relation. It uses the as clause ,t a k i n gt h ef o r m :\n", "108": "80 Chapter 3 Introduction to SQL\nNote 3.1 SQL AND MULTISET RELATIONAL ALGEBRA - PART 1\nThere is a close connection between relational algebra operations and SQL op-\nerations. One key di\ufb00erence is that, unlike the relational algebra, SQL allows du-\nplicates. The SQL standard de\ufb01nes how many copies of each tuple are there in\nthe output of a query, which depends, in turn, on how many copies of tuples are\npresent in the input relations.\nTo model this behavior of SQL, a version of relational algebra, called the mul-\ntiset relational algebra , is de\ufb01ned to work on multisets: sets that may contain dupli-\ncates. The basic operations in the multiset relational algebra are de\ufb01ned as follows:\n1.If there are c1copies of tuple t1inr1,a n d t1satis\ufb01es selection \u03c3\u03b8,t h e n\nthere are c1copies of t1in\u03c3\u03b8(r1).\n2.For each copy of tuple t1inr1, there is a copy of tuple \u03a0A(t1)i n\u03a0A(r1),\nwhere\u03a0A(t1) denotes the projection of the single tuple t1.\n3.If there are c1copies of tuple t1inr1andc2copies of tuple t2inr2,t h e r e\narec1\u2217c2copies of the tuple t1.t2inr1\u00d7r2.\nFor example, suppose that relations r1with schema ( A,B)a n d r2with schema\n(C) are the following multisets: r1={(1,a), (2, a)}andr2={(2), (3), (3) }.T h e n\n\u03a0B(r1)w o u l db e {(a), (a)},w h e r e a s \u03a0B(r1)\u00d7r2would be:\n{(a,2 ) ,( a,2 ) ,( a,3 ) ,( a,3 ) ,( a,3 ) ,( a,3 )}\nNow consider a basic SQL query of the form:\nselect A1,A2,\u2026,An\nfrom r1,r2,\u2026,rm\nwhere P\nEach Airepresents an attribute, and each riar e l a t i o n . Pis a predicate. If the where\nclause is omitted, the predicate Pistrue. The query is equivalent to the multiset\nrelational-algebra expression:\n\u03a0A1,A2,\u2026,An(\u03c3P(r1\u00d7r2\u00d7\u22ef\u00d7rm))\nThe relational algebra select operation corresponds to the SQL where clause,\nnot to the SQL select clause; the di\ufb00erence in meaning is an unfortunate historical\nfact. We discuss the representation of more complex SQL queries in Note 3.2 on\npage 97.\nThe relational-algebra representation of SQL queries helps to formally de\ufb01ne\nthe meaning of the SQL program. Further, database systems typically translate\nSQL queries into a lower-level representation based on relational algebra, and they\nperform query optimization and query evaluation using this representation.\n", "109": "3.4 Additional Basic Operations 81\nold-name asnew-name\nThe asclause can appear in both the select and from clauses.3\nFor example, if we want the attribute name name to be replaced with the name\ninstructor\n name ,w ec a nr e w r i t et h ep r e c e d i n gq u e r ya s :\nselect name asinstructor\n name ,course\n id\nfrom instructor ,teaches\nwhere instructor .ID=teaches .ID;\nThe asclause is particularly useful in renaming relations. One reason to rename\na relation is to replace a long relation name with a shortened version that is more\nconvenient to use elsewhere in the query. To illustrate, we rewrite the query \u201cFor all\ninstructors in the university who have taught some course, \ufb01nd their names and the\ncourse IDof all courses they taught.\u201d\nselect T.name ,S.course\n id\nfrom instructor asT,teaches asS\nwhere T.ID=S.ID;\nAnother reason to rename a relation is a case where we wish to compare tuples\nin the same relation. We then need to take the Cartesian product of a relation with\nitself and, without renaming, it becomes impossible to distinguish one tuple from the\nother. Suppose that we want to write the query \u201cFind the names of all instructors whose\nsalary is greater than at least one instructor in the Biology department.\u201d We can write\ntheSQL expression:\nselect distinct T.name\nfrom instructor asT,instructor asS\nwhere T.salary >S.salary andS.dept\n name='Biology';\nObserve that we could not use the notation instructor.salary , since it would not be clear\nwhich reference to instructor is intended.\nIn the above query, TandScan be thought of as copies of the relation instructor ,\nbut more precisely, they are declared as aliases, that is, as alternative names, for the\nrelation instructor .A ni d e n t i \ufb01 e r ,s u c ha s TandS, that is used to rename a relation is\nreferred to as a correlation name in the SQL standard, but it is also commonly referred\nto as a table alias ,o ra correlation variable ,o ra tuple variable .\n3Early versions of SQL did not include the keyword as. As a result, some implementations of SQL, notably Oracle,\ndo not permit the keyword asin the from clause. In Oracle, \u201c old-name asnew-name \u201d is written instead as \u201c old-name\nnew-name \u201di nt h e from clause. The keyword asis permitted for renaming attributes in the select clause, but it is optional\na n dm a yb eo m i t t e di nO r a c l e .\n", "110": "82 Chapter 3 Introduction to SQL\nNote that a better way to phrase the previous query in English would be \u201cFind the\nnames of all instructors who earn more than the lowest paid instructor in the Biology\ndepartment.\u201d Our original wording \ufb01ts more closely with the SQL that we wrote, but\nthe latter wording is more intuitive, and it can in fact be expressed directly in SQL as\nwe shall see in Section 3.8.2.\n3.4.2 String Operations\nSQL speci\ufb01es strings by enclosing them in single quotes, for example, 'Computer'. A\nsingle quote character that is part of a string can be speci\ufb01ed by using two single quote\ncharacters; for example, the string \u201cIt\u2019s right\u201d can be speci\ufb01ed by 'It''s right'.\nThe SQL standard speci\ufb01es that the equality operation on strings is case sensitive;\nas a result, the expression \u201c'comp. sci.' = 'Comp. Sci.'\u201d evaluates to false. However,\nsome database systems, such as MySQL and SQL S erver, do not distinguish uppercase\nfrom lowercase when matching strings; as a result, \u201c'comp. sci.' = 'Comp. Sci.'\u201d would\nevaluate to true on these systems. This default behavior can, however, be changed,\neither at the database level or at the level of speci\ufb01c attributes.\nSQL also permits a variety of functions on character strings, such as concatenating\n(using \u201c\u2225\u201d), extracting substrings, \ufb01nding the length of strings, converting strings to\nuppercase (using the function upper (s)w h e r e sis a string) and lowercase (using the\nfunction lower (s)), removing spaces at the end of the string (using trim(s)), and so\non. There are variations on the exact set of string functions supported by di\ufb00erent\ndatabase systems. See your database system\u2019s manual for more details on exactly what\nstring functions it supports.\nPattern matching can be performed on strings using the operator like.W ed e s c r i b e\npatterns by using two special characters:\n\u2022Percent (%): The % character matches any substring.\n\u2022Underscore (\n ): The\n character matches any character.\nPatterns are case sensitive;4that is, uppercase characters do not match lowercase char-\nacters, or vice versa. To illustrate pattern matching, we consider the following examples:\n\u2022'Intro%' matches any string beginning with \u201cIntro\u201d.\n\u2022'%Comp%' matches any string containing \u201cComp\u201d as a substring, for example,\n'Intro. to Computer Science', and 'Computational Biology'.\n\u2022'\n' matches any string of exactly three characters.\n\u2022'\n%' matches any string of at least three characters.\n4Except for MySQL,o rw i t ht h e ilike operator in Postgre SQL, where patterns are case insensitive.\n", "111": "3.4 Additional Basic Operations 83\nSQL expresses patterns by using the likecomparison operator. Consider the query\n\u201cFind the names of all departments whose building name includes the substring 'Wat-\nson'.\u201d This query can be written as:\nselect dept\n name\nfrom department\nwhere building like'%Watson%';\nFor patterns to include the special pattern characters (that is, % and\n ),SQL allows the\nspeci\ufb01cation of an escape character. The escape character is used immediately before\na special pattern character to indicate that the special pattern character is to be treated\nlike a normal character. We de\ufb01ne the escape character for a likecomparison using the\nescape keyword. To illustrate, consider the following patterns, which use a backslash\n(\u2216) as the escape character:\n\u2022like'ab\u2216%cd%' escape '\u2216' matches all strings beginning with \u201cab%cd\u201d.\n\u2022like'ab\u2216\u2216cd%' escape '\u2216' matches all strings beginning with \u201cab \u2216cd\u201d.\nSQL allows us to search for mismatches instead of matches by using the not like com-\nparison operator. Some implementations provide variants of the likeoperation that do\nnot distinguish lower- and uppercase.\nSome SQL implementations, notably Postgre SQL,o \ufb00 e ra similar to operation that\nprovides more powerful pattern matching than the likeoperation; the syntax for speci-\nfying patterns is similar to that used in Unix regular expressions.\n3.4.3 Attribute Specification in the Select Clause\nThe asterisk symbol \u201c * \u201d can be used in the select clause to denote \u201call attributes.\u201d\nThus, the use of instructor .* in the select clause of the query:\nselect instructor .*\nfrom instructor ,teaches\nwhere instructor .ID=teaches .ID;\nindicates that all attributes of instructor are to be selected. A select clause of the form\nselect * indicates that all attributes of the result relation of the from clause are selected.\n3.4.4 Ordering the Display of Tuples\nSQL o\ufb00ers the user some control over the order in which tuples in a relation are dis-\nplayed. The order by clause causes the tuples in the result of a query to appear in sorted\norder. To list in alphabetic order all instructors in the Physics department, we write:\n", "112": "84 Chapter 3 Introduction to SQL\nselect name\nfrom instructor\nwhere dept\n name='Physics'\norder by name ;\nBy default, the order by clause lists items in ascending order. To specify the sort order,\nwe may specify desc for descending order or ascfor ascending order. Furthermore,\nordering can be performed on multiple attributes. Suppose that we wish to list the\nentire instructor relation in descending order of salary .I fs e v e r a li n s t r u c t o r sh a v et h e\nsame salary, we order them in ascending order by name. We express this query in SQL\nas follows:\nselect *\nfrom instructor\norder by salary desc,name asc;\n3.4.5 Where-Clause Predicates\nSQL includes a between comparison operator to simplify where clauses that specify\nthat a value be less than or equal to some value and greater than or equal to some other\nvalue. If we wish to \ufb01nd the names of instructors with salary amounts between $90,000\nand $100,000, we can use the between comparison to write:\nselect name\nfrom instructor\nwhere salary between 90000 and100000;\ninstead of:\nselect name\nfrom instructor\nwhere salary <=100000 andsalary >=90000;\nSimilarly, we can use the not between comparison operator.\nSQL permits us to use the notation ( v1,v2,\u2026,vn)t od e n o t eat u p l eo fa r i t y ncon-\ntaining values v1,v2,\u2026,vn; the notation is called a row constructor .T h ec o m p a r i s o n\noperators can be used on tuples, and the ordering is de\ufb01ned lexicographically. For ex-\nample, ( a1,a2)<=(b1,b2)i st r u ei f a1<=b1anda2<=b2; similarly, the two tuples\nare equal if all their attributes are equal. Thus, the SQL query:\nselect name ,course\n id\nfrom instructor ,teaches\nwhere instructor .ID=teaches .IDanddept\n name='Biology';\n", "113": "3.5 Set Operations 85\ncourse\n id\nCS-101\nCS-347\nPHY-101\nFigure 3.8 The c1relation, listing courses taught in Fall 2017.\ncan be rewritten as follows:5\nselect name ,course\n id\nfrom instructor ,teaches\nwhere (instructor .ID,dept\n name )=(teaches .ID, 'Biology');\n3.5 Set Operations\nThe SQL operations union ,intersect ,a n d except operate on relations and correspond to\nthe mathematical set operations \u222a,\u2229,a n d\u2212. We shall now construct queries involving\ntheunion ,intersect ,a n d except operations over two sets.\n\u2022The set of all courses taught in the Fall 2017 semester:\nselect course\n id\nfrom section\nwhere semester='Fall' andyear=2017;\n\u2022The set of all courses taught in the Spring 2018 semester:\nselect course\n id\nfrom section\nwhere semester='Spring' andyear=2018;\nIn our discussion that follows, we shall refer to the relations obtained as the result of the\npreceding queries as c1and c2, respectively, and show the results when these queries\nare run on the section relation of Figure 2.6 in Figure 3.8 and Figure 3.9. Observe that\nc2contains two tuples corresponding to course\n idCS-319, since two sections of the\ncourse were o\ufb00ered in Spring 2018.\n5Although it is part of the SQL-92 standard, some SQLimplementations, notably Oracle, do not support this syntax.\n", "114": "86 Chapter 3 Introduction to SQL\ncourse\n id\nCS-101\nCS-315\nCS-319\nCS-319\nFIN-201\nHIS-351\nMU-199\nFigure 3.9 The c2relation, listing courses taught in Spring 2018.\n3.5.1 The Union Operation\nTo \ufb01nd the set of all courses taught either in Fall 2017 or in Spring 2018, or both, we\nwrite the following query. Note that the parentheses we include around each select -\nfrom-where statement below are optional but useful for ease of reading; some databases\ndo not allow the use of the parentheses, in which case they may be dropped.\n(select course\n id\nfrom section\nwhere semester='Fall' andyear=2017)\nunion\n(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nThe union operation automatically eliminates duplicates, unlike the select clause. Thus,\nusing the section relation of Figure 2.6, where two sections of CS-319 are o\ufb00ered in\nSpring 2018, and a section of CS-101 is o\ufb00ered in the Fall 2017 as well as in the Spring\n2018 semesters, CS-101 and CS-319 appear only once in the result, shown in Figure\n3.10.\nIf we want to retain all duplicates, we must write union all in place of union :\n(select course\n id\nfrom section\nwhere semester='Fall' andyear=2017)\nunion all\n(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nThe number of duplicate tuples in the result is equal to the total number of duplicates\nthat appear in both c1andc2. So, in the above query, each of CS-319 and CS-101 would\n", "115": "3.5 Set Operations 87\ncourse\n id\nCS-101\nCS-315\nCS-319\nCS-347\nFIN-201\nHIS-351\nMU-199\nPHY-101\nFigure 3.10 The result relation for c1union c2.\nbe listed twice. As a further example, if it were the case that four sections of ECE-101\nwere taught in the Fall 2017 semester and two sections of ECE-101 were taught in the\nSpring 2018 semester, then there would be six tuples with ECE-101 in the result.\n3.5.2 The Intersect Operation\nTo \ufb01nd the set of all courses taught in both the Fall 2017 and Spring 2018, we write:\n(select course\n id\nfrom section\nwhere semester='Fall' andyear=2017)\nintersect\n(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nThe result relation, shown in Figure 3.11, contains only one tuple with CS-101. The in-\ntersect operation automatically eliminates duplicates.6For example, if it were the case\nthat four sections of ECE-101 were taught in the Fall 2017 semester and two sections of\nECE-101 were taught in the Spring 2018 semester, then there would be only one tuple\nwith ECE-101 in the result.\ncourse\n id\nCS-101\nFigure 3.11 The result relation for c1intersect c2.\n6MySQL does not implement the intersect operation; a work-around is to use subqueries as discussed in Section 3.8.1.\n", "116": "88 Chapter 3 Introduction to SQL\ncourse\n id\nCS-347\nPHY-101\nFigure 3.12 The result relation for c1except c2.\nIf we want to retain all duplicates, we must write intersect all in place of intersect :\n(select course\n id\nfrom section\nwhere semester='Fall' andyear=2017)\nintersect all\n(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nThe number of duplicate tuples that appear in the result is equal to the minimum num-\nber of duplicates in both c1andc2. For example, if four sections of ECE-101 were taught\nin the Fall 2017 semester and two sections of ECE-101 were taught in the Spring 2018\nsemester, then there would be two tuples with ECE-101 in the result.\n3.5.3 The Except Operation\nTo \ufb01nd all courses taught in the Fall 2017 semester but not in the Spring 2018 semester,\nwe write:\n(select course\n id\nfrom section\nwhere semester='Fall' andyear=2017)\nexcept\n(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nThe result of this query is shown in Figure 3.12. Note that this is exactly relation c1\nof Figure 3.8 except that the tuple for CS-101 does not appear. The except operation7\noutputs all tuples from its \ufb01rst input that do not occur in the second input; that is, it\n7Some SQL implementations, notably Oracle, use the keyword minus in place of except , while Oracle 12c uses the\nkeywords multiset except in place of except all .MySQL does not implement it at all; a work-around is to use subqueries\nas discussed in Section 3.8.1.\n", "117": "3.6 Null Values 89\nperforms set di\ufb00erence. The operation automatically eliminates duplicates in the inputs\nbefore performing set di\ufb00erence. For example, if four sections of ECE-101 were taught\nin the Fall 2017 semester and two sections of ECE-101 were taught in the Spring 2018\nsemester, the result of the except operation would not have any copy of ECE-101.\nIf we want to retain duplicates, we must write except all in place of except :\n(select course\n id\nfrom section\nwhere semester='Fall' andyear=2017)\nexcept all\n(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nThe number of duplicate copies of a tuple in the result is equal to the number of dupli-\ncate copies in c1minus the number of duplicate copies in c2, provided that the di\ufb00er-\nence is positive. Thus, if four sections of ECE-101 were taught in the Fall 2017 semester\nand two sections of ECE-101 were taught in Spring 2018, then there are two tuples with\nECE-101 in the result. If, however, there were two or fewer sections of ECE-101 in the\nFall 2017 semester and two sections of ECE-101 in the Spring 2018 semester, there is\nno tuple with ECE-101 in the result.\n3.6 Null Values\nNull values present special problems in relational operations, including arithmetic op-\nerations, comparison operations, and set operations.\nThe result of an arithmetic expression (involving, for example, +,\u2212,\u2217,o r\u2215)i sn u l l\nif any of the input values is null. For example, if a query has an expression r.A+5, and\nr.Ais null for a particular tuple, then the expression result must also be null for that\ntuple.\nComparisons involving nulls are more o f a problem. For example, consider the\ncomparison \u201c1 <null\u201d. It would be wrong to say this is true since we do not know\nwhat the null value represents. But it woul d likewise be wrong to claim this expression\nis false; if we did, \u201c not(1<null)\u201d would evaluate to true, which does not make sense.\nSQL therefore treats as unknown the result of any comparison involving a null value\n(other than predicates is null and is not null , which are described later in this section).\nThis creates a third logical value in addition to trueandfalse.\nSince the predicate in a where clause can involve Boolean operations such as and,\nor,a n d noton the results of comparisons, the de\ufb01nitions of the Boolean operations are\nextended to deal with the value unknown .\n", "118": "90 Chapter 3 Introduction to SQL\n\u2022and:T h er e s u l to f true andunknown isunknown ,false andunknown isfalse, while\nunknown andunknown isunknown .\n\u2022or:T h er e s u l to f true orunknown istrue,false orunknown isunknown , while un-\nknown orunknown isunknown .\n\u2022not:T h er e s u l to f notunknown isunknown .\nYou can verify that if r.Ais null, then \u201c1 <r.A\u201da sw e l la s\u201c not(1<r.A)\u201d evaluate\nto unknown.\nIf the where clause predicate evaluates to either false orunknown for a tuple, that\ntuple is not added to the result.\nSQL uses the special keyword nullin a predicate to test for a null value. Thus, to\n\ufb01nd all instructors who appear in the instructor relation with null values for salary ,w e\nwrite:\nselect name\nfrom instructor\nwhere salary is null ;\nThe predicate is not null succeeds if the value on which it is applied is not null.\nSQL allows us to test whether the result of a comparison is unknown, rather than\ntrue or false, by using the clauses is unknown and is not unknown .8For example,\nselect name\nfrom instructor\nwhere salary >10000 is unknown ;\nWhen a query uses the select distinct clause, duplicate tuples must be eliminated.\nFor this purpose, when comparing values of corresponding attributes from two tuples,\nthe values are treated as identical if either both are non-null and equal in value, or both\nare null. Thus, two copies of a tuple, such as {('A',null), ('A',null) }, are treated as being\nidentical, even if some of the attributes have a null value. Using the distinct clause then\nretains only one copy of such identical tuples. Note that the treatment of null above is\ndi\ufb00erent from the way nulls are treated in predicates, where a comparison \u201cnull=null\u201d\nwould return unknown, rather than true.\nThe approach of treating tuples as identical if they have the same values for all\nattributes, even if some of the values are null, is also used for the set operations union,\nintersection, and except.\n8The is unknown and is not unknown constructs are not supported by several databases.\n", "119": "3.7 Aggregate Functions 91\n3.7 Aggregate Functions\nAggregate functions are functions that take a collection (a set or multiset) of values as\ninput and return a single value. SQL o\ufb00ers \ufb01ve standard built-in aggregate functions:9\n\u2022Average: avg\n\u2022Minimum: min\n\u2022Maximum: max\n\u2022Total: sum\n\u2022Count: count\nThe input to sumand avgmust be a collection of numbers, but the other operators can\noperate on collections of nonnumeric data types, such as strings, as well.\n3.7.1 Basic Aggregation\nConsider the query \u201cFind the average salary of instructors in the Computer Science\ndepartment.\u201d We write this query as follows:\nselect avg (salary )\nfrom instructor\nwhere dept\n name='Comp. Sci.';\nThe result of this query is a relation with a sing le attribute containing a single tuple with\na numerical value corresponding to the average salary of instructors in the Computer\nScience department. The database system may give an awkward name to the result\nrelation attribute that is generated by aggre gation, consisting of the text of the expres-\nsion; however, we can give a meaningful name to the attribute by using the asclause as\nfollows:\nselect avg (salary )asavg\nsalary\nfrom instructor\nwhere dept\n name='Comp. Sci.';\nIn the instructor relation of Figure 2.1, the salaries in the Computer Science de-\npartment are $75,000, $65,000, and $92,000. The average salary is $232,000 \u22153=\n$77,333.33.\nRetaining duplicates is important in computing an average. Suppose the Computer\nScience department adds a fourth instructor whose salary happens to be $75,000. If du-\n9Most implementations of SQLo\ufb00er a number of additional aggregate functions.\n", "120": "92 Chapter 3 Introduction to SQL\nplicates were eliminated, we would obtain the wrong answer ($232,000 \u22154=$58,000)\nrather than the correct answer of $76,750.\nThere are cases where we must eliminate duplicates before computing an aggre-\ngate function. If we do want to eliminate duplicates, we use the keyword distinct in the\naggregate expression. An example arises in the query \u201cFind the total number of instruc-\ntors who teach a course in the Spring 2018 semester.\u201d In this case, an instructor counts\nonly once, regardless of the number of course sections that the instructor teaches. The\nrequired information is contained in the relation teaches , and we write this query as\nfollows:\nselect count (distinct ID)\nfrom teaches\nwhere semester='Spring' andyear=2018;\nBecause of the keyword distinct preceding ID, even if an instructor teaches more than\none course, she is counted only once in the result.\nWe use the aggregate function count frequently to count the number of tuples in a\nrelation. The notation for this function in SQL iscount (*). Thus, to \ufb01nd the number\nof tuples in the course relation, we write\nselect count (*)\nfrom course ;\nSQL does not allow the use of distinct with count (*). It is legal to use distinct with\nmax and min, even though the result does not change. We can use the keyword allin\nplace of distinct to specify duplicate retention, but since allis the default, there is no\nneed to do so.\n3.7.2 Aggregation with Grouping\nThere are circumstances where we would like to apply the aggregate function not only\nto a single set of tuples, but also to a group of sets of tuples; we specify this in SQL\nusing the group by clause. The attribute or attributes given in the group by clause are\nused to form groups. Tuples with the same value on all attributes in the group by clause\nare placed in one group.\nAs an illustration, consider the query \u201cFind the average salary in each department.\u201d\nWe write this query as follows:\nselect dept\n name ,avg(salary )asavg\nsalary\nfrom instructor\ngroup by dept\n name ;\n", "121": "3.7 Aggregate Functions 93\nID\n name\n dept\n name\n salary\n76766\n Crick\n Biology\n 72000\n45565\n Katz\n Comp. Sci.\n 75000\n10101\n Srinivasan\n Comp. Sci.\n 65000\n83821\n Brandt\n Comp. Sci.\n 92000\n98345\n Kim\n Elec. Eng.\n 80000\n12121\n Wu\n Finance\n 90000\n76543\n Singh\n Finance\n 80000\n32343\n El Said\n History\n 60000\n58583\n Cali\ufb01eri\n History\n 62000\n15151\n Mozart\n Music\n 40000\n33456\n Gold\n Physics\n 87000\n22222\n Einstein\n Physics\n 95000\nFigure 3.13 Tuples of the instructor relation, grouped by the dept\n name attribute.\nFigure 3.13 shows the tuples in the instructor relation grouped by the dept\n name\nattribute, which is the \ufb01rst step in computing the query result. The speci\ufb01ed aggregate\nis computed for each group, and the result of the query is shown in Figure 3.14.\nIn contrast, consider the query \u201cFind the average salary of all instructors.\u201d We\nwrite this query as follows:\nselect avg (salary )\nfrom instructor ;\nIn this case the group by clause has been omitted, so the entire relation is treated as a\nsingle group.\ndept\n name\n avg\nsalary\nBiology\n 72000\nComp. Sci.\n 77333\nElec. Eng.\n 80000\nFinance\n 85000\nHistory\n 61000\nMusic\n 40000\nPhysics\n 91000\nFigure 3.14 The result relation for the query \u201cFind the average salary in each\ndepartment\u201d.\n", "122": "94 Chapter 3 Introduction to SQL\nAs another example of aggregation on groups of tuples, consider the query \u201cFind\nthe number of instructors in each department who teach a course in the Spring 2018\nsemester.\u201d Information about which instructors teach which course sections in which\nsemester is available in the teaches relation. However, this information has to be joined\nwith information from the instructor relation to get the department name of each in-\nstructor. Thus, we write this query as follows:\nselect dept\n name ,count (distinct ID)asinstr\n count\nfrom instructor ,teaches\nwhere instructor .ID=teaches .IDand\nsemester='Spring' andyear=2018\ngroup by dept\n name ;\nThe result is shown in Figure 3.15.\nWhen an SQL query uses grouping, it is important to ensure that the only attributes\nthat appear in the select statement without being aggregated are those that are present\nin the group by clause. In other words, any attribute that is not present in the group by\nclause may appear in the select clause only as an argument to an aggregate function,\notherwise the query is treated as erroneous. For example, the following query is erro-\nneous since IDdoes not appear in the group by clause, and yet it appears in the select\nclause without being aggregated:\n/* erroneous query */\nselect dept\n name ,ID,avg(salary )\nfrom instructor\ngroup by dept\n name ;\nIn the preceding query, each instructor in a particular group (de\ufb01ned by dept\n name )\ncan have a di\ufb00erent ID, and since only one tuple is output for each group, there is no\nunique way of choosing which IDvalue to output. As a result, such cases are disallowed\nbySQL.\nThe preceding query also illustrates a comment written in SQL by enclosing text\nin \u201c/* */\u201d; the same comment could have also been written as \u201c\u2013\u2013 erroneous query\u201d.\ndept\n name\n instr\n count\nComp. Sci.\n 3\nFinance\n 1\nHistory\n 1\nMusic\n 1\nFigure 3.15 The result relation for the query \u201cFind the number of instructors in each\ndepartment who teach a course in the Spring 2018 semester.\u201d\n", "123": "3.7 Aggregate Functions 95\ndept\n name\n avg\nsalary\nPhysics\n 91000\nElec. Eng.\n 80000\nFinance\n 85000\nComp. Sci.\n 77333\nBiology\n 72000\nHistory\n 61000\nFigure 3.16 The result relation for the query \u201cFind the average salary of instructors\nin those departments where the average salary is more than $42,000.\u201d\n3.7.3 The Having Clause\nAt times, it is useful to state a condition that applies to groups rather than to tuples. For\nexample, we might be interested in only those departments where the average salary of\nthe instructors is more than $42,000. This condition does not apply to a single tuple;\nrather, it applies to each group constructed by the group by clause. To express such\na query, we use the having clause of SQL.SQL applies predicates in the having clause\nafter groups have been formed, so aggregate functions may be used in the having clause.\nWe express this query in SQL as follows:\nselect dept\n name ,avg(salary )asavg\nsalary\nfrom instructor\ngroup by dept\n name\nhaving avg (salary )>42000;\nThe result is shown in Figure 3.16.\nAs was the case for the select clause, any attribute that is present in the having\nclause without being aggregated must appear in the group by clause, otherwise the\nquery is erroneous.\nThe meaning of a query containing aggregation, group by ,o rhaving clauses is de-\n\ufb01ned by the following sequence of operations:\n1.As was the case for queries without aggregation, the from clause is \ufb01rst evaluated\nto get a relation.\n2.If awhere clause is present, the predicate in the where clause is applied on the\nresult relation of the from clause.\n3.Tuples satisfying the where predicate are then placed into groups by the group\nbyclause if it is present. If the group by clause is absent, the entire set of tuples\nsatisfying the where predicate is treated as being in one group.\n", "124": "96 Chapter 3 Introduction to SQL\n4.The having clause, if it is present, is applied to each group; the groups that do not\nsatisfy the having clause predicate are removed.\n5.The select clause uses the remaining groups to generate tuples of the result of the\nquery, applying the aggregate functions to get a single result tuple for each group.\nTo illustrate the use of both a having clause and a where clause in the same query,\nwe consider the query \u201cFor each course section o\ufb00ered in 2017, \ufb01nd the average total\ncredits ( tot\ncred) of all students enrolled in the section, if the section has at least 2\nstudents.\u201d\nselect course\n id,semester ,year,sec\nid,avg(tot\ncred)\nfrom student ,takes\nwhere student .ID=takes .IDandyear=2017\ngroup by course\n id,semester ,year,sec\nid\nhaving count (ID)>=2;\nNote that all the required information for the preceding query is available from the\nrelations takes and student , and that although the query pertains to sections, a join\nwith section is not needed.\n3.7.4 Aggregation with Null and Boolean Values\nNull values, when they exist, complicate the processing of aggregate operators. For\nexample, assume that some tuples in the instructor relation have a null value for salary .\nConsider the following query to total all salary amounts:\nselect sum (salary )\nfrom instructor ;\nThe values to be summed in the preceding query include null values, since we assumed\nthat some tuples have a null value for salary . Rather than say that the overall sum is\nitself null,t h e SQL standard says that the sumoperator should ignore nullvalues in its\ninput.\nIn general, aggregate functions treat nulls according to the following rule: All aggre-\ngate functions except count (*) ignore null values in their input collection. As a result\nof null values being ignored, the collection of values may be empty. The count of an\nempty collection is de\ufb01ned to be 0, and all other aggregate operations return a value\nof null when applied on an empty collection. The e\ufb00ect of null values on some of the\nmore complicated SQL constructs can be subtle.\nABoolean data type that can take values true,false,a n d unknown was introduced\ninSQL:1999 . The aggregate functions some and every can be applied on a collection of\nBoolean values, and compute the disjunction ( or) and conjunction ( and), respectively,\nof the values.\n", "125": "3.7 Aggregate Functions 97\nNote 3.2 SQL AND MULTISET RELATIONAL ALGEBRA - PART 2\nAs we saw earlier in Note 3.1 on page 80, the SQL select ,from,a n d where clauses\ncan be represented in the multiset relational algebra, using the multiset versions of\nthe select, project, and Cartesian product operations.\nThe relational algebra union, intersection, and set di\ufb00erence ( \u222a,\u2229,a n d\u2212)\noperations can also be extended to the multiset relational algebra in a similar way,\nfollowing the corresponding de\ufb01nitions of union all ,intersect all ,a n d except all in\nSQL, which we saw in Section 3.5; the SQL union ,intersect ,a n d except correspond\nto the set version of \u222a,\u2229,a n d\u2212.\nThe extended relational algebra aggregate operation \u03b3permits the use of aggre-\ngate functions on relation attributes. (The symbol \ue233is also used to represent the\naggregate operation and was used in earlier editions of the book.) The operation\ndept\n name\u03b3average (salary )(instructor )g r o u p st h e instructor relation on the dept\n name at-\ntribute and computes the average salary for each group, as we saw earlier in Section\n3.7.2. The subscript on the left side may be omitted, resulting in the entire input\nrelation being in a single group. Thus, \u03b3average (salary )(instructor )c o m p u t e st h ea v e r -\nage salary of all instructors. The aggregated values do not have an attribute name;\nt h e yc a nb eg i v e nan a m ee i t h e rb yu s i n gt h er e n a m eo p e r a t o r \u03c1or for convenience\nusing the following syntax:\ndept\n name\u03b3average (salary )asavg\nsalary(instructor )\nMore complex SQL queries can also be rewritten in relational algebra. For\nexample, the query:\nselect A1,A2,sum(A3)\nfrom r1,r2,\u2026,rm\nwhere P\ngroup by A1,A2having count (A4)>2\nis equivalent to:\nt1\u2190\u03c3P(r1\u00d7r2\u00d7\u22ef\u00d7rm)\n\u03a0A1,A2,SumA3 (\u03c3countA4 >2(A1,A2\u03b3sum( A3)asSumA3 ,count (A4)ascountA4 (t1))\nJoin expressions in the from clause can be written using equivalent join expres-\nsions in relational algebra; we leave the details as an exercise for the reader. How-\never, subqueries in the where orselect clause cannot be rewritten into relational\nalgebra in such a straightforward manner, since there is no relational algebra oper-\nation equivalent to the subquery construct. Extensions of relational algebra have\nbeen proposed for this task, but they are beyond the scope of this book.\n", "126": "98 Chapter 3 Introduction to SQL\n3.8 Nested Subqueries\nSQL provides a mechanism for nesting subqueries. A subquery is a select -from-where\nexpression that is nested within another query. A common use of subqueries is to per-\nform tests for set membership, make set comparisons, and determine set cardinality\nby nesting subqueries in the where clause. We study such uses of nested subqueries\nin the where clause in Section 3.8.1 through Section 3.8.4. In Section 3.8.5, we study\nnesting of subqueries in the from clause. In Section 3.8.7, we see how a class of sub-\nqueries called scalar subqueries can appear wherever an expression returning a value\ncan occur.\n3.8.1 Set Membership\nSQL allows testing tuples for membership in a relation. The inconnective tests for set\nmembership, where the set is a collection of values produced by a select clause. The\nnot in connective tests for the absence of set membership.\nAs an illustration, reconsider the query \u201cFind all the courses taught in the both the\nFall 2017 and Spring 2018 semesters.\u201d Earlier, we wrote such a query by intersecting\ntwo sets: the set of courses taught in Fall 2017 and the set of courses taught in Spring\n2018. We can take the alternative approach of \ufb01nding all courses that were taught in\nFall 2017 and that are also members of the set of courses taught in Spring 2018. This\nformulation generates the same results as the previous one did, but it leads us to write\nour query using the inconnective of SQL. We begin by \ufb01nding all courses taught in\nSpring 2018, and we write the subquery:\n(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018)\nWe then need to \ufb01nd those courses that were taught in the Fall 2017 and that appear\nin the set of courses obtained in the subquery. We do so by nesting the subquery in the\nwhere clause of an outer query. The resulting query is:\nselect distinct course\n id\nfrom section\nwhere semester='Fall' andyear=2017 and\ncourse\n idin(select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nNote that we need to use distinct here because the intersect operation removes dupli-\ncates by default.\nThis example shows that it is possible to write the same query several ways in SQL.\nThis \ufb02exibility is bene\ufb01cial, since it allows a user to think about the query in the way\n", "127": "3.8 Nested Subqueries 99\nthat seems most natural. We shall see that there is a substantial amount of redundancy\ninSQL.\nWe use the not in construct in a way similar to the inconstruct. For example, to \ufb01nd\nall the courses taught in the Fall 2017 semester but not in the Spring 2018 semester,\nwhich we expressed earlier using the except operation, we can write:\nselect distinct course\n id\nfrom section\nwhere semester='Fall' andyear=2017 and\ncourse\n idnot in (select course\n id\nfrom section\nwhere semester='Spring' andyear=2018);\nThe inand not in operators can also be used on enumerated sets. The following\nquery selects the names of instructors whose names are neither \u201cMozart\u201d nor \u201cEin-\nstein\u201d.\nselect distinct name\nfrom instructor\nwhere name not in ('Mozart', 'Einstein');\nIn the preceding examples, we tested membership in a one-attribute relation. It is\nalso possible to test for membership in an arbitrary relation in SQL.F o re x a m p l e ,w e\ncan write the query \u201c\ufb01nd the total number of (distinct) students who have taken course\nsections taught by the instructor with ID110011\u201d as follows:\nselect count (distinct ID)\nfrom takes\nwhere (course\n id,sec\nid,semester ,year)in(select course\n id,sec\nid,semester ,year\nfrom teaches\nwhere teaches .ID= '10101');\nN o t e ,h o w e v e r ,t h a ts o m e SQL implementations do not support the row construc-\ntion syntax \u201c( course\n id,sec\nid,semester ,year)\u201d used above. We will see alternative ways\nof writing this query in Section 3.8.3.\n3.8.2 Set Comparison\nAs an example of the ability of a nested subquery to compare sets, consider the query\n\u201cFind the names of all instructors whose salary is greater than at least one instructor\nin the Biology department.\u201d In Section 3 .4.1, we wrote this query as follows:\n", "128": "100 Chapter 3 Introduction to SQL\nselect distinct T.name\nfrom instructor asT,instructor asS\nwhere T.salary >S.salary andS.dept\n name='Biology';\nSQLdoes, however, o\ufb00er an alternative style for writing the preceding query. The phrase\n\u201cgreater than at least one\u201d is represented in SQL by>some . This construct allows us\nto rewrite the query in a form that resembles closely our formulation of the query in\nEnglish.\nselect name\nfrom instructor\nwhere salary >some (select salary\nfrom instructor\nwhere dept\n name='Biology');\nThe subquery:\n(select salary\nfrom instructor\nwhere dept\n name='Biology')\ngenerates the set of all salary values of all instructors in the Biology department. The >\nsome comparison in the where clause of the outer select is true if the salary value of the\ntuple is greater than at least one member of the set of all salary values for instructors\nin Biology.\nSQL also allows <some ,<=some ,>=some ,=some ,a n d <>some comparisons.\nAs an exercise, verify that =some is identical to in,w h e r e a s <>some isnotthe same\nasnot in .10\nNow we modify our query slightly. Let us \ufb01nd the names of all instructors that\nhave a salary value greater than that of each instructor in the Biology department. The\nconstruct >allcorresponds to the phrase \u201cgreater than all.\u201d Using this construct, we\nwrite the query as follows:\nselect name\nfrom instructor\nwhere salary >all(select salary\nfrom instructor\nwhere dept\n name='Biology');\nAs it does for some ,SQLalso allows <all,<=all,>=all,=all,a n d <>allcomparisons.\nAs an exercise, verify that <>allis identical to not in ,w h e r e a s =allisnotthe same as\nin.\n10The keyword anyis synonymous to some inSQL. Early versions of SQL allowed only any. Later versions added the\nalternative some to avoid the linguistic ambiguity of the word anyin English.\n", "129": "3.8 Nested Subqueries 101\nAs another example of set comparisons, consider the query \u201cFind the departments\nthat have the highest average salary.\u201d We begin by writing a query to \ufb01nd all average\nsalaries, and then nest it as a subquery of a larger query that \ufb01nds those departments\nf o rw h i c ht h ea v e r a g es a l a r yi sg r e a t e rt h a no re q u a lt oa l la v e r a g es a l a r i e s :\nselect dept\n name\nfrom instructor\ngroup by dept\n name\nhaving avg (salary )>=all(select avg (salary )\nfrom instructor\ngroup by dept\n name );\n3.8.3 Test for Empty Relations\nSQL includes a feature for testing whether a subquery has any tuples in its result. The\nexists construct returns the value trueif the argument subquery is nonempty. Using the\nexists construct, we can write the query \u201cFind all courses taught in both the Fall 2017\nsemester and in the Spring 2018 semester\u201d in still another way:\nselect course\n id\nfrom section asS\nwhere semester='Fall' andyear=2017 and\nexists (select *\nfrom section asT\nwhere semester='Spring' andyear=2018 and\nS.course\n id=T.course\n id);\nThe above query also illustrates a feature of SQL where a correlation name from\nan outer query ( Sin the above query), can be used in a subquery in the where clause.\nA subquery that uses a correlation name from an outer query is called a correlated\nsubquery .\nIn queries that contain subqueries, a scoping rule applies for correlation names.\nIn a subquery, according to the rule, it is legal to use only correlation names de\ufb01ned\nin the subquery itself or in any query that contains the subquery. If a correlation name\nis de\ufb01ned both locally in a subquery and globally in a containing query, the local def-\ninition applies. This rule is analogous to the usual scoping rules used for variables in\nprogramming languages.\nWe can test for the nonexistence of tuples in a subquery by using the not exists\nconstruct. We can use the not exists construct to simulate the set containment (that\nis, superset) operation: We can write \u201crelation Acontains relation B\u201da s\u201c not exists (B\nexcept A).\u201d (Although it is not part of the current SQL standards, the contains opera-\ntor was present in some early relational systems.) To illustrate the not exists operator,\n", "130": "102 Chapter 3 Introduction to SQL\nconsider the query \u201cFind all students who have taken all courses o\ufb00ered in the Biology\ndepartment.\u201d Using the except construct, we can write the query as follows:\nselect S.ID,S.name\nfrom student asS\nwhere not exists ((select course\n id\nfrom course\nwhere dept\n name='Biology')\nexcept\n(select T.course\n id\nfrom takes asT\nwhere S.ID=T.ID));\nHere, the subquery:\n(select course\n id\nfrom course\nwhere dept\n name='Biology')\n\ufb01nds the set of all courses o\ufb00ered in the Biology department. The subquery:\n(select T.course\n id\nfrom takes asT\nwhere S.ID=T.ID)\n\ufb01nds all the courses that student S.IDhas taken. Thus, the outer select takes each stu-\ndent and tests whether the set of all courses that the student has taken contains the set\nof all courses o\ufb00ered in the Biology department.\nWe saw in Section 3.8.1, an SQL query to \u201c\ufb01nd the total number of (distinct) stu-\ndents who have taken course sections taught by the instructor with ID110011\u201d. That\nquery used a tuple constructor syntax that is not supported by some databases. An\nalternative way to write the query, using the exists construct, is as follows:\nselect count (distinct ID)\nfrom takes\nwhere exists (select course\n id,sec\nid,semester ,year\nfrom teaches\nwhere teaches .ID= '10101'\nandtakes .course\n id=teaches .course\n id\nandtakes .sec\nid=teaches .sec\nid\nandtakes .semester =teaches .semester\nandtakes .year =teaches .year\n);\n", "131": "3.8 Nested Subqueries 103\n3.8.4 Test for the Absence of Duplicate Tuples\nSQL includes a Boolean function for testing whether a subquery has duplicate tuples\nin its result. The unique construct11returns the value true if the argument subquery\ncontains no duplicate tuples. Using the unique construct, we can write the query \u201cFind\nall courses that were o\ufb00ered at most once in 2017\u201d as follows:\nselect T.course\n id\nfrom course asT\nwhere unique (select R.course\n id\nfrom section asR\nwhere T.course\n id=R.course\n idand\nR.year=2017);\nNote that if a course were not o\ufb00ered in 2017, the subquery would return an empty\nresult, and the unique predicate would evaluate to true on the empty set.\nAn equivalent version of this query not using the unique construct is:\nselect T.course\n id\nfrom course asT\nwhere 1>=(select count (R.course\n id)\nfrom section asR\nwhere T.course\n id=R.course\n idand\nR.year=2017);\nWe can test for the existence of duplicate tuples in a subquery by using the not\nunique construct. To illustrate this construct, consider the query \u201cFind all courses that\nwere o\ufb00ered at least twice in 2017\u201d as follows:\nselect T.course\n id\nfrom course asT\nwhere not unique (select R.course\n id\nfrom section asR\nwhere T.course\n id=R.course\n idand\nR.year=2017);\nFormally, the unique test on a relation is de\ufb01ned to fail if and only if the relation\ncontains two distinct tuples t1and t2such that t1=t2.S i n c et h et e s t t1=t2fails if\nany of the \ufb01elds of t1ort2are null, it is possible for unique to be true even if there are\nmultiple copies of a tuple, as long as at least one of the attributes of the tuple is null.\n11This construct is not yet widely implemented.\n", "132": "104 Chapter 3 Introduction to SQL\n3.8.5 Subqueries in the From Clause\nSQL allows a subquery expression to be used in the from clause. The key concept ap-\nplied here is that any select -from-where expression returns a relation as a result and,\ntherefore, can be inserted into another select -from-where anywhere that a relation can\nappear.\nConsider the query \u201cFind the average instructors\u2019 salaries of those departments\nwhere the average salary is greater than $42,000.\u201d We wrote this query in Section 3.7\nby using the having clause. We can now rewrite this query, without using the having\nclause, by using a subquery in the from clause, as follows:\nselect dept\n name ,avg\nsalary\nfrom (select dept\n name ,avg(salary )asavg\nsalary\nfrom instructor\ngroup by dept\n name )\nwhere avg\nsalary >42000;\nThe subquery generates a relation consisting of the names of all departments and their\ncorresponding average instructors\u2019 salaries. The attributes of the subquery result can\nbe used in the outer query, as can be seen in the above example.\nNote that we do not need to use the having clause, since the subquery in the from\nclause computes the average salary, and the predicate that was in the having clause\nearlier is now in the where clause of the outer query.\nWe can give the subquery result relation a name, and rename the attributes, using\ntheasclause, as illustrated below.\nselect dept\n name ,avg\nsalary\nfrom (select dept\n name ,avg(salary )\nfrom instructor\ngroup by dept\n name )\nasdept\n avg(dept\n name ,avg\nsalary )\nwhere avg\nsalary >42000;\nThe subquery result relation is named dept\n avg, with the attributes dept\n name andavg\nsalary .\nNested subqueries in the from clause are supported by most but not all SQL imple-\nmentations. Note that some SQL implementations, notably MySQL and Postgre SQL,\nrequire that each subquery relation in the from clause must be given a name, even if the\nname is never referenced; Oracle allows a subquery result relation to be given a name\n(with the keyword asomitted) but does not allow renaming of attributes of the relation.\nAn easy workaround for that is to do the attribute renaming in the select clause of the\nsubquery; in the above query, the select clause of the subquery would be replaced by\nselect dept\n name ,avg(salary )asavg\nsalary\n", "133": "3.8 Nested Subqueries 105\nand\n\u201casdept\n avg(dept\n name ,avg\nsalary )\u201d\nw o u l db er e p l a c e db y\n\u201casdept\n avg\u201d.\nAs another example, suppose we wish to \ufb01nd the maximum across all departments\nof the total of all instructors\u2019 salaries in each department. The having clause does not\nhelp us in this task, but we can write this query easily by using a subquery in the from\nclause, as follows:\nselect max (tot\nsalary )\nfrom (select dept\n name ,sum(salary )\nfrom instructor\ngroup by dept\n name )asdept\n total (dept\n name ,tot\nsalary );\nWe note that nested subqueries in the from clause cannot use correlation variables\nfrom other relations in the same from c l a u s e .H o w e v e r ,t h e SQL standard, starting with\nSQL:2003 , allows a subquery in the from clause that is pre\ufb01xed by the lateral keyword\nto access attributes of preceding tables or subqueries in the same from clause. For\nexample, if we wish to print the names of each instructor, along with their salary and\nthe average salary in their department, we could write the query as follows:\nselect name ,salary ,avg\nsalary\nfrom instructor I1 ,lateral (select avg (salary )a savg\nsalary\nfrom instructor I2\nwhere I2.dept\n name =I1.dept\n name );\nWithout the lateral clause, the subquery cannot access the correlation variable I1from\nthe outer query. Only the more recent implementations of SQL support the lateral\nclause.\n3.8.6 The With Clause\nThe with clause provides a way of de\ufb01ning a temporary relation whose de\ufb01nition is\navailable only to the query in which the with clause occurs. Consider the following\nquery, which \ufb01nds those departments with the maximum budget.\nwith max\n budget (value )as\n(select max (budget )\nfrom department )\nselect budget\nfrom department ,max\n budget\nwhere department .budget=max\n budget.value ;\n", "134": "106 Chapter 3 Introduction to SQL\nThe with clause in the query de\ufb01nes the temporary relation max\n budget containing the\nresults of the subquery de\ufb01ning the relation. The relation is available for use only within\nlater parts of the same query.12The with clause, introduced in SQL:1999 , is supported\nby many, but not all, database systems.\nWe could have written the preceding query by using a nested subquery in either the\nfrom clause or the where clause. However, using nested subqueries would have made\nthe query harder to read and understand. The withclause makes the query logic clearer;\nit also permits this temporary relation to be used in multiple places within a query.\nFor example, suppose we want to \ufb01nd all departments where the total salary is\ngreater than the average of the total salary at all departments. We can write the query\nusing the with clause as follows.\nwith dept\n total (dept\n name ,value )as\n(select dept\n name ,sum(salary )\nfrom instructor\ngroup by dept\n name ),\ndept\n total\n avg(value )as\n(select avg (value )\nfrom dept\n total)\nselect dept\n name\nfrom dept\n total,dept\n total\n avg\nwhere dept\n total.value >dept\n total\n avg.value ;\nWe can create an equivalent query without the with clause, but it would be more com-\nplicated and harder to understand. You can write the equivalent query as an exercise.\n3.8.7 Scalar Subqueries\nSQL allows subqueries to occur wherever an expression returning a value is permitted,\nprovided the subquery returns only one tuple containing a single attribute; such sub-\nqueries are called scalar subqueries . For example, a subquery can be used in the select\nclause as illustrated in the following example that lists all departments along with the\nnumber of instructors in each department:\nselect dept\n name ,\n(select count (*)\nfrom instructor\nwhere department .dept\n name =instructor .dept\n name )\nasnum\n instructors\nfrom department ;\n12The SQL evaluation engine may not physically create the relation and is free to compute the overall query result in\nalternative ways, as long as the result of the query is the same as if the relation had been created.\n", "135": "3.8 Nested Subqueries 107\nThe subquery in this example is guaranteed to return only a single value since it has\nacount (*) aggregate without a group by . The example also illustrates the usage of cor-\nrelation variables, that is, attributes of relations in the from clause of the outer query,\nsuch as department .dept\n name in the above example.\nScalar subqueries can occur in select ,where ,a n d having clauses. Scalar subqueries\nmay also be de\ufb01ned without aggregates. It is not always possible to \ufb01gure out at compile\ntime if a subquery can return more than one tuple in its result; if the result has more\nthan one tuple when the subquery is executed, a run-time error occurs.\nNote that technically the type of a scalar subquery result is still a relation, even if\nit contains a single tuple. However, when a scalar subquery is used in an expression\nwhere a value is expected, SQL implicitly extracts the value from the single attribute of\nthe single tuple in the relation and returns that value.\n3.8.8 Scalar Without a From Clause\nCertain queries require a calculation but no reference to any relation. Similarly, certain\nqueries may have subqueries that contain a from clause without the top-level query\nneeding a from clause.\nAs an example, suppose we wish to \ufb01nd the average number of sections taught (re-\ngardless of year or semester) per instructor, with sections taught by multiple instructors\ncounted once per instructor. We need to count the number of tuples in teaches to \ufb01nd\nthe total number of sections taught and count the number of tuples in instructor to \ufb01nd\nthe number of instructors. Then a simple division gives us the desired result. One might\nwrite this as:\n(select count (*)from teaches )/(select count (*)from instructor );\nWhile this is legal in some systems, others will report an error due to the lack of a\nfrom clause.13In the latter case, a special dummy relation called, for example, dual can\nbe created, containing a single tuple. This allows the preceding query to be written as:\nselect (select count (*)from teaches )/(select count (*)from instructor )\nfrom dual;\nOracle provides a prede\ufb01ned relation called dual, containing a single tuple, for uses\nsuch as the above (the relation has a single attribute, which is not relevant for our\npurposes); you can create an equivalent relation if you use any other database.\nSince the above queries divide one integer by another, the result would, on most\ndatabases, be an integer, which would result in loss of precision. If you wish to get the\nresult as a \ufb02oating point number, you could multiply one of the two subquery results by\n1.0 to convert it to a \ufb02oating point number, before the division operation is performed.\n13This construct is legal, for example, in SQL S erver, but not legal, for example, in Oracle.\n", "136": "108 Chapter 3 Introduction to SQL\nNote 3.3 SQL AND MULTISET RELATIONAL ALGEBRA - PART 3\nUnlike the SQL set and aggregation operations that we studied earlier in this chap-\nter,SQL subqueries do not have directly equivale nt operations in the relational al-\ngebra. Most SQL queries involving subqueries can be rewritten in a way that does\nnot require the use of subqueries, and thus they have equivalent relational algebra\nexpressions.\nRewriting to relational algebra can bene\ufb01t from two extended relational al-\ngebra operations called semijoin , denoted \u22c9,a n d antijoin , denoted\n \u22c9,w h i c ha r e\nsupported internally by many database implementations (the symbol \u22b3is some-\ntimes used in place of\n \u22c9to denote antijoin). For example, given relations rands,\nr\u22c9r.A=s.Bsoutputs all tuples in rthat have at least one tuple in swhose s.Battribute\nvalue matches that tuples r.Aattribute value. Conversely, r\n\u22c9r.A=s.Bsoutputs all tu-\nples in rthat have do not have any such matching tuple in s. These operators can\nbe used to rewrite many subqueries that use the exists and not exists connectives.\nSemijoin and antijoin can be expressed using other relational algebra opera-\ntions, so they do not add any expressive power, but they are nevertheless quite\nuseful in practice since they can be implemented very e\ufb03ciently.\nHowever, the process of rewriting SQL queries that contain subqueries is in\ngeneral not straightforward. Database system implementations therefore extend\nthe relational algebra by allowing \u03c3and\u03a0operators to invoke subqueries in their\npredicates and projection lists.\n3.9 Modification of the Database\nWe have restricted our attention until now to the extraction of information from the\ndatabase. Now, we show how to add, remove, or change information with SQL.\n3.9.1 Deletion\nAdelete request is expressed in much the same way as a query. We can delete only\nwhole tuples; we cannot delete values on only particular attributes. SQL expresses a\ndeletion by:\ndelete from r\nwhere P;\nwhere Prepresents a predicate and rrepresents a relation. The delete statement \ufb01rst\n\ufb01nds all tuples tinrfor which P(t) is true, and then deletes them from r.T h e where\nclause can be omitted, in which case all tuples in rare deleted.\n", "137": "3.9 Modi\ufb01cation of the Database 109\nNote that a delete command operates on only one relation. If we want to delete\ntuples from several relations, we must use one delete command for each relation. The\npredicate in the where clause may be as complex as a select command\u2019s where clause.\nAt the other extreme, the where clause may be empty. The request:\ndelete from instructor ;\ndeletes all tuples from the instructor relation. The instructor relation itself still exists,\nbut it is empty.\nHere are examples of SQL delete requests:\n\u2022Delete all tuples in the instructor relation pertaining to instructors in the Finance\ndepartment.\ndelete from instructor\nwhere dept\n name='Finance';\n\u2022Delete all instructors with a sal ary between $13,000 and $15,000.\ndelete from instructor\nwhere salary between 13000 and15000;\n\u2022Delete all tuples in the instructor relation for those instructors associated with a\ndepartment located in the Watson building.\ndelete from instructor\nwhere dept\n name in(select dept\n name\nfrom department\nwhere building='Watson');\nThis delete request \ufb01rst \ufb01nds all departments located in Watson and then deletes\nallinstructor tuples pertaining to those departments.\nNote that, although we may delete tuples from only one relation at a time, we may\nreference any number of relations in a select-from-where nested in the where clause of a\ndelete .T h e delete request can contain a nested select that references the relation from\nwhich tuples are to be deleted. For example, suppose that we want to delete the records\nof all instructors with salary below the average at the university. We could write:\ndelete from instructor\nwhere salary <(select avg (salary )\nfrom instructor );\n", "138": "110 Chapter 3 Introduction to SQL\nThe delete statement \ufb01rst tests each tuple in the relation instructor to check whether\nthe salary is less than the average salary of instructors in the university. Then, all tuples\nthat pass the test\u2014that is, represent an instructor with a lower-than-average salary\u2014are\ndeleted. Performing all the tests before performing any deletion is important\u2014if some\ntuples are deleted before other tuples have been tested, the average salary may change,\nand the \ufb01nal result of the delete would depend on the order in which the tuples were\nprocessed!\n3.9.2 Insertion\nTo insert data into a relation, we either specify a tuple to be inserted or write a query\nwhose result is a set of tuples to be inserted. The attribute values for inserted tuples\nmust be members of the corresponding attribute\u2019s domain. Similarly, tuples inserted\nmust have the correct number of attributes.\nThe simplest insert statement is a request to insert one tuple. Suppose that we wish\nto insert the fact that there is a course CS-437 in the Computer Science department\nwith title \u201cDatabase Systems\u201d and four credit hours. We write:\ninsert into course\nvalues ('CS-437', 'Database Systems', 'Comp. Sci.', 4);\nIn this example, the values are speci\ufb01ed in the order in which the corresponding at-\ntributes are listed in the relation schema. For the bene\ufb01t of users who may not re-\nmember the order of the attributes, SQL allows the attributes to be speci\ufb01ed as part of\ntheinsert statement. For example, the following SQL insert statements are identical in\nfunction to the preceding one:\ninsert into course (course\n id,title,dept\n name ,credits )\nvalues ('CS-437', 'Database Systems', 'Comp. Sci.', 4);\ninsert into course (title,course\n id,credits ,dept\n name )\nvalues ('Database Systems', ' CS-437', 4, 'Comp. Sci.');\nMore generally, we might want to insert tuples on the basis of the result of a query.\nSuppose that we want to make each student in the Music department who has earned\nmore than 144 credit hours an instructor in the Music department with a salary of\n$18,000. We write:\ninsert into instructor\nselect ID,name ,dept\n name , 18000\nfrom student\nwhere dept\n name='Music' andtot\ncred>144;\n", "139": "3.9 Modi\ufb01cation of the Database 111\nInstead of specifying a tuple as we did earlier in this section, we use a select to specify a\nset of tuples. SQL evaluates the select statement \ufb01rst, giving a set of tuples that is then\ninserted into the instructor relation. Each tuple has an ID,aname ,adept\n name (Music),\nand a salary of $18,000.\nIt is important that the system evaluate the select statement fully before it performs\nany insertions. If it were to carry out some insertions while the select statement was\nbeing evaluated, a request such as:\ninsert into student\nselect *\nfrom student ;\nmight insert an in\ufb01nite number of tuples, if the primary key constraint on student were\nabsent. Without the primary key constraint, the request would insert the \ufb01rst tuple in\nstudent again, creating a second copy of the tuple. Since this second copy is part of\nstudent now, the select statement may \ufb01nd it, and a third copy would be inserted into\nstudent .T h e select statement may then \ufb01nd this third copy and insert a fourth copy, and\nso on, forever. Evaluating the select statement completely before performing insertions\navoids such problems. Thus, the above insert statement would simply duplicate every\ntuple in the student relation if the relation did not have a primary key constraint.\nOur discussion of the insert statement considered only examples in which a value\nis given for every attribute in inserted tuples. It is possible for inserted tuples to be given\nvalues on only some attributes of the schema. The remaining attributes are assigned a\nnull value denoted by null. Consider the request:\ninsert into student\nvalues ('3003', 'Green', 'Finance', null);\nThe tuple inserted by this request speci\ufb01ed that a student with ID\u201c3003\u201d is in the\nFinance department, but the tot\ncredvalue for this student is not known.\nMost relational database products have special \u201cbulk loader\u201d utilities to insert a\nlarge set of tuples into a relation. These utilities allow data to be read from format-\nted text \ufb01les, and they can execute much faster than an equivalent sequence of insert\nstatements.\n3.9.3 Updates\nIn certain situations, we may wish to change a value in a tuple without changing all\nvalues in the tuple. For this purpose, the update statement can be used. As we could\nforinsert and delete , we can choose the tuples to be updated by using a query.\nSuppose that annual salary increases are being made, and salaries of all instructors\nare to be increased by 5 percent. We write:\n", "140": "112 Chapter 3 Introduction to SQL\nupdate instructor\nsetsalary=salary * 1.05;\nThe preceding update statement is applied once to each of the tuples in the instructor\nrelation.\nIf a salary increase is to be paid only to instructors with a salary of less than\n$70,000, we can write:\nupdate instructor\nsetsalary=salary *1 . 0 5\nwhere salary <70000;\nIn general, the where clause of the update statement may contain any construct legal\nin the where clause of the select statement (including nested select s). As with insert\nand delete ,an e s t e d select within an update statement may reference the relation that\nis being updated. As before, SQL \ufb01rst tests all tuples in the relation to see whether\nthey should be updated, and it carries out the updates afterward. For example, we can\nwrite the request \u201cGive a 5 percent salary raise to instructors whose salary is less than\naverage\u201d as follows:\nupdate instructor\nsetsalary=salary *1 . 0 5\nwhere salary <(select avg (salary )\nfrom instructor );\nLet us now suppose that all instructors with salary over $100,000 receive a 3 per-\ncent raise, whereas all others receive a 5 percent raise. We could write two update\nstatements:\nupdate instructor\nsetsalary=salary *1 . 0 3\nwhere salary >100000;\nupdate instructor\nsetsalary=salary *1 . 0 5\nwhere salary <=100000;\nNote that the order of the two update statements is important. If we changed the order\nof the two statements, an instructor with a salary just under $100,000 would receive a\nraise of over 8 percent.\nSQL provides a case construct that we can use to perform both updates with a\nsingle update statement, avoiding the problem with the order of updates.\n", "141": "3.9 Modi\ufb01cation of the Database 113\nupdate instructor\nsetsalary=case\nwhen salary <=100000 then salary *1 . 0 5\nelsesalary *1 . 0 3\nend\nThe general form of the case statement is as follows:\ncase\nwhen pred1then result1\nwhen pred2then result2\n\u2026\nwhen prednthen resultn\nelseresult0\nend\nThe operation returns resulti,w h e r e iis the \ufb01rst of pred1,pred2,. . . , prednthat is satis-\n\ufb01ed; if none of the predicates is satis\ufb01ed, the operation returns result0.C a s es t a t e m e n t s\ncan be used in any place where a value is expected.\nScalar subqueries are useful in SQL update statements, where they can be used in\nthesetclause. We illustrate this using the student andtakes relations that we introduced\nin Chapter 2. Consider an update where we set the tot\ncred attribute of each student\ntuple to the sum of the credits of courses successfully completed by the student. We\nassume that a course is successfully completed if the student has a grade that is neither\n'F' nor null. To specify this update, we need to use a subquery in the setclause, as\nshown below:\nupdate student\nsettot\ncred=(\nselect sum (credits )\nfrom takes ,course\nwhere student .ID=takes .IDand\ntakes .course\n id=course .course\n idand\ntakes .grade <>'F'and\ntakes .grade is not null );\nIn case a student has not successfully completed any course, the preceding statement\nwould set the tot\ncred attribute value to null. To set the value to 0 instead, we could\nuse another update statement to replace null values with 0; a better alternative is to\nreplace the clause \u201c select sum (credits )\u201d in the preceding subquery with the following\nselect clause using a case expression:\nselect case\nwhen sum (credits )is not null then sum (credits )\nelse0\nend\n", "142": "114 Chapter 3 Introduction to SQL\nMany systems support a coalesce function, which we describe in more detail later,\nin Section 4.5.2, which provides a concise way of replacing nulls by other values. In\nt h ea b o v ee x a m p l e ,w ec o u l dh a v eu s e d coalesce (sum(credits ), 0) instead of the case\nexpression; this expression wo uld return the aggregate result sum(credits )i fi ti sn o t\nnull, and 0 otherwise.\n3.10 Summary\n\u2022SQL is the most in\ufb02uential commercially marketed relational query language. The\nSQL language has several parts:\n\u00b0Data-de\ufb01nition language (DDL ), which provides commands for de\ufb01ning rela-\ntion schemas, deleting relations, and modifying relation schemas.\n\u00b0Data-manipulation language (DML ), which includes a query language and com-\nmands to insert tuples into, delete tuples from, and modify tuples in the\ndatabase.\n\u2022The SQL data-de\ufb01nition language is used to create relations with speci\ufb01ed\nschemas. In addition to specifying the names and types of relation attributes,\nSQL also allows the speci\ufb01cation of integrity constraints such as primary-key con-\nstraints and foreign-key constraints.\n\u2022SQL includes a variety of language constructs for queries on the database. These\ninclude the select ,from,a n d where clauses.\n\u2022SQL also provides mechanisms to rename both attributes and relations, and to\norder query results by sorting on speci\ufb01ed attributes.\n\u2022SQL supports basic set operations on relations, including union ,intersect ,a n d ex-\ncept, which correspond to the mathematical set operations \u222a,\u2229,a n d\u2212.\n\u2022SQL handles queries on relations containing null values by adding the truth value\n\u201cunknown\u201d to the usual truth values of true and false.\n\u2022SQL supports aggregation, including the ability to divide a relation into groups,\napplying aggregation separately on each group. SQL also supports set operations\non groups.\n\u2022SQL supports nested subqueries in the where and from clauses of an outer query.\nIt also supports scalar subqueries wherever an expression returning a value is per-\nmitted.\n\u2022SQL provides constructs for updating, inserting, and deleting information.\n", "143": "Practice Exercises 115\nReview Terms\n\u2022Data-de\ufb01nition language\n\u2022Data-manipulation language\n\u2022Database schema\n\u2022Database instance\n\u2022Relation schema\n\u2022Relation instance\n\u2022Primary key\n\u2022Foreign key\n\u00b0Referencing relation\n\u00b0Referenced relation\n\u2022Null value\n\u2022Query language\n\u2022SQL query structure\n\u00b0select clause\n\u00b0from clause\n\u00b0where clause\n\u2022Multiset relational algebra\n\u2022asclause\n\u2022order by clause\n\u2022Table alias\n\u2022Correlation name (correlation vari-\nable, tuple variable)\u2022Set operations\n\u00b0union\n\u00b0intersect\n\u00b0except\n\u2022Aggregate functions\n\u00b0avg, min, max, sum, count\n\u00b0group by\n\u00b0having\n\u2022Nested subqueries\n\u2022Set comparisons\n\u00b0{<,<=,>,>=}{some, all }\n\u00b0exists\n\u00b0unique\n\u2022lateral clause\n\u2022with clause\n\u2022Scalar subquery\n\u2022Database modi\ufb01cation\n\u00b0Delete\n\u00b0Insert\n\u00b0Update\nPractice Exercises\n3.1 Write the following queries in SQL, using the university schema. (We suggest\nyou actually run these queries on a database, using the sample data that we\nprovide on the web site of the book, db-book.com . Instructions for setting up\na database, and loading sample data, are provided on the above web site.)\na. Find the titles of courses in the Comp. Sci. department that have 3 credits.\nb. Find the IDs of all students who were taught by an instructor named Ein-\nstein; make sure there are no duplicates in the result.\n", "144": "116 Chapter 3 Introduction to SQL\nc. Find the highest salary of any instructor.\nd. Find all instructors earning the highest salary (there may be more than\none with the same salary).\ne. Find the enrollment of each section that was o\ufb00ered in Fall 2017.\nf. Find the maximum enrollment, across all sections, in Fall 2017.\ng. Find the sections that had the maximum enrollment in Fall 2017.\n3.2 Suppose you are given a relation grade\n points (grade\n ,points ) that provides a con-\nv e r s i o nf r o ml e t t e rg r a d e si nt h e takes relation to numeric scores; for example,\nan \u201cA\u201d grade could be speci\ufb01ed to correspond to 4 points, an \u201cA \u2212\u201dt o3 . 7p o i n t s ,\na \u201cB+\u201d to 3.3 points, a \u201cB\u201d to 3 points, and so on. The grade points earned by a\nstudent for a course o\ufb00ering (section) is de\ufb01ned as the number of credits for the\ncourse multiplied by the numeric points for the grade that the student received.\nGiven the preceding relation, and our university schema, write each of the\nfollowing queries in SQL. You may assume for simplicity that no takes tuple has\nthenullvalue for grade .\na. Find the total grade points earned by the student with ID'12345', across\nall courses taken by the student.\nb. Find the grade point average ( GPA) for the above student, that is, the total\ngrade points divided by the total credits for the associated courses.\nc. Find the IDand the grade-point average of each student.\nd. Now reconsider your answers to the earlier parts of this exercise under\nthe assumption that some grades might be null. Explain whether your\nsolutions still work and, if not, provide versions that handle nulls properly.\n3.3 Write the following inserts, deletes, or updates in SQL, using the university\nschema.\na. Increase the salary of each instructor in the Comp. Sci. department by\n10%.\nb. Delete all courses that have never been o\ufb00ered (i.e., do not occur in the\nsection relation).\nc. Insert every student whose tot\ncred attribute is greater than 100 as an in-\nstructor in the same department, with a salary of $10,000.\n3.4 Consider the insurance database of Figure 3.17, where the primary keys are\nunderlined. Construct the following SQL queries for this relational database.\na. Find the total number of people who owned cars that were involved in\naccidents in 2017.\n", "145": "Practice Exercises 117\nperson (driver\n id\n,name ,address )\ncar(license\n plate\n,model ,year)\naccident (report\n number\n ,year,location )\nowns (driver\n id\n,license\n plate\n)\nparticipated (report\n number\n ,license\n plate\n,driver\n id,damage\n amount )\nFigure 3.17 Insurance database\nb. Delete all year-2010 cars belonging to the person whose IDis '12345'.\n3.5 Suppose that we have a relation marks (ID,score ) and we wish to assign grades\nto students based on the score as follows: grade Fifscore < 40, grade Cif 40\n\u2264score < 60, grade Bif 60\u2264score < 80, and grade Aif 80\u2264score .W r i t e SQL\nqueries to do the following:\na. Display the grade for each student, based on the marks relation.\nb. Find the number of students with each grade.\n3.6 The SQL likeoperator is case sensitive (in most systems), but the lower() func-\ntion on strings can be used to perform case-insensitive matching. To show how,\nwrite a query that \ufb01nds departments whose names contain the string \u201csci\u201d as a\nsubstring, regardless of the case.\n3.7 Consider the SQL query\nselect p.a1\nfrom p,r1,r2\nwhere p.a1=r1.a1orp.a1=r2.a1\nUnder what conditions does the preceding query select values of p.a1t h a ta r e\neither in r1o ri n r2? Examine carefully the cases where either r1o r r2m a yb e\nempty.\n3.8 Consider the bank database of Figure 3.18, where the primary keys are under-\nlined. Construct the following SQL queries for this relational database.\na. Find the IDof each customer of the bank who has an account but not a\nloan.\nb. Find the IDof each customer who lives on the same street and in the same\ncity as customer '12345'.\nc. Find the name of each branch that has at least one customer who has an\naccount in the bank and who lives in \u201cHarrison\u201d.\n", "146": "118 Chapter 3 Introduction to SQL\nbranch (branch\n name\n ,branch\n city, assets )\ncustomer (ID\n,customer\n name ,customer\n street, customer\n city)\nloan (loan\n number\n ,branch\n name, amount )\nborrower (ID\n,loan\n number\n )\naccount (account\n number\n ,branch\n name, balance )\ndepositor (ID\n,account\n number\n )\nFigure 3.18 Banking database.\n3.9 Consider the relational database of Figure 3.19, where the primary keys are\nunderlined. Give an expression in SQL for each of the following queries.\na. Find the ID, name, and city of residence of each employee who works for\n\u201cFirst Bank Corporation\u201d.\nb. Find the ID, name, and city of residence of each employee who works for\n\u201cFirst Bank Corporation\u201d and earns more than $10000.\nc. Find the IDof each employee who does not work for \u201cFirst Bank Corpo-\nration\u201d.\nd. Find the IDof each employee who earns more than every employee of\n\u201cSmall Bank Corporation\u201d.\ne. Assume that companies may be located in several cities. Find the name\nof each company that is located in every city in which \u201cSmall Bank Cor-\nporation\u201d is located.\nf. Find the name of the company that has the most employees (or compa-\nnies, in the case where there is a tie for the most).\ng. Find the name of each company whose employees earn a higher salary,\non average, than the average salary at \u201cFirst Bank Corporation\u201d.\nemployee (ID\n,person\n name ,street ,city)\nworks (ID\n,company\n name ,salary )\ncompany (company\n name\n ,city)\nmanages (ID\n,manager\n id)\nFigure 3.19 Employee database.\n", "147": "Exercises 119\n3.10 Consider the relational database of Figure 3.19. Give an expression in SQL for\neach of the following:\na. Modify the database so that the employee whose IDis '12345' now lives\nin \u201cNewtown\u201d.\nb. Give each manager of \u201cFirst Bank Corporation\u201d a 10 percent raise unless\nthe salary becomes greater than $100000; in such cases, give only a 3\npercent raise.\nExercises\n3.11 Write the following queries in SQL, using the university schema.\na. Find the IDand name of each student who has taken at least one Comp.\nSci. course; make sure there are no duplicate names in the result.\nb. Find the IDand name of each student who has not taken any course\no\ufb00ered before 2017.\nc. For each department, \ufb01nd the maximum salary of instructors in that de-\npartment. You may assume that every department has at least one instruc-\ntor.\nd. Find the lowest, across all departments, of the per-department maximum\nsalary computed by the preceding query.\n3.12 Write the SQL statements using the university schema to perform the following\noperations:\na. Create a new course \u201cCS-001\u201d, titled \u201cWeekly Seminar\u201d, with 0 credits.\nb. Create a section of this course in Fall 2017, with sec\nidof 1, and with the\nlocation of this section not yet speci\ufb01ed.\nc. Enroll every student in the Comp. Sci. department in the above section.\nd. Delete enrollments in the above section where the student\u2019s IDis 12345.\ne. Delete the course CS-001. What will happen if you run this delete state-\nment without \ufb01rst deleting o\ufb00erings (sections) of this course?\nf. Delete all takes tuples corresponding to any section of any course with\nthe word \u201cadvanced\u201d as a part of the title; ignore case when matching the\nword with the title.\n3.13 Write SQL DDL corresponding to the schema in Figure 3.17. Make any reason-\nable assumptions about data types, and be sure to declare primary and foreign\nkeys.\n", "148": "120 Chapter 3 Introduction to SQL\n3.14 Consider the insurance database of Figure 3.17, where the primary keys are\nunderlined. Construct the following SQL queries for this relational database.\na. Find the number of accidents involving a car belonging to a person named\n\u201cJohn Smith\u201d.\nb . U p d a t et h ed a m a g ea m o u n tf o rt h ec a rw i t hl i c e n s e\n plate \u201cAABB2000\u201d\nin the accident with report number \u201cAR2197\u201d to $3000.\n3.15 Consider the bank database of Figure 3.18, where the primary keys are under-\nlined. Construct the following SQL queries for this relational database.\na. Find each customer who has an account at every branch located in \u201cBrook-\nlyn\u201d.\nb. Find the total sum of all loan amounts in the bank.\nc. Find the names of all branches that have assets greater than those of at\nleast one branch located in \u201cBrooklyn\u201d.\n3.16 Consider the employee database of Figure 3.19, where the primary keys are\nunderlined. Give an expression in SQL for each of the following queries.\na. Find IDand name of each employee who lives in the same city as the\nlocation of the company for which the employee works.\nb. Find IDand name of each employee who lives in the same city and on the\ns a m es t r e e ta sd o e sh e ro rh i sm a n a g e r .\nc. Find IDand name of each employee who earns more than the average\nsalary of all employees of her or his company.\nd. Find the company that has the smallest payroll.\n3.17 Consider the employee database of Figure 3.19. Give an expression in SQL for\neach of the following queries.\na. Give all employees of \u201cFirst Bank Corporation\u201d a 10 percent raise.\nb. Give all managers of \u201cFirst Bank Corporation\u201d a 10 percent raise.\nc. Delete all tuples in the works relation for employees of \u201cSmall Bank Cor-\nporation\u201d.\n3.18 Give an SQL schema de\ufb01nition for the employee database of Figure 3.19.\nChoose an appropriate domain for each attribute and an appropriate primary\nkey for each relation schema. Include any foreign-key constraints that might be\nappropriate.\n3.19 List two reasons why null values might be introduced into the database.\n3.20 Show that, in SQL,<>allis identical to not in .\n", "149": "Exercises 121\nmember (memb\n no\n,name )\nbook(isbn\n,title,authors ,publisher )\nborrowed (memb\n no\n,isbn\n,date)\nFigure 3.20 Library database.\n3.21 Consider the library database of Figure 3.20. Write the following queries in SQL.\na. Find the member number and name of each member who has borrowed\nat least one book published by \u201cMcGraw-Hill\u201d.\nb. Find the member number and name of each member who has borrowed\nevery book published by \u201cMcGraw-Hill\u201d.\nc. For each publisher, \ufb01nd the member number and name of each member\nwho has borrowed more than \ufb01ve books of that publisher.\nd. Find the average number of books borrowed per member. Take into ac-\ncount that if a member does not borrow any books, then that member does\nnot appear in the borrowed relation at all, but that member still counts in\nthe average.\n3.22 Rewrite the where clause\nwhere unique (select title from course )\nwithout using the unique construct.\n3.23 Consider the query:\nwith dept\n total (dept\n name ,value )as\n(select dept\n name ,sum(salary )\nfrom instructor\ngroup by dept\n name ),\ndept\n total\n avg(value )as\n(select avg (value )\nfrom dept\n total)\nselect dept\n name\nfrom dept\n total,dept\n total\n avg\nwhere dept\n total.value >=dept\n total\n avg.value ;\nRewrite this query without using the with construct.\n3.24 Using the university schema, write an SQL query to \ufb01nd the name and ID of\nthose Accounting students advised by an instructor in the Physics department.\n", "150": "122 Chapter 3 Introduction to SQL\n3.25 Using the university schema, write an SQL query to \ufb01nd the names of those\ndepartments whose budget is higher than that of Philosophy. List them in al-\nphabetic order.\n3.26 Using the university schema, use SQL to do the following: For each student who\nhas retaken a course at least twice (i.e., the student has taken the course at least\nthree times), show the course IDand the student\u2019s ID.\nPlease display your results in order of course IDand do not display duplicate\nrows.\n3.27 Using the university schema, write an SQL query to \ufb01nd the IDso ft h o s es t u -\ndents who have retaken at least three distinct courses at least once (i.e, the\nstudent has taken the course at least two times).\n3.28 Using the university schema, write an SQL query to \ufb01nd the names and IDso f\nthose instructors who teach every course taught in his or her department (i.e.,\nevery course that appears in the course relation with the instructor\u2019s department\nname). Order result by name.\n3.29 Using the university schema, write an SQL query to \ufb01nd the name and IDof\neach History student whose name begins with the letter \u2018D\u2019 and who has not\ntaken at least \ufb01ve Music courses.\n3.30 Consider the following SQL query on the university schema:\nselect avg (salary )-(sum(salary )/count (*))\nfrom instructor\nWe might expect that the result of this query is zero since the average of a set\nof numbers is de\ufb01ned to be the sum of the numbers divided by the number of\nnumbers. Indeed this is true for the example instructor relation in Figure 2.1.\nHowever, there are other possible instances of that relation for which the result\nwould notbe zero. Give one such instance, and explain why the result would\nnot be zero.\n3.31 Using the university schema, write an SQLquery to \ufb01nd the IDand name of each\ninstructor who has never given an A grade in any course she or he has taught.\n(Instructors who have never taught a course trivially satisfy this condition.)\n3.32 Rewrite the preceding query, but also ensure that you include only instructors\nwho have given at least one other non-null grade in some course.\n3.33 Using the university schema, write an SQL query to \ufb01nd the IDand title of each\ncourse in Comp. Sci. that has had at least one section with afternoon hours (i.e.,\nends at or after 12:00). (You should eliminate duplicates if any.)\n3.34 Using the university schema, write an SQL query to \ufb01nd the number of students\nin each section. The result columns should appear in the order \u201ccourseid, secid,\nyear, semester, num\u201d. You do not need to output sections with 0 students.\n", "151": "Tools 123\n3.35 Using the university schema, write an SQL query to \ufb01nd section(s) with max-\nimum enrollment. The result columns should appear in the order \u201ccourseid,\nsecid, year, semester, num\u201d. (It may be convenient to use the withconstruct.)\nTools\nA number of relational database systems are available commercially, including IBM\nDB2,IBM Informix, Oracle, SAP Adaptive Server Enterprise (formerly Sybase), and\nMicrosoft SQL S erver. In addition several open-source database systems can be down-\nloaded and used free of charge, including Postgre SQL and MySQL (free except for cer-\ntain kinds of commercial use). Some commercial vendors o\ufb00er free versions of their\nsystems with certain use limitations. These include Oracle Express edition, Microsoft\nSQL S erver Express, and IBM DB2 E xpress- C.\nThe sql.js database is version of the embedded SQL database SQLite which can be\nrun directly in a web browser, allowing SQL commands to be executed directly in the\nbrowser. All data are temporary and vanishes when you close the browser, but it can\nbe useful for learning SQL; be warned that the subset of SQL that is supported by sql.js\nandSQLite is considerably smaller than what is supported by other databases. An SQL\ntutorial using sql.js as the execution engine is hosted at www.w3schools.com/sql .\nThe web site of our book, db-book.com , provides a signi\ufb01cant amount of support-\ning material for the book. By following the link on the site titled Laboratory Material,\nyou can get access to the following:\n\u2022Instructions on how to set up and access some popular database systems, including\nsql.js (which you can run in your browser), MySQL,a n d Postgre SQL.\n\u2022SQL schema de\ufb01nitions for the University schema.\n\u2022SQL scripts for loading sample datasets.\n\u2022Tips on how to use the XData system, developed at IIT Bombay, to test queries for\ncorrectness by executing them on multiple datasets generated by the system; and,\nfor instructors, tips on how to use XData to automate SQL query grading.\n\u2022Get tips on SQL variations across di\ufb00erent databases.\nSupport for di\ufb00erent SQL features varies by databases, and most databases also\nsupport some non-standard extensions to SQL. Read the system manuals to understand\nthe exact SQL features that a database supports.\nMost database systems provide a command line interface for submitting SQL com-\nmands. In addition, most databases also provide graphical user interfaces ( GUIs),\nwhich simplify the task of browsing the database, creating and submitting queries, and\nadministering the database. For Postgre SQL, the pgAdmin tool provides GUI func-\ntionality, while for MySQL,p h p M y A d m i np r o v i d e s GUI functionality. Oracle provides\n", "152": "124 Chapter 3 Introduction to SQL\nOracle SQL Developer, while Microsoft SQL S erver comes with the SQL S erver Man-\nagement Studio.\nThe NetBeans IDEsSQLEditor provides a GUI front end which works with a num-\nber of di\ufb00erent database systems, but with limited functionality, while the Eclipse\nIDEsupports similar functionality through the Data Tools Platform ( DTP). Commer-\ncialIDEs that support SQL access across multiple database platforms include Embar-\ncadero\u2019s RAD Studio and Aqua Data Studio.\nFurther Reading\nThe original Sequel language that became SQL is described in [Chamberlin et al.\n(1976)].\nThe most important SQL reference is likely to be the online documentation pro-\nvided by the vendor or the particular database system you are using. That documenta-\ntion will identify any features that deviate from the SQL standard features presented in\nthis chapter. Here are links to the SQL reference manuals for the current (as of 2018)\nversions of some of the popular databases.\n\u2022MySQL 8.0:dev.mysql.com/doc/refman/8.0/en/\n\u2022Oracle 12c: docs.oracle.com/database/121/SQLRF/\n\u2022Postgre SQL:www.postgresql.org/docs/current/static/sql.html\n\u2022SQLite:www.sqlite.org/lang.html\n\u2022SQL S erver: docs.microsoft.com/en-us/sql/t-sql\nBibliography\n[Chamberlin et al. (1976)] D. D. Chamberlin, M. M. Astrahan, K. P. Eswaran, P. P. Gri\ufb03ths,\nR. A. Lorie, J. W. Mehl, P. Reisner, and B. W. Wade, \u201cSEQUEL 2: A Uni\ufb01ed Approach to\nData De\ufb01nition, Manipulation, and Control\u201d, IBM Journal of Research and Development ,\nVolume 20, Number 6 (1976), pages 560\u2013575.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "153": "CHAPTER4\nIntermediate SQL\nIn this chapter, we continue our study of SQL. We consider more complex forms of\nSQL queries, view de\ufb01nition, transactions, integrity constraints, more details regarding\nSQL data de\ufb01nition, and authorization.\n4.1 Join Expressions\nIn all of the example queries we used in Chapter 3 (except when we used set opera-\ntions), we combined information from multip le relations using the Cartesian product\noperator. In this section, we introduce a number of \u201cjoin\u201d operations that allow the\nprogrammer to write some queries in a more natural way and to express some queries\nthat are di\ufb03cult to do with only the Cartesian product.\nID\n name\n dept\n name\n tot\ncred\n00128\n Zhang\n Comp. Sci.\n 102\n12345\n Shankar\n Comp. Sci.\n 32\n19991\n Brandt\n History\n 80\n23121\n Chavez\n Finance\n 110\n44553\n Peltier\n Physics\n 56\n45678\n Levy\n Physics\n 46\n54321\n Williams\n Comp. Sci.\n 54\n55739\n Sanchez\n Music\n 38\n70557\n Snow\n Physics\n 0\n76543\n Brown\n Comp. Sci.\n 58\n76653\n Aoi\n Elec. Eng.\n 60\n98765\n Bourikas\n Elec. Eng.\n 98\n98988\n Tanaka\n Biology\n 120\nFigure 4.1 The student relation.\n125\n", "154": "126 Chapter 4 Intermediate SQL\nID\n course\n id\n sec\nid\n semester\n year\n grade\n00128\n CS-101\n 1\n Fall\n 2017\n A\n00128\n CS-347\n 1\n Fall\n 2017\n A-\n12345\n CS-101\n 1\n Fall\n 2017\n C\n12345\n CS-190\n 2\n Spring\n 2017\n A\n12345\n CS-315\n 1\n Spring\n 2018\n A\n12345\n CS-347\n 1\n Fall\n 2017\n A\n19991\n HIS-351\n 1\n Spring\n 2018\n B\n23121\n FIN-201\n 1\n Spring\n 2018\n C+\n44553\n PHY-101\n 1\n Fall\n 2017\n B-\n45678\n CS-101\n 1\n Fall\n 2017\n F\n45678\n CS-101\n 1\n Spring\n 2018\n B+\n45678\n CS-319\n 1\n Spring\n 2018\n B\n54321\n CS-101\n 1\n Fall\n 2017\n A-\n54321\n CS-190\n 2\n Spring\n 2017\n B+\n55739\n MU-199\n 1\n Spring\n 2018\n A-\n76543\n CS-101\n 1\n Fall\n 2017\n A\n76543\n CS-319\n 2\n Spring\n 2018\n A\n76653\n EE-181\n 1\n Spring\n 2017\n C\n98765\n CS-101\n 1\n Fall\n 2017\n C-\n98765\n CS-315\n 1\n Spring\n 2018\n B\n98988\n BIO-101\n 1\n Summer\n 2017\n A\n98988\n BIO-301\n 1\n Summer\n 2018\n null\nFigure 4.2 The takes relation.\nAll the examples used in this section involve the two relations student and takes ,\nshown in Figure 4.1 and Figure 4.2, respectively. Observe that the attribute grade has\na value null for the student with ID98988, for the course BIO-301, section 1, taken in\nSummer 2018. The null value indicates that the grade has not been awarded yet.\n4.1.1 The Natural Join\nConsider the following SQL query, which computes for each student the set of courses\na student has taken:\nselect name ,course\n id\nfrom student ,takes\nwhere student .ID=takes .ID;\nNote that this query outputs only students who have taken some course. Students who\nhave not taken any course are not output.\n", "155": "4.1 Join Expressions 127\nNote that in the student and takes table, the matching condition required student .ID\nto be equal to takes .ID. These are the only attributes in the two relations that have the\nsame name. In fact, this is a common case; that is, the matching condition in the from\nclause most often requires all attributes with matching names to be equated.\nTo make the life of an SQL programmer easier for this common case, SQL supports\nan operation called the natural join , which we describe below. In fact, SQL supports sev-\neral other ways in which information from two or more relations can be joined together.\nWe have already seen how a Cartesian product along with a where clause predicate can\nbe used to join information from multiple relations. Other ways of joining information\nfrom multiple relations are discussed in Section 4.1.2 through Section 4.1.4.\nThenatural join operation operates on two relations and produces a relation as the\nresult. Unlike the Cartesian product of two relations, which concatenates each tuple of\nthe \ufb01rst relation with every tuple of the second, natural join considers only those pairs\nof tuples with the same value on those attributes that appear in the schemas of both\nrelations. So, going back to the example of the relations student and takes ,c o m p u t i n g :\nstudent natural join takes\nconsiders only those pairs of tuples where both the tuple from student and the tuple\nfrom takes have the same value on the common attribute, ID.\nThe resulting relation, shown in Figure 4.3, has only 22 tuples, the ones that give\ninformation about a student and a course that the student has actually taken. Notice\nthat we do not repeat those attributes that appear in the schemas of both relations;\nrather they appear only once. Notice also the order in which the attributes are listed:\n\ufb01rst the attributes common to the schemas of both relations, second those attributes\nunique to the schema of the \ufb01rst relation, and \ufb01nally, those attributes unique to the\nschema of the second relation.\nEarlier we wrote the query \u201cFor all students in the university who have taken some\ncourse, \ufb01nd their names and the course IDof all courses they took\u201d as:\nselect name ,course\n id\nfrom student ,takes\nwhere student .ID=takes .ID;\nThis query can be written more concisely using the natural-join operation in SQL as:\nselect name ,course\n id\nfrom student natural join takes ;\nBoth of the above queries generate the same result.1\n1For notational symmetry, SQLallows the Cartesian product, which we have denoted with a comma, to be denoted by\nthe keywords cross join .T h u s ,\u201c from student ,takes \u201d could be expressed equivalently as \u201c from student cross join takes \u201d.\n", "156": "128 Chapter 4 Intermediate SQL\nID\n name\n dept\n name\n tot\ncred\n course\n id\nsec\nid\nsemester\n year\n grade\n00128\n Zhang\n Comp. Sci.\n 102\n CS-101\n 1\n Fall\n 2017\n A\n00128\n Zhang\n Comp. Sci.\n 102\n CS-347\n 1\n Fall\n 2017\n A-\n12345\n Shankar\n Comp. Sci.\n 32\nCS-101\n 1\n Fall\n 2017\n C\n12345\n Shankar\n Comp. Sci.\n 32\nCS-190\n 2\n Spring\n 2017\n A\n12345\n Shankar\n Comp. Sci.\n 32\nCS-315\n 1\n Spring\n 2018\n A\n12345\n Shankar\n Comp. Sci.\n 32\nCS-347\n 1\n Fall\n 2017\n A\n19991\n Brandt\n History\n 80\nHIS-351\n 1\n Spring\n 2018\n B\n23121\n Chavez\n Finance\n 110\n FIN-201\n 1\n Spring\n 2018\n C+\n44553\n Peltier\n Physics\n 56\nPHY-101\n 1\n Fall\n 2017\n B-\n45678\n Levy\n Physics\n 46\nCS-101\n 1\n Fall\n 2017\n F\n45678\n Levy\n Physics\n 46\nCS-101\n 1\n Spring\n 2018\n B+\n45678\n Levy\n Physics\n 46\nCS-319\n 1\n Spring\n 2018\n B\n54321\n Williams\n Comp. Sci.\n 54\nCS-101\n 1\n Fall\n 2017\n A-\n54321\n Williams\n Comp. Sci.\n 54\nCS-190\n 2\n Spring\n 2017\n B+\n55739\n Sanchez\n Music\n 38\nMU-199\n 1\n Spring\n 2018\n A-\n76543\n Brown\n Comp. Sci.\n 58\nCS-101\n 1\n Fall\n 2017\n A\n76543\n Brown\n Comp. Sci.\n 58\nCS-319\n 2\n Spring\n 2018\n A\n76653\n Aoi\n Elec. Eng.\n 60\nEE-181\n 1\n Spring\n 2017\n C\n98765\n Bourikas\n Elec. Eng.\n 98\nCS-101\n 1\n Fall\n 2017\n C-\n98765\n Bourikas\n Elec. Eng.\n 98\nCS-315\n 1\n Spring\n 2018\n B\n98988\n Tanaka\n Biology\n 120\n BIO-101\n 1\n Summer\n 2017\n A\n98988\n Tanaka\n Biology\n 120\n BIO-301\n 1\n Summer\n 2018\n null\nFigure 4.3 The natural join of the student relation with the takes relation.\nThe result of the natural join operation is a relation. Conceptually, expression \u201c stu-\ndent natural join takes \u201di nt h e from clause is replaced by the relation obtained by evalu-\nating the natural join.2Thewhere andselect clauses are then evaluated on this relation,\nas we saw in Section 3.3.2.\nAfrom clause in an SQL query can have multiple relations combined using natural\njoin, as shown here:\nselect A1,A2,\u2026,An\nfrom r1natural join r2natural join ...natural join rm\nwhere P;\nMore generally, a from clause can be of the form\n2As a consequence, it may not be possible in some systems to use attribute names containing the original relation\nnames, for instance, student .IDortakes .ID, to refer to attributes in the natural join result. While some systems allow\nit, others don\u2019t, and some allow it for all attributes except the join attributes (i.e., those that appear in both relation\nschemas). We can, however, use attribute names such as name and course\n idwithout the relation names.\n", "157": "4.1 Join Expressions 129\nfrom E1,E2,..., En\nwhere each Eican be a single relation or an expression involving natural joins. For\nexample, suppose we wish to answer the query \u201cList the names of students along with\nthe titles of courses that they have taken.\u201d The query can be written in SQL as follows:\nselect name ,title\nfrom student natural join takes ,course\nwhere takes .course\n id=course .course\n id;\nThe natural join of student and takes is \ufb01rst computed, as we saw earlier, and a Cartesian\nproduct of this result with course is computed, from which the where clause extracts\nonly those tuples where the course identi\ufb01er from the join result matches the course\nidenti\ufb01er from the course relation. Note that takes .course\n idin the where clause refers\nto the course\n id\ufb01eld of the natural join result, since this \ufb01eld, in turn, came from the\ntakes relation.\nIn contrast, the following SQL query does notcompute the same result:\nselect name ,title\nfrom student natural join takes natural join course ;\nTo see why, note that the natural join of student and takes contains the attributes ( ID,\nname ,dept\n name ,tot\ncred,course\n id,sec\nid), while the course relation contains the at-\ntributes ( course\n id,title,dept\n name ,credits ). As a result, the natural join would require\nthat the dept\n name attribute values from the two relations be the same in addition to\nrequiring that the course\n idvalues be the same. This query would then omit all (stu-\ndent name, course title) pairs where the student takes a course in a department other\nthan the student\u2019s own department. The previous query, on the other hand, correctly\noutputs such pairs.\nTo provide the bene\ufb01t of natural join while avoiding the danger of equating at-\ntributes erroneously, SQL provides a form of the natural join construct that allows you\nto specify exactly which columns should be equated. This feature is illustrated by the\nfollowing query:\nselect name ,title\nfrom (student natural join takes )join course using (course\n id);\nThe operation join\u2026using requires a list of attribute names to be speci\ufb01ed. Both\nrelations being joined must have attributes with the speci\ufb01ed names. Consider the op-\neration r1join r2using (A1,A2). The operation is similar to r1natural join r2,e x c e p tt h a t\nap a i ro ft u p l e s t1from r1and t2from r2match if t1.A1=t2.A1and t1.A2=t2.A2;e v e n\nifr1and r2both have an attribute named A3,i ti s notrequired that t1.A3=t2.A3.\n", "158": "130 Chapter 4 Intermediate SQL\nThus, in the preceding SQL query, the joinconstruct permits student .dept\n name and\ncourse .dept\n name to di\ufb00er, and the SQL query gives the correct answer.\n4.1.2 Join Conditions\nIn Section 4.1.1, we saw how to express natural joins, and we saw the join\u2026using\nclause, which is a form of natural join that requires values to match only on speci\ufb01ed\nattributes. SQL supports another form of join, in which an arbitrary join condition can\nbe speci\ufb01ed.\nTheoncondition allows a general predicate over the relations being joined. This\npredicate is written like a where clause predicate except for the use of the keyword on\nrather than where .L i k et h e using condition, the oncondition appears at the end of the\njoin expression.\nConsider the following query, which has a join expression containing the oncon-\ndition:\nselect *\nfrom student join takes onstudent .ID=takes .ID;\nTheoncondition above speci\ufb01es that a tuple from student matches a tuple from takes\nif their IDvalues are equal. The join expression in this case is almost the same as the\njoin expression student natural join takes , since the natural join operation also requires\nthat for a student tuple and a takes t u p l et om a t c h .T h eo n ed i \ufb00 e r e n c ei st h a tt h er e s u l t\nhas the IDattribute listed twice, in the join result, once for student and once for takes ,\neven though their IDvalues must be the same.\nIn fact, the preceding query is equivalent to the following query:\nselect *\nfrom student ,takes\nwhere student .ID=takes .ID;\nAs we have seen earlier, the relation name is used to disambiguate the attribute name ID,\nand thus the two occurrences can be referred to as student .IDand takes .ID, respectively.\nA version of this query that displays the IDvalue only once is as follows:\nselect student .IDasID,name ,dept\n name ,tot\ncred,\ncourse\n id,sec\nid,semester ,year,grade\nfrom student join takes onstudent .ID=takes .ID;\nThe result of this query is exactly the same as the result of the natural join of student\nand takes , which we showed in Figure 4.3.\nTheoncondition can express any SQL predicate, and thus join expressions using\ntheoncondition can express a richer class of join conditions than natural join .H o w e v e r ,\n", "159": "4.1 Join Expressions 131\nas illustrated by our preceding example, a query using a join expression with an on\ncondition can be replaced by an equivalent expression without the oncondition, with\nthe predicate in the onclause moved to the where clause. Thus, it may appear that the\noncondition is a redundant feature of SQL.\nHowever, there are two good reasons for introducing the oncondition. First, we\nshall see shortly that for a kind of join called an outer join, onconditions do behave\nin a manner di\ufb00erent from where conditions. Second, an SQL query is often more\nreadable by humans if the join condition is speci\ufb01ed in the onclause and the rest of\nthe conditions appear in the where clause.\n4.1.3 Outer Joins\nSuppose we wish to display a list of all students, displaying their ID,a n d name ,dept\nname ,a n d tot\ncred, along with the courses that they have taken. The following SQL\nquery may appear to retrieve the required information:\nselect *\nfrom student natural join takes ;\nUnfortunately, the above query does not work quite as intended. Suppose that there\nis some student who takes no courses. Then the tuple in the student relation for that\nparticular student would not satisfy the condition of a natural join with any tuple in the\ntakes relation, and that student\u2019s data would not appear in the result. We would thus\nnot see any information about students who have not taken any courses. For example,\nin the student and takes relations of Figure 4.1 and Figure 4.2, note that student Snow,\nwith ID70557, has not taken any courses. Snow appears in student ,b u tS n o w \u2019 s ID\nnumber does not appear in the IDcolumn of takes . Thus, Snow does not appear in the\nresult of the natural join.\nMore generally, some tuples in either or both of the relations being joined may\nbe \u201clost\u201d in this way. The outer-join operation works in a manner similar to the join\noperations we have already studied, but it preserves those tuples that would be lost in\na join by creating tuples in the result containing null values.\nFor example, to ensure that the student named Snow from our earlier example ap-\npears in the result, a tuple could be added to the join result with all attributes from the\nstudent relation set to the corresponding values for the student Snow, and all the remain-\ning attributes which come from the takes relation, namely, course\n id,sec\nid,semester ,\nand year,s e tt o null. Thus, the tuple for the student Snow is preserved in the result of\nthe outer join.\nThere are three forms of outer join:\n\u2022Theleft outer join preserves tuples only in the relation named before (to the left\nof) the left outer join operation.\n", "160": "132 Chapter 4 Intermediate SQL\n\u2022Theright outer join preserves tuples only in the relation named after (to the right\nof) the right outer join operation.\n\u2022Thefull outer join preserves tuples in both relations.\nIn contrast, the join operations we studied earlier that do not preserve nonmatched tu-\nples are called inner-join operations, to distinguish them from the outer-join operations.\nWe now explain exactly how each form of outer join operates. We can compute\nthe left outer-join operation as follows: First, compute the result of the inner join as\nbefore. Then, for every tuple tin the left-hand-side relation that does not match any\ntuple in the right-hand-side relation in the inner join, add a tuple rto the result of the\njoin constructed as follows:\n\u2022The attributes of tuple rthat are derived from the left-hand-side relation are \ufb01lled\nin with the values from tuple t.\n\u2022The remaining attributes of rare \ufb01lled with null values.\nFigure 4.4 shows the result of:\nselect *\nfrom student natural left outer join takes ;\nThat result includes student Snow ( ID70557), unlike the result of an inner join, but\nthe tuple for Snow includes nulls for the attributes that appear only in the schema of\nthetakes relation.3\nAs another example of the use of the outer-join operation, we can write the query\n\u201cFind all students who have not taken a course\u201d as:\nselect ID\nfrom student natural left outer join takes\nwhere course\n idisnull;\nTheright outer join is symmetric to the left outer join . Tuples from the right-hand-\nside relation that do not match any tuple in the left-hand-side relation are padded with\nnulls and are added to the result of the right outer join. Thus, if we rewrite the preceding\nquery using a right outer join and swapping the order in which we list the relations as\nfollows:\nselect *\nfrom takes natural right outer join student ;\nwe get the same result except for the order in which the attributes appear in the result\n(see Figure 4.5).\n3We show null values in tables using null, but most systems display null values as a blank \ufb01eld.\n", "161": "4.1 Join Expressions 133\nID\n name\n dept\n name\n tot\ncred\n course\n id\nsec\nid\nsemester\n year\n grade\n00128\n Zhang\n Comp. Sci.\n 102\n CS-101\n 1\n Fall\n 2017\n A\n00128\n Zhang\n Comp. Sci.\n 102\n CS-347\n 1\n Fall\n 2017\n A-\n12345\n Shankar\n Comp. Sci.\n 32\nCS-101\n 1\n Fall\n 2017\n C\n12345\n Shankar\n Comp. Sci.\n 32\nCS-190\n 2\n Spring\n 2017\n A\n12345\n Shankar\n Comp. Sci.\n 32\nCS-315\n 1\n Spring\n 2018\n A\n12345\n Shankar\n Comp. Sci.\n 32\nCS-347\n 1\n Fall\n 2017\n A\n19991\n Brandt\n History\n 80\nHIS-351\n 1\n Spring\n 2018\n B\n23121\n Chavez\n Finance\n 110\n FIN-201\n 1\n Spring\n 2018\n C+\n44553\n Peltier\n Physics\n 56\nPHY-101\n 1\n Fall\n 2017\n B-\n45678\n Levy\n Physics\n 46\nCS-101\n 1\n Fall\n 2017\n F\n45678\n Levy\n Physics\n 46\nCS-101\n 1\n Spring\n 2018\n B+\n45678\n Levy\n Physics\n 46\nCS-319\n 1\n Spring\n 2018\n B\n54321\n Williams\n Comp. Sci.\n 54\nCS-101\n 1\n Fall\n 2017\n A-\n54321\n Williams\n Comp. Sci.\n 54\nCS-190\n 2\n Spring\n 2017\n B+\n55739\n Sanchez\n Music\n 38\nMU-199\n 1\n Spring\n 2018\n A-\n70557\n Snow\n Physics\n 0\nnull\n null\n null\n null\n null\n76543\n Brown\n Comp. Sci.\n 58\nCS-101\n 1\n Fall\n 2017\n A\n76543\n Brown\n Comp. Sci.\n 58\nCS-319\n 2\n Spring\n 2018\n A\n76653\n Aoi\n Elec. Eng.\n 60\nEE-181\n 1\n Spring\n 2017\n C\n98765\n Bourikas\n Elec. Eng.\n 98\nCS-101\n 1\n Fall\n 2017\n C-\n98765\n Bourikas\n Elec. Eng.\n 98\nCS-315\n 1\n Spring\n 2018\n B\n98988\n Tanaka\n Biology\n 120\n BIO-101\n 1\n Summer\n 2017\n A\n98988\n Tanaka\n Biology\n 120\n BIO-301\n 1\n Summer\n 2018\n null\nFigure 4.4 Result of student natural left outer join takes .\nThefull outer join is a combination of the left and right outer-join types. After the\noperation computes the result of the inner join, it extends with nulls those tuples from\nthe left-hand-side relation that did not match with any from the right-hand-side relation\nand adds them to the result. Similarly, it extends with nulls those tuples from the right-\nhand-side relation that did not match with any tuples from the left-hand-side relation\nand adds them to the result. Said di\ufb00erently, full outer join is the union of a left outer\njoin and the corresponding right outer join.4\nAs an example of the use of full outer join, consider the following query: \u201cDisplay\na list of all students in the Comp. Sci. department, along with the course sections, if\nany, that they have taken in Spring 2017; all course sections from Spring 2017 must\n4In those systems, notably MySQL, that implement only left and right outer join, this is exactly how one has to write a\nfull outer join.\n", "162": "134 Chapter 4 Intermediate SQL\nID\n course\n id\nsec\nid\nsemester\n year\n grade\n name\n dept\n name\n tot\ncred\n00128\n CS-101\n 1\n Fall\n 2017\n A\n Zhang\n Comp. Sci.\n 102\n00128\n CS-347\n 1\n Fall\n 2017\n A-\n Zhang\n Comp. Sci.\n 102\n12345\n CS-101\n 1\n Fall\n 2017\n C\n Shankar\n Comp. Sci.\n 32\n12345\n CS-190\n 2\n Spring\n 2017\n A\n Shankar\n Comp. Sci.\n 32\n12345\n CS-315\n 1\n Spring\n 2018\n A\n Shankar\n Comp. Sci.\n 32\n12345\n CS-347\n 1\n Fall\n 2017\n A\n Shankar\n Comp. Sci.\n 32\n19991\n HIS-351\n 1\n Spring\n 2018\n B\n Brandt\n History\n 80\n23121\n FIN-201\n 1\n Spring\n 2018\n C+\n Chavez\n Finance\n 110\n44553\n PHY-101\n 1\n Fall\n 2017\n B-\n Peltier\n Physics\n 56\n45678\n CS-101\n 1\n Fall\n 2017\n F\n Levy\n Physics\n 46\n45678\n CS-101\n 1\n Spring\n 2018\n B+\n Levy\n Physics\n 46\n45678\n CS-319\n 1\n Spring\n 2018\n B\n Levy\n Physics\n 46\n54321\n CS-101\n 1\n Fall\n 2017\n A-\n Williams\n Comp. Sci.\n 54\n54321\n CS-190\n 2\n Spring\n 2017\n B+\n Williams\n Comp. Sci.\n 54\n55739\n MU-199\n 1\n Spring\n 2018\n A-\n Sanchez\n Music\n 38\n70557\n null\n null\n null\n null\n null\n Snow\n Physics\n 0\n76543\n CS-101\n 1\n Fall\n 2017\n A\n Brown\n Comp. Sci.\n 58\n76543\n CS-319\n 2\n Spring\n 2018\n A\n Brown\n Comp. Sci.\n 58\n76653\n EE-181\n 1\n Spring\n 2017\n C\n Aoi\n Elec. Eng.\n 60\n98765\n CS-101\n 1\n Fall\n 2017\n C-\n Bourikas\n Elec. Eng.\n 98\n98765\n CS-315\n 1\n Spring\n 2018\n B\n Bourikas\n Elec. Eng.\n 98\n98988\n BIO-101\n 1\n Summer\n 2017\n A\n Tanaka\n Biology\n 120\n98988\n BIO-301\n 1\n Summer\n 2018\n null\n Tanaka\n Biology\n 120\nFigure 4.5 The result of takes natural right outer join student .\nbe displayed, even if no student from the Comp. Sci. department has taken the course\nsection.\u201d This query can be written as:\nselect *\nfrom (select *\nfrom student\nwhere dept\n name =' C o m p .S c i . ' )\nn a t u r a lf u l lo u t e rj o i n\n(select *\nfrom takes\nwhere semester =' S p r i n g ' and year =2 0 1 7 ) ;\nThe result appears in Figure 4.6.\n", "163": "4.1 Join Expressions 135\nID\n name\n dept\n name\n tot\ncred\n course\n id\nsec\nid\nsemester\n year\n grade\n00128\n Zhang\n Comp. Sci.\n 102\n null\n null\n null\n null\n null\n12345\n Shankar\n Comp. Sci.\n 32\nCS-190\n 2\n Spring\n 2017\n A\n54321\n Williams\n Comp. Sci.\n 54\nCS-190\n 2\n Spring\n 2017\n B+\n76543\n Brown\n Comp. Sci.\n 58\n null\n null\n null\n null\n null\n76653\n null\n null\n null\n ECE-181\n 1\n Spring\n 2017\n C\nFigure 4.6 Result of full outer join example (see text).\nTheonclause can be used with outer joins. The following query is identical to the\n\ufb01rst query we saw using \u201c student natural left outer join takes ,\u201d except that the attribute\nIDappears twice in the result.\nselect *\nfrom student left outer join takes onstudent .ID=takes .ID;\nAs we noted earlier, onandwhere behave di\ufb00erently for outer join. The reason\nfor this is that outer join adds null-padded tuples only for those tuples that do not\ncontribute to the result of the corresponding \u201cinner\u201d join. The oncondition is part of\nthe outer join speci\ufb01cation, but a where clause is not. In our example, the case of the\nstudent tuple for student \u201cSnow\u201d with ID70557, illustrates this distinction. Suppose we\nmodify the preceding query by moving the onclause predicate to the where clause and\ninstead using an oncondition of true.5\nselect *\nfrom student left outer join takes ontrue\nwhere student .ID=takes .ID;\nThe earlier query, using the left outer join with the oncondition, includes a tuple\n(70557, Snow, Physics, 0, null,null,null,null,null,null) because there is no tuple\nintakes with ID=70557. In the latter query, however, every tuple satis\ufb01es the join\ncondition true, so no null-padded tuples are generated by the outer join. The outer join\nactually generates the Cartesian product of the two relations. Since there is no tuple\nintakes with ID=70557, every time a tuple appears in the outer join with name=\n\u201cSnow\u201d, the values for student .IDand takes .IDmust be di\ufb00erent, and such tuples would\nbe eliminated by the where clause predicate. Thus, student Snow never appears in the\nresult of the latter query.\n5Some systems do not allow the use of the Boolean constant true. To test this on those systems, use a tautology (i.e., a\npredicate that always evaluates to true), like \u201c1=1\u201d.\n", "164": "136 Chapter 4 Intermediate SQL\nNote 4.1 SQL AND MULTISET RELATIONAL ALGEBRA - PART 4\nThe relational algebra supports the left outer-join operation, denoted by \u27d5\u03b8,t h e\nright outer-join operation, denoted by \u27d6\u03b8, and the full outer-join operation, de-\nnoted by\u27d7\u03b8. It also supports the natural join operation, denoted by \u22c8,a sw e l la s\nthe natural join versions of the left, right and full outer-join operations, denoted\nby\u27d5,\u27d6,a n d\u27d7. The de\ufb01nitions of all these operations are identical to the def-\ninitions of the corresponding operations in SQL, which we have seen in Section\n4.1.\n4.1.4 Join Types and Conditions\nTo distinguish normal joins from outer joins, normal joins are called inner joins inSQL.\nA join clause can thus specify inner join instead of outer join to specify that a normal\njoin is to be used. The keyword inner is, however, optional. The default join type, when\nthejoinclause is used without the outer pre\ufb01x, is the inner join .T h u s ,\nselect *\nfrom student join takes using (ID);\nis equivalent to:\nselect *\nfrom student inner join takes using (ID);\nSimilarly, natural join is equivalent to natural inner join .\nFigure 4.7 shows a full list of the various types of join that we have discussed. As\ncan be seen from the \ufb01gure, any form of join (inner, left outer, right outer, or full outer)\ncan be combined with any join condition (natural, using, or on).\nJoin types\ninner join\nleft outer join\nright outer join\nfull outer joinJoin conditions\nnatural\non < predicate>\nusing (A1,A2, . . ., An)\nFigure 4.7 Join types and join conditions.\n", "165": "4.2 Views 137\n4.2 Views\nIt is not always desirable for all users to see the entire set of relations in the database.\nIn Section 4.7, we shall see how to use the SQL authorization mechanism to restrict\naccess to relations, but security considerations may require that only certain data in a\nrelation be hidden from a user. Consider a clerk who needs to know an instructor\u2019s ID,\nname, and department name, but does not have authorization to see the instructor\u2019s\nsalary amount. This person should see a relation described in SQL by:\nselect ID,name ,dept\n name\nfrom instructor ;\nAside from security concerns, we may wish to create a personalized collection of \u201cvir-\ntual\u201d relations that is better matched to a certain user\u2019s intuition of the structure of the\nenterprise. In our university example, we may want to have a list of all course sections\no\ufb00ered by the Physics department in the Fall 2017 semester, with the building and\nroom number of each section. The relation that we would create for obtaining such a\nlist is:\nselect course .course\n id,sec\nid,building ,room\n number\nfrom course ,section\nwhere course .course\n id=section .course\n id\nand course .dept\n name='Physics'\nand section .semester='Fall'\nand section .year=2017;\nIt is possible to compute and store the results of these queries and then make the\nstored relations available to users. However, if we did so, and the underlying data in the\nrelations instructor ,course ,o r section changed, the stored query results would then no\nlonger match the result of reexecuting the query on the relations. In general, it is a bad\nidea to compute and store query results such as those in the above examples (although\nthere are some exceptions that we study later).\nInstead, SQL allows a \u201cvirtual relation\u201d to be de\ufb01ned by a query, and the relation\nconceptually contains the result of the query. The virtual relation is not precomputed\nand stored but instead is computed by executing the query whenever the virtual relation\nis used. We saw a feature for this in Section 3.8.6, where we described the with clause.\nThewith clause allows us to to assign a name to a subquery for use as often as desired,\nbut in one particular query only. Here, we present a way to extend this concept beyond\na single query by de\ufb01ning a view. It is possible to support a large number of views on\ntop of any given set of actual relations.\n", "166": "138 Chapter 4 Intermediate SQL\n4.2.1 View Definition\nWe de\ufb01ne a view in SQL by using the create view command. To de\ufb01ne a view, we must\ngive the view a name and must state the query that computes the view. The form of the\ncreate view command is:\ncreate view vas<query expression >;\nwhere <query expression >is any legal query expression. The view name is represented\nbyv.\nConsider again the clerk who needs to access all data in the instructor relation,\nexcept salary . The clerk should not be authorized to access the instructor relation (we\nsee in Section 4.7, how authorizations can be speci\ufb01ed). Instead, a view relation faculty\ncan be made available to the clerk, with the view de\ufb01ned as follows:\ncreate view faculty as\nselect ID,name ,dept\n name\nfrom instructor ;\nAs explained earlier, the view relation conceptually contains the tuples in the query\nresult, but it is not precomputed and stored. Instead, the database system stores the\nquery expression associated with the view relation. Whenever the view relation is ac-\ncessed, its tuples are created by computing the query result. Thus, the view relation is\ncreated whenever needed, on demand.\nTo create a view that lists all course sections o\ufb00ered by the Physics department in\nthe Fall 2017 semester with the building and room number of each section, we write:\ncreate view physics\n fall\n2017 as\nselect course .course\n id,sec\nid,building ,room\n number\nfrom course ,section\nwhere course .course\n id=section .course\n id\nand course .dept\n name='Physics'\nand section .semester='Fall'\nand section .year=2017;\nLater, when we study the SQL authorization mechanism in Section 4.7, we shall see\nthat users can be given access to views in place of, or in addition to, access to relations.\nViews di\ufb00er from the with statement in that views, once created, remain available\nuntil explicitly dropped. The named subquery de\ufb01ned by with is local to the query in\nwhich it is de\ufb01ned.\n4.2.2 Using Views in SQL Queries\nOnce we have de\ufb01ned a view, we can use the view name to refer to the virtual relation\nthat the view generates. Using the view physics\n fall\n2017, we can \ufb01nd all Physics courses\no\ufb00ered in the Fall 2017 semester in the Watson building by writing:\n", "167": "4.2 Views 139\nselect course\n id\nfrom physics\n fall\n2017\nwhere building='Watson';\nView names may appear in a query any place where a relation name may appear,\nThe attribute names of a view can be speci\ufb01ed explicitly as follows:\ncreate view departments\n total\n salary (dept\n name ,total\n salary )as\nselect dept\n name ,sum(salary )\nfrom instructor\ngroup by dept\n name ;\nT h ep r e c e d i n gv i e wg i v e sf o re a c hd e p a r t m e n tt h es u mo ft h es a l a r i e so fa l lt h ei n s t r u c -\ntors at that department. Since the expression sum(salary )d o e sn o th a v ean a m e ,t h e\nattribute name is speci\ufb01ed explicitly in the view de\ufb01nition.\nIntuitively, at any given time, the set of tuples in the view relation is the result\nof evaluation of the query expression that de\ufb01nes the view. Thus, if a view relation is\ncomputed and stored, it may become out of date if the relations used to de\ufb01ne it are\nmodi\ufb01ed. To avoid this, views are usually implemented as follows: When we de\ufb01ne a\nview, the database system stores the de\ufb01nition of the view itself, rather than the result\nof evaluation of the query expression that de\ufb01nes the view. Wherever a view relation\nappears in a query, it is replaced by the stored query expression. Thus, whenever we\nevaluate the query, the view relation is recomputed.\nOne view may be used in the expression de\ufb01ning another view. For example, we\ncan de\ufb01ne a view physics\n fall\n2017\n watson that lists the course IDand room number of\nall Physics courses o\ufb00ered in the Fall 2017 semester in the Watson building as follows:\ncreate view physics\n fall\n2017\n watson as\nselect course\n id,room\n number\nfrom physics\n fall\n2017\nwhere building='Watson';\nwhere physics\n fall\n2017\n watson is itself a view relation. This is equivalent to:\ncreate view physics\n fall\n2017\n watson as\nselect course\n id,room\n number\nfrom (select course .course\n id,building ,room\n number\nfrom course ,section\nwhere course .course\n id=section .course\n id\nand course .dept\n name='Physics'\nand section .semester='Fall'\nand section .year=2017)\nwhere building='Watson';\n", "168": "140 Chapter 4 Intermediate SQL\n4.2.3 Materialized Views\nCertain database systems allow view relations to be stored, but they make sure that, if\nthe actual relations used in the view de\ufb01nition change, the view is kept up-to-date. Such\nviews are called materialized views .\nF o re x a m p l e ,c o n s i d e rt h ev i e w departments\n total\n salary . If that view is material-\nized, its results would be stored in the database, allowing queries that use the view to\npotentially run much faster by using the precomputed view result, instead of recomput-\ning it.\nHowever, if an instructor tuple is added to or deleted from the instructor relation,\nthe result of the query de\ufb01ning the view would change, and as a result the materialized\nview\u2019s contents must be updated. Similarly, if an instructor\u2019s salary is updated, the\ntuple in departments\n total\n salary corresponding to that instructor\u2019s department must\nbe updated.\nThe process of keeping the materialized view up-to-date is called materialized view\nmaintenance (or often, just view maintenance ) and is covered in Section 16.5. View\nmaintenance can be done immediately when any of the relations on which the view is\nde\ufb01ned is updated. Some database systems, however, perform view maintenance lazily,\nwhen the view is accessed. Some systems update materialized views only periodically;\nin this case, the contents of the materialized view may be stale, that is, not up-to-date,\nwhen it is used, and it should not be used if the application needs up-to-date data.\nAnd some database systems permit the database administrator to control which of the\npreceding methods is used for each materialized view.\nApplications that use a view frequently may bene\ufb01t if the view is materialized.\nApplications that demand fast response to certain queries that compute aggregates over\nlarge relations can also bene\ufb01t greatly by creating materialized views corresponding to\nthe queries. In this case, the aggregated result is likely to be much smaller than the\nlarge relations on which the view is de\ufb01ned; as a result the materialized view can be\nused to answer the query very quickly, avoiding reading the large underlying relations.\nThe bene\ufb01ts to queries from the materialization of a view must be weighed against the\nstorage costs and the added overhead for updates.\nSQL does not de\ufb01ne a standard way of specifying that a view is materialized,\nbut many database systems provide their own SQL extensions for this task. Some\ndatabase systems always keep materialized views up-to-date when the underlying re-\nlations change, while others permit them to become out of date and periodically re-\ncompute them.\n4.2.4 Update of a View\nAlthough views are a useful tool for queries, they present serious problems if we express\nupdates, insertions, or deletions with them. The di\ufb03culty is that a modi\ufb01cation to the\ndatabase expressed in terms of a view must be translated to a modi\ufb01cation to the actual\nrelations in the logical model of the database.\n", "169": "4.2 Views 141\nSuppose the view faculty , which we saw earlier, is made available to a clerk. Since\nwe allow a view name to appear wherever a relation name is allowed, the clerk can\nwrite:\ninsert into faculty\nvalues ('30765', 'Green', 'Music');\nThis insertion must be represented by an insertion into the relation instructor ,s i n c e\ninstructor is the actual relation from which the database system constructs the view\nfaculty .H o w e v e r ,t oi n s e r tat u p l ei n t o instructor ,w em u s th a v es o m ev a l u ef o r salary .\nThere are two reasonable approaches to dealing with this insertion:\n\u2022Reject the insertion, and return an error message to the user.\n\u2022Insert a tuple ('30765', 'Green', 'Music', null) into the instructor relation.\nAnother problem with modi\ufb01cation of the database through views occurs with a\nview such as:\ncreate view instructor\n infoas\nselect ID,name ,building\nfrom instructor ,department\nwhere instructor .dept\n name=department .dept\n name ;\nThis view lists the ID,name , and building-name of each instructor in the university.\nConsider the following insertion through this view:\ninsert into instructor\n info\nvalues ('69987', 'White', 'Taylor');\nSuppose there is no instructor with ID69987, and no department in the Taylor\nbuilding. Then the only possible method of inserting tuples into the instructor and de-\npartment relations is to insert ('69987', 'White', null,null)i n t o instructor and ( null,\n'Taylor', null)i n t o department . Then we obtain the relations shown in Figure 4.8. How-\never, this update does not have the desired e\ufb00ect, since the view relation instructor\n info\nstill does notinclude the tuple ('69987', 'White', 'Taylor'). Thus, there is no way to up-\ndate the relations instructor and department by using nulls to get the desired update on\ninstructor\n info.\nBecause of problems such as these, modi\ufb01cations are generally not permitted on\nview relations, except in limited cases. Di\ufb00e rent database systems specify di\ufb00erent con-\nditions under which they permit updates on view relations; see the database system\nmanuals for details.\nIn general, an SQL view is said to be updatable (i.e., inserts, updates, or deletes can\nbe applied on the view) if the following conditions are all satis\ufb01ed by the query de\ufb01ning\nthe view:\n\u2022Thefrom clause has only one database relation.\n", "170": "142 Chapter 4 Intermediate SQL\nID\n name\n dept\n name\n salary\n10101\n Srinivasan\n Comp. Sci.\n 65000\n12121\n Wu\n Finance\n 90000\n15151\n Mozart\n Music\n 40000\n22222\n Einstein\n Physics\n 95000\n32343\n El Said\n History\n 60000\n33456\n Gold\n Physics\n 87000\n45565\n Katz\n Comp. Sci.\n 75000\n58583\n Cali\ufb01eri\n History\n 62000\n76543\n Singh\n Finance\n 80000\n76766\n Crick\n Biology\n 72000\n83821\n Brandt\n Comp. Sci.\n 92000\n98345\n Kim\n Elec. Eng.\n 80000\n69987\n White\n null\n null\ninstructor\ndept\n name\n building\n budget\nBiology\n Watson\n 90000\nComp. Sci.\n Taylor\n 100000\nElectrical Eng.\n Taylor\n 85000\nFinance\n Painter\n 120000\nHistory\n Painter\n 50000\nMusic\n Packard\n 80000\nPhysics\n Watson\n 70000\nnull\n Taylor\n null\ndepartment\nFigure 4.8 Relations instructor and department after insertion of tuples.\n\u2022Theselect clause contains only attribute names of the relation and does not have\nany expressions, aggregates, or distinct speci\ufb01cation.\n\u2022Any attribute not listed in the select clause can be set to null; that is, it does not\nhave a not null constraint and is not part of a primary key.\n\u2022The query does not have a group by orhaving clause.\nUnder these constraints, the update ,insert ,a n d delete operations would be allowed on\nthe following view:\n", "171": "4.3 Transactions 143\ncreate view history\n instructors as\nselect *\nfrom instructor\nwhere dept\n name='History';\nEven with the conditions on updatability, the following problem still remains. Sup-\npose that a user tries to insert the tuple ('25566', 'Brown', 'Biology', 100000) into the\nhistory\n instructors view. This tuple can be inserted into the instructor relation, but it\nwould not appear in the history\n instructors view since it does not satisfy the selection\nimposed by the view.\nBy default, SQL would allow the above update to proceed. However, views can be\nde\ufb01ned with a with check option clause at the end of the view de\ufb01nition; then, if a tuple\ninserted into the view does not satisfy the view\u2019s where clause condition, the insertion\nis rejected by the database system. Updates are similarly rejected if the new value does\nnot satisfy the where clause conditions.\nSQL:1999 has a more complex set of rules about when inserts, updates, and deletes\ncan be executed on a view that allows updates through a larger class of views; however,\nthe rules are too complex to be discussed here.\nAn alternative, and often preferable, approach to modifying the database through a\nview is to use the trigger mechanism discussed in Section 5.3. The instead of feature in\ndeclaring triggers allows one to replace the default insert, update, and delete operations\non a view with actions designed especially for each particular case.\n4.3 Transactions\nAtransaction consists of a sequence of query and/or update statements. The SQL stan-\ndard speci\ufb01es that a transaction begins implicitly when an SQL statement is executed.\nOne of the following SQL statements must end the transaction:\n\u2022Commit work commits the current transaction; that is, it makes the updates per-\nformed by the transaction become permanent in the database. After the transac-\ntion is committed, a new transaction is automatically started.\n\u2022Rollback work causes the current transaction to be rolled back; that is, it undoes\nall the updates performed by the SQL statements in the transaction. Thus, the\ndatabase state is restored to what it was before the \ufb01rst statement of the transaction\nwas executed.\nThe keyword work is optional in both the statements.\nTransaction rollback is useful if some error condition is detected during execution\nof a transaction. Commit is similar, in a sense, to saving changes to a document that\nis being edited, while rollback is similar to quitting the edit session without saving\n", "172": "144 Chapter 4 Intermediate SQL\nchanges. Once a transaction has executed commit work , its e\ufb00ects can no longer be\nundone by rollback work . The database system guarantees that in the event of some\nfailure, such as an error in one of the SQL statements, a power outage, or a system\ncrash, a transaction\u2019s e\ufb00ects will be rolled back if it has not yet executed commit work .\nIn the case of power outage or other system crash, the rollback occurs when the system\nrestarts.\nFor instance, consider a banking application where we need to transfer money\nfrom one bank account to another in the same bank. To do so, we need to update\ntwo account balances, subtracting the amount transferred from one, and adding it to\nthe other. If the system crashes after subtracting the amount from the \ufb01rst account\nbut before adding it to the second account, the bank balances will be inconsistent. A\nsimilar problem occurs if the second account is credited before subtracting the amount\nfrom the \ufb01rst account and the system crashes just after crediting the amount.\nAs another example, consider our running example of a university application. We\nassume that the attribute tot\ncred of each tuple in the student relation is kept up-to-\ndate by modifying it whenever the student successfully completes a course. To do so,\nwhenever the takes relation is updated to record successful completion of a course by a\nstudent (by assigning an appropriate grade), the corresponding student tuple must also\nbe updated. If the application performing these two updates crashes after one update\nis performed, but before the second one is performed, the data in the database will be\ninconsistent.\nBy either committing the actions of a transaction after all its steps are completed,\nor rolling back all its actions in case the transaction could not complete all its actions\nsuccessfully, the database provides an abstraction of a transaction as being atomic ,t h a t\nis, indivisible. Either all the e\ufb00ects of the transaction are re\ufb02ected in the database or\nnone are (after rollback).\nApplying the notion of transactions to the above applications, the update state-\nments should be executed as a single transaction. An error while a transaction executes\none of its statements would result in undoing the e\ufb00ects of the earlier statements of the\ntransaction so that the database is not left in a partially updated state.\nIf a program terminates without executing either of these commands, the updates\nare either committed or rolled back. The standard does not specify which of the two\nhappens, and the choice is implementation dependent.\nIn many SQL implementations, including MySQL andPostgre SQL, by default each\nSQL statement is taken to be a transaction on its own, and it gets committed as soon\nas it is executed. Such automatic commit of individual SQL statements must be turned\no\ufb00 if a transaction consisting of multiple SQL statements needs to be executed. How\nto turn o\ufb00 automatic commit depends on the speci\ufb01c SQL implementation, although\nmany databases support the command s e ta u t o c o m m i to \ufb00 .6\n6There is a standard way of turning autocommit on or o\ufb00 when using application program interfaces such as JDBC or\nODBC , which we study in Section 5.1.1 and Section 5.1.3, respectively.\n", "173": "4.4 Integrity Constraints 145\nA better alternative, which is part of the SQL:1999 standard is to allow multiple SQL\nstatements to be enclosed between the keywords begin atomic \u2026end. All the statements\nbetween the keywords then form a single transaction, which is committed by default if\nexecution reaches the endstatement. Only some databases, such as SQL S erver, support\nthe above syntax. However, several other databases, such as MySQL and Postgre SQL,\nsupport a begin statement which starts a transaction containing all subsequent SQL\nstatements, but do not support the endstatement; instead, the transaction must be\nended by either a commit work or arollback work command.\nIf you use a database such as Oracle, where the automatic commit is not the default\nforDML s t a t e m e n t s ,b es u r et oi s s u ea commit command after adding or modifying\ndata, or else when you disconnect, all your database modi\ufb01cations will be rolled back!7\nYou should be aware that although Oracle has automatic commit turned o\ufb00 by default,\nthat default may be overridden by local con\ufb01guration settings.\nWe study further properties of transactions in Chapter 17; issues in implementing\ntransactions are addressed in Chapter 18 and Chapter 19.\n4.4 Integrity Constraints\nIntegrity constraints ensure that changes made to the database by authorized users\ndo not result in a loss of data consistency. Thus, integrity constraints guard against\naccidental damage to the database. This is in contrast to security constraints ,w h i c h\nguard against access to the database by unauthorized users.\nExamples of integrity constraints are:\n\u2022An instructor name cannot be null.\n\u2022No two instructors can have the same instructor ID.\n\u2022Every department name in the course relation must have a matching department\nname in the department relation.\n\u2022The budget of a department must be greater than $0.00.\nIn general, an integrity constraint can be an arbitrary predicate pertaining to the\ndatabase. However, arbitrary predicates may be costly to test. Thus, most database\nsystems allow one to specify only those integrity constraints that can be tested with\nminimal overhead.\nWe have already seen some forms of integrity constraints in Section 3.2.2. We study\nsome more forms of integrity constraints in this section. In Chapter 7, we study another\nform of integrity constraint, called functional dependencies , that is used primarily in the\nprocess of schema design.\n7Oracle does automatically commit DDL statements.\n", "174": "146 Chapter 4 Intermediate SQL\nIntegrity constraints are usually identi\ufb01ed as part of the database schema design\nprocess and declared as part of the create table command used to create relations.\nHowever, integrity constraints can also be added to an existing relation by using the\ncommand alter table table-name add constraint ,w h e r e constraint can be any constraint\non the relation. When such a command is executed, the system \ufb01rst ensures that the re-\nlation satis\ufb01es the speci\ufb01ed constraint. If it does, the constraint is added to the relation;\nif not, the command is rejected.\n4.4.1 Constraints on a Single Relation\nWe described in Section 3.2 how to de\ufb01ne tables using the create table command. The\ncreate table command may also include integrity-constraint statements. In addition to\nthe primary-key constraint, there are a number of other ones that can be included in\nthecreate table command. The allowed integrity constraints include\n\u2022not null\n\u2022unique\n\u2022check (<predicate >)\nWe cover each of these types of constraints in the following sections.\n4.4.2 Not Null Constraint\nAs we discussed in Chapter 3, the null value is a member of all domains, and as a result\nit is a legal value for every attribute in SQL by default. For certain attributes, however,\nnull values may be inappropriate. Consider a tuple in the student relation where name\nisnull. Such a tuple gives student information for an unknown student; thus, it does not\ncontain useful information. Similarly, we would not want the department budget to be\nnull. In cases such as this, we wish to forbid null values, and we can do so by restricting\nthe domain of the attributes name and budget to exclude null values, by declaring it as\nfollows:\nname varchar (20) not null\nbudget numeric (12,2) not null\nThenot null constraint prohibits the insertion of a null value for the attribute, and is\nan example of a domain constraint . Any database modi\ufb01cation that would cause a null\nto be inserted in an attribute declared to be not null generates an error diagnostic.\nThere are many situations where we want to avoid null values. In particular, SQL\nprohibits null values in the primary key of a relation schema. Thus, in our university\nexample, in the department relation, if the attribute dept\n name is declared as the primary\nkey for department ,i tc a n n o tt a k ean u l lv a l u e .A sar e s u l ti tw o u l dn o tn e e dt ob e\ndeclared explicitly to be not null .\n", "175": "4.4 Integrity Constraints 147\n4.4.3 Unique Constraint\nSQL also supports an integrity constraint:\nunique (Aj1,Aj2,\u2026,Ajm)\nTheunique speci\ufb01cation says that attributes Aj1,Aj2,\u2026,Ajmform a superkey; that is, no\ntwo tuples in the relation can be equal on all the listed attributes. However, attributes\ndeclared as unique are permitted to be nullunless they have explicitly been declared to\nbenot null . Recall that a null value does not equal any other value. (The treatment of\nnulls here is the same as that of the unique construct de\ufb01ned in Section 3.8.4.)\n4.4.4 The Check Clause\nWhen applied to a relation declaration, the clause check (P) speci\ufb01es a predicate Pthat\nmust be satis\ufb01ed by every tuple in a relation.\nA common use of the check clause is to ensure that attribute values satisfy speci-\n\ufb01ed conditions, in e\ufb00ect creating a powerful type system. For instance, a clause check\n(budget >0 )i nt h e create table command for relation department would ensure that the\nvalue of budget is nonnegative.\nAs another example, consider the following:\ncreate table section\n(course\n id varchar (8),\nsec\nid varchar (8),\nsemester varchar (6),\nyear numeric (4,0),\nbuilding varchar (15),\nroom\n number varchar (7),\ntime\n slot\nid varchar (4),\nprimary key (course\n id,sec\nid,semester ,year),\ncheck (semester in('Fall', 'Winter', 'Spring', 'Summer')));\nHere, we use the check clause to simulate an enumerated type by specifying that\nsemester must be one of 'Fall', 'Winter', 'Spring', or 'Summer'. Thus, the check clause\npermits attribute domains to be restricted in powerful ways that most programming-\nlanguage type systems do not permit.\nNull values present an interesting special case in the evaluation of a check clause.\nAcheck clause is satis\ufb01ed if it is not false, so clauses that evaluate to unknown are not\nviolations. If null values are not desired, a separate not null constraint (see Section\n4.4.2) must be speci\ufb01ed.\nAcheck clause may appear on its own, as shown above, or as part of the declaration\nof an attribute. In Figure 4.9, we show the check constraint for the semester attribute\n", "176": "148 Chapter 4 Intermediate SQL\ncreate table classroom\n(building varchar (15),\nroom\n number varchar (7),\ncapacity numeric (4,0),\nprimary key (building ,room\n number ));\ncreate table department\n(dept\n name varchar (20),\nbuilding varchar (15),\nbudget numeric (12,2) check (budget >0 ) ,\nprimary key (dept\n name ));\ncreate table course\n(course\n id varchar (8),\ntitle varchar (50),\ndept\n name varchar (20),\ncredits numeric (2,0) check (credits >0 ) ,\nprimary key (course\n id),\nforeign key (dept\n name )references department );\ncreate table instructor\n(ID varchar (5),\nname varchar (20) not null ,\ndept\n name varchar (20),\nsalary numeric (8,2) check (salary > 29000),\nprimary key (ID),\nforeign key (dept\n name )references department );\ncreate table section\n(course\n id varchar (8),\nsec\nid varchar (8),\nsemester varchar (6)check (semester in\n(\u2019Fall\u2019, \u2019Winter\u2019, \u2019Spring\u2019, \u2019Summer\u2019)),\nyear numeric (4,0) check (year >1 7 5 9a n d year < 2100),\nbuilding varchar (15),\nroom\n number varchar (7),\ntime\n slot\nid varchar (4),\nprimary key (course\n id,sec\nid,semester ,year),\nforeign key (course\n id)references course ,\nforeign key (building ,room\n number )references classroom );\nFigure 4.9 SQL data definition for part of the university database.\n", "177": "4.4 Integrity Constraints 149\nas part of the declaration of semester . The placement of a check clause is a matter of\ncoding style. Typically, constraints on the value of a single attribute are listed with that\nattribute, while more complex check clauses are listed separately at the end of a create\ntable statement.\nThe predicate in the check clause can, according to the SQL standard, be an ar-\nbitrary predicate that can include a subquery. However, currently none of the widely\nused database products allows the predicate to contain a subquery.\n4.4.5 Referential Integrity\nOften, we wish to ensure that a value that appears in one relation (the referencing rela-\ntion) for a given set of attributes also appears for a certain set of attributes in another\nrelation (the referenced relation). As we saw earlier, in Section 2.3, such conditions\nare called referential integrity constraints ,a n d foreign keys are a form of a referential in-\ntegrity constraint where the referenced attributes form a primary key of the referenced\nrelation.\nForeign keys can be speci\ufb01ed as part of the SQL create table statement by using the\nforeign key clause, as we saw in Section 3.2.2. We illustrate foreign-key declarations by\nusing the SQL DDL de\ufb01nition of part of our university database, shown in Figure 4.9.\nThe de\ufb01nition of the course table has a declaration\n\u201cforeign key (dept\n name )references department \u201d.\nThis foreign-key declaration speci\ufb01es that for each course tuple, the department name\nspeci\ufb01ed in the tuple must exist in the department relation. Without this constraint, it\nis possible for a course to specify a nonexistent department name.\nBy default, in SQL a foreign key references the primary-key attributes of the ref-\nerenced table. SQL also supports a version of the references clause where a list of at-\ntributes of the referenced relation can be speci\ufb01ed explicitly.8For example, the foreign\nkey declaration for the course relation can be speci\ufb01ed as:\nforeign key (dept\n name )references department (dept\n name )\nThe speci\ufb01ed list of attributes must, however, be declared as a superkey of the\nreferenced relation, using either a primary key constraint or a unique constraint. A\nmore general form of a referential-integrity constraint, where the referenced columns\nneed not be a candidate key, cannot be directly speci\ufb01ed in SQL.T h e SQL standard\nspeci\ufb01es other constructs that can be used to implement such constraints, which are\ndescribed in Section 4.4.8; however, these a lternative constructs are not supported by\nany of the widely used database systems.\nNote that the foreign key must reference a compatible set of attributes, that is, the\nnumber of attributes must be the same and the data types of corresponding attributes\nmust be compatible.\n8Some systems, notably MySQL, do not support the default and require that the attributes of the referenced relations\nbe speci\ufb01ed.\n", "178": "150 Chapter 4 Intermediate SQL\nWe can use the following as part of a table de\ufb01nition to declare that an attribute\nforms a foreign key:\ndept\n name varchar (20) references department\nWhen a referential-integrity constraint is violated, the normal procedure is to reject\nthe action that caused the violation (i.e., the transaction performing the update action\nis rolled back). However, a foreign key clause can specify that if a delete or update action\non the referenced relation violates the constraint, then, instead of rejecting the action,\nthe system must take steps to change the tuple in the referencing relation to restore the\nconstraint. Consider this de\ufb01nition of an integrity constraint on the relation course :\ncreate table course\n(\u2026\nforeign key (dept\n name )references department\non delete cascade\non update cascade ,\n\u2026);\nBecause of the clause on delete cascade associated with the foreign-key declaration, if a\ndelete of a tuple in department results in this referential-integrity constraint being vio-\nlated, the system does not reject the delete. Instead, the delete \u201c cascades \u201dt ot h e course\nrelation, deleting the tuple that refers to the department that was deleted. Similarly, the\nsystem does not reject an update to a \ufb01eld referenced by the constraint if it violates the\nconstraint; instead, the system updates the \ufb01eld dept\n name in the referencing tuples in\ncourse to the new value as well. SQL also allows the foreign key clause to specify actions\nother than cascade , if the constraint is violated: The referencing \ufb01eld (here, dept\n name )\ncan be set to null(by using set null in place of cascade ), or to the default value for the\ndomain (by using set default ).\nIf there is a chain of foreign-key dependencies across multiple relations, a deletion\nor update at one end of the chain can propagate across the entire chain. An interesting\ncase where the foreign key constraint on a relation references the same relation appears\nin Exercise 4.9. If a cascading update or delete causes a constraint violation that cannot\nbe handled by a further cascading operation, the system aborts the transaction. As a\nresult, all the changes caused by the transaction and its cascading actions are undone.\nNull values complicate the semantics of referential-integrity constraints in SQL.\nAttributes of foreign keys are allowed to be null, provided that they have not otherwise\nbeen declared to be not null . If all the columns of a foreign key are nonnull in a given\ntuple, the usual de\ufb01nition of foreign-key constraints is used for that tuple. If any of the\nforeign-key columns is null, the tuple is de\ufb01ned automatically to satisfy the constraint.\nThis de\ufb01nition may not always be the right choice, so SQL also provides constructs that\nallow you to change the behavior with null values; we do not discuss the constructs here.\n", "179": "4.4 Integrity Constraints 151\n4.4.6 Assigning Names to Constraints\nIt is possible for us to assign a name to integrity constraints. Such names are useful if\nwe want to drop a constraint that was de\ufb01ned previously.\nTo name a constraint, we precede the constraint with the keyword constraint and\nt h en a m ew ew i s ht oa s s i g ni t .S o ,f o re x a m p l e ,i fw ew i s ht oa s s i g nt h en a m e minsalary\nto the check constraint on the salary attribute of instructor (see Figure 4.9), we would\nmodify the declaration for salary to:\nsalary numeric (8,2), constraint minsalary check (salary >29000),\nLater, if we decide we no longer want this constraint, we can write:\nalter table instructor drop constraint minsalary ;\nLacking a name, we would need \ufb01rst to use system-speci\ufb01c features to identify the\nsystem-assigned name for the constraint. Not all systems support this, but, for example,\nin Oracle, the system table user\n constraints contains this information.\n4.4.7 Integrity Constraint Violation During a Transaction\nTransactions may consist of several steps, and integrity constraints may be violated\ntemporarily after one step, but a later step may remove the violation. For instance,\nsuppose we have a relation person with primary key name , and an attribute spouse ,a n d\nsuppose that spouse is a foreign key on person . That is, the constraint says that the spouse\nattribute must contain a name that is present in the person table. Suppose we wish to\nnote the fact that John and Mary are married to each other by inserting two tuples,\none for John and one for Mary, in the preceding relation, with the spouse attributes\nset to Mary and John, respectively. The insertion of the \ufb01rst tuple would violate the\nforeign-key constraint, regardless of which of the two tuples is inserted \ufb01rst. After the\nsecond tuple is inserted, the foreign-key constraint would hold again.\nTo handle such situations, the SQL standard allows a clause initially deferred to\nbe added to a constraint speci\ufb01cation; the constraint would then be checked at the\nend of a transaction and not at intermediate steps. A constraint can alternatively be\nspeci\ufb01ed as deferrable , which means it is checked immediately by default but can be\ndeferred when desired. For constraints declared as deferrable, executing a statement\nset constraints constraint-list deferred as part of a transaction causes the checking of\nthe speci\ufb01ed constraints to be deferred to the end of that transaction. Constraints that\nare to appear in a constraint list must have names assigned. The default behavior is\nto check constraints immediately, and many database implementations do not support\ndeferred constraint checking.\nWe can work around the problem in the preceding example in another way, if the\nspouse attribute can be set to null: We set the spouse attributes to nullwhen inserting the\n", "180": "152 Chapter 4 Intermediate SQL\ntuples for John and Mary, and we update them later. However, this technique requires\nmore programming e\ufb00ort, and it does not work if the attributes cannot be set to null.\n4.4.8 Complex Check Conditions and Assertions\nThere are additional constructs in the SQL standard for specifying integrity constraints\nthat are not currently supported by most systems. We discuss some of these in this\nsection.\nAs de\ufb01ned by the SQL standard, the predicate in the check clause can be an ar-\nbitrary predicate that can include a subquery. If a database implementation supports\nsubqueries in the check clause, we could specify the following referential-integrity con-\nstraint on the relation section :\ncheck (time\n slot\nidin(select time\n slot\nidfrom time\n slot))\nThecheck condition veri\ufb01es that the time\n slot\nidin each tuple in the section relation is\nactually the identi\ufb01er of a time slot in the time\n slotrelation. Thus, the condition has to\nbe checked not only when a tuple is inserted or modi\ufb01ed in section , but also when the\nrelation time\n slotchanges (in this case, when a tuple is deleted or modi\ufb01ed in relation\ntime\n slot).\nAnother natural constraint on our university schema would be to require that every\nsection has at least one instructor teaching the section. In an attempt to enforce this,\nwe may try to declare that the attributes ( course\n id,sec\nid,semester ,year)o ft h e section\nrelation form a foreign key referencing the corresponding attributes of the teaches rela-\ntion. Unfortunately, these attributes do not form a candidate key of the relation teaches .\nA check constraint similar to that for the time\n slotattribute can be used to enforce this\nconstraint, if check constraints with subqueries were supported by a database system.\nComplex check conditions can be useful when we want to ensure the integrity of\nd a t a ,b u tt h e ym a yb ec o s t l yt ot e s t .I no u re x a m p l e ,t h ep r e d i c a t ei nt h e check clause\nwould not only have to be evaluated when a modi\ufb01cation is made to the section relation,\nbut it may have to be checked if a modi\ufb01cation is made to the time\n slotrelation because\nthat relation is referenced in the subquery.\nAnassertion is a predicate expressing a condition that we wish the database always\nto satisfy. Consider the following constraints, which can be expressed using assertions.\n\u2022For each tuple in the student relation, the value of the attribute tot\ncredmust equal\nthe sum of credits of courses that the student has completed successfully.\n\u2022An instructor cannot teach in two di\ufb00erent classrooms in a semester in the same\ntime slot.9\n9We assume that lectures are not displayed remotely in a se cond classroom! An alternative constraint that speci\ufb01es\nthat \u201can instructor cannot teach two courses in a given semester in the same time slot\u201d may not hold since courses are\nsometimes cross-listed; that is, the same co urse is given two identi\ufb01ers and titles.\n", "181": "4.5 SQL Data Types and Schemas 153\ncreate assertion credits\n earned\n constraint check\n(not exists (select ID\nfrom student\nwhere tot\ncred<>(select coalesce (sum(credits ), 0)\nfrom takes natural join course\nwhere student .ID=takes .ID\nand grade is not null and grade <>\u2019F\u2019 )))\nFigure 4.10 An assertion example.\nAn assertion in SQL takes the form:\ncreate assertion <assertion-name >check <predicate >;\nIn Figure 4.10, we show how the \ufb01rst example of constraints can be written in SQL.\nSince SQL does not provide a \u201cfor all X,P(X)\u201d construct (where Pis a predicate), we\nare forced to implement the constraint by an equivalent construct, \u201cnot exists Xsuch\nthat not P(X)\u201d, that can be expressed in SQL.\nWe leave the speci\ufb01cation of the second constraint as an exercise. Although these\ntwo constraints can be expressed using check predicates, using an assertion may be\nmore natural, especially for the second constraint.\nWhen an assertion is created, the system tests it for validity. If the assertion is valid,\nthen any future modi\ufb01cation to the database is allowed only if it does not cause that\nassertion to be violated. This testing may introduce a signi\ufb01cant amount of overhead\nif complex assertions have been made. Hence, assertions should be used with great\ncare. The high overhead of testing and maintaining assertions has led some system\ndevelopers to omit support for general assertions, or to provide specialized forms of\nassertion that are easier to test.\nCurrently, none of the widely used database systems supports either subqueries in\nthecheck clause predicate or the create assertion construct. However, equivalent func-\ntionality can be implemented using triggers, which are described in Section 5.3, if they\nare supported by the database system. Section 5.3 also describes how the referential\nintegrity constraint on time\n slot\nidcan be implemented using triggers.\n4.5 SQL Data Types and Schemas\nIn Chapter 3, we covered a number of built-in data types supported in SQL,s u c ha s\ninteger types, real types, and character types. There are additional built-in data types\nsupported by SQL, which we describe below. We also describe how to create basic\nuser-de\ufb01ned types in SQL.\n", "182": "154 Chapter 4 Intermediate SQL\n4.5.1 Date and Time Types in SQL\nIn addition to the basic data types we introduced in Section 3.2, the SQL standard\nsupports several data types relating to dates and times:\n\u2022date: A calendar date containing a (four-digit) year, month, and day of the month.\n\u2022time: The time of day, in hours, minutes, and seconds. A variant, time(p), can be\nused to specify the number of fractional di gits for seconds (the default being 0).\nIt is also possible to store time-zone infor mation along with the time by specifying\ntime with timezone .\n\u2022timestamp : A combination of date andtime.Av a r i a n t , timestamp (p), can be used\nto specify the number of fractional digits for seconds (the default here being 6).\nTime-zone information is also stored if with timezone is speci\ufb01ed.\nDate and time values can be speci\ufb01ed like this:\ndate '2018-04-25'\ntime '09:30:00'\ntimestamp '2018-04-25 10:29:01.45'\nDates must be speci\ufb01ed in the format year followed by month followed by day, as\nshown.10The seconds \ufb01eld of time ortimestamp can have a fractional part, as in the\ntimestamp above.\nTo extract individual \ufb01elds of a dateortime value d,w ec a nu s e extract (\ufb01eld from\nd), where \ufb01eld can be one of y e a r ,m o n t h ,d a y ,h o u r ,m i n u t e ,o rsecond .T i m e - z o n e\ninformation can be extracted using timezone\n hour andtimezone\n minute .\nSQL de\ufb01nes several functions to get the current date and time. For example, cur-\nrent\n date returns the current date, current\n time returns the current time (with time\nzone), and localtime returns the current local time (without time zone). Timestamps\n(date plus time) are returned by current\n timestamp (with time zone) and localtimes-\ntamp (local date and time without time zone).\nSome systems, including MySQL o\ufb00er the datetime data type that represents a time\nthat is not adjustable for time zone. In practice, speci\ufb01cation of time has numerous\nspecial cases, including the use of standard time versus \u201cdaylight\u201d or \u201csummer\u201d time.\nSystems vary in the range of times representable.\nSQL allows comparison operations on all the types listed here, and it allows both\narithmetic and comparison operations on the various numeric types. SQL also provides\nad a t at y p ec a l l e d interval , and it allows computations based on dates and times and\non intervals. For example, if xand yare of type date,t h e n x\u2212yis an interval whose\nvalue is the number of days from date xto date y. Similarly, adding or subtracting an\ninterval from a date or time gives back a date or time, respectively.\n10Many database systems o\ufb00er greater \ufb02exibility in default conversions of strings to dates and timestamps.\n", "183": "4.5 SQL Data Types and Schemas 155\n4.5.2 Type Conversion and Formatting Functions\nAlthough systems perform some data type conversions automatically, others need to\nbe requested explicitly. We can use an expression of the form cast(east)t oc o n v e r t\nan expression eto the type t. Data-type conversions may be needed to perform certain\noperations or to enforce certain sort orders. For example, consider the IDattribute of\ninstructor , which we have speci\ufb01ed as being a string ( varchar (5)). If we were to order\noutput by this attribute, the ID11111 comes before the ID9, because the \ufb01rst character,\n'1', comes before '9'. However, if we were to write:\nselect cast (IDas numeric (5))asinst\n id\nfrom instructor\norder by inst\n id\nthe result would be the sorted order we desire.\nA di\ufb00erent type of conversion may be required for data to be displayed as the result\nof a query. For example, we may wish numbers to be shown with a speci\ufb01c number\nof digits, or data to be displayed in a particular format (such as month-day-year or\nday-month-year). These changes in display format are not conversion of data type but\nrather conversion of format. Database systems o\ufb00er a variety of formatting functions,\nand details vary among the leading systems. MySQL o\ufb00ers a format function. Oracle\nand Postgre SQL o\ufb00er a set of functions, to\nchar, to\n number, andto\ndate.SQL S erver\no\ufb00ers a convert function.\nAnother issue in displaying results is the handling of null values. In this text, we use\nnullfor clarity of reading, but the default in most systems is just to leave the \ufb01eld blank.\nWe can choose how null values are output in a query result using the coalesce function.\nIt takes an arbitrary number of arguments, all of which must be of the same type, and\nreturns the \ufb01rst non-null argument. For example, if we wished to display instructor IDs\nand salaries but to show null salaries as 0, we would write:\nselect ID,coalesce (salary ,0 )assalary\nfrom instructor\nA limitation of coalesce is the requirement that all the arguments must be of the same\ntype. If we had wanted null salaries to appear as 'N/A' to indicate \u201cnot available\u201d, we\nwould not be able to use coalesce .S y s t e m - s p e c i \ufb01 cf u n c t i o n s ,s u c ha sO r a c l e \u2019 s decode ,\ndo allow such conversions. The general form of decode is:\ndecode (value, match-1, replacement-1, match-2, replacement-2, \u2026,\nmatch-N, replacement-N, default-replacement );\nIt compares value against the match values and if a match is found, it replaces the at-\ntribute value with the corresponding replacement value. If no match succeeds, then\nthe attribute value is replaced with the defau lt replacement value. There are no require-\n", "184": "156 Chapter 4 Intermediate SQL\nments that datatypes match. Conveniently, the value nullmay appear as a match value\nand, unlike the usual case, nullis treated as being equal to null. Thus, we could replace\nnull salaries with 'N/A' as follows:\nselect ID,decode (salary ,null,' N / A ' , salary )assalary\nfrom instructor\n4.5.3 Default Values\nSQL allows a default value to be speci\ufb01ed for an attribute as illustrated by the following\ncreate table statement:\ncreate table student\n(ID varchar (5),\nname varchar (20) not null ,\ndept\n name varchar (20),\ntot\ncred numeric (3,0) default 0,\nprimary key (ID));\nThe default value of the tot\ncred attribute is declared to be 0. As a result, when a tuple\nis inserted into the student relation, if no value is provided for the tot\ncredattribute, its\nvalue is set to 0. The following insert statement illustrates how an insertion can omit\nthe value for the tot\ncredattribute.\ninsert into student (ID,name ,dept\n name )\nvalues ('12789', 'Newman', 'Comp. Sci.');\n4.5.4 Large-Object Types\nMany database applications need to store attributes whose domain consists of large\ndata items such as a photo, a high-resolution medical image, or a video. SQL, therefore,\nprovides large-object data types for character data ( clob) and binary data ( blob). The\nletters \u201clob\u201d in these data types stand for \u201cLarge OBject.\u201d For example, we may declare\nattributes\nbook\n review clob(10KB)\nimage blob(10MB)\nmovie blob(2GB)\nFor result tuples containing large objects (multiple megabytes to gigabytes), it is\nine\ufb03cient or impractical to retrieve an entire large object into memory. Instead, an\napplication would usually use an SQL query to retrieve a \u201clocator\u201d for a large object\nand then use the locator to manipulate the object from the host language in which\nthe application itself is written. For instance, the JDBC application program interface\n(described in Section 5.1.1) permits a locator to be fetched instead of the entire large\n", "185": "4.5 SQL Data Types and Schemas 157\nNote 4.2 TEMPORAL VALIDITY\nIn some situations, there is a need to include historical data, as, for example, if we\nwish to store not only the current salary of each instructor but also entire salary\nhistories. It is easy enough to do this by adding two attributes to the instructor\nrelation schema indicating the starting date for a given salary value and another\nindicating the end date. Then, an instructor may have several salary values, each\ncorresponding to a speci\ufb01c pair of start and end dates. Those start and end dates\nare called the valid time values for the corresponding salary value.\nObserve that there may now be more than one tuple in the instructor relation\nwith the same value of ID. Issues in specifying primary key and foreign key con-\nstraints in the context of such temporal data are discussed in Section 7.10.\nFor a database system to support such temporal constructs, a \ufb01rst step is to\nprovide syntax to specify that certain attributes de\ufb01ne a valid time interval. We use\nOracle 12\u2019s syntax as an example. The SQL DDL forinstructor is augmented using\naperiod declaration as follows, to indicate that start\n date and end\n date attributes\nspecify a valid-time interval.\ncreate table instructor\n(\u2026\nstart\n date date,\nend\n date date,\nperiod for valid\n time (start\n date, end\n date) ,\n\u2026);\nOracle 12c also provides several DML extensions to ease querying with temporal\ndata. The as of period for construct can then be used in query to fetch only those\ntuples whose valid time period includes a speci\ufb01c time. To \ufb01nd instructors and\ntheir salaries as of some time in the past, say January 20, 2014, we write:\nselect name ,salary ,start\n date, end\n date\nfrom instructor as of period for valid\n time '20-JAN-2014';\nIf we wish to \ufb01nd tuples whose period of validity includes all or part of a period\nof time, say, January 20, 2014 to January 30, 2014, we write:\nselect name ,salary ,start\n date, end\n date\nfrom instructor versions period for valid\n time between '20-JAN-2014' and'30-JAN-2014';\nOracle 12c also implements a feature that allows stored database procedures (cov-\nered in Chapter 5) to be run as of a speci\ufb01ed time period.\nThe above constructs ease the speci\ufb01cation of the queries, although the queries\ncan be written without using the constructs.\n", "186": "158 Chapter 4 Intermediate SQL\nobject; the locator can then be used to fetch the large object in small pieces, rather than\nall at once, much like reading data from an operating system \ufb01le using a read function\ncall.\n4.5.5 User-Defined Types\nSQL supports two forms of user-de\ufb01ned data types . The \ufb01rst form, which we cover here,\nis called distinct types . The other form, called structured data types , allows the creation\nof complex data types with nested record structures, arrays, and multisets. We do not\ncover structured data types in this chapter, but we describe them in Section 8.2.\nIt is possible for several attributes to have the same data type. For example, the\nname attributes for student name and instructor name might have the same domain:\nthe set of all person names. However, the domains of budget and dept\n name certainly\nought to be distinct. It is perhaps less clear whether name and dept\n name should have\nthe same domain. At the implementation level, both instructor names and department\nnames are character strings. However, we would normally not consider the query \u201cFind\nall instructors who have the same name as a department\u201d to be a meaningful query.\nThus, if we view the database at the conceptual, rather than the physical, level, name\nand dept\n name should have distinct domains.\nMore importantly, at a practical level, assigning an instructor\u2019s name to a depart-\nment name is probably a programming error; similarly, comparing a monetary value\nexpressed in dollars directly with a monetary value expressed in pounds is also almost\nsurely a programming error. A good type system should be able to detect such assign-\nments or comparisons. To support such checks, SQL provides the notion of distinct\ntypes .\nThecreate type clause can be used to de\ufb01ne new types. For example, the statements:\ncreate type Dollars as numeric (12,2) \ufb01nal;\ncreate type Pounds as numeric (12,2) \ufb01nal;\nde\ufb01ne the user-de\ufb01ned types Dollars and Pounds to be decimal numbers with a total of\n12 digits, two of which are placed after the decimal point.11The newly created types\ncan then be used, for example, as types of attributes of relations. For example, we can\ndeclare the department table as:\ncreate table department\n(dept\n name varchar (20),\nbuilding varchar (15),\nbudget Dollars );\nAn attempt to assign a value of type Dollars to a variable of type Pounds results in a\ncompile-time error, although both are of the same numeric type. Such an assignment\nis likely to be due to a programmer error, where the programmer forgot about the\n11The keyword \ufb01nal isn\u2019t really meaningful in this context but is required by the SQL:1999 standard for reasons we won\u2019t\nget into here; some implementations allow the \ufb01nal keyword to be omitted.\n", "187": "4.5 SQL Data Types and Schemas 159\ndi\ufb00erences in currency. Declaring di\ufb00erent types for di\ufb00erent currencies helps catch\nsuch errors.\nAs a result of strong type checking, the expression ( department.budget +20) would\nnot be accepted since the attribute and the integer constant 20 have di\ufb00erent types.\nAs we saw in Section 4.5.2, values of one type can be converted to another domain, as\nillustrated below:\ncast(department.budget tonumeric (12,2))\nWe could do addition on the numeric type, but to save the result back to an attribute\nof type Dollars we would have to use another cast expression to convert the type back\ntoDollars .\nSQL provides drop type andalter type clauses to drop or modify types that have\nbeen created earlier.\nEven before user-de\ufb01ned types were added to SQL (inSQL:1999 ),SQL had a similar\nbut subtly di\ufb00erent notion of domain (introduced in SQL-92 ), which can add integrity\nconstraints to an underlying type. For example, we could de\ufb01ne a domain DDollars as\nfollows.\ncreate domain DDollars as numeric (12,2) not null ;\nThe domain DDollars can be used as an attribute type, just as we used the type Dollars .\nHowever, there are two signi\ufb01cant di\ufb00erences between types and domains:\n1.Domains can have constraints, such as not null , speci\ufb01ed on them, and can have\ndefault values de\ufb01ned for variables of the domain type, whereas user-de\ufb01ned types\ncannot have constraints or default values speci\ufb01ed on them. User-de\ufb01ned types\nare designed to be used not just for specifying attribute types, but also in proce-\ndural extensions to SQL where it may not be possible to enforce constraints.\n2.Domains are not strongly typed. As a result, values of one domain type can be\nassigned to values of another domain type as long as the underlying types are\ncompatible.\nWhen applied to a domain, the check clause permits the schema designer to specify\na predicate that must be satis\ufb01ed by any attribute declared to be from this domain. For\ninstance, a check clause can ensure that an instructor\u2019s salary domain allows only values\ngreater than a speci\ufb01ed value:\ncreate domain YearlySalary numeric (8,2)\nconstraint salary\n value\n testcheck (value >=29000.00);\nThe domain YearlySalary has a constraint that ensures that the YearlySalary is greater\nthan or equal to $29,000.00. The clause constraint salary\n value\n testis optional and is\n", "188": "160 Chapter 4 Intermediate SQL\nNote 4.3 SUPPORT FOR TYPES AND DOMAINS\nAlthough the create type andcreate domain constructs described in this section\nare part of the SQL standard, the forms of these constructs described here are\nnot fully supported by most database implementations. Postgre SQL supports the\ncreate domain construct, but its create type construct has a di\ufb00erent syntax and\ninterpretation.\nIBM DB2 supports a version of the create type that uses the syntax create dis-\ntinct type , but it does not support create domain .M i c r o s o f t SQL S erver implements\na version of create type construct that supports domain constraints, similar to the\nSQL create domain construct.\nOracle does not support either construct as described here. Oracle, IBM DB2 ,\nPostgre SQL,a n d SQL S erver all support object-oriented type systems using di\ufb00er-\nent forms of the create type construct.\nHowever, SQL also de\ufb01nes a more complex object-oriented type system, which\nwe study in Section 8.2. Types may have structure within them, like, for example,\naName type consisting of \ufb01rstname and lastname . Subtyping is allowed as well;\nfor example, a Person type may have subtypes Student ,Instructor , etc. Inheritance\nrules are similar to those in object-oriented programming languages. It is possible\nto use references to tuples that behave much like references to objects in object-\noriented programming languages. SQL allows array and multiset datatypes along\nwith ways to manipulate those types.\nWe do not cover the details of these features here. Database systems di\ufb00er in\nhow they implement them, if they are implemented at all.\nused to give the name salary\n value\n testt ot h ec o n s t r a i n t .T h en a m ei su s e db yt h es y s t e m\nto indicate the constraint that an update violated.\nAs another example, a domain can be restricted to contain only a speci\ufb01ed set of\nvalues by using the inclause:\ncreate domain degree\n level varchar (10)\nconstraint degree\n level\n test\ncheck (value in ('Bachelors', 'Masters', 'Doctorate'));\n4.5.6 Generating Unique Key Values\nIn our university example, we have seen primary-key attributes with di\ufb00erent data types.\nSome, like dept\n name , hold actual real-world information. Others, like ID,h o l dv a l -\nues created by the enterprise solely for identi\ufb01cation purposes. Those latter types of\nprimary-key domains generate the practical problem of new-value creation. Suppose\n", "189": "4.5 SQL Data Types and Schemas 161\nthe university hires a new instructor. What IDshould be assigned? How do we deter-\nmine that the new IDis unique? Although it is possible to write an SQL statement to\ndo this, such a statement would need to check all preexisting IDs, which would harm\nsystem performance. Alternatively, one could set up a special table holding the largest\nIDvalue issued so far. Then, when a new IDis needed, that value can be incremented\nto the next one in sequence and stored as the new largest value.\nDatabase systems o\ufb00er automatic management of unique key-value generation. The\nsyntax di\ufb00ers among the most popular systems and, sometimes, between versions of\nsystems. The syntax we show here is close to that of Oracle and DB2. Suppose that\ninstead of declaring instructor IDsi nt h e instructor relation as \u201c IDvarchar (5)\u201d, we in-\nstead choose to let the system select a unique instructor IDvalue. Since this feature\nworks only for numeric key- value data types, we change the type of IDtonumber ,a n d\nwrite:\nIDnumber (5)generated always as identity\nWhen the always option is used, any insert statement must avoid specifying a value\nfor the automatically generated key. To do this, use the syntax for insert in which the\nattribute order is speci\ufb01ed (see Section 3.9.2). For our example of instructor , we need\nspecify only the values for name ,dept\n name ,a n d salary , as shown in the following ex-\nample:\ninsert into instructor (name ,dept\n name ,salary )\nvalues ('Newprof', 'Comp. Sci.', 100000);\nThe generated IDvalue can be found via a normal select query. If we replace always\nwith by default , we have the option of specifying our own choice of IDor relying on the\nsystem to generate one.\nInPostgre SQL, we can de\ufb01ne the type of IDasserial , which tells Postgre SQL to au-\ntomatically generate identi\ufb01ers; in MySQL we use auto\n increment in place of generated\nalways as identity , while in SQL S erver we can use just identity .\nAdditional options can be speci\ufb01ed, with the identity speci\ufb01cation, depending on\nthe database, including setting minimum and maximum values, choosing the starting\nvalue, choosing the increment from one value to the next, and so on.\nFurther, many databases support a create sequence construct, which creates a se-\nquence counter object separate from any relation, and allow SQL queries to get the\nnext value from the sequence. Each call to get the next value increments the sequence\ncounter. See the system manuals of the database to \ufb01nd the exact syntax for creating\nsequences, and for retrieving the next value. Using sequences, we can generate iden-\nti\ufb01ers that are unique across multiple relations, for example, across student .ID,a n d\ninstructor .ID.\n", "190": "162 Chapter 4 Intermediate SQL\n4.5.7 Create Table Extensions\nApplications often require the creation of tables that have the same schema as an ex-\nisting table. SQL provides a create table like extension to support this task:12\ncreate table temp\n instructor like instructor ;\nThe above statement creates a new table temp\n instructor that has the same schema as\ninstructor .\nWhen writing a complex query, it is often useful to store the result of a query as\na new table; the table is usually temporary. Two statements are required, one to create\nthe table (with appropriate columns) and the second to insert the query result into the\ntable. SQL:2003 provides a simpler technique to create a table containing the results of\na query. For example, the following statement creates a table t1containing the results\nof a query.\ncreate table t1as\n(select *\nfrom instructor\nwhere dept\n name =' M u s i c ' )\nwith data ;\nBy default, the names and data types of the columns are inferred from the query result.\nNames can be explicitly given to the columns by listing the column names after the\nrelation name.\nAs de\ufb01ned by the SQL:2003 standard, if the with data clause is omitted, the table\nis created but not populated with data. However, many implementations populate the\ntable with data by default even if the with data clause is omitted. Note that several\nimplementations support the functionality of create table \u2026likeandcreate table \u2026as\nusing di\ufb00erent syntax; see the respective system manuals for further details.\nThe above create table \u2026asstatement, closely resembles the create view statement\nand both are de\ufb01ned by using queries. The main di\ufb00erence is that the contents of the\ntable are set when the table is created, whereas the contents of a view always re\ufb02ect the\ncurrent query result.\n4.5.8 Schemas, Catalogs, and Environments\nTo understand the motivation for schemas and catalogs, consider how \ufb01les are named\nin a \ufb01le system. Early \ufb01le systems were \ufb02at; that is, all \ufb01les were stored in a single\ndirectory. Current \ufb01le systems have a direct ory (or, synonymously, folder) structure,\nwith \ufb01les stored within subdirectories. To n ame a \ufb01le uniquely, we must specify the full\npath name of the \ufb01le, for example, /users/avi/db-book/chapter3.tex .\n12This syntax is not supported in all systems.\n", "191": "4.5 SQL Data Types and Schemas 163\nLike early \ufb01le systems, early database systems also had a single name space for all\nrelations. Users had to coordinate to make sure they did not try to use the same name\nfor di\ufb00erent relations. Contemporary database systems provide a three-level hierarchy\nfor naming relations. The top level of the hierarchy consists of catalogs ,e a c ho fw h i c h\ncan contain schemas .SQL objects such as relations and views are contained within a\nschema . (Some database implementations use the term database in place of the term\ncatalog.)\nIn order to perform any actions on a database, a user (or a program) must \ufb01rst\nconnect to the database. The user must provide the user name and usually, a password\nfor verifying the identity of the user. Each user has a default catalog and schema, and\nthe combination is unique to the user. When a user connects to a database system,\nthe default catalog and schema are set up for the connection; this corresponds to the\ncurrent directory being set to the user\u2019s home directory when the user logs into an\noperating system.\nTo identify a relation uniquely, a three-part name may be used, for example,\ncatalog5.univ\n schema.course\nWe may omit the catalog component, in which case the catalog part of the name is\nconsidered to be the default catalog for the connection. Thus, if catalog5 is the default\ncatalog, we can use univ\n schema.course to identify the same relation uniquely.\nIf a user wishes to access a relation that exists in a di\ufb00erent schema than the default\nschema for that user, the name of the schema must be speci\ufb01ed. However, if a relation is\nin the default schema for a particular user, then even the schema name may be omitted.\nThus, we can use just course if the default catalog is catalog5 and the default schema is\nuniv\n schema .\nWith multiple catalogs and schemas availa ble, di\ufb00erent applications and di\ufb00erent\nusers can work independently without worrying about name clashes. Moreover, multi-\nple versions of an application\u2014one a production version, other test versions\u2014can run\non the same database system.\nThe default catalog and schema are part of an SQL environment that is set up for\neach connection. The environment additionally contains the user identi\ufb01er (also re-\nferred to as the authorization identi\ufb01er ). All the usual SQL statements, including the\nDDL and DML statements, operate in the context of a schema.\nWe can create and drop schemas by means of create schema anddrop schema state-\nments. In most database systems, schemas are also created automatically when user ac-\ncounts are created, with the schema name set to the user account name. The schema is\ncreated in either a default catalog or a catalog speci\ufb01ed when creating the user account.\nThe newly created schema becomes the default schema for the user account.\nCreation and dropping of catalogs is implementation dependent and not part of\ntheSQL standard.\n", "192": "164 Chapter 4 Intermediate SQL\n4.6 Index Definition in SQL\nMany queries reference only a small proportion of the records in a \ufb01le. For example, a\nquery like \u201cFind all instructors in the Physics department\u201d or \u201cFind the salary value of\nthe instructor with ID22201\u201d references only a fraction of the instructor records. It is\nine\ufb03cient for the system to read every record and to check ID\ufb01eld for the ID\u201c32556,\u201d\nor the building \ufb01eld for the value \u201cPhysics\u201d.\nAnindex on an attribute of a relation is a data structure that allows the database\nsystem to \ufb01nd those tuples in the relation that have a speci\ufb01ed value for that attribute\ne\ufb03ciently, without scanning through all the tuples of the relation. For example, if we\ncreate an index on attribute dept\n name of relation instructor , the database system can\n\ufb01nd the record with any speci\ufb01ed dept\n name value, such as \u201cPhysics\u201d, or \u201cMusic\u201d, di-\nrectly, without reading all the tuples of the instructor r e l a t i o n .A ni n d e xc a na l s ob e\ncreated on a list of attributes, for example, on attributes name and dept\n name ofinstruc-\ntor.\nIndices are not required for correctness, since they are redundant data structures.\nIndices form part of the physical schema of the database, as opposed to its logical\nschema.\nHowever, indices are important for e\ufb03cient processing of transactions, including\nboth update transactions and queries. Indices are also important for e\ufb03cient enforce-\nment of integrity constraints such as primary-key and foreign-key constraints. In prin-\nciple, a database system can decide automatically what indices to create. However, be-\ncause of the space cost of indices, as well as the e\ufb00ect of indices on update processing,\nit is not easy to automatically make the right choices about what indices to maintain.\nTherefore, most SQL implementations provide the programmer with control over\nthe creation and removal of indices via data-de\ufb01nition-language commands. We illus-\ntrate the syntax of these commands next. Although the syntax that we show is widely\nused and supported by many database systems, it is not part of the SQL standard. The\nSQL standard does not support control of the physical database schema; it restricts\nitself to the logical database schema.\nWe create an index with the create index command, which takes the form:\ncreate index <index-name >on<relation-name >(<attribute-list >);\nThe attribute-list is the list of attributes of the relations that form the search key for the\nindex.\nTo de\ufb01ne an index named dept\n index on the instructor relation with dept\n name as\nthe search key, we write:\ncreate index dept\n index oninstructor (dept\n name );\nWhen a user submits an SQL query that can bene\ufb01t from using an index, the SQL\nquery processor automatically uses the index. For example, given an SQL query that\n", "193": "4.7 Authorization 165\nselects the instructor tuple with dept\n name \u201cMusic\u201d, the SQL query processor would use\nthe index dept\n index de\ufb01ned above to \ufb01nd the required tuple without reading the whole\nrelation.\nIf we wish to declare that the search key is a candidate key, we add the attribute\nunique to the index de\ufb01nition. Thus, the command:\ncreate unique index dept\n index oninstructor (dept\n name );\ndeclares dept\n name to be a candidate key for instructor (which is probably not what\nwe actually would want for our university database). If, at the time we enter the create\nunique index command, dept\n name is not a candidate key, the system will display an\nerror message, and the attempt to create the index will fail. If the index-creation attempt\nsucceeds, any subsequent attempt to insert a tuple that violates the key declaration will\nfail. Note that the unique feature is redundant if the database system supports the unique\ndeclaration of the SQL standard.\nThe index name we speci\ufb01ed for an index is required to drop an index. The drop\nindex command takes the form:\ndrop index <index-name >;\nMany database systems also provide a way to specify the type of index to be used,\nsuch as B+-tree or hash indices, which we study in Chapter 14. Some database systems\nalso permit one of the indices on a relation to be declared to be clustered; the system\nthen stores the relation sorted by the search key of the clustered index. We study in\nChapter 14 how indices are actually implemented, as well as what indices are automat-\nically created by databases, and how to decide on what additional indices to create.\n4.7 Authorization\nWe may assign a user several forms of authorizations on parts of the database. Autho-\nrizations on data include:\n\u2022Authorization to read data.\n\u2022Authorization to insert new data.\n\u2022Authorization to update data.\n\u2022Authorization to delete data.\nEach of these types of authorizations is called a privilege .W em a ya u t h o r i z et h eu s e r\nall, none, or a combination of these types of privileges on speci\ufb01ed parts of a database,\nsuch as a relation or a view.\n", "194": "166 Chapter 4 Intermediate SQL\nWhen a user submits a query or an update, the SQL implementation \ufb01rst checks if\nthe query or update is authorized, based on the authorizations that the user has been\ngranted. If the query or update is not authorized, it is rejected.\nIn addition to authorizations on data, users may also be granted authorizations on\nthe database schema, allowing them, for example, to create, modify, or drop relations.\nA user who has some form of authorization may be allowed to pass on (grant) this\nauthorization to other users, or to withdraw (revoke) an authorization that was granted\nearlier. In this section, we see how each of these authorizations can be speci\ufb01ed in SQL.\nThe ultimate form of authority is that given to the database administrator. The\ndatabase administrator may authorize new users, restructure the database, and so on.\nThis form of authorization is analogous to that of a superuser , administrator, or oper-\nator for an operating system.\n4.7.1 Granting and Revoking of Privileges\nThe SQL standard includes the privileges select ,insert ,update ,a n d delete . The privilege\nall privileges can be used as a short form for all the allowable privileges. A user who\ncreates a new relation is given all privileges on that relation automatically.\nThe SQL data-de\ufb01nition language includes commands to grant and revoke privi-\nleges. The grant statement is used to confer authorization. The basic form of this state-\nment is:\ngrant <privilege list >\non<relation name or view name >\nto<user/role list >;\nThe privilege list allows the granting of several privileges in one command. The notion\nof roles is covered in Section 4.7.2.\nTheselect authorization on a relation is required to read tuples in the relation. The\nfollowing grant statement grants database users Amit and Satoshi select authorization\non the department relation:\ngrant select on department toAmit,Satoshi ;\nThis allows those users to run queries on the department relation.\nThe update authorization on a relation allows a user to update any tuple in the\nrelation. The update authorization may be given either on all attributes of the relation\nor on only some. If update authorization is included in a grant statement, the list of\nattributes on which update authorization is to be granted optionally appears in paren-\ntheses immediately after the update keyword. If the list of attributes is omitted, the\nupdate privilege will be granted on all attributes of the relation.\nThis grant statement gives users Amit and Satoshi update authorization on the\nbudget attribute of the department relation:\n", "195": "4.7 Authorization 167\ngrant update (budget )ondepartment toAmit,Satoshi ;\nTheinsert authorization on a relation allows a user to insert tuples into the relation.\nThe insert privilege may also specify a list of attributes; any inserts to the relation\nmust specify only these attributes, and the system either gives each of the remaining\nattributes default values (if a default is de\ufb01ned for the attribute) or sets them to null.\nThedelete authorization on a relation allows a user to delete tuples from a relation.\nThe user name public refers to all current and future users of the system. Thus,\nprivileges granted to public are implicitly granted to all current and future users.\nBy default, a user/role that is granted a privilege is not authorized to grant that\nprivilege to another user/role. SQL allows a privilege grant to specify that the recipient\nmay further grant the privilege to another user. We describe this feature in more detail\nin Section 4.7.5.\nIt is worth noting that the SQL authorization mechanism grants privileges on an\nentire relation, or on speci\ufb01ed attributes of a relation. However, it does not permit\nauthorizations on speci\ufb01c tuples of a relation.\nTo revoke an authorization, we use the revoke statement. It takes a form almost\nidentical to that of grant :\nrevoke <privilege list >\non<relation name or view name >\nfrom <user/role list >;\nThus, to revoke the privileges that we granted previously, we write\nrevoke select on department from Amit,Satoshi ;\nrevoke update (budget) ondepartment from Amit,Satoshi ;\nRevocation of privileges is more complex if the user from whom the privilege is\nrevoked has granted the privilege to another user. We return to this issue in Section\n4.7.5.\n4.7.2 Roles\nConsider the real-world roles of various people in a university. Each instructor must\nhave the same types of authorizations on the same set of relations. Whenever a new\ninstructor is appointed, she will have to be given all these authorizations individually.\nA better approach would be to specify the authorizations that every instructor is\nto be given, and to identify separately which database users are instructors. The sys-\ntem can use these two pieces of information to determine the authorizations of each\ninstructor. When a new instructor is hired, a user identi\ufb01er must be allocated to him,\nand he must be identi\ufb01ed as an instructor. Individual permissions given to instructors\nneed not be speci\ufb01ed again.\n", "196": "168 Chapter 4 Intermediate SQL\nThe notion of roles captures this concept. A set of roles is created in the database.\nAuthorizations can be granted to roles, in exactly the same fashion as they are granted\nto individual users. Each database user is granted a set of roles (which may be empty)\nthat she is authorized to perform.\nIn our university database, examples of roles could include instructor ,teaching\nassistant ,student ,dean,a n d department\n chair .\nA less preferable alternative would be to create an instructor userid and permit each\ninstructor to connect to the database using the instructor userid. The problem with this\napproach is that it would not be possible to identify exactly which instructor carried\nout a database update, and this could create security risks. Furthermore, if an instruc-\ntor leaves the university or is moved to a non instructional role, then a new instructor\npassword must be created and distributed in a secure manner to all instructors. The\nuse of roles has the bene\ufb01t of requiring users to connect to the database with their own\nuserid.\nAny authorization that can be granted to a user can be granted to a role. Roles are\ngranted to users just as authorizations are.\nRoles can be created in SQL as follows:\ncreate role instructor ;\nRoles can then be granted privileges just as the users can, as illustrated in this state-\nment:\ngrant select on takes\ntoinstructor ;\nRoles can be granted to users, as well as to other roles, as these statements show:\ncreate role dean;\ngrant instructor todean;\ngrant dean toSatoshi;\nThus, the privileges of a user or a role consist of:\n\u2022All privileges directly granted to the user/role.\n\u2022All privileges granted to roles that have been granted to the user/role.\nNote that there can be a chain of roles; for example, the role teaching\n assistant may\nbe granted to all instructors .I nt u r n ,t h er o l e instructor is granted to all deans. Thus, the\ndean role inherits all privileges granted to the roles instructor and to teaching\n assistant\nin addition to privileges granted directly to dean.\nWhen a user logs in to the database system, the actions executed by the user during\nthat session have all the privileges granted directly to the user, as well as all privileges\n", "197": "4.7 Authorization 169\ngranted to roles that are granted (directly or indirectly via other roles) to that user.\nThus, if a user Amit has been granted the role dean, user Amit holds all privileges\ngranted directly to Amit, as well as privileges granted to dean, plus privileges granted\ntoinstructor and teaching\n assistant if, as above, those roles were granted (directly or\nindirectly) to the role dean.\nIt is worth noting that the concept of role-based authorization is not speci\ufb01c to\nSQL, and role-based authorization is used for access control in a wide variety of shared\napplications.\n4.7.3 Authorization on Views\nIn our university example, consider a sta\ufb00 member who needs to know the salaries of\nall faculty in a particular department, say the Geology department. This sta\ufb00 member\nis not authorized to see information regarding faculty in other departments. Thus, the\nsta\ufb00 member must be denied direct access to the instructor relation. But if he is to have\naccess to the information for the Geology department, he might be granted access to a\nview that we shall call geo\n instructor , consisting of only those instructor tuples pertaining\nto the Geology department. This view can be de\ufb01ned in SQL as follows:\ncreate view geo\n instructor as\n(select *\nfrom instructor\nwhere dept\n name = 'Geology');\nSuppose that the sta\ufb00 member issues the following SQL query:\nselect *\nfrom geo\n instructor ;\nThe sta\ufb00 member is authorized to see the result of this query. However, when the query\nprocessor translates it into a query on the actual relations in the database, it replaces\nuses of a view by the de\ufb01nition of the view, producing a query on instructor .T h u s ,t h e\nsystem must check authorization on the clerk\u2019s query before it replaces views by their\nde\ufb01nitions.\nA user who creates a view does not necessarily receive all privileges on that view.\nShe receives only those privileges that provide no additional authorization beyond\nthose that she already had. For example, a user who creates a view cannot be given up-\ndateauthorization on a view without having update authorization on the relations used\nto de\ufb01ne the view. If a user creates a view on which no authorization can be granted,\nthe system will deny the view creation request. In our geo\n instructor view example, the\ncreator of the view must have select authorization on the instructor relation.\nAs we will see in Section 5.2, SQL supports the creation of functions and proce-\ndures, which may, in turn, contain queries and updates. The execute privilege can be\ngranted on a function or procedure, enabling a user to execute the function or proce-\n", "198": "170 Chapter 4 Intermediate SQL\ndure. By default, just like views, functions and procedures have all the privileges that\nthe creator of the function or procedure had. In e\ufb00ect, the function or procedure runs\nas if it were invoked by the user who created the function.\nAlthough this behavior is appropriate in many situations, it is not always appropri-\nate. Starting with SQL:2003 , if the function de\ufb01nition has an extra clause sql security\ninvoker , then it is executed under the privileges of the user who invokes the function,\nrather than the privileges of the de\ufb01ner of the function. This allows the creation of\nlibraries of functions that can run under the same authorization as the invoker.\n4.7.4 Authorizations on Schema\nThe SQL standard speci\ufb01es a primitive authorization mechanism for the database\nschema: Only the owner of the schema can carry out any modi\ufb01cation to the schema,\nsuch as creating or deleting relations, addi ng or dropping attributes of relations, and\nadding or dropping indices.\nHowever, SQL includes a references privilege that permits a user to declare foreign\nkeys when creating relations. The SQL references privilege is granted on speci\ufb01c at-\ntributes in a manner like that for the update privilege. The following grant statement\nallows user Mariano to create relations that reference the key dept\n name of the depart-\nment relation as a foreign key:\ngrant references (dept\n name )ondepartment toMariano;\nInitially, it may appear that there is no reason ever to prevent users from creating\nforeign keys referencing another relation. However, recall that foreign-key constraints\nrestrict deletion and update operations on t he referenced relation. Suppose Mariano\ncreates a foreign key in a relation rreferencing the dept\n name attribute of the department\nrelation and then inserts a tuple into rpertaining to the Geology department. It is no\nlonger possible to delete the Geology department from the department relation without\nalso modifying relation r. Thus, the de\ufb01nition of a foreign key by Mariano restricts\nfuture activity by other users; therefore, there is a need for the references privilege.\nContinuing to use the example of the department relation, the references privilege\nondepartment is also required to create a check constraint on a relation rif the con-\nstraint has a subquery referencing department .T h i si sr e a s o n a b l ef o rt h es a m er e a s o na s\nthe one we gave for foreign-key constraints; a check constraint that references a relation\nlimits potential updates to that relation.\n4.7.5 Transfer of Privileges\nA user who has been granted some form of authorization may be allowed to pass on\nthis authorization to other users. By default, a user/role that is granted a privilege is not\nauthorized to grant that privilege to another user/role. If we wish to grant a privilege\nand to allow the recipient to pass the privilege on to other users, we append the with\ngrant option clause to the appropriate grant command. For example, if we wish to allow\n", "199": "4.7 Authorization 171\nAmit the select privilege on department and allow Amit to grant this privilege to others,\nwe write:\ngrant select on department toAmit with grant option ;\nThe creator of an object (relation/view/role) holds all privileges on the object, including\nthe privilege to grant privileges to others.\nConsider, as an example, the granting of update authorization on the teaches rela-\ntion of the university database. Assume that, initially, the database administrator grants\nupdate authorization on teaches to users U1,U2,a n d U3, who may, in turn, pass on this\nauthorization to other users. The passing of a speci\ufb01c authorization from one user to\nanother can be represented by an authorization graph . The nodes of this graph are the\nusers.\nConsider the graph for update authorization on teaches . The graph includes an\nedge Ui\u2192Ujif user Uigrants update authorization on teaches toUj.T h er o o to ft h e\ngraph is the database administrator. In the sample graph in Figure 4.11, observe that\nuser U5is granted authorization by both U1and U2;U4is granted authorization by\nonly U1.\nA user has an authorization if and only if there is a path from the root of the\nauthorization graph (the node representing the database administrator) down to the\nnode representing the user.\n4.7.6 Revoking of Privileges\nSuppose that the database administrator decides to revoke the authorization of user\nU1.S i n c e U4has authorization from U1, that authorization should be revoked as well.\nHowever, U5was granted authorization by both U1and U2. Since the database ad-\nministrator did not revoke update authorization on teaches from U2,U5retains update\nU3DBAU1\nU5 U2U4\nFigure 4.11 Authorization-grant graph ( U1,U2,\u2026,U5are users and DBA refers to the\ndatabase administrator).\n", "200": "172 Chapter 4 Intermediate SQL\nauthorization on teaches .I fU2eventually revokes authorization from U5,t h e n U5loses\nthe authorization.\nA pair of devious users might attempt to defeat the rules for revocation of autho-\nrization by granting authorization to each other. For example, U2is initially granted an\nauthorization by the database administrator, and U2further grants it to U3. Suppose\nU3now grants the privilege back to U2. If the database administrator revokes autho-\nrization from U2, it might appear that U2retains authorization through U3.H o w e v e r ,\nnote that once the administrator revokes authorization from U2, there is no path in the\nauthorization graph from the root either to U2or to U3.T h u s , SQL ensures that the\nauthorization is revoked from both the users.\nAs we just saw, revocation of a privilege from a user/role may cause other\nusers/roles also to lose that privilege. This behavior is called cascading revocation .I n\nmost database systems, cascading is the default behavior. However, the revoke state-\nment may specify restrict in order to prevent cascading revocation:\nrevoke select on department from Amit, Satoshi restrict ;\nIn this case, the system returns an error if there are any cascading revocations and does\nnot carry out the revoke action.\nThe keyword cascade can be used instead of restrict to indicate that revocation\nshould cascade; however, it can be omitted, as we have done in the preceding examples,\nsince it is the default behavior.\nThe following revoke statement revokes only the grant option, rather than the actual\nselect privilege:\nrevoke grant option for select on department from Amit;\nNote that some database implementations do not support the above syntax; instead,\nthe privilege itself can be revoked and then granted again without the grant option.\nCascading revocation is inappropriate in many situations. Suppose Satoshi has the\nrole of dean,g r a n t s instructor to Amit, and later the role dean is revoked from Satoshi\n(perhaps because Satoshi leaves the university); Amit continues to be employed on the\nfaculty and should retain the instructor role.\nTo deal with this situation, SQL permits a privilege to be granted by a role rather\nthan by a user. SQLhas a notion of the current role associated with a session. By default,\nthe current role associated with a session is null (except in some special cases). The\ncurrent role associated with a session can be set by executing set role role\n name .T h e\nspeci\ufb01ed role must have been granted to the user, otherwise the set role statement fails.\nTo grant a privilege with the grantor set to t he current role associated with a session,\nwe can add the clause:\ngranted by current\n role\nto the grant statement, provided the current role is not null.\n", "201": "4.8 Summary 173\nSuppose the granting of the role instructor (or other privileges) to Amit is done\nusing the granted by current\n roleclause, with the current role set to dean,i n s t e a do f\nthe grantor being the user Satoshi. Then, revoking of roles/privileges (including the\nrole dean) from Satoshi will not result in revoking of privileges that had the grantor set\nto the role dean, even if Satoshi was the user who executed the grant; thus, Amit would\nretain the instructor role even after Satoshi\u2019s privileges are revoked.\n4.7.7 Row-Level Authorization\nThe types of authorization privileges we have studied apply at the level of relations or\nviews. Some database systems provide mechanisms for \ufb01ne-grained authorization at\nthe level of speci\ufb01c tuples within a relation.\nSuppose, for example, that we wish to allow a student to see her or his own data\nin the takes relation but not those data of other users. We can enforce such a restric-\ntion using row-level authorization, if the database supports it. We describe row-level\nauthorization in Oracle below; Postgre SQL and SQL S erver too support row-level au-\nthorization using a conceptually similar mechanism, but using a di\ufb00erent syntax.\nThe Oracle Virtual Private Database (VPD) feature supports row-level authoriza-\ntion as follows. It allows a system administrator to associate a function with a relation;\nthe function returns a predicate that gets added automatically to any query that uses\nthe relation. The predicate can use the function sys\ncontext , which returns the identi-\n\ufb01er of the user on whose behalf a query is being executed. For our example of students\naccessing their data in the takes relation, we would specify the following predicate to\nbe associated with the takes relation:\nID=sys\ncontext ('USERENV ', 'SESSION\n USER ')\nThis predicate is added by the system to the where clause of every query that uses the\ntakes relation. As a result, each student can see only those takes tuples whose IDvalue\nmatches her ID.\nVPD provides authorization at the level of speci\ufb01c tuples, or rows, of a relation,\nand is therefore said to be a row-level authorization mechanism. A potential pitfall with\nadding a predicate as described above is that it may change the meaning of a query\nsigni\ufb01cantly. For example, if a user wrote a query to \ufb01nd the average grade over all\ncourses, she would end up getting the average of hergrades, not all grades. Although\nthe system would give the \u201cright\u201d answer for the rewritten query, that answer would not\ncorrespond to the query the user may have thought she was submitting.\n4.8 Summary\n\u2022SQL supports several types of joins including natural join, inner and outer joins,\nand several types of join conditions.\n", "202": "174 Chapter 4 Intermediate SQL\n\u00b0Natural join provides a simple way to write queries over multiple relations\nin which a where predicate would otherwise equate attributes with matching\nnames from each relation. This convenience comes at the risk of query seman-\ntics changing if a new attribute is added to the schema.\n\u00b0Thejoin-using construct provides a simple way to write queries over multiple\nrelations in which equality is desired for some but not necessarily all attributes\nwith matching names.\n\u00b0Thejoin-on construct provides a way to include a join predicate in the from\nclause.\n\u00b0O u t e rj o i np r o v i d e sam e a n st or e t a i nt u p l e st h a t ,d u et oaj o i np r e d i c a t e\n(whether a natural join, a join-using, or a join-on), would otherwise not ap-\npear anywhere in the result relation. The retained tuples are padded with null\nvalues so as to conform to the result schema.\n\u2022View relations can be de\ufb01ned as relations containing the result of queries. Views\nare useful for hiding unneeded information and for gathering together information\nfrom more than one relation into a single view.\n\u2022Transactions are sequences of queries and updates that together carry out a task.\nTransactions can be committed, or rolled back; when a transaction is rolled back,\nthe e\ufb00ects of all updates performed by the transaction are undone.\n\u2022Integrity constraints ensure that changes made to the database by authorized users\ndo not result in a loss of data consistency.\n\u2022Referential-integrity constraints ensure that a value that appears in one relation\nfor a given set of attributes also appears for a certain set of attributes in another\nrelation.\n\u2022Domain constraints specify the set of possible values that may be associated with\nan attribute. Such constraints may also prohibit the use of null values for particular\nattributes.\n\u2022Assertions are declarative expressions that state predicates that we require always\nto be true.\n\u2022The SQL data-de\ufb01nition language provides support for de\ufb01ning built-in domain\ntypes such as date andtime as well as user-de\ufb01ned domain types.\n\u2022Indices are important for e\ufb03cient processing of queries, as well as for e\ufb03cient\nenforcement of integrity constraints. Although not part of the SQL standard, SQL\ncommands for creation of indices are supported by most database systems.\n\u2022SQL authorization mechanisms allow one to di\ufb00erentiate among the users of the\ndatabase on the type of access they are permitted on various data values in the\ndatabase.\n", "203": "Practice Exercises 175\n\u2022Roles enable us to assign a set of privileges to a user according to the role that the\nuser plays in the organization.\nReview Terms\n\u2022Join types\n\u00b0Natural join\n\u00b0Inner join with using andon\n\u00b0Left, right and full outer join\n\u00b0Outer join with using andon\n\u2022View de\ufb01nition\n\u00b0Materialized views\n\u00b0View maintenance\n\u00b0View update\n\u2022Transactions\n\u00b0Commit work\n\u00b0Rollback work\n\u00b0Atomic transaction\n\u2022Constraints\n\u00b0Integrity constraints\n\u00b0Domain constraints\n\u00b0Unique constraint\n\u00b0Check clause\n\u00b0Referential integrity\n\u22c4Cascading deletes\n\u22c4Cascading updates\n\u00b0Assertions\n\u2022Data types\n\u00b0Date and time types\u00b0Default values\n\u00b0Large objects\n\u22c4clob\n\u22c4blob\n\u00b0User-de\ufb01ned types\n\u00b0distinct types\n\u00b0Domains\n\u00b0Type conversions\n\u2022Catalogs\n\u2022Schemas\n\u2022Indices\n\u2022Privileges\n\u00b0Types of privileges\n\u22c4select\n\u22c4insert\n\u22c4update\n\u00b0Granting of privileges\n\u00b0Revoking of privileges\n\u00b0Privilege to grant privileges\n\u00b0Grant option\n\u2022Roles\n\u2022Authorization on views\n\u2022Execute authorization\n\u2022Invoker privileges\n\u2022Row-level authorization\n\u2022Virtual private database ( VPD)\n", "204": "176 Chapter 4 Intermediate SQL\nPractice Exercises\n4.1 Consider the following SQL query that seeks to \ufb01nd a list of titles of all courses\ntaught in Spring 2017 along with the name of the instructor.\nselect name, title\nfrom instructor natural join teaches natural join section natural join course\nwhere semester =' S p r i n g ' andyear = 2017\nWhat is wrong with this query?\n4.2 Write the following queries in SQL:\na. Display a list of all instructors, showing each instructor\u2019s IDand the num-\nber of sections taught. Make sure to show the number of sections as 0 for\ninstructors who have not taught any section. Your query should use an\nouter join, and should not use subqueries.\nb. Write the same query as in part a, but using a scalar subquery and not\nusing outer join.\nc. Display the list of all course sections o\ufb00ered in Spring 2018, along with\ntheIDand name of each instructor teaching the section. If a section has\nmore than one instructor, that section should appear as many times in\nthe result as it has instructors. If a section does not have any instructor,\nit should still appear in the result with the instructor name set to \u201c\u2014\u201d.\nd. Display the list of all departments, with the total number of instructors\nin each department, without using subqueries. Make sure to show depart-\nments that have no instructors, and list those departments with an instruc-\ntor count of zero.\n4.3 Outer join expressions can be computed in SQL without using the SQL outer\njoinoperation. To illustrate this fact, show how to rewrite each of the following\nSQL queries without using the outer join expression.\na.select *from student natural left outer join takes\nb.select *from student natural full outer join takes\n4.4 Suppose we have three relations r(A,B),s(B,C), and t(B,D), with all attributes\ndeclared as not null .\na. Give instances of relations r,s,a n d tsuch that in the result of\n(rnatural left outer join s)natural left outer join t\nattribute Chas a null value but attribute Dhas a non-null value.\nb. Are there instances of r,s,a n d tsuch that the result of\nrnatural left outer join (snatural left outer join t)\n", "205": "Exercises 177\nemployee (ID\n,person\n name ,street ,city)\nworks (ID\n,company\n name ,salary )\ncompany (company\n name\n ,city)\nmanages (ID\n,manager\n id)\nFigure 4.12 Employee database.\nhas a null value for Cbut a non-null value for D? Explain why or why not.\n4.5 Testing SQL queries : To test if a query speci\ufb01ed in English has been correctly\nwritten in SQL,t h e SQL query is typically executed on multiple test databases,\nand a human checks if the SQL query result on each test database matches the\nintention of the speci\ufb01cation in English.\na. In Section 4.1.1 we saw an example of an erroneous SQL query which was\nintended to \ufb01nd which courses had been taught by each instructor; the\nquery computed the natural join of instructor ,teaches ,a n d course ,a n da s\na result it unintentionally equated the dept\n name attribute of instructor and\ncourse . Give an example of a dataset that would help catch this particular\nerror.\nb. When creating test databases, it is important to create tuples in referenced\nrelations that do not have any matching tuple in the referencing relation\nfor each foreign key. Explain why, using an example query on the univer-\nsity database.\nc. When creating test databases, it is important to create tuples with null\nvalues for foreign-key attributes, provided the attribute is nullable ( SQL\nallows foreign-key attributes to take on null values, as long as they are not\npart of the primary key and have not been declared as not null ). Explain\nwhy, using an example query on the university database.\nHint: Use the queries from Exercise 4.2.\n4.6 Show how to de\ufb01ne the view student\n grades (ID, GPA ) giving the grade-point\naverage of each student, based on the query in Exercise 3.2; recall that we used\nar e l a t i o n grade\n points (grade\n ,points ) to get the numeric points associated with\na letter grade. Make sure your view de\ufb01nition correctly handles the case of null\nvalues for the grade attribute of the takes relation.\n4.7 Consider the employee database of Figure 4.12. Give an SQL DDL de\ufb01nition\nof this database. Identify referential-integrity constraints that should hold, and\ninclude them in the DDL de\ufb01nition.\n", "206": "178 Chapter 4 Intermediate SQL\n4.8 As discussed in Section 4.4.8, we expect the constraint \u201can instructor cannot\nt e a c hs e c t i o n si nt w od i \ufb00 e r e n tc l a s s r o o m si nas e m e s t e ri nt h es a m et i m es l o t \u201d\nto hold.\na. Write an SQL query that returns all ( instructor ,section ) combinations that\nviolate this constraint.\nb. Write an SQL assertion to enforce this constraint (as discussed in Sec-\ntion 4.4.8, current generation database systems do not support such as-\nsertions, although they are part of the SQL standard).\n4.9 SQL allows a foreign-key dependency to refer to the same relation, as in the\nfollowing example:\ncreate table manager\n(employee\n ID char(20),\nmanager\n ID char(20),\nprimary key employee\n ID,\nforeign key (manager\n ID)references manager (employee\n ID)\non delete cascade )\nHere, employee\n IDis a key to the table manager , meaning that each employee\nhas at most one manager. The foreign-key clause requires that every manager\nalso be an employee. Explain exactly what happens when a tuple in the relation\nmanager is deleted.\n4.10 Given the relations a(name, address, title )a n d b(name, address, salary ), show\nhow to express an a t u r a lf u l lo u t e rj o i n busing the full outer-join operation with\nanoncondition rather than using the natural join syntax. This can be done using\nthecoalesce operation. Make sure that the result relation does not contain two\ncopies of the attributes name and address and that the solution is correct even\nif some tuples in aand bhave null values for attributes name oraddress .\n4.11 Operating systems usually o\ufb00er only two types of authorization control for data\n\ufb01les: read access and write access. Why do database systems o\ufb00er so many kinds\nof authorization?\n4.12 Suppose a user wants to grant select access on a relation to another user. Why\nshould the user include (or not include) the clause granted by current role in the\ngrant statement?\n4.13 Consider a view vwhose de\ufb01nition references only relation r.\n\u2022If a user is granted select authorization on v, does that user need to have\nselect authorization on ras well? Why or why not?\n\u2022If a user is granted update authorization on v, does that user need to have\nupdate authorization on ras well? Why or why not?\n", "207": "Exercises 179\n\u2022Give an example of an insert operation on a view vto add a tuple tthat is\nnot visible in the result of select * from v. Explain your answer.\nExercises\n4.14 Consider the query\nselect course\n id,semester ,year,sec\nid,avg(tot\ncred)\nfrom takes natural join student\nwhere year=2017\ngroup by course\n id,semester ,year,sec\nid\nhaving count (ID)>=2;\nExplain why appending natural join section in the from clause would not change\nthe result.\n4.15 Rewrite the query\nselect *\nfrom section natural join classroom\nwithout using a natural join but instead using an inner join with a using condi-\ntion.\n4.16 Write an SQL query using the university schema to \ufb01nd the IDof each student\nwho has never taken a course at the university. Do this using no subqueries and\nno set operations (use an outer join).\n4.17 Express the following query in SQL using no subqueries and no set operations.\nselect ID\nfrom student\nexcept\nselect s\nid\nfrom advisor\nwhere i\nIDis not null\n4.18 For the database of Figure 4.12, write a query to \ufb01nd the IDof each employee\nwith no manager. Note that an employee may simply have no manager listed or\nmay have a nullmanager. Write your query using an outer join and then write\nit again using no outer join at all.\n4.19 Under what circumstances would the query\n", "208": "180 Chapter 4 Intermediate SQL\nselect *\nfrom student natural full outer join takes\nnatural full outer join course\ninclude tuples with null values for the titleattribute?\n4.20 Show how to de\ufb01ne a view tot\ncredits (year, num\n credits ), giving the total number\nof credits taken in each year.\n4.21 For the view of Exercise 4.18, explain why the database system would not allow\na tuple to be inserted into the database through this view.\n4.22 Show how to express the coalesce function using the case construct.\n4.23 Explain why, when a manager, say Satoshi, grants an authorization, the grant\nshould be done by the manager role, rather than by the user Satoshi.\n4.24 Suppose user A, who has all authorization privileges on a relation r,g r a n t s select\non relation rtopublic with grant option. Suppose user Bthen grants select onr\ntoA. Does this cause a cycle in the authorization graph? Explain why.\n4.25 Suppose a user creates a new relation r1 with a foreign key referencing another\nrelation r2. What authorization privilege does the user need on r2? Why should\nthis not simply be allowed without any such authorization?\n4.26 Explain the di\ufb00erence between integrity constraints and authorization con-\nstraints.\nFurther Reading\nGeneral SQL references were provided in Chapter 3. As noted earlier, many systems\nimplement features in a non-standard manner, and, for that reason, a reference speci\ufb01c\nto the database system you are using is an essential guide. Most vendors also provide\nextensive support on the web.\nThe rules used by SQL to determine the updatability of a view, and how updates\nare re\ufb02ected on the underlying database relations appeared in SQL:1999 and are sum-\nmarized in [Melton and Simon (2001)].\nThe original SQL proposals for assertions date back to [Astrahan et al. (1976)],\n[Chamberlin et al. (1976)], and [Chamberlin et al. (1981)].\nBibliography\n[Astrahan et al. (1976)] M. M. Astrahan, M. W. Blasgen, D. D. Chamberlin, K. P. Eswaran,\nJ .N .G r a y ,P .P .G r i \ufb03 t h s ,W .F .K i n g ,R .A .L o r i e ,P .R .M c J o n e s ,J .W .M e h l ,G .R .P u t z o l u ,\nI. L. Traiger, B. W. Wade, and V. Watson, \u201cSystem R, A Relational Approach to Data Base\n", "209": "Further Reading 181\nManagement\u201d, ACM Transactions on Database Systems , Volume 1, Number 2 (1976), pages\n97\u2013137.\n[Chamberlin et al. (1976)] D. D. Chamberlin, M. M. Astrahan, K. P. Eswaran, P. P. Gri\ufb03ths,\nR. A. Lorie, J. W. Mehl, P. Reisner, and B. W. Wade, \u201cSEQUEL 2: A Uni\ufb01ed Approach to\nData De\ufb01nition, Manipulation, and Control\u201d, IBM Journal of Research and Development ,\nVolume 20, Number 6 (1976), pages 560\u2013575.\n[Chamberlin et al. (1981)] D .D .C h a m b e r l i n ,M .M .A s t r a h a n ,M .W .B l a s g e n ,J .N .G r a y ,\nW. F. King, B. G. Lindsay, R. A. Lorie, J. W. Mehl, T. G. Price, P. G. Selinger, M. Schkolnick,\nD .R .S l u t z ,I .L .T r a i g e r ,B .W .W a d e ,a n dR .A .Y o s t ,\u201c AH i s t o r ya n dE v a l u a t i o no fS y s t e m\nR\u201d,Communications of the ACM , Volume 24, Number 10 (1981), pages 632\u2013646.\n[Melton and Simon (2001)] J. Melton and A. R. Simon, SQL:1999, Understanding Relational\nLanguage Components , Morgan Kaufmann (2001).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "210": "", "211": "CHAPTER5\nAdvanced SQL\nChapter 3 and Chapter 4 provided detailed coverage of the basic structure of SQL.I n\nthis chapter, we \ufb01rst address the issue of how to access SQL from a general-purpose\nprogramming language, which is very important for building applications that use a\ndatabase to manage data. We then cover some of the more advanced features of SQL,\nstarting with how procedural code can be executed within the database either by extend-\ning the SQL language to support procedural actions or by allowing functions de\ufb01ned in\nprocedural languages to be executed within the database. We describe triggers, which\ncan be used to specify actions that are to be ca rried out automatically on certain events\nsuch as insertion, deletion, or update of tuples in a speci\ufb01ed relation. Finally, we discuss\nrecursive queries and advanced aggregation features supported by SQL.\n5.1 Accessing SQL from a Programming Language\nSQL provides a powerful declarative query language. Writing queries in SQL is usually\nmuch easier than coding the same queries in a general-purpose programming language.\nHowever, a database programmer must have access to a general-purpose programming\nlanguage for at least two reasons:\n1.Not all queries can be expressed in SQL,s i n c e SQL does not provide the full ex-\npressive power of a general-purpose language. That is, there exist queries that can\nbe expressed in a language such as C, Java, or Python that cannot be expressed in\nSQL. To write such queries, we can embed SQL within a more powerful language.\n2.Nondeclarative actions\u2014such as printing a report, interacting with a user, or\nsending the results of a query to a graphical user interface\u2014cannot be done from\nwithin SQL. Applications usually have several components, and querying or up-\ndating data are only one component; other components are written in general-\npurpose programming languages. For an integrated application, there must be a\nmeans to combine SQL with a general-purpose programming language.\nThere are two approaches to accessing SQL from a general-purpose programming\nlanguage:\n183\n", "212": "184 Chapter 5 Advanced SQL\n1.Dynamic SQL: A general-purpose program can connect to and communicate with\na database server using a collection of functions (for procedural languages) or\nmethods (for object-oriented languages). Dynamic SQL allows the program to\nconstruct an SQL query as a character string at runtime, submit the query, and\nthen retrieve the result into program variables a tuple at a time. The dynamic\nSQLcomponent of SQL allows programs to construct and submit SQL queries at\nruntime.\nIn this chapter, we look at two standards for connecting to an SQL database\nand performing queries and updates. One, JDBC (Section 5.1.1), is an application\nprogram interface for the Java language. The other, ODBC (Section 5.1.3), is\nan application program interface originally developed for the C language, and\nsubsequently extended to other languages such as C++, C#, Ruby, Go, PHP,a n d\nVisual Basic. We also illustrate how programs written in Python can connect to\na database using the Python Database API(Section 5.1.2).\nThe ADO.NET API , designed for the Visual Basic .NET and C# languages,\nprovides functions to access data, which at a high level are similar to the JDBC\nfunctions, although details di\ufb00er. The ADO.NET API can also be used with some\nkinds of non-relational data sources. Details of ADO.NET may be found in the\nmanuals available online and are not covered further in this chapter.\n2.Embedded SQL: Like dynamic SQL, embedded SQL provides a means by which\na program can interact with a database server. However, under embedded SQL,\ntheSQL statements are identi\ufb01ed at compile time using a preprocessor, which\ntranslates requests expressed in embedded SQL into function calls. At runtime,\nthese function calls connect to the database using an APIthat provides dynamic\nSQL facilities but may be speci\ufb01c to the database that is being used. Section 5.1.4\nbrie\ufb02y covers embedded SQL.\nA major challenge in mixing SQL with a general-purpose language is the mismatch\nin the ways these languages manipulate data. In SQL, the primary type of data are\nrelations. SQL statements operate on relations and return relations as a result. Pro-\ngramming languages normally operate on a variable at a time, and those variables cor-\nrespond roughly to the value of an attribute in a tuple in a relation. Thus, integrating\nthese two types of languages into a single application requires providing a mechanism\nto return the result of a query in a manner that the program can handle.\nOur examples in this section assume that we are accessing a database on a server\nthat runs a database system. An alternative approach using an embedded database is\ndiscussed in Note 5.1 on page 198.\n5.1.1 JDBC\nThe JDBC standard de\ufb01nes an application program interface ( API)that Java programs\ncan use to connect to database servers. (The word JDBC was originally an abbreviation\nforJava Database Connectivity , but the full form is no longer used.)\n", "213": "5.1 Accessing SQL from a Programming Language 185\nFigure 5.1 shows example Java code that uses the JDBC interface. The Java program\nmust import java.sql.* , which contains the interface de\ufb01nitions for the functionality\nprovided by JDBC .\n5.1.1.1 Connecting to the Database\nThe \ufb01rst step in accessing a database from a Java program is to open a connection to\nthe database. This step is required to select which database to use, such as an instance\nof Oracle running on your machine, or a Postgre SQL database running on another\nm a c h i n e .O n l ya f t e ro p e n i n gac o n n e c t i o nc a naJ a v ap r o g r a me x e c u t e SQL statements.\npublic static void JDBCexample(String userid, String passwd)\n{\ntry (\nConnection conn = DriverManager.getConnection(\n\"jdbc:oracle:thin:@db.yale.edu:1521:univdb\",\nuserid, passwd);\nStatement stmt = conn.createStatement();\n){\ntry {\nstmt.executeUpdate(\n\"insert into instructor values( \u201977987\u2019,\u2019Kim\u2019,\u2019Physics\u2019,98000)\");\n}\ncatch ( SQLException sqle) {\nSystem.out.println(\"Could not insert tuple. \" + sqle);\n}\nResultSet rset = stmt.executeQuery(\n\"select dept\n name, avg (salary) \"+\n\" from instructor \"+\n\" group by dept\n name\");\nwhile (rset.next()) {\nSystem.out.println(rset.getString(\"dept\n n a m e \" )+\"\"+\nrset.getFloat(2));\n}\n}\ncatch (Exception sqle)\n{\nSystem.out.println(\"Exception : \" + sqle);\n}\n}\nFigure 5.1 An example of JDBC code.\n", "214": "186 Chapter 5 Advanced SQL\nA connection is opened using the getConnection() method of the DriverManager\nclass (within java.sql ). This method takes three parameters.1\n1.The \ufb01rst parameter to the getConnection() call is a string that speci\ufb01es the URL,\nor machine name, where the server runs (in our example, db.yale.edu ), along\nwith possibly some other information such as the protocol to be used to commu-\nnicate with the database (in our example, jdbc:oracle:thin: ;w es h a l ls h o r t l ys e e\nwhy this is required), the port number the database system uses for communica-\ntion (in our example, 2000), and the speci\ufb01c database on the server to be used\n(in our example, univdb ). Note that JDBC speci\ufb01es only the API,n o tt h ec o m m u -\nnication protocol. A JDBC driver may support multiple protocols, and we must\nspecify one supported by both the database and the driver. The protocol details\nare vendor speci\ufb01c.\n2.The second parameter to getConnection() is a database user identi\ufb01er, which is\nas t r i n g .\n3.The third parameter is a password, which is also a string. (Note that the need to\nspecify a password within the JDBC code presents a security risk if an unautho-\nrized person accesses your Java code.)\nIn our example in the \ufb01gure, we have created a Connection object whose handle is\nconn .\nEach database product that supports JDBC (all the major database vendors do)\nprovides a JDBC driver that must be dynamically loaded in order to access the database\nfrom Java. In fact, loading the driver must be done \ufb01rst, before connecting to the\ndatabase. If the appropriate driver has been downloaded from the vendor\u2019s web site\nand is in the classpath, the getConnection() method will locate the needed driver.2The\ndriver provides for the translation of product-independent JDBC calls into the product-\nspeci\ufb01c calls needed by the speci\ufb01c database management system being used. The ac-\ntual protocol used to exchange information with the database depends on the driver\nthat is used, and it is not de\ufb01ned by the JDBC standard. Some drivers support more\nthan one protocol, and a suitable protocol must be chosen depending on what protocol\nthe particular database product supports. In our example, when opening a connection\nwith the database, the string jdbc:oracle:thin: speci\ufb01es a particular protocol supported\nby Oracle. The MySQL equivalent is jdbc:mysql:\n5.1.1.2 Shipping SQL Statements to the Database System\nOnce a database connection is open, the program can use it to send SQL statements to\nthe database system for execution. This is done via an instance of the class Statement .\n1There are multiple versions of the getConnection() method, which di\ufb00er in the parameters that they accept. We present\nthe most commonly used version.\n2Prior to version 4, locating the driver was done manually by invoking Class.forName with one argument specifying a\nconcrete class implementing the java.sql.Driver interface, in a line of code prior to the getConnection call.\n", "215": "5.1 Accessing SQL from a Programming Language 187\nAStatement object is not the SQL statement itself, but rather an object that allows the\nJava program to invoke methods that ship an SQL statement given as an argument for\nexecution by the database system. Our example creates a Statement handle ( stmt)o n\nthe connection conn .\nTo execute a statement, we invoke either the executeQuery() method or the exe-\ncuteUpdate() method, depending on whether the SQL statement is a query (and, thus,\nreturns a result set) or nonquery statement such as update ,insert ,delete ,o rcreate ta-\nble.I no u re x a m p l e , stmt.executeUpdate() executes an update statement that inserts\ninto the instructor relation. It returns an integer giving the number of tuples inserted,\nupdated, or deleted. For DDL statements, the return value is zero.\n5.1.1.3 Exceptions and Resource Management\nExecuting any SQL method might result in an exception being thrown. The try {\u2026}\ncatch {\u2026}construct permits us to catch any exceptions (error conditions) that arise\nwhen JDBC calls are made and take appropriate action. In JDBC programming, it may\nbe useful to distinguish between an SQLexception ,w h i c hi sa n SQL-speci\ufb01c exception,\nand the general case of an Exception , which could be any Java exception such as a\nnull-pointer exception, or array-index-out-of-bounds exception. We show both in Figure\n5.1. In practice, one would write more complete exception handlers than we do (for\nthe sake of conciseness) in our example code.\nOpening a connection, a statement, and other JDBC objects are all actions that\nconsume system resources. Programmers must take care to ensure that programs close\nall such resources. Failure to do so may cause the database system\u2019s resource pools\nto become exhausted, rendering the system inaccessible or inoperative until a time-out\nperiod expires. One way to do this is to code explicit calls to close connections and\nstatements. This approach fails if the code exits due to an exception and, in so do-\ning, avoids the Java statement with the close invocation. For this reason, the preferred\napproach is to use the try-with-resources construct in Java. In the example of Figure\n5.1, the opening of the connection and statement objects is done within parentheses\nrather than in the main body of the tryin curly braces. Resources opened in the code\nwithin parentheses are closed automatically at the end of the tryblock. This protects us\nfrom leaving connections or statements unclosed. Since closing a statement implicitly\ncloses objects opened for that statement (i.e., the ResultSet objects we shall discuss\nin the next section, this coding practice protects us from leaving resources unclosed.3\nIn the example of Figure 5.1, we could have closed the connection explicitly with the\nstatement conn.close() and closed the statement explicitly with stmt.close() ,t h o u g h\ndoing so was not necessary in our example.\n5.1.1.4 Retrieving the Result of a Query\nThe example code of Figure 5.1 executes a query by using stmt.executeQuery() .I t\nretrieves the set of tuples in the result into a ResultSet object rsetand fetches them one\n3This Java feature, called try-with-resources , was introduced in Java 7.\n", "216": "188 Chapter 5 Advanced SQL\ntuple at a time. The next() method on the result set tests whether or not there remains\nat least one unfetched tuple in the result set and if so, fetches it. The return value of\nthenext() method is a Boolean indicating whether it fetched a tuple. Attributes from\nthe fetched tuple are retrieved using various methods whose names begin with get.\nThe method getString() can retrieve any of the basic SQL data types (converting the\nvalue to a Java String object), but more restrictive methods such as getFloat() can be\nused as well. The argument to the various getmethods can either be an attribute name\nspeci\ufb01ed as a string, or an integer indicating the position of the desired attribute within\nthe tuple. Figure 5.1 shows two ways of retrieving the values of attributes in a tuple:\nusing the name of the attribute ( dept\n name ) and using the position of the attribute (2,\nto denote the second attribute).\n5.1.1.5 Prepared Statements\nWe can create a prepared statement in which some values are replaced by \u201c?\u201d, thereby\nspecifying that actual values will be provided later. The database system compiles the\nquery when it is prepared. Each time the query is executed (with new values to replace\nthe \u201c?\u201ds), the database system can reuse the previously compiled form of the query\nand apply the new values as parameters. The code fragment in Figure 5.2 shows how\nprepared statements can be used.\nTheprepareStatement() method of the Connection class de\ufb01nes a query that may\ncontain parameter values; some JDBC drivers may submit the query to the database\nfor compilation as part of the method, but other drivers do not contact the database at\nthis point. The method returns an object of class PreparedStatement . At this point, no\nSQLstatement has been executed. The executeQuery() andexecuteUpdate() methods\nofPreparedStatement class do that. But before they can be invoked, we must use\nmethods of class PreparedStatement that assign values for the \u201c?\u201d parameters. The\nsetString() method and other similar methods such as setInt() for other basic SQL\nt y p e sa l l o wu st os p e c i f yt h ev a l u e sf o rt h ep a r a m e t e r s .T h e\ufb01 r s ta r g u m e n ts p e c i \ufb01 e st h e\n\u201c?\u201d parameter for which we are assigning a value (the \ufb01rst parameter is 1, unlike most\nother Java constructs, which start with 0). The second argument speci\ufb01es the value to\nbe assigned.\nIn the example in Figure 5.2, we prepare an insert statement, set the \u201c?\u201d parame-\nters, and then invoke executeUpdate() . The \ufb01nal two lines of our example show that\nparameter assignments remain unchanged until we speci\ufb01cally reassign them. Thus, the\n\ufb01nal statement, which invokes executeUpdate() , inserts the tuple (\u201c88878\u201d, \u201cPerry\u201d,\n\u201cFinance\u201d, 125000).\nPrepared statements allow for more e\ufb03cient execution in cases where the same\nquery can be compiled once and then run multiple times with di\ufb00erent parameter val-\nues. However, there is an even more signi\ufb01cant advantage to prepared statements that\nmakes them the preferred method of executing SQL queries whenever a user-entered\nvalue is used, even if the query is to be run only once. Suppose that we read in a user-\nentered value and then use Java string manipulation to construct the SQL statement.\n", "217": "5.1 Accessing SQL from a Programming Language 189\nPreparedStatement pStmt = conn.prepareStatement(\n\"insert into instructor values(?,?,?,?)\");\npStmt.setString(1, \"88877\");\npStmt.setString(2, \"Perry\");\npStmt.setString(3, \"Finance\");\npStmt.setInt(4, 125000);\npStmt.executeUpdate();\npStmt.setString(1, \"88878\");\npStmt.executeUpdate();\nFigure 5.2 Prepared statements in JDBC code.\nIf the user enters certain special characters, such as a single quote, the resulting SQL\nstatement may be syntactically incorrect unless we take extraordinary care in checking\nthe input. The setString() method does this for us automatically and inserts the needed\nescape characters to ensure syntactic correctness.\nIn our example, suppose that the values for the variables ID,name ,dept\n name ,\nandsalary have been entered by a user, and a corresponding row is to be inserted into\ntheinstructor relation. Suppose that, instead of using a prepared statement, a query is\nconstructed by concatenating the strings using the following Java expression:\n\"insert into instructor values(\u2019 \" + ID+\"\u2019 ,\u2019\"+n a m e+\"\u2019 ,\"+\n\"\u2019 \"+d e p t\n n a m e+\"\u2019 ,\"+s a l a r y+\" ) \"\nand the query is executed directly using the executeQuery() method of a Statement\nobject. Observe the use of single quotes in the string, which would surround the values\nofID,name anddept\n name in the generated SQL query.\nNow, if the user typed a single quote in the IDor name \ufb01elds, the query string would\nhave a syntax error. It is quite possible tha t an instructor name may have a quotation\nmark in its name (for example, \u201cO\u2019Henry\u201d).\nWhile the above example might be considered an annoyance, the situation can be\nmuch worse. A technique called SQL injection can be used by malicious hackers to steal\ndata or damage the database.\nS u p p o s eaJ a v ap r o g r a mi n p u t sas t r i n g name and constructs the query:\n\"select * from instructor where name = \u2019\" + name + \"\u2019\"\nIf the user, instead of entering a name, enters:\nX\u2019 or \u2019Y\u2019 = \u2019Y\nthen the resulting statement becomes:\n", "218": "190 Chapter 5 Advanced SQL\n\"select * from instructor where name = \u2019\" + \"X\u2019 or \u2019Y\u2019 = \u2019Y\" + \"\u2019\"\nwhich is:\nselect * from instructor where name = \u2019X\u2019 or \u2019Y\u2019 = \u2019Y\u2019\nIn the resulting query, the where clause is always true and the entire instructor relation\nis returned.\nMore clever malicious users could arrange to output even more data, including\ncredentials such as passwords that allow the user to connect to the database and per-\nform any actions they want. SQL injection attacks on update statements can be used to\nchange the values that are being stored in updated columns. In fact there have been a\nnumber of attacks in the real world using SQL injections; attacks on multiple \ufb01nancial\nsites have resulted in theft of large amounts of money by using SQL injection attacks.\nUse of a prepared statement would prevent this problem because the input string\nwould have escape characters inserted, so the resulting query becomes:\n\"select * from instructor where name = \u2019X \u2216\u2019o r\u2216\u2019Y\u2216\u2019=\u2216\u2019Y\u2019\nwhich is harmless and returns the empty relation.\nProgrammers must pass user-input strings to the database only through parameters of\nprepared statements; creating SQL queries by concatenating strings with user-input values\nis an extremely serious security risk and should never be done in any program.\nSome database systems allow multiple SQL statements to be executed in a single\nJDBC execute method, with statements separated by a semicolon. This feature has\nbeen turned o\ufb00 by default on some JDBC drivers because it allows malicious hackers\nto insert whole SQL statements using SQL injection. For instance, in our earlier SQL\ninjection example a malicious user could enter:\nX\u2019; drop table instructor; \u2013 \u2013\nwhich will result in a query string with two statements separated by a semicolon being\nsubmitted to the database. Because these statements run with the privileges of the\ndatabase userid used by the JDBC connection, devastating SQL statements such as\ndrop table , or updates to any table of the user\u2019s choice, could be executed. However,\nsome databases still allow execution of multiple statements as above; it is thus very\nimportant to correctly use prepared statements to avoid the risk of SQL injection.\n5.1.1.6 Callable Statements\nJDBC also provides a CallableStatement interface that allows invocation of SQLstored\nprocedures and functions (described in Section 5.2). These play the same role for func-\ntions and procedures as prepareStatement does for queries.\n", "219": "5.1 Accessing SQL from a Programming Language 191\nCallableStatement cStmt1 = conn.prepareCall(\"{? = call some\n function(?)}\");\nCallableStatement cStmt2 = conn.prepareCall(\"{call some\n procedure(?,?)}\");\nThe data types of function return values and out parameters of procedures must be\nregistered using the method registerOutParameter() , and can be retrieved using get\nmethods similar to those for result sets. See a JDBC manual for more details.\n5.1.1.7 Metadata Features\nAs we noted earlier, a Java application program does not include declarations for data\nstored in the database. Those declarations are part of the SQL DDL statements. There-\nfore, a Java program that uses JDBC must either have assumptions about the database\nschema hard-coded into the program or determine that information directly from the\ndatabase system at runtime. The latter approach is usually preferable, since it makes\nthe application program more robust to changes in the database schema.\nRecall that when we submit a query using the executeQuery() method, the result of\nthe query is contained in a ResultSet object. The interface ResultSet has a method, get-\nMetaData() , that returns a ResultSetMetaData object that contains metadata about\nthe result set. ResultSetMetaData , in turn, has methods to \ufb01nd metadata information,\ns u c ha st h en u m b e ro fc o l u m n si nt h er e s u l t ,t h en a m eo fas p e c i \ufb01 e dc o l u m n ,o rt h e\ntype of a speci\ufb01ed column. In this way, we can write code to execute a query even if we\nhave no prior knowledge of the schema of the result.\nThe following Java code segment uses JDBC to print out the names and types of all\ncolumns of a result set. The variable rsin the code is assumed to refer to a ResultSet\ninstance obtained by executing a query.\nResultSetMetaData rsmd = rs.getMetaData();\nf o r ( i n ti=1 ;i <= rsmd.getColumnCount(); i++) {\nSystem.out.println(rsmd.getColumnName(i));\nSystem.out.println(rsmd.getColumnTypeName(i));\n}\nThe getColumnCount() method returns the arity (number of attributes) of the\nresult relation. That allows us to iterate through each attribute (note that we start at\n1, as is conventional in JDBC ). For each attribute, we retrieve its name and data type\nusing the methods getColumnName() andgetColumnTypeName() , respectively.\nTheDatabaseMetaData interface provides a way to \ufb01nd metadata about the data-\nbase. The interface Connection has a method getMetaData() that returns a Database-\nMetaData object. The DatabaseMetaData interface in turn has a very large number\nof methods to get metadata about the database and the database system to which the\napplication is connected.\nFor example, there are methods that return the product name and version number\nof the database system. Other methods allow the application to query the database\nsystem about its supported features.\n", "220": "192 Chapter 5 Advanced SQL\nDatabaseMetaData dbmd = conn.getMetaData();\nResultSet rs = dbmd.getColumns(null, \"univdb\", \"department\", \"%\");\n// Arguments to getColumns: Catalog, Schema-pattern, Table-pattern,\n// and Column-Pattern\n// Returns: One row for each column; row has a number of attributes\n// such as COLUMN\n NAME, TYPE\n NAME\nwhile( rs.next()) {\nSystem.out.println(rs.getString(\"COLUMN\n NAME\"),\nrs.getString(\"TYPE\n NAME\");\n}\nFigure 5.3 Finding column information in JDBC using DatabaseMetaData .\nStill other methods return information about the database itself. The code in Fig-\nure 5.3 illustrates how to \ufb01nd information about columns (attributes) of relations in a\ndatabase. The variable conn is assumed to be a handle for an already opened database\nconnection. The method getColumns() takes four arguments: a catalog name (null\nsigni\ufb01es that the catalog name is to be ignored), a schema name pattern, a table name\npattern, and a column name pattern. The schema name, table name, and column name\npatterns can be used to specify a name or a pattern. Patterns can use the SQL string\nmatching special characters \u201c%\u201d and \u201c\n \u201d; for instance, the pattern \u201c%\u201d matches all\nnames. Only columns of tables of schemas satisfying the speci\ufb01ed name or pattern\nare retrieved. Each row in the result set contains information about one column. The\nrows have a number of columns such as the name of the catalog, schema, table and\ncolumn, the type of the column, and so on.\nThegetTables() method allows you to get a list of all tables in the database. The\n\ufb01rst three parameters to getTables() are the same as for getColumns() .T h ef o u r t h\nparameter can be used to restrict the types of tables returned; if set to null, all tables,\nincluding system internal tables are returned, but the parameter can be set to restrict\nthe tables returned to only user-created tables.\nExamples of other methods provided by DatabaseMetaData that provide informa-\ntion about the database include those for primary keys ( getPrimaryKeys() ), foreign-key\nreferences ( getCrossReference()) , authorizations, database limits such as maximum\nnumber of connections, and so on.\nThe metadata interfaces can be used for a variety of tasks. For example, they can\nbe used to write a database browser that allows a user to \ufb01nd the tables in a database,\nexamine their schema, examine rows in a table, apply selections to see desired rows,\nand so on. The metadata information can be used to make code used for these tasks\ngeneric; for example, code to display the rows in a relation can be written in such a way\nthat it would work on all possible relations regardless of their schema. Similarly, it is\n", "221": "5.1 Accessing SQL from a Programming Language 193\npossible to write code that takes a query string, executes the query, and prints out the\nresults as a formatted table; the code can work regardless of the actual query submitted.\n5.1.1.8 Other Features\nJDBC provides a number of other features, such as updatable result sets .I tc a nc r e a t e\nan updatable result set from a query that performs a selection and/or a projection on\na database relation. An update to a tuple in the result set then results in an update to\nthe corresponding tuple of the database relation.\nRecall from Section 4.3 that a transaction allows multiple actions to be treated as a\nsingle atomic unit which can be committed or rolled back. By default, each SQL state-\nment is treated as a separate transaction that is committed automatically. The method\nsetAutoCommit() in the JDBC Connection interface allows this behavior to be turned\non or o\ufb00. Thus, if conn is an open connection, conn.setAutoCommit(false) turns o\ufb00\nautomatic commit. Transactions must then be committed or rolled back explicitly using\neither conn.commit() orconn.rollback() .conn.setAutoCommit(true) turns on auto-\nmatic commit.\nJDBC provides interfaces to deal with large objects without requiring an entire large\nobject to be created in memory. To fetch large objects, the ResultSet interface provides\nmethods getBlob() andgetClob() that are similar to the getString() method, but return\nobjects of type Blob andClob , respectively. These objects do not store the entire large\nobject, but instead store \u201clocators\u201d for the large objects, that is, logical pointers to the\nactual large object in the database. Fetching data from these objects is very much like\nfetching data from a \ufb01le or an input stream, and it can be performed using methods\nsuch as getBytes() andgetSubString() .\nConversely, to store large objects in the database, the PreparedStatement class\npermits a database column whose type is blob to be linked to an input stream (such\nas a \ufb01le that has been opened) using the method setBlob(int parameterIndex, Input-\nStream inputStream) . When the prepared statement is executed, data are read from\nthe input stream and written to the blob in the database. Similarly, a clobcolumn can\nbe set using the setClob() m e t h o d ,w h i c ht a k e sa sa r g u m e n t sap a r a m e t e ri n d e xa n da\ncharacter stream.\nJDBC includes a row set feature that allows result sets to be collected and shipped\nto other applications. Row sets can be scanned both backward and forward and can be\nmodi\ufb01ed.\n5.1.2 Database Access from Python\nDatabase access can be done from Python as illustrated by the method shown in Figure\n5.4. The statement containing the insert query shows how to use the Python equivalent\nofJDBC prepared statements, with parameters identi\ufb01ed in the SQL q u e r yb y\u201c % s \u201d ,\nand parameter values provided as a list. Updates are not committed to the database\nautomatically; the commit() method needs to be called to commit an update.\n", "222": "194 Chapter 5 Advanced SQL\nimport psycopg2\ndef PythonDatabaseExample(userid, passwd)\ntry:\nconn = psycopg2.connect( host=\"db.yale.edu\", port=5432,\ndbname=\"univdb\", user=userid, password=passwd)\ncur = conn.cursor()\ntry:\ncur.execute(\"insert into instructor values(%s, %s, %s, %s)\",\n(\"77987\",\"Kim\",\"Physics\",98000))\nconn.commit();\nexcept Exception as sqle:\nprint(\"Could not insert tuple. \", sqle)\nconn.rollback()\ncur.execute( (\"select dept\n name, avg (salary) \"\n\" from instructor group by dept\n name\"))\nfor dept in cur:\nprint dept[0], dept[1]\nexcept Exception as sqle:\nprint(\"Exception : \", sqle)\nFigure 5.4 Database access from Python\nThetry:,except\u2026:block shows how to catch exceptions and to print information\nabout the exception. The forloop illustrates how to loop over the result of a query\nexecution, and to access individual attributes of a particular row.\nThe preceding program uses the psycopg2 driver, which allows connection to\nPostgre SQL databases and is imported in the \ufb01rst line of the program. Drivers are usu-\nally database speci\ufb01c, with the MySQLdb driver to connect to MySQL,a n dc x\n Oracle to\nconnect to Oracle; but the pyodbc driver can connect to most databases that support\nODBC . The Python Database APIused in the program is implemented by drivers for\nmany databases, but unlike with JDBC , there are minor di\ufb00erences in the APIacross\ndi\ufb00erent drivers, in particular in the parameters to the connect() function.\n5.1.3 ODBC\nThe Open Database Connectivity (ODBC ) standard de\ufb01nes an APIthat applications\ncan use to open a connection with a database, send queries and updates, and get back\nresults. Applications such as graphical user interfaces, statistics packages, and spread-\nsheets can make use of the same ODBC API to connect to any database server that\nsupports ODBC .\nEach database system supporting ODBC provides a library that must be linked\nwith the client program. When the client program makes an ODBC API call, the code\n", "223": "5.1 Accessing SQL from a Programming Language 195\nvoid ODBCexample ()\n{\nRETCODE error;\nHENV env; /* environment */\nHDBC conn; /* database connection */\nSQLAllocEnv (&env);\nSQLAllocConnect (env, &conn);\nSQLConnect (conn, \"db.yale.edu\", SQL\n NTS,\" a v i \" , SQL\n NTS,\n\"avipasswd\", SQL\n NTS);\n{\nchar deptname[80];\nfloat salary;\nint lenOut1, lenOut2;\nHSTMT stmt;\nchar * sqlquery = \"select dept\n name, sum (salary)\nfrom instructor\ngroup by dept\n name\";\nSQLAllocStmt (conn, &stmt);\nerror = SQLExecDirect (stmt, sqlquery, SQL\n NTS);\nif (error == SQL\n SUCCESS ){\nSQLBindCol (stmt, 1, SQL\n C\nCHAR , deptname , 80, &lenOut1);\nSQLBindCol (stmt, 2, SQL\n C\nFLOAT ,& s a l a r y ,0,& l e n O u t 2 ) ;\nwhile ( SQLFetch (stmt)==SQL\n SUCCESS ){\nprintf (\" %s %g \u2216n\", deptname, salary);\n}\n}\nSQLFreeStmt (stmt, SQL\n DROP );\n}\nSQLDisconnect (conn);\nSQLFreeConnect (conn);\nSQLFreeEnv (env);\n}\nFigure 5.5 ODBC code example.\nin the library communicates with the server to carry out the requested action and fetch\nresults.\nFigure 5.5 shows an example of C code using the ODBC API . The \ufb01rst step in using\nODBC to communicate with a server is to set up a connection with the server. To do so,\nthe program \ufb01rst allocates an SQL environment, then a database connection handle.\nODBC de\ufb01nes the types HENV ,HDBC ,a n d RETCODE . The program then opens the\n", "224": "196 Chapter 5 Advanced SQL\ndatabase connection by using SQLConnect . This call takes several parameters, includ-\ning the connection handle, the server to which to connect, the user identi\ufb01er, and the\npassword for the database. The constant SQL\n NTS denotes that the previous argument\nis a null-terminated string.\nOnce the connection is set up, the program can send SQL commands to the\ndatabase by using SQLExecDirect . C language variables can be bound to attributes of\nthe query result, so that when a result tuple is fetched using SQLFetch , its attribute val-\nues are stored in corresponding C variables. The SQLBindCol f u n c t i o nd o e st h i st a s k ;\nthe second argument identi\ufb01es the position of the attribute in the query result, and the\nthird argument indicates the type conversion required from SQL to C. The next argu-\nment gives the address of the variable. For variable-length types like character arrays,\nthe last two arguments give the maximum length of the variable and a location where\nt h ea c t u a ll e n g t hi st ob es t o r e dw h e nat u p l ei sf e t c h e d .An e g a t i v ev a l u er e t u r n e df o r\nthe length \ufb01eld indicates that the value is null. For \ufb01xed-length types such as integer\nor \ufb02oat, the maximum length \ufb01eld is ignored, while a negative value returned for the\nlength \ufb01eld indicates a null value.\nTheSQLFetch statement is in a while loop that is executed until SQLFetch returns\na value other than SQL\n SUCCESS .O ne a c hf e t c h ,t h ep r o g r a ms t o r e st h ev a l u e si nC\nvariables as speci\ufb01ed by the calls on SQLBindCol and prints out these values.\nAt the end of the session, the program frees the statement handle, disconnects\nfrom the database, and frees up the connection and SQL environment handles. Good\nprogramming style requires that the result of every function call must be checked to\nmake sure there are no errors; we have omitted most of these checks for brevity.\nIt is possible to create an SQL statement with parameters; for example, consider the\nstatement insert into department values(?,?,?) . The question marks are placeholders\nfor values which will be supplied later. The above statement can be \u201cprepared,\u201d that is,\ncompiled at the database, and repeatedly executed by providing actual values for the\nplaceholders\u2014in this case, by providing a department name, building, and budget for\nthe relation department .\nODBC de\ufb01nes functions for a variety of tasks, such as \ufb01nding all the relations in\nthe database and \ufb01nding the names and types of columns of a query result or a relation\nin the database.\nBy default, each SQL statement is treated as a separate transaction that is commit-\nted automatically. The SQLSetConnectOption (conn ,SQL\n AUTOCOMMIT, 0) turns\no\ufb00 automatic commit on connection conn , and transactions must then be committed\nexplicitly by SQLTransact(conn, SQL\n COMMIT) or rolled back by SQLTransact(conn,\nSQL\n ROLLBACK) .\nThe ODBC standard de\ufb01nes conformance levels , which specify subsets of the func-\ntionality de\ufb01ned by the standard. An ODBC implementation may provide only core\nlevel features, or it may provide more advanced (level 1 or level 2) features. Level 1\nrequires support for fetching information about the catalog, such as information about\nwhat relations are present and the types of their attributes. Level 2 requires further fea-\n", "225": "5.1 Accessing SQL from a Programming Language 197\ntures, such as the ability to send and retrieve arrays of parameter values and to retrieve\nmore detailed catalog information.\nThe SQL standard de\ufb01nes a call level interface (CLI) that is similar to the ODBC\ninterface.\n5.1.4 Embedded SQL\nThe SQL standard de\ufb01nes embeddings of SQL in a variety of programming languages,\nsuch as C, C++, Cobol, Pascal, Java, PL/I, and Fortran. A language in which SQL\nqueries are embedded is referred to as a host language, and the SQL structures per-\nmitted in the host language constitute embedded SQL.\nPrograms written in the host language can use the embedded SQL syntax to access\nand update data stored in a database. An embedded SQL program must be processed\nby a special preprocessor prior to compilation. The preprocessor replaces embedded\nSQL requests with host-language declarations and procedure calls that allow runtime\nexecution of the database accesses. Then the resulting program is compiled by the host-\nlanguage compiler. This is the main distinction between embedded SQL and JDBC or\nODBC .\nTo identify embedded SQL requests to the preprocessor, we use the EXEC SQL\nstatement; it has the form:\nEXEC SQL <embedded SQL statement >;\nBefore executing any SQL statements, the program must \ufb01rst connect to the database.\nVariables of the host language can be used within embedded SQL statements, but they\nmust be preceded by a colon (:) to distinguish them from SQL variables.\nTo iterate over the results of an embedded SQL query, we must declare a cursor\nvariable, which can then be opened, and fetch commands issued in a host language\nloop to fetch consecutive rows of the query result. Attributes of a row can be fetched\ninto host language variables. Database updates can also be performed using a cursor\non a relation to iterate through the rows of the relation, optionally using a where clause\nto iterate through only selected rows. Embedded SQL commands can be used to update\nthe current row where the cursor is pointing.\nThe exact syntax for embedded SQL requests depends on the language in which\nSQL is embedded. You may refer to the manuals of the speci\ufb01c language embedding\nthat you use for further details.\nInJDBC ,SQL statements are interpreted at runtime (even if they are created using\nthe prepared statement feature). When embedded SQL is used, there is a potential for\ncatching some SQL-related errors (including data-type errors) at the time of prepro-\ncessing. SQL queries in embedded SQL programs are also easier to comprehend than\nin programs using dynamic SQL. However, there are also some disadvantages with em-\nbedded SQL. The preprocessor creates new host language code, which may complicate\ndebugging of the program. The constructs used by the preprocessor to identify SQL\n", "226": "198 Chapter 5 Advanced SQL\nNote 5.1 EMBEDDED DATABASES\nBoth JDBC and ODBC assume that a server is running on the database system\nhosting the database. Some applications use a database that exists entirely within\nthe application. Such applications maintain the database only for internal use and\no\ufb00er no accessibility to the database except through the application itself. In such\ncases, one may use an embedded database and use one of several packages that\nimplement an SQL database accessible from within a programming language. Pop-\nular choices include Java DB, SQLite,HSQLBD ,a n d \u02dd2. There is also an embedded\nversion of MySQL.\nEmbedded database systems lack many of the features of full server-based\ndatabase systems, but they o\ufb00er advantages for applications that can bene\ufb01t from\nthe database abstractions but do not need to support very large databases or large-\nscale transaction processing.\nDo not confuse embedded databases with embedded SQL; the latter is a means\nof connecting to a database running on a server.\nstatements may clash syntactically with host language syntax introduced in subsequent\nversions of the host language.\nAs a result, most current systems use dynamic SQL, rather than embedded SQL.\nOne exception is the Microsoft Language Integrated Query ( LINQ ) facility, which ex-\ntends the host language to include support for queries instead of using a preprocessor\nto translate embedded SQL queries into the host language.\n5.2 Functions and Procedures\nWe have already seen several functions that are built into the SQL language. In this\nsection, we show how developers can write their own functions and procedures, store\nthem in the database, and then invoke them from SQL statements. Functions are par-\nticularly useful with specialized data types such as images and geometric objects. For\ninstance, a line-segment data type used in a map database may have an associated func-\ntion that checks whether two line segments overlap, and an image data type may have\nassociated functions to compare two images for similarity.\nProcedures and functions allow \u201cbusiness logic\u201d to be stored in the database and ex-\necuted from SQL statements. For example, universities usually have many rules about\nhow many courses a student can take in a given semester, the minimum number of\ncourses a full-time instructor must teach in a year, the maximum number of majors a\nstudent can be enrolled in, and so on. While such business logic can be encoded as\nprogramming-language procedures stored entirely outside the database, de\ufb01ning them\nas stored procedures in the database has several advantages. For example, it allows\n", "227": "5.2 Functions and Procedures 199\ncreate function dept\n count (dept\n name varchar(20) )\nreturns integer\nbegin\ndeclare d\ncount integer ;\nselect count (*)into d\ncount\nfrom instructor\nwhere instructor .dept\n name=dept\n name\nreturn d\ncount ;\nend\nFigure 5.6 Function defined in SQL.\nmultiple applications to access the procedures, and it allows a single point of change in\ncase the business rules change, without changing other parts of the application. Appli-\ncation code can then call the stored procedures instead of directly updating database\nrelations.\nSQL allows the de\ufb01nition of functions, procedures, and methods. These can be\nde\ufb01ned either by the procedural component of SQL or by an external programming\nlanguage such as Java, C, or C++. We look at de\ufb01nitions in SQL \ufb01rst and then see how\nto use de\ufb01nitions in external languages in Section 5.2.3.\nAlthough the syntax we present here is de\ufb01ned by the SQL standard, most\ndatabases implement nonstandard versions of this syntax. For example, the procedural\nlanguages supported by Oracle ( PL/SQL ), Microsoft SQL S erver (Transact SQL), and\nPostgre SQL (PL/pgSQL) all di\ufb00er from the standard syntax we present here. We illus-\ntrate some of the di\ufb00erences for the case of Oracle in Note 5.2 on page 204. See the\nrespective system manuals for further details. Although parts of the syntax we present\nhere may not be supported on such systems, the concepts we describe are applicable\nacross implementations, although with a di\ufb00erent syntax.\n5.2.1 Declaring and Invoking SQL Functions and Procedures\nSuppose that we want a function that, given the name of a department, returns the\ncount of the number of instructors in that department. We can de\ufb01ne the function as\nshown in Figure 5.6.4This function can be used in a query that returns names and\nbudgets of all departments with more than 12 instructors:\nselect dept\n name ,budget\nfrom department\nwhere dept\n count (dept\n name )>12;\n4If you are entering your own functions or procedures, you should write \u201c create or replace \u201d rather than create so that it\nis easy to modify your code (by replacing the function) during debugging.\n", "228": "200 Chapter 5 Advanced SQL\ncreate function instructor\n of(dept\n name varchar (20))\nreturns table (\nIDvarchar (5),\nname varchar (20),\ndept\n name varchar (20),\nsalary numeric (8,2))\nreturn table\n(select ID,name ,dept\n name ,salary\nfrom instructor\nwhere instructor .dept\n name =instructor\n of.dept\n name );\nFigure 5.7 Table function in SQL.\nPerformance problems have been observed on many database systems when in-\nvoking complex user-de\ufb01ned functions within a query, if the functions are invoked on\na large number of tuples. Programmers should therefore take performance into consid-\neration when deciding whether to use user-de\ufb01ned functions in a query.\nThe SQL standard supports functions that can return tables as results; such func-\ntions are called table functions . Consider the function de\ufb01ned in Figure 5.7. The func-\ntion returns a table containing all the instructors of a particular department. Note that\nthe function\u2019s parameter is referenced by pre\ufb01xing it with the name of the function\n(instructor\n of.dept\n name ).\nThe function can be used in a query as follows:\nselect *\nfrom table (instructor\n of('Finance'));\nThis query returns all instructors of the 'Finance' department. In this simple case it\nis straightforward to write this query without using table-valued functions. In general,\nhowever, table-valued functions can be thought of as parameterized views that generalize\nthe regular notion of views by allowing parameters.\nSQL also supports procedures. The dept\n count function could instead be written as\nap r o c e d u r e :\ncreate procedure dept\n count\n proc(indept\n name varchar(20) ,\noutd\ncount integer )\nbegin\nselect count (*)into d\ncount\nfrom instructor\nwhere instructor .dept\n name=dept\n count\n proc.dept\n name\nend\n", "229": "5.2 Functions and Procedures 201\nThe keywords inand outindicate, respectively, parameters that are expected to\nhave values assigned to them and parameters whose values are set in the procedure in\norder to return results.\nProcedures can be invoked either from an SQL procedure or from embedded SQL\nby the callstatement:\ndeclare d\ncount integer ;\ncall dept\n count\n proc('Physics', d\ncount );\nProcedures and functions can be invoked from dynamic SQL, as illustrated by the JDBC\nsyntax in Section 5.1.1.5.\nSQL permits more than one procedure of the same name, so long as the number of\narguments of the procedures with the same name is di\ufb00erent. The name, along with the\nnumber of arguments, is used to identify the procedure. SQL also permits more than\none function with the same name, so long as the di\ufb00erent functions with the same name\neither have di\ufb00erent numbers of arguments, or for functions with the same number of\narguments, they di\ufb00er in the type of at least one argument.\n5.2.2 Language Constructs for Procedures and Functions\nSQL supports constructs that give it almost all the power of a general-purpose program-\nming language. The part of the SQL standard that deals with these constructs is called\nthePersistent Storage Module ( PSM ).\nVariables are declared using a declare statement and can have any valid SQL data\ntype. Assignments are performed using a setstatement.\nA compound statement is of the form begin\u2026end, and it may contain multiple\nSQL statements between the begin and the end. Local variables can be declared within\na compound statement, as we have seen in Section 5.2.1. A compound statement of\nthe form begin atomic \u2026endensures that all the statements contained within it are\nexecuted as a single transaction.\nThe syntax for while statements and repeat statements is:\nwhile boolean expression do\nsequence of statements ;\nend while\nrepeat\nsequence of statements ;\nuntil boolean expression\nend repeat\nThere is also a forloop that permits iteration over all the results of a query:\n", "230": "202 Chapter 5 Advanced SQL\ndeclare ninteger default 0;\nforras\nselect budget from department\nwhere dept\n name=\u2018Music\u2018\ndo\nsetn=n\u2212r.budget\nend for\nThe program fetches the query results one row at a time into the forloop variable ( r,i n\nthe above example). The statement leave can be used to exit the loop, while iterate starts\non the next tuple, from the beginning of the loop, skipping the remaining statements.\nThe conditional statements supported by SQL include if-then-else statements by\nusing this syntax:\nifboolean expression\nthen statement or compound statement\nelseif boolean expression\nthen statement or compound statement\nelse statement or compound statement\nend if\nSQL also supports a case statement similar to the C/C++ language case statement\n(in addition to case expressions, which we saw in Chapter 3).\nFigure 5.8 provides a larger example of the use of procedural constructs in SQL.\nThe function registerStudent de\ufb01ned in the \ufb01gure registers a student in a course section\nafter verifying that the number of students in the section does not exceed the capacity of\nthe room allocated to the section. The function returns an error code\u2014a value greater\nthan or equal to 0 signi\ufb01es success, and a negative value signi\ufb01es an error condition\u2014\nand a message indicating the reason for the failure is returned as an outparameter.\nThe SQL procedural language also supports the signaling of exception conditions\nand declaring of handlers that can handle the exception, as in this code:\ndeclare out\nof\nclassroom\n seats condition\ndeclare exit handler for out\nof\nclassroom\n seats\nbegin\nsequence of statements\nend\nThe statements between the begin and the endcan raise an exception by executing sig-\nnalout\nof\nclassroom\n seats. The handler says that if the condition arises, the action to\nbe taken is to exit the enclosing begin end statement. Alternative actions would be con-\ntinue , which continues execution from the next statement following the one that raised\nthe exception. In addition to explicitly de \ufb01ned conditions, there are also prede\ufb01ned\nconditions such as sqlexception ,sqlwarning ,a n d not found .\n", "231": "5.2 Functions and Procedures 203\n\u2013 \u2013 Registers a student after ensuring classroom capacity is not exceeded\n\u2013\u2013R e t u r n s0o ns u c c e s s ,a n d- 1i fc a p a c i t yi se x c e e d e d .\ncreate function registerStudent (\nins\nidvarchar (5),\nins\ncourseid varchar (8),\nins\nsecid varchar (8),\nins\nsemester varchar (6),\nins\nyear numeric (4,0),\nouterrorMsg varchar (100)\nreturns integer\nbegin\ndeclare currEnrol int;\nselect count (*)into currEnrol\nfrom takes\nwhere course\n id=s\ncourseid and sec\nid=s\nsecid\nand semester =s\nsemester and year =s\nyear;\ndeclare limit int;\nselect capacity into limit\nfrom classroom natural join section\nwhere course\n id=s\ncourseid and sec\nid=s\nsecid\nand semester =s\nsemester and year =s\nyear;\nif(currEnrol <limit)\nbegin\ninsert into takes values\n(s\nid,s\ncourseid ,s\nsecid ,s\nsemester ,s\nyear, null);\nreturn (0);\nend\n\u2013 \u2013 Otherwise, section capacity limit already reached\nseterrorMsg = \u2019Enrollment limit reached for course \u2019 ||s\ncourseid\n||\u2019s e c t i o n\u2019 ||s\nsecid ;\nreturn (-1);\nend;\nFigure 5.8 Procedure to register a student for a course section.\n5.2.3 External Language Routines\nAlthough the procedural extensions to SQL can be very useful, they are unfortunately\nnot supported in a standard way across databases. Even the most basic features have\ndi\ufb00erent syntax or semantics in di\ufb00erent database products. As a result, programmers\nhave to learn a new language for each database product. An alternative that is gaining\n", "232": "204 Chapter 5 Advanced SQL\nNote 5.2 NONSTANDARD SYNTAX FOR PROCEDURES AND FUNCTIONS\nAlthough the SQL standard de\ufb01nes the syntax for procedures and functions, most\ndatabases do not follow the standard strictly, and there is considerable variation in\nthe syntax supported. One of the reasons for this situation is that these databases\ntypically introduced support for procedures and functions before the syntax was\nstandardized, and they continue to support their original syntax. It is not possi-\nble to list the syntax supported by each database here, but we illustrate a few of\nthe di\ufb00erences in the case of Oracle\u2019s PL/SQL by showing below a version of the\nfunction from Figure 5.6 as it would be de\ufb01ned in PL/SQL.\ncreate function dept\n count (dname ininstructor.dept\n name %type )return integer\nas\nd\ncount integer ;\nbegin\nselect count (*)into d\ncount\nfrom instructor\nwhere instructor.dept\n name =dname ;\nreturn d\ncount ;\nend;\nWhile the two versions are similar in concept, there are a number of minor syn-\ntactic di\ufb00erences, some of which are evident when comparing the two versions of\nthe function. Although not shown here, the syntax for control \ufb02ow in PL/SQL also\nhas several di\ufb00erences from the syntax presented here.\nObserve that PL/SQL a l l o w sat y p et ob es p e c i \ufb01 e da st h et y p eo fa na t t r i b u t eo f\na relation, by adding the su\ufb03x %type . On the other hand, PL/SQL does not directly\nsupport the ability to return a table, although there is an indirect way of implement-\ning this functionality by creating a table type. The procedural languages supported\nby other databases also have a number of syntactic and semantic di\ufb00erences. See\nthe respective language references for more information. The use of nonstandard\nsyntax for stored procedures and functions is an impediment to porting an appli-\ncation to a di\ufb00erent database.\nsupport is to de\ufb01ne procedures in an imperative programming language, but allow them\nto be invoked from SQL queries and trigger de\ufb01nitions.\nSQL allows us to de\ufb01ne functions in a programming language such as Java, C#, C,\nor C++. Functions de\ufb01ned in this fashion can be more e\ufb03cient than functions de\ufb01ned\ninSQL, and computations that cannot be carried out in SQL can be executed by these\nfunctions.\n", "233": "5.2 Functions and Procedures 205\nExternal procedures and functions can be speci\ufb01ed in this way (note that the exact\nsyntax depends on the speci\ufb01c database system you use):\ncreate procedure dept\n count\n proc( indept\n name varchar (20),\noutcount integer )\nlanguage C\nexternal name '/usr/avi/bin/dept\n count\n proc'\ncreate function dept\n count ( dept\n name varchar (20))\nreturns integer\nlanguage C\nexternal name '/usr/avi/bin/dept\n count'\nIn general, the external language procedures need to deal with null values in parameters\n(both inand out) and return values. They also need to communicate failure/success\nstatus and to deal with exceptions. This information can be communicated by extra\nparameters: an sqlstate value to indicate failure/success status, a parameter to store the\nreturn value of the function, and indicator variables for each parameter/function result\nto indicate if the value is null. Other mechanisms are possible to handle null values,\nfor example, by passing pointers instead of values. The exact mechanisms depend on\nthe database. However, if a function does not deal with these situations, an extra line\nparameter style general can be added to the declaration to indicate that the external\nprocedures/functions take only the arguments shown and do not handle null values or\nexceptions.\nFunctions de\ufb01ned in a programming language and compiled outside the database\nsystem may be loaded and executed with the database-system code. However, doing\nso carries the risk that a bug in the program can corrupt the internal structures of\nthe database and can bypass the access-control functionality of the database system.\nDatabase systems that are concerned more about e\ufb03cient performance than about se-\ncurity may execute procedures in such a fashion. Database systems that are concerned\nabout security may execute such code as part of a separate process, communicate the\nparameter values to it, and fetch results back via interprocess communication. How-\never, the time overhead of interprocess communication is quite high; on typical CPU\narchitectures, tens to hundreds of thousands of instructions can execute in the time\ntaken for one interprocess communication.\nIf the code is written in a \u201csafe\u201d language such as Java or C#, there is another\npossibility: executing the code in a sandbox within the database query execution process\nitself. The sandbox allows the Java or C# code to access its own memory area, but it\nprevents the code from reading or updating the memory of the query execution process,\nor accessing \ufb01les in the \ufb01le system. (Creating a sandbox is not possible for a language\nsuch as C, which allows unrestricted access to memory through pointers.) Avoiding\ninterprocess communication reduces function call overhead greatly.\n", "234": "206 Chapter 5 Advanced SQL\nSeveral database systems today support external language routines running in a\nsandbox within the query execution process. For example, Oracle and IBM DB2 allow\nJava functions to run as part of the database process. Microsoft SQL S erver allows\nprocedures compiled into the Common Language Runtime (CLR) to execute within\nthe database process; such procedures could have been written, for example, in C# or\nVisual Basic. Postgre SQL allows functions de\ufb01ned in several languages, such as Perl,\nPython, and Tcl.\n5.3 Triggers\nAtrigger is a statement that the system executes automatically as a side e\ufb00ect of a\nmodi\ufb01cation to the database. To de\ufb01ne a trigger, we must:\n\u2022Specify when a trigger is to be executed. This is broken up into an event that causes\nthe trigger to be checked and a condition that must be satis\ufb01ed for trigger execution\nto proceed.\n\u2022Specify the actions to be taken when the trigger executes.\nOnce we enter a trigger into the database, the database system takes on the responsibil-\nity of executing it whenever the speci\ufb01ed event occurs and the corresponding condition\nis satis\ufb01ed.\n5.3.1 Need for Triggers\nTriggers can be used to implement certain integrity constraints that cannot be speci-\n\ufb01ed using the constraint mechanism of SQL. Triggers are also useful mechanisms for\nalerting humans or for starting certain tasks automatically when certain conditions are\nmet. As an illustration, we could design a trigger that, whenever a tuple is inserted into\nthetakes relation, updates the tuple in the student relation for the student taking the\ncourse by adding the number of credits for the course to the student\u2019s total credits. As\nanother example, suppose a warehouse wishes to maintain a minimum inventory of\neach item; when the inventory level of an item falls below the minimum level, an order\ncan be placed automatically. On an update of the inventory level of an item, the trigger\ncompares the current inventory level with the minimum inventory level for the item,\nand if the level is at or below the minimum, a new order is created.\nNote that triggers cannot usually perform updates outside the database, and hence,\nin the inventory replenishment example, we cannot use a trigger to place an order in the\nexternal world. Instead, we add an order to a relation holding reorders. We must create\na separate permanently running system process that periodically scans that relation\nand places orders. Some database systems provide built-in support for sending email\nfrom SQL queries and triggers using this approach.\n", "235": "5.3 Triggers 207\ncreate trigger timeslot\n check1 after insert on section\nreferencing new row as nrow\nfor each row\nwhen (nrow.time\n slot\n idnot in (\nselect time\n slot\n id\nfrom time\n slot)) /* time\n slot\n idnot present in time\n slot*/\nbegin\nrollback\nend;\ncreate trigger timeslot\n check2 after delete on timeslot\nreferencing old row as orow\nfor each row\nwhen (orow.time\n slot\n idnot in (\nselect time\n slot\n id\nfrom time\n slot)/ *l a s tt u p l ef o r time\n slot\n iddeleted from time\n slot*/\nand orow.time\n slot\n idin(\nselect time\n slot\n id\nfrom section )) /* and time\n slot\n idstill referenced from section */\nbegin\nrollback\nend;\nFigure 5.9 Using triggers to maintain referential integrity.\n5.3.2 Triggers in SQL\nWe now consider how to implement triggers in SQL. The syntax we present here is de-\n\ufb01ned by the SQL standard, but most databases implement nonstandard versions of this\nsyntax. Although the syntax we present here may not be supported on such systems,\nthe concepts we describe are applicable across implementations. We discuss nonstan-\ndard trigger implementations in Note 5.3 on page 212. In each system, trigger syntax\nis based upon that system\u2019s syntax for coding functions and procedures.\nFigure 5.9 shows how triggers can be used to ensure referential integrity on the time\nslot\n idattribute of the section relation. The \ufb01rst trigger de\ufb01nition in the \ufb01gure speci\ufb01es\nthat the trigger is initiated after any insert on the relation section and it ensures that the\ntime\n slot\n idvalue being inserted is valid. SQL bf insert statement could insert multiple\ntuples of the relation, and the for each row clause in the trigger code would then explic-\nitly iterate over each inserted row. The referencing new row as clause creates a variable\nnrow (called a transition variable ) that stores the value of the row being inserted.\n", "236": "208 Chapter 5 Advanced SQL\nThe when statement speci\ufb01es a condition. The system executes the rest of the trig-\nger body only for tuples that satisfy the condition. The begin atomic \u2026endclause can\nserve to collect multiple SQL statements into a single compound statement. In our ex-\nample, though, there is only one statement, which rolls back the transaction that caused\nthe trigger to get executed. Thus, any transaction that violates the referential integrity\nconstraint gets rolled back, ensuring the data in the database satis\ufb01es the constraint.\nIt is not su\ufb03cient to check referential integrity on inserts alone; we also need to\nconsider updates of section , as well as deletes and updates to the referenced table time\nslot. The second trigger de\ufb01nition in Figure 5.9 considers the case of deletes to time\nslot. This trigger checks that the time\n slot\n idof the tuple being deleted is either still\npresent in time\n slot, or that no tuple in section contains that particular time\n slot\n idvalue;\notherwise, referential integrity would be violated.\nTo ensure referential integrity, we would also have to create triggers to handle up-\ndates to section and time\n slot; we describe next how triggers can be executed on updates,\nbut we leave the de\ufb01nition of these triggers as an exercise to the reader.\nFor updates, the trigger can specify attributes whose update causes the trigger to\nexecute; updates to other attributes would not cause it to be executed. For example, to\nspecify that a trigger executes after an update to the grade attribute of the takes relation,\nwe write:\nafter update of takes ongrade\nThe referencing old row as clause can be used to create a variable storing the old\nvalue of an updated or deleted row. The referencing new row as clause can be used with\nupdates in addition to inserts.\nFigure 5.10 shows how a trigger can be used to keep the tot\ncred attribute value of\nstudent tuples up-to-date when the grade attribute is updated for a tuple in the takes\nrelation. The trigger is executed only when the grade attribute is updated from a value\nthat is either null or \u2019F\u2019 to a grade that indicates the course is successfully completed.\nThe update statement is normal SQL syntax except for the use of the variable nrow.\nA more realistic implementation of this example trigger would also handle grade\ncorrections that change a successful completion grade to a failing grade and handle\ninsertions into the takes relation where the grade indicates successful completion. We\nleave these as an exercise for the reader.\nAs another example of the use of a trigger, the action on delete of a student tuple\ncould be to check if the student has any entries in the takes relation, and if so, to delete\nthem.\nMany database systems support a variety of other triggering events, such as when\na user (application) logs on to the database (that is, opens a connection), the system\nshuts down, or changes are made to system settings.\nTriggers can be activated before the event ( insert ,delete ,o rupdate )i n s t e a do f after\nthe event. Triggers that execute before an event can serve as extra constraints that can\nprevent invalid updates, inserts, or deletes. Instead of letting the invalid action proceed\n", "237": "5.3 Triggers 209\ncreate trigger credits\n earned after update of takes ongrade\nreferencing new row as nrow\nreferencing old row as orow\nfor each row\nwhen nrow.grade <>\u2019F\u2019and nrow.grade is not null\nand(orow.grade =\u2019F\u2019ororow.grade is null )\nbegin atomic\nupdate student\nsettot\ncred=tot\ncred+\n(select credits\nfrom course\nwhere course .course\n id=nrow.course\n id)\nwhere student.id =nrow.id ;\nend;\nFigure 5.10 Using a trigger to maintain credits\n earned values.\nand cause an error, the trigger might take action to correct the problem so that the\nupdate ,insert ,o rdelete becomes valid. For example, if we attempt to insert an instructor\ninto a department whose name does not appear in the department relation, the trigger\ncould insert a tuple into the department relation for that department name before the\ninsertion generates a foreign-key violation. As another example, suppose the value of an\ninserted grade is blank, presumably to indicate the absence of a grade. We can de\ufb01ne\na trigger that replaces the value with the nullvalue. The setstatement can be used to\ncarry out such modi\ufb01cations. An example of such a trigger appears in Figure 5.11.\nInstead of carrying out an action for each a\ufb00ected row, we can carry out a single\naction for the entire SQL statement that caused the insert, delete, or update. To do\nso, we use the for each statement clause instead of the for each row clause. The clauses\ncreate trigger setnull before update of takes\nreferencing new row as nrow\nfor each row\nwhen (nrow.grade =\u2019\u2019 )\nbegin atomic\nsetnrow.grade =null;\nend;\nFigure 5.11 Example of using setto change an inserted value.\n", "238": "210 Chapter 5 Advanced SQL\nreferencing old table as orreferencing new table as can then be used to refer to temporary\ntables (called transition tables ) containing all the a\ufb00ected rows. Transition tables cannot\nbe used with before triggers, but they can be used with after triggers, regardless of\nwhether they are statement triggers or row triggers. A single SQL statement can then\nbe used to carry out multiple actions on the basis of the transition tables.\nTriggers can be disabled or enabled; by default they are enabled when they are\ncreated, but they can be disabled by using alter trigger trigger\n name disable (some\ndatabases use alternative syntax such as disable trigger trigger\n name ). A trigger that\nhas been disabled can be enabled again. A trigger can instead be dropped, which re-\nmoves it permanently, by using the command drop trigger trigger\n name .\nReturning to our inventory-replenishment example from Section 5.3.1, suppose we\nhave the following relations:\n\u2022inventory (item, level ), which notes the current amount of the item in the ware-\nhouse.\n\u2022minlevel (item, level ), which notes the minimum amount of the item to be main-\ntained.\n\u2022reorder (item, amount ), which notes the amount of the item to be ordered when its\nlevel falls below the minimum.\n\u2022orders (item, amount ), which notes the amount of the item to be ordered.\nTo place a reorder when inventory falls below a speci\ufb01ed minimum, we can use the\nt r i g g e rs h o w ni nF i g u r e5 . 1 2 .N o t et h a tw eh a v eb e e nc a r e f u lt op l a c ea no r d e ro n l y\nwhen the amount falls from above the minimum level to below the minimum level. If\nwe check only that the new value after an update is below the minimum level, we may\nplace an order erroneously when the item has already been reordered.\nSQL-based database systems use triggers widely, although before SQL:1999 they\nw e r en o tp a r to ft h e SQL standard. Unfortunately, as a result, each database system\nimplemented its own syntax for triggers, leading to incompatibilities. The SQL:1999\nsyntax for triggers that we use here is similar, but not identical, to the syntax in the\nIBM DB2 and Oracle database systems. See Note 5.3 on page 212.\n5.3.3 When Not to Use Triggers\nThere are many good uses for triggers, such as those we have just seen in Section 5.3.2,\nbut some uses are best handled by alternative techniques. For example, we could imple-\nment the on delete cascade feature of a foreign-key constraint by using a trigger instead\nof using the cascade feature. Not only would this be more work to implement, but\nalso it would be much harder for a database user to understand the set of constraints\nimplemented in the database.\n", "239": "5.3 Triggers 211\ncreate trigger reorder after update of level oninventory\nreferencing old row as orow,new row as nrow\nfor each row\nwhen nrow.level <=(select level\nfrom minlevel\nwhere minlevel.item =orow.item )\nand orow.level >(select level\nfrom minlevel\nwhere minlevel.item =orow.item )\nbegin atomic\ninsert into orders\n(select item, amount\nfrom reorder\nwhere reorder.item =orow.item );\nend;\nFigure 5.12 Example of trigger for reordering an item.\nAs another example, triggers can be used to maintain materialized views. For in-\nstance, if we wished to support very fast access to the total number of students regis-\ntered for each course section, we could do this by creating a relation\nsection\n registration (course\n id,sec\nid,semester ,year,total\n students )\nde\ufb01ned by the query\nselect course\n id,sec\nid,semester ,year,count (ID)astotal\n students\nfrom takes\ngroup by course\n id,sec\nid,semester ,year;\nThe value of total\n students for each course must be maintained up-to-date by triggers\non insert, delete, or update of the takes relation. Such maintenance may require inser-\ntion, update or deletion of tuples from section\n registration , and triggers must be written\naccordingly.\nHowever, many database systems now support materialized views, which are auto-\nmatically maintained by the database system (see Section 4.2.3). As a result, there is\nno need to write trigger code for maintaining such materialized views.\nTriggers have been used for maintaining co pies, or replicas, of databases. A collec-\ntion of triggers on insert, delete, or update can be created on each relation to record\nthe changes in relations called change ordelta relations. A separate process copies over\nthe changes to the replica of the database. Modern database systems, however, provide\n", "240": "212 Chapter 5 Advanced SQL\nNote 5.3 NONSTANDARD TRIGGER SYNTAX\nAlthough the trigger syntax we describe here is part of the SQL standard, and\nis supported by IBM DB2 , most other database systems have nonstandard syntax\nfor specifying triggers and may not implement all features in the SQL standard.\nWe outline a few of the di\ufb00erences below; see the respective system manuals for\nfurther details.\nFor example, in the Oracle syntax, unlike the SQLstandard syntax, the keyword\nrowdoes not appear in the referencing statement. The keyword atomic does not\nappear after begin . The reference to nrow in the select statement nested in the\nupdate statement must begin with a colon (:) to inform the system that the variable\nnrow is de\ufb01ned externally from the SQL statement. Further, subqueries are not\nallowed in the when and ifclauses. It is possible to work around this problem by\nmoving complex predicates from the when clause into a separate query that saves\nthe result into a local variable, and then reference that variable in an ifclause, and\nthe body of the trigger then moves into the corresponding then c l a u s e .F u r t h e r ,i n\nOracle, triggers are not allowed to execute a transaction rollback directly; however,\nthey can instead use a function called raise\n application\n error to not only roll\nback the transaction but also return an error message to the user/application that\nperformed the update.\nAs another example, in Microsoft SQL S erver the keyword onis used instead\nofafter.T h e referencing clause is omitted, and old and new rows are referenced by\nthe tuple variables deleted andinserted .F u r t h e r ,t h e for each row clause is omitted,\nandwhen is replaced by if.T h e before speci\ufb01cation is not supported, but an instead\nofspeci\ufb01cation is supported.\nInPostgre SQL, triggers do not have a body, but instead invoke a procedure for\neach row, which can access variables newandoldcontaining the old and new values\nof the row. Instead of performing a rollback, the trigger can raise an exception with\nan associated error message.\nbuilt-in facilities for database replication, making triggers unnecessary for replication\nin most cases. Replicated databases are discussed in detail in Chapter 23.\nAnother problem with triggers lies in unintended execution of the triggered action\nwhen data are loaded from a backup copy,5or when database updates at a site are\nreplicated on a backup site. In such cases, the triggered action has already been exe-\ncuted, and typically it should not be executed again. When loading data, triggers can be\ndisabled explicitly. For backup replica systems that may have to take over from the pri-\nmary system, triggers would have to be disabled initially and enabled when the backup\n5We discuss database backup and recovery from failures in detail in Chapter 19.\n", "241": "5.4 Recursive Queries 213\ncourse\n id\n prereq\n id\nBIO-301\n BIO-101\nBIO-399\n BIO-101\nCS-190\n CS-101\nCS-315\n CS-190\nCS-319\n CS-101\nCS-319\n CS-315\nCS-347\n CS-319\nFigure 5.13 An instance of the prereq relation.\nsite takes over processing from the primary system. As an alternative, some database\nsystems allow triggers to be speci\ufb01ed as not for replication , which ensures that they are\nnot executed on the backup site during database replication. Other database systems\nprovide a system variable that denotes that the database is a replica on which database\nactions are being replayed; the trigger body should check this variable and exit if it is\ntrue. Both solutions remove the need for explicit disabling and enabling of triggers.\nTriggers should be written with great care, since a trigger error detected at runtime\ncauses the failure of the action statement that set o\ufb00 the trigger. Furthermore, the\naction of one trigger can set o\ufb00 another trigger. In the worst case, this could even lead\nto an in\ufb01nite chain of triggering. For example, suppose an insert trigger on a relation\nhas an action that causes another (new) insert on the same relation. The insert action\nthen triggers yet another insert action, and so on ad in\ufb01nitum. Some database systems\nlimit the length of such chains of triggers (for example, to 16 or 32) and consider longer\nchains of triggering an error. Other systems \ufb02ag as an error any trigger that attempts\nto reference the relation whose modi\ufb01cati on caused the trigger to execute in the \ufb01rst\nplace.\nTriggers can serve a very useful purpose, but they are best avoided when alterna-\ntives exist. Many trigger applications can be substituted by appropriate use of stored\nprocedures, which we di scussed in Section 5.2.\n5.4 Recursive Queries\nConsider the instance of the relation prereq shown in Figure 5.13 containing informa-\ntion about the various courses o\ufb00ered at the university and the prerequisite for each\ncourse.6\nSuppose now that we want to \ufb01nd out which courses are a prerequisite whether di-\nrectly or indirectly, for a speci\ufb01c course\u2014say, CS-347. That is, we wish to \ufb01nd a course\n6This instance of prereq di\ufb00ers from that used earlier for reasons that will become apparent as we use it to explain\nrecursive queries.\n", "242": "214 Chapter 5 Advanced SQL\nthat is a direct prerequisite for CS-347, or is a prerequisite for a course that is a prereq-\nuisite for CS-347, and so on.\nThus, since CS-319 is a prerequisite for CS-347 and CS-315 and CS-101 are pre-\nrequisites for CS-319, CS-315 and CS-101 are also prerequisites (indirectly) for CS-347.\nThen, since CS-190 is a prerequisite for CS-315, CS-190 is another indirect prerequisite\nfor CS-347. Continuing, we see that CS-101 is a prerequisite for CS-190, but note that\nCS-101 was already added to the list of prerequisites for CS-347. In a real university,\nrather than our example, we would not expect such a complex prerequisite structure,\nbut this example serves to show some of the situations that might possibly arise.\nThe transitive closure of the relation prereq is a relation that contains all pairs ( cid,\npre)s u c ht h a t preis a direct or indirect prerequisite of cid.T h e r ea r en u m e r o u sa p -\nplications that require computation of similar transitive closures on hierarchies .F o r\ninstance, organizations typically consist of several levels of organizational units. Ma-\nchines consist of parts that in turn have subparts, and so on; for example, a bicycle\nmay have subparts such as wheels and pedals, which in turn have subparts such as\ntires, rims, and spokes. Transitive closure can be used on such hierarchies to \ufb01nd, for\nexample, all parts in a bicycle.\n5.4.1 Transitive Closure Using Iteration\nOne way to write the preceding query is to use iteration: First \ufb01nd those courses that\nare a direct prerequisite of CS-347, then tho se courses that are a prerequisite of all the\ncourses under the \ufb01rst set, and so on. This iterative process continues until we reach an\niteration where no courses are added. Figure 5.14 shows a function \ufb01ndAllPrereqs (cid)\nto carry out this task; the function takes the course\n idof the course as a parameter ( cid),\ncomputes the set of all direct and indirect prerequisites of that course, and returns the\nset.\nThe procedure uses three temporary tables:\n\u2022c\nprereq : stores the set of tuples to be returned.\n\u2022new\n c\nprereq :s t o r e st h ec o u r s e sf o u n di nt h ep r e v i o u si t e r a t i o n .\n\u2022temp: used as temporary storage while sets of courses are manipulated.\nNote that SQL allows the creation of temporary tables using the command create tem-\nporary table ; such tables are available only within the transaction executing the query\nand are dropped when the transaction \ufb01nishes. Moreover, if two instances of \ufb01ndAll-\nPrereqs run concurrently, each gets its own copy of the temporary tables; if they shared\na copy, their result could be incorrect.\nThe procedure inserts all direct prerequisites of course cidinto new\n c\nprereq before\ntherepeat loop. The repeat loop \ufb01rst adds all courses in new\n c\nprereq toc\nprereq .N e x t ,\nit computes prerequisites of all those courses in new\n c\nprereq , except those that have\nalready been found to be prerequisites of cid, and stores them in the temporary table\n", "243": "5.4 Recursive Queries 215\ncreate function \ufb01ndAllPrereqs (cidvarchar(8) )\n\u2013 \u2013 Finds all courses that are prerequisite (directly or indirectly) for cid\nreturns table (course\n idvarchar (8))\n\u2013\u2013T h er e l a t i o n prereq (course\n id,prereq\n id) speci\ufb01es which course is\n\u2013 \u2013 directly a prerequisite for another course.\nbegin\ncreate temporary table c\nprereq (course\n idvarchar(8) );\n\u2013\u2013t a b l e c\nprereq stores the set of courses to be returned\ncreate temporary table new\n c\nprereq (course\n idvarchar(8) );\n\u2013\u2013t a b l e new\n c\nprereq contains courses found in the previous iteration\ncreate temporary table temp (course\n idvarchar (8));\n\u2013\u2013t a b l e temp is used to store intermediate results\ninsert into new\n c\nprereq\nselect prereq\n id\nfrom prereq\nwhere course\n id=cid;\nrepeat\ninsert into c\nprereq\nselect course\n id\nfrom new\n c\nprereq ;\ninsert into temp\n(select prereq .prereq\n id\nfrom new\n c\nprereq, prereq\nwhere new\n c\nprereq .course\n id=prereq .course\n id\n)\nexcept (\nselect course\n id\nfrom c\nprereq\n);\ndelete from new\n c\nprereq ;\ninsert into new\n c\nprereq\nselect *\nfrom temp;\ndelete from temp;\nuntil not exists (select * from new\n c\nprereq )\nend repeat ;\nreturn table c\nprereq ;\nend\nFigure 5.14 Finding all prerequisites of a course.\n", "244": "216 Chapter 5 Advanced SQL\nIteration Number\n Tuples in c1\n0\n1\n (CS-319)\n2\n (CS-319), (CS-315), (CS-101)\n3\n (CS-319), (CS-315), (CS-101), (CS-190)\n4\n (CS-319), (CS-315), (CS-101), (CS-190)\n5\n done\nFigure 5.15 Prerequisites of CS-347 in iterations of function \ufb01ndAllPrereqs.\ntemp. Finally, it replaces the contents of new\n c\nprereq with the contents of temp.T h e\nrepeat loop terminates when it \ufb01nds no new (indirect) prerequisites.\nFigure 5.15 shows the prerequisites that a re found in each iteration when the proce-\ndure is called for CS-347. While c\nprereq could have been updated in one SQLstatement,\nwe need \ufb01rst to construct new\n c\nprereq so we can tell when nothing is being added in\nthe (\ufb01nal) iteration.\nThe use of the except clause in the function ensures that the function works even\nin the (abnormal) case where there is a cycle of prerequisites. For example, if ais a\nprerequisite for b,bis a prerequisite for c,a n d cis a prerequisite for a, there is a cycle.\nWhile cycles may be unrealistic in course prerequisites, cycles are possible in other\napplications. For instance, suppose we have a relation \ufb02ights (to, from ) that says which\ncities can be reached from which other cities by a direct \ufb02ight. We can write code\nsimilar to that in the \ufb01ndAllPrereqs function, to \ufb01nd all cities that are reachable by a\nsequence of one or more \ufb02ights from a given city. All we have to do is to replace prereq\nwith \ufb02ight and replace attribute names correspondingly. In this situation, there can be\ncycles of reachability, but the function would work correctly since it would eliminate\ncities that have already been seen.\n5.4.2 Recursion in SQL\nIt is rather inconvenient to specify transitiv e closure using iteration. There is an alter-\nnative approach, using recursive view de\ufb01nitions, that is easier to use.\nWe can use recursion to de\ufb01ne the set of courses that are prerequisites of a par-\nticular course, say CS-347, as follows. The co urses that are prere quisites (directly or\nindirectly) of CS-347 are:\n\u2022Courses that are prerequisites for CS-347.\n\u2022Courses that are prerequisites for those courses that are prerequisites (directly or\nindirectly) for CS-347.\nNote that case 2 is recursive, since it de\ufb01nes the set of courses that are prerequisites of\nC S - 3 4 7i nt e r m so ft h es e to fc o u r s e st h a ta r ep r e r e q u i s i t e so fC S - 3 4 7 .O t h e re x a m p l e s\n", "245": "5.4 Recursive Queries 217\nwith recursive rec\nprereq (course\n id,prereq\n id)as(\nselect course\n id,prereq\n id\nfrom prereq\nunion\nselect rec\nprereq .course\n id,prereq .prereq\n id\nfrom rec\nprereq ,prereq\nwhere rec\nprereq .prereq\n id=prereq .course\n id\n)\nselect\u2217\nfrom rec\nprereq ;\nFigure 5.16 Recursive query in SQL.\nof transitive closure, such as \ufb01nding all subparts (direct or indirect) of a given part can\nalso be de\ufb01ned in a similar manner, recursively.\nThe SQL standard supports a limited form of recursion, using the with recursive\nclause, where a view (or temporary view) is expressed in terms of itself. Recursive\nqueries can be used, for example, to express transitive closure concisely. Recall that\nthewith clause is used to de\ufb01ne a temporary view whose de\ufb01nition is available only\nto the query in which it is de\ufb01ned. The additional keyword recursive speci\ufb01es that the\nview is recursive.7\nFor example, we can \ufb01nd every pair ( cid,pre)s u c ht h a t preis directly or indirectly\na prerequisite for course cid, using the recursive SQL view shown in Figure 5.16.\nAny recursive view must be de\ufb01ned as the union8of two subqueries: a base query\nthat is nonrecursive and a recursive query that uses the recursive view. In the example\nin Figure 5.16, the base query is the select on prereq while the recursive query computes\nthe join of prereq and rec\nprereq .\nThe meaning of a recursive view is best understood as follows: First compute the\nbase query and add all the resultant tuples to the recursively de\ufb01ned view relation\nrec\nprereq (which is initially empty). Next compute the recursive query using the current\ncontents of the view relation, and add all the resulting tuples back to the view relation.\nKeep repeating the above step until no new tuples are added to the view relation. The\nresultant view relation instance is called a \ufb01xed point of the recursive view de\ufb01nition.\n(The term \u201c\ufb01xed\u201d refers to the fact that there is no further change.) The view relation\nis thus de\ufb01ned to contain exactly the tuples in the \ufb01xed-point instance.\nApplying this logic to our example, we \ufb01rst \ufb01nd all direct prerequisites of each\ncourse by executing the base query. The rec ursive query adds one more level of courses\n7Some systems treat the recursive keyword as optional; others disallow it.\n8Some systems, notably Oracle, require use of union all .\n", "246": "218 Chapter 5 Advanced SQL\nin each iteration, until the maximum depth of the course-prereq relationship is reached.\nAt this point no new tuples are added to the view, and a \ufb01xed point is reached.\nTo \ufb01nd the prerequisites of a speci\ufb01c course, such as CS-347, we can modify the\nouter level query by adding a where clause \u201c where rec\nprereq.course\n id= \u2018CS-347\u2018\u201d. One\nway to evaluate the query with the selection is to compute the full contents of rec\nprereq\nusing the iterative technique, and then select from this result only those tuples whose\ncourse\n idis CS-347. However, this would result in computing (course, prerequisite)\npairs for all courses, all of which are irrelevant except for those for the course CS-347.\nIn fact the database system is not required to use this iterative technique to compute\nthe full result of the recursive query and then perform the selection. It may get the\nsame result using other techniques that may be more e\ufb03cient, such as that used in the\nfunction \ufb01ndAllPrereqs which we saw earlier. See the bibliographic notes for references\nto more information on this topic.\nThere are some restrictions on the recursi ve query in a recursive view; speci\ufb01cally,\nthe query must be monotonic , that is, its result on a view relation instance V1must be a\nsuperset of its result on a view relation instance V2ifV1is a superset of V2.I n t u i t i v e l y ,\nif more tuples are added to the view relation, the recursive query must return at least\nthe same set of tuples as before, and possibly return additional tuples.\nIn particular, recursive queries may not use any of the following constructs, since\nthey would make the query nonmonotonic:\n\u2022Aggregation on the recursive view.\n\u2022not exists on a subquery that uses the recursive view.\n\u2022Set di\ufb00erence ( except ) whose right-hand side uses the recursive view.\nFor instance, if the recursive query was of the form r\u2212v,w h e r e vis the recursive view,\nif we add a tuple to v, the result of the query can become smaller; the query is therefore\nnot monotonic.\nThe meaning of recursive views can be de\ufb01ned by the iterative procedure as long as\nthe recursive query is monotonic; if the recursive query is nonmonotonic, the meaning\nof the view is hard to de\ufb01ne. SQL therefore requires the queries to be monotonic. Recur-\nsive queries are discussed in more detail in the context of the Datalog query language,\nin Section 27.4.6.\nSQL also allows creation of recursively de\ufb01ned permanent views by using create re-\ncursive view in place of with recursive . Some implementations support recursive queries\nusing a di\ufb00erent syntax. This includes the Oracle start with /connect by prior syntax\nfor what it calls hierarchical queries.9See the respective system manuals for further\ndetails.\n9Staring with Oracle 12.c, the standard syntax is accepted in addition to the legacy hierarchical syntax, with the recursive\nkeyword omitted and with the requirement in our example that union all be used instead of union .\n", "247": "5.5 Advanced Aggregation Features 219\n5.5 Advanced Aggregation Features\nThe aggregation support in SQL is quite powerful and handles most common tasks\nwith ease. However, there are some tasks that are hard to implement e\ufb03ciently with\nthe basic aggregation features. In this section, we study features in SQL to handle some\nsuch tasks.\n5.5.1 Ranking\nFinding the position of a value within a set is a common operation. For instance, we\nmay wish to assign students a rank in class based on their grade-point average ( GPA),\nwith the rank 1 going to the student with the highest GPA, the rank 2 to the student\nwith the next highest GPA, and so on. A related type of query is to \ufb01nd the percentile\nin which a value in a (multi)set belongs, for example, the bottom third, middle third,\nor top third. While such queries can be expressed using the SQL constructs we have\nseen so far, they are di\ufb03cult to express and ine\ufb03cient to evaluate. Programmers may\nresort to writing the query partly in SQL and partly in a programming language. We\nstudy SQL support for direct expression of these types of queries here.\nIn our university example, the takes relation shows the grade each student earned\nin each course taken. To illustrate ranking, let us assume we have a view student\n grades\n(ID, GPA ) giving the grade-point average of each student.10\nRanking is done with an order by speci\ufb01cation. The following query gives the rank\nof each student:\nselect ID,rank()over(order by (GPA)desc)ass\nrank\nfrom student\n grades ;\nNote that the order of tuples in the output is not de\ufb01ned, so they may not be sorted by\nrank. An extra order by clause is needed to get them in sorted order, as follows:\nselect ID,rank ()over(order by (GPA)desc)ass\nrank\nfrom student\n grades\norder by s\nrank;\nA basic issue with ranking is how to deal with the case of multiple tuples that are\nthe same on the ordering attribute(s). In our example, this means deciding what to do\nif there are two students with the same GPA.T h e rank function gives the same rank to\nall tuples that are equal on the order by attributes. For instance, if the highest GPA is\nshared by two students, both would get rank 1. The next rank given would be 3, not 2,\nso if three students get the next highest GPA, they would all get rank 3, and the next\n10The SQL statement to create the view student\n grades is somewhat complex since we must convert the letter grades\nin the takes relation to numbers and weight the grades for each course by the number of credits for that course. The\nde\ufb01nition of this view is the goal of Exercise 4.6.\n", "248": "220 Chapter 5 Advanced SQL\nstudent(s) would get rank 6, and so on. There is also a dense\n rank function that does\nnot create gaps in the ordering. In the preceding example, the tuples with the second\nhighest value all get rank 2, and tuples with the third highest value get rank 3, and so\non.\nIf there are null values among the values being ranked, they are treated as the\nhighest values. That makes sense in some situations, although for our example, it would\nresult in students with no courses being shown as having the highest GPAs. Thus, we\nsee that care needs to be taken in writing ranking queries in cases where null values\nmay appear. SQL permits the user to specify where they should occur by using nulls\n\ufb01rstornulls last ,f o ri n s t a n c e :\nselect ID,rank ()over(order by GPA desc nulls last )ass\nrank\nfrom student\n grades ;\nIt is possible to express the preceding query with the basic SQL aggregation func-\ntions, using the following query:\nselect ID,( 1+( select count (*)\nfrom student\n grades B\nwhere B.GPA>A.GPA))ass\nrank\nfrom student\n grades A\norder by s\nrank;\nIt should be clear that the rank of a student is merely 1 plus the number of students\nwith a higher GPA, which is exactly what the query speci\ufb01es.11However, this compu-\ntation of each student\u2019s rank takes time linear in the size of the relation, leading to an\noverall time quadratic in the size of the relation. On large relations, the above query\ncould take a very long time to execute. In contrast, the system\u2019s implementation of the\nrank clause can sort the relation and compute the rank in much less time.\nRanking can be done within partitions of the data. For instance, suppose we wish\nto rank students by department rather than across the entire university. Assume that a\nview is de\ufb01ned like student\n grades but including the department name: dept\n grades (ID,\ndept\n name, GPA ). The following query then gives the rank of students within each sec-\ntion:\nselect ID,dept\n name ,\nrank ()over(partition by dept\n name order by GPA desc)asdept\n rank\nfrom dept\n grades\norder by dept\n name ,dept\n rank;\n11There is a slight technical di\ufb00erence if a student has not taken any courses and therefore has a nullGPA. Due to how\ncomparisons of null values work in SQL, a student with a null GPA does not contribute to other students\u2019 count values.\n", "249": "5.5 Advanced Aggregation Features 221\nThe outer order by clause orders the result tuples by department name, and within each\ndepartment by the rank.\nMultiple rank expressions can be used within a single select statement; thus, we\ncan obtain the overall rank and the rank within the department by using two rank\nexpressions in the same select clause. When ranking (possibly with partitioning) occurs\nalong with a group by clause, the group by clause is applied \ufb01rst, and partitioning and\nranking are done on the results of the group by . Thus, aggregate values can then be\nused for ranking.\nIt is often the case, especially for large results, that we may be interested only in\nthe top-ranking tuples of the result rather than the entire list. For rank queries, this can\nbe done by nesting the ranking query within a containing query whose where clause\nchooses only those tuples whose rank is lower than some speci\ufb01ed value. For example,\nto \ufb01nd the top 5 ranking students based on GPA we could extend our earlier example\nby writing:\nselect *\nfrom (select ID,rank()over(order by (GPA)desc)ass\nrank\nfrom student\n grades )\nwhere s\nrank<=5;\nThis query does not necessarily give 5 students, since there could be ties. For example,\nif 2 students tie for \ufb01fth, the result would contain a total of 6 tuples. Note that the\nbottom nis simply the same as the top nwith a reverse sorting order.\nSeveral database systems provide nonstandard SQL syntax to specify directly that\nonly the top nresults are required. In our example, this would allow us to \ufb01nd the\ntop 5 students without the need to use the rank function. However, those constructs\nresult in exactly the number of tuples speci\ufb01ed (5 in our example), and so ties for the\n\ufb01nal position are broken arbitrarily. The exact syntax for these \u201ctop n\u201d queries varies\nwidely among systems; see Note 5.4 on page 222. Note that the top nconstructs do\nnot support partitioning; so we cannot get the top nwithin each partition without\nperforming ranking.\nSeveral other functions can be used in place of rank.F o ri n s t a n c e , percent\n rank of\na tuple gives the rank of the tuple as a fraction. If there are ntuples in the partition12\nand the rank of the tuple is r, then its percent rank is de\ufb01ned as ( r\u22121)\u2215(n\u22121) (and\nasnullif there is only one tuple in the partition). The function cume\n dist,s h o r tf o r\ncumulative distribution, for a tuple is de\ufb01ned as p\u2215nwhere pis the number of tuples\nin the partition with ordering values preceding or equal to the ordering value of the\ntuple and nis the number of tuples in the partition. The function row\n number sorts\nthe rows and gives each row a unique number corresponding to its position in the sort\norder; di\ufb00erent rows with the same orderin g value would get di\ufb00erent row numbers, in\na nondeterministic fashion.\n12The entire set is treated as a single partition if no explicit partition is used.\n", "250": "222 Chapter 5 Advanced SQL\nNote 5.4 TOP-N QUERIES\nOften, only the \ufb01rst few tuples of a query result are required. This may occur in\na ranking query where only top-ranked results are of interest. Another case where\nthis may occur is in a query with an order by from which only the top values are\nof interest. Restricting results to the top-ranked results can be done using the rank\nfunction as we saw earlier, but that syntax is rather cumbersome. Many databases\nsupport a simpler syntax for such restriction, but the syntax varies widely among\nthe leading database systems. We provide a few examples here.\nSome systems (including MySQL andPostgre SQL) allow a clause limit nto be\na d d e da tt h ee n do fa n SQL query to specify that only the \ufb01rst ntuples should be\noutput. This clause can be used in conjunction with an order by clause to fetch the\ntopntuples, as illustrated by the following query, which retrieves the IDand GPA\nof the top 10 students in order of GPA:\nselect ID,GPA\nfrom student\n grades\norder by GPA desc\nlimit 10;\nInIBM DB2 and the most recent versions of Oracle, the equivalent of the limit\nclause is fetch \ufb01rst 10rows only .M i c r o s o f t SQL S erver places its version of this\nfeature in the select clause rather than adding a separate limit clause. The select\nclause is written as: select top 10ID,GPA.\nOracle (both current and older versions) o\ufb00ers the concept of a row number to\nprovide this feature. A special, hidden attribute rownum n u m b e r st u p l e so far e s u l t\nrelation in order of retrieval. This attribute can then be used in a where clause\nwithin a containing query. However, the use of this feature is a bit tricky, since the\nrownum is decided before rows are sorted by an order by clause. To use it properly,\na nested query should be used as follows:\nselect *\nfrom (select ID,GPA\nfrom student\n grades\norder by GPA desc)\nwhere rownum <=10;\nThe nested query ensures that the predicate on rownum is applied only after the\norder by is applied.\nSome database systems have features allowing tuple limits to be exceeded in\ncase of ties. See your system\u2019s documentation for details.\n", "251": "5.5 Advanced Aggregation Features 223\nFinally, for a given constant n, the ranking function ntile(n) takes the tuples in each\npartition in the speci\ufb01ed order and divides them into nbuckets with equal numbers of\ntuples.13For each tuple, ntile(n) then gives the number of the bucket in which it is\nplaced, with bucket numbers starting with 1. This function is particularly useful for\nconstructing histograms based on percentiles. We can show the quartile into which\neach student falls based on GPA by the following query:\nselect ID,ntile(4)over(order by (GPA desc))asquartile\nfrom student\n grades ;\n5.5.2 Windowing\nWindow queries compute an aggregate function over ranges of tuples. This is useful,\nfor example, to compute an aggregate of a \ufb01xed range of time; the time range is called a\nwindow . Windows may overlap, in which case a tuple may contribute to more than one\nwindow. This is unlike the partitions we saw earlier, where a tuple could contribute to\nonly one partition.\nAn example of the use of windowing is trend analysis. Consider our earlier sales\nexample. Sales may \ufb02uctuate widely from day to day based on factors like weather (e.g.,\na snowstorm, \ufb02ood, hurricane, or earthquake might reduce sales for a period of time).\nHowever, over a su\ufb03ciently long period of time, \ufb02uctuations might be less (continuing\nthe example, sales may \u201cmake up\u201d for weather-related downturns). Stock-market trend\nanalysis is another example of the use of the windowing concept. Various \u201cmoving\naverages\u201d are found on business and investment web sites.\nIt is relatively easy to write an SQL query using those features we have already\nstudied to compute an aggregate over one window, for example, sales over a \ufb01xed 3-\nday period. However, if we want to do this for every 3-day period, the query becomes\ncumbersome.\nSQL provides a windowing feature to support such queries. Suppose we are given a\nview tot\ncredits (year, num\n credits ) giving the total number of credits taken by students\nin each year.14Note that this relation can contain at most one tuple for each year.\nConsider the following query:\nselect year,avg(num\n credits )\nover(order by year rows 3preceding )\nasavg\ntotal\n credits\nfrom tot\ncredits ;\n13If the total number of tuples in a partition is not divisible by n, then the number of tuples in each bucket can di\ufb00er by at\nmost 1. Tuples with the same value for the ordering attribute may be assigned to di\ufb00erent buckets, nondeterministically,\nin order to make the number of tuples in each bucket equal.\n14We leave the de\ufb01nition of this view in terms of our university example as an exercise.\n", "252": "224 Chapter 5 Advanced SQL\nT h i sq u e r yc o m p u t e sa v e r a g e so v e rt h et h r e e preceding tuples in the speci\ufb01ed sort order.\nThus, for 2019, if tuples for years 2018 and 2017 are present in the relation tot\ncredits ,\nsince each year is represented by only one tuple, the result of the window de\ufb01nition is\nthe average of the values for years 2017, 2018, and 2019. The averages each year would\nbe computed in a similar manner. For the earliest year in the relation tot\ncredits ,t h e\naverage would be over only that year itself, while for the next year, the average would\nbe over 2 years. Note that this example makes sense only because each year appears\nonly once in tot\nweight . Were this not the case, then there would be several possible\no r d e r i n g so ft u p l e ss i n c et u p l e sf o rt h es a m ey e a rc o u l db ei na n yo r d e r .W es h a l ls e e\nshortly a windowing query that uses a range of values instead of a speci\ufb01c number of\ntuples.\nSuppose that instead of going back a \ufb01xed number of tuples, we want the window\nto consist of all prior years. That means the number of prior years considered is not\n\ufb01xed. To get the average total credits over all prior years, we write:\nselect year,avg(num\n credits )\nover(order by year rows unbounded preceding )\nasavg\ntotal\n credits\nfrom tot\ncredits ;\nIt is possible to use the keyword following in place of preceding . If we did this in\nour example, the year value speci\ufb01es the beginning of the window instead of the end.\nSimilarly, we can specify a window beginning before the current tuple and ending after\nit:\nselect year,avg(num\n credits )\nover(order by year rows between 3preceding and 2following )\nasavg\ntotal\n credits\nfrom tot\ncredits ;\nIn our example, all tuples pertain to the entire university. Suppose instead we have\ncredit data for each department in a view tot\ncredits\n dept(dept\n name, year, num\n credits )\ngiving the total number of credits students took with the particular department in the\nspeci\ufb01ed year. (Again, we leave writing this view de\ufb01nition as an exercise.) We can\nwrite windowing queries that treat each department separately by partitioning by dept\nname :\nselect dept\n name, year ,avg(num\n credits )\nover(partition by dept\n name\norder by year rows between 3preceding and current row )\nasavg\ntotal\n credits\nfrom tot\ncredits\n dept;\n", "253": "5.5 Advanced Aggregation Features 225\nitem\n name\n color\n clothes\n size\n quantity\ndress\n dark\n small\n 2\ndress\n dark\n medium\n 6\ndress\n dark\n large\n 12\ndress\n pastel\n small\n 4\ndress\n pastel\n medium\n 3\ndress\n pastel\n large\n 3\ndress\n white\n small\n 2\ndress\n white\n medium\n 3\ndress\n white\n large\n 0\npants\n dark\n small\n 14\npants\n dark\n medium\n 6\npants\n dark\n large\n 0\npants\n pastel\n small\n 1\npants\n pastel\n medium\n 0\npants\n pastel\n large\n 1\npants\n white\n small\n 3\npants\n white\n medium\n 0\npants\n white\n large\n 2\nshirt\n dark\n small\n 2\nshirt\n dark\n medium\n 6\nshirt\n dark\n large\n 6\nshirt\n pastel\n small\n 4\nshirt\n pastel\n medium\n 1\nshirt\n pastel\n large\n 2\nshirt\n white\n small\n 17\nshirt\n white\n medium\n 1\nshirt\n white\n large\n 10\nskirt\n dark\n small\n 2\nskirt\n dark\n medium\n 5\nskirt\n dark\n large\n 1\nskirt\n pastel\n small\n 11\nskirt\n pastel\n medium\n 9\nskirt\n pastel\n large\n 15\nskirt\n white\n small\n 2\nskirt\n white\n medium\n 5\nskirt\n white\n large\n 3\nFigure 5.17 An example of sales relation.\n", "254": "226 Chapter 5 Advanced SQL\nitem\n name\n clothes\n size\n dark\n pastel\n white\ndress\n small\n 2\n 4\n 2\ndress\n medium\n 6\n 3\n 3\ndress\n large\n 12\n 3\n 0\npants\n small\n 14\n 1\n 3\npants\n medium\n 6\n 0\n 0\npants\n large\n 0\n 1\n 2\nshirt\n small\n 2\n 4\n 17\nshirt\n medium\n 6\n 1\n 1\nshirt\n large\n 6\n 2\n 10\nskirt\n small\n 2\n 11\n 2\nskirt\n medium\n 5\n 9\n 5\nskirt\n large\n 1\n 15\n 3\nFigure 5.18 Result of SQL pivot operation on the sales relation of Figure 5.17.\nT h eu s eo ft h ek e y w o r d range in place of rowallows the windowing query to cover\nall tuples with a particular value rather than covering a speci\ufb01c number of tuples. Thus\nfor example, rows current row refers to exactly one tuple, while range current row refers\nto all tuples whose value for the sortattribute is the same as that of the current tuple.\nThe range keyword is not implemented fully in every system.15\n5.5.3 Pivoting\nConsider an application where a shop wants to \ufb01nd out what kinds of clothes are pop-\nular. Let us suppose that clothes are characterized by their item\n name, color, and size,\nand that we have a relation sales with the schema.\nsales (item\n name ,color ,clothes\n size,quantity )\nSuppose that item\n name can take on the values (skirt, dress, shirt, pants), color can\ntake on the values (dark, pastel, white), clothes\n sizecan take on values (small, medium,\nlarge), and quantity is an integer value representing the total number of items sold of a\ngiven ( item\n name ,color ,clothes\n size) combination. An instance of the sales relation is\nshown in Figure 5.17.\nFigure 5.18 shows an alternative way to view the data that is present in Figure 5.17;\nthe values \u201cdark\u201d, \u201cpastel\u201d, and \u201cwhite\u201d of attribute color have become attribute names\nin Figure 5.18. The table in Figure 5.18 is an example of a cross-tabulation (orcross-tab ,\nfor short), also referred to as a pivot-table .\nThe values of the new attributes dark, pastel and white in our example are de\ufb01ned as\nfollows. For a particular combination of item\n name ,clothes\n size(e.g., (\u201cdress\u201d, \u201cdark\u201d))\n15Some systems, such as Postgre SQL, allow range only with unbounded .\n", "255": "5.5 Advanced Aggregation Features 227\nif there is a single tuple with color value \u201cdark\u201d, the quantity value of that attribute ap-\npears as the value for the attribute dark. If there are multiple such tuples, the values\nare aggregated using the sumaggregate in our example; in general other aggregate func-\ntions could be used instead. Values for the other two attributes, pastel and white ,a r e\nsimilarly de\ufb01ned.\nIn general, a cross-tab is a table derived from a relation (say, R), where values for\nsome attribute of relation R(say, A) become attribute names in the result; the attribute\nAis the pivot attribute. Cross-tabs are widely used for data analysis, and are discussed\nin more detail in Section 11.3.\nSeveral SQL implementations, such as Microsoft SQL S erver, and Oracle, support\napivot clause that allows creation of cross-tabs. Given the sales relation from Figure\n5.17, the query:\nselect *\nfrom sales\npivot (\nsum(quantity )\nforcolor in(\u2019dark\u2019, \u2019pastel\u2019, \u2019white\u2019)\n)\nreturns the result shown in Figure 5.18.\nNote that the forclause within the pivot clause speci\ufb01es (i) a pivot attribute ( color ,\nin the above query), (ii) the values of that attribute that should appear as attribute\nnames in the pivot result (dark, pastel and white, in the above query), and (iii) the\naggregate function that should be used to compute the value of the new attributes (ag-\ngregate function sum, on the attribute quantity ,i nt h ea b o v eq u e r y ) .\nThe attribute color and quantity do not appear in the result, but all other attributes\nare retained. In case more than one tuple contributes values to a given cell, the aggregate\noperation within the pivot clause speci\ufb01es how the values should be combined. In the\nabove example, the quantity values are aggregated using the sumfunction.\nA query using pivot can be written using basic SQL constructs, without using the\npivot construct, but the construct simpli\ufb01es the task of writing such queries.\n5.5.4 Rollup and Cube\nSQL supports generalizations of the group by construct using the rollup and cube op-\nerations, which allow multiple group by queries to be run in a single query, with the\nresult returned as a single relation.\nConsider again our retail shop example and the relation:\nsales (item\n name ,color ,clothes\n size,quantity )\nWe can \ufb01nd the number of items sold in each item name by writing a simple group by\nquery:\n", "256": "228 Chapter 5 Advanced SQL\nselect item\n name ,sum(quantity )asquantity\nfrom sales\ngroup by item\n name ;\nSimilarly, we can \ufb01nd the number of items sold in each color, and each size. We\ncan further \ufb01nd a breakdown of sales by item-name and color by writing:\nselect item\n name ,color ,sum(quantity )asquantity\nfrom sales\ngroup by item\n name ,color ;\nSimilarly, a query with group by item\n name ,color ,clothes\n sizewould allow us to see the\nsales breakdown by ( item\n name ,color ,clothes\n size) combinations.\nData analysts often need to view data aggregated in multiple ways as illustrated\nabove. The SQL rollup and cube constructs provide a concise way to get multiple such\naggregates using a single query, instead of writing multiple queries.\nThe rollup construct is illustrated using the following query:\nselect item\n name ,color ,sum(quantity )\nfrom sales\ngroup by rollup (item\n name ,color );\nThe result of the query is shown in Figure 5.19. The above query is equivalent to the\nfollowing query using the union operation.\n(select item\n name ,color ,sum(quantity )asquantity\nfrom sales\ngroup by item\n name ,color )\nunion\n(select item\n name ,null ascolor ,sum(quantity )asquantity\nfrom sales\ngroup by item\n name )\nunion\n(select null asitem\n name ,null ascolor ,sum(quantity )asquantity\nfrom sales)\nThe construct group by rollup (item\n name ,color ) generates 3 groupings:\n{(item\n name ,color ), (item\n name ), () }\nwhere () denotes an empty group by list. Observe that a grouping is present for each\npre\ufb01x of the attributes listed in the rollup clause, including the empty pre\ufb01x. The query\nresult contains the union of the results by these groupings. The di\ufb00erent groupings\ngenerate di\ufb00erent schemas; to bring the results of the di\ufb00erent groupings to a common\n", "257": "5.5 Advanced Aggregation Features 229\nitem\n name\n color\n quantity\nskirt\n dark\n 8\nskirt\n pastel\n 35\nskirt\n white\n 10\ndress\n dark\n 20\ndress\n pastel\n 10\ndress\n white\n 5\nshirt\n dark\n 14\nshirt\n pastel\n 7\nshirt\n white\n 28\npants\n dark\n 20\npants\n pastel\n 2\npants\n white\n 5\nskirt\n null\n 53\ndress\n null\n 35\nshirt\n null\n 49\npants\n null\n 27\nnull\n null\n 164\nFigure 5.19 Query result: group by rollup (item\n name ,color).\nschema, tuples in the result contain nullas the value of those attributes not present in\na particular grouping.16\nThe cube construct generates an even larger number of groupings, consisting of all\nsubsets of the attributes listed in the cube construct. For example, the query:\nselect item\n name ,color ,clothes\n size,sum(quantity )\nfrom sales\ngroup by cube (item\n name ,color ,clothes\n size);\ngenerates the following groupings:\n{(item\n name ,color ,clothes\n size), (item\n name ,color ), (item\n name ,clothes\n size),\n(color ,clothes\n size), (item\n name ), (color ), (clothes\n size), () }\nTo bring the results of the di\ufb00erent groupings to a common schema, as with rollup ,t u -\nples in the result contain nullas the value of those attributes not present in a particular\ngrouping.\n16The SQL outer union operation can be used to perform a union of relations that may not have a common schema.\nThe resultant schema has the union of all the attributes ac ross the inputs; each input tuple is mapped to an output\ntuple by adding all the attributes missing in that tuple, with the value set to null. Our union query can be written using\nouter union, and in that case we do not need to exp licitly generate null-value attributes using null asattribute-name\nconstructs, as we have done in the above query.\n", "258": "230 Chapter 5 Advanced SQL\nMultiple rollup sa n d cubes can be used in a single group by clause. For instance,\nthe following query:\nselect item\n name ,color ,clothes\n size,sum(quantity )\nfrom sales\ngroup by rollup (item\n name ),rollup (color ,clothes\n size);\ngenerates the groupings:\n{(item\n name ,color ,clothes\n size), (item\n name ,color ), (item\n name ),\n(color ,clothes\n size), (color ), () }\nTo understand why, observe that rollup (item\n name ) generates a set of two group-\nings, {(item\n name ), () }, while rollup (color ,clothes\n size) generates a set of three group-\nings, {(color ,clothes\n size), (color ), () }.T h eC a r t e s i a np r o d u c to ft h et w os e t sg i v e su s\nthe six groupings shown.\nNeither the rollup nor the cube clause gives complete control on the groupings\nthat are generated. For instance, we cannot use them to specify that we want only\ngroupings {(color ,clothes\n size), (clothes\n size,item\n name )}. Such restricted groupings can\nbe generated by using the grouping sets construct, in which one can specify the speci\ufb01c\nlist of groupings to be used. To obtain only groupings {(color ,clothes\n size), (clothes\n size,\nitem\n name )}, we would write:\nselect item\n name ,color ,clothes\n size,sum(quantity )\nfrom sales\ngroup by grouping sets ((color ,clothes\n size), (clothes\n size,item\n name ));\nAnalysts may want to distinguish those nulls generated by rollup and cube opera-\ntions from \u201cnormal\u201d nulls actually stored in the database or arising from an outer join.\nThe grouping () function returns 1 if its argument is a null value generated by a rollup\norcube and 0 otherwise (note that the grouping function is di\ufb00erent from the grouping\nsetsconstruct). If we wish to display the rollup query result shown in Figure 5.19, but\nusing the value \u201call\u201d in place of nulls generated by rollup ,w ec a nu s et h eq u e r y :\nselect (case when grouping (item\n name )=1 then \u2019all\u2019\nelse item\n name end)asitem\n name ,\n(case when grouping (color )=1 then \u2019all\u2019\nelse color end)ascolor ,\nsum(quantity )asquantity\nfrom sales\ngroup by rollup (item\n name ,color );\nOne might consider using the following query using coalesce , but it would incor-\nrectly convert null item names and colors to all:\n", "259": "Review Terms 231\nselect coalesce (item\n name ,\u2019all\u2019)asitem\n name ,\ncoalesce (color ,\u2019all\u2019)ascolor ,\nsum(quantity )asquantity\nfrom sales\ngroup by rollup (item\n name ,color );\n5.6 Summary\n\u2022SQL queries can be invoked from host languages via embedded and dynamic SQL.\nThe ODBC and JDBC standards de\ufb01ne application program interfaces to access\nSQL databases from C and Java language programs.\n\u2022Functions and procedures can be de\ufb01ned using SQL procedural extensions that\nallow iteration and conditional (if-then-else) statements.\n\u2022Triggers de\ufb01ne actions to be executed automatically when certain events occur and\ncorresponding conditions are satis\ufb01ed. Triggers have many uses, such as business\nrule implementation and audit logging. They may carry out actions outside the\ndatabase system by means of external language routines.\n\u2022Some queries, such as transitive closure, can be expressed either by using itera-\ntion or by using recursive SQL queries. Recursion can be expressed using either\nrecursive views or recursive with clause de\ufb01nitions.\n\u2022SQL supports several advanced aggregation features, including ranking and win-\ndowing queries, as well as pivot, and rollup/cube operations. These simplify the\nexpression of some aggregates and allow more e\ufb03cient evaluation.\nReview Terms\n\u2022JDBC\n\u2022Prepared statements\n\u2022SQL injection\n\u2022Metadata\n\u2022Updatable result sets\n\u2022Open Database Connectivity\n(ODBC )\n\u2022Embedded SQL\n\u2022Embedded database\n\u2022Stored procedures and functions\u2022Table functions.\n\u2022Parameterized views\n\u2022Persistent Storage Module ( PSM).\n\u2022Exception conditions\n\u2022Handlers\n\u2022External language routines\n\u2022Sandbox\n\u2022Trigger\n\u2022Transitive closure\n\u2022Hierarchies\n", "260": "232 Chapter 5 Advanced SQL\n\u2022Create temporary table\n\u2022Base query\n\u2022Recursive query\n\u2022Fixed point\n\u2022Monotonic\n\u2022Windowing\u2022Ranking functions\n\u2022Cross-tabulation\n\u2022Cross-tab\n\u2022Pivot-table\n\u2022Pivot\n\u2022SQL group by cube ,group by rollup\nPractice Exercises\n5.1 Consider the following relations for a company database:\n\u2022emp(ename ,dname ,salary )\n\u2022mgr(ename ,mname )\nand the Java code in Figure 5.20, which uses the JDBC API . Assume that the\nuserid, password, machine name, etc. are all okay. Describe in concise English\nwhat the Java program does. (That is, produce an English sentence like \u201cIt \ufb01nds\nthe manager of the toy department,\u201d not a line-by-line description of what each\nJava statement does.)\n5.2 Write a Java method using JDBC metadata features that takes a ResultSet as\nan input parameter and prints out the result in tabular form, with appropriate\nn a m e sa sc o l u m nh e a d i n g s .\n5.3 Suppose that we wish to \ufb01nd all courses that must be taken before some given\ncourse. That means \ufb01nding not only the prerequisites of that course, but prereq-\nuisites of prerequi sites, and so on. Write a complete Java program using JDBC\nthat:\n\u2022Takes a course\n idvalue from the keyboard.\n\u2022Finds prerequisites of that course using an SQL query submitted via JDBC .\n\u2022For each course returned, \ufb01nds its prerequisites and continues this process\niteratively until no new prerequisite courses are found.\n\u2022Prints out the result.\nFor this exercise, do not use a recursive SQL query, but rather use the iterative\napproach described previously. A well-developed solution will be robust to the\nerror case where a university has accidentally created a cycle of prerequisites\n(that is, for example, course Ais a prerequisite for course B,c o u r s e Bis a pre-\nrequisite for course C,a n dc o u r s e Cis a prerequisite for course A).\n", "261": "Practice Exercises 233\nimport java.sql.*;\npublic class Mystery {\npublic static void main(String[] args) {\ntry (\nConnection con=DriverManager.getConnection(\n\"jdbc:oracle:thin:star/X@//edgar.cse.lehigh.edu:1521/XE\");\nq = \"select mname from mgr where ename = ?\";\nPreparedStatement stmt=con.prepareStatement();\n)\n{\nString q;\nString empName = \"dog\";\nboolean more;\nResultSet result;\ndo {\nstmt.setString(1, empName);\nresult = stmt.executeQuery(q);\nmore = result.next();\nif (more) {\nempName = result.getString(\"mname\");\nSystem.out.println (empName);\n}\n} while (more);\ns.close();\ncon.close();\n}\ncatch(Exception e){\ne.printStackTrace();\n}\n}\n}\nFigure 5.20 Java code for Exercise 5.1 (using Oracle JDBC ).\n5.4 Describe the circumstances in which you would choose to use embedded SQL\nrather than SQL alone or only a general-purpose programming language.\n5.5 Show how to enforce the constraint \u201can instructor cannot teach two di\ufb00erent\nsections in a semester in the same time slot.\u201d using a trigger (remember that the\nconstraint can be violated by changes to the teaches relation as well as to the\nsection relation).\n", "262": "234 Chapter 5 Advanced SQL\nbranch (branch\n name\n ,branch\n city, assets )\ncustomer (customer\n name\n ,customer\n street, cust omer\n city)\nloan (loan\n number\n ,branch\n name, amount )\nborrower (customer\n name\n ,loan\n number\n )\naccount (account\n number\n ,branch\n name, balance )\ndepositor (customer\n name\n ,account\n number\n )\nFigure 5.21 Banking database for Exercise 5.6.\n5.6 Consider the bank database of Figure 5.21. Let us de\ufb01ne a view branch\n custas\nfollows:\ncreate view branch\n cust as\nselect branch\n name, customer\n name\nfrom depositor, account\nwhere depositor.account\n number =account.account\n number\nSuppose that the view is materialized ; that is, the view is computed and stored.\nWrite triggers to maintain the view, that is, to keep it up-to-date on insertions\ntodepositor oraccount . It is not necessary to handle deletions or updates. Note\nthat, for simplicity, we have not required the elimination of duplicates.\n5.7 Consider the bank database of Figure 5.21. Write an SQL trigger to carry out\nthe following action: On delete of an account, for each customer-owner of the\naccount, check if the owner has any remaining accounts, and if she does not,\ndelete her from the depositor relation.\n5.8 Given a relation S(student ,subject ,marks ), write a query to \ufb01nd the top 10 stu-\ndents by total marks, by using SQL ranking. Include all students tied for the \ufb01nal\nspot in the ranking, even if that results in more than 10 total students.\n5.9 Given a relation nyse(y e a r ,m o n t h ,d a y ,s h a r e s\n traded, dollar\n volume )w i t ht r a d -\ning data from the New York Stock Exchange, list each trading day in order of\nnumber of shares traded, and show each day\u2019s rank.\n5.10 Using the relation from Exercise 5.9, write an SQL query to generate a report\nshowing the number of shares traded, number of trades, and total dollar volume\nbroken down by year, each month of each year, and each trading day.\n5.11 Show how to express group by cube (a,b,c,d)u s i n g rollup ; your answer should\nhave only one group by clause.\n", "263": "Exercises 235\nExercises\n5.12 Write a Java program that allows university administrators to print the teaching\nrecord of an instructor.\na. Start by having the user input the login IDand password; then open the\nproper connection.\nb. The user is asked next for a search substring and the system returns ( ID,\nname ) pairs of instructors whose names match the substring. Use the like\n('%substring%') construct in SQL to do this. If the search comes back\nempty, allow continued searches until there is a nonempty result.\nc. Then the user is asked to enter an IDnumber, which is a number between\n0 and 99999. Once a valid number is entered, check if an instructor with\nthat IDexists. If there is no instructor with the given ID, print a reasonable\nmessage and quit.\nd. If the instructor has taught no courses, print a message saying that. Other-\nwise print the teaching record for the instructor, showing the department\nname, course identi\ufb01er, course title, section number, semester, year, and\ntotal enrollment (and sort those by dept\n name, course\n id, year, semester ).\nTest carefully for bad input. Make sure your SQL queries won\u2019t throw an excep-\ntion. At login, exceptions may occur since the user might type a bad password,\nbut catch those exceptions and allow the user to try again.\n5.13 Suppose you were asked to de\ufb01ne a class MetaDisplay in Java, containing a\nmethod static void printTable(String r) ; the method takes a relation name ras\ninput, executes the query \u201c select *from r\u201d, and prints the result out in tabular\nformat, with the attribute names displayed in the header of the table.\na. What do you need to know about relation rto be able to print the result\nin the speci\ufb01ed tabular format?\nb. What JDBC methods(s) can get you the required information?\nc. Write the method printTable(String r) using the JDBC API .\n5.14 Repeat Exercise 5.13 using ODBC , de\ufb01ning void printTable(char *r) as a func-\ntion instead of a method.\n5.15 Consider an employee database with two relations\nemployee (employee\n name\n ,street ,city)\nworks (employee\n name\n ,company\n name ,salary )\n", "264": "236 Chapter 5 Advanced SQL\nwhere the primary keys are underlined. Write a function avg\n salary that takes a\ncompany name as an argument and \ufb01nds the average salary of employees at that\ncompany. Then, write an SQL statement, using that function, to \ufb01nd companies\nwhose employees earn a higher salary, on average, than the average salary at\n\u201cFirst Bank\u201d.\n5.16 Consider the relational schema\npart(part\n id\n,name ,cost)\nsubpart (part\n id\n,subpart\n id\n,count )\nwhere the primary-key attributes are underlined. A tuple ( p1,p2,3 )i nt h e subpart\nrelation denotes that the part with part\n id p2is a direct subpart of the part\nwith part\n id p1,a n d p1has 3 copies of p2.N o t et h a t p2may itself have further\nsubparts. Write a recursive SQL query that outputs the names of all subparts of\nthe part with part-id 'P-100'.\n5.17 Consider the relational schema from Exercise 5.16. Write a JDBC function using\nnonrecursive SQL to \ufb01nd the total cost of part \u201cP-100\u201d, including the costs of all\nits subparts. Be sure to take into account the fact that a part may have multiple\noccurrences of a subpart. You may use recursion in Java if you wish.\n5.18 Redo Exercise 5.12 using the language of your database system for coding stored\nprocedures and functions. Note that you are likely to have to consult the online\ndocumentation for your system as a reference, since most systems use syntax\ndi\ufb00ering from the SQL standard version followed in the text. Speci\ufb01cally, write\na prodedure that takes an instructor IDas an argument and produces printed\noutput in the format speci\ufb01ed in Exercise 5.12, or an appropriate message if\nthe instructor does not exist or has taught no courses. (For a simpler version\nof this exercise, rather than providing printed output, assume a relation with\nthe appropriate schema and insert your answer there without worrying about\ntesting for erroneous argument values.)\n5.19 Suppose there are two relations rand s, such that the foreign key Bofrreferences\nthe primary key Aofs. Describe how the trigger mechanism can be used to\nimplement the on delete cascade option when a tuple is deleted from s.\n5.20 The execution of a trigger can cause another action to be triggered. Most\ndatabase systems place a limit on how deep the nesting can be. Explain why\nthey might place such a limit.\n5.21 Modify the recursive query in Figure 5.16 to de\ufb01ne a relation\nprereq\n depth (course\n id,prereq\n id,depth )\n", "265": "Tools 237\nbuilding\n room\n number\n time\n slot\n id\n course\n id\n sec\nid\nGar\ufb01eld\n 359\n A\n BIO-101\n 1\nGar\ufb01eld\n 359\n B\n BIO-101\n 2\nSaucon\n 651\n A\n CS-101\n 2\nSaucon\n 550\n C\n CS-319\n 1\nPainter\n 705\n D\n MU-199\n 1\nPainter\n 403\n D\n FIN-201\n 1\nFigure 5.22 The relation rfor Exercise 5.24.\nwhere the attribute depth indicates how many levels of intermediate prerequi-\nsites there are between the course and the prerequisite. Direct prerequisites have\na depth of 0. Note that a prerequisite course may have multiple depths and thus\nmay appear more than once.\n5.22 Given relation s(a,b,c), write an SQL statement to generate a histogram show-\ning the sum of cvalues versus a, dividing ainto 20 equal-sized partitions (i.e.,\nwhere each partition contains 5 percent of the tuples in s,s o r t e db y a).\n5.23 Consider the nyse relation of Exercise 5.9. For each month of each year, show\nthe total monthly dollar volume and the average monthly dollar volume for that\nmonth and the two prior months. ( Hint: First write a query to \ufb01nd the total\ndollar volume for each month of each year. Once that is right, put that in the\nfrom clause of the outer query that solves the full problem. That outer query\nwill need windowing. The subquery does not.)\n5.24 Consider the relation, r, shown in Figure 5.22. Give the result of the following\nquery:\nselect building ,room\n number ,time\n slot\n id,count (*)\nfrom r\ngroup by rollup (building ,room\n number ,time\n slot\n id)\nTools\nWe provide sample JDBC code on our book web site db-book.com .\nMost database vendors, including IBM, Microsoft, and Oracle, provide OLAP tools\nas part of their database systems, or as add-on applications. Tools may be integrated\nwith a larger \u201cbusiness intelligence\u201d product such as IBMCognos. Many companies also\nprovide analysis tools for speci\ufb01c applications, such as customer relationship manage-\nment (e.g., Oracle Siebel CRM ).\n", "266": "238 Chapter 5 Advanced SQL\nFurther Reading\nMore details about JDBC may be found at docs.oracle.com/javase/tutorial/jdbc .\nIn order to write stored procedures, stored functions, and triggers that can be exe-\ncuted on a given system, you need to refer to the system documentation.\nAlthough our discussion of recursive queries focused on SQL syntax, there are\nother approaches to recursion in relational databases. Datalog is a database language\nbased on the Prolog programming language and is described in more detail in Section\n27.4 (available online).\nOLAP features in SQL, including rollup, and cubes were introduced in SQL:1999 ,\nand window functions with ranking and partitioning were added in SQL:2003 .OLAP\nfeatures, including window functions, are supported by most databases today. Although\nmost follow the SQL standard syntax that we have presented, there are some di\ufb00er-\nences; refer to the system manuals of the system that you are using for further details.\nMicrosoft\u2019s Multidimensional Expressions ( MDX )i sa n SQL-like query language de-\nsigned for querying OLAP cubes.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "267": "PART2\nDATABASE DESIGN\nThe task of creating a database application is a complex one, involving design of the\ndatabase schema, design of the programs that access and update the data, and design\nof a security scheme to control access to data. The needs of the users play a central\nrole in the design process. In this part, we focus primarily on the design of the database\nschema. We also outline some of the other design tasks.\nThe entity-relationship ( E-R) model described in Chapter 6 is a high-level data\nmodel. Instead of representing all data in tables, it distinguishes between basic objects,\ncalled entities ,a n d relationships among these objects. It is often used as a \ufb01rst step in\ndatabase-schema design.\nRelational database design\u2014the design of the relational schema\u2014 was covered in-\nformally in earlier chapters. There are, however, principles that can be used to distin-\nguish good database designs from bad ones. These are formalized by means of several\n\u201cnormal forms\u201d that o\ufb00er di\ufb00erent trade-o\ufb00s between the possibility of inconsistencies\nand the e\ufb03ciency of certain queries. Chapter 7 describes the formal design of relational\nschemas.\n239\n", "268": "", "269": "CHAPTER6\nDatabase Design Using the E-R\nModel\nUp to this point in the text, we have assumed a given database schema and studied how\nqueries and updates are expressed. We now consider how to design a database schema\nin the \ufb01rst place. In this chapter, we focus on the entity-relationship data model ( E-\nR), which provides a means of identifying entities to be represented in the database\nand how those entities are related. Ultimately, the database design will be expressed in\nterms of a relational database design and an associated set of constraints. We show in\nthis chapter how an E-Rdesign can be transformed into a set of relation schemas and\nhow some of the constraints can be captured in that design. Then, in Chapter 7, we\nconsider in detail whether a set of relation schemas is a good or bad database design\nand study the process of creating good designs using a broader set of constraints. These\ntwo chapters cover the fundamental concepts of database design.\n6.1 Overview of the Design Process\nThe task of creating a database application is a complex one, involving design of the\ndatabase schema, design of the programs that access and update the data, and design\nof a security scheme to control access to data. The needs of the users play a central role\nin the design process. In this chapter, we focus on the design of the database schema,\nalthough we brie\ufb02y outline some of the other design tasks later in the chapter.\n6.1.1 Design Phases\nFor small applications, it may be feasible for a database designer who understands\nthe application requirements to decide directly on the relations to be created, their\nattributes, and constraints on the relations. However, such a direct design process is\ndi\ufb03cult for real-world applications, since they are often highly complex. Often no one\nperson understands the complete data needs of an application. The database designer\nmust interact with users of the application to understand the needs of the applica-\ntion, represent them in a high-level fashion that can be understood by the users, and\n241\n", "270": "242 Chapter 6 Database Design Using the E-R Model\nthen translate the requirements into lower levels of the design. A high-level data model\nserves the database designer by providing a conceptual framework in which to specify,\nin a systematic fashion, the data requirements of the database users, and a database\nstructure that ful\ufb01lls these requirements.\n\u2022The initial phase of database design is to characterize fully the data needs of the\nprospective database users. The database designer needs to interact extensively\nwith domain experts and users to carry out this task. The outcome of this phase is\na speci\ufb01cation of user requirements. While there are techniques for diagrammati-\ncally representing user requirements, in this chapter we restrict ourselves to textual\ndescriptions of user requirements.\n\u2022Next, the designer chooses a data model and, by applying the concepts of the cho-\nsen data model, translates these requirements into a conceptual schema of the\ndatabase. The schema developed at this conceptual-design phase provides a de-\ntailed overview of the enterprise. The entity-relationship model, which we study in\nthe rest of this chapter, is typically used to represent the conceptual design. Stated\nin terms of the entity-relationship model, the conceptual schema speci\ufb01es the enti-\nties that are represented in the database, the attributes of the entities, the relation-\nships among the entities, and constraints on the entities and relationships. Typi-\ncally, the conceptual-design phase results in the creation of an entity-relationship\ndiagram that provides a graphic representation of the schema.\nThe designer reviews the schema to con\ufb01rm that all data requirements are\nindeed satis\ufb01ed and are not in con\ufb02ict with one another. She can also examine the\ndesign to remove any redundant features. Her focus at this point is on describing\nthe data and their relationships, rather than on specifying physical storage details.\n\u2022A fully developed conceptual schema also indicates the functional requirements\nof the enterprise. In a speci\ufb01cation of functional requirements , users describe the\nkinds of operations (or transactions) that will be performed on the data. Example\noperations include modifying or updating data, searching for and retrieving spe-\nci\ufb01c data, and deleting data. At this stage of conceptual design, the designer can\nreview the schema to ensure that it meets functional requirements.\n\u2022The process of moving from an abstract data model to the implementation of the\ndatabase proceeds in two \ufb01nal design phases.\n\u00b0In the logical-design phase , the designer maps the high-level conceptual schema\nonto the implementation data model of the database system that will be used.\nThe implementation data model is typically the relational data model, and this\nstep typically consists of mapping the conceptual schema de\ufb01ned using the\nentity-relationship model into a relation schema.\n\u00b0Finally, the designer uses the resulting system-speci\ufb01c database schema in the\nsubsequent physical-design phase ,i nw h i c ht h ep h y s i c a lf e a t u r e so ft h ed a t a b a s e\n", "271": "6.1 Overview of the Design Process 243\nare speci\ufb01ed. These features include the form of \ufb01le organization and choice\nof index structures, discussed in Chapter 13 and Chapter 14.\nThe physical schema of a database can be changed relatively easily after an applica-\ntion has been built. However, changes to the logical schema are usually harder to carry\nout, since they may a\ufb00ect a number of queries and updates scattered across application\ncode. It is therefore important to carry out the database design phase with care, before\nbuilding the rest of the database application.\n6.1.2 Design Alternatives\nA major part of the database design process is deciding how to represent in the design\nthe various types of \u201cthings\u201d such as people, places, products, and the like. We use the\nterm entity to refer to any such distinctly identi\ufb01able item. In a university database,\nexamples of entities would include instructors, students, departments, courses, and\ncourse o\ufb00erings. We assume that a course may have run in multiple semesters, as well\nas multiple times in a semester; we refer to each such o\ufb00ering of a course as a section.\nThe various entities are related to each other in a variety of ways, all of which need to be\ncaptured in the database design. For example, a student takes a course o\ufb00ering, while\nan instructor teaches a course o\ufb00ering; teaches and takes are examples of relationships\nbetween entities.\nIn designing a database schema, we must ensure that we avoid two major pitfalls:\n1.Redundancy: A bad design may repeat information. For example, if we store the\ncourse identi\ufb01er and title of a course with each course o\ufb00ering, the title would be\nstored redundantly (i.e., multiple times, unnecessarily) with each course o\ufb00ering.\nIt would su\ufb03ce to store only the course identi\ufb01er with each course o\ufb00ering, and\nto associate the title with the course identi\ufb01er only once, in a course entity.\nRedundancy can also occur in a relational schema. In the university example\nwe have used so far, we have a relation with section information and a separate\nrelation with course information. Suppose that instead we have a single relation\nwhere we repeat all of the course information (course\n id, title, dept\n name, credits)\nonce for each section (o\ufb00ering) of the course. Information about courses would\nthen be stored redundantly.\nThe biggest problem with such redundant representation of information is that\nthe copies of a piece of information can become inconsistent if the information\nis updated without taking precautions to update all copies of the information.\nFor example, di\ufb00erent o\ufb00erings of a course may have the same course identi\ufb01er,\nbut may have di\ufb00erent titles. It would then become unclear what the correct title\nof the course is. Ideally, information should appear in exactly one place.\n2.Incompleteness: A bad design may make certain aspects of the enterprise di\ufb03-\ncult or impossible to model. For example, suppose that, as in case (1) above,\nwe only had entities corresponding to course o\ufb00ering, without having an entity\n", "272": "244 Chapter 6 Database Design Using the E-R Model\ncorresponding to courses. Equivalently, in terms of relations, suppose we have\na single relation where we repeat all of the course information once for each\nsection that the course is o\ufb00ered. It would then be impossible to represent infor-\nmation about a new course, unless that course is o\ufb00ered. We might try to make\ndo with the problematic design by storing null values for the section information.\nSuch a work-around is not only unattractive but may be prevented by primary-key\nconstraints.\nAvoiding bad designs is not enough. There may be a large number of good designs\nfrom which we must choose. As a simple example, consider a customer who buys a\nproduct. Is the sale of this product a relationship between the customer and the prod-\nuct? Alternatively, is the sale itself an entity that is related both to the customer and to\nthe product? This choice, though simple, may make an important di\ufb00erence in what\naspects of the enterprise can be modeled well. Considering the need to make choices\nsuch as this for the large number of entities and relationships in a real-world enterprise,\nit is not hard to see that database design can be a challenging problem. Indeed we shall\nsee that it requires a combination of both science and \u201cgood taste.\u201d\n6.2 The Entity-Relationship Model\nThe entity-relationship (E-R)data model was developed to facilitate database design by\nallowing speci\ufb01cation of an enterprise schema that represents the overall logical struc-\nture of a database.\nThe E-Rmodel is very useful in mapping the meanings and interactions of real-\nworld enterprises onto a conceptual schema. Because of this usefulness, many database-\ndesign tools draw on concepts from the E-Rmodel. The E-Rdata model employs three\nbasic concepts: entity sets, relationship sets, and attributes. The E-Rmodel also has an\nassociated diagrammatic representation, the E-Rdiagram. As we saw brie\ufb02y in Section\n1.3.1, an E-Rdiagram can express the overall logical structure of a database graphically.\nE-Rdiagrams are simple and clear\u2014qualities that may well account in large part for the\nwidespread use of the E-Rmodel.\nThe Tools section at the end of the chapter provides information about several\ndiagram editors that you can use to create E-Rdiagrams.\n6.2.1 Entity Sets\nAnentity is a \u201cthing\u201d or \u201cobject\u201d in the real world that is distinguishable from all other\nobjects. For example, each person in a university is an entity. An entity has a set of prop-\nerties, and the values for some set of properties must uniquely identify an entity. For\ninstance, a person may have a person\n idproperty whose value uniquely identi\ufb01es that\nperson. Thus, the value 677-89-9011 for person\n idwould uniquely identify one particu-\nlar person in the university. Similarly, courses can be thought of as entities, and course\niduniquely identi\ufb01es a course entity in the university. An entity may be concrete, such\n", "273": "6.2 The Entity-Relationship Model 245\nas a person or a book, or it may be abstract, such as a course, a course o\ufb00ering, or a\n\ufb02ight reservation.\nAnentity set i sas e to fe n t i t i e so ft h es a m et y p et h a ts h a r et h es a m ep r o p e r t i e s ,\nor attributes. The set of all people who are instructors at a given university, for exam-\nple, can be de\ufb01ned as the entity set instructor . Similarly, the entity set student might\nrepresent the set of all students in the university.\nIn the process of modeling, we often use the term entity set in the abstract, without\nreferring to a particular set of individual entities. We use the term extension of the entity\nset to refer to the actual collection of entities belonging to the entity set. Thus, the set\nof actual instructors in the university forms the extension of the entity set instructor .\nThis distinction is similar to the di\ufb00erence between a relation and a relation instance,\nwhich we saw in Chapter 2.\nEntity sets do not need to be disjoint. For example, it is possible to de\ufb01ne the entity\nsetperson consisting of all people in a university. A person entity may be an instructor\nentity, a student entity, both, or neither.\nAn entity is represented by a set of attributes . Attributes are descriptive properties\npossessed by each member of an entity set. The designation of an attribute for an en-\ntity set expresses that the database stores similar information concerning each entity\nin the entity set; however, each entity may have its own value for each attribute. Pos-\nsible attributes of the instructor entity set are ID,name ,dept\nname ,a n d salary .I nr e a l\nlife, there would be further attributes, such as street number, apartment number, state,\npostal code, and country, but we generally omit them to keep our examples simple.\nPossible attributes of the course entity set are course\n id,title,dept\nname ,a n d credits .\nIn this section we consider only attributes that are simple \u2014 those not divided into\nsubparts. In Section 6.3, we discuss more c omplex situations where attributes can be\ncomposite and multivalued.\nEach entity has a value for each of its attributes. For instance, a particular instructor\nentity may have the value 12121 for ID,t h ev a l u eW uf o r name , the value Finance for\ndept\nname , and the value 90000 for salary .\nThe IDattribute is used to identify instructors uniquely, since there may be more\nthan one instructor with the same name. Historically, many enterprises found it con-\nvenient to use a government-issued identi\ufb01cation number as an attribute whose value\nuniquely identi\ufb01es the person. However, that is considered bad practice for reasons of\nsecurity and privacy. In general, the enterprise would have to create and assign its own\nunique identi\ufb01er for each instructor.\nA database thus includes a collection of entity sets, each of which contains any\nnumber of entities of the same type. A database for a university may include a number\nof other entity sets. For example, in addition to keeping track of instructors and stu-\ndents, the university also has information about courses, which are represented by the\nentity set course with attributes course\n id,title,dept\nname andcredits . In a real setting,\na university database may keep dozens of entity sets.\nAn entity set is represented in an E-Rdiagram by a rectangle , which is divided\ninto two parts. The \ufb01rst part, which in this text is shaded blue, contains the name of\n", "274": "246 Chapter 6 Database Design Using the E-R Model\ninstructor\nID\nname\nsalarystudent\nID\nname\ntot_cred\nFigure 6.1 E-Rdiagram showing entity sets instructor and student .\nthe entity set. The second part contains the names of all the attributes of the entity\nset. The E-Rdiagram in Figure 6.1 shows two entity sets instructor andstudent .T h e\nattributes associated with instructor areID,name ,a n d salary . The attributes associated\nwith student areID,name ,a n d tot\ncred. Attributes that are part of the primary key are\nunderlined (see Section 6.5).\n6.2.2 Relationship Sets\nArelationship is an association among several entities. For example, we can de\ufb01ne a\nrelationship advisor that associates instructor Katz with student Shankar. This relation-\nship speci\ufb01es that Katz is an advisor to student Shankar. A relationship set is a set of\nrelationships of the same type.\nConsider two entity sets instructor andstudent . We de\ufb01ne the relationship set ad-\nvisor to denote the associations between students and the instructors who act as their\nadvisors. Figure 6.2 depicts this association. To keep the \ufb01gure simple, only some of\nthe attributes of the two entity sets are shown.\nArelationship instance in an E-Rschema represents an association between the\nnamed entities in the real-world enterprise that is being modeled. As an illustration,\nthe individual instructor entity Katz, who has instructor ID45565, and the student en-\ntity Shankar, who has student ID12345, participate in a relationship instance of advi-\ninstructor\nstudent76766 Crick\nKatz\nSrinivasan\nKim\nSingh\nEinstein45565\n10101\n98345\n76543\n2222298988\n12345\n00128\n76543\n76653\n23121\n44553Tanaka\nShankar\nZhang\nBrown\nAoi\nChavez\nPeltier\nFigure 6.2 Relationship set advisor (only some attributes of instructor and student are\nshown).\n", "275": "6.2 The Entity-Relationship Model 247\ninstructor\nID\nname\nsalarystudent\nID\nname\ntot_credadvisor\nFigure 6.3 E-Rdiagram showing relationship set advisor .\nsor. This relationship instance represents that in the university, the instructor Katz is\nadvising student Shankar.\nA relationship set is represented in an E-Rdiagram by a diamond , which is linked\nvialines to a number of di\ufb00erent entity sets (rectangles). The E-Rdiagram in Figure 6.3\nshows the two entity sets instructor andstudent , related through a binary relationship\nsetadvisor .\nAs another example, consider the two entity sets student andsection ,w h e r e section\ndenotes an o\ufb00ering of a course. We can de\ufb01ne the relationship set takes to denote the\nassociation between a student and a section in which that student is enrolled.\nAlthough in the preceding examples each relationship set was an association be-\ntween two entity sets, in general a relationship set may denote the association of more\nthan two entity sets.\nFormally, a relationship set is a mathematical relation on n\u22652 (possibly nondis-\ntinct) entity sets. If E1,E2,\u2026,Enare entity sets, then a relationship set Ris a subset\nof\n{(e1,e2,\u2026,en)|e1\u2208E1,e2\u2208E2,\u2026,en\u2208En}\nwhere ( e1,e2,\u2026,en) is a relationship instance.\nThe association between entity sets is referred to as participation; i.e., the entity\nsetsE1,E2,\u2026,Enparticipate in relationship set R.\nThe function that an entity plays in a relationship is called that entity\u2019s role.S i n c e\nentity sets participating in a relationship set are generally distinct, roles are implicit and\nare not usually speci\ufb01ed. However, they are useful when the meaning of a relationship\nneeds clari\ufb01cation. Such is the case when the entity sets of a relationship set are not\ndistinct; that is, the same entity set participates in a relationship set more than once,\nin di\ufb00erent roles. In this type of relationship set, sometimes called a recursive relation-\nship set, explicit role names are necessary to specify how an entity participates in a\nrelationship instance. For example, consider the entity set course that records informa-\ntion about all the courses o\ufb00ered in the university. To depict the situation where one\ncourse (C2) is a prerequisite for another course (C1) we have relationship set prereq\nthat is modeled by ordered pairs of course entities. The \ufb01rst course of a pair takes the\nrole of course C1, whereas the second takes t h er o l eo fp r e r e q u i s i t ec o u r s eC 2 .I nt h i s\nway, all relationships of prereq are characterized by (C1, C2) pairs; (C2, C1) pairs are\nexcluded. We indicate roles in E-Rdiagrams by labeling the lines that connect diamonds\n", "276": "248 Chapter 6 Database Design Using the E-R Model\ncourse\ncourse_id\ntitle\ncreditscourse_id\nprereq_idprereq\nFigure 6.4 E-Rdiagram with role indicators.\nto rectangles. Figure 6.4 shows the role indicators course\n idandprereq\n idbetween the\ncourse entity set and the prereq relationship set.\nA relationship may also have attributes called descriptive attributes .A sa ne x a m p l e\nof descriptive attributes for relationships, consider the relationship set takes which re-\nlates entity sets student andsection . We may wish to store a descriptive attribute grade\nwith the relationship to record the grade that a student received in a course o\ufb00ering.\nAn attribute of a relationship set is represented in an E-Rdiagram by an undivided\nrectangle . We link the rectangle with a dashed line to the diamond representing that\nrelationship set. For example, Figure 6.5 shows the relationship set takes between the\nentity sets section andstudent .W eh a v et h ed e s c r i p t i v ea t t r i b u t e grade attached to the\nrelationship set takes . A relationship set may have multiple descriptive attributes; for\nexample, we may also store a descriptive attribute for\ncredit with the takes relationship\nset to record whether a student has taken the section for credit, or is auditing (or sitting\nin on) the course.\nObserve that the attributes of the two entity sets have been omitted from the E-R\ndiagram in Figure 6.5, with the understanding that they are speci\ufb01ed elsewhere in the\ncomplete E-Rdiagram for the university; we have already seen the attributes for student ,\nand we will see the attributes of section later in this chapter. Complex E-Rdesigns may\nneed to be split into multiple diagrams that may be located in di\ufb00erent pages. Rela-\ntionship sets should be shown in only one location, but entity sets may be repeated in\nmore than one location. The attributes of an entity set should be shown in the \ufb01rst oc-\ncurrence. Subsequent occurrences of the entity set should be shown without attributes,\nto avoid repetition of information and the resultant possibility of inconsistency in the\nattributes shown in di\ufb00erent occurrences.\ngrade\ntakes section student\nFigure 6.5 E-Rdiagram with an attribute attached to a relationship set.\n", "277": "6.3 Complex Attributes 249\nIt is possible to have more than one relationship set involving the same entity sets.\nFor example, suppose that students may be teaching assistants for a course. Then, the\nentity sets section andstudent may participate in a relationship set teaching\n assistant ,\nin addition to participating in the takes relationship set.\nThe formal de\ufb01nition of a relationship set, which we saw earlier, de\ufb01nes a rela-\ntionship set as a set of relationship instances. Consider the takes relationship between\nstudent andsection . Since a set cannot have duplicates, it follows that a particular stu-\ndent can have only one association with a particular section in the takes relationship.\nThus, a student can have only one grade associated with a section, which makes sense\nin this case. However, if we wish to allow a student to have more than one grade for\nthe same section, we need to have an attribute grades which stores a set of grades; such\nattributes are called multivalued attributes, and we shall see them later in Section 6.3.\nThe relationship sets advisor andtakes provide examples of a binary relationship\nset\u2014that is, one that involves two entity sets. Most of the relationship sets in a database\nsystem are binary. Occasionally, however, relationship sets involve more than two entity\nsets. The number of entity sets that participate in a relationship set is the degree of the\nrelationship set . A binary relationship set is of degree 2; a ternary relationship set is of\ndegree 3.\nAs an example, suppose that we have an entity set project that represents all the re-\nsearch projects carried out in the university. Consider the entity sets instructor ,student ,\nandproject . Each project can have multiple associated students and multiple associated\ninstructors. Furthermore, each student w orking on a project must have an associated\ninstructor who guides the student on the project. For now, we ignore the \ufb01rst two re-\nlationships, between project and instructor, and between project and student. Instead,\nwe focus on the information about which instructor is guiding which student on a par-\nticular project.\nTo represent this information, we relate the three entity sets through a ternary re-\nlationship set proj\nguide , which relates entity sets instructor ,student ,a n d project .A n\ninstance of proj\nguide indicates that a particular student is guided by a particular in-\nstructor on a particular project. Note that a student could have di\ufb00erent instructors as\nguides for di\ufb00erent projects, which cannot be captured by a binary relationship between\nstudents and instructors.\nNonbinary relationship sets can be speci\ufb01ed easily in an E-Rdiagram. Figure 6.6\nshows the E-Rdiagram representation of the ternary relationship set proj\nguide .\n6.3 Complex Attributes\nFor each attribute, there is a set of permitted values, called the domain ,o rvalue set ,o f\nthat attribute. The domain of attribute course\n idmight be the set of all text strings of\na certain length. Similarly, the domain of attribute semester might be strings from the\nset {Fall, Winter, Spring, Summer }.\n", "278": "250 Chapter 6 Database Design Using the E-R Model\ninstructor\nID\nname\nsalarystudent\nID\nname\ntot_cred...project\nproj_guide\nFigure 6.6 E-Rdiagram with a ternary relationship proj\nguide .\nname address\n\ufb01rst_name middle_initial last_name street city state postal_code\nstreet_number street_name apartment_numbercomposite\nattributes\ncomponent\nattributes\nFigure 6.7 Composite attributes instructor name and address .\nAn attribute, as used in the E-Rmodel, can be characterized by the following at-\ntribute types.\n\u2022Simple and composite attributes. In our examples thus far, the attributes have been\nsimple ; that is, they have not been divided into subparts. Composite attributes, on\nthe other hand, can be divided into subparts (i.e., other attributes). For exam-\nple, an attribute name could be structured as a composite attribute consisting of\n\ufb01rst\nname ,middle\n initial ,a n d last\nname . Using composite attributes in a design\nschema is a good choice if a user will wish to refer to an entire attribute on some\noccasions, and to only a component of the attribute on other occasions. Suppose\nwe were to add an address to the student entity-set. The address can be de\ufb01ned\nas the composite attribute address with the attributes street ,city,state,a n d postal\ncode.1Composite attributes help us to group together related attributes, making\nthe modeling cleaner.\nNote also that a composite attribute may appear as a hierarchy. In the compos-\nite attribute address , its component attribute street can be further divided into street\nnumber ,street\n name ,a n d apartment\n number . Figure 6.7 depicts these examples of\ncomposite attributes for the instructor entity set.\n1We assume the address format used in the United States, which includes a numeric postal code called a zip code.\n", "279": "6.3 Complex Attributes 251\n\u2022Single-valued and multivalued attributes. The attributes in our examples all have\na single value for a particular entity. For instance, the student\n IDattribute for a\nspeci\ufb01c student entity refers to only one student ID. Such attributes are said to be\nsingle valued . There may be instances where an attribute has a set of values for\na speci\ufb01c entity. Suppose we add to the instructor entity set a phone\n number at-\ntribute. An instructor may have zero, one, or several phone numbers, and di\ufb00erent\ninstructors may have di\ufb00erent numbers of phones. This type of attribute is said to\nbemultivalued . As another example, we could add to the instructor entity set an\nattribute dependent\n name listing all the dependents. This attribute would be multi-\nvalued, since any particular instructor may have zero, one, or more dependents.\n\u2022Derived attributes . The value for this type of attribute can be derived from the val-\nues of other related attributes or entities. For instance, let us say that the instructor\nentity set has an attribute students\n advised , which represents how many students\nan instructor advises. We can derive the value for this attribute by counting the\nnumber of student entities associated with that instructor.\nAs another example, suppose that the instructor entity set has an attribute age\nthat indicates the instructor\u2019s age. If the instructor entity set also has an attribute\ndate\n of\nbirth, we can calculate agefrom date\n of\nbirth and the current date. Thus,\nageis a derived attribute. In this case, date\n of\nbirth may be referred to as a base\nattribute, or a stored attribute. The value of a derived attribute is not stored but is\ncomputed when required.\nFigure 6.8 shows how composite attributes can be represented in the E-Rnotation.\nHere, a composite attribute name with component attributes \ufb01rst\nname ,middle\n initial ,\nandlast\nname replaces the simple attribute name ofinstructor . As another example,\nsuppose we were to add an address to the instructor entity set. The address can be de-\n\ufb01ned as the composite attribute address with the attributes street ,city,state,a n d postal\ncode. The attribute street is itself a composite attribute whose component attributes\narestreet\n number ,street\n name ,a n d apartment\n number . The \ufb01gure also illustrates a mul-\ntivalued attribute phone\n number , denoted by \u201c {phone\n number }\u201d, and a derived attribute\nage,d e p i c t e db y\u201c age() \u201d .\nAn attribute takes a nullvalue when an entity does not have a value for it. The null\nvalue may indicate \u201cnot applicable\u201d\u2014that is , the value does not exist for the entity. For\nexample, a person who has no middle name may have the middle\n initial attribute set\ntonull.Null can also designate that an attribute value is unknown. An unknown value\nmay be either missing (the value does exist, but we do not have that information) or\nnot known (we do not know whether or not the value actually exists).\nFor instance, if the name value for a particular instructor is null, we assume that\nthe value is missing, since every instructor must have a name. A null value for the\napartment\n number attribute could mean that the address does not include an apartment\nnumber (not applicable), that an apartment number exists but we do not know what\n", "280": "252 Chapter 6 Database Design Using the E-R Model\ninstructor\nID\nname\n\ufb01rst_name\nmiddle_initial\nlast_name\naddress\nstreet\nstreet_number\nstreet_name\napt_number\ncity\nstate\nzip\n{ phone_number }\ndate_of_birth\nage ( )\nFigure 6.8 E-Rdiagram with composite, multivalued, and derived attributes.\nit is (missing), or that we do not know whether or not an apartment number is part of\nthe instructor\u2019s address (unknown).\n6.4 Mapping Cardinalities\nMapping cardinalities , or cardinality ratios, express the number of entities to which\nanother entity can be associated via a relationship set. Mapping cardinalities are most\nuseful in describing binary relationship sets, although they can contribute to the de-\nscription of relationship sets that involve more than two entity sets.\nFor a binary relationship set Rbetween entity sets AandB, the mapping cardinality\nmust be one of the following:\n\u2022One-to-one .A ne n t i t yi n Ais associated with at most one entity in B,a n da ne n t i t y\ninBis associated with at most one entity in A.( S e eF i g u r e6 . 9 a . )\n\u2022One-to-many .A ne n t i t yi n Ais associated with any number (zero or more) of enti-\nties in B.A ne n t i t yi n B, however, can be associated with at most one entity in A.\n(See Figure 6.9b.)\n\u2022Many-to-one .A ne n t i t yi n Ais associated with at most one entity in B.A ne n t i t y\ninB, however, can be associated with any number (zero or more) of entities in A.\n(See Figure 6.10a.)\n", "281": "6.4 Mapping Cardinalities 253\n(b) (a)a1\na2\na3\na4b1\nb2\nb3a2a1\na3b1\nb2\nb3\nb4\nb5AB AB\nFigure 6.9 Mapping cardinalities. (a) One-to-one. (b) One-to-many.\n\u2022Many-to-many .A ne n t i t yi n Ais associated with any number (zero or more) of\nentities in B,a n da ne n t i t yi n Bis associated with any number (zero or more) of\nentities in A.( S e eF i g u r e6 . 1 0 b . )\nThe appropriate mapping cardinality for a particular relationship set obviously depends\non the real-world situation that the relationship set is modeling.\nAs an illustration, consider the advisor relationship set. If a student can be advised\nby several instructors (as in the case of students advised jointly), the relationship set is\nmany-to-many. In contrast, if a particular university imposes a constraint that a student\ncan be advised by only one instructor, and an instructor can advise several students,\nthen the relationship set from instructor tostudent must be one-to-many. Thus, mapping\na\na2\na3\na5a1\na2\na4a2a1\na3\na4b1\nb2\nb3A BB A\nb1\nb2\nb3\nb4\n(a) (b)\nFigure 6.10 Mapping cardinalities. (a) Many-to-one. (b) Many-to-many.\n", "282": "254 Chapter 6 Database Design Using the E-R Model\ncardinalities can be used to specify constraints on what relationships are permitted in\nthe real world.\nIn the E-Rdiagram notation, we indicate cardinality constraints on a relationship\nby drawing either a directed line ( \u2192) or an undirected line (\u2014) between the relationship\nset and the entity set in question. Speci\ufb01cally, for the university example:\n\u2022One-to-one . We draw a directed line from the relationship set to both entity sets.\nFor example, in Figure 6.11a, the directed lines to instructor andstudent indicate\nthat an instructor may advise at most one student, and a student may have at most\none advisor.\ninstructor student\nID\nname\nsalary\ninstructor\nID\nname\nsalary\ninstructor\nID\nname\nsalary\ninstructor\nID\nname\nsalaryID\nname\ntot_cred\nstudent\nID\nname\ntot_cred\nstudent\nID\nname\ntot_cred\nstudent\nID\nname\ntot_credadvisor\nadvisor\nadvisor\nadvisor(a) One-to-one\n(b) One-to-many\n(c) Many-to-one\n(d) Many-to-many\nFigure 6.11 Relationship cardinalities.\n", "283": "6.4 Mapping Cardinalities 255\n\u2022One-to-many . We draw a directed line from the relationship set to the \u201cone\u201d side of\nthe relationship. Thus, in Figure 6.11b, there is a directed line from relationship set\nadvisor to the entity set instructor , and an undirected line to the entity set student .\nThis indicates that an instructor may advise many students, but a student may have\nat most one advisor.\n\u2022Many-to-one . We draw a directed line from the relationship set to the \u201cone\u201d side\nof the relationship. Thus, in Figure 6.11c, there is an undirected line from the\nrelationship set advisor to the entity set instructor and a directed line to the entity\nsetstudent . This indicates that an instructor may advise at most one student, but\na student may have many advisors.\n\u2022Many-to-many . We draw an undirected line from the relationship set to both entity\nsets. Thus, in Figure 6.11d, there are undirected lines from the relationship set\nadvisor to both entity sets instructor andstudent . This indicates that an instructor\nmay advise many students, and a student may have many advisors.\nThe participation of an entity set Ein a relationship set Ris said to be total if every\nentity in Emust participate in at least one relationship in R.I fi ti sp o s s i b l et h a ts o m e\nentities in Edo not participate in relationships in R, the participation of entity set Ein\nrelationship Ris said to be partial .\nFor example, a university may require every student to have at least one advisor;\nin the E-Rmodel, this corresponds to requiring each entity to be related to at least\none instructor through the advisor relationship. Therefore, the participation of student\nin the relationship set advisor is total. In contrast, an instructor need not advise any\nstudents. Hence, it is possible that only some of the instructor entities are related to the\nstudent entity set through the advisor relationship, and the participation of instructor in\ntheadvisor relationship set is therefore partial.\nWe indicate total participation of an entity in a relationship set using double lines.\nFigure 6.12 shows an example of the advisor relationship set where the double line\nindicates that a student must have an advisor.\nE-Rdiagrams also provide a way to indicate more complex constraints on the num-\nber of times each entity participates in relationships in a relationship set. A line may\nhave an associated minimum and maximum cardinality, shown in the form l..h,w h e r e l\ninstructor\nID\nname\nsalarystudent\nID\nname\ntot_credadvisor\nFigure 6.12 E-Rdiagram showing total participation.\n", "284": "256 Chapter 6 Database Design Using the E-R Model\ninstructor\nID\nname\nsalarystudent\nID\nname\ntot_credadvisor1..1 0..*\nFigure 6.13 Cardinality limits on relationship sets.\nis the minimum and hthe maximum cardinality. A minimum value of 1 indicates total\nparticipation of the entity set in the relationship set; that is, each entity in the entity\nset occurs in at least one relationship in that relationship set. A maximum value of\n1 indicates that the entity participates in at most one relationship, while a maximum\nvalue\u2217indicates no limit.\nFor example, consider Figure 6.13. The line between advisor andstudent has a car-\ndinality constraint of 1 ..1, meaning the minimum and the maximum cardinality are\nboth 1. That is, each student must have exactly one advisor. The limit 0 ..\u2217on the\nline between advisor andinstructor indicates that an instructor can have zero or more\nstudents. Thus, the relationship advisor is one-to-many from instructor tostudent ,a n d\nfurther the participation of student inadvisor is total, implying that a student must have\nan advisor.\nIt is easy to misinterpret the 0 ..\u2217on the left edge and think that the relationship ad-\nvisor is many-to-one from instructor tostudent \u2014this is exactly the reverse of the correct\ninterpretation.\nIf both edges have a maximum value of 1, the relationship is one-to-one. If we had\nspeci\ufb01ed a cardinality limit of 1 ..\u2217on the left edge, we would be saying that each\ninstructor must advise at least one student.\nThe E-Rdiagram in Figure 6.13 could alternatively have been drawn with a double\nline from student toadvisor , and an arrow on the line from advisor toinstructor ,i np l a c e\nof the cardinality constraints shown. This alternative diagram would enforce exactly the\nsame constraints as the constraints shown in the \ufb01gure.\nIn the case of nonbinary relationship sets, we can specify some types of many-to-\none relationships. Suppose a student can have at most one instructor as a guide on a\nproject. This constraint can be speci\ufb01ed by an arrow pointing to instructor on the edge\nfrom proj\nguide .\nWe permit at most one arrow out of a nonbinary relationship set, since an E-R\ndiagram with two or more arrows out of a nonbinary relationship set can be interpreted\nin two ways. We elaborate on this issue in Section 6.5.2.\n6.5 Primary Key\nWe must have a way to specify how entities within a given entity set and relationships\nwithin a given relationship set are distinguished.\n", "285": "6.5 Primary Key 257\n6.5.1 Entity Sets\nConceptually, individual entities are distinct; from a database perspective, however, the\ndi\ufb00erences among them must be expressed in terms of their attributes.\nTherefore, the values of the attribute val ues of an entity must be such that they can\nuniquely identify the entity. In other words, no two entities in an entity set are allowed\nto have exactly the same value for all attributes.\nThe notion of a keyfor a relation schema, as de\ufb01ned in Section 2.3, applies directly\nto entity sets. That is, a key for an entity is a set of attributes that su\ufb03ce to distinguish\nentities from each other. The concepts of superkey, candidate key, and primary key are\napplicable to entity sets just as they are applicable to relation schemas.\nKeys also help to identify relationships uniquely, and thus distinguish relationships\nfrom each other. Next, we de\ufb01ne the corresponding notions of keys for relationship sets.\n6.5.2 Relationship Sets\nWe need a mechanism to distinguish among the various relationships of a relationship\nset.\nLetRbe a relationship set involving entity sets E1,E2,\u2026,En.L e t primary-key (Ei)\ndenote the set of attributes that forms the primary key for entity set Ei. Assume for\nnow that the attribute names of all primary keys are unique. The composition of the\nprimary key for a relationship set depends on the set of attributes associated with the\nrelationship set R.\nIf the relationship set Rhas no attributes associated with it, then the set of attributes\nprimary-key (E1)\u222aprimary-key (E2)\u222a\u22ef\u222aprimary-key (En)\ndescribes an individual relationship in set R.\nIf the relationship set Rhas attributes a1,a2,\u2026,amassociated with it, then the set\nof attributes\nprimary-key (E1)\u222aprimary-key (E2)\u222a\u22ef\u222aprimary-key (En)\u222a{a1,a2,\u2026,am}\ndescribes an individual relationship in set R.\nIf the attribute names of primary keys are not unique across entity sets, the at-\ntributes are renamed to distinguish them; the name of the entity set combined with\nthe name of the attribute would form a unique name. If an entity set participates more\nthan once in a relationship set (as in the prereq relationship in Section 6.2.2), the role\nname is used instead of the name of the entity set, to form a unique attribute name.\nRecall that a relationship set is a set of relationship instances, and each instance is\nuniquely identi\ufb01ed by the entities that participate in it. Thus, in both of the preceding\ncases, the set of attributes\nprimary-key (E1)\u222aprimary-key (E2)\u222a\u22ef\u222aprimary-key (En)\nforms a superkey for the relationship set.\n", "286": "258 Chapter 6 Database Design Using the E-R Model\nThe choice of the primary key for a binary relationship set depends on the map-\nping cardinality of the relationship set. For many-to-many relationships, the preceding\nunion of the primary keys is a minimal superkey and is chosen as the primary key.\nAs an illustration, consider the entity sets instructor andstudent , and the relationship\nsetadvisor , in Section 6.2.2. Suppose that the relationship set is many-to-many. Then\nthe primary key of advisor consists of the union of the primary keys of instructor and\nstudent .\nFor one-to-many and many-to-one relationships, the primary key of the \u201cmany\u201d side\nis a minimal superkey and is used as the primary key. For example, if the relationship\nis many-to-one from student toinstructor \u2014that is, each student can have at most one\nadvisor\u2014then the primary key of advisor is simply the primary key of student .H o w e v e r ,\nif an instructor can advise only one student\u2014that is, if the advisor relationship is many-\nto-one from instructor tostudent \u2014then the primary key of advisor is simply the primary\nkey of instructor .\nFor one-to-one relationships, the primary key of either one of the participating\nentity sets forms a minimal superkey, and either one can be chosen as the primary\nkey of the relationship set. However, if an instructor can advise only one student, and\neach student can be advised by only one instructor\u2014that is, if the advisor relationship\nis one-to-one\u2014then the primary key of either student orinstructor c a nb ec h o s e na st h e\nprimary key for advisor .\nFor nonbinary relationships, if no cardinality constraints are present, then the su-\nperkey formed as described earlier in this section is the only candidate key, and it is\nchosen as the primary key. The choice of the primary key is more complicated if cardi-\nnality constraints are present. As we noted in Section 6.4, we permit at most one arrow\nout of a relationship set. We do so because an E-Rdiagram with two or more arrows out\nof a nonbinary relationship set can be interpreted in the two ways we describe below.\nSuppose there is a relationship set Rbetween entity sets E1,E2,E3,E4, and the only\narrows are on the edges to entity sets E3andE4. Then, the two possible interpretations\nare:\n1.A particular combination of entities from E1,E2can be associated with at most\none combination of entities from E3,E4. Thus, the primary key for the relation-\nship Rcan be constructed by the union of the primary keys of E1andE2.\n2.A particular combination of entities from E1,E2,E3can be associated with at\nmost one combination of entities from E4, and further a particular combination\nof entities from E1,E2,E4can be associated with at most one combination of\nentities from E3, Then the union of the primary keys of E1,E2,a n d E3forms a\ncandidate key, as does the union of the primary keys of E1,E2,a n d E4.\nEach of these interpretations has been used in practice and both are correct for particu-\nlar enterprises being modeled. Thus, to avoid confusion, we permit only one arrow out\nof a nonbinary relationship set, in which case the two interpretations are equivalent.\n", "287": "6.5 Primary Key 259\nIn order to represent a situation where one of the multiple-arrow situations holds,\ntheE-Rdesign can be modi\ufb01ed by replacing the non-binary relationship set with an\nentity set. That is, we treat each instance of the non-binary relationship set as an entity.\nThen we can relate each of those entities to corresponding instances of E1,E2,E4via\nseparate relationship sets. A simpler approach is to use functional dependencies ,w h i c h\nwe study in Chapter 7 (Section 7.4). Functional dependencies which allow either of\nthese interpretations to be speci\ufb01ed simply in an unambiguous manner.\nT h ep r i m a r yk e yf o rt h er e l a t i o n s h i ps e t Ris then the union of the primary keys of\nthose participating entity sets Eithat do not have an incoming arrow from the relation-\nship set R.\n6.5.3 Weak Entity Sets\nConsider a section entity, which is uniquely identi\ufb01ed by a course identi\ufb01er, semester,\nyear, and section identi\ufb01er. Section entiti es are related to course entities. Suppose we\ncreate a relationship set sec\ncourse between entity sets section andcourse .\nNow, observe that the information in sec\ncourse is redundant, since section already\nhas an attribute course\n id, which identi\ufb01es the course with which the section is related.\nOne option to deal with this redundancy is to get rid of the relationship sec\ncourse ;\nhowever, by doing so the relationship between section andcourse becomes implicit in\nan attribute, which is not desirable.\nAn alternative way to deal with this redundancy is to not store the attribute course\nidin the section entity and to only store the remaining attributes sec\nid,year,a n d\nsemester .2However, the entity set section then does not have enough attributes to iden-\ntify a particular section entity uniquely; although each section entity is distinct, sections\nfor di\ufb00erent courses may share the same sec\nid,year,a n d semester .T od e a lw i t ht h i s\nproblem, we treat the relationship sec\ncourse as a special relationship that provides extra\ninformation, in this case the course\n id,r e q u i r e dt oi d e n t i f y section entities uniquely.\nThe notion of weak entity set formalizes the above intuition. A weak entity set is\none whose existence is dependent on another entity set, called its identifying entity set ;\ninstead of associating a primary key with a weak entity, we use the primary key of the\nidentifying entity, along with extra attributes, called discriminator attributes to uniquely\nidentify a weak entity. An entity set that is not a weak entity set is termed a strong entity\nset.\nEvery weak entity must be associated with an identifying entity; that is, the weak\nentity set is said to be existence dependent on the identifying entity set. The identifying\nentity set is said to ownthe weak entity set that it identi\ufb01es. The relationship associating\nthe weak entity set with the identifying entity set is called the identifying relationship .\nThe identifying relationship is many-to-one from the weak entity set to the identi-\nfying entity set, and the participation of the weak entity set in the relationship is total.\n2Note that the relational schema we eventually create from the entity set section does have the attribute course\n id,f o r\nreasons that will become clear later, even though we have dropped the attribute course\n idfrom the entity set section .\n", "288": "260 Chapter 6 Database Design Using the E-R Model\nThe identifying relationship set should not have any descriptive attributes, since any\nsuch attributes can instead be associated with the weak entity set.\nIn our example, the identifying entity set for section iscourse , and the relationship\nsec\ncourse , which associates section entities with their corresponding course entities, is\nthe identifying relationship. The primary key of section is formed by the primary key\nof the identifying entity set (that is, course ), plus the discriminator of the weak entity\nset (that is, section ). Thus, the primary key is {course\n id,sec\nid,year,semester }.\nNote that we could have chosen to make sec\nidglobally unique across all courses\no\ufb00ered in the university, in which case the section entity set would have had a primary\nkey. However, conceptually, a section is still dependent on a course for its existence,\nwhich is made explicit by making it a weak entity set.\nInE-Rdiagrams, a weak entity set is depicted via a double rectangle with the dis-\ncriminator being underlined with a dashed line. The relationship set connecting the\nweak entity set to the identifying strong entity set is depicted by a double diamond. In\nFigure 6.14, the weak entity set section depends on the strong entity set course via the\nrelationship set sec\ncourse .\nThe \ufb01gure also illustrates the use of double lines to indicate that the participation\nof the (weak) entity set section in the relationship sec\ncourse istotal, meaning that every\nsection must be related via sec\ncourse to some course. Finally, the arrow from sec\ncourse\ntocourse indicates that each section is related to a single course.\nIn general, a weak entity set must have a total participation in its identifying rela-\ntionship set, and the relationship is many-to-one toward the identifying entity set.\nA weak entity set can participate in relationships other than the identifying rela-\ntionship. For instance, the section entity could participate in a relationship with the\ntime\n slotentity set, identifying the time when a particular class section meets. A weak\nentity set may participate as owner in an identifying relationship with another weak en-\ntity set. It is also possible to have a weak entity set with more than one identifying entity\nset. A particular weak entity would then be identi\ufb01ed by a combination of entities, one\nfrom each identifying entity set. The primary key of the weak entity set would consist\nof the union of the primary keys of the identifying entity sets, plus the discriminator\nof the weak entity set.\ncourse\ncourse_id\ntitle\ncreditssection\nsec_id\nsemester\nyearsec_course\nFigure 6.14 E-Rdiagram with a weak entity set.\n", "289": "6.6 Removing Redundant Attributes in Entity Sets 261\n6.6 Removing Redundant Attributes in Entity Sets\nWhen we design a database using the E-Rmodel, we usually start by identifying those\nentity sets that should be included. For example, in the university organization we\nhave discussed thus far, we decided to include such entity sets as student andinstruc-\ntor. Once the entity sets are decided upon, we must choose the appropriate attributes.\nThese attributes are supposed to represent the various values we want to capture in the\ndatabase. In the university organization, we decided that for the instructor entity set, we\nwill include the attributes ID,name ,dept\nname ,a n d salary . We could have added the\nattributes phone\n number ,o\ufb03ce\n number ,home\n page, and others. The choice of what at-\ntributes to include is up to the designer, who has a good understanding of the structure\nof the enterprise.\nOnce the entities and their corresponding attributes are chosen, the relationship\nsets among the various entities are formed. These relationship sets may result in a situ-\nation where attributes in the various entity sets are redundant and need to be removed\nfrom the original entity sets. To illustrate, consider the entity sets instructor anddepart-\nment :\n\u2022The entity set instructor includes the attributes ID,name ,dept\nname ,a n d salary ,\nwith IDforming the primary key.\n\u2022The entity set department includes the attributes dept\nname ,building ,a n d budget ,\nwith dept\nname forming the primary key.\nWe model the fact that each instructor has an associated department using a relation-\nship set inst\ndeptrelating instructor anddepartment .\nThe attribute dept\nname appears in both entity sets. Since it is the primary key for\nthe entity set department , it is redundant in the entity set instructor and needs to be\nremoved.\nRemoving the attribute dept\nname from the instructor entity set may appear rather\nunintuitive, since the relation instructor that we used in the earlier chapters had an\nattribute dept\nname . As we shall see later, when we create a relational schema from the\nE-Rdiagram, the attribute dept\nname in fact gets added to the relation instructor ,b u t\nonly if each instructor has at most one associated department. If an instructor has more\nthan one associated department, the relationship between instructors and departments\nis recorded in a separate relation inst\ndept.\nTreating the connection between instructors and departments uniformly as a rela-\ntionship, rather than as an attribute of instructor , makes the logical relationship explicit,\nand it helps avoid a premature assumption that each instructor is associated with only\none department.\nSimilarly, the student entity set is related to the department entity set through the\nrelationship set student\n deptand thus there is no need for a dept\nname attribute in stu-\ndent.\n", "290": "262 Chapter 6 Database Design Using the E-R Model\nAs another example, consider course o\ufb00eri ngs (sections) along with the time slots\nof the o\ufb00erings. Each time slot is identi\ufb01ed by a time\n slot\nid, and has associated with\nit a set of weekly meetings, each identi\ufb01ed by a day of the week, start time, and end\ntime. We decide to model the set of weekly meeting times as a multivalued composite\nattribute. Suppose we model entity sets section andtime\n slotas follows:\n\u2022The entity set section includes the attributes course\n id,sec\nid,semester ,year,build-\ning,room\n number ,a n d time\n slot\nid,w i t h( course\n id,sec\nid,year,semester )f o r m i n g\nthe primary key.\n\u2022The entity set time\n slotincludes the attributes time\n slot\nid,w h i c hi st h ep r i m a r y\nkey,3and a multivalued composite attribute {(day,start\n time,end\ntime)}.4\nThese entities are related through the relationship set sec\ntime\n slot.\nThe attribute time\n slot\nidappears in both entity sets. Since it is the primary key for\nthe entity set time\n slot, it is redundant in the entity set section and needs to be removed.\nAs a \ufb01nal example, suppose we have an entity set classroom , with attributes building ,\nroom\n number ,a n d capacity ,w i t h building androom\n number forming the primary key.\nSuppose also that we have a relationship set sec\nclass that relates section toclassroom .\nThen the attributes {building ,room\n number }a r er e d u n d a n ti nt h ee n t i t ys e t section .\nA good entity-relationship design does not contain redundant attributes. For our\nuniversity example, we list the entity sets and their attributes below, with primary keys\nunderlined:\n\u2022classroom : with attributes ( building\n ,room\n number\n ,capacity ).\n\u2022department : with attributes ( dept\nname\n ,building ,budget ).\n\u2022course :w i t ha t t r i b u t e s( course\n id\n,title,credits ).\n\u2022instructor : with attributes ( ID\n,name ,salary ).\n\u2022section : with attributes ( course\n id\n,sec\nid\n,semester\n ,year\n).\n\u2022student : with attributes ( ID\n,name ,tot\ncred).\n\u2022time\n slot:w i t ha t t r i b u t e s( time\n slot\nid\n,{(day,start\n time,end\ntime)}).\nThe relationship sets in our design are listed below:\n\u2022inst\ndept: relating instructors with departments.\n\u2022stud\n dept: relating students with departments.\n3We shall see later on that the primary key for the relation created from the entity set time\n slotincludes dayandstart\ntime;h o w e v e r , dayandstart\n time do not form part of the primary key of the entity set time\n slot.\n4We could optionally give a name, such as meeting , for the composite attribute containing day,start\n time,a n d end\ntime.\n", "291": "6.6 Removing Redundant Attributes in Entity Sets 263\n\u2022teaches : relating instructors with sections.\n\u2022takes : relating students with sections, with a descriptive attribute grade .\n\u2022course\n dept: relating courses with departments.\n\u2022sec\ncourse : relating sections with courses.\n\u2022sec\nclass: relating sections with classrooms.\n\u2022sec\ntime\n slot: relating sections with time slots.\n\u2022advisor : relating students with instructors.\n\u2022prereq : relating courses with prerequisite courses.\nYou can verify that none of the entity sets has any attribute that is made redundant\nby one of the relationship sets. Further, you can verify that all the information (other\nthan constraints) in the relational schema for our university database, which we saw\nearlier in Figure 2.9, has been captured by the above design, but with several attributes\nin the relational design replaced by relationships in the E-Rdesign.\nWe are \ufb01nally in a position to show (Figure 6.15) the E-Rdiagram that corresponds\nto the university enterprise that we have been using thus far in the text. This E-Rdiagram\nis equivalent to the textual description of the university E-Rmodel, but with several\nadditional constraints.\nIn our university database, we have a constraint that each instructor must have\nexactly one associated department. As a result, there is a double line in Figure 6.15\nbetween instructor andinst\ndept, indicating total participation of instructor ininst\ndept;\nthat is, each instructor must be associated with a department. Further, there is an ar-\nrow from inst\ndepttodepartment , indicating that each instructor can have at most one\nassociated department.\nSimilarly, entity set course has a double line to relationship set course\n dept,i n d i c a t -\ning that every course must be in some department, and entity set student has a double\nline to relationship set stud\ndept, indicating that every student must be majoring in some\ndepartment. In each case, an arrow points to the entity set department to show that a\ncourse (and, respectively, a student) can be related to only one department, not several.\nSimilarly, entity set course has a double line to relationship set course\n dept,i n d i c a t -\ning that every course must be in some department, and entity set student has a double\nline to relationship set stud\ndept, indicating that every student must be majoring in some\ndepartment. In each case, an arrow points to the entity set department to show that a\ncourse (and, respectively, a student) can be related to only one department, not several.\nFurther, Figure 6.15 shows that the relationship set takes has a descriptive attribute\ngrade , and that each student has at most one advisor. The \ufb01gure also shows that section\nis a weak entity set, with attributes sec\nid,semester ,a n d yearforming the discriminator;\nsec\ncourse is the identifying relationship set relating weak entity set section to the strong\nentity set course .\n", "292": "264 Chapter 6 Database Design Using the E-R Model\ntime_slot coursestudent\nID\nname\nsalaryID\nname\ntot_cred\ncourse_id\ntitle\ncreditstime_slot_id\n{  day\n   start_time\n   end_time\n}\ncourse_id prereq_idadvisor\nteaches takes\nsec_course sec_time_slotgrade\nprereqinst_dept stud_dept\ninstructordepartment\ndept_name\nbuilding\nbudget\nsection\nsec_id\nsemester\nyearcourse_dept\nsec_class\nclassroom\nbuilding\nroom_number\ncapacity\nFigure 6.15 E-Rdiagram for a university enterprise.\nIn Section 6.7, we show how this E-Rdiagram can be used to derive the various\nrelation schemas we use.\n6.7 Reducing E-R Diagrams to Relational Schemas\nBoth the E-Rmodel and the relational database model are abstract, logical representa-\ntions of real-world enterprises. Because the two models employ similar design princi-\nples, we can convert an E-Rdesign into a relational design. For each entity set and for\neach relationship set in the database design, there is a unique relation schema to which\nwe assign the name of the corresponding entity set or relationship set.\n", "293": "6.7 Reducing E-R Diagrams to Relational Schemas 265\nIn this section, we describe how an E-Rschema can be represented by relation\nschemas and how constraints arising from the E-Rdesign can be mapped to constraints\non relation schemas.\n6.7.1 Representation of Strong Entity Sets\nLetEbe a strong entity set with only simple descriptive attributes a1,a2,\u2026,an.W e\nrepresent this entity with a schema called Ewith ndistinct attributes. Each tuple in a\nrelation on this schema corresponds to one entity of the entity set E.\nFor schemas derived from strong entity sets, the primary key of the entity set serves\nas the primary key of the resulting schema. This follows directly from the fact that each\ntuple corresponds to a speci\ufb01c entity in the entity set.\nAs an illustration, consider the entity set student of the E-Rdiagram in Figure 6.15.\nThis entity set has three attributes: ID,name ,tot\ncred. We represent this entity set by a\nschema called student with three attributes:\nstudent (ID\n,name ,tot\ncred)\nNote that since student IDis the primary key of the entity set, it is also the primary key\nof the relation schema.\nContinuing with our example, for the E-Rdiagram in Figure 6.15, all the strong\nentity sets, except time\n slot, have only simple attributes. The schemas derived from\nthese strong entity sets are depicted in Figure 6.16. Note that the instructor ,student ,a n d\ncourse schemas are di\ufb00erent from the schemas we have used in the previous chapters\n(they do not contain the attribute dept\nname ). We shall revisit this issue shortly.\n6.7.2 Representation of Strong Entity Sets with Complex Attributes\nWhen a strong entity set has nonsimple attributes, things are a bit more complex. We\nhandle composite attributes by creating a separate attribute for each of the component\nattributes; we do not create a separate attribute for the composite attribute itself. To\nillustrate, consider the version of the instructor entity set depicted in Figure 6.8. For the\ncomposite attribute name , the schema generated for instructor contains the attributes\nclassroom (building\n ,room\n number\n ,capacity )\ndepartment (dept\nname\n ,building ,budget )\ncourse (course\n id\n,title,credits )\ninstructor (ID\n,name ,salary )\nstudent (ID\n,name ,tot\ncred)\nFigure 6.16 Schemas derived from the entity sets in the E-Rdiagram in Figure 6.15.\n", "294": "266 Chapter 6 Database Design Using the E-R Model\n\ufb01rst\nname ,middle\n initial ,a n d last\nname ; there is no separate attribute or schema for\nname . Similarly, for the composite attribute address , the schema generated contains\nthe attributes street ,city,state,a n d postal\n code.S i n c e street is a composite attribute it is\nreplaced by street\n number ,street\n name ,a n d apt\nnumber .\nMultivalued attributes are treated di\ufb00erently from other attributes. We have seen\nthat attributes in an E-Rdiagram generally map directly into attributes for the appropri-\nate relation schemas. Multivalued attributes, however, are an exception; new relation\nschemas are created for these attributes, as we shall see shortly.\nDerived attributes are not explicitly represented in the relational data model. How-\never, they can be represented as stored procedures, functions, or methods in other data\nmodels.\nThe relational schema derived from the version of entity set instructor with complex\nattributes, without including the multivalued attribute, is thus:\ninstructor (ID,\ufb01rst\nname ,middle\n initial ,last\nname ,\nstreet\n number ,street\n name ,apt\nnumber ,\ncity,state,postal\n code,date\n of\nbirth)\nFor a multivalued attribute M,w ec r e a t ear e l a t i o ns c h e m a Rwith an attribute A\nthat corresponds to Mand attributes corresponding to the primary key of the entity\nset or relationship set of which Mis an attribute.\nAs an illustration, consider the E-Rdiagram in Figure 6.8 that depicts the entity set\ninstructor , which includes the multivalued attribute phone\n number .T h ep r i m a r yk e yo f\ninstructor isID. For this multivalued attribute, we create a relation schema\ninstructor\n phone (ID\n,phone\n number\n )\nEach phone number of an instructor is represented as a unique tuple in the relation on\nthis schema. Thus, if we had an instructor with ID22222, and phone numbers 555-1234\nand 555-4321, the relation instructor\n phone would have two tuples (22222, 555-1234)\nand (22222, 555-4321).\nWe create a primary key of the relation schema consisting of all attributes of the\nschema. In the above example, the primary key consists of both attributes of the relation\nschema instructor\n phone .\nIn addition, we create a foreign-key constraint on the relation schema created from\nthe multivalued attribute. In that newly created schema, the attribute generated from\nthe primary key of the entity set must reference the relation generated from the entity\nset. In the above example, the foreign-key constraint on the instructor\n phone relation\nwould be that attribute IDreferences the instructor relation.\nIn the case that an entity set consists of only two attributes\u2014a single primary-key\nattribute Band a single multivalued attribute M\u2014 the relation schema for the entity\nset would contain only one attribute, namely, the primary-key attribute B.W ec a nd r o p\n", "295": "6.7 Reducing E-R Diagrams to Relational Schemas 267\nthis relation, while retaining the relation schema with the attribute Band attribute A\nthat corresponds to M.\nTo illustrate, consider the entity set time\n slotdepicted in Figure 6.15. Here, time\nslot\nidis the primary key of the time\n slotentity set, and there is a single multivalued\nattribute that happens also to be composite. The entity set can be represented by just\nthe following schema created from the multivalued composite attribute:\ntime\n slot(time\n slot\nid\n,day\n,start\n time\n,end\ntime)\nAlthough not represented as a constraint on the E-Rdiagram, we know that there can-\nnot be two meetings of a class that start at the same time of the same day of the week\nbut end at di\ufb00erent times; based on this constraint, end\ntimehas been omitted from the\nprimary key of the time\n slotschema.\nThe relation created from the entity set would have only a single attribute time\nslot\nid; the optimization of dropping this relation has the bene\ufb01t of simplifying the\nresultant database schema, although it has a drawback related to foreign keys, which\nwe brie\ufb02y discuss in Section 6.7.4.\n6.7.3 Representation of Weak Entity Sets\nLetAbe a weak entity set with attributes a1,a2,\u2026,am.L e t Bbe the strong entity set\non which Adepends. Let the primary key of Bconsist of attributes b1,b2,\u2026,bn.W e\nrepresent the entity set Aby a relation schema called Awith one attribute for each\nmember of the set:\n{a1,a2,\u2026,am}\u222a{b1,b2,\u2026,bn}\nFor schemas derived from a weak entity set, the combination of the primary key of\nthe strong entity set and the discriminator of the weak entity set serves as the primary\nkey of the schema. In addition to creating a primary key, we also create a foreign-key\nconstraint on the relation A, specifying that the attributes b1,b2,\u2026,bnreference the\nprimary key of the relation B. The foreign-key constraint ensures that for each tuple rep-\nresenting a weak entity, there is a corresponding tuple representing the corresponding\nstrong entity.\nAs an illustration, consider the weak entity set section in the E-Rdiagram of Figure\n6.15. This entity set has the attributes: sec\nid,semester ,a n d year.T h ep r i m a r yk e yo f\nthecourse entity set, on which section depends, is course\n id. Thus, we represent section\nby a schema with the following attributes:\nsection (course\n id\n,sec\nid\n,semester\n ,year\n)\nThe primary key consists of the primary key of the entity set course , along with the\ndiscriminator of section ,w h i c hi s sec\nid,semester ,a n d year.W ea l s oc r e a t eaf o r e i g n - k e y\n", "296": "268 Chapter 6 Database Design Using the E-R Model\nconstraint on the section schema, with the attribute course\n idreferencing the primary\nkey of the course schema.5\n6.7.4 Representation of Relationship Sets\nLetRbe a relationship set, let a1,a2,\u2026,ambe the set of attributes formed by the union\nof the primary keys of each of the entity sets participating in R, and let the descriptive\nattributes (if any) of Rbeb1,b2,\u2026,bn. We represent this relationship set by a relation\nschema called Rwith one attribute for each member of the set:\n{a1,a2,\u2026,am}\u222a{b1,b2,\u2026,bn}\nWe described in Section 6.5, how to choose a primary key for a binary relationship\nset. The primary key attributes of the relationship set are also used as the primary key\nattributes of the relational schema R.\nAs an illustration, consider the relationship set advisor in the E-Rdiagram of Figure\n6.15. This relationship set involves the following entity sets:\n\u2022instructor , with the primary key ID.\n\u2022student , with the primary key ID.\nSince the relationship set has no attributes, the advisor schema has two attributes, the\nprimary keys of instructor andstudent . Since both attributes have the same name, we re-\nname them i\nIDand s\n ID.S i n c et h e advisor relationship set is many-to-one from student\ntoinstructor the primary key for the advisor relation schema is s\nID.\nWe also create foreign-key constraints on the relation schema Ras follows: For each\nentity set Eirelated by relationship set R, we create a foreign-key constraint from rela-\ntion schema R, with the attributes of Rthat were derived from primary-key attributes\nofEireferencing the primary key of the relation schema representing Ei.\nReturning to our earlier example, we thus create two foreign-key constraints on\ntheadvisor relation, with attribute i\nIDreferencing the primary key of instructor and\nattribute s\nIDreferencing the primary key of student .\nApplying the preceding techniques to the other relationship sets in the E-Rdiagram\nin Figure 6.15, we get the relational schemas depicted in Figure 6.17.\nObserve that for the case of the relationship set prereq , the role indicators asso-\nciated with the relationship are used as attribute names, since both roles refer to the\nsame relation course .\nSimilar to the case of advisor , the primary key for each of the relations sec\ncourse ,\nsec\ntime\n slot,sec\nclass,inst\ndept,stud\ndept,a n d course\n dept consists of the primary key\n5Optionally, the foreign-key constraint could have an \u201c on delete cascade\u201d speci\ufb01cation, so that deletion of a course\nentity automatically deletes any section entities that reference the course entity. Without that speci\ufb01cation, each section\nof a course would have to be deleted before the corresponding course can be deleted.\n", "297": "6.7 Reducing E-R Diagrams to Relational Schemas 269\nteaches (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n)\ntakes (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n,grade )\nprereq (course\n id\n,prereq\n id\n)\nadvisor (s\nID\n,i\nID)\nsec\ncourse (course\n id\n,sec\nid\n,semester\n ,year\n)\nsec\ntime\n slot(course\n id\n,sec\nid\n,semester\n ,year\n,time\n slot\nid)\nsec\nclass (course\n id\n,sec\nid\n,semester\n ,year\n,building ,room\n number )\ninst\ndept(ID\n,dept\nname )\nstud\ndept(ID\n,dept\nname )\ncourse\n dept(course\n id\n,dept\nname )\nFigure 6.17 Schemas derived from relationship sets in the E-Rdiagram in Figure 6.15.\nof only one of the two related entity sets, since each of the corresponding relationships\nis many-to-one.\nForeign keys are not shown in Figure 6.17, but for each of the relations in the\n\ufb01gure there are two foreign-key constraints, referencing the two relations created from\nthe two related entity sets. Thus, for example, sec\ncourse has foreign keys referencing\nsection andclassroom ,teaches has foreign keys referencing instructor andsection ,a n d\ntakes has foreign keys referencing student andsection .\nThe optimization that allowed us to create only a single relation schema from the\nentity set time\n slot, which had a multivalued attribute, prevents the creation of a foreign\nkey from the relation schema sec\ntime\n slotto the relation created from entity set time\nslot, since we dropped the relation created from the entity set time\n slot. We retained the\nrelation created from the multivalued attribute and named it time\n slot, but this relation\nmay potentially have no tuples corresponding to a time\n slot\nid, or it may have multiple\ntuples corresponding to a time\n slot\nid;t h u s , time\n slot\nidinsec\ntime\n slotcannot reference\nthis relation.\nThe astute reader may wonder why we have not seen the schemas sec\ncourse ,sec\ntime\n slot,sec\nclass,inst\ndept,stud\ndept,a n d course\n dept in the previous chapters. The\nreason is that the algorithm we have presented thus far results in some schemas that\ncan be either eliminated or combined with other schemas. We explore this issue next.\n6.7.5 Redundancy of Schemas\nA relationship set linking a weak entity set to the corresponding strong entity set is\ntreated specially. As we noted in Section 6.5.3, these relationships are many-to-one and\nhave no descriptive attributes. Furthermore, the of a weak entity set includes the pri-\nmary key of the strong entity set. In the E-Rd i a g r a mo fF i g u r e6 . 1 4 ,t h ew e a ke n t i t ys e t\nsection is dependent on the strong entity set course via the relationship set sec\ncourse .\n", "298": "270 Chapter 6 Database Design Using the E-R Model\nThe primary key of section is{course\n id,sec\nid,semester ,year },a n dt h ep r i m a r yk e yo f\ncourse iscourse\n id.S i n c e sec\ncourse has no descriptive attributes, the sec\ncourse schema\nhas attributes course\n id,sec\nid,semester ,a n d year. The schema for the entity set sec-\ntionincludes the attributes course\n id,sec\nid,semester ,a n d year (among others). Every\n(course\n id,sec\nid,semester ,year) combination in a sec\ncourse relation would also be\npresent in the relation on schema section ,a n dv i c ev e r s a .T h u s ,t h e sec\ncourse schema\nis redundant.\nIn general, the schema for the relationship set linking a weak entity set to its corre-\nsponding strong entity set is redundant and does not need to be present in a relational\ndatabase design based upon an E-Rdiagram.\n6.7.6 Combination of Schemas\nConsider a many-to-one relationship set ABfrom entity set Ato entity set B.U s i n go u r\nrelational-schema construction algorithm outlined previously, we get three schemas: A,\nB,a n d AB. Suppose further that the participation of Ain the relationship is total; that\nis, every entity ain the entity set Amust participate in the relationship AB.T h e nw e\ncan combine the schemas AandABto form a single schema consisting of the union of\nattributes of both schemas. The primary key of the combined schema is the primary\nkey of the entity set into whose schema the relationship set schema was merged.\nTo illustrate, let\u2019s examine the various relations in the E-Rdiagram of Figure 6.15\nthat satisfy the preceding criteria:\n\u2022inst\ndept. The schemas instructor anddepartment correspond to the entity sets A\nandB, respectively. Thus, the schema inst\ndeptcan be combined with the instructor\nschema. The resulting instructor schema consists of the attributes {ID,name ,dept\nname ,salary }.\n\u2022stud\ndept. The schemas student anddepartment correspond to the entity sets A\nandB, respectively. Thus, the schema stud\ndeptcan be combined with the student\nschema. The resulting student schema consists of the attributes {ID,name ,dept\nname ,tot\ncred }.\n\u2022course\n dept. The schemas course anddepartment correspond to the entity sets A\nandB, respectively. Thus, the schema course\n deptc a nb ec o m b i n e dw i t ht h e course\nschema. The resulting course schema consists of the attributes {course\n id,title,dept\nname ,credits }.\n\u2022sec\nclass. The schemas section andclassroom correspond to the entity sets AandB,\nrespectively. Thus, the schema sec\nclass can be combined with the section schema.\nThe resulting section schema consists of the attributes {course\n id,sec\nid,semester ,\nyear,building ,room\n number }.\n\u2022sec\ntime\n slot. The schemas section andtime\n slotcorrespond to the entity sets Aand\nBrespectively, Thus, the schema sec\ntime\n slotc a nb ec o m b i n e dw i t ht h e section\n", "299": "6.8 Extended E-R Features 271\nschema obtained in the previous step. The resulting section schema consists of the\nattributes {course\n id,sec\nid,semester ,year,building ,room\n number ,time\n slot\nid}.\nIn the case of one-to-one relationships, the relation schema for the relationship set\ncan be combined with the schemas for either of the entity sets.\nWe can combine schemas even if the participation is partial by using null values.\nIn the preceding example, if inst\ndeptwere partial, then we would store null values for\nthedept\nname attribute for those instructors who have no associated department.\nFinally, we consider the foreign-key constraints that would have appeared in the\nschema representing the relationship set. There would have been foreign-key con-\nstraints referencing each of the entity sets participating in the relationship set. We drop\nthe constraint referencing the entity set into whose schema the relationship set schema\nis merged, and add the other foreign-key constraints to the combined schema. For ex-\nample, inst\ndepthas a foreign key constraint of the attribute dept\nname referencing the\ndepartment relation. This foreign constraint is enforced implicitly by the instructor re-\nlation when the schema for inst\ndeptis merged into instructor .\n6.8 Extended E-R Features\nAlthough the basic E-Rconcepts can model most database features, some aspects of\na database may be more aptly expressed by certain extensions to the basic E-Rmodel.\nIn this section, we discuss the extended E-Rfeatures of specialization, generalization,\nhigher- and lower-level entity sets, attribute inheritance, and aggregation.\nTo help with the discussions, we shall use a slightly more elaborate database\nschema for the university. In particular, we shall model the various people within a\nuniversity by de\ufb01ning an entity set person , with attributes ID,name ,street ,a n d city.\n6.8.1 Specialization\nAn entity set may include subgroupings of entities that are distinct in some way from\nother entities in the set. For instance, a subset of entities within an entity set may have\nattributes that are not shared by all the entities in the entity set. The E-Rmodel provides\na means for representing these distinctive entity groupings.\nAs an example, the entity set person may be further classi\ufb01ed as one of the follow-\ning:\n\u2022employee .\n\u2022student .\nEach of these person types is described by a set of attributes that includes all the at-\ntributes of entity set person plus possibly additional attributes. For example, employee\nentities may be described further by the attribute salary ,w h e r e a s student entities may\n", "300": "272 Chapter 6 Database Design Using the E-R Model\nbe described further by the attribute tot\ncred. The process of designating subgroupings\nwithin an entity set is called specialization . The specialization of person allows us to\ndistinguish among person entities according to whether they correspond to employees\nor students: in general, a person could be an employee, a student, both, or neither.\nAs another example, suppose the university divides students into two categories:\ngraduate and undergraduate. Graduate students have an o\ufb03ce assigned to them. Un-\ndergraduate students are assigned to a residential college. Each of these student types\nis described by a set of attributes that includes all the attributes of the entity set student\nplus additional attributes.\nWe can apply specialization repeatedly to re\ufb01ne a design. The university could\ncreate two specializations of student ,n a m e l y graduate andundergraduate .A sw es a w\nearlier, student entities are described by the attributes ID,name ,street, city ,a n d tot\ncred.T h ee n t i t ys e t graduate would have all the attributes of student and an additional\nattribute o\ufb03ce\n number .T h ee n t i t ys e t undergraduate would have all the attributes of\nstudent , and an additional attribute residential\n college . As another example, university\nemployees may be further classi\ufb01ed as one of instructor orsecretary .\nEach of these employee types is described by a set of attributes that includes all the\nattributes of entity set employee plus additional attributes. For example, instructor enti-\nties may be described further by the attribute rank while secretary entities are described\nby the attribute hours\n per\nweek .F u r t h e r , secretary entities may participate in a relation-\nship secretary\n forbetween the secretary andemployee entity sets, which identi\ufb01es the\nemployees who are assisted by a secretary.\nAn entity set may be specialized by more than one distinguishing feature. In our\nexample, the distinguishing feature among employee entities is the job the employee\nperforms. Another, coexistent, specialization could be based on whether the person is\na temporary (limited\n term) employee or a permanent employee, resulting in the entity\nsetstemporary\n employee andpermanent\n employee . When more than one specialization\nis formed on an entity set, a particular entity may belong to multiple specializations.\nFor instance, a given employee may be a temporary employee who is a secretary.\nIn terms of an E-Rdiagram, specialization is depicted by a hollow arrow-head point-\ning from the specialized entity to the other entity (see Figure 6.18). We refer to this re-\nlationship as the ISArelationship, which stands for \u201cis a\u201d and represents, for example,\nthat an instructor \u201cis a\u201d employee.\nThe way we depict specialization in an E-Rdiagram depends on whether an en-\ntity may belong to multiple specialized entity sets or if it must belong to at most one\nspecialized entity set. The former case (multiple sets permitted) is called overlapping\nspecialization , while the latter case (at most one permitted) is called disjoint special-\nization . For an overlapping specialization (as is the case for student andemployee as\nspecializations of person ), two separate arrows are used. For a disjoint specialization\n(as is the case for instructor andsecretary as specializations of employee ), a single arrow\nis used. The specialization relationship may also be referred to as a superclass-subclass\nrelationship. Higher- and lower-level entity sets are depicted as regular entity sets\u2014that\nis, as rectangles containing the name of the entity set.\n", "301": "6.8 Extended E-R Features 273\nperson\nID\nname\nstreet\ncity\nstudent\ninstructor\nranksecretary\nhours_ per_weekemployee\nsalary tot_credits\nFigure 6.18 Specialization and generalization.\n6.8.2 Generalization\nThe re\ufb01nement from an initial entity set into successive levels of entity subgroupings\nrepresents a top-down design process in which distinctions are made explicit. The design\nprocess may also proceed in a bottom-up manner, in which multiple entity sets are\nsynthesized into a higher-level entity set on the basis of common features. The database\ndesigner may have \ufb01rst identi\ufb01ed:\n\u2022instructor entity set with attributes instructor\n id,instructor\n name ,instructor\n salary ,\nandrank.\n\u2022secretary entity set with attributes secretary\n id,secretary\n name ,secretary\n salary ,a n d\nhours\n per\nweek .\nThere are similarities between the instructor entity set and the secretary entity set in\nthe sense that they have several attributes that are conceptually the same across the two\nentity sets: namely, the identi\ufb01er, name, and salary attributes. This commonality can be\nexpressed by generalization , which is a containment relationship that exists between a\nhigher-level entity set and one or more lower-level entity sets. In our example, employee is\nthe higher-level entity set and instructor andsecretary are lower-level entity sets. In this\ncase, attributes that are conceptually the same had di\ufb00erent names in the two lower-\nlevel entity sets. To create a generalization, the attributes must be given a common name\nand represented with the higher-level entity person . We can use the attribute names ID,\nname ,street ,a n d city, as we saw in the example in Section 6.8.1.\n", "302": "274 Chapter 6 Database Design Using the E-R Model\nHigher- and lower-level entity sets also may be designated by the terms superclass\nand subclass , respectively. The person entity set is the superclass of the employee and\nstudent subclasses.\nFor all practical purposes, generalization is a simple inversion of specialization.\nWe apply both processes, in combination, in the course of designing the E-Rschema\nfor an enterprise. In terms of the E-Rdiagram itself, we do not distinguish between\nspecialization and generalization. New levels of entity representation are distinguished\n(specialization) or synthesized (generalization) as the design schema comes to express\nfully the database application and the user requirements of the database. Di\ufb00erences\nin the two approaches may be characterized by their starting point and overall goal.\nSpecialization stems from a single entity set; it emphasizes di\ufb00erences among en-\ntities within the set by creating distinct lower-level entity sets. These lower-level entity\nsets may have attributes, or may participate in relationships, that do not apply to all\nthe entities in the higher-level entity set. Indeed, the reason a designer applies special-\nization is to represent such distinctive features. If student andemployee have exactly the\nsame attributes as person entities, and participate in exactly the same relationships as\nperson entities, there would be no need to specialize the person entity set.\nGeneralization proceeds from the recognition that a number of entity sets share\nsome common features (namely, they are described by the same attributes and partici-\npate in the same relationship sets). On the basis of their commonalities, generalization\nsynthesizes these entity sets into a single, higher-level entity set. Generalization is used\nto emphasize the similarities among lower-level entity sets and to hide the di\ufb00erences;\nit also permits an economy of representation in that shared attributes are not repeated.\n6.8.3 Attribute Inheritance\nA crucial property of the higher- and lower-level entities created by specialization and\ngeneralization is attribute inheritance . The attributes of the higher-level entity sets are\nsaid to be inherited by the lower-level entity sets. For example, student andemployee in-\nherit the attributes of person .T h u s , student is described by its ID,name ,street ,a n d city\nattributes, and additionally a tot\ncred attribute; employee is described by its ID,name ,\nstreet ,a n d cityattributes, and additionally a salary attribute. Attribute inheritance ap-\nplies through all tiers of lower-level entity sets; thus, instructor andsecretary ,w h i c ha r e\nsubclasses of employee , inherit the attributes ID,name ,street, andcityfrom person ,i n\naddition to inheriting salary from employee .\nA lower-level entity set (or subclass) also inherits participation in the relationship\nsets in which its higher-level entity (or superclass) participates. Like attribute inheri-\ntance, participation inheritance applies through all tiers of lower-level entity sets. For\nexample, suppose the person entity set participates in a relationship person\n dept with\ndepartment . Then, the student ,employee ,instructor andsecretary entity sets, which are\nsubclasses of the person entity set, also implicitly participate in the person\n deptrelation-\nship with department . These entity sets can participate in any relationships in which\ntheperson entity set participates.\n", "303": "6.8 Extended E-R Features 275\nWhether a given portion of an E-Rmodel was arrived at by specialization or gen-\neralization, the outcome is basically the same:\n\u2022A higher-level entity set with attributes and relationships that apply to all of its\nlower-level entity sets.\n\u2022Lower-level entity sets with distinctive features that apply only within a particular\nlower-level entity set.\nIn what follows, although we often refer to only generalization, the properties that\nwe discuss belong ful ly to both processes.\nFigure 6.18 depicts a hierarchy of entity sets. In the \ufb01gure, employee is a lower-level\nentity set of person and a higher-level entity set of the instructor andsecretary entity sets.\nIn a hierarchy, a given entity set may be involved as a lower-level entity set in only one\nISArelationship; that is, entity sets in this diagram have only single inheritance .I fa n\nentity set is a lower-level entity set in more than one ISArelationship, then the entity\nset has multiple inheritance , and the resulting structure is said to be a lattice .\n6.8.4 Constraints on Specializations\nTo model an enterprise more accurately, the database designer may choose to place\ncertain constraints on a particular generalization/specialization.\nOne type of constraint on specialization which we saw earlier speci\ufb01es whether\na specialization is disjoint or overlapping. Another type of constraint on a specializa-\ntion/generalization is a completeness constraint , which speci\ufb01es whether or not an en-\ntity in the higher-level entity set must belong to at least one of the lower-level entity sets\nwithin the generalization/specialization. This constraint may be one of the following:\n\u2022Total specialization orgeneralization . Each higher-level entity must belong to a\nlower-level entity set.\n\u2022Partial specialization orgeneralization . Some higher-level entities may not belong\nto any lower-level entity set.\nPartial specialization is the default. We can specify total specialization in an E-Rdia-\ngram by adding the keyword \u201ctotal\u201d in the diagram and drawing a dashed line from the\nkeyword to the corresponding hollow arrowhead to which it applies (for a total spe-\ncialization), or to the set of hollow arrowheads to which it applies (for an overlapping\nspecialization).\nThe specialization of person tostudent oremployee is total if the university does not\nneed to represent any person who is neither a student nor an employee .H o w e v e r ,i ft h e\nuniversity needs to represent such persons, then the specialization would be partial.\nThe completeness and disjointness constraints, do not depend on each other. Thus,\nspecializations may be partial-overlapping, partial-disjoint, total-overlapping, and total-\ndisjoint.\n", "304": "276 Chapter 6 Database Design Using the E-R Model\nWe can see that certain insertion and deletion requirements follow from the con-\nstraints that apply to a given generalization or specialization. For instance, when a total\ncompleteness constraint is in place, an entity inserted into a higher-level entity set must\nalso be inserted into at least one of the lower-level entity sets. An entity that is deleted\nfrom a higher-level entity set must also be deleted from all the associated lower-level\nentity sets to which it belongs.\n6.8.5 Aggregation\nOne limitation of the E-Rmodel is that it cannot express relationships among relation-\nships. To illustrate the need for such a construct, consider the ternary relationship proj\nguide , which we saw earlier, between an instructor ,student andproject (see Figure 6.6).\nNow suppose that each instructor guiding a student on a project is required to \ufb01le\na monthly evaluation report. We model the evaluation report as an entity evaluation ,\nwith a primary key evaluation\n id. One alternative for recording the ( student ,project ,\ninstructor ) combination to which an evaluation corresponds is to create a quaternary\n(4-way) relationship set eval\nforbetween instructor ,student ,project ,a n d evaluation .( A\nquaternary relationship is required\u2014a binary relationship between student andevalua-\ntion, for example, would not permit us to represent the ( project ,instructor ) combination\nto which an evaluation corresponds.) Using the basic E-Rmodeling constructs, we ob-\ntain the E-Rd i a g r a mo fF i g u r e6 . 1 9 .( W eh a v eo m i t t e dt h ea t t r i b u t e so ft h ee n t i t ys e t s ,\nfor simplicity.)\nIt appears that the relationship sets proj\nguide andeval\nforcan be combined into\none single relationship set. Nevertheless, we should not combine them into a single\nproject\nevaluationinstructor student\neval_  forproj_ guide\nFigure 6.19 E-Rdiagram with redundant relationships.\n", "305": "6.8 Extended E-R Features 277\nevaluationproj_ guideinstructor student\neval_  forproject\nFigure 6.20 E-Rdiagram with aggregation.\nrelationship, since some instructor ,student ,project combinations may not have an as-\nsociated evaluation .\nThere is redundant information in the resultant \ufb01gure, however, since every instruc-\ntor,student ,project combination in eval\nformust also be in proj\nguide .I fevaluation was\nmodeled as a value rather than an entity, we could instead make evaluation am u l t i -\nvalued composite attribute of the relationship set proj\nguide . However, this alternative\nmay not be an option if an evaluation may also be related to other entities; for example,\neach evaluation report may be associated with a secretary who is responsible for further\nprocessing of the evaluation report to make scholarship payments.\nThe best way to model a situation such as the one just described is to use aggrega-\ntion. Aggregation is an abstraction through which relationships are treated as higher-\nlevel entities. Thus, for our example, we regard the relationship set proj\nguide (relating\nthe entity sets instructor ,student ,a n d project ) as a higher-level entity set called proj\nguide . Such an entity set is treated in the same manner as is any other entity set. We\nc a nt h e nc r e a t eab i n a r yr e l a t i o n s h i p eval\nforbetween proj\nguide andevaluation to rep-\nresent which ( student ,project ,instructor ) combination an evaluation is for. Figure 6.20\nshows a notation for aggregation common ly used to represent this situation.\n6.8.6 Reduction to Relation Schemas\nW ea r ei nap o s i t i o nn o wt od e s c r i b eh o wt h ee x t e n d e d E-Rfeatures can be translated\ninto relation schemas.\n", "306": "278 Chapter 6 Database Design Using the E-R Model\n6.8.6.1 Representation of Generalization\nThere are two di\ufb00erent methods of designing relation schemas for an E-Rdiagram that\nincludes generalization. Although we refer to the generalization in Figure 6.18 in this\ndiscussion, we simplify it by including only the \ufb01rst tier of lower-level entity sets\u2014that\nis,employee andstudent . We assume that IDis the primary key of person .\n1.Create a schema for the higher-level entity set. For each lower-level entity set,\ncreate a schema that includes an attribute for each of the attributes of that entity\nset plus one for each attribute of the primary key of the higher-level entity set.\nThus, for the E-Rdiagram of Figure 6.18 (ignoring the instructor andsecretary\nentity sets) we have three schemas:\nperson (ID\n,name, street, city )\nemployee (ID\n,salary )\nstudent (ID\n,tot\ncred)\nThe primary-key attributes of the higher-level entity set become primary-key at-\ntributes of the higher-level entity set as well as all lower-level entity sets. These\ncan be seen underlined in the preceding example.\nIn addition, we create foreign-key constraints on the lower-level entity sets,\nwith their primary-key attributes referencing the primary key of the relation cre-\nated from the higher-level entity set. In the preceding example, the IDattribute\nofemployee would reference the primary key of person , and similarly for student .\n2.An alternative representation is possible, if the generalization is disjoint and com-\nplete\u2014that is, if no entity is a member of two lower-level entity sets directly below\na higher-level entity set, and if every entity in the higher-level entity set is also a\nmember of one of the lower-level entity sets. Here, we do not create a schema\nfor the higher-level entity set. Instead, for each lower-level entity set, we create a\nschema that includes an attribute for each of the attributes of that entity set plus\none for each attribute of the higher-level entity set. Then, for the E-Rdiagram of\nF i g u r e6 . 1 8 ,w eh a v et w os c h e m a s :\nemployee (ID\n,name ,street ,city,salary )\nstudent (ID\n,name ,street ,city,tot\ncred)\nBoth these schemas have ID, which is the primary-key attribute of the higher-level\nentity set person , as their primary key.\nOne drawback of the second method lies in de\ufb01ning foreign-key constraints. To\nillustrate the problem, suppose we have a relationship set Rinvolving entity set person .\nWith the \ufb01rst method, when we create a relation schema Rfrom the relationship set,\nwe also de\ufb01ne a foreign-key constraint on R, referencing the schema person .U n f o r t u -\nnately, with the second method, we do not have a single relation to which a foreign-key\n", "307": "6.9 Entity-Relationship Design Issues 279\nconstraint on Rcan refer. To avoid this problem, we need to create a relation schema\nperson containing at least the primary-key attributes of the person entity.\nIf the second method were used for an overlapping generalization, some values\nwould be stored multiple times, unnecessarily. For instance, if a person is both an\nemployee and a student, values for street andcitywould be stored twice.\nIf the generalization were disjoint but not complete\u2014that is, if some person is nei-\nther an employee nor a student\u2014then an extra schema\nperson (ID\n,name, street, city )\nwould be required to represent such people. However, the problem with foreign-key\nconstraints mentioned above would remain. As an attempt to work around the problem,\nsuppose employees and students are additionally represented in the person relation.\nUnfortunately, name, street, and city information would then be stored redundantly\nin the person relation and the student relation for students, and similarly in the person\nrelation and the employee relation for employees. That suggests storing name, street,\nand city information only in the person relation and removing that information from\nstudent andemployee .I fw ed ot h a t ,t h er e s u l ti se x a c t l yt h e\ufb01 r s tm e t h o dw ep r e s e n t e d .\n6.8.6.2 Representation of Aggregation\nDesigning schemas for an E-Rdiagram containing aggregation is straightforward. Con-\ns i d e rF i g u r e6 . 2 0 .T h es c h e m af o rt h er e l a t i o n s h i ps e t eval\nforbetween the aggregation\nofproj\nguide and the entity set evaluation includes an attribute for each attribute in\nthe primary keys of the entity set evaluation and the relationship set proj\nguide .I ta l s o\nincludes an attribute for any descriptive attributes, if they exist, of the relationship set\neval\nfor. We then transform the relationship sets and entity sets within the aggregated\nentity set following the rules we have already de\ufb01ned.\nThe rules we saw earlier for creating primary-key and foreign-key constraints on\nrelationship sets can be applied to relationship sets involving aggregations as well, with\nthe aggregation treated like any other entity set. The primary key of the aggregation\nis the primary key of its de\ufb01ning relationship set. No separate relation is required to\nrepresent the aggregation; the relation created from the de\ufb01ning relationship is used\ninstead.\n6.9 Entity-Relationship Design Issues\nThe notions of an entity set and a relationship set are not precise, and it is possible\nto de\ufb01ne a set of entities and the relationships among them in a number of di\ufb00erent\nways. In this section, we examine basic issues in the design of an E-Rdatabase schema.\nSection 6.11 covers the design process in further detail.\n", "308": "280 Chapter 6 Database Design Using the E-R Model\nstudent\nstudentID\nname\ntot_cred\ndept_namestud \u2014dept\nstud_section sectiondept_name\nbuilding\nbudgetdepartment\nassignment\nmarks(a) Incorrect use of attribute\n(b) Erroneous use of relationship attributes\nFigure 6.21 Example of erroneous E-Rdiagrams\n6.9.1 Common Mistakes in E-RDiagrams\nA common mistake when creating E-Rmodels is the use of the primary key of an entity\nset as an attribute of another entity set, instead of using a relationship. For example, in\nour university E-Rmodel, it is incorrect to have dept\nname as an attribute of student ,a s\ndepicted in Figure 6.21a, even though it is present as an attribute in the relation schema\nforstudent . The relationship stud\ndeptis the correct way to represent this information\nin the E-Rmodel, since it makes the relationship between student anddepartment ex-\nplicit, rather than implicit via an attribute. Having an attribute dept\nname as well as a\nrelationship stud\ndeptwould result in duplication of information.\nAnother related mistake that people sometimes make is to designate the primary-\nkey attributes of the related entity sets as attributes of the relationship set. For example,\nID(the primary-key attributes of student )a n d ID(the primary key of instructor )s h o u l d\nnot appear as attributes of the relationship advisor . This should not be done since the\nprimary-key attributes are already implicit in the relationship set.6\nA third common mistake is to use a relationship with a single-valued attribute in\na situation that requires a multivalued attribute. For example, suppose we decided to\nrepresent the marks that a student gets in di\ufb00erent assignments of a course o\ufb00ering\n(section ). A wrong way of doing this would be to add two attributes assignment and\nmarks to the relationship takes , as depicted in Figure 6.21b. The problem with this\ndesign is that we can only represent a single assignment for a given student-section pair,\n6When we create a relation schema from the E-Rschema, the attributes may appear in a schema created from the advisor\nrelationship set, as we shall see later; however, they should not appear in the advisor relationship set.\n", "309": "6.9 Entity-Relationship Design Issues 281\nstudent sec_assign\nstud_section section student{assignment_marks\n     assignment\n     marks\n}section assignmentmarks\nmarks_in\n(c) Correct alternative to erroneous E-R diagram (b)\n(d) Correct alternative to erroneous E-R diagram (b) \nFigure 6.22 Correct versions of the E-Rdiagram of Figure 6.21.\nsince relationship instances must be uniquely identi\ufb01ed by the participating entities,\nstudent andsection .\nOne solution to the problem depicted in Figure 6.21c, shown in Figure 6.22a, is to\nmodel assignment as a weak entity identi\ufb01ed by section , and to add a relationship marks\ninbetween assignment andstudent ; the relationship would have an attribute marks .A n\nalternative solution, shown in Figure 6.22d, is to use a multivalued composite attribute\n{assignment\n marks }totakes ,w h e r e assignment\n marks has component attributes assign-\nment andmarks . Modeling an assignment as a weak entity is preferable in this case,\nsince it allows recording other information about the assignment, such as maximum\nmarks or deadlines.\nWhen an E-Rdiagram becomes too big to draw in a single piece, it makes sense\nto break it up into pieces, each showing part of the E-Rmodel. When doing so, you\nmay need to depict an entity set in more than one page. As discussed in Section 6.2.2,\nattributes of the entity set should be shown only once, in its \ufb01rst occurrence. Subse-\nquent occurrences of the entity set should be shown without any attributes, to avoid\nrepeating the same information at multiple places, which may lead to inconsistency.\n6.9.2 Use of Entity Sets versus Attributes\nConsider the entity set instructor with the additional attribute phone\n number (Figure\n6.23a.) It can be argued that a phone is an entity in its own right with attributes phone\n", "310": "282 Chapter 6 Database Design Using the E-R Model\ninstructor\nID\nname\nsalaryphone\nphone_number\nlocationinstructor\nID\nname\nsalary\nphone_number\n(a) (b)inst_phone\nFigure 6.23 Alternatives for adding phone to the instructor entity set.\nnumber andlocation ; the location may be the o\ufb03ce or home where the phone is lo-\ncated, with mobile (cell) phones perhaps represented by the value \u201cmobile.\u201d If we take\nthis point of view, we do not add the attribute phone\n number to the instructor .R a t h e r ,\nwe create:\n\u2022Aphone entity set with attributes phone\n number andlocation .\n\u2022A relationship set inst\nphone , denoting the association between instructors and the\nphones that they have.\nThis alternative is shown in Figure 6.23b.\nWhat, then, is the main di\ufb00erence between these two de\ufb01nitions of an instructor?\nTreating a phone as an attribute phone\n number implies that instructors have precisely\none phone number each. Treating a phone as an entity phone permits instructors to\nhave several phone numbers (including zero) associated with them. However, we could\ninstead easily de\ufb01ne phone\n number as a multivalued attribute to allow multiple phones\nper instructor.\nThe main di\ufb00erence then is that treating a phone as an entity better models a\nsituation where one may want to keep extra information about a phone, such as its\nlocation, or its type (mobile, IPphone, or plain old phone), or all who share the phone.\nThus, treating phone as an entity is more general than treating it as an attribute and is\nappropriate when the generality may be useful.\nIn contrast, it would not be appropriate to treat the attribute name (of an instruc-\ntor) as an entity; it is di\ufb03cult to argue that name is an entity in its own right (in contrast\nto the phone). Thus, it is appropriate to have name as an attribute of the instructor entity\nset.\nTwo natural questions thus arise: What constitutes an attribute, and what consti-\ntutes an entity set? Unfortunately, there are no simple answers. The distinctions mainly\ndepend on the structure of the real-world enterprise being modeled and on the seman-\ntics associated with the attribute in question.\n6.9.3 Use of Entity Sets versus Relationship Sets\nIt is not always clear whether an object is best expressed by an entity set or a relationship\nset. In Figure 6.15, we used the takes relationship set to model the situation where a\n", "311": "6.9 Entity-Relationship Design Issues 283\nregistration\n...\n...\n...\nsection\nsec_id\nsemester\nyearstudent\nID\nname\ntot_credsection_reg student_reg\nFigure 6.24 Replacement of takes byregistration and two relationship sets.\nstudent takes a (section of a) course. An alternative is to imagine that there is a course-\nregistration record for each course that each student takes. Then, we have an entity\nset to represent the course-registration record. Let us call that entity set registration .\nEach registration entity is related to exactly one student and to exactly one section, so\nwe have two relationship sets, one to relate course-registration records to students and\none to relate course-registration records t o sections. In Figure 6.24, we show the entity\nsetssection andstudent from Figure 6.15 with the takes relationship set replaced by one\nentity set and two relationship sets:\n\u2022registration , the entity set representing course-registration records.\n\u2022section\n reg, the relationship set relating registration andcourse .\n\u2022student\n reg, the relationship set relating registration andstudent .\nNote that we use double lines to indicate total participation by registration entities.\nBoth the approach of Figure 6.15 and that of Figure 6.24 accurately represent the\nuniversity\u2019s information, but the use of takes is more compact and probably preferable.\nHowever, if the registrar\u2019s o\ufb03ce associates other information with a course-registration\nrecord, it might be best to make it an entity in its own right.\nOne possible guideline in determining whet her to use an entity set or a relationship\nset is to designate a relationship set to describe an action that occurs between entities.\nThis approach can also be useful in deciding whether certain attributes may be more\nappropriately expressed as relationships.\n6.9.4 Binary versus n-ary Relationship Sets\nRelationships in databases are often binary. Some relationships that appear to be nonbi-\nnary could actually be better represented by several binary relationships. For instance,\none could create a ternary relationship parent , relating a child to his/her mother and\nfather. However, such a relationship could also be represented by two binary relation-\nships, mother andfather , relating a child to his/her mother and father separately. Using\n", "312": "284 Chapter 6 Database Design Using the E-R Model\nthe two relationships mother andfather provides us with a record of a child\u2019s mother,\neven if we are not aware of the father\u2019s identity; a null value would be required if the\nternary relationship parent were used. Using binary relationship sets is preferable in\nthis case.\nIn fact, it is always possible to replace a nonbinary ( n-ary, for n>2) relationship set\nby a number of distinct binary relationship sets. For simplicity, consider the abstract\nternary ( n=3) relationship set R, relating entity sets A,B,a n d C.W er e p l a c et h e\nrelationship set Rwith an entity set E, and we create three relationship sets as shown\nin Figure 6.25:\n\u2022RA, a many-to-one relationship set from EtoA.\n\u2022RB, a many-to-one relationship set from EtoB.\n\u2022RC, a many-to-one relationship set from EtoC.\nEis required to have total participation in each of RA,RB,a n d RC. If the relationship\nsetRhad any attributes, these are assigned to entity set E; further, a special identifying\nattribute is created for E(since it must be possible to distinguish di\ufb00erent entities in\nan entity set on the basis of their attribute values). For each relationship ( ai,bi,ci)i n\nthe relationship set R,w ec r e a t ean e we n t i t y eiin the entity set E. Then, in each of the\nthree new relationship sets, we insert a relationship as follows:\n\u2022(ei,ai)i nRA.\n\u2022(ei,bi)i nRB.\n\u2022(ei,ci)i nRC.\nWe can generalize this process in a straightforward manner to n-ary relationship\nsets. Thus, conceptually, we can restrict the E-Rmodel to include only binary relation-\nship sets. However, this restriction is not always desirable.\nBR CA\nC BEA\nRA\nRB RC\n(a) (b)\nFigure 6.25 Ternary relationship versus three binary relationships.\n", "313": "6.10 Alternative Notations for Modeling Data 285\n\u2022An identifying attribute may have to be created for the entity set created to rep-\nresent the relationship set. This attribute, along with the extra relationship sets\nrequired, increases the complexity of the design and (as we shall see in Section\n6.7) overall storage requirements.\n\u2022Ann-ary relationship set shows more clearly that several entities participate in a\nsingle relationship.\n\u2022There may not be a way to translate constraints on the ternary relationship into\nconstraints on the binary relationships. For example, consider a constraint that\nsays that Ris many-to-one from A,BtoC; that is, each pair of entities from Aand\nBis associated with at most one Centity. This constraint cannot be expressed by\nusing cardinality constraints on the relationship sets RA,RB,a n d RC.\nConsider the relationship set proj\nguide in Section 6.2.2, relating instructor ,stu-\ndent,a n d project . We cannot directly split proj\nguide into binary relationships between\ninstructor andproject and between instructor andstudent . If we did so, we would be able\nto record that instructor Katz works on projects AandBwith students Shankar and\nZhang; however, we would not be able to record that Katz works on project Awith\nstudent Shankar and works on project Bwith student Zhang, but does not work on\nproject Awith Zhang or on project Bwith Shankar.\nThe relationship set proj\nguide can be split into binary relationships by creating a\nnew entity set as described above. However, doing so would not be very natural.\n6.10 Alternative Notations for Modeling Data\nA diagrammatic representation of the data model of an application is a very important\npart of designing a database schema. Creation of a database schema requires not only\ndata modeling experts, but also domain experts who know the requirements of the\napplication but may not be familiar with data modeling. An intuitive diagrammatic\nrepresentation is particularly important since it eases communication of information\nbetween these groups of experts.\nA number of alternative notations for modeling data have been proposed, of which\nE-Rdiagrams and UML c l a s sd i a g r a m sa r et h em o s tw i d e l yu s e d .T h e r ei sn ou n i v e r s a l\nstandard for E-Rdiagram notation, and di\ufb00erent books and E-Rdiagram software use\ndi\ufb00erent notations.\nIn the rest of this section, we study some of the alternative E-Rdiagram notations,\nas well as the UML class diagram notation. To aid in comparison of our notation with\nthese alternatives, Figure 6.26 summarizes the set of symbols we have used in our E-R\ndiagram notation.\n6.10.1 Alternative E-R Notations\nFigure 6.27 indicates some of the alternative E-Rnotations that are widely used. One\nalternative representation of attributes of entities is to show them in ovals connected\n", "314": "286 Chapter 6 Database Design Using the E-R Model\nE\nR\nR\nRRR\nrole-\nnameR\nEA1\nA2\nA2.1\nA2.2\n{A3}\nA4E\nRl..hE\nE1\nE2 E3E1\nE2 E3\nE1\nE2 E3entity set\nrelationship set\nidentifying\nrelationship set\nfor weak entity set primary key\nmany-to-many\nrelationshipmany-to-one\nrelationship\none-to-one\nrelationshipcardinality\nlimitsdiscriminating\nattribute of\nweak entity settotal participation\nof entity set in\nrelationshipattributes:\nsimple (A1),\ncomposite (A2) and\nmultivalued (A3)\nderived (A4)\nISA: generalization\nor specialization\ndisjoint\ngeneralizationtotal (disjoint)\ngeneralizationrole indicator\ntotalA1E\nA1E\nR E()\nFigure 6.26 Symbols used in the E-Rnotation.\nto the box representing the entity; primary key attributes are indicated by underlining\nthem. The above notation is shown at the top of the \ufb01gure. Relationship attributes\ncan be similarly represented, by connecting the ovals to the diamond representing the\nrelationship.\nCardinality constraints on relationships can be indicated in several di\ufb00erent ways,\nas shown in Figure 6.27. In one alternative, shown on the left side of the \ufb01gure, labels\n\u2217and 1 on the edges out of the relationship are used for depicting many-to-many, one-\n", "315": "6.10 Alternative Notations for Modeling Data 287\nparticipation\nin R: total (E1)\nand partial (E2)E1 E2E2 E1 RRRentity set E with\nsimple attribute A1,\ncomposite attribute A2,\nmultivalued attribute A3,\nderived attribute A4,\nand primary key A1\nmany-to-many\nrelationship\none-to-one\nrelationship\nmany-to-one\nrelationshipR\nR*\n**\n1\n11R\nE1E1E1\nE2E2E2 E1 E2\ngeneralizationISA ISAtotal\ngeneralizationweak entity setA1A2A3A2.1 A2.2\nA4E\nRE1 E2\nRE1 E2\nFigure 6.27 Alternative E-Rnotations.\nto-one, and many-to-one relationships. The case of one-to-many is symmetric to many-\nto-one and is not shown.\nIn another alternative notation shown on the right side of Figure 6.27, relationship\nsets are represented by lines between entity sets, without diamonds; only binary rela-\ntionships can be modeled thus. Cardinality constraints in such a notation are shown by\n\u201ccrow\u2019s-foot\u201d notation, as in the \ufb01gure. In a relationship Rbetween E1a n d E2, crow\u2019s\nfeet on both sides indicate a many-to-many relationship, while crow\u2019s feet on just the\nE1 side indicate a many-to-one relationship from E1t oE2. Total participation is spec-\ni\ufb01ed in this notation by a vertical bar. Note however, that in a relationship Rbetween\nentities E1a n d E2, if the participation of E1i nRis total, the vertical bar is placed on\nthe opposite side, adjacent to entity E2. Similarly, partial participation is indicated by\nusing a circle, again on the opposite side.\nThe bottom part of Figure 6.27 shows an alternative representation of generaliza-\ntion, using triangles instead of hollow arrowheads.\n", "316": "288 Chapter 6 Database Design Using the E-R Model\nIn prior editions of this text up to the \ufb01fth edition, we used ovals to represent\nattributes, with triangles representing generalization, as shown in Figure 6.27. The no-\ntation using ovals for attributes and diamonds for relationships is close to the original\nform of E-Rdiagrams used by Chen in his paper that introduced the notion of E-R\nmodeling. That notation is now referred to as Chen\u2019s notation.\nThe U.S. National Institute for Standards and Technology de\ufb01ned a standard called\nIDEF1X in 1993. IDEF1X uses the crow\u2019s-foot notation, with vertical bars on the rela-\ntionship edge to denote total participation and hollow circles to denote partial partici-\npation, and it includes other notations that we have not shown.\nWith the growth in the use of Uni\ufb01ed Markup Language ( UML ), described in Sec-\nt i o n6 . 1 0 . 2 ,w eh a v ec h o s e nt ou p d a t eo u r E-Rnotation to make it closer to the form of\nUML class diagrams; the connections will become clear in Section 6.10.2. In compari-\nson with our previous notation, our new notation provides a more compact representa-\ntion of attributes, and it is also closer to the notation supported by many E-Rmodeling\ntools, in addition to being closer to the UML class diagram notation.\nThere are a variety of tools for constructing E-Rdiagrams, each of which has its\nown notational variants. Some of the tools even provide a choice between several E-R\nnotation variants. See the tools section at the end of the chapter for references.\nOne key di\ufb00erence between entity sets in an E-Rdiagram and the relation schemas\ncreated from such entities is that attributes in the relational schema corresponding to\nE-Rrelationships, such as the dept\nname attribute of instructor , are not shown in the\nentity set in the E-Rdiagram. Some data modeling tools allow designers to choose\nbetween two views of the same entity, one an entity view without such attributes, and\nother a relational view with such attributes.\n6.10.2 The Unified Modeling Language UML\nEntity-relationship diagrams help model the data representation component of a soft-\nware system. Data representation, however, forms only one part of an overall system\ndesign. Other components include models of user interactions with the system, spec-\ni\ufb01cation of functional modules of the system and their interaction, etc. The Uni\ufb01ed\nModeling Language (UML ) is a standard developed under the auspices of the Object\nManagement Group (OMG ) for creating speci\ufb01cations of various components of a soft-\nware system. Some of the parts of UML are:\n\u2022Class diagram . A class diagram is similar to an E-Rdiagram. Later in this section\nwe illustrate a few features of class diagrams and how they relate to E-Rdiagrams.\n\u2022Use case diagram . Use case diagrams show the interaction between users and the\nsystem, in particular the steps of tasks that users perform (such as withdrawing\nmoney or registering for a course).\n\u2022Activity diagram . Activity diagrams depict the \ufb02ow of tasks between various com-\nponents of a system.\n", "317": "6.10 Alternative Notations for Modeling Data 289\n\u2022Implementation diagram . Implementation diagrams show the system components\nand their interconnections, both at the software component level and the hardware\ncomponent level.\nWe do not attempt to provide detailed coverage of the di\ufb00erent parts of UML here.\nInstead we illustrate some features of that part of UML that relates to data modeling\nthrough examples. See the Further Reading section at the end of the chapter for refer-\nences on UML .\nFigure 6.28 shows several E-Rdiagram constructs and their equivalent UML class\ndiagram constructs. We describe these constructs below. UML actually models objects,\nwhereas E-Rmodels entities. Objects are like entities, and have attributes, but addition-\nally provide a set of functions (called methods) that can be invoked to compute values\non the basis of attributes of the objects, or to update the object itself. Class diagrams\ncan depict methods in addition to attributes. We cover objects in Section 8.2. UML does\nnot support composite or multivalued attributes, and derived attributes are equivalent\nto methods that take no parameters. Since classes support encapsulation, UML allows\nattributes and methods to be pre\ufb01xed with a \u201c+\u201d, \u201c-\u201d, or \u201c#\u201d, which denote respectively\npublic, private, and protected access. Private attributes can only be used in methods of\nthe class, while protected attributes can be used only in methods of the class and its\nsubclasses; these should be familiar to anyone who knows Java, C++, or C#.\nInUML terminology, relationship sets are referred to as associations ;w es h a l lr e f e r\nto them as relationship sets for consistency with E-Rterminology. We represent binary\nrelationship sets in UML by just drawing a line connecting the entity sets. We write the\nrelationship set name adjacent to the line. We may also specify the role played by an\nentity set in a relationship set by writing the role name on the line, adjacent to the entity\nset. Alternatively, we may write the relationship set name in a box, along with attributes\nof the relationship set, and connect the box by a dotted line to the line depicting the\nrelationship set. This box can then be treated as an entity set, in the same way as an\naggregation in E-Rdiagrams, and can participate in relationships with other entity sets.\nSince UML version 1.3, UML supports nonbinary relationships, using the same di-\namond notation used in E-Rdiagrams. Nonbinary relationships could not be directly\nrepresented in earlier versions of UML \u2014they had to be converted to binary relation-\nships by the technique we have seen earlier in Section 6.9.4. UML allows the diamond\nnotation to be used even for binary relationships, but most designers use the line nota-\ntion.\nCardinality constraints are speci\ufb01ed in UML in the same way as in E-Rdiagrams,\nin the form l..h,w h e r e ldenotes the minimum and hthe maximum number of relation-\nships an entity can participate in. However, you should be aware that the positioning\nof the constraints is exactly the reverse of the positioning of constraints in E-Rdia-\ngrams, as shown in Figure 6.28. The constraint 0 ..\u2217on the E2s i d ea n d0 ..1o nt h e\nE1 side means that each E2 entity can participate in at most one relationship, whereas\neach E1 entity can participate in many relationships; in other words, the relationship\nis many-to-one from E2t oE1.\n", "318": "290 Chapter 6 Database Design Using the E-R Model\nE\nA1\nM10E\n\u2013A1\nE1 E2 Rrole1E1 E2role1 role2 R role2\nE1 E2 RA1\nrole1 role2\nE1 E2\nE2\nE2 E3\nE2\nE2E3E3R\nE1\nE1 E1 E2 RE1R0..* 0..1binary\nrelationshipclass with simple attributes\nand methods (attribute\npre\ufb01xes: + = public,\n\u2013 = private, # = protected) \nrelationship\nattributes\ncardinality\nconstraints\nn-ary\nrelationships\noverlappingoverlapping\ngeneralization\ndisjoint\ngeneralization\nweak-entity\ncompositiondisjointE1 E2R0_1 0_*entity with\nattributes (simple,\ncomposite,\nmultivalued, derived)ER Diagram Notation Equivalent in UML\n+M10\nE1R\nE2A1\nrole1 role2\nE2\nE3E1 R\nE2 E3E1\nE2 E3E1E1\nFigure 6.28 Symbols used in the UML class diagram notation.\nSingle values such as 1 or \u2217may be written on edges; the single value 1 on an edge is\ntreated as equivalent to 1 ..1, while\u2217is equivalent to 0 ..\u2217.UML supports generalization;\nthe notation is basically the same as in our E-Rnotation, including the representation\nof disjoint and overlapping generalizations.\nUML c l a s sd i a g r a m si n c l u d es e v e r a lo t h e rn o t a t i o n st h a ta p p r o x i m a t e l yc o r r e s p o n d\nto the E-Rnotations we have seen. A line between two entity sets with a small shaded\nd i a m o n da to n ee n di n UML speci\ufb01es \u201ccomposition\u201d in UML . The composition rela-\ntionship between E2a n d E1 in Figure 6.28 indicates that E2 is existence dependent\nonE1; this is roughly equivalent to denoting E2 as a weak entity set that is existence\n", "319": "6.11 Other Aspects of Database Design 291\ndependent on the identifying entity set E1. (The term aggregation inUML denotes a\nvariant of composition where E2 is contained in E1b u tm a ye x i s ti n d e p e n d e n t l y ,a n d\nit is denoted using a small hollow diamond.)\nUML class diagrams also provide notations to represent object-oriented language\nfeatures such as interfaces. See the Further Reading section for more information on\nUML class diagrams.\n6.11 Other Aspects of Database Design\nOur extensive discussion of schema design in this chapter may create the false impres-\nsion that schema design is the only component of a database design. There are indeed\nseveral other considerations that we address more fully in subsequent chapters, and\nsurvey brie\ufb02y here.\n6.11.1 Functional Requirements\nAll enterprises have rules on what kinds of functionality are to be supported by an\nenterprise application. These could include transactions that update the data, as well\nas queries to view data in a desired fashion. In addition to planning the functionality,\ndesigners have to plan the interfaces to be built to support the functionality.\nNot all users are authorized to view all data, or to perform all transactions. An\nauthorization mechanism is very important for any enterprise application. Such autho-\nrization could be at the level of the database, using database authorization features.\nBut it could also be at the level of higher-level functionality or interfaces, specifying\nwho can use which functions/interfaces.\n6.11.2 Data Flow, Workflow\nDatabase applications are often part of a larger enterprise application that interacts\nnot only with the database system but also with various specialized applications. As\nan example, consider a travel-expense report. It is created by an employee returning\nfrom a business trip (possibly by means of a special software package) and is subse-\nquently routed to the employee\u2019s manager, perhaps other higher-level managers, and\neventually to the accounting department for payment (at which point it interacts with\nthe enterprise\u2019s accounting information systems).\nThe term work\ufb02ow refers to the combination of data and tasks involved in processes\nlike those of the preceding examples. Work\ufb02ows interact with the database system as\nthey move among users and users perform their tasks on the work\ufb02ow. In addition to the\ndata on which work\ufb02ows operate, the database may store data about the work\ufb02ow itself,\nincluding the tasks making up a work\ufb02ow and how they are to be routed among users.\nWork\ufb02ows thus specify a series of queries and updates to the database that may be taken\ninto account as part of the database-design process. Put in other terms, modeling the\n", "320": "292 Chapter 6 Database Design Using the E-R Model\nenterprise requires us not only to understand the semantics of the data but also the\nbusiness processes that use those data.\n6.11.3 Schema Evolution\nDatabase design is usually not a one-time activity. The needs of an organization evolve\ncontinually, and the data that it needs to store also evolve correspondingly. During\nthe initial database-design phases, or during the development of an application, the\ndatabase designer may realize that changes are required at the conceptual, logical, or\nphysical schema levels. Changes in the schema can a\ufb00ect all aspects of the database\napplication. A good database design anticipates future needs of an organization and\nensures that the schema requires minimal changes as the needs evolve.\nIt is important to distinguish between fundamental constraints that are expected\nto be permanent and constraints that are anticipated to change. For example, the con-\nstraint that an instructor-id identify a unique instructor is fundamental. On the other\nhand, a university may have a policy that an instructor can have only one department,\nwhich may change at a later date if joint appointments are allowed. A database design\nthat only allows one department per instructor might require major changes if joint\nappointments are allowed. Such joint appointments can be represented by adding an\nextra relationship without modifying the instructor relation, as long as each instructor\nhas only one primary department a\ufb03liation; a policy change that allows more than one\nprimary a\ufb03liation may require a larger change in the database design. A good design\nshould account not only for current policies, but should also avoid or minimize the\nneed for modi\ufb01cations due to changes that are anticipated or have a reasonable chance\nof happening.\nFinally, it is worth noting that database design is a human-oriented activity in two\nsenses: the end users of the system are people (even if an application sits between the\ndatabase and the end users); and the database designer needs to interact extensively\nwith experts in the application domain to understand the data requirements of the\napplication. All of the people involved with the data have needs and preferences that\nshould be taken into account in order for a database design and deployment to succeed\nwithin the enterprise.\n6.12 Summary\n\u2022Database design mainly involves the design of the database schema. The entity-\nrelationship ( E-R) data model is a widely used data model for database design.\nIt provides a convenient graphical representation to view data, relationships, and\nconstraints.\n\u2022The E-Rmodel is intended primarily for the database-design process. It was de-\nveloped to facilitate database design by allowing the speci\ufb01cation of an enterprise\nschema. Such a schema represents the overall logical structure of the database.\nThis overall structure can be expressed graphically by an E-Rdiagram.\n", "321": "Review Terms 293\n\u2022An entity is an object that exists in the real world and is distinguishable from other\nobjects. We express the distinction by associating with each entity a set of attributes\nthat describes the object.\n\u2022A relationship is an association among several entities. A relationship set is a col-\nlection of relationships of the same type, and an entity set is a collection of entities\nof the same type.\n\u2022The terms superkey, candidate key, and primary key apply to entity and relation-\nship sets as they do for relation schemas. Identifying the primary key of a relation-\nship set requires some care, since it is c omposed of attributes from one or more of\nthe related entity sets.\n\u2022Mapping cardinalities express the number of entities to which another entity can\nbe associated via a relationship set.\n\u2022An entity set that does not have su\ufb03cient attributes to form a primary key is termed\na weak entity set. An entity set that has a primary key is termed a strong entity set.\n\u2022The various features of the E-Rmodel o\ufb00er the database designer numerous\nchoices in how to best represent the enterprise being modeled. Concepts and ob-\njects may, in certain cases, be represented by entities, relationships, or attributes.\nAspects of the overall structure of the enterprise may be best described by using\nweak entity sets, generalization, specialization, or aggregation. Often, the designer\nmust weigh the merits of a simple, compact model versus those of a more precise,\nbut more complex one.\n\u2022A database design speci\ufb01ed by an E-Rdiagram can be represented by a collection of\nrelation schemas. For each entity set and for each relationship set in the database,\nthere is a unique relation schema that is assigned the name of the corresponding\nentity set or relationship set. This forms the basis for deriving a relational database\ndesign from an E-Rdiagram.\n\u2022Specialization and generalization de\ufb01ne a containment relationship between a\nhigher-level entity set and one or more lower-level entity sets. Specialization is\nthe result of taking a subset of a higher-level entity set to form a lower-level entity\nset. Generalization is the result of taking the union of two or more disjoint (lower-\nlevel) entity sets to produce a higher-level entity set. The attributes of higher-level\nentity sets are inherited by lower-level entity sets.\n\u2022Aggregation is an abstraction in which relationship sets (along with their asso-\nciated entity sets) are treated as higher-level entity sets, and can participate in\nrelationships.\n\u2022Care must be taken in E-Rdesign. There are a number of common mistakes to\navoid. Also, there are choices among the use of entity sets, relationship sets, and\n", "322": "294 Chapter 6 Database Design Using the E-R Model\nattributes in representing aspects of the enterprise whose correctness may depend\non subtle details speci\ufb01c to the enterprise.\n\u2022UML is a popular modeling language. UML class diagrams are widely used for\nmodeling classes, as well as for general-purpose data modeling.\nReview Terms\n\u2022Design Process\n\u00b0Conceptual-design\n\u00b0Logical-design\n\u00b0Physical-design\n\u2022Entity-relationship (E-R) data model\n\u2022Entity and entity set\n\u00b0Simple and composite attributes\n\u00b0Single-valued and multivalued at-\ntributes\n\u00b0Derived attribute\n\u2022Key\n\u00b0Superkey\n\u00b0Candidate key\n\u00b0Primary key\n\u2022Relationship and relationship set\n\u00b0Binary relationship set\n\u00b0Degree of relationship set\n\u00b0Descriptive attributes\u00b0Superkey, candidate key, and pri-\nmary key\n\u00b0Role\n\u00b0Recursive relationship set\n\u2022E-Rdiagram\n\u2022Mapping cardinality:\n\u00b0One-to-one relationship\n\u00b0One-to-many relationship\n\u00b0Many-to-one relationship\n\u00b0Many-to-many relationship\n\u2022Total and partial participation\n\u2022Weak entity sets and strong entity sets\n\u00b0Discriminator attributes\n\u00b0Identifying relationship\n\u2022Specialization and generalization\n\u2022Aggregation\n\u2022Design choices\n\u2022United Modeling Language ( UML )\nPractice Exercises\n6.1 Construct an E-Rdiagram for a car insurance company whose customers own\none or more cars each. Each car has associated with it zero to any number of\nrecorded accidents. Each insurance policy covers one or more cars and has one\nor more premium payments associated with it. Each payment is for a particular\nperiod of time, and has an associated due date, and the date when the payment\nwas received.\n", "323": "Practice Exercise 295\n6.2 Consider a database that includes the entity sets student ,course ,a n d section\nfrom the university schema and that additionally records the marks that students\nreceive in di\ufb00erent exams of di\ufb00erent sections.\na. Construct an E-Rdiagram that models exams as entities and uses a ternary\nrelationship as part of the design.\nb. Construct an alternative E-Rdiagram that uses only a binary relationship\nbetween student andsection . Make sure that only one relationship exists\nbetween a particular student andsection pair, yet you can represent the\nmarks that a student gets in di\ufb00erent exams.\n6.3 Design an E-Rdiagram for keeping track of the scoring statistics of your favorite\nsports team. You should store the matches played, the scores in each match, the\nplayers in each match, and individual player scoring statistics for each match.\nSummary statistics should be modeled as derived attributes with an explanation\nas to how they are computed.\n6.4 Consider an E-Rdiagram in which the same entity set appears several times,\nwith its attributes repeated in more than one occurrence. Why is allowing this\nredundancy a bad practice that one should avoid?\n6.5 AnE-Rdiagram can be viewed as a graph. What do the following mean in terms\nof the structure of an enterprise schema?\na. The graph is disconnected.\nb. The graph has a cycle.\n6.6 Consider the representation of the ternary relationship of Figure 6.29a using\nthe binary relationships illustrated in Figure 6.29b (attributes not shown).\na. Show a simple instance of E,A,B,C,RA,RB,a n d RCthat cannot corre-\nspond to any instance of A,B,C,a n d R.\nb. Modify the E-Rdiagram of Figure 6.29b to introduce constraints that will\nguarantee that any instance of E,A,B,C,RA,RB,a n d RCthat satis\ufb01es the\nconstraints will correspond to an instance of A,B,C,a n d R.\nc. Modify the preceding translation to handle total participation constraints\non the ternary relationship.\n6.7 A weak entity set can always be made into a strong entity set by adding to its\nattributes the primary-key attributes of its identifying entity set. Outline what\nsort of redundancy will result if we do so.\n6.8 Consider a relation such as sec\ncourse , generated from a many-to-one relation-\nship set sec\ncourse . Do the primary and foreign key constraints created on the\nrelation enforce the many-to-one cardinality constraint? Explain why.\n", "324": "296 Chapter 6 Database Design Using the E-R Model\nBCA\nC BEA\nRA\nRBRC\n(a) (b)\n(c)A\nB CR\nRBCRABRAC\nFigure 6.29 Representation of a ternary relationship using binary relationships.\n6.9 Suppose the advisor relationship set were one-to-one. What extra constraints\nare required on the relation advisor to ensure that the one-to-one cardinality\nconstraint is enforced?\n6.10 Consider a many-to-one relationship Rbetween entity sets AandB. Suppose\nthe relation created from Ris combined with the relation created from A.I n\nSQL, attributes participating in a foreign key constraint can be null. Explain\nhow a constraint on total participation of AinRcan be enforced using not null\nconstraints in SQL.\n6.11 InSQL, foreign key constraints can reference only the primary key attributes of\nthe referenced relation or other attributes declared to be a superkey using the\nunique constraint. As a result, total participation constraints on a many-to-many\nrelationship set (or on the \u201cone\u201d side of a one-to-many relationship set) cannot\nbe enforced on the relations created from the relationship set, using primary\nkey, foreign key, and not null constraints on the relations.\na. Explain why.\nb. Explain how to enforce total participation constraints using complex\ncheck constraints or assertions (see Section 4.4.8). (Unfortunately, these\nfeatures are not supported on any widely used database currently.)\n6.12 Consider the following lattice structure of generalization and specialization (at-\ntributes not shown).\n", "325": "Exercises 297\nXY\nAB C\nFor entity sets A,B,a n d C, explain how attributes are inherited from the higher-\nlevel entity sets XandY. Discuss how to handle a case where an attribute of X\nhas the same name as some attribute of Y.\n6.13 AnE-Rdiagram usually models the state of an enterprise at a point in time.\nSuppose we wish to track temporal changes , that is, changes to data over time.\nFor example, Zhang may have been a student between September 2015 and\nMay 2019, while Shankar may have had instructor Einstein as advisor from May\n2018 to December 2018, and again from June 2019 to January 2020. Similarly,\nattribute values of an entity or relationship, such as titleandcredits ofcourse ,\nsalary ,o re v e n name ofinstructor ,a n d tot\ncredofstudent , can change over time.\nOne way to model temporal changes is as follows: We de\ufb01ne a new data type\ncalled valid\n time, which is a time interval, or a set of time intervals. We then\nassociate a valid\n time attribute with each entity and relationship, recording the\ntime periods during which the entity or relationship is valid. The end time of an\ninterval can be in\ufb01nity; for example, if Shankar became a student in September\n2018, and is still a student, we can represent the end time of the valid\n time in-\nterval as in\ufb01nity for the Shankar entity. Similarly, we model attributes that can\nchange over time as a set of values, each with its own valid\n time.\na. Draw an E-Rdiagram with the student andinstructor entities, and the ad-\nvisor relationship, with the above extensions to track temporal changes.\nb. Convert the E-Rdiagram discussed above into a set of relations.\nIt should be clear that the set of relations generated is rather complex, leading\nto di\ufb03culties in tasks such as writing queries in SQL. An alternative approach,\nwhich is used more widely, is to ignore temporal changes when designing the\nE-Rmodel (in particular, temporal changes to attribute values), and to modify\nthe relations generated from the E-Rmodel to track temporal changes.\nExercises\n6.14 Explain the distinctions among the terms primary key ,candidate key ,a n d su-\nperkey .\n", "326": "298 Chapter 6 Database Design Using the E-R Model\n6.15 Construct an E-Rdiagram for a hospital with a set of patients and a set of med-\nical doctors. Associate with each patient a log of the various tests and examina-\ntions conducted.\n6.16 Extend the E-Rdiagram of Exercise 6.3 to track the same information for all\nteams in a league.\n6.17 Explain the di\ufb00erence between a weak and a strong entity set.\n6.18 Consider two entity sets AandBthat both have the attribute X(among others\nwhose names are not relevant to this question).\na. If the two Xs are completely unrelated, how should the design be im-\nproved?\nb. If the two Xs represent the same property and it is one that applies both to\nAand to B, how should the design be improved? Consider three subcases:\n\u2022Xis the primary key for Abut not B\n\u2022Xis the primary key for both AandB\n\u2022Xis not the primary key for Anor for B\n6.19 We can convert any weak entity set to a strong entity set by simply adding ap-\npropriate attributes. Why, then, do we have weak entity sets?\n6.20 Construct appropriate relation schemas for each of the E-Rdiagrams in:\na. Exercise 6.1.\nb. Exercise 6.2.\nc. Exercise 6.3.\nd. Exercise 6.15.\n6.21 Consider the E-Rdiagram in Figure 6.30, which models an online bookstore.\na. Suppose the bookstore adds Blu-ray discs and downloadable video to its\ncollection. The same item may be present in one or both formats, with dif-\nfering prices. Draw the part of the E-Rdiagram that models this addition,\nshowing just the parts related to video.\nb. Now extend the full E-Rdiagram to model the case where a shopping bas-\nket may contain any combination of books, Blu-ray discs, or downloadable\nvideo.\n6.22 Design a database for an automobile company to provide to its dealers to assist\nthem in maintaining customer records and dealer inventory and to assist sales\nsta\ufb00 in ordering cars.\n", "327": "Exercises 299\nauthor\nname\naddress\nURL\nwritten_by\npublished_by\ncontainsnumber\nnumber\nstocksbook\nshopping_basket\nbasket_id\nwarehousebasket_ofISBN\ntitle\nyear\nprice\ncode\naddress\nphonepublisher\nname\naddress\nphone\nURL\ncustomer\nemail\nname\naddress\nphone\nFigure 6.30 E-Rdiagram for modeling an online bookstore.\nEach vehicle is identi\ufb01ed by a vehicle identi\ufb01cation number ( VIN). Each indi-\nvidual vehicle is a particular model of a particular brand o\ufb00ered by the company\n(e.g., the XFis a model of the car brand Jaguar of Tata Motors). Each model\ncan be o\ufb00ered with a variety of options, but an individual car may have only\nsome (or none) of the available options. The database needs to store informa-\ntion about models, brands, and options, as well as information about individual\ndealers, customers, and cars.\nYour design should include an E-Rdiagram, a set of relational schemas, and\na list of constraints, including primary-key and foreign-key constraints.\n6.23 Design a database for a worldwide package delivery company (e.g., DHL or\nFedEx). The database must be able to keep track of customers who ship items\nand customers who receive items; some customers may do both. Each package\nmust be identi\ufb01able and trackable, so the database must be able to store the\nlocation of the package and its history of locations. Locations include trucks,\nplanes, airports, and warehouses.\nYour design should include an E-Rdiagram, a set of relational schemas, and\na list of constraints, including primary-key and foreign-key constraints.\n6.24 Design a database for an airline. The database must keep track of customers\nand their reservations, \ufb02ights and their status, seat assignments on individual\n\ufb02ights, and the schedule and routing of future \ufb02ights.\nYour design should include an E-Rdiagram, a set of relational schemas, and\na list of constraints, including primary-key and foreign-key constraints.\n", "328": "300 Chapter 6 Database Design Using the E-R Model\n6.25 In Section 6.9.4, we represented a ternary relationship (repeated in Figure\n6.29a) using binary relationships, as sh own in Figure 6.29b. Consider the alter-\nnative shown in Figure 6.29c. Discuss the relative merits of these two alternative\nrepresentations of a ternary relationship by binary relationships.\n6.26 Design a generalization\u2013specialization hierarchy for a motor vehicle sales com-\npany. The company sells motorcycles, passenger cars, vans, and buses. Justify\nyour placement of attributes at each level of the hierarchy. Explain why they\nshould not be placed at a higher or lower level.\n6.27 Explain the distinction between disjoint and overlapping constraints.\n6.28 Explain the distinction between total and partial constraints.\nTools\nMany database systems provide tools for database design that support E-Rdiagrams.\nThese tools help a designer create E-Rdiagrams, and they can automatically create cor-\nresponding tables in a database. See bibliographical notes of Chapter 1 for references\nto database-system vendors\u2019 web sites.\nThere are also several database-independent data modeling tools that support E-R\ndiagrams and UML class diagrams.\nDia, which is a free diagram editor that runs on multiple platforms such as Linux\nand Windows, supports E-Rdiagrams and UML class diagrams. To represent entities\nwith attributes, you can use either classes from the UML library or tables from the\nDatabase library provided by Dia, since the default E-Rnotation in Dia represents\nattributes as ovals. The free online diagram editor LucidChart allows you to create E-R\ndiagrams with entities represented in the same ways as we do. To create relationships,\nwe suggest you use diamonds from the Flowchart shape collection. Draw.io is another\nonline diagram editor that supports E-Rdiagrams.\nCommercial tools include IBM Rational Rose Modeler, Microsoft Visio, ERwin\nData Modeler, Poseidon for UML ,a n dS m a r t D r a w .\nFurther Reading\nThe E-Rdata model was introduced by [Chen (1976)]. The Integration De\ufb01nition\nfor Information Modeling (IDEF1X) standard [NIST (1993)] released by the United\nStates National Institute of Standards and Technology (NIST) de\ufb01ned standards for\nE-Rd i a g r a m s .H o w e v e r ,av a r i e t yo f E-Rnotations are in use today.\n[Thalheim (2000)] provides a detailed textbook coverage of research in E-Rmod-\neling.\nAs of 2018, the current UML version was 2.5, which was released in June 2015. See\nwww.uml.org for more information on UML standards and tools.\n", "329": "Further Reading 301\nBibliography\n[Chen (1976)] P. P. Chen, \u201cThe Entity-Relationship Model: Toward a Uni\ufb01ed View of Data\u201d,\nACM Transactions on Database Systems , Volume 1, Number 1 (1976), pages 9\u201336.\n[NIST (1993)] NIST, \u201cIntegration De\ufb01nition for Information Modeling (IDEF1X)\u201d, Tech-\nnical Report Federal Information Processing Standards Publication 184, National Institute\nof Standards and Technology (NIST) (1993).\n[Thalheim (2000)] B. Thalheim, Entity-Relationship Modeling: Foundations of Database Tech-\nnology , Springer Verlag (2000).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "330": "", "331": "CHAPTER7\nRelational Database Design\nIn this chapter, we consider the problem of designing a schema for a relational database.\nMany of the issues in doing so are similar to design issues we considered in Chapter 6\nusing the E-Rmodel.\nIn general, the goal of relational database design is to generate a set of relation\nschemas that allows us to store information without unnecessary redundancy, yet also\nallows us to retrieve information easily. This is accomplished by designing schemas that\na r ei na na p p r o p r i a t e normal form . To determine whether a relation schema is in one of\nthe desirable normal forms, we need information about the real-world enterprise that\nwe are modeling with the database. Some of this information exists in a well-designed\nE-Rdiagram, but additional information about the enterprise may be needed as well.\nIn this chapter, we introduce a formal approach to relational database design based\non the notion of functional dependencies. We then de\ufb01ne normal forms in terms of\nfunctional dependencies and other types of data dependencies. First, however, we view\nthe problem of relational design from the standpoint of the schemas derived from a\ngiven entity-relationship design.\n7.1 Features of Good Relational Designs\nOur study of entity-relationship design in Chapter 6 provides an excellent starting point\nfor creating a relational database design. We saw in Section 6.7 that it is possible to gen-\nerate a set of relation schemas directly from the E-Rdesign. The goodness (or badness)\nof the resulting set of schemas depends on how good the E-Rdesign was in the \ufb01rst\nplace. Later in this chapter, we shall study precise ways of assessing the desirability of\na collection of relation schemas. However, we can go a long way toward a good design\nusing concepts we have already studied. For ease of reference, we repeat the schemas\nfor the university database in Figure 7.1.\nSuppose that we had started out when designing the university enterprise with the\nschema in\ndep.\nin\ndep(ID,name ,salary ,dept\nname ,building ,budget )\n303\n", "332": "304 Chapter 7 Relational Database Design\nclassroom (building\n ,room\n number\n ,capacity )\ndepartment (dept\nname\n ,building ,budget )\ncourse (course\n id\n,title,dept\nname ,credits )\ninstructor (ID\n,name ,dept\nname ,salary )\nsection (course\n id\n,sec\nid\n,semester\n ,year\n,building ,room\n number ,time\n slot\nid)\nteaches (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n)\nstudent (ID\n,name ,dept\nname ,tot\ncred)\ntakes (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n,grade )\nadvisor (s\nID\n,i\nID)\ntime\n slot(time\n slot\nid\n,day\n,start\n time\n,end\ntime)\nprereq (course\n id\n,prereq\n id\n)\nFigure 7.1 Database schema for the university example.\nThis represents the result of a natural join on the relations corresponding to instructor\nanddepartment . This seems like a good idea because some queries can be expressed\nusing fewer joins, until we think carefully about the facts about the university that led\nto our E-Rdesign.\nLet us consider the instance of the in\ndeprelation shown in Figure 7.2. Notice\nthat we have to repeat the department information (\u201cbuilding\u201d and \u201cbudget\u201d) once for\neach instructor in the department. For example, the information about the Comp. Sci.\ndepartment (Taylor, 100000) is included in the tuples of instructors Katz, Srinivasan,\nand Brandt.\nIt is important that all these tuples agree as to the budget amount since otherwise\nour database would be inconsistent. In our original design using instructor anddepart-\nment , we stored the amount of each budget exactly once. This suggests that using in\ndepis a bad idea since it stores the budget amounts redundantly and runs the risk that\nsome user might update the budget amount in one tuple but not all, and thus create\ninconsistency.\nEven if we decided to live with the redundancy problem, there is still another prob-\nlem with the in\ndepschema. Suppose we are creating a new department in the uni-\nversity. In the alternative design above, we cannot represent directly the information\nconcerning a department ( dept\nname ,building ,budget ) unless that department has at\nleast one instructor at the university. This is because tuples in the in\ndeptable require\nvalues for ID,name ,a n d salary . This means that we cannot record information about\nthe newly created department until the \ufb01rst instructor is hired for the new department.\nIn the old design, the schema department can handle this, but under the revised design,\nwe would have to create a tuple with a null value for building andbudget .I ns o m ec a s e s\nnull values are troublesome, as we saw in our study of SQL. However, if we decide that\n", "333": "7.1 Features of Good Relational Designs 305\nID\n name\n salary\n dept\nname\n building\n budget\n22222\n Einstein\n 95000\n Physics\n Watson\n 70000\n12121\n Wu\n 90000\n Finance\n Painter\n 120000\n32343\n El Said\n 60000\n History\n Painter\n 50000\n45565\n Katz\n 75000\n Comp. Sci.\n Taylor\n 100000\n98345\n Kim\n 80000\n Elec. Eng.\n Taylor\n 85000\n76766\n Crick\n 72000\n Biology\n Watson\n 90000\n10101\n Srinivasan\n 65000\n Comp. Sci.\n Taylor\n 100000\n58583\n Cali\ufb01eri\n 62000\n History\n Painter\n 50000\n83821\n Brandt\n 92000\n Comp. Sci.\n Taylor\n 100000\n15151\n Mozart\n 40000\n Music\n Packard\n 80000\n33456\n Gold\n 87000\n Physics\n Watson\n 70000\n76543\n Singh\n 80000\n Finance\n Painter\n 120000\nFigure 7.2 The in\ndeprelation.\nthis is not a problem to us in this case, then we can proceed to use the revised design,\nthough, as we noted, we would still have the redundancy problem.\n7.1.1 Decomposition\nThe only way to avoid the repetition-of-information problem in the in\ndepschema is to\ndecompose it into two schemas (in this case, the instructor anddepartment schemas).\nLater on in this chapter we shall present algorithms to decide which schemas are ap-\npropriate and which ones are not. In general, a schema that exhibits repetition of in-\nformation may have to be decomposed into several smaller schemas.\nNot all decompositions of schemas are helpful. Consider an extreme case in which\nall schemas consist of one attribute. No interesting relationships of any kind could\nbe expressed. Now consider a less extreme case where we choose to decompose the\nemployee schema (Section 6.8):\nemployee (ID,name ,street ,city,salary )\ninto the following two schemas:\nemployee1 (ID,name )\nemployee2 (name ,street ,city,salary )\nThe \ufb02aw in this decomposition arises from the possibility that the enterprise has two\nemployees with the same name. This is not unlikely in practice, as many cultures have\ncertain highly popular names. Each person would have a unique employee-id, which\nis why IDcan serve as the primary key. As an example, let us assume two employees,\n", "334": "306 Chapter 7 Relational Database Design\nboth named Kim, work at the university and have the following tuples in the relation\non schema employee in the original design:\n(57766, Kim, Main, Perryridge, 75000)\n(98776, Kim, North, Hampton, 67000)\nFigure 7.3 shows these tuples, the resulting tuples using the schemas resulting from\nthe decomposition, and the result if we attempted to regenerate the original tuples us-\ning a natural join. As we see in the \ufb01gure, the two original tuples appear in the result\nalong with two new tuples that incorrectly mix data values pertaining to the two em-\nployees named Kim. Although we have more tuples, we actually have less information\nin the following sense. We can indicate that a certain street, city, and salary pertain\nto someone named Kim, but we are unable to distinguish which of the Kims. Thus,\nour decomposition is unable to represent certain important facts about the university\nID name street city salary\n...\n57766\n98776...Kim\nKimMain\nNorthPerryridge\nHampton75000\n67000\nID name\n...\n57766\n98776...Kim\nKimname street city salary\n75000\n67000Main\nNorthPerryridge\nHampton...\nKim\nKim...\nID name street city salaryemployee\n...\n57766\n57766\n98776\n98776...75000\n67000\n75000\n67000Perryridge\nHampton\nPerryridge\nHamptonMain\nNorth\nMain\nNorthKim\nKim\nKim\nKimnatural join\nFigure 7.3 Loss of information via a bad decomposition.\n", "335": "7.1 Features of Good Relational Designs 307\nemployees. We would like to avoid such decompositions. We shall refer to such decom-\np o s i t i o n sa sb e i n g lossy decompositions , and, conversely, to those that are not as lossless\ndecompositions .\nFor the remainder of the text we shall insist that all decompositions should be\nlossless decompositions.\n7.1.2 Lossless Decomposition\nLetRbe a relation schema and let R1andR2form a decomposition of R\u2014thatis,view-\ningR,R1,a n d R2as sets of attributes, R=R1\u222aR2. We say that the decomposition is a\nlossless decomposition if there is no loss of information by replacing Rwith two relation\nschemas R1andR2. Loss of information occurs if it is possible to have an instance of\nar e l a t i o n r(R) that includes information that cannot be represented if instead of the\ninstance of r(R)w em u s tu s ei n s t a n c e so f r1(R1)a n d r2(R2). More precisely, we say\nthe decomposition is lossless if, for all legal (we shall formally de\ufb01ne \u201clegal\u201d in Section\n7.2.2.) database instances, relation rcontains the same set of tuples as the result of the\nfollowing SQL query:1\nselect *\nfrom (select R1from r)\nnatural join\n(select R2from r)\nThis is stated more succinctly in the relational algebra as:\n\u03a0R1(r)\u22c8\u03a0R2(r)=r\nIn other words, if we project ronto R1andR2, and compute the natural join of the\nprojection results, we get back exactly r.\nConversely, a decomposition is lossy if when we compute the natural join of the\nprojection results, we get a proper superset of the original relation. This is stated more\nsuccinctly in the relational algebra as:\nr\u2282\u03a0R1(r)\u22c8\u03a0R2(r)\nLet us return to our decomposition of the employee schema into employee1 and\nemployee2 (Figure 7.3) and a case where two or more employees have the same name.\nThe result of employee1 natural join employee2 is a superset of the original relation\nemployee , but the decomposition is lossy since the join result has lost information about\nwhich employee identi\ufb01ers correspond to which addresses and salaries.\n1The de\ufb01nition of lossless is stated assuming that no attribut e that appears on the left side of a functional dependency\ncan have a null value. This is explored further in Exercise 7.10.\n", "336": "308 Chapter 7 Relational Database Design\nIt may seem counterintuitive that we have more tuples but lessinformation, but\nthat is indeed the case. The decomposed version is unable to represent the absence of\na connection between a name and an address or salary, and absence of a connection\nis indeed information.\n7.1.3 Normalization Theory\nWe are now in a position to de\ufb01ne a general methodology for deriving a set of\nschemas each of which is in \u201cgood form\u201d; that is, does not su\ufb00er from the repetition-\nof-information problem.\nThe method for designing a relational database is to use a process commonly\nknown as normalization . The goal is to generate a set of relation schemas that allows\nus to store information without unnecessary redundancy, yet also allows us to retrieve\ninformation easily. The approach is:\n\u2022Decide if a given relation schema is in \u201cgood form.\u201d There are a number of di\ufb00erent\nforms (called normal form s), which we cover in Section 7.3.\n\u2022If a given relation schema is not in \u201cgood form,\u201d then we decompose it into a\nnumber of smaller relation schemas, each of which is in an appropriate normal\nform. The decomposition must be a lossless decomposition.\nTo determine whether a relation schema is in one of the desirable normal forms, we\nneed additional information about the real-world enterprise that we are modeling with\nthe database. The most common approach is to use functional dependencies ,w h i c hw e\ncover in Section 7.2.\n7.2 Decomposition Using Functional Dependencies\nA database models a set of entities and relationships in the real world. There are usually\na variety of constraints (rules) on the data in the real world. For example, some of the\nconstraints that are expected to hold in a university database are:\n1.Students and instructors are uniquely identi\ufb01ed by their ID.\n2.Each student and instructor has only one name.\n3.Each instructor and student is (primarily) associated with only one department.2\n4.Each department has only one value for its budget, and only one associated build-\ning.\n2An instructor, in most real universities, can be associated with more than one department, for example, via a joint\nappointment or in the case of adjunct faculty. Similarly, a student may have two (or more) majors or a minor. Our\nsimpli\ufb01ed university schema models only the primary department associated with each instructor or student.\n", "337": "7.2 Decomposition Using Functional Dependencies 309\nAn instance of a relation that satis\ufb01es all such real-world constraints is called a\nlegal instance of the relation; a legal instance of a database is one where all the relation\ninstances are legal instances.\n7.2.1 Notational Conventions\nIn discussing algorithms for relational database design, we shall need to talk about\narbitrary relations and their schema, rather than talking only about examples. Recalling\nour introduction to the relational model in Chapter 2, we summarize our notation here.\n\u2022In general, we use Greek letters for sets of attributes (e.g., \u03b1). We use an uppercase\nRoman letter to refer to a relation schema. We use the notation r(R) to show that\nthe schema Ris for relation r.\nA relation schema is a set of attributes, but not all sets of attributes are schemas.\nWhen we use a lowercase Greek letter, we are referring to a set of attributes that\nmay or may not be a schema. A Roman letter is used when we wish to indicate\nthat the set of attributes is de\ufb01nitely a schema.\n\u2022When a set of attributes is a superkey, we may denote it by K. A superkey pertains\nto a speci\ufb01c relation schema, so we use the terminology \u201c Kis a superkey for R.\u201d\n\u2022We use a lowercase name for relations. In our examples, these names are intended\nto be realistic (e.g., instructor ), while in our de\ufb01nitions and algorithms, we use\nsingle letters, like r.\n\u2022The notation r(R) thus refers to the relation rwith schema R.W h e nw ew r i t e r(R),\nwe thus refer both to the relation and its schema.\n\u2022A relation, has a particular value at any given time; we refer to that as an instance\nand use the term \u201cinstance of r.\u201d When it is clear that we are talking about an\ninstance, we may use simply the relation name (e.g., r).\nFor simplicity, we assume that attribute names have only one meaning within the\ndatabase schema.\n7.2.2 Keys and Functional Dependencies\nSome of the most commonly used types of real-world constraints can be represented\nformally as keys (superkeys, candidate keys, and primary keys), or as functional depen-\ndencies, which we de\ufb01ne below.\nIn Section 2.3, we de\ufb01ned the notion of a superkey as a set of one or more attributes\nthat, taken collectively, allows us to identify uniquely a tuple in the relation. We restate\nthat de\ufb01nition here as follows: Given r(R), a subset KofRis asuperkey ofr(R)i f ,i n\nany legal instance of r(R), for all pairs t1andt2of tuples in the instance of rift1\u2260t2,\nthen t1[K]\u2260t2[K]. That is, no two tuples in any legal instance of relation r(R)m a y\n", "338": "310 Chapter 7 Relational Database Design\nhave the same value on attribute set K.3If no two tuples in rhave the same value on\nK,t h e na K-value uniquely identi\ufb01es a tuple in r.\nWhereas a superkey is a set of attributes that uniquely identi\ufb01es an entire tuple, a\nfunctional dependency allows us to express constraints that uniquely identify the values\nof certain attributes. Consider a relation schema r(R), and let \u03b1\u2286Rand\u03b2\u2286R.\n\u2022Given an instance of r(R), we say that the instance satis\ufb01es thefunctional de-\npendency \u03b1\u2192\u03b2if for all pairs of tuples t1and t2in the instance such that\nt1[\u03b1]=t2[\u03b1], it is also the case that t1[\u03b2]=t2[\u03b2].\n\u2022We say that the functional dependency \u03b1\u2192\u03b2holds on schema r(R) if, every legal\ninstance of r(R) satis\ufb01es the functional dependency.\nUsing the functional-dependency notation, we say that Ki sas u p e r k e yf o rr (R)i f\nthe functional dependency K\u2192Rholds on r(R). In other words, Kis a superkey if,\nfor every legal instance of r(R), for every pair of tuples t1andt2from the instance,\nwhenever t1[K]=t2[K], it is also the case that t1[R]=t2[R]( i . e . , t1=t2).4\nFunctional dependencies allow us to express constraints that we cannot express\nwith superkeys. In Section 7.1, we considered the schema:\nin\ndep(ID,name ,salary ,dept\nname ,building ,budget )\nin which the functional dependency dept\nname\u2192budget holds because for each de-\npartment (identi\ufb01ed by dept\nname ) there is a unique budget amount.\nWe denote the fact that the pair of attributes ( ID,dept\nname ) forms a superkey for\nin\ndepby writing:\nID,dept\nname\u2192name ,salary ,building ,budget\nWe shall use functional dependencies in two ways:\n1.To test instances of relations to see whether they satisfy ag i v e ns e t Fof functional\ndependencies.\n2.To specify constraints on the set of legal relations. We shall thus concern our-\nselves with onlythose relation instances that satisfy a given set of functional de-\npendencies. If we wish to constrain ourselves to relations on schema r(R)t h a t\nsatisfy a set Fof functional dependencies, we say that Fholds onr(R).\n3In our discussion of functional dependencies, we use equality ( =) in the normal mathematical sense, not the three-\nvalued-logic sense of SQL. Said di\ufb00erently, in discussing functiona l dependencies, we assume no null values.\n4Note that we assume here that relations are sets. SQLdeals with multisets, and a primary key declaration in SQLfor a\nset of attributes Krequires not only that t1=t2ift1[K]=t2[K], but also that there be no duplicate tuples. SQLalso\nrequires that attributes in the set Kcannot be assigned a nullvalue.\n", "339": "7.2 Decomposition Using Functional Dependencies 311\nA\n B\n C\n D\na1\n b1\n c1\n d1\na1\n b2\n c1\n d2\na2\n b2\n c2\n d2\na2\n b3\n c2\n d3\na3\n b3\n c2\n d4\nFigure 7.4 Sample instance of relation r.\nLet us consider the instance of relation rof Figure 7.4, to see which functional\ndependencies are satis\ufb01ed. Observe that A\u2192Cis satis\ufb01ed. There are two tuples that\nhave an Avalue of a1. These tuples have the same Cvalue\u2014namely, c1. Similarly, the\ntwo tuples with an Avalue of a2have the same Cvalue, c2.T h e r ea r en oo t h e rp a i r s\nof distinct tuples that have the same Avalue. The functional dependency C\u2192Ais not\nsatis\ufb01ed, however. To see that it is not, consider the tuples t1=(a2,b3,c2,d3)a n d t2=\n(a3,b3,c2,d4). These two tuples have the same Cvalues, c2, but they have di\ufb00erent A\nvalues, a2anda3, respectively. Thus, we have found a pair of tuples t1andt2such that\nt1[C]=t2[C], but t1[A]\u2260t2[A].\nSome functional dependencies are said to be trivial because they are satis\ufb01ed by all\nrelations. For example, A\u2192Ais satis\ufb01ed by all relations involving attribute A.R e a d i n g\nthe de\ufb01nition of functional dependency literally, we see that, for all tuples t1andt2such\nthatt1[A]=t2[A], it is the case that t1[A]=t2[A]. Similarly, AB\u2192Ais satis\ufb01ed\nby all relations involving attribute A. In general, a functional dependency of the form\n\u03b1\u2192\u03b2is trivial if \u03b2\u2286\u03b1.\nIt is important to realize that an instance of a relation may satisfy some functional\ndependencies that are not required to hold on the relation\u2019s schema. In the instance of\ntheclassroom relation of Figure 7.5, we see that room\n number\u2192capacity is satis\ufb01ed.\nHowever, we believe that, in the real world, two classrooms in di\ufb00erent buildings can\nhave the same room number but with di\ufb00erent room capacity. Thus, it is possible, at\nsome time, to have an instance of the classroom relation in which room\n number\u2192\ncapacity is not satis\ufb01ed. So, we would not include room\n number\u2192capacity in the set of\nbuilding\n room\n number\n capacity\nPackard\n 101\n 500\nPainter\n 514\n 10\nTaylor\n 3128\n 70\nWatson\n 100\n 30\nWatson\n 120\n 50\nFigure 7.5 An instance of the classroom relation.\n", "340": "312 Chapter 7 Relational Database Design\nfunctional dependencies that hold on the schema for the classroom relation. However,\nwe would expect the functional dependency building ,room\n number\u2192capacity to hold\non the classroom schema.\nBecause we assume that attribute names have only one meaning in the database\nschema, if we state that a functional dependency \u03b1\u2192\u03b2holds as a constraint on the\ndatabase, then for any schema Rsuch that \u03b1\u2286Rand\u03b2\u2286R,\u03b1\u2192\u03b2must hold.\nGiven that a set of functional dependencies Fholds on a relation r(R), it may\nbe possible to infer that certain other functional dependencies must also hold on the\nrelation. For example, given a schema r(A,B,C), if functional dependencies A\u2192Band\nB\u2192Chold on r, we can infer the functional dependency A\u2192Cmust also hold on r.\nThis is because, given any value of A, there can be only one corresponding value for B,\nand for that value of B, there can only be one corresponding value for C.W es t u d yi n\nSection 7.4.1, how to make such inferences.\nWe shall use the notation F+to denote the closure of the set F,t h a ti s ,t h es e to f\nall functional dependencies that can be inferred given the set F.F+contains all of the\nfunctional dependencies in F.\n7.2.3 Lossless Decomposition and Functional Dependencies\nWe can use functional dependencies to show when certain decompositions are lossless.\nLetR,R1,R2,a n d Fbe as above. R1andR2form a lossless decomposition of Rif at\nleast one of the following functional dependencies is in F+:\n\u2022R1\u2229R2\u2192R1\n\u2022R1\u2229R2\u2192R2\nIn other words, if R1\u2229R2forms a superkey for either R1orR2, the decomposition of Ris\na lossless decomposition. We can use attribut e closure to test e\ufb03ciently for superkeys,\nas we have seen earlier.\nTo illustrate this, consider the schema\nin\ndep(ID,name ,salary ,dept\nname ,building ,budget )\nthat we decomposed in Section 7.1 into the instructor anddepartment schemas:\ninstructor (ID,name ,dept\nname ,salary )\ndepartment (dept\nname ,building ,budget )\nConsider the intersection of these two schemas, which is dept\nname .W es e et h a tb e -\ncause dept\nname\u2192dept\nname ,building ,budget , the lossless-decomposition rule is sat-\nis\ufb01ed.\n", "341": "7.3 Normal Forms 313\nFor the general case of decomposition of as c h e m ai n t om u l t i p l es c h e m a sa to n c e ,\nthe test for lossless decomposition is more complicated. See the Further Reading sec-\ntion at the end of this chapter for references on this topic.\nWhile the test for binary decomposition is clearly a su\ufb03cient condition for lossless\ndecomposition, it is a necessary condition o nly if all constraints are functional depen-\ndencies. We shall see other types of constraints later (in particular, a type of constraint\ncalled multivalued dependencies discussed in Section 7.6.1) that can ensure that a de-\ncomposition is lossless even if no functional dependencies are present.\nSuppose we decompose a relation schema r(R)i n t o r1(R1)a n d r2(R2), where R1\u2229\nR2\u2192R1.5Then the following SQL constraints must be imposed on the decomposed\nschema to ensure their contents are consistent with the original schema.\n\u2022R1\u2229R2is the primary key of r1.\nThis constraint enforces the functional dependency.\n\u2022R1\u2229R2is a foreign key from r2referencing r1.\nThis constraint ensures that each tuple in r2has a matching tuple in r1,w i t h o u t\nwhich it would not appear in the natural join of r1andr2.\nIfr1orr2is decomposed further, as long as the decomposition ensures that all attributes\ninR1\u2229R2are in one relation, the primary or foreign-key constraint on r1orr2would\nbe inherited by that relation.\n7.3 Normal Forms\nAs stated in Section 7.1.3, there are a number of di\ufb00erent normal forms that are used\nin designing relational databases. In this section, we cover two of the most common\nones.\n7.3.1 Boyce\u2013Codd Normal Form\nO n eo ft h em o r ed e s i r a b l en o r m a lf o r m st h a tw ec a no b t a i ni s Boyce\u2013Codd normal form\n(BCNF ). It eliminates all redundancy that can be discovered based on functional depen-\ndencies, though, as we shall see in Section 7.6, there may be other types of redundancy\nremaining.\n7.3.1.1 De\ufb01nition\nA relation schema Ris in BCNF with respect to a set Fof functional dependencies if,\nfor all functional dependencies in F+of the form \u03b1\u2192\u03b2,w h e r e\u03b1\u2286Rand\u03b2\u2286R,a t\nleast one of the following holds:\n5The case for R1\u2229R2\u2192R2is symmetrical, and ignored.\n", "342": "314 Chapter 7 Relational Database Design\n\u2022\u03b1\u2192\u03b2is a trivial functional dependency (i.e., \u03b2\u2286\u03b1).\n\u2022\u03b1is a superkey for schema R.\nA database design is in BCNF if each member of the set of relation schemas that con-\nstitutes the design is in BCNF .\nWe have already seen in Section 7.1 an example of a relational schema that is not\ninBCNF :\nin\ndep(ID,name ,salary ,dept\nname ,building ,budget )\nThe functional dependency dept\nname\u2192budget holds on in\ndep, but dept\nname is not a\nsuperkey (because a department may have a number of di\ufb00erent instructors). In Section\n7.1 we saw that the decomposition of in\ndepintoinstructor anddepartment is a better\ndesign. The instructor schema is in BCNF . All of the nontrivial functional dependencies\nthat hold, such as:\nID\u2192name ,dept\nname ,salary\ninclude IDo nt h el e f ts i d eo ft h ea r r o w ,a n d IDis a superkey (actually, in this case, the\nprimary key) for instructor . (In other words, there is no nontrivial functional depen-\ndency with any combination of name ,dept\nname ,a n d salary ,w i t h o u t ID,o nt h el e f t\nside.) Thus, instructor is in BCNF .\nSimilarly, the department schema is in BCNF because all of the nontrivial functional\ndependencies that hold, such as:\ndept\nname\u2192building ,budget\ninclude dept\nname on the left side of the arrow, and dept\nname is a superkey (and the\nprimary key) for department .T h u s , department is in BCNF .\nWe now state a general rule for decomposing schemas that are not in BCNF .L e t\nRb eas c h e m at h a ti sn o ti n BCNF . Then there is at least one nontrivial functional\ndependency \u03b1\u2192\u03b2such that \u03b1is not a superkey for R.W er e p l a c e Rin our design with\ntwo schemas:\n\u2022(\u03b1\u222a\u03b2 )\n\u2022(R\u2212(\u03b2\u2212\u03b1 ))\nIn the case of in\ndepabove,\u03b1=dept\nname ,\u03b2= {building ,budget },a n d in\ndepis replaced\nby\n\u2022(\u03b1\u222a\u03b2 )=(dept\nname ,building ,budget )\n\u2022(R\u2212(\u03b2\u2212\u03b1 )) = ( ID,name ,dept\nname ,salary )\n", "343": "7.3 Normal Forms 315\ndept_name\nbuilding\nbudgetdepartment\ndept_advisorinstructor\nID\nname\nsalarystudent\nID\nname\ntot_cred\nFigure 7.6 The dept\nadvisor relationship set.\nIn this example, it turns out that \u03b2\u2212\u03b1=\u03b2 . We need to state the rule as we did so as\nto deal correctly with functional dependencies that have attributes that appear on both\nsides of the arrow. The technical reasons for this are covered later in Section 7.5.1.\nWhen we decompose a schema that is not in BCNF , it may be that one or more\nof the resulting schemas are not in BCNF .I ns u c hc a s e s ,f u r t h e rd e c o m p o s i t i o ni s\nrequired, the eventual result of which is a set of BCNF schemas.\n7.3.1.2 BCNF and Dependency Preservation\nWe have seen several ways in which to express database consistency constraints:\nprimary-key constraints, functional dependencies, check constraints, assertions, and\ntriggers. Testing these constraints each time the database is updated can be costly and,\ntherefore, it is useful to design the database in a way that constraints can be tested e\ufb03-\nciently. In particular, if testing a functional dependency can be done by considering just\none relation, then the cost of testing this constraint is low. We shall see that, in some\ncases, decomposition into BCNF can prevent e\ufb03cient testing of certain functional de-\npendencies.\nTo illustrate this, suppose that we make a small change to our university organiza-\ntion. In the design of Figure 6.15, a student may have only one advisor. This follows\nfrom the relationship set advisor being many-to-one from student toadvisor . The \u201csmall\u201d\nchange we shall make is that an instructor can be associated with only a single depart-\nment, and a student may have more than one advisor, but no more than one from a\ngiven department.6\nOne way to implement this change using the E-Rdesign is by replacing the advi-\nsorrelationship set with a ternary relationship set, dept\nadvisor , involving entity sets\ninstructor ,student ,a n d department that is many-to-one from the pair {student ,instruc-\ntor}todepartment as shown in Figure 7.6. The E-Rdiagram speci\ufb01es the constraint that\n6Such an arrangement makes sense for students with a double major.\n", "344": "316 Chapter 7 Relational Database Design\n\u201ca student may have more than one advisor, but at most one corresponding to a given\ndepartment.\u201d\nWith this new E-Rdiagram, the schemas for the instructor ,department ,a n d student\nrelations are unchanged. However, the schema derived from the dept\nadvisor relation-\nship set is now:\ndept\nadvisor (s\nID,i\nID,dept\nname )\nAlthough not speci\ufb01ed in the E-Rdiagram, suppose we have the additional con-\nstraint that \u201can instructor can act as advisor for only a single department.\u201d\nThen, the following functional dependencies hold on dept\nadvisor :\ni\nID\u2192dept\nname\ns\nID,dept\nname\u2192i\nID\nThe \ufb01rst functional dependency follows from our requirement that \u201can instructor can\nact as an advisor for only one department.\u201d The second functional dependency fol-\nlows from our requirement that \u201ca student may have at most one advisor for a given\ndepartment.\u201d\nNotice that with this design, we are forced to repeat the department name once\nf o re a c ht i m ea ni n s t r u c t o rp a r t i c i p a t e si na dept\nadvisor relationship. We see that dept\nadvisor is not in BCNF because i\nIDis not a superkey. Following our rule for BCNF\ndecomposition, we get:\n(s\nID,i\nID)\n(i\nID,dept\nname )\nBoth the above schemas are BCNF . (In fact, you can verify that any schema with only\ntwo attributes is in BCNF by de\ufb01nition.)\nN o t e ,h o w e v e r ,t h a ti no u r BCNF design, there is no schema that includes all the\nattributes appearing in the functional dependency s\nID,dept\nname\u2192i\nID.T h eo n l y\ndependency that can be enforced on the individual decomposed relations is ID\u2192dept\nname . The functional dependency s\nID,dept\nname\u2192i\nIDcan only be checked by\ncomputing the join of the decomposed relations.7\nBecause our design does not permit the enforcement of this functional dependency\nwithout a join, we say that our design is notdependency preserving (we provide a formal\nde\ufb01nition of dependency preservation in Section 7.4.4). Because dependency preser-\nvation is usually considered desirable, we consider another normal form, weaker than\nBCNF , that will allow us to preserve dependencies. That normal form is called the third\nnormal form.8\n7Technically, it is possible that a dependency whose attri butes do not all appear in any one schema is still implicitly\nenforced, because of the presence of other dependencies that imply it logically. We address that case in Section 7.4.4.\n8You may have noted that we skipped second normal form. It is of historical signi\ufb01cance only and, in practice, one of\nthird normal form or BCNF is always a better choice. We explore second normal form in Exercise 7.19. First normal\nform pertains to attribute domains, not decomposition. We discuss it in Section 7.8.\n", "345": "7.3 Normal Forms 317\n7.3.2 Third Normal Form\nBCNF requires that all nontrivial dependencies be of the form \u03b1\u2192\u03b2,w h e r e\u03b1is a\nsuperkey. Third normal form ( 3NF) relaxes this constraint slightly by allowing certain\nnontrivial functional dependencies whose left side is not a superkey. Before we de\ufb01ne\n3NF, we recall that a candidate key is a minimal superkey\u2014that is, a superkey no proper\nsubset of which is also a superkey.\nA relation schema Ris in third normal form with respect to a set Fof functional\ndependencies if, for all functional dependencies in F+of the form \u03b1\u2192\u03b2,w h e r e\u03b1\u2286R\nand\u03b2\u2286R, at least one of the following holds:\n\u2022\u03b1\u2192\u03b2is a trivial functional dependency.\n\u2022\u03b1is a superkey for R.\n\u2022Each attribute Ain\u03b2\u2212\u03b1 is contained in a candidate key for R.\nNote that the third condition above does not say that a single candidate key must con-\ntain all the attributes in \u03b2\u2212\u03b1 ; each attribute Ain\u03b2\u2212\u03b1 may be contained in a di\ufb00erent\ncandidate key.\nThe \ufb01rst two alternatives are the same as the two alternatives in the de\ufb01nition of\nBCNF . The third alternative in the 3NF de\ufb01nition seems rather unintuitive, and it is not\nobvious why it is useful. It represents, in some sense, a minimal relaxation of the BCNF\nconditions that helps ensure that every schema has a dependency-preserving decompo-\nsition into 3NF. Its purpose will become more clear later, when we study decomposition\ninto 3NF.\nObserve that any schema that satis\ufb01es BCNF also satis\ufb01es 3NF, since each of its\nfunctional dependencies would satisfy one of the \ufb01rst two alternatives. BCNF is there-\nfore a more restrictive normal form than is 3NF.\nThe de\ufb01nition of 3NF allows certain functional dependencies that are not allowed\ninBCNF .Ad e p e n d e n c y \u03b1\u2192\u03b2that satis\ufb01es only the third alternative of the 3NF\nde\ufb01nition is not allowed in BCNF but is allowed in 3NF.9\nNow, let us again consider the schema for the dept\nadvisor relation, which has the\nfollowing functional dependencies:\ni\nID\u2192dept\nname\ns\nID,dept\nname\u2192i\nID\nIn Section 7.3.1.2, we argued that the functional dependency \u201c i\nID\u2192dept\nname \u201d\ncaused the dept\nadvisor schema not to be in BCNF .N o t et h a th e r e \u03b1=i\nID,\u03b2=dept\nname ,a n d\u03b2\u2212\u03b1= dept\nname . Since the functional dependency s\nID,dept\nname\u2192\n9These dependencies are examples of transitive dependencies (see Practice Exercise 7.18). The original de\ufb01nition of\n3NFwas in terms of transitive dependencies. The de\ufb01ni tion we use is equivalent but easier to understand.\n", "346": "318 Chapter 7 Relational Database Design\ni\nIDholds on dept\nadvisor , the attribute dept\nname is contained in a candidate key and,\ntherefore, dept\nadvisor is in 3NF.\nWe have seen the trade-o\ufb00 that must be made between BCNF and3NFwhen there is\nno dependency-preserving BCNF design. These trade-o\ufb00s are described in more detail\nin Section 7.3.3.\n7.3.3 Comparison of BCNF and 3NF\nOf the two normal forms for relational database schemas, 3NF and BCNF there are\nadvantages to 3NF in that we know that it is always possible to obtain a 3NF design\nwithout sacri\ufb01cing losslessness or depende ncy preservation. Nevertheless, there are\ndisadvantages to 3NF: We may have to use null values to represent some of the possible\nmeaningful relationships among data items, and there is the problem of repetition of\ninformation.\nOur goals of database design with functional dependencies are:\n1.BCNF .\n2.Losslessness.\n3.Dependency preservation.\nSince it is not always possible to satisfy all three, we may be forced to choose between\nBCNF and dependency preservation with 3NF.\nIt is worth noting that SQL does not provide a way of specifying functional depen-\ndencies, except for the special case of declaring superkeys by using the primary key or\nunique constraints. It is possible, although a little complicated, to write assertions that\nenforce a functional dependency (see Practice Exercise 7.9); unfortunately, currently\nno database system supports the complex assertions that are required to enforce ar-\nbitrary functional dependencies, and the assertions would be expensive to test. Thus\neven if we had a dependency-preserving decomposition, if we use standard SQL we can\ntest e\ufb03ciently only those functional dependencies whose left-hand side is a key.\nAlthough testing functional dependencies may involve a join if the decomposition\nis not dependency preserving, if the database system supports materialized views, we\ncould in principle reduce the cost by storing the join result as materialized view; how-\never, this approach is feasible only if the database system supports primary key con-\nstraints or unique constraints on materialized views. On the negative side, there is a\nspace and time overhead due to the materialized view, but on the positive side, the\napplication programmer need not worry about writing code to keep redundant data\nconsistent on updates; it is the job of the database system to maintain the material-\nized view, that is, keep it up to date when the database is updated. (In Section 16.5, we\noutline how a database system can perform materialized view maintenance e\ufb03ciently.)\nUnfortunately, most current database systems limit constraints on materialized\nviews or do not support them at all. Even if such constraints are allowed, there is an\nadditional requirement: the database must update the view and check the constraint\n", "347": "7.3 Normal Forms 319\nimmediately (as part of the same transaction) when an underlying relation is updated.\nOtherwise, a constraint violation may get detected well after the update has been per-\nformed and the transaction that caused the violation has committed.\nIn summary, even if we are not able to get a dependency-preserving BCNF decom-\nposition, it is still preferable to opt for BCNF , since checking functional dependencies\nother than primary key constraints is di\ufb03cult in SQL.\n7.3.4 Higher Normal Forms\nUsing functional dependencies to decompose schemas may not be su\ufb03cient to avoid\nunnecessary repetition of information in certain cases. Consider a slight variation in the\ninstructor entity-set de\ufb01nition in which we record with each instructor a set of children\u2019s\nnames and a set of landline phone numbers that may be shared by multiple people.\nThus, phone\n number andchild\n name would be multivalued attributes and, following\nour rules for generating schemas from an E-Rdesign, we would have two schemas, one\nfor each of the multivalued attributes, phone\n number andchild\n name :\n(ID,child\n name )\n(ID,phone\n number )\nIf we were to combine these schemas to get\n(ID,child\n name ,phone\n number )\nw ew o u l d\ufb01 n dt h er e s u l tt ob ei n BCNF because no nontrivial functional dependencies\nhold. As a result we might think that such a combination is a good idea. However, such\na combination is a bad idea, as we can see by considering the example of an instruc-\ntor with two children and two phone numbers. For example, let the instructor with\nID99999 have two children named \u201cDavid\u201d and \u201cWilliam\u201d and two phone numbers,\n512-555-1234 and 512-555-4321. In the combined schema, we must repeat the phone\nnumbers once for each dependent:\n(99999, David, 512-555-1234)\n(99999, David, 512-555-4321)\n(99999, William, 512-555-1234)\n(99999, William, 512-555-4321)\nIf we did not repeat the phone numbers, and we stored only the \ufb01rst and last tu-\nples, we would have recorded the dependent names and the phone numbers, but the\nresultant tuples would imply that David corresponded to 512-555-1234, while William\ncorresponded to 512-555-4321. This would be incorrect.\nBecause normal forms based on functional dependencies are not su\ufb03cient to deal\nwith situations like this, other dependencies and normal forms have been de\ufb01ned. We\ncover these in Section 7.6 and Section 7.7.\n", "348": "320 Chapter 7 Relational Database Design\n7.4 Functional-Dependency Theory\nWe have seen in our examples that it is useful to be able to reason systematically about\nfunctional dependencies as part of a process of testing schemas for BCNF or3NF.\n7.4.1 Closure of a Set of Functional Dependencies\nWe shall see that, given a set Fof functional dependencies on a schema, we can prove\nthat certain other functional dependencies also hold on the schema. We say that such\nfunctional dependencies are \u201clogically implied\u201d by F. When testing for normal forms,\nit is not su\ufb03cient to consider the given set of functional dependencies; rather, we need\nto consider allfunctional dependencies that hold on the schema.\nMore formally, given a relation schema r(R), a functional dependency fonRislog-\nically implied by a set of functional dependencies FonRif every instance of a relation\nr(R) that satis\ufb01es Falso satis\ufb01es f.\nSuppose we are given a relation schema r(A,B,C,G,H,I) and the set of functional\ndependencies:\nA\u2192B\nA\u2192C\nCG\u2192H\nCG\u2192I\nB\u2192H\nThe functional dependency:\nA\u2192H\nis logically implied. That is, we can show that, whenever a relation instance satis\ufb01es\nour given set of functional dependencies, A\u2192Hmust also be satis\ufb01ed by that relation\ninstance. Suppose that t1andt2are tuples such that:\nt1[A]=t2[A]\nSince we are given that A\u2192B, it follows from the de\ufb01nition of functional dependency\nthat:\nt1[B]=t2[B]\nThen, since we are given that B\u2192H, it follows from the de\ufb01nition of functional de-\npendency that:\nt1[H]=t2[H]\n", "349": "7.4 Functional-Dependency Theory 321\nTherefore, we have shown that, whenever t1andt2are tuples such that t1[A]=t2[A],\nit must be that t1[H]=t2[H]. But that is exactly the de\ufb01nition of A\u2192H.\nLetFbe a set of functional dependencies. The closure ofF, denoted by F+,i st h e\nset of all functional dependencies logically implied by F.G i v e n F,w ec a nc o m p u t e F+\ndirectly from the formal de\ufb01nition of functional dependency. If Fwere large, this pro-\ncess would be lengthy and di\ufb03cult. Such a computation of F+requires arguments of the\ntype just used to show that A\u2192His in the closure of our example set of dependencies.\nAxioms , or rules of inference, provide a simpler technique for reasoning about func-\ntional dependencies. In the rules that follow, we use Greek letters ( \u03b1,\u03b2,\u03b3,\u2026)f o rs e t s\nof attributes and uppercase Roman letters from the beginning of the alphabet for indi-\nvidual attributes. We use \u03b1\u03b2to denote \u03b1\u222a\u03b2 .\nWe can use the following three rules to \ufb01nd logically implied functional dependen-\ncies. By applying these rules repeatedly , we can \ufb01nd all of F+,g i v e n F. This collection\nof rules is called Armstrong\u2019s axioms in honor of the person who \ufb01rst proposed it.\n\u2022Re\ufb02exivity rule .I f\u03b1is a set of attributes and \u03b2\u2286\u03b1,t h e n\u03b1\u2192\u03b2holds.\n\u2022Augmentation rule .I f\u03b1\u2192\u03b2holds and \u03b3is a set of attributes, then \u03b3\u03b1\u2192\u03b3\u03b2\nholds.\n\u2022Transitivity rule .I f\u03b1\u2192\u03b2holds and \u03b2\u2192\u03b3holds, then \u03b1\u2192\u03b3holds.\nArmstrong\u2019s axioms are sound , because they do not generate any incorrect func-\ntional dependencies. They are complete , because, for a given set Fof functional de-\npendencies, they allow us to generate all F+. The Further Reading section provides\nreferences for proofs of soundness and completeness.\nAlthough Armstrong\u2019s axioms are complete, it is tiresome to use them directly\nfor the computation of F+. To simplify matters further, we list additional rules. It is\npossible to use Armstrong\u2019s axioms to prove that these rules are sound (see Practice\nExercise 7.4, Practice Exercise 7.5, and Exercise 7.27).\n\u2022Union rule .I f\u03b1\u2192\u03b2holds and \u03b1\u2192\u03b3holds, then \u03b1\u2192\u03b2\u03b3holds.\n\u2022Decomposition rule .I f\u03b1\u2192\u03b2\u03b3holds, then \u03b1\u2192\u03b2holds and \u03b1\u2192\u03b3holds.\n\u2022Pseudotransitivity rule .I f\u03b1\u2192\u03b2holds and \u03b3\u03b2\u2192\u03b4holds, then \u03b1\u03b3\u2192\u03b4holds.\nLet us apply our rules to the example of schema R=(A,B,C,G,H,I)a n dt h es e t\nFof functional dependencies {A\u2192B,A\u2192C,CG\u2192H,CG\u2192I,B\u2192H}. We list\nseveral members of F+here:\n\u2022A\u2192H.S i n c e A\u2192BandB\u2192Hhold, we apply the transitivity rule. Observe that\nit was much easier to use Armstrong\u2019s axioms to show that A\u2192Hholds than it\nwas to argue directly from the de\ufb01nitions, as we did earlier in this section.\n\u2022CG\u2192HI.S i n c e CG\u2192HandCG\u2192I, the union rule implies that CG\u2192HI.\n", "350": "322 Chapter 7 Relational Database Design\nF+=F\napply the re\ufb02exivity rule /* Generates all trivial dependencies */\nrepeat\nfor each functional dependency finF+\napply the augmentation rule on f\nadd the resulting functional dependencies to F+\nfor each pair of functional dependencies f1andf2inF+\niff1andf2can be combined using transitivity\nadd the resulting functional dependency to F+\nuntil F+does not change any further\nFigure 7.7 A procedure to compute F+.\n\u2022AG\u2192I.S i n c e A\u2192CandCG\u2192I, the pseudotransitivity rule implies that\nAG\u2192Iholds.\nAnother way of \ufb01nding that AG\u2192Iholds is as follows: We use the augmen-\ntation rule on A\u2192Cto infer AG\u2192CG. Applying the transitivity rule to this\ndependency and CG\u2192I,w ei n f e r AG\u2192I.\nFigure 7.7 shows a procedure that demonstrates formally how to use Armstrong\u2019s\naxioms to compute F+. In this procedure, when a functional dependency is added to\nF+, it may be already present, and in that case there is no change to F+.W es h a l ls e e\nan alternative way of computing F+in Section 7.4.2.\nThe left-hand and right-hand sides of a functional dependency are both subsets\nofR.S i n c eas e to fs i z e nhas 2nsubsets, there are a total of 2n\u00d72n=22npossible\nfunctional dependencies, where nis the number of attributes in R. Each iteration of\nthe repeat loop of the procedure, except the last iteration, adds at least one functional\ndependency to F+. Thus, the procedure is guaranteed to terminate, though it may be\nvery lengthy.\n7.4.2 Closure of Attribute Sets\nWe say that an attribute Bisfunctionally determined by\u03b1if\u03b1\u2192B. To test whether\nas e t\u03b1is a superkey, we must devise an algorithm for computing the set of attributes\nfunctionally determined by \u03b1. One way of doing this is to compute F+,t a k ea l lf u n c -\ntional dependencies with \u03b1as the left-hand side, and take the union of the right-hand\nsides of all such dependencies. However, doing so can be expensive, since F+can be\nlarge.\nAn e\ufb03cient algorithm for computing the set of attributes functionally determined\nby\u03b1is useful not only for testing whether \u03b1is a superkey, but also for several other\ntasks, as we shall see later in this section.\n", "351": "7.4 Functional-Dependency Theory 323\nLet\u03b1be a set of attributes. We call the set of all attributes functionally determined\nby\u03b1under a set Fof functional dependencies the closure of \u03b1under F;w ed e n o t ei tb y\n\u03b1+. Figure 7.8 shows an algorithm, written in pseudocode, to compute \u03b1+.T h ei n p u t\nis a set Fof functional dependencies and the set \u03b1of attributes. The output is stored\nin the variable result .\nTo illustrate how the algorithm works, we shall use it to compute ( AG)+with the\nfunctional dependencies de\ufb01ned in Section 7.4.1. We start with result=AG. The \ufb01rst\ntime that we execute the repeat loop to test each functional dependency, we \ufb01nd that:\n\u2022A\u2192Bcauses us to include Binresult .T os e et h i sf a c t ,w eo b s e r v et h a t A\u2192Bis\ninF,A\u2286result (which is AG), so result :=result\u222aB.\n\u2022A\u2192Ccauses result to become ABCG .\n\u2022CG\u2192Hcauses result to become ABCGH .\n\u2022CG\u2192Icauses result to become ABCGHI .\nT h es e c o n dt i m et h a tw ee x e c u t et h e repeat loop, no new attributes are added to result ,\nand the algorithm terminates.\nLet us see why the algorithm of Figure 7.8 is correct. The \ufb01rst step is correct be-\ncause\u03b1\u2192\u03b1always holds (by the re\ufb02exivity rule). We claim that, for any subset \u03b2of\nresult ,\u03b1\u2192\u03b2.S i n c ew es t a r tt h e repeat loop with \u03b1\u2192result being true, we can add \u03b3to\nresult only if\u03b2\u2286result and\u03b2\u2192\u03b3.B u tt h e n result\u2192\u03b2by the re\ufb02exivity rule, so \u03b1\u2192\u03b2\nby transitivity. Another application of transitivity shows that \u03b1\u2192\u03b3(using\u03b1\u2192\u03b2and\n\u03b2\u2192\u03b3). The union rule implies that \u03b1\u2192result\u222a\u03b3,s o\u03b1functionally determines any\nnew result generated in the repeat loop. Thus, any attribute returned by the algorithm\nis in\u03b1+.\nIt is easy to see that the algorithm \ufb01nds all of \u03b1+.C o n s i d e ra na t t r i b u t e Ain\u03b1+that\nis not yet in result at any point during the execution. There must be a way to prove that\nresult\u2192Ausing the axioms. Either result\u2192Ais in Fitself (making the proof trivial\nand ensuring Ais added to result ) or there must a proof step using transitivity to show\nresult :=\u03b1;\nrepeat\nfor each functional dependency \u03b2\u2192\u03b3inFdo\nbegin\nif\u03b2\u2286result then result :=result\u222a\u03b3;\nend\nuntil (result does not change)\nFigure 7.8 An algorithm to compute \u03b1+, the closure of \u03b1under F.\n", "352": "324 Chapter 7 Relational Database Design\nfor some attribute Bthatresult\u2192B. If it happens that A=B, then we have shown that\nAis added to result .I fn o t , B\u2260Ais added. Then repeating this argument, we see that\nAmust eventually be added to result .\nIt turns out that, in the worst case, this algorithm may take an amount of time\nquadratic in the size of F. There is a faster (although slightly more complex) algorithm\nthat runs in time linear in the size of F; that algorithm is presented as part of Practice\nExercise 7.8.\nThere are several uses of the attribute closure algorithm:\n\u2022To test if\u03b1is a superkey, we compute \u03b1+and check if \u03b1+contains all attributes in\nR.\n\u2022We can check if a functional dependency \u03b1\u2192\u03b2holds (or, in other words, is in\nF+), by checking if \u03b2\u2286\u03b1+.T h a ti s ,w ec o m p u t e \u03b1+by using attribute closure, and\nthen check if it contains \u03b2. This test is particularly useful, as we shall see later in\nthis chapter.\n\u2022I tg i v e su sa na l t e r n a t i v ew a yt oc o m p u t e F+:F o re a c h \u03b3\u2286R, we \ufb01nd the closure\n\u03b3+,a n df o re a c h S\u2286\u03b3+, we output a functional dependency \u03b3\u2192S.\n7.4.3 Canonical Cover\nSuppose that we have a set of functional dependencies Fon a relation schema. When-\never a user performs an update on the relation, the database system must ensure that\nthe update does not violate any functional dependencies, that is, all the functional de-\npendencies in Fare satis\ufb01ed in the new database state.\nThe system must roll back the update if it violates any functional dependencies in\nthe set F.\nWe can reduce the e\ufb00ort spent in checking for violations by testing a simpli\ufb01ed set\nof functional dependencies that has the same closure as the given set. Any database\nthat satis\ufb01es the simpli\ufb01ed set of functional dependencies also satis\ufb01es the original set,\nand vice versa, since the two sets have the same closure. However, the simpli\ufb01ed set\nis easier to test. We shall see how the simpli\ufb01ed set can be constructed in a moment.\nFirst, we need some de\ufb01nitions.\nAn attribute of a functional dependency is said to be extraneous if we can remove\nit without changing the closure of the set of functional dependencies.\n\u2022Removing an attribute from the left side of a functional dependency could make\nit a stronger constraint. For example, if we have AB\u2192Cand remove B,w eg e t\nthe possibly stronger result A\u2192C. It may be stronger because A\u2192Clogically\nimplies AB\u2192C, but AB\u2192Cdoes not, on its own, logically imply A\u2192C.\nBut, depending on what our set Fof functional dependencies happens to be, we\nmay be able to remove Bfrom AB\u2192Csafely. For example, suppose that the set\n", "353": "7.4 Functional-Dependency Theory 325\nF={AB\u2192C,A\u2192D,D\u2192C}. Then we can show that Flogically implies\nA\u2192C,m a k i n g Bextraneous in AB\u2192C.\n\u2022Removing an attribute from the right side of a functional dependency could make\nit a weaker constraint. For example, if we have AB\u2192CDand remove C,w eg e t\nthe possibly weaker result AB\u2192D. It may be weaker because using just AB\u2192D,\nwe can no longer infer AB\u2192C. But, depending on what our set Fof functional\ndependencies happens to be, we may be able to remove Cfrom AB\u2192CDsafely.\nFor example, suppose that F={AB\u2192CD,A\u2192C}. Then we can show that\neven after replacing AB\u2192CDbyAB\u2192D, we can still infer AB\u2192Cand thus\nAB\u2192CD.\nThe formal de\ufb01nition of extraneous attributes is as follows: Consider a set Fof\nfunctional dependencies and the functional dependency \u03b1\u2192\u03b2inF.\n\u2022Removal from the left side: Attribute Ais extraneous in \u03b1ifA\u2208\u03b1andFlogically\nimplies ( F\u2212{\u03b1\u2192\u03b2})\u222a{(\u03b1\u2212A)\u2192\u03b2}.\n\u2022Removal from the right side: Attribute Ais extraneous in \u03b2ifA\u2208\u03b2 and the set of\nfunctional dependencies ( F\u2212{\u03b1\u2192\u03b2})\u222a{ \u03b1\u2192(\u03b2\u2212 A)}logically implies F.\nBeware of the direction of the implications when using the de\ufb01nition of extraneous\nattributes: If you reverse the statement, the implication will always hold. That is, ( F\u2212\n{\u03b1\u2192\u03b2})\u222a{(\u03b1\u2212A)\u2192\u03b2}always logically implies F,a n da l s o Falways logically\nimplies ( F\u2212{\u03b1\u2192\u03b2})\u222a{ \u03b1\u2192(\u03b2\u2212 A)}.\nHere is how we can test e\ufb03ciently if an attribute is extraneous. Let Rbe the relation\nschema, and let Fbe the given set of functional dependencies that hold on R.C o n s i d e r\nan attribute Ain a dependency \u03b1\u2192\u03b2.\n\u2022IfA\u2208\u03b2,t oc h e c ki f Ais extraneous, consider the set\nF\u2032=(F\u2212{\u03b1\u2192\u03b2})\u222a{\u03b1\u2192(\u03b2\u2212 A)}\nand check if \u03b1\u2192Acan be inferred from F\u2032.T od os o ,c o m p u t e \u03b1+(the closure of\n\u03b1)u n d e r F\u2032;i f\u03b1+includes A,t h e n Ais extraneous in \u03b2.\n\u2022IfA\u2208\u03b1,t oc h e c ki f Ais extraneous, let \u03b3=\u03b1\u2212{ A},a n dc h e c ki f \u03b3\u2192\u03b2can be\ninferred from F.T od os o ,c o m p u t e \u03b3+(the closure of \u03b3)u n d e r F;i f\u03b3+includes\nall attributes in \u03b2,t h e n Ais extraneous in \u03b1.\nFor example, suppose Fcontains AB\u2192CD,A\u2192E,a n d E\u2192C.T oc h e c ki f Cis\nextraneous in AB\u2192CD, we compute the attribute closure of ABunder F\u2032={AB\u2192D,\nA\u2192E,E\u2192C}.T h ec l o s u r ei s ABCDE ,w h i c hi n c l u d e s CD,s ow ei n f e rt h a t Cis\nextraneous.\n", "354": "326 Chapter 7 Relational Database Design\nFc=F\nrepeat\nUse the union rule to replace any dependencies in Fcof the form\n\u03b11\u2192\u03b21and\u03b11\u2192\u03b22with\u03b11\u2192\u03b21\u03b22.\nFind a functional dependency \u03b1\u2192\u03b2inFcwith an extraneous\nattribute either in \u03b1or in\u03b2.\n/* Note: the test for extraneous attributes is done using Fc,n o t F*/\nIf an extraneous attribute is found, delete it from \u03b1\u2192\u03b2inFc.\nuntil (Fcdoes not change)\nFigure 7.9 Computing canonical cover.\nHaving de\ufb01ned the concept of extraneous attributes, we can explain how we can\nconstruct a simpli\ufb01ed set of functional dependencies equivalent to a given set of func-\ntional dependencies.\nAcanonical cover FcforFis a set of dependencies such that Flogically implies\nall dependencies in Fc,a n d Fclogically implies all dependencies in F.F u r t h e r m o r e , Fc\nmust have the following properties:\n\u2022No functional dependency in Fccontains an extraneous attribute.\n\u2022Each left side of a functional dependency in Fcis unique. That is, there are no two\ndependencies \u03b11\u2192\u03b21and\u03b12\u2192\u03b22inFcsuch that \u03b11=\u03b12.\nA canonical cover for a set of functional dependencies Fcan be computed as de-\nscribed in Figure 7.9. It is important to note that when checking if an attribute is extra-\nneous, the check uses the dependencies in the current value of Fc,a n d notthe depen-\ndencies in F. If a functional dependency contains only one attribute in its right-hand\nside, for example A\u2192C, and that attribute is found to be extraneous, we would get a\nfunctional dependency with an empty right-hand side. Such functional dependencies\nshould be deleted.\nSince the algorithm permits a choice of any extraneous attribute, it is possible that\nthere may be several possible canonical covers for a given F.A n ys u c h Fcis equally\nacceptable. Any canonical cover of F,Fc, can be shown to have the same closure as\nF;h e n c e ,t e s t i n gw h e t h e r Fcis satis\ufb01ed is equivalent to testing whether Fis satis\ufb01ed.\nHowever, Fcis minimal in a certain sense\u2014it does not contain extraneous attributes,\nand it combines functional dependencies with the same left side. It is cheaper to test\nFcthan it is to test Fitself.\nWe now consider an example. Assume we are given the following set Fof functional\ndependencies on schema ( A,B,C):\n", "355": "7.4 Functional-Dependency Theory 327\nA\u2192BC\nB\u2192C\nA\u2192B\nAB\u2192C\nLet us compute a canonical cover for F.\n\u2022There are two functional dependencies with the same set of attributes on the left\nside of the arrow:\nA\u2192BC\nA\u2192B\nWe combine these functional dependencies into A\u2192BC.\n\u2022Ais extraneous in AB\u2192Cbecause Flogically implies ( F\u2212{AB\u2192C})\u222a{B\u2192\nC}. This assertion is true because B\u2192Cis already in our set of functional depen-\ndencies.\n\u2022Cis extraneous in A\u2192BC,s i n c e A\u2192BCis logically implied by A\u2192BandB\u2192\nC.\nThus, our canonical cover is:\nA\u2192B\nB\u2192C\nGiven a set Fof functional dependencies, it may be that an entire functional de-\npendency in the set is extraneous, in the sense that dropping it does not change the\nclosure of F. We can show that a canonical cover FcofFcontains no such extraneous\nfunctional dependency. Suppose that, to the contrary, there were such an extraneous\nfunctional dependency in Fc. The right-side attributes of the dependency would then\nbe extraneous, which is not possible by the de\ufb01nition of canonical covers.\nAs we noted earlier, a canonical cover might not be unique. For instance, consider\nthe set of functional dependencies F={A\u2192BC,B\u2192AC,a n d C\u2192AB}. If we apply\nthe test for extraneous attributes to A\u2192BC, we \ufb01nd that both BandCare extraneous\nunder F. However, it is incorrect to delete both! The algorithm for \ufb01nding the canonical\ncover picks one of the two and deletes it. Then,\n1.IfCis deleted, we get the set F\u2032={A\u2192B,B\u2192AC,a n d C\u2192AB}.N o w , Bis not\nextraneous on the right side of A\u2192Bunder F\u2032. Continuing the algorithm, we\n\ufb01ndAandBare extraneous in the right side of C\u2192AB, leading to two choices\nof canonical cover:\n", "356": "328 Chapter 7 Relational Database Design\ncompute F+;\nfor each schema RiinDdo\nbegin\nFi:=the restriction of F+toRi;\nend\nF\u2032:=\u2205\nfor each restriction Fido\nbegin\nF\u2032=F\u2032\u222aFi\nend\ncompute F\u2032+;\nif(F\u2032+=F+)then return (true)\nelsereturn (false);\nFigure 7.10 Testing for dependency preservation.\nFc={A\u2192B,B\u2192C,C\u2192A}\nFc={A\u2192B,B\u2192AC,C\u2192B}.\n2.IfBis deleted, we get the set {A\u2192C,B\u2192AC,a n d C\u2192AB}.T h i sc a s e\nis symmetrical to the previous case, leading to two more choices of canonical\ncover:\nFc={A\u2192C,C\u2192B,a n d B\u2192A}\nFc={A\u2192C,B\u2192C,a n d C\u2192AB}.\nAs an exercise, can you \ufb01nd one more canonical cover for F?\n7.4.4 Dependency Preservation\nUsing the theory of functional dependencies, there is a way to describe dependency\npreservation that is simpler than the ad hoc approach we used in Section 7.3.1.2.\nLetFbe a set of functional dependencies on a schema R,a n dl e t R1,R2,\u2026,Rnbe a\ndecomposition of R.T h e restriction of FtoRiis the set Fiof all functional dependencies\ninF+that include onlyattributes of Ri. Since all functional dependencies in a restriction\ninvolve attributes of only one relation schema, it is possible to test such a dependency\nfor satisfaction by checking only one relation.\nNote that the de\ufb01nition of restriction uses all dependencies in F+,n o tj u s tt h o s e\ninF. For instance, suppose F={A\u2192B,B\u2192C}, and we have a decomposition into\nACandAB.T h er e s t r i c t i o no f FtoACincludes A\u2192C,s i n c e A\u2192Cis in F+,e v e n\nthough it is not in F.\n", "357": "7.4 Functional-Dependency Theory 329\nThe set of restrictions F1,F2,\u2026,Fnis the set of dependencies that can be checked\ne\ufb03ciently. We now must ask whether testing only the restrictions is su\ufb03cient. Let F\u2032=\nF1\u222aF2\u222a\u22ef\u222aFn.F\u2032is a set of functional dependencies on schema R,b u t ,i ng e n e r a l ,\nF\u2032\u2260F.H o w e v e r ,e v e ni f F\u2032\u2260F,i tm a yb et h a t F\u2032+=F+. If the latter is true, then\nevery dependency in Fis logically implied by F\u2032,a n d ,i fw ev e r i f yt h a t F\u2032is satis\ufb01ed,\nwe have veri\ufb01ed that Fis satis\ufb01ed. We say that a decomposition having the property\nF\u2032+=F+is adependency-preserving decomposition .\nFigure 7.10 shows an algorithm for testing dependency preservation. The input is\nas e t D={R1,R2,\u2026,Rn}of decomposed relation schemas, and a set Fof functional\ndependencies. This algorithm is expensive since it requires computation of F+.I n s t e a d\nof applying the algorithm of Figure 7.10, we consider two alternatives.\nFirst, note that if each member of Fcan be tested on one of the relations of the\ndecomposition, then the decomposition is dependency preserving. This is an easy way\nto show dependency preservation; however, it does not always work. There are cases\nwhere, even though the decomposition is dependency preserving, there is a dependency\ninFthat cannot be tested in any one relation in the decomposition. Thus, this alter-\nnative test can be used only as a su\ufb03cient condition that is easy to check; if it fails we\ncannot conclude that the decomposition is not dependency preserving; instead we will\nhave to apply the general test.\nWe now give a second alternative test for dependency preservation that avoids\ncomputing F+. We explain the intuition behind the test after presenting the test. The\ntest applies the following procedure to each \u03b1\u2192\u03b2inF.\nresult =\u03b1\nrepeat\nfor each Riin the decomposition\nt=(result\u2229Ri)+\u2229Ri\nresult =result\u222at\nuntil (result does not change)\nThe attribute closure here is under the set of functional dependencies F.I fresult con-\ntains all attributes in \u03b2, then the functional dependency \u03b1\u2192\u03b2is preserved. The de-\ncomposition is dependency preserving if and only if the procedure shows that all the\ndependencies in Fare preserved.\nThe two key ideas behind the preceding test are as follows:\n\u2022The \ufb01rst idea is to test each functional dependency \u03b1\u2192\u03b2inFto see if it is\npreserved in F\u2032(where F\u2032is as de\ufb01ned in Figure 7.10). To do so, we compute\nthe closure of \u03b1under F\u2032; the dependency is preserved exactly when the closure\nincludes\u03b2. The decomposition is dependency preserving if (and only if) all the\ndependencies in Fare found to be preserved.\n", "358": "330 Chapter 7 Relational Database Design\n\u2022The second idea is to use a modi\ufb01ed form of the attribute-closure algorithm to\ncompute closure under F\u2032, without actually \ufb01rst computing F\u2032. We wish to avoid\ncomputing F\u2032since computing it is quite expensive. Note that F\u2032is the union of\nallFi,w h e r e Fiis the restriction of FonRi. The algorithm computes the attribute\nclosure of ( result\u2229Ri)w i t hr e s p e c tt o F, intersects the closure with Ri, and adds the\nresultant set of attributes to result ; this sequence of steps is equivalent to computing\nthe closure of result under Fi. Repeating this step for each iinside the while loop\ngives the closure of result under F\u2032.\nTo understand why this modi\ufb01ed attribute-closure approach works correctly, we\nnote that for any \u03b3\u2286Ri,\u03b3\u2192\u03b3+is a functional dependency in F+,a n d\u03b3\u2192\u03b3+\u2229Ri\nis a functional dependency that is in Fi,t h er e s t r i c t i o no f F+toRi.C o n v e r s e l y ,i f\n\u03b3\u2192\u03b4were in Fi,t h e n\u03b4would be a subset of \u03b3+\u2229Ri.\nThis test takes polynomial time, instead of the exponential time required to com-\npute F+.\n7.5 Algorithms for Decomposition Using Functional Dependencies\nReal-world database schemas are much larger than the examples that \ufb01t in the pages\nof a book. For this reason, we need algorithms for the generation of designs that are in\nappropriate normal form. In this section, we present algorithms for BCNF and 3NF.\n7.5.1 BCNF Decomposition\nThe de\ufb01nition of BCNF can be used directly to test if a relation is in BCNF .H o w e v e r ,\ncomputation of F+can be a tedious task. We \ufb01rst describe simpli\ufb01ed tests for verifying\nif a relation is in BCNF . If a relation is not in BCNF , it can be decomposed to create\nrelations that are in BCNF . Later in this section, we describe an algorithm to create a\nlossless decomposition of a relation, such that the decomposition is in BCNF .\n7.5.1.1 Testing for BCNF\nTesting of a relation schema Rto see if it satis\ufb01es BCNF can be simpli\ufb01ed in some\ncases:\n\u2022To check if a nontrivial dependency \u03b1\u2192\u03b2causes a violation of BCNF ,c o m p u t e\n\u03b1+(the attribute closure of \u03b1), and verify that it includes all attributes of R;t h a t\nis, it is a superkey for R.\n\u2022To check if a relation schema Ris in BCNF , it su\ufb03ces to check only the dependen-\ncies in the given set Ffor violation of BCNF , rather than check all dependencies\ninF+.\nWe can show that if none of the dependencies in Fcauses a violation of BCNF ,\nthen none of the dependencies in F+will cause a violation of BCNF ,e i t h e r .\n", "359": "7.5 Algorithms for Decomposition Using Functional Dependencies 331\nresult :={R};\ndone :=false;\nwhile (notdone)do\nif(there is a schema Riinresult that is not in BCNF )\nthen begin\nlet\u03b1\u2192\u03b2be a nontrivial functional dependency that holds\nonRisuch that \u03b1+does not contain Riand\u03b1\u2229\u03b2=\u2205 ;\nresult :=(result\u2212Ri)\u222a(Ri\u2212\u03b2)\u222a(\u03b1,\u03b2);\nend\nelsedone :=true;\nFigure 7.11 BCNF decomposition algorithm.\nUnfortunately, the latter procedure does not work when a relation schema is decom-\nposed. That is, it does not su\ufb03ce to use Fwhen we test a relation schema Ri,i na\ndecomposition of R, for violation of BCNF . For example, consider relation schema\n(A,B,C,D,E), with functional dependencies Fcontaining A\u2192BandBC\u2192D.S u p -\npose this were decomposed into ( A,B)a n d( A,C,D,E). Now, neither of the depen-\ndencies in Fcontains only attributes from ( A,C,D,E), so we might be misled into\nthinking that it is in BCNF . In fact, there is a dependency AC\u2192DinF+(which can be\ninferred using the pseudotransitivity rule from the two dependencies in F) that shows\nthat ( A,C,D,E)i sn o ti n BCNF . Thus, we may need a dependency that is in F+,b u ti s\nnot in F, to show that a decomposed relation is not in BCNF .\nAn alternative BCNF test is sometimes easier than computing every dependency\ninF+. To check if a relation schema Riin a decomposition of Ris in BCNF , we apply\nthis test:\n\u2022For every subset \u03b1of attributes in Ri,c h e c kt h a t \u03b1+(the attribute closure\nof\u03b1under F) either includes no attribute of Ri\u2212\u03b1, or includes all attributes of Ri.\nIf the condition is violated by some set of attributes \u03b1inRi, consider the following\nfunctional dependency, which can be shown to be present in F+:\n\u03b1\u2192(\u03b1+\u2212\u03b1)\u2229Ri.\nThis dependency shows that Riviolates BCNF .\n7.5.1.2 BCNF Decomposition Algorithm\nWe are now able to state a general method to decompose a relation schema so as to\nsatisfy BCNF . Figure 7.11 shows an algorithm for this task. If Ris not in BCNF ,w e\ncan decompose Rinto a collection of BCNF schemas R1,R2,\u2026,Rnby the algorithm.\n", "360": "332 Chapter 7 Relational Database Design\nThe algorithm uses dependencies that demonstrate violation of BCNF to perform the\ndecomposition.\nThe decomposition that the algorithm generates is not only in BCNF ,b u ti sa l s o\na lossless decomposition. To see why our algorithm generates only lossless decom-\npositions, we note that, when we replace a schema Riwith ( Ri\u2212\u03b2)a n d(\u03b1,\u03b2), the\ndependency \u03b1\u2192\u03b2holds, and ( Ri\u2212\u03b2)\u2229(\u03b1,\u03b2)=\u03b1.\nIf we did not require \u03b1\u2229\u03b2=\u2205 , then those attributes in \u03b1\u2229\u03b2 would not appear in\nthe schema ( Ri\u2212\u03b2), and the dependency \u03b1\u2192\u03b2would no longer hold.\nIt is easy to see that our decomposition of in\ndepin Section 7.3.1 would result\nfrom applying the algorithm. The functional dependency dept\nname\u2192building ,budget\nsatis\ufb01es the \u03b1\u2229\u03b2=\u2205 condition and would therefore be chosen to decompose the\nschema.\nThe BCNF decomposition algorithm takes time exponential to the size of the initial\nschema, since the algorithm for checking whether a relation in the decomposition sat-\nis\ufb01es BCNF can take exponential time. There is an algorithm that can compute a BCNF\ndecomposition in polynomial time; however, the algorithm may \u201covernormalize,\u201d that\nis, decompose a relation unnecessarily.\nAs a longer example of the use of the BCNF decomposition algorithm, suppose we\nhave a database design using the class relation, whose schema is as shown below:\nclass (course\n id,title,dept\nname ,credits ,sec\nid,semester ,year,building ,\nroom\n number ,capacity ,time\n slot\nid)\nThe set of functional dependencies that we need to hold on this schema are:\ncourse\n id\u2192title,dept\nname ,credits\nbuilding ,room\n number\u2192capacity\ncourse\n id,sec\nid,semester ,year\u2192building ,room\n number ,time\n slot\nid\nA candidate key for this schema is {course\n id,sec\nid,semester ,year }.\nWe can apply the algorithm of Figure 7.11 to the class example as follows:\n\u2022The functional dependency:\ncourse\n id\u2192title,dept\nname ,credits\nholds, but course\n idis not a superkey. Thus, class is not in BCNF .W er e p l a c e class\nwith two relations with the following schemas:\ncourse (course\n id,title,dept\nname ,credits )\nclass-1 (course\n id,sec\nid,semester ,year,building ,room\n number\ncapacity ,time\n slot\nid)\n", "361": "7.5 Algorithms for Decomposition Using Functional Dependencies 333\nThe only nontrivial functional dependencies that hold on course include course\n id\no nt h el e f ts i d eo ft h ea r r o w .S i n c e course\n idis a superkey for course ,course is in\nBCNF .\n\u2022A candidate key for class-1 is{course\n id,sec\nid,semester ,year }. The functional de-\npendency:\nbuilding ,room\n number\u2192capacity\nholds on class-1 , but {building ,room\n number }is not a superkey for class-1 .W er e -\nplace class-1 two relations with the following schemas:\nclassroom (building ,room\n number ,capacity )\nsection (course\n id,sec\nid,semester ,year,\nbuilding ,room\n number ,time\n slot\nid)\nThese two schemas are in BCNF .\nThus, the decomposition of class results in the three relation schemas course ,classroom ,\nandsection , each of which is in BCNF . These correspond to the schemas that we have\nused in this and previous chapters. You can verify that the decomposition is lossless\nand dependency preserving.\n7.5.2 3NF Decomposition\nFigure 7.12 shows an algorithm for \ufb01nding a dependency-preserving, lossless decompo-\nsition into 3NF. The set of dependencies Fcused in the algorithm is a canonical cover\nforF. Note that the algorithm considers the set of schemas Rj,j=1, 2,\u2026,i;i n i t i a l l y\ni=0, and in this case the set is empty.\nLet us apply this algorithm to our example of dept\nadvisor from Section 7.3.2, where\nwe showed that:\ndept\nadvisor (s\nID,i\nID,dept\nname )\nis in 3NF even though it is not in BCNF . The algorithm uses the following functional\ndependencies in F:\nf1:i\nID\u2192dept\nname\nf2:s\nID,dept\nname\u2192i\nID\nThere are no extraneous attributes in any of the functional dependencies in F,s o\nFccontains f1andf2. The algorithm then generates as R1the schema, ( i\nIDdept\nname ),\nand as R2the schema ( s\nID,dept\nname ,i\nID). The algorithm then \ufb01nds that R2contains\na candidate key, so no further relation schema is created.\n", "362": "334 Chapter 7 Relational Database Design\nletFcbe a canonical cover for F;\ni:=0;\nfor each functional dependency \u03b1\u2192\u03b2inFc\ni:=i+1 ;\nRi:=\u03b1\u03b2 ;\nifnone of the schemas Rj,j=1, 2,\u2026,icontains a candidate key for R\nthen\ni:=i+1;\nRi:=any candidate key for R;\n/* Optionally, remove redundant relations */\nrepeat\nifany schema Rjis contained in another schema Rk\nthen\n/* Delete Rj*/\nRj:=Ri;\ni:=i-1 ;\nuntil no more Rjsc a nb ed e l e t e d\nreturn (R1,R2,\u2026,Ri)\nFigure 7.12 Dependency-preserving, lossless decomposition into 3NF.\nThe resultant set of schemas can contain redundant schemas, with one schema Rk\ncontaining all the attributes of another schema Rj.F o re x a m p l e , R2above contains all\nthe attributes from R1. The algorithm deletes all such schemas that are contained in\nanother schema. Any dependencies that could be tested on an Rjthat is deleted can\nalso be tested on the corresponding relation Rk, and the decomposition is lossless even\nifRjis deleted.\nNow let us consider again the schema of the class relation of Section 7.5.1.2 and\napply the 3NF decomposition algorithm . The set of functional dependencies we listed\nthere happen to be a canonical cover. As a result, the algorithm gives us the same three\nschemas course, classroom ,a n d section .\nThe preceding example illustrates an interesting property of the 3NF algorithm.\nSometimes, the result is not only in 3NF, but also in BCNF . This suggests an alterna-\ntive method of generating a BCNF design. First use the 3NF algorithm. Then, for any\nschema in the 3NF design that is not in BCNF ,d e c o m p o s eu s i n gt h e BCNF algorithm.\nIf the result is not dependency-preserving, revert to the 3NF design.\n7.5.3 Correctness of the 3NF Algorithm\nThe 3NF algorithm ensures the preservation of dependencies by explicitly building a\nschema for each dependency in a canonical cover. It ensures that the decomposition is a\n", "363": "7.5 Algorithms for Decomposition Using Functional Dependencies 335\nlossless decomposition by guaranteeing that at least one schema contains a candidate\nkey for the schema being decomposed. Practice Exercise 7.16 provides some insight\ninto the proof that this su\ufb03ces to guarantee a lossless decomposition.\nThis algorithm is also called the 3NF synthesis algorithm , since it takes a set of de-\npendencies and adds one schema at a time, instead of decomposing the initial schema\nrepeatedly. The result is not uniquely de\ufb01ned, since a set of functional dependencies\ncan have more than one canonical cover. The algorithm may decompose a relation even\nif it is already in 3NF; however, the decomposition is still guaranteed to be in 3NF.\nTo see that the algorithm produces a 3NF design, consider a schema Riin the\ndecomposition. Recall that when we test for 3NF it su\ufb03ces to consider functional\ndependencies whose right-hand side consists of a single attribute. Therefore, to see that\nRiis in 3NF you must convince yourself that any functional dependency \u03b3\u2192Bthat\nholds on Risatis\ufb01es the de\ufb01nition of 3NF. Assume that the dependency that generated\nRiin the synthesis algorithm is \u03b1\u2192\u03b2.Bmust be in \u03b1or\u03b2,s i n c e Bis in Riand\u03b1\u2192\u03b2\ngenerated Ri. Let us consider the three possible cases:\n\u2022Bis in both \u03b1and\u03b2. In this case, the dependency \u03b1\u2192\u03b2would not have been in\nFcsince Bwould be extraneous in \u03b2. Thus, this case cannot hold.\n\u2022Bis in\u03b2but not\u03b1. Consider two cases:\n\u00b0\u03b3is a superkey. The second condition of 3NF is satis\ufb01ed.\n\u00b0\u03b3is not a superkey. Then \u03b1must contain some attribute not in \u03b3.N o w ,s i n c e \u03b3\u2192\nBis inF+, it must be derivable from Fcby using the attribute closure algorithm\non\u03b3. The derivation could not have used \u03b1\u2192\u03b2, because if it had been used,\n\u03b1must be contained in the attribute closure of \u03b3, which is not possible, since\nwe assumed \u03b3is not a superkey. Now, using \u03b1\u2192(\u03b2\u2212{ B})a n d\u03b3\u2192B,w e\ncan derive \u03b1\u2192B(since\u03b3\u2286\u03b1\u03b2,a n d\u03b3cannot contain Bbecause\u03b3\u2192B\nis nontrivial). This would imply that Bis extraneous in the right-hand side of\n\u03b1\u2192\u03b2, which is not possible since \u03b1\u2192\u03b2is in the canonical cover Fc.T h u s ,i f\nBis in\u03b2,t h e n\u03b3must be a superkey, and the second condition of 3NF must be\nsatis\ufb01ed.\n\u2022Bis in\u03b1but not\u03b2.\nSince\u03b1is a candidate key, the third alternative in the de\ufb01nition of 3NF is satis\ufb01ed.\nInterestingly, the algorithm we described for decomposition into 3NF can be im-\nplemented in polynomial time, even though testing a given schema to see if it satis\ufb01es\n3NF isNP-hard (which means that it is very unlikely that a polynomial-time algorithm\nwill ever be invented for this task).\n", "364": "336 Chapter 7 Relational Database Design\n7.6 Decomposition Using Multivalued Dependencies\nSome relation schemas, even though they are in BCNF , do not seem to be su\ufb03ciently\nnormalized, in the sense that they still su\ufb00er from the problem of repetition of infor-\nmation. Consider a variation of the university organization where an instructor may be\nassociated with multiple departments, and we have a relation:\ninst(ID,dept\nname ,name ,street ,city)\nThe astute reader will recognize this schema as a non- BCNF schema because of the\nfunctional dependency\nID\u2192name ,street ,city\nand because IDis not a key for inst.\nFurther assume that an instructor may have several addresses (say, a winter home\nand a summer home). Then, we no longer wish to enforce the functional dependency\n\u201cID\u2192street ,city\u201d ,t h o u g h ,w es t i l lw a n tt oe n f o r c e\u201c ID\u2192name \u201d (i.e., the university is\nnot dealing with instructors who operate under multiple aliases!). Following the BCNF\ndecomposition algorithm, we obtain two schemas:\nr1(ID,name )\nr2(ID,dept\nname ,street ,city)\nBoth of these are in BCNF (recall that an instructor can be associated with multiple\ndepartments and a department may have several instructors, and therefore, neither \u201c ID\n\u2192dept\nname \u201dn o r\u201c dept\nname\u2192ID\u201dh o l d ) .\nDespite r2being in BCNF , there is redundancy. We repeat the address information\nof each residence of an instructor once for each department with which the instructor\nis associated. We could solve this problem by decomposing r2further into:\nr21(dept\nname ,ID)\nr22(ID,street ,city)\nb u tt h e r ei sn oc o n s t r a i n tt h a tl e a d su st od ot h i s .\nTo deal with this problem, we must de\ufb01ne a new form of constraint, called a mul-\ntivalued dependency . As we did for functional dependencies, we shall use multivalued\ndependencies to de\ufb01ne a normal form for relation schemas. This normal form, called\nfourth normal form (4NF), is more restrictive than BCNF .W es h a l ls e et h a te v e r y 4NF\nschema is also in BCNF but there are BCNF schemas that are not in 4NF.\n", "365": "7.6 Decomposition Using Multivalued Dependencies 337\n\u03b1\u03b1 \u03b2 \u03b2\n \u2015 R\u2015\nt1\nt2\nt3\nt4a1 ...ai\na1 ...ai\na1 ...ai\na1 ...aiai + 1 ...aj\nbi + 1 ...bj\nai + 1 ...aj\nbi + 1 ...bjaj + 1 ...an\nbj + 1 ...bn\nbj + 1 ...bn\naj + 1 ...an\nFigure 7.13 Tabular representation of \u03b1\u2192\u2192\u03b2.\n7.6.1 Multivalued Dependencies\nFunctional dependencies rule out certain tuples from being in a relation. If A\u2192B,\nthen we cannot have two tuples with the same Avalue but di\ufb00erent Bvalues. Multival-\nued dependencies, on the other hand, do not rule out the existence of certain tuples.\nInstead, they require that other tuples of a certain form be present in the relation. For\nthis reason, functional dependencies sometimes are referred to as equality-generating\ndependencies , and multivalued dependencies are referred to as tuple-generating depen-\ndencies .\nLetr(R)b ear e l a t i o ns c h e m aa n dl e t \u03b1\u2286Rand\u03b2\u2286R.T h e multivalued dependency\n\u03b1\u2192\u2192\u03b2\nholds on Rif, in any legal instance of relation r(R), for all pairs of tuples t1andt2inr\nsuch that t1[\u03b1]=t2[\u03b1], there exist tuples t3andt4inrsuch that\nt1[\u03b1]=t2[\u03b1]=t3[\u03b1]=t4[\u03b1]\nt3[\u03b2]=t1[\u03b2]\nt3[R\u2212\u03b2]=t2[R\u2212\u03b2]\nt4[\u03b2]=t2[\u03b2]\nt4[R\u2212\u03b2]=t1[R\u2212\u03b2]\nThis de\ufb01nition is less complicated than it appears to be. Figure 7.13 gives a tabular\npicture of t1,t2,t3,a n d t4. Intuitively, the multivalued dependency \u03b1\u2192\u2192\u03b2says that the\nrelationship between \u03b1and\u03b2is independent of the relationship between \u03b1andR\u2212\u03b2.\nIf the multivalued dependency \u03b1\u2192\u2192\u03b2is satis\ufb01ed by all relations on schema R,t h e n\n\u03b1\u2192\u2192\u03b2is atrivial multivalued dependency on schema R.T h u s ,\u03b1\u2192\u2192\u03b2is trivial if\n\u03b2\u2286\u03b1or\u03b2\u222a\u03b1= R. This can be seen by looking at Figure 7.13 and considering the two\nspecial cases \u03b2\u2286\u03b1and\u03b2\u222a\u03b1= R. In each case, the table reduces to just two columns\nand we see that t1andt2a r ea b l et os e r v ei nt h er o l e so f t3andt4.\nTo illustrate the di\ufb00erence between functional and multivalued dependencies, we\nconsider the schema r2again, and an example relation on that schema is shown in Fig-\nure 7.14. We must repeat the department name once for each address that an instructor\nhas, and we must repeat the address for each department with which an instructor is\nassociated. This repetition is unnecessary, since the relationship between an instructor\n", "366": "338 Chapter 7 Relational Database Design\nID\n dept\nname\n street\n city\n22222\n Physics\n North\n Rye\n22222\n Physics\n Main\n Manchester\n12121\n Finance\n Lake\n Horseneck\nFigure 7.14 An example of redundancy in a relation on a BCNF schema.\nand his address is independent of the relationship between that instructor and a de-\npartment. If an instructor with ID22222 is associated with the Physics department, we\nwant that department to be associated with all of that instructor\u2019s addresses. Thus, the\nrelation of Figure 7.15 is illegal. To make this relation legal, we need to add the tuples\n(Physics, 22222, Main, Manchester) and (Math, 22222, North, Rye) to the relation of\nFigure 7.15.\nComparing the preceding example with our de\ufb01nition of multivalued dependency,\nwe see that we want the multivalued dependency:\nID\u2192\u2192street ,city\nto hold. (The multivalued dependency ID\u2192\u2192dept\nname will do as well. We shall\nsoon see that they are equivalent.)\nAs with functional dependencies, we shall use multivalued dependencies in two\nways:\n1.To test relations to determine whether they are legal under a given set of func-\ntional and multivalued dependencies.\n2.To specify constraints on the set of legal relations; we shall thus concern ourselves\nwith only those relations that satisfy a given set of functional and multivalued\ndependencies.\nNote that, if a relation rfails to satisfy a given multivalued dependency, we can con-\nstruct a relation r\u2032thatdoes satisfy the multivalued dependency by adding tuples to r.\nLetDdenote a set of functional and multivalued dependencies. The closure D+\nofDis the set of all functional and multivalued dependencies logically implied by D.\nAs we did for functional dependencies, we can compute D+from D,u s i n gt h ef o r m a l\nde\ufb01nitions of functional dependencies and multivalued dependencies. We can manage\nID\n dept\nname\n street\n city\n22222\n Physics\n North\n Rye\n22222\n Math\n Main\n Manchester\nFigure 7.15 An illegal r2relation.\n", "367": "7.6 Decomposition Using Multivalued Dependencies 339\nwith such reasoning for very simple multivalued dependencies. Luckily, multivalued\ndependencies that occur in practice appear to be quite simple. For complex dependen-\ncies, it is better to reason about sets of dependencies by using a system of inference\nrules.\nFrom the de\ufb01nition of multivalued dependency, we can derive the following rules\nfor\u03b1,\u03b2\u2286R:\n\u2022If\u03b1\u2192\u03b2,t h e n\u03b1\u2192\u2192\u03b2. In other words, every functional dependency is also a\nmultivalued dependency.\n\u2022If\u03b1\u2192\u2192\u03b2,t h e n\u03b1\u2192\u2192R\u2212\u03b1\u2212\u03b2\nSection 28.1.1 outlines a system of inference rules for multivalued dependencies.\n7.6.2 Fourth Normal Form\nConsider again our example of the BCNF schema:\nr2(ID,dept\nname ,street ,city)\nin which the multivalued dependency ID\u2192\u2192street ,cityholds. We saw in the opening\nparagraphs of Section 7.6 that, although this schema is in BCNF , the design is not ideal,\nsince we must repeat an instructor\u2019s address information for each department. We shall\nsee that we can use the given multivalued dependency to improve the database design\nby decomposing this schema into a fourth normal form decomposition.\nA relation schema Ris in fourth normal form (4NF)w i t hr e s p e c tt oas e t Dof\nfunctional and multivalued dependencies if, for all multivalued dependencies in D+of\nthe form\u03b1\u2192\u2192\u03b2,w h e r e\u03b1\u2286Rand\u03b2\u2286R, at least one of the following holds:\n\u2022\u03b1\u2192\u2192\u03b2is a trivial multivalued dependency.\n\u2022\u03b1is a superkey for R.\nA database design is in 4NF if each member of the set of relation schemas that consti-\ntutes the design is in 4NF.\nNote that the de\ufb01nition of 4NF di\ufb00ers from the de\ufb01nition of BCNF in only the use\nof multivalued dependencies. Every 4NF schema is in BCNF .T os e et h i sf a c t ,w en o t e\nthat, if a schema Ris not in BCNF , then there is a nontrivial functional dependency\n\u03b1\u2192\u03b2holding on R,w h e r e\u03b1is not a superkey. Since \u03b1\u2192\u03b2implies\u03b1\u2192\u2192\u03b2,Rcannot\nbe in 4NF.\nLetRbe a relation schema, and let R1,R2,\u2026,Rnbe a decomposition of R.T o\ncheck if each relation schema Riin the decomposition is in 4NF, we need to \ufb01nd what\nmultivalued dependencies hold on each Ri.R e c a l lt h a t ,f o ras e t Fof functional depen-\ndencies, the restriction FiofFtoRiis all functional dependencies in F+that include\nonly attributes of Ri.N o wc o n s i d e ras e t Dof both functional and multivalued depen-\ndencies. The restriction ofDtoRiis the set Diconsisting of:\n", "368": "340 Chapter 7 Relational Database Design\n1.All functional dependencies in D+that include only attributes of Ri.\n2.All multivalued dependencies of the form:\n\u03b1\u2192\u2192\u03b2\u2229Ri\nwhere\u03b1\u2286Riand\u03b1\u2192\u2192\u03b2is in D+.\n7.6.3 4NF Decomposition\nThe analogy between 4NF and BCNF applies to the algorithm for decomposing a\nschema into 4NF.F i g u r e7 . 1 6s h o w st h e 4NF decomposition algorithm. It is identical\nto the BCNF decomposition algorithm of Figure 7.11, except that it uses multivalued\ndependencies and uses the restriction of D+toRi.\nIf we apply the algorithm of Figure 7.16 to ( ID,dept\nname ,street ,city), we \ufb01nd that\nID\u2192\u2192dept\nname is a nontrivial multivalued dependency, and IDis not a superkey for\nthe schema. Following the algorithm, we replace it with two schemas:\n(ID,dept\nname )\n(ID,street ,city)\nThis pair of schemas, which is in 4NF, eliminates the redundancy we encountered ear-\nlier.\nAs was the case when we were dealing solely with functional dependencies, we are\ninterested in decompositions that are lossless and that preserve dependencies. The fol-\nlowing fact about multivalued dependencies and losslessness shows that the algorithm\nof Figure 7.16 generates onl y lossless decompositions:\nresult :={R};\ndone :=false;\ncompute D+;G i v e ns c h e m a Ri,l e tDidenote the restriction of D+toRi\nwhile (notdone)do\nif(there is a schema Riinresult that is not in 4NF w.r.t. Di)\nthen begin\nlet\u03b1\u2192\u2192\u03b2be a nontrivial multivalued dependency that holds\nonRisuch that \u03b1\u2192Riis not in Di,a n d\u03b1\u2229\u03b2=\u2205 ;\nresult :=(result\u2212Ri)\u222a(Ri\u2212\u03b2)\u222a(\u03b1,\u03b2);\nend\nelsedone :=true;\nFigure 7.16 4NF decomposition algorithm.\n", "369": "7.7 More Normal Forms 341\n\u2022Letr(R) be a relation schema, and let Dbe a set of functional and multivalued\ndependencies on R.L e t r1(R1)a n d r2(R2) form a decomposition of R.T h i sd e -\ncomposition of Ris lossless if and only if at least one of the following multivalued\ndependencies is in D+:\nR1\u2229R2\u2192\u2192R1\nR1\u2229R2\u2192\u2192R2\nRecall that we stated in Section 7.2.3 that, if R1\u2229R2\u2192R1orR1\u2229R2\u2192R2,t h e n\nr1(R1)a n d r2(R2) forms a lossless decomposition of r(R). The preceding fact about\nmultivalued dependencies is a more general statement about losslessness. It says that,\nforevery lossless decomposition of r(R) into two schemas r1(R1)a n d r2(R2), one of\nthe two dependencies R1\u2229R2\u2192\u2192R1orR1\u2229R2\u2192\u2192R2must hold. To see that this\nis true, we need to show \ufb01rst that if at least one of these dependencies holds, then\n\u03a0R1(r)\u22c8\u03a0R2(r)=rand next we need to show that if \u03a0R1(r)\u22c8\u03a0R2(r)=rthen r(R)\nmust satisfy at least one of these dependencies. See the Further Reading section for\nreferences to a full proof.\nThe issue of dependency preservation when we decompose a relation schema be-\ncomes more complicated in the presence of multivalued dependencies. Section 28.1.2\npursues this topic.\nA further complication arises from the fact that it is possible for a multivalued\ndependency to hold only on a proper subset of the given schema, with no way to express\nthat multivalued dependency on that given schema. Such a multivalued dependency\nmay appear as the result of a decomposition. Fortunately, such cases, called embedded\nmultivalued dependencies , are rare. See the Further Reading section for details.\n7.7 More Normal Forms\nThe fourth normal form is by no means the \u201cultimate\u201d normal form. As we saw earlier,\nmultivalued dependencies help us understand and eliminate some forms of repetition\nof information that cannot be understood in terms of functional dependencies. There\nare types of constraints called join dependencies that generalize multivalued dependen-\ncies and lead to another normal form called project-join normal form ( PJNF ).PJNF is\ncalled \ufb01fth normal form in some books. There is a class of even more general constraints\nthat leads to a normal form called domain-key normal form ( DKNF ).\nA practical problem with the use of these generalized constraints is that they are\nnot only hard to reason with, but there is also no set of sound and complete inference\nrules for reasoning about the constraints. Hence PJNF andDKNF are used quite rarely.\nChapter 28 provides more details about these normal forms.\nConspicuous by its absence from our discussion of normal forms is second normal\nform (2NF). We have not discussed it because it is of historical interest only. We simply\n", "370": "342 Chapter 7 Relational Database Design\nde\ufb01ne it and let you experiment with it in Practice Exercise 7.19. First normal form\ndeals with a di\ufb00erent issue than the normal forms we have seen so far. It is discussed\nin the next section.\n7.8 Atomic Domains and First Normal Form\nThe E-Rmodel allows entity sets and relationship sets to have attributes that have\nsome degree of substructure. Speci\ufb01cally, it allows multivalued attributes such as phone\nnumber in Figure 6.8 and composite attributes (such as an attribute address with com-\nponent attributes street ,city,a n d state). When we create tables from E-Rdesigns that\ncontain these types of attributes, we eliminate this substructure. For composite at-\ntributes, we let each component be an attribute in its own right. For multivalued at-\ntributes, we create one tuple for each item in a multivalued set.\nIn the relational model, we formalize this idea that attributes do not have any sub-\nstructure. A domain is atomic if elements of the domain are considered to be indivisible\nunits. We say that a relation schema Ris in \ufb01rst normal form (1NF)i ft h ed o m a i n so f\nall attributes of Rare atomic.\nA set of names is an example of a non-atomic value. For example, if the schema of\nar e l a t i o n employee included an attribute children whose domain elements are sets of\nnames, the schema would not be in \ufb01rst normal form.\nComposite attributes, such as an attribute address with component attributes street\nandcityalso have non-atomic domains.\nIntegers are assumed to be atomic, so the set of integers is an atomic domain;\nhowever, the set of all sets of integers is a non-atomic domain. The distinction is that\nwe do not normally consider integers to have subparts, but we consider sets of integers\nto have subparts\u2014namely, the integers making up the set. But the important issue is\nnot what the domain itself is, but rather how we use domain elements in our database.\nThe domain of all integers would be non-atomic if we considered each integer to be an\nordered list of digits.\nAs a practical illustration of this point, consider an organization that assigns em-\nployees identi\ufb01cation numbers of the following form: The \ufb01rst two letters specify the\ndepartment and the remaining four digits are a unique number within the department\nfor the employee. Examples of such numbers would be \u201cCS001\u201d and \u201cEE1127\u201d. Such\nidenti\ufb01cation numbers can be divided into smaller units and are therefore non-atomic.\nIf a relation schema had an attribute whose domain consists of identi\ufb01cation numbers\nencoded as above, the schema would not be in \ufb01rst normal form.\nWhen such identi\ufb01cation numbers are used, the department of an employee can be\nfound by writing code that breaks up the structure of an identi\ufb01cation number. Doing so\nrequires extra programming, and information gets encoded in the application program\nrather than in the database. Further problems arise if such identi\ufb01cation numbers are\nused as primary keys: When an employee changes departments, the employee\u2019s identi-\n\ufb01cation number must be changed everywhere it occurs, which can be a di\ufb03cult task,\nor the code that interprets the number would give a wrong result.\n", "371": "7.9 Database-Design Process 343\nFrom this discussion, it may appear that our use of course identi\ufb01ers such as \u201cCS-\n101\u201d, where \u201cCS\u201d indicates the Computer Science department, means that the domain\nof course identi\ufb01ers is not atomic. Such a domain is not atomic as far as humans using\nthe system are concerned. However, the database application still treats the domain\nas atomic, as long as it does not attempt to split the identi\ufb01er and interpret parts of\nthe identi\ufb01er as a department abbreviation. The course schema stores the department\nname as a separate attribute, and the database application can use this attribute value\nto \ufb01nd the department of a course, instead of interpreting particular characters of the\ncourse identi\ufb01er. Thus, our university schema can be considered to be in \ufb01rst normal\nform.\nThe use of set-valued attributes can lead to designs with redundant storage of data,\nwhich in turn can result in inconsistencies. For instance, instead of having the relation-\nship between instructors and sections being represented as a separate relation teaches ,\na database designer may be tempted to store a set of course section identi\ufb01ers with\neach instructor and a set of instructor identi\ufb01ers with each section. (The primary keys\nofsection andinstructor are used as identi\ufb01ers.) Whenever data pertaining to which\ninstructor teaches which section is changed, the update has to be performed at two\nplaces: in the set of instructors for the section, and in the set of sections for the instruc-\ntor. Failure to perform both updates can leave the database in an inconsistent state.\nKeeping only one of these sets would avoid repeated information; however keeping\nonly one of these would complicate some queries, and it is unclear which of the two it\nwould be better to retain.\nSome types of non-atomic values can be useful, although they should be used\nwith care. For example, composite-valued attributes are often useful, and set-valued\nattributes are also useful in many cases, which is why both are supported in the E-\nRmodel. In many domains where entities have a complex structure, forcing a \ufb01rst\nnormal form representation represents an unnecessary burden on the application pro-\ngrammer, who has to write code to convert data into atomic form. There is also the\nruntime overhead of converting data back and forth from the atomic form. Support for\nnon-atomic values can thus be very useful in such domains. In fact, modern database\nsystems do support many types of non-atomic values, as we shall see in Chapter 29\nrestrict ourselves to relations in \ufb01rst normal form, and thus all domains are atomic.\n7.9 Database-Design Process\nSo far we have looked at detailed issues about normal forms and normalization. In this\nsection, we study how normalization \ufb01ts into the overall database-design process.\nEarlier in the chapter starting in Section 7.1.1, we assumed that a relation schema\nr(R) is given, and we proceeded to normalize it. There are several ways in which we\ncould have come up with the schema r(R):\n1.r(R) could have been generated in converting an E-Rdiagram to a set of relation\nschemas.\n", "372": "344 Chapter 7 Relational Database Design\n2.r(R) could have been a single relation schema containing allattributes that are\nof interest. The normalization process then breaks up r(R) into smaller schemas.\n3.r(R) could have been the result of an ad hoc design of relations that we then test\nto verify that it satis\ufb01es a desired normal form.\nI nt h er e s to ft h i ss e c t i o n ,w ee x a m i n et h ei m p l i c a t i o n so ft h e s ea p p r o a c h e s .W ea l s o\nexamine some practical issues in database design, including denormalization for per-\nformance and examples of bad design that are not detected by normalization.\n7.9.1 E-R Model and Normalization\nWhen we de\ufb01ne an E-Rdiagram carefully, identifying all entity sets correctly, the rela-\ntion schemas generated from the E-Rdiagram should not need much further normal-\nization. However, there can be functional dependencies among attributes of an entity\nset. For instance, suppose an instructor entity set had attributes dept\nname anddept\naddress , and there is a functional dependency dept\nname\u2192dept\naddress .W ew o u l d\nthen need to normalize the relation generated from instructor .\nMost examples of such dependencies arise out of poor E-Rdiagram design. In\nthe preceding example, if we had designed the E-Rdiagram correctly, we would have\ncreated a department entity set with attribute dept\naddress and a relationship set between\ninstructor anddepartment . Similarly, a relationship set involving more than two entity\nsets may result in a schema that may not be in a desirable normal form. Since most\nrelationship sets are binary, such cases are relatively rare. (In fact, some E-R-diagram\nvariants actually make it di\ufb03cult or impossibl e to specify nonbinary relationship sets.)\nFunctional dependencies can help us detect poor E-Rdesign. If the generated re-\nlation schemas are not in desired normal form, the problem can be \ufb01xed in the E-R\ndiagram. That is, normalization can be done formally as part of data modeling. Alter-\nnatively, normalization can be left to the designer\u2019s intuition during E-Rmodeling, and\nit can be done formally on the relation schemas generated from the E-Rmodel.\nA careful reader will have noted that in order for us to illustrate a need for mul-\ntivalued dependencies and fourth normal form, we had to begin with schemas that\nwere not derived from our E-Rdesign. Indeed, the process of creating an E-Rdesign\ntends to generate 4NF designs. If a multivalued dependency holds and is not implied by\nthe corresponding functional dependency, it usually arises from one of the following\nsources:\n\u2022A many-to-many relationship set.\n\u2022A multivalued attribute of an entity set.\nFor a many-to-many relationship set, each related entity set has its own schema, and\nthere is an additional schema for the relationship set. For a multivalued attribute, a\nseparate schema is created consisting of that attribute and the primary key of the entity\nset (as in the case of the phone\n number attribute of the entity set instructor ).\n", "373": "7.9 Database-Design Process 345\nThe universal-relation approach to relational database design starts with an as-\nsumption that there is one single relation schema containing all attributes of interest.\nThis single schema de\ufb01nes how users and applications interact with the database.\n7.9.2 Naming of Attributes and Relationships\nA desirable feature of a database design is the unique-role assumption ,w h i c hm e a n s\nthat each attribute name has a unique meaning in the database. This prevents us from\nusing the same attribute to mean di\ufb00erent things in di\ufb00erent schemas. For example, we\nmight otherwise consider using the attribute number for phone number in the instructor\nschema and for room number in the classroom schema. The join of a relation on schema\ninstructor with one on classroom is meaningless. While users and application developers\ncan work carefully to ensure use of the right number in each circumstance, having a\ndi\ufb00erent attribute name for phone number and for room number serves to reduce user\nerrors.\nWhile it is a good idea to keep names for incompatible attributes distinct, if at-\ntributes of di\ufb00erent relations have the same meaning, it may be a good idea to use the\nsame attribute name. For this reason we used the same attribute name \u201c name \u201df o rb o t h\ntheinstructor and the student entity sets. If this was not the case (i.e., if we used dif-\nferent naming conventions for the instructor and student names), then if we wished to\ngeneralize these entity sets by creating a person entity set, we would have to rename\nthe attribute. Thus, even if we did not currently have a generalization of student and\ninstructor , if we foresee such a possibility, it is best to use the same name in both entity\nsets (and relations).\nAlthough technically, the order of attribute names in a schema does not matter, it\nis a convention to list primary-key attributes \ufb01rst. This makes reading default output\n(as from select * )e a s i e r .\nIn large database schemas, relationship sets (and schemas derived therefrom) are\noften named via a concatenation of the names of related entity sets, perhaps with an\nintervening hyphen or underscore. We have used a few such names, for example, inst\nsecandstudent\n sec. We used the names teaches andtakes instead of using the longer\nconcatenated names. This was acceptable since it is not hard for you to remember the\nassociated entity sets for a few relationship sets. We cannot always create relationship-\nset names by simple concatenation; for example, a manager or works-for relationship\nbetween employees would not make much sense if it were called employee\n employee !\nSimilarly, if there are multiple relationship sets possible between a pair of entity sets,\nthe relationship-set names must include extra parts to identify the relationship set.\nDi\ufb00erent organizations have di\ufb00erent conventions for naming entity sets. For ex-\nample, we may call an entity set of students student orstudents .W eh a v ec h o s e nt ou s e\nthe singular form in our database designs. Using either singular or plural is acceptable,\nas long as the convention is used consistently across all entity sets.\n", "374": "346 Chapter 7 Relational Database Design\nAs schemas grow larger, with increasing numbers of relationship sets, using con-\nsistent naming of attributes, relationships, and entities makes life much easier for the\ndatabase designer and application programmers.\n7.9.3 Denormalization for Performance\nOccasionally database designers choose a schema that has redundant information; that\nis, it is not normalized. They use the redundancy to improve performance for speci\ufb01c\napplications. The penalty paid for not using a normalized schema is the extra work (in\nterms of coding time and execution time) to keep redundant data consistent.\nFor instance, suppose all course prerequisites have to be displayed along with the\ncourse information, every time a course is accessed. In our normalized schema, this\nrequires a join of course with prereq .\nOne alternative to computing the join on the \ufb02y is to store a relation containing all\nthe attributes of course andprereq . This makes displaying the \u201cfull\u201d course information\nfaster. However, the information for a course is repeated for every course prerequisite,\nand all copies must be updated by the application, whenever a course prerequisite is\nadded or dropped. The process of taking a normalized schema and making it non-\nnormalized is called denormalization , and designers use it to tune the performance of\nsystems to support time-critical operations.\nA better alternative, supported by many database systems today, is to use the nor-\nmalized schema and additionally store the join of course andprereq as a materialized\nview. (Recall that a materialized view is a view whose result is stored in the database\nand brought up to date when the relations used in the view are updated.) Like denor-\nmalization, using materialized views does have space and time overhead; however, it\nhas the advantage that keeping the view up to date is the job of the database system,\nnot the application programmer.\n7.9.4 Other Design Issues\nThere are some aspects of database design that are not addressed by normalization and\ncan thus lead to bad database design. Data pertaining to time or to ranges of time have\nseveral such issues. We give examples here; obviously, such designs should be avoided.\nConsider a university database, where we want to store the total number of instruc-\ntors in each department in di\ufb00erent years. A relation total\n inst(dept\nname ,year,size)\ncould be used to store the desired information. The only functional dependency on this\nrelation is dept\nname ,year\u2192size,a n dt h er e l a t i o ni si n BCNF .\nAn alternative design is to use multiple relations, each storing the size informa-\ntion for a di\ufb00erent year. Let us say the years of interest are 2017, 2018, and 2019; we\nwould then have relations of the form total\n inst\n2017,total\n inst\n2018,total\n inst\n2019,a l l\nof which are on the schema ( dept\nname ,size). The only functional dependency here on\neach relation would be dept\nname\u2192size, so these relations are also in BCNF .\nHowever, this alternative design is clearly a bad idea\u2014we would have to create a\nnew relation every year, and we would also have to write new queries every year, to take\n", "375": "7.10 Modeling Temporal Data 347\neach new relation into account. Queries would also be more complicated since they\nmay have to refer to many relations.\nYet another way of representing the same data is to have a single relation dept\nyear(dept\nname ,total\n inst\n2017,total\n inst\n2018,total\n inst\n2019). Here the only func-\ntional dependencies are from dept\nname to the other attributes, and again the relation\nis in BCNF . This design is also a bad idea since it has problems similar to the previous\ndesign\u2014namely, we would have to modify the relation schema and write new queries\nevery year. Queries would also be more complicated, since they may have to refer to\nmany attributes.\nRepresentations such as those in the dept\nyear relation, with one column for each\nvalue of an attribute, are called crosstabs ; they are widely used in spreadsheets and\nreports and in data analysis tools. While such representations are useful for display\nto users, for the reasons just given, they are not desirable in a database design. SQL\nincludes features to convert data from a normal relational representation to a cross-\ntab, for display, as we discussed in Section 11.3.1.\n7.10 Modeling Temporal Data\nSuppose we retain data in our university organization showing not only the address of\neach instructor, but also all former addresses of which the university is aware. We may\nthen ask queries, such as \u201cFind all instructors who lived in Princeton in 1981.\u201d In this\ncase, we may have multiple addresses for in structors. Each address has an associated\nstart and end date, indicating when the instructor was resident at that address. A special\nvalue for the end date, for example, null, or a value well into the future, such as 9999-\n12-31, can be used to indicate that the instructor is still resident at that address.\nIn general, temporal data are data that have an associated time interval during\nwhich they are valid.10\nModeling temporal data is a challenging problem for several reasons. For example,\nsuppose we have an instructor entity set with which we wish to associate a time-varying\naddress. To add temporal information to an address, we would then have to create a\nmultivalued attribute, each of whose values is a composite value containing an address\nand a time interval. In addition to time-varying attribute values, entities may themselves\nhave an associated valid time. For example, a student entity may have a valid time from\nthe date the student entered the university to the date the student graduated (or left\nthe university). Relationships too may have associated valid times. For example, the\nprereq relationship may record when a course became a prerequisite for another course.\nWe would thus have to add valid time intervals to attribute values, entity sets, and\nrelationship sets. Adding such detail to an E-Rdiagram makes it very di\ufb03cult to create\nand to comprehend. There have been several proposals to extend the E-Rnotation to\n10There are other models of temporal data that distinguish between valid time and transaction time , the latter recording\nwhen a fact was recorded in the database. We ignore such details for simplicity.\n", "376": "348 Chapter 7 Relational Database Design\ncourse\n id\n title\n dept\nname\n credits\n start\n end\nBIO-101\n Intro. to Biology\n Biology\n 4\n 1985-01-01\n 9999-12-31\nCS-201\n Intro. to C\n Comp. Sci.\n 4\n 1985-01-01\n 1999-01-01\nCS-201\n Intro. to Java\n Comp. Sci.\n 4\n 1999-01-01\n 2010-01-01\nCS-201\n Intro. to Python\n Comp. Sci.\n 4\n 2010-01-01\n 9999-12-31\nFigure 7.17 A temporal version of the course relation\nspecify in a simple manner that an attribute value or relationship is time varying, but\nthere are no accepted standards.\nIn practice, database designers fall back to simpler approaches to designing tempo-\nral databases. One commonly used approach is to design the entire database (including\nE-Rdesign and relational design) ignoring temporal changes. After this, the designer\nstudies the various relations and decides which relations require temporal variation to\nbe tracked.\nThe next step is to add valid time information to each such relation by adding start\nand end time as attributes. For example, consider the course relation. The title of the\ncourse may change over time, which can be handled by adding a valid time range; the\nresultant schema would be:\ncourse (course\n id,title,dept\nname ,credits ,start, end )\nAn instance of the relation is shown in Figure 7.17. Each tuple has a valid interval\nassociated with it. Note that as per the SQL:2011 standard, the interval is closed on\nthe left-hand side, that is, the tuple is valid at time start,b u ti s open on the right-hand\nside, that is, the tuple is valid until just before time end, but is invalid at time end.T h i s\nallows a tuple to have the same start time as the end time of another tuple, without\noverlapping. In general, left and right endpoints that are closed are denoted by [ and\n], while left and right endpoints that are open are denoted by ( and ). Intervals in\nSQL:2011 are of the form [ start,end), that is they are closed on the left and open on\nthe right, Note that 9999-12-31 is the highest possible date as per the SQL standard.\nIt can be seen in Figure 7.17 that the title of the course CS-201 has changed several\ntimes. Suppose that on 2020-01-01 the title of the course is updated again to, say, \u201cIntro.\nto Scala\u201d. Then, the endattribute value of the tuple with title \u201cIntro. to Python\u201d would\nbe updated to 2020-01-01, and a new tuple (CS-201, Intro. to Scala, Comp. Sci., 4,\n2020-01-01, 9999-12-31) would be added to the relation.\nWhen we track data values across time, functional dependencies that we assumed\nto hold, such as:\ncourse\n id\u2192title,dept\nname ,credits\nmay no longer hold. The following constraint (expressed in English) would hold in-\nstead: \u201cA course course\n idhas only one titleanddept\nname value at any given time\nt.\u201d\n", "377": "7.10 Modeling Temporal Data 349\nFunctional dependencies that hold at a particular point in time are called temporal\nfunctional dependencies. We use the term snapshot of data to mean the value of the\ndata at a particular point in time. Thus, a snapshot of course data gives the values of\nall attributes, such as title and department, of all courses at a particular point in time.\nFormally, a temporal functional dependency \u03b1\u03c4\u2192\u03b2holds on a relation schema r(R)\nif, for all legal instances of r(R), all snapshots of rsatisfy the functional dependency\n\u03b1\u2192\u03b2.\nThe original primary key for a temporal relation would no longer uniquely identify\na tuple. We could try to \ufb01x the problem by adding start and end time attributes to\nthe primary key, ensuring no two tuples have the same primary key value. However,\nthis solution is not correct, since it is possible to store data with overlapping valid\ntime intervals, which would not be caught by merely adding the start and end time\nattributes to the primary-key constraint. Instead, the temporal version of the primary\nkey constraint must ensure that if any two tuples have the same primary key values,\ntheir valid time intervals do not overlap. Formally, if r.Ais a temporal primary key of\nrelation r, then whenever two tuples t1andt2inrare such that t1.A=t2.A, their valid\ntime intervals of t1andt2must not overlap.\nForeign-key constraints are also more complicated when the referenced relation\nis a temporal relation. A temporal foreign key should ensure that not only does each\ntuple in the referencing relation, say r, have a matching tuple in the referenced relation,\nsays, but also their time intervals are accounted for. It is not required that there be a\nmatching tuple in swith exactly the same time interval, nor even that a single tuple in\nshas a time interval containing the time interval of the rtuple. Instead, we allow the\ntime interval of the rtuple to be covered by one or more stuples. Formally, a temporal\nforeign-key constraint from r.Atos.Bensures the following: for each tuple tinr,w i t h\nvalid time interval ( l,u), there is a subset stof one or more tuples in ssuch that each\ntuple si\u2208sthassi.B=t.A, and further the union of the temporal intervals of all the si\ncontains ( l,u).\nA record in a student\u2019s transcript should refer to the course title at the time when\nthe student took the course. Thus, the referencing relation must also record time in-\nformation, to identify a particular record from the course relation. In our university\nschema, takes .course\n idis a foreign key referencing course .T h e year andsemester val-\nues of a takes tuple could be mapped to a representative date, such as the start date of\nthe semester; the resulting date value could be used to identify a tuple in the temporal\nversion of the course relation whose valid time interval contains the speci\ufb01ed date. Al-\nternatively, a takes tuple may be associated with a valid time interval from the start date\nof the semester until the end date of the semester, and course tuples with a matching\ncourse\n idand an overlapping valid time may be retrieved; as long as course tuples are\nnot updated during a semester, there would be only one such record.\nInstead of adding temporal information to each relation, some database designers\ncreate for each relation a corresponding history relation that stores the history of up-\ndates to the tuples. For example, a designer may leave the course relation unchanged,\n", "378": "350 Chapter 7 Relational Database Design\nbut create a relation course\n history containing all the attributes of course ,w i t ha na d -\nditional timestamp attribute indicating when a record was added to the course\n history\ntable. However, such a scheme has limitations, such as an inability to associate a takes\nrecord with the correct course title.\nThe SQL:2011 standard added support for temporal data. In particular, it allows\nexisting attributes to be declared to specify a valid time interval for a tuple. For example,\nfor the extended course relation we saw above, we could declare\nperiod for validtime (start, end )\nto specify that the tuple is valid in the interval speci\ufb01ed by the start andend(which are\notherwise ordinary attributes).\nTemporal primary keys can be declared in SQL:2011 , as illustrated below, using the\nextended course schema:\nprimary key (course\n id,validtime without overlaps )\nSQL:2011 also supports temporal foreign-key constraints that allow a period to be\nspeci\ufb01ed along with the referencing relatio n attributes, as well as with the referenced\nrelation attributes. Most databases, with the exception of IBM DB2 , Teradata, and pos-\nsibly a few others, do not support temporal primary-key constraints. To the best of\nour knowledge, no database system currently supports temporal foreign-key constraints\n(Teradata allows them to be speci\ufb01ed, but at least as of 2018, does not enforce them).\nSome databases that do not directly support temporal primary-key constraints al-\nlow workarounds to enforce such constraints. For example, although Postgre SQL does\nnot support temporal primary-key constraints natively, such constraints can be en-\nforced using the exclude constraint feature supported by Postgre SQL.F o re x a m p l e ,\nconsider the course relation, whose primary key is course\n id.I n Postgre SQL,w ec a n\nadd an attribute validtime ,o ft y p e tsrange ;t h e tsrange data type of Postgre SQL stores\na timestamp range with a start and end timestamp. Postgre SQL supports an && oper-\nator on a pair of ranges, which returns true if two ranges overlap and false otherwise.\nThe temporal primary key can be enforced by adding the following exclude constraint\n(a type of constraint supported by Postgre SQL)t ot h e course relation as follows:\nexclude (course\n idwith =,validtime with &&)\nThe above constraint ensures that if two course tuples have the same course\n idvalue,\nthen their validtime intervals do not overlap.\nRelational algebra operations, such as select, project, or join, can be extended to\nt a k et e m p o r a lr e l a t i o n sa si n p u t sa n dg e n e r a te temporal relations as outputs. Selection\nand projection operations on temporal relations output tuples whose valid time inter-\nvals are the same as that of their corresponding input tuples. A temporal join is slightly\ndi\ufb00erent: the valid time of a tuple in the join result is de\ufb01ned as the intersection of the\nvalid times of the tuples from which it is derived. If the valid times do not intersect, the\ntuple is discarded from the result. To the best of our knowledge, no database supports\ntemporal joins natively, although they can be expressed by SQL queries that explicitly\n", "379": "7.11 Summary 351\nhandle the temporal conditions. Predicates, such as overlaps, contains, before, and after\nand operations such as intersection anddi\ufb00erence on pairs of intervals are supported by\nseveral database systems.\n7.11 Summary\n\u2022We showed pitfalls in database design and how to design a database schema sys-\ntematically in a way that avoids those pitfalls. The pitfalls included repeated infor-\nmation and inability to represent some information.\n\u2022Chapter 6 showed the development of a relational database design from an E-R\ndesign and when schemas may be combined safely.\n\u2022Functional dependencies are consistency constraints that are used to de\ufb01ne two\nwidely used normal forms, Boyce\u2013Codd normal form ( BCNF )a n dt h i r dn o r m a l\nform ( 3NF).\n\u2022If the decomposition is dependency preserving, all functional dependencies can\nbe inferred logically by considering only those dependencies that apply to one\nrelation. This permits the validity of an update to be tested without the need to\ncompute a join of relations in the decomposition.\n\u2022A canonical cover is a set of functional dependencies equivalent to a given set\nof functional dependencies, that is minimized in a speci\ufb01c manner to eliminate\nextraneous attributes.\n\u2022The algorithm for decomposing relations into BCNF ensures a lossless decompo-\nsition. There are relation schemas with a given set of functional dependencies for\nwhich there is no dependency-preserving BCNF decomposition.\n\u2022A canonical cover is used to decompose a relation schema into 3NF,w h i c hi s\na small relaxation of the BCNF condition. This algorithm produces designs that\nare both lossless and dependency-preserving. Relations in 3NF may have some\nredundancy, but that is deemed an acceptable trade-o\ufb00 in cases where there is no\ndependency-preserving decomposition into BCNF .\n\u2022Multivalued dependencies specify certain constraints that cannot be speci\ufb01ed with\nfunctional dependencies alone. Fourth normal form ( 4NF) is de\ufb01ned using the\nconcept of multivalued dependencies. Section 28.1.1 gives details on reasoning\nabout multivalued dependencies.\n\u2022Other normal forms exist, including PJNF andDKNF , which eliminate more subtle\nforms of redundancy. However, these are hard to work with and are rarely used.\nChapter 28 gives details on these normal forms. Second normal form is of only\nhistorical interest since it provides no bene\ufb01t over 3NF.\n\u2022Relational designs typically are based on simple atomic domains for each attribute.\nThis is called \ufb01rst normal form.\n", "380": "352 Chapter 7 Relational Database Design\n\u2022Time plays an important role in database systems. Databases are models of the\nreal world. Whereas most databases model the state of the real world at a point in\ntime (at the current time), temporal databases model the states of the real world\nacross time.\n\u2022There are possible database designs that are bad despite being lossless,\ndependency-preserving, and in an appropriate normal form. We showed examples\nof some such designs to illustrate that functional-dependency-based normalization,\nthough highly important, is not the only aspect of good relational design.\n\u2022In order for a database to store not only current data but also historical data, the\ndatabase must also store for each such tuple the time period for which the tuple\nis or was valid. It then becomes necessary to de\ufb01ne temporal functional depen-\ndencies to represent the idea that the functional dependency holds at any point\nin time but not over the entire relation. Similarly, the join operation needs to be\nmodi\ufb01ed so as to appropriately join only tuples with overlapping time intervals.\n\u2022In reviewing the issues in this chapter, note that the reason we could de\ufb01ne rigorous\napproaches to relational database design is that the relational data model rests on\na \ufb01rm mathematical foundation. That is one of the primary advantages of the\nrelational model compared with the other data models that we have studied.\nReview Terms\n\u2022Decomposition\n\u00b0Lossy decompositions\n\u00b0Lossless decompositions\n\u2022Normalization\n\u2022Functional dependencies\n\u2022Legal instance\n\u2022Superkey\n\u2022Rsatis\ufb01es F\n\u2022Functional dependency\n\u00b0Holds\n\u00b0Trivial\n\u00b0Trivial\n\u2022C l o s u r eo fas e to ff u n c t i o n a l\ndependencies\n\u2022Dependency preserving\u2022Third normal form\n\u2022Transitive dependencies\n\u2022Logically implied\n\u2022Axioms\n\u2022Armstrong\u2019s axioms\n\u2022Sound\n\u2022Complete\n\u2022Functionally determined\n\u2022Extraneous attributes\n\u2022Canonical cover\n\u2022Restriction of FtoRi\n\u2022Dependency-preserving decomposi-\ntion\n\u2022Boyce\u2013Codd normal form\n(BCNF )\n\u2022BCNF decomposition algorithm\n", "381": "Practice Exercises 353\n\u2022Third normal form ( 3NF)\n\u20223NF decomposition algorithm\n\u20223NF synthesis algorithm\n\u2022Multivalued dependency\n\u00b0Equality-generating dependencies\n\u00b0Tuple-generating dependencies\n\u00b0Embedded multivalued dependen-\ncies\n\u2022Closure\n\u2022Fourth normal form ( 4NF)\n\u2022Restriction of DtoRi\n\u2022Fifth normal form\u2022Domain-key normal form ( DKNF )\n\u2022Atomic domains\n\u2022First normal form ( 1NF)\n\u2022Unique-role assumption\n\u2022Denormalization\n\u2022Crosstabs\n\u2022Temporal data\n\u2022Snapshot\n\u2022Temporal functional dependency\n\u2022Temporal primary key\n\u2022Temporal foreign-key\n\u2022Temporal join\nPractice Exercises\n7.1 Suppose that we decompose the schema R=(A,B,C,D,E)i n t o\n(A,B,C)\n(A,D,E).\nShow that this decomposition is a lossless decomposition if the following set F\nof functional dependencies holds:\nA\u2192BC\nCD\u2192E\nB\u2192D\nE\u2192A\n7.2 List all nontrivial functional dependencies satis\ufb01ed by the relation of Figure\n7.18.\nA\n B\n C\na1\n b1\n c1\na1\n b1\n c2\na2\n b1\n c1\na2\n b1\n c3\nFigure 7.18 Relation of Exercise 7.2.\n", "382": "354 Chapter 7 Relational Database Design\n7.3 Explain how functional dependencies can be used to indicate the following:\n\u2022A one-to-one relationship set exists between entity sets student andinstruc-\ntor.\n\u2022A many-to-one relationship set exists between entity sets student andinstruc-\ntor.\n7.4 Use Armstrong\u2019s axioms to prove the soundness of the union rule. ( Hint:U s et h e\naugmentation rule to show that, if \u03b1\u2192\u03b2,t h e n\u03b1\u2192\u03b1\u03b2. Apply the augmentation\nrule again, using \u03b1\u2192\u03b3, and then apply the transitivity rule.)\n7.5 Use Armstrong\u2019s axioms to prove the soundness of the pseudotransitivity rule.\n7.6 Compute the closure of the following set Fof functional dependencies for rela-\ntion schema R=(A,B,C,D,E).\nA\u2192BC\nCD\u2192E\nB\u2192D\nE\u2192A\nList the candidate keys for R.\n7.7 Using the functional dependencies of Exercise 7.6, compute the canonical\ncover Fc.\n7.8 Consider the algorithm in Figure 7.19 to compute \u03b1+. Show that this algorithm\nis more e\ufb03cient than the one presented in Figure 7.8 (Section 7.4.2) and that it\ncomputes \u03b1+correctly.\n7.9 Given the database schema R(A,B,C), and a relation ron the schema R,w r i t e\nanSQL query to test whether the functional dependency B\u2192Cholds on re-\nlation r. Also write an SQL assertion that enforces the functional dependency.\nAssume that no null values are present. (Although part of the SQL standard,\nsuch assertions are not supported by any database implementation currently.)\n7.10 Our discussion of lossless decomposition implicitly assumed that attributes on\nthe left-hand side of a functional dependency cannot take on null values. What\ncould go wrong on decomposition, if this property is violated?\n7.11 In the BCNF decomposition algorithm, suppose you use a functional depen-\ndency\u03b1\u2192\u03b2to decompose a relation schema r(\u03b1,\u03b2,\u03b3)i n t o r1(\u03b1,\u03b2)a n d r2(\u03b1,\u03b3).\na. What primary and foreign-key constraint do you expect to hold on the\ndecomposed relations?\nb. Give an example of an inconsistency that can arise due to an erroneous\nupdate, if the foreign-key constraint were not enforced on the decomposed\nrelations above.\n", "383": "Practice Exercises 355\nresult :=\u2205;\n/*fdcount is an array whose ith element contains the number\nof attributes on the left side of the ithFDthat are\nnot yet known to be in \u03b1+*/\nfori:=1t o |F|do\nbegin\nlet\u03b2\u2192\u03b3denote the ithFD;\nfdcount [i]:=|\u03b2|;\nend\n/*appears is an array with one entry for each attribute. The\nentry for attribute Ais a list of integers. Each integer\nion the list indicates that Aappears on the left side\nof the ithFD*/\nfor each attribute Ado\nbegin\nappears [A]:=NIL;\nfori:=1t o |F|do\nbegin\nlet\u03b2\u2192\u03b3denote the ithFD;\nifA\u2208\u03b2 then additoappears [A];\nend\nend\naddin (\u03b1);\nreturn (result );\nprocedure addin (\u03b1);\nfor each attribute Ain\u03b1do\nbegin\nifA\u2209result then\nbegin\nresult :=result\u222a{A};\nfor each element iofappears [A]do\nbegin\nfdcount [i]:=fdcount [i]\u22121;\niffdcount [i]:=0then\nbegin\nlet\u03b2\u2192\u03b3denote the ithFD;\naddin (\u03b3);\nend\nend\nend\nend\nFigure 7.19 An algorithm to compute \u03b1+.\n", "384": "356 Chapter 7 Relational Database Design\nc. When a relation schema is decomposed into 3NF using the algorithm in\nSection 7.5.2, what primary and foreign-key dependencies would you ex-\npect to hold on the decomposed schema?\n7.12 LetR1,R2,\u2026,Rnbe a decomposition of schema U.L e t u(U)b ear e l a t i o n ,a n d\nletri=\u03a0RI(u). Show that\nu\u2286r1\u22c8r2\u22c8\u22ef\u22c8 rn\n7.13 Show that the decomposition in Exercise 7.1 is not a dependency-preserving\ndecomposition.\n7.14 Show that there can be more than one canonical cover for a given set of func-\ntional dependencies, using the following set of dependencies:\nX\u2192YZ,Y\u2192XZ,a n d Z\u2192XY.\n7.15 The algorithm to generate a canonical cover only removes one extraneous at-\ntribute at a time. Use the functional dependencies from Exercise 7.14 to show\nwhat can go wrong if two attributes inferred to be extraneous are deleted at\nonce.\n7.16 Show that it is possible to ensure that a dependency-preserving decomposition\ninto 3NF is a lossless decomposition by guaranteeing that at least one schema\ncontains a candidate key for the schema being decomposed. ( Hint: Show that\nthe join of all the projections onto the schemas of the decomposition cannot\nhave more tuples than the original relation.)\n7.17 Give an example of a relation schema R\u2032and set F\u2032of functional dependen-\ncies such that there are at least three distinct lossless decompositions of R\u2032into\nBCNF .\n7.18 Let a prime attribute be one that appears in at least one candidate key. Let \u03b1and\n\u03b2be sets of attributes such that \u03b1\u2192\u03b2holds, but \u03b2\u2192\u03b1does not hold. Let Abe\nan attribute that is not in \u03b1,i sn o ti n \u03b2, and for which \u03b2\u2192Aholds. We say that\nAistransitively dependent on\u03b1. We can restate the de\ufb01nition of 3NF as follows:\nA relation schema Ris in 3NF with respect to a set Fof functional dependencies\nif there are no nonprime attributes AinRfor which Ais transitively dependent\non a key for R. Show that this new de\ufb01nition is equivalent to the original one.\n7.19 A functional dependency \u03b1\u2192\u03b2is called a partial dependency if there is a\nproper subset \u03b3of\u03b1such that \u03b3\u2192\u03b2;w es a yt h a t \u03b2ispartially dependent on\u03b1.A\nrelation schema Ris in second normal form (2NF) if each attribute AinRmeets\none of the following criteria:\n\u2022It appears in a candidate key.\n", "385": "Exercises 357\n\u2022It is not partially dependent on a candidate key.\nShow that every 3NF schema is in 2NF.(Hint: Show that every partial depen-\ndency is a transitive dependency.)\n7.20 Give an example of a relation schema Rand a set of dependencies such that R\nis in BCNF but is not in 4NF.\nExercises\n7.21 Give a lossless decomposition into BCNF of schema Rof Exercise 7.1.\n7.22 Give a lossless, dependency-preserving decomposition into 3NF of schema Rof\nExercise 7.1.\n7.23 Explain what is meant by repetition of information andinability to represent in-\nformation . Explain why each of these properties may indicate a bad relational-\ndatabase design.\n7.24 Why are certain functional dependencies called trivial functional dependencies?\n7.25 Use the de\ufb01nition of functional dependency to argue that each of Armstrong\u2019s\naxioms (re\ufb02exivity, augmentation, and transitivity) is sound.\n7.26 Consider the following proposed rule for functional dependencies: If \u03b1\u2192\u03b2and\n\u03b3\u2192\u03b2,t h e n\u03b1\u2192\u03b3. Prove that this rule is notsound by showing a relation rthat\nsatis\ufb01es\u03b1\u2192\u03b2and\u03b3\u2192\u03b2,b u td o e sn o ts a t i s f y \u03b1\u2192\u03b3.\n7.27 Use Armstrong\u2019s axioms to prove the soundness of the decomposition rule.\n7.28 Using the functional dependencies of Exercise 7.6, compute B+.\n7.29 Show that the following decomposition of the schema Rof Exercise 7.1 is not a\nlossless decomposition:\n(A,B,C)\n(C,D,E).\nHint: Give an example of a relation r(R)s u c ht h a t \u03a0A,B,C(r)\u22c8\u03a0C,D,E(r)\u2260r\n7.30 Consider the following set Fof functional dependencies on the relation schema\n(A,B,C,D,E,G):\nA\u2192BCD\nBC\u2192DE\nB\u2192D\nD\u2192A\n", "386": "358 Chapter 7 Relational Database Design\na. Compute B+.\nb. Prove (using Armstrong\u2019s axioms) that AGis a superkey.\nc. Compute a canonical cover for this set of functional dependencies F;g i v e\neach step of your derivation with an explanation.\nd. Give a 3NF decomposition of the given schema based on a canonical\ncover.\ne. Give a BCNF decomposition of the given schema using the original set F\nof functional dependencies.\n7.31 Consider the schema R=(A,B,C,D,E,G)a n dt h es e t Fof functional depen-\ndencies:\nAB\u2192CD\nB\u2192D\nDE\u2192B\nDEG\u2192AB\nAC\u2192DE\nRis not in BCNF for many reasons, one of which arises from the functional\ndependency AB\u2192CD. Explain why AB\u2192CDshows that Ris not in BCNF\nand then use the BCNF decomposition algorithm starting with AB\u2192CDto\ngenerate a BCNF decomposition of R. Once that is done, determine whether\nyour result is or is not dependency preserving, and explain your reasoning.\n7.32 Consider the schema R=(A,B,C,D,E,G)a n dt h es e t Fof functional depen-\ndencies:\nA\u2192BC\nBD\u2192E\nCD\u2192AB\na. Find a nontrivial functional dependency containing no extraneous at-\ntributes that is logically implied by the above three dependencies and ex-\nplain how you found it.\nb. Use the BCNF decomposition algorithm to \ufb01nd a BCNF decomposition\nofR.S t a r tw i t h A\u2192BC. Explain your steps.\nc. For your decomposition, state whether it is lossless and explain why.\nd. For your decomposition, state whether it is dependency preserving and\nexplain why.\n", "387": "Exercises 359\n7.33 Consider the schema R=(A,B,C,D,E,G)a n dt h es e t Fof functional depen-\ndencies:\nAB\u2192CD\nADE\u2192GDE\nB\u2192GC\nG\u2192DE\nUse the 3NF decomposition algorithm to generate a 3NF decomposition of R,\nand show your work. This means:\na. A list of all candidate keys\nb. A canonical cover for F, along with an explanation of the steps you took\nto generate it\nc. The remaining steps of the algorithm, with explanation\nd. The \ufb01nal decomposition\n7.34 Consider the schema R=(A,B,C,D,E,G,H)a n dt h es e t Fof functional de-\npendencies:\nAB\u2192CD\nD\u2192C\nDE\u2192B\nDEH\u2192AB\nAC\u2192DC\nUse the 3NF decomposition algorithm to generate a 3NF decomposition of R,\nand show your work. This means:\na. A list of all candidate keys\nb. A canonical cover for F\nc. The steps of the algorithm, with explanation\nd. The \ufb01nal decomposition\n7.35 Although the BCNF algorithm ensures that the resulting decomposition is loss-\nless, it is possible to have a schema and a decomposition that was not generated\nby the algorithm, that is in BCNF , and is not lossless. Give an example of such\na schema and its decomposition.\n7.36 Show that every schema consisting of exactly two attributes must be in BCNF\nregardless of the given set Fof functional dependencies.\n", "388": "360 Chapter 7 Relational Database Design\n7.37 List the three design goals for relational databases, and explain why each is\ndesirable.\n7.38 In designing a relational database, why might we choose a non- BCNF design?\n7.39 Given the three goals of relational database design, is there any reason to design\na database schema that is in 2NF, but is in no higher-order normal form? (See\nExercise 7.19 for the de\ufb01nition of 2NF.)\n7.40 Given a relational schema r(A,B,C,D), does A\u2192\u2192BClogically imply A\u2192\u2192B\nandA\u2192\u2192C? If yes prove it, or else give a counter example.\n7.41 Explain why 4NF is a normal form more desirable than BCNF .\n7.42 Normalize the following schema, with given constraints, to 4NF.\nbooks (accessionno ,isbn,title,author ,publisher )\nusers (userid ,name ,deptid ,deptname )\naccessionno \u2192isbn\nisbn\u2192title\nisbn\u2192publisher\nisbn\u2192\u2192author\nuserid\u2192name\nuserid\u2192deptid\ndeptid\u2192deptname\n7.43 Although SQL does not support functional dependency constraints, if the\ndatabase system supports constraints on materialized views, and materialized\nviews are maintained immediately, it is possible to enforce functional depen-\ndency constraints in SQL. Given a relation r(A,B,C), explain how constraints\non materialized views can be used to enforce the functional dependency B\u2192C.\n7.44 Given two relations r(A,B,validtime )a n d s(B,C,validtime ), where validtime de-\nnotes the valid time interval, write an SQL query to compute the temporal nat-\nural join of the two relations. You can use the && operator to check if two\nintervals overlap and the \u2217operator to compute the intersection of two inter-\nvals.\nFurther Reading\nThe \ufb01rst discussion of relational database design theory appeared in an early paper by\n[Codd (1970)]. In that paper, Codd also introduced functional dependencies and \ufb01rst,\nsecond, and third normal forms.\nArmstrong\u2019s axioms were introduced in [Armstrong (1974)]. BCNF was introduced\nin [Codd (1972)]. [Maier (1983)] is a classic textbook that provides detailed coverage\nof normalization and the theory of functional and multivalued dependencies.\n", "389": "Further Reading 361\nBibliography\n[Armstrong (1974)] W. W. Armstrong, \u201cDependency Structures of Data Base Relation-\nships\u201d, In Proc. of the 1974 IFIP Congress (1974), pages 580\u2013583.\n[Codd (1970)] E. F. Codd, \u201cA Relational Model for Large Shared Data Banks\u201d, Communi-\ncations of the ACM , Volume 13, Number 6 (1970), pages 377\u2013387.\n[Codd (1972)] E. F. Codd. \u201cFurther Normalization of the Data Base Relational Model\u201d, In\n[Rustin (1972)] , pages 33\u201364 (1972).\n[Maier (1983)] D. Maier, The Theory of Relational Databases , Computer Science Press\n(1983).\n[Rustin (1972)] R. Rustin, Data Base Systems , Prentice Hall (1972).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "390": "", "391": "PART3\nAPPLICATION DESIGN\nAND DEVELOPMENT\nOne of the key requirements of the relational model is that data values be atomic: mul-\ntivalued, composite, and other complex data types are disallowed by the core relational\nmodel. However, there are many applications where the constraints on data types im-\nposed by the relational model cause more problems than they solve. In Chapter 8, we\ndiscuss several complex data types , including semistructured data types that are widely\nused in building applications, object-based data, textual data, and spatial data.\nPractically all use of databases occurs from within application programs. Corre-\nspondingly, almost all user interaction with d atabases is indirect, via application pro-\ngrams. Database-backed applications are ubiquitous on the web as well as on mobile\nplatforms. In Chapter 9, we study tools and technologies that are used to build appli-\ncations, focusing on interactive applications that use databases to store and retrieve\ndata.\n363\n", "392": "", "393": "CHAPTER8\nComplex Data Types\nThe relational model is very widely used for data representation for a large number\nof application domains. One of the key requirements of the relational model is that\ndata values be atomic: multivalued, composite, and other complex data types are dis-\nallowed by the core relational model. However, there are many applications where the\nconstraints on data types imposed by the relational model cause more problems than\nthey solve. In this chapter, we discuss several non-atomic data types that are widely used,\nincluding semi-structured data, object-based data, textual data, and spatial data.\n8.1 Semi-structured Data\nRelational database designs have tables with a \ufb01xed number of attributes, each of which\ncontains an atomic value. Changes to the schema, such as adding an extra attribute,\nare rare events, and may require changing of application code. Such a design is well\nsuited to many organizational applications.\nHowever, there are many application domains that need to store more complex\ndata, whose schema changes often. Fast evolving web applications are an example of\nsuch a domain. As an example of the data management needs of such applications,\nconsider the pro\ufb01le of a user which needs to be accessible to a number of di\ufb00erent\napplications. The pro\ufb01le contains a variety of attributes, and there are frequent addi-\ntions to the attributes stored in the pro\ufb01le. Some attributes may contain complex data;\nfor example, an attribute may store a set of interests that can be used to show the user\narticles related to the set of interests. While such a set can be stored in a normalized\nfashion in a separate relation, a set data type allows signi\ufb01cantly more e\ufb03cient access\nthan does a normalized representation. We study a number of data models that support\nrepresentation of semi-structured data in this section.\nData exchange is another very important motivation for semi-structured data rep-\nresentations; it is perhaps even more important than storage for many applications. A\npopular architecture for building information systems today is to create a web service\nthat allows retrieval of data and to build application code that displays the data and al-\nlows user interaction. Such application cod e may be developed as mobile applications,\n365\n", "394": "366 Chapter 8 Complex Data Types\nor it may be written in JavaScript and run on the browser. In either case, the ability\nto run on the client\u2019s machine allows developers to create very responsive user inter-\nfaces, unlike the early generation of web interfaces where backend servers send HTML\nmarked-up text to browsers, which display the HTML . A key to building such applica-\ntions is the ability to e\ufb03ciently exchange and process complex data between backend\nservers and clients. We study the JSON and XML data models that have been widely\nadopted for this task.\n8.1.1 Overview of Semi-structured Data Models\nThe relational data model has been extended in several ways to support the storage and\ndata exchange needs of modern applications.\n8.1.1.1 Flexible Schema\nSome database systems allow each tuple to potentially have a di\ufb00erent set of attributes;\nsuch a representation is referred to as a wide column data representation. The set of\nattributes is not \ufb01xed in such a representation; each tuple may have a di\ufb00erent set of\nattributes, and new attributes may be added as needed.\nA more restricted form of this representation is to have a \ufb01xed but very large num-\nber of attributes, with each tuple using only those attributes that it needs, leaving the\nrest with null values; such a representation is called a sparse column representation.\n8.1.1.2 Multivalued Data Types\nMany data representations allow attributes to contain non-atomic values. Many\ndatabases allow the storage of sets,multisets ,o rarrays as attribute values. For exam-\nple, an application that stores topics of interest to a user, and uses the topics to target\narticles or advertisements to the user, may store the topics as a set. An example of such\nas e tm a yb e :\n{basketball, La Liga, cooking, anime, Jazz }\nAlthough a set-valued attribute can be stored in a normalized form as we saw earlier in\nSection 6.7.2, doing so provides no bene\ufb01ts in this case, since lookups are always based\non the user, and normalization would signi\ufb01cantly increase the storage and querying\noverhead.\nSome representations allow attributes to store key-value maps ,w h i c hs t o r ek e y -\nvalue pairs. A key-value map , often just called a map,i sas e to f( key, value )p a i r s ,s u c h\nthat each key occurs in at most one element. For example, e-commerce sites often list\nspeci\ufb01cations or details for each product that they sell, such as brand, model, size,\ncolor, and numerous other product-speci\ufb01c details. The set of speci\ufb01cations may be\ndi\ufb00erent for each product. Such speci\ufb01cations can be represented as a map, where\n", "395": "8.1 Semi-structured Data 367\nthe speci\ufb01cations form the key, and the associated value is stored with the key. The\nfollowing example illustrates such a map:\n{(brand, Apple), (ID, MacBook Air), (size, 13), (color, silver) }\nThe put(key, value) method can be used to add a key-value pair, while the get(key)\nmethod can be used to retrieve the value associated with a key. The delete(key) method\ncan be used to delete a key-value pair from the map.\nArrays are very important for scienti\ufb01c and monitoring applications. For example,\nscienti\ufb01c applications may need to store images, which are basically two-dimensional\narrays of pixel values. Scienti\ufb01c experiments as well as industrial monitoring applica-\ntions often use multiple sensors that provide r eadings at regular intervals. Such readings\ncan be viewed as an array. In fact, treating a stream of readings as an array requires far\nless space than storing each reading as a separate tuple, with attributes such as ( time,\nreading ). Not only do we avoid storing the time attribute explicitly (it can be inferred\nfrom the o\ufb00set), but we can also reduce per-tuple overhead in the database, and most\nimportantly we can use compression techniques to reduce the space needed to store\nan array of readings.\nSupport for multivalued attribute types was proposed early in the history of\ndatabases, and the associated data model was called the non \ufb01rst-normal-form ,o r NFNF ,\ndata model. Several relational databases such as Oracle and Postgre SQL support set\nand array types.\nAnarray database is a database that provides specialized support for arrays, in-\ncluding e\ufb03cient compressed storage, and query language extensions to support oper-\nations on arrays. Examples include the Oracle GeoRaster, the PostGISextension to\nPostgre SQL,t h e SciQLextension of Monet DB,a n d SciDB, a database tailored for sci-\nenti\ufb01c applications, with a number of features tailored for array data types.\n8.1.1.3 Nested Data Types\nMany data representations allow attributes to be structured, directly modeling compos-\nite attributes in the E-Rmodel. For example, an attribute name may have component\nattributes \ufb01rstname ,a n d lastname . These representations also support multivalued data\ntypes such as sets, arrays, and maps. All of these data types represent a hierarchy of\ndata types, and that structure leads to the use of the term nested data types .M a n y\ndatabases support such types as part of their support for object-oriented data, which\nwe describe in Section 8.2.\nIn this section, we outline two widely used data representations that allow values\nto have complex internal structures and that are \ufb02exible in that values are not forced\nto adhere to a \ufb01xed schema. These are the JavaScript Object Notation (JSON ), which\nwe describe in Section 8.1.2, and the Extensible Markup Language (XML ), which we\ndescribe in Section 8.1.3.\nLike the wide-table approach, the JSON andXML representations provide \ufb02exibility\nin the set of attributes that a record contains, as well as the types of these attributes.\n", "396": "368 Chapter 8 Complex Data Types\nHowever, the JSON andXML representations permit a more \ufb02exible structuring of data,\nwhere objects could have sub-objects; each object thus corresponds to a tree structure.\nSince they allow multiple pieces of information about a business object to be pack-\naged into a single structure, the JSON and XML representations have both found sig-\nni\ufb01cant acceptance in the context of data exchange between applications.\nToday, JSON is widely used today for exchanging data between the backends and\nthe user-facing sides of applications, such as mobile apps, and Web apps. JSON has also\nfound favor for storing complex objects in storage systems that collect di\ufb00erent data\nrelated to a particular user into one large object (sometimes referred to as a document),\nallowing data to be retrieved without the need for joins. XML is an older representation\nand is used by many systems for storing con\ufb01guration and other information, and for\ndata exchange.\n8.1.1.4 Knowledge Representation\nRepresentation of human knowledge has long been a goal of the arti\ufb01cial intelligence\ncommunity. A variety of models were proposed for this task, with varying degrees of\ncomplexity; these could represent facts as well as rules about facts. With the growth of\nthe web, a need arose to represent extremely large knowledge bases, with potentially\nbillions of facts. The Resource Description Format (RDF) data representation is one such\nrepresentation that has found very wide acceptance. The representation actually has\nfar fewer features than earlier representations, but it was better suited to handle very\nlarge data volumes than the earlier knowledge representations.\nLike the E-Rmodel which we studied earlier, RDF models data as objects that have\nattributes and have relationships with other objects. RDF data can be viewed as a set\nof triples (3-tuples), or as a graph, with obj ects and attribute values modeled as nodes\nand relationships and attribute names as edges. We study RDF in more detail in Section\n8.1.4.\n8.1.2 JSON\nThe JavaScript Object Notation (JSON ), is a textual representation of complex data\ntypes that is widely used to transmit data between applications and to store complex\ndata. JSON supports the primitive data types integer, real and string, as well as arrays,\nand \u201cobjects,\u201d which are a collection of (attribute name, value) pairs.\nFigure 8.1 shows an example of data represented using JSON . Since objects do not\nhave to adhere to any \ufb01xed schema, they are basically the same as key-value maps, with\nthe attribute names as keys and the attribute values as the associated values.\nThe example also illustrates arrays, shown in square brackets. In JSON ,a na r r a y\ncan be thought of as a map from integer o\ufb00sets to values, with the square-bracket syntax\nviewed as just a convenient way of creating such maps.\nJSON is today theprimary data representation used for communication between\napplications and web services. Many modern applications use web services to store\n", "397": "8.1 Semi-structured Data 369\n{\n\"ID\": \"22222\",\n\"name\": {\n\"firstname: \"Albert\",\n\"lastname: \"Einstein\"\n},\n\"deptname\": \"Physics\",\n\"children\": [\n{\"firstname\": \"Hans\", \"lastname\": \"Einstein\" },\n{\"firstname\": \"Eduard\", \"lastname\": \"Einstein\" }\n]\n}\nFigure 8.1 Example of JSON data.\nand retrieve data and to perform computations at a backend server; web services are\ndescribed in more detail in Section 9.5.2. A pplications invoke web services by sending\nparameters either as simple values such as strings or numbers, or by using JSON for\nmore complex parameters. The web service then returns results using JSON .F o re x a m -\nple, an email user interface may invoke web services for each of these tasks: authenti-\ncating the user, fetching email header information to show a list of emails, fetching an\nemail body, sending email, and so on.\nThe data exchanged in each of these steps are complex and have an internal struc-\nture. The ability of JSON to represent complex structures, and its ability to allow \ufb02exible\nstructuring, make it a good \ufb01t for such applications.\nA number of libraries are available that make it easy to transform data between\ntheJSON representation and the object representation used in languages such as\nJavaScript, Java, Python, PHP, and other languages. The ease of interfacing between\nJSON and programming language data structures has played a signi\ufb01cant role in the\nwidespread use of JSON .\nUnlike a relational representation, JSON is verbose and takes up more storage space\nfor the same data. Further, parsing the text to retrieve required \ufb01elds can be very CPU\nintensive. Compressed representations that also make it easier to retrieve values with-\nout parsing are therefore popular for storage of data. For example, a compressed binary\nformat called BSON (short for Binary JSON ) is used in many systems for storing JSON\ndata.\nThe SQL language itself has been extended to support the JSON representation in\nseveral ways:\n\u2022JSON data can be stored as a JSON data type.\n\u2022SQL queries can generate JSON data from relational data:\n", "398": "370 Chapter 8 Complex Data Types\n\u00b0There are SQL extensions that allow construction of JSON objects in each row\nof a query result. For example, Postgre SQLsupports a json\n build\n object() func-\ntion. As an example of its use, json\n build\n object('ID', 12345, 'name' 'Ein-\nstein') returns a JSON object {\"ID\": 12345, \"name\", \"Einstein\"} .\n\u00b0There are also SQL extensions that allow creation of a JSON object from a\ncollection of rows by using an aggregate function. For example, the json\n agg\naggregate function in Postgre SQL allows creation of a single JSON object from\na collection of JSON objects. Oracle supports a similar aggregate function\njson\n objectagg ,a sw e l la sa na g g r e g a t e json\n arraytagg ,w h i c hc r e a t e sa JSON\narray with objects in a speci\ufb01ed order. SQL S erver supports a FOR JSON AUTO\nclause that formats the result of an SQL query as a JSON array, with one ele-\nment per row in the SQL query.\n\u2022SQL queries can extract data from a JSON object using some form of path con-\nstructs. For example, in Postgre SQL,i fav a l u e vis of type JSON and has an at-\ntribute \u201cID\u201d, v\u2212>\u2019ID\u2019would return the value of the \u201cID\u201d attribute of v.O r a c l e\nsupports a similar feature, using a \u201c.\u201d instead of \u201c \u2212>\u201d, while SQL S erver uses a\nfunction JSON\n VALUE(value, path) to extract values from JSON objects using a\nspeci\ufb01ed path.\nThe exact syntax and semantics of these extensions, unfortunately, depend entirely on\nthe speci\ufb01c database system. You can \ufb01nd references to more details on these exten-\nsions in the bibliographic notes for this chapter, available online.\n8.1.3 XML\nThe XML data representation adds tags enclosed in angle brackets, <>,t om a r ku p\ninformation in a textual representation. Tags are used in pairs, with <tag>and</tag>\ndelimiting the beginning and the end of the portion of the text to which the tag refers.\nFor example, the title of a document might be marked up as follows:\n<title>Database System Concepts </title>\nSuch tags can be used to represent relational data specifying relation names and at-\ntribute names as tags, as shown below:\n<course>\n<course\n id> CS-101 </course\n id>\n<title> Intro. to Computer Science </title>\n<dept\n name> Comp. Sci. </dept\n name>\n<credits> 4 </credits>\n</course>\n", "399": "8.1 Semi-structured Data 371\n<purchase\n order>\n<identifier >P-101 </identifier >\n<purchaser >\n<name >Cray Z. Coyote </name >\n<address >Route 66, Mesa Flats, Arizona 86047, USA </address >\n</purchaser >\n<supplier >\n<name >Acme Supplies </name >\n<address >1 Broadway, New York, NY, USA </address >\n</supplier >\n<itemlist >\n<item>\n<identifier >RS1</identifier >\n<description >Atom powered rocket sled </description >\n<quantity >2</quantity >\n<price>199.95 </price >\n</item>\n<item>\n<identifier >SG2</identifier >\n<description >Superb glue </description >\n<quantity >1</quantity >\n<unit-of-measure >liter</unit-of-measure >\n<price>29.95 </price >\n</item>\n</itemlist >\n<total\n cost>429.85 </total\n cost>\n<payment\n terms >Cash-on-delivery </payment\n terms >\n<shipping\n mode >1-second-delivery </shipping\n mode >\n</purchase\n order>\nFigure 8.2 XML representation of a purchase order.\nUnlike with a relational schema, new tags can be introduced easily, and with suit-\nable names the data are \u201cself-documenting\u201d in that a human can understand or guess\nwhat a particular piece of data means based on the name.\nFurthermore, tags can be used to create hierarchical structures, which is not pos-\nsible with the relational model. Hierarchical structures are particularly important for\nrepresenting business objects that must be exchanged between organizations; examples\ninclude bills, purchase orders, and so forth.\nFigure 8.2, which shows how information about a purchase order can be repre-\nsented in XML , illustrates a more realistic use of XML .P u r c h a s eo r d e r sa r et y p i c a l l y\ngenerated by one organization and sent to another. A purchase order contains a variety\nof information; the nested representation allows all information in a purchase order to\n", "400": "372 Chapter 8 Complex Data Types\nbe represented naturally in a single document. (Real purchase orders have consider-\nably more information than that depicted in this simpli\ufb01ed example.) XML provides a\nstandard way of tagging the data; the two organizations must of course agree on what\ntags appear in the purchase order and what they mean.\nThe XQuery language was developed to support querying of XML data. Further\ndetails of XML and XQuery may be found in Chapter 30. Although XQuery implemen-\ntations are available from several vendors, unlike SQL,a d o p t i o no f XQuery has been\nrelatively limited.\nHowever, the SQL language itself has been extended to support XML in several\nways:\n\u2022XML data can be stored as an XML data type.\n\u2022SQL queries can generate XML data from relational data. Such extensions are very\nuseful for packaging related pieces of data into one XML document, which can\nthen be sent to another application.\nThe extensions allow the construction of XML representations from individual\nr o w s ,a sw e l la st h ec r e a t i o no fa n XML document from a collection of rows by\nusing an XMLAGG aggregate function.\n\u2022SQL queries can extract data from an XML data type value. For example, the XPath\nlanguage supports \u201cpath expressions\u201d that allow the extraction of desired parts of\ndata from an XML document.\nYou can \ufb01nd more details on these extensions in Chapter 30.\n8.1.4 RDF and Knowledge Graphs\nThe Resource Description Framework (RDF) is a data representation standard based on\nthe entity-relationship model. We provide an overview of RDF in this section.\n8.1.4.1 Triple Representation\nThe RDF model represents data by a set of triples that are in one of these two forms:\n1.(ID, attribute-name, value )\n2.(ID1, relationship-name, ID2 )\nwhere ID, ID1 and ID2are identi\ufb01ers of entities; entities are also referred to as resources\ninRDF. Note that unlike the E-Rmodel, the RDF model only supports binary relation-\nships, and it does not support more general n-ary relationships; we return to this issue\nlater.\nThe \ufb01rst attribute of a triple is called its subject , the second attribute is called\nitspredicate , and the last attribute is called its object .T h u s ,at r i p l eh a st h es t r u c t u r e\n(subject, predicate, object).\n", "401": "8.1 Semi-structured Data 373\n10101 instance-of instructor .\n10101 name \"Srinivasan\" .\n10101 salary \"6500\" .\n00128 instance-of student .\n00128 name \"Zhang\" .\n00128 tot\n cred \"102\" .\ncomp\n sci instance-of department .\ncomp\n sci dept\n name \"Comp. Sci.\" .\nbiology instance-of department .\nCS-101 instance-of course .\nCS-101 title \"Intro. to Computer Science\" .\nCS-101 course\n dept comp\n sci .\nsec1 instance-of section .\nsec1 sec\n course CS-101 .\nsec1 sec\n id \"1\" .\nsec1 semester \"Fall\" .\nsec1 year \"2017\" .\nsec1 classroom packard-101 .\nsec1 time\n slot\nid \"H\" .\n10101 inst\n dept comp\n sci .\n00128 stud\n dept comp\n sci .\n00128 takes sec1 .\n10101 teaches sec1 .\nFigure 8.3 RDF representation of part of the University database.\nFigure 8.3 shows a triple representation of a small part of the University database.\nAll attribute values are shown in quotes, while identi\ufb01ers are shown without quotes.\nAttribute and relationship names (which form the predicate part of each triple) are\nalso shown without quotes.\nIn our example, we use the IDvalues to identify instructors and students and course\nidto identify courses. Each of their attributes is represented as a separate triple. The\ntype information of objects is provided by the instance-of relationship; for example,\n10101 is identi\ufb01ed as an instance of instructor, while 00128 is an instance of student. To\nfollow RDF syntax, the identi\ufb01er of the Comp. Sci. department is denoted as comp\n sci.\nOnly one attribute of the department, dept\n name ,i ss h o w n .S i n c et h ep r i m a r yk e yo f\nsection is composite, we have created new identi\ufb01ers to identify sections; \u201csec1\u201d iden-\nti\ufb01es one such section, shown with its semester ,year and sec\nidattributes, and with a\nrelationship course to CS-101.\nRelationships shown in the \ufb01gure include the takes and teaches relationships, which\nappear in the university schema. The departments of instructors, students and courses\nare shown as relationships inst\n dept,stud\n dept and course\n dept respectively, following\ntheE-Rmodel; similarly, the classroom associated with a section is also shown as a\nclassroom relationship with a classroom object (packard-101, in our example), and the\n", "402": "374 Chapter 8 Complex Data Types\ncourse associated with a section is shown as a relationship sec\ncourse between the sec-\ntion and the course.\nAs we saw, entity type information is represented using instance-of relationships\nbetween entities and objects representing types; type-subtype relationships can also be\nrepresented as subtype edges between type objects.\nIn contrast to the E-Rmodel and relational schemas, RDF allows new attributes to\nbe easily added to an object and also to create new types of relationships.\n8.1.4.2 Graph Representation of RDF\nThe RDF representation has a very natural graph interpretation. Entities and attribute\nvalues can be considered as nodes, and attribute names and relationships can be con-\nsidered as edges between the nodes. The attribute/relationship name can be viewed as\nthe label of the corresponding edge. Figure 8.4 shows a graph representation of the data\nfrom Figure 8.3. Objects are shown as ovals, attribute values in rectangles, and relation-\nships as edges with associated labels identifying the relationship. We have omitted the\ninstance-of relationships for brevity.\nA representation of information using the RDF graph model (or its variants and\nextensions) is referred to as a knowledge graph . Knowledge graphs are used for a variety\nof purposes. One such application is to store facts that are harvested from a variety of\ndata sources, such as Wikipedia, Wikidata, and other sources on the web. An example\nof a fact is \u201cWashington, D.C. is the capital of U.S.A.\u201d Such a fact can be represented\nas an edge labeled capital-of connecting two nodes, one representing the entity Wash-\nington, D.C., and the other representing the entity U.S.A.\n10101Srinivasan\nname salary\nteaches course_deptdept_name\ntakescomp_sci\nCS-101 sec1\npackard-1016500\nFall 2017100128  Zhang\nname tot_cred102 Comp. Sci.\nIntro. to Computer Sciencesec_course\ntitle classroom semester yearsec_idstud_dept inst_dept\nFigure 8.4 Graph representation of RDF data.\n", "403": "8.1 Semi-structured Data 375\nQuestions about entities can be answered using a knowledge graph that contains\nrelevant information. For example, the question \u201cWhich city is the capital of the\nU.S.A.?\u201d can be answered by looking for an edge labeled capital-of , linking an entity\nto the country U.S.A. (If type information is available, the query may also verify that\nthere is an instance-of edge connecting Washington, D.C., to a node representing the\nentity type City).\n8.1.4.3 SPARQL\nSPARQL is a query language designed to query RDF data. The language is based on\ntriple patterns, which look like RDF triples but may contain variables. For example,\nt h et r i p l ep a t t e r n :\n?cid title \"Intro. to Computer Science\"\nwould match all triples whose predicate is \u201ctitle\u201d and object is \u201cIntro. to Computer\nScience\u201d. Here, ?cid is a variable that can match any value.\nQueries can have multiple triple patterns, with variables shared across triples. Con-\nsider the following pair of triples:\n?cid title \"Intro. to Computer Science\"\n?sid course ?cid\nOn the university-triple dataset shown in Figure 8.3, the \ufb01rst triple pattern matches the\ntriple (CS-101, title, \"Intro. to Computer Science\") ,w h i l et h es e c o n dt r i p l ep a t t e r n\nmatches (sec1, course, CS-101) . The shared variable ?cid enforces a join condition\nbetween the two triple patterns.\nWe can now show a complete SPARQL query. The following query retrieves names\nof all students who have taken a section whose course is titled \u201cIntro. to Computer\nScience\u201d.\nselect ?name\nwhere {\n?cid title\"Intro. to Computer Science\" .\n?sid course ?cid .\n?idtakes ?sid .\n?idname ?name .\n}\nThe shared variables between these triples enforce a join condition between the tuples\nmatching each of these triples.\nNote that unlike in SQL, the predicate in a triple pattern can be a variable, which\ncan match any relationship or attribute name. SPARQL has many more features, such\n", "404": "376 Chapter 8 Complex Data Types\nas aggregation, optional joins (similar to outerjoins), and subqueries. For more infor-\nmation in SPARQL , see the references in Further Reading.\n8.1.4.4 Representing N-ary Relationships\nRelationships represented as edges can model only binary relationships. Knowledge\ngraphs have been extended to store more complex relationships. For example, knowl-\nedge graphs have been extended with temporal information to record the time period\nduring which a fact is true; if the capital of the U.S.A. changed from Washington, DC.,\nto say, New York, in 2050, this would be represented by two facts, one for the period\nending in 2050 when Washington was the capital, and one for the period after 2050.\nAs we saw in Section 6.9.4, an n-ary relationship can be represented using binary\nrelationships by creating an arti\ufb01cial entity corresponding to a tuple in an n-ary re-\nlationship and linking that arti\ufb01cial entity to each of the entities participating in the\nrelationship. In the preceding example, we can create an arti\ufb01cial entity e1to represent\nthe fact that Barack Obama was president of the U.S.A. from 2008 to 2016. We link\ne1to the entities representing Obama and U.S.A. by person and country relationship\nedges respectively, and to the values 2008 and 2016 by attribute edges president-from\nand president-till respectively. If we chose to represent years as entities, the edges cre-\nated to the two years above would represent relationships instead of attributes.\nThe above idea is similar to the E-Rmodel notion of aggregation which, as we saw\nin Section 6.8.5, can treat a relationship as an entity; this idea is called rei\ufb01cation in\nRDF. Rei\ufb01cation is used in many knowledge-graph representations, where the extra\ninformation such as time period of validity are treated as quali\ufb01ers of the underlying\nedge.\nOther models add a fourth attribute, called the context, to triples; thus, instead of\nstoring triples, they store quads . The basic relationship is still binary, but the fourth\nattribute allows a context entity to be associated with a relationship. Information such\nas valid time period can be treated as attributes of the context entity.\nThere are several knowledge bases, such as Wikidata, DBPedia, Freebase, and\nYago, that provide an RDF/knowledge graph representation of a wide variety of knowl-\nedge. In addition, there are a very large number of domain-speci\ufb01c knowledge graphs.\nThe linked open data project is aimed at making a variety of such knowledge graphs\nopen source and further creating links between these independently created knowledge\ngraphs. Such links allow queries to make inferences using information from multiple\nknowledge graphs along with links to the knowledge graphs. References to more infor-\nmation on this topic may be found in the bibliographic notes for this chapter, available\nonline.\n8.2 Object Orientation\nThe object-relational data model extends the relational data model by providing a richer\ntype system, including complex data types and object orientation. Relational query\n", "405": "8.2 Object Orientation 377\nlanguages, in particular SQL, have been extended correspondingly to deal with the\nricher type system. Such extensions attempt to preserve the relational foundations\u2014\nin particular, the declarative access to data\u2014while extending the modeling power.\nMany database applications are written using an object-oriented programming\nlanguage, such as Java, Python, or C++, but they need to store and fetch data from\ndatabases. Due to the type di\ufb00erence between the native type system of the object-\noriented programming language and the relational model supported by databases, data\nneed to be translated between the two models whenever they are fetched or stored.\nMerely extending the type system supported by the database was not enough to solve\nthis problem completely. Having to express database access using a language ( SQL)\nthat is di\ufb00erent from the programming language again makes the job of the program-\nmer harder. It is desirable, for many applications, to have programming language con-\nstructs or extensions that permit direct access to data in the database, without having\nto go through an intermediate language such as SQL.\nThree approaches are used in practice for integrating object orientation with\ndatabase systems:\n1.Build an object-relational database system , which adds object-oriented features to\na relational database system.\n2.Automatically convert data from the native object-oriented type system of the\nprogramming language to a relational representation for storage, and vice versa\nfor retrieval. Data conversion is speci\ufb01ed using an object-relational mapping .\n3.Build an object-oriented database system , that is, a database system that natively\nsupports an object-oriented type system and allows direct access to data from\nan object-oriented programming language using the native type system of the\nlanguage.\nWe provide a brief introduction to the \ufb01rst two approaches in this section. While the\nthird approach, the object-oriented database approach, has some bene\ufb01ts over the \ufb01rst\ntwo approaches in terms of language integration, it has not seen much success for\ntwo reasons. First, declarative querying is very important for e\ufb03ciently accessing data,\nand such querying is not supported by imperative programming languages. Second,\ndirect access to objects via pointers was found to result in increased risk of database\ncorruption due to pointer errors. We do not describe the object-oriented approach any\nfurther.\n8.2.1 Object-Relational Database Systems\nIn this section, we outline how object-oriented features can be added to relational\ndatabase systems.\n", "406": "378 Chapter 8 Complex Data Types\n8.2.1.1 User-De\ufb01ned Types\nObject extensions to SQL allow creation of structured user-de\ufb01ned types, references to\nsuch types, and tables containing tuples of such types.1\ncreate type Person\n(IDvarchar (20) primary key ,\nname varchar (20),\naddress varchar (20))\nref from (ID);\ncreate table people ofPerson ;\nWe can create a new person as follows:\ninsert into people (ID, name, address )values\n('12345', 'Srinivasan', '23 Coyote Run');\nMany database systems support array and table types; attributes of relations and\nof user-de\ufb01ned types can be declared to be of such array or table types. The support\nfor such features as well as the syntax varies widely by database system. In Postgre SQL,\nfor example, integer [] denotes an array of integers whose size is not prespeci\ufb01ed, while\nOracle supports the syntax varray (10) of integer to specify an array of 10 integers. SQL\nServer allows table-valued types to be declared as shown in the following example:\ncreate type interest as table (\ntopic varchar (20),\ndegree\n of\ninterest int\n);\ncreate table users (\nIDvarchar (20),\nname varchar (20),\ninterests interest\n);\nUser-de\ufb01ned types can also have methods associated with them. Only a few\ndatabase systems, such as Oracle, support this feature; we omit details.\n8.2.1.2 Type Inheritance\nConsider the earlier de\ufb01nition of the type Person and the table people .W em a yw a n tt o\nstore extra information in the database about people who are students and about people\n1Structured types are di\ufb00erent from the simpler \u201cdis tinct\u201d data types that we covered in Section 4.5.5.\n", "407": "8.2 Object Orientation 379\nwho are teachers. Since students and teachers are also people, we can use inheritance\nto de\ufb01ne the student and teacher types in SQL:\ncreate type Student under Person\n(degree varchar (20)) ;\ncreate type Teacher under Person\n(salary integer );\nBoth Student and Teacher inherit the attributes of Person \u2014namely , ID,name ,a n d ad-\ndress .Student and Teacher are said to be subtypes of Person ,a n d Person is a supertype\nofStudent ,a sw e l la so f Teacher .\nMethods of a structured type are inherited by its subtypes, just as attributes are.\nHowever, a subtype can rede\ufb01ne the e\ufb00ect of a method. We omit details.\n8.2.1.3 Table Inheritance\nTable inheritance allows a table to be declared as a subtable of another table and cor-\nresponds to the E-Rnotion of specialization/generalization. Several database systems\nsupport table inheritance, but in di\ufb00erent ways.\nInPostgre SQL, we could create a table people and then create tables students and\nteachers as subtables of people as follows:\ncreate table students\n(degree varchar (20))\ninherits people ;\ncreate table teachers\n(salary integer )\ninherits people ;\nAs a result, every attribute present in the table people is also present in the subtables\nstudents and teachers .\nSQL:1999 supports table inheritance but requires table types to be speci\ufb01ed \ufb01rst.\nThus, in Oracle, which supports SQL:1999 ,w ec o u l du s e :\ncreate table people ofPerson ;\ncreate table students ofStudent\nunder people ;\ncreate table teachers ofTeacher\nunder people ;\nwhere the types Student and Teacher have been declared to be subtypes of Person as\ndescribed earlier.\n", "408": "380 Chapter 8 Complex Data Types\nIn either case, we can insert a tuple into the student table as follows:\ninsert into student values ('00128', 'Zhang', '235 Coyote Run', 'Ph.D.');\nwhere we provide values for the attributes inherited from people as well as the local\nattributes of student .\nWhen we declare students and teachers as subtables of people ,e v e r yt u p l ep r e s e n t\ninstudents orteachers becomes implicitly present in people . Thus, if a query uses the\ntable people , it will \ufb01nd not only tuples directly inserted into that table but also tuples\ninserted into its subtables, namely, students and teachers . However, only those attributes\nthat are present in people can be accessed by that query. SQL permits us to \ufb01nd tuples\nthat are in people but not in its subtables by using \u201c only people \u201di np l a c eo f people in a\nquery.\n8.2.1.4 Reference Types in SQL\nSome SQL implementations such as Oracle support reference types. For example, we\ncould de\ufb01ne the Person type as follows, with a reference-type declaration:\ncreate type Person\n(IDvarchar (20) primary key ,\nname varchar (20),\naddress varchar (20))\nref from (ID);\ncreate table people ofPerson ;\nBy default, SQL assigns system-de\ufb01ned identi\ufb01ers for tuples, but an existing primary-\nkey value can be used to reference a tuple by including the ref from clause in the type\nde\ufb01nition as shown above.\nWe can de\ufb01ne a type Department with a \ufb01eld name and a \ufb01eld head that is a refer-\nence to the type Person . We can then create a table departments of type Department ,a s\nfollows:\ncreate type Department (\ndept\n name varchar(20) ,\nhead ref(Person )scope people );\ncreate table departments ofDepartment ;\nNote that the scope clause above completes the de\ufb01nition of the foreign key from de-\npartments.head to the people relation.\nWhen inserting a tuple for departments , we can then use:\ninsert into departments\nvalues ('CS', '12345');\n", "409": "8.2 Object Orientation 381\nsince the IDa t t r i b u t ei su s e da sar e f e r e n c et o Person . Alternatively, the de\ufb01nition of\nPerson can specify that the reference must be generated automatically by the system\nwhen a Person object is created. System-generated identi\ufb01ers can be retrieved using\nref(r)w h e r e ris a table name of table alias used in a query. Thus, we could create a\nPerson tuple, and, using the ID or name of the person, we could retrieve the reference\nto the tuple in a subquery, which is used to create the value for the head attribute when\ninserting a tuple into the departments table. Since most database systems do not allow\nsubqueries in an insert into departments values statement, the following two queries can\nbe used to carry out the task:\ninsert into departments\nvalues ('CS', null);\nupdate departments\nsethead =(select ref (p)\nfrom people asp\nwhere ID= '12345')\nwhere dept\n name =' C S ' ;\nReferences are dereferenced in SQL:1999 by the\u2212>symbol. Consider the depart-\nments table de\ufb01ned earlier. We can use this query to \ufb01nd the names and addresses of\nthe heads of all departments:\nselect head\u2212>name ,head\u2212>address\nfrom departments ;\nAn expression such as \u201c head\u2212>name \u201d is called a path expression .\nSince head is a reference to a tuple in the people table, the attribute name in the\npreceding query is the name attribute of the tuple from the people table. References\nc a nb eu s e dt oh i d ej o i no p e r a t i o n s ;i nt h ep r e c e d i n ge x a m p l e ,w i t h o u tt h er e f e r e n c e s ,\nthehead \ufb01eld of department would be declared a foreign key of the table people .T o\n\ufb01nd the name and address of the head of a department, we would require an explicit\njoin of the relations departments and people . The use of references simpli\ufb01es the query\nconsiderably.\nWe can use the operation deref to return the tuple pointed to by a reference and\nthen access its attributes, as shown below:\nselect deref (head ).name\nfrom departments ;\n8.2.2 Object-Relational Mapping\nObject-relational mapping (ORM ) systems allow a programmer to de\ufb01ne a mapping\nbetween tuples in database relations and objects in the programming language.\n", "410": "382 Chapter 8 Complex Data Types\nAn object, or a set of objects, can be retrieved based on a selection condition on\nits attributes; relevant data are retrieved from the underlying database based on the\nselection conditions, and one or more objects are created from the retrieved data, based\non the prespeci\ufb01ed mapping between objects and relations.\nThe program can update retrieved objects, create new objects, or specify that an\nobject is to be deleted, and then issue a save command; the mapping from objects to\nrelations is then used to correspondingly update, insert, or delete tuples in the database.\nThe primary goal of object-relational mapping systems is to ease the job of pro-\ngrammers who build applications by providing them an object model while retaining\nthe bene\ufb01ts of using a robust relational database underneath. As an added bene\ufb01t,\nwhen operating on objects cached in memory, object-relational systems can provide\nsigni\ufb01cant performance gains over direct access to the underlying database.\nObject-relational mapping systems also provide query languages that allow pro-\ngrammers to write queries directly on the object model; such queries are translated\ninto SQL queries on the underlying relational database, and result objects are created\nfrom the SQL query results.\nA fringe bene\ufb01t of using an ORM is that any of a number of databases can be used\nto store data, with exactly the same high-level code. ORM s hide minor SQL di\ufb00erences\nbetween databases from the higher levels. Migration from one database to another is\nthus relatively straightforward when using an ORM ,w h e r e a s SQL di\ufb00erences can make\nsuch migration signi\ufb01cantly harder if an application uses SQL to communicate with\nthe database.\nOn the negative side, object-relational mapping systems can su\ufb00er from signi\ufb01cant\nperformance ine\ufb03ciencies for bulk database updates, as well as for complex queries\nthat are written directly in the imperative language. It is possible to update the database\ndirectly, bypassing the object-relational mapping system, and to write complex queries\ndirectly in SQL in cases where such ine\ufb03ciencies are discovered.\nThe bene\ufb01ts of object-relational models exceed the drawbacks for many applica-\ntions, and object-relational mapping systems have seen widespread adoption in recent\nyears. In particular, Hibernate has seen wide adoption with Java, while several ORM s\nincluding Django and SQLA lchemy are widely used with Python. More information on\nthe Hibernate ORM system, which provides an object-relational mapping for Java, and\nthe Django ORM system, which provides an object-relational mapping for Python, can\nbe found in Section 9.6.2.\n8.3 Textual Data\nTextual data consists of unstructured text. The term information retrieval generally\nrefers to the querying of unstructured textual data. In the traditional model used in\nthe \ufb01eld of information retrieval, textual information is organized into documents .I na\ndatabase, a text-valued attribute can be considered a document. In the context of the\nweb, each web page can be considered to be a document.\n", "411": "8.3 Textual Data 383\n8.3.1 Keyword Queries\nInformation retrieval systems support the ability to retrieve documents with some de-\nsired information. The desired documen ts are typically described by a set of keywords\n\u2014for example, the keywords \u201cdatabase system\u201d may be used to locate documents on\ndatabase systems, and the keywords \u201cstock\u201d and \u201cscandal\u201d may be used to locate arti-\ncles about stock-market scandals. Documents have associated with them a set of key-\nwords; typically, all the words in the documents are considered keywords. A keyword\nquery retrieves documents whose set of keywords contains all the keywords in the query.\nIn its simplest form, an information-retrieval system locates and returns all docu-\nments that contain all the keywords in the query. More-sophisticated systems estimate\nthe relevance of documents to a query so that the documents can be shown in order\nof estimated relevance. They use information about keyword occurrences, as well as\nhyperlink information, to estimate relevance.\nKeyword search was originally targeted at document repositories within organi-\nzations or domain-speci\ufb01c document repositories such as research publications. But\ninformation retrieval is also important for documents stored in a database.\nKeyword-based information retrieval can be used not only for retrieving textual\ndata, but also for retrieving other types of data, such as video and audio data, that\nhave descriptive keywords associated with them. For instance, a video movie may have\nassociated with it keywords such as its title, director, actors, and genre, while an image\nor video clip may have tags, which are keywords describing the image or video clip,\nassociated with it.\nWeb search engines are, at core, information retrieval systems. They retrieve and\nstore web pages by crawling the web. Users submit keyword queries, and the information\nretrieval part of the web search engine \ufb01nds stored web pages containing the required\nkeyword. Web search engines have today evolved beyond just retrieving web pages.\nToday, search engines aim to satisfy a user\u2019s information needs by judging what topic\na query is about and displaying not only web pages judged as relevant but also other\nkinds of information about the topic. For example, given a query term \u201ccricket\u201d, a\nsearch engine may display scores from ongoing or recent cricket matches, rather than\njust top-ranked documents related to cricket. As another example, in response to a\nquery \u201cNew York\u201d, a search engine may show a map of New York and images of New\nYork in addition to web pages related to New York.\n8.3.2 Relevance Ranking\nThe set of all documents that contain the keywords in a query may be very large; in\nparticular, there are billions of documents on the web, and most keyword queries on\na web search engine \ufb01nd hundreds of thousands of documents containing some or\nall of the keywords. Not all the documents are equally relevant to a keyword query.\nInformation-retrieval systems therefore estimate relevance of documents to a query\nand return only highly ranked documents as answers. Relevance ranking is not an exact\nscience, but there are some well-accepted approaches.\n", "412": "384 Chapter 8 Complex Data Types\n8.3.2.1 Ranking Using TF-IDF\nThe word term refers to a keyword occurring in a document, or given as part of a query.\nThe \ufb01rst question to address is, given a particular term t, how relevant is a particular\ndocument dto the term. One approach is to use the number of occurrences of the term\nin the document as a measure of its relevance, on the assumption that more relevant\nterms are likely to be mentioned many times in a document. Just counting the number\nof occurrences of a term is usually not a good indicator: \ufb01rst, the number of occur-\nrences depends on the length of the document, and second, a document containing 10\noccurrences of a term may not be 10 times as relevant as a document containing one\noccurrence.\nOne way of measuring TF(d,t), the relevance of a term tto a document d,i s :\nTF(d,t)=log(\n1+n(d,t)\nn(d))\nwhere n(d) denotes the number of term occurrences in the document and n(d,t)d e -\nnotes the number of occurrences of term tin the document d. Observe that this metric\ntakes the length of the document into account. The relevance grows with more occur-\nrences of a term in the document, although it is not directly proportional to the number\nof occurrences.\nMany systems re\ufb01ne the above metric by using other information. For instance,\nif the term occurs in the title, or the author list, or the abstract, the document would\nbe considered more relevant to the term. Similarly, if the \ufb01rst occurrence of a term is\nlate in the document, the document may be considered less relevant than if the \ufb01rst\noccurrence is early in the document. The ab ove notions can be formalized by extensions\nof the formula we have shown for TF(d,t). In the information retrieval community, the\nrelevance of a document to a term is referred to as term frequency (TF), regardless of\nthe exact formula used.\nA query Qmay contain multiple keywords. The relevance of a document to a query\nwith two or more keywords is estimated by combining the relevance measures of the\ndocument for each keyword. A simple way of combining the measures is to add them up.\nHowever, not all terms used as keywords are equal. Suppose a query uses two terms, one\nof which occurs frequently, such as \u201cdatabase \u201d, and another that is less frequent, such\nas \u201cSilberschatz\u201d. A document containing \u201cSilberschatz\u201d but not \u201cdatabase\u201d should be\nranked higher than a document containing the term \u201cdatabase\u201d but not \u201cSilberschatz\u201d.\nTo \ufb01x this problem, weights are assigned to terms using the inverse document fre-\nquency (IDF), de\ufb01ned as:\nIDF(t)=1\nn(t)\nwhere n(t) denotes the number of documents (among those indexed by the system) that\ncontain the term t.T h e relevance of a document dto a set of terms Qis then de\ufb01ned\nas:\n", "413": "8.3 Textual Data 385\nr(d,Q)=\u2211\nt\u2208QTF(d,t)\u2217IDF(t)\nThis measure can be further re\ufb01ned if the user is permitted to specify weights w(t)f o r\nterms in the query, in which case the user-speci\ufb01ed weights are also taken into account\nby multiplying TF(t)b y w(t) in the preceding formula.\nThe above approach of using term frequency and inverse document frequency as\na measure of the relevance of a document is called the TF\u2013IDF approach.\nAlmost all text documents (in English) contain words such as \u201cand,\u201d \u201cor,\u201d \u201ca,\u201d and\nso on, and hence these words are useless for querying purposes since their inverse doc-\nument frequency is extremely low. Information-retrieval systems de\ufb01ne a set of words,\ncalled stop words , containing 100 or so of the most common words, and ignore these\nwords when indexing a document. Such words are not used as keywords, and they are\ndiscarded if present in the keywords supplied by the user.\nAnother factor taken into account when a query contains multiple terms is the\nproximity of the terms in the document. If the terms occur close to each other in the\ndocument, the document will be ranked high er than if they occur far apart. The formula\nforr(d,Q) can be modi\ufb01ed to take proximity of the terms into account.\nGiven a query Q, the job of an information-retrieval system is to return documents\nin descending order of their relevance to Q. Since there may be a very large number\nof documents that are relevant, information-retrieval systems typically return only the\n\ufb01rst few documents with the highest degree of estimated relevance and permit users to\ninteractively request further documents.\n8.3.2.2 Ranking Using Hyperlinks\nHyperlinks between documents can be used to decide on the overall importance of\na document, independent of the keyword query; for example, documents linked from\nmany other documents are considered more important.\nThe web search engine Google introduced PageRank , which is a measure of pop-\nularity of a page based on the popularity of pages that link to the page. Using the\nPageRank popularity measure to rank answers to a query gave results so much better\nthan previously used ranking techniques that Google became the most widely used\nsearch engine in a rather short period of time.\nNote that pages that are pointed to from many web pages are more likely to be\nvisited, and thus should have a higher PageRank. Similarly, pages pointed to by web\npages with a high PageRank will also have a higher probability of being visited, and\nthus should have a higher PageRank.\nThe PageRank of a document dis thus de\ufb01ned (circularly) based on the PageRank\nof other documents that link to document d. PageRank can be de\ufb01ned by a set of\nlinear equations, as follows: First, web pages are given integer identi\ufb01ers. The jump\nprobability matrix Tis de\ufb01ned with T[i,j] set to the probability that a random walker\nwho is following a link out of page ifollows the link to page j. Assuming that each link\n", "414": "386 Chapter 8 Complex Data Types\nfrom ihas an equal probability of being followed T[i,j]=1\u2215Ni,w h e r e Niis the number\nof links out of page i. Then the PageRank P[j]f o re a c hp a g e jcan be de\ufb01ned as:\nP[j]=\u03b4 \u2215 N+(1\u2212\u03b4)\u2217N\u2211\ni=1(T[i,j]\u2217P[i])\nwhere\u03b4is a constant between 0 and 1, usually set to 0.15, and Nis the number of pages.\nThe set of equations generated as above are usually solved by an iterative technique,\nstarting with each P[i]s e tt o1 \u2215N. Each step of the iteration computes new values\nfor each P[i]u s i n gt h e Pvalues from the previous iteration. Iteration stops when the\nmaximum change in any P[i] value in an iteration goes below some cuto\ufb00 value.\nNote that PageRank is a static measure, independent of the keyword query; given\na keyword query, it is used in combination with TF\u2013IDF scores of a document to judge\nits relevance of the document to the keyword query.\nPageRank is not the only measure of the popularity of a site. Information about how\noften a site is visited is another useful measure of popularity. Further, search engines\ntrack what fraction of times users click on a page when it is returned as an answer.\nKeywords that occur in the anchor text associated with the hyperlink to a page are\nviewed as very important and are given a higher term frequency. These and a number\nof other factors are used to rank answers to a keyword query.\n8.3.3 Measuring Retrieval Effectiveness\nRanking of results of a keyword query is not an exact science. Two metrics are used to\nmeasure how well an information-retrieval system is able to answer queries. The \ufb01rst,\nprecision , measures what percentage of the retrieved documents are actually relevant\nto the query. The second, recall , measures what percentage of the documents relevant\nto the query were retrieved. Since search engines \ufb01nd a very large number of answers,\nand users typically stop after browsing some number (say, 10 or 20) of the answers,\nthe precision and recall numbers are usually measured \u201c@K\u201d, where Kis the number\nof answers viewed. Thus, one can talk of precision@10 or recall@20.\n8.3.4 Keyword Querying on Structured Data and Knowledge Graphs\nAlthough querying on structured data are typically done using query languages such\nasSQL, users who are not familiar with the schema or the query language \ufb01nd it di\ufb03-\ncult to get information from such data. Based on the success of keyword querying in\nthe context of information retrieval from the web, techniques have been developed to\nsupport keyword queries on structured and semi-structured data.\nOne approach is to represent the data using graphs, and then perform keyword\nqueries on the graphs. For example, tuples can be treated as nodes in the graph, and\nforeign key and other connections between tuples can be treated as edges in the graph.\nKeyword search is then modeled as \ufb01nding tuples containing the given keywords and\n\ufb01nding connecting paths between them in the corresponding graph.\n", "415": "8.4 Spatial Data 387\nFor example, a query \u201cZhang Katz\u201d on a university database may \ufb01nd the name\n\u201cZhang\u201d occurring in a student tuple, and the name \u201cKatz\u201d in an instructor tuple, a path\nthrough the advisor relation connecting the two tuples. Other paths, such as student\n\u201cZhang\u201d taking a course taught by \u201cKatz\u201d may also be found in response to this query.\nSuch queries may be used for ad hoc browsing and querying of data when the user does\nnot know the exact schema and does not wish to take the e\ufb00ort to write an SQL query\nde\ufb01ning what she is searching for. Indeed it is unreasonable to expect lay users to write\nqueries in a structured query language, whereas keyword querying is quite natural.\nSince queries are not fully de\ufb01ned, they may have many di\ufb00erent types of answers,\nwhich must be ranked. A number of techniques have been proposed to rank answers in\nsuch a setting, based on the lengths of connecting paths and on techniques for assigning\ndirections and weights to edges. Techniques have also been proposed for assigning\npopularity ranks to tuples based on foreign key links. More information on keyword\nsearching of structured data may be found in the bibliographic notes for this chapter,\navailable online.\nFurther, knowledge graphs can be used along with textual information to answer\nqueries. For example, knowledge graphs can be used to provide unique identi\ufb01ers to\nentities, which are used to annotate mentions of the entities in textual documents. Now\na particular mention of a person in a document may have the phrase \u201cStonebraker de-\nveloped PostgreSQL\u201d; from the context, the word Stonebraker may be inferred to be the\ndatabase researcher \u201cMichael Stonebraker\u201d and annotated by linking the word Stone-\nbraker to the entity \u201cMichael Stonebraker\u201d. The knowledge graph may also record the\nfact that Stonebraker won the Turing award. A query asking for \u201cturing award post-\ngresql\u201d can now be answered by using information from the document and the knowl-\nedge graph.2\nWeb search engines today use large knowledge graphs, in addition to crawled doc-\numents, to answer user queries.\n8.4 Spatial Data\nSpatial data support in database systems is important for e\ufb03ciently storing, indexing,\nand querying of data on the basis of spatial locations.\nTwo types of spatial data are particularly important:\n\u2022Geographic data such as road maps, land-usage maps, topographic elevation maps,\npolitical maps showing boundaries, land-ownership maps, and so on. Geographic\ninformation systems are special-purpose database systems tailored for storing geo-\ngraphic data. Geographic data is based on a round-earth coordinate system, with\nlatitude, longitude, and elevation.\n2In this case the knowledge graph may already record that Stonebraker developed PostgreSQL, but there are many\nother pieces of information that may exist only in documents, and not in the knowledge graphs.\n", "416": "388 Chapter 8 Complex Data Types\n\u2022Geometric data , which include spatial information about how objects\u2014such as\nbuildings, cars, or aircraft\u2014are constructed. Geometric data is based on a two-\ndimensional or three-dimensional Euclidean space, with X,Y,a n d Zcoordinates.\nGeographic and geometric data types are supported by many database systems, such\nas Oracle Spatial and Graph, the PostGISextension of Postgre SQL,SQL S erver, and\ntheIBM DB2 Spatial Extender.\nIn this section we describe the modeling and querying of spatial data; implemen-\ntation techniques such as indexing and query processing techniques are covered in\nChapter 14 and in Chapter 15.\nThe syntax for representing geographic and geometric data varies by database, al-\nthough representations based on the Open Geospatial Consortium (OGC )s t a n d a r da r e\nnow increasingly supported. See the manuals of the database you use to learn more\nabout the speci\ufb01c syntax supported by the database.\n8.4.1 Representation of Geometric Information\nFigure 8.5 illustrates how various geometric constructs can be represented in a data-\nbase, in a normalized fashion. We stress here that geometric information can be repre-\nsented in several di\ufb00erent ways, only some of which we describe.\nAline segment can be represented by the coordinates of its endpoints. For example,\nin a map database, the two coordinates of a point would be its latitude and longitude.\nApolyline (also called a linestring ) consists of a connected sequence of line segments\nand can be represented by a list containing the coordinates of the endpoints of the seg-\nments, in sequence. We can approximately represent an arbitrary curve with polylines\nby partitioning the curve into a sequence of segments. This representation is useful for\ntwo-dimensional features such as roads; here, the width of the road is small enough\nrelative to the size of the full map that it can be considered to be a line. Some systems\nalso support circular arcs as primitives, allowing curves to be represented as sequences\nof arcs.\nWe can represent a polygon by listing its vertices in order, as in Figure 8.5.3The\nlist of vertices speci\ufb01es the boundary of a polygonal region. In an alternative represen-\ntation, a polygon can be divided into a set of triangles, as shown in Figure 8.5. This\nprocess is called triangulation , and any polygon can be triangulated. The complex poly-\ngon can be given an identi\ufb01er, and each of the triangles into which it is divided carries\nthe identi\ufb01er of the polygon. Circles and ellipses can be represented by corresponding\ntypes or approximated by polygons.\nList-based representations of polylines or polygons are often convenient for query\nprocessing. Such non-\ufb01rst-normal-form representations are used when supported by\nthe underlying database. So that we can use \ufb01xed-size tuples (in \ufb01rst normal form)\nfor representing polylines, we can give the polyline or curve an identi\ufb01er, and we can\n3Some references use the term closed polygon to refer to what we call polygons and refer to polylines as open polygons.\n", "417": "8.4 Spatial Data 389\n12\n1\n13\n3\n452\n2\n13\n452line segment\ntriangle\npolygon\npolygon{(x1,y1), (x2,y2)}\n{(x1,y1), (x2,y2), (x3,y3)}\n{(x1,y1), (x2,y2), (x3,y3), (x4,y4), (x5,y5)}\n{(x1,y1), (x2,y2), (x3,y3), ID1}\n{(x1,y1), (x3,y3), (x4,y4), ID1}\n{(x1,y1), (x4,y4), (x5,y5), ID1}\nobject representation\nFigure 8.5 Representation of geometric constructs.\nrepresent each segment as a separate tuple that also carries with it the identi\ufb01er of the\npolyline or curve. Similarly, the triangulated representation of polygons allows a \ufb01rst\nnormal form relational representation of polygons.\nThe representation of points and line segments in three-dimensional space is sim-\nilar to their representation in two-dimensional space, the only di\ufb00erence being that\npoints have an extra zcomponent. Similarly, the representation of planar \ufb01gures\u2014such\nas triangles, rectangles, and other polygons\u2014does not change much when we move to\nthree dimensions. Tetrahedrons and cuboids can be represented in the same way as\ntriangles and rectangles. We can represent arbitrary polyhedra by dividing them into\ntetrahedrons, just as we triangulate polygons. We can also represent them by listing\ntheir faces, each of which is itself a polygon, along with an indication of which side of\nthe face is inside the polyhedron.\n", "418": "390 Chapter 8 Complex Data Types\nFor example, SQL S erver and PostGISsupport the geometry and geography types,\neach of which has subtypes such as point, linestring, curve, polygon, as well as col-\nlections of these types called multipoint, multilinestring, multicurve and multipoly-\ngon. Textual representations of these types are de\ufb01ned by the OGC standards, and\ncan be converted to internal representations using conversion functions. For example,\nLINESTRING(1 1, 2 3, 4 4) de\ufb01nes a line that connects points (1, 1), (2, 3) and (4,\n4), while POLYGON((1 1, 2 3, 4 4, 1 1)) de\ufb01nes a triangle de\ufb01ned by these points.\nFunctions ST\nGeometryFromText () and ST\nGeographyFromText () convert the textual\nrepresentations to geometry and geography objects respectively. Operations on geom-\netry and geography types that return objects of the same type include the ST\nUnion ()\nand ST\nIntersection () functions which compute the union and intersection of geomet-\nric objects such as linestrings and polygons. The function names as well as syntax di\ufb00er\nby system; see the system manuals for details.\nIn the context of map data, the various line segments representing the roads are\nactually interconnected to form a graph. Such a spatial network orspatial graph has spa-\ntial locations for vertices of the graph, along with interconnection information between\nthe vertices, which form the edges of the graph. The edges have a variety of associated\ninformation, such as distance, number of lanes, average speed at di\ufb00erent times of the\nday, and so on.\n8.4.2 Design Databases\nComputer-aided-design ( CAD ) systems traditionally stored data in memory during edit-\ning or other processing and wrote the data back to a \ufb01le at the end of a session of editing.\nThe drawbacks of such a scheme include the cost (programming complexity, as well as\ntime cost) of transforming data from one form to another and the need to read in an\nentire \ufb01le even if only parts of it are required. For large designs, such as the design of\na large-scale integrated circuit or the design of an entire airplane, it may be impossible\nto hold the complete design in memory. Designers of object-oriented databases were\nmotivated in large part by the database requirements of CAD systems. Object-oriented\ndatabases represent components of the design as objects, and the connections between\nthe objects indicate how the design is structured.\nThe objects stored in a design database are generally geometric objects. Simple\ntwo-dimensional geometric objects include points, lines, triangles, rectangles, and, in\ngeneral, polygons. Complex two-dimensional objects can be formed from simple ob-\njects by means of union, intersection, and di\ufb00erence operations. Similarly, complex\nthree-dimensional objects may be formed from simpler objects such as spheres, cylin-\nders, and cuboids by union, intersection, and di\ufb00erence operations, as in Figure 8.6.\nThree-dimensional surfaces may also be represented by wireframe models , which essen-\ntially model the surface as a set of simpler objects, such as line segments, triangles, and\nrectangles.\nDesign databases also store nonspatial information about objects, such as the ma-\nterial from which the objects are constructed. We can usually model such information\n", "419": "8.4 Spatial Data 391\n(a) Di\ufb00erence of cylinders (b) Union of cylinders\nFigure 8.6 Complex three-dimensional objects.\nby standard data-modeling techniques. We concern ourselves here with only the spatial\naspects.\nVarious spatial operations must be performed on a design. For instance, the de-\nsigner may want to retrieve that part of the design that corresponds to a particular\nregion of interest. Spatial-index structures , discussed in Section 14.10.1, are useful for\nsuch tasks. Spatial-index structures are multidimensional, dealing with two- and three-\ndimensional data, rather than dealing with just the simple one-dimensional ordering\nprovided by the B+-trees.\nSpatial-integrity constraints, such as \u201ctwo pipes should not be in the same loca-\ntion,\u201d are important in design databases to prevent interference errors. Such errors\noften occur if the design is performed manually and are detected only when a proto-\ntype is being constructed. As a result, these errors can be expensive to \ufb01x. Database\nsupport for spatial-integrity constraints helps people to avoid design errors, thereby\nkeeping the design consistent. Implementing such integrity checks again depends on\nthe availability of e\ufb03cient multidimensional index structures.\n8.4.3 Geographic Data\nGeographic data are spatial in nature but di\ufb00er from design data in certain ways. Maps\nand satellite images are typical examples of geographic data. Maps may provide not\nonly location information\u2014about boundaries, rivers, and roads, for example\u2014but also\nmuch more detailed information associated wi th locations, such as elevation, soil type,\nland usage, and annual rainfall.\n8.4.3.1 Applications of Geographic Data\nGeographic databases have a variety of uses, including online map and navigation ser-\nvices, which are ubiquitous today. Other applications include distribution-network in-\nformation for public-service utilities such as telephone, electric-power, and water-supply\n", "420": "392 Chapter 8 Complex Data Types\nsystems, and land-usage information for ecologists and planners, land records to track\nland ownership, and many more.\nGeographic databases for public-utility information have become very important as\nthe network of buried cables and pipes has grown. Without detailed maps, work carried\nout by one utility may damage the structure of another utility, resulting in large-scale\ndisruption of service. Geographic databases, coupled with accurate location-\ufb01nding\nsystems using GPS help avoid such problems.\n8.4.3.2 Representation of Geographic Data\nGeographic data can be categorized into two types:\n\u2022Raster data . Such data consist of bitmaps or pixel maps, in two or more dimen-\nsions. A typical example of a two-dimensional raster image is a satellite image of\nan area. In addition to the actual image, the data include the location of the image,\nspeci\ufb01ed, for example, by the latitude and longitude of its corners, and the reso-\nlution, speci\ufb01ed either by the total number of pixels, or, more commonly in the\ncontext of geographic data, by the area covered by each pixel.\nRaster data are often represented as tiles, each covering a \ufb01xed-size area. A\nlarger area can be displayed by displaying all the tiles that overlap with the area.\nTo allow the display of data at di\ufb00erent zoom levels, a separate set of tiles is created\nfor each zoom level. Once the zoom level is set by the user interface (e.g., a web\nbrowser), tiles at the speci\ufb01ed zoom level that overlap the area being displayed are\nretrieved and displayed.\nRaster data can be three-dimensional\u2014for example, the temperature at di\ufb00er-\nent altitudes at di\ufb00erent regions, again measured with the help of a satellite. Time\ncould form another dimension\u2014for example, the surface temperature measure-\nments at di\ufb00erent points in time.\n\u2022Vector data . Vector data are constructed from basic geometric objects, such as\npoints, line segments, polylines, triangles, and other polygons in two dimensions,\nand cylinders, spheres, cuboids, and other polyhedrons in three dimensions. In\nthe context of geographic data, points are usually represented by latitude and lon-\ngitude, and where the height is relevant, additionally by elevation.\nMap data are often represented in vector format. Roads are often represented as\npolylines. Geographic features, such as large lakes, or even political features such\nas states and countries, are represented as complex polygons. Some features, such\nas rivers, may be represented either as complex curves or as complex polygons,\ndepending on whether their width is relevant.\nGeographic information related to regions, such as annual rainfall, can be repre-\nsented as an array\u2014that is, in raster form. For space e\ufb03ciency, the array can be stored\nin a compressed form. In Section 24.4.1, we study an alternative representation of such\narrays by a data structure called a quadtree .\n", "421": "8.4 Spatial Data 393\nAs another alternative, we can represent region information in vector form, using\npolygons, where each polygon is a region within which the array value is the same. The\nvector representation is more compact than the raster representation in some applica-\ntions. It is also more accurate for some tasks, such as depicting roads, where dividing\nthe region into pixels (which may be fairly large) leads to a loss of precision in location\ninformation. However, the vector representation is unsuitable for applications where\nthe data are intrinsically raster based, such as satellite images.\nTopographical information , that is information about the elevation (height) of each\npoint on a surface, can be represented in raster form. Alternatively, it can be represented\nin vector form by dividing the surface into polygons covering regions of (approximately)\nequal elevation, with a single elevation value associated with each polygon. As another\nalternative, the surface can be triangulated (i.e., divided into triangles), with each tri-\nangle represented by the latitude, longitude, and elevation of each of its corners. The\nlatter representation, called the triangulated irregular network (TIN) representation, is\na compact representation which is particularly useful for generating three-dimensional\nviews of an area.\nGeographic information systems usually contain both raster and vector data, and\nthey can merge the two kinds of data when displaying results to users. For example,\nmap applications usually contain both satellite images and vector data about roads,\nbuildings, and other landmarks. A map display usually overlays di\ufb00erent kinds of infor-\nmation; for example, road information can be overlaid on a background satellite image\nto create a hybrid display. In fact, a map typi cally consists of multiple layers, which are\ndisplayed in bottom-to-top order; data from higher layers appear on top of data from\nlower layers.\nIt is also interesting to note that even information that is actually stored in vector\nform may be converted to raster form before it is sent to a user interface such as a web\nbrowser. One reason is that even web browsers in which JavaScript has been disabled\ncan then display map data; a second reason may be to prevent end users from extracting\nand using the vector data.\nMap services such as Google Maps and Bing Maps provide APIs that allow users to\ncreate specialized map displays, containing application-speci\ufb01c data overlaid on top of\nstandard map data. For example, a web site may show a map of an area with information\nabout restaurants overlaid on the map. The overlays can be constructed dynamically,\ndisplaying only restaurants with a speci\ufb01c c uisine, for example, or allowing users to\nchange the zoom level or pan the display.\n8.4.4 Spatial Queries\nThere are a number of types of queries that involve spatial locations.\n\u2022Region queries deal with spatial regions. Such a query can ask for objects that lie\npartially or fully inside a speci\ufb01ed region. A query to \ufb01nd all retail shops within the\ngeographic boundaries of a given town is an example. PostGISsupports predicates\nbetween two geometry or geography objects such as ST\nContains (),ST\nOverlaps (),\n", "422": "394 Chapter 8 Complex Data Types\nST\nDisjoint () and ST\nTouches (). These can be used to \ufb01nd objects that are con-\ntained in, or intersect, or are disjoint from a region. SQL S erver supports equivalent\nfunctions with slightly di\ufb00erent names.\nSuppose we have a shop relation, with an attribute location of type point ,a n da\ngeography object of type polygon . Then the ST\nContains () function can be used to\nretrieve all shops whose location is contained in the given polygon.\n\u2022Nearness queries request objects that lie near a speci\ufb01ed location. A query to \ufb01nd\nall restaurants that lie within a given distance of a given point is an example of a\nnearness query. The nearest-neighbor query requests the object that is nearest to\na speci\ufb01ed point. For example, we may want to \ufb01nd the nearest gasoline station.\nNote that this query does not have to specify a limit on the distance, and hence\nwe can ask it even if we have no idea how far the nearest gasoline station lies.\nThe PostGIS ST\nDistance () function gives the minimum distance between two such\nobjects, and can be used to \ufb01nd objects that are within a speci\ufb01ed distance from a\npoint or region. Nearest neighbors can be found by \ufb01nding objects with minimum\ndistance.\n\u2022Spatial graph queries request information based on spatial graphs such as road\nmaps. For example, a query may ask for the shortest path between two locations\nvia the road network, or via a train network, each of which can be represented as\na spatial graph. Such queries are ubiquitous for navigation systems.\nQueries that compute intersections of regions can be thought of as computing the\nspatial join of two spatial relations\u2014for example, one representing rainfall and the other\nrepresenting population density\u2014with the location playing the role of join attribute. In\ngeneral, given two relations, each containing spatial objects, the spatial join of the two\nrelations generates either pairs of objects that intersect or the intersection regions of\nsuch pairs. Spatial predicates such as ST\nContains () or ST\nOverlaps () can be used as\njoin predicates when performing spatial joins.\nIn general, queries on spatial data may have a combination of spatial and nonspa-\ntial requirements. For instance, we may want to \ufb01nd the nearest restaurant that has\nvegetarian selections and that charges less than $10 for a meal.\n8.5 Summary\n\u2022There are many application domains that need to store more complex data than\nsimple tables with a \ufb01xed number of attributes.\n\u2022The SQL standard includes extensions of the SQL data-de\ufb01nition and query lan-\nguage to deal with new data types and with object orientation. These include sup-\nport for collection-valued attributes, inheritance, and tuple references. Such exten-\nsions attempt to preserve the relational foundations\u2014in particular, the declarative\naccess to data\u2014while extending the modeling power.\n", "423": "8.5 Summary 395\n\u2022Semi-structured data are characterized by complex data, whose schema changes\noften.\n\u2022A popular architecture for building information systems today is to create a web\nservice that allows retrieval of data and to build application code that displays the\ndata and allows user interaction.\n\u2022The relational data model has been extended in several ways to support the storage\nand data exchange needs of modern applications.\n\u00b0Some database systems allow each tuple to potentially have a di\ufb00erent set of\nattributes.\n\u00b0Many data representations allow attributes to non-atomic values.\n\u00b0Many data representations allow attributes to be structured, directly modeling\ncomposite attributes in the E-Rmodel.\n\u2022The JavaScript Object Notation (JSON )i sat e x t u a lr e p r e s e n t a t i o no fc o m p l e xd a t a\ntypes which is widely used for transmitting data between applications and for stor-\ning complex data.\n\u2022XML representations provide \ufb02exibility in the set of attributes that a record con-\nt a i n sa sw e l la st h et y p e so ft h e s ea t t r i b u t e s .\n\u2022The Resource Description Framework (RDF) is a data representation standard\nbased on the entity-relationship model. The RDF representation has a very nat-\nural graph interpretation. Entities and attribute values can be considered nodes,\nand attribute names and relationships can be considered edges between the nodes.\n\u2022SPARQL is a query language designed to query RDF data and is based on triple\npatterns.\n\u2022Object orientation provides inheritance with subtypes and subtables as well as\nobject (tuple) references.\n\u2022The object-relational data model extends the relational data model by providing a\nricher type system, including collection types and object orientation.\n\u2022Object-relational database systems (i.e., database systems based on the object-\nrelational model) provide a convenient migration path for users of relational\ndatabases who wish to use object-oriented features.\n\u2022Object-relational mapping systems provide an object view of data that are stored\nin a relational database. Objects are transient, and there is no notion of persistent\nobject identity. Objects are created on demand from relational data, and updates to\nobjects are implemented by updating the relational data. Object-relational mapping\nsystems have been widely adopted, unlike the more limited adoption of persistent\nprogramming languages.\n", "424": "396 Chapter 8 Complex Data Types\n\u2022Information-retrieval systems are used to store and query textual data such as doc-\numents. They use a simpler data model than do database systems but provide more\npowerful querying capabilities within the restricted model.\n\u2022Queries attempt to locate documents that are of interest by specifying, for example,\nsets of keywords. The query that a user has in mind usually cannot be stated pre-\ncisely; hence, information-retrieval systems order answers on the basis of potential\nrelevance.\n\u2022Relevance ranking makes use of several types of information, such as:\n\u00b0Term frequency: how important each term is to each document.\n\u00b0Inverse document frequency.\n\u00b0Popularity ranking.\n\u2022Spatial data management is important for many applications. Geometric and geo-\ngraphic data types are supported by many database systems, with subtypes includ-\ning points, linestrings and polygons. Region queries, nearest neighbor queries, and\nspatial graph queries are among the commonly used types of spatial queries.\nReview Terms\n\u2022Wide column\n\u2022Sparse column\n\u2022Key-value map\n\u2022Map\n\u2022Array database\n\u2022Tags\n\u2022Triples\n\u2022Resources\n\u2022Subject\n\u2022Predicate\n\u2022Object\n\u2022Knowledge graph\n\u2022Rei\ufb01cation\n\u2022Quads\n\u2022Linked open data\n\u2022Object-relational data model\n\u2022Object-relational database system\n\u2022Object-relational mapping\u2022Object-oriented database system\n\u2022Path expression\n\u2022Keywords\n\u2022Keyword query\n\u2022Term\n\u2022Relevance\n\u2022TF\u2013IDF\n\u2022Stop words\n\u2022Proximity\n\u2022PageRank\n\u2022Precision\n\u2022Recall\n\u2022Geographic data\n\u2022Geometric data\n\u2022Geographic information system\n\u2022Computer-aided-design ( CAD )\n\u2022Polyline\n\u2022Linestring\n", "425": "Practice Exercises 397\n\u2022Triangulation\n\u2022Spatial network\n\u2022Spatial graph\n\u2022Raster data\n\u2022Tiles\n\u2022Vector data\n\u2022Topographical information\u2022Triangulated irregular network ( TIN)\n\u2022Overlays\n\u2022Nearness queries\n\u2022Nearest-neighbor query\n\u2022Region queries\n\u2022Spatial graph queries\n\u2022Spatial join\nPractice Exercises\n8.1 Provide information about the student named Shankar in our sample univer-\nsity database, including information from the student tuple corresponding to\nShankar, the takes tuples corresponding to Shankar and the course tuples cor-\nresponding to these takes tuples, in each of the following representations:\na. Using JSON ,w i t ha na p p r o p r i a t en e s t e dr e p r e s e n t a t i o n .\nb. Using XML , with the same nested representation.\nc. Using RDF triples.\nd. As an RDF graph.\n8.2 Consider the RDF representation of information from the university schema as\nshown in Figure 8.3. Write the following queries in SPARQL .\na. Find the titles of all courses taken by any student named Zhang.\nb. Find titles of all courses such that a student named Zhang takes a section\nof the course that is taught by an instructor named Srinivasan.\nc. Find the attribute names and values of all attributes of the instruc-\ntor named Srinivasan, without enumerating the attribute names in your\nquery.\n8.3 A car-rental company maintains a database for all vehicles in its current \ufb02eet.\nFor all vehicles, it includes the vehicle identi\ufb01cation number, license number,\nmanufacturer, model, date of purchase, and color. Special data are included for\ncertain types of vehicles:\n\u2022Trucks: cargo capacity.\n\u2022Sports cars: horsepower, renter age requirement.\n\u2022Vans: number of passengers.\n\u2022O\ufb00-road vehicles: ground clearance, drivetrain (four- or two-wheel drive).\n", "426": "398 Chapter 8 Complex Data Types\ninstructo r\nID\nname\n\ufb01rst_name\nmiddle_inital\nlast_name\naddress\nstreet\nstreet_number\nstreet_name\napt_number\ncity\nstate\nzip\n{phone_number }\ndate_of_birth\nage ( )\nFigure 8.7 E-Rdiagram with composite, multivalued, and derived attributes.\nConstruct an SQL schema de\ufb01nition for this database. Use inheritance where\nappropriate.\n8.4 Consider a database schema with a relation Emp w h o s ea t t r i b u t e sa r ea ss h o w n\nbelow, with types speci\ufb01ed for multivalued attributes.\nEmp = (ename, ChildrenSet multiset (Children), SkillSet multiset (Skills))\nChildren = (name, birthday)\nSkills = (type, ExamSet setof (Exams))\nExams = (year, city)\nDe\ufb01ne the above schema in SQL,u s i n gt h e SQL S erver table type syntax from\nSection 8.2.1.1 to declare multiset attributes.\n8.5 Consider the E-R diagram in Figure 8.7 showing entity set instructor .\nGive an SQL schema de\ufb01nition corresponding to the E-Rdiagram, treating\nphone\n number as an array of 10 elements, using Oracle or Postgre SQL syntax.\n8.6 Consider the relational schema shown in Figure 8.8.\na. Give a schema de\ufb01nition in SQL corresponding to the relational schema\nbut using references to express foreign-key relationships.\nb. Write each of the following queries on the schema, using SQL.\ni. Find the company with the most employees.\n", "427": "Exercises 399\nemployee (person\n name\n ,street ,city)\nworks (person\n name\n ,company\n name ,salary )\ncompany (company\n name\n ,city)\nmanages (person\n name\n ,manager\n name )\nFigure 8.8 Relational database for Exercise 8.6.\nii. Find the company with the smallest payroll.\niii. Find those companies whose employees earn a higher salary, on aver-\nage, than the average salary at First Bank Corporation.\n8.7 Compute the relevance (using appropriate de\ufb01nitions of term frequency and\ninverse document frequency) of each of the Practice Exercises in this chapter\nto the query \u201c SQL relation\u201d.\n8.8 Show how to represent the matrices used for computing PageRank as relations.\nThen write an SQL query that implements one iterative step of the iterative\ntechnique for \ufb01nding PageRank; the entire algorithm can then be implemented\nas a loop containing the query.\n8.9 Suppose the student relation has an attribute named location of type point, and\ntheclassroom relation has an attribute location of type polygon. Write the fol-\nlowing queries in SQL using the PostGISspatial functions and predicates that\nwe saw earlier:\na. Find the names of all students whose location is within the classroom\nPackard 101.\nb. Find all classrooms that are within 100 meters or Packard 101; assume all\ndistances are represented in units of meters.\nc. Find the ID and name of student who is geographically nearest to the\nstudent with ID 12345.\nd. Find the ID and names of all pairs of students whose locations are less\nthan 200 meters apart.\nExercises\n8.10 Redesign the database of Exercise 8.4 into \ufb01rst normal form and fourth normal\nform. List any functional or multivalued dependencies that you assume. Also\nlist all referential-integrity constraints that should be present in the \ufb01rst and\nfourth normal form schemas.\n", "428": "400 Chapter 8 Complex Data Types\nID\nname\naddress\nrank hours_ per_weeksalary tot_creditsperson\nstudent\ninstructor secretaryemployee\nFigure 8.9 Specialization and generalization.\n8.11 Consider the schemas for the table people ,a n dt h et a b l e s students and teachers ,\nwhich were created under people , in Section 8.2.1.3. Give a relational schema in\nthird normal form that represents the same information. Recall the constraints\non subtables, and give all constraints that must be imposed on the relational\nschema so that every database instance of the relational schema can also be\nrepresented by an instance of the schema with inheritance.\n8.12 Consider the E-Rdiagram in Figure 8.9, which contains specializations, using\nsubtypes and subtables.\na. Give an SQL schema de\ufb01nition of the E-Rdiagram.\nb. Give an SQL query to \ufb01nd the names of all people who are not secretaries.\nc. Give an SQL query to print the names of people who are neither employ-\nees nor students.\nd. Can you create a person who is an employee and a student with the\nschema you created? Explain how, or explain why it is not possible.\n8.13 Suppose you wish to perform keyword querying on a set of tuples in a database,\nwhere each tuple has only a few attributes, each containing only a few words.\nDoes the concept of term frequency make sense in this context? And that of\ninverse document frequency? Explain your answer. Also suggest how you can\nde\ufb01ne the similarity of two tuples using TF\u2013IDF concepts.\n8.14 Web sites that want to get some publicity can join a web ring, where they create\nlinks to other sites in the ring in exchange for other sites in the ring creating links\n", "429": "Further Reading 401\nto their site. What is the e\ufb00ect of such rings on popularity ranking techniques\nsuch as PageRank?\n8.15 The Google search engine provides a feature whereby web sites can display ad-\nvertisements supplied by Google. The advertisements supplied are based on the\ncontents of the page. Suggest how Google might choose which advertisements\nto supply for a page, given the page contents.\nFurther Reading\nAt u t o r i a lo n JSON can be found at www.w3schools.com/js/js\n json\n intro.asp .M o r e\ninformation about XML can be found in Chapter 30, available online. More informa-\ntion about RDF can be found at www.w3.org/RDF/ .A p a c h eJ e n ap r o v i d e sa n RDF\nimplementation, with support for SPARQL ;at u t o r i a lo n SPARQL can be found at\njena.apache.org/tutorials/sparql.html\nPOSTGRES ([Stonebraker and Rowe (1986)] and [Stonebraker (1986)]) was an\nearly implementation of an object-relational system. Oracle provides a fairly complete\nimplementation of the object-relational features of SQL, while Postgre SQL provides a\nsmaller subset of those features. More inf ormation on support for these features may\nbe found in their respective manuals.\n[Salton (1989)] is an early textbook on information-retrieval systems, while [Man-\nning et al. (2008)] is a modern textbook on the subject. Information about spatial\ndatabase support in Oracle, Postgre SQL andSQL S erver may be found in their respec-\ntive manuals online.\nBibliography\n[Manning et al. (2008)] C. D. Manning, P. Raghavan, and H. Sch\u00a8 utze, Introduction to Infor-\nmation Retrieval , Cambridge University Press (2008).\n[Salton (1989)] G. Salton, Automatic Text Processing , Addison Wesley (1989).\n[Stonebraker (1986)] M. Stonebraker, \u201cInclusion of New Types in Relational Database Sys-\ntems\u201d, In Proc. of the International Conf. on Data Engineering (1986), pages 262\u2013269.\n[Stonebraker and Rowe (1986)] M. Stonebraker and L. Rowe, \u201cThe Design of POSTGRES\u201d,\nInProc. of the ACM SIGMOD Conf. on Management of Data (1986), pages 340\u2013355.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock\n", "430": "", "431": "CHAPTER9\nApplication Development\nPractically all use of databases occurs from within application programs. Correspond-\ningly, almost all user interaction with databases is indirect, via application programs.\nIn this chapter, we study tools and technologies that are used to build applications,\nfocusing on interactive applications that use databases to store and retrieve data.\nA key requirement for any user-centric application is a good user interface. The\ntwo most common types of user interfaces today for database-backed applications are\nthe web and mobile app interfaces.\nIn the initial part of this chapter, we provide an introduction to application pro-\ngrams and user interfaces (Section 9.1), and to web technologies (Section 9.2). We then\ndiscuss development of web applications using the widely used Java Servlets technology\nat the back end (Section 9.3), and using other frameworks (Section 9.4). Client-side\ncode implemented using JavaScript or mobile app technologies is crucial for building\nresponsive user interfaces, and we discuss s ome of these technologies (Section 9.5). We\nthen provide an overview of web application architectures (Section 9.6) and cover per-\nformance issues in building large web applications (Section 9.7). Finally, we discuss\nissues in application security that are key to making applications resilient to attacks\n(Section 9.8), and encryption and its use in applications (Section 9.9).\n9.1 Application Programs and User Interfaces\nAlthough many people interact with databases, very few people use a query language\nto interact with a database system directly. The most common way in which users\ninteract with databases is through an application program that provides a user interface\nat the front end and interfaces with a database at the back end. Such applications take\ninput from users, typically through a forms-based interface, and either enter data into\na database or extract information from a database based on the user input, and they\nthen generate output, which is displayed to the user.\nAs an example of an application, consider a university registration system. Like\nother such applications, the registration system \ufb01rst requires you to identify and authen-\nticate yourself, typically by a user name and password. The application then uses your\n403\n", "432": "404 Chapter 9 Application Development\nidentity to extract information, such as your name and the courses for which you have\nregistered, from the database and displays the information. The application provides a\nnumber of interfaces that let you register for courses and query other information, such\nas course and instructor information. Organizations use such applications to automate\na variety of tasks, such as sales, purchases, accounting and payroll, human-resources\nmanagement, and inventory management, among many others.\nApplication programs may be used even when it is not apparent that they are being\nused. For example, a news site may provide a page that is transparently customized to\nindividual users, even if the user does not explicitly \ufb01ll any forms when interacting with\nthe site. To do so, it actually runs an application program that generates a customized\npage for each user; customization can, for example, be based on the history of articles\nbrowsed by the user.\nA typical application program includes a front-end component, which deals with\nthe user interface, a backend component, which communicates with a database, and\na middle layer, which contains \u201cbusiness logic,\u201d that is, code that executes speci\ufb01c\nrequests for information or updates, enforcing rules of business such as what actions\nshould be carried out to execute a given task or who can carry out what task.\nApplications such as airline reservations have been around since the 1960s. In\nthe early days of computer applications, applications ran on large \u201cmainframe\u201d com-\nputers, and users interacted with the application through terminals, some of which\neven supported forms. The growth of personal computers resulted in the development\nof database applications with graphical user interfaces, or GUIs. These interfaces de-\npended on code running on a personal computer that directly communicated with\na shared database. Such an architecture was called a client\u2013server architecture .T h e r e\nwere two drawbacks to using such applications: \ufb01rst, user machines had direct access\nto databases, leading to security risks. Second, any change to the application or the\ndatabase required all the copies of the application, located on individual computers, to\nbe updated together.\nTwo approaches have evolved to avoid the above problems:\n\u2022Web browsers provide a universal front end , used by all kinds of information\nservices. Browsers use a standardized syntax, the HyperText Markup Language\n(HTML )standard, which supports both formatted display of information and cre-\nation of forms-based interfaces. The HTML standard is independent of the operat-\ning system or browser, and pretty much every computer today has a web browser\ninstalled. Thus a web-based application can be accessed from any computer that\nis connected to the internet.\nUnlike client\u2013server architectures, there is no need to install any application-\nspeci\ufb01c software on client machines in order to use web-based applications.\nHowever, sophisticated user interfaces, supporting features well beyond what\nis possible using plain HTML , are now widely used, and are built with the scripting\nlanguage JavaScript, which is supported by most web browsers. JavaScript pro-\ngrams, unlike programs written in C, can be run in a safe mode, guaranteeing\n", "433": "9.2 Web Fundamentals 405\nthey cannot cause security problems. JavaScript programs are downloaded trans-\nparently to the browser and do not need any explicit software installation on the\nuser\u2019s computer.\nWhile the web browser provides the front end for user interaction, application\nprograms constitute the back end. Typically, requests from a browser are sent to a\nweb server, which in turn executes an application program to process the request.\nA variety of technologies are available for creating application programs that run\nat the back end, including Java servlets, Java Server Pages ( JSP), Active Server\nPage ( ASP), or scripting languages such as PHP and Python.\n\u2022Application programs are installed on individual devices, which are primarily mo-\nbile devices. They communicate with backend applications through an APIand do\nnot have direct access to the database. The back end application provides services,\nincluding user authentication, and ensures that users can only access services that\nthey are authorized to access.\nThis approach is widely used in mobile applications. One of the motivations\nfor building such applications was to customize the display for the small screen of\nmobile devices. A second was to allow application code, which can be relatively\nlarge, to be downloaded or updated when the device is connected to a high-speed\nnetwork, instead of downloading such code when a web page is accessed, perhaps\nover a lower bandwidth or more expensive mobile network.\nWith the increasing use of JavaScript code as part of web front ends, the di\ufb00erence\nbetween the two approaches above has today signi\ufb01cantly decreased. The back end\noften provides an APIthat can be invoked from either mobile app or JavaScript code\nto carry out any required task at the back end. In fact, the same back end is often used\nto build multiple front ends, which could include web front ends with JavaScript, and\nmultiple mobile platforms (primarily Android and iOS, today).\n9.2 Web Fundamentals\nIn this section, we review some of the fundamental technology behind the World Wide\nWeb, for readers who are not familiar with the technology underlying the web.\n9.2.1 Uniform Resource Locators\nAuniform resource locator (URL) is a globally unique name for each document that\ncan be accessed on the web. An example of a URL is:\nhttp://www.acm.org/sigmod\nThe \ufb01rst part of the URL indicates how the document is to be accessed: \u201chttp\u201d indi-\ncates that the document is to be accessed by the HyperText Transfer Protocol (HTTP ),\n", "434": "406 Chapter 9 Application Development\n<html>\n<body>\n<table border >\n<tr><th>ID</th>< th>Name </th>< th>Department </th></tr>\n<tr><td>00128 </td><td>Zhang </td>< td>Comp. Sci. </td></tr>\n<tr><td>12345 </td><td>Shankar </td><td>Comp. Sci. </td></tr>\n<tr><td>19991</td><td>Brandt </td>< td>History </td></tr>\n</table >\n</body >\n</html >\nFigure 9.1 Tabular data in HTML format.\nwhich is a protocol for transferring HTML documents; \u201c https \u201d would indicate that the\nsecure version of the HTTP protocol must be used, and is the preferred mode today.\nThe second part gives the name of a machine that has a web server. The rest of the URL\nis the path name of the \ufb01le on the machine, or other unique identi\ufb01er of a document\nwithin the machine.\nAURL can contain the identi\ufb01er of a program located on the web server machine,\nas well as arguments to be given to the program. An example of such a URL is\nhttps://www.google.com/search?q=silberschatz\nwhich says that the program search on the server www.google.com should be executed\nwith the argument q=silberschatz . On receiving a request for such a URL,t h ew e b\nserver executes the program, using the given arguments. The program returns an HTML\ndocument to the web server, which sends it back to the front end.\n9.2.2 HyperText Markup Language\nFigure 9.1 is an example of a table represented in the HTML format, while Figure 9.2\nshows the displayed image generated by a browser from the HTML representation of\nthe table. The HTML source shows a few of the HTML tags. Every HTML page should\nbe enclosed in an html tag, while the body of the page is enclosed in a body tag. A table\nID\n00128\n12345\n19991Zhang\nShankar\nBrandtName\nComp. Sci.\nComp. Sci.\nHistoryDepartment\nFigure 9.2 Display of HTML source from Figure 9.1.\n", "435": "9.2 Web Fundamentals 407\n<html>\n<body>\n<form action=\"PersonQuery\" method=get >\nSearch for:\n<select name=\"persontype\" >\n<option value=\"student\" selected >Student </option >\n<option value=\"instructor\" >Instructor </option >\n</select ><br>\nName: <input type=text size=20 name=\"name\" >\n<input type=submit value=\"submit\" >\n</form >\n</body >\n</html >\nFigure 9.3 AnHTML form.\nis speci\ufb01ed by a table tag, which contains rows speci\ufb01ed by a trtag. The header row of\nthe table has table cells speci\ufb01ed by a thtag, while regular rows have table cells speci\ufb01ed\nby a tdtag. We do not go into more details about the tags here; see the bibliographical\nnotes for references containing more detailed descriptions of HTML .\nFigure 9.3 shows how to specify an HTML form that allows users to select the\nperson type (student or instructor) from a menu and to input a number in a text box.\nFigure 9.4 shows how the above form is displayed in a web browser. Two methods\nof accepting input are illustrated in the form, but HTML also supports several other\ninput methods. The action attribute of the form tag speci\ufb01es that when the form is\nsubmitted (by clicking on the submit button), the form data should be sent to the URL\nPersonQuery (the URL is relative to that of the page). The web server is con\ufb01gured\nsuch that when this URL is accessed, a corresponding application program is invoked,\nwith the user-provided values for the arguments persontype andname (speci\ufb01ed in\ntheselect andinput \ufb01elds). The application program generates an HTML document,\nwhich is then sent back and displayed to the user; we shall see how to construct such\nprograms later in this chapter.\nHTTP de\ufb01nes two ways in which values entered by a user at the browser can be sent\nto the web server. The getmethod encodes the values as part of the URL.F o re x a m p l e ,\nif the Google search page used a form with an input parameter named qwith the get\nSearch for:\nName:Student\nsubmit\nFigure 9.4 Display of HTML source from Figure 9.3.\n", "436": "408 Chapter 9 Application Development\nmethod, and the user typed in the string \u201csilberschatz\u201d and submitted the form, the\nbrowser would request the following URL from the web server:\nhttps://www.google.com/search?q=silberschatz\nThepost method would instead send a request for the URL https://www.google.com ,\nand send the parameter values as part of the HTTP protocol exchange between the web\nserver and the browser. The form in Figure 9.3 speci\ufb01es that the form uses the get\nmethod.\nAlthough HTML code can be created using a plain text editor, there are a number\nof editors that permit direct creation of HTML text by using a graphical interface. Such\neditors allow constructs such as forms, menus, and tables to be inserted into the HTML\ndocument from a menu of choices, instead of manually typing in the code to generate\nthe constructs.\nHTML supports stylesheets , which can alter the default de\ufb01nitions of how an HTML\nformatting construct is displayed, as well as other display attributes such as background\ncolor of the page. The cascading stylesheet (CSS) standard allows the same stylesheet to\nbe used for multiple HTML documents, giving a distinctive but uniform look to all the\npages on a web site. You can \ufb01nd more information on stylesheets online, for example\natwww.w3schools.com/css/ .\nThe HTML 5 standard, which was released in 2014, provides a wide variety of form\ninput types, including the following:\n\u2022Date and time selection, using <input type=\"date\" name=\"abc\" >,a n d<input\ntype=\"time\" name=\"xyz\" >. Browsers would typically display a graphical date or\ntime picker for such an input \ufb01eld; the input value is saved in the form attributes\nabcandxyz. The optional attributes minandmax can be used to specify minimum\nand maximum values that can be chosen.\n\u2022File selection, using <input type=\"file\", name=\"xyz\" >, which allows a \ufb01le to be\nchosen, and its name saved in the form attribute xyz.\n\u2022Input restrictions (constraints) on a variety of input types, including minimum,\nmaximum, format matching a regular expression, and so on. For example,\n<input type=\"number\" name=\"start\" min=\"0\" max=\"55\" step=\"5\" value=\"0\" >\nallows the user to choose one of 0, 5, 10, 15, and so on till 55, with a default value\nof 0.\n9.2.3 Web Servers and Sessions\nAweb server is a program running on the server machine that accepts requests from\na web browser and sends back results in the form of HTML documents. The browser\nand web server communicate via HTTP . Web servers provide powerful features, beyond\nthe simple transfer of documents. The most important feature is the ability to execute\n", "437": "9.2 Web Fundamentals 409\nHTTP\nbrowser\nserverweb server\napplication server\ndatabase server\ndatanetwork\nFigure 9.5 Three-layer web application architecture.\nprograms, with arguments supplied by the user, and to deliver the results back as an\nHTML document.\nAs a result, a web server can act as an intermediary to provide access to a variety\nof information services. A new service can be created by creating and installing an\napplication program that provides the service. The common gateway interface ( CGI)\nstandard de\ufb01nes how the web server communicates with application programs. The\napplication program typically communicates with a database server, through ODBC ,\nJDBC , or other protocols, in order to get or store data.\nFigure 9.5 shows a web application built using a three-layer architecture, with a web\nserver, an application server, and a database server. Using multiple levels of servers in-\ncreases system overhead; the CGIinterface starts a new process to service each request,\nwhich results in even greater overhead.\nMost web applications today therefore use a two-layer web application architecture,\nwhere the web and application servers are combined into a single server, as shown in\nFigure 9.6. We study systems based on the tw o-layer architecture in more detail in\nsubsequent sections.\nThere is no continuous connection between the client and the web server; when a\nweb server receives a request, a connection is temporarily created to send the request\nand receive the response from the web server. But the connection may then be closed,\nand the next request could come over a new connection. In contrast, when a user logs\non to a computer, or connects to a database using ODBC orJDBC , a session is created,\nand session information is retained at the server and the client until the session is termi-\nnated\u2014information such as the user-identi\ufb01er of the user and session options that the\nuser has set. One important reason that HTTP isconnectionless is that most computers\nhave limits on the number of simultaneous connections they can accommodate, and if\na large number of sites on the web open connections to a single server, this limit would\nbe exceeded, denying service to further users. With a connectionless protocol, the con-\n", "438": "410 Chapter 9 Application Development\nHTTP\nbrowser\nserverdatadatabase serverweb server and\napplication servernetwork\nFigure 9.6 Two-layer web application architecture.\nnection can be broken as soon as a request is satis\ufb01ed, leaving connections available\nfor other requests.1\nMost web applications, however, need session information to allow meaningful\nuser interaction. For instance, applications typically restrict access to information, and\ntherefore need to authenticate users. Authentication should be done once per session,\nand further interactions in the session should not require reauthentication.\nTo implement sessions in spite of connections getting closed, extra information has\nto be stored at the client and returned with each request in a session; the server uses\nthis information to identify that a request is part of a user session. Extra information\nabout the session also has to be maintained at the server.\nThis extra information is usually maintained in the form of a cookie at the client;\na cookie is simply a small piece of text containing identifying information and with\nan associated name. For example, google.com may set a cookie with the name prefs ,\nwhich encodes preferences set by the user such as the preferred language and the num-\nber of answers displayed per page. On each search request, google.com can retrieve\nthe cookie named prefs from the user\u2019s browser, and display results according to the\nspeci\ufb01ed preferences. A domain (web site) is permitted to retrieve only cookies that\nit has set, not cookies set by other domains, and cookie names can be reused across\ndomains.\nFor the purpose of tracking a user session, an application may generate a session\nidenti\ufb01er (usually a random number not currently in use as a session identi\ufb01er), and\nsend a cookie named (for instance) sessionid containing the session identi\ufb01er. The\nsession identi\ufb01er is also stored locally at the server. When a request comes in, the\napplication server requests the cookie named sessionid from the client. If the client\n1For performance reasons, connections may be kept open for a short while, to allow subsequent requests to reuse the\nconnection. However, there is no guarantee that the connection will be kept open, and applications must be designed\nassuming the connection may be closed as soon as a request is serviced.\n", "439": "9.3 Servlets 411\ndoes not have the cookie stored, or returns a value that is not currently recorded as a\nvalid session identi\ufb01er at the server, the application concludes that the request is not\npart of a current session. If the cookie value matches a stored session identi\ufb01er, the\nrequest is identi\ufb01ed as part of an ongoing session.\nIf an application needs to identify users securely, it can set the cookie only after\nauthenticating the user; for example a user may be authenticated only when a valid user\nname and password are submitted.2\nFor applications that do not require high security, such as publicly available news\nsites, cookies can be stored permanently at the browser and at the server; they identify\nthe user on subsequent visits to the same site, without any identi\ufb01cation information\nbeing typed in. For applications that require higher security, the server may invalidate\n(drop) the session after a time-out period, o r when the user logs out. (Typically a user\nlogs out by clicking on a logout button, which submits a logout form, whose action is\nto invalidate the current session.) Invalidating a session merely consists of dropping\nthe session identi\ufb01er from the list of active sessions at the application server.\n9.3 Servlets\nThe Java servlet speci\ufb01cation de\ufb01nes an application programming interface for com-\nmunication between the web/application server and the application program. The\nHttpServlet c l a s si nJ a v ai m p l e m e n t st h es e r v l e t APIspeci\ufb01cation; servlet classes used\nto implement speci\ufb01c functions are de\ufb01ned as subclasses of this class.3Often the word\nservlet is used to refer to a Java program (and class) that implements the servlet inter-\nface. Figure 9.7 shows a servlet example; we explain it in detail shortly.\nThe code for a servlet is loaded into the web/application server when the server\nis started, or when the server receives a remote HTTP request to execute a particular\nservlet. The task of a servlet is to process such a request, which may involve accessing a\ndatabase to retrieve necessary information, and dynamically generating an HTML page\nto be returned to the client browser.\n9.3.1 A Servlet Example\nServlets are commonly used to generate dynamic responses to HTTP requests. They\ncan access inputs provided through HTML forms, apply \u201cbusiness logic\u201d to decide what\n2The user identi\ufb01er could be stored at the client end, in a cookie named, for example, userid . Such cookies can be\nused for low-security applications, such as free web sites ident ifying their users. However, for applications that require a\nhigher level of security, such a mechanism creates a security risk: The value of a cookie can be changed at the browser\nby a malicious user, who can then masquerade as a di\ufb00erent user. Setting a cookie (named sessionid ,f o re x a m p l e )t o\na randomly generated session identi\ufb01er (from a large spac e of numbers) makes it highly improbable that a user can\nmasquerade as (i.e., pretend to be) another user. A sequent ially generated session identi\ufb01er, on the other hand, would\nbe susceptible to masquerading.\n3The servlet interface can also support non-HTTP requests, although our example uses only HTTP.\n", "440": "412 Chapter 9 Application Development\nresponse to provide, and then generate HTML output to be sent back to the browser.\nServlet code is executed on a web or application server.\nFigure 9.7 shows an example of servlet code to implement the form in Fig-\nure 9.3. The servlet is called PersonQueryServlet , while the form speci\ufb01es that\n\u201caction=\"PersonQuery\" .\u201d The web/application server must be told that this servlet\nis to be used to handle requests for PersonQuery , which is done by using the anno-\nimport java.io.*;\nimport javax.servlet.*;\nimport javax.servlet.http.*;\n@WebServlet(\"PersonQuery\")\npublic class PersonQueryServlet extends HttpServlet {\npublic void doGet(HttpServletRequest request,\nHttpServletResponse response)\nthrows ServletException, IOException\n{\nresponse.setContentType(\"text/html\");\nPrintWriter out = response.getWriter();\n... check if user is logged in ...\nout.println(\" <HEAD><TITLE>Query Result </TITLE></HEAD>\");\nout.println(\" <BODY>\");\nString persontype = request.getParameter(\"persontype\");\nString name = request.getParameter(\"name\");\nif(persontype.equals(\"student\")) {\n... code to find students with the specified name ...\n... using JDBC to communicate with the database ..\n... Assume ResultSet rs has been retrieved, and\n... contains attributes ID, name, and department name\nString headers = new String[]{\"ID\", \"Name\", \"Department Name\"};\nUtil::resultSetToHTML(rs, headers, out);\n}\nelse {\n... as above, but for instructors ...\n}\nout.println(\" </BODY>\");\nout.close();\n}\n}\nFigure 9.7 Example of servlet code.\n", "441": "9.3 Servlets 413\ntation @WebServlet(\"PersonQuery\") shown in the code. The form speci\ufb01es that the\nHTTP getmechanism is used for transmitting parameters. So the doGet() method of\nthe servlet, as de\ufb01ned in the code, is invoked.\nEach servlet request results in a new thread within which the call is executed, so\nmultiple requests can be handled in parallel. Any values from the form menus and input\n\ufb01elds on the web page, as well as cookies, pass through an object of the HttpServletRe-\nquest class that is created for the request, and the reply to the request passes through\nan object of the class HttpServletResponse .\nThedoGet() method in the example extracts values of the parameters persontype\nand name by using request.getParameter() , and uses these values to run a query\nagainst a database. The code used to access the database and to get attribute values\nfrom the query result is not shown; refer to Section 5.1.1.5 for details of how to use\nJDBC to access a database. We assume that the result of the query in the form of a\nJDBC ResultSet is available in the variable resultset .\nThe servlet code returns the results of the query to the requester by outputting them\nto the HttpServletResponse object response . Outputting the results to response is\nimplemented by \ufb01rst getting a PrintWriter object outfrom response , and then printing\nthe query result in HTML format to out. In our example, the query result is printed by\ncalling the function Util::resultSetToHTML(resultset, header, out) ,w h i c hi ss h o w ni n\nFigure 9.8. The function uses JDBC metadata function on the ResultSet rsto \ufb01gure out\nhow many columns need to be printed. An array of column headers is passed to this\nfunction to be printed out; the column names could have been obtained using JDBC\nmetadata, but the database column names may not be appropriate for display to a user,\nso we provide meaningful column names to the function.\n9.3.2 Servlet Sessions\nRecall that the interaction between a browser and a web/application server is stateless.\nThat is, each time the browser makes a request to the server, the browser needs to con-\nnect to the server, request some information, then disconnect from the server. Cookies\ncan be used to recognize that a request is from the same browser session as an ear-\nlier request. However, cookies form a low-level mechanism, and programmers require\na better abstraction to deal with sessions.\nThe servlet APIprovides a method of tracking a session and storing information\npertaining to it. Invocation of the method getSession(false) of the class HttpServle-\ntRequest retrieves the HttpSession object corresponding to the browser that sent the\nrequest. An argument value of true would have speci\ufb01ed that a new session object must\nbe created if the request is a new request.\nWhen the getSession() method is invoked, the server \ufb01rst asks the client to return\na cookie with a speci\ufb01ed name. If the client does not have a cookie of that name, or\nreturns a value that does not match any ongoing session, then the request is not part of\nan ongoing session. In this case, getSession() would return a null value, and the servlet\ncould direct the user to a login page.\n", "442": "414 Chapter 9 Application Development\nimport java.io.*;\nimport javax.servlet.*;\nimport javax.servlet.http.*;\npublic class Util {\npublic static void resultSetToHTML(ResultSet rs,\nString headers[], PrintWriter out) {\nResultSetMetaData rsmd = rs.getMetaData();\nint numCols = rsmd.getColumnCount();\nout.println(\" <table border=1 >\");\nout.println(\" <tr>\");\nfor (int i=0; i <numCols; i++)\nout.println(\" <th>\"+ headers[i] + <\u2215th>\nout.println(\" <\u2215tr>\");\nwhile (rs.next()) {\nout.println(\" <tr>\");\nfor (int i=0; i <numCols; i++)\nout.println(\" <td>\"+ rs.getString(i) + <\u2215td>\nout.println(\" <\u2215tr>\");\n}\n}\n}\nFigure 9.8 Utility function to output ResultSet as a table.\nThe login page could allow the user to provide a user name and password. The\nservlet corresponding to the login page could verify that the password matches the\nuser; for example, by using the user name to retrieve the password from the database\nand checking if the password entered matches the stored password.4\nIf the user is properly authenticated, the login servlet would execute getSes-\nsion(true) , which would return a new session object. To create a new session, the server\nwould internally carry out the following tasks: set a cookie (called, for example, ses-\nsionId) with a session identi\ufb01er as its associated value at the client browser, create a\nnew session object, and associate the session identi\ufb01er value with the session object.\n4It is a bad idea to store unencrypted passwords in the database, since anyone with access to the database contents, such\nas a system administrator or a hacker, could steal the password. Instead, a hashing function is applied to the password,\nand the result is stored in the database; even if someone sees the hashing result stored in the database, it is very hard to\ninfer what was the original password. The same hashing function is applied to the user-supplied password, and the result\nis compared with the stored hashing result. Further, to ensure that even if two users use the same password the hash\nvalues are di\ufb00erent, the password system typic ally stores a di\ufb00erent random string (called the salt) for each user, and it\nappends the random string to the password before computing the hash value. Thus, the password relation would have\nthe schema user\n password (user, salt, passwordhash ), where passwordhash is generated by hash(append (password ,salt)).\nEncryption is described in more detail in Section 9.9.1.\n", "443": "9.3 Servlets 415\nThe servlet code can also store and look up (attribute-name, value) pairs in the\nHttpSession object, to maintain state across multiple requests within a session. For\nexample, after the user is authenticated and the session object has been created, the\nlogin servlet could store the user-id of the u ser as a session parameter by executing the\nmethod\nsession.setAttribute(\u201cuserid\u201d, userid)\non the session object returned by getSession() ; the Java variable userid is assumed to\ncontain the user identi\ufb01er.\nIf the request was part of an ongoing session, the browser would have returned the\ncookie value, and the corresponding session object would be returned by getSession() .\nThe servlet could then retrieve session pa rameters such as user-id from the session\nobject by executing the method\nsession.getAttribute(\u201cuserid\u201d)\non the session object returned above. If the attribute userid is not set, the function\nwould return a null value, which would indicate that the client user has not been au-\nthenticated.\nConsider the line in the servlet code in Figure 9.7 that says \u201c... check if user is\nlogged in...\u201d. The following code implements this check; if the user is not logged in, it\nsends an error message, and after a gap of 5 seconds, redirects the user to the login\npage.\nSession session = request.getSession(false);\nif (session == null || session.getAttribute(userid) == null) {\nout.println(\"You are not logged in.\");\nresponse.setHeader(\"Refresh\", \"5;url=login.html\");\nreturn();\n}\n9.3.3 Servlet Life Cycle\nThe life cycle of a servlet is controlled by the web/application server in which the servlet\nhas been deployed. When there is a client request for a speci\ufb01c servlet, the server \ufb01rst\nchecks if an instance of the servlet exists or not. If not, the server loads the servlet\nclass into the Java virtual machine ( JVM) and creates an instance of the servlet class.\nIn addition, the server calls the init() method to initialize the servlet instance. Notice\nthat each servlet instance is initialized only once when it is loaded.\nAfter making sure the servlet instance does exist, the server invokes the service\nmethod of the servlet, with a request object and a response object as parameters. By\ndefault, the server creates a new thread to execute the service method; thus, multiple\n", "444": "416 Chapter 9 Application Development\nrequests on a servlet can execute in parallel, without having to wait for earlier requests\nto complete execution. The service method calls doGet ordoPost as appropriate.\nWhen no longer required, a servlet can be shut down by calling the destroy()\nmethod. The server can be set up to shut down a servlet automatically if no requests\nhave been made on a servlet within a time-out period; the time-out period is a server\nparameter that can be set as appropriate for the application.\n9.3.4 Application Servers\nMany application servers provide built-in support for servlets. One of the most popular\nis the Tomcat Server from the Apache Jakarta Project. Other application servers that\nsupport servlets include Glass\ufb01sh, JBoss, BEA Weblogic Application Server, Oracle\nApplication Server, and IBM\u2019s WebSphere Application Server.\nThe best way to develop servlet applications is by using an IDEsuch as Eclipse or\nNetBeans, which come with Tomcat or Glass\ufb01sh servers built in.\nApplication servers usually provide a variety of useful services, in addition to basic\nservlet support. They allow applications to be deployed or stopped, and they provide\nfunctionality to monitor the status of the application server, including performance\nstatistics. Many application servers also support the Java 2 Enterprise Edition ( J2EE )\nplatform, which provides support and APIs for a variety of tasks, such as for handling\nobjects, and parallel processing across multiple application servers.\n9.4 Alternative Server-Side Frameworks\nThere are several alternatives to Java Servlets for processing requests at the application\nserver, including scripting languages and web application frameworks developed for\nlanguages such as Python.\n9.4.1 Server-Side Scripting\nWriting even a simple web application in a programming language such as Java or C\nis a time-consuming task that requires many lines of code and programmers who are\nfamiliar with the intricacies of the language. An alternative approach, that of server-\nside scripting , provides a much easier method for creating many applications. Scripting\nlanguages provide constructs that can be embedded within HTML documents.\nIn server-side scripting, before delivering a web page, the server executes the scripts\nembedded within the HTML contents of the page. Each piece of script, when executed,\ncan generate text that is added to the page (or may even delete content from the page).\nT h es o u r c ec o d eo ft h es c r i p t si sr e m o v e df r o mt h ep a g e ,s ot h ec l i e n tm a yn o te v e n\nbe aware that the page originally had any code in it. The executed script may also con-\ntain SQL code that is executed against a database. Many of these languages come with\nlibraries and tools that together constitute a framework for web application develop-\nment.\n", "445": "9.4 Alternative Server-Side Frameworks 417\n<html>\n<head><title>Hello </title></head >\n<body>\n<% if (request.getParameter(\u201cname\u201d) == null)\n{ out.println(\u201cHello World\u201d); }\nelse { out.println(\u201cHello, \u201d + request.getParameter(\u201cname\u201d)); }\n%>\n</body >\n</html >\nFigure 9.9 AJSPpage with embedded Java code.\nSome of the widely used scripting frameworks include Java Server Pages ( JSP),\nASP.NET from Microsoft, PHP, and Ruby on Rails. These frameworks allow code writ-\nten in languages such as Java, C#, VBScript, and Ruby to be embedded into or invoked\nfrom HTML pages. For instance, JSPallows Java code to be embedded in HTML pages,\nwhile Microsoft\u2019s ASP.NET and ASP support embedded C# and VBScript.\n9.4.1.1 Java Server Pages\nNext we brie\ufb02y describe Java Server Pages (JSP), a scripting language that allows\nHTML programmers to mix static HTML with dynamically generated HTML .T h em o -\ntivation is that, for many dynamic web pages, most of their content is still static (i.e.,\nthe same content is present whenever the page is generated). The dynamic content of\nthe web pages (which are generated, for example, on the basis of form parameters) is\noften a small part of the page. Creating such pages by writing servlet code results in a\nlarge amount of HTML being coded as Java strings. JSPinstead allows Java code to be\nembedded in static HTML ; the embedded Java code generates the dynamic part of the\npage. JSPscripts are actually translated into servlet code that is then compiled, but the\napplication programmer is saved the trouble of writing much of the Java code to create\nthe servlet.\nF i g u r e9 . 9s h o w st h es o u r c et e x to fa JSPpage that includes embedded Java code.\nThe Java code in the script is distinguished from the surrounding HTML code by being\nenclosed in <%\u2026%>.T h ec o d eu s e s request.getParameter() to get the value of the\nattribute name .\nWhen a JSPpage is requested by a browser, the application server generates HTML\noutput from the page, which is sent to the browser. The HTML part of the JSPpage is\noutput as is.5Wherever Java code is embedded within <%\u2026%>,t h ec o d ei sr e p l a c e d\nin the HTML output by the text it prints to the object out.I nt h e JSPcode in Figure 9.9,\n5JSPallows a more complex embedding, where HTML code is within a Java if-else statement, and gets output condition-\nally depending on whether the if condition evaluates to true or not. We omit details here.\n", "446": "418 Chapter 9 Application Development\nif no value was entered for the form parameter name , the script prints \u201cHello World\u201d;\nif a value was entered, the script prints \u201cHello\u201d followed by the name.\nA more realistic example may perform more complex actions, such as looking up\nvalues from a database using JDBC .\nJSPalso supports the concept of a tag library , which allows the use of tags that\nlook much like HTML tags but are interpreted at the server and are replaced by appro-\npriately generated HTML code. JSPprovides a standard set of tags that de\ufb01ne variables\nand control \ufb02ow (iterators, if-then-else), along with an expression language based on\nJavaScript (but interpreted at the server). The set of tags is extensible, and a number of\ntag libraries have been implemented. For example, there is a tag library that supports\npaginated display of large data sets and a library that simpli\ufb01es display and parsing of\ndates and times.\n9.4.1.2 PHP\nPHP is a scripting language that is widely used for server-side scripting. PHP code can\nbe intermixed with HTML in a manner similar to JSP. The characters \u201c <?php\u201d indicate\nthe start of PHP code, while the characters \u201c? >\u201d indicate the end of PHP code. The\nfollowing code performs the same actions as the JSPcode in Figure 9.9.\n<html>\n<head><title>Hello </title></head >\n<body>\n<?php if (!isset($\n REQUEST['name']))\n{ echo 'Hello World'; }\nelse { echo 'Hello, ' . $\n REQUEST['name']; }\n?>\n</body >\n</html >\nThe array $\n REQUEST contains the request parameters. Note that the array is\nindexed by the parameter name; in PHP arrays can be indexed by arbitrary strings, not\njust numbers. The function isset checks if the element of the array has been initialized.\nTheecho function prints its argument to the output HTML . The operator \u201c.\u201d between\ntwo strings concatenates the strings.\nA suitably con\ufb01gured web server would interpret any \ufb01le whose name ends in\n\u201c.php\u201d to be a PHP \ufb01le. If the \ufb01le is requested, the web server processes it in a manner\nsimilar to how JSP\ufb01les are processed and returns the generated HTML to the browser.\nA number of libraries are available for the PHP language, including libraries for\ndatabase access using ODBC (similar to JDBC in Java).\n9.4.2 Web Application Frameworks\nWeb application development frameworks ease the task of constructing web applica-\ntions by providing features such as these:\n", "447": "9.4 Alternative Server-Side Frameworks 419\n\u2022A library of functions to support HTML and HTTP features, including sessions.\n\u2022A template scripting system.\n\u2022A controller that maps user interaction events such as form submits to appropriate\nfunctions that handle the event. The controller also manages authentication and\nsessions. Some frameworks also provide tools for managing authorizations.\n\u2022A (relatively) declarative way of specifying a form with validation constraints on\nuser inputs, from which the system generates HTML and Javascript/Ajax code to\nimplement the form.\n\u2022An object-oriented model with an object-relational mapping to store data in a re-\nlational database (described in Section 9.6.2).\nThus, these frameworks provide a variety of features that are required to build web\napplications in an integrated manner. By generating forms from declarative speci\ufb01ca-\ntions and managing data access transparently, the frameworks minimize the amount\nof coding that a web application programmer has to carry out.\nThere are a large number of such frameworks, based on di\ufb00erent languages. Some\no ft h em o r ew i d e l yu s e df r a m e w o r k si n c l u d et h eD j a n g of r a m e w o r kf o rt h eP y t h o n\nlanguage, Ruby on Rails, which supports the Rails framework on the Ruby program-\nming language, Apache Struts, Swing, Tapestry, and WebObjects, all based on Java/JSP.\nMany of these frameworks also make it easy to create simple CRUD web interfaces; that\nis, interfaces that support create, read, update and delete of objects/tuples by generat-\ning code from an object model or a database. Such tools are particularly useful to get\nsimple applications running quickly, and the generated code can be edited to build\nmore sophisticated web interfaces.\n9.4.3 The Django Framework\nThe Django framework for Python is a widely used web application framework. We\nillustrate a few features of the framework through examples.\nViews in Django are functions that are equivalent to servlets in Java. Django re-\nquires a mapping, typically speci\ufb01ed in a \ufb01le urls.py ,w h i c hm a p s URLst oD j a n g o\nviews. When the Django application server receives an HTTP request, it uses the URL\nmapping to decide which view function to invoke.\nFigure 9.10 shows sample code implementing the person query task that we earlier\nimplemented using Java servlets. The code shows a view called person\n query\n view .\nWe assume that the PersonQuery URL is mapped to the view person\n query\n view ,a n d\nis invoked from the HTML form shown earlier in Figure 9.3.\nWe also assume that the root of the application is mapped to a login\n view .W e\nhave not shown the code for login\n view , but we assume it displays a login form, and on\nsubmit it invokes the authenticate view. We have not shown the authenticate view,\n", "448": "420 Chapter 9 Application Development\neither, but we assume that it checks the login name and password. If the password is\nvalidated, the authenticate view redirects to a person\n query\n form ,w h i c hd i s p l a y st h e\nHTML code that we saw earlier in Figure 9.3; if password validation fails, it redirects\nto the login\n view .\nReturning to Figure 9.10, the view person\n query\n view() \ufb01rst checks if the user is\nlogged in by checking the session variable username . If the session variable is not set,\nthe browser is redirected to the login screen. Otherwise, the requested user informa-\nfrom django.http import HttpResponse\nfrom django.db import connection\ndef result\n set\nto\nhtml(headers, cursor):\nhtml = \"<table border=1>\"\nhtml += \"<tr>\"\nfor header in headers:\nhtml += \"<th>\" + header + \"</th>\"\nhtml += \"</tr>\"\nfor row in cursor.fetchall():\nhtml += \"<tr>\"\nfor col in row:\nhtml += \"<td>\" + col + \"</td>\"\nhtml += \"</tr>\"\nhtml += \"</table>\"\nreturn html\ndef person\n query\n view(request):\nif \"username\" not in request.session:\nreturn login\n view(request)\npersontype = request.GET.get(\"persontype\")\npersonname = request.GET.get(\"personname\")\nif persontype == \"student\":\nquery\n tmpl = \"select id, name, dept\n name from student where name=%s\"\nelse:\nquery\n tmpl = \"select id, name, dept\n name from instructor where name=%s\"\nwith connection.cursor() as cursor:\ncursor.execute(query\n tmpl, [personname])\nheaders = [\"ID\", \"Name\", \"Department Name\"]\nreturn HttpResponse(result\n set\nto\nhtml(headers, cursor))\nFigure 9.10 The person query application in Django.\n", "449": "9.5 Client-Side Code and Web Services 421\ntion is fetched by connecting to the database; connection details for the database are\nspeci\ufb01ed in a Django con\ufb01guration \ufb01le settings.py and are omitted in our description.\nA cursor (similar to a JDBC statement) is opened on the connection, and the query is\nexecuted using the cursor. Note that the \ufb01rst argument of cursor.execute is the query,\nwith parameters marked by \u201c%s\u201d, and the second argument is a list of values for the\nparameters. The result of the database query is then displayed by calling a function\nresult\n set\nto\nhtml() , which iterates over the result set fetched from the database and\noutputs the results in HTML format to a string; the string is then returned as an HttpRe-\nsponse .\nDjango provides support for a number of other features, such as creating HTML\nforms and validating data entered in the forms, annotations to simplify checking of\nauthentication, and templates for creating HTML pages, which are somewhat similar to\nJSPpages. Django also supports an object-re lation mapping system, which we describe\nin Section 9.6.2.2.\n9.5 C l i e n t - S i d eC o d ea n dW e bS e r v i c e s\nThe two most widely used classes of user interfaces today are the web interfaces and\nmobile application interfaces.\nWhile early generation web browsers only displayed HTML code, the need was\nsoon felt to allow code to run on the browsers. Client-side scripting languages are lan-\nguages designed to be executed on the client\u2019s web browser. The primary motivation for\nsuch scripting languages is \ufb02exible interaction with the user, providing features beyond\nthe limited interaction power provided by HTML andHTML forms. Further, executing\nprograms at the client site speeds up interaction greatly compared to every interaction\nbeing sent to a server site for processing.\nThe JavaScript language is by far the most widely used client-side scripting lan-\nguage. The current generation of web interfaces uses the JavaScript scripting language\nextensively to construct sophisticated user interfaces.\nAny client-side interface needs to store and retrieve data from the back end. Di-\nrectly accessing a database is not a good idea, since it not only exposes low-level details,\nbut it also exposes the database to attacks. Instead, back ends provide access to store\nand retrieve data through web services. We discuss web services in Section 9.5.2.\nMobile applications are very widely used, and user interfaces for mobile devices\nare very important today. Although we do not cover mobile application development\nin this book, we o\ufb00er pointers to some mobile application development frameworks in\nSection 9.5.4.\n9.5.1 JavaScript\nJavaScript is used for a variety of tasks, including validation, \ufb02exible user interfaces,\nand interaction with web services, which we now describe.\n", "450": "422 Chapter 9 Application Development\n9.5.1.1 Input Validation\nFunctions written in JavaScript can be used to perform error checks (validation) on\nuser input, such as a date string being properly formatted, or a value entered (such as\nage) being in an appropriate range. These checks are carried out on the browser as data\nare entered even before the data are sent to the web server.\nWith HTML 5, many validation constraints can be speci\ufb01ed as part of the input tag.\nFor example, the following HTML code:\n<input type=\"number\" name=\"credits\" size=\"2\" min=\"1\" max=\"15\" >\nensures that the input for the parameter \u201ccredits\u201d is a number between 1 and 15. More\ncomplex validations that cannot be performed using HTML 5f e a t u r e sa r eb e s td o n e\nusing JavaScript.\nFigure 9.11 shows an example of a form with a JavaScript function used to validate\na form input. The function is declared in the head section of the HTML document. The\nform accepts a start and an end date. The validation function ensures that the start date\n<html>\n<head>\n<script type=\"text/javascript\">\nfunction validate() {\nvar startdate = new Date (document.getElementById(\"start\").value);\nvar enddate = new Date (document.getElementById(\"end\").value);\nif(startdate > enddate) {\nalert(\"Start date is > end date\");\nreturn false;\n}\n}\n</script>\n</head>\n<body>\n<form action=\"submitDates\" onsubmit=\"return validate()\">\nStart Date: <input type=\"date\" id=\"start\"><br />\nEnd Date : <input type=\"date\" id=\"end\"><br />\n<input type=\"submit\" value=\"Submit\">\n</form>\n</body>\n</html>\nFigure 9.11 Example of JavaScript used to validate form input.\n", "451": "9.5 Client-Side Code and Web Services 423\nis not greater than the end date. The form tag speci\ufb01es that the validation function is\nto be invoked when the form is submitted. If the validation fails, an alert box is shown\nto the user, and if it succeeds, the form is submitted to the server.\n9.5.1.2 Responsive User Interfaces\nThe most important bene\ufb01t of JavaScript is the ability to create highly responsive user\ninterfaces within a browser using JavaScript. The key to building such a user interface is\nthe ability to dynamically modify the HTML code being displayed by using JavaScript.\nThe browser parses HTML code into an in-memory tree structure de\ufb01ned by a stan-\ndard called the Document Object Model (DOM ). JavaScript code can modify the tree\nstructure to carry out certain operations. For example, suppose a user needs to enter a\nnumber of rows of data, for example multiple items in a single bill. A table containing\ntext boxes and other form input methods can be used to gather user input. The table\nmay have a default size, but if more rows are needed, the user may click on a button\nlabeled (for example) \u201cAdd Item.\u201d This button can be set up to invoke a JavaScript\nfunction that modi\ufb01es the DOM tree by adding an extra row in the table.\nAlthough the JavaScript language has been standardized, there are di\ufb00erences be-\ntween browsers, particularly in the details of the DOM model. As a result, JavaScript\ncode that works on one browser may not work on another. To avoid such problems,\nit is best to use a JavaScript library, such as the JQuery library, which allows code to\nbe written in a browser-independent way. Internally, the functions in the library can\n\ufb01nd out which browser is in use and send appropriately generated JavaScript to the\nbrowser.\nJavaScript libraries such as JQuery provide a number of UIelements, such as\nmenus, tabs, widgets such as sliders, and features such as autocomplete, that can be\ncreated and executed using library functions.\nThe HTML 5 standard supports a number of features for rich user interaction, in-\ncluding drag-and-drop, geolocation (which allows the user\u2019s location to be provided\nto the application with user permission), allowing customization of the data/interface\nbased on location. HTML 5 also supports Server-Side Events ( SSE), which allows a back-\nend to notify the front end when some event occurs.\n9.5.1.3 Interfacing with Web Services\nToday, JavaScript is widely used to create dynamic web pages, using several technolo-\ngies that are collectively called Ajax. Programs written in JavaScript can communicate\nwith the web server asynchronously (that is, in the background, without blocking user\ninteraction with the web browser), and can fetch data and display it. The JavaScript\nObject Notation ,o rJSON , representation described in Section 8.1.2 is the most widely\nused data format for transferring data, although other formats such as XML are also\nused.\nThe role of the code for the above tasks, which runs at the application server,\nis to send data to the JavaScript code, which then renders the data on the browser.\n", "452": "424 Chapter 9 Application Development\nSuch backend services, which serve the role of functions which can be invoked to fetch\nrequired data, are known as web services . Such services can be implemented using Java\nServlets, Python, or any of a number of other language frameworks.\nAs an example of the use of Ajax, consider the autocomplete feature implemented\nby many web applications. As the user types a value in a text box, the system suggests\ncompletions for the value being typed. Suc h autocomplete is very useful for helping a\nuser choose a value from a large number of values where a drop-down list would not\nbe feasible. Libraries such as jQuery provide support for autocomplete by associating\na function with a text box; the function takes partial input in the box, connected to\na web back end to get possible completions, and displays them as suggestions for the\nautocomplete.\nThe JavaScript code shown in Figure 9.12 uses the jQuery library to implement\nautocomplete and the DataTables plug-in for the jQuery library to provide a tabular\ndisplay of data. The HTML code has a text input box for name, which has an idattribute\nset to name . The script associates an autocomplete function from the jQuery library\nwith the text box by using $(\"#name\") syntax of jQuery to locate the DOM node for\ntext box with id\u201cname\u201d, and then associating the autocomplete function with the\nnode. The attribute source passed to the function identi\ufb01es the web service that must\nbe invoked to get values for the autocomplete functionality. We assume that a servlet\n/autocomplete\n name has been de\ufb01ned, which accepts a parameter term containing\nthe letters typed so far by the user, even as they are being typed. The servlet should\nreturn a JSON array of names of students/instructors that match the letters in the term\nparameter.\nThe JavaScript code also illustrates how data can be retrieved from a web service\nand then displayed. Our sample code uses the DataTables jQuery plug-in; there are a\nnumber of other alternative libraries for displaying tabular data. We assume that the\nperson\n query\n ajax Servlet, which is not shown, returns the ID, name, and department\nname of students or instructors with a given name, as we saw earlier in Figure 9.7, but\nencoded in JSON as an object with attribute data containing an array of rows; each\nrow is a JSON object with attributes id,name ,a n d dept\n name .\nThe line starting with myTable shows how the jQuery plug-in DataTable is associ-\nated with the HTML table shown later in the \ufb01gure, whose identi\ufb01er is personTable .\nWhen the button \u201cShow details\u201d is clicked, the function loadTableAsync() is invoked.\nThis function \ufb01rst creates a URL string urlthat is used to invoke person\n query\n ajax\nwith values for person type and name. The function ajax.url(url).load() invoked on\nmyTable \ufb01lls the rows of the table using the JSON data fetched from the web service\nwhose URL we created above. This happens asynchronously; that is, the function re-\nturns immediately, but when the data have been fetched, the table rows are \ufb01lled with\nthe returned data.\nFigure 9.13 shows a screenshot of a browser displaying the result of the code in\nFigure 9.12.\nAs another example of the use of Ajax, consider a web site with a form that allows\nyou to select a country, and once a country has been selected, you are allowed to select\n", "453": "9.5 Client-Side Code and Web Services 425\n<html><head>\n<script src=\"https://code.jquery.com/jquery-3.3.1.js\" ></script >\n<script src=\"https://cdn.datatables.net/ 1.10.19/js/jquery.dataTables.min.js\" ></script >\n<script src=\"https://code.jquery.c om/ui/1.12.1/jquery-ui.min.js\" ></script >\n<script src=\"https://cdn.datatables.net/ 1.10.19/js/jquery.dataTables.min.js\" ></script >\n<link rel=\"stylesheet\"\nhref=\"https://code.jquery.com/ui/1. 12.1/themes/base/jquery-ui.css\" / >\n<link rel=\"stylesheet\"\nhref=\"https://cdn.datatables.net/1.10.19/css/jquery.dataTables.min.css\"/ >\n<script>\nvar myTable;\n$(document).ready(function() {\n$(\"#name\").autocomplete({ source: \"/autocomplete\n name\" });\nmyTable = $(\"#personTable\").DataTable({\ncolumns: [{data:\"id\"}, {data:\"name\"}, {data:\"dept\n name\"}]\n});\n});\nfunction loadTableAsync() {\nvar params = {persontype:$(\"#persontyp e\").val(), name:$(\"#name\").val()};\nvar url = \"/person\n query\n ajax?\" + jQuery.param(params);\nmyTable.ajax.url(url).load();\n}\n</script >\n</head ><body>\nSearch for:\n<select id=\"persontype\" >\n<option value=\"student\" selected >Student </option >\n<option value=\"instructor\" >Instructor </option >\n</select ><br>\nName: <input type=text size=20 id=\"name\" >\n<button onclick=\"loadTableAsync()\" >Show details </button >\n<table id=\"personTable\" border=\"1\" >\n<thead >\n<tr><th>ID</th><th>Name </th><th>Dept. Name </th></tr>\n</thead >\n</table >\n</body ></html >\nFigure 9.12 HTML page using JavaScript and Ajax.\na state from a list of states in that country. Until the country is selected, the drop-down\nlist of states is empty. The Ajax framework allows the list of states to be downloaded\n", "454": "426 Chapter 9 Application Development\nFigure 9.13 Screenshot of display generated by Figure 9.12.\nfrom the web site in the background when the country is selected, and as soon as the\nlist has been fetched, it is added to the drop-down list, which allows you to select the\nstate.\n9.5.2 Web Services\nAweb service is an application component that can be invoked over the web and func-\ntions, in e\ufb00ect, like an application programming interface. A web service request is sent\nusing the HTTP protocol, it is executed at an application server, and the results are sent\nback to the calling program.\nTwo approaches are widely used to implement web services. In the simpler ap-\nproach, called Representation State Transfer (orREST ), web service function calls are\nexecuted by a standard HTTP request to a URL at an application server, with parameters\nsent as standard HTTP request parameters. The application server executes the request\n(which may involve updating the database at the server), generates and encodes the\nresult, and returns the result as the result of the HTTP request. The most widely used\nencoding for the results today is the JSON representation, although XML ,w h i c hw es a w\nearlier in Section 8.1.3, is also used. The requestor parses the returned page to access\nthe returned data.\nIn many applications of such REST ful web services (i.e., web services using REST ),\nthe requestor is JavaScript code running in a web browser; the code updates the browser\nscreen using the result of the function call. For example, when you scroll the display\non a map interface on the web, the part of the map that needs to be newly displayed\nmay be fetched by JavaScript code using a REST ful interface and then displayed on the\nscreen.\nWhile some web services are not publicly documented and are used only inter-\nnally by speci\ufb01c applications, other web services have their interfaces documented and\ncan be used by any application. Such services may allow use without any restriction,\n", "455": "9.5 Client-Side Code and Web Services 427\nmay require users to be logged in before accessing the service, or may require users\nor application developers to pay the web service provider for the privilege of using the\nservice.\nToday, a very large variety of REST ful web services are available, and most front-end\napplications use one or more such services to perform backend activities. For exam-\nple, your web-based email system, your social media web page, or your web-based map\nservice would almost surely be built with JavaScript code for rendering and would use\nbackend web services to fetch data as well as to perform updates. Similarly, any mobile\napp that stores data at the back end almost surely uses web services to fetch data and\nto perform updates.\nWeb services are also increasingly used at the backend, to make use of function-\nalities provided by other backend systems. For example, web-based storage systems\nprovide a web service APIfor storing and retrieving data; such services are provided\nby a number of providers, such as Amazon S3, Google Cloud Storage, and Microsoft\nAzure. They are very popular with application developers since they allow storage of\nvery large amounts of data, and they support a very large number of operations per\nsecond, allowing scalability far beyond what a centralized database can support.\nThere are many more such web-service APIs. For example, text-to-speech, speech\nrecognition, and vision web-service APIs allow developers to construct applications\nincorporating speech and image recognition with very little development e\ufb00ort.\nA more complex and less frequently used approach, sometimes referred to as \u201cBig\nWeb Services,\u201d uses XML encoding of parameters as well as results, has a formal def-\ninition of the web APIusing a special language, and uses a protocol layer built on top\nof the HTTP protocol.\n9.5.3 Disconnected Operation\nMany applications wish to support some operations even when a client is disconnected\nfrom the application server. For example, a student may wish to complete an applica-\ntion form even if her laptop is disconnected from the network but have it saved back\nwhen the laptop is reconnected. As another example, if an email client is built as a web\napplication, a user may wish to compose an email even if her laptop is disconnected\nfrom the network and have it sent when it is reconnected. Building such applications\nrequires local storage in the client machine.\nThe HTML 5 standard supports local storage, which can be accessed using\nJavaScript. The code:\nif (typeof(Storage) !== \"undefined\") { // browser supports local storage\n...\n}\nchecks if the browser supports local storage. If it does, the following functions can be\nused to store, load, or delete values for a given key.\n", "456": "428 Chapter 9 Application Development\nlocalStorage.setItem(key, value)\nlocalStorage.getItem(key)\nlocalStorage.deleteItem(key)\nTo avoid excessive data storage, the browser may limit a web site to storing at most\nsome amount of data; the default maximum is typically 5 megabytes.\nThe above interface only allows storage/retrieval of key/value pairs. Retrieval re-\nquires that a key be provided; otherwise the entire set of key/value pairs will need to\nbe scanned to \ufb01nd a required value. Applications may need to store tuples indexed on\nmultiple attributes, allowing e\ufb03cient access based on values of any of the attributes.\nHTML 5 supports IndexedDB, which allows storage of JSON objects with indices on\nmultiple attributes. IndexedDB also supports schema versions and allows the developer\nto provide code to migrate data from one schema version to the next version.\n9.5.4 Mobile Application Platforms\nMobile applications (or mobile apps, for short) are widely used today, and they form\nthe primary user interface for a large class of users. The two most widely used mo-\nbile platforms today are Android and iOS. Each of these platforms provides a way of\nbuilding applications with a graphical user interface, tailored to small touch-screen de-\nvices. The graphical user interface provides a variety of standard GUI features such as\nmenus, lists, buttons, check boxes, progress bars, and so on, and the ability to display\ntext, images, and video.\nMobile apps can be downloaded and stored and used later. Thus, the user can\ndownload apps when connected to a high-speed network and then use the app with a\nlower-speed network. In contrast, web apps may get downloaded when they are used,\nresulting in a lot of data transfer when a user may be connected to a lower-speed net-\nwork or a network where data transfer is expensive. Further, mobile apps can be better\ntuned to small-sized devices than web apps, with user interfaces that work well on small\ndevices. Mobile apps can also be compiled to machine code, resulting in lower power\ndemands than web apps. More importantly, unlike (earlier generation) web apps, mo-\nbile apps can store data locally, allowing o\ufb04ine usage. Further, mobile apps have a\nwell-developed authorization model, allowing them to use information and device fea-\ntures such as location, cameras, contacts, and so on with user authorization.\nHowever, one of the drawbacks of using mobile-app interfaces is that code written\nfor the Android platform can only run on that platform and not on iOS, and vice versa.\nAs a result, developers are forced to code every application twice, once for Android\nand once for iOS, unless they decide to ignore one of the platforms completely, which\nis not very desirable.\nThe ability to create applications where the same high-level code can run on ei-\nther Android or iOS is clearly very important. The React Native framework based on\nJavaScript, developed by Facebook, and the Flutter framework based on the Dart lan-\nguage developed by Google, are designed to allow cross-platform development. (Dart\n", "457": "9.6 Application Architectures 429\nis a language optimized for developing user interfaces, providing features such as asyn-\nchronous function invocation and functions on streams.) Both frameworks allow much\nof the application code to be common for both Android and iOS, but some function-\nality can be made speci\ufb01c to the underlying platform in case it is not supported in the\nplatform-independent part of the framework.\nWith the wide availability of high-speed mobile networks, some of the motivation\nfor using mobile apps instead of web apps, such as the ability to download ahead of\ntime, is not as important anymore. A new generation of web apps, called Progressive\nWeb Apps (PWA ) that combine the bene\ufb01ts of mobile apps with web apps is seeing\nincreasing usage. Such apps are built using JavaScript and HTML 5 and are tailored for\nmobile devices.\nA key enabling feature for PWA si st h e HTML 5 support for local data storage, which\nallows apps to be used even when the device is o\ufb04ine. Another enabling feature is the\nsupport for compilation of JavaScript code; compilation is restricted to code that fol-\nlows a restricted syntax, since compilation of arbitrary JavaScript code is not practical.\nSuch compilation is typically done just-in-time, that is, it is done when the code needs\nto be executed, or if it has already been executed multiple times. Thus, by writing CPU-\nheavy parts of a web application using only JavaScript features that allow compilation,\nit is possible to ensure CPU and energy-e\ufb03cient execution of the code on a mobile\ndevice.\nPWA sa l s om a k eu s eo f HTML 5 service workers, which allow a script to run in the\nbackground in the browser, separate from a web page. Such service workers can be used\nto perform background synchronization operations between the local store and a web\nservice, or to receive or push noti\ufb01cations from a backend service. HTML 5 also allows\napps to get device location (after user authorization), allowing PWA st ou s el o c a t i o n\ninformation.\nThus, PWA s are likely to see increasing use, replacing many (but certainly not all)\nof the use cases for mobile apps.\n9.6 Application Architectures\nTo handle their complexity, large applications are often broken into several layers:\n\u2022The presentation oruser-interface layer, which deals with user interaction. A sin-\ngle application may have several di\ufb00erent versions of this layer, corresponding to\ndistinct kinds of interfaces such as web browsers and user interfaces of mobile\nphones, which have much smaller screens.\nIn many implementations, the presentation/user-interface layer is itself concep-\ntually broken up into layers, based on the model-view-controller (MVC )a r c h i t e c t u r e .\nThemodel corresponds to the business-logic layer, described below. The view de-\n\ufb01nes the presentation of data; a single underlying model can have di\ufb00erent views\ndepending on the speci\ufb01c software/device used to access the application. The con-\ntroller receives events (user actions), executes actions on the model, and returns\n", "458": "430 Chapter 9 Application Development\na view to the user. The MVC architecture is used in a number of web application\nframeworks.\n\u2022Thebusiness-logic layer, which provides a high-level view of data and actions on\ndata. We discuss the business-logic layer in more detail in Section 9.6.1.\n\u2022Thedata-access layer, which provides the interface between the business-logic layer\nand the underlying database. Many applications use an object-oriented language\nto code the business-logic layer and use an object-oriented model of data, while\nthe underlying database is a relational database. In such cases, the data-access\nlayer also provides the mapping from the object-oriented data model used by the\nbusiness logic to the relational model supported by the database. We discuss such\nmappings in more detail in Section 9.6.2.\nFigure 9.14 shows these layers, along with a sequence of steps taken to process a\nrequest from the web browser. The labels on the arrows in the \ufb01gure indicate the order\nof the steps. When the request is received by the application server, the controller sends\na request to the model. The model processes the request, using business logic, which\nmay involve updating objects that are part of the model, followed by creating a result\nobject. The model in turn uses the data-access layer to update or retrieve information\nfrom a database. The result object created by the model is sent to the view module,\nwhich creates an HTML view of the result to be displayed on the web browser. The\nview may be tailored based on the characteristics of the device used to view the result\n\u2014for example, whether it is a computer monitor with a large screen or a small screen\non a phone. Increasingly, the view layer is implemented by code running at the client,\ninstead of at the server.\nweb browserinternet1\n8\n7652\n3\n4\ndatabase\nweb/application serverviewcontroller\nmodel\ndata-access\nlayer\nFigure 9.14 Web application architecture.\n", "459": "9.6 Application Architectures 431\n9.6.1 The Business-Logic Layer\nThe business-logic layer of an application for managing a university may provide ab-\nstractions of entities such as students, instructors, courses, sections, etc., and actions\nsuch as admitting a student to the university, enrolling a student in a course, and so\non. The code implementing these actions ensures that business rules are satis\ufb01ed; for\nexample, the code would ensure that a student can enroll for a course only if she has\nalready completed course prerequisites and has paid her tuition fees.\nIn addition, the business logic includes work\ufb02ows , which describe how a particular\ntask that involves multiple participants is handled. For example, if a candidate applies\nto the university, there is a work\ufb02ow that de\ufb01nes who should see and approve the ap-\nplication \ufb01rst, and if approved in the \ufb01rst step, who should see the application next,\nand so on until either an o\ufb00er is made to the student, or a rejection note is sent out.\nWork\ufb02ow management also needs to deal with error situations; for example, if a dead-\nline for approval/rejection is not met, a supervisor may need to be informed so she can\nintervene and ensure the application is processed.\n9.6.2 The Data-Access Layer and Object-Relational Mapping\nIn the simplest scenario, where the business-logic layer uses the same data model as the\ndatabase, the data-access layer simply hides the details of interfacing with the database.\nHowever, when the business-logic layer is written using an object-oriented programming\nlanguage, it is natural to model data as objects, with methods invoked on objects.\nIn early implementations, programmers had to write code for creating objects by\nfetching data from the database and for storing updated objects back in the database.\nHowever, such manual conversions between data models is cumbersome and error\nprone. One approach to handling this problem was to develop a database system\nthat natively stores objects, and relationships between objects, and allows objects in\nthe database to be accessed in exactly the same way as in-memory objects. Such\ndatabases, called object-oriented databases , were discussed in Section 8.2. However,\nobject-oriented databases did not achieve commercial success for a variety of technical\nand commercial reasons.\nAn alternative approach is to use traditional relational databases to store data, but\nto automate the mapping of data in relation to in-memory objects, which are created\non demand (since memory is usually not su\ufb03cient to store all data in the database), as\nwell as the reverse mapping to store updated objects back as relations in the database.\nSeveral systems have been developed to implement such object-relational mappings .\nWe describe the Hibernate and Django ORM sn e x t .\n9.6.2.1 Hibernate ORM\nTheHibernate system is widely used for mapping from Java objects to relations. Hiber-\nnate provides an implementation of the Java Persistence API(JPA). In Hibernate, the\nmapping from each Java class to one or more relations is speci\ufb01ed in a mapping \ufb01le.\n", "460": "432 Chapter 9 Application Development\nThe mapping \ufb01le can specify, for example, that a Java class called Student is mapped\nto the relation student , with the Java attribute IDmapped to the attribute student .ID,\nand so on. Information about the database, such as the host on which it is running and\nuser name and password for connecting to the database, are speci\ufb01ed in a properties\n\ufb01le. The program has to open a session , which sets up the connection to the database.\nOnce the session is set up, a Student object stud created in Java can be stored in the\ndatabase by invoking session.save(stud) . The Hibernate code generates the SQL com-\nmands required to store corresponding data in the student relation.\nWhile entities in an E-Rmodel naturally correspond to objects in an object-oriented\nlanguage such as Java, relationships often do not. Hibernate supports the ability to map\nsuch relationships as sets associated with objects. For example, the takes relationship\nbetween student and section can be modeled by associating a set of section sw i t he a c h\nstudent ,a n das e to f student sw i t he a c h section . Once the appropriate mapping is spec-\ni\ufb01ed, Hibernate populates these sets automatically from the database relation takes ,\nand updates to the sets are re\ufb02ected back to the database relation on commit.\nAs an example of the use of Hibernate, we create a Java class corresponding to the\nstudent relation as follows:\n@Entity public class Student {\n@Id String ID;\nString name;\nString department;\nint tot\n cred;\n}\nTo be precise, the class attributes should be declared as private, and getter/setter meth-\nods should be provided to access the attributes, but we omit these details.\nThe mapping of the class attributes of Student to attributes of the relation student\ncan be speci\ufb01ed in a mapping \ufb01le, in an XML format, or more conveniently, by means\nof annotations of the Java code. In the example above, the annotation @Entity denotes\nthat the class is mapped to a database relation, whose name by default is the class name,\nand whose attributes are by default the same as the class attributes. The default relation\nname and attribute names can be overridden using @Table and@Column annotations.\nThe@Idannotation in the example speci\ufb01es that IDis the primary key attribute.\nThe following code snippet then creates a Student object and saves it to the\ndatabase.\nSession session = getSessionFactory().openSession();\nTransaction txn = session.beginTransaction();\nStudent stud = new Student(\"12328\", \"John Smith\", \"Comp. Sci.\", 0);\nsession.save(stud);\ntxn.commit();\nsession.close();\n", "461": "9.6 Application Architectures 433\nHibernate automatically generates the required SQL insert statement to create a student\ntuple in the database.\nObjects can be retrieved either by primary key or by a query, as illustrated in the\nfollowing code snippet:\nSession session = getSessionFactory().openSession();\nTransaction txn = session.beginTransaction();\n// Retrieve student object by identifier\nStudent stud1 = session.get(Student.class, \"12328\");\n.. print out the Student information ..\nList students =\nsession.createQuery(\"from Student as s order by s.ID asc\").list();\nfor ( Iterator iter = students.iterator(); iter.hasNext(); ) {\nStudent stud = (Student) iter.next();\n.. print out the Student information ..\n}\ntxn.commit();\nsession.close();\nA single object can be retrieved using the session.get() method by providing its\nclass and its primary key. The retrieved object can be updated in memory; when\nthe transaction on the ongoing Hibernate s ession is committed, Hibernate automat-\nically saves the updated objects by making corresponding updates on relations in the\ndatabase.\nThe preceding code snippet also shows a query in Hibernate\u2019s HQL query language,\nwhich is based on SQL but designed to allow objects to be used directly in the query.\nThe HQL query is automatically translated to SQL by Hibernate and executed, and the\nresults are converted into a list of Student objects. The forloop iterates over the objects\nin this list.\nThese features help to provide the programmer with a high-level model of data\nwithout bothering about the details of the rel ational storage. However, Hibernate, like\nother object-relational mapping systems, also allows queries to be written using SQL on\nthe relations stored in the database; such direct database access, bypassing the object\nmodel, can be quite useful for writing complex queries.\n9.6.2.2 The Django ORM\nSeveral ORM s have been developed for the Python language. The ORM component of\nthe Django framework is one of the most popular such ORM s, while SQLA lchemy is\nanother popular Python ORM .\nFigure 9.15 shows a model de\ufb01nition for Student andInstructor in Django. Ob-\nserve that all of the \ufb01elds of student and instructor have been de\ufb01ned as \ufb01elds in the\nclass Student andInstructor , with appropriate type de\ufb01nitions.\nIn addition, the relation advisor has been modeled here as a many-to-many rela-\ntionship between Student andInstructor . The relationship is accessed by an attribute\n", "462": "434 Chapter 9 Application Development\nfrom django.db import models\nclass student(models.Model):\nid = models.CharField(primary\n key=True, max\n length=5)\nname = models.CharField(max\n length=20)\ndept\n name = models.CharField(max\n length=20)\ntot\ncred = models.DecimalField(max\n digits=3, decimal\n places=0)\nclass instructor(models.Model):\nid = models.CharField(primary\n key=True, max\n length=5)\nname = models.CharField(max\n length=20)\ndept\n name = models.CharField(max\n length=20)\nsalary = models.DecimalField(max\n digits=8, decimal\n places=2)\nadvisees = models.ManyToManyField(student, related\n name=\"advisors\")\nFigure 9.15 Model definition in Django.\ncalled advisees inInstructor , which stores a set of references to Student objects. The\nreverse relationship from Student toInstructor is created automatically, and the model\nspeci\ufb01es that the reverse relationship attribute in the Student class is named advisors ;\nthis attribute stores a set of references to Instructor objects.\nThe Django view person\n query\n model shown in Figure 9.16 illustrates how to ac-\ncess database objects directly from the Python language, without using SQL.T h ee x p r e s -\nsion Student.objects.filter() returns all student objects that satisfy the speci\ufb01ed \ufb01lter\ncondition; in this case, students with the given name. The student names are printed out\nalong with the names of their advisors. The expression Student.advisors.all() returns\na list of advisors (advisor objects) of a given student, whose names are then retrieved\na n dr e t u r n e db yt h e get\nnames() function. The case for instructors is similar, with\ninstructor names being printed out along with the names of their advisees.\nDjango provides a tool called migrate , which creates database relations from a given\nmodel. Models can be given version numbe rs. When migrate is invoked on a model\nwith a new version number, while an earlier version number is already in the database,\nthe migrate tool also generates SQL code for migrating the existing data from the old\ndatabase schema to the new database schema. It is also possible to create Django mod-\nels from existing database schemas.\n9.7 Application Performance\nWeb sites may be accessed by millions of people from across the globe, at rates of\nthousands of requests per second, or even more, for the most popular sites. Ensuring\n", "463": "9.7 Application Performance 435\nfrom models import Student, Instructor\ndef get\n names(persons):\nres = \"\"\nfor p in persons:\nres += p.name + \", \"\nreturn res.rstrip(\", \")\ndef person\n query\n model(request):\npersontype = request.GET.get(\u2019persontype\u2019)\npersonname = request.GET.get(\u2019personname\u2019)\nhtml = \"\"\nif persontype == \u2019student\u2019:\nstudents = Student.objects.filter(name=personname)\nfor student in students:\nadvisors = students.advisors.all()\nhtml = html + \"Advisee: \" + student.name + \"<br>Advisors: \"\n+g e t\n names(advisors) + \" <br>\u2216n\"\nelse:\ninstructors = Instructor.objects.filter(name=personname)\nfor instructor in instructors:\nadvisees = instructor.advisees.all()\nhtml = html+\"Advisor: \" + instructor.name + \"<br>Advisees: \"\n+g e t\n names(advisees) + \" <br>\u2216n\"\nreturn HttpResponse(html)\nFigure 9.16 View definition in Django using models.\nthat requests are served with low response times is a major challenge for web-site de-\nvelopers. To do so, application developers try to speed up the processing of individual\nrequests by using techniques such as caching, and they exploit parallel processing by\nusing multiple application servers. We describe these techniques brie\ufb02y next. Tuning\nof database applications is another way to improve performance and is described in\nSection 25.1.\n9.7.1 Reducing Overhead by Caching\nSuppose that the application code for servicing each user request connects to a\ndatabase through JDBC .C r e a t i n gan e w JDBC connection may take several millisec-\nonds, so opening a new connection for each user request is not a good idea if very high\ntransaction rates are to be supported.\n", "464": "436 Chapter 9 Application Development\nTheconnection pooling method is used to reduce this overhead; it works as follows:\nThe connection pool manager (typically a part of the application server) creates a pool\n(that is, a set) of open ODBC /JDBC connections. Instead of opening a new connection\nto the database, the code servicing a user request (typically a servlet) asks for (requests)\na connection from the connection pool and returns the connection to the pool when the\ncode (servlet) completes its processing. If the pool has no unused connections when a\nconnection is requested, a new connection is opened to the database (taking care not\nto exceed the maximum number of connections that the database system can support\nconcurrently). If there are many open connections that have not been used for a while,\nthe connection pool manager may close some of the open database connections. Many\napplication servers and newer ODBC /JDBC drivers provide a built-in connection pool\nmanager.\nDetails of how to create a connection pool vary by application server or JDBC\ndriver, but most implementations require the creation of a DataSource object using the\nJDBC connection details such as the machine, port, database, user-id and password, as\nwell as other parameters related to connection pooling. The getConnection() method\ninvoked on the DataSource object gets a connection from the connection pool. Closing\nthe connection returns the connection to the pool.\nCertain requests may result in exactly the same query being resubmitted to the\ndatabase. The cost of communication with the database can be greatly reduced by\ncaching the results of earlier queries and reusing them, so long as the query result\nhas not changed at the database. Some web servers support such query-result caching;\ncaching can otherwise be done explicitly in application code.\nCosts can be further reduced by caching the \ufb01nal web page that is sent in response\nto a request. If a new request comes with exactly the same parameters as a previous\nrequest, the request does not perform any updates, and the resultant web page is in\nthe cache, that page can be reused to avoid the cost of recomputing the page. Caching\ncan be done at the level of fragments of web pages, which are then assembled to create\ncomplete web pages.\nCached query results and cached web pages are forms of materialized views. If the\nunderlying database data change, the cached results must be discarded, or recomputed,\nor even incrementally updated, as in materialized-view maintenance (described in Sec-\ntion 16.5). Some database systems (such as Microsoft SQL Server) provide a way for\nthe application server to register a query with the database and get a noti\ufb01cation from\nthe database when the result of the query changes. Such a noti\ufb01cation mechanism can\nbe used to ensure that query results cached at the application server are up-to-date.\nThere are several widely used main-memory caching systems; among the more pop-\nular ones are memcached andRedis . Both systems allow applications to store data with\nan associated key and retrieve data for a speci\ufb01ed key. Thus, they act as hash-map data\nstructures that allow data to be stored in the main memory but also provide cache\neviction of infrequently used data.\n", "465": "9.8 Application Security 437\nFor example, with memcached, data can be stored using memcached.add(key,\ndata) and fetched using memcached.fetch(key) . Instead of issuing a database query to\nf e t c hu s e rd a t aw i t has p e c i \ufb01 e dk e y ,s a y key1,f r o mar e l a t i o n r, an application would \ufb01rst\ncheck if the required data are already cached by issuing a fetch(\"r:\"+key1) (here, the\nkey is appended to the relation name, to distinguish data from di\ufb00erent relations that\nmay be stored in the same memcached instance). If the fetch returns null, the database\nquery is issued, a copy of the data fetched from the database is stored in memcached,\nand the data are then returned to the user. If the fetch does \ufb01nd the requested data, it\ncan be used without accessing the database, leading to much faster access.\nA client can connect to multiple memcached instances, which may run on di\ufb00er-\nent machines and store/retrieve data from any of them. How to decide what data are\nstored on which instance is left to the client code. By partitioning the data storage\nacross multiple machines, an application can bene\ufb01t from the aggregate main memory\navailable across all the machines.\nMemcached does not support automatic invalidation of cached data, but the ap-\nplication can track database changes and issue updates (using memcached\n set(key,\nnewvalue) ) or deletes (using memcached\n delete(key) ) for the key values a\ufb00ected by\nupdate or deletion in the database. Redis o\ufb00ers very similar functionality. Both mem-\ncached and Redis provide APIs in multiple languages.\n9.7.2 Parallel Processing\nA commonly used approach to handling such very heavy loads is to use a large number\nof application servers running in parallel, each handling a fraction of the requests. A\nweb server or a network router can be used to route each client request to one of the\napplication servers. All requests from a particular client session must go to the same\napplication server, since the server maintains state for a client session. This property\ncan be ensured, for example, by routing all requests from a particular IPaddress to\nthe same application server. The underlying database is, however, shared by all the\napplication servers, so users see a consistent view of the database.\nWhile the above architecture ensures that application servers do not become bot-\ntlenecks, it cannot prevent the database from becoming a bottleneck, since there is only\none database server. To avoid overloading the database, application designers often use\ncaching techniques to reduce the number of requests to the database. In addition, par-\nallel database systems, described in Chapter 21 through Chapter 23, are used when\nthe database needs to handle very large amounts of data, or a very large query load.\nParallel data storage systems that are accessible via web service APIs are also popular\nin applications that need to scale to a very large number of users.\n9.8 Application Security\nApplication security has to deal with several security threats and issues beyond those\nhandled by SQL authorization.\n", "466": "438 Chapter 9 Application Development\nThe \ufb01rst point where security has to be enforced is in the application. To do so,\napplications must authenticate users and ensure that users are only allowed to carry\nout authorized tasks.\nThere are many ways in which an application\u2019s security can be compromised, even\nif the database system is itself secure, due to badly written application code. In this\nsection, we \ufb01rst describe several security loopholes that can permit hackers to carry\nout actions that bypass the authentication and authorization checks carried out by the\napplication, and we explain how to prevent such loopholes. Later in the section, we\ndescribe techniques for secure authentication, and for \ufb01ne-grained authorization. We\nthen describe audit trails that can help in recovering from unauthorized access and\nfrom erroneous updates. We conclude the section by describing issues in data privacy.\n9.8.1 SQL Injection\nInSQL injection attacks, the attacker manages to get an application to execute an SQL\nquery created by the attacker. In Section 5.1.1.5, we saw an example of an SQL injection\nvulnerability if user inputs are concatenated directly with an SQL query and submitted\nto the database. As another example of SQL injection vulnerability, consider the form\nsource text shown in Figure 9.3. Suppose the corresponding servlet shown in Figure\n9.7 creates an SQL query string using the following Java expression:\nString query = \u201cselect * from student where name like \u2019%\u201d\n+name+\u201c%\u2019 \u201d\nwhere name is a variable containing the string input by the user, and then executes the\nquery on the database. A malicious attacker using the web form can then type a string\nsuch as \u201c \u2019;<some SQLstatement >;\u2013\u2013\u201d, where <some SQLstatement >denotes any\nSQL statement that the attacker desires, in place of a valid student name. The servlet\nwould then execute the following string.\ns e l e c t*f r o ms t u d e n tw h e r en a m el i k e' % ' ; <some SQL statement >;\u2013\u2013% \u2019\nThe quote inserted by the attacker closes the string, the following semicolon terminates\nthe query, and the following text inserted by the attacker gets interpreted as a second\nSQL query, while the closing quote has been commented out. Thus, the malicious user\nhas managed to insert an arbitrary SQL statement that is executed by the application.\nThe statement can cause signi\ufb01cant damage, since it can perform any action on the\ndatabase, bypassing all security measures implemented in the application code.\nAs discussed in Section 5.1.1.5, to avoid such attacks, it is best to use prepared\nstatements to execute SQL q u e r i e s .W h e ns e t t i n gap a r a m e t e ro fap r e p a r e dq u e r y ,\nJDBC automatically adds escape characters so that the user-supplied quote would no\nlonger be able to terminate the string. Equivalently, a function that adds such escape\n", "467": "9.8 Application Security 439\ncharacters could be applied on input strings before they are concatenated with the SQL\nquery, instead of using prepared statements.\nAnother source of SQL-injection risk comes from applications that create queries\ndynamically, based on selection conditions and ordering attributes speci\ufb01ed in a form.\nFor example, an application may allow a user to specify what attribute should be used\nfor sorting the results of a query. An appropriate SQL query is constructed, based on\nthe attribute speci\ufb01ed. Suppose the application takes the attribute name from a form,\nin the variable orderAttribute , and creates a query string such as the following:\nString query = \u201cselect * from takes order by \u201d + orderAttribute;\nA malicious user can send an arbitrary string in place of a meaningful orderAt-\ntribute value, even if the HTML form used to get the input tried to restrict the allowed\nvalues by providing a menu. To avoid this kind of SQL injection, the application should\nensure that the orderAttribute variable value is one of the allowed values (in our ex-\nample, attribute names) before appending it.\n9.8.2 Cross-Site Scripting and Request Forgery\nA web site that allows users to enter text, such as a comment or a name, and then stores\nit and later displays it to other users, is potentially vulnerable to a kind of attack called a\ncross-site scripting (XSS) attack. In such an attack, a malicious user enters code written\nin a client-side scripting language such as JavaScript or Flash instead of entering a valid\nname or comment. When a di\ufb00erent user views the entered text, the browser executes\nthe script, which can carry out actions such as sending private cookie information back\nto the malicious user or even executing an action on a di\ufb00erent web server that the user\nmay be logged into.\nFor example, suppose the user happens to be logged into her bank account at the\ntime the script executes. The script could send cookie information related to the bank\naccount login back to the malicious user, who could use the information to connect to\nthe bank\u2019s web server, fooling it into believing that the connection is from the original\nuser. Or the script could access appropriate pages on the bank\u2019s web site, with appro-\npriately set parameters, to execute a money transfer. In fact, this particular problem\ncan occur even without scripting by simply using a line of code such as\n<img src=\n\"https://mybank.com/transferm oney?amount=1000&toaccount=14523\" >\nassuming that the URL mybank.com/transfermoney accepts the speci\ufb01ed parameters\nand carries out a money transfer. This latter kind of vulnerability is also called cross-site\nrequest forgery orXSRF (sometimes also called CSRF ).\nXSSc a nb ed o n ei no t h e rw a y s ,s u c ha sl u r i n gau s e ri n t ov i s i t i n gaw e bs i t et h a th a s\nmalicious scripts embedded in its pages. There are other more complex kinds of XSS\n", "468": "440 Chapter 9 Application Development\nand XSRF attacks, which we shall not get into here. To protect against such attacks,\ntwo things need to be done:\n\u2022Prevent your web site from being used to launch XSSorXSRF attacks.\nThe simplest technique is to disallow any HTML tags whatsoever in text input by\nusers. There are functions that detect or strip all such tags. These functions can\nbe used to prevent HTML tags, and as a result, any scripts, from being displayed to\nother users. In some cases HTML formatting is useful, and in that case functions\nthat parse the text and allow limited HTML constructs but disallow other dangerous\nconstructs can be used instead; these must be designed carefully, since something\nas innocuous as an image include could potentially be dangerous in case there is\na bug in the image display software that can be exploited.\n\u2022Protect your web site from XSSorXSRF attacks launched from other sites.\nIf the user has logged into your web site and visits a di\ufb00erent web site vulnerable\ntoXSS, the malicious code executing on the user\u2019s browser could execute actions\non your web site or pass session information related to your web site back to the\nmalicious user, who could try to exploit it. This cannot be prevented altogether,\nbut you can take a few steps to minimize the risk.\n\u00b0The HTTP protocol allows a server to check the referer of a page access, that\nis, the URL of the page that had the link that the user clicked on to initiate the\npage access. By checking that the referer is valid, for example, that the referer\nURL is a page on the same web site, XSSattacks that originated on a di\ufb00erent\nweb page accessed by the user can be prevented.\n\u00b0Instead of using only the cookie to identify a session, the session could also\nbe restricted to the IPaddress from which it was originally authenticated. As a\nresult, even if a malicious user gets a cookie, he may not be able to log in from\na di\ufb00erent computer.\n\u00b0Never use a GET method to perform any updates. This prevents attacks using\n<img src .. >such as the one we saw earlier. In fact, the HTTP standard speci\ufb01es\nthatGET methods should not perform any updates.\n\u00b0If you use a web application framework like Django, make sure to use the\nXSRF /CSRF protection mechanisms provided by the framework.\n9.8.3 Password Leakage\nAnother problem that application developers must deal with is storing passwords in\nclear text in the application code. For example, programs such as JSPscripts often\ncontain passwords in clear text. If such scripts are stored in a directory accessible by\na web server, an external user may be able to access the source code of the script and\nget access to the password for the database account used by the application. To avoid\nsuch problems, many application servers provide mechanisms to store passwords in\n", "469": "9.8 Application Security 441\nencrypted form, which the server decrypts before passing it on to the database. Such a\nfeature removes the need for storing passwords as clear text in application programs.\nHowever, if the decryption key is also vulnerable to being exposed, this approach is not\nfully e\ufb00ective.\nAs another measure against compromised database passwords, many database sys-\ntems allow access to the database to be restricted to a given set of internet addresses,\ntypically, the machines running the application servers. Attempts to connect to the\ndatabase from other internet addresses are rejected. Thus, unless the malicious user\nis able to log into the application server, she cannot do any damage even if she gains\naccess to the database password.\n9.8.4 Application-Level Authentication\nAuthentication refers to the task of verifying the identity of a person/software connect-\ning to an application. The simplest form of authentication consists of a secret password\nthat must be presented when a user connects to the application. Unfortunately, pass-\nwords are easily compromised, for example, by guessing, or by sni\ufb03ng of packets on\nthe network if the passwords are not sent encrypted. More robust schemes are needed\nfor critical applications, such as online bank accounts. Encryption is the basis for more\nrobust authentication schemes. Authentication through encryption is addressed in Sec-\ntion 9.9.3.\nMany applications use two-factor authentication , where two independent factors\n(i.e., pieces of information or processes) are used to identify a user. The two factors\nshould not share a common vulnerability; for example, if a system merely required\ntwo passwords, both could be vulnerable to leakage in the same manner (by network\nsni\ufb03ng, or by a virus on the computer used by the user, for example). While biometrics\nsuch as \ufb01ngerprints or iris scanners can be used in situations where a user is physically\npresent at the point of authentication, they are not very meaningful across a network.\nPasswords are used as the \ufb01rst factor in most such two-factor authentication\nschemes. Smart cards or other encryption devices connected through the USB inter-\nface, which can be used for authentication based on encryption techniques (see Section\n9.9.3), are widely used as second factors.\nOne-time password devices, which generate a new pseudo-random number (say)\nevery minute are also widely used as a second factor. Each user is given one of these\ndevices and must enter the number displayed by the device at the time of authenti-\ncation, along with the password, to authenticate himself. Each device generates a dif-\nferent sequence of pseudo-random numbers. The application server can generate the\nsame sequence of pseudo-random numbers as the device given to the user, stopping at\nthe number that would be displayed at the time of authentication, and verify that the\nnumbers match. This scheme requires that the clock in the device and at the server are\nsynchronized reasonably closely.\nYet another second-factor approach is to send an SMS with a (randomly generated)\none-time password to the user\u2019s phone (whose number is registered earlier) whenever\n", "470": "442 Chapter 9 Application Development\nthe user wishes to log in to the application. The user must possess a phone with that\nnumber to receive the SMS and then enter the one-time password, along with her regular\npassword, to be authenticated.\nIt is worth noting that even with two-factor authentication, users may still be vulner-\nable to man-in-the-middle attacks . In such attacks, a user attempting to connect to the\napplication is diverted to a fake web site, which accepts the password (including second\nfactor passwords) from the user and uses it immediately to authenticate to the original\napplication. The HTTPS protocol, described in Section 9.9.3.2, is used to authenticate\nthe web site to the user (so the user does not connect to a fake site believing it to be the\nintended site). The HTTPS protocol also encrypts data and prevents man-in-the-middle\nattacks.\nWhen users access multiple web sites, it is often annoying for a user to have to\nauthenticate herself to each site separately, often with di\ufb00erent passwords on each site.\nThere are systems that allow the user to authenticate herself to one central authenti-\ncation service, and other web sites and applications can authenticate the user through\nthe central authentication service; the same password can then be used to access mul-\ntiple sites. The LDAP protocol is widely used to implement such a central point of\nauthentication for applications within a single organization; organizations implement\nanLDAP server containing user names and password information, and applications\nuse the LDAP server to authenticate users.\nIn addition to authenticating users, a central authentication service can provide\nother services, for example, providing information about the user such as name, email,\nand address information, to the application. This obviates the need to enter this infor-\nmation separately in each application. LDAP can be used for this task, as described\nin Section 25.5.2. Other directory systems such Microsoft\u2019s Active Directories also\nprovide mechanisms for authenticating users as well as for providing user information.\nAsingle sign-on system further allows the user to be authenticated once, and mul-\ntiple applications can then verify the user\u2019s identity through an authentication service\nwithout requiring reauthentication. In other words, once a user is logged in at one site,\nhe does not have to enter his user name and password at other sites that use the same\nsingle sign-on service. Such single sign-on mechanisms have long been used in network\nauthentication protocols such as Kerberos, and implementations are now available for\nweb applications.\nThe Security Assertion Markup Language (SAML ) is a protocol for exchanging\nauthentication and authorization information between di\ufb00erent security domains, to\nprovide cross-organization single sign-on. For example, suppose an application needs\nto provide access to all students from a particular university, say Yale. The university\ncan set up a web-based service that carries out authentication. Suppose a user connects\nto the application with a username such as \u201cjoe@yale.edu\u201d. The application, instead of\ndirectly authenticating a user, diverts the user to Yale University\u2019s authentication ser-\nvice, which authenticates the user and then tells the application who the user is and\n", "471": "9.8 Application Security 443\nmay provide some additional information such as the category of the user (student or\ninstructor) or other relevant information. The user\u2019s password and other authentication\nfactors are never revealed to the application, and the user need not register explicitly\nwith the application. However, the application must trust the university\u2019s authentica-\ntion service when authenticating a user.\nThe OpenID protocol is an alternative for single sign-on across organizations,\nwhich works in a manner similar to SAML .T h e OAuth protocol is another protocol\nthat allows users to authorize access to certain resources, via sharing of an authoriza-\ntion token.\n9.8.5 Application-Level Authorization\nAlthough the SQL standard supports a fairly \ufb02exible system of authorization based on\nroles (described in Section 4.7), the SQL authorization model plays a very limited role\nin managing user authorizations in a typical application. For instance, suppose you\nwant all students to be able to see their own grades, but not the grades of anyone else.\nSuch authorization cannot be speci\ufb01ed in SQL for at least two reasons:\n1.Lack of end-user information . With the growth in the web, database accesses come\nprimarily from web application servers. The end users typically do not have indi-\nvidual user identi\ufb01ers on the database itself, and indeed there may only be a single\nuser identi\ufb01er in the database corresponding to all users of an application server.\nThus, authorization speci\ufb01cation in SQL cannot be used in the above scenario.\nIt is possible for an application server to authenticate end users and then pass\nthe authentication information on to the database. In this section we will assume\nthat the function syscontext.user\n id() returns the identi\ufb01er of the application user\non whose behalf a query is being executed.6\n2.Lack of \ufb01ne-grained authorization . Authorization must be at the level of individual\ntuples if we are to authorize students to see only their own grades. Such autho-\nr i z a t i o ni sn o tp o s s i b l ei nt h ec u r r e n t SQL standard, which permits authorization\nonly on an entire relation or view, or on spe ci\ufb01ed attributes of relations or views.\nWe could try to get around this limitation by creating for each student a view\non the takes relation that shows only that student\u2019s grades. While this would work\nin principle, it would be extremely cumbersome since we would have to create one\nsuch view for every single student enrolled in the university, which is completely\nimpractical.7\nAn alternative is to create a view of the form\n6In Oracle, a JDBC connection using Oracle\u2019s JDBC drivers can set the end user identi\ufb01er using the method\nOracleConnection.setClientIdentifier(userId) ,a n da n SQL query can use the function sys\ncontext('USERENV',\n'CLIENT\n IDENTIFIER') to retrieve the user identi\ufb01er.\n7Database systems are designed to manage large relations but to manage schema information such as views in a way\nthat assumes smaller data volumes so as to enhance overall performance.\n", "472": "444 Chapter 9 Application Development\ncreate view studentTakes as\nselect *\nfrom takes\nwhere takes .ID=syscontext.user\n id()\nUsers are then given authorization to this view, rather than to the underlying takes\nrelation. However, queries executed on behalf of students must now be written on\nthe view studentTakes , rather than on the original takes relation, whereas queries\nexecuted on behalf of instructors may need to use a di\ufb00erent view. The task of\ndeveloping applications becomes more complex as a result.\nThe task of authorization is often typically carried out entirely in the application,\nbypassing the authorization facilities of SQL. At the application level, users are autho-\nrized to access speci\ufb01c interfaces, and they may further be restricted to view or update\ncertain data items only.\nWhile carrying out authorization in the application gives a great deal of \ufb02exibility\nto application developers, there are problems, too.\n\u2022The code for checking authorization becomes intermixed with the rest of the ap-\nplication code.\n\u2022Implementing authorization through application code, rather than specifying it\ndeclaratively in SQL, makes it hard to ensure the absence of loopholes. Because\nof an oversight, one of the application programs may not check for authorization,\nallowing unauthorized users access to con\ufb01dential data.\nVerifying that all application programs make all required authorization checks involves\nreading through all the application-server code, a formidable task in a large system. In\nother words, applications have a very large \u201csurface area,\u201d making the task of protecting\nthe application signi\ufb01cantly harder. And in fact, security loopholes have been found in\na variety of real-life applications.\nIn contrast, if a database directly supported \ufb01ne-grained authorization, authoriza-\ntion policies could be speci\ufb01ed and enforced at the SQL level, which has a much smaller\nsurface area. Even if some of the application interfaces inadvertently omit required\nauthorization checks, the SQL-level authorization could prevent unauthorized actions\nfrom being executed.\nSome database systems provide mechanisms for row-level authorization as we saw\nin Section 4.7.7. For example, the Oracle Virtual Private Database (VPD) allows a sys-\ntem administrator to associate a function with a relation; the function returns a predi-\ncate that must be added to any query that uses the relation (di\ufb00erent functions can be\nde\ufb01ned for relations that are being updated). For example, using our syntax for retriev-\ning application user identi\ufb01ers, the function for the takes relation can return a predicate\nsuch as:\n", "473": "9.8 Application Security 445\nID=sys\ncontext.user\n id()\nThis predicate is added to the where clause of every query that uses the takes relation.\nAs a result (assuming that the application program sets the user\n idvalue to the student\u2019s\nID), each student can see only the tuples corresponding to courses that she took.\nAs we discussed in Section 4.7.7, a poten tial pitfall with adding a predicate as\ndescribed above is that it may change the meaning of a query. For example, if a user\nwrote a query to \ufb01nd the average grade over all courses, she would end up getting the\naverage of hergrades, not all grades. Although the system would give the \u201cright\u201d answer\nfor the rewritten query, that answer would not correspond to the query the user may\nhave thought she was submitting.\nPostgre SQL and Microsoft SQL S erver o\ufb00er row-level authorization support with\nsimilar functionality to Oracle VPD. More information on Oracle VPD andPostgre SQL\nandSQL S erver row-level authorization may be found in their respective system manu-\nals available online.\n9.8.6 Audit Trails\nAnaudit trail is a log of all changes (inserts, deletes, and updates) to the application\ndata, along with information such as which user performed the change and when the\nchange was performed. If application security is breached, or even if security was not\nbreached, but some update was carried out erroneously, an audit trail can (a) help \ufb01nd\nout what happened, and who may have carried out the actions, and (b) aid in \ufb01xing\nthe damage caused by the security breach or erroneous update.\nFor example, if a student\u2019s grade is found to be incorrect, the audit log can be\nexamined to locate when and how the grade was updated, as well as to \ufb01nd which user\ncarried out the updates. The university could then also use the audit trail to trace all the\nupdates performed by this user in order to \ufb01nd other incorrect or fraudulent updates,\nand then correct them.\nAudit trails can also be used to detect security breaches where a user\u2019s account is\ncompromised and accessed by an intruder. For example, each time a user logs in, she\nmay be informed about all updates in the audit trail that were done from that login\nin the recent past; if the user sees an update that she did not carry out, it is likely the\naccount has been compromised.\nIt is possible to create a database-level audit trail by de\ufb01ning appropriate triggers on\nrelation updates (using system-de\ufb01ned variables that identify the user name and time).\nHowever, many database systems provide built-in mechanisms to create audit trails\nthat are much more convenient to use. Details of how to create audit trails vary across\ndatabase systems, and you should refer to the database-system manuals for details.\nDatabase-level audit trails are usually insu\ufb03cient for applications, since they are\nusually unable to track who was the end user of the application. Further, updates are\nrecorded at a low level, in terms of updates to tuples of a relation, rather than at a\nhigher level, in terms of the business logic. Applications, therefore, usually create a\n", "474": "446 Chapter 9 Application Development\nhigher-level audit trail, recording, for example, what action was carried out, by whom,\nwhen, and from which IPaddress the request originated.\nA related issue is that of protecting the audit trail itself from being modi\ufb01ed or\ndeleted by users who breach application security. One possible solution is to copy the\naudit trail to a di\ufb00erent machine, to which the intruder would not have access, with\neach record in the trail copied as soon as it is generated. A more robust solution is to\nuse blockchain techniques, which are described in Chapter 26; blockchain techniques\nstore logs in multiple machines and use a hashing mechanism that makes it very di\ufb03cult\nfor an intruder to modify or delete data without being detected.\n9.8.7 Privacy\nIn a world where an increasing amount of personal data are available online, people\nare increasingly worried about the privacy of their data. For example, most people\nwould want their personal medical data to be kept private and not revealed publicly.\nHowever, the medical data must be made available to doctors and emergency medical\ntechnicians who treat the patient. Many countries have laws on privacy of such data\nthat de\ufb01ne when and to whom the data may be revealed. Violation of privacy law can\nresult in criminal penalties in some countries. Applications that access such private\ndata must be built carefully, keeping the privacy laws in mind.\nOn the other hand, aggregated private data can play an important role in many\ntasks such as detecting drug side e\ufb00ects, or in detecting the spread of epidemics. How to\nmake such data available to researchers carrying out such tasks without compromising\nthe privacy of individuals is an important real-world problem. As an example, suppose\na hospital hides the name of the patient but provides a researcher with the date of birth\nand the postal code of the patient (both of which may be useful to the researcher).\nJust these two pieces of information can be used to uniquely identify the patient in\nmany cases (using information from an external database), compromising his privacy.\nIn this particular situation, one solution would be to give the year of birth but not the\ndate of birth, along with the address, which may su\ufb03ce for the researcher. This would\nnot provide enough information to uniquely identify most individuals.8\nAs another example, web sites often collect personal data such as address, tele-\nphone, email, and credit-card information. Such information may be required to carry\no u tat r a n s a c t i o ns u c ha sp u r c h a s i n ga ni t e mf r o mas t o r e .H o w e v e r ,t h ec u s t o m e rm a y\nnot want the information to be made available to other organizations, or may want part\nof the information (such as credit-card numbers) to be erased after some period of time\nas a way to prevent it from falling into unauthorized hands in the event of a security\nbreach. Many web sites allow customers to specify their privacy preferences, and those\nweb sites must then ensure that these preferences are respected.\n8For extremely old people, who are relatively rare, even the year of birth plus postal code may be enough to uniquely\nidentify the individual, so a range of values, such as 90 yea rs or older, may be provided instead of the actual age for\npeople older than 90 years.\n", "475": "9.9 Encryption and Its Applications 447\n9.9 Encryption and Its Applications\nEncryption refers to the process of transforming data into a form that is unreadable,\nunless the reverse process of decryption is applied. Encryption algorithms use an en-\ncryption key to perform encryption, and they require a decryption key (which could\nbe the same as the encryption key, depending on the encryption algorithm used) to\nperform decryption.\nThe oldest uses of encryption were for transmitting messages, encrypted using a\nsecret key known only to the sender and the intended receiver. Even if the message is\nintercepted by an enemy, the enemy, not knowing the key, will not be able to decrypt and\nunderstand the message. Encryption is widely used today for protecting data in transit\nin a variety of applications such as data transfer on the internet, and on cell-phone\nnetworks. Encryption is also used to carry out other tasks, such as authentication, as\nwe will see in Section 9.9.3.\nIn the context of databases, encryption is used to store data in a secure way, so\nthat even if the data are acquired by an unauthorized user (e.g., a laptop computer\ncontaining the data is stolen), the data will not be accessible without a decryption key.\nMany databases today store sensitive customer information, such as credit-card\nnumbers, names, \ufb01ngerprints, signatures, and identi\ufb01cation numbers such as, in the\nUnited States, social security numbers. A criminal who gets access to such data can\nuse them for a variety of illegal activities, such as purchasing goods using a credit-card\nnumber, or even acquiring a credit card in someone else\u2019s name. Organizations such as\ncredit-card companies use knowledge of personal information as a way of identifying\nwho is requesting a service or goods. Leakage of such personal information allows a\ncriminal to impersonate someone else and get access to service or goods; such imper-\nsonation is referred to as identity theft . Thus, applications that store such sensitive data\nmust take great care to protect them from theft.\nTo reduce the chance of sensitive information being acquired by criminals, many\ncountries and states today require by law that any database storing such sensitive in-\nformation must store the information in an encrypted form. A business that does not\nprotect its data thus could be held criminally liable in case of data theft. Thus, encryp-\ntion is a critical component of any application that stores such sensitive information.\n9.9.1 Encryption Techniques\nThere are a vast number of techniques for the encryption of data. Simple encryption\ntechniques may not provide adequate security, since it may be easy for an unauthorized\nuser to break the code. As an example of a weak encryption technique, consider the\nsubstitution of each character with the next character in the alphabet. Thus,\nPerryridge\nbecomes\nQfsszsjehf\n", "476": "448 Chapter 9 Application Development\nIf an unauthorized user sees only \u201cQfsszsjehf,\u201d she probably has insu\ufb03cient infor-\nmation to break the code. However, if the intruder sees a large number of encrypted\nbranch names, she could use statistical data regarding the relative frequency of char-\nacters to guess what substitution is being made (for example, Eis the most common\nletter in English text, followed by T, A, O, N, I ,a n ds oo n ) .\nA good encryption technique has the following properties:\n\u2022It is relatively simple for authorized users to encrypt and decrypt data.\n\u2022It depends not on the secrecy of the algorithm, but rather on a parameter of the al-\ngorithm called the encryption key , which is used to encrypt data. In a symmetric-key\nencryption technique, the encryption key is also used to decrypt data. In contrast,\ninpublic-key (also known as asymmetric-key ) encryption techniques, there are two\ndi\ufb00erent keys, the public key and the private key, used to encrypt and decrypt the\ndata.\n\u2022Its decryption key is extremely di\ufb03cult for an intruder to determine, even if the\nintruder has access to encrypted data. In the case of asymmetric-key encryption,\nit is extremely di\ufb03cult to infer the private key even if the public key is available.\nTheAdvanced Encryption Standard (AES) is a symmetric-key encryption algorithm\nthat was adopted as an encryption standard by the U.S. government in 2000 and is now\nwidely used. The standard is based on the Rijndael algorithm (named for the inventors\nV. Rijmen and J. Daemen). The algorithm operates on a 128-bit block of data at a time,\nwhile the key can be 128, 192, or 256 bits in length. The algorithm runs a series of steps\nto jumble up the bits in a data block in a way that can be reversed during decryption,\nand it performs an XOR operation with a 128-bit \u201cround key\u201d that is derived from the\nencryption key. A new round key is generated from the encryption key for each block\nof data that is encrypted. During decryption, the round keys are generated again from\nthe encryption key and the encryption process is reversed to recover the original data.\nAn earlier standard called the Data Encryption Standard (DES), adopted in 1977, was\nvery widely used earlier.\nFor any symmetric-key encryption scheme to work, authorized users must be pro-\nvided with the encryption key via a secure mechanism. This requirement is a major\nweakness, since the scheme is no more secure than the security of the mechanism by\nwhich the encryption key is transmitted.\nPublic-key encryption is an alternative scheme that avoids some of the problems\nfaced by symmetric-key encryption techniques. It is based on two keys: a public key and\naprivate key .E a c hu s e r Uihas a public key Eiand a private key Di. All public keys are\npublished: They can be seen by anyone. Each private key is known to only the one user\nto whom the key belongs. If user U1wants to store encrypted data, U1encrypts them\nusing public key E1. Decryption requires the private key D1.\nBecause the encryption key for each user is public, it is possible to exchange infor-\nmation securely by this scheme. If user U1wants to share data with U2,U1encrypts\n", "477": "9.9 Encryption and Its Applications 449\nthe data using E2, the public key of U2.S i n c eo n l yu s e r U2knows how to decrypt the\ndata, information can be transferred securely.\nFor public-key encryption to work, there must be a scheme for encryption such\nthat it is infeasible (that is, extremely hard) to deduce the private key, given the public\nkey. Such a scheme does exist and is based on these conditions:\n\u2022There is an e\ufb03cient algorithm for testing whether or not a number is prime.\n\u2022No e\ufb03cient algorithm is known for \ufb01nding the prime factors of a number.\nFor purposes of this scheme, data are treated as a collection of integers. We create a\npublic key by computing the product of two large prime numbers: P1andP2.T h ep r i v a t e\nkey consists of the pair ( P1,P2). The decryption algorithm cannot be used successfully\nif only the product P1P2is known; it needs the individual values P1andP2.S i n c ea l lt h a t\nis published is the product P1P2, an unauthorized user would need to be able to factor\nP1P2to steal data. By choosing P1and P2to be su\ufb03ciently large (over 100 digits),\nwe can make the cost of factoring P1P2prohibitively high (on the order of years of\ncomputation time, on even the fastest computers).\nThe details of public-key encryption and the mathematical justi\ufb01cation of this tech-\nnique\u2019s properties are referenced in the bibliographical notes.\nAlthough public-key encryption by this scheme is secure, it is also computationally\nvery expensive. A hybrid scheme widely used for secure communication is as follows:\na symmetric encryption key (based, for example, on AES) is randomly generated and\nexchanged in a secure manner using a public-key encryption scheme, and symmetric-\nkey encryption using that key is used on the data transmitted subsequently.\nEncryption of small values, such as identi\ufb01ers or names, is made complicated by\nthe possibility of dictionary attacks , particularly if the encryption key is publicly avail-\nable. For example, if date-of-birth \ufb01elds are encrypted, an attacker trying to decrypt a\nparticular encrypted value ecould try encrypting every possible date of birth until he\n\ufb01nds one whose encrypted value matches e. Even if the encryption key is not publicly\navailable, statistical information about data distributions can be used to \ufb01gure out what\nan encrypted value represents in some cases, such as age or address. For example, if\nthe age 18 is the most common age in a database, the encrypted age value that occurs\nmost often can be inferred to represent 18.\nDictionary attacks can be deterred by adding extra random bits to the end of the\nvalue before encryption (and removing them after decryption). Such extra bits, referred\nto as an initialization vector inAES,o ra s saltbits in other contexts, provide good\nprotection against dictionary attack.\n9.9.2 Encryption Support in Databases\nMany \ufb01le systems and database systems today support encryption of data. Such en-\ncryption protects the data from someone who is able to access the data but is not able\nto access the decryption key. In the case of \ufb01le-system encryption, the data to be en-\ncrypted are usually large \ufb01les and directories containing information about \ufb01les.\n", "478": "450 Chapter 9 Application Development\nIn the context of databases, encryption can be done at several di\ufb00erent levels. At\nthe lowest level, the disk blocks containing database data can be encrypted, using a\nkey available to the database-system software. When a block is retrieved from disk, it\nis \ufb01rst decrypted and then used in the usual fashion. Such disk-block-level encryption\nprotects against attackers who can access the disk contents but do not have access to\nthe encryption key.\nAt the next higher level, speci\ufb01ed (or all) attributes of a relation can be stored in\nencrypted form. In this case, each attribute of a relation could have a di\ufb00erent encryp-\ntion key. Many databases today support encryption at the level of speci\ufb01ed attributes\nas well as at the level of an entire relation, or all relations in a database. Encryption\nof speci\ufb01ed attributes minimizes the overhead of decryption by allowing applications\nto encrypt only attributes that contain sensitive values such as credit-card numbers.\nEncryption also then needs to use extra random bits to prevent dictionary attacks, as\ndescribed earlier. However, databases typically do not allow primary and foreign key\nattributes to be encrypted, and they do not support indexing on encrypted attributes.\nA decryption key is obviously required to get access to encrypted data. A single\nmaster encryption key may be used for all the encrypted data; with attribute level en-\ncryption, di\ufb00erent encryption keys could be used for di\ufb00erent attributes. In this case,\nthe decryption keys for di\ufb00erent attributes can be stored in a \ufb01le or relation (often\nreferred to as \u201cwallet\u201d), which is itself encrypted using a master key.\nA connection to the database that needs to access encrypted attributes must then\nprovide the master key; unless this is provided, the connection will not be able to access\nencrypted data. The master key would be stored in the application program (typically\non a di\ufb00erent computer), or memorized by the database user, and provided when the\nuser connects to the database.\nEncryption at the database level has the advantage of requiring relatively low time\nand space overhead and does not require modi\ufb01cation of applications. For example, if\ndata in a laptop computer database need to be protected from theft of the computer\nitself, such encryption can be used. Similarly, someone who gets access to backup tapes\nof a database would not be able to access the data contained in the backups without\nknowing the decryption key.\nAn alternative to performing encryption in the database is to perform it before\nthe data are sent to the database. The application must then encrypt the data before\nsending it to the database and decrypt the data when they are retrieved. This approach\nto data encryption requires signi\ufb01cant modi\ufb01cations to be done to the application,\nunlike encryption performed in a database system.\n9.9.3 Encryption and Authentication\nPassword-based authentication is used widely by operating systems as well as database\nsystems. However, the use of passwords has some drawbacks, especially over a network.\nIf an eavesdropper is able to \u201csni\ufb00\u201d the data being sent over the network, she may be\nable to \ufb01nd the password as it is being sent across the network. Once the eavesdropper\n", "479": "9.9 Encryption and Its Applications 451\nhas a user name and password, she can connect to the database, pretending to be the\nlegitimate user.\nA more secure scheme involves a challenge-response system. The database system\nsends a challenge string to the user. The user encrypts the challenge string using a secret\npassword as encryption key and then returns the result. The database system can verify\nthe authenticity of the user by decrypting the string with the same secret password and\nchecking the result with the original challenge string. This scheme ensures that no\npasswords travel across the network.\nPublic-key systems can be used for encryption in challenge\u2013response systems. The\ndatabase system encrypts a challenge string using the user\u2019s public key and sends it to\nthe user. The user decrypts the string using her private key and returns the result to\nthe database system. The database system then checks the response. This scheme has\nthe added bene\ufb01t of not storing the secret password in the database, where it could\npotentially be seen by system administrators.\nStoring the private key of a user on a computer (even a personal computer) has\nthe risk that if the computer is compromised, the key may be revealed to an attacker\nwho can then masquerade as the user. Smart cards provide a solution to this problem.\nIn a smart card, the key can be stored on an embedded chip; the operating system of\nthe smart card guarantees that the key can never be read, but it allows data to be sent\nto the card for encryption or decryption, using the private key.9\n9.9.3.1 Digital Signatures\nAnother interesting application of public-key encryption is in digital signatures to verify\nauthenticity of data; digital signatures play the electronic role of physical signatures on\ndocuments. The private key is used to \u201csign,\u201d that is, encrypt, data, and the signed data\nc a nb em a d ep u b l i c .A n y o n ec a nv e r i f yt h es i g n a t u r eb yd e c r y p t i n gt h ed a t au s i n gt h e\npublic key, but no one could have generated the signed data without having the private\nkey. (Note the reversal of the roles of the public and private keys in this scheme.) Thus,\nwe can authenticate the data; that is, we can verify that the data were indeed created\nby the person who is supposed to have created them.\nFurthermore, digital signatures also serve to ensure nonrepudiation . That is, in case\nthe person who created the data later claims she did not create them (the electronic\nequivalent of claiming not to have signed the check), we can prove that that person\nmust have created the data (unless her private key was leaked to others).\n9.9.3.2 Digital Certi\ufb01cates\nAuthentication is, in general, a two-way process, where each of a pair of interacting\nentities authenticates itself to the other. Such pairwise authentication is needed even\n9Smart cards provide other functionality too, such as the ability to store cash digitally and make payments, which is\nnot relevant in our context.\n", "480": "452 Chapter 9 Application Development\nwhen a client contacts a web site, to prevent a malicious site from masquerading as a\nlegal web site. Such masquerading could be done, for example, if the network routers\nwere compromised and data rerouted to the malicious site.\nFor a user to ensure that she is interacting with an authentic web site, she must\nhave the site\u2019s public key. This raises the problem of how the user can get the public\nkey\u2014if it is stored on the web site, the malicious site could supply a di\ufb00erent key, and\nthe user would have no way of verifying if the supplied public key is itself authentic.\nAuthentication can be handled by a system of digital certi\ufb01cates , whereby public keys\nare signed by a certi\ufb01cation agency, whose public key is well known. For example, the\npublic keys of the root certi\ufb01cation authorities are stored in standard web browsers. A\ncerti\ufb01cate issued by them can be veri\ufb01ed by using the stored public keys.\nA two-level system would place an excessive burden of creating certi\ufb01cates on the\nroot certi\ufb01cation authorities, so a multilevel system is used instead, with one or more\nroot certi\ufb01cation authorities and a tree of certi\ufb01cation authorities below each root.\nEach authority (other than the root authorities) has a digital certi\ufb01cate issued by its\nparent.\nA digital certi\ufb01cate issued by a certi\ufb01cation authority Aconsists of a public key KA\nand an encrypted text Ethat can be decoded by using the public key KA.T h ee n c r y p t e d\ntext contains the name of the party to whom the certi\ufb01cate was issued and her public\nkeyKc. In case the certi\ufb01cation authority Ais not a root certi\ufb01cation authority, the\nencrypted text also contains the digital certi\ufb01cate issued to Aby its parent certi\ufb01cation\nauthority; this certi\ufb01cate authenticates the key KAitself. (That certi\ufb01cate may in turn\ncontain a certi\ufb01cate from a further parent authority, and so on.)\nTo verify a certi\ufb01cate, the encrypted text Eis decrypted by using the public key\nKAto retrieve the name of the party (i.e., the name of the organization owning the\nweb site); additionally, if Ais not a root authority whose public key is known to the\nveri\ufb01er, the public key KAis veri\ufb01ed recursively by using the digital certi\ufb01cate con-\ntained within E; recursion terminates when a certi\ufb01cate issued by the root authority is\nreached. Verifying the certi\ufb01cate establishes the chain through which a particular site\nwas authenticated and provides the name and authenticated public key for the site.\nDigital certi\ufb01cates are widely used to authenticate web sites to users, to prevent ma-\nlicious sites from masquerading as other web sites. In the HTTPS protocol (the secure\nversion of the HTTP protocol), the site provides its di gital certi\ufb01cate to the browser,\nwhich then displays it to the user. If the user accepts the certi\ufb01cate, the browser then\nuses the provided public key to encrypt data. A malicious site will have access to the\ncerti\ufb01cate, but not the private key, and will thus not be able to decrypt the data sent\nby the browser. Only the authentic site, which has the corresponding private key, can\ndecrypt the data sent by the browser. We note that public-/private-key encryption and\ndecryption costs are much higher than encryption/decryption costs using symmetric\nprivate keys. To reduce encryption costs, HTTPS actually creates a one-time symmetric\nkey after authentication and uses it to encrypt data for the rest of the session.\n", "481": "9.10 Summary 453\nDigital certi\ufb01cates can also be used for au thenticating users. The user must submit\na digital certi\ufb01cate containing her public key to a site, which veri\ufb01es that the certi\ufb01cate\nhas been signed by a trusted authority. The user\u2019s public key can then be used in a\nchallenge-response system to ensure that the user possesses the corresponding private\nkey, thereby authenticating the user.\n9.10 Summary\n\u2022Application programs that use databases as back ends and interact with users have\nbeen around since the 1960s. Application architectures have evolved over this pe-\nriod. Today most applications use web browsers as their front end, and a database\nas their back end, with an application server in between.\n\u2022HTML provides the ability to de\ufb01ne interfaces that combine hyperlinks with forms\nfacilities. Web browsers communicate with web servers by the HTTP protocol. Web\nservers can pass on requests to application programs and return the results to the\nbrowser.\n\u2022Web servers execute application programs to implement desired functionality.\nServlets are a widely used mechanism to write application programs that run as\npart of the web server process, in order to reduce overhead. There are also many\nserver-side scripting languages that are interpreted by the web server and provide\napplication-program functionality as part of the web server.\n\u2022There are several client-side scripting languages\u2014JavaScript is the most widely\nused\u2014that provide richer user interaction at the browser end.\n\u2022Complex applications usually have a multilayer architecture, including a model\nimplementing business logic, a controller, and a view mechanism to display results.\nThey may also include a data access layer that implements an object-relational\nmapping. Many applications implement and use web services, allowing functions\nto be invoked over HTTP .\n\u2022Techniques such as caching of various forms, including query result caching and\nconnection pooling, and parallel processing are used to improve application per-\nformance.\n\u2022Application developers must pay careful attention to security, to prevent attacks\nsuch as SQL injection attacks and cross-site scripting attacks.\n\u2022SQL authorization mechanisms are coarse grained and of limited value to appli-\ncations that deal with large numbers of users. Today application programs imple-\nment \ufb01ne-grained, tuple-level authorization, dealing with a large number of ap-\nplication users, completely outside the database system. Database extensions to\nprovide tuple-level access control and to deal with large numbers of application\nusers have been developed, but are not standard as yet.\n", "482": "454 Chapter 9 Application Development\n\u2022Protecting the privacy of data are an important task for database applications.\nMany countries have legal requirements on protection of certain kinds of data,\nsuch as credit-card information or medical data.\n\u2022Encryption plays a key role in protecting information and in authentication of\nusers and web sites. Symmetric-key encryption and public-key encryption are two\ncontrasting but widely used approaches to encryption. Encryption of certain sen-\nsitive data stored in databases is a legal requirement in many countries and states.\n\u2022Encryption also plays a key role in authentication of users to applications, of Web\nsites to users, and for digital signatures.\nReview Terms\n\u2022Application programs\n\u2022Web interfaces to databases\n\u2022HTML\n\u2022Hyperlinks\n\u2022Uniform resource locator ( URL)\n\u2022Forms\n\u2022HyperText Transfer Protocol\n(HTTP )\n\u2022Connectionless protocols\n\u2022Cookie\n\u2022Session\n\u2022Servlets and Servlet sessions\n\u2022Server-side scripting\n\u2022Java Server Pages ( JSP)\n\u2022PHP\n\u2022Client-side scripting\n\u2022JavaScript\n\u2022Document Object Model ( DOM )\n\u2022Ajax\n\u2022Progressive Web Apps\n\u2022Application architecture\n\u2022Presentation layer\n\u2022Model-view-controller ( MVC )\narchitecture\u2022Business-logic layer\n\u2022Data-access layer\n\u2022Object-relational mapping\n\u2022Hibernate\n\u2022Django\n\u2022Web services\n\u2022REST ful web services\n\u2022Web application frameworks\n\u2022Connection pooling\n\u2022Query result caching\n\u2022Application security\n\u2022SQL injection\n\u2022Cross-site scripting ( XSS)\n\u2022Cross-site request forgery ( XSRF )\n\u2022Authentication\n\u2022Two-factor authentication\n\u2022Man-in-the-middle attack\n\u2022Central authentication\n\u2022Single sign-on\n\u2022OpenID\n\u2022Authorization\n\u2022Virtual Private Database ( VPD)\n\u2022Audit trail\n", "483": "Practice Exercises 455\n\u2022Encryption\n\u2022Symmetric-key encryption\n\u2022Public-key encryption\n\u2022Dictionary attack\u2022Challenge\u2013response\n\u2022Digital signatures\n\u2022Digital certi\ufb01cates\nPractice Exercises\n9.1 What is the main reason why servlets give better performance than programs\nthat use the common gateway interface ( CGI), even though Java programs gen-\nerally run slower than C or C++ programs?\n9.2 List some bene\ufb01ts and drawbacks of connectionless protocols over protocols\nthat maintain connections.\n9.3 Consider a carelessly written web application for an online-shopping site, which\nstores the price of each item as a hidden form variable in the web page sent to\nthe customer; when the customer submits the form, the information from the\nhidden form variable is used to compute the bill for the customer. What is the\nloophole in this scheme? (There was a real instance where the loophole was\nexploited by some customers of an online- shopping site before the problem was\ndetected and \ufb01xed.)\n9.4 Consider another carelessly written web application which uses a servlet that\nchecks if there was an active session but does not check if the user is autho-\nrized to access that page, instead depending on the fact that a link to the page is\nshown only to authorized users. What is the risk with this scheme? (There was\na real instance where applicants to a college admissions site could, after logging\ninto the web site, exploit this loophole and view information they were not au-\nthorized to see; the unauthorized access was, however, detected, and those who\naccessed the information were punished by being denied admission.)\n9.5 Why is it important to open JDBC connections using the try-with-resources ( try\n(\u2026){\u2026})s y n t a x ?\n9.6 List three ways in which caching can be used to speed up web server perfor-\nmance.\n9.7 Thenetstat command (available on Linux and on Windows) shows the active\nnetwork connections on a computer. Explain how this command can be used to\n\ufb01nd out if a particular web page is not closing connections that it opened, or if\nconnection pooling is used, not returning connections to the connection pool.\nYou should account for the fact that with connection pooling, the connection\nmay not get closed immediately.\n", "484": "456 Chapter 9 Application Development\n9.8 Testing for SQL-injection vulnerability:\na. Suggest an approach for testing an application to \ufb01nd if it is vulnerable to\nSQL injection attacks on text input.\nb. Can SQL injection occur with forms of HTML input other than text boxes?\nIf so, how would you test for vulnerability?\n9.9 A database relation may have the values of certain attributes encrypted for se-\ncurity. Why do database systems not support indexing on encrypted attributes?\nUsing your answer to this question, explain why database systems do not allow\nencryption of primary-key attributes.\n9.10 Exercise 9.9 addresses the problem of encryption of certain attributes. However,\nsome database systems support encryption of entire databases. Explain how the\nproblems raised in Exercise 9.9 are avoided if the entire database is encrypted.\n9.11 Suppose someone impersonates a company and gets a certi\ufb01cate from a\ncerti\ufb01cate-issuing authority. What is the e\ufb00ect on things (such as purchase or-\nders or programs) certi\ufb01ed by the impersonated company, and on things certi-\n\ufb01ed by other companies?\n9.12 Perhaps the most important data items in any database system are the passwords\nthat control access to the database. Suggest a scheme for the secure storage\nof passwords. Be sure that your scheme allows the system to test passwords\nsupplied by users who are attempting to log into the system.\nExercises\n9.13 Write a servlet and associated HTML code for the following very simple appli-\ncation: A user is allowed to submit a form containing a value, say n,a n ds h o u l d\nget a response containing n\u201c*\u201d symbols.\n9.14 Write a servlet and associated HTML code for the following simple application:\nA user is allowed to submit a form containing a number, say n,a n ds h o u l dg e ta\nresponse saying how many times the value nhas been submitted previously. The\nnumber of times each value has been submitted previously should be stored in\na database.\n9.15 Write a servlet that authenticates a user (based on user names and passwords\nstored in a database relation) and sets a session variable called userid after au-\nthentication.\n9.16 What is an SQL injection attack? Explain how it works and what precautions\nmust be taken to prevent SQL injection attacks.\n9.17 Write pseudocode to manage a connection pool. Your pseudocode must include\na function to create a pool (providing a database connection string, database\nuser name, and password as parameters), a function to request a connection\n", "485": "Exercises 457\nfrom the pool, a connection to release a connection to the pool, and a function\nto close the connection pool.\n9.18 Explain the terms CRUD and REST .\n9.19 Many web sites today provide rich user interfaces using Ajax. List two features\neach of which reveals if a site uses Ajax, without having to look at the source\ncode. Using the above features, \ufb01nd three sites which use Ajax; you can view\ntheHTML source of the page to check if the site is actually using Ajax.\n9.20 XSSattacks:\na. What is an XSSattack?\nb. How can the referer \ufb01eld be used to detect some XSSattacks?\n9.21 What is multifactor authentication? How does it help safeguard against stolen\npasswords?\n9.22 Consider the Oracle Virtual Private Database ( VPD) feature described in Sec-\ntion 9.8.5 and an application based on our university schema.\na. What predicate (using a subquery) should be generated to allow each fac-\nulty member to see only takes tuples corresponding to course sections\nthat they have taught?\nb. Give an SQL query such that the query with the predicate added gives\na result that is a subset of the original query result without the added\npredicate.\nc. Give an SQL query such that the query with the predicate added gives\na result containing a tuple that is not in the result of the original query\nwithout the added predicate.\n9.23 What are two advantages of encrypting data stored in the database?\n9.24 Suppose you wish to create an audit trail of changes to the takes relation.\na. De\ufb01ne triggers to create an audit trail, logging the information into a re-\nlation called, for example, takes\n trail. The logged information should in-\nclude the user-id (assume a function user\n id() provides this information)\nand a timestamp, in addition to old and new values. You must also provide\nthe schema of the takes\n trailrelation.\nb. Can the preceding implementation guarantee that updates made by a ma-\nlicious database administrator (or someone who manages to get the ad-\nministrator\u2019s password) will be in the audit trail? Explain your answer.\n9.25 Hackers may be able to fool you into believing that their web site is actually a\nweb site (such as a bank or credit card web site) that you trust. This may be\ndone by misleading email, or even by breaking into the network infrastructure\n", "486": "458 Chapter 9 Application Development\nand rerouting network tra\ufb03c destined for, say mybank.com , to the hacker\u2019s\nsite. If you enter your user name and password on the hacker\u2019s site, the site can\nrecord it and use it later to break into your account at the real site. When you\nuse a URL such as https://mybank.com ,t h e HTTPS protocol is used to prevent\nsuch attacks. Explain how the protocol might use digital certi\ufb01cates to verify\nauthenticity of the site.\n9.26 Explain what is a challenge\u2013response system for authentication. Why is it more\nsecure than a traditional password-based system?\nProject Suggestions\nEach of the following is a large project, which can be a semester-long project done by\na group of students. The di\ufb03culty of the project can be adjusted easily by adding or\ndeleting features.\nYou can choose to use either a web front-end using HTML 5, or a mobile front-end\non Android or iOS for your project.\nProject 9.1 Pick your favorite interactive web site, such as Bebo, Blogger, Facebook,\nFlickr, Last.FM, Twitter, Wikipedia; these are just a few examples, there are many\nmore. Most of these sites manage a large amount of data and use databases to\nstore and process the data. Implement a subset of the functionality of the web\nsite you picked. Implementing even a signi\ufb01cant subset of the features of such a\nsite is well beyond a course project, but it is possible to \ufb01nd a set of features that\nis interesting to implement yet small enough for a course project.\nMost of today\u2019s popular web sites make extensive use of Javascript to create\nrich interfaces. You may wish to go easy on this for your project, at least initially,\nsince it takes time to build such interfaces, and then add more features to your\ninterfaces, as time permits.\nMake use of web application development frameworks, or Javascript libraries\navailable on the web, such as the jQuery library, to speed up your front-end de-\nvelopment. Alternatively, implement the application as a mobile app on Android\nor iOS.\nProject 9.2 Create a \u201cmashup\u201d which uses web services such as Google or Yahoo\nmap APIs to create an interactive web site. For example, the map APIsp r o v i d e\na way to display a map on the web page, with other information overlaid on the\nmaps. You could implement a restaurant recommendation system, with users\ncontributing information about restaurants such as location, cuisine, price range,\nand ratings. Results of user searches could be displayed on the map. You could\nallow Wikipedia-like features, such as a llowing users to add information and edit\n", "487": "Project Suggestions 459\ninformation added by other users, along with moderators who can weed out\nmalicious updates. You could also implement social features, such as giving more\nimportance to ratings provided by your friends.\nProject 9.3 Your university probably uses a course-management system such as Moo-\ndle, Blackboard, or WebCT. Implement a subset of the functionality of such a\ncourse-management system. For example, you can provide assignment submis-\nsion and grading functionality, including mechanisms for students and teach-\ners/teaching assistants to discuss gradi ng of a particular assignment. You could\nalso provide polls and other mechanisms for getting feedback.\nProject 9.4 Consider the E-Rschema of Practice Exercise 6.3 (Chapter 6), which rep-\nresents information about teams in a league. Design and implement a web-based\nsystem to enter, update, and view the data.\nProject 9.5 Design and implement a shopping cart system that lets shoppers collect\nitems into a shopping cart (you can decide what information is to be supplied\nfor each item) and purchased together. You can extend and use the E-Rschema\nof Exercise 6.21 of Chapter 6. You should check for availability of the item and\ndeal with nonavailable items as you feel appropriate.\nProject 9.6 Design and implement a web-based system to record student registration\nand grade information for courses at a university.\nProject 9.7 Design and implement a system that permits recording of course perfor-\nmance information\u2014speci\ufb01cally, the marks given to each student in each assign-\nment or exam of a course, and computation of a (weighted) sum of marks to\nget the total course marks. The number of assignments/exams should not be\nprede\ufb01ned; that is, more assignments/exams can be added at any time. The sys-\ntem should also support grading, permitting cuto\ufb00s to be speci\ufb01ed for various\ngrades.\nYou may also wish to integrate it with the student registration system of\nProject 9.6 (perhaps being implemented by another project team).\nProject 9.8 Design and implement a web-based system for booking classrooms at your\nuniversity. Periodic booking (\ufb01xed days/times each week for a whole semester)\nmust be supported. Cancellation of speci\ufb01c lectures in a periodic booking should\nalso be supported.\nYou may also wish to integrate it with the student registration system of\nProject 9.6 (perhaps being implemented by another project team) so that class-\nrooms can be booked for courses, and cancellations of a lecture or addition of\nextra lectures can be noted at a single interface and will be re\ufb02ected in the class-\nroom booking and communicated to students via email.\n", "488": "460 Chapter 9 Application Development\nProject 9.9 Design and implement a system for managing online multiple-choice tests.\nYou should support distributed contributi on of questions (by teaching assistants,\nfor example), editing of questions by whoever is in charge of the course, and\ncreation of tests from the available set of questions. You should also be able\nto administer tests online, either at a \ufb01xed time for all students or at any time\nbut with a time limit from start to \ufb01nish (support one or both), and the system\nshould give students feedback on their scores at the end of the allotted time.\nProject 9.10 Design and implement a system for managing email customer service.\nIncoming mail goes to a common pool. There is a set of customer service agents\nwho reply to email. If the email is part of an ongoing series of replies (tracked\nusing the in-reply-to \ufb01eld of email) the mail should preferably be replied to by\nthe same agent who replied earlier. The system should track all incoming mail\nand replies, so an agent can see the history of questions from a customer before\nreplying to an email.\nProject 9.11 Design and implement a simple electronic marketplace where items can\nbe listed for sale or for purchase under various categories (which should form a\nhierarchy). You may also wish to support alerting services, whereby a user can\nregister interest in items in a particular category, perhaps with other constraints\nas well, without publicly advertising her interest, and is noti\ufb01ed when such an\nitem is listed for sale.\nProject 9.12 Design and implement a web-based system for managing a sports \u201clad-\nder.\u201d Many people register and may be given some initial rankings (perhaps\nbased on past performance). Anyone can challenge anyone else to a match, and\nthe rankings are adjusted according to the result. One simple system for adjust-\ning rankings just moves the winner ahead of the loser in the rank order, in case\nthe winner was behind earlier. You can try to invent more complicated rank-\nadjustment systems.\nProject 9.13 Design and implement a publication-listing service. The service should\npermit entering of information about publications, such as title, authors, year,\nwhere the publication appeared, and pages. Authors should be a separate entity\nwith attributes such as name, institution, department, email, address, and home\npage.\nYour application should support multiple views on the same data. For in-\nstance, you should provide all publications by a given author (sorted by year, for\nexample), or all publications by authors from a given institution or department.\nYou should also support search by keywords, on the overall database as well as\nwithin each of the views.\nProject 9.14 A common task in any organization is to collect structured information\nfrom a group of people. For example, a manager may need to ask employees to\nenter their vacation plans, a professor may wish to collect feedback on a particu-\n", "489": "Project Suggestions 461\nlar topic from students, or a student organizing an event may wish to allow other\nstudents to register for the event, or someone may wish to conduct an online\nvote on some topic. Google Forms can be used for such activities; your task is\nto create something like Google Forms, but with authorization on who can \ufb01ll\naf o r m .\nSpeci\ufb01cally, create a system that will allow users to easily create information\ncollection events. When creating an event, the event creator must de\ufb01ne who is\neligible to participate; to do so, your system must maintain user information and\nallow predicates de\ufb01ning a subset of users. The event creator should be able to\nspecify a set of inputs (with types, default values, and validation checks) that the\nusers will have to provide. The event should have an associated deadline, and the\nsystem should have the ability to send reminders to users who have not yet sub-\nmitted their information. The event creator may be given the option of automatic\nenforcement of the deadline based on a speci\ufb01ed date/time, or choosing to login\nand declare the deadline is over. Statistics about the submissions should be gen-\nerated\u2014to do so, the event creator may be allowed to create simple summaries\non the entered information. The event creator may choose to make some of the\nsummaries public, viewable by all users, either continually (e.g., how many peo-\nple have responded) or after the deadline (e.g., what was the average feedback\nscore).\nProject 9.15 Create a library of functions to simplify creation of web interfaces, us-\ning jQuery. You must implement at least the following functions: a function to\ndisplay a JDBC result set (with tabular formatting), functions to create di\ufb00erent\ntypes of text and numeric inputs (with validation criteria such as input type and\noptional range, enforced at the client by appropriate JavaScript code), and func-\ntions to create menu items based on a result set. Also implement functions to get\ninput for speci\ufb01ed \ufb01elds of speci\ufb01ed relations, ensuring that database constraints\nsuch as type and foreign-key constraints are enforced at the client side. Foreign\nkey constraints can also be used to provide either autocomplete or drop-down\nmenus, to ease the task of data entry for the \ufb01elds.\nFor extra credit, use support CSSstyles which allow the user to change style\nparameters such as colors and fonts. Build a sample database application to\nillustrate the use of these functions.\nProject 9.16 Design and implement a web-based multiuser calendar system. The sys-\ntem must track appointments for each person, including multioccurrence events,\nsuch as weekly meetings, and shared events (where an update made by the event\ncreator gets re\ufb02ected to all those who share the event). Provide interfaces to\nschedule multiuser events, where an event creator can add a number of users\nwho are invited to the event. Provide email noti\ufb01cation of events. For extra cred-\nits implement a web service that can be used by a reminder program running on\nthe client machine.\n", "490": "462 Chapter 9 Application Development\nTools\nThere are several integrated development environments that provide support for web\napplication development. Eclipse ( www.eclipse.org ) and Netbeans ( netbeans.org )\nare popular open-source IDEs. IntelliJ IDEA (www.jetbrains.com/idea/ )i s\na popular commercial IDE which provides free licenses for students, teach-\ners and non-commercial open source projects. Microsoft\u2019s Visual Studio\n(visualstudio.microsoft.com ) also supports web application development. All\nthese IDEs support integration with applicatio n servers, to allow web applications to\nbe executed directly from the IDE.\nT h eA p a c h eT o m c a t( jakarta.apache.org ), Glass\ufb01sh\n(javaee.github.io/glassfish/ ), JBoss Enterprise Application Platform\n(developers.redhat.com/products/eap/overview/ ), WildFly ( wildfly.org )( w h i c hi s\nthe community edition of JBoss) and Caucho\u2019s Resin ( www.caucho.com ), are appli-\ncation servers that support servlets and JSP. The Apache web server ( apache.org )i s\nthe most widely used web server today. Microsoft\u2019s IIS(Internet Information Services)\nis a web and application server that is widely used on Microsoft Windows platforms,\nsupporting Microsoft\u2019s ASP.NET (msdn.microsoft.com/asp.net/ ).\nThe jQuery JavaScript library jquery.com is among the most widely used\nJavaScript libraries for creating interactive web interfaces.\nAndroid Studio ( developer.android.com/studio/ )i saw i d e l yu s e d IDE for de-\nveloping Android apps. XCode ( developer.apple.com/xcode/ ) from Apple and App-\nCode ( www.jetbrains.com/objc/ ) are popular IDEs for iOS application development.\nGoogle\u2019s Flutter framework ( flutter.io ), which is based on the Dart language, and Face-\nbook\u2019s React Native ( facebook.github.io/react-native/ ) which is based on Javascript,\nare frameworks that support cross-platform application development across Android\nand iOS.\nThe Open Web Application Security Project ( OWASP )(www.owasp.org )p r o -\nvides a variety of resources related to application security, including technical articles,\nguides, and tools.\nFurther Reading\nThe HTML tutorials at www.w3schools.com/html ,t h e CSS tutorials at\nwww.w3schools.com/css are good resources for learning HTML and CSS.At u t o r i a l\non Java Servlets can be found at docs.oracle.com/javaee/7/tutorial/servlets.htm .\nThe JavaScript tutorials at www.w3schools.com/js are an excellent source of learning\nmaterial on JavaScript. You can also learn more about JSON and Ajax as part of the\nJavaScript tutorial. The jQuery tutorial at www.w3schools.com/Jquery is a very good\nresource for learning how to use jQuery. These tutorials allow you to modify sample\ncode and test it in the browser, with no software download. Information about the\n", "491": "Further Reading 463\n.NET framework and about web application development using ASP.NET can be found\natmsdn.microsoft.com .\nYou can learn more about the Hibernate ORM and Django (including the\nDjango ORM ) from the tutorials and documentation at hibernate.org/orm and\ndocs.djangoproject.com respectively.\nThe Open Web Application Security Project ( OWASP )(www.owasp.org )p r o v i d e s\na variety of technical material such as the OWASP Testing Guide, the OWASP Top Ten\ndocument which describes critical security risks, and standards for application security\nveri\ufb01cation.\nThe concepts behind cryptographic hash functions and public-key encryption were\nintroduced in [Di\ufb03e and Hellman (1976)] and [Rivest et al. (1978)]. A good reference\nfor cryptography is [Katz and Lindell (2014)], while [Stallings (2017)] provides text-\nbook coverage of cryptography and network security.\nBibliography\n[Di\ufb03e and Hellman (1976)] W. Di\ufb03e and M. E. Hellman, \u201cNew Directions in Cryptogra-\nphy\u201d, IEEE Transactions on Information Theory , Volume 22, Number 6 (1976), pages 644\u2013\n654.\n[Katz and Lindell (2014)] J. Katz and Y. Lindell, Introduction to Modern Cryptography ,3 r d\nedition, Chapman and Hall/CRC (2014).\n[Rivest et al. (1978)] R. L. Rivest, A. Shamir, and L. Adleman, \u201cA Method for Obtaining\nDigital Signatures and Public-Key Cryptosystems\u201d, Communications of the ACM ,V o l u m e2 1 ,\nNumber 2 (1978), pages 120\u2013126.\n[Stallings (2017)] W. Stallings, Cryptography and Network Security - Principles and Practice ,\n7th edition, Pearson (2017).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "492": "", "493": "PART4\nBIG DATA ANALYTICS\nTraditional applications of relational databases are based on structured data and they\ndeal with data from a single enterprise. Modern data management applications often\nneed to deal with data that are not necessarily in relational form; further, such appli-\ncations also need to deal with volumes of data that are far larger than what a single\ntraditional organization would have generated. In Chapter 10, we study techniques for\nmanaging such data, often referred to as Big Data. Our coverage of Big Data in this\nchapter is from the perspective of a programmer who uses Big Data systems. We start\nwith storage systems for Big Data, and then cover querying techniques, including the\nMapReduce framework, algebraic operations, steaming data, and graph databases.\nOne major application of Big Data is data analytics, which refers broadly to the\nprocessing of data to infer patterns, correlations, or models for prediction. The \ufb01nancial\nbene\ufb01ts of making correct decisions can be substantial, as can the costs of making\nwrong decisions. Organizations therefore make substantial investments both to gather\nor purchase required data and to build systems for data analytics. In Chapter 11, we\ncover data analytics in general and, in particular, decision-making tasks that bene\ufb01t\ngreatly by using data about the past to predict the future and using the predictions to\nmake decisions. Topics covered include data warehousing, online analytical processing,\nand data mining.\n465\n", "494": "", "495": "CHAPTER10\nBig Data\nTraditional applications of relational databases are based on structured data, and they\ndeal with data from a single enterprise. Modern data management applications often\nneed to deal with data that are not necessarily in relational form; further, such appli-\ncations also need to deal with volumes of data that are far larger than what a single\nenterprise would generate. We study techniques for managing such data, often referred\nto as Big Data, in this chapter.\n10.1Motivation\nThe growth of the World Wide Web in the 1990s and 2000s resulted in the need to\nstore and query data with volumes that far exceeded the enterprise data that relational\ndatabases were designed to manage. Although much of the user-visible data on the web\nin the early days was static, web sites generated a very large amount of data about users\nwho visited their sites, what web pages they accessed, and when. These data were typi-\ncally stored on log \ufb01les on the web server, in textual form. People managing web sites\nsoon realized that there was a wealth of information in the web logs that could be used\nby companies to understand more about their users and to target advertisements and\nmarketing campaigns at users. Such information included details of which pages had\nbeen accessed by users, which could also be linked with user pro\ufb01le data, such as age,\ngender, income level, and so on, that were collected by many web sites. Transactional\nweb sites such as shopping sites had other kinds of data as well, such as what prod-\nucts a user had browsed or purchased. The 2000s saw exceptionally large growth in the\nvolume of user-generated data, in particular social-media data.\nThe volume of such data soon grew well beyond the scale that could be handled\nby traditional database systems, and both storage and processing require a very high\ndegree of parallelism. Furthermore, much of the data were in textual form such as\nlog records, or in other semi-structured forms that we saw in Chapter 8. Such data,\nare characterized by their size, speed at which they are generated, and the variety of\nformats, are generically called Big Data.\n467\n", "496": "468Chapter 10 Big Data\nBig Data has been contrasted with traditional relational databases on the following\nmetrics:\n\u2022Volume : The amount of data to be stored and processed is much larger than tradi-\ntional databases, including traditional parallel relational databases, were designed\nto handle. Although there is a long history of parallel database systems, early gen-\neration parallel databases were designed to work on tens to a few hundreds of\nmachines. In contrast, some of the new applications require the use of thousands\nof machines in parallel to store and process the data.\n\u2022Velocity : The rate of arrival of data are much higher in today\u2019s networked world\nthan in earlier days. Data management systems must be able to ingest and store\ndata at very high rates. Further, many applications need data items to be processed\nas they arrive, in order to detect and respond quickly to certain events (such sys-\ntems are referred to a streaming data systems ). Thus, processing velocity is very\nimportant for many applications today.\n\u2022Variety : The relational representation of data, relational query languages, and re-\nlational database systems have been very successful over the past several decades,\nand they form the core of the data representation of most organizations. However,\nclearly, not all data are relational.\nAs we saw in Chapter 8, a variety of data representations are used for di\ufb00er-\nent purposes today. While much of today\u2019s data can be e\ufb03ciently represented in\nrelational form, there are many data sources that have other forms of data, such\nas semi-structured data, textual data, and graph data. The SQL query language is\nwell suited to specifying a variety of queries on relational data, and it has been\nextended to handle semi-structured data. However, many computations cannot be\neasily expressed in SQL or e\ufb03ciently evaluated if represented using SQL.\nA new generation of languages and frameworks has been developed for speci-\nfying and e\ufb03ciently executing complex queries on new forms of data.\nWe shall use the term Big Data in a generic sense, to refer to any data-processing\nneed that requires a high degree of parallelism to handle, regardless of whether the data\nare relational or otherwise.\nOver the past decade, several systems have been developed for storing and process-\ning Big Data, using very large clusters of machines, with thousands, or in some cases,\ntens of thousands of machines. The term node is often used to refer to a machine in a\ncluster.\n10.1.1 Sources and Uses of Big Data\nThe rapid growth of the web was the key driver for the enormous growth of data vol-\numes in the late 1990s and early 2000s. The initial sources of data were logs from web\nserver software, which recorded user interactions with the web servers. With each user\n", "497": "10.1 Motivation 469\nclicking on multiple links each day, and hundreds of millions of users, which later grew\nto billions of users, the large web companies found they were generating multiple ter-\nabytes of data each day. Web companies soon realized that there was a lot of important\ninformation in the web logs, which could be used for multiple purposes, such as these:\n\u2022Deciding what posts, news, and other information to present to which user, to keep\nthem more engaged with the site. Information on what the user had viewed earlier,\nas well as information on what other users with similar preferences had viewed,\nare key to making these decisions.\n\u2022Deciding what advertisements to show to which users, to maximize the bene\ufb01t to\nthe advertiser, while also ensuring the advertisements that a user sees are more\nlikely to be of relevance to the user. Again, information on what pages a user had\nvisited, or what advertisements a user had clicked on earlier, are key to making\nsuch decisions.\n\u2022Deciding how a web site should be structured, to make it easy for most users to\n\ufb01nd information that they are looking for. Knowing to what pages users typically\nnavigate, and what page they typically view after visiting a particular page, is key\nto making such decisions.\n\u2022Determining user preferences and trends based on page views, which can help\nam a n u f a c t u r e ro rv e n d o rd e c i d ew h a ti t e m st op r o d u c eo rs t o c km o r eo f ,a n d\nwhat to produce or stock less of. This is part of a more general topic of business\nintelligence.\n\u2022Advertisement display and click-through information. A click-through refers to a\nuser clicking on an advertisement to get more information, and is a measure of\nthe success of the advertisement in getting user attention. A conversion occurs\nwhen the user actually purchases the advertised product or service. Web sites are\noften paid when a click-through or conversion occurs. This makes click-through\nand conversion rates for di\ufb00erent advertisements a key metric for a site to decide\nwhich advertisements to display.\nToday, there are many other sources of very high-volume data. Examples include\nthe following:\n\u2022Data from mobile phone apps that help in understanding user interaction with\nthe app, in the same way that clicks on a web site help in understanding user\ninteraction with the web site.\n\u2022Transaction data from retain enterprises (both online and o\ufb04ine). Early users\nof very large volumes of data included large retail chains such as Walmart, who\nused parallel database systems even in the years preceding the web, to manage and\nanalyze their data.\n", "498": "470Chapter 10 Big Data\n\u2022Data from sensors. High-end equipment today typically has a large number of sen-\nsors to monitor the health of the equipment. Collecting such data centrally helps\nto track status and predict the chances of problems with the equipment, helping\n\ufb01x problems before they result in failure. The increasing use of such sensors to\nthe connection of sensors and other computing devices embedded within other\nobjects such as vehicles, buildings, machinery, and so forth to the internet, often\nreferred to as the internet of things . The number of such devices is now more than\nthe number of humans on the internet.\n\u2022Metadata from communication networks, including tra\ufb03c and other monitoring\ninformation for data networks, and call information for voice networks. Such data\nare important for detecting potential problems before they occur, for detecting\nproblems as they occur, and for capacity planning and other related decisions.\nThe amount of data stored in databases has been growing rapidly for multiple\ndecades, well before the term Big Data came into use. But the extremely rapid growth\nof the web created an in\ufb02ection point, with the major web sites having to handle data\ngenerated by hundreds of millions to billions of users; this was a scale signi\ufb01cantly\ngreater than most of the earlier applications.\nEven companies that are not web related have found it necessary to deal with very\nlarge amounts of data. Many companies procure and analyze large volumes of data\ngenerated by other companies. For example, web search histories annotated with user\npro\ufb01le information, have become available to many companies, which can use such\ninformation to make a variety of business decisions, such as planning advertising cam-\npaigns, planning what products to manufacture and when, and so on.\nCompanies today \ufb01nd it essential to make use of social media data to make business\ndecisions. Reactions to new product launches by a company, or a change in existing of-\nferings can be found on Twitter and other social media sites. Not only is the volume\nof data on social media sites such as Twitter very high, but the data arrives at a very\nhigh velocity, and needs to be analyzed and responded to very quickly. For example, if\na company puts out an advertisement, and there is strong negative reaction on Twitter,\nthe company would want to detect the issue quickly, and perhaps stop using the adver-\ntisement before there is too much damage. Thus, Big Data has become a key enabler\nfor a variety of activities of many organizations today.\n10.1.2 Querying Big Data\nSQL is by far the most widely used language for querying relational databases. However,\nthere is a wider variety of query language options for Big Data applications, driven by\nthe need to handle more variety of data types, and by the need to scale to very large\ndata volumes/velocity.\nBuilding data management systems that can scale to a large volume/velocity of\ndata requires parallel storage and processing of data. Building a relational database\nthat supports SQL along with other database features, such as transactions (which we\n", "499": "10.1Motivation 471\nstudy later in Chapter 17), and at the same time can support very high performance\nby running on a very large number of machines, is not an easy task. There are two\ncategories of such applications:\n1.Transaction-processing systems that need very high scalability :T r a n s a c t i o n -\nprocessing systems support a large number of short running queries and updates.\nIt is much easier for a database designed to support transaction processing to\nscale to very large numbers of machines if the requirements to support all features\nof a relational database are relaxed. Conversely, many transaction-processing ap-\nplications that need to scale to very high volumes/velocity can manage without\nfull database support.\nThe primary mode of data access for such applications is to store data with an\nassociated key, and to retrieve data with that key; such a storage system is called a\nkey-value store. In the preceding user pro\ufb01le example, the key for user-pro\ufb01le data\nwould be the user\u2019s identi\ufb01er. There are applications that conceptually require\njoins but implement the joins either in application code or by a form of view\nmaterialization.\nFor example, in a social-networking application, when a user connects to the\nsystem, the user should be shown new posts from all her friends. If the data\nabout posts and friends is maintained in relational format, this would require\na join. Suppose that instead, the system maintains an object for each user in a\nkey-value store, containing their friend information as well as their posts. Instead\nof a join done in the database, the application code could implement the join\nby \ufb01rst \ufb01nding the set of friends of the user, and then querying the data object\nof each friend to \ufb01nd their posts. Another alternative is as follows: whenever a\nuser u0makes a post, for each friend uiof the user, a message is sent to the data\nobject representing ui, and the data associated with the friend are updated with a\nsummary of the new post. When that user uichecks for updates, all data required\nto provide a summary view of posts by friends are available in one place and can\nbe retrieved quickly.\nThere are trade-o\ufb00s between the two alternatives, such as higher cost at query\ntime for the \ufb01rst alternative, versus higher storage cost and higher cost at the time\nof writes for the second alternative. 1But both approaches allow the application\nto carry out its tasks without support for joins in the key-value storage system.\n2.Query processing systems that need very high scalability, and need to support non-\nrelational data : Typical examples of such systems are those designed to perform\nanalysis on logs generated by web servers and other applications. Other examples\ninclude document and knowledge storage and indexing systems, such as those\nthat support keyword search on the web.\n1It is worth mentioning that it appears (based on limited publicly available information as of 2018) that Facebook uses\nthe \ufb01rst alternative for its news feed to avoid the high storage overhead of the second alternative.\n", "500": "472Chapter 10 Big Data\nThe data consumed by many such applications are stored in multiple \ufb01les. A\nsystem designed to support such applications \ufb01rst needs to be able to store a\nlarge number of large \ufb01les. Second, it must be able to support parallel querying\nof data stored in such \ufb01les. Since the data are not necessarily relational, a system\ndesigned for querying such data must support arbitrary program code, not just\nrelational algebra or SQL queries.\nBig Data applications often require proce ssing of very large volumes of text, image,\nand video data. Traditionally such data were stored in \ufb01le systems and processed using\nstand-alone applications. For example, keyword search on textual data, and its succes-\nsor, keyword search on the web, both depend on preprocessing textual data, followed by\nquery processing using data structures such as indices built during the preprocessing\nstep. It should be clear that the SQL constructs we have seen earlier are not suited for\ncarrying out such tasks, since the input data are not in relational form, and the output\ntoo may not be in relational form.\nIn earlier days, processing of such data was done using stand-alone programs; this is\nvery similar to how organizational data were processed prior to the advent of database\nmanagement systems. However, with the very rapid growth of data sizes, the limitations\nof stand-alone programs became clear. Parallel processing is critical given the very large\nscale of Big Data. Writing programs that can process data in parallel while dealing with\nfailures (which are common with large scale parallelism) is not easy.\nIn this chapter, we study techniques for querying of Big Data that are widely used\ntoday. A key to the success of these techniques is the fact that they allow speci\ufb01cation of\ncomplex data processing tasks, while enabling easy parallelization of the tasks. These\ntechniques free the programmer from having to deal with issues such as how to perform\nparallelization, how to deal with failures, how to deal with load imbalances between\nmachines, and many other similar low-level issues.\n10.2 Big Data Storage Systems\nApplications on Big Data have extremely high scalability requirements. Popular appli-\ncations have hundreds of millions of users, and many applications have seen their load\nincrease many-fold within a single year, or even within a few months. To handle the\ndata management needs of such applications, data must be stored partitioned across\nthousands of computing and storage nodes.\nA number of systems for Big Data storage have been developed and deployed over\nthe past two decades to address the data management requirements of such applica-\ntions. These include the following:\n\u2022Distributed File Systems . These allow \ufb01les to be stored across a number of ma-\nchines, while allowing access to \ufb01les using a traditional \ufb01le-system interface. Dis-\ntributed \ufb01le systems are used to store large \ufb01les, such as log \ufb01les. They are also\nused as a storage layer for systems that support storage of records.\n", "501": "10.2 Big Data Storage Systems 473\n\u2022Sharding across multiple databases. Sharding refers to the process of partition-\ning of records across multiple systems; in other words, the records are divided up\namong the systems. A typical use case for sharding is to partition records corre-\nsponding to di\ufb00erent users across a collection of databases. Each database is a\ntraditional centralized database, which may not have any information about the\nother databases. It is the job of client software to keep track of how records are\npartitioned, and to send each query to the appropriate database.\n\u2022Key-Value Storage Systems . These allow records to be stored and retrieved based\non a key, and may additionally provide limited query facilities. However, they are\nnot full-\ufb02edged database systems; they are sometimes called No SQL systems, since\nsuch storage systems typically do not support the SQL language.\n\u2022Parallel and Distributed Databases . These provide a traditional database interface\nbut store data across multiple machines, and they perform query processing in\nparallel across multiple machines.\nParallel and distributed database storage systems, including distributed \ufb01le systems and\nkey-value stores, are described in detail in Chapter 21. We provide a user-level overview\nof these Big Data storage systems in this section.\n10.2.1 Distributed File Systems\nAdistributed \ufb01le system stores \ufb01les across a large collection of machines while giving a\nsingle-\ufb01le-system view to clients. As with any \ufb01le system, there is a system of \ufb01le names\nand directories, which clients can use to identify and access \ufb01les. Clients do not need\nto bother about where the \ufb01les are stored. Such distributed \ufb01le systems can store very\nlarge amounts of data, and support very large numbers of concurrent clients. Such\nsystems are ideal for storing unstructured data, such as web pages, web server logs,\nimages, and so on, that are stored as large \ufb01les.\nA landmark system in this context was the Google File System (GFS), developed\nin the early 2000s, which saw widespread use within Google. The open-source Hadoop\nFile System (HDFS) is based on the GFS architecture and is now very widely used.\nDistributed \ufb01le systems are designed for e\ufb03cient storage of large \ufb01les, whose sizes\nrange from tens of megabytes to hundreds of gigabytes or more.\nThe data in a distributed \ufb01le system is stored across a number of machines. Files are\nbroken up into multiple blocks. The blocks of a single \ufb01le can be partitioned across mul-\ntiple machines. Further, each \ufb01le block is replicated across multiple (typically three)\nmachines, so that a machine failure does not result in the \ufb01le becoming inaccessible.\nFile systems, whether centralized or distributed, typically support the following:\n\u2022A directory system, which allows a hierarchical organization of \ufb01les into directo-\nries and subdirectories.\n\u2022A mapping from a \ufb01le name to the sequence of identi\ufb01ers of blocks that store the\nactual data in each \ufb01le.\n", "502": "474Chapter 10 Big Data\n\u2022The ability to store and retrieve data to/from a block with a speci\ufb01ed identi\ufb01er.\nIn the case of a centralized \ufb01le system, the block identi\ufb01ers help locate blocks in a\nstorage device such as a disk. In the case of a distributed \ufb01le system, in addition to pro-\nviding a block identi\ufb01er, the \ufb01le system must provide the location (machine identi\ufb01er)\nwhere the block is stored; in fact, due to replication, the \ufb01le system provides a set of\nmachine identi\ufb01ers along with each block identi\ufb01er.\nFigure 10.1 shows the architecture of the Hadoop File System ( HDFS ), which is\nderived from the architecture of the Google File System ( GFS) .T h ec o r eo f HDFS is\nNameNode\nRack 1 Rack 2ClientMetadata Ops\nDataNodes\nBlocksBlock Read\nBlock Write\nReplicationMetadata (name, replicas, ..)\nBackupNode\nMetadata (name, replicas, ..)\nClient\nFigure 10.1 Hadoop Distributed File System (HDFS) architecture.\n", "503": "10.2 Big Data Storage Systems 475\na server running a machine referred to as the NameNode . All \ufb01le system requests are\nsent to the NameNode. A \ufb01le system client program that wants to read an existing\n\ufb01le sends the \ufb01le name (which can be a path, such as /home/avi/book/ch10 )t ot h e\nNameNode. The NameNode stores a list of block identi\ufb01ers of the blocks in each \ufb01le;\nfor each block identi\ufb01er, the NameNode also stores the identi\ufb01ers of machines that\nstore copies of that block. The machines that store data blocks in HDFS are called\nDataNodes .\nFor a \ufb01le read request, the HDFS server sends back a list of block identi\ufb01ers of\nthe blocks in the \ufb01le and the identi\ufb01ers of the machines that contain each block. Each\nblock is then fetched from one of the machines that store a copy of the block.\nFor a \ufb01le write, the HDFS server creates new block identi\ufb01ers and assigns each\nblock identi\ufb01er to several (usually three) machines, and returns the block identi\ufb01ers\nand machine assignment to the client. The client then sends the block identi\ufb01ers and\nblock data to the assigned machines, which store the data.\nFiles can be accessed by programs by using HDFS \ufb01le system APIs that are available\nin multiple languages, such as Java and Python; the APIs allow a program to connect\nto the HDFS server and access data.\nAnHDFS distributed \ufb01le system can also be connected to the local \ufb01le system of\na machine in such a way that \ufb01les in HDFS can be accessed as though they are stored\nlocally. This requires providing the address of the NameNode machine, and the port\non which the HDFS server listens for requests, to the local \ufb01le system. The local \ufb01le\nsystem recognizes which \ufb01le accesses are to \ufb01les in HDFS based on the \ufb01le path, and\nsends appropriate requests to the HDFS server.\nMore details about distributed \ufb01le system implementation may be found in Section\n21.6.\n10.2.2 Sharding\nA single database system typically has su\ufb03cient storage and performance to handle all\nthe transaction processing needs of an enterprise. However, using a single database is\nnot su\ufb03cient for applications with millions or even billions of users, including social-\nmedia or similar web-scale applications, but also the user-facing applications of very\nlarge organizations such as large banks.\nSuppose an organization has built an application with a centralized database, but\nneeds to scale to handle more users, and the centralized database is not capable of\nhandling the storage or processing speed requirements. A commonly used way to deal\nwith such a situation is to partition the data across multiple databases, with a subset of\nusers assigned to each of the databases. The term sharding refers to the partitioning of\ndata across multiple databases or machines.\nPartitioning is usually done on one or more attributes, referred to as partitioning\nattributes ,partitioning keys ,o r shard keys . User or account identi\ufb01ers are commonly\nused as partitioning keys. Partitioning can be done by de\ufb01ning a range of keys that each\nof the databases handles; for example, keys from 1 to 100,000 may be assigned to the\n", "504": "476Chapter 10 Big Data\n\ufb01rst database, keys from 100,001 to 200,000 to the second database, and so on. Such\npartitioning is called range partitioning . Partitioning may also be done by computing a\nhash function that maps a key value to a partition number; such partitioning is called\nhash partitioning . We study partitioning of data in detail in Chapter 21.\nWhen sharding is done in application code, the application must keep track of\nwhich keys are stored on which database, and must route queries to the appropriate\ndatabase. Queries that read or update data from multiple databases cannot be pro-\ncessed in a simple manner, since it is not possible to submit a single query that gets\nexecuted across all the databases. Instead, the application would need to read data from\nmultiple databases and compute the \ufb01nal query result. Updates across databases cause\nfurther issues, which we discuss in Section 10.2.5.\nWhile sharding performed by modifying application code provided a simple way\nto scale applications, the limitations of the approach soon became apparent. First, the\napplication code has to track how data was partitioned and route queries appropriately.\nIf a database becomes overloaded, parts of the data in that database have to be o\ufb04oaded\nto a new database, or to one of the other existing databases; managing this process is\na non-trivial task. As more databases are a dded, there is a greater chance of failure\nleading to loss of access to data. Replication is needed to ensure data is accessible\ndespite failures, but managing the replicas, and ensuring they are consistent, poses\nfurther challenges. Key-value stores, which we study next, address some of these issues.\nChallenges related to consistency and availability are discussed later, in Section 10.2.5.\n10.2.3 Key-Value Storage Systems\nMany web applications need to store very large numbers (many billions or in extreme\ncases, trillions) of relatively small records (of size ranging from a few kilobytes to a\nfew megabytes). Storing each record as a separate \ufb01le is infeasible, since \ufb01le systems,\nincluding distributed \ufb01le systems, are not designed to store such large numbers of \ufb01les.\nIdeally, a massively parallel relational database should be used to store such data.\nHowever, it is not easy to build relational database systems that can run in parallel\nacross a large number of machines while also supporting standard database features\nsuch as foreign-key constraints and transactions.\nA number of storage systems have been developed that can scale to the needs\nof web applications and store large amounts of data, scaling to thousands to tens of\nthousands of machines, but typically o\ufb00ering only a simple key-value storage interface.\nAkey-value storage system (orkey-value store ) is a system that provides a way to store or\nupdate a record (value) with an associated key and to retrieve the record with a given\nkey.\nParallel key-value stores partition keys across multiple machines, and route updates\nand lookups to the correct machine. They also support replication, and ensure that\nreplicas are kept consistent. Further, they provide the ability to add more machines to\na system when required, and ensure that the load is automatically balanced across the\nmachines in a system In contrast to systems that implement sharding in the application\n", "505": "10.2 Big Data Storage Systems 477\ncode, systems that use a parallel key-value store do not need to worry about any of the\nabove issues. Parallel key-value stores are therefore more widely used than sharding\ntoday.\nWidely used parallel key-value stores include Bigtable from Google, Apache HBase,\nDynamo from Amazon, Cassandra from Facebook, MongoDB, Azure cloud storage\nfrom Microsoft, and Sherpa/ PNUTS from Yahoo!, among many others.\nWhile several key-value data stores view the values stored in the data store as an un-\ninterpreted sequence of bytes, and do not look at their content, other data stores allow\nsome form of structure or schema to be associated with each record. Several such key-\nvalue storage systems require the stored data to follow a speci\ufb01ed data representation,\nallowing the data store to interpret the stored values and execute simple queries based\non stored values. Such data stores are called document stores . MongoDB is a widely\nused data store that accepts values in the JSON format.\nKey-value storage systems are, at their core, based on two primitive functions,\nput(key, value) , used to store values with an associated key, and get(key) ,u s e dt or e -\ntrieve the stored value associated with the speci\ufb01ed key. Some systems, such as Bigtable,\nadditionally provide range queries on key val ues. Document stores additionally support\nlimited forms of querying on the data values.\nAn important motivation for the use of key-value stores is their ability to handle\nvery large amounts of data as well as queries, by distributing the work across a cluster\nconsisting of a large number of machines. Records are partitioned (divided up) among\nthe machines in the cluster, with each machine storing a subset of the records and\nprocessing lookups and updates on those records.\nNote that key-value stores are not full-\ufb02edged databases, since they do not provide\nmany of the features that are viewed as standard on database systems today. Key-value\nstores typically do not support declarative querying (using SQLor any other declarative\nquery language) and do not support transactions (which, as we shall see in Chapter 17,\nallow multiple updates to be committed atomically to ensure that the database state\nremains consistent despite failures, and c ontrol concurrent access to data to ensure\nthat problems do not arise due to concurrent access by multiple transactions). Key-\nvalue stores also typically do not support retrieval of records based on selections on\nnon-key attributes, although some document stores do support such retrieval.\nAn important reason for not supporting such features is that some of them are\nnot easy to support on very large clusters; thus, most systems sacri\ufb01ce these features in\norder to achieve scalability. Applications that need scalability may be willing to sacri\ufb01ce\nthese features in exchange for scalability.\nKey-value stores are also called NoSQL systems, to emphasize that they do not\nsupport SQL , and the lack of support for SQLwas initially viewed as something positive,\nrather than a limitation. However, it soon became clear that lack of database features\nsuch as transaction support and support for SQL, make application development more\ncomplicated. Thus, many key-value stores have evolved to support features, such as the\nSQL language and transactions.\n", "506": "478 Chapter 10 Big Data\nshow dbs // Shows available databases\nuse sampledb // Use database sampledb, creating it if it does not exist\ndb.createCollection(\"student\") // Create a collection\ndb.createCollection(\"instructor\")\nshow collections // Shows all collections in the database\ndb.student.insert({ \"id\" : \"00128\", \"name\" : \"Zhang\",\n\"dept\n name\" : \"Comp. Sci.\", \"tot\n cred\" : 102, \"advisors\" : [\"45565\"] })\ndb.student.insert({ \"id\" : \"12345\", \"name\" : \"Shankar\",\n\"dept\n name\" : \"Comp. Sci.\", \"tot\n cred\" : 32, \"advisors\" : [\"45565\"] })\ndb.student.insert({ \"id\" : \"19991\", \"name\" : \"Brandt\",\n\"dept\n name\" : \"History\", \"tot\n c r e d \":8 0 ,\" a d v i s o r s \":[ ]} )\ndb.instructor.insert({ \"id\" : \"45565\", \"name\" : \"Katz\",\n\"dept\n name\" : \"Comp. Sci.\", \"salary\" : 75000,\n\"advisees\" : [\"00128\",\"12345\"] })\ndb.student.find() // Fetch all students in JSON format\ndb.student.findOne({\"ID\": \"00128\"}) // Find one matching student\ndb.student.remove({\"dept\n name\": \"Comp. Sci.\"}) // Delete matching students\ndb.student.drop() // Drops the entire collection\nFigure 10.2 MongoDB shell commands.\nTheAPIs provided by these systems to store and access data are widely used. While\nthe basic get() andput() functions mentioned earlier are straightforward, most systems\nsupport further features. As an example of such APIs, we provide a brief overview of\nthe MongoDB API.\nFigure 10.2 illustrates access to the MongoDB document store through a\nJavaScript shell interface. Such a shell can be opened by executing the mongo com-\nmand on a system that has MongoDB installed and con\ufb01gured. MongoDB also pro-\nvides equivalent APIfunctions in a variety of languages, including Java and Python.\nThe usecommand shown in the \ufb01gure opens the speci\ufb01ed database, creating it if it\ndoes not already exist. The db.createCollection() command is used to create collec-\ntions, which store documents ; a document in MongoDB is basically a JSON object. The\ncode in the \ufb01gure creates two collections, student andinstructor ,a n di n s e r t s JSON\nobjects representing students and instructors into the two collections.\nMongoDB automatically creates identi\ufb01ers for the inserted objects, which can be\nused as keys to retrieve the objects. The key associated with an object can be fetched\nusing the\n idattribute, and an index on this attribute is created by default.\nMongoDB also supports queries based on the stored values. The db.student.find()\nfunction returns a collection of all objects in the student collection, while the find-\nOne() function returns one object from the collection. Both functions can take as argu-\nment a JSON object that speci\ufb01es a selection on desired attributes. In our example, the\n", "507": "10.2 Big Data Storage Systems 479\nstudent with ID00128 is retrieved. Similarly, all objects matching such a selection can\nbe deleted by the remove() function shown in the \ufb01gure. The drop() function shown\nin the \ufb01gure drops an entire collection.\nMongoDB supports a variety of other features such as creation of indices on spec-\ni\ufb01ed attributes of the stored JSON objects, such as the IDandname attributes.\nSince a key goal of MongoDB is to enable scaling to very large data sizes and\nquery/update loads, MongoDB allows multiple machines to be part of a single Mon-\ngoDB cluster. Data are then sharded (partitioned) across these machines. We study\npartitioning of data across machines in detail in Chapter 21, and we study parallel pro-\ncessing of queries in detail in Chapter 22. However we outline key ideas in this section.\nIn MongoDB (as in many other databases), partitioning is done based on the value\nof a speci\ufb01ed attribute, called the partitioning attribute orshard key .F o re x a m p l e ,i fw e\nspecify that the student collection should be partitioned on the dept\n name attribute,\nall objects of a particular department are stored on one machine, but objects of di\ufb00erent\ndepartments may be stored on di\ufb00erent machines. To ensure data can be accessed even\nif a machine has failed, each partition is replicated on multiple machines. This way, even\nif one machine fails, the data in that partition can be fetched from another machine.\nRequests from a MongoDB client are sent to a router, which then forwards requests\nto the appropriate partitions in a cluster.\nBigtable is another key-value store that requires data values to follow a format that\nallows the storage system access to individual parts of a stored value. In Bigtable, data\nvalues (records) can have multiple attributes; the set of attribute names is not prede-\ntermined and can vary across di\ufb00erent records. Thus, the key for an attribute value\nconceptually consists of (record-identi\ufb01er, attribute-name). Each attribute value is just\na string as far as Bigtable is concerned. To fetch all attributes of a record, a range query,\nor more precisely a pre\ufb01x-match query consisting of just the record identi\ufb01er, is used.\nTheget() function returns the attribute names along with the values. For e\ufb03cient re-\ntrieval of all attributes of a record, the storage system stores entries sorted by the key,\nso all attribute values of a particular record are clustered together.\nIn fact, the record identi\ufb01er can itself be structured hierarchically, although to\nBigtable itself the record identi\ufb01er is just a string. For example, an application that\nstores pages retrieved from a web crawl could map a URL of the form:\nwww.cs.yale.edu/people/silberschatz.html\nto the record identi\ufb01er:\nedu.yale.cs.www/people/silberschatz.html\nWith this representation, all URL so fcs.yale.edu can be retried by a query that fetches\nall keys with the pre\ufb01x edu.yale.cs , which would be stored in a consecutive range of\nkey values in the sorted key order. Similarly, all URL so fyale.edu would have a pre\ufb01x\nofedu.yale and would be stored in a consecutive range of key values.\n", "508": "480 Chapter 10 Big Data\nAlthough Bigtable does not support JSON natively, JSON data can be mapped to\nthe data model of Bigtable. For example, consider the following JSON data:\n{ \"ID\": \"22222\",\n\"name\": { \"firstname: \"Albert\", \"lastname: \"Einstein\" },\n\"deptname\": \"Physics\",\n\"children\": [\n{\"firstname\": \"Hans\", \"lastname\": \"Einstein\" },\n{\"firstname\": \"Eduard\", \"lastname\": \"Einstein\" } ]\n}\nThe above data can be represented by a Bigtable record with identi\ufb01er \u201c22222\u201d,\nwith multiple attribute names such as \u201cname.\ufb01rstname\u201d, \u201cdeptname\u201d, \u201cchil-\ndren[1].\ufb01rstname\u201d or \u201cchildren[2].lastname\u201d.\nFurther, a single instance of Bigtable can store data for multiple applications, with\nmultiple tables per application, by simply pre\ufb01xing the application name and table\nname to the record identi\ufb01er.\nMany data-storage systems allow multiple versions of data items to be stored. Ver-\nsions are often identi\ufb01ed by timestamp, but they may be alternatively identi\ufb01ed by an\ninteger value that is incremented whenever a new version of a data item is created.\nLookups can specify the required version of a data item or can pick the version with\nthe highest version number. In Bigtable, for example, a key actually consists of three\nparts: (record-identi\ufb01er, attribute-name, timestamp). Bigtable can be accessed as a ser-\nvice from Google. The open-source version of Bigtable, HBase, is widely used.\n10.2.4 Parallel and Distributed Databases\nParallel databases are databases that run on multiple machines (together referred to\nas a cluster) and are designed to store data across multiple machines and to process\nlarge queries using multiple machines. Parallel databases were initially developed in\nthe 1980s, and thus they predate the modern generation of Big Data systems. From a\nprogrammer viewpoint, parallel databases can be used just like databases running on\na single machine.\nEarly generation parallel databases designed for transaction processing supported\nonly a few machines in a cluster, while those designed to process large analytical queries\nwere designed to support tens to hundreds of machines. Data are replicated across\nmultiple machines in a cluster, to ensure that data are not lost, and they continue to be\naccessible, even if a machine in a cluster fails. Although failures do occur and need to\nbe dealt with, failures during the processing of a query are not common in systems with\ntens to hundreds of machines. If a query was being processed on a node that failed, the\nquery is simply restarted, using replicas of data that are on other nodes.\nIf such database systems are run on clusters with thousands of machines, the prob-\nability of failure during execution of a query increases signi\ufb01cantly for queries that pro-\ncess a large amount of data and consequently run for a long time. Restarting a query in\n", "509": "10.2 Big Data Storage Systems 481\nthe event of a failure is no longer an option, since there is a fairly high probability that a\nfailure will happen yet again while the query is executing. Techniques to avoid complete\nrestart, allowing only computation on the failed machines to be redone, were developed\nin the context of map-reduce systems, which we study in Section 10.3. However, these\ntechniques introduce signi\ufb01cant overhead; given the fact that computation spanning\nthousands to tens of thousands of nodes is needed only by some exceptionally large\napplications, even today most parallel relational database systems target applications\nthat run on tens to hundreds of machines and just restart queries in the event of failure.\nQuery processing in such parallel and distributed databases is covered in detail in\nChapter 22, while transaction processing in such databases is covered in Chapter 23.\n10.2.5 Replication and Consistency\nReplication is key to ensuring availability of data, ensuring a data item can be accessed\ndespite failure of some of the machines storing the data item. Any update to a data item\nmust be applied to all replicas of the data item. As long as all the machines containing\nthe replicas are up and connected to each other, applying the update to all replicas is\nstraightforward.\nHowever, since machines do fail, there are two key problems. The \ufb01rst is how to\nensure atomic execution of a transaction that updates data at more than one machine:\nthe transaction execution is said to be atomic if despite failures, either all the data items\nupdated by the transaction are successfully updated, or all the data items are reverted\nback to their original values. The second problem is, how to perform updates on a\ndata item that has been replicated, when some of the replicas of the data item are on a\nmachine that has failed. A key requirement here is consistency , that is, all live replicas\nof a data item have the same value, and each read sees the latest version of the data\nitem. There are several possible solutions, which o\ufb00er di\ufb00erent degrees of resilience to\nfailures. We study solutions to the both these problems in Chapter 23.\nWe note that the solutions to the second problem typically require that a majority\nof the replicas are available for reading and update. If we had 3 replicas, this would\nrequire not more than 1 fail, but if we had 5 replicas, even if two machines fail we\nwould still have a majority of replicas available. Under these assumptions, writes will\nnot get blocked, and reads will see the latest value for any data item.\nWhile the probability of multiple machines failing is relatively low, network link\nfailures can cause further problems. In particular, a network partition is said to occur if\ntwo live machines in a network are unable to communicate with each other.\nIt has been shown that no protocol can ensure availability , that is, the ability to\nread and write data, while also guaranteeing consistency, in the presence of network\npartitions. Thus, distributed systems need to make tradeo\ufb00s: if they want high availabil-\nity, they need to sacri\ufb01ce consistency, for example by allowing reads to see old values\nof data items, or to allow di\ufb00erent replicas to have di\ufb00erent values. In the latter case,\nhow to bring the replicas to a common value by merging the updates is a task that the\n", "510": "482 Chapter 10 Big Data\nNote 10.1 Building Scalable Database Applications\nWhen faced with the task of creating a database application that can scale to a very\nlarge number of users, application developers typically have to choose between a\ndatabase system that runs on a single server, and a key-value store that can scale by\nrunning on a large number of servers. A database that supports SQL and atomic\ntransactions, and at the same time is highly scalable, would be ideal; as of 2018,\nGoogle Cloud Spanner, which is only available on the cloud, and the recently\ndeveloped open source database CockroachDB are the only such databases.\nSimple applications can be written using only key-value stores, but more com-\nplex applications bene\ufb01t greatly from having SQL support. Application developers\ntherefore typically use a combination of parallel key-value stores and databases.\nSome relations, such as those that store user account and user pro\ufb01le data\nare queried frequently, but with simple select queries on a key, typically on the\nuser identi\ufb01er. Such relations are stored in a parallel key-value store. In case select\nqueries on other attributes are required, key-value stores that support indexing on\nattributes other than the primary key, such as MongoDB, could still be used.\nOther relations that are used in more complex queries are stored in a relational\ndatabase that runs on a single server. Databases running on a single server do\nexploit the availability of multiple cores to execute transactions in parallel, but are\nlimited by the number of cores that can be supported in a single machine.\nMost relational databases support a form of replication where update transac-\ntions run on only one database (the primary), but the updates are propagated to\nreplicas of the database running on other servers. Applications can execute read-\nonly queries on these replicas, but with the understanding that they may see data\nthat is a few seconds behind in time, as compared to the primary database. Of-\n\ufb02oading read-only queries from the primary database allows the system to handle\na load larger than what a single database server can handle.\nIn-memory caching systems, such as memcached or Redis, are also used to\nget scalable read-only access to relations stored in a database. Applications may\nstore some relations, or some parts of some relations, in such an in-memory cache,\nwhich may be replicated or partitioned across multiple machines. Thereby, appli-\ncations can get fast and scalable read-only access to the cached data. Updates must\nhowever be performed on the database, and the application is responsible for up-\ndating the cache whenever the data is updated on the database.\napplication has to deal with. Some applications, or some parts of an application, may\nchoose to prioritize availability over consistency. But other applications, or some parts\nof an application, may choose to prioritize consistency even at the cost of potential\nnon-availability of the system in the event of failures. The above issues are discussed in\nmore detail in Chapter 23.\n", "511": "10.3 The MapReduce Paradigm 483\n10.3The MapReduce Paradigm\nThe MapReduce paradigm models a common situation in parallel processing, where\nsome processing, identi\ufb01ed by the map() function, is applied to each of a large num-\nber of input records, and then some form of aggregation, identi\ufb01ed by the reduce()\nfunction, is applied to the result of the map() function. The map() function is also\npermitted to specify grouping keys, such that the aggregation speci\ufb01ed in the reduce()\nfunction is applied within each group, identi\ufb01ed by the grouping key, of the map() out-\nput. We examine the MapReduce paradigm, and the map() andreduce() functions in\ndetail, in the rest of this section.\nThe MapReduce paradigm for parallel processing has a long history, dating back\nseveral decades, in the functional programming and parallel processing community\n(the map and reduce functions were supported in the Lisp language, for example).\n10.3.1 Why MapReduce?\nAs a motivating example for the use of the MapReduce paradigm, we consider the\nfollowing word count application, which takes a large number of \ufb01les as input, and\noutputs a count of the number of times each word appears, across all the \ufb01les. Here,\nt h ei n p u tw o u l db ei nt h ef o r mo fap o t e n t i a l l yl a r g en u m b e ro f\ufb01 l e ss t o r e di nad i r e c t o r y .\nWe start by considering the case of a single \ufb01le. In this case, it is straightforward\nto write a program that reads in the words in the \ufb01le and maintains an in-memory data\nstructure that keeps track of all the words encountered so far, along with their counts.\nThe question is, how to extend the above algorithm, which is sequential in nature, to\nan environment where there are tens of thousands of \ufb01les, each containing tens to\nhundreds of megabytes of data. It is infeasible to process such a large volume of data\nsequentially.\nOne solution is to extend the above scheme by coding it as a parallel program\nthat would run across many machines with each machine processing a part of the\n\ufb01les. The counts computed locally at each machine must then be combined to get the\n\ufb01nal counts. In this case, the programmer would be responsible for all the \u201cplumbing\u201d\nrequired to start up jobs on di\ufb00erent mach ines, coordinate them, and to compute the\n\ufb01nal answer. In addition, the \u201cplumbing\u201d co de must also deal with ensuring completion\nof the program in spite of machine failures; failures are quite frequent when the number\nof participating machines is large, such as in the thousands, and the program runs for\nal o n gd u r a t i o n .\nThe \u201cplumbing\u201d code to implement the above requirements is quite complex; it\nmakes sense to write it just once and reuse it for all desired applications.\nMapReduce systems provide the programmer a way of specifying the core logic\nneeded for an application, with the details of the earlier-mentioned plumbing handled\nby the MapReduce system. The programmer needs to provide only map() and reduce()\nfunctions, plus optionally functions for reading and writing data. The map() andre-\nduce() functions provided by the programmer are invoked on the data by the MapRe-\n", "512": "484Chapter 10 Big Data\nduce system to process data in parallel. The programmer does not need to be aware of\nthe plumbing or its complexity; in fact, she can for the most part ignore the fact that\nthe program is to be executed in parallel on multiple machines.\nThe MapReduce approach can be used to process large amounts of data for a va-\nriety of applications. The above-mentioned word count program is a toy example of\na class of text and document processing applications. Consider, for example, search\nengines which take keywords and return documents containing the keywords. MapRe-\nduce can, for example, be used to process documents and create text indices, which are\nthen used to e\ufb03ciently \ufb01nd documents containing speci\ufb01ed keywords.\n10.3.2 MapReduce By Example 1: Word Count\nOur word count application can be implemented in the MapReduce framework using\nthe following functions, which we de\ufb01ned i n pseudocode. Note that our pseudocode\nis not in any speci\ufb01c programming language; it is intended to introduce concepts. We\ndescribe how to write MapReduce code in speci\ufb01c languages in later sections.\n1.In the MapReduce paradigm, the map() function provided by the programmer is\ninvoked on each input record and emits zero or more output data items, which are\nthen passed on to the reduce() function. The \ufb01rst question is, what is a record?\nMapReduce systems provide defaults, treating each line of each input \ufb01le as a\nrecord; such a default works well for our word count application, but the pro-\ngrammers are allowed to specify their own functions to break up input \ufb01les into\nrecords.\nFor the word count application, the map() function could break up each\nrecord (line) into individual words and output a number of records, each of which\nis a pair ( word, count ), where count is the number of occurrences of the word in\nthe record. In fact in our simpli\ufb01ed implementation, the map() function does\neven less work and outputs each word as it is found, with a count of 1. These\ncounts are added up later by the reduce() . Pseudocode for the map() function\nfor the word count program is shown in Figure 10.3.\nThe function breaks up the record (line) into individual words. 2As each word\nis found, the map() function emits (outputs) a record ( word, 1). Thus, if the \ufb01le\ncontained just the sentence:\n\u201cOne a penny, two a penny, hot cross buns.\u201d\nthe records output by the map() function would be\n(\u201cone\u201d, 1), (\u201ca\u201d, 1), (\u201cpenny\u201d, 1),( \u201ctwo\u201d, 1), (\u201ca\u201d, 1), (\u201cpenny\u201d, 1),\n(\u201chot\u201d, 1), (\u201ccross\u201d, 1), (\u201cbuns\u201d, 1).\n2We omit details of how a line is broken up into words. In a real implementation, non-alphabet characters would be\nremoved, and uppercase characters mapped to lowercase, bef ore breaking up the line based on spaces to generate a list\nof words.\n", "513": "10.3 The MapReduce Paradigm 485\nmap(String record) {\nFor each word in record\nemit(word, 1).\n}\nreduce(String key, List value\n list) {\nString word = key;\nint count = 0;\nFor each value in value\n list\ncount = count + value\noutput(word, count)\n}\nFigure 10.3 Pseudocode of map-reduce job for word counting in a set of files.\nIn general, the map() function outputs a set of ( key, value ) pairs for each input\nrecord. The \ufb01rst attribute (key) of the map() output record is referred to as a\nreduce key , since it is used by the reduce step, which we study next.\n2.T h eM a p R e d u c es y s t e mt a k e sa l lt h e( key, value )p a i r se m i t t e db yt h e map() func-\ntions and sorts (or at least, groups them) such that all records with a particular\nkey are gathered together. All records whose keys match are grouped together,\nand a list of all the associated values is created. The ( key, list ) pairs are then\npassed to the reduce() function.\nIn our word count example, each key is a word, and the associated list is a list\nof counts generated for di\ufb00erent lines of di\ufb00erent \ufb01les. With our example data,\nthe result of this step is the following:\n(\u201ca\u201d, [1,1]), (\u201cbuns\u201d, [1]) (\u201ccross\u201d, [1]), (\u201chot\u201d, [1]), (\u201cone\u201d, [1]),\n(\u201cpenny\u201d, [1,1]), (\u201ctwo\u201d, [1])\nThe reduce() function for our example combines the list of word counts by\nadding the counts, and outputs ( word, total-count ) pairs. For the example input,\nthe records output by the reduce() function would be as follows:\n(\u201cone\u201d, 1), (\u201ca\u201d, 2), (\u201cpenny\u201d, 2), (\u201ctwo\u201d, 1), (\u201chot\u201d, 1), (\u201ccross\u201d, 1),\n(\u201cbuns\u201d, 1).\nPseudocode for the reduce() function for the word count program is shown in\nFigure 10.3. The counts generated by the map() function are all 1, so the reduce()\nfunction could have just counted the n umber of values in the list, but adding up\nthe values allows some optimizations that we will see later.\nA key issue here is that with many \ufb01les, there may be many occurrences of the\nsame word across di\ufb00erent \ufb01les. Reorganizing the outputs of the map() functions\n", "514": "486 Chapter 10 Big Data\n\u2026\n2013/02/21 10:31:22.00EST /slide-dir/11.ppt\n2013/02/21 10:43:12.00EST /slide-dir/12.ppt\n2013/02/22 18:26:45.00EST /slide-dir/13.ppt\n2013/02/22 18:26:48.00EST /exer-dir/2.pdf\n2013/02/22 18:26:54.00EST /exer-dir/3.pdf\n2013/02/22 20:53:29.00EST /slide-dir/12.ppt\n\u2026\nFigure 10.4 Log files.\nis required to bring all the values for a particular key together. In a parallel system\nwith many machines, this requires data for di\ufb00erent reduce keys to be exchanged\nbetween machines, so all the values for any particular reduce key are available\nat a single machine. This work is done by the shu\ufb04e step ,w h i c hp e r f o r m sd a t a\nexchange between machines and then sorts the ( key, value ) pairs to bring all the\nvalues for a key together. Observe in our example that the words have actually\nbeen sorted alphabetically. Sorting the output records from the map() is one\nway for the system to collect all occurrences of a word together; the lists for each\nword are created from the sorted records.\nBy default, the output of the reduce() function is sent to one or more \ufb01les, but\nMapReduce systems allow programmers to control what happens to the output.\n10.3.3 MapReduce by Example 2: Log Processing\nAs another example of the use of the MapReduce paradigm, which is closer to tradi-\ntional database query processing, suppose we have a log \ufb01le recording accesses to a\nweb site, which is structured as shown in Figure 10.4. The goal of our \ufb01le access count\napplication is to \ufb01nd how many times each of the \ufb01les in the slide-dir directory was ac-\ncessed between 2013/01/01 and 2013/01/31. The application illustrates one of a variety\nof kinds of questions an analyst may ask using data from web log \ufb01les.\nFor our log-\ufb01le processing application, each line of the input \ufb01le can be treated\nas a record. The map() function would do the following: it would \ufb01rst break up the\ninput record into individual \ufb01elds, namely date, time, and \ufb01lename. If the date is in\nt h er e q u i r e dd a t er a n g e ,t h e map() function would emit a record ( \ufb01lename, 1 ), which\nindicates that the \ufb01lename appeared once in that record. Pseudocode for the map()\nfunction for this example is shown in Figure 10.5.\nThe shu\ufb04e step brings all the values for a particular reduce key (in our case, a \ufb01le\nname) together as a list. The reduce() function provided by the programmer, shown in\nFigure 10.6, is then invoked for each reduce key value. The \ufb01rst argument of reduce()\nis the reduce key itself, while the second argument is a list containing the values in the\n", "515": "10.3 The MapReduce Paradigm 487\nmap(String record) {\nString attribute[3];\nbreak up record into tokens (based on space character), and\nstore the tokens in array attributes\nString date = attribute[0];\nString time = attribute[1];\nString filename = attribute[2];\nif(date between 2013/01/01 and 2013/01/31\nand filename starts with \u201chttp://db-book.com/slide-dir\u201d)\nemit(filename, 1).\n}\nFigure 10.5 Pseudocode of map functions for counting file accesses.\nrecords emitted by the map() function for that reduce key. In our example, the values\nfor a particular key are added to get the total number of accesses for a \ufb01le. This number\nis then output by the reduce() function.\nIf we were to use the values generated by the map() function, the values would be\n\u201c1\u201d for all emitted records, and we could have just counted the number of elements\nin the list. However, MapReduce systems support optimizations such as performing\na partial addition of values from each input \ufb01le, before they are redistributed. In that\ncase, the values received by the reduce() function may not necessarily be ones, and we\ntherefore add the values.\nFigure 10.7 shows a schematic view of the \ufb02ow of keys and values through the\nmap() andreduce() functions. In the \ufb01gure the mk i\u2019s denote map keys, mv i\u2019s denote\nmap input values, rk i\u2019s denote reduce keys, and rv i\u2019s denote reduce input values. Reduce\noutputs are not shown.\nreduce(String key, List value\n list) {\nString filename = key;\nint count = 0;\nFor each value in value\n list\ncount = count + value\noutput(filename, count)\n}\nFigure 10.6 Pseudocode of reduce functions for counting file accesses.\n", "516": "488 Chapter 10 Big Data\nmk1rk1 rk1 \nrk2 \nrk3\nrk7 \nrkirv3,... \nrv2,...\n... rv n,...rv1,rv7,...\nrv8,rvi,...rk7 \nrk3 \nrk1 \nrk2 \nrk2 \nrkirvi\nrvnrv1 \nrv2\nrv3 \nrv7 \nrv8mv1 \nmv2 mk2\nmkn\nmap inputs \n(key, value)reduce inputs \n(key, value)map outputsmvn\nFigure 10.7 Flow of keys and values in a MapReduce job.\n10.3.4 Parallel Processing of MapReduce Tasks\nOur description of the map() andreduce() functions so far has ignored the issue of\nparallel processing. We can understand the meaning of MapReduce code without con-\nsidering parallel processing. However, our goal in using the MapReduce paradigm is to\nenable parallel processing. Thus, MapReduce systems execute the map() function in\nparallel on multiple machines, with each map task processing some part of the data,\nfor example some of the \ufb01les, or even parts of a \ufb01le in case the input \ufb01les are very large.\nSimilarly, the reduce() functions are also executed in parallel on multiple machines,\nwith each reduce task processing a subset of the reduce keys (note that a particular call\nto the reduce() function is still for a single reduce key).\nParallel execution of map andreduce tasks is shown pictorially in Figure 10.8.\nIn the \ufb01gure, the input \ufb01le partitions, denoted as Part i, could be \ufb01les or parts of \ufb01les.\nThe nodes denoted as Map iare the map tasks, and the nodes denoted Reduce iare the\nreduce tasks. The master node sends copies of the map() andreduce() code to the map\nand reduce tasks. The map tasks execute the code and write output data to local \ufb01les\non the machines where the tasks are executed, after being sorted and partitioned based\non the reduce key values; separate \ufb01les are created for each reduce task at each Map\nnode. These \ufb01les are fetched across the network by the reduce tasks; the \ufb01les fetched\nby a reduce task (from di\ufb00erent map tasks) are merged and sorted to ensure that all\noccurrences of a particular reduce key are together in the sorted \ufb01le. The reduce keys\nand values are then fed to the reduce() functions.\n", "517": "10.3 The MapReduce Paradigm 489\nUser \nProgram\ncopy copy copy\nMaster\nassign \nmap\nMap 1 Part 1\nPart 2 \nPart 3 \nPart 4\nPart n\nread Remote \nRead, SortMap n\nInput \ufb01le \npartitionsIntermediate \n\ufb01lesOutput \ufb01lesMap 2\nlocal \nwritewriteassign \nreduce\nReduce 1 File 1 \nFile 2\nFile mReduce 1 \nReduce m\nFigure 10.8 Parallel processing of MapReduce job.\nMapReduce systems also need to parallelize \ufb01le input and output across multiple\nmachines; otherwise the single machine storing the data will become a bottleneck. Par-\nallelization of \ufb01le input and output can be done by using a distributed \ufb01le system, such\nas the Hadoop File System (HDFS ). As we saw in Section 10.2, distributed \ufb01le systems\nallow a number of machines to cooperate in storing \ufb01les, partitioning the \ufb01les across\nthe machines. Further, \ufb01le system data are replicated (copied) across several (typically\nthree) machines, so that even if a few of the machines fail, the data are available from\nother machines which have copies of the data in the failed machine.\nToday, in addition to distributed \ufb01le systems such as HDFS ,M a p R e d u c es y s t e m s\nsupport input from a variety of Big Data storage systems such as HBase, MongoDB,\nCassandra, and Amazon Dynamo, by using storage adapters. Output can similarly be\nsent to any of these storage systems.\n10.3.5 MapReduce in Hadoop\nThe Hadoop project provides a widely used open-source implementation of MapRe-\nduce in the Java language. We summarize its main features here using the Java API\nprovided by Hadoop. We note that Hadoop provides MapReduce APIsi ns e v e r a lo t h e r\nlanguages, such as Python and C++.\nUnlike our MapReduce pseudocode, real implementations such as Hadoop require\ntypes to be speci\ufb01ed for the input keys and values, as well as the output keys and value,\nof the map() function. Similarly, the types of the input as well as output keys and val-\nues of the reduce() function need to be speci\ufb01ed. Hadoop requires the programmer\nto implement map() andreduce() functions as member functions of classes that ex-\ntend Hadoop Mapper andReducer classes. Hadoop allows the programmer to provide\n", "518": "490 Chapter 10 Big Data\nfunctions to break up the \ufb01le into records, or to specify that the \ufb01le is one of the \ufb01le\ntypes for which Hadoop provides built-in functions to break up \ufb01les into records. For\nexample, the TextInputFormat speci\ufb01es that the \ufb01le should be broken up into lines,\nwith each line being a separate record. Compressed \ufb01le formats are widely used today,\nwith Avro,ORC, and Parquet being the most widely used compressed \ufb01le formats in the\nHadoop world (compressed \ufb01le formats are di scussed in Section 13.6). Decompression\nis done by the system, and a programmer writing a query need only specify one of the\nsupported types, and the uncompressed representation is made available to the code\nimplementing the query.\nInput \ufb01les in Hadoop can come from a \ufb01le system of a single machine, but for large\ndatasets, a \ufb01le system on a single machine would become a performance bottleneck.\nHadoop MapReduce allows input and output \ufb01les to be stored in a distributed \ufb01le\nsystem such as HDFS, allowing multiple machines to read and write data in parallel.\nIn addition to the reduce() function, Hadoop also allows the programmer to de\ufb01ne\nacombine() function, which can perform a part of the reduce() operation at the node\nwhere the map() function is executed. In our word count example, the combine() func-\ntion would be the same as the reduce() function we saw earlier. The reduce() function\nwould then receive a list of partial counts for a particular word; since the reduce() func-\ntion for word count adds up the values, it would work correctly even with the combine()\nfunction. One bene\ufb01t of using the combine() function is that it reduces the amount of\ndata that has to be sent over the network: each node that runs map tasks would send\nonly one entry for a word across the network, instead of multiple entries.\nA single MapReduce step in Hadoop executes a map and a reduce function. A\nprogram may have multiple MapReduce steps, with each step having its own map\nandreduce functions. The Hadoop APIallows a program to execute multiple such\nMapReduce steps. The reduce() output from each step is written to the (distributed)\n\ufb01le system and read back in the following step. Hadoop also allows the programmer to\ncontrol the number of map andreduce tasks to be run in parallel for the job.\nThe rest of this section assumes a basic knowledge of Java (you may skip the rest\nof this section without loss of continuity, if you are not familiar with Java).\nFigure 10.9 shows the Java implementation in Hadoop of the word count appli-\ncation we saw earlier. For brevity we have omitted Java import statements. The code\nde\ufb01nes two classes, one that implements the Mapper interface, and another that im-\nplements the Reducer interface. The Mapper andReducer classes are generic classes\nwhich take as arguments the types of the keys and values. Speci\ufb01cally, the generic Map-\nperandReducer interfaces both takes four type arguments that specify the types of\nthe input key, input value, output key, and output value, respectively.\nT h et y p ed e \ufb01 n i t i o no ft h e Map class in Figure 10.9, which implements the Mapper\ninterface, speci\ufb01es that the map key is of type LongWritable , is basically a long integer,\nand the value which is (all or part of) a document is of type Text. The output of map\nhas a key of type Text, since the key is a word, while the value is of type IntWritable ,\nwhich is an integer value.\n", "519": "10.3 The MapReduce Paradigm 491\npublic class WordCount {\npublic static class Map extends Mapper<LongWritable, Text, Text, IntWritable> {\nprivate final static IntWritable one = new IntWritable(1);\nprivate Text word = new Text();\npublic void map(LongWritable key, Text value, Context context)\nthrows IOException, InterruptedException {\nString line = value.toString();\nStringTokenizer tokenizer = new StringTokenizer(line);\nwhile (tokenizer.hasMoreTokens()) {\nword.set(tokenizer.nextToken());\ncontext.write(word, one);\n}\n}\n}\npublic static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable> {\npublic void reduce(Text key, Iterable< IntWritable> values, Context context)\nthrows IOException, InterruptedException {\nint sum = 0;\nfor (IntWritable val : values) {\nsum += val.get();\n}\ncontext.write(key, new IntWritable(sum));\n}\n}\npublic static void main(String[] args) throws Exception {\nConfiguration conf = new Configuration();\nJob job = new Job(conf, \"wordcount\");\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(IntWritable.class);\njob.setMapperClass(Map.class);\njob.setReducerClass(Reduce.class);\njob.setInputFormatClass(TextInputFormat.class);\njob.setOutputFormatClass(TextOutputFormat.class);\nFileInputFormat.addInputPath(job, new Path(args[0]));\nFileOutputFormat.setOutputPath(job, new Path(args[1]));\njob.waitForCompletion(true);\n}\n}\nFigure 10.9 The word count program written in Hadoop.\nThemap() code for the word count example breaks up the input text value into\nwords using StringTokenizer , and then for each word, it invokes context.write(word,\n", "520": "492 Chapter 10 Big Data\none) to output a key and value pair; note that oneis an IntWritable object with nu-\nmeric value 1.\nAll the values output by the map() invocations that have a particular key (word,\nin our example) are collected in a list by the MapReduce system infrastructure. Doing\nso requires interchange of data from multi ple map tasks to multiple reduce tasks; in a\ndistributed setting, the data would have to be sent over the network. To ensure that all\nvalues for a particular key come together, the MapReduce system typically sorts the\nkeys output by the map functions, ensuring all values for a particular key will come\ntogether in the sorted order. This list of values for each key is provided to the reduce()\nfunction.\nT h et y p eo ft h e reduce() input key is the same as the type of the map output\nkey. The reduce() input value in our example is a Java Iterable <IntWritable >object,\nwhich contains a list of map output values ( IntWritable is the type of the map output\nvalue). The output key for reduce() is a word, of type Text, while the output value is a\nword count, of type IntWritable .\nIn our example, the reduce() simply adds up the values it receives in its input to get\nthe total count; reduce() writes the word and the total count using the context.write()\nfunction.\nNote that in our simple example, the values are all 1, so reduce() just needs to count\nthe number of values it receives. In general, however, Hadoop allows the programmer\nto declare a Combiner class, whose combine() function is run on the output of a single\nmap job; the output of this function replaces multiple map() output values for a single\nkey with a single value. In our example, a combine() function could just count the\nnumber of occurrences of each word and output a single value, which is the local word\ncount at the map task. These outputs are then passed on to the reduce() function,\nwhich would add up the local counts to get the overall count. The Combiner \u2019s job is to\nreduce the tra\ufb03c over the network.\nAM a p R e d u c ej o br u n sa map and a reduce step. A program may have multiple\nMapReduce steps, and each step would have its own settings for the map andreduce\nfunctions. The main() function sets up the parameters for each MapReduce job, and\nthen executes it.\nThe example code in Figure 10.9 executes a single MapReduce job; the parameters\nfor the job are as follows:\n\u2022The classes that contain the map andreduce functions for the job, set by the\nmethods setMapperClass andsetReducerClass .\n\u2022The types of the job\u2019s output key and values, set to Text (for the words) and\nIntWritable (for the count), respectively, by methods setOutputKeyClass and\nsetOutputValueClass , respectively.\n\u2022The input format of the job, set to TextInputFormat by the method\njob.setInputFormatClass . The default input format in Hadoop is the TextInput-\nFormat ,w h i c hc r e a t e sa map key whose value is a byte o\ufb00set into the \ufb01le, and the\n", "521": "10.3 The MapReduce Paradigm 493\nmap value is the contents of one line of the \ufb01le. Since \ufb01les are allowed to be big-\nger than 4 gigabytes, the o\ufb00set is of type LongWritable . Programmers can provide\ntheir own implementations for the input format class, which would process input\n\ufb01les and break the \ufb01les into records.\n\u2022The output format of the job, set to TextOutputFormat by the method\njob.setOutputFormatClass .\n\u2022The directories where the input \ufb01les are stored, and where the output \ufb01les must be\ncreated, set by the methods addInputPath andaddOutputPath .\nHadoop supports many more parameters for MapReduce jobs, such as the number of\nmap andreduce tasks to be run in parallel for the job and the amount of memory to\nbe allocated to each map and reduce task, among many others.\n10.3.6 SQLon MapReduce\nMany of the applications of MapReduce are for parallel processing of large amounts\nof non-relational data, using computations that cannot be expressed easily in SQL.F o r\nexample, our word count program cannot be expressed easily in SQL. There are many\nreal-world uses of MapReduce that cannot be expressed in SQL . Examples include com-\nputation of \u201cinverted indices\u201d which are key for web search engines to e\ufb03ciently answer\nkeyword queries, and computation of Google\u2019s PageRank, which is an important mea-\nsure of the importance of web sites, and is used to rank answers to web search queries.\nHowever, there are a large number of applications that have used the MapReduce\nparadigm for data processing of various kinds, whose logic can be easily expressed\nusing SQL. If the data were in a database, it would make sense to write such queries\nusing SQL and execute the queries on a parallel database system (parallel database\nsystems are discussed in detail in Chapter 22. Using SQL is much easier for users than\nis coding in the MapReduce paradigm. However, the data for many such applications\nreside in a \ufb01le system, and there are signi\ufb01cant time and space overhead demands when\nloading them into a database.\nRelational operations can be implemented using map and reduce steps, as illus-\ntrated by the following examples:\n\u2022The relational selection operation can be implemented by a single map() function,\nwithout a reduce() function (or with a reduce() function that simply outputs its\ninputs, without any change).\n\u2022The relational group by and aggregate function \u03b3can be implemented using a single\nMapReduce step: the map() outputs records with the group by attribute values as\nthereduce key; the reduce() function receives a list of all the attribute values for\na particular group by key and computes the required aggregate on the values in its\ninput list.\n", "522": "494Chapter 10 Big Data\n\u2022A join operation can be implemented using a single MapReduce step, Consider the\nequijoin operation r\u22c8r.A=s.As. We de\ufb01ne a map() function which for each input\nrecord rioutputs a pair ( ri.A,ri), and similarly for each input record sioutputs a pair\n(si.A,si); the map output also includes a tag to indicate which relation ( rors)t h e\noutput came from. The reduce() function is invoked for each join-attribute value,\nwith a list of all the riand sirecords with that join-attribute value. The function\nseparates out the rand stuples, and then outputs a cross products of the rtuples\nand the stuples, since all of them have the same value for the join attribute.\nWe leave details as an exercise to the reader (Exercise 10.4). More complex tasks, for\nexample a query with multiple operations, can be expressed using multiple stages of\nmap and reduce tasks.\nWhile it is indeed possible for relational queries to be expressed using the MapRe-\nduce paradigm, it can be very cumbersome for a human to do so. Writing queries in\nSQL is much more concise and easy to understand, but traditional databases did not\nallow data access from \ufb01les, nor did they support parallel processing of such queries.\nA new generation of systems have been developed that allows queries written in\n(variants of) the SQL language to be executed in parallel on data stored in \ufb01le sys-\ntems. These systems include Apache Hive (which was initially developed at Facebook),\nSCOPE , which developed by Microsoft, both of which use variants of SQL,a n d Apache\nPig(which was initially developed at Yahoo!), which uses a declarative language called\nPig Latin , based on the relational algebra. All these systems allow data to be read di-\nrectly from the \ufb01le system but allow the programmer to de\ufb01ne functions that convert\nthe input data to a record format.\nAll these systems generate a program containing a sequence of map andreduce\ntasks to execute a given query. The programs are compiled and executed on a MapRe-\nduce framework such as Hadoop. These systems became very popular, and far more\nqueries are written using these systems today than are written directly using the MapRe-\nduce paradigm.\nToday, Hive implementations provide an option of compiling SQL code to a tree\nof algebraic operations that are executed on a parallel environment. Apache Tez and\nSpark are two widely used platforms that support the execution of a tree (or DAG )o f\nalgebraic operations on a parallel environment, which we study next in Section 10.4.\n10.4 Beyond MapReduce: Algebraic Operations\nRelational algebra forms the foundation of relational query processing, allowing queries\nto be modeled as trees of operations. This i dea is extended to settings with more com-\nplex data types by supporting algebraic operators that can work on datasets containing\nrecords with complex data types, and returning datasets with records containing similar\ncomplex data types.\n", "523": "10.4 Beyond MapReduce: Algebraic Operations 495\n10.4.1 Motivation for Algebraic Operations\nAs we saw in Section 10.3.6, relational operations can be expressed by a sequence of\nmap andreduce steps. Expressing tasks in such as fashion can be quite cumbersome.\nFor example, if programmers need to compute the join of two inputs, they should\nbe able to express it as a single algebraic operation, instead of having to express it\nindirectly via map andreduce functions. Having access to functions such as joins can\ngreatly simplify the job of a programmer.\nThe join operation can be executed in parallel, using a variety of techniques that\nwe will see later in Section 22.3. In fact, doing so can be much more e\ufb03cient than\nimplementing the join using map andreduce functions. Thus, even systems like Hive,\nwhere programmers do not directly write MapReduce code, can bene\ufb01t from direct\nsupport for operations such as join.\nLater-generation parallel data-processing systems therefore added support for other\nrelational operations such as joins (including variants such as outerjoins and semi-\njoins), as well as a variety of other operations to support data analytics. For example,\nmany machine-learning models can be modeled as operators that take a set of records\nas input then output a set of records that have an extra attribute containing the value\npredicted by the model based on the other attributes of the record. Machine-learning\nalgorithms can themselves be modeled as operators that take a set of training records\nas input and output a learned model. Processing of data often involves multiple steps,\nwhich can be modeled as a sequence (pipeline) or tree of operators.\nA unifying framework for these operations is to treat them as algebraic operations\nthat take one or more datasets as inputs and output one or more datasets.\nRecall that in the relational algebra (Section 2.6) each operation takes one or\nmore relations as input, and outputs a relation. These later-generation parallel query-\nprocessing systems are based on the same idea, but there are several di\ufb00erences. A key\ndi\ufb00erence is that the input data could be of arbitrary types, instead of just consisting\nof columns with atomic data types as in the relational model. Recall that the extended\nrelational algebra required to support SQL could restrict itself to simple arithmetic,\nstring, and boolean expressions. In contrast, the new-generation algebraic operators\nneed to support more complex expressions, requiring the full power of a programming\nlanguage.\nThere are a number of frameworks that support algebraic operations on complex\ndata; the most widely used ones today are Apache Tez and Apache Spark.\nApache Tez provides a low-level APIwhich is suitable for system implementors. For\nexample, Hive on Tez compiles SQL queries into algebraic operations that run on Tez.\nTez programmers can create trees (or in general Directed Acyclic Graphs, or DAG s) of\nnodes, and they provide code that is to be executed on each of the nodes. Input nodes\nwould read in data from data sources and pass them to other nodes, which operate\non the data. Data can be partitioned across multiple machines, and the code for each\nnode can be executed on each of the machines. Since Tez is not really designed for\napplication programmers to use directly, we do not describe it in further detail.\n", "524": "496 Chapter 10 Big Data\nHowever, Apache Spark provides higher-level APIs which are suitable for applica-\ntion programmers. We describe Spark in more detail next.\n10.4.2 Algebraic Operations in Spark\nApache Spark is a widely used parallel data processing system that supports a variety of\nalgebraic operations. Data can be input from or output to a variety of storage systems.\nJust as relational databases use a relation as the primary abstraction for data rep-\nresentation, Spark uses a representation called a Resilient Distributed Dataset (RDD),\nwhich is a collection of records that can be stored across multiple machines. The term\ndistributed refers to the records being stored on di\ufb00erent machines, and resilient refers\nto the resilience to failure, in that even if one of the machines fails, records can be\nretrieved from other machines where they are stored.\nOperators in Spark take one or more RDD sa si n p u t ,a n dt h e i ro u t p u ti sa n RDD .\nThe types of records stored in RDD s is not prede\ufb01ned and can be anything that the ap-\nplication desires. Spark also supports a relational data representation called a DataSet,\nwhich we describe later.\nSpark provides APIsf o rJ a v a ,S c a l a ,a n dP y t h o n .O u rc o v e r a g eo fS p a r ki sb a s e d\non the Java API.\nFigure 10.10 shows our word count application, written in Java using Apache\nSpark; this program uses the RDD data representation, whose Java type is called\nJavaRDD .N o t et h a t JavaRDD s require a type for the record, speci\ufb01ed in angular brack-\nets (\u201c <>\u201d). In the program we have RDD s of Java Strings. The program also has Java-\nPairRDD types, which store records with two attributes of speci\ufb01ed types. Records with\nmultiple attributes can be represented by using structured data types instead of primi-\ntive data types. While any user-de\ufb01ned data type can be used, the prede\ufb01ned data types\nTuple2 which stores two attributes, Tuple3 , which stores three attributes, and Tuple4 ,\nwhich stores four attributes, are widely used.\nThe \ufb01rst step in processing data using Spark is to convert data from input rep-\nresentation to the RDD representation, which is done by the spark.read().textfile()\nfunction, which creates a record for each line in the input. Note that the input can be\na \ufb01le or a directory with multiple \ufb01les; a Sp ark system running on multiple nodes will\nactually partition the RDD across multiple machines, although the program can treat\nit (for most purposes) as if it is a data structure on a single machine. In our sample\ncode in Figure 10.10, the result is the RDD called lines .\nThe next step in our Spark program is to split each line into an array of words, by\ncalling s.split(\" \")) on the line; this function breaks up the line based on spaces and\nreturns an array of words; a more complete function would split the input on other\npunctuation characters such as periods, semicolons, and so on. The split function can\nbe invoked on each line in the input RDD by calling the map() function, which in Spark\nreturns a single record for each input record. In our example, we instead use a variant\ncalled flatMap() , which works as follows: like map() ,flatMap() invokes a user-de\ufb01ned\n", "525": "10.4 Beyond MapReduce: Algebraic Operations 497\nimport java.util.Arrays;\nimport java.util.List;\nimport scala.Tuple2;\nimport org.apache.spark.api.java.JavaPairRDD;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.sql.SparkSession;\npublic class WordCount {\npublic static void main(String[] args) throws Exception {\nif (args.length <1) {\nSystem.err.println(\"Usage: WordCount <file-or-directory-name>\");\nSystem.exit(1);\n}\nSparkSession spark =\nSparkSession.builder().appName( \"WordCount\").getOrCreate();\nJavaRDD<String> lines = spark.read ().textFile(args[0]).javaRDD();\nJavaRDD<String> words = lines.flatMap(s -> Arrays.asList(s.split(\" \")).iterator());\nJavaPairRDD<String, Integer> ones = words.mapToPair(s -> new Tuple2<>(s, 1));\nJavaPairRDD<String, Integer> counts = ones.reduceByKey((i1, i2) -> i1 + i2);\ncounts.saveAsTextFile(\"outputDir\"); // Save output files in this directory\nList<Tuple2<String, Integer\u00bb output = counts.collect();\nfor (Tuple2<String,Integer> tuple : output) {\nSystem.out.println(tuple);\n}\nspark.stop();\n}\n}\nFigure 10.10 Word count program in Spark.\nfunction on each input record; the function is expected to return an iterator. A Java\niterator supports a next() function that can be used to fetch multiple results by calling\nt h ef u n c t i o nm u l t i p l et i m e s .T h e flatMap() function invokes the user-de\ufb01ned function\nto get an iterator, invokes the next() function repeatedly on the iterator to get multiple\nvalues, and then returns an RDD containing the union of all the values across all input\nrecords.\nThe code shown in Figure 10.10 uses the \u201clambda expression\u201d syntax introduced\nin Java 8, which allows functions to be de\ufb01ned compactly, without even giving them a\nname; in the Java code, the syntax\ns\u2212>Arrays.asList(s.split(\" \")).iterator()\n", "526": "498 Chapter 10 Big Data\nde\ufb01nes a function that takes a parameter sand returns an expression that does the\nfollowing: it applies the split function described earlier to create an array of words,\nthen uses Arrays.asList to convert the array to a list, and \ufb01nally applies the iterator()\nmethod on the list to create an iterator. The flatMap() function works on this iterator\nas described earlier.\nThe result of the above steps is an RDD called words , where each record contains\nas i n g l ew o r d .\nThe next step is to create a JavaPairRDD called ones , which contains pairs of the\nform \u201c(word, 1)\u201d for each word in words ; if a word appears multiple times in the input\n\ufb01le, there would correspondingly be as many records in words and in ones .\nFinally the algebraic operation reduceByKey() implements a group by and aggre-\ngation step. In the sample code, we specify that addition is to be used for aggregation,\nby passing the lambda function (i1, i2)\u2212>i1+i2 to the reduceByKey() function. The\nreduceByKey() function works on a JavaPairRDD , grouping by the \ufb01rst attribute, and\naggregating the values of the second attribute using the provided lambda function.\nWhen applied on the ones RDD , grouping would be on the word, which is the \ufb01rst\nattribute, and the values of the second attribute (all ones, in the ones RDD )w o u l db e\na d d e du p .T h er e s u l ti ss t o r e di nt h e JavaPairRDD counts .\nIn general, any binary function can be used to perform the aggregation, as long as\nit gives the same result regardless of the order in which it is applied on a collection of\nvalues.\nFinally, the counts RDD is stored to the \ufb01le system by saveAsTextFile() .I n s t e a do f\ncreating just one \ufb01le, the function creates multiple \ufb01les if the RDD itself is partitioned\nacross machines.\nKey to understanding how parallel processing is achieved is to understand that\n\u2022RDD s may be partitioned and stored on multiple machines, and\n\u2022each operation may be executed in parallel on multiple machines, on the RDD\npartition available at the machine. Operations may \ufb01rst repartition their input, to\nbring related records to the same machine before executing operations in parallel.\nFor example, reduceByKey() would repartition the input RDD to bring all records\nbelonging to a group together on a single machine; records of di\ufb00erent groups may\nbe on di\ufb00erent machines.\nAnother important aspect of Spark is that the algebraic operations are not neces-\nsarily evaluated immediately on the function call, although the code seems to imply\nthat this is what happens. Instead, the code shown in the \ufb01gure actually creates a tree\nof operations; in our code, the leaf operation textFile() reads data from a \ufb01le; the next\noperation flatMap() has the textFile() operation as its child; the mapToPairs() in turn\nhasflatMap() as child, and so on. The operators can be thought of in relational terms\nas de\ufb01ning views, which are not executed as soon as they are de\ufb01ned but get executed\nlater.\n", "527": "10.4 Beyond MapReduce: Algebraic Operations 499\nThe entire tree of operations actually get evaluated only when certain operations\nd e m a n dt h a tt h et r e eb ee v a l u a t e d .F o re x a m p l e , saveAsTextFile() forces the tree to be\nevaluated; other such functions include collect() , which evaluates the tree and brings\nall records to a single machine, where they can subsequently be processed, for example\nby printing them out.\nAn important bene\ufb01t of such lazy evaluation o ft h et r e e( i . e . ,t h et r e ei se v a l u a t e d\nwhen required, rather than when it is de\ufb01ned) is that before actual evaluation, it is\npossible for a query optimizer to rewrite the tree to another tree that computes the\nsame result but may execute faster. Query optimization techniques, which we study in\nChapter 16 can be applied to optimize such trees.\nWhile the preceding example created a tree of operations, in general the operations\nmay form a Directed Acyclic Graph (DAG) structure, if the result of an operation is con-\nsumed by more than one other operation. That would result in operations having more\nthan one parent, leading to a DAG structure, whereas operations in a tree can have at\nmost one parent.\nWhile RDD s are well suited for representing certain data types such as textual data,\na very large fraction of Big Data applications need to deal with structured data, where\neach record may have multiple attributes. Spark therefore introduced the DataSet type,\nwhich supports records with attributes. The DataSet type works well with widely used\nParquet, ORC, and Avro \ufb01le formats (discussed in more detail later in Section 13.6),\nwhich are designed to store records with multiple attributes in a compressed fashion.\nSpark also supports JDBC connectors that can read relations from a database.\nThe following code illustrates how data in Parquet format can be read and pro-\ncessed in Spark, where spark is a Spark session that has been opened earlier.\nDataset <Row>instructor = spark.read().parquet(\"...\");\nDataset <Row>department = spark.read().parquet(\"...\");\ninstructor.filter(instructor.col(\"salary\").gt(100000))\n.join(department, instructor.col(\"dept\n name\")\n.equalTo(department.col(\"dept\n name\")))\n.groupBy(department.col(\"building\"))\n.agg(count(instructor.col(\"ID\")));\nTheDataSet <Row>type above uses the type Row, which allows access to column\nvalues by name. The code reads instructor and department relations from Parquet \ufb01les\n(whose names are omitted in the code above); Parquet \ufb01les store metadata such as\ncolumn names in addition to the values, which allows Spark to create a schema for the\nrelations. The Spark code then applies a \ufb01lter (selection) operation on the instructor\nrelation, which retains only instructors with salary greater than 100000, then joins the\nresult with the department relation on the dept\n name attribute, performs a group by\non the building attribute (an attribute of the department relation), and for each group\n(here, each building), a count of the number of IDvalues is computed.\n", "528": "500Chapter 10 Big Data\nThe ability to de\ufb01ne new algebraic operations and to use them in queries has been\nfound to be very useful for many applications and has led to wide adoption of Spark.\nThe Spark system also supports compilation of Hive SQL queries into Spark operation\ntrees, which are then executed.\nSpark also allows classes other than Row to be used with DataSets. Spark requires\nthat for each attribute Attrk of the class, methods getAttrk () and setAttrk () must be\nde\ufb01ned to allow retrieval and storage of attribute values. Suppose we have created a\nclass Instructor , and we have a Parquet \ufb01le whose attributes match those of the class.\nThen we can read data from Parquet \ufb01les as follows:\nDataset <Instructor >instructor = spark.read().parquet(\"...\").\nas(Encoders.bean(Instructor.class));\nIn this case Parquet provides the names of attributes in the input \ufb01le, which are used\nto map their values to attributes of the Instructor class. Unlike with Row,w h e r et h e\ntypes are not known at compile time the types of attributes of Instructor are known\nat compile time, and can be represented more compactly than if we used the Row\nt y p e .F u r t h e r ,t h em e t h o d so ft h ec l a s s Instructor can be used to access attributes; for\nexample, we could use getSalary() instead of using col(\"salary\") ,w h i c ha v o i d st h e\nruntime cost of mapping attribute names to locations in the underlying records. More\ninformation on how to use these constructs can be found on the Spark documentation\navailable online at spark.apache.org .\nOur coverage of Spark has focused on database operations, but as mentioned ear-\nlier, Spark supports a number of other algebraic operations such as those related to\nmachine learning, which can be invoked on DataSet types.\n10.5 Streaming Data\nQuerying of data can be done in an ad hoc manner\u2014for example, whenever an analyst\nwants to extract some information from a database. It can also be done in a periodic\nmanner\u2014for example, queries may be executed at the beginning of each day to get a\nsummary of transactions that happened on the previous day.\nHowever, there are many applications where queries need to be executed contin-\nuously, on data that arrive in a continuous fashion. The term streaming data refers to\ndata that arrive in a continuous fashion. Many application domains need to process\nincoming data in real time (i.e., as they arrive, with delays, if any, guaranteed to be less\nthan some bound).\n10.5.1 Applications of Streaming Data\nHere are a few examples of streaming data, and the real-time needs of applications that\nuse the data.\n", "529": "10.5 Streaming Data 501\n\u2022Stock market : In a stock market, each trade that is made (i.e., a stock is sold by\nsomeone and bought by someone else) is represented by a tuple. Trades are sent\nas a stream to processing systems.\nStock market traders analyze the stream of trades to look for patterns, and\nthey make buy or sell decisions based on the patterns that they observe. Real-time\nrequirements in such systems used to be of the order of seconds in earlier days, but\nmany of today\u2019s systems require delays to be of the order of tens of microseconds\n(usually to be able to react before others do, to the same patterns).\nStock market regulators may use the same stream, but for a di\ufb00erent purpose:\nto see if there are patterns of trades that are indicative of illegal activities. In both\ncases, there is a need for continuous queries that look for patterns; the results of\nthe queries are used to carry out further actions.\n\u2022E-commerce : In an e-commerce site, each purchase made is represented as a tuple,\nand the sequence of all purchases forms a stream. Further, even the searches exe-\ncuted by a customer are of value to the e-commerce site, even if no actual purchase\nis made; thus, the searches executed by users form a stream.\nThese streams can be used for multiple purposes. For example, if an advertising\ncampaign is launched, the e-commerce site may wish to monitor in real time how\nthe campaign is a\ufb00ecting searches and purchases. The e-commerce site may also\nwish to detect any spikes in sales of speci\ufb01c products to which it may need to\nrespond by ordering more of that product. Similarly, the site may wish to monitor\nusers for patterns of activities such as fre quently returning items, and block further\nreturns or purchases by the user.\n\u2022Sensors : Sensors are very widely used in systems such as vehicles, buildings, and\nfactories. These sensors send readings periodically, and thus the readings form a\nstream. Readings in the stream are used to monitor the health of the system. If\nsome readings are abnormal, actions may need to be taken to raise alarms, and to\ndetect and \ufb01x any underlying faults, with minimal delay.\nDepending on the complexity of the system and the required frequency of read-\nings, the stream can be of very high volume. In many cases, the monitoring is done\nat a central facility in the cloud, which monitors a very large number of systems.\nParallel processing is essential in such a system to handle very large volumes of\ndata in the incoming streams.\n\u2022Network data : Any organization that manages a large computer network needs to\nmonitor activity on the network to detect network problems as well as attacks on\nthe network by malicious software (malware). The underlying data being moni-\ntored can be represented as a stream of tuples containing data observed by each\nmonitoring point (such as network switch). The tuples could contain information\nabout individual network packets, such as source and destination addresses, size\nof the packet, and timestamp of packet generation. However, the rate of creation\nof tuples in such a stream is extremely high, and they cannot be handled except us-\n", "530": "502Chapter 10 Big Data\ning special-purpose hardware. Instead, data can be aggregated to reduce the rate at\nwhich tuples are generated: for example, individual tuples could record data such\nas source and destination addresses, time interval, and total bytes transmitted in\nt h et i m ei n t e r v a l .\nThis aggregated stream must then be processed to detect problems. For exam-\nple, link failures could be detected by observing a sudden drop in tuples traversing\na particular link. Excessive tra\ufb03c from multiple hosts to a single or a few destina-\ntions could indicate a denial-of-service attack. Packets sent from one host to many\nother hosts in the network could indicate malware trying to propagate itself to\nother hosts in the network. Detection of such patterns must be done in real time\nso that links can be \ufb01xed or action taken to stop the malware attack.\n\u2022Social media : Social media such as Facebook and Twitter get a continuous stream\nof messages (such as posts or tweets) from users. Each message in the stream of\nmessages must be routed appropriately, for example by sending it to friends or\nfollowers. The messages that can potentially be delivered to a subscriber are then\nranked and delivered in rank order, based on user preferences, past interactions,\nor advertisement charges.\nSocial-media streams can be consumed not just by humans, but also by soft-\nware. For example, companies may keep a lookout for tweets regarding the com-\npany and raise an alert if there are many tweets that re\ufb02ect a negative sentiment\nabout the company or its products. If a company launches an advertisement cam-\npaign, it may analyze tweets to see if the campaign had an impact on users.\nThere are many more examples of the need to process and query streaming data\nacross a variety of domains.\n10.5.2 Querying Streaming Data\nData stored in a database are sometimes referred to as data-at-rest , in contrast to stream-\ning data. In contrast to stored data, streams are unbounded, that is, conceptually they\nmay never end. Queries that can output results only after seeing all the tuples in a\nstream would then never be able to output any result. As an example, a query that asks\nfor the number of tuples in a stream can never give a \ufb01nal result.\nOne way to deal with the unbounded nature of streams is to de\ufb01ne windows on\nthe streams, where each window contains tuples with a certain timestamp range or\na certain number of tuples. Given information about timestamps of incoming tuples\n(e.g., they are increasing), we can infer when all tuples in a particular window have\nbeen seen. Based on the above, some query languages for streaming data require that\nwindows be de\ufb01ned on streams, and queries can refer to one or a few windows of tuples\nrather than to a stream.\nAnother option is to output results that are correct at a particular point in the\nstream, but to output updates as more tuples arrive. For example, a count query can\n", "531": "10.5 Streaming Data 503\noutput the number of tuples seen at a particular point in time, and as more tuples arrive,\nthe query updates its result based on the new count.\nSeveral approaches have been developed for querying streaming data, based on the\ntwo options described above.\n1.Continuous queries . In this approach the incoming data stream is treated as in-\nserts to a relation, and queries on the relations can be written in SQL,o ru s i n gr e -\nlational algebra operations. These queries can be registered as continuous queries ,\nthat is, queries that are running continuously. The result of the query on initial\ndata are output when the system starts up. Each incoming tuple may result in\ninsertion, update, or deletion of tuples in the result of the continuous query. The\noutput of a continuous query is then a stream of updates to the query result, as\nthe underlying database is updated by the incoming streams.\nThis approach has some bene\ufb01ts in applications where users wish to view all\ndatabase inserts that satisfy some conditions. However, a major drawback of the\napproach is that consumers of a query result would be \ufb02ooded with a large num-\nber of updates to continuous queries if the input rate is high. In particular, this\napproach is not desirable for applications that output aggregated values, where\nusers may wish to see \ufb01nal aggregates for some period of time, rather than every\nintermediate result as each incoming tuple is inserted.\n2.Stream query languages . A second approach is to de\ufb01ne a query language by\nextending SQL or relational algebra, where streams are treated di\ufb00erently from\nstored relations.\nMost stream query languages use window operations, which are applied to\nstreams, and create relations corresponding to the contents of a window. For\nexample, a window operation on a stream may create sets of tuples for each hour\nof the day; each such set is thus a relation. Relational operations can be executed\non each such set of tuples, including aggregation, selection, and joins with stored\nrelational data or with windows of other streams, to generate outputs.\nWe provide an outline of stream query languages in Section 10.5.2.1. These\nlanguages separate streaming data from stored relations at the language level and\nrequire window operations to be applied before performing relational operations.\nDoing so ensures that results can be output after seeing only part of a stream. For\nexample, if a stream guarantees that tuples have increasing timestamps, a window\nb a s e do nt i m ec a nb ed e d u c e dt oh a v en om o r et u p l e so n c eat u p l ew i t hah i g h e r\ntimestamp than the window end has been seen. The aggregation result for the\nwindow can be output at this point.\nSome streams do not guarantee that tuples have increasing timestamps. How-\never, such streams would contain punctuations , that is, metadata tuples that state\nthat all future tuples will have a timestamp greater than some value. Such punc-\ntuations are emitted periodically and can be used by window operators to decide\n", "532": "504Chapter 10 Big Data\nwhen an aggregate result, such as aggregates for an hourly window, is complete\nand can be output.\n3.Algebraic operators on streams . A third approach is to allow users to write oper-\nators (user-de\ufb01ned functions) that are executed on each incoming tuple. Tuples\nare routed from inputs to operators; outputs of an operator may be routed to\nanother operator, to a system output, or may be stored in a database. Operators\ncan maintain internal state across the tuples that are processed, allowing them to\naggregate incoming data. They may also be permitted to store data persistently\nin a database, for long-term use.\nThis approach has seen widespread adoption in recent years, and we describe\nit in more detail later.\n4.Pattern matching . A fourth option is to de\ufb01ne a pattern matching language and\nallow users to write multiple rules, each with a pattern and an action. When the\nsystem \ufb01nds a subsequence of tuples that match a particular pattern, the action\ncorresponding to the pattern is executed. Such systems are called complex event\nprocessing (CEP) systems. Popular complex event processing systems include Or-\nacle Event Processing, Microsoft StreamInsight, and FlinkCEP, which is part of\nthe Apache Flink project,\nWe discuss stream query languages and algebraic operations in more detail later in this\nsection.\nMany stream-processing systems keep data in-memory and do not provide persis-\ntence guarantees; their goal is to generate results with minimum delay, to enable fast\nresponse based on analysis of streaming data. On the other hand, the incoming data\nmay also need to be stored in a database for later processing. To support both pat-\nterns of querying, many applications use a so-called lambda architecture ,w h e r eac o p y\nof the input data is provided to the stream-processing system and another copy is pro-\nvided to a database for storage and later processing. Such an architecture allows stream-\nprocessing systems to be developed rapidly, without worrying about persistence-related\nissues. However, the streaming system and database system are separate, resulting in\nthese problems:\n\u2022Queries may need to be written twice, once for the streaming system and once for\nthe database system, in di\ufb00erent languages.\n\u2022Streaming queries may not be able to access stored data e\ufb03ciently.\nSystems that support streaming queries along with persistent storage and queries that\nspan streams and stored data avoid these problems.\n10.5.2.1 Stream Extensions to SQL\nSQL window operations were described in Sec tion 5.5.2, but stream query languages\nsupport further window types that are not supported by SQL window functions. For\n", "533": "10.5 Streaming Data 505\nexample, a window that contains tuples for each hour cannot be speci\ufb01ed using SQL\nwindow functions; note, however, that aggregates on such windows can be speci\ufb01ed in\nSQL in a more roundabout fashion, \ufb01rst computing an extra attribute that contains just\nthe hour component of a timestamp, and then grouping on the hour attribute. Win-\ndow functions in streaming query languages simplify speci\ufb01cation of such aggregation.\nCommonly supported window functions include:\n\u2022Tumbling window : Hourly windows are an example of tumbling windows. Windows\ndo not overlap but are adjacent to each other. Windows are speci\ufb01ed by their win-\ndow size (for example, number of hours, minutes, or seconds).\n\u2022Hopping window : An hourly window computed every 20 minutes would be an exam-\nple of a hopping window; the window width is \ufb01xed, similar to tumbling windows,\nbut adjacent windows can overlap.\n\u2022Sliding window : Sliding windows are windows of a speci\ufb01ed size (based on time, or\nnumber of tuples) around each incoming tuple. These are supported by the SQL\nstandard.\n\u2022Session window : Session windows model users who perform multiple operations\nas part of a session. A window is identi\ufb01ed by a user and a time-out interval, and\ncontains a sequence of operations such that each operation occurs within the time-\nout interval from the previous operation. For example, if the time-out is 5 minutes,\nand a user performs an operation at 10 AM, a second operation at 10:04 AM, and\na third operation at 11 AM, then the \ufb01rst two operations are part of one session,\nwhile the third is part of a di\ufb00erent session. A maximum duration may also be\nspeci\ufb01ed, so once that duration expires, the session window is closed even if some\noperations have been performed within the time-out interval.\nThe exact syntax for specifying windows varies by implementation. Suppose we\nhave a relation order (orderid, datetime, itemid, amount ). In Azure Stream Analytics,\nthe total order amount for each item for each hour can be speci\ufb01ed by the following\ntumbling window:\nselect item,System.Timestamp aswindow\n end,sum(amount )\nfrom order timestamp by datetime\ngroup by itemid ,tumblingwindow (hour,1 )\nEach output tuple has a timestamp whose value is equal to the timestamp of the end\nof the window; the timestamp can be accessed using the keyword System.Timestamp as\nshown in the query above.\nSQL extensions to support streams di\ufb00erentiate between streams, where tuples\nhave implicit timestamps and are expected to receive a potentially unbounded number\nof tuples and relations whose content is \ufb01xed at any point. For example, customers,\nsuppliers, and items associated with orders would be treated as relations, rather than\n", "534": "506Chapter 10 Big Data\nas streams. The results of queries with windowing are treated as relations, rather than\nas streams.\nJoins are permitted between a stream and a relation, and the result is a stream;\nthe timestamp of a join result tuple is the same as the timestamp of the input stream\ntuple. Joins between two streams have the problem that a tuple early in one stream\nmay match a tuple that occurs much later in the other stream; such a join condition\nwould require storing the entire stream for a potentially unbounded amount of time. To\navoid this problem, streaming SQL systems allow stream-to-stream join only if there is\na join condition that bounds the time di\ufb00erence between matching tuples. A condition\nthat the timestamps of the two tuples di\ufb00er by at most 1 hour is an example of such a\ncondition.\n10.5.3 Algebraic Operations on Streams\nWhile SQL queries on streaming data are quite useful, there are many applications\nwhere SQL queries are not a good \ufb01t. With the algebraic operations approach to stream\nprocessing, user-de\ufb01ned code can be provided for implementing an algebraic operation;\na number of prede\ufb01ned algebraic operations, such as selection and windowed aggrega-\ntion, are also provided.\nTo perform computation, incoming tuples must be routed to operators that con-\nsume the tuples, and outputs of operators must be routed to their consumers. A key task\nof the implementation is to provide fault-tolerant routing of tuples between system in-\nput, operators, and outputs. Apache Storm andKafka are widely used implementations\nthat support such routing of data.\nThe logical routing of tuples is done by creating a directed acyclic graph (DAG) with\noperators as nodes. Edges between nodes de\ufb01ne the \ufb02ow of tuples. Each tuple output by\nan operator is sent along all the out-edges of the operator, to the consuming operators.\nEach operator receives tuples from all its in-edges. Figure 10.11a depicts the logical\nPublish-Subscribe \n          SystemData \nSink\nData \nSink\nData \nSinkData \nSink\nData \nSink\nData \nSink  Data \nSource\n  Data \nSource\n  Data \nSource\n  Data \nSourceOp Op\nOp OpOp\nOp OpOp  Data \nSource\n  Data \nSource\n  Data \nSource\n  Data \nSource\n(a) DAG representation of streaming data \ufb02ow (b) Publish-subscribe representation of streaming data \ufb02ow\nFigure 10.11 Routing of streams using DAG and publish-subscribe representations.\n", "535": "10.5 Streaming Data 507\nrouting of stream tuples through a DAG structure. Operation nodes are denoted as\n\u201cOp\u201d nodes in the \ufb01gure. The entry points to the stream-processing system are the data-\nsource nodes of the DAG; these nodes consume tuples from the stream sources and\ninject them into the stream-processing system. The exit points of the stream-processing\nsystem are data-sink nodes; tuples exiting the system through a data sink may be stored\nin a data store or \ufb01le system or may be output in some other manner.\nOne way of implementing a stream-processing system is by specifying the graph\nas part of the system con\ufb01guration, which is read when the system starts processing\ntuples, and is then used to route tuples. The Apache Storm stream-processing system\nis an example of a system that uses a con\ufb01guration \ufb01le to de\ufb01ne the graph, which is\ncalled a topology in the Storm system. Data-source nodes are called spouts in the Storm\nsystem, while operator nodes are called bolts, and edges connect these nodes.\nAn alternative way of creating such a routing graph is by using publish-subscribe\nsystems. A publish-subscribe system allows publication of documents or other forms\nof data, with an associated topic. Subscribers correspondingly subscribe to speci\ufb01ed\ntopics. Whenever a document is published to a particular topic, a copy of the document\nis sent to all subscribers who have subscribed to that topic. Publish-subscribe systems\nare also referred to as pub-sub systems for short.\nWhen a publish-subscribe system is used for routing tuples in a stream-processing\nsystem, tuples are considered documents, and each tuple is tagged with a topic. The\nentry points to the system conceptually \u201cpublish\u201d tuples, each with an associated topic.\nOperators subscribe to one or more topics; the system routes all tuples with a speci\ufb01c\ntopic to all subscribers of that topic. Opera tors can also publish their outputs back to\nthe publish-subscribe system, with an associated topic.\nA major bene\ufb01t of the publish-subscribe approach is that operators can be added to\nthe system, or removed from it, with relative ease. Figure 10.11b depicts the routing of\ntuples using a publish-subscribe representation. Each data source is assigned a unique\ntopic name; the output of each operator is also assigned a unique topic name. Each\noperator subscribes to the topics of its inputs and publishes to the topics corresponding\nto its output. Data sources publish to their associated topic, while data sinks subscribe\nto the topics of the operators whose output goes to the sink.\nThe Apache Kafka system uses the publish-subscribe model to manage routing\nof tuples in streams. In the Kafka system, tuples published for a topic are retained\nfor a speci\ufb01ed period of time (called the retention period), even if there is currently no\nsubscriber for the topic. Subscribers usually process tuples at the earliest possible time,\nbut in case processing is delayed or temporarily stopped due to failures, the tuples are\nstill available for processing until the retention time expires.\nMore details of routing, and in particular how publish-subscribe is implemented in\na parallel system, are provided in Section 22.8.\nThe next detail to be addressed is how to implement the algebraic operations. We\nsaw earlier how algebraic operations can be computed using data from \ufb01les and other\nd a t as o u r c e sa si n p u t s .\n", "536": "508 Chapter 10 Big Data\nApache Spark allows streaming data sources to be used as inputs for such opera-\ntions. The key issue is that some of the operations may not output any results at all until\nthe entire stream is consumed, which may take potentially unbounded time. To avoid\nthis problem, Spark breaks up streams into discretized streams ,w h e r et h es t r e a md a t a\nfor a particular time window are treated as a data input to algebraic operators. When\nthe data in that window have been consumed, the operator generates its output, just as\nit would have if the data source were a \ufb01le or a relation.\nHowever, the above approach has the problem that the discretization of streams\nhas to be done before any algebraic operations are executed. Other systems such as\nApache Storm and Apache Flink support stream operations, which take a stream as\ninput and output another stream. This is straightforward for operations such as map\nor relational select operations; each output tuple inherits a timestamp from the input\ntuple. On the other hand, relational aggregate operations and reduce operations may\nbe unable to generate any output until the entire stream is consumed. To support such\noperations, Flink supports a window operation which breaks up the stream into win-\ndows; aggregates are computed within each window and are output once the window\nis complete. Note that the output is treated as a stream, where tuples have a timestamp\nbased on the end of the window. 3\n10.6 Graph Databases\nGraphs are an important type of data that databases need to deal with. For example,\na computer network with multiple routers and links between them can be modeled as\na graph, with routers as nodes and network links as edges. Road networks are another\ncommon type of graph, with road intersections modeled as nodes and the road links\nbetween intersections as edges. Web pages with hyperlinks between them are yet an-\nother example of graphs, where web pages can be modeled as nodes and hyperlinks\nbetween them as edges.\nIn fact, if we consider an E-Rmodel of an enterprise, every entity can be modeled\nas a node of a graph, and every binary relationship can be modeled as an edge of the\ngraph. Ternary and higher-degree relationships are harder to model, but as we saw in\nSection 6.9.4, such relationships can be modeled as a set of binary relationships if\ndesired.\nGraphs can be represented using the relational model using the following two re-\nlations:\n1.node(ID, label, node\n data)\n2.edge(fromID, toID, label, edge\n data)\nwhere node\n data and edge\n data contain all the data related to nodes and edges, respec-\ntively.\n3Some systems generate timestamps based on when the window is processed, but doing so results in output timestamps\nthat are nondeterministic.\n", "537": "10.6 Graph Databases 509\nModeling a graph using just two relations is too simplistic for complex database\nschemas. For example, applications require modeling of many types of nodes, each with\nits own set of attributes, and many types of edges, each with its own set of attributes.\nWe can correspondingly have multiple relations that store nodes of di\ufb00erent types and\nmultiple relations that store edges of di\ufb00erent types.\nAlthough graph data can be easily stored in relational databases, graph databases\nsuch as the widely used Neo4j provide several extra features:\n\u2022They allow relations to be identi\ufb01ed as representing nodes or edges and o\ufb00er spe-\ncial syntax for de\ufb01ning such relations\n\u2022They support query languages designed for easily expressing path queries, which\nmay be harder to express in SQL.\n\u2022They provide e\ufb03cient implementations for such queries, which can execute queries\nmuch faster than if they were expressed in SQL and executed on a regular database.\n\u2022They provide support for other features such as graph visualization.\nAs an example of a graph query, we consider a query in the Cypher query language\nsupported by Neo4j. Suppose the input graph has nodes corresponding to students\n(stored in a relation student ) and instructors (stored in a relation instructor ,a n da n\nedge type advisor from student toinstructor . We omit details of how to create such node\nand edge types in Neo4j and assume appropriate schemas for these types. We can then\nwrite the following query:\nmatch (i:instructor )<\u2212[:advisor ]\u2212(s:student )\nwhere i.dept\n name =' C o m p .S c i . '\nreturn i.IDasID,i.name asname ,collect (s.name )asadvisees\nObserve that the match clause in the query connects instructors to students via the\nadvisor relation, which is modeled as a graph path that traverses the advisor edge in\nthe backwards direction (the edge points from student to instructor), by using the syn-\ntax ( i:instructor )<\u2212[:advisor ]\u2212(s:student ). This step basically performs a join of the\ninstructor, advisor and student relations. The query then performs a group by on in-\nstructor ID and name, and collects all the students advised by the instructor into a\nset called advisees. We omit details, and refer the interested reader to online tutorials\navailable at neo4j.com/developer .\nNeo4J also supports recursive traversal of edges. For example, suppose we wish to\n\ufb01nd direct and indirect prerequisites of courses, with the relation course modeled with\ntype node, and relation prereq (course\n id, prereq\n id) modeled with type edge. We can\nthen write the following query:\nmatch (c1:course )\u2212[:prereq *1..]\u2212>(c2:course )\nreturn c1.course\n id,c2.course\n id\n", "538": "510Chapter 10 Big Data\nHere, the annotation \u201c*1..\u201d indicates we want to consider paths with multiple prereq\nedges, with a minimum of 1 edge (with a minimum of 0, a course would appear as its\nown prerequisite).\nWe note that Neo4j is a centralized system and does not (as of 2018) support\nparallel processing. However, there are many applications that need to process very\nlarge graphs, and parallel processing is key for such applications.\nComputation of PageRank (which we saw earlier in Section 8.3.2.2) on very large\ngraphs containing a node for every web page, and an edge for every hyperlink from one\npage to another, is a good example of a complex computation on very large graphs.\nThe web graph today has hundreds of billions of nodes and trillions of edges. Social\nnetworks are another example of very large graphs, containing billions of nodes and\nedges; computations on such graphs include shortest paths (to \ufb01nd connectivity be-\ntween people), or computing how in\ufb02uential people are based on edges in the social\nnetwork.\nThere are two popular approaches for parallel graph processing:\n1.Map-reduce and algebraic frameworks : Graphs can be represented as relations,\nand individual steps of many parallel graph algorithms can be represented as\njoins. Graphs can thus be stored in a parallel storage system, partitioned across\nmultiple machines. We can then use map-reduce programs, algebraic frameworks\nsuch as Spark, or parallel relational database implementations to process each\nstep of a graph algorithm in parallel across multiple nodes.\nSuch approaches work well for many applications. However, when performing\niterative computations that traverse long paths in graphs, these approaches are\nquite ine\ufb03cient, since they typically read the entire graph in each iteration.\n2.Bulk synchronous processing frameworks :T h e bulk synchronous processing (BSP)\nframework for graph algorithms frames graph algorithms as computations asso-\nciated with vertices that operate in an iterative manner. Unlike the preceding\napproach, here the graph is typically stored in memory, with vertices partitioned\nacross multiple machines; most importantly, the graph does not have to be read\nin each iteration.\nEach vertex (node) of the graph has data (state) associated with it. Similar\nto how programmers provide map() andreduce() functions in the MapReduce\nframework, in the BSP framework programmers provide methods that are exe-\ncuted for each node of the graph. The methods can send messages to neighboring\nnodes and receive messages from neighboring nodes of the graph. In each iter-\nation, called a superstep , the method associated with each node is executed; the\nmethod consumes any incoming messages, updates the data associated with the\nnode, and may optionally send messages to neighboring nodes. Messages sent\nin one iteration are received by the recipients in the next iteration. The method\nexecuting at each vertex may vote to halt if they decide they have no more compu-\ntation to carry out. If in some iteration all vertices vote to halt, and no messages\nare sent out, the computation can be halted.\n", "539": "10.7Summary 511\nThe result of the computation is contained in the state at each node. The state\ncan be collected and output as the result of the computation.\nThe idea of bulk synchronous processing is quite old but was popularized by the\nPregel system developed by Google, which provided a fault-tolerant implementation\nof the framework. The Apache Giraph system is an open-source version of the Pregel\nsystem.\nThe GraphX component of Apache Spark supports graph computations on large\ngraphs. It provides an APIbased on Pregel, as well as a number of other operations that\ntake a graph as input, and output a graph. Operations supported by GraphX include\nmap functions applied on vertices and edges of graphs, join of a graph with an RDD ,\nand an aggregation operation that works as follows: a user-de\ufb01ned function is used\nto create messages that are sent to all the neighbors of each node, and another user-\nde\ufb01ned function is used to aggregate the messages. All these operations can be executed\nin parallel to handle large graphs.\nFor more information on how to write graph algorithms in such settings, see the\nreferences in the Further Reading section at the end of the chapter.\n10.7Summary\n\u2022Modern data management applications often need to deal with data that are not\nnecessarily in relational form, and these applications also need to deal with vol-\numes of data that are far larger than what a single traditional organization would\nhave generated.\n\u2022The increasing use of data sensors leads to the connection of sensors and other\ncomputing devices embedded within other objects to the internet, often referred\nto as the \u201cinternet of things.\u201d\n\u2022There is now a wider variety of query language options for Big Data applications,\ndriven by the need to handle more varied of data types, and by the need to scale\nto very large data volumes/velocity.\n\u2022Building data management systems that can scale to large volume/velocity of data\nrequires parallel storage and processing of data.\n\u2022Distributed \ufb01le systems allow \ufb01les to be stored across a number of machines, while\nallowing access to \ufb01les using a traditional \ufb01le-system interface.\n\u2022Key-value storage systems allow records to be stored and retrieved based on a key\nand may additionally provide limited query facilities. These systems are not full-\n\ufb02edged database systems, and they are sometimes called No SQL systems.\n\u2022Parallel and distributed databases provide a traditional database interface, but they\nstore data across multiple machines, and they perform query processing in parallel\nacross multiple machines.\n", "540": "512Chapter 10 Big Data\n\u2022The MapReduce paradigm models a common situation in parallel processing,\nwhere some processing, identi\ufb01ed by the map() function, is applied to each of\na large number of input records, and then some form of aggregation, identi\ufb01ed by\nthereduce() function, is applied to the result of the map() function.\n\u2022The Hadoop system provides a widely used open-source implementation of\nMapReduce in the Java language.\n\u2022There are a large number of applications that use the MapReduce paradigm for\ndata processing of various kinds whose logic can be easily expressed using SQL.\n\u2022Relational algebra forms the foundation of relational query processing, allowing\nqueries to be modeled as trees of operations. This idea is extended to settings\nwith more complex data types by supporting algebraic operators that can work on\ndatasets containing records with complex data types, and returning datasets with\nrecords containing similar complex data types.\n\u2022There are many applications where queries need to be executed continuously, on\ndata that arrive in a continuous fashion. The term streaming data refers to data\nthat arrive in a continuous fashion. Many application domains need to process\nincoming data in real time.\n\u2022Graphs are an important type of data that databases need to deal with.\nReview Terms\n\u2022Volume\n\u2022Velocity\n\u2022Conversion\n\u2022Internet of things\n\u2022Distributed \ufb01le system\n\u2022NameNode server\n\u2022DataNodes machines\n\u2022Sharding\n\u2022Partitioning attribute\n\u2022Key-value storage system\n\u2022Key-value store\n\u2022Document stores\n\u2022NoSQL systems\n\u2022Shard key\n\u2022Parallel databases\n\u2022Reduce key\u2022Shu\ufb04e step\n\u2022Streaming data\n\u2022Data-at-rest\n\u2022Windows on the streams\n\u2022Continuous queries\n\u2022Punctuations\n\u2022Lambda architecture\n\u2022Tumbling window\n\u2022Hopping window\n\u2022Sliding window\n\u2022Session window\n\u2022Publish-subscribe systems\n\u2022Pub-sub systems\n\u2022Discretized streams\n\u2022Superstep\n", "541": "Practice Exercises 513\nPractice Exercises\n10.1 Suppose you need to store a very large number of small \ufb01les, each of size say 2\nkilobytes. If your choice is between a distributed \ufb01le system and a distributed\nkey-value store, which would you prefer, and explain why.\n10.2 Suppose you need to store data for a very large number of students in a dis-\ntributed document store such as MongoDB. Suppose also that the data for\neach student correspond to the data in the student and the takes relations.\nHow would you represent the above data about students, ensuring that all the\ndata for a particular student can be accessed e\ufb03ciently? Give an example of\nthe data representation for one student.\n10.3 Suppose you wish to store utility bills for a large number of users, where each\nbill is identi\ufb01ed by a customer IDand a date. How would you store the bills in\na key-value store that supports range queries, if queries request the bills of a\nspeci\ufb01ed customer for a speci\ufb01ed date range.\n10.4 Give pseudocode for computing a join r\u22c8r.A=s.Asusing a single MapReduce\nstep, assuming that the map() function is invoked on each tuple of rand s.\nAssume that the map() function can \ufb01nd the name of the relation using con-\ntext.relname() .\n10.5 What is the conceptual problem with the following snippet of Apache Spark\ncode meant to work on very large data. Note that the collect() function returns\na Java collection, and Java collections (from Java 8 onwards) support map and\nreduce functions.\nJavaRDD <String <lines = sc.textFile(\"logDirectory\");\nint totalLength = lines.collect().map(s \u2212>s.length())\n.reduce(0,(a,b) \u2212>a+b);\n10.6 Apache Spark:\na. How does Apache Spark perform computations in parallel?\nb. Explain the statement: \u201cApache Spark performs transformations on\nRDD s in a lazy manner.\u201d\nc. What are some of the bene\ufb01ts of lazy evaluation of operations in Apache\nSpark?\n10.7 Given a collection of documents, for each word wi,l e t nidenote the number of\ntimes the word occurs in the collection. Let Nbe the total number of word oc-\ncurrences across all documents. Next, consider all pairs of consecutive words\n", "542": "514Chapter 10 Big Data\n(wi,wj) in the document; let ni,jdenote the number of occurrences of the word\npair ( wi,wj) across all documents.\nWrite an Apache Spark program that, given a collection of documents in a\ndirectory, computes N, all pairs ( wi,ni), and all pairs (( wi,wj),ni,j). Then output\nall word pairs such that ni,j\u2215N\u226510\u2217(ni\u2215N)\u2217(nj\u2215N). These are word pairs\nthat occur 10 times or more as frequently as they would be expected to occur\nif the two words occurred independently of each other.\nYou will \ufb01nd the join operation on RDD s useful for the last step, to bring\nrelated counts together. For simplicity, do not bother about word pairs that\ncross lines. Also assume for simplicity that words only occur in lowercase and\nthat there are no punctuation marks.\n10.8 Consider the following query using the tumbling window operator:\nselect item,System.Timestamp aswindow\n end,sum(amount )\nfrom order timestamp by datetime\ngroup by itemid ,tumblingwindow (hour,1 )\nGive an equivalent query using normal SQL constructs, without using the tum-\nbling window operator. You can assume that the timestamp can be converted\nto an integer value that represents the number of seconds elapsed since (say)\nmidnight, January 1, 1970, using the function to\nseconds (timestamp ). You can\nalso assume that the usual arithmetic functions are available, along with the\nfunction \ufb02oor(a) which returns the largest integer \u2264a.\n10.9 Suppose you wish to model the university schema as a graph. For each of the\nfollowing relations, explain whether the relation would be modeled as a node\nor as an edge:\n(i)student , (ii) instructor , (iii) course ,( i v ) section ,( v ) takes ,( v i ) teaches .\nDoes the model capture connections between sections and courses?\nExercises\n10.10 Give four ways in which information in web logs pertaining to the web pages\nvisited by a user can be used by the web site.\n10.11 One of the characteristics of Big Data is the variety of data. Explain why this\ncharacteristic has resulted in the need for languages other than SQL for pro-\ncessing Big Data.\n10.12 Suppose your company has built a database application that runs on a cen-\ntralized database, but even with a high-end computer and appropriate indices\ncreated on the data, the system is not able to handle the transaction load, lead-\n", "543": "Tools 515\ning to slow processing of queries. What would be some of your options to allow\nthe application to handle the transaction load?\n10.13 The map-reduce framework is quite useful for creating inverted indices on a set\nof documents. An inverted index stores for each word a list of all document\nIDs that it appears in (o\ufb00sets in the documents are also normally stored, but\nwe shall ignore them in this question).\nFor example, if the input document IDs and contents are as follows:\n1: data clean\n2: data base\n3: clean base\nthen the inverted lists would\ndata: 1, 2\nclean: 1, 3\nbase: 2, 3\nGive pseudocode for map and reduce functions to create inverted indices on a\ngiven set of \ufb01les (each \ufb01le is a document). Assume the document ID is available\nusing a function context.getDocumentID() , and the map function is invoked\nonce per line of the document. The output inverted list for each word should be\na list of document IDs separated by commas. The document IDs are normally\nsorted, but for the purpose of this question you do not need to bother to sort\nthem.\n10.14 Fill in the blanks below to complete the following Apache Spark program\nwhich computes the number of occurrences of each word in a \ufb01le. For simplic-\nity we assume that words only occur in lowercase, and there are no punctuation\nmarks.\nJavaRDD <String >textFile = sc.textFile(\"hdfs://...\");\nJavaPairRDD <String, Integer >counts =\ntextFile.\n (s\u2212>Arrays.asList(s.split(\" \")).\n ())\n.mapToPair(word -> new\n ).reduceByKey((a, b) \u2212>a+b ) ;\n10.15 Suppose a stream can deliver tuples out of order with respect to tuple times-\ntamps. What extra information should the stream provide, so a stream query\nprocessing system can decide when all tuples in a window have been seen?\n10.16 Explain how multiple operations can b e executed on a stream using a publish-\nsubscribe system such as Apache Kafka.\nTools\nA wide variety of open-source Big Data tools are available, in addition to some\ncommercial tools. In addition, a number of these tools are available on cloud plat-\n", "544": "516Chapter 10 Big Data\nforms. We list below several popular tools, along with the URLsw h e r et h e yc a n\nbe found. Apache HDFS (hadoop.apache.org ) is a widely used distributed \ufb01le sys-\ntem implementation. Open-source distributed/parallel key-value stores include Apache\nHBase ( hbase.apache.org ), Apache Cassandra ( cassandra.apache.org ), MongoDB\n(www.mongodb.com ), and Riak ( basho.com ).\nHosted cloud storage systems include the Amazon S3 storage system\n(aws.amazon.com/s3 ) and Google Cloud Storage ( cloud.google.com/storage ).\nHosted key-value stores include Google BigTable ( cloud.google.com/bigtable ), and\nAmazon DynamoDB ( aws.amazon.com/dynamodb ).\nGoogle Spanner ( cloud.google.com/spanner )a n dt h eo p e ns o u r c eC o c k -\nroachDB ( www.cockroachlabs.com ) are scalable parallel databases that support SQL\nand transactions, and strongly consistent storage.\nOpen-source MapReduce systems include Apache Hadoop ( hadoop.apache.org ),\nand Apache Spark ( spark.apache.org ), while Apache Tez ( tez.apache.org )s u p -\nports data processing using a DAG of algebraic operators. These are also available\nas cloud-based o\ufb00erings from Amazon Elastic MapReduce ( aws.amazon.com/emr ),\nwhich also supports Apache HDFS and Apache HBase, and from Microsoft Azure\n(azure.microsoft.com ).\nApache Hive ( hive.apache.org ) is a popular open-source SQL implementation\nthat runs on top of the Apache MapReduce, Apache Tez, as well as Apache Spark;\nthese systems are designed to support large queries running in parallel on multiple ma-\nchines. Apache Impala ( impala.apache.org )i sa n SQL implementation that runs on\nHadoop, and is designed to handle a large number of queries, and to return query results\nwith minimal delays (latency). Hosted SQL o\ufb00erings on the cloud that support paral-\nlel processing include Amazon EMR ( aws.amazon.com/emr ), Google Cloud SQL\n(cloud.google.com/sql ), and Microsoft Azure SQL (azure.microsoft.com ).\nApache Kafka ( kafka.apache.org ) and Apache Flink ( flink.apache.org )a r e\nopen-source stream-processing systems; Apache Spark also provides support for\nstream processing. Hosted stream-proce ssing platforms include Amazon Kinesis\n(aws.amazon.com/kinesis ), Google Cloud Data\ufb02ow ( cloud.google.com/dataflow )\nand Microsoft Stream Analytics ( azure.microsoft.com ). Open source graph process-\ning platforms include Neo4J ( neo4j.com ) and Apache Giraph ( giraph.apache.org ).\nFurther Reading\n[Davoudian et al. (2018)] provides a nice survey of No SQL data stores, including data\nmodel querying and internals. More information about Apache Hadoop, including doc-\numentation on HDFS and Hadoop MapReduce, can be found on the Apache Hadoop\nhomepage, hadoop.apache.org . Information about Apache Spark may be found on\nthe Spark homepage, spark.apache.org . Information about the Apache Kafka stream-\ni n gd a t ap l a t f o r mm a yb ef o u n do n kafka.apache.org , and details of stream process-\ning in Apache Flink may be found on flink.apache.org . Bulk Synchronous Processing\n", "545": "Further Reading 517\nwas introduced in [Valiant (1990)]. A description of the Pregel system, including its\nsupport for bulk synchronous processing, may be found in [Malewicz et al. (2010)],\nwhile information about the open source equivalent, Apache Giraph, may be found on\ngiraph.apache.org .\nBibliography\n[Davoudian et al. (2018)] A. Davoudian, L. Chen, and M. Liu, \u201cA Survey of NoSQL Stores\u201d,\nACM Computing Surveys , Volume 51, Number 2 (2018), pages 2\u201342.\n[Malewicz et al. (2010)] G .M a l e w i c z ,M .H .A u s t e r n ,A .J .C .B i k ,J .C .D e h n e r t ,I .H o r n ,\nN. Leiser, and G. Czajkowski, \u201cPregel: a system for large-scale graph processing\u201d, In Proc. of\nthe ACM SIGMOD Conf. on Management of Data (2010), pages 135\u2013146.\n[Valiant (1990)] L. G. Valiant, \u201cA Bridging Model for Parallel Computation\u201d, Communica-\ntions of the ACM , Volume 33, Number 8 (1990), pages 103\u2013111.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "546": "", "547": "CHAPTER11\nData Analytics\nDecision-making tasks bene\ufb01t greatly by using data about the past to predict the future\nand using the predictions to make decisions. For example, online advertisement sys-\ntems need to decide what advertisement to show to each user. Analysis of past actions\nand pro\ufb01les of other users, as well as past actions and pro\ufb01le of the current user, are\nkey to deciding which advertisement the user is most likely to respond to. Here, each\ndecision is low-value, but with high volumes the overall value of making the right deci-\nsions is very high. At the other end of the value spectrum, manufacturers and retailers\nneed to decide what items to manufacture or stock many months ahead of the actual\nsale of the items. Predicting future demand of di\ufb00erent types of items based on past\nsales and other indicators is key to avoidi ng both overproduction or overstocking of\nsome items, and underproduction or understocking of other items. Errors can lead to\nmonetary loss due to unsold inventory of some items, or loss of potential revenue due\nto nonavailability of some items.\nThe term data analytics refers broadly to the processing of data to infer patterns,\ncorrelations, or models for prediction. The results of analytics are then used to drive\nbusiness decisions.\nThe \ufb01nancial bene\ufb01ts of making correct decisions can be substantial, as can the\ncosts of making wrong decisions. Organizations therefore invest a lot of money to\ngather or purchase required data and build systems for data analytics.\n11.1 Overview of Analytics\nLarge companies have diverse sources of data that they need to use for making business\ndecisions. The sources may store the data under di\ufb00erent schemas. For performance\nreasons (as well as for reasons of organization control), the data sources usually will\nnot permit other parts of the company to retrieve data on demand.\nOrganizations therefore typically gather data from multiple sources into one lo-\ncation, referred to as a data warehouse . Data warehouses gather data from multiple\nsources at a single site, under a uni\ufb01ed sch ema (which is usually designed to support\ne\ufb03cient analysis, even at the cost of redundant storage). Thus, they provide the user\n519\n", "548": "520 Chapter 11 Data Analytics\na single uniform interface to data. However, data warehouses today also collect and\nstore data from non-relational sources, where schema uni\ufb01cation is not possible. Some\nsources of data have errors that can be detected and corrected using business con-\nstraints; further, organizations may collect data from multiple sources, and there may\nbe duplicates in the data collected from di\ufb00erent sources. These steps of collecting\ndata, cleaning/deduplicating the data, and loading the data into a warehouse are re-\nferred to as extract, transform and load (ETL) tasks. We study issues in building and\nmaintaining a data warehouse in Section 11.2.\nThe most basic form of analytics is generation of aggregates and reports summa-\nrizing the data in ways that are meaningful to the organization. Analysts need to get a\nnumber of di\ufb00erent aggregates and compare them to understand patterns in the data.\nAggregates, and in some cases the underlying data, are typically presented graphically\nas charts, to make it easy for humans to visualize the data. Dashboards that display\ncharts summarizing key organizational parameters, such as sales, expenses, product\nreturns, and so forth, are popular means of monitoring the health of an organization.\nAnalysts also need to visualize data in ways that can highlight anomalies or give insights\ninto causes for changes in the business.\nSystems that support very e\ufb03cient analysis, where aggregate queries on large data\nare answered in almost real time (as opposed to being answered after tens of minutes or\nmultiple hours) are popular with analysts. Such systems are referred to as online analyti-\ncal processing (OLAP ) systems. We discuss online analytical processing in Section 11.3,\nwhere we cover the concept of multidimensional data, OLAP operations, relational rep-\nresentation of multidimensional summaries . We also discuss graphical representation\nof data and visualization in Section 11.3.\nStatistical analysis is an important part of data analysis. There are several tools\nthat are designed for statistical analysis, including the R language/environment, which\nis open source, and commercial systems such as SAS and SPSS . The R language is\nwidely used today, and in addition to features for statistical analysis, it supports facilities\nfor graphical display of data. A large number of R packages (libraries) are available\nthat implement a wide variety of data analysis tasks, including many machine-learning\nalgorithms. R has been integrated with databases as well as with Big Data systems such\nas Apache Spark, which allows R programs to be executed in parallel on large datasets.\nStatistical analysis is a large area by itself, and we do not discuss it further in this book.\nReferences providing more information may be found in the Further Reading section\nat the end of this chapter.\nPrediction of di\ufb00erent forms is another key aspect of analytics. For example, banks\nneed to decide whether to give a loan to a loan applicant, and online advertisers need\nto decide which advertisement to show to a particular user. As another example, man-\nufacturers and retailers need to make decisions on what items to manufacture or order\nin what quantities.\nThese decisions are driven signi\ufb01cantly by techniques for analyzing past data and\nusing the past to predict the future. For example, the risk of loan default can be pre-\ndicted as follows. First, the bank would examine the loan default history of past cus-\n", "549": "11.2 Data Warehousing 521\ntomers, \ufb01nd key features of customers, such as salary, education level, job history, and\nso on, that help in prediction of loan default. The bank would then build a prediction\nmodel (such as a decision tree, which we study later in this chapter) using the chosen\nfeatures. When a customer then applies for a loan, the features of that particular cus-\ntomer are fed into the model which makes a prediction, such as an estimated likelihood\nof loan default. The prediction is used to make business decisions, such as whether to\ngive a loan to the customer.\nSimilarly, analysts may look at the past history of sales and use it to predict future\nsales, to make decisions on what and how much to manufacture or order, or how to\ntarget their advertising. For example, a car company may search for customer attributes\nthat help predict who buys di\ufb00erent types of cars. It may \ufb01nd that most of its small\nsports cars are bought by young women whose annual incomes are above $50,000.\nThe company may then target its marketing to attract more such women to buy its\nsmall sports cars and may avoid wasting money trying to attract other categories of\npeople to buy those cars.\nMachine-learning techniques are key to \ufb01nding patterns in data and in making pre-\nd i c t i o n sf r o mt h e s ep a t t e r n s .T h e\ufb01 e l do f data mining combines knowledge-discovery\ntechniques invented by machine-learning researchers with e\ufb03cient implementation\ntechniques that enable them to be used on extremely large databases. Section 11.4\ndiscusses data mining.\nThe term business intelligence (BI) is used in a broadly similar sense to data an-\nalytics. The term decision support is also used in a related but narrower sense, with\na focus on reporting and aggregation, but not including machine learning/data min-\ning. Decision-support tasks typically use SQL queries to process large amounts of data.\nDecision-support queries are can be contrasted with queries for online transaction pro-\ncessing , where each query typically reads only a small amount of data and may perform\na few small updates.\n11.2 Data Warehousing\nLarge organizations have a complex internal organization structure, and therefore dif-\nferent data may be present in di\ufb00erent locations, or on di\ufb00erent operational systems,\nor under di\ufb00erent schemas. For instance, manufacturing-problem data and customer-\ncomplaint data may be stored on di\ufb00erent database systems. Organizations often pur-\nchase data from external sources, such as mailing lists that are used for product pro-\nmotions, or credit scores of customers tha t are provided by credit bureaus, to decide\non creditworthiness of customers.1\nCorporate decision makers require access to information from multiple such\nsources. Setting up queries on individual sources is both cumbersome and ine\ufb03cient.\n1Credit bureaus are companies that gather information about consumers from multiple sources and compute a credit-\nworthiness score for each consumer.\n", "550": "522 Chapter 11 Data Analytics\nMoreover, the sources of data may store only current data, whereas decision makers\nmay need access to past data as well; for instance, information about how purchase pat-\nterns have changed in the past few years could be of great importance. Data warehouses\nprovide a solution to these problems.\nAdata warehouse is a repository (or archive) of information gathered from multi-\nple sources, stored under a uni\ufb01ed schema, at a single site. Once gathered, the data are\nstored for a long time, permitting access to historical data. Thus, data warehouses pro-\nvide the user a single consolidated interface to data, making decision-support queries\neasier to write. Moreover, by accessing information for decision support from a data\nwarehouse, the decision maker ensures that online transaction-processing systems are\nnot a\ufb00ected by the decision-support workload.\n11.2.1 Components of a Data Warehouse\nFigure 11.1 shows the architecture of a typical data warehouse and illustrates the gath-\nering of data, the storage of data, and the querying and data analysis support. Among\nthe issues to be addressed in building a warehouse are the following:\n\u2022When and how to gather data. In a source-driven architecture for gathering data,\nthe data sources transmit new information, either continually (as transaction pro-\ncessing takes place), or periodically (nightly, for example). In a destination-driven\narchitecture , the data warehouse periodically sends requests for new data to the\nsources.\nUnless updates at the sources are \u201csynchronously\u201d replicated at the warehouse,\nthe warehouse will never be quite up-to-date with the sources. Synchronous repli-\ncation can be expensive, so many data warehouses do not use synchronous repli-\ncation, and they perform queries only on data that are old enough that they have\ndata\nloaders\nDBMS\ndata warehousequery and\nanalysis tools\ndata source n data source 2data source 1\n...\nFigure 11.1 Data-warehouse architecture.\n", "551": "11.2 Data Warehousing 523\nbeen completely replicated. Traditionally, analysts were happy with using yester-\nday\u2019s data, so data warehouses could be loaded with data up to the end of the\nprevious day. However, increasingly organizations want more up-to-date data. The\ndata freshness requirements depend on the application. Data that are within a few\nhours old may be su\ufb03cient for some applications; others that require real-time re-\nsponses to events may use stream processing infrastructure (described in Section\n10.5) instead of depending on a warehouse infrastructure.\n\u2022What schema to use. Data sources that have been constructed independently are\nlikely to have di\ufb00erent schemas. In fact, they may even use di\ufb00erent data models.\nPart of the task of a warehouse is to perform schema integration and to convert\ndata to the integrated schema before they are stored. As a result, the data stored\nin the warehouse are not just a copy of the data at the sources. Instead, they can\nbe thought of as a materialized view of the data at the sources.\n\u2022Data transformation and cleansing. The task of correcting and preprocessing data\nis called data cleansing . Data sources often deliver data with numerous minor in-\nconsistencies, which can be corrected. For example, names are often misspelled,\nand addresses may have street, area, or city names misspelled, or postal codes en-\ntered incorrectly. These can be corrected to a reasonable extent by consulting a\ndatabase of street names and postal codes in each city. The approximate matching\nof data required for this task is referred to as fuzzy lookup .\nAddress lists collected from multiple sources may have duplicates that need to\nbe eliminated in a merge\u2013purge operation (this operation is also referred to as dedu-\nplication ). Records for multiple individuals in a house may be grouped together so\nonly one mailing is sent to each house; this operation is called householding .\nData may be transformed in ways other than cleansing, such as changing the\nunits of measure, or converting the data to a di\ufb00erent schema by joining data\nfrom multiple source relations. Data warehouses typically have graphical tools\nto support data transformation. Such tools allow transformation to be speci\ufb01ed\nas boxes, and edges can be created between boxes to indicate the \ufb02ow of data.\nConditional boxes can route data to an appropriate next step in transformation.\n\u2022How to propagate updates . Updates on relations at the data sources must be propa-\ngated to the data warehouse. If the relations at the data warehouse are exactly the\nsame as those at the data source, the propagation is straightforward. If they are\nnot, the problem of propagating updates is basically the view-maintenance prob-\nlem, which was discussed in Section 4.2.3, and is covered in more detail in Section\n16.5.\n\u2022What data to summarize. The raw data generated by a transaction-processing sys-\ntem may be too large to store online. However, we can answer many queries by\nmaintaining just summary data obtained by aggregation on a relation, rather than\nmaintaining the entire relation. For example, instead of storing data about every\nsale of clothing, we can store total sales of clothing by item name and category.\n", "552": "524 Chapter 11 Data Analytics\nThe di\ufb00erent steps involved in getting data into a data warehouse are called extract,\ntransform, and load orETL tasks; extraction refers to getting data from the sources,\nwhile load refers to loading the data into the data warehouse. In current generation\ndata warehouses that support user-de\ufb01ned functions or MapReduce frameworks, data\nmay be extracted, loaded into the warehouse, and then transformed. The steps are then\nreferred to as extract, load, and transform orELTtasks. The ELTapproach permits the\nuse of parallel processing frameworks for data transformation.\n11.2.2 Multidimensional Data and Warehouse Schemas\nData warehouses typically have schemas that are designed for data analysis, using tools\nsuch as OLAP tools. The relations in a data warehouse schema can usually be classi-\n\ufb01ed as fact tables and dimension tables .Fact tables record information about individual\nevents, such as sales, and are usually very large. A table recording sales information\nfor a retail store, with one tuple for each item that is sold, is a typical example of a\nfact table. The attributes in fact table can be classi\ufb01ed as either dimension attributes or\nmeasure attributes ,T h e measure attributes store quantitative information, which can be\naggregated upon; the measure attributes of a sales table would include the number of\nitems sold and the price of the items. In contrast, dimension attributes are dimensions\nupon which measure attributes, and summaries of measure attributes, are grouped and\nviewed. The dimension attributes of a sales table would include an item identi\ufb01er, the\ndate when the item is sold, which location (store) the item was sold from, the customer\nwho bought the item, and so on.\nData that can be modeled using dimension attributes and measure attributes are\ncalled multidimensional data .\nTo minimize storage requirements, dimension attributes are usually short identi-\n\ufb01ers that are foreign keys into other tables called dimension tables . For instance, a fact\ntable sales would have dimension attributes item\n id,store\n id,customer\n id,a n d date,a n d\nmeasure attributes number and price. The attribute store\n idis a foreign key into a dimen-\nsion table store, which has other attributes such as store location (city, state, country).\nThe item\n idattribute of the sales table would be a foreign key into a dimension table\nitem\n info, which would contain information such as the name of the item, the category\nto which the item belongs, and other item details such as color and size. The customer\nidattribute would be a foreign key into a customer table containing attributes such as\nname and address of the customer. We can also view the dateattribute as a foreign key\ninto a date\n infotable giving the month, quarter, and year of each date.\nThe resultant schema appears in Figure 11.2. Such a schema, with a fact table,\nmultiple dimension tables, and foreign keys from the fact table to the dimension tables\nis called a star schema . More complex data-warehouse designs may have multiple levels\nof dimension tables; for instance, the item\n infotable may have an attribute manufacturer\nidthat is a foreign key into another table giving details of the manufacturer. Such\nschemas are called snow\ufb02ake schemas . Complex data-warehouse designs may also have\nmore than one fact table.\n", "553": "11.2 Data Warehousing 525\nitem_info\nsalesstore\ndate_info\ndate\nmonth\nquarter\nyearitem_id\nitem_id\nstore_idstore_id\ncustomercustomer_id\ncustomer_iddate\nnumber\nprice name\nstreet\ncity\nstate\nzipcode\ncountrycity\nstate\ncountryitemname\ncolor\nsize\ncategory\nFigure 11.2 Star schema for a data warehouse.\n11.2.3 Database Support for Data Warehouses\nThe requirements of a database system desi gned for transaction processing are some-\nwhat di\ufb00erent from one designed to support a data-warehouse system. One key di\ufb00er-\nence is that a transaction-processing database needs to support many small queries,\nwhich may involve updates in addition to reads. In contrast, data warehouses typically\nneed to process far fewer queries, but each query accesses a much larger amount of\ndata.\nMost importantly, while new records are inserted into relations in a data ware-\nhouse, and old records may be deleted once they are no longer needed, to make space\nfor new data, records are typically never updated once they are added to a relation.\nThus, data warehouses do not need to pay any overhead for concurrency control. (As\ndescribed in Chapter 17 and Chapter 18, if concurrent transactions read and write the\nsame data, the resultant data may become inconsistent. Concurrency control restricts\nconcurrent accesses in a way that ensures there is no erroneous update to the database.)\nThe overhead of concurrency control can be signi\ufb01cant in terms of not just time taken\nfor query processing, but also in terms of sto rage, since databases often store multiple\nversions of data to avoid con\ufb02icts between small update transactions and long read-only\ntransactions. None of these overheads are needed in a data warehouse.\nDatabases traditionally store all attributes of a tuple together, and tuples are stored\nsequentially in a \ufb01le. Such a storage layout is referred to as row-oriented storage .I nc o n -\ntrast, in column-oriented storage , each attribute of a relation is stored in a separate \ufb01le,\nwith values from successive tuples stored at successive positions in the \ufb01le. Assuming\n\ufb01xed-size data types, the value of attribute Aof the ith tuple of a relation can be found\n", "554": "526 Chapter 11 Data Analytics\nby accessing the \ufb01le corresponding to attribute Aand reading the value at o\ufb00set ( i\u22121)\ntimes the size (in bytes) of values in attribute A.\nColumn-oriented storage has at least two major bene\ufb01ts over row-oriented storage:\n1.When a query needs to access only a few attributes of a relation with a large\nnumber of attributes, the remaining attributes need not be fetched from disk into\nmemory. In contrast, in row-oriented storage, not only are irrelevant attributes\nfetched into memory, but they may also get prefetched into processor cache, wast-\ning cache space and memory bandwidth, if they are stored adjacent to attributes\nused in the query.\n2.Storing values of the same type together increases the e\ufb00ectiveness of compres-\nsion; compression can greatly reduce both the disk storage cost and the time to\nretrieve data from disk.\nOn the other hand, column-oriented storage has the drawback that storing or fetching\na single tuple requires multiple I/Ooperations.\nAs a result of these trade-o\ufb00s, column-oriented storage is not widely used for\ntransaction-processing applications. However, column-oriented storage is today widely\nused for data-warehousing applications, where accesses are rarely to individual tuples\nbut rather require scanning and aggregating multiple tuples. Column-oriented storage\nis described in more detail in Section 13.6.\nDatabase implementations that are designed purely for data warehouse applica-\ntions include Teradata, Sybase IQ, and Amazon Redshift. Many traditional databases\nsupport e\ufb03cient execution of data warehousing applications by adding features such\nas columnar storage; these include Oracle, SAP HANA ,M i c r o s o f t SQL S erver, and IBM\nDB2.\nIn the 2010s there has been an explosive growth in Big Data systems that are de-\nsigned to process queries over data stored in \ufb01les. Such systems are now a key part of\nthe data warehouse infrastructure. As we saw in Section 10.3, the motivation for such\nsystems was the growth of data generated by online systems in the form of log \ufb01les,\nwhich have a lot of valuable information that can be exploited for decision support.\nHowever, these systems can handle any kind of data, including relational data. Apache\nHadoop is one such system, and the Hive system allows SQL queries to be executed on\ntop of the Hadoop system.\nA number of companies provide software to optimize Hive query processing, in-\ncluding Cloudera and Hortonworks. Apache Spark is another popular Big Data system\nthat supports SQL queries on data stored in \ufb01les. Compressed \ufb01le structures contain-\ning records with columns, such as Orc and Parquet, are increasingly used to store such\nlog records, simplifying integration with SQL. Such \ufb01le formats are discussed in more\ndetail in Section 13.6.\n", "555": "11.3 Online Analytical Processing 527\n11.2.4 Data Lakes\nWhile data warehouses pay a lot of attention to ensuring a common data schema to\nease the job of querying the data, there are situations where organizations want to store\ndata without paying the cost of creating a common schema and transforming data to\nt h ec o m m o ns c h e m a .T h et e r m data lake is used to refer to a repository where data\ncan be stored in multiple formats, including structured records and unstructured \ufb01le\nformats. Unlike data warehouses, data lakes do not require up-front e\ufb00ort to preprocess\ndata, but they do require more e\ufb00ort when creating queries. Since data may be stored in\nmany di\ufb00erent formats, querying tools also need to be quite \ufb02exible. Apache Hadoop\nand Apache Spark are popular tools for querying such data, since they support querying\nof both unstructured and structured data.\n11.3 Online Analytical Processing\nData analysis often involves looking for patterns that arise when data values are\ngrouped in \u201cinteresting\u201d ways. As a simple example, summing credit hours for each\ndepartment is a way to discover which departments have high teaching responsibili-\nties. In a retail business, we might group sales by product, the date or month of the\nsale, the color or size of the product, or the pro\ufb01le (such as age group and gender) of\nthe customer who bought the product.\n11.3.1 Aggregation on Multidimensional Data\nConsider an application where a shop wants to \ufb01nd out what kinds of clothes are pop-\nular. Let us suppose that clothes are characterized by their item\n name, color, and size,\nand that we have a relation sales with the schema.\nsales (item\n name ,color ,clothes\n size,quantity )\nSuppose that item\n name can take on the values (skirt, dress, shirt, pants), color can\ntake on the values (dark, pastel, white), clothes\n sizecan take on values (small, medium,\nlarge), and quantity is an integer value representing the total number of items of a given\n{item\n name ,color ,clothes\n size }. An instance of the sales relation is shown in Figure 11.3.\nStatistical analysis often requires group ing on multiple attributes. The attribute\nquantity of the sales relation is a measure attribute, since it measures the number of\nunits sold, while item\n name ,color ,a n d clothes\n sizeare dimension attributes. (A more\nrealistic version of the sales relation would have additional dimensions, such as time\nand sales location, and additional measures such as monetary value of the sale.)\nTo analyze the multidimensional data, a manager may want to see data laid out as\nshown in the table in Figure 11.4. The table shows total quantities for di\ufb00erent combi-\nnations of item\n name and color .T h ev a l u eo f clothes\n sizeis speci\ufb01ed to be all, indicating\nthat the displayed values are a summary across all values of clothes\n size(i.e., we want\nto group the \u201csmall,\u201d \u201cmedium,\u201d and \u201clarge\u201d items into one single group.\n", "556": "528 Chapter 11 Data Analytics\nitem\n name\n color\n clothes\n size\n quantity\ndress\n dark\n small\n 2\ndress\n dark\n medium\n 6\ndress\n dark\n large\n 12\ndress\n pastel\n small\n 4\ndress\n pastel\n medium\n 3\ndress\n pastel\n large\n 3\ndress\n white\n small\n 2\ndress\n white\n medium\n 3\ndress\n white\n large\n 0\npants\n dark\n small\n 14\npants\n dark\n medium\n 6\npants\n dark\n large\n 0\npants\n pastel\n small\n 1\npants\n pastel\n medium\n 0\npants\n pastel\n large\n 1\npants\n white\n small\n 3\npants\n white\n medium\n 0\npants\n white\n large\n 2\nshirt\n dark\n small\n 2\nshirt\n dark\n medium\n 6\nshirt\n dark\n large\n 6\nshirt\n pastel\n small\n 4\nshirt\n pastel\n medium\n 1\nshirt\n pastel\n large\n 2\nshirt\n white\n small\n 17\nshirt\n white\n medium\n 1\nshirt\n white\n large\n 10\nskirt\n dark\n small\n 2\nskirt\n dark\n medium\n 5\nskirt\n dark\n large\n 1\nskirt\n pastel\n small\n 11\nskirt\n pastel\n medium\n 9\nskirt\n pastel\n large\n 15\nskirt\n white\n small\n 2\nskirt\n white\n medium\n 5\nskirt\n white\n large\n 3\nFigure 11.3 An example of sales relation.\nThe table in Figure 11.4 is an example of a cross-tabulation (orcross-tab , for short),\nalso referred to as a pivot-table . In general, a cross-tab is a table derived from a relation\n", "557": "11.3 Online Analytical Processing 529\nskirt\ndress\nshirt\npantscolor\nitem_nameclothes_size all\ndark pastel white total\ntotal8\n20\n14\n20\n6235\n10\n7\n2\n5410\n5\n28\n5\n4853\n35\n49\n27\n164\nFigure 11.4 Cross-tabulation of sales byitem\n name and color.\n(say R), where values for one attribute (say A) form the column headers and values for\nanother attribute (say B) form the row header. For example, in Figure 11.4, the attribute\ncolor corresponds to A(with values \u201cdark,\u201d \u201cpastel,\u201d and \u201cwhite\u201d), and the attribute item\nname corresponds to B(with values \u201cskirt,\u201d \u201cdress,\u201d \u201cshirt,\u201d and \u201cpants\u201d).\nEach cell in the pivot-table can be identi\ufb01ed by ( ai,bj), where aiis a value for A\nand bjav a l u ef o r B. The values of the various cells in the pivot-table are derived from\nthe relation Ras follows: If there is at most one tuple in Rwith any ( ai,bj)v a l u e ,t h e\nvalue in the cell is derived from that single tuple (if any); for instance, it could be the\nvalue of one or more other attributes of the tuple. If there can be multiple tuples with\nan (ai,bj) value, the value in the cell must be derived by aggregation on the tuples with\nthat value. In our example, the aggregation used is the sum of the values for attribute\nquantity , across all values for clothes\n size,a si n d i c a t e db y\u201c clothes\n size:all\u201da b o v et h e\ncross-tab in Figure 11.4. Thus, the value for cell (skirt, pastel) is 35, since there are\nthree tuples in the sales table that meet that criteria, with values 11, 9, and 15.\nIn our example, the cross-tab also has an extra column and an extra row storing\nthe totals of the cells in the row/column. Most cross-tabs have such summary rows and\ncolumns.\nThe generalization of a cross-tab, which is two-dimensional, to ndimensions can\nbe visualized as an n-dimensional cube, called the data cube .F i g u r e1 1 . 5s h o w sad a t a\ncube on the sales relation. The data cube has three dimensions, item\n name ,color ,a n d\nclothes\n size, and the measure attribute is quantity . Each cell is identi\ufb01ed by values for\nthese three dimensions. Each cell in the data cube contains a value, just as in a cross-\ntab. In Figure 11.5, the value contained in a cell is shown on one of the faces of the\ncell; other faces of the cell are shown blank if they are visible. All cells contain values,\neven if they are not visible. The value for a dimension may be all,i nw h i c hc a s et h ec e l l\ncontains a summary over all values of that dimension, as in the case of cross-tabs.\nThe number of di\ufb00erent ways in which the tuples can be grouped for aggregation\nc a nb el a r g e .I nt h ee x a m p l eo fF i g u r e1 1 . 5 ,t h e r ea r e3c o l o r s ,4i t e m s ,a n d3s i z e s\nresulting in a cube size of 3 \u00d74\u00d73=36. Including the summary values, we obtain a\n", "558": "530 Chapter 11 Data Analytics\n82 0 1 42 0 6 2\n35 10 7 2 54\n10 5 28 5 48\n53 35 49 27 16434\n21\n774\n9\n4216\n18\n45\nalllargemediumsmall\nskirt dress shirt pants allclothes_size25 3 1 1 1\n47 6 1 2 2 9\n28 5 7 2 2\ndark\npastel\nwhite\nall\nitem_namecolor\nFigure 11.5 Three-dimensional data cube.\n4\u00d75\u00d74 cube, whose size is 80. In fact, for a table with ndimensions, aggregation can\nbe performed with grouping on each of the 2nsubsets of the ndimensions.2\nAnonline analytic processing (OLAP ) system allows a data analyst to look at di\ufb00er-\ne n tc r o s s - t a b so nt h es a m ed a t ab y interactively selecting the attributes in the cross-tab.\nEach cross-tab is a two-dimensional view on a multidimensional data cube. For in-\nstance, the analyst may select a cross-tab on item\n name and clothes\n sizeor a cross-tab\noncolor and clothes\n size. The operation of changing the dimensions used in a cross-tab\nis called pivoting .\nOLAP systems allow an analyst to see a cross-tab on item\n name and color for a\n\ufb01xed value of clothes\n size, for example, large, instead of the sum across all sizes. Such\nan operation is referred to as slicing , since it can be thought of as viewing a slice of\nthe data cube. The operation is sometimes called dicing , particularly when values for\nmultiple dimensions are \ufb01xed.\nWhen a cross-tab is used to view a multidimensional cube, the values of dimension\nattributes that are not part of the cross-tab are shown above the cross-tab. The value\nof such an attribute can be all, as shown in Figure 11.4, indicating that data in the\ncross-tab are a summary over all values for the attribute. Slicing/dicing simply consists\nof selecting speci\ufb01c values for these attributes, which are then displayed on top of the\ncross-tab.\nOLAP systems permit users to view data at any desired level of granularity. The\noperation of moving from \ufb01ner-granularity data to a coarser granularity (by means\nof aggregation) is called a rollup . In our example, starting from the data cube on the\n2Grouping on the set of all ndimensions is useful only if the table may have duplicates.\n", "559": "11.3 Online Analytical Processing 531\nsales table, we got our example cross-tab by rolling up on the attribute clothes\n size.T h e\nopposite operation\u2014that of moving from coarser-granularity data to \ufb01ner-granularity\ndata\u2014is called a drill down . Finer-granularity data cannot be generated from coarse-\ngranularity data; they must be generated either from the original data or from even\n\ufb01ner-granularity summary data.\nAnalysts may wish to view a dimension at di\ufb00erent levels of detail. For instance,\nconsider an attribute of type datetime that contains a date and a time of day. Using\ntime precise to a second (or less) may not be meaningful: An analyst who is interested\nin rough time of day may look at only the hour value. An analyst who is interested in\nsales by day of the week may map the date to a day of the week and look only at that.\nAnother analyst may be interested in aggregates over a month, or a quarter, or for an\nentire year.\nThe di\ufb00erent levels of detail for an attribute can be organized into a hierarchy .\nFigure 11.6a shows a hierarchy on the datetime attribute. As another example, Figure\n11.6b shows a hierarchy on location, with the city being at the bottom of the hierarchy,\nstate above it, country at the next level, and region being the top level. In our earlier\nexample, clothes can be grouped by category (for instance, menswear or womenswear);\ncategory would then lie above item\n name in our hierarchy on clothes. At the level of\nactual values, skirts and dresses would fall under the womenswear category and pants\nand shirts under the menswear category.\nhour of day date\ndate timeday of week monthquarteryear\nstatecountryregion\ncity\n(a) time hierarchy (b) location hierarchy\nFigure 11.6 Hierarchies on dimensions.\n", "560": "532 Chapter 11 Data Analytics\nwomenswearcategory          item_name               colorclothes_size: all\ndark pastel white  total\ntotalmenswearskirt\nsubtotal dress8\n28208\n282010\n15553\n35\n14\n342014\n342028\n33\n62 62 4888\n76\n164549\n27pants\nsubtotalshirt\nFigure 11.7 Cross-tabulation of sales with hierarchy on item\n name .\nAn analyst may be interested in viewing sales of clothes divided as menswear and\nwomenswear, and not interested in individual values. After viewing the aggregates at\nthe level of womenswear and menswear, an analyst may drill down the hierarchy to look\nat individual values. An analyst looking at the detailed level may drill up the hierarchy\nand look at coarser-level aggregates. Both levels can be displayed on the same cross-tab,\nas in Figure 11.7.\n11.3.2 Relational Representation of Cross-Tabs\nA cross-tab is di\ufb00erent from relational tables usually stored in databases, since the num-\nber of columns in the cross-tab depends on the actual data. A change in the data values\nmay result in adding more columns, which is not desirable for data storage. However,\na cross-tab view is desirable for display to users. It is straightforward to represent a\ncross-tab without summary values in a relational form with a \ufb01xed number of columns.\nA cross-tab with summary rows/columns can be represented by introducing a special\nvalue allto represent subtotals, as in Figure 11.8. The SQL standard actually uses the\nnullvalue in place of all, but to avoid confusion with regular null values, we shall con-\ntinue to use all.\nConsider the tuples (skirt, all,all, 53) and (dress, all,all, 35). We have obtained\nthese tuples by eliminating individual tuples with di\ufb00erent values for color and clothes\nsize, and by replacing the value of quantity with an aggregate\u2014namely, the sum of\nthe quantities. The value allcan be thought of as representing the set of all values for\nan attribute. Tuples with the value allfor the color and clothes\n sizedimensions can be\nobtained by an aggregation on the sales relation with a group by on the column item\nname . Similarly, a group by oncolor ,clothes\n sizecan be used to get the tuples with the\nvalue allforitem\n name ,a n da group by with no attributes (which can simply be omitted\ninSQL) can be used to get the tuple with value allforitem\n name ,color ,a n d clothes\n size.\n", "561": "11.3 Online Analytical Processing 533\nitem\n name\n color\n clothes\n size\n quantity\nskirt\n dark\n all\n 8\nskirt\n pastel\n all\n 35\nskirt\n white\n all\n 10\nskirt\n all\n all\n 53\ndress\n dark\n all\n 20\ndress\n pastel\n all\n 10\ndress\n white\n all\n 5\ndress\n all\n all\n 35\nshirt\n dark\n all\n 14\nshirt\n pastel\n all\n 7\nshirt\n white\n all\n 28\nshirt\n all\n all\n 49\npants\n dark\n all\n 20\npants\n pastel\n all\n 2\npants\n white\n all\n 5\npants\n all\n all\n 27\nall\n dark\n all\n 62\nall\n pastel\n all\n 54\nall\n white\n all\n 48\nall\n all\n all\n 164\nFigure 11.8 Relational representation of the data in Figure 11.4.\nHierarchies can also be represented by relations. For example, the fact that skirts\nand dresses fall under the womenswear category and the pants and shirts under the\nmenswear category can be represented by a relation itemcategory (item\n name ,category ).\nThis relation can be joined with the sales relation to get a relation that includes the\ncategory for each item. Aggregation on this joined relation allows us to get a cross-\ntab with hierarchy. As another example, a hierarchy on city can be represented by a\nsingle relation city\n hierarchy (ID,city,state,country ,region ), or by multiple relations,\neach mapping values in one level of the hierarchy to values at the next level. We assume\nhere that cities have unique identi\ufb01ers, stored in the attribute ID, to avoid confusing\nbetween two cities with the same name, for example, the Spring\ufb01eld in Missouri and\nthe Spring\ufb01eld in Illinois.\n11.3.3 OLAP in SQL\nAnalysts using OLAP systems need answers to multiple aggregates to be generated in-\nteractively, without having to wait for mult iple minutes or hours. This led initially to\nthe development of specialized systems for OLAP (see Note 11.1 on page 535). Many\ndatabase systems now implement OLAP along with SQL constructs to express OLAP\nqueries. As we saw in Section 5.5.3, several SQL implementations, such as Microsoft\n", "562": "534 Chapter 11 Data Analytics\nSQL S erver and Oracle, support a pivot clause that allows creation of cross-tabs. Given\nthesales relation from Figure 11.3, the query:\nselect *\nfrom sales\npivot (\nsum(quantity )\nforcolor in('dark','pastel','white')\n)\norder by item\n name ;\nreturns the cross-tab shown in Figure 11.9. Note that the forclause within the pivot\nclause speci\ufb01es the color values that appear as attribute names in the pivot result. The\nattribute color itself does not appear in the result, although all other attributes are\nretained, except that the values for the newly created attributes are speci\ufb01ed to come\nfrom the attribute quantity .I nc a s em o r et h a no n et u p l ec o n t r i b u t e sv a l u e st oag i v e n\ncell, the aggregate operation within the pivot clause speci\ufb01es how the values should be\ncombined. In the above example, the quantity values are summed up.\nNote that the pivot clause by itself does not compute the subtotals we saw in the\npivot table from Figure 11.4. However, we can \ufb01rst generate the relational representa-\ntion shown in Figure 11.8, using a cube operation, as outlined shortly, and then apply\nthe pivot clause on that representation to get an equivalent result. In this case, the value\nallmust also be listed in the forclause, and the order by clause needs to be modi\ufb01ed to\norder allat the end.\nThe data in a data cube cannot be generated by a single SQLquery if we use only the\nbasic group by constructs, since aggregates are computed for several di\ufb00erent groupings\nitem\n name\n clothes\n size\n dark\n pastel\n white\ndress\n small\n 2\n 4\n 2\ndress\n medium\n 6\n 3\n 3\ndress\n large\n 12\n 3\n 0\npants\n small\n 14\n 1\n 3\npants\n medium\n 6\n 0\n 0\npants\n large\n 0\n 1\n 2\nshirt\n small\n 2\n 4\n 17\nshirt\n medium\n 6\n 1\n 1\nshirt\n large\n 6\n 2\n 10\nskirt\n small\n 2\n 11\n 2\nskirt\n medium\n 5\n 9\n 5\nskirt\n large\n 1\n 15\n 3\nFigure 11.9 Result of SQL pivot operation on the sales relation of Figure 11.3.\n", "563": "11.3 Online Analytical Processing 535\nNote 11.1 OLAP IMPLEMENTATION\nThe earliest OLAP systems used multidimensional arrays in memory to store data\ncubes and are referred to as multidimensional OLAP (MOLAP )systems. Later,\nOLAP facilities were integrated into relational systems, with data stored in a rela-\ntional database. Such systems are referred to as relational OLAP (ROLAP )systems.\nHybrid systems, which store some summaries in memory and store the base data\nand other summaries in a relational database, are called hybrid OLAP (HOLAP )\nsystems.\nMany OLAP systems are implemented as client-server systems. The server con-\ntains the relational database as well as any MOLAP data cubes. Client systems\nobtain views of the data by communicating with the server.\nAn a\u00a8\u0131ve way of computing the entire data cube (all groupings) on a relation is\nto use any standard algorithm for computing aggregate operations, one grouping at\nat i m e .T h en a \u00a8\u0131ve algorithm would require a large number of scans of the relation.\nA simple optimization is to compute an aggregation on, say, ( item\n name ,color )\nfrom an aggregation ( item\n name ,color ,clothes\n size), instead of from the original\nrelation. The amount of data read drops signi\ufb01cantly by computing an aggregate\nfrom another aggregate, instead of from the original relation. Further improve-\nments are possible; for instance, multiple groupings can be computed on a single\nscan of the data.\nEarly OLAP implementations precomputed and stored entire data cubes, that\nis, groupings on all subsets of the dimension attributes. Precomputation allows\nOLAP queries to be answered within a few seconds, even on datasets that may\ncontain millions of tuples adding up to gigabytes of data. However, there are 2n\ngroupings with ndimension attributes; hierarchies on attributes increase the num-\nber further. As a result, the entire data cube is often larger than the original relation\nthat formed the data cube and in many cases it is not feasible to store the entire\ndata cube.\nInstead of precomputing and storing all possible groupings, it makes sense to\nprecompute and store some of the groupings, and to compute others on demand.\nInstead of computing queries from the original relation, which may take a very\nlong time, we can compute them from other precomputed queries. For instance,\nsuppose that a query requires grouping by ( item\n name ,color ), and this has not\nbeen precomputed. The query result can be computed from summaries by ( item\nname ,color ,clothes\n size), if that has been precomputed. See the bibliographical\nnotes for references on how to select a good set of groupings for precomputation,\ngiven limits on the storage available for precomputed results.\n", "564": "536 Chapter 11 Data Analytics\nof the dimension attributes. Using only the basic group by construct, we would have\nto write many separate SQL queries and combine them using a union operation. SQL\nsupports special syntax to allow multiple group by operations to be speci\ufb01ed concisely.\nAs we saw in Section 5.5.4, SQL supports generalizations of the group by construct\nto perform the cube androllup operations. The cube androllup constructs in the group\nbyclause allow multiple group by queries to be run in a single query with the result\nreturned as a single relation in a style similar to that of the relation of Figure 11.8.\nConsider again our retail shop example and the relation:\nsales (item\n name ,color ,clothes\n size,quantity )\nIf we want to generate the entire data cube using individual group by queries, we\nhave to write a separate query for each of the following eight sets of group by attributes:\n{(item\n name ,color ,clothes\n size), (item\n name ,color ), (item\n name ,clothes\n size),\n(color ,clothes\n size), (item\n name ), (color ), (clothes\n size), () }\nwhere () denotes an empty group by list.\nAs we saw in Section 5.5.4, the cube construct allows us to accomplish this in one\nquery:\nselect item\n name , color, clothes\n size,sum(quantity )\nfrom sales\ngroup by cube (item\n name ,color ,clothes\n size);\nThe preceding query produces a relation whose schema is:\n(item\n name ,color ,clothes\n size,sum(quantity ))\nSo that the result of this query is indeed a relation, tuples in the result contain nullas\nthe value of those attributes not present in a particular grouping. For example, tuples\nproduced by grouping on clothes\n sizehave a schema ( clothes\n size,sum(quantity )). They\nare converted to tuples on ( item\n name ,color ,clothes\n size,sum(quantity )) by inserting\nnullforitem\n name and color .\nData cube relations are often very large. The cube query above, with 3 possible\ncolors, 4 possible item names, and 3 sizes, has 80 tuples. The relation of Figure 11.8\nis generated by doing a group by cube onitem\n name and color, with an extra column\nspeci\ufb01ed in the select clause showing allforclothes\n size.\nTo generate that relation in SQL, we substitute allfornullusing the grouping func-\ntion, as we saw earlier in Section 5.5.4. The grouping function distinguishes those nulls\ngenerated by OLAP operations from \u201cnormal\u201d nulls actually stored in the database or\narising from an outer join. Recall that the grouping function returns 1 if its argument\n", "565": "11.3 Online Analytical Processing 537\nis a null value generated by a cube orrollup and 0 otherwise. We may then operate on\nthe result of a call to grouping using case expressions to replace OLAP -generated nulls\nwith all. Then the relation in Figure 11.8, with occurrences of nullreplaced by all,c a n\nbe computed by the query:\nselect case when grouping (item\n name )=1 then 'all'\nelse item\n name end as item\n name ,\ncase when grouping (color )=1 then 'all'\nelse color end as color ,\n'all'asclothes\n size,sum(quantity )asquantity\nfrom sales\ngroup by cube (item\n name ,color );\nTherollup construct is the same as the cube construct except that rollup generates\nfewer group by queries. We saw that group by cube (item\n name ,color ,clothes\n size)g e n -\nerated all eight ways of forming a group by query using some (or all or none) of the\nattributes. In:\nselect item\n name ,color ,clothes\n size,sum(quantity )\nfrom sales\ngroup by rollup (item\n name ,color ,clothes\n size);\nthe clause group by rollup (item\n name ,color ,clothes\n size) generates only four groupings:\n{(item\n name ,color ,clothes\n size), (item\n name ,color ), (item\n name ), () }\nNotice that the order of the attributes in the rollup makes a di\ufb00erence; the last\nattribute ( clothes\n size, in our example) appears in only one grouping, the penultimate\n(second last) attribute in two groupings, and so on, with the \ufb01rst attribute appearing\ni na l lg r o u p sb u to n e( t h ee m p t yg r o u p i n g ) .\nWhy might we want the speci\ufb01c groupings that are used in rollup ?T h e s eg r o u p sa r e\nof frequent practical interest for hierarchies (as in Figure 11.6, for example). For the\nlocation hierarchy ( R e g i o n ,C o u n t r y ,S t a t e ,C i t y ), we may want to group by Region to get\nsales by region. Then we may want to \u201cdrill down\u201d to the level of countries within each\nregion, which means we would group by Region, Country . Drilling down further, we may\nwish to group by Region, Country, State and then by R e g i o n ,C o u n t r y ,S t a t e ,C i t y .T h e\nrollup construct allows us to specify this sequence of drilling down for further detail.\nAs we saw earlier in Section 5.5.4, multiple rollup sa n d cubes can be used in a single\ngroup by clause. For instance, the following query:\nselect item\n name ,color ,clothes\n size,sum(quantity )\nfrom sales\ngroup by rollup (item\n name ),rollup (color ,clothes\n size);\n", "566": "538 Chapter 11 Data Analytics\ngenerates the groupings:\n{(item\n name ,color ,clothes\n size), (item\n name ,color ), (item\n name ),\n(color ,clothes\n size), (color ), () }\nTo understand why, observe that rollup (item\n name ) generates two groupings, {(item\nname ), () },a n d rollup (color ,clothes\n size) generates three groupings, {(color ,clothes\nsize), (color ), () }. The Cartesian product of the two gives us the six groupings shown.\nNeither the rollup nor the cube clause gives complete control on the groupings\nthat are generated. For instance, we cannot use them to specify that we want only\ngroupings {(color ,clothes\n size), (clothes\n size,item\n name )}. Such restricted groupings can\nbe generated by using the grouping sets construct, in which one can specify the speci\ufb01c\nlist of groupings to be used. To obtain only groupings {(color ,clothes\n size), (clothes\n size,\nitem\n name )}, we would write:\nselect item\n name ,color ,clothes\n size,sum(quantity )\nfrom sales\ngroup by grouping sets ((color ,clothes\n size), (clothes\n size,item\n name ));\nSpecialized languages have been developed for querying multidimensional OLAP\nschemas, which allow some common tasks to be expressed more easily than with SQL.\nThese include the MDX and DAX query languages developed by Microsoft.\n11.3.4 Reporting and Visualization Tools\nReport generators are tools to generate human-readable summary reports from a\ndatabase. They integrate querying the database with the creation of formatted text and\nsummary charts (such as bar or pie charts). For example, a report may show the total\nsales in each of the past 2 months for each sales region.\nThe application developer can specify report formats by using the formatting fa-\ncilities of the report generator. Variables can be used to store parameters such as the\nmonth and the year and to de\ufb01ne \ufb01elds in the report. Tables, graphs, bar charts, or\nother graphics can be de\ufb01ned via queries on the database. The query de\ufb01nitions can\nmake use of the parameter values stored in the variables.\nOnce we have de\ufb01ned a report structure on a report-generator facility, we can store\nit and can execute it at any time to generate a report. Report-generator systems provide\na variety of facilities for structuring tabular output, such as de\ufb01ning table and column\nheaders, displaying subtotals for each group i n a table, automatically splitting long ta-\nbles into multiple pages, and displaying subtotals at the end of each page.\nFigure 11.10 is an example of a formatted report. The data in the report are gener-\nated by aggregation on information about orders.\nReport-generation tools are available from a variety of vendors, such as SAPCrystal\nReports and Microsoft ( SQL S erver Reporting Services). Several application suites,\nsuch as Microsoft O\ufb03ce, provide a way of embedding formatted query results from\n", "567": "11.3 Online Analytical Processing 539\nRegion Category Sales\nNorth Computer Hardware 1,000,000\n Computer Software 500,000\n All categories  1,500,000\nSouth Computer Hardware 200,000\n Computer Software 400,000\n All categories  600,000\n   2,100,000Acme Supply Company, Inc. \nQuarterly Sales Report\nPeriod:  Jan. 1 to March 31, 2009\nTotal SalesSubtotal\nFigure 11.10 Af o r m a t t e dr e p o r t .\na database directly into a document. Chart-generation facilities provided by Crystal\nReports or by spreadsheets such as Excel can be used to access data from databases and\nto generate tabular depictions of data or graphical depictions using charts or graphs.\nSuch charts can be embedded within text documents. The charts are created initially\nfrom data generated by executing queries against the database; the queries can be re-\nexecuted and the charts regenerated when required, to generate a current version of\nthe overall report.\nTechniques for data visualization , that is, graphical representation of data, that go\nbeyond the basic chart types, are very important for data analysis. Data-visualization\nsystems help users to examine large volumes of data and to detect patterns visually.\nVisual displays of data\u2014such as maps, charts, and other graphical representations\u2014\nallow data to be presented compactly to users. A single graphical screen can encode as\nmuch information as a far larger number of text screens.\nFor example, if the user wants to \ufb01nd out whether the occurrence of a disease is\ncorrelated to the locations of the patients, the locations of patients can be encoded in\na special color\u2014say, red\u2014on a map. The user can then quickly discover locations where\nproblems are occurring. The user may then form hypotheses about why problems are\noccurring in those locations and may verify the hypotheses quantitatively against the\ndatabase.\nAs another example, information about values can be encoded as a color and can\nbe displayed with as little as one pixel of screen area. To detect associations between\npairs of items, we can use a two-dimensional pixel matrix, with each row and each\ncolumn representing an item. The percentage of transactions that buy both items can\nbe encoded by the color intensity of the pixel. Items with high association will show\nup as bright pixels in the screen\u2014easy to detect against the darker background.\n", "568": "540 Chapter 11 Data Analytics\nIn recent years a number of tools have been developed for web-based data visual-\nization and for the creation of dashboards that display multiple charts showing key or-\nganizational information. These include Tableau ( www.tableau.com ), FusionCharts\n(www.fusioncharts.com ), plotly ( plot.ly ), Datawrapper ( www.datawrapper.de ), and\nGoogle Charts ( developers.google.com/chart ), among others. Since their display is\nbased on HTML and JavaScript, they can be used on a wide variety of browsers and on\nmobile devices.\nInteraction is a key element of visualization. For example, a user can \u201cdrill down\u201d\ninto areas of interest, such as moving from an aggregate view showing the total sales\nacross an entire year to the monthly sales \ufb01gures for a particular year. Analysts may wish\nto interactively add selection conditions to visualize subsets of data. Data visualization\ntools such as Tableau o\ufb00er a rich set of features for interactive visualization.\n11.4 Data Mining\nThe term data mining refers loosely to the process of analyzing large databases to \ufb01nd\nuseful patterns. Like knowledge discovery in arti\ufb01cial intelligence (also called machine\nlearning) or statistical analysis, data mining attempts to discover rules and patterns\nfrom data. However, data mining di\ufb00ers from traditional machine learning and statis-\ntics in that it deals with large volumes of data, stored primarily on disk. Today, many\nmachine-learning algorithms also work on very large volumes of data, blurring the dis-\ntinction between data mining and machine le arning. Data-mining techniques form part\nof the process of knowledge discovery in databases (KDD ).\nSome types of knowledge discovered from a database can be represented by a set\nofrules . The following is an example of a rule, stated informally: \u201cYoung women with\nannual incomes greater than $50,000 are the most likely people to buy small sports\ncars.\u201d Of course such rules are not universally true and have degrees of \u201csupport\u201d and\n\u201ccon\ufb01dence,\u201d as we shall see. Other types of knowledge are represented by equations\nrelating di\ufb00erent variables to each other. More generally, knowledge discovered by ap-\nplying machine-learning techniques on past instances in a database is represented by\namodel , which is then used for predicting outcomes for new instances. Features or at-\ntributes of instances are inputs to the model, and the output of a model is a prediction.\nThere are a variety of possible types of patterns that may be useful, and di\ufb00erent\ntechniques are used to \ufb01nd di\ufb00erent types of patterns. We shall study a few examples\nof patterns and see how they may be automatically derived from a database.\nUsually there is a manual component to data mining, consisting of preprocessing\ndata to a form acceptable to the algorithms and post-processing of discovered patterns\nto \ufb01nd novel ones that could be useful. There may also be more than one type of pattern\nthat can be discovered from a given database, and manual interaction may be needed\nto pick useful types of patterns. For this reason, data mining is really a semiautomatic\nprocess in real life. However, in our description we concentrate on the automatic aspect\nof mining.\n", "569": "11.4 Data Mining 541\n11.4.1 Types of Data-Mining Tasks\nThe most widely used applications of data mining are those that require some sort\nofprediction . For instance, when a person applies for a credit card, the credit-card\ncompany wants to predict if the person is a good credit risk. The prediction is to be\nbased on known attributes of the person, such as age, income, debts, and past debt-\nrepayment history. Rules for making the prediction are derived from the same attributes\nof past and current credit-card holders, along with their observed behavior, such as\nwhether they defaulted on their credit-card dues. Other types of prediction include\npredicting which customers may switch over to a competitor (these customers may be\no\ufb00ered special discounts to tempt them not to switch), predicting which people are\nlikely to respond to promotional mail (\u201cjunk mail\u201d), or predicting what types of phone\ncalling-card usage are likely to be fraudulent.\nAnother class of applications looks for associations , for instance, books that tend\nto be bought together. If a customer buys a book, an online bookstore may suggest other\nassociated books. If a person buys a camera, the system may suggest accessories that\ntend to be bought along with cameras. A good salesperson is aware of such patterns\nand exploits them to make additional sales. The challenge is to automate the process.\nOther types of associations may lead to discovery of causation. For instance, discovery\nof unexpected associations between a newly introduced medicine and cardiac problems\nled to the \ufb01nding that the medicine may cause cardiac problems in some people. The\nmedicine was then withdrawn from the market.\nAssociations are an example of descriptive patterns .Clusters are another example\nof such patterns. For example, over a century ago a cluster of typhoid cases was found\naround a well, which led to the discovery that the water in the well was contaminated\nand was spreading typhoid. Detection of clusters of disease remains important even\ntoday.\n11.4.2 Classification\nAbstractly, the classi\ufb01cation problem is this: Given that items belong to one of several\nclasses, and given past instances (called training instances )o fi t e m sa l o n gw i t ht h e\nclasses to which they belong, the problem is to predict the class to which a new item\nbelongs. The class of the new instance is not known, so other attributes of the instance\nmust be used to predict the class.\nAs an example, suppose that a credit-card company wants to decide whether or not\nto give a credit card to an applicant. The company has a variety of information about\nthe person, such as her age, educational background, annual income, and current debts,\nthat it can use for making a decision.\nTo make the decision, the company assigns a credit worthiness level of excellent,\ngood, average, or bad to each of a sample set of current or past customers according to\neach customer\u2019s payment history. These instances form the set of training instances.\nThen, the company attempts to learn rules or models that classify the credit-\nworthiness of a new applicant as excellent, good, average, or bad, on the basis of the\n", "570": "542 Chapter 11 Data Analytics\ninformation about the person, other than the actual payment history (which is unavail-\nable for new customers). There are a number of techniques for classi\ufb01cation, and we\noutline a few of them in this section.\n11.4.2.1 Decision-Tree Classi\ufb01ers\nDecision-tree classi\ufb01ers are a widely used technique for classi\ufb01cation. As the name\nsuggests, decision-tree classi\ufb01ers use a tree; each leaf node has an associated class, and\neach internal node has a predicate (or more generally, a function) associated with it.\nFigure 11.11 shows an example of a decision tree. To keep the example simple, we use\njust two attributes: education level (highest degree earned) and income.\nTo classify a new instance, we start at the root and traverse the tree to reach a leaf;\nat an internal node we evaluate the predicate (or function) on the data instance to \ufb01nd\nwhich child to go to. The process continues until we reach a leaf node. For example, if\nthe degree level of a person is masters, and the person\u2019s income is 40K, starting from\nthe root we follow the edge labeled \u201cmasters,\u201d and from there the edge labeled \u201c25K to\n75K,\u201d to reach a leaf. The class at the leaf is \u201cgood,\u201d so we predict that the credit risk\nof that person is good.\nThere are a number of techniques for building decision-tree classi\ufb01ers from a given\ntraining set. We omit details, but you can learn more details from the references pro-\nvided in the Further Reading section.\ndegree\nincome income income incomebachelors masters doctorate none\nbad average good\nbad average good excellent <50K >100K<25K >=25K\n>=50K <50K<25K >75K\n25 to 75K 50 to 100K\nFigure 11.11 Classification tree.\n", "571": "11.4 Data Mining 543\n11.4.2.2 Bayesian Classi\ufb01ers\nBayesian classi\ufb01ers \ufb01nd the distribution of attribute values for each class in the training\ndata; when given a new instance d, they use the distribution information to estimate,\nfor each class cj, the probability that instance dbelongs to class cj, denoted by p(cj|d),\nin a manner outlined here. The class with maximum probability becomes the predicted\nclass for instance d.\nTo \ufb01nd the probability p(cj|d) of instance dbeing in class cj, Bayesian classi\ufb01ers\nuseBayes\u2019 theorem ,w h i c hs a y s :\np(cj|d)=p(d|cj)p(cj)\np(d)\nwhere p(d|cj) is the probability of generating instance dgiven class cj,p(cj)i st h ep r o b -\nability of occurrence of class cj,a n d p(d) is the probability of instance doccurring. Of\nthese, p(d) can be ignored since it is the same for all classes. p(cj)i ss i m p l yt h ef r a c t i o n\nof training instances that belong to class cj.\nFor example, let us consider a special case where only one attribute, income ,i s\nused for classi\ufb01cation, and suppose we need to classify a person whose income is\n76,000. We assume that income values are broken up into buckets, and we assume\nthat the bucket containing 76,000 contains values in the range (75,000, 80,000). Sup-\npose among instances of class excellent , the probability of income being in (75,000,\n80,000) is 0 .1, while among instances of class good, the probability of income being in\n(75,000, 80,000) is 0 .05. Suppose also that overall 0 .1 fraction of people are classi\ufb01ed\nasexcellent ,a n d0 .3 are classi\ufb01ed as good. Then, p(d|cj)p(cj)f o rc l a s s excellent is.01,\nwhile for class good,i ti s0 .015. The person would therefore be classi\ufb01ed in class good.\nIn general, multiple attributes need to be considered for classi\ufb01cation. Then, \ufb01nd-\ningp(d|cj) exactly is di\ufb03cult, since it requires the distribution of instances of cj,a c r o s s\nall combinations of values for the attributes used for classi\ufb01cation. The number of such\ncombinations (for example of income buckets, with degree values and other attributes)\ncan be very large. With a limited training set used to \ufb01nd the distribution, most combi-\nnations would not have even a single training set matching them, leading to incorrect\nclassi\ufb01cation decisions. To avoid this problem, as well as to simplify the task of classi\ufb01-\ncation, naive Bayesian classi\ufb01ers assume attributes have independent distributions and\nthereby estimate:\np(d|cj)=p(d1|cj)\u2217p(d2|cj)\u2217\u22ef\u2217p(dn|cj)\nThat is, the probability of the instance doccurring is the product of the probability of\noccurrence of each of the attribute values diofd, given the class is cj.\nThe probabilities p(di|cj) derive from the distribution of values for each attribute i,\nfor each class cj. This distribution is computed from the training instances that belong\nto each class cj; the distribution is usually approximated by a histogram. For instance,\nwe may divide the range of values of attribute iinto equal intervals, and store the frac-\ntion of instances of class cjthat fall in each interval. Given a value difor attribute i,t h e\n", "572": "544 Chapter 11 Data Analytics\nvalue of p(di|cj) is simply the fraction of instances belonging to class cjthat fall in the\ninterval to which dibelongs.\n11.4.2.3 Support Vector Machine Classi\ufb01ers\nTheSupport Vector Machine (SVM) is a type of classi\ufb01er that has been found to give\nvery accurate classi\ufb01cation across a range of applications. We provide some basic in-\nformation about Support Vector Machine classi\ufb01ers here; see the references in the\nbibliographical notes for further information.\nSupport Vector Machine classi\ufb01ers can best be understood geometrically. In the\nsimplest case, consider a set of points in a two-dimensional plane, some belonging to\nclass A, and some belonging to class B. We are given a training set of points whose class\n(AorB) is known, and we need to build a classi\ufb01er of points using these training points.\nThis situation is illustrated in Figure 11.12, where the points in class Aare denoted by\nXmarks, while those in class Bare denoted by Omarks.\nSuppose we can draw a line on the plane, such that all points in class Alie to one\nside and all points in line Blie to the other. Then, the line can be used to classify\nnew points, whose class we don\u2019t already know. But there may be many possible such\nlines that can separate points in class Afrom points in class B. A few such lines are\nshown in Figure 11.12. The Support Vector Machine classi\ufb01er chooses the line whose\ndistance from the nearest point in either class (from the points in the training dataset)\nis maximum. This line (called the maximum margin line ) is then used to classify other\npoints into class AorB, depending on which side of the line they lie on. In Figure 11.12,\nthe maximum margin line is shown in bold, while the other lines are shown as dashed\nlines.\nFigure 11.12 Example of a Support Vector Machine classifier.\n", "573": "11.4 Data Mining 545\nThe preceding intuition can be generalized to more than two dimensions, allow-\ning multiple attributes to be used for classi\ufb01cation; in this case, the classi\ufb01er \ufb01nds a\ndividing plane, not a line. Further, by \ufb01rst transforming the input points using certain\nfunctions, called kernel functions , Support Vector Machine classi\ufb01ers can \ufb01nd nonlin-\near curves separating the sets of points. This is important for cases where the points\nare not separable by a line or plane. In the presence of noise, some points of one class\nmay lie in the midst of points of the other class. In such cases, there may not be any\nline or meaningful curve that separates the points in the two classes; then, the line or\ncurve that most accurately divides the points into the two classes is chosen.\nAlthough the basic formulation of Support Vector Machines is for binary classi-\n\ufb01ers, i.e., those with only two classes, they can be used for classi\ufb01cation into multiple\nc l a s s e sa sf o l l o w s :I ft h e r ea r e Nclasses, we build Nclassi\ufb01ers, with classi\ufb01er iperform-\ning a binary classi\ufb01cation, classifying a point either as in class ior not in class i.G i v e n\na point, each classi\ufb01er ialso outputs a value indicating how related a given point is to\nclass i. We then apply all Nclassi\ufb01ers on a given point and choose the class for which\nthe relatedness value is the highest.\n11.4.2.4 Neural Network Classi\ufb01ers\nNeural-net classi\ufb01ers use the training data to train arti\ufb01cial neural nets. There is a large\nbody of literature on neural nets; we do not provide details here, but we outline a few\nkey properties of neural network classi\ufb01ers.\nNeural networks consist of several layers of \u201cneurons,\u201d each of which are connected\nto neurons in the preceding layer. An input instance of the problem is fed to the \ufb01rst\nlayer; neurons at each layer are \u201cactivated\u201d based on some function applied to the\ninputs at the preceding layer. The function applied at each neuron computes a weighted\ncombination of the activations of the input neurons and generates an output based on\nthe weighted combination. The activation of a neuron in one layer thus a\ufb00ects the\nactivation of neurons in the next layer. The \ufb01nal output layer typically has one neuron\ncorresponding to each class of the classi\ufb01cation problem being addressed. The neuron\nwith maximum activation for a given input decides the predicted class for that input.\nKey to the success of a neural network is the weights used in the computation\ndescribed above. These weights are learned, based on training data. They are initially\nset to some default value, and then training data are used to learn the weights. Training\nis typically done by applying each input to the current state of the neural network and\nchecking if the prediction is correct. If not, a backpropagation algorithm is used to tweak\nthe weights of the neurons in the network, to bring the prediction closer to the correct\none for the current input. Repeating this process results in a trained neural network,\nwhich can then be used for classi\ufb01cation on new inputs.\nIn recent years, neural networks have achieved a great degree of success for tasks\nwhich were earlier considered very hard, such as vision (e.g., recognition of objects\nin images), speech recognition, and natural language translation. A simple example\nof a vision task is that of identifying the species, such as cat or dog, given an image of\n", "574": "546 Chapter 11 Data Analytics\nan animal; such problems are basically classi \ufb01cation problems. Other examples include\nidentifying object occurrences in an image and assigning a class label to each identi\ufb01ed\nobject.\nDeep neural networks , which are neural networks with a large number of layers,\nhave proven very successful at such tasks, if given a very large number of training in-\nstances. The term deep learning refers to the machine-learning techniques that create\nsuch deep neural networks, and train them on very large numbers of training instances.\n11.4.3 Regression\nRegression deals with the prediction of a value, rather than a class. Given values for a\nset of variables, X1,X2,\u2026,Xn,w ew i s ht op r e d i c tt h ev a l u eo fav a r i a b l e Y.F o ri n s t a n c e ,\nwe could treat the level of education as a number and income as another number, and,\non the basis of these two variables, we wish to predict the likelihood of default, which\ncould be a percentage chance of defaulting, or the amount involved in the default.\nOne way is to infer coe\ufb03cients a0,a1,a2,\u2026,ansuch that:\nY=a0+a1\u2217X1+a2\u2217X2+\u22ef+an\u2217Xn\nFinding such a linear polynomial is called linear regression . In general, we wish to \ufb01nd\na curve (de\ufb01ned by a polynomial or other formula) that \ufb01ts the data; the process is also\ncalled curve \ufb01tting .\nThe \ufb01t may be only approximate, because of noise in the data or because the re-\nlationship is not exactly a polynomial, so regression aims to \ufb01nd coe\ufb03cients that give\nthe best possible \ufb01t. There are standard techniques in statistics for \ufb01nding regression\ncoe\ufb03cients. We do not discuss these techniques here, but the bibliographical notes\nprovide references.\n11.4.4 Association Rules\nRetail shops are often interested in associations between di\ufb00erent items that people\nbuy. Examples of such associations are:\n\u2022Someone who buys bread is quite likely also to buy milk.\n\u2022A person who bought the book Database System Concepts is quite likely also to buy\nthe book Operating System Concepts .\nAssociation information can be used in several ways. When a customer buys a particu-\nlar book, an online shop may suggest associated books. A grocery shop may decide to\nplace bread close to milk, since they are often bought together, to help shoppers \ufb01nish\ntheir task faster. Or, the shop may place them at opposite ends of a row and place other\nassociated items in between to tempt people to buy those items as well as the shoppers\nwalk from one end of the row to the other. A shop that o\ufb00ers discounts on one associ-\n", "575": "11.4 Data Mining 547\nated item may not o\ufb00er a discount on the other, since the customer will probably buy\nthe other anyway.\nAn example of an association rule is:\nbread\u21d2milk\nIn the context of grocery-store purchases, the rule says that customers who buy bread\nalso tend to buy milk with a high probability. An association rule must have an as-\nsociated population : The population consists of a set of instances . In the grocery-store\nexample, the population may consist of all grocery-store purchases; each purchase is an\ninstance. In the case of a bookstore, the population may consist of all people who made\npurchases, regardless of when they made a purchase. Each customer is an instance. In\nthe bookstore example, the analyst has decided that when a purchase is made is not\nsigni\ufb01cant, whereas for the grocery-store example, the analyst may have decided to\nconcentrate on single purchases, ignoring multiple visits by the same customer.\nRules have an associated support , as well as an associated con\ufb01dence .T h e s ea r e\nde\ufb01ned in the context of the population:\n\u2022Support is a measure of what fraction of the population satis\ufb01es both the an-\ntecedent and the consequent of the rule.\nFor instance, suppose only 0 .001 percent of all purchases include milk and\nscrewdrivers. The support for the rule:\nmilk\u21d2screwdrivers\nis low. The rule may not even be statistically signi\ufb01cant\u2014perhaps there was only a\nsingle purchase that included both milk and screwdrivers. Businesses are usually\nnot interested in rules that have low support, since they involve few customers and\nare not worth bothering about.\nOn the other hand, if 50 percent of all purchases involve milk and bread, then\nsupport for rules involving bread and milk (and no other item) is relatively high,\nand such rules may be worth attention. Exactly what minimum degree of support\nis considered desirable depends on the application.\n\u2022Con\ufb01dence is a measure of how often the consequent is true when the antecedent\nis true. For instance, the rule:\nbread\u21d2milk\nhas a con\ufb01dence of 80 percent if 80 percent of the purchases that include bread\nalso include milk. A rule with a low con\ufb01dence is not meaningful. In business\napplications, rules usually have con\ufb01dences signi\ufb01cantly less than 100 percent,\nwhereas in other domains, such as in physics, rules may have high con\ufb01dences.\nNote that the con\ufb01dence of bread\u21d2milk may be very di\ufb00erent from the\ncon\ufb01dence of milk\u21d2bread ,a l t h o u g hb o t hh a v et h es a m es u p p o r t .\n", "576": "548 Chapter 11 Data Analytics\n11.4.5 Clustering\nIntuitively, clustering refers to the problem of \ufb01nding clusters of points in the given\ndata. The problem of clustering can be formalized from distance metrics in several\nways. One way is to phrase it as the problem of grouping points into ksets (for a given\nk) so that the average distance of points from the centroid of their assigned cluster\nis minimized.3Another way is to group points so that the average distance between\nevery pair of points in each cluster is minimized. There are other de\ufb01nitions too; see\nthe bibliographical notes for details. But the intuition behind all these de\ufb01nitions is to\ngroup similar points together in a single set.\nAnother type of clustering appears in classi\ufb01cation systems in biology. (Such clas-\nsi\ufb01cation systems do not attempt to predict classes; rather they attempt to cluster re-\nlated items together.) For instance, leopards and humans are clustered under the class\nmammalia, while crocodiles and snakes are clustered under reptilia. Both mammalia\nand reptilia come under the common class chordata. The clustering of mammalia has\nfurther subclusters, such as carnivora and primates. We thus have hierarchical cluster-\ning. Given characteristics of di\ufb00erent species, biologists have created a complex hier-\narchical clustering scheme grouping related species together at di\ufb00erent levels of the\nhierarchy.\nThe statistics community has studied clustering extensively. Database research has\nprovided scalable clustering algorithms that can cluster very large datasets (that may\nn o t\ufb01 ti nm e m o r y ) .\nAn interesting application of clustering is to predict what new movies (or books or\nmusic) a person is likely to be interested in on the basis of:\n1.The person\u2019s past preferences in movies.\n2.Other people with similar past preferences.\n3.The preferences of such people for new movies.\nOne approach to this problem is as follows: To \ufb01nd people with similar past preferences\nwe create clusters of people based on their preferences for movies. The accuracy of\nclustering can be improved by previously clustering movies by their similarity, so even\nif people have not seen the same movies, if they have seen similar movies they would\nbe clustered together. We can repeat the clustering, alternately clustering people, then\nmovies, then people, and so on until we reach an equilibrium. Given a new user, we\n\ufb01nd a cluster of users most similar to that user, on the basis of the user\u2019s preferences\nfor movies already seen. We then predict movies in movie clusters that are popular\nwith that user\u2019s cluster as likely to be interesting to the new user. In fact, this problem\n3The centroid of a set of points is de\ufb01ned as a point whose coordinate on each dimension is the average of the coor-\ndinates of all the points of that set on that dimension. For example, in two dimensions, the centroid of a set of points\n{(x1,y1), (x2,y2),\u2026,(xn,yn)}is given by(\u2211n\ni=1xi\nn,\u2211n\ni=1yi\nn)\n.\n", "577": "11.4 Data Mining 549\nis an instance of collaborative \ufb01ltering , where users collaborate in the task of \ufb01ltering\ninformation to \ufb01nd information of interest.\n11.4.6 Text Mining\nText mining applies data-mining techniques to textual documents. There are a number\nof di\ufb00erent text mining tasks. One such task is sentiment analysis . For example, sup-\npose a company wishes to \ufb01nd out how users have reacted to a new product. There\nare typically a large number of product reviews on the web\u2014for example, reviews by\ndi\ufb00erent users on e-commerce platforms. Reading each review to \ufb01nd out reactions\nis not practical for a human. Instead, the company may analyze reviews to \ufb01nd the\nsentiment of the reviews of the product; the sentiment could be positive, negative, or\nneutral. The occurrence of speci\ufb01c words such as excellent, good, awesome, beautiful,\nand so on are correlated with a positive sentiment, while words such as awful, average,\nworthless, poor quality, and so on are correlated with a negative sentiment. Sentiment\nanalysis techniques can be used to analyze the reviews and come up with an overall\nscore re\ufb02ecting the broad sense of the reviews.\nAnother task is information extraction , which creates structured information from\nunstructured textual descriptions, or semi -structured data such as tabular displays of\ndata in documents. A key subtask of this process is entity recognition ,t h a ti s ,t h et a s ko f\nidentifying mentions of entities in text and disambiguating them. For example, an arti-\ncle may mention the name Michael Jordan. There are at least two famous people named\nMichael Jordan: one was a basketball player, while the other is a professor who is a well\nknown machine-learning expert. Disambiguation is the process of \ufb01guring out which\nof these two is being referred to in a particular article, and it can be done based on the\narticle context; in this case, an occurrence of the name Michael Jordan in a sports arti-\ncle probably refers to the basketball player, while an occurrence in a machine-learning\npaper probably refers to the professor. After entity recognition, other techniques may\nbe used to learn attributes of entities and to learn relationships between entities.\nInformation extraction can be used in many ways. For example, it can be used to\nanalyze customer support conversations or reviews posted on social media, to judge\ncustomer satisfaction, and to decide when intervention is needed to retain customers.\nService providers may want to know what aspect of the service such as pricing, quality,\nhygiene, or behavior of the person providing the service, a review was positive or nega-\ntive about; information extraction techniques can be used to infer what aspect an article\nor a part of an article is about, and to infer the associated sentiment. Attributes such\nas the location of service can also be extracted and are important for taking corrective\naction.\nInformation extracted from the enormous collection of documents and other re-\nsources on the web can be valuable for many tasks. Such extracted information can be\nrepresented in a graph, called a knowledge graph , which we outlined in Section 8.1.4.\nSuch knowledge graphs are used by web search engines to generate more meaningful\nanswers to user queries.\n", "578": "550 Chapter 11 Data Analytics\n11.5 Summary\n\u2022Data analytics systems analyze online data collected by transaction-processing sys-\ntems, along with data collected from other sources, to help people make business\ndecisions. Decision-support systems come in various forms, including OLAP sys-\ntems and data-mining systems.\n\u2022Data warehouses help gather and archive important operational data. Warehouses\nare used for decision support and analysis on historical data, for instance, to pre-\ndict trends. Data cleansing from input data sources is often a major task in data\nwarehousing. Warehouse schemas tend to be multidimensional, involving one or\na few very large fact tables and several much smaller dimension tables.\n\u2022Online analytical processing ( OLAP ) tools help analysts view data summarized in\ndi\ufb00erent ways, so that they can gain insight into the functioning of an organization.\n\u00b0OLAP tools work on multidimensional data, characterized by dimension at-\ntributes and measure attributes.\n\u00b0The data cube consists of multidimensional data summarized in di\ufb00erent ways.\nPrecomputing the data cube helps speed up queries on summaries of data.\n\u00b0Cross-tab displays permit users to view two dimensions of multidimensional\ndata at a time, along with summaries of the data.\n\u00b0Drill down, rollup, slicing, and dicing are among the operations that users per-\nform with OLAP tools.\n\u2022The SQL standard provides a variety of operators for data analysis, including cube,\nrollup ,a n d pivot operations.\n\u2022Data mining is the process of semiautomatically analyzing large databases to \ufb01nd\nuseful patterns. There are a number of applications of data mining, such as predic-\ntion of values based on past examples, \ufb01nding of associations between purchases,\nand automatic clustering of people and movies.\n\u2022Classi\ufb01cation deals with predicting the class of test instances by using attributes\nof the test instances, based on attributes of training instances, and the actual class\nof training instances. There are several types of classi\ufb01ers, such as:\n\u00b0Decision-tree classi\ufb01ers, which perform classi\ufb01cation by constructing a tree\nbased on training instances with leaves having class labels.\n\u00b0Bayesian classi\ufb01ers, which are based on probability theory.\n\u00b0The support vector machine is another widely used classi\ufb01cation technique.\n\u00b0Neural networks, and in particular deep learning, has been very successful in\nclassi\ufb01cation and related tasks in the con text of vision, speech recognition, and\nlanguage understanding and translation.\n", "579": "Practice Exercises 551\n\u2022Association rules identify items that co-occur frequently, for instance, items that\ntend to be bought by the same customer. Correlations look for deviations from\nexpected levels of association.\n\u2022Other types of data mining include clustering and text mining.\nReview Terms\n\u2022Decision-support systems\n\u2022Business intelligence\n\u2022Data warehousing\n\u00b0Gathering data\n\u00b0Source-driven architecture\n\u00b0Destination-driven architecture\n\u00b0Data cleansing\n\u00b0Extract, transform, load ( ETL)\n\u00b0Extract, load, transform ( ELT)\n\u2022Warehouse schemas\n\u00b0Fact table\n\u00b0Dimension tables\n\u00b0Star schema\n\u00b0Snow\ufb02ake schema\n\u2022Column-oriented storage\n\u2022Online analytical processing\n(OLAP )\n\u2022Multidimensional data\n\u00b0Measure attributes\n\u00b0Dimension attributes\n\u00b0Hierarchy\n\u00b0Cross-tabulation / Pivoting\n\u00b0Data cube\u00b0Rollup and drill down\n\u00b0SQL group by cube ,group by rollup\n\u2022Data visualization\n\u2022Data mining\n\u2022Prediction\n\u2022Classi\ufb01cation\n\u00b0Training data\n\u00b0Test data\n\u2022Decision-tree classi\ufb01ers\n\u2022Bayesian classi\ufb01ers\n\u00b0Bayes\u2019 theorem\n\u00b0Naive Bayesian classi\ufb01ers\n\u2022Support Vector Machine ( SVM )\n\u2022Regression\n\u2022Neural-networks\n\u2022Deep learning\n\u2022Association rules\n\u2022Clustering\n\u2022Text mining\n\u00b0Sentiment analysis\n\u00b0Information extraction\n\u00b0Named entity recognition\n\u00b0Knowledge graph\n", "580": "552 Chapter 11 Data Analytics\nPractice Exercises\n11.1 Describe bene\ufb01ts and drawbacks of a source-driven architecture for gathering\nof data at a data warehouse, as compared to a destination-driven architecture.\n11.2 Draw a diagram that shows how the classroom relation of our university exam-\nple as shown in Appendix A would be stored under a column-oriented storage\nstructure.\n11.3 Consider the takes relation. Write an SQL query that computes a cross-tab\nthat has a column for each of the years 2017 and 2018, and a column for all,\nand one row for each course, as well as a row for all.E a c hc e l li nt h et a b l e\nshould contain the number of students who took the corresponding course in\nthe corresponding year, with column allcontaining the aggregate across all\nyears, and row allcontaining the aggregate across all courses.\n11.4 Consider the data warehouse schema depicted in Figure 11.2. Give an SQL\nquery to summarize sales numbers and price by store and date, along with the\nhierarchies on store and date.\n11.5 Classi\ufb01cation can be done using classi\ufb01cation rules , which have a condition ,a\nclass,a n da con\ufb01dence ; the con\ufb01dence is the percentage of the inputs satisfying\nthe condition that fall in the speci\ufb01ed class.\nFor example, a classi\ufb01cation rule for credit ratings may have a condition\nthat salary is between $30,000 and $50,000, and education level is graduate,\nwith the credit rating class of good, and a con\ufb01dence of 80%. A second rule may\nhave a condition that salary is between $30,000 and $50,000, and education\nlevel is high-school, with the credit rating class of satisfactory , and a con\ufb01dence\nof 80%. A third rule may have a condition that salary is above $50,001, with\nthe credit rating class of excellent , and con\ufb01dence of 90%. Show a decision tree\nclassi\ufb01er corresponding to the above rules.\nShow how the decision tree classi\ufb01er can be extended to record the con\ufb01-\ndence values.\n11.6 Consider a classi\ufb01cation problem where the classi\ufb01er predicts whether a per-\nson has a particular disease. Suppose that 95% of the people tested do not\nsu\ufb00er from the disease. Let posdenote the fraction of true positives ,w h i c hi s\n5% of the test cases, and let negdenote the fraction of true negatives ,w h i c hi s\n95% of the test cases. Consider the following classi\ufb01ers:\n\u2022Classi\ufb01er C1, which always predicts negative (a rather useless classi\ufb01er, of\ncourse).\n\u2022Classi\ufb01er C2, which predicts positive in 80% of the cases where the person\nactually has the disease but also pred icts positive in 5% of the cases where\nthe person does not have the disease.\n", "581": "Exercises 553\n\u2022Classi\ufb01er C3, which predicts positive in 95% of the cases where the person\nactually has the disease but also pred icts positive in 20% of the cases where\nthe person does not have the disease.\nFor each classi\ufb01er, let t\nposdenote the true positive fraction, that is the fraction\nof cases where the classi\ufb01er prediction was positive, and the person actually\nhad the disease. Let f\nposdenote the false positive fraction, that is the fraction\nof cases where the prediction was positive, but the person did not have the\ndisease. Let t\nnegdenote true negative and f\nnegdenote false negative fractions,\nwhich are de\ufb01ned similarly, but for the cases where the classi\ufb01er prediction\nwas negative.\na. Compute the following metrics for each classi\ufb01er:\ni.Accuracy , de\ufb01ned as ( t\npos+t\nneg)\u2215(pos+neg), that is, the fraction of\nthe time when the classi\ufb01er gives the correct classi\ufb01cation.\nii. Recall (also known as sensitivity ) de\ufb01ned as t\npos\u2215pos, that is, how\nmany of the actual positive cases are classi\ufb01ed as positive.\niii. Precision , de\ufb01ned as t\npos/(t\npos+f\npos), that is, how often the positive\nprediction is correct.\niv. Speci\ufb01city , de\ufb01ned as t\nneg/neg.\nb. If you intend to use the results of classi\ufb01cation to perform further screen-\ning for the disease, how would you choose between the classi\ufb01ers?\nc. On the other hand, if you intend to use the result of classi\ufb01cation to start\nmedication, where the medication could have harmful e\ufb00ects if given to\nsomeone who does not have the disease, how would you choose between\nthe classi\ufb01ers?\nExercises\n11.7 Why is column-oriented storage potentially advantageous in a database system\nthat supports a data warehouse?\n11.8 Consider each of the takes and teaches relations as a fact table; they do not have\nan explicit measure attribute, but assu me each table has a measure attribute\nreg\ncount whose value is always 1. What would the dimension attributes and\ndimension tables be in each case. Would the resultant schemas be star schemas\nor snow\ufb02ake schemas?\n11.9 Consider the star schema from Figure 11.2. Suppose an analyst \ufb01nds that\nmonthly total sales (sum of the price values of all sales tuples) have decreased,\ninstead of growing, from April 2018 to May 2018. The analyst wishes to check\nif there are speci\ufb01c item categories, stores, or customer countries that are re-\nsponsible for the decrease.\n", "582": "554 Chapter 11 Data Analytics\na. What are the aggregates that the analyst would start with, and what are\nthe relevant drill-down operations that the analyst would need to exe-\ncute?\nb. Write an SQL query that shows the item categories that were responsible\nfor the decrease in sales, ordered by the impact of the category on the\nsales decrease, with categories that had the highest impact sorted \ufb01rst.\n11.10 Suppose half of all the transactions in a clothes shop purchase jeans, and one-\nthird of all transactions in the shop purchase T-shirts. Suppose also that half\nof the transactions that purchase jeans also purchase T-shirts. Write down all\nthe (nontrivial) association rules you can deduce from the above information,\ngiving support and con\ufb01dence of each rule.\n11.11 The organization of parts, chapters, sections, and subsections in a book is re-\nlated to clustering. Explain why, and to what form of clustering.\n11.12 Suggest how predictive mining techniques can be used by a sports team, using\nyour favorite sport as an example.\nTools\nData warehouse systems are available from Teradata, Teradata Aster, SAP IQ (formerly\nknown as Sybase IQ), and Amazon Redshift, all of which support parallel process-\ning across a large number of machines. A number of databases including Oracle, SAP\nHANA ,M i c r o s o f t SQL S erver, and IBM DB2 support data warehouse applications by\nadding features such as columnar storage. There are a number of commercial ETL\ntools including tools from Informatica, Bus iness Objects, IBM InfoSphere, Microsoft\nAzure Data Factory, Microsoft SQL Server Integration Services, Oracle Warehouse\nBuilder, and Pentaho Data Integration. Open-source ETL tools include Apache NiFi\n(nifi.apache.org ), Jasper ETL (www.jaspersoft.com/data-integration ) and Talend\n(sourceforge.net/projects/talend-studio ). Apache Kafka ( kafka.apache.org )c a n\nalso be used to build ETL systems.\nMost database vendors provide OLAP tools as part of their database systems, or as\nadd-on applications. These include OLAP tools from Microsoft Corp., Oracle, IBMand\nSAP. The Mondrian OLAP server ( github.com/pentaho/mondrian )i sa no p e n - s o u r c e\nOLAP server. Apache Kylin ( kylin.apache.org ) is an open-source distributed analytics\nengine which can process data stored in Hadoop, build OLAP cubes and store them in\nthe HBase key-value store, and then query the stored cubes using SQL.M a n yc o m p a -\nnies also provide analysis tools for speci\ufb01c applications, such as customer relationship\nmanagement.\nTools for visualization include Tableau ( www.tableau.com ), FusionCharts\n(www.fusioncharts.com ), plotly ( plot.ly ), Datawrapper ( www.datawrapper.de ), and\nGoogle Charts ( developers.google.com/chart ).\n", "583": "Further Reading 555\nThe Python language is very popular for machine-learning tasks, due to the avail-\nability of a number of open-source libraries fo r machine-learning tasks. The R language\nis also used widely for statistical analysis and machine learning, for the same reasons.\nPopular utility libraries in Python include are NumPy ( www.numpy.org )w h i c hp r o -\nvides operations on arrays and matrices, SciPy ( www.scipy.org ), which provides lin-\near algebra, optimization and statistics functions, and Pandas ( pandas.pydata.org ),\nwhich provides a relational abstraction of data. Popular machine-learning libraries\nin Python include SciKit-Learn ( scikit-learn.org ), which adds image-processing and\nmachine-learning functionality to SciPy. Deep learning libraries in Python include\nKeras ( keras.io ), and TensorFlow ( www.tensorflow.org ) which was developed by\nGoogle; TensorFlow provides APIs in several languages, with particularly good support\nfor Python. Text mining is supported by natural language processing libraries, such as\nNLTK ( www.nltk.org ) and web crawling libraries, such as Scrapy ( scrapy.org ). Visu-\nalization is supported by libraries such as Matplotlib ( matplotlib.org ), Plotly ( plot.ly )\nand Bokeh ( bokeh.pydata.org ).\nOpen-source tools for data mining include RapidMiner ( rapidminer.com ), Weka\n(www.cs.waikato.ac.nz/ml/weka ), and Orange ( orange.biolab.si ). Commercial tools\ninclude SASEnterprise Miner, IBM Intelligent Miner, and Oracle Data Mining.\nFurther Reading\n[Kimball et al. (2008)] and [Kimball and Ross (2013)] provide textbook coverage of\ndata warehouses and multidimensional modeling.\n[Mitchell (1997)] is a classic textbook on machine learning and covers classi\ufb01ca-\ntion techniques in detail. [Goodfellow et al. (2016)] is a de\ufb01nitive text on deep learning.\n[Witten et al. (2011)] and [Han et al. (2011)] provide textbook coverage of data mining.\n[Agrawal et al. (1993)] introduced the notion of association rules.\nInformation about the R language and environment may be found at\nwww.r-project.org ; information about the SparkR package, which provides an R fron-\ntend to Apache Spark, may be found at spark.apache.org/docs/latest/sparkr.html .\n[Chakrabarti (2002)], [Manning et al. ( 2008)] and [Baeza-Yates and Ribeiro-Neto\n(2011)] provide textbook description of information retrieval, including extensive cov-\nerage of data-mining tasks related to textual and hypertext data, such as classi\ufb01cation\nand clustering.\nBibliography\n[Agrawal et al. (1993)] R. Agrawal, T. Imielinski, and A. Swami, \u201cMining Association Rules\nbetween Sets of Items in Large Databases\u201d, In Proc. of the ACM SIGMOD Conf. on Manage-\nment of Data (1993), pages 207\u2013216.\n[Baeza-Yates and Ribeiro-Neto (2011)] R. Baeza-Yates and B. Ribeiro-Neto, Modern Informa-\ntion Retrieval , 2nd edition, ACM Press (2011).\n", "584": "556 Chapter 11 Data Analytics\n[Chakrabarti (2002)] S. Chakrabarti, Mining the Web: Discovering Knowledge from HyperText\nData , Morgan Kaufmann (2002).\n[Goodfellow et al. (2016)] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning ,M I T\nPress (2016).\n[Han et al. (2011)] J. Han, M. Kamber, and J. Pei, Data Mining: Concepts and Techniques ,\n3rd edition, Morgan Kaufmann (2011).\n[Kimball and Ross (2013)] R. Kimball and M. Ross, \u201cThe Data Warehouse Tookit: The\nDe\ufb01nitive Guide to Dimensional Modeling\u201d, John Wiley and Sons (2013).\n[Kimball et al. (2008)] R. Kimball, M. Ross, W. Thornthwaite, J. Mundy, and B. Becker,\n\u201cThe Data Warehouse Lifecycle Toolkit\u201d, John Wiley and Sons (2008).\n[Manning et al. (2008)] C. D. Manning, P. Raghavan, and H. Sch\u00a8 utze, Introduction to Infor-\nmation Retrieval , Cambridge University Press (2008).\n[Mitchell (1997)] T. M. Mitchell, Machine Learning ,M c G r a wH i l l( 1 9 9 7 ) .\n[Witten et al. (2011)] I. H. Witten, E. Frank, and M. Hall, Data Mining: Practical Machine\nLearning Tools and Techniques with Java Implementations , 3rd edition, Morgan Kaufmann\n(2011).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "585": "PART5\nSTORAGE MANAGEMENT\nAND INDEXING\nAlthough a database system provides a high-level view of data, ultimately data have to\nbe stored as bits on one or more storage devices. A vast majority of database systems\ntoday store data on magnetic disk, with data having higher performance requirements\nstored on \ufb02ash-based solid-state drives. Database systems fetch data into main memory\nfor processing, and write data back to storage for persistence. Data are also copied\nto tapes and other backup devices for archival storage. The physical characteristics of\nstorage devices play a major role in the way data are stored, in particular because access\nto a random piece of data on magnetic disk is much slower than main-memory access.\nMagnetic disk access takes tens of milliseconds, \ufb02ash-based storage access takes 20 to\n100 microseconds, whereas main-memory access takes a tenth of a microsecond.\nChapter 12 begins with an overview of physical storage media, including magnetic\ndisks and \ufb02ash-based solid-state drives ( SSD). The chapter then covers mechanisms to\nminimize the chance of data loss due to device failures, including RAID .T h ec h a p t e r\nconcludes with a discussion of techniques for e\ufb03cient disk-block access.\nChapter 13 describes how records are mapped to \ufb01les, which in turn are mapped\nto bits on the disk. The chapter then covers techniques for the e\ufb03cient management\nof the main-memory bu\ufb00er for disk-based data. Column-oriented storage, used in data\nanalytics systems, is also covered in this chapter.\nMany queries reference only a small proportion of the records in a \ufb01le. An index\nis a structure that helps locate desired records of a relation quickly, without examining\nall records. Chapter 14 describes several types of indices used in database systems.\n557\n", "586": "", "587": "CHAPTER12\nPhysical Storage Systems\nIn preceding chapters, we have emphasized the higher-level models of a database. For\nexample, at the conceptual orlogical level, we viewed the database, in the relational\nmodel, as a collection of tables. Indeed, the logical model of the database is the correct\nlevel for database users to focus on. This is because the goal of a database system is\nto simplify and facilitate access to data; users of the system should not be burdened\nunnecessarily with the physical details of the implementation of the system.\nIn this chapter, however, as well as in Chapter 13, Chapter 14, Chapter 15, and\nChapter 16, we probe below the higher levels as we describe various methods for imple-\nmenting the data models and languages presented in preceding chapters. We start with\ncharacteristics of the underlying storage media, with a particular focus on magnetic\ndisks and \ufb02ash-based solid-state disks, and then discuss how to create highly reliable\nstorage structures by using multiple storage devices.\n12.1Overview of Physical Storage Media\nSeveral types of data storage exist in most computer systems. These storage media are\nclassi\ufb01ed by the speed with which data can be accessed, by the cost per unit of data to\nbuy the medium, and by the medium\u2019s reliability. Among the media typically available\nare these:\n\u2022Cache . The cache is the fastest and most costly form of storage. Cache memory is\nrelatively small; its use is managed by the computer system hardware. We shall not\nbe concerned about managing cache storage in the database system. It is, however,\nworth noting that database implementors do pay attention to cache e\ufb00ects when\ndesigning query processing data structures and algorithms, and we shall return to\nthis issue in later chapters.\n\u2022Main memory . The storage medium used for data that are available to be operated\non is main memory. The general-purpose machine instructions operate on main\nmemory. Main memory may contain tens of gigabytes of data on a personal com-\n559\n", "588": "560 Chapter 12 Physical Storage Systems\nputer, and even hundreds to thousands of gigabytes of data in large server systems.\nIt is generally too small (or too expensive) for storing the entire database for very\nlarge databases, but many enterprise databases can \ufb01t in main memory. However,\nthe contents of main memory are lost in the event of a power failure or system\ncrash; main memory is therefore said to be volatile .\n\u2022Flash memory . Flash memory di\ufb00ers from main memory in that stored data are\nretained even if power is turned o\ufb00 (or fails)\u2014that is, it is non-volatile . Flash mem-\nory has a lower cost per byte than main memory, but a higher cost per byte than\nmagnetic disks.\nFlash memory is widely used for data storage in devices such as cameras and\ncell phones. Flash memory is also used for storing data in \u201c USB \ufb02ash drives,\u201d also\nk n o w na s\u201c p e nd r i v e s , \u201dw h i c hc a nb ep l u g g e di n t ot h e Universal Serial Bus (USB)\nslots of computing devices.\nFlash memory is also increasingly used as a replacement for magnetic disks\nin personal computers as well as in servers. A solid-state drive (SSD)u s e s\ufb02 a s h\nmemory internally to store data but provides an interface similar to a magnetic\ndisk, allowing data to be stored or retrieved in units of a block; such an interface\nis called a block-oriented interface . Block sizes typically range from 512 bytes to 8-\nkilobytes. As of 2018, a 1-terabyte SSDcosts around $250. We provide more details\nabout \ufb02ash memory in Section 12.4.\n\u2022Magnetic-disk storage . The primary medium for the long-term online storage of\ndata is the magnetic disk drive, which is also referred to as the hard disk drive\n(HDD ). Magnetic disk, like \ufb02ash memory, is non-volatile: that is, magnetic disk\nstorage survives power failures and system crashes. Disks may sometimes fail and\ndestroy data, but such failures are quite rare compared to system crashes or power\nfailures.\nTo access data stored on magnetic disk, the system must \ufb01rst move the data\nfrom disk to main memory, from where they can be accessed. After the system has\nperformed the designated operations, the data that have been modi\ufb01ed must be\nwritten to disk.\nDisk capacities have grown steadily over the years. As of 2018, the size of mag-\nnetic disks ranges from 500 gigabytes to 14 terabytes, and a 1-terabyte disk costs\nabout $50, while an 8-terabyte disk around $150. Although signi\ufb01cantly cheaper\nthan SSDs , magnetic disks provide lower performance in terms of number of data\naccess operations that they can support per second. We provide further details\nabout magnetic disks in Section 12.3.\n\u2022Optical storage .T h e digital video disk (DVD ) is an optical storage medium, with\ndata written and read back using a laser light source. The Blu-ray DVD format has\na capacity of 27 gigabytes to 128 gigabytes, depending on the number of layers\nsupported. Although the original (and still main) use of DVD sw a st os t o r ev i d e o\ndata, they are capable of storing any type of digital data, including backups of\n", "589": "12.1 Overview of Physical Storage Media 561\ndatabase contents. DVD s are not suitable for storing active database data since the\ntime required to access a given piece of data can be quite long compared to the\ntime taken by a magnetic disk.\nSome DVD versions are read-only, written at the factory where they are pro-\nduced, other versions support write-once , allowing them to be written once, but not\noverwritten, and some versions can be rewritten multiple times. Disks that can be\nwritten only once are called write-once, read-many (WORM )d i s k s .\nOptical disk jukebox systems contain a few drives and numerous disks that can\nbe loaded into one of the drives automatically (by a robot arm) on demand.\n\u2022Tape storage . Tape storage is used primarily for backup and archival data. Archival\ndata refers to data that must be stored safely for a long period of time, often for legal\nreasons. Magnetic tape is cheaper than disks and can safely store data for many\nyears. However, access to data is much slower because the tape must be accessed\nsequentially from the beginning of the tape; tapes can be very long, requiring tens\nto hundreds of seconds to access data. For this reason, tape storage is referred to as\nsequential-access storage. In contrast, magnetic disk and SSD storage are referred\nto as direct-access storage because it is possible to read data from any location on\ndisk.\nTapes have a high capacity (1 to 12 terabyte capacities are currently available),\nand can be removed from the tape drive. Tape drives tend to be expensive, but in-\ndividual tapes are usually signi\ufb01cantly cheaper than magnetic disks of the same\ncapacity. As a result, tapes are well suited to cheap archival storage and to trans-\nferring large amounts of data between di\ufb00erent locations. Archival storage of large\nvideo \ufb01les, as well as storage of large volumes of scienti\ufb01c data, which can range\nup to many petabytes (1 petabyte = 1015bytes) of data, are two common use cases\nfor tapes.\nTape libraries (jukeboxes) are used to hold large collections of tapes, allowing\nautomated storage and retrieval of tapes without human intervention.\nThe various storage media can be organized in a hierarchy (Figure 12.1) according\nto their speed and their cost. The higher levels are expensive, but fast. As we move\ndown the hierarchy, the cost per bit decrease s, whereas the access time increases. This\ntrade-o\ufb00 is reasonable; if a given storage system were both faster and less expensive\nthan another\u2014other properties being the same\u2014then there would be no reason to use\nthe slower, more expensive memory.\nThe fastest storage media\u2014for example, cache and main memory\u2014are referred to\nasprimary storage . The media in the next level in the hierarchy\u2014for example, \ufb02ash\nmemory and magnetic disks\u2014are referred to as secondary storage ,o ronline storage .\nThe media in the lowest level in the hierarchy\u2014for example, magnetic tape and optical-\ndisk jukeboxes\u2014are referred to as tertiary storage ,o ro\ufb04ine storage .\nIn addition to the speed and cost of the various storage systems, there is also the\nissue of storage volatility. In the hierarchy shown in Figure 12.1, the storage systems\n", "590": "562 Chapter 12 Physical Storage Systems\ncache\nmain memory\n\ufb02ash memory\nmagnetic disk\noptical disk\nmagnetic tapes\nFigure 12.1 Storage device hierarchy.\nfrom main memory up are volatile, whereas the storage systems from \ufb02ash memory\ndown are non-volatile. Data must be written to non-volatile storage for safekeeping. We\nshall return to the subject of safe storage of data in the face of system failures later, in\nChapter 19.\n12.2 Storage Interfaces\nM a g n e t i cd i s k sa sw e l la s\ufb02 a s h - b a s e ds o l i d - s t a t ed i s k sa r ec o n n e c t e dt oac o m p u t e r\nsystem through a high-speed interconnection. Disks typically support either the Serial\nATA (SATA ) interface, or the Serial Attached SCSI (SAS) interface; the SAS interface\nis typically used only in servers. The SATA -3 version of SATA nominally supports 6\ngigabytes per second, allowing data transfer speeds of up to 600 megabytes per second,\nwhile SAS version 3 supports data transfer rates of 12 gigabits per second. The Non-\nVolatile Memory Express (NVM e) interface is a logical interface standard developed to\nbetter support SSDs and is typically used with the PCIe interface (the PCIei n t e r f a c e\nprovides high-speed data transfer internal to computer systems).\nWhile disks are usually connected directly by cables to the disk interface of the\ncomputer system, they can be situated remotely and connected by a high-speed network\nto the computer. In the storage area network (SAN) architecture, large numbers of disks\nare connected by a high-speed network to a number of server computers. The disks\nare usually organized locally using a storage organization technique called redundant\narrays of independent disks (RAID ) (described later, in Section 12.5), to give the servers\na logical view of a very large and very reliable disk. Interconnection technologies used\n", "591": "12.3 Magnetic Disks 563\nin storage area networks include iSCSI , which allows SCSI commands to be sent over\nanIPnetwork, Fiber Channel FC, which supports transfer rates of 1.6 to 12 gigabytes\nper second, depending on the version, and In\ufb01niBand, which provides very low latency\nhigh-bandwidth network communication.\nNetwork attached storage (NAS)i sa na l t e r n a t i v et o SAN.NAS is much like SAN,\nexcept that instead of the networked storage appearing to be a large disk, it provides a\n\ufb01le system interface using networked \ufb01le system protocols such as NFS orCIFS . Recent\ny e a r sh a v ea l s os e e nt h eg r o w t ho f cloud storage , where data are stored in the cloud\nand accessed via an API. Cloud storage has a very high latency of tens to hundreds of\nmilliseconds, if the data are not co-located with the database, and is thus not ideal as\nthe underlying storage for databases. However, applications often use cloud storage for\nstoring objects. Cloud-based storage systems are discussed further in Section 21.7.\n12.3 Magnetic Disks\nMagnetic disks provide the bulk of secondary storage for modern computer systems.\nMagnetic disk capacities have been growing steadily year after year, but the storage\nrequirements of large applications have also been growing very fast, in some cases even\nfaster than the growth rate of disk capacities. Very large databases at \u201cweb-scale\u201d require\nthousands to tens of thousands of disks to store their data.1\nIn recent years, SSD storage sizes have grown rapidly, and the cost of SSDs has\ncome down signi\ufb01cantly; the increasing a\ufb00ordability of SSDs coupled with their much\nbetter performance has resulted in SSDs increasingly becoming a competitor to mag-\nnetic disk storage for several applications. However, the fact that the per-byte cost of\nstorage on SSDs is around six to eight times the per-byte cost of storage on magnetic\ndisks means that magnetic disks continue to be the preferred choice for storing very\nlarge volumes of data in many applications. Example of such data include video and\nimage data, as well as data that is accessed less frequently, such as user-generated data\nin many web-scale applications. SSDs have however, increasingly become the preferred\nchoice for enterprise data.\n12.3.1 Physical Characteristics of Disks\nFigure 12.2 shows a schematic diagram of a magnetic disk, while Figure 12.3 shows\nthe internals of an actual magnetic disk. Each disk platter has a \ufb02at, circular shape. Its\ntwo surfaces are covered with a magnetic material, and information is recorded on the\nsurfaces. Platters are made from rigid metal or glass.\nWhen the disk is in use, a drive motor spins it at a constant high speed, typically\n5400 to 10,000 revolutions per minute, depending on the model. There is a read-write\nhead positioned just above the surface of the platter. The disk surface is logically di-\n1We study later, in Chapter 21, how to partition such large amounts of data across multiple nodes in a parallel computing\nsystem.\n", "592": "564 Chapter 12 Physical Storage Systems\ntrack  t\nsector sspindle\ncylinder c\nplatter\narmread\u2013write\nheadarm assembl y\nrotation\nFigure 12.2 Schematic diagram of a magnetic disk.\nvided into tracks , which are subdivided into sectors .Asector is the smallest unit of\ninformation that can be read from or written to the disk. Sector sizes are typically 512\nbytes, and current generation disks have between 2 billion and 24 billion sectors. The\ninner tracks (closer to the spindle) are of smaller length than the outer tracks, and the\nouter tracks contain more sectors than the inner tracks.\nTheread\u2013write head stores information on a sector magnetically as reversals of\nthe direction of magnetization of the magnetic material.\nFigure 12.3 Internals of an actual magnetic disk.\n", "593": "12.3 Magnetic Disks 565\nEach side of a platter of a disk has a read-write head that moves across the platter\nto access di\ufb00erent tracks. A disk typically contains many platters, and the read-write\nheads of all the tracks are mounted on a single assembly called a disk arm and move\ntogether. The disk platters mounted on a spindle and the heads mounted on a disk\narm are together known as head-disk assemblies . Since the heads on all the platters\nmove together, when the head on one platter is on the ith track, the heads on all other\nplatters are also on the ith track of their respective platters. Hence, the ith tracks of all\nthe platters together are called the ithcylinder .\nThe read-write heads are kept as close as possible to the disk surface to increase the\nrecording density. The head typically \ufb02oats or \ufb02ies only microns from the disk surface;\nthe spinning of the disk creates a small breeze, and the head assembly is shaped so that\nthe breeze keeps the head \ufb02oating just above the disk surface. Because the head \ufb02oats\nso close to the surface, platters must be machined carefully to be \ufb02at.\nHead crashes can be a problem. If the head contacts the disk surface, the head can\nscrape the recording medium o\ufb00 the disk, destroying the data that had been there. In\nolder-generation disks, the head touching the surface caused the removed medium to\nbecome airborne and to come between the other heads and their platters, causing more\ncrashes; a head crash could thus result in failure of the entire disk. Current-generation\ndisk drives use a thin \ufb01lm of magnetic metal as recording medium. They are much\nless susceptible to failure of the entire disk, but are susceptible to failure of individual\nsectors.\nAdisk controller interfaces between the computer system and the actual hardware\nof the disk drive; in modern disk systems, the disk controller is implemented within\nthe disk drive unit. A disk controller accepts high-level commands to read or write a\nsector, and initiates actions, such as moving the disk arm to the right track and actually\nreading or writing the data. Disk controllers also attach checksums to each sector that\nis written; the checksum is computed from the data written to the sector. When the\nsector is read back, the controller computes the checksum again from the retrieved\ndata and compares it with the stored checksum; if the data are corrupted, with a high\nprobability the newly computed checksum will not match the stored checksum. If such\nan error occurs, the controller will retry the read several times; if the error continues\nto occur, the controller will signal a read failure.\nAnother interesting task that disk controllers perform is remapping of bad sectors .\nIf the controller detects that a sector is damaged when the disk is initially formatted, or\nwhen an attempt is made to write the sector, it can logically map the sector to a di\ufb00erent\nphysical location (allocated from a pool of extra sectors set aside for this purpose). The\nremapping is noted on disk or in non-volatile memory, and the write is carried out on\nthe new location.\n12.3.2 Performance Measures of Disks\nThe main measures of the qualities of a disk are capacity, access time, data-transfer\nrate, and reliability.\n", "594": "566 Chapter 12 Physical Storage Systems\nAccess time is the time from when a read or write request is issued to when data\ntransfer begins. To access (i.e., to read or write) data on a given sector of a disk, the arm\n\ufb01rst must move so that it is positioned over the correct track, and then must wait for\nt h es e c t o rt oa p p e a ru n d e ri ta st h ed i s kr o t a t e s .T h et i m ef o rr e p o s i t i o n i n gt h ea r mi s\ncalled the seek time , and it increases with the distance that the arm must move. Typical\nseek times range from 2 to 20 milliseconds depending on how far the track is from the\ninitial arm position. Smaller disks tend to have lower seek times since the head has to\ntravel a smaller distance.\nTheaverage seek time is the average of the seek times, measured over a sequence of\n(uniformly distributed) random requests. If all tracks have the same number of sectors,\na n dw ed i s r e g a r dt h et i m er e q u i r e df o rt h eh e a dt os t a r tm o v i n ga n dt os t o pm o v i n g ,w e\ncan show that the average seek time is one-third the worst-case seek time. Taking these\nfactors into account, the average seek ti me is around one-half of the maximum seek\ntime. Average seek times currently range between 4 and 10 milliseconds, depending on\nthe disk model.2\nOnce the head has reached the desired track, the time spent waiting for the sector\nto be accessed to appear under the head is called the rotational latency time .R o t a t i o n a l\nspeeds of disks today range from 5400 rotations per minute (90 rotations per second)\nup to 15,000 rotations per minute (250 rota tions per second), or, equivalently, 4 mil-\nliseconds to 11.1 milliseconds per rotation. On an average, one-half of a rotation of the\ndisk is required for the beginning of the desired sector to appear under the head. Thus,\ntheaverage latency time of the disk is one-half the time for a full rotation of the disk.\nDisks with higher rotational speeds are used for applications where latency needs to\nbe minimized.\nThe access time is then the sum of the seek time and the latency; average access\ntimes range from 5 to 20 milliseconds depending on the disk model. Once the \ufb01rst\nsector of the data to be accessed has come under the head, data transfer begins. The\ndata-transfer rate is the rate at which data can be retrieved from or stored to the disk.\nCurrent disk systems support maximum transfer rates of 50 to 200 megabytes per sec-\nond; transfer rates are signi\ufb01cantly lower than the maximum transfer rates for inner\ntracks of the disk, since they have fewer sectors. For example, a disk with a maximum\ntransfer rate of 100 megabytes per second may have a sustained transfer rate of around\n30 megabytes per second on its inner tracks.\nRequests for disk I/Oare typically generated by the \ufb01le system but can be generated\ndirectly by the database system. Each request speci\ufb01es the address on the disk to be\nreferenced; that address is in the form of a block number .Adisk block is a logical unit\nof storage allocation and retrieval, and block sizes today typically range from 4 to 16\n2Smaller 2.5-inch diameter disks have a lesser arm movement distance than larger 3.5-inch disks, and thus have lower\nseek times. As a result 2.5-inch disks have been the preferred choice for applications where latency needs to be mini-\nmized, although SSDs are increasingly preferred for such applications. Larger 3.5-inch diameter disks have a lower cost\nper byte and are used in data storage applications where cost is an important factor.\n", "595": "12.4 Flash Memory 567\nkilobytes. Data are transferred between disk and main memory in units of blocks. The\nterm page is often used to refer to blocks, although in a few contexts (such as \ufb02ash\nmemory) they refer to di\ufb00erent things.\nA sequence of requests for blocks from di sk may be classi\ufb01ed as a sequential access\npattern or a random access pattern. In a sequential access pattern, successive requests\nare for successive block numbers, which are on the same track, or on adjacent tracks.\nTo read blocks in sequential access, a disk seek may be required for the \ufb01rst block, but\nsuccessive requests would either not require a seek, or require a seek to an adjacent\ntrack, which is faster than a seek to a track that is farther away. Data transfer rates are\nhighest with a sequential access pattern, since seek time is minimal.\nIn contrast, in a random access pattern, successive requests are for blocks that are\nrandomly located on disk. Each such request would require a seek. The number of I/O\noperations per second (IOPS ), that is, the number random block accesses that can be\nsatis\ufb01ed by a disk in a second, depends on the access time, and the block size, and\nthe data transfer rate of the disk. With a 4-kilobyte block size, current generation disks\nsupport between 50 and 200 IOPS , depending on the model. Since only a small amount\n(one block) of data are read per seek, the data transfer rate is signi\ufb01cantly lower with\na random access pattern than with a sequential access pattern.\nThe \ufb01nal commonly used measure of a disk is the mean time to failure (MTTF ),3\nwhich is a measure of the reliability of the disk. The mean time to failure of a disk (or\nof any other system) is the amount of time that, on average, we can expect the system\nto run continuously without any failure. According to vendors\u2019 claims, the mean time\nto failure of disks today ranges from 500,000 to 1,200,000 hours\u2014about 57 to 136\nyears. In practice the claimed mean time to failure is computed on the probability of\nfailure when the disk is new\u2014the \ufb01gure means that given 1000 relatively new disks,\nif the MTTF is 1,200,000 hours, on an average one of them will fail in 1200 hours. A\nmean time to failure of 1,200,000 hours does not imply that the disk can be expected\nto function for 136 years! Most disks have an expected life span of about 5 years and\nhave signi\ufb01cantly higher rates of failure once they become more than a few years old.\n12.4 Flash Memory\nThere are two types of \ufb02ash memory, NOR \ufb02ash and NAND \ufb02ash. NAND \ufb02ash is the\nvariant that is predominantly used for data storage. Reading from NAND \ufb02ash requires\nan entire page of data, which is very commonly 4096 bytes, to be fetched from NAND\n\ufb02ash into main memory. Pages in a NAND \ufb02ash are thus similar to sectors in a magnetic\ndisk.\n3The term mean time between failures (MTBF ) is often used to refer to MTTF in the context of disk drives, although\ntechnically MTBF should only be used in the context of systems that can be repaired after failure, and may fail again;\nMTBF would then be the sum of MTTF and the mean time to repair. Magnetic disks can almost never be repaired after\na failure.\n", "596": "568 Chapter 12 Physical Storage Systems\nSolid-state disks ( SSDs ) are built using NAND \ufb02ash and provide the same block-\noriented interface as disk storage. Compared to magnetic disks, SSDs can provide much\nfaster random access: the latency to retrieve a page of data ranges from 20 to 100 mi-\ncroseconds for SSDs , whereas a random access on disk would take 5 to 10 milliseconds.\nThe data transfer rate of SSDs is higher than that of magnetic disks and is usually lim-\nited by the interconnect technology; transfer rates range from around 500 megabytes\nper second with SATA interfaces, up to 3 gigabytes per second using NVM ePCIei n -\nterfaces, depending on the speci\ufb01c SSD model, in contrast to a maximum of about\n200 megabytes per second with magnetic disk. The power consumption of SSDs is also\nsigni\ufb01cantly lower than that of magnetic disks.\nWrites to \ufb02ash memory are a little more complicated. A write to a page of \ufb02ash\nmemory typically takes about 100 microseconds. However, once written, a page of\n\ufb02ash memory cannot be directly overwritten. Instead, it has to be erased and rewritten\nsubsequently. The erase operation must be performed on a group of pages, called an\nerase block , erasing all the pages in the block, and takes about 2 to 5 milliseconds.\nAn erase block (often referred to as just \u201cblock\u201d in \ufb02ash literature), is typically 256\nkilobytes to 1 megabyte, and contains around 128 to 256 pages. Further, there is a limit\nto how many times a \ufb02ash page can be erased, typically around 100,000 to 1,000,000\ntimes. Once this limit is reached, errors in storing bits are likely to occur.\nFlash memory systems limit the impact of both the slow erase speed and the update\nlimits by mapping logical page numbers to physical page numbers. When a logical page\nis updated, it can be remapped to any already erased physical page, and the original\nlocation can be erased later. Each physical page has a small area of memory where its\nlogical address is stored; if the logical address is remapped to a di\ufb00erent physical page,\nthe original physical page is marked as deleted. Thus, by scanning the physical pages,\nwe can \ufb01nd where each logical page resides. The logical-to-physical page mapping is\nreplicated in an in-memory translation table for quick access.\nBlocks containing multiple deleted pages are periodically erased, taking care to\n\ufb01rst copy nondeleted pages in those blocks to a di\ufb00erent block (the translation table is\nupdated for these nondeleted pages). Since each physical page can be updated only a\n\ufb01xed number of times, physical pages that have been erased many times are assigned\n\u201ccold data,\u201d that is, data that are rarely updated, while pages that have not been erased\nmany times are used to store \u201chot data,\u201d that is, data that are updated frequently. This\nprinciple of evenly distributing erase operations across physical blocks is called wear\nleveling and is usually performed transparently by \ufb02ash-memory controllers. If a phys-\nical page is damaged due to an excessive number of updates, it can be removed from\nusage, without a\ufb00ecting the \ufb02ash memory as a whole.\nAll the above actions are carried out by a layer of software called the \ufb02ash transla-\ntion layer ; above this layer, \ufb02ash storage looks identical to magnetic disk storage, pro-\nviding the same page/sector-oriented interface, except that \ufb02ash storage is much faster.\nFile systems and database storage structures can thus see an identical logical view of\nthe underlying storage structure, regardless of whether it is \ufb02ash or magnetic storage.\n", "597": "12.4 Flash Memory 569\nNote 12.1 STORAGE CLASS MEMORY\nAlthough \ufb02ash is the most widely used type of non-volatile memory, there have\nbeen a number of alternative non-volatile memory technologies developed over\nthe years. Several of these technologies allow direct read and write access to indi-\nvidual bytes or words, avoiding the need to read or write in units of pages (and also\navoiding the erase overhead of NAND \ufb02ash). Such types of non-volatile memory\nare referred to as storage class memory , since they can be treated as a large non-\nvolatile block of memory. The 3D-XPoint memory technology, developed by Intel\nand Micron, is a recently developed storage class memory technology. In terms\nof cost per byte, latency of access, and capacity, 3D-XPoint memory lies in be-\ntween main memory and \ufb02ash memory. Intel Optane SSDsb a s e do n3 D - X P o i n t\nstarted shipping in 2017, and Optane persistent memory modules were announced\nin 2018.\nSSD performance is usually expressed in terms of:\n1.The number of random block reads per second , with 4-kilobyte blocks being the\nstandard. Typical values in 2018 are about 10,000 random reads per second (also\nreferred to as 10,000 IOPS ) with 4-kilobyte blocks, although some models support\nhigher rates.\nUnlike magnetic disks, SSDs can support multiple random requests in paral-\nlel, with 32 parallel requests being commonly supported; a \ufb02ash disk with SATA\ninterface supports nearly 100,000 random 4-kilobyte block reads in a second with\n32 requests sent in parallel, while SSDs connected using NVM ePCIe can support\nover 350,000 random 4-kilobyte block reads per second. These numbers are spec-\ni\ufb01ed as QD-1 for rates without parallelism and QD-n for n-way parallelism, with\nQD-32 being the most commonly used number.\n2.Thedata transfer rate for sequential reads and sequential writes. Typical rates for\nboth sequential reads and sequential writes are 400 to 500 megabytes per second\nforSSDs with a SATA 3 interface, and 2 to 3 gigabytes per second for SSDs using\nNVM eo v e rt h e PCIe3 . 0 x 4i n t e r f a c e .\n3.The number of random block writes per second , with 4-kilobyte blocks being the\nstandard. Typical values in 2018 are about 40,000 random 4-kilobyte writes per\nsecond for QD-1 (without parallelism), and around 100,000 IOPS for QD-32.\nalthough some models support higher rates for both QD-1 and QD-32.\nHybrid disk drives are hard-disk systems that combine magnetic storage with a\nsmaller amount of \ufb02ash memory, which is used as a cache for frequently accessed data.\nFrequently accessed data that are rarely updated are ideal for caching in \ufb02ash memory.\n", "598": "570 Chapter 12 Physical Storage Systems\nModern SAN and NAS systems support the use of a combination of magnetic disks\nand SSDs , and they can be con\ufb01gured to use the SSDs as a cache for data that reside\non magnetic disks.\n12.5 RAID\nThe data-storage requirements of some applications (in particular web, database, and\nmultimedia applications) have been growing so fast that a large number of disks are\nneeded to store their data, even though disk-drive capacities have been growing very\nfast.\nHaving a large number of disks in a system presents opportunities for improving\nthe rate at which data can be read or written, if the disks are operated in parallel. Several\nindependent reads or writes can also be performed in parallel. Furthermore, this setup\no\ufb00ers the potential for improving the reliability of data storage, because redundant\ninformation can be stored on multiple disks. Thus, failure of one disk does not lead to\nloss of data.\nA variety of disk-organization techniques, collectively called redundant arrays of\nindependent disks (RAID ), have been proposed to achieve improved performance and\nreliability.\nIn the past, system designers viewed storage systems composed of several small,\ncheap disks as a cost-e\ufb00ective alternative to using large, expensive disks; the cost per\nmegabyte of the smaller disks was less than that of larger disks. In fact, the IinRAID ,\nwhich now stands for independent , originally stood for inexpensive .T o d a y ,h o w e v e r ,\nall disks are physically small, and larger-capacity disks actually have a lower cost per\nmegabyte. RAID systems are used for their higher reliability and higher performance\nrate, rather than for economic reasons. Another key justi\ufb01cation for RAID use is easier\nmanagement and operations.\n12.5.1 Improvement of Reliability via Redundancy\nLet us \ufb01rst consider reliability. The chance that at least one disk out of a set of Ndisks\nwill fail is much higher than the chance that a speci\ufb01c single disk will fail. Suppose that\nthe mean time to failure of a disk is 100,000 hours, or slightly over 11 years. Then, the\nmean time to failure of some disk in an array of 100 disks will be 100,000 \u2215100 = 1000\nhours, or around 42 days, which is not long at all! If we store only one copy of the data,\nthen each disk failure will result in loss of a signi\ufb01cant amount of data (as discussed in\nSection 12.3.1). Such a high frequency of data loss is unacceptable.\nThe solution to the problem of reliability is to introduce redundancy ;t h a ti s ,w e\nstore extra information that is not needed normally but that can be used in the event\nof failure of a disk to rebuild the lost information. Thus, even if a disk fails, data are\nnot lost, so the e\ufb00ective mean time to failure is increased, provided that we count only\nfailures that lead to loss of data or to non-availability of data.\n", "599": "12.5 RAID 571\nThe simplest (but most expensive) approach to introducing redundancy is to du-\nplicate every disk. This technique is called mirroring (or, sometimes, shadowing ). A\nlogical disk then consists of two physical disks, and every write is carried out on both\ndisks. If one of the disks fails, the data can be read from the other. Data will be lost\nonly if the second disk fails before the \ufb01rst failed disk is repaired.\nThe mean time to failure (where failure is the loss of data) of a mirrored disk\ndepends on the mean time to failure of the individual disks, as well as on the mean time\nto repair , which is the time it takes (on an average) to replace a failed disk and to restore\nthe data on it. Suppose that the failures of the two disks are independent ; that is, there is\nno connection between the failure of one disk and the failure of the other. Then, if the\nmean time to failure of a single disk is 100,000 hours, and the mean time to repair is\n10 hours, the mean time to data loss of a mirrored disk system is 100, 0002\u2215(2\u221710)=\n500\u2217106hours, or 57,000 years! (We do not go into the derivations here; references\nin the bibliographical notes provide the details.)\nYou should be aware that the assumption of independence of disk failures is not\nvalid. Power failures and natural disasters such as earthquakes, \ufb01res, and \ufb02oods may\nresult in damage to both disks at the same time. As disks age, the probability of failure\nincreases, increasing the chance that a second disk will fail while the \ufb01rst is being\nrepaired. In spite of all these considerations, however, mirrored-disk systems o\ufb00er much\nhigher reliability than do single-disk systems. Mirrored-disk systems with mean time to\ndata loss of about 500,000 to 1,000,000 hours, or 55 to 110 years, are available today.\nPower failures are a particular source of concern, since they occur far more fre-\nquently than do natural disasters. Power failures are not a concern if there is no data\ntransfer to disk in progress when they occur. However, even with mirroring of disks,\nif writes are in progress to the same block in both disks, and power fails before both\nblocks are fully written, the two blocks can be in an inconsistent state. The solution to\nthis problem is to write one copy \ufb01rst, then the next, so that one of the two copies is al-\nways consistent. Some extra actions are required when we restart after a power failure,\nto recover from incomplete writes. This matter is examined in Practice Exercise 12.6.\n12.5.2 Improvement in Performance via Parallelism\nNow let us consider the bene\ufb01t of parallel access to multiple disks. With disk mirroring,\nthe rate at which read requests can be handled is doubled, since read requests can be\nsent to either disk (as long as both disks in a pair are functional, as is almost always\nthe case). The transfer rate of each read is the same as in a single-disk system, but the\nnumber of reads per unit time has doubled.\nWith multiple disks, we can improve the transfer rate as well (or instead) by striping\ndata across multiple disks. In its simplest form, data striping consists of splitting the\nbits of each byte across multiple disks; such striping is called bit-level striping . For\nexample, if we have an array of eight disks, we write bit iof each byte to disk i. In such\nan organization, every disk participates in every access (read or write), so the number\nof accesses that can be processed per second is about the same as on a single disk, but\neach access can read eight times as much data in the same time as on a single disk.\n", "600": "572 Chapter 12 Physical Storage Systems\nBlock-level striping stripes blocks across multiple disks. It treats the array of disks\nas a single large disk, and it gives blocks logical numbers; we assume the block numbers\nstart from 0. With an array of ndisks, block-level striping assigns logical block iof the\ndisk array to disk ( imod n)+1; it uses the \u230ai\u2215n\u230bth physical block of the disk to store\nlogical block i. For example, with eight disks, logical block 0 is stored in physical block\n0 of disk 1, while logical block 11 is stored in physical block 1 of disk 4. When reading\na large \ufb01le, block-level striping fetches nblocks at a time in parallel from the ndisks,\ngiving a high data-transfer rate for large reads. When a single block is read, the data-\ntransfer rate is the same as on one disk, but the remaining n\u22121 disks are free to perform\nother actions.\nBlock-level striping o\ufb00ers several advantages over bit-level striping, including the\nability to support a larger number of block reads per second, and lower latency for\nsingle block reads. As a result, bit-level striping is not used in any practical system.\nIn summary, there are two main goals of parallelism in a disk system:\n1.Load-balance multiple small accesses (block accesses), so that the throughput of\nsuch accesses increases.\n2.Parallelize large accesses so that the response time of large accesses is reduced.\n12.5.3 RAID Levels\nMirroring provides high reliability, but it is expensive. Striping provides high data-\ntransfer rates, but does not improve reliability. Various alternative schemes aim to pro-\nvide redundancy at lower cost by combining disk striping with \u201cparity blocks\u201d.\nBlocks in a RAID system are partitioned into sets, as we shall see. For a given set\nof blocks, a parity block can be computed and stored on disk; the ith bit of the parity\nblock is computed as the \u201cexclusive or\u201d (XOR) of the ith bits of the all blocks in the\nset. If the contents of any one of the blocks in a set is lost due to a failure, the block\ncontents can be recovered by computing the bitwise-XOR of the remaining blocks in\nthe set, along with the parity block.\nWhenever a block is written, the parity block for its set must be recomputed and\nwritten to disk. The new value of the parity block can be computed by either (i) reading\nall the other blocks in the set from disk and computing the new parity block, or (ii) by\ncomputing the XOR of the old value of the parity block with the old and new value of\nthe updated block.\nThese schemes have di\ufb00erent cost-performance trade-o\ufb00s. The schemes are classi-\n\ufb01ed into RAID levels.4. Figure 12.4 illustrates the four levels that are used in practice. In\nthe \ufb01gure, P indicates error-correcting bits, and C indicates a second copy of the data.\nFor all levels, the \ufb01gure depicts four disks\u2019 worth of data, and the extra disks depicted\nare used to store redundant information for failure recovery.\n4There are 7 di\ufb00erent RAID levels, numbered 0 to 6; Levels 2, 3, and 4 are not used in practice anymore and thus are\nnot covered in the text\n", "601": "12.5 RAID 573\n(a) RAID 0: nonredundant striping\n(b) RAID 1: mirrored disks\n(c) RAID 5: block-interleaved distributed parity\n(d) RAID 6: P + Q redundancyP\nP\nQPP P\nP PPPC C C C\nP PQ Q Q Q Q\nFigure 12.4 RAID levels.\n\u2022RAID level 0 refers to disk arrays with striping at the level of blocks, but without\nany redundancy (such as mirroring or parity bits). Figure 12.4a shows an array of\nsize 4.\n\u2022RAID level 1 refers to disk mirroring with block striping. Figure 12.4b shows a\nmirrored organization that holds four disks\u2019 worth of data.\nNote that some vendors use the term RAID level 1+0 orRAID level 10 to refer\nto mirroring with striping, and they use the term RAID level 1 to refer to mirroring\nwithout striping. Mirroring without striping can also be used with arrays of disks,\nto give the appearance of a single large, reliable disk: if each disk has Mblocks,\nlogical blocks 0 to M\u22121 are stored on disk 0, Mto 2M\u22121 on disk 1(the second\ndisk), and so on, and each disk is mirrored.5\n\u2022RAID level 5 refers to block-interleaved distributed parity. The data and parity are\npartitioned among all N+1d i s k s .F o re a c hs e to f Nlogical blocks, one of the\ndisks stores the parity, and the other Ndisks store the blocks. The parity blocks\nare stored on di\ufb00erent disks for di\ufb00erent sets of Nblocks. Thus, all disks can\nparticipate in satisfying read requests.6\nF i g u r e1 2 . 4 cs h o w st h es e t u p .T h eP \u2019 sa r ed i s t r i b u t e da c r o s sa l lt h ed i s k s .F o r\nexample, with an array of \ufb01ve disks, the parity block, labeled Pk, for logical blocks\n5Note that some vendors use the term RAID 0 + 1t or e f e rt oav e r s i o no f RAID that uses striping to create a RAID 0\narray, and mirrors the array onto another array, with the di\ufb00erence from RAID 1 being that if a disk fails, the RAID\n0 array containing the disk becomes unusable. The mirrored array can still be used, so there is no loss of data. This\narrangement is inferior to RAID 1 when a disk has failed, since the other disks in the RAID 0 array can continue to be\nused in RAID 1, but remain idle in RAID 0+1.\n6InRAID level 4 (which is not used in practice) all parity blocks are stored on one disk. That disk would not be useful\nfor reads, and it would also have a higher load than other disks if there were many random writes.\n", "602": "574 Chapter 12 Physical Storage Systems\n4k,4k+1, 4k+2, 4k+3i ss t o r e di nd i s k kmod 5; the corresponding blocks of\nthe other four disks store the four data blocks 4 kto 4k+3. The following table\nindicates how the \ufb01rst 20 blocks, numbered 0 to 19, and their parity blocks are\nlaid out. The pattern shown gets repeated on further blocks.\nP0\n4\n8\n12\n160\nP1\n9\n13\n171\n5\nP2\n14\n182\n6\n10\nP3\n193\n7\n11\n15\nP4\nNote that a parity block cannot store parity for blocks in the same disk, since\nthen a disk failure would result in loss of data as well as of parity, and hence would\nnot be recoverable.\n\u2022RAID level 6 , the P + Q redundancy scheme, is much like RAID level 5, but it stores\nextra redundant information to guard against multiple disk failures. Instead of us-\ning parity, level 6 uses error-correcting codes such as the Reed-Solomon codes (see\nthe bibliographical notes). In the scheme in Figure 12.4g, two bits of redundant\ndata are stored for every four bits of data\u2014unlike one parity bit in level 5\u2014and the\nsystem can tolerate two disk failures.\nThe letters P and Q in the \ufb01gure denote blocks containing the two correspond-\ning error-correcting blocks for a given set of data blocks. The layout of blocks is an\nextension of that for RAID 5. For example, with six disks, the two parity blocks,\nlabeled Pk and Qk, for logical blocks 4 k,4k+1, 4k+2,and4k+3a r es t o r e di n\ndisk kmod 6 and ( k+1) mod 6, and the corresponding blocks of the other four\nd i s k ss t o r et h ef o u rd a t ab l o c k s4 kto 4k+3.\nFinally, we note that several variations have been proposed to the basic RAID\nschemes described here, and di\ufb00erent vendors use di\ufb00erent terminologies for the vari-\nants. Some vendors support nested schemes that create multiple separate RAID arrays,\nand then stripe data across the RAID arrays; one of RAID levels 1, 5 or 6 is chosen for\nthe individual arrays. References to further information on this idea are provided in\nthe Further Reading section at the end of the chapter.\n12.5.4 Hardware Issues\nRAID can be implemented with no change at the hardware level, using only software\nmodi\ufb01cation. Such RAID implementations are called software RAID .H o w e v e r ,t h e r ea r e\nsigni\ufb01cant bene\ufb01ts to be had by building special-purpose hardware to support RAID ,\nwhich we outline below; systems with special hardware support are called hardware\nRAID systems.\n", "603": "12.5 RAID 575\nHardware RAID implementations can use non-volatile RAM to record writes be-\nfore they are performed. In case of power failure, when the system comes back up,\nit retrieves information about any incomplete writes from non-volatile RAM and then\ncompletes the writes. Normal operations can then commence.\nIn contrast, with software RAID extra work needs to be done to detect blocks that\nmay have been partially written before power failure. For RAID 1, all blocks of the disks\nare scanned to see if any pair of blocks on the two disks have di\ufb00erent contents. For\nRAID 5, the disks need to be scanned and parity recomputed for each set of blocks and\ncompared to the stored parity. Such scans take a long time, and they are done in the\nbackground using a small fraction of the disks\u2019 available bandwidth. See Practice Ex-\nercise 12.6 for details of how to recover data to the latest value, when an inconsistency\nis detected; we revisit this issue in the context of database system recovery in Section\n19.2.1. The RAID system is said to be resynchronizing (orresynching ) during this phase;\nnormal reads and writes are allowed while resynchronization is in progress, but a failure\nof a disk during this phase could result in data loss for blocks with incomplete writes.\nHardware RAID does not have this limitation.\nEven if all writes are completed properly, there is a small chance of a sector in a\ndisk becoming unreadable at some point, even though it was successfully written earlier.\nReasons for loss of data on individual sectors could range from manufacturing defects\nto data corruption on a track when an adjacent track is written repeatedly. Such loss of\ndata that were successfully written earlier is sometimes referred to as a latent failure ,o r\nasbit rot . When such a failure happens, if it is detected early the data can be recovered\nfrom the remaining disks in the RAID organization. However, if such a failure remains\nundetected, a single disk failure could lead to data loss if a sector in one of the other\ndisks has a latent failure.\nTo minimize the chance of such data loss, good RAID controllers perform scrub-\nbing; that is, during periods when disks are idle, every sector of every disk is read, and\nif any sector is found to be unreadable, the data are recovered from the remaining disks\nin the RAID organization, and the sector is written back. (If the physical sector is dam-\naged, the disk controller would remap the logical sector address to a di\ufb00erent physical\nsector on disk.)\nServer hardware is often designed to permit hot swapping ;t h a ti s ,f a u l t yd i s k sc a n\nbe removed and replaced by new ones without turning power o\ufb00. The RAID controller\ncan detect that a disk was replaced by a new one and can immediately proceed to\nreconstruct the data that was on the old disk, and write it to the new disk. Hot swapping\nreduces the mean time to repair, since replacement of a disk does not have to wait until\na time when the system can be shut down. In fact, many critical systems today run on\na2 4\u00d77 schedule; that is, they run 24 hours a day, 7 days a week, providing no time for\nshutting down and replacing a failed disk. Further, many RAID implementations assign\na spare disk for each array (or for a set of disk arrays). If a disk fails, the spare disk is\nimmediately used as a replacement. As a result, the mean time to repair is reduced\ngreatly, minimizing the chance of any data loss. The failed disk can be replaced at\nleisure.\n", "604": "576 Chapter 12 Physical Storage Systems\nThe power supply, or the disk controller, or even the system interconnection in a\nRAID system could become a single point of failure that could stop the functioning\nof the RAID system. To avoid this possibility, good RAID implementations have multi-\nple redundant power supplies (with battery backups so they continue to function even\nif power fails). Such RAID systems have multiple disk interfaces and multiple inter-\nconnections to connect the RAID system to the computer system (or to a network of\ncomputer systems). Thus, failure of any single component will not stop the functioning\nof the RAID system.\n12.5.5 Choice of RAID Level\nThe factors to be taken into account in choosing a RAID level are:\n\u2022Monetary cost of extra disk-storage requirements.\n\u2022Performance requirements in terms of number of I/Ooperations per second.\n\u2022Performance when a disk has failed.\n\u2022Performance during rebuild (i.e., while the data in a failed disk are being rebuilt\non a new disk).\nThe time to rebuild the data of a failed disk can be signi\ufb01cant, and it varies with\ntheRAID level that is used. Rebuilding is easiest for RAID level 1, since data can be\ncopied from another disk; for the other levels, we need to access all the other disks in\nthe array to rebuild data of a failed disk. The rebuild performance of a RAID system\nmay be an important factor if continuous availability of data is required, as it is in high-\nperformance database systems. Furthermore, since rebuild time can form a signi\ufb01cant\npart of the repair time, rebuild performance also in\ufb02uences the mean time to data loss.\nRAID level 0 is used in a few high-performance applications where data safety is\nnot critical, but not anywhere else.\nRAID level 1 is popular for applications such as storage of log \ufb01les in a database\nsystem, since it o\ufb00ers the best write performance. RAID level 5 has a lower storage\noverhead than level 1, but it has a higher time overhead for writes. For applications\nwhere data are read frequently, and written rarely, level 5 is the preferred choice.\nDisk-storage capacities have been increasing rapidly for many years. Capacities\nwere e\ufb00ectively doubling every 13 months at one point; although the current rate of\ngrowth is much less now, capacities have continued to increase rapidly. The cost per\nbyte of disk storage has been falling at about the same rate as the capacity increase. As\na result, for many existing database applications with moderate storage requirements,\nthe monetary cost of the extra disk storage needed for mirroring has become rela-\ntively small (the extra monetary cost, however, remains a signi\ufb01cant issue for storage-\nintensive applications such as video data storage). Disk access speeds have not im-\nproved signi\ufb01cantly in recent years, while the number of I/Ooperations required per\nsecond has increased tremendously, particularly for web application servers.\n", "605": "12.6 Disk-Block Access 577\nRAID level 5 has a signi\ufb01cant overhead for random writes, since a single random\nblock write requires 2 block reads (to get the old values of the block and parity block)\nand 2 block writes to write these blocks back. In contrast, the overhead is low for large\nsequential writes, since the parity block can be computed from the new blocks in most\ncases, without any reads. RAID level 1 is therefore the RAID level of choice for many\napplications with moderate storage requirements and high random I/Orequirements.\nRAID level 6 o\ufb00ers better reliability than level 1 or 5, since it can tolerate two\ndisk failures without losing data. In terms of performance during normal operation,\nit is similar to RAID level 5, but it has a higher storage cost than RAID level 5. RAID\nlevel 6 is used in applications where data safety is very important. It is being viewed as\nincreasingly important since latent sector failures are not uncommon, and it may take a\nlong time to be detected and repaired. A failure of a di\ufb00erent disk before a latent failure\nis detected and repaired would then be similar to a two-disk failure for that sector and\nresult in loss of data of that sector. RAID levels 1 and 5 would su\ufb00er from data loss in\nsuch a scenario, unlike level 6.\nMirroring can also be extended to store copies on three disks instead of two to\nsurvive two-disk failures. Such triple-redundancy schemes are not commonly used in\nRAID systems, although they are used in distributed \ufb01le systems, where data are stored\nin multiple machines, since the probability of machine failure is signi\ufb01cantly higher\nthan that of disk failure.\nRAID system designers have to make several other decisions as well. For example,\nhow many disks should there be in an array? How many bits should be protected by\neach parity bit? If there are more disks in an array, data-transfer rates are higher, but the\nsystem will be more expensive. If there are more bits protected by a parity bit, the space\noverhead due to parity bits is lower, but there is an increased chance that a second disk\nwill fail before the \ufb01rst failed disk is repaired, and that will result in data loss.\n12.5.6 Other RAID Applications\nThe concepts of RAID have been generalized to other storage devices, including in the\n\ufb02ash memory devices within SSDs , arrays of tapes, and even to the broadcast of data\nover wireless systems. Individual \ufb02ash pages have a higher rate of data loss than sectors\nof magnetic disks. Flash devices such as SSDs implement RAID internally, to ensure\nthat the device does not lose data due to the loss of a \ufb02ash page. When applied to\narrays of tapes, the RAID structures are able to recover data even if one of the tapes in\nan array of tapes is damaged. When applied to broadcast of data, a block of data are\nsplit into short units and is broadcast along with a parity unit; if one of the units is not\nreceived for any reason, it can be reconstructed from the other units.\n12.6 Disk-Block Access\nRequests for disk I/Oare generated by the database system, with the query processing\nsubsystem responsible for most of the disk I/O. Each request speci\ufb01es a disk identi\ufb01er\nand a logical block number on the disk; in case database data are stored in operating\n", "606": "578 Chapter 12 Physical Storage Systems\nsystem \ufb01les, the request instead speci\ufb01es the \ufb01le identi\ufb01er and a block number within\nthe \ufb01le. Data are transferred between disk and main memory in units of blocks.\nAs we saw earlier, a sequence of requests for blocks from disk may be classi\ufb01ed as\na sequential access pattern or a random access pattern. In a sequential access pattern,\nsuccessive requests are for successive block numbers, which are on the same track,\nor on adjacent tracks. In contrast, in a random access pattern, successive requests are\nfor blocks that are randomly located on disk. Each such request would require a seek,\nresulting in a longer access time, and a lower number of random I/Ooperations per\nsecond.\nA number of techniques have been developed for improving the speed of access\nto blocks, by minimizing the number of accesses, and in particular minimizing the\nnumber of random accesses. We describe these techniques below. Reducing the number\nof random accesses is very important for data stored on magnetic disks; SSDs support\nmuch faster random access than do magnetic disks, so the impact of random access is\nless with SSDs , but data access from SSDs can still bene\ufb01t from some of the techniques\ndescribed below.\n\u2022Bu\ufb00ering . Blocks that are read from disk are stored temporarily in an in-memory\nbu\ufb00er, to satisfy future requests. Bu\ufb00ering is done by both the operating system\nand the database system. Database bu\ufb00ering is discussed in more detail in Section\n13.5.\n\u2022Read-ahead . When a disk block is accessed, consecutive blocks from the same track\nare read into an in-memory bu\ufb00er even if there is no pending request for the blocks.\nIn the case of sequential access, such read-ahead ensures that many blocks are\nalready in memory when they are requested, and it minimizes the time wasted in\ndisk seeks and rotational latency per block read. Operating systems also routinely\nperform read-ahead for consecutive blocks of an operating system \ufb01le. Read-ahead\nis, however, not very useful for random block accesses.\n\u2022Scheduling . If several blocks from a cylinder need to be transferred from disk to\nmain memory, we may be able to save access time by requesting the blocks in the\norder in which they will pass under the heads. If the desired blocks are on di\ufb00er-\nent cylinders, it is advantageous to request the blocks in an order that minimizes\ndisk-arm movement. Disk-arm\u2013scheduling algorithms attempt to order accesses to\ntracks in a fashion that increases the number of accesses that can be processed.\nA commonly used algorithm is the elevator algorithm , which works in the same\nway many elevators do. Suppose that, initially, the arm is moving from the inner-\nmost track toward the outside of the disk. Under the elevator algorithm\u2019s control,\nfor each track for which there is an access request, the arm stops at that track,\nservices requests for the track, and then continues moving outward until there are\nno waiting requests for tracks farther out. At this point, the arm changes direction\nand moves toward the inside, again stopping at each track for which there is a re-\n", "607": "12.6 Disk-Block Access 579\nquest, until it reaches a track where there is no request for tracks farther toward\nthe center. Then, it reverses direction and starts a new cycle.\nDisk controllers usually perform the task of reordering read requests to improve\nperformance, since they are intimately aware of the organization of blocks on disk,\nof the rotational position of the disk platters, and of the position of the disk arm. To\nenable such reordering, the disk controller interface must allow multiple requests\nto be added to a queue; results may be returned in a di\ufb00erent order from the request\norder.\n\u2022File organization . To reduce block-access time, we can organize blocks on disk in a\nway that corresponds closely to the way we expect data to be accessed. For exam-\nple, if we expect a \ufb01le to be accessed sequentially, then we should ideally keep all\nthe blocks of the \ufb01le sequentially on adjacent cylinders. Modern disks hide the ex-\nact block location from the operating system but use a logical numbering of blocks\nthat gives consecutive numbers to blocks that are adjacent to each other. By allo-\ncating consecutive blocks of a \ufb01le to disk blocks that are consecutively numbered,\noperating systems ensure that \ufb01les are stored sequentially.\nStoring a large \ufb01le in a single long sequence of consecutive blocks poses chal-\nlenges to disk block allocation; instead, operating systems allocate some number\nof consecutive blocks (an extent ) at a time to a \ufb01le. Di\ufb00erent extents allocated to\na \ufb01le may not be adjacent to each other on disk. Sequential access to the \ufb01le needs\none seek per extent, instead of one seek per block if blocks are randomly allocated;\nwith large enough extents, the cost of seeks relative to data transfer costs can be\nminimized.\nOver time, a sequential \ufb01le that has multiple small appends may become frag-\nmented ; that is, its blocks become scattered all over the disk. To reduce fragmen-\ntation, the system can make a backup copy of the data on disk and restore the\nentire disk. The restore operation writes back the blocks of each \ufb01le contiguously\n(or nearly so). Some systems (such as di\ufb00erent versions of the Windows operat-\ning system) have utilities that scan the disk and then move blocks to decrease the\nfragmentation. The performance increases realized from these techniques can be\nquite signi\ufb01cant.\n\u2022Non-volatile write bu\ufb00ers . Since the contents of main memory are lost in a power\nfailure, information about database updates has to be recorded on disk to sur-\nvive possible system crashes. For this reason, the performance of update-intensive\ndatabase applications, such as transaction-processing systems, is heavily depen-\ndent on the latency of disk writes.\nWe can use non-volatile random-access memory (NVRAM ) to speed up disk\nwrites. The contents of NVRAM are not lost in power failure. NVRAM was im-\nplemented using battery-backed-up RAM in earlier days, but \ufb02ash memory is cur-\nrently the primary medium for non-volatile write bu\ufb00ering. The idea is that, when\nthe database system (or the operating system) requests that a block be written to\ndisk, the disk controller writes the block to a non-volatile write bu\ufb00er and imme-\n", "608": "580 Chapter 12 Physical Storage Systems\ndiately noti\ufb01es the operating system that the write completed successfully. The\ncontroller can subsequently write the data to their destination on disk in a way\nthat minimizes disk arm movement, using the elevator algorithm, for example. If\nsuch write reordering is done without using non-volatile write bu\ufb00ers, the database\nstate may become inconsistent in the event of a system crash; recovery algorithms\nthat we study later in Chapter 19 depend on writes being written in the speci\ufb01ed\norder. When the database system requests a block write, it notices a delay only if\ntheNVRAM bu\ufb00er is full. On recovery from a system crash, any pending bu\ufb00ered\nwrites in the NVRAM are written back to the disk. NVRAM bu\ufb00ers are found in\ncertain high-end disks, but are more frequently found in RAID controllers.\nIn addition to the above low-level optimizations, optimizations to minimize random\naccesses can be done at a higher level, by clever design of query processing algorithms.\nWe study e\ufb03cient query processing techniques in Chapter 15.\n12.7 Summary\n\u2022Several types of data storage exist in most computer systems. They are classi\ufb01ed\nby the speed with which they can access data, by their cost per unit of data to buy\nthe memory, and by their reliability. Among the media available are cache, main\nmemory, \ufb02ash memory, magnetic disks, optical disks, and magnetic tapes.\n\u2022Magnetic disks are mechanical devices, and data access requires a read\u2013write head\nto move to the required cylinder, and the rotation of the platters must then bring\nthe required sector under the read\u2013write head. Magnetic disks thus have a high\nlatency for data access.\n\u2022SSDs have a much lower latency for data access, and higher data transfer band-\nwidth than magnetic disks. However, they also have a higher cost per byte than\nmagnetic disks.\n\u2022Disks are vulnerable to failure, which could result in loss of data stored on the\ndisk. We can reduce the likelihood of irretrievable data loss by retaining multiple\ncopies of data.\n\u2022Mirroring reduces the probability of data loss greatly. More sophisticated methods\nbased on redundant arrays of independent disks ( RAID ) o\ufb00er further bene\ufb01ts.\nBy striping data across disks, these methods o\ufb00er high throughput rates on large\naccesses; by introducing redundancy across disks, they improve reliability greatly.\n\u2022Several di\ufb00erent RAID organizations are possible, each with di\ufb00erent cost, perfor-\nmance, and reliability characteristics. RAID level 1 (mirroring) and RAID level 5\nare the most commonly used.\n\u2022Several techniques have been developed to optimize disk block access, such as read\nahead, bu\ufb00ering, disk arm scheduling, prefetching, and non-volatile write bu\ufb00ers.\n", "609": "Review Terms 581\nReview Terms\n\u2022Physical storage media\n\u00b0Cache\n\u00b0Main memory\n\u00b0Flash memory\n\u00b0Magnetic disk\n\u00b0Optical storage\n\u00b0Tape storage\n\u2022Volatile storage\n\u2022Non-volatile storage\n\u2022Sequential-access\n\u2022Direct-access\n\u2022Storage interfaces\n\u00b0Serial ATA (SATA )\n\u00b0Serial Attached SCSI (SAS)\n\u00b0Non-Volatile Memory Express\n(NVM e)\n\u00b0Storage area network ( SAN)\n\u00b0Network attached storage ( NAS)\n\u2022Magnetic disk\n\u00b0Platter\n\u00b0Hard disks\n\u00b0Tracks\n\u00b0Sectors\n\u00b0Read\u2013write head\n\u00b0Disk arm\n\u00b0Cylinder\n\u00b0Disk controller\n\u00b0Checksums\n\u00b0Remapping of bad sectors\u2022Disk block\n\u2022Performance measures of disks\n\u00b0Access time\n\u00b0Seek time\n\u00b0Latency time\n\u00b0I/O operations per second ( IOPS )\n\u00b0Rotational latency\n\u00b0Data-transfer rate\n\u00b0Mean time to failure ( MTTF )\n\u2022Flash Storage\n\u00b0Erase Block\n\u00b0Wear leveling\n\u00b0Flash translation table\n\u00b0Flash Translation Layer\n\u2022Storage class memory\n\u00b03D-XPoint\n\u2022Redundant arrays of independent\ndisks ( RAID )\n\u00b0Mirroring\n\u00b0Data striping\n\u00b0Bit-level striping\n\u00b0Block-level striping\n\u2022RAID levels\n\u00b0Level 0 (block striping,\nno redundancy)\n\u00b0Level 1 (block striping,\nmirroring)\n\u00b0Level 5 (block striping,\ndistributed parity)\n", "610": "582 Chapter 12 Physical Storage Systems\n\u00b0Level 6 (block striping,\nP+Qr e d u n d a n c y )\n\u2022Rebuild performance\n\u2022Software RAID\n\u2022Hardware RAID\n\u2022Hot swapping\n\u2022Optimization of disk-block access\u00b0Disk-arm scheduling\n\u00b0Elevator algorithm\n\u00b0File organization\n\u00b0Defragmenting\n\u00b0Non-volatile write bu\ufb00ers\n\u00b0Log disk\nPractice Exercises\n12.1 SSDs can be used as a storage layer between memory and magnetic disks, with\nsome parts of the database (e.g., some relations) stored on SSDs and the rest\non magnetic disks. Alternatively, SSDs c a nb eu s e da sab u \ufb00 e ro rc a c h ef o r\nmagnetic disks; frequently used blocks would reside on the SSD layer, while\ninfrequently used blocks would reside on magnetic disk.\na. Which of the two alternatives would you choose if you need to support\nreal-time queries that must be answered within a guaranteed short period\nof time? Explain why.\nb. Which of the two alternatives would you choose if you had a very large\ncustomer relation, where only some disk blocks of the relation are ac-\ncessed frequently, with other blocks rarely accessed.\n12.2 Some databases use magnetic disks in a way that only sectors in outer tracks are\nused, while sectors in inner tracks are left unused. What might be the bene\ufb01ts\nof doing so?\n12.3 Flash storage:\na. How is the \ufb02ash translation table, which is used to map logical page\nnumbers to physical page numbers, created in memory?\nb. Suppose you have a 64-gigabyte \ufb02ash storage system, with a 4096-byte\npage size. How big would the \ufb02ash translation table be, assuming each\npage has a 32-bit address, and the table is stored as an array?\nc. Suggest how to reduce the size of the translation table if very often long\nranges of consecutive logical page numbers are mapped to consecutive\nphysical page numbers.\n12.4 Consider the following data and parity-block arrangement on four disks:\n", "611": "Exercises 583\nDisk 1 Disk 2 Disk 3 Disk 4\nB1\nP1\nB8\u2026B2\nB5\nP2\u2026B3\nB6\nB9\u2026B4\nB7\nB10\u2026\nThe Bis represent data blocks; the Pis represent parity blocks. Parity block Pi\nis the parity block for data blocks B4i\u22123toB4i. What, if any, problem might this\narrangement present?\n12.5 A database administrator can choose how many disks are organized into a\nsingle RAID 5 array. What are the trade-o\ufb00s between having fewer disks ver-\nsus more disks, in terms of cost, reliability, performance during failure, and\nperformance during rebuild?\n12.6 A power failure that occurs while a disk block is being written could result in\nthe block being only partially written. Assume that partially written blocks can\nbe detected. An atomic block write is one where either the disk block is fully\nwritten or nothing is written (i.e., there are no partial writes). Suggest schemes\nfor getting the e\ufb00ect of atomic block writes with the following RAID schemes.\nYour schemes should involve work on recovery from failure.\na.RAID level 1 (mirroring)\nb. RAID level 5 (block interleaved, distributed parity)\n12.7 Storing all blocks of a large \ufb01le on consecutive disk blocks would minimize\nseeks during sequential \ufb01le reads. Why is it impractical to do so? What do op-\nerating systems do instead, to minimize the number of seeks during sequential\nreads?\nExercises\n12.8 List the physical storage media available on the computers you use routinely.\nGive the speed with which data can be accessed on each medium.\n12.9 How does the remapping of bad sectors by disk controllers a\ufb00ect data-retrieval\nrates?\n12.10 Operating systems try to ensure that consecutive blocks of a \ufb01le are stored on\nconsecutive disk blocks. Why is doing so very important with magnetic disks?\nIfSSDs were used instead, is doing so still important, or is it irrelevant? Explain\nwhy.\n", "612": "584 Chapter 12 Physical Storage Systems\n12.11 RAID systems typically allow you to replace failed disks without stopping ac-\ncess to the system. Thus, the data in the failed disk must be rebuilt and written\nto the replacement disk while the system is in operation. Which of the RAID\nlevels yields the least amount of interference between the rebuild and ongoing\ndisk accesses? Explain your answer.\n12.12 What is scrubbing, in the context of RAID systems, and why is scrubbing im-\nportant?\n12.13 Suppose you have data that should not be lost on disk failure, and the applica-\ntion is write-intensive. How would you store the data?\nFurther Reading\n[Hennessy et al. (2017)] is a popular textb ook on computer architecture, which in-\ncludes coverage of cache and memory organization.\nThe speci\ufb01cations of current-generation magnetic disk drives can be obtained from\nthe web sites of their manufacturers, such as Hitachi, Seagate, Maxtor, and Western\nDigital. The speci\ufb01cations of current-generation SSDs can be obtained from the web\nsites of their manufacturers, such as Crucial, Intel, Micron, Samsung, SanDisk, Toshiba\nand Western Digital.\n[Patterson et al. (1988)] provided early coverage of RAID levels and helped stan-\ndardize the terminology. [Chen et al. (1994)] presents a survey of RAID principles and\nimplementation.\nA comprehensive coverage of RAID levels supported by most modern RAID sys-\ntems, including the nested RAID levels, 10, 50 and 60, which combine RAID levels 1,\n5 and 6 with striping as in RAID level 0, can be found in the \u201cIntroduction to RAID \u201d\nchapter of [Cisco (2018)]. Reed-Solom on codes are covered in [Pless (1998)].\nBibliography\n[Chen et al. (1994)] P .M .C h e n ,E .K .L e e ,G .A .G i b s o n ,R .H .K a t z ,a n dD .A .P a t t e r s o n ,\n\u201cRAID: High-Performance, Reliable Secondary Storage\u201d, ACM Computing Surveys ,V o l u m e\n26, Number 2 (1994), pages 145\u2013185.\n[Cisco (2018)] Cisco UCS Servers RAID Guide . Cisco (2018).\n[Hennessy et al. (2017)] J. L. Hennessy, D. A. Patterson, and D. Goldberg, Computer Archi-\ntecture: A Quantitative Approach , 6th edition, Morgan Kaufmann (2017).\n[Patterson et al. (1988)] D. A. Patterson, G. Gibson, and R. H. Katz, \u201cA Case for Redundant\nArrays of Inexpensive Disks (RAID)\u201d, In Proc. of the ACM SIGMOD Conf. on Management\nof Data (1988), pages 109\u2013116.\n", "613": "Further Reading 585\n[Pless (1998)] V. Pless, Introduction to the Theory of Error-Correcting Codes , 3rd edition, John\nWiley and Sons (1998).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\nFigure 12.3 is due to \u00a9Silberschatz, Korth, and Sudarshan.\n", "614": "", "615": "CHAPTER13\nData Storage Structures\nIn Chapter 12 we studied the characteristics of physical storage media, focusing on\nmagnetic disks and SSDs , and saw how to build fast and reliable storage systems using\nmultiple disks in a RAID structure. In this chapter, we focus on the organization of data\nstored on the underlying storage media, and how data are accessed.\n13.1 Database Storage Architecture\nPersistent data are stored on non-volatile storage, which, as we saw in Chapter 12, is\ntypically magnetic disk or SSD.M a g n e t i cd i s k sa sw e l la s SSDs are block structured\ndevices, that is, data are read or written in units of a block. In contrast, databases deal\nwith records, which are usually much smaller than a block (although in some cases\nrecords may have attributes that are very large).\nMost databases use operating system \ufb01les as an intermediate layer for storing\nrecords, which abstract away some details of the underlying blocks. However, to en-\nsure e\ufb03cient access, as well as to support recovery from failures (as we will see later in\nChapter 19), databases must continue to be aware of blocks. Thus, in Section 13.2, we\nstudy how individual records are stored in \ufb01les, taking block structure into account.\nGiven a set of records, the next decision lies in how to organize them in the \ufb01le\nstructure; for example, they may stored in sorted order, in the order they are created,\nor in an arbitrary order. Section 13.3 studies several alternative \ufb01le organizations.\nSection 13.4 then describes how databases organize data about the relational\nschemas as well as storage organization, in the data dictionary. Information in the\ndata dictionary is crucial for many tasks, for example, to locate and retrieve records of\na relation when given the name of the relation.\nFor a CPU to access data, it must be in main memory, whereas persistent data must\nbe resident on non-volatile storage such as magnetic disks or SSDs .F o rd a t a b a s e st h a t\nare larger than main memory, which is the usual case, data must be fetched from non-\nvolatile storage and saved back if it is updated. Section 13.5 describes how databases\nuse a region of memory called the database bu\ufb00er to store blocks that are fetched from\nnon-volatile storage.\n587\n", "616": "588 Chapter 13 Data Storage Structures\nAn approach to storing data based on storing all values of a particular column\ntogether, rather than storing all attributes of a particular row together, has been found to\nwork very well for analytical query processing. This idea, called column-oriented storage ,\nis discussed in Section 13.6.\nSome applications need very fast access to data and have small enough data sizes\nthat the entire database can \ufb01t into the main memory of a database server machine.\nIn such cases, we can keep a copy of the entire database in memory.1Databases that\nstore the entire database in memory and optimize in-memory data structures as well as\nquery processing and other algorithms used by the database to exploit the memory resi-\ndency of data are called main-memory databases . Storage organization in main-memory\ndatabases is discussed in Section 13.7. We note that non-volatile memory that allows\ndirect access to individual bytes or cache lines, called storage class memory ,i su n d e r\ndevelopment. Main-memory database architectures can be further optimized for such\nstorage.\n13.2 File Organization\nA database is mapped into a number of di\ufb00erent \ufb01les that are maintained by the un-\nderlying operating system. These \ufb01les reside permanently on disks. A \ufb01leis organized\nlogically as a sequence of records. These records are mapped onto disk blocks. Files are\nprovided as a basic construct in operating systems, so we shall assume the existence of\nan underlying \ufb01le system . We need to consider ways of representing logical data models\nin terms of \ufb01les.\nEach \ufb01le is also logically partitioned into \ufb01xed-length storage units called blocks ,\nwhich are the units of both storage allocation and data transfer. Most databases use\nblock sizes of 4 to 8 kilobytes by default, but many databases allow the block size to be\nspeci\ufb01ed when a database instance is created. Larger block sizes can be useful in some\ndatabase applications.\nA block may contain several records; the exact set of records that a block contains\nis determined by the form of physical data organization being used. We shall assume\nthatno record is larger than a block . This assumption is realistic for most data-processing\napplications, such as our university example. There are certainly several kinds of large\ndata items, such as images, that can be signi\ufb01cantly larger than a block. We brie\ufb02y\ndiscuss how to handle such large data items in Section 13.2.2, by storing large data\nitems separately, and storing a pointer to the data item in the record.\nIn addition, we shall require that each record is entirely contained in a single block ;\nthat is, no record is contained partly in one block, and partly in another. This restriction\nsimpli\ufb01es and speeds up access to data items.\n1To be safe, not only should the current database \ufb01t in memory, but there should be a reasonable certainty that the\ndatabase will continue to \ufb01t in memory in the medium term future, despite potential growth of the organization.\n", "617": "13.2 File Organization 589\nIn a relational database, tuples of distinct relations are generally of di\ufb00erent sizes.\nOne approach to mapping the database to \ufb01les is to use several \ufb01les and to store records\nof only one \ufb01xed length in any given \ufb01le. An alternative is to structure our \ufb01les so\nthat we can accommodate multiple lengths for records; however, \ufb01les of \ufb01xed-length\nrecords are easier to implement than are \ufb01les of variable-length records. Many of the\ntechniques used for the former can be applied to the variable-length case. Thus, we\nbegin by considering a \ufb01le of \ufb01xed-length records and consider storage of variable-\nlength records later.\n13.2.1 Fixed-Length Records\nAs an example, let us consider a \ufb01le of instructor records for our university database.\nEach record of this \ufb01le is de\ufb01ned (in pseudocode) as:\ntypeinstructor =record\nIDvarchar (5);\nname varchar (20);\ndept\nname varchar (20);\nsalary numeric (8,2);\nend\nAssume that each character occupies 1 byte and that numeric (8,2) occupies 8\nbytes. Suppose that instead of allocating a variable amount of bytes for the attributes\nID,name ,a n d dept\nname , we allocate the maximum number of bytes that each attribute\ncan hold. Then, the instructor record is 53 bytes long. A simple approach is to use the\n\ufb01rst 53 bytes for the \ufb01rst record, the next 53 bytes for the second record, and so on\n(Figure 13.1).\nSrinivasan Comp. Sci. 65000\nWu Finance 90000\nMozart Music 40000\nEinstein Physics 95000\nEl Said History 60000\nGold Physics 87000\nKatz Comp. Sci. 75000\nCali\ufb01eri History 62000\nSingh Finance 80000\nCrick Biology 72000\nBrandt Comp. Sci. 920001515110101\n12121\n22222\n32343\n33456\n45565\n58583\n76543\n76766\n83821\n98345 Kim Elec. Eng. 80000record 0\nrecord 1\nrecord 2\nrecord 3\nrecord 4\nrecord 5\nrecord 6\nrecord 7\nrecord 8\nrecord 9\nrecord 10\nrecord 11\nFigure 13.1 File containing instructor records.\n", "618": "590 Chapter 13 Data Storage Structures\nSrinivasan Comp. Sci. 65000\nWu Finance 90000\nMozart Music 40000\nEl Said History 60000\nGold Physics 87000\nKatz Comp. Sci. 75000\nCali\ufb01eri History 62000\nSingh Finance 80000\nCrick Biology 72000\nBrandt Comp. Sci. 920001515110101\n12121\n32343\n33456\n45565\n58583\n76543\n76766\n83821\n98345 Kim Elec. Eng. 80000record 0\nrecord 1\nrecord 2\nrecord 4\nrecord 5\nrecord 6\nrecord 7\nrecord 8\nrecord 9\nrecord 10\nrecord 11\nFigure 13.2 File of Figure 13.1, with record 3 deleted and all records moved.\nHowever, there are two problems with this simple approach:\n1.Unless the block size happens to be a multiple of 53 (which is unlikely), some\nrecords will cross block boundaries. That is, part of the record will be stored in\none block and part in another. It would thus require two block accesses to read\nor write such a record.\n2.It is di\ufb03cult to delete a record from this structure. The space occupied by the\nrecord to be deleted must be \ufb01lled with some other record of the \ufb01le, or we must\nhave a way of marking deleted records so that they can be ignored.\nTo avoid the \ufb01rst problem, we allocate only as many records to a block as would \ufb01t\nentirely in the block (this number can be co mputed easily by dividing the block size by\nthe record size, and discarding the fractional part). Any remaining bytes of each block\nare left unused.\nWhen a record is deleted, we could move the record that comes after it into the\nspace formerly occupied by the deleted record, and so on, until every record following\nthe deleted record has been moved ahead (Figure 13.2). Such an approach requires\nmoving a large number of records. It might be easier simply to move the \ufb01nal record\nof the \ufb01le into the space occupied by the deleted record (Figure 13.3).\nIt is undesirable to move records to occupy the space freed by a deleted record,\nsince doing so requires additional block a ccesses. Since insertions tend to be more\nfrequent than deletions, it is acceptable to leave open the space occupied by the deleted\nrecord and to wait for a subsequent insertion before reusing the space. A simple marker\non a deleted record is not su\ufb03cient, since it is hard to \ufb01nd this available space when\nan insertion is being done. Thus, we need to introduce an additional structure.\nAt the beginning of the \ufb01le, we allocate a certain number of bytes as a \ufb01le header .\nThe header will contain a variety of information about the \ufb01le. For now, all we need\n", "619": "13.2 File Organization 591\nSrinivasan Comp. Sci. 65000\nWu Finance 90000\nMozart Music 40000\nEl Said History 60000\nGold Physics 87000\nKatz Comp. Sci. 75000\nCali\ufb01eri History 62000\nSingh Finance 80000\nCrick Biology 72000\nBrandt Comp. Sci. 920001515110101\n12121\n32343\n33456\n45565\n58583\n76543\n76766\n83821record 0\nrecord 1\nrecord 2\nrecord 4\nrecord 5\nrecord 6\nrecord 7\nrecord 8\nrecord 9\nrecord 1098345 Kim Elec. Eng. 80000 record 11\nFigure 13.3 File of Figure 13.1, with record 3 deleted and final record moved.\nto store there is the address of the \ufb01rst record whose contents are deleted. We use this\n\ufb01rst record to store the address of the second available record, and so on. Intuitively,\nwe can think of these stored addresses as pointers , since they point to the location of a\nrecord. The deleted records thus form a linked list, which is often referred to as a free\nlist. Figure 13.4 shows the \ufb01le of Figure 13.1, with the free list, after records 1, 4, and\n6 have been deleted.\nOn insertion of a new record, we use the record pointed to by the header. We\nchange the header pointer to point to the next available record. If no space is available,\nwe add the new record to the end of the \ufb01le.\nheader\nrecord 0\nrecord 1\nrecord 2\nrecord 3\nrecord 4\nrecord 5\nrecord 6\nrecord 7\nrecord 8\nrecord 9\nrecord 10\nrecord 1172000\n92000\n8000065000\n40000\n95000\n87000\n62000\n76766\n83821\n9834510101\n15151\n22222\n33456\n58583\n76543\nCrick\nBrandt\nKimSrinivasan\nMozart\nEinstein\nGold\nCali\ufb01eri\nSingh\nBiology\nElec. Eng.Comp. Sci.\nComp. Sci.Music\nPhysics\nPhysics\nHistory\nFinance 80000\nFigure 13.4 File of Figure 13.1, with free list after deletion of records 1, 4, and 6.\n", "620": "592 Chapter 13 Data Storage Structures\nInsertion and deletion for \ufb01les of \ufb01xed-length records are simple to implement\nb e c a u s et h es p a c em a d ea v a i l a b l eb yad e l e t e dr e c o r di se x a c t l yt h es p a c en e e d e dt o\ninsert a record. If we allow records of variable-length in a \ufb01le, this match no longer\nholds. An inserted record may not \ufb01t in the space left free by a deleted record, or it\nmay \ufb01ll only part of that space.\n13.2.2 Variable-Length Records\nVariable-length records arise in database systems due to several reasons. The most com-\nmon reason is the presence of variable length \ufb01elds, such as strings. Other reasons\ninclude record types that contain repeating \ufb01elds such as arrays or multisets, and the\npresence of multiple record types within a \ufb01le.\nDi\ufb00erent techniques for implementing variable-length records exist. Two di\ufb00erent\nproblems must be solved by any such technique:\n1.How to represent a single record in such a way that individual attributes can be\nextracted easily, even if they are of variable length\n2.How to store variable-length records within a block, such that records in a block\ncan be extracted easily\nThe representation of a record with variable-length attributes typically has two\nparts: an initial part with \ufb01xed-length information, whose structure is the same for\nall records of the same relation, followed by the contents of variable-length attributes.\nFixed-length attributes, such as numeric values, dates, or \ufb01xed-length character strings\nare allocated as many bytes as required to store their value. Variable-length attributes,\nsuch as varchar types, are represented in the initial part of the record by a pair ( o\ufb00-\nset,length ), where o\ufb00set denotes where the data for that attribute begins within the\nrecord, and length is the length in bytes of the variable-sized attribute. The values for\nthe variable-length attributes are stored consecutively, after the initial \ufb01xed-length part\nof the record. Thus, the initial part of the record stores a \ufb01xed size of information about\neach attribute, whether it is \ufb01xed-length or variable-length.\nAn example of such a record representation is shown in Figure 13.5. The \ufb01gure\nshows an instructor record whose \ufb01rst three attributes ID,name ,a n d dept\nname are\nvariable-length strings, and whose fourth attribute salary is a \ufb01xed-sized number. We\nassume that the o\ufb00set and length values are stored in two bytes each, for a total of 4\n21, 5 26, 10 36, 10 65000 10101 Srinivasan Comp. Sci.\nBytes 0 4 8 12 20 21 26 36 450000Null bitmap (stored in 1 byte)\nFigure 13.5 Representation of a variable-length record of the instructor relation.\n", "621": "13.2 File Organization 593\n# Entries Size\nLocationBlock Header Records\nFree Space\nEnd of Free Space\nFigure 13.6 Slotted-page structure.\nbytes per attribute. The salary attribute is assumed to be stored in 8 bytes, and each\nstring takes as many bytes as it has characters.\nThe \ufb01gure also illustrates the use of a null bitmap , which indicates which attributes\nof the record have a null value. In this particular record, if the salary were null, the\nfourth bit of the bitmap would be set to 1, and the salary value stored in bytes 12\nthrough 19 would be ignored. Since the record has four attributes, the null bitmap for\nthis record \ufb01ts in 1 byte, although more bytes may be required with more attributes. In\nsome representations, the null bitmap is stored at the beginning of the record, and for\nattributes that are null, no data (value, or o\ufb00set/length) are stored at all. Such a repre-\nsentation would save some storage space, at the cost of extra work to extract attributes\nof the record. This representation is particularly useful for certain applications where\nrecords have a large number of \ufb01elds, most of which are null.\nWe next address the problem of storing variable-length records in a block. The\nslotted-page structure is commonly used for organizing records within a block and is\nshown in Figure 13.6.2There is a header at the beginning of each block, containing the\nfollowing information:\n\u2022The number of record entries in the header\n\u2022The end of free space in the block\n\u2022An array whose entries contain the location and size of each record\nT h ea c t u a lr e c o r d sa r ea l l o c a t e d contiguously in the block, starting from the end\nof the block. The free space in the block is contiguous between the \ufb01nal entry in the\nheader array and the \ufb01rst record. If a record is inserted, space is allocated for it at the\nend of free space, and an entry containing its size and location is added to the header.\nIf a record is deleted, the space that it occupies is freed, and its entry is set to deleted\n(its size is set to \u22121, for example). Further, the records in the block before the deleted\nrecord are moved, so that the free space created by the deletion gets occupied, and all\n2Here, \u201cpage\u201d is synonymous with \u201cblock.\u201d\n", "622": "594 Chapter 13 Data Storage Structures\nfree space is again between the \ufb01nal entry in the header array and the \ufb01rst record. The\nend-of-free-space pointer in the header is appropriately updated as well. Records can\nbe grown or shrunk by similar techniques, as long as there is space in the block. The\ncost of moving the records is not too high, since the size of a block is limited: typical\nvalues are around 4 to 8 kilobytes.\nThe slotted-page structure requires that there be no pointers that point directly to\nrecords. Instead, pointers must point to the entry in the header that contains the actual\nlocation of the record. This level of indirection allows records to be moved to prevent\nfragmentation of space inside a block, while supporting indirect pointers to the record.\n13.2.3 Storing Large Objects\nDatabases often store data that can be much larger than a disk block. For instance, an\nimage or an audio recording may be multiple megabytes in size, while a video object\nmay be multiple gigabytes in size. Recall that SQL supports the types blob andclob,\nwhich store binary and character large objects.\nMany databases internally restrict the size of a record to be no larger than the size\nof a block.3These databases allow records to logically contain large objects, but they\nstore the large objects separate from the other (short) attributes of records in which\nthey occur. A (logical) pointer to the object is then stored in the record containing the\nlarge object.\nLarge objects may be stored either as \ufb01les in a \ufb01le system area managed by the\ndatabase, or as \ufb01le structures stored in and managed by the database. In the latter case,\nsuch in-database large objects can optionally be represented using B+-tree \ufb01le organiza-\ntions, which we study in Section 14.4.1, to allow e\ufb03cient access to any location within\nthe object. B+-tree \ufb01le organizations permit us to read an entire object, or speci\ufb01ed byte\nranges in the object, as well as to insert and delete parts of the object.\nHowever, there are some performance issues with storing very large objects in\ndatabases. The e\ufb03ciency of accessing large objects via database interfaces is one con-\ncern. A second concern is the size of database backups. Many enterprises periodically\ncreate \u201cdatabase dumps,\u201d that is, backup copies of their databases; storing large objects\nin the database can result in a large increase in the size of the database dumps.\nMany applications therefore choose to store very large objects, such as video data,\noutside of the database, in a \ufb01le system. In such cases, the application may store the\n\ufb01le name (usually a path in the \ufb01le system) as an attribute of a record in the database.\nStoring data in \ufb01les outside the database can result in \ufb01le names in the database point-\ning to \ufb01les that do not exist, perhaps because they have been deleted, which results in\na form of foreign-key constraint violation. Further, database authorization controls are\nnot applicable to data stored in the \ufb01le system.\n3This restriction helps simplify bu\ufb00er management; as we see in Section 13.5, disk blocks are brought into an area of\nmemory called the bu\ufb00er before they are accessed. Records larger than a block would get split between blocks, which\nmay be di\ufb00erent areas of the bu\ufb00er, and thus cannot be guaranteed to be in a contiguous area of memory.\n", "623": "13.3 Organization of Records in Files 595\nSome databases support \ufb01le system integration with the database, to ensure that\nconstraints are satis\ufb01ed (for example, deletion of \ufb01les will be blocked if the database\nhas a pointer to the \ufb01le), and to ensure that access authorizations are enforced. Files\ncan be accessed both from a \ufb01le system interface and from the database SQL interface.\nFor example, Oracle supports such integration through its SecureFiles and Database\nFile System features.\n13.3 Organization of Records in Files\nSo far, we have studied how records are represented in a \ufb01le structure. A relation is a\nset of records. Given a set of records, the next question is how to organize them in a\n\ufb01le. Several of the possible ways of organizing records in \ufb01les are:\n\u2022Heap \ufb01le organization . Any record can be placed anywhere in the \ufb01le where there\nis space for the record. There is no ordering of records. Typically, there is either a\nsingle \ufb01le or a set of \ufb01les for each relation. Heap \ufb01le organization is discussed in\nSection 13.3.1.\n\u2022Sequential \ufb01le organization . Records are stored in sequential order, according to\nthe value of a \u201csearch key\u201d of each record. Section 13.3.2 describes this organiza-\ntion.\n\u2022Multitable clustering \ufb01le organization : Generally, a separate \ufb01le or set of \ufb01les is\nused to store the records of each relation. However, in a multitable clustering \ufb01le\norganization , records of several di\ufb00erent relations are stored in the same \ufb01le, and\nin fact in the same block within a \ufb01le, to reduce the cost of certain join operations.\nSection 13.3.3 describes the multitable clustering \ufb01le organization.\n\u2022B+-tree \ufb01le organization . The traditional sequential \ufb01le organization described in\nSection 13.3.2 does support ordered access even if there are insert, delete, and\nupdate operations, which may change the ordering of records. However, in the\nface of a large number of such operations, e\ufb03ciency of ordered access su\ufb00ers.\nWe study another way of organizing records, called the B+-tree \ufb01le organization ,i n\nSection 14.4.1. The B+-tree \ufb01le organization is related to the B+-tree index structure\ndescribed in that chapter and can provide e\ufb03cient ordered access to records even if\nthere are a large number of insert, delete, or update operations. Further, it supports\nvery e\ufb03cient access to speci\ufb01c records, based on the search key.\n\u2022Hashing \ufb01le organization . A hash function is computed on some attribute of each\nrecord. The result of the hash function speci\ufb01es in which block of the \ufb01le the record\nshould be placed. Section 14.5 describes this organization; it is closely related to\nthe indexing structures described in that chapter.\n", "624": "596 Chapter 13 Data Storage Structures\n13.3.1 Heap File Organization\nIn a heap \ufb01le organization, a record may be stored anywhere in the \ufb01le corresponding\nto a relation. Once placed in a particular location, the record is not usually moved.4\nWhen a record is inserted in a \ufb01le, one option for choosing the location is to always\nadd it at the end of the \ufb01le. However, if records get deleted, it makes sense to use the\nspace thus freed up to store new records. It is important for a database system to be\nable to e\ufb03ciently \ufb01nd blocks that have free sp ace, without having to sequentially search\nthrough all the blocks of the \ufb01le.\nMost databases use a space-e\ufb03cient data structure called a free-space map to track\nw h i c hb l o c k sh a v ef r e es p a c et os t o r er e c o r d s .T h ef r e e - s p a c em a pi sc o m m o n l yr e p r e -\nsented by an array containing 1 entry for each block in the relation. Each entry rep-\nresents a fraction fsuch that at least a fraction fof the space in the block is free. In\nPostgre SQL, for example, an entry is 1 byte, and the value stored in the entry must\nbe divided by 256 to get the free-space fraction. The array is stored in a \ufb01le, whose\nblocks are fetched into memory,5as required. Whenever a record is inserted, deleted,\nor changed in size, if the occupancy fraction changes enough to a\ufb00ect the entry value,\nthe entry is updated in the free-space map. An example of a free-space map for a \ufb01le\nwith 16 blocks is shown below. We assume that 3 bits are used to store the occupancy\nfraction; the value at position ishould be divided by 8 to get the free-space fraction for\nblock i.\n4\n2\n1\n4\n7\n3\n6\n5\n1\n2\n0\n1\n1\n0\n5\n6\nFor example, a value of 7 indicates that at least 7 \u22158thof the space in the block is free.\nTo \ufb01nd a block to store a new record of a given size, the database can scan the\nfree-space map to \ufb01nd a block that has enough free space to store that record. If there\nis no such block, a new block is allocated for the relation.\nWhile such a scan is much faster than actually fetching blocks to \ufb01nd free space,\nit can still be very slow for large \ufb01les. To further speed up the task of locating a block\nwith su\ufb03cient free space, we can create a second-level free-space map, which has, say 1\nentry for every 100 entries of the main free-space map. That 1 entry stores the maximum\nvalue amongst the 100 entries in the main free-space map that it corresponds to. The\nfree-space map below is a second level free-space map for our earlier example, with 1\nentry for every 4 entries in the main free-space map.\n4\n7\n2\n6\n4Records may be occasionally moved, for example, if the database sorts the records of the relation; but note that even\nif the relation is reordered by sorting, subsequent insertions and updates may result in the records no longer being\nordered.\n5Via the database bu\ufb00er, which we discuss in Section 13.5.\n", "625": "13.3 Organization of Records in Files 597\nWith 1 entry for every 100 entries in the main free-space map, a scan of the second-\nlevel free space map would take only 1/100th of the time to scan the main free-space\nmap; once a suitable entry indicating enough free space is found there, its correspond-\ning 100 entries in the main free-space map can be examined to \ufb01nd a block with su\ufb03-\ncient free space. Such a block must exist, since the second-level free-space map entry\nstores the maximum of the entries in the main free-space map. To deal with very large\nrelations, we can create more levels beyond the second level, using the same idea.\nWriting the free-space map to disk every time an entry in the map is updated would\nbe very expensive. Instead, the free-space map is written periodically; as a result, the\nfree-space map on disk may be outdated, and when a database starts up, it may get\noutdated data about available free space. The free-space map may, as a result, claim a\nblock has free space when it does not; such an error will be detected when the block is\nfetched, and can be dealt with by a further search in the free-space map to \ufb01nd another\nblock. On the other hand, the free-space map may claim that a block does not have free\nspace when it does; generally this will not result in any problem other than unused free\nspace. To \ufb01x any such errors, the relation is scanned periodically and the free-space\nmap recomputed and written to disk.\n13.3.2 Sequential File Organization\nAsequential \ufb01le is designed for e\ufb03cient processing of records in sorted order based on\nsome search key. A search key is any attribute or set of attributes; it need not be the\nprimary key, or even a superkey. To permit fast retrieval of records in search-key order,\nwe chain together records by pointers. The pointer in each record points to the next\nrecord in search-key order. Furthermore, to minimize the number of block accesses in\nsequential \ufb01le processing, we store records physically in search-key order, or as close\nto search-key order as possible.\nFigure 13.7 shows a sequential \ufb01le of instructor records taken from our university\nexample. In that example, the records are stored in search-key order, using IDas the\nsearch key.\nThe sequential \ufb01le organization allows records to be read in sorted order; that can\nbe useful for display purposes, as well as for certain query-processing algorithms that\nwe shall study in Chapter 15.\nIt is di\ufb03cult, however, to maintain physical sequential order as records are inserted\nand deleted, since it is costly to move many records as a result of a single insertion or\ndeletion. We can manage deletion by using pointer chains, as we saw previously. For\ninsertion, we apply the following two rules:\n1.Locate the record in the \ufb01le that comes before the record to be inserted in search-\nkey order.\n2.If there is a free record (i.e., space left after a deletion) within the same block\nas this record, insert the new record there. Otherwise, insert the new record in\n", "626": "598 Chapter 13 Data Storage Structures\n10101 Srinivasan\n45565 Katz\n58583 Cali\ufb01eri\n76543 Singh\n76766 Crick\n83821 Brandt\n98345 Kim12121 Wu\n15151 Mozart\n22222 Einstein\n32343 El Said\n33456 GoldComp. Sci.\nComp. Sci.Comp. Sci.\nHistory\nFinance\nBiology\nElec. Eng.Finance\nMusic\nPhysics\nHistory\nPhysics65000\n75000\n62000\n80000\n72000\n92000\n8000090000\n40000\n95000\n60000\n87000\nFigure 13.7 Sequential file for instructor records.\nanover\ufb02ow block . In either case, adjust the pointers so as to chain together the\nrecords in search-key order.\nFigure 13.8 shows the \ufb01le of Figure 13.7 after the insertion of the record (32222,\nVerdi, Music, 48000). The structure in Figure 13.8 allows fast insertion of new records,\nbut it forces sequential \ufb01le-processing applications to process records in an order that\ndoes not match the physical order of the records.\nIf relatively few records need to be stored in over\ufb02ow blocks, this approach works\nwell. Eventually, however, the correspondence between search-key order and physical\norder may be totally lost over a period of time , in which case sequential processing will\nbecome much less e\ufb03cient. At this point, the \ufb01le should be reorganized so that it is once\nagain physically in sequential order. Such reorganizations are costly and must be done\nduring times when the system load is low. The frequency with which reorganizations\nare needed depends on the frequency of insertion of new records. In the extreme case\nin which insertions rarely occur, it is possible always to keep the \ufb01le in physically sorted\norder. In such a case, the pointer \ufb01eld in Figure 13.7 is not needed.\nThe B+-tree \ufb01le organization, which we describe in Section 14.4.1, provides e\ufb03cient\nordered access even if there are many inserts, deletes, and updates, without requiring\nexpensive reorganizations.\n13.3.3 Multitable Clustering File Organization\nMost relational database systems store each relation in a separate \ufb01le, or a separate set\nof \ufb01les. Thus, each \ufb01le, and as a result, each block, stores records of only one relation,\nin such a design.\n", "627": "13.3 Organization of Records in Files 599\n10101\n45565\n76543\n76766\n83821\n9834512121\n15151\n22222\n32343\n33456\n58583Srinivasan\nKatz\nSingh\nCrick\nBrandt\nKimWu\nMozart\nEinstein\nEl Said\nGold\nCali\ufb01eriComp. Sci.\nComp. Sci.\nFinance\nBiology\nComp. Sci.\nElec. Eng.Finance\nMusic\nPhysics\nHistory\nPhysics\nHistory65000\n75000\n80000\n72000\n92000\n8000090000\n40000\n95000\n60000\n87000\n62000\n48000 Music Verdi 32222\nFigure 13.8 Sequential file after an insertion.\nHowever, in some cases it can be useful to store records of more than one relation\nin a single block. To see the advantage of storing records of multiple relations in one\nblock, consider the following SQL query for the university database:\nselect dept\nname ,building ,budget ,ID,name ,salary\nfrom department natural join instructor ;\nThis query computes a join of the department andinstructor relations. Thus, for each\ntuple of department , the system must locate the instructor tuples with the same value\nfordept\nname . Ideally, these records will be located with the help of indices ,w h i c hw e\nshall discuss in Chapter 14. Regardless of how these records are located, however, they\nneed to be transferred from disk into main memory. In the worst case, each record will\nreside on a di\ufb00erent block, forcing us to do one block read for each record required by\nthe query.\nAs a concrete example, consider the department andinstructor relations of Figure\n13.9 and Figure 13.10, respectively (for brevity, we include only a subset of the tuples\ndept\nname\n building\n budget\nComp. Sci.\n Taylor\n 100000\nPhysics\n Watson\n 70000\nFigure 13.9 The department relation.\n", "628": "600 Chapter 13 Data Storage Structures\nID\n name\n dept\nname\n salary\n10101\n Srinivasan\n Comp. Sci.\n 65000\n33456\n Gold\n Physics\n 87000\n45565\n Katz\n Comp. Sci.\n 75000\n83821\n Brandt\n Comp. Sci.\n 92000\nFigure 13.10 The instructor relation.\nof the relations we have used thus far). In Figure 13.11, we show a \ufb01le structure de-\nsigned for the e\ufb03cient execution of queries involving the natural join of department\nandinstructor . All the instructor tuples for a particular dept\nname are stored near the\ndepartment tuple for that dept\nname . We say that the two relations are clustered on the\nkeydept\nname . We assume that each record contains the identi\ufb01er of the relation to\nwhich it belongs, although this is not shown in Figure 13.11.\nAlthough not depicted in the \ufb01gure, it is possible to store the value of the dept\nname attribute, which de\ufb01nes the clustering, only once for a group of tuples (from\nboth relations), reducing storage overhead.\nThis structure allows for e\ufb03cient processing of the join. When a tuple of the de-\npartment relation is read, the entire block containing that tuple is copied from disk into\nmain memory. Since the corresponding instructor tuples are stored on the disk near the\ndepartment tuple, the block containing the department tuple contains tuples of the in-\nstructor relation needed to process the query. If a department has so many instructors\nthat the instructor records do not \ufb01t in one block, the remaining records appear on\nnearby blocks.\nAmultitable clustering \ufb01le organization is a \ufb01le organization, such as that illustrated\nin Figure 13.11, that stores related records of two or more relations in each block.6\nThecluster key is the attribute that de\ufb01nes which records are stored together; in our\npreceding example, the cluster key is dept\nname .\nComp. Sci.\n Taylor\n 100000\n10101\n Srinivasan\n Comp. Sci.\n 65000\n45565\n Katz\n Comp. Sci.\n 75000\n83821\n Brandt\n Comp. Sci.\n 92000\nPhysics\n Watson\n 70000\n33456\n Gold\n Physics\n 87000\nFigure 13.11 Multitable clustering file structure.\n6Note that the word cluster is often used to refer to a group of machines th at together constitute a parallel database;\nthat use of the word cluster is unrelated to the concept of multitable clustering.\n", "629": "13.3 Organization of Records in Files 601\nAlthough a multitable clustering \ufb01le organization can speed up certain join queries,\nit can result in slowing processing of other types of queries. For example, in our pre-\nceding example,\nselect *\nfrom department ;\nrequires more block accesses than it did in the scheme under which we stored each\nrelation in a separate \ufb01le, since each block now contains signi\ufb01cantly fewer department\nrecords. To locate e\ufb03ciently all tuples of the department relation within a particular\nblock, we can chain together all the records of that relation using pointers; however,\nthe number of blocks read does not get a\ufb00ected by using such chains.\nWhen multitable clustering is to be used depends on the types of queries that the\ndatabase designer believes to be most frequent . Careful use of multitable clustering can\nproduce signi\ufb01cant performance gains in query processing.\nMultitable clustering is supported by the Oracle database system. Clusters can be\ncreated by using a create cluster command, with a speci\ufb01ed cluster key. An extension\nof the create table command can be used to specify that a relation is to be stored in\na speci\ufb01c cluster, with a particular attribute used as the cluster key. Multiple relations\ncan thus be allocated to a cluster.\n13.3.4 Partitioning\nMany databases allow the records in a relation to be partitioned into smaller relations\nthat are stored separately. Such table partitioning is typically done on the basis of an\nattribute value; for example, records in a transaction relation in an accounting database\nmay be partitioned by year into smaller relations corresponding to each year, such as\ntransaction\n 2018,transaction\n 2019, and so on. Queries can be written based on the trans-\naction relation but are translated into queries on the year-wise relations. Most accesses\nare to records of the current year and include a selection based on the year. Query op-\ntimizers can rewrite such a query to only a ccess the smaller relation corresponding to\nthe requested year, and they can avoid reading records corresponding to other years.\nFor example, a query\nselect *\nfrom transaction\nwhere year=2019\nwould only access the relation transaction\n 2019, ignoring the other relations, while a\nquery without the selection condition would read all the relations.\nThe cost of some operations, such as \ufb01nding free space for a record, increase with\nrelation size; by reducing the size of each rela tion, partitioning helps reduce such over-\nheads. Partitioning can also be used to store di\ufb00erent parts of a relation on di\ufb00erent\n", "630": "602 Chapter 13 Data Storage Structures\nstorage devices; for example, in the year 2019, transaction\n 2018 and earlier year trans-\nactions can which are infrequently accessed could be stored on magnetic disk, while\ntransaction\n 2019 could be stored on SSD, for faster access.\n13.4 Data-Dictionary Storage\nSo far, we have considered only the representation of the relations themselves. A rela-\ntional database system needs to maintain data about the relations, such as the schema\nof the relations. In general, such \u201cdata about data\u201d are referred to as metadata .\nRelational schemas and other metadata about relations are stored in a structure\ncalled the data dictionary orsystem catalog . Among the types of information that the\nsystem must store are these:\n\u2022Names of the relations\n\u2022Names of the attributes of each relation\n\u2022Domains and lengths of attributes\n\u2022Names of views de\ufb01ned on the database, and de\ufb01nitions of those views\n\u2022Integrity constraints (e.g., key constraints)\nIn addition, many systems keep the following data on users of the system:\n\u2022Names of users, the default schemas of the users, and passwords or other informa-\ntion to authenticate users\n\u2022Information about authorizations for each user\nFurther, the database may store statistical and descriptive data about the relations and\nattributes, such as the number of tuples in each relation, or the number of distinct\nvalues for each attribute.\nThe data dictionary may also note the storage organization (heap, sequential, hash,\netc.) of relations, and the location where each relation is stored:\n\u2022If relations are stored in operating system \ufb01les, the dictionary would note the\nnames of the \ufb01le (or \ufb01les) containing each relation.\n\u2022If the database stores all relations in a single \ufb01le, the dictionary may note the blocks\ncontaining records of each relation in a data structure such as a linked list.\nIn Chapter 14, in which we study indices, we shall see a need to store information about\neach index on each of the relations:\n\u2022Name of the index\n", "631": "13.4 Data-Dictionary Storage 603\n\u2022Name of the relation being indexed\n\u2022Attributes on which the index is de\ufb01ned\n\u2022Type of index formed\nAll this metadata information constitutes, in e\ufb00ect, a miniature database. Some\ndatabase systems store such metadata by using special-purpose data structures and\ncode. It is generally preferable to store the data about the database as relations in the\ndatabase itself. By using database relations to store system metadata, we simplify the\noverall structure of the system and harness the full power of the database for fast access\nto system data.\nThe exact choice of how to represent system metadata by relations must be made\nby the system designers. We show the schema diagram of a toy data dictionary in Figure\n13.12, storing part of the information mentioned above. The schema is only illustrative;\nreal implementations store far more information than what the \ufb01gure shows. Read the\nmanuals for whichever database you use to see what system metadata it maintains.\nIn the metadata representation shown, the attribute index\n attributes of the relation\nIndex\n metadata is assumed to contain a list of one or more attributes, which can be\nrepresented by a character string such as \u201c dept\nname ,building \u201d. The Index\n metadata\nrelation is thus not in \ufb01rst normal form; it can be normalized, but the preceding repre-\nsentation is likely to be more e\ufb03cient to access. The data dictionary is often stored in\na nonnormalized form to achieve fast access.\nRelation_metadata\nrelation_name\nnumber_of_attributes\nstorage_organization\nlocation\nIndex_metadata\nindex_name\nrelation_name\nindex_type\nindex_attributes\nView_metadata\nview_name\nde\ufb01nitionAttribute_metadata\nrelation_name\nattribute_name\ndomain_type\nposition\nlength\nUser_metadata\nuser_name\nencrypted_password\ngroup\nFigure 13.12 Relational schema representing part of the system metadata.\n", "632": "604 Chapter 13 Data Storage Structures\nWhenever the database system needs to retrieve records from a relation, it must\n\ufb01rst consult the Relation\n metadata relation to \ufb01nd the location and storage organization\nof the relation, and then fetch records using this information.\nHowever, the storage organization and location of the Relation\n metadata relation\nitself must be recorded elsewhere (e.g., in the database code itself, or in a \ufb01xed loca-\ntion in the database), since we need this information to \ufb01nd the contents of Relation\nmetadata .\nSince system metadata are frequently accessed, most databases read it from the\ndatabase into in-memory data structures that can be accessed very e\ufb03ciently. This is\ndone as part of the database startup, before the database starts processing any queries.\n13.5 Database Buffer\nThe size of main memory on servers has increased greatly over the years, and many\nmedium-sized databases can \ufb01t in memory. However, a server has many demands on\nits memory, and the amount of memory it can give to a database may be much smaller\nthan the database size even for medium-sized databases. And many large databases are\nmuch larger than the available memory on servers.\nThus, even today, database data reside primarily on disk in most databases, and\nthey must be brought into memory to be read or updated; updated data blocks must be\nwritten back to disk subsequently.\nSince data access from disk is much slower than in-memory data access, a major\ngoal of the database system is to minimize the number of block transfers between the\ndisk and memory. One way to reduce the number of disk accesses is to keep as many\nblocks as possible in main memory. The goal is to maximize the chance that, when a\nblock is accessed, it is already in main memory, and, thus, no disk access is required.\nSince it is not possible to keep all blocks in main memory, we need to manage the\nallocation of the space available in main memory for the storage of blocks. The bu\ufb00er\nis that part of main memory available for storage of copies of disk blocks. There is\nalways a copy kept on disk of every block, but the copy on disk may be a version of the\nblock older than the version in the bu\ufb00er. The subsystem responsible for the allocation\nof bu\ufb00er space is called the bu\ufb00er manager .\n13.5.1 Buffer Manager\nPrograms in a database system make requests (i.e., calls) on the bu\ufb00er manager when\nthey need a block from disk. If the block is already in the bu\ufb00er, the bu\ufb00er manager\npasses the address of the block in main memory to the requester. If the block is not in\nthe bu\ufb00er, the bu\ufb00er manager \ufb01rst allocates space in the bu\ufb00er for the block, throwing\nout some other block, if necessary, to make space for the new block. The thrown-out\nblock is written back to disk only if it has been modi\ufb01ed since the most recent time\nthat it was written to the disk. Then, the bu\ufb00er manager reads in the requested block\nfrom the disk to the bu\ufb00er, and passes the address of the block in main memory to the\n", "633": "13.5 Database Bu\ufb00er 605\nrequester. The internal actions of the bu\ufb00er manager are transparent to the programs\nthat issue disk-block requests.\nIf you are familiar with operating-system concepts, you will note that the bu\ufb00er\nmanager appears to be nothing more than a virtual-memory manager, like those found\nin most operating systems. One di\ufb00erence is that the size of the database might be\nlarger than the hardware address space of a machine, so memory addresses are not\nsu\ufb03cient to address all disk blocks. Further, to serve the database system well, the\nbu\ufb00er manager must use techniques more sophisticated than typical virtual-memory\nmanagement schemes:\n13.5.1.1 Bu\ufb00er replacement strategy\nWhen there is no room left in the bu\ufb00er, a block must be evicted ,t h a ti s ,r e m o v e d ,f r o m\nthe bu\ufb00er before a new one can be read in. Most operating systems use a least recently\nused (LRU) scheme, in which the block that was referenced least recently is written\nback to disk and is removed from the bu\ufb00er. This simple approach can be improved on\nfor database applications(see Section 13.5.2).\n13.5.1.2 Pinned blocks\nOnce a block has been brought into the bu\ufb00er, a database process can read the con-\ntents of the block from the bu\ufb00er memory. However, while the block is being read, if\na concurrent process evicts the block and replaces it with a di\ufb00erent block, the reader\nthat was reading the contents of the old block will see incorrect data; if the block was\nbeing written when it was evicted, the writer would end up damaging the contents of\nthe replacement block.\nIt is therefore important that before a process reads data from a bu\ufb00er block, it\nensures that the block will not get evicted. To do so, the process executes a pinoperation\non the block; the bu\ufb00er manager never evicts a pinned block. When it has \ufb01nished\nreading data, the process should execute an unpin operation, allowing the block to be\nevicted when required. The database code should be written carefully to avoid pinning\ntoo many blocks: if all the blocks in the bu\ufb00er get pinned, no blocks can be evicted,\nand no other block can be brought into the bu\ufb00er. If this happens, the database will be\nunable to carry out any further processing!\nMultiple processes can read data from a block that is in the bu\ufb00er. Each of them is\nrequired to execute a pin operation before accessing data, and an unpin after completing\naccess. The block cannot be evicted until all processes that have executed a pin have\nthen executed an unpin operation. A simple way to ensure this property is to keep a pin\ncount for each bu\ufb00er block. Each pin operation increments the count, and an unpin\noperation decrements the count. A page can be evicted only if the pin count equals 0.\n13.5.1.3 Shared and Exclusive Locks on Bu\ufb00er\nAp r o c e s st h a ta d d so rd e l e t e sat u p l ef r o map a g em a yn e e dt om o v et h ep a g ec o n t e n t s\naround; during this period, no other process should read the contents of the page, since\n", "634": "606 Chapter 13 Data Storage Structures\nthey may be inconsistent. Database bu\ufb00er managers allow processes to get shared and\nexclusive locks on the bu\ufb00er.\nWe will study locking in more detail in Chapter 18, but here we discuss a limited\nform of locking in the context of the bu\ufb00er manager. The locking system provided by\nthe bu\ufb00er manager allows a database process to lock a bu\ufb00er block either in shared\nmode or in exclusive mode before accessing the block, and to release the lock later,\nafter the access is completed. Here are the rules for locking:\n\u2022Any number of processes may have shared locks on a block at the same time.\n\u2022Only one process is allowed to get an exclusive lock at a time, and further when\na process has an exclusive lock, no other process may have a shared lock on the\nblock. Thus, an exclusive lock can be granted only when no other process has a\nlock on the bu\ufb00er block.\n\u2022If a process requests an exclusive lock when a block is already locked in shared or\nexclusive mode, the request is kept pending until all earlier locks are released.\n\u2022If a process requests a shared lock when a block is not locked, or already shared\nlocked, the lock may be granted; however, if another process has an exclusive lock,\nthe shared lock is granted only after the exclusive lock has been released.\nLocks are acquired and released as follows:\n\u2022Before carrying out any operation on a block, a process must pin the block as we\nsaw earlier. Locks are obtained subsequently and must be released before unpin-\nning the block.\n\u2022Before reading data from a bu\ufb00er block, a process must get a shared lock on the\nblock. When it is done reading the data, the process must release the lock.\n\u2022Before updating the contents of a bu\ufb00er block, a process must get an exclusive lock\non the block; the lock must be released after the update is complete.\nThese rules ensure that a block cannot be updated while another process is reading\nit, and conversely, a block cannot be read while another process is updating it. These\nrules are required for safety of bu\ufb00er access; however, to protect a database system from\nproblems due to concurrent access, these steps are not su\ufb03cient: further steps need to\nbe taken. These are discussed further in Chapter 17 and Chapter 18.\n13.5.1.4 Output of blocks\nIt is possible to output a block only when the bu\ufb00er space is needed for another block.\nHowever, it makes sense to not wait until the bu\ufb00er space is needed, but to rather write\nout updated blocks ahead of such a need. Then, when space is required in the bu\ufb00er,\n", "635": "13.5 Database Bu\ufb00er 607\na block that has already been written out can be evicted, provided it is not currently\npinned.\nHowever, for the database system to be able to recover from crashes (Chapter 19),\nit is necessary to restrict those times when a block may be written back to disk. For\ninstance, most recovery systems require that a block should not be written to disk while\nan update on the block is in progress. To enforce this requirement, a process that wishes\nto write the block to disk must obtain a shared lock on the block.\nMost databases have a process that continually detects updated blocks and writes\nthem back to disk.\n13.5.1.5 Forced output of blocks\nThere are situations in which it is necessary to write a block to disk, to ensure that\ncertain data on disk are in a consistent state. Such a write is called a forced output of a\nblock. We shall see the reason for forced output in Chapter 19.\nMemory contents and thus bu\ufb00er contents are lost in a crash, whereas data on\ndisk (usually) survive a crash. Forced out put is used in conjunction with a logging\nmechanism to ensure that when a transaction that has performed updates commits,\nenough data has been written to disk to ensure the updates of the transaction are not\nlost. How exactly this is done is covered in detail in Chapter 19.\n13.5.2 Buffer-Replacement Strategies\nThe goal of a replacement strategy for blocks in the bu\ufb00er is to minimize accesses to\nthe disk. For general-purpose programs, it is not possible to predict accurately which\nblocks will be referenced. Therefore, operating systems use the past pattern of block\nreferences as a predictor of future references. The assumption generally made is that\nblocks that have been referenced recently are likely to be referenced again. Therefore, if\na block must be replaced, the least recently referenced block is replaced. This approach\nis called the least recently used (LRU) block-replacement scheme.\nLRU is an acceptable replacement scheme in operating systems. However, a data-\nbase system is able to predict the pattern of future references more accurately than\nan operating system. A user request to the database system involves several steps. The\ndatabase system is often able to determine in advance which blocks will be needed by\nlooking at each of the steps required to perform the user-requested operation. Thus,\nunlike operating systems, which must rely on the past to predict the future, database\nsystems may have information regarding at least the short-term future.\nTo illustrate how information about future block access allows us to improve the\nLRU strategy, consider the processing of the SQL query:\nselect *\nfrom instructor natural join department ;\n", "636": "608 Chapter 13 Data Storage Structures\nAssume that the strategy chosen to process this request is given by the pseudocode\nprogram shown in Figure 13.13. (We shall study other, more e\ufb03cient, strategies in\nChapter 15.)\nAssume that the two relations of this example are stored in separate \ufb01les. In this\nexample, we can see that, once a tuple of instructor has been processed, that tuple is\nnot needed again. Therefore, once processing of an entire block of instructor tuples is\ncompleted, that block is no longer needed in main memory, even though it has been\nused recently. The bu\ufb00er manager should be instructed to free the space occupied by an\ninstructor block as soon as the \ufb01nal tuple has been processed. This bu\ufb00er-management\nstrategy is called the toss-immediate strategy.\nNow consider blocks containing department tuples. We need to examine every\nblock of department tuples once for each tuple of the instructor relation. When process-\ning of a department block is completed, we know that that block will not be accessed\nagain until all other department blocks have been processed. Thus, the most recently\nused department block will be the \ufb01nal block to be re-referenced, and the least recently\nused department block is the block that will be referenced next. This assumption set\nis the exact opposite of the one that forms the basis for the LRU strategy. Indeed, the\noptimal strategy for block replacement for the above procedure is the most recently\nused (MRU )s t r a t e g y .I fa department block must be removed from the bu\ufb00er, the MRU\nstrategy chooses the most recently used block (blocks are not eligible for replacement\nwhile they are being used).\nfor each tuple iofinstructor do\nfor each tuple dofdepartment do\nifi[dept\nname ]=d[dept\nname ]\nthen begin\nletxbe a tuple de\ufb01ned as follows:\nx[ID]: =i[ID]\nx[dept\nname ]: =i[dept\nname ]\nx[name ]: =i[name ]\nx[salary ]: =i[salary ]\nx[building ]: =d[building ]\nx[budget ]: =d[budget ]\ninclude tuple xas part of result of instructor \u22c8department\nend\nend\nend\nFigure 13.13 Procedure for computing join.\n", "637": "13.5 Database Bu\ufb00er 609\nFor the MRU strategy to work correctly for our example, the system must pin the\ndepartment block currently being processed. After the \ufb01nal department tuple has been\nprocessed, the block is unpinned, and it becomes the most recently used block.\nIn addition to using knowledge that the system may have about the request being\nprocessed, the bu\ufb00er manager can use statistical information about the probability that\na request will reference a particular relation. For example, the data dictionary, which\nwe saw in Section 13.4, is one of the most frequently accessed parts of the database,\nsince the processing of every query needs to access the data dictionary. Thus, the bu\ufb00er\nmanager should try not to remove data-dictionary blocks from main memory, unless\nother factors dictate that it do so. In Chapter 14, we discuss indices for \ufb01les. Since an\nindex for a \ufb01le may be accessed more frequently than the \ufb01le itself, the bu\ufb00er man-\nager should, in general, not remove index blocks from main memory if alternatives are\navailable.\nThe ideal database block-replacement strategy needs knowledge of the database\noperations\u2014both those being performed and those that will be performed in the fu-\nture. No single strategy is known that handles all the possible scenarios well. Indeed,\na surprisingly large number of database systems use LRU, despite that strategy\u2019s faults.\nThe practice questions and exercises explore alternative strategies.\nThe strategy that the bu\ufb00er manager uses for block replacement is in\ufb02uenced by\nfactors other than the time at which the block will be referenced again. If the system is\nprocessing requests by several users concurrently, the concurrency-control subsystem\n(Chapter 18) may need to delay certain requests, to ensure preservation of database\nconsistency. If the bu\ufb00er manager is given information from the concurrency-control\nsubsystem indicating which requests are being delayed, it can use this information to\nalter its block-replacement strategy. Speci\ufb01cally, blocks needed by active (nondelayed)\nrequests can be retained in the bu\ufb00er at the expense of blocks needed by the delayed\nrequests.\nThe crash-recovery subsystem (Chapter 19) imposes stringent constraints on block\nreplacement. If a block has been modi\ufb01ed, the bu\ufb00er manager is not allowed to write\nback the new version of the block in the bu\ufb00er to disk, since that would destroy the\nold version. Instead, the block manager must seek permission from the crash-recovery\nsubsystem before writing out a block. The crash-recovery subsystem may demand that\ncertain other blocks be force-output before it grants permission to the bu\ufb00er manager to\noutput the block requested. In Chapter 19, we de\ufb01ne precisely the interaction between\nthe bu\ufb00er manager and the crash-recovery subsystem.\n13.5.3 Reordering of Writes and Recovery\nDatabase bu\ufb00ers allow writes to be performed in-memory and output to disk at a later\ntime, possibly in an order di\ufb00erent from the order in which the writes were performed.\nFile systems, too, routinely reorder write operations. However, such reordering can lead\nto inconsistent data on disk in the event of a system crash.\n", "638": "610 Chapter 13 Data Storage Structures\nTo understand the problem in the context of a \ufb01le system, suppose that a \ufb01le system\nuses a linked list to track which blocks are part of a \ufb01le. Suppose also that it inserts a\nnew node at the end of the list by \ufb01rst writing the data for the new node, then updating\nthe pointer from the previous node. Suppose further that the writes were reordered, so\nthe pointer was updated \ufb01rst, and the system crashes before the new node is written.\nThe contents of the node would then be whatever happened to be on that disk earlier,\nresulting in a corrupted data structure.\nTo deal with the possibility of such data structure corruption, earlier-generation \ufb01le\nsystems had to perform a \ufb01le system consistency check on system restart, to ensure that\nthe data structures were consistent. And if they were not, extra steps had to be taken\nto restore them to consistency. These checks resulted in long delays in system restart\nafter a crash, and the delays became worse as disk systems grew to higher capacities.\nThe \ufb01le system can avoid inconsistencies in many cases if it writes updates to meta-\ndata in a carefully chosen order. But doing so would mean that optimizations such as\ndisk arm scheduling cannot be done, a\ufb00ecting the e\ufb03ciency of the update. If a non-\nvolatile write bu\ufb00er were available, it could be used to perform the writes in order to\nnon-volatile RAM and later reorder the writes when writing them to disk.\nHowever, most disks do not come with a non-volatile write bu\ufb00er; instead, mod-\nern \ufb01le systems assign a disk for storing a log of the writes in the order that they are\nperformed. Such a disk is called a log disk . For each write, the log contains the block\nnumber to be written to, and the data to be written, in the order in which the writes\nwere performed. All access to the log disk is sequential, essentially eliminating seek\ntime, and several consecutive blocks can be written at once, making writes to the log\ndisk several times faster than random writes. As before, the data have to be written to\ntheir actual location on disk as well, but the write to the actual location can be done\nlater; the writes can be reordered to minimize disk-arm movement.\nIf the system crashes before some writes to the actual disk location have been\ncompleted, when the system comes back up it reads the log disk to \ufb01nd those writes\nthat had not been completed and carries them out then. After the writes have been\nperformed, the records are deleted from the log disk.\nFile systems that support log disks as above are called journaling \ufb01le systems .J o u r -\nnaling \ufb01le systems can be implemented even without a separate log disk, keeping data\nand the log on the same disk. Doing so reduces the monetary cost at the expense of\nlower performance.\nMost modern \ufb01le systems implement journaling and use the log disk when writing\n\ufb01le system metadata such as \ufb01le allocation information. Journaling \ufb01le systems allow\nquick restart without the need for such \ufb01le system consistency checks.\nHowever, writes performed by applications are usually not written to the log disk.\nDatabase systems instead implement their own forms of logging, which we study in\nChapter 19, to ensure that the contents of a database can be safely recovered in the\nevent of a failure, even if writes were reordered.\n", "639": "13.6 Column-Oriented Storage 611\n13.6 Column-Oriented Storage\nDatabases traditionally store all attributes of a tuple together in a record, and tuples are\nstored in a \ufb01le as we have just seen. Such a storage layout is referred to as a row-oriented\nstorage .\nIn contrast, in column-oriented storage , also called a columnar storage ,e a c ha t -\ntribute of a relation is stored separately, with values of the attribute from successive\ntuples stored at successive positions in the \ufb01le. Figure 13.14 shows how the instructor\nrelation would be stored in column-oriented storage, with each attribute stored sepa-\nrately.\nIn the simplest form of column-oriented storage, each attribute is stored in a sepa-\nrate \ufb01le. Further, each \ufb01le is compressed , to reduce its size. (We discuss more complex\nschemes that store columns consecutively in a single \ufb01le later in this section.)\nIf a query needs to access the entire contents of the ith row of a table, the values at\ntheith position in each of the columns are retrieved and used to reconstruct the row.\nColumn-oriented storage thus has the drawback that fetching multiple attributes of a\nsingle tuple requires multiple I/Ooperations. Thus, it is not suitable for queries that\nfetch multiple attributes from a few rows of a relation.\nHowever, column-oriented storage is well suited for data analysis queries, which\nprocess many rows of a relation, but often only access some of the attributes. The\nreasons are as follows:\n\u2022Reduced I/O. When a query needs to access only a few attributes of a relation with a\nlarge number of attributes, the remaining attributes need not be fetched from disk\ninto memory. In contrast, in row-oriented storage, irrelevant attributes are fetched\ninto memory from disk. The reduction in I/Ocan lead to signi\ufb01cant reduction in\nquery execution cost.\n10101\n12121\n15151\n22222\n32343\n33456\n45565\n58583\n76543\n76766\n83821\n98345\nSrinivasan\nWu\nMozart\nEinstein\nEl Said\nGold\nKatz\nCali\ufb01eri\nSingh\nCrick\nBrandt\nKim\nComp. Sci.\nFinance\nMusic\nPhysics\nHistory\nPhysics\nComp. Sci.\nHistory\nFinance\nBiology\nComp. Sci.\nElec. Eng.\n65000\n90000\n40000\n95000\n60000\n87000\n75000\n62000\n80000\n72000\n92000\n80000\nFigure 13.14 Columnar representation of the instructor relation.\n", "640": "612 Chapter 13 Data Storage Structures\n\u2022Improved CPU cache performance . When the query processor fetches the con-\ntents of a particular attribute, with modern CPU architectures multiple consecutive\nbytes, called a cache line, are fetched from memory to CPU cache. If these bytes\nare accessed later, access is much faster if they are in cache than if they have to\nbe fetched from main memory. However, if these adjacent bytes contain values for\nattributes that are not be needed by the query, fetching them into cache wastes\nmemory bandwidth and uses up cache space that could have been used for other\ndata. Column-oriented storage does not su\ufb00er from this problem, since adjacent\nbytes are from the same column, and data analysis queries usually access all these\nvalues consecutively.\n\u2022Improved compression . Storing values of the same type together signi\ufb01cantly in-\ncreases the e\ufb00ectiveness of compressi on, when compared to compressing data\nstored in row format; in the latter case, adjacent attributes are of di\ufb00erent types, re-\nducing the e\ufb03ciency of compression. Com pression signi\ufb01cantly reduces the time\ntaken to retrieve data from disk, which is often the highest-cost component for\nmany queries. If the compressed \ufb01les are stored in memory, the in-memory stor-\nage space is also reduced correspondingly, which is particularly important since\nmain memory is signi\ufb01cantly more expensive than disk storage.\n\u2022Vector processing .M a n ym o d e r n CPU architectures support vector processing ,\nwhich allows a CPU operation to be applied in parallel on a number of elements\nof an array. Storing data columnwise allows vector processing of operations such\nas comparing an attribute with a constant, which is important for applying selec-\ntion conditions on a relation. Vector processing can also be used to compute an\naggregate of multiple values in parallel, instead of aggregating the values one at a\ntime.\nAs a result of these bene\ufb01ts, column-oriented storage is increasingly used in data-\nwarehousing applications, where queries are primarily data analysis queries. It should\nbe noted that indexing and query processing techniques need to be carefully designed\nto get the performance bene\ufb01ts of column-oriented storage. We outline indexing and\nquery processing techniques based on bitmap representations, which are well suited to\ncolumn-oriented storage, in Section 14.9; further details are provided in Section 24.3.\nDatabases that use column-oriented storage are referred to as column stores , while\ndatabases that use row-oriented storage are referred to as row stores .\nIt should be noted that column-oriented storage does have several drawbacks,\nwhich make them unsuitable for transaction processing.\n\u2022Cost of tuple reconstruction . As we saw earlier, reconstructing a tuple from the\nindividual columns can be expensive, negating the bene\ufb01ts of columnar represen-\ntation if many columns need to be reconstructed. While tuple reconstruction is\ncommon in transaction-processing applications, data analysis applications usually\n", "641": "13.6 Column-Oriented Storage 613\noutput only a few columns out of many that are stored in \u201cfact tables\u201d in data\nwarehouses.\n\u2022Cost of tuple deletion and update . Deleting or updating a single tuple in a com-\npressed representation would require rewriting the entire sequence of tuples that\nare compressed as one unit. Since updates and deletes are common in transaction-\nprocessing applications, column-oriented storage would result in a high cost for\nthese operations if a large number of tuples were compressed as one unit.\nIn contrast, data-warehousing systems typically do not support updates to tu-\nples, and instead support only insert of new tuples and bulk deletes of a large\nnumber of old tuples at a time. Inserts are done at the end of the relation repre-\nsentation, that is, new tuples are appended to the relation. Since small deletes and\nupdates do not occur in a data warehouse, large sequences of attribute values can\nbe stored and compressed together as one unit, allowing for better compression\nthan with small sequences.\n\u2022Cost of decompression . Fetching data from a compressed representation requires\ndecompression , which in the simplest compressed representations requires reading\nall the data from the beginning of a \ufb01le. T ransaction processing queries usually\nonly need to fetch a few records; sequential access is expensive in such a scenario,\nsince many irrelevant records may have to be decompressed to access a few relevant\nrecords.\nSince data analysis queries tend to access many consecutive records, the time spent\non decompression is typically not wasted. However, even data analysis queries do not\nneed to access records that fail selection conditions, and attributes of such records\nshould be skipped to reduce disk I/O.\nTo allow skipping of attribute values from such records, compressed representa-\ntions for column stores allow decompression to start at any of a number of points in\nthe \ufb01le, skipping earlier parts of the \ufb01le. This could be done by starting compression\nafresh after every 10,000 values (for example). By keeping track of where in the \ufb01le the\ndata start for each group of 10,000 values, it is possible to access the ith value by going\nto the start of the group \u230ai\u221510000 \u230band starting decompression from there.\nORC and Parquet are columnar \ufb01le representations used in many big-data pro-\ncessing applications. In ORC, a row-oriented representation is converted to column-\noriented representation as follows: A sequence of tuples occupying several hundred\nmegabytes is broken up into a columnar representation called a stripe .A n ORC \ufb01le\ncontains several such stripes, with each stripe occupying around 250 megabytes.\nFigure 13.15 illustrates some details of the ORC \ufb01le format. Each stripe has index\ndata followed by row data. The row data area stores a compressed representation of\nthe sequence of value for the \ufb01rst column, followed by the compressed representation\nof the second column, and so on. The index data region of a stripe stores for each\nattribute the starting point within the stripe for each group of (say) 10,000 values of\n", "642": "614 Chapter 13 Data Storage Structures\nCol1 Data\nCol2 Data\nCol3 Data\nCol4 Data\nCol5 DataStripe 1\nStripe 2\nStripe nCol2 Index\nCol3 Index\nCol4 Index\nCol5 IndexIndex Data\nStripe FooterRow Data\nIndex Data\nStripe FooterRow Data\nIndex Data\nStripe FooterRow DataCol1 Index\nFile Footer\nFigure 13.15 Columnar data representation in the ORC file format.\nthat attribute.7The index is useful for quick access to a desired tuple or sequence of\ntuples; the index also allows queries containing selections to skip groups of tuples if\nthe query determines that no tuple in those groups satis\ufb01es the selections. ORC \ufb01les\nstore several other pieces of information in the stripe footer and \ufb01le footer, which we\nskip here.\nSome column-store systems allow groups of columns that are often accessed to-\ngether to be stored together, instead of breaking up each column into a di\ufb00erent \ufb01le.\nSuch systems thus allow a spectrum of choices that range from pure column-oriented\nstorage, where every column is stored separately, to pure row-oriented storage, where all\ncolumns are stored together. The choice of which attributes to store together depends\non the query workload.\n7ORC \ufb01les have some other information that we ignore here.\n", "643": "13.7 Storage Organization in Main-Memory Databases 615\nSome of the bene\ufb01ts of column-oriented storage can be obtained even in a row-\noriented storage system by logically decomposing a relation into multiple relations. For\nexample, the instructor relation could be decomposed into three relations, containing\n(ID,name ), (ID,dept\nname )a n d( ID,salary ), respectively. Then, queries that access\no n l yt h en a m ed on o th a v et of e t c ht h e dept\nname andsalary attributes. However, in\nthis case the same IDattribute occurs in three tuples, resulting in wasted space.\nSome database systems use a column-oriented representation for data within a disk\nblock, without using compression.8Thus, a block contains data for a set of tuples, and\nall attributes for that set of tuples are stored in the same block. Such a scheme is useful\nin transaction-processing systems, since retrieving all attribute values does not require\nmultiple disk accesses. At the same time, using column-oriented storage within the\nblock provides the bene\ufb01ts of more e\ufb03cient memory access and cache usage, as well\nas the potential for using vector processing on the data. However, this scheme does\nnot allow irrelevant disk blocks to be skipped when only a few attributes are retrieved,\nnor does it give the bene\ufb01ts of compression. Thus, it represents a point in the space\nbetween pure row-oriented storage and pure column-oriented storage.\nSome databases, such as SAP HANA support two underlying storage systems, one\na row-oriented one designed for transaction processing, and the second a column-\noriented one, designed for data analysis. Tuples are normally created in the row-\noriented store but are later migrated to the column-oriented store when they are no\nlonger likely to be accessed in a row-oriented manner. Such systems are called hybrid\nrow/column stores .\nIn other cases, applications store transactional data in a row-oriented store, but\nc o p yd a t ap e r i o d i c a l l y( e . g . ,o n c ead a yo raf e wt i m e sad a y )t oad a t aw a r e h o u s e ,\nwhich may use a column-oriented storage system.\nSybase IQwas one of the early products to use column-oriented storage, but there\nare now several research projects and companies that have developed database systems\nbased on column stores, including C-Store, Vertica, MonetDB, Vectorwise, among oth-\ners. See Further Reading at the end of the chapter for more details.\n13.7 Storage Organization in Main-Memory Databases\nToday, main-memory sizes are large enough, and main memory is cheap enough, that\nmany organizational databases \ufb01t entirely in memory. Such large main memories can\nbe used by allocating a large amount of memory to the database bu\ufb00er, which will allow\nthe entire database to be loaded into bu\ufb00er, avoiding disk I/Ooperations for reading\ndata; updated blocks still have to be written back to disk for persistence. Thus, such a\nsetup would provide much better performance than one where only part of the database\nc a n\ufb01 ti nt h eb u \ufb00 e r .\n8Compression can be applied to data in a disk block, but accessing them requires decompression, and the decom-\npressed data may no longer \ufb01t in a block. Signi\ufb01cant changes need to be made to the database code, including bu\ufb00er\nmanagement, to handle such issues.\n", "644": "616 Chapter 13 Data Storage Structures\nHowever, if the entire database \ufb01ts in memory, performance can be improved sig-\nni\ufb01cantly by tailoring the storage organization and database data structures to exploit\nthe fact that data are fully in memory. A main-memory database is a database where\nall data reside in memory; main-memory database systems are typically designed to\noptimize performance by making use of this fact. In particular, they do away entirely\nwith the bu\ufb00er manager.\nAs an example of optimizations that can be done with memory-resident data, con-\nsider the cost of accessing a record, given a record pointer. With disk-based databases,\nrecords are stored in blocks, and pointers to records consist of a block identi\ufb01er and\nan o\ufb00set or slot number within the block. Following such a record pointer requires\nchecking if the block is in the bu\ufb00er (usually done by using an in-memory hash index),\nand if it is, \ufb01nding where in the bu\ufb00er it is located. If it is not in bu\ufb00er, it has to be\nfetched. All these actions take a signi\ufb01cant number of CPU cycles.\nIn contrast, in a main-memory database, it is possible to keep direct pointers to\nrecords in memory, and accessing a record is just an in-memory pointer traversal, which\nis a very e\ufb03cient operation. This is possible as long as records are not moved around.\nIndeed, one reason for such movement, namely loading into bu\ufb00er and eviction from\nbu\ufb00er, is no longer an issue.\nIf records are stored in a slotted-page structure within a block, records may move\nwithin a block as other records are deleted or resized. Direct pointers to records are\nnot possible in that case, although records can be accessed with one level of indirection\nthrough the entries in the slotted page header. Locking of the block may be required to\nensure that a record does not get moved while another process is reading its data. To\navoid these overheads, many main-memory databases do not use a slotted-page struc-\nture for allocating records. Instead they directly allocate records in main memory, and\nensure that records never get moved due to updates to other records. However, a prob-\nlem with direct allocation of records is that memory may get fragmented if variable\nsized records are repeatedly inserted and deleted. The database must ensure that main\nmemory does not get fragmented over time, either by using suitably designed memory\nmanagement schemes or by periodically performing compaction of memory; the latter\nscheme will result in record movement, but it can be done without acquiring locks on\nblocks.\nIf a column-oriented storage scheme is used in main memory, all the values of\na column can be stored in consecutive memory locations. However, if there are ap-\npends to the relation, ensuring contiguous allocation would require existing data be\nreallocated. To avoid this overhead, the logical array for a column may be divided into\nmultiple physical arrays. An indirection table stores pointers to all the physical arrays.\nThis scheme is depicted in Figure 13.16. To \ufb01nd the ith element of a logical array, the\nindirection table is used to locate the physical array containing the ith element, and\nthen an appropriate o\ufb00set is computed and looked up within that physical array.\nThere are other ways in which processing can be optimized with main-memory\ndatabases, as we shall see in later chapters.\n", "645": "13.8 Summary 617\nCol1 Data\nCol2 Data\nCol3 Data\nCol4 Data\nCol5 DataIndirection TableCol1 Data\nCol2 Data\nCol3 Data\nCol4 Data\nCol5 DataCol1 Data\nCol2 Data\nCol3 Data\nCol4 Data\nCol5 Data\nFigure 13.16 In-memory columnar data representation.\n13.8 Summary\n\u2022We can organize a \ufb01le logically as a sequence of records mapped onto disk blocks.\nOne approach to mapping the database to \ufb01les is to use several \ufb01les, and to store\nrecords of only one \ufb01xed length in any given \ufb01le. An alternative is to structure\n\ufb01les so that they can accommodate multiple lengths for records. The slotted-page\nmethod is widely used to handle varying-length records within a disk block.\n\u2022Since data are transferred between disk storage and main memory in units of a\nblock, it is worthwhile to assign \ufb01le records to blocks in such a way that a single\nblock contains related records. If we can access several of the records we want with\nonly one block access, we save disk accesses. Since disk accesses are usually the\nbottleneck in the performance of a database system, careful assignment of records\nto blocks can pay signi\ufb01cant performance dividends.\n\u2022The data dictionary, also referred to as the system catalog, keeps track of metadata,\nthat is, data about data, such as relation names, attribute names and types, storage\ninformation, integrity constraints, and user information.\n", "646": "618 Chapter 13 Data Storage Structures\n\u2022One way to reduce the number of disk accesses is to keep as many blocks as pos-\nsible in main memory. Since it is not po ssible to keep all blocks in main memory,\nwe need to manage the allocation of the space available in main memory for the\nstorage of blocks. The bu\ufb00er is that part of main memory available for storage of\ncopies of disk blocks. The subsystem responsible for the allocation of bu\ufb00er space\nis called the bu\ufb00er manager .\n\u2022Column-oriented storage systems provide good performance for many data ware-\nhousing applications.\nReview Terms\n\u2022File Organization\n\u00b0File\n\u00b0Blocks\n\u2022Fixed-length records\n\u2022File header\n\u2022Free list\n\u2022Variable-length records\n\u2022Null bitmap\n\u2022Slotted-page structure\n\u2022Large objects\n\u2022Organization of records\n\u00b0Heap \ufb01le organization\n\u00b0Sequential \ufb01le organization\n\u00b0Multitable clustering \ufb01le organiza-\ntion\n\u00b0B+-tree \ufb01le organizations\n\u00b0Hashing \ufb01le organization\n\u2022Free-space map\n\u2022Sequential \ufb01le\n\u2022Search key\n\u2022Cluster key\n\u2022Table partitioning\n\u2022Data-dictionary storage\n\u00b0Metadata\n\u00b0Data dictionary\u00b0System catalog\n\u2022Database bu\ufb00er\n\u00b0Bu\ufb00er manager\n\u00b0Pinned blocks\n\u00b0Evicted blocks\n\u00b0Forced output of blocks\n\u00b0Shared and exclusive locks\n\u2022Bu\ufb00er-replacement strategies\n\u00b0Least recently used ( LRU)\n\u00b0Toss-immediate\n\u00b0Most recently used ( MRU )\n\u2022Output of blocks\n\u2022Forced output of blocks\n\u2022Log disk\n\u2022Journaling \ufb01le systems\n\u2022Column-oriented storage\n\u00b0Columnar storage\n\u00b0Vector processing\n\u00b0Column stores\n\u00b0Row stores\n\u00b0Stripe\n\u00b0Hybrid row/column storage\n\u2022Main-memory database\n", "647": "Practice Exercises 619\nPractice Exercises\n13.1 Consider the deletion of record 5 from the \ufb01le of Figure 13.3. Compare the\nrelative merits of the following techniques for implementing the deletion:\na. Move record 6 to the space occupied by record 5, and move record 7 to\nthe space occupied by record 6.\nb. Move record 7 to the space occupied by record 5.\nc. Mark record 5 as deleted, and move no records.\n13.2 Show the structure of the \ufb01le of Figure 13.4 after each of the following steps:\na. Insert (24556, Turnamian, Finance, 98000).\nb. Delete record 2.\nc. Insert (34556, Thompson, Music, 67000).\n13.3 Consider the relations section andtakes . Give an example instance of these\ntwo relations, with three sections, each of which has \ufb01ve students. Give a \ufb01le\nstructure of these relations that uses multitable clustering.\n13.4 Consider the bitmap representation of the free-space map, where for each\nblock in the \ufb01le, two bits are maintained in the bitmap. If the block is between\n0 and 30 percent full the bits are 00, between 30 and 60 percent the bits are\n01, between 60 and 90 percent the bits are 10, and above 90 percent the bits\nare 11. Such bitmaps can be kept in memory even for quite large \ufb01les.\na. Outline two bene\ufb01ts and one drawback to using two bits for a block,\ninstead of one byte as described earlier in this chapter.\nb. Describe how to keep the bitmap up to date on record insertions and\ndeletions.\nc. Outline the bene\ufb01t of the bitmap technique over free lists in searching\nfor free space and in updating free space information.\n13.5 It is important to be able to quickly \ufb01nd out if a block is present in the bu\ufb00er,\nand if so where in the bu\ufb00er it resides. Given that database bu\ufb00er sizes are\nvery large, what (in-memory) data structure would you use for this task?\n13.6 Suppose your university has a very large number of takes records, accumulated\nover many years. Explain how table partitioning can be done on the takes rela-\ntion, and what bene\ufb01ts it could o\ufb00er. Explain also one potential drawback of\nthe technique.\n", "648": "620 Chapter 13 Data Storage Structures\n13.7 Give an example of a relational-algebra ex pression and a query-processing strat-\negy in each of the following situations:\na.MRU is preferable to LRU.\nb. LRU is preferable to MRU .\n13.8 PostgreSQL normally uses a small bu\ufb00er, leaving it to the operating system\nbu\ufb00er manager to manage the rest of main memory available for \ufb01le system\nbu\ufb00ering. Explain (a) what is the bene\ufb01t of this approach, and (b) one key\nlimitation of this approach.\nExercises\n13.9 In the variable-length record representation, a null bitmap is used to indicate\nif an attribute has the null value.\na. For variable-length \ufb01elds, if the value is null, what would be stored in the\no\ufb00set and length \ufb01elds?\nb. In some applications, tuples have a very large number of attributes, most\nof which are null. Can you modify the record representation such that\nthe only overhead for a null attribute is the single bit in the null bitmap?\n13.10 Explain why the allocation of records to blocks a\ufb00ects database-system perfor-\nmance signi\ufb01cantly.\n13.11 List two advantages and two disadvantages of each of the following strategies\nfor storing a relational database:\na. Store each relation in one \ufb01le.\nb . S t o r em u l t i p l er e l a t i o n s( p e r h a p se v e nt h ee n t i r ed a t a b a s e )i no n e\ufb01 l e .\n13.12 In the sequential \ufb01le organization, why is an over\ufb02ow block used even if there\nis, at the moment, only one over\ufb02ow record?\n13.13 Give a normalized version of the Index\n metadata relation, and explain why\nusing the normalized version would result in worse performance.\n13.14 Standard bu\ufb00er managers assume each block is of the same size and costs the\nsame to read. Consider a bu\ufb00er manager that, instead of LRU,u s e st h er a t e\nof reference to objects, that is, how often an object has been accessed in the\nlastnseconds. Suppose we want to store in the bu\ufb00er objects of varying sizes,\nand varying read costs (such as web pages, whose read cost depends on the\nsite from which they are fetched). Suggest how a bu\ufb00er manager may choose\nwhich block to evict from the bu\ufb00er.\n", "649": "Further Reading 621\nFurther Reading\n[Hennessy et al. (2017)] is a popular textb ook on computer architecture, which in-\ncludes coverage of hardware aspects of translation look-aside bu\ufb00ers, caches, and\nmemory-management units.\nThe storage structure of speci\ufb01c database systems, such as IBM DB2, Oracle, Mi-\ncrosoft SQL Server, and PostgreSQL are documented in their respective system man-\nuals, which are available online.\nAlgorithms for bu\ufb00er management in database systems, along with a performance\nevaluation, were presented by [Chou and Dewitt (1985)]. Bu\ufb00er management in op-\nerating systems is discussed in most operating-system texts, including in [Silberschatz\net al. (2018)].\n[Abadi et al. (2008)] presents a comparison of column-oriented and row-oriented\nstorage, including issues related to query processing and optimization.\nSybase IQ, developed in the mid 1990s, was the \ufb01rst commercially successful\ncolumn-oriented database, designed for analytics. MonetDB and C-Store were column-\noriented databases developed as academic research projects. The Vertica column-\noriented database is a commercial database that grew out of C-Store, while VectorWise\nis a commercial database that grew out of MonetDB. As its name suggests, VectorWise\nsupports vector processing of data, and as a result supports very high processing rates\nfor many analytical queries. [Stonebraker e t al. (2005)] describe C-Store, while [Idreos\net al. (2012)] give an overview of the MonetDB project and [Zukowski et al. (2012)]\ndescribes Vectorwise.\nTheORC and Parquet columnar \ufb01le formats were developed to support compressed\nstorage of data for big-data applications that run on the Apache Hadoop platform.\nBibliography\n[Abadi et al. (2008)] D. J. Abadi, S. Madden, and N. Hachem, \u201cColumn-Stores vs. Row-S-\ntores: How Di\ufb00erent Are They Really?\u201d, In Proc. of the ACM SIGMOD Conf. on Management\nof Data (2008), pages 967\u2013980.\n[Chou and Dewitt (1985)] H. T. Chou and D. J. Dewitt, \u201cAn Evaluation of Bu\ufb00er Manage-\nment Strategies for Relational Database Systems\u201d, In Proc. of the International Conf. on Very\nLarge Databases (1985), pages 127\u2013141.\n[Hennessy et al. (2017)] J. L. Hennessy, D. A. Patterson, and D. Goldberg, Computer Archi-\ntecture: A Quantitative Approach , 6th edition, Morgan Kaufmann (2017).\n[Idreos et al. (2012)] S. Idreos, F. Gro\ufb00en, N. Nes, S. Manegold, K. S. Mullender, and M. L.\nKersten, \u201cMonetDB: Two Decades of Research in Column-oriented Database Architec-\ntures\u201d, IEEE Data Engineering Bulletin , Volume 35, Number 1 (2012), pages 40\u201345.\n[Silberschatz et al. (2018)] A. Silberschatz, P. B. Galvin, and G. Gagne, Operating System\nConcepts , 10th edition, John Wiley and Sons (2018).\n", "650": "622 Chapter 13 Data Storage Structures\n[Stonebraker et al. (2005)] M. Stonebraker, D. J. Abadi, A. Batkin, X. Chen, M. Cherniack,\nM. Ferreira, E. Lau, A. Lin, S. Madden, E. J. O\u2019Neil, P. E. O\u2019Neil, A. Rasin, N. Tran, and\nS. B. Zdonik, \u201cC-Store: A Column-oriented DBMS\u201d, In Proc. of the International Conf. on\nVery Large Databases (2005), pages 553\u2013564.\n[Zukowski et al. (2012)] M. Zukowski, M. van de Wiel, and P. A. Boncz, \u201cVectorwise: A\nVectorized Analytical DBMS\u201d, In Proc. of the International Conf. on Data Engineering (2012),\npages 1349\u20131350.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "651": "CHAPTER14\nIndexing\nMany queries reference only a small proportion of the records in a \ufb01le. For example, a\nquery like \u201cFind all instructors in the Physics department\u201d or \u201cFind the total number of\ncredits earned by the student with ID22201\u201d references only a fraction of the instructor\norstudent records. It is ine\ufb03cient for the system to read every tuple in the instructor\nrelation to check if the dept\n name value is \u201cPhysics\u201d. Likewise, it is ine\ufb03cient to read\nthe entire student relation just to \ufb01nd the one tuple for the ID\u201c22201\u201d. Ideally, the\nsystem should be able to locate these records directly. To allow these forms of access,\nwe design additional structures that we associate with \ufb01les.\n14.1 Basic Concepts\nAn index for a \ufb01le in a database system works in much the same way as the index in this\ntextbook. If we want to learn about a partic ular topic (speci\ufb01ed by a word or a phrase)\nin this textbook, we can search for the topic in the index at the back of the book, \ufb01nd\nthe pages where it occurs, and then read the pages to \ufb01nd the information for which\nwe are looking. The words in the index are in sorted order, making it easy to \ufb01nd the\nword we want. Moreover, the index is much smaller than the book, further reducing\nthe e\ufb00ort needed.\nDatabase-system indices play the same role as book indices in libraries. For exam-\nple, to retrieve a student record given an ID, the database system would look up an index\nto \ufb01nd on which disk block1the corresponding record resides, and then fetch the disk\nblock, to get the appropriate student record.\nIndices are critical for e\ufb03cient processing of queries in databases. Without indices,\nevery query would end up reading the entire contents of every relation that it uses;\ndoing so would be unreasonably expensive for queries that only fetch a few records, for\nexample, a single student record, or the records in the takes relation corresponding to\nas i n g l es t u d e n t .\n1As in earlier chapters, we use the term diskto refer to persistent storage devices, such as magnetic disks and solid-state\ndrives.\n623\n", "652": "624 Chapter 14 Indexing\nImplementing an index on the student relation by keeping a sorted list of students\u2019\nIDwould not work well on very large databases, since (i) the index would itself be\nvery big, (ii) even though keeping the index sorted reduces the search time, \ufb01nding a\nstudent can still be rather time-consuming, and (iii) updating a sorted list as students are\nadded or removed from the database can be very expensive. Instead, more sophisticated\nindexing techniques are used in database systems. We shall discuss several of these\ntechniques in this chapter.\nThere are two basic kinds of indices:\n\u2022Ordered indices . Based on a sorted ordering of the values.\n\u2022Hash indices . Based on a uniform distribution of values across a range of buckets.\nThe bucket to which a value is assigned is determined by a function, called a hash\nfunction .\nWe shall consider several techniques for ordered indexing. No one technique is the\nbest. Rather, each technique is best suited to particular database applications. Each\ntechnique must be evaluated on the basis of these factors:\n\u2022Access types : The types of access that are supported e\ufb03ciently. Access types can\ninclude \ufb01nding records with a speci\ufb01ed attribute value and \ufb01nding records whose\nattribute values fall in a speci\ufb01ed range.\n\u2022Access time : The time it takes to \ufb01nd a particular data item, or set of items, using\nthe technique in question.\n\u2022Insertion time : The time it takes to insert a new data item. This value includes the\ntime it takes to \ufb01nd the correct place to insert the new data item, as well as the\ntime it takes to update the index structure.\n\u2022Deletion time : The time it takes to delete a data item. This value includes the time\nit takes to \ufb01nd the item to be deleted, as well as the time it takes to update the\nindex structure.\n\u2022Space overhead : The additional space occupied by an index structure. Provided that\nthe amount of additional space is moderate, it is usually worthwhile to sacri\ufb01ce\nt h es p a c et oa c h i e v ei m p r o v e dp e r f o r m a n c e .\nWe often want to have more than one index for a \ufb01le. For example, we may wish\nto search for a book by author, by subject, or by title.\nAn attribute or set of attributes used to look up records in a \ufb01le is called a search\nkey. Note that this de\ufb01nition of keydi\ufb00ers from that used in primary key ,candidate\nkey,a n d superkey . This duplicate meaning for keyis (unfortunately) well established in\npractice. Using our notion of a search key, we see that if there are several indices on a\n\ufb01le, there are several search keys.\n", "653": "14.2 Ordered Indices 625\n10101 Srinivasan\n45565 Katz\n58583 Cali\ufb01eri\n76543 Singh\n76766 Crick\n83821 Brandt\n98345 Kim12121 Wu\n15151 Mozart\n22222 Einstein\n32343 El Said\n33456 GoldComp. Sci.\nComp. Sci.Comp. Sci.\nHistory\nFinance\nBiology\nElec. Eng.Finance\nMusic\nPhysics\nHistory\nPhysics65000\n75000\n62000\n80000\n72000\n92000\n8000090000\n40000\n95000\n60000\n87000\nFigure 14.1 Sequential file for instructor records.\n14.2 Ordered Indices\nTo gain fast random access to records in a \ufb01le, we can use an index structure. Each\nindex structure is associated with a particular search key. Just like the index of a book\nor a library catalog, an ordered index stores the values of the search keys in sorted order\nand associates with each search key the records that contain it.\nThe records in the indexed \ufb01le may themselves be stored in some sorted order, just\nas books in a library are stored according to some attribute such as the Dewey decimal\nnumber. A \ufb01le may have several indices, on di\ufb00erent search keys. If the \ufb01le containing\nthe records is sequentially ordered, a clustering index is an index whose search key\nalso de\ufb01nes the sequential order of the \ufb01le. Clustering indices are also called primary\nindices ;t h et e r m primary index may appear to denote an index on a primary key, but\nsuch indices can in fact be built on any search key. The search key of a clustering index\nis often the primary key, although that is not necessarily so. Indices whose search key\nspeci\ufb01es an order di\ufb00erent from the sequential order of the \ufb01le are called nonclustering\nindices ,o rsecondary indices . The terms \u201cclustered\u201d and \u201cnonclustered\u201d are often used\nin place of \u201cclustering\u201d and \u201cnonclustering.\u201d\nIn Section 14.2.1 through Section 14.2.3, we assume that all \ufb01les are ordered se-\nquentially on some search key. Such \ufb01les, with a clustering index on the search key,\nare called index-sequential \ufb01les . They represent one of the oldest index schemes used\nin database systems. They are designed for applications that require both sequential\nprocessing of the entire \ufb01le and random access to individual records. In Section 14.2.4\nwe cover secondary indices.\nFigure 14.1 shows a sequential \ufb01le of instructor records taken from our university\nexample. In the example of Figure 14.1, the records are stored in sorted order of in-\nstructor ID, which is used as the search key.\n", "654": "626 Chapter 14 Indexing\n14.2.1 Dense and Sparse Indices\nAnindex entry ,o rindex record , consists of a search-key value and pointers to one or\nmore records with that value as their search-key value. The pointer to a record consists\nof the identi\ufb01er of a disk block and an o\ufb00set within the disk block to identify the record\nwithin the block.\nThere are two types of ordered indices that we can use:\n\u2022Dense index : In a dense index, an index entry appears for every search-key value\nin the \ufb01le. In a dense clustering index, the index record contains the search-key\nvalue and a pointer to the \ufb01rst data record with that search-key value. The rest of\nt h er e c o r d sw i t ht h es a m es e a r c h - k e yv a l u ew o u l db es t o r e ds e q u e n t i a l l ya f t e rt h e\n\ufb01rst record, since, because the index is a clustering one, records are sorted on the\nsame search key.\nIn a dense nonclustering index, the index must store a list of pointers to all\nrecords with the same search-key value.\n\u2022Sparse index : In a sparse index, an index entry appears for only some of the search-\nkey values. Sparse indices can be used only if the relation is stored in sorted order\nof the search key; that is, if the index is a clustering index. As is true in dense\nindices, each index entry contains a search-key value and a pointer to the \ufb01rst data\nrecord with that search-key value. To locate a record, we \ufb01nd the index entry with\nthe largest search-key value that is less than or equal to the search-key value for\nwhich we are looking. We start at the record pointed to by that index entry and\nfollow the pointers in the \ufb01le until we \ufb01nd the desired record.\nFigure 14.2 and Figure 14.3 show dense and sparse indices, respectively, for the\ninstructor \ufb01le. Suppose that we are looking up the record of instructor with ID\u201c22222\u201d.\nUsing the dense index of Figure 14.2, we follow the pointer directly to the desired\nrecord. Since IDis a primary key, there exists only one such record and the search is\ncomplete. If we are using the sparse index (Figure 14.3), we do not \ufb01nd an index entry\nfor \u201c22222\u201d. Since the last entry (in numerical order) before \u201c22222\u201d is \u201c10101\u201d, we\nfollow that pointer. We then read the instructor \ufb01le in sequential order until we \ufb01nd the\ndesired record.\nConsider a (printed) dictionary. The header of each page lists the \ufb01rst word alpha-\nbetically on that page. The words at the top of each page of the book index together\nform a sparse index on the contents of the dictionary pages.\nAs another example, suppose that the search-key value is not a primary key. Figure\n14.4 shows a dense clustering index for the instructor \ufb01le with the search key being\ndept\n name . Observe that in this case the instructor \ufb01le is sorted on the search key dept\nname ,i n s t e a do f ID, otherwise the index on dept\n name would be a nonclustering index.\nSuppose that we are looking up records for the History department. Using the dense\nindex of Figure 14.4, we follow the pointer directly to the \ufb01rst History record. We\nprocess this record and follow the pointer in that record to locate the next record in\n", "655": "14.2 Ordered Indices 627\n10101\n12121\n15151\n22222\n32343\n33456\n45565\n58583\n76543\n76766\n83821\n9834510101 Srinivasan\n45565 Katz\n58583 Cali\ufb01eri\n76543 Singh\n76766 Crick\n83821 Brandt\n98345 Kim12121 Wu\n15151 Mozart\n22222 Einstein\n32343 El Said\n33456 GoldComp. Sci.\nComp. Sci.Comp. Sci.\nHistory\nFinance\nBiology\nElec. Eng.Finance\nMusic\nPhysics\nHistory\nPhysics65000\n75000\n62000\n80000\n72000\n92000\n8000090000\n40000\n95000\n60000\n87000\nFigure 14.2 Dense index.\nsearch-key ( dept\n name ) order. We continue processing records until we encounter a\nrecord for a department other than History.\nAs we have seen, it is generally faster to locate a record if we have a dense index\nrather than a sparse index. However, sparse indices have advantages over dense indices\nin that they require less space and they impose less maintenance overhead for insertions\nand deletions.\nThere is a trade-o\ufb00 that the system designer must make between access time and\nspace overhead. Although the decision regarding this trade-o\ufb00 depends on the speci\ufb01c\napplication, a good compromise is to have a sparse index with one index entry per\n10101\n32343\n7676610101 Srinivasan\n45565 Katz\n58583 Cali\ufb01eri\n76543 Singh\n76766 Crick\n83821 Brandt\n98345 Kim12121 Wu\n15151 Mozart\n22222 Einstein\n32343 El Said\n33456 GoldComp. Sci.\nComp. Sci.Comp. Sci.\nHistory\nFinance\nBiology\nElec. Eng.Finance\nMusic\nPhysics\nHistory\nPhysics65000\n75000\n62000\n80000\n72000\n92000\n8000090000\n40000\n95000\n60000\n87000\nFigure 14.3 Sparse index.\n", "656": "628 Chapter 14 Indexing\nBiology\nComp. Sci.\nElec. Eng.\nFinance\nHistory\nMusic\nPhysics76766 Crick\n76543 Singh\n32343 El Said\n58583 Cali\ufb01eri\n15151 Mozart\n22222 Einstein\n33465 Gold10101 Srinivasan\n45565 Katz\n83821 Brandt\n98345 Kim\n12121 WuBiology\nPhysicsFinance\nHistory\nHistory\nMusic\nPhysicsComp. Sci.\nComp. Sci.\nComp. Sci.\nElec. Eng.\nFinance72000\n80000\n60000\n62000\n40000\n95000\n8700065000\n75000\n92000\n80000\n90000\nFigure 14.4 Dense index with search key dept\n name .\nblock. The reason this design is a good trade-o\ufb00 is that the dominant cost in processing a\ndatabase request is the time that it takes to bring a block from disk into main memory.\nOnce we have brought in the block, the time to scan the entire block is negligible.\nUsing this sparse index, we locate the block containing the record that we are seeking.\nThus, unless the record is on an over\ufb02ow block (see Section 13.3.2), we minimize block\naccesses while keeping the size of the index (and thus our space overhead) as small as\npossible.\nFor the preceding technique to be fully general, we must consider the case where\nrecords for one search-key value occupy several blocks. It is easy to modify our scheme\nto handle this situation.\n14.2.2 Multilevel Indices\nSuppose we build a dense index on a relation with 1,000,000 tuples. Index entries are\nsmaller than data records, so let us assume that 100 index entries \ufb01t on a 4-kilobyte\nblock. Thus, our index occupies 10,000 blocks. If the relation instead had 100,000,000\ntuples, the index would instead occupy 1,000,000 blocks, or 4 gigabytes of space. Such\nlarge indices are stored as sequential \ufb01les on disk.\nIf an index is small enough to be kept entirely in main memory, the search time\nto \ufb01nd an entry is low. However, if the index is so large that not all of it can be kept\nin memory, index blocks must be fetched from disk when required. (Even if an index\nis smaller than the main memory of a computer, main memory is also required for a\nnumber of other tasks, so it may not be possible to keep the entire index in memory.)\nThe search for an entry in the index then requires several disk-block reads.\nBinary search can be used on the index \ufb01le to locate an entry, but the search still\nh a sal a r g ec o s t .I ft h ei n d e xw o u l do c c u p y bblocks, binary search requires as many as\n\u2308log2(b)\u2309blocks to be read. ( \u2308x\u2309denotes the least integer that is greater than or equal\ntox; that is, we round upward.) Note that the blocks that are read are not adjacent\n", "657": "14.2 Ordered Indices 629\nto each other, so each read requires a random (i.e., non-sequential) I/Ooperation. For\na 10,000-block index, binary search requires 14 random block reads. On a magnetic\ndisk system where a random block read takes on average 10 milliseconds, the index\nsearch will take 140 milliseconds. This may not seem much, but we would be able to\ncarry out only seven index searches a second on a single disk, whereas a more e\ufb03cient\nsearch mechanism would let us carry out far more searches per second, as we shall see\nshortly. Note that, if over\ufb02ow blocks have been used, binary search is only possible on\nthe non-over\ufb02ow blocks, and the actual cost may be even higher than the logarithmic\nbound above. A sequential search requires bsequential block reads, which may take\neven longer (although in some cases the lower cost of sequential block reads may result\nin sequential search being faster than a binary search). Thus, the process of searching\nal a r g ei n d e xm a yb ec o s t l y .\nTo deal with this problem, we treat the index just as we would treat any other\nsequential \ufb01le, and we construct a sparse outer index on the original index, which we\nnow call the inner index, as shown in Figure 14.5. Note that the index entries are always\nin sorted order, allowing the outer index to be sparse. To locate a record, we \ufb01rst use\nbinary search on the outer index to \ufb01nd the record for the largest search-key value less\u2026\u2026 \u2026\u2026outer indexindex\nblock 0\nindex\nblock 1data\nblock 0\ndata\nblock 1\ninner index\nFigure 14.5 Two-level sparse index.\n", "658": "630 Chapter 14 Indexing\nthan or equal to the one that we desire. The pointer points to a block of the inner index.\nWe scan this block until we \ufb01nd the record that has the largest search-key value less\nthan or equal to the one that we desire. The pointer in this record points to the block\nof the \ufb01le that contains the record for which we are looking.\nIn our example, an inner index with 10,000 blocks would require 10,000 entries in\nthe outer index, which would occupy just 100 blocks. If we assume that the outer index\nis already in main memory, we would read only one index block for a search using a\nmultilevel index, rather than the 14 blocks we read with binary search. As a result, we\ncan perform 14 times as many index searches per second.\nIf our \ufb01le is extremely large, even the outer index may grow too large to \ufb01t in main\nmemory. With a 100,000,000-tuple relation, the inner index would occupy 1,000,000\nblocks, and the outer index would occupy 10,000 blocks, or 40 megabytes. Since there\nare many demands on main memory, it may not be possible to reserve that much main\nmemory just for this particular outer index. In such a case, we can create yet another\nlevel of index. Indeed, we can repeat this process as many times as necessary. Indices\nwith two or more levels are called multilevel indices . Searching for records with a mul-\ntilevel index requires signi\ufb01cantly fewer I/Ooperations than does searching for records\nby binary search.2\nMultilevel indices are closely related to tree structures, such as the binary trees\nused for in-memory indexing. We shall examine the relationship later, in Section 14.3.\n14.2.3 Index Update\nRegardless of what form of index is used, every index must be updated whenever a\nrecord is either inserted into or deleted from the \ufb01le. Further, in case a record in the\n\ufb01le is updated, any index whose search-key attribute is a\ufb00ected by the update must also\nbe updated; for example, if the department of an instructor is changed, an index on\nthedept\n name attribute of instructor must be updated correspondingly. Such a record\nupdate can be modeled as a deletion of the old record, followed by an insertion of\nthe new value of the record, which results in an index deletion followed by an index\ninsertion. As a result we only need to consider insertion and deletion on an index, and\nwe do not need to consider updates explicitly.\nWe \ufb01rst describe algorithms for updating single-level indices.\n14.2.3.1 Insertion\nFirst, the system performs a lookup using the search-key value that appears in the record\nto be inserted. The actions the system takes next depend on whether the index is dense\nor sparse:\n2In the early days of disk-based indices, each level of the index corresponded to a unit of physical storage. Thus, we may\nhave indices at the track, cylinder, and disk levels. Such a hierarchy does not make sense today since disk subsystems\nhide the physical details of disk storage, and the number of disks and platters per disk is very small compared to the\nnumber of cylinders or bytes per track.\n", "659": "14.2 Ordered Indices 631\n\u2022Dense indices:\n1.If the search-key value does not appear in the index, the system inserts an\nindex entry with the search-key value in the index at the appropriate position.\n2.Otherwise the following actions are taken:\na. If the index entry stores pointers to all records with the same search-key\nvalue, the system adds a pointer to the new record in the index entry.\nb. Otherwise, the index entry stores a pointer to only the \ufb01rst record with\nthe search-key value. The system then places the record being inserted\nafter the other records with the same search-key values.\n\u2022Sparse indices: We assume that the index stores an entry for each block. If the\nsystem creates a new block, it inserts the \ufb01rst search-key value (in search-key order)\nappearing in the new block into the index. On the other hand, if the new record has\nthe least search-key value in its block, the system updates the index entry pointing\nto the block; if not, the system makes no change to the index.\n14.2.3.2 Deletion\nTo delete a record, the system \ufb01rst looks up the record to be deleted. The actions the\nsystem takes next depend on whether the index is dense or sparse:\n\u2022Dense indices:\n1.If the deleted record was the only record with its particular search-key value,\nthen the system deletes the corresponding index entry from the index.\n2.Otherwise the following actions are taken:\na. If the index entry stores pointers to all records with the same search-key\nvalue, the system deletes the pointer to the deleted record from the index\nentry.\nb. Otherwise, the index entry stores a pointer to only the \ufb01rst record with\nthe search-key value. In this case, if the deleted record was the \ufb01rst record\nwith the search-key value, the system updates the index entry to point to\nthe next record.\n\u2022Sparse indices:\n1.If the index does not contain an index entry with the search-key value of the\ndeleted record, nothing needs to be done to the index.\n2.Otherwise the system takes the following actions:\na. If the deleted record was the only record with its search key, the system\nreplaces the corresponding index record with an index record for the\nnext search-key value (in search-key order). If the next search-key value\nalready has an index entry, the entry is deleted instead of being replaced.\n", "660": "632 Chapter 14 Indexing\nb. Otherwise, if the index entry for the search-key value points to the record\nbeing deleted, the system updates the index entry to point to the next\nrecord with the same search-key value.\nInsertion and deletion algorithms for multilevel indices are a simple extension of\nthe scheme just described. On deletion or insertion, the system updates the lowest-\nlevel index as described. As far as the second level is concerned, the lowest-level index\nis merely a \ufb01le containing records\u2014thus, if there is any change in the lowest-level index,\nt h es y s t e mu p d a t e st h es e c o n d - l e v e li n d e xa sd e s c r i b e d .T h es a m et e c h n i q u ea p p l i e st o\nfurther levels of the index, if there are any.\n14.2.4 Secondary Indices\nSecondary indices must be dense, with an index entry for every search-key value, and a\npointer to every record in the \ufb01le. A clustering index may be sparse, storing only some\nof the search-key values, since it is always possible to \ufb01nd records with intermediate\nsearch-key values by a sequential access to a part of the \ufb01le, as described earlier. If a\nsecondary index stores only some of the search-key values, records with intermediate\nsearch-key values may be anywhere in the \ufb01le and, in general, we cannot \ufb01nd them\nwithout searching the entire \ufb01le.\nA secondary index on a candidate key looks just like a dense clustering index,\nexcept that the records pointed to by successive values in the index are not stored\nsequentially. In general, however, secondary indices may have a di\ufb00erent structure from\nclustering indices. If the search key of a clustering index is not a candidate key, it su\ufb03ces\nif the index points to the \ufb01rst record with a particular value for the search key, since\nthe other records can be fetched by a sequential scan of the \ufb01le.\nIn contrast, if the search key of a secondary index is not a candidate key, it is\nnot enough to point to just the \ufb01rst record with each search-key value. The remaining\nrecords with the same search-key value could be anywhere in the \ufb01le, since the records\nare ordered by the search key of the clustering index, rather than by the search key\nof the secondary index. Therefore, a secondary index must contain pointers to all the\nrecords.\nIf a relation can have more than one record containing the same search key value\n(that is, two or more records can have the same values for the indexed attributes), the\nsearch key is said to be a nonunique search key .\nOne way to implement secondary indices on nonunique search keys is as follows:\nUnlike the case of primary indices, the pointers in such a secondary index do not point\ndirectly to the records. Instead, each pointer in the index points to a bucket that in\nturn contains pointers to the \ufb01le. Figure 14.6 shows the structure of a secondary index\nthat uses such an extra level of indirection on the instructor \ufb01le, on the search key dept\nname .\nHowever, this approach has a few drawbacks. First, index access takes longer, due\nto an extra level of indirection, which may require a random I/Ooperation. Second,\n", "661": "14.2 Ordered Indices 633\n10101 Srinivasan Comp. Sci. 65000\n 12121 Wu Finance 90000\n15151 Mozart Music 40000\n22222 Einstein Physics 95000\n32343 El Said History 60000\n33456 Gold Physics 87000\n45565 Katz Comp. Sci. 75000\n58583 Cali\ufb01eri History 62000\n76543 Singh Finance 80000\n76766 Crick Biology 72000\n83821 Brandt Comp. Sci. 92000\n98345 Kim Elec. Eng. 80000Biology\nComp. Sci.\nElec. Eng.\nFinance\nHistory\nMusic\nPhysics\nFigure 14.6 Secondary index on instructor file, on noncandidate key dept\n name .\nif a key has very few or no duplicates, if a whole block is allocated to its associated\nbucket, a lot of space would be wasted. Later in this chapter, we study more e\ufb03cient\nalternatives for implementing secondary indices, which avoid these drawbacks.\nA sequential scan in clustering index order is e\ufb03cient because records in the \ufb01le\nare stored physically in the same order as the index order. However, we cannot (except\nin rare special cases) store a \ufb01le physically ordered by both the search key of the clus-\ntering index and the search key of a secondary index. Because secondary-key order and\nphysical-key order di\ufb00er, if we attempt to scan the \ufb01le sequentially in secondary-key\norder, the reading of each record is likely to require the reading of a new block from\ndisk, which is very slow.\nThe procedure described earlier for deletion and insertion can also be applied to\nsecondary indices; the actions taken are those described for dense indices storing a\npointer to every record in the \ufb01le. If a \ufb01le has multiple indices, whenever the \ufb01le is\nmodi\ufb01ed, every index must be updated.\nSecondary indices improve the performance of queries that use keys other than\nthe search key of the clustering index. However, they impose a signi\ufb01cant overhead\non modi\ufb01cation of the database. The designer of a database decides which secondary\nindices are desirable on the basis of an estimate of the relative frequency of queries and\nmodi\ufb01cations.\n14.2.5 Indices on Multiple Keys\nAlthough the examples we have seen so far have had a single attribute in a search key,\nin general a search key can have more than one attribute. A search key containing more\nt h a no n ea t t r i b u t ei sr e f e r r e dt oa sa composite search key . The structure of the index is\nthe same as that of any other index, the only di\ufb00erence being that the search key is not\na single attribute, but rather is a list of attri butes. The search key can be represented as\na tuple of values, of the form ( a1,\u2026,an), where the indexed attributes are A1,\u2026,An.\n", "662": "634 Chapter 14 Indexing\nThe ordering of search-key values is the lexicographic ordering .F o re x a m p l e ,f o rt h e\ncase of two attribute search keys, ( a1,a2)<(b1,b2)i fe i t h e r a1<b1ora1=b1and\na2<b2. Lexicographic ordering is basically the same as alphabetic ordering of words.\nAs an example, consider an index on the takes relation, on the composite search\nkey ( course\n id,semester ,year). Such an index would be useful to \ufb01nd all students who\nhave registered for a particular course in a particular semester/year. An ordered index\non a composite key can also be used to answer several other kinds of queries e\ufb03ciently,\nas we shall see in Section 14.6.2.\n14.3 B+-Tree Index Files\nThe main disadvantage of the index-sequential \ufb01le organization is that performance\ndegrades as the \ufb01le grows, both for index lookups and for sequential scans through the\ndata. Although this degradation can be remedied by reorganization of the \ufb01le, frequent\nreorganizations are undesirable.\nTheB+-tree index structure is the most widely used of several index structures that\nmaintain their e\ufb03ciency despite insertion and deletion of data. A B+-tree index takes\nthe form of a balanced tree in which every path from the root of the tree to a leaf of\nthe tree is of the same length. Each nonleaf node in the tree (other than the root)\nhas between \u2308n\u22152\u2309and nchildren, where nis \ufb01xed for a particular tree; the root has\nbetween 2 and nchildren.\nWe shall see that the B+-tree structure imposes performance overhead on insertion\nand deletion and adds space overhead. The overhead is acceptable even for frequently\nmodi\ufb01ed \ufb01les, since the cost of \ufb01le reorganization is avoided. Furthermore, since nodes\nmay be as much as half empty (if they have the minimum number of children), there\nis some wasted space. This space overhead, too, is acceptable given the performance\nbene\ufb01ts of the B+-tree structure.\n14.3.1 Structure of a B+-Tree\nAB+-tree index is a multilevel index, but it has a structure that di\ufb00ers from that of the\nmultilevel index-sequential \ufb01le. We assume for now that there are no duplicate search\nkey values, that is, each search key is unique and occurs in at most one record; we\nconsider the issue of nonunique search keys later.\nFigure 14.7 shows a typical node of a B+-tree. It contains up to n\u22121s e a r c h - k e y\nvalues K1,K2,\u2026,Kn\u22121,a n d npointers P1,P2,\u2026,Pn. The search-key values within a\nnode are kept in sorted order; thus, if i<j,t h e n Ki<Kj.\nP1 K1 P2 Pn 1 Kn 1 Pn \u2026\nFigure 14.7 Typical node of a B+-tree.\n", "663": "14.3 B+-Tree Index Files 635\nleaf node\nPointer to next leaf node\ninstructor \ufb01leBrandt\nSrinivasanCali\ufb01eri Crick\nComp. Sci. 65000\nWu Finance 90000\nMozart Music 40000\nEinstein Physics 95000\nEl Said History 80000\nGold Physics 87000\nKatz Comp. Sci. 75000\nCali\ufb01eri History 60000\nSingh Finance 80000\nCrick Biology 72000\nBrandt Comp. Sci. 920001515110101\n12121\n22222\n32343\n33456\n45565\n58583\n76543\n76766\n83821\n98345 Kim Elec. Eng. 80000\nFigure 14.8 Al e a fn o d ef o r instructor B+-tree index ( n=4).\nWe consider \ufb01rst the structure of the leaf nodes .F o r i=1, 2,\u2026,n\u22121, pointer Pi\npoints to a \ufb01le record with search-key value Ki.P o i n t e r Pnhas a special purpose that\nwe shall discuss shortly.\nFigure 14.8 shows one leaf node of a B+-tree for the instructor \ufb01le, in which we have\nchosen nto be 4, and the search key is name .\nNow that we have seen the structure of a leaf node, let us consider how search-key\nvalues are assigned to particular nodes. Each leaf can hold up to n\u22121v a l u e s .W e\nallow leaf nodes to contain as few as \u2308(n\u22121)\u22152\u2309values. With n=4i no u re x a m p l e\nB+-tree, each leaf must contain at least two values, and at most three values.\nIfLiand Ljare leaf nodes and i<j(that is, Liis to the left of Ljin the tree), then\nevery search-key value viinLiis less than every search-key value vjinLj.\nIf the B+-tree index is used as a dense index (as is usually the case), every search-key\nvalue must appear in some leaf node.\nNow we can explain the use of the pointer Pn. Since there is a linear order on the\nleaves based on the search-key values that they contain, we use Pnto chain together the\nleaf nodes in search-key order. This ordering allows for e\ufb03cient sequential processing\nof the \ufb01le.\nThenonleaf nodes of the B+-tree form a multilevel (sparse) index on the leaf nodes.\nThe structure of nonleaf nodes is the same as that for leaf nodes, except that all pointers\nare pointers to tree nodes. A nonleaf node may hold up to npointers and must hold\nat least \u2308n\u22152\u2309pointers. The number of pointers in a node is called the fanout of the\nnode. Nonleaf nodes are also referred to as internal nodes .\n", "664": "636 Chapter 14 Indexing\nGold Katz Kim Mozart Singh Srinivasan WuInternal nodesRoot node\nLeaf nodesEinstein\nEinstein El SaidGoldMozart\nSrinivasan\nSrinivasan Comp. Sci. 65000\nWu Finance 90000\nMozart Music 40000\nEinstein Physics 95000\nEl Said History 80000\nGold Physics 87000\nKatz Comp. Sci. 75000\nCali\ufb01eri History 60000\nSingh Finance 80000\nCrick Biology 72000\nBrandt Comp. Sci. 920001515110101Brandt Cali\ufb01eri Crick\n12121\n22222\n32343\n33456\n45565\n58583\n76543\n76766\n83821\n98345 Kim Elec. Eng. 80000\nFigure 14.9 B+-tree for instructor file (n=4).\nLet us consider a node containing mpointers ( m\u2264n). For i=2, 3,\u2026,m\u22121,\npointer Pipoints to the subtree that contains search-key values less than Kiand greater\nthan or equal to Ki\u22121.P o i n t e r Pmpoints to the part of the subtree that contains those\nkey values greater than or equal to Km\u22121,a n dp o i n t e r P1points to the part of the subtree\nthat contains those search-key values less than K1.\nUnlike other nonleaf nodes, the root node can hold fewer than \u2308n\u22152\u2309pointers;\nhowever, it must hold at least two pointers, unless the tree consists of only one node. It\nis always possible to construct a B+-tree, for any n, that satis\ufb01es the preceding require-\nments.\nFigure 14.9 shows a complete B+-tree for the instructor \ufb01le (with n=4). We have\nomitted null pointers for simplicity; any pointer \ufb01eld in the \ufb01gure that does not have\nan arrow is understood to have a null value.\nFigure 14.10 shows another B+-tree for the instructor \ufb01le, this time with n=6.\nObserve that the height of this tree is less than that of the previous tree, which had\nn=4.\nBrandt Crick Cali\ufb01eri Einstein El Said Gold Katz Kim Mozart Singh Srinivasan WuEl Said Mozart\nFigure 14.10 B+-tree for instructor file with n=6.\n", "665": "14.3 B+-Tree Index Files 637\nThese examples of B+-trees are all balanced. That is, the length of every path from\nthe root to a leaf node is the same. This property is a requirement for a B+-tree. Indeed,\nthe \u201cB\u201d in B+-tree stands for \u201cbalanced.\u201d It is the balance property of B+-trees that\nensures good performance for lookup, insertion, and deletion.\nIn general, search keys could have duplicates. One way to handle the case of\nnonunique search keys is to modify the tree structure to store each search key at a leaf\nnode as many times as it appears in records, with each copy pointing to one record.\nThe condition that Ki<Kjifi<jwill need to be modi\ufb01ed to Ki\u2264Kj.H o w e v e r ,t h i s\napproach can result in duplicate search key values at internal nodes, making the inser-\ntion and deletion procedures more complicated and expensive. Another alternative is\nto store a set (or bucket) of record pointers with each search key value, as we saw ear-\nlier. This approach is more complicated and can result in ine\ufb03cient access, especially\nif the number of record pointers for a particular key is very large.\nMost database implementations instead make search keys unique as follows: Sup-\npose the desired search key attribute aiof relation ris nonunique. Let Apbe the primary\nkey of r. Then the unique composite search key ( ai,Ap)i su s e di n s t e a do f aiwhen build-\ning the index. (Any set of attributes that together with aiguarantee uniqueness can also\nbe used instead of Ap.) For example, if we wished to create an index on the instructor\nrelation on the attribute name , we instead create an index on the composite search key\n(name ,ID), since IDis the primary key for instructor . Index lookups on just name can\nbe e\ufb03ciently handled using this index, as we shall see shortly. Section 14.3.5 covers\nissues in handling of nonunique search keys in more detail.\nIn our examples, we show indices on some nonunique search keys, such as instruc-\ntor.name , assuming for simplicity that there are no duplicates; in reality most databases\nwould automatically add extra attributes inte rnally, to ensure the absence of duplicates.\n14.3.2 Queries on B+-Trees\nLet us consider how we process queries on a B+-tree. Suppose that we wish to \ufb01nd a\nrecord with a given value vfor the search key. Figure 14.11 presents pseudocode for a\nfunction \ufb01nd(v) to carry out this task, assuming there are no duplicates, that is, there\nis at most one record with a particular search key. We address the issue of nonunique\nsearch keys later in this section.\nIntuitively, the function starts at the root of the tree and traverses the tree down\nuntil it reaches a leaf node that would contain the speci\ufb01ed value if it exists in the\ntree. Speci\ufb01cally, starting with the root as the current node, the function repeats the\nfollowing steps until a leaf node is reached. First, the current node is examined, looking\nfor the smallest isuch that search-key value Kiis greater than or equal to v. Suppose\nsuch a value is found; then, if Kiis equal to v, the current node is set to the node pointed\nto by Pi+1,o t h e r w i s e Ki>v, and the current node is set to the node pointed to by Pi.I f\nno such value Kiis found, then v>Km\u22121,w h e r e Pmis the last nonnull pointer in the\nnode. In this case the current node is set to that pointed to by Pm.T h ea b o v ep r o c e d u r e\nis repeated, traversing down the tree until a leaf node is reached.\n", "666": "638 Chapter 14 Indexing\nfunction \ufb01nd(v)\n/* Assumes no duplicate keys, and returns pointer to the record with\n*s e a r c hk e yv a l u e vif such a record exists, and null otherwise */\nSetC= root node\nwhile (Cis not a leaf node) begin\nLet i= smallest number such that v\u2264C.Ki\nifthere is no such number ithen begin\nLet Pm= last non-null pointer in the node\nSetC=C.Pm\nend\nelse if (v=C.Ki)then SetC=C.Pi+1\nelseSetC=C.Pi/*v<C.Ki*/\nend\n/*Cis a leaf node */\niffor some i,Ki=v\nthen return Pi\nelsereturn null ; /* No record with key value vexists*/\nFigure 14.11 Querying a B+-tree.\nAt the leaf node, if there is a search-key value Ki=v,p o i n t e r Pidirects us to a\nrecord with search-key value Ki. The function then returns the pointer to the record,\nPi.I fn os e a r c hk e yw i t hv a l u e vis found in the leaf node, no record with key value v\nexists in the relation, and function \ufb01ndreturns null, to indicate failure.\nB+-trees can also be used to \ufb01nd all records with search key values in a speci\ufb01ed\nrange [ lb,ub]. For example, with a B+-tree on attribute salary ofinstructor , we can \ufb01nd\nallinstructor records with salary in a speci\ufb01ed range such as [50000, 100000] (in other\nwords, all salaries between 50000 and 100000). Such queries are called range queries .\nTo execute such queries, we can create a procedure \ufb01ndRange (lb,ub), shown in\nFigure 14.12. The procedure does the following: it \ufb01rst traverses to a leaf in a manner\nsimilar to \ufb01nd(lb); the leaf may or may not actually contain value lb.I tt h e ns t e p s\nthrough records in that and subsequent leaf nodes collecting pointers to all records\nwith key values C.Kis.t.lb\u2264C.Ki\u2264ubinto a set resultSet. The function stops when\nC.Ki>ub,o rt h e r ea r en om o r ek e y si nt h et r e e .\nA real implementation would provide a version of \ufb01ndRange supporting an iterator\ninterface similar to that provided by the JDBC ResultSet , which we saw in Section 5.1.1.\nSuch an iterator interface would provide a method next(), which can be called repeat-\nedly to fetch successive records. The next() method would step through the entries at\nthe leaf level, in a manner similar to \ufb01ndRange , but each call takes only one step and\nrecords where it left o\ufb00, so that successive calls to next() step through successive en-\n", "667": "14.3 B+-Tree Index Files 639\nfunction \ufb01ndRange (lb,ub)\n/* Returns all records with search key value Vsuch that lb\u2264V\u2264ub.* /\nSet resultSet = {};\nSetC= root node\nwhile (Cis not a leaf node) begin\nLet i= smallest number such that lb\u2264C.Ki\nifthere is no such number ithen begin\nLet Pm= last non-null pointer in the node\nSetC=C.Pm\nend\nelse if (lb=C.Ki)then SetC=C.Pi+1\nelseSetC=C.Pi/*lb<C.Ki*/\nend\n/*Cis a leaf node */\nLet ib et h el e a s tv a l u es u c ht h a t Ki\u2265lb\nifthere is no such i\nthen Seti=1+n u m b e ro fk e y si n C;/ *T of o r c em o v et on e x tl e a f* /\nSet done = false;\nwhile (not done) begin\nLet n= number of keys in C.\nif(i\u2264nand C.Ki\u2264ub)then begin\nAdd C.Pito resultSet\nSeti=i+1\nend\nelse if (i\u2264nand C.Ki>ub)\nthen Set done = true;\nelse if (i>nand C.Pn+1is not null)\nthen SetC=C.Pn+1,a n d i=1/ *M o v et on e x tl e a f* /\nelseS e td o n e=t r u e ;/ *N om o r el e a v e st ot h er i g h t* /\nend\nreturn resultSet;\nFigure 14.12 Range query on a B+-tree.\ntries. We omit details for simplicity, and leave the pseudocode for the iterator interface\nas an exercise for the interested reader.\nWe now consider the cost of querying on a B+-tree index. In processing a query,\nwe traverse a path in the tree from the root to some leaf node. If there are Nrecords in\nthe \ufb01le, the path is no longer than \u2308log\u2308n\u22152\u2309(N)\u2309.\nT y p i c a l l y ,t h en o d es i z ei sc h o s e nt ob et h es a m ea st h es i z eo fad i s kb l o c k ,w h i c h\nis typically 4 kilobytes. With a search-key size of 12 bytes, and a disk-pointer size of\n", "668": "640 Chapter 14 Indexing\n8b y t e s , nis around 200. Even with a more conservative estimate of 32 bytes for the\nsearch-key size, nis around 100. With n=100, if we have 1 million search-key values in\nthe \ufb01le, a lookup requires only \u2308log50(1,000,000) \u2309=4 nodes to be accessed. Thus, at\nmost four blocks need to be read from disk to traverse the path from the root to a leaf.\nThe root node of the tree is usually heavily accessed and is likely to be in the bu\ufb00er, so\ntypically only three or fewer blocks need to be read from disk.\nAn important di\ufb00erence between B+-tree structures and in-memory tree structures,\nsuch as binary trees, is the size of a node, and as a result, the height of the tree. In a\nbinary tree, each node is small and has at most two pointers. In a B+-tree, each node\nis large\u2014typically a disk block\u2014and a node can have a large number of pointers. Thus,\nB+-trees tend to be fat and short, unlike thin and tall binary trees. In a balanced binary\ntree, the path for a lookup can be of length \u2308log2(N)\u2309,w h e r e Nis the number of\nrecords in the \ufb01le being indexed. With N=1,000,000 as in the previous example, a\nbalanced binary tree requires around 20 node accesses. If each node were on a di\ufb00erent\ndisk block, 20 block reads would be required to process a lookup, in contrast to the\nfour block reads for the B+-tree. The di\ufb00erence is signi\ufb01cant with a magnetic disk, since\neach block read could require a disk arm seek which, together with the block read, takes\nabout 10 milliseconds on a magnetic disk. The di\ufb00erence is not quite as drastic with\n\ufb02ash storage, where a read of a 4 kilobyte page takes around 10 to 100 microseconds,\nbut it is still signi\ufb01cant.\nAfter traversing down to the leaf level, queries on a single value of a unique search\nkey require one more random I/Ooperation to fetch any matching record.\nRange queries have an additional cost, after traversing down to the leaf level: all\nthe pointers in the given range must be retrieved. These pointers are in consecutive leaf\nnodes; thus, if Msuch pointers are retrieved, at most \u2308M\u2215(n\u22152)\u2309+1 leaf nodes need\nto be accessed to retrieve the pointers (since each leaf node has at least n\u22152p o i n t e r s ,\nbut even two pointers may be split across two pages). To this cost, we need to add the\ncost of accessing the actual records. For secondary indices, each such record may be\non a di\ufb00erent block, which could result in Mrandom I/Ooperations in the worst case.\nFor clustered indices, these records would be in consecutive blocks, with each block\ncontaining multiple records, resulting in a signi\ufb01cantly lower cost.\nNow, let us consider the case of nonunique keys. As explained earlier, if we wish\nto create an index on an attribute aithat is not a candidate key, and may thus have\nduplicates, we instead create an index on a composite key that is duplicate-free. The\ncomposite key is created by adding extra attributes, such as the primary key, to ai,t o\nensure uniqueness. Suppose we created an index on the composite key ( ai,Ap)i n s t e a d\nof creating an index on ai.\nAn important question, then, is how do we retrieve all tuples with a given value\nvforaiusing the above index? This question is easily answered by using the function\n\ufb01ndRange( lb,ub), with lb=(v,\u2212\u221e)a n d ub=(v,\u221e), where\u2212\u221e and\u221edenote the\nsmallest and largest possible values of Ap. All records with ai=vwould be returned\nby the above function call. Range queries on aican be handled similarly. These range\n", "669": "14.3 B+-Tree Index Files 641\nqueries retrieve pointers to the records quite e\ufb03ciently, although retrieval of the records\nmay be expensive, as discussed earlier.\n14.3.3 Updates on B+-Trees\nWhen a record is inserted into, or deleted from a relation, indices on the relation must\nbe updated correspondingly. Recall that updates to a record can be modeled as a dele-\ntion of the old record followed by insertion of the updated record. Hence we only\nconsider the case of insertion and deletion.\nInsertion and deletion are more complicated than lookup, since it may be neces-\nsary to split a node that becomes too large as the result of an insertion, or to coalesce\nnodes (i.e., combine nodes) if a node becomes too small (fewer than \u2308n\u22152\u2309pointers).\nFurthermore, when a node is split or a pair o f nodes is combined, we must ensure that\nbalance is preserved. To introduce the idea behind insertion and deletion in a B+-tree,\nwe shall assume temporarily that nodes never become too large or too small. Under\nthis assumption, insertion and deletion are performed as de\ufb01ned next.\n\u2022Insertion . Using the same technique as for lookup from the \ufb01nd() function (Figure\n14.11), we \ufb01rst \ufb01nd the leaf node in which the search-key value would appear. We\nthen insert an entry (i.e., a search-key value and record pointer pair) in the leaf\nnode, positioning it such that the search keys are still in order.\n\u2022Deletion .U s i n gt h es a m et e c h n i q u ea sf o rl o o k u p ,w e\ufb01 n dt h el e a fn o d ec o n t a i n i n g\nthe entry to be deleted by performing a lookup on the search-key value of the\ndeleted record; if there are multiple entries with the same search-key value, we\nsearch across all entries with the same search-key value until we \ufb01nd the entry that\npoints to the record being deleted. We then remove the entry from the leaf node.\nAll entries in the leaf node that are to the right of the deleted entry are shifted left\nby one position, so that there are no gaps in the entries after the entry is deleted.\nWe now consider the general case of insertion and deletion, dealing with node\nsplitting and node coalescing.\n14.3.3.1 Insertion\nWe now consider an example of insertion in which a node must be split. Assume that\na record is inserted on the instructor relation, with the name value being Adams. We\nthen need to insert an entry for \u201cAdams\u201d into the B+-tree of Figure 14.9. Using the\nalgorithm for lookup, we \ufb01nd that \u201cAdams\u201d should appear in the leaf node containing\n\u201cBrandt\u201d, \u201cCali\ufb01eri\u201d, and \u201cCrick.\u201d There is no room in this leaf to insert the search-\nkey value \u201cAdams.\u201d Therefore, the node is splitinto two nodes. Figure 14.13 shows the\ntwo leaf nodes that result from the split of the leaf node on inserting \u201cAdams\u201d. The\nsearch-key values \u201cAdams\u201d and \u201cBrandt\u201d are in one leaf, and \u201cCali\ufb01eri\u201d and \u201cCrick\u201d\nare in the other. In general, we take the nsearch-key values (the n\u22121v a l u e si nt h el e a f\n", "670": "642 Chapter 14 Indexing\nAdams Cali\ufb01eri Crick Brandt\nFigure 14.13 Split of leaf node on insertion of \u201cAdams\u201d.\nnode plus the value being inserted), and put the \ufb01rst \u2308n\u22152\u2309in the existing node and\nthe remaining values in a newly created node.\nHaving split a leaf node, we must insert the new leaf node into the B+-tree structure.\nIn our example, the new node has \u201cCali\ufb01eri\u201d as its smallest search-key value. We need\nto insert an entry with this search-key value, and a pointer to the new node, into the\nparent of the leaf node that was split. The B+- t r e eo fF i g u r e1 4 . 1 4s h o w st h er e s u l to ft h e\ninsertion. It was possible to perform this insertion with no further node split, because\nthere was room in the parent node for the new entry. If there were no room, the parent\nwould have had to be split, requiring an entry to be added to its parent. In the worst\ncase, all nodes along the path to the root must be split. If the root itself is split, the\nentire tree becomes deeper.\nSplitting of a nonleaf node is a little di\ufb00erent from splitting of a leaf node. Figure\n14.15 shows the result of inserting a record with search key \u201cLamport\u201d into the tree\nshown in Figure 14.14. The leaf node in which \u201cLamport\u201d is to be inserted already has\nentries \u201cGold\u201d, \u201cKatz\u201d, and \u201cKim\u201d, and as a result the leaf node has to be split. The\nnew right-hand-side node resulting from the split contains the search-key values \u201cKim\u201d\nand \u201cLamport\u201d. An entry (Kim, n1) must then be added to the parent node, where n1\nis a pointer to the new node, However, there is no space in the parent node to add a new\nentry, and the parent node has to be split. To do so, the parent node is conceptually\nexpanded temporarily, the entry added, and the overfull node is then immediately split.\nWhen an overfull nonleaf node is split, the child pointers are divided among the\noriginal and the newly created nodes; in our example, the original node is left with\nthe \ufb01rst three pointers, and the newly created node to the right gets the remaining two\npointers. The search key values are, however, handled a little di\ufb00erently. The search key\nvalues that lie between the pointers moved to the right node (in our example, the value\n\u201cKim\u201d) are moved along with the pointers, while those that lie between the pointers\nthat stay on the left (in our example, \u201cCali\ufb01eri\u201d and \u201cEinstein\u201d) remain undisturbed.\nAdams Brandt Einstein El Said Gold Katz Kim Mozart Singh Srinivasan WuGold SrinivasanMozart\nEinstein Cali\ufb01eri\nCrick Cali\ufb01eri\nFigure 14.14 Insertion of \u201cAdams\u201d into the B+-tree of Figure 14.9.\n", "671": "14.3 B+-Tree Index Files 643\nSrinivasanGold\nCali\ufb01eri EinsteinMozart\nKim\nAdams Brandt Einstein El Said Gold Katz Kim Lamport Mozart Singh Srinivasan Wu Crick Cali\ufb01eri\nFigure 14.15 Insertion of \u201cLamport\u201d into the B+-tree of Figure 14.14.\nHowever, the search key value that lies between the pointers that stay on the left,\nand the pointers that move to the right node is treated di\ufb00erently. In our example, the\nsearch key value \u201cGold\u201d lies between the three pointers that went to the left node, and\nthe two pointers that went to the right node. The value \u201cGold\u201d is not added to either of\nthe split nodes. Instead, an entry (Gold, n2) is added to the parent node, where n2i sa\npointer to the newly created node that resulted from the split. In this case, the parent\nnode is the root, and it has enough space for the new entry.\nThe general technique for insertion into a B+-tree is to determine the leaf node l\ninto which insertion must occur. If a split re sults, insert the new node into the parent\nprocedure insert (value K ,pointer P )\nif(tree is empty) create an empty leaf node L,w h i c hi sa l s ot h er o o t\nelseFind the leaf node Lthat should contain key value K\nif(Lh a sl e s st h a n n\u22121k e yv a l u e s )\nthen insert\n in\nleaf ( L,K,P)\nelse begin /*Lhas n\u22121 key values already, split it */\nCreate node L\u2032\nCopy L.P1\u2026L.Kn\u22121to a block of memory Tthat can\nhold n(pointer, key-value) pairs\ninsert\n in\nleaf ( T,K,P)\nSetL\u2032.Pn=L.Pn;S e t L.Pn=L\u2032\nErase L.P1through L.Kn\u22121from L\nCopy T.P1through T.K\u2308n\u22152\u2309from Tinto Lstarting at L.P1\nCopy T.P\u2308n\u22152\u2309+1through T.Knfrom Tinto L\u2032starting at L\u2032.P1\nLet K\u2032be the smallest key-value in L\u2032\ninsert\n in\nparent( L,K\u2032,L\u2032)\nend\nFigure 14.16 Insertion of entry in a B+-tree.\n", "672": "644 Chapter 14 Indexing\nof node l. If this insertion causes a split, proceed recursively up the tree until either an\ninsertion does not cause a split or a new root is created.\nFigure 14.16 outlines the insertion algorithm in pseudocode. The procedure in-\nsertinserts a key-value pointer pair into the index, using two subsidiary procedures\ninsert\n in\nleaf and insert\n in\nparent , shown in Figure 14.17. In the pseudocode, L,N,P\nand Tdenote pointers to nodes, with Lbeing used to denote a leaf node. L.Kiand L.Pi\ndenote the ith value and the ith pointer in node L, respectively; T.Kiand T.Piare used\nsimilarly. The pseudocode also makes use of the function parent (N) to \ufb01nd the parent\nof a node N. We can compute a list of nodes in the path from the root to the leaf while\ninitially \ufb01nding the leaf node, and we can use it later to \ufb01nd the parent of any node in\nthe path e\ufb03ciently.\nprocedure insert\n in\nleaf(node L ,value K ,pointer P )\nif(K<L.K1)\nthen insert P,Kinto Ljust before L.P1\nelse begin\nLet Kibe the highest value in Lthat is less than or equal to K\nInsert P,Kinto Ljust after L.Ki\nend\nprocedure insert\n in\nparent (node N ,value K\u2032,node N\u2032)\nif(Nis the root of the tree)\nthen begin\nCreate a new node Rcontaining N,K\u2032,N\u2032/*Nand N\u2032are pointers */\nMake Rthe root of the tree\nreturn\nend\nLet P=parent (N)\nif(Ph a sl e s st h a n npointers)\nthen insert ( K\u2032,N\u2032)i n Pjust after N\nelse begin /* Split P*/\nCopy Pto a block of memory Tthat can hold Pand ( K\u2032,N\u2032)\nInsert ( K\u2032,N\u2032)i n t o Tjust after N\nErase all entries from P;C r e a t en o d e P\u2032\nCopy T.P1\u2026T.P\u2308(n+1)\u22152\u2309into P\nLet K\u2032\u2032=T.K\u2308(n+1)\u22152\u2309\nCopy T.P\u2308(n+1)\u22152\u2309+1\u2026T.Pn+1into P\u2032\ninsert\n in\nparent( P,K\u2032\u2032,P\u2032)\nend\nFigure 14.17 Subsidiary procedures for insertion of entry in a B+-tree.\n", "673": "14.3 B+-Tree Index Files 645\nThe procedure insert\n in\nparent takes as parameters N,K\u2032,N\u2032, where node Nwas\nsplit into Nand N\u2032,w i t h K\u2032being the least value in N\u2032. The procedure modi\ufb01es the\nparent of Nto record the split. The procedures insert\n into\n index and insert\n in\nparent\nuse a temporary area of memory Tto store the contents of a node being split. The\nprocedures can be modi\ufb01ed to copy data from the node being split directly to the newly\ncreated node, reducing the time required for copying data. However, the use of the\ntemporary space Tsimpli\ufb01es the procedures.\n14.3.3.2 Deletion\nWe now consider deletions that cause tree nodes to contain too few pointers. First, let\nus delete \u201cSrinivasan\u201d from the B+-tree of Figure 14.14. The resulting B+-tree appears\nin Figure 14.18. We now consider how the deletion is performed. We \ufb01rst locate the\nentry for \u201cSrinivasan\u201d by using our lookup algorithm. When we delete the entry for\n\u201cSrinivasan\u201d from its leaf node, the node is left with only one entry, \u201cWu\u201d. Since, in\nour example, n=4a n d1 <\u2308(n\u22121)\u22152\u2309,w em u s te i t h e rm e r g et h en o d ew i t ha\nsibling node or redistribute the entries between the nodes, to ensure that each node\nis at least half-full. In our example, the underfull node with the entry for \u201cWu\u201d can be\nmerged with its left sibling node. We merge the nodes by moving the entries from both\nthe nodes into the left sibling and deleting the now-empty right sibling. Once the node\nis deleted, we must also delete the entry in the parent node that pointed to the just\ndeleted node.\nIn our example, the entry to be deleted is (Srinivasan, n3), where n3i sap o i n t e r\nto the leaf containing \u201cSrinivasan\u201d. (In this case the entry to be deleted in the nonleaf\nnode happens to be the same value as that deleted from the leaf; that would not be the\ncase for most deletions.) After deleting the above entry, the parent node, which had\na search key value \u201cSrinivasan\u201d and two pointers, now has one pointer (the leftmost\npointer in the node) and no search-key values. Since 1 <\u2308n\u22152\u2309forn=4, the parent\nnode is underfull. (For larger n, a node that becomes underfull would still have some\nvalues as well as pointers.)\nAdams Brandt Cali\ufb01eri Crick Einstein El Said Gold Katz Kim Mozart Singh WuCali\ufb01eriGold\nMozart Einstein\nFigure 14.18 Deletion of \u201cSrinivasan\u201d from the B+-tree of Figure 14.14.\n", "674": "646 Chapter 14 Indexing\nIn this case, we look at a sibling node; in our example, the only sibling is the nonleaf\nnode containing the search keys \u201cCali\ufb01eri\u201d, \u201cEinstein\u201d, and \u201cGold\u201d. If possible, we try\nto coalesce the node with its sibling. In this case, coalescing is not possible, since the\nnode and its sibling together have \ufb01ve pointers, against a maximum of four. The solution\nin this case is to redistribute the pointers between the node and its sibling, such that each\nhas at least \u2308n\u22152\u2309=2 child pointers. To do so, we move the rightmost pointer from\nthe left sibling (the one pointing to the leaf node containing \u201cGold\u201d) to the underfull\nright sibling. However, the underfull right sibling would now have two pointers, namely,\nits leftmost pointer, and the newly moved pointer, with no value separating them. In\nfact, the value separating them is not present in either of the nodes, but is present\nin the parent node, between the pointers from the parent to the node and its sibling.\nIn our example, the value \u201cMozart\u201d separates the two pointers and is present in the\nright sibling after the redistribution. Redistribution of the pointers also means that the\nvalue \u201cMozart\u201d in the parent no longer correctly separates search-key values in the two\nsiblings. In fact, the value that now correctly separates search-key values in the two\nsibling nodes is the value \u201cGold\u201d, which was in the left sibling before redistribution.\nAs a result, as can be seen in the B+-tree in Figure 14.18, after redistribution of\npointers between siblings, the value \u201cGold\u201d has moved up into the parent, while the\nvalue that was there earlier, \u201cMozart\u201d, has moved down into the right sibling.\nWe next delete the search-key values \u201cSingh\u201d and \u201cWu\u201d from the B+-tree of Figure\n14.18. The result is shown in Figure 14.19. The deletion of the \ufb01rst of these values does\nnot make the leaf node underfull, but the deletion of the second value does. It is not\npossible to merge the underfull node with its sibling, so a redistribution of values is\ncarried out, moving the search-key value \u201cKim\u201d into the node containing \u201cMozart\u201d,\nresulting in the tree shown in Figure 14.19. The value separating the two siblings has\nbeen updated in the parent, from \u201cMozart\u201d to \u201cKim\u201d.\nNow we delete \u201cGold\u201d from the above tree; the result is shown in Figure 14.20. This\nresults in an underfull leaf, which can now be merged with its sibling. The resultant\ndeletion of an entry from the parent node (the nonleaf node containing \u201cKim\u201d) makes\nthe parent underfull (it is left with just one pointer). This time around, the parent\nnode can be merged with its sibling. This merge results in the search-key value \u201cGold\u201d\nAdams Brandt Cali\ufb01eri Crick Einstein El Said Gold Katz Kim      MozartCali\ufb01eri Einstein KimGold\nFigure 14.19 Deletion of \u201cSingh\u201d and \u201cWu\u201d from the B+-tree of Figure 14.18.\n", "675": "14.3 B+-Tree Index Files 647\nAdams Brandt Einstein El Said Katz Kim MozartGold Cali\ufb01eri\nCali\ufb01eriEinstein\nCrick\nFigure 14.20 Deletion of \u201cGold\u201d from the B+-tree of Figure 14.19.\nmoving down from the parent into the merged node. As a result of this merge, an entry\nis deleted from its parent, which happens to be the root of the tree. And as a result\nof that deletion, the root is left with only one child pointer and no search-key value,\nviolating the condition that the root must have at least two children. As a result, the\nroot node is deleted and its sole child becomes the root, and the depth of the B+-tree\nhas been decreased by 1.\nIt is worth noting that, as a result of deletion, a key value that is present in a nonleaf\nnode of the B+-tree may not be present at any leaf of the tree. For example, in Figure\n14.20, the value \u201cGold\u201d has been deleted from the leaf level but is still present in a\nnonleaf node.\nIn general, to delete a value in a B+-tree, we perform a lookup on the value and\ndelete it. If the node is too small, we delete it from its parent. This deletion results\nin recursive application of the deletion algorithm until the root is reached, a parent\nremains adequately full after deletion, or redistribution is applied.\nFigure 14.21 outlines the pseudocode for deletion from a B+-tree. The procedure\nswap\n variables (N,N\u2032) merely swaps the values of the (pointer) variables Nand N\u2032;\nthis swap has no e\ufb00ect on the tree itself. The pseudocode uses the condition \u201ctoo few\npointers/values.\u201d For nonleaf nodes, this criterion means less than \u2308n\u22152\u2309pointers; for\nleaf nodes, it means less than \u2308(n\u22121)\u22152\u2309values. The pseudocode redistributes entries\nby borrowing a single entry from an adjacent node. We can also redistribute entries by\nrepartitioning entries equally between the two nodes. The pseudocode refers to deleting\nan entry ( K,P) from a node. In the case of leaf nodes, the pointer to an entry actually\nprecedes the key value, so the pointer Pprecedes the key value K. For nonleaf nodes,\nPfollows the key value K.\n14.3.4 Complexity of B+-Tree Updates\nAlthough insertion and deletion operations on B+-trees are complicated, they require\nrelatively few I/Ooperations, which is an important bene\ufb01t since I/Ooperations are\nexpensive. It can be shown that the number of I/Ooperations needed in the worst case\nfor an insertion is proportional to log\u2308n\u22152\u2309(N), where nis the maximum number of\npointers in a node, and Nis the number of records in the \ufb01le being indexed.\nThe worst-case complexity of the deletion procedure is also proportional to\nlog\u2308n\u22152\u2309(N) ,p r o v i d e dt h e r ea r en od u p l i c a t ev a l u e sf o rt h es e a r c hk e y ;w ed i s c u s st h e\ncase of nonunique search keys later in this chapter.\n", "676": "648 Chapter 14 Indexing\nprocedure delete (value K ,pointer P )\n\ufb01nd the leaf node Lthat contains ( K,P)\ndelete\n entry( L,K,P)\nprocedure delete\n entry (node N ,value K ,pointer P )\ndelete ( K,P)f r o m N\nif(Nis the root and Nhas only one remaining child)\nthen make the child of Nt h en e wr o o to ft h et r e ea n dd e l e t e N\nelse if (Nhas too few values/pointers) then begin\nLet N\u2032be the previous or next child of parent (N)\nLet K\u2032be the value between pointers Nand N\u2032inparent (N)\nif(entries in Nand N\u2032can \ufb01t in a single node)\nthen begin /* Coalesce nodes */\nif(Nis a predecessor of N\u2032)then swap\n variables( N,N\u2032)\nif(Nis not a leaf)\nthen append K\u2032and all pointers and values in NtoN\u2032\nelse append all ( Ki,Pi)p a i r si n NtoN\u2032;s e t N\u2032.Pn=N.Pn\ndelete\n entry( parent (N),K\u2032,N); delete node N\nend\nelse begin /* Redistribution: borrow an entry from N\u2032*/\nif(N\u2032is a predecessor of N)then begin\nif(Nis a nonleaf node) then begin\nletmbe such that N\u2032.Pmis the last pointer in N\u2032\nremove ( N\u2032.Km\u22121,N\u2032.Pm)f r o m N\u2032\ninsert ( N\u2032.Pm,K\u2032) as the \ufb01rst pointer and value in N,\nby shifting other pointers and values right\nreplace K\u2032inparent (N)b y N\u2032.Km\u22121\nend\nelse begin\nletmbe such that ( N\u2032.Pm,N\u2032.Km) is the last pointer/value\npair in N\u2032\nremove ( N\u2032.Pm,N\u2032.Km)f r o m N\u2032\ninsert ( N\u2032.Pm,N\u2032.Km) as the \ufb01rst pointer and value in N,\nby shifting other pointers and values right\nreplace K\u2032inparent (N)b y N\u2032.Km\nend\nend\nelse\u2026symmetric to the then case\u2026\nend\nend\nFigure 14.21 Deletion of entry from a B+-tree.\n", "677": "14.3 B+-Tree Index Files 649\nIn other words, the cost of insertion and deletion operations in terms of I/Ooper-\nations is proportional to the height of the B+-tree, and is therefore low. It is the speed\nof operation on B+-trees that makes them a frequently used index structure in database\nimplementations.\nIn practice, operations on B+-trees result in fewer I/Ooperations than the worst-\ncase bounds. With fanout of 100, and assuming accesses to leaf nodes are uniformly\ndistributed, the parent of a leaf node is 100 times more likely to get accessed than the\nleaf node. Conversely, with the same fanout, the total number of nonleaf nodes in a B+-\ntree would be just a little more than 1/100th of the number of leaf nodes. As a result,\nwith memory sizes of several gigabytes being common today, for B+-trees that are used\nfrequently, even if the relation is very large it is quite likely that most of the nonleaf\nnodes are already in the database bu\ufb00er when they are accessed. Thus, typically only\none or two I/Ooperations are required to perform a lookup. For updates, the probability\nof a node split occurring is correspondingly very small. Depending on the ordering of\ninserts, with a fanout of 100, only from 1 in 100 to 1 in 50 insertions will result in a\nnode split, requiring more than one block to be written. As a result, on an average an\ninsert will require just a little more than one I/Ooperation to write updated blocks.\nAlthough B+-trees only guarantee that nodes will be at least half full, if entries are\ninserted in random order, nodes can be expected to be more than two-thirds full on\naverage. If entries are inserted in sorted order, on the other hand, nodes will be only\nhalf full. (We leave it as an exercise to the reader to \ufb01gure out why nodes would be only\nhalf full in the latter case.)\n14.3.5 Nonunique Search Keys\nWe have assumed so far that search keys are unique. Recall also that we described\nearlier, in Section 14.3.1, how to make search keys unique by creating a composite\nsearch key containing the original search key and extra attributes, that together are\nunique across all records.\nThe extra attribute can be a record-id, which is a pointer to the record, or a primary\nkey, or any other attribute whose value is unique among all records with the same\nsearch-key value. The extra attribute is called a uniqui\ufb01er attribute.\nA search with the original search-key attribute can be carried out using a range\nsearch as we saw in Section 14.3.2; alternatively, we can create a variant of the \ufb01ndRange\nfunction that takes only the original search key value as parameter and ignores the value\nof the uniqui\ufb01er attribute when comparing search-key values.\nIt is also possible to modify the B+-tree structure to support duplicate search keys.\nThe insert, delete, and lookup methods all have to be modi\ufb01ed correspondingly.\n\u2022One alternative is to store each key value only once in the tree, and to keep a\nbucket (or list) of record pointers with a search-key value, to handle nonunique\nsearch keys. This approach is space e\ufb03cient since it stores the key value only once;\nhowever, it creates several complications when B+-trees are implemented. If the\n", "678": "650 Chapter 14 Indexing\nbuckets are kept in the leaf node, extra code is needed to deal with variable-size\nbuckets, and to deal with buckets that grow larger than the size of the leaf node. If\nthe buckets are stored in separate blocks, an extra I/Ooperation may be required\nto fetch records.\n\u2022Another option is to store the search key value once per record; this approach\nallows a leaf node to be split in the usual way if it is found to be full during an in-\nsert. However, this approach makes handling of split and search on internal nodes\nsigni\ufb01cantly more complicated, since two leaves may contain the same search key\nvalue. It also has a higher space overhead, since key values are stored as many\ntimes as there are records containing that value.\nA major problem with both these approaches, as compared to the unique search-\nkey approach, lies in the e\ufb03ciency of record deletion. (The complexity of lookup and\ninsertion are the same with both these approaches, as well as with the unique search-key\napproach.) Suppose a particular search-key value occurs a large number of times, and\none of the records with that search key is to be deleted. The deletion may have to search\nthrough a number of entries with the same search-key value, potentially across multiple\nleaf nodes, to \ufb01nd the entry corresponding to the particular record being deleted. Thus,\nthe worst-case complexity of deletion may be linear in the number of records.\nIn contrast, record deletion can be done e\ufb03ciently using the unique search key\napproach. When a record is to be deleted, the composite search-key value is computed\nfrom the record and then used to look up the index. Since the value is unique, the\ncorresponding leaf-level entry can be found with a single traversal from root to leaf,\nwith no further accesses at the leaf level. The worst-case cost of deletion is logarithmic\nin the number of records, as we saw earlier.\nDue to the ine\ufb03ciency of deletion, as well as other complications due to du-\nplicate search keys, B+-tree implementations in most database systems only handle\nunique search keys, and they automatically add record-ids or other attributes to make\nnonunique search keys unique.\n14.4 B+-Tree Extensions\nIn this section, we discuss several extensions and variations of the B+-tree index struc-\nture.\n14.4.1 B+- T r e eF i l eO r g a n i z a t i o n\nAs mentioned in Section 14.3, the main drawback of index-sequential \ufb01le organization\nis the degradation of performance as the \ufb01le grows: With growth, an increasing percent-\nage of index entries and actual records become out of order and are stored in over\ufb02ow\nblocks. We solve the degradation of index lookups by using B+-tree indices on the \ufb01le.\n", "679": "14.4 B+-Tree Extensions 651\nI\nC MK\n(A,4) (C,1) (B,8) (D,9) (E,4) (F,7) (G,3) (H,3)\n(I,4) (J,8) (K,1) (L,6) (M,4) (N,8) (P,6)F\nFigure 14.22 B+-tree file organization.\nWe solve the degradation problem for storing the actual records by using the leaf level\nof the B+-tree to organize the blocks containing the actual records. We use the B+-tree\nstructure not only as an index, but also as an organizer for records in a \ufb01le. In a B+-tree\n\ufb01le organization , the leaf nodes of the tree store records, instead of storing pointers to\nrecords. Figure 14.22 shows an example of a B+-tree \ufb01le organization. Since records\nare usually larger than pointers, the maximum number of records that can be stored\nin a leaf node is less than the number of pointers in a nonleaf node. However, the leaf\nnodes are still required to be at least half full.\nInsertion and deletion of records from a B+-tree \ufb01le organization are handled in\nthe same way as insertion and deletion of entries in a B+-tree index. When a record\nwith a given key value vis inserted, the system locates the block that should contain\nthe record by searching the B+- t r e ef o rt h el a r g e s tk e yi nt h et r e et h a ti s \u2264v.I ft h eb l o c k\nlocated has enough free space for the record, the system stores the record in the block.\nOtherwise, as in B+-tree insertion, the system splits the block in two and redistributes\nthe records in it (in the B+-tree\u2013key order) to create space for the new record. The split\npropagates up the B+-tree in the normal fashion. When we delete a record, the system\n\ufb01rst removes it from the block containing it. If a block Bbecomes less than half full\nas a result, the records in Bare redistributed with the records in an adjacent block B\u2032.\nAssuming \ufb01xed-sized records, each block will hold at least one-half as many records as\nthe maximum that it can hold. The system updates the nonleaf nodes of the B+-tree in\nthe usual fashion.\nWhen we use a B+-tree for \ufb01le organization, space utilization is particularly impor-\ntant, since the space occupied by the records is likely to be much more than the space\noccupied by keys and pointers. We can improve the utilization of space in a B+-tree by\ninvolving more sibling nodes in redistribution during splits and merges. The technique\nis applicable to both leaf nodes and nonleaf nodes, and it works as follows:\nDuring insertion, if a node is full, the system attempts to redistribute some of its\nentries to one of the adjacent nodes, to make space for a new entry. If this attempt fails\nbecause the adjacent nodes are themselves full, the system splits the node and divides\nthe entries evenly among one of the adjacent nodes and the two nodes that it obtained\nby splitting the original node. Since the three nodes together contain one more record\n", "680": "652 Chapter 14 Indexing\nthan can \ufb01t in two nodes, each node will be about two-thirds full. More precisely, each\nnode will have at least \u230a2n\u22153\u230bentries, where nis the maximum number of entries that\nthe node can hold. ( \u230ax\u230bdenotes the greatest integer that is less than or equal to x;t h a t\nis, we drop the fractional part, if any.)\nDuring deletion of a record, if the occupancy of a node falls below \u230a2n\u22153\u230b,t h e\nsystem attempts to borrow an entry from one of the sibling nodes. If both sibling nodes\nhave \u230a2n\u22153\u230brecords, instead of borrowing an entry, the system redistributes the entries\nin the node and in the two siblings evenly between two of the nodes and deletes the third\nnode. We can use this approach because the total number of entries is 3 \u230a2n\u22153\u230b\u22121,\nwhich is less than 2 n. With three adjacent nodes used for redistribution, each node can\nbe guaranteed to have \u230a3n\u22154\u230bentries. In general, if mnodes ( m\u22121 siblings) are involved\nin redistribution, each node can be guaranteed to contain at least \u230a(m\u22121)n\u2215m\u230bentries.\nHowever, the cost of update becomes higher as more sibling nodes are involved in the\nredistribution.\nNote that in a B+-tree index or \ufb01le organization, leaf nodes that are adjacent to each\nother in the tree may be located at di\ufb00erent places on disk. When a \ufb01le organization\nis newly created on a set of records, it is possible to allocate blocks that are mostly\ncontiguous on disk to leaf nodes that are contiguous in the tree. Thus, a sequential\nscan of leaf nodes would correspond to a mostly sequential scan on disk. As insertions\nand deletions occur on the tree, sequentiality is increasingly lost, and sequential access\nhas to wait for disk seeks increasingly often. An index rebuild may be required to restore\nsequentiality.\nB+-tree \ufb01le organizations can also be used to store large objects, such as SQL clobs\nand blobs, which may be larger than a disk block, and as large as multiple gigabytes.\nSuch large objects can be stored by splitting them into sequences of smaller records that\nare organized in a B+-tree \ufb01le organization. The records can be sequentially numbered,\nor numbered by the byte o\ufb00set of the record within the large object, and the record\nnumber can be used as the search key.\n14.4.2 Secondary Indices and Record Relocation\nSome \ufb01le organizations, such as the B+-tree \ufb01le organization, may change the location\nof records even when the records have not been updated. As an example, when a leaf\nnode is split in a B+-tree \ufb01le organization, a number of records are moved to a new\nnode. In such cases, all secondary indices that store pointers to the relocated records\nwould have to be updated, even though the values in the records may not have changed.\nEach leaf node may contain a fairly large number of records, and each of them may be\nin di\ufb00erent locations on each secondary index. Thus, a leaf-node split may require tens\nor even hundreds of I/Ooperations to update all a\ufb00ected secondary indices, making it\na very expensive operation.\nA widely used solution for this problem is as follows: In secondary indices, in place\nof pointers to the indexed records, we store the values of the primary-index search-key\n", "681": "14.4 B+-Tree Extensions 653\nattributes. For example, suppose we have a primary index on the attribute IDof relation\ninstructor ; then a secondary index on dept\n name would store with each department\nname a list of instructor\u2019s IDvalues of the corresponding records, instead of storing\npointers to the records.\nRelocation of records because of leaf-node splits then does not require any update\non any such secondary index. However, locating a record using the secondary index\nnow requires two steps: First we use the secondary index to \ufb01nd the primary-index\nsearch-key values, and then we use the primary index to \ufb01nd the corresponding records.\nThis approach thus greatly reduces the cost of index update due to \ufb01le reorganiza-\ntion, although it increases the cost of accessing data using a secondary index.\n14.4.3 Indexing Strings\nCreating B+-tree indices on string-valued attributes raises two problems. The \ufb01rst prob-\nlem is that strings can be of variable length. The second problem is that strings can be\nlong, leading to a low fanout and a correspondingly increased tree height.\nWith variable-length search keys, di\ufb00erent nodes can have di\ufb00erent fanouts even if\nthey are full. A node must then be split if it is full, that is, there is no space to add a\nnew entry, regardless of how many search entries it has. Similarly, nodes can be merged\nor entries redistributed depending on what fraction of the space in the nodes is used,\ninstead of being based on the maximum number of entries that the node can hold.\nThe fanout of nodes can be increased by using a technique called pre\ufb01x compres-\nsion. With pre\ufb01x compression, we do not store the entire search key value at nonleaf\nnodes. We only store a pre\ufb01x of each search key value that is su\ufb03cient to distinguish\nbetween the key values in the subtrees that it separates. For example, if we had an index\non names, the key value at a nonleaf node could be a pre\ufb01x of a name; it may su\ufb03ce\nto store \u201cSilb\u201d at a nonleaf node, instead of the full \u201cSilberschatz\u201d if the closest values\nin the two subtrees that it separates are, say, \u201cSilas\u201d and \u201cSilver\u201d respectively.\n14.4.4 Bulk Loading of B+-Tree Indices\nAs we saw earlier, insertion of a record in a B+-tree requires a number of I/Ooperations\nthat in the worst case is proportional to the height of the tree, which is usually fairly\nsmall (typically \ufb01ve or less, even for large relations).\nNow consider the case where a B+-tree is being built on a large relation. Suppose\nthe relation is signi\ufb01cantly larger than main memory, and we are constructing a non-\nclustering index on the relation such that the index is also larger than main memory.\nIn this case, as we scan the relation and add entries to the B+-tree, it is quite likely that\neach leaf node accessed is not in the database bu\ufb00er when it is accessed, since there is\nno particular ordering of the entries. With such randomly ordered accesses to blocks,\neach time an entry is added to the leaf, a disk seek will be required to fetch the block\ncontaining the leaf node. The block will probably be evicted from the disk bu\ufb00er before\nanother entry is added to the block, leading to another disk seek to write the block back\n", "682": "654 Chapter 14 Indexing\nto disk. Thus, a random read and a random write operation may be required for each\nentry inserted.\nFor example, if the relation has 100 million records, and each I/Ooperation takes\nabout 10 milliseconds on a magnetic disk, it would take at least 1 million seconds to\nbuild the index, counting only the cost of reading leaf nodes, not even counting the cost\nof writing the updated nodes back to disk. This is clearly a very large amount of time;\nin contrast, if each record occupies 100 bytes, and the disk subsystem can transfer data\nat 50 megabytes per second, it would take just 200 seconds to read the entire relation.\nInsertion of a large number of entries at a time into an index is referred to as bulk\nloading of the index. An e\ufb03cient way to perform bulk loading of an index is as follows:\nFirst, create a temporary \ufb01le containing index entries for the relation, then sort the\n\ufb01le on the search key of the index being constructed, and \ufb01nally scan the sorted \ufb01le\nand insert the entries into the index. There are e\ufb03cient algorithms for sorting large\nrelations, described later in Section 15.4, which can sort even a large \ufb01le with an I/O\ncost comparable to that of reading the \ufb01le a few times, assuming a reasonable amount\nof main memory is available.\nThere is a signi\ufb01cant bene\ufb01t to sorting the entries before inserting them into the\nB+-tree. When the entries are inserted in sorted order, all entries that go to a particular\nleaf node will appear consecutively, and the leaf needs to be written out only once;\nnodes will never have to be read from disk during bulk load, if the B+-tree was empty\nto start with. Each leaf node will thus incur only one I/Oo p e r a t i o ne v e nt h o u g hm a n y\nentries may be inserted into the node. If each leaf contains 100 entries, the leaf level\nwill contain 1 million nodes, resulting in only 1 million I/Ooperations for creating\nthe leaf level. Even these I/Ooperations can be expected to be sequential, if succes-\nsive leaf nodes are allocated on successive disk blocks, and few disk seeks would be\nrequired. With magnetic disks, 1 millisecond per block is a reasonable estimate for\nmostly sequential I/Ooperations, in contrast to 10 milliseconds per block for random\nI/Ooperations.\nWe shall study the cost of sorting a large relation later, in Section 15.4, but as a\nrough estimate, the index which would have otherwise taken up to 1,000,000 seconds\nto build on a magnetic disk can be constructed in well under 1000 seconds by sorting\nthe entries before inserting them into the B+-tree.\nIf the B+-tree is initially empty, it can be constructed faster by building it bottom-\nup, from the leaf level, instead of using the usual insert procedure. In bottom-up B+-\ntree construction , after sorting the entries as we just described, we break up the sorted\nentries into blocks, keeping as many entries in a block as can \ufb01t in the block; the\nresulting blocks form the leaf level of the B+-tree. The minimum value in each block,\nalong with the pointer to the block, is used to create entries in the next level of the B+-\ntree, pointing to the leaf blocks. Each further level of the tree is similarly constructed\nusing the minimum values associated with each node one level below, until the root is\ncreated. We leave details as an exercise for the reader.\nMost database systems implement e\ufb03cient techniques based on sorting of entries,\nand bottom-up construction, when creating an index on a relation, although they use\n", "683": "14.4 B+-Tree Extensions 655\nBrandt Cali\ufb01eri Crick El Said Gold Kim Mozart Srinivasan WuEinstein Katz Singh\nEinstein\nrecordKatz\nrecordSingh\nrecord\nBrandt\nrecordCali\ufb01eri\nrecord... and so on for other records...\nFigure 14.23 B-tree equivalent of B+-tree in Figure 14.9.\nthe normal insertion procedure when tuples are added one at a time to a relation with\nan existing index. Some database systems recommend that if a very large number of\ntuples are added at once to an already existing relation, indices on the relation (other\nthan any index on the primary key) should be dropped, and then re-created after the\ntuples are inserted, to take advantage of e\ufb03cient bulk-loading techniques.\n14.4.5 B-Tree Index Files\nB-tree indices are similar to B+-tree indices. The primary distinction between the two\napproaches is that a B-tree eliminates the redundant storage of search-key values. In the\nB+-tree of Figure 14.9, the search keys \u201cEinstein\u201d, \u201cGold\u201d, \u201cMozart\u201d, and \u201cSrinivasan\u201d\nappear in nonleaf nodes, in addition to appearing in the leaf nodes. Every search-key\nvalue appears in some leaf node; several are repeated in nonleaf nodes.\nA B-tree allows search-key values to appear only once (if they are unique), unlike\naB+-tree, where a value may appear in a nonleaf node, in addition to appearing in\na leaf node. Figure 14.23 shows a B-tree that represents the same search keys as the\nB+-tree of Figure 14.9. Since search keys are not repeated in the B-tree, we may be\nable to store the index in fewer tree nodes than in the corresponding B+-tree index.\nHowever, since search keys that appear in nonleaf nodes appear nowhere else in the\nB-tree, we are forced to include an additional pointer \ufb01eld for each search key in a\nnonleaf node. These additional pointers point to either \ufb01le records or buckets for the\nassociated search key.\nIt is worth noting that many database system manuals, articles in industry litera-\nture, and industry professionals use the term B-tree to refer to the data structure that\nwe call the B+-tree. In fact, it would be fair to say that in current usage, the term B-tree\nis assumed to be synonymous with B+-tree. However, in this book we use the terms\nB-tree and B+-tree as they were originally de\ufb01ned, to avoid confusion between the two\ndata structures.\nA generalized B-tree leaf node appears in Figure 14.24a; a nonleaf node appears in\nFigure 14.24b. Leaf nodes are the same as in B+-trees. In nonleaf nodes, the pointers\nPiare the tree pointers that we used also for B+-trees, while the pointers Biare bucket\nor \ufb01le-record pointers. In the generalized B-tree in the \ufb01gure, there are n\u22121k e y si n\n", "684": "656 Chapter 14 Indexing\nP1 K1 P2Pn-1 Kn-1 Pn\u2026\nP1 B1 K1 P2 B2 K2\u2026 Pm-1 Bm-1 Km-1 Pm(a)\n(b)\nFigure 14.24 Typical nodes of a B-tree. (a) Leaf node. (b) Nonleaf node.\nthe leaf node, but there are m\u22121 keys in the nonleaf node. This discrepancy occurs\nbecause nonleaf nodes must include pointers Bi, thus reducing the number of search\nk e y st h a tc a nb eh e l di nt h e s en o d e s .C l e a r l y , m<n, but the exact relationship between\nmand ndepends on the relative size of search keys and pointers.\nThe number of nodes accessed in a lookup in a B-tree depends on where the search\nkey is located. A lookup on a B+-tree requires traversal of a path from the root of the\ntree to some leaf node. In contrast, it is sometimes possible to \ufb01nd the desired value in a\nB-tree before reaching a leaf node. However, roughly nt i m e sa sm a n yk e y sa r es t o r e di n\nthe leaf level of a B-tree as in the nonleaf levels, and, since nis typically large, the bene\ufb01t\nof \ufb01nding certain values early is relatively small. Moreover, the fact that fewer search\nkeys appear in a nonleaf B-tree node, compared to B+-trees, implies that a B-tree has\na smaller fanout and therefore may have depth greater than that of the corresponding\nB+-tree. Thus, lookup in a B-tree is faster for some search keys but slower for others,\nalthough, in general, lookup time is still proportional to the logarithm of the number\nof search keys.\nDeletion in a B-tree is more complicated. In a B+-tree, the deleted entry always\nappears in a leaf. In a B-tree, the deleted entry may appear in a nonleaf node. The proper\nvalue must be selected as a replacement from the subtree of the node containing the\ndeleted entry. Speci\ufb01cally, if search key Kiis deleted, the smallest search key appearing\nin the subtree of pointer Pi+1must be moved to the \ufb01eld formerly occupied by Ki.\nFurther actions need to be taken if the leaf node now has too few entries. In contrast,\ninsertion in a B-tree is only slightly more complicated than is insertion in a B+-tree.\nThe space advantages of B-trees are marginal for large indices and usually do not\noutweigh the disadvantages that we have noted. Thus, pretty much all database-system\nimplementations use the B+-tree data structure, even if (as we discussed earlier) they\nrefer to the data structure as a B-tree.\n14.4.6 Indexing on Flash Storage\nIn our description of indexing so far, we have assumed that data are resident on mag-\nnetic disks. Although this assumption continues to be true for the most part, \ufb02ash stor-\nage capacities have grown signi\ufb01cantly, and the cost of \ufb02ash storage per gigabyte has\ndropped correspondingly, and \ufb02ash based SSDstorage has now replaced magnetic-disk\nstorage for many applications.\n", "685": "14.4 B+-Tree Extensions 657\nStandard B+-tree indices can continue to be used even on SSDs, with acceptable\nupdate performance and signi\ufb01cantly improved lookup performance compared to disk\nstorage.\nFlash storage is structured as pages, and the B+-tree index structure can be used\nwith \ufb02ash based SSDs.SSDsp r o v i d em u c hf a s t e rr a n d o m I/Ooperations than magnetic\ndisks, requiring only around 20 to 100 microseconds for a random page read, instead\nof about 5 to 10 milliseconds with magnetic disks. Thus, lookups run much faster with\ndata on SSDs , compared to data on magnetic disks.\nThe performance of write operations is more complicated with \ufb02ash storage. An\nimportant di\ufb00erence between \ufb02ash storage and magnetic disks is that \ufb02ash storage does\nnot permit in-place updates to data at the physical level, although it appears to do so\nlogically. Every update turns into a copy+write of an entire \ufb02ash-storage page, requiring\nthe old copy of the page to be erased subsequently. A new page can be written in 20 to\n100 microseconds, but eventually old pages need to be erased to free up the pages for\nfurther writes. Erases are done at the level of blocks containing multiple pages, and a\nblock erase takes 2 to 5 milliseconds.\nThe optimum B+-tree node size for \ufb02ash storage is smaller than that with magnetic\ndisk, since \ufb02ash pages are smaller than disk blocks; it makes sense for tree-node sizes\nto match to \ufb02ash pages, since larger nodes would lead to multiple page writes when a\nnode is updated. Although smaller pages lead to taller trees and more I/Ooperations\nto access data, random page reads are so much faster with \ufb02ash storage that the overall\nimpact on read performance is quite small.\nAlthough random I/Ois much cheaper with SSDs than with magnetic disks, bulk\nloading still provides signi\ufb01cant performance bene\ufb01ts, compared to tuple-at-a-time in-\nsertion, with SSDs. In particular, bottom-up construction reduces the number of page\nwrites compared to tuple-at-a-time insertion, even if the entries are sorted on the search\nkey. Since page writes on \ufb02ash cannot be done in place and require relatively expen-\nsive block erases at a later point in time, the reduction of number of page writes with\nbottom-up B+-tree construction provides signi\ufb01cant performance bene\ufb01ts.\nSeveral extensions and alternatives to B+-trees have been proposed for \ufb02ash stor-\nage, with a focus on reducing the number of erase operations that result due to page\nrewrites. One approach is to add bu\ufb00ers to internal nodes of B+-trees and record up-\ndates temporarily in bu\ufb00ers at higher levels, pushing the updates down to lower levels\nlazily. The key idea is that when a page is updated, multiple updates are applied to-\ngether, reducing the number of page writes per update. Another approach creates mul-\ntiple trees and merges them; the log-structured merge tree and its variants are based on\nthis idea. In fact, both these approaches are also useful for reducing the cost of writes\non magnetic disks; we outline both these approaches in Section 14.8.\n14.4.7 Indexing in Main Memory\nMain memory today is large and cheap enough that many organizations can a\ufb00ord\nto buy enough main memory to \ufb01t all their operational data in-memory. B+-trees can\n", "686": "658 Chapter 14 Indexing\nbe used to index in-memory data, with no change to the structure. However, some\noptimizations are possible.\nFirst, since memory is costlier than disk space, internal data structures in main\nmemory databases have to be designed to reduce space requirements. Techniques that\nwe saw in Section 14.4.1 to improve B+-tree storage utilization can be used to reduce\nmemory usage for in-memory B+-trees.\nData structures that require traversal of multiple pointers are acceptable for in-\nmemory data, unlike in the case of disk-based data, where the cost of the I/Ost ot r a -\nverse multiple pages would be excessively high. Thus, tree structures in main memory\ndatabases can be relatively deep, unlike B+-trees.\nThe speed di\ufb00erence between cache memory and main memory, and the fact that\ndata are transferred between main memory and cache in units of a cache-line (typically\nabout 64 bytes), results in a situation where the relationship between cache and main\nmemory is not dissimilar to the relationship between main memory and disk (although\nwith smaller speed di\ufb00erences). When reading a memory location, if it is present in\ncache the CPU can complete the read in 1 or 2 nanoseconds, whereas a cache miss\nresults in about 50 to 100 nanoseconds of delay to read data from main memory.\nB+-trees with small nodes that \ufb01t in a cache line have been found to provide very\ngood performance with in-memory data. Such B+-trees allow index operations to be\ncompleted with far fewer cache misses than tall, skinny tree structures such as binary\ntrees, since each node traversal is likely to result in a cache miss. Compared to B+-trees\nwith nodes that match cache lines, trees with large nodes also tend to have more cache\nmisses since locating data within a node requires either a full scan of the node content,\nspanning multiple cache lines, or a binary search, which also results in multiple cache\nmisses.\nFor databases where data do not \ufb01t entirely in memory, but frequently used data\nare often memory resident, the following idea is used to create B+-tree structures that\no\ufb00er good performance on disk as well as in-memory. Large nodes are used to optimize\ndisk-based access, but instead of treating data in a node as single large array of keys and\npointers, the data within a node are structured as a tree, with smaller nodes that match\nthe size of a cache line. Instead of scanning data linearly or using binary search within\na node, the tree-structure within the large B+- t r e en o d ei su s e dt oa c c e s st h ed a t aw i t h\na minimal number of cache misses.\n14.5 Hash Indices\nHashing is a widely used technique for building indices in main memory; such indices\nmay be transiently created to process a join operation (as we will see in Section 15.5.5)\nor may be a permanent structure in a main memory database. Hashing has also been\nused as a way of organizing records in a \ufb01le, although hash \ufb01le organizations are not\nvery widely used. We initially consider only in-memory hash indices, and we consider\ndisk-based hashing later in this section.\n", "687": "14.5 Hash Indices 659\nIn our description of hashing, we shall use the term bucket to denote a unit of\nstorage that can store one or more records. For in-memory hash indices, a bucket could\nbe a linked list of index entries or records. For disk-based indices, a bucket would be a\nlinked list of disk blocks. In a hash \ufb01le organization , instead of record pointers, buckets\nstore the actual records; such structures only make sense with disk-resident data. The\nrest of our description does not depend on whether the buckets store record pointers\nor actual records.\nFormally, let Kdenote the set of all search-key values, and let Bdenote the set\nof all bucket addresses. A hash function his a function from KtoB.L e t hdenote a\nhash function. With in-memory hash indices, the set of buckets is simply an array of\npointers, with the ith bucket at o\ufb00set i. Each pointer stores the head of a linked list\ncontaining the entries in that bucket.\nTo insert a record with search key Ki,w ec o m p u t e h(Ki), which gives the address\nof the bucket for that record. We add the index entry for the record to the list at o\ufb00set\ni. Note that there are other variants of hash indices that handle the case of multiple\nrecords in a bucket di\ufb00erently; the form described here is the most widely used variant\nand is called over\ufb02ow chaining .\nHash indexing using over\ufb02ow chaining is also called closed addressing (or, less\ncommonly, closed hashing ). An alternative hashing scheme called open addressing is\nused in some applications, but is not suitable for most database indexing applications\nsince open addressing does not support deletes e\ufb03ciently. We do not consider it further.\nHash indices e\ufb03ciently support equality queries on search keys. To perform a\nlookup on a search-key value Ki, we simply compute h(Ki), then search the bucket with\nthat address. Suppose that two search keys, K5and K7, have the same hash value; that\nis,h(K5)=h(K7). If we perform a lookup on K5,t h eb u c k e t h(K5) contains records\nwith search-key values K5and records with search-key values K7. Thus, we have to check\nthe search-key value of every record in the bucket to verify that the record is one that\nwe want.\nUnlike B+-tree indices, hash indices do not support range queries; for example, a\nquery that wishes to retrieve all search key values vsuch that l\u2264v\u2264ucannot be\ne\ufb03ciently answered using a hash index.\nDeletion is equally straightforward. If the search-key value of the record to be\ndeleted is Ki,w ec o m p u t e h(Ki), then search the corresponding bucket for that record\nand delete the record from the bucket. With a linked list representation, deletion from\nthe linked list is straightforward.\nIn a disk-based hash index, when we insert a record, we locate the bucket by using\nhashing on the search key, as described earlier. Assume for now that there is space in the\nbucket to store the record. Then, the record is stored in that bucket. If the bucket does\nnot have enough space, a bucket over\ufb02ow is said to occur. We handle bucket over\ufb02ow by\nusing over\ufb02ow buckets . If a record must be inserted into a bucket b,a n d bis already full,\nthe system provides an over\ufb02ow bucket for band inserts the record into the over\ufb02ow\nbucket. If the over\ufb02ow bucket is also full, the system provides another over\ufb02ow bucket,\nand so on. All the over\ufb02ow buckets of a given bucket are chained together in a linked\n", "688": "660 Chapter 14 Indexing\nover\ufb02ow buckets for bucket 1bucket 0\nbucket 1\nbucket 2\nbucket 3\nFigure 14.25 Overflow chaining in a disk-based hash structure.\nlist, as in Figure 14.25. With over\ufb02ow chaining, given search key k, the lookup algorithm\nmust then search not only bucket h(k), but also the over\ufb02ow buckets linked from bucket\nh(k).\nBucket over\ufb02ow can occur if there are insu\ufb03cient buckets for the given number of\nrecords. If the number of records that are indexed is known ahead of time, the required\nnumber of buckets can be allocated; we will shortly see how to deal with situations\nwhere the number of records becomes signi\ufb01cantly more than what was initially antic-\nipated. Bucket over\ufb02ow can also occur if some buckets are assigned more records than\nare others, resulting in one bucket over\ufb02owing even when other buckets still have a lot\nof free space.\nSuch skew in the distribution of records can occur if multiple records may have\nthe same search key. But even if there is only one record per search key, skew may\noccur if the chosen hash function results in nonuniform distribution of search keys.\nThis chance of this problem can be minimized by choosing hash functions carefully,\nto ensure the distribution of keys across buckets is uniform and random. Nevertheless,\nsome skew may occur.\nSo that the probability of bucket over\ufb02ow is reduced, the number of buckets is\nchosen to be ( nr\u2215fr)\u2217(1+d), where nrdenotes the number of records, frdenotes the\nnumber of records per bucket, dis a fudge factor, typically around 0 .2. With a fudge\nfactor of 0 .2, about 20 percent of the space in the buckets will be empty. But the bene\ufb01t\nis that the probability of over\ufb02ow is reduced.\nDespite allocation of a few more buckets than required, bucket over\ufb02ow can still\noccur, especially if the number of records increases beyond what was initially expected.\n", "689": "14.6 Multiple-Key Access 661\nHash indexing as described above, where the number of buckets is \ufb01xed when the\nindex is created, is called static hashing . One of the problems with static hashing is that\nwe need to know how many records are going to be stored in the index. If over time a\nlarge number of records are added, resulting in far more records than buckets, lookups\nwould have to search through a large number of records stored in a single bucket, or in\none or more over\ufb02ow buckets, and would thus become ine\ufb03cient.\nTo handle this problem, the hash index can be rebuilt with an increased number of\nbuckets. For example, if the number of records becomes twice the number of buckets,\nthe index can be rebuilt with twice as many buckets as before. However, rebuilding the\nindex has the drawback that it can take a long time if the relations are large, causing\ndisruption of normal processing. Several schemes have been proposed that allow the\nnumber of buckets to be increased in a more incremental fashion. Such schemes are\ncalled dynamic hashing techniques; the linear hashing technique and the extendable\nhashing technique are two such schemes; see Section 24.5 for further details of these\ntechniques.\n14.6 Multiple-Key Access\nUntil now, we have assumed implicitly that only one index on one attribute is used to\nprocess a query on a relation. However, for certain types of queries, it is advantageous\nto use multiple indices if they exist, or to use an index built on a multiattribute search\nkey.\n14.6.1 Using Multiple Single-Key Indices\nAssume that the instructor \ufb01le has two indices: one for dept\n name and one for salary .\nConsider the following query: \u201cFind all instructors in the Finance department with\nsalary equal to $80,000.\u201d We write\nselect ID\nfrom instructor\nwhere dept\n name = 'Finance' and salary = 80000;\nThere are three strategies possible for processing this query:\n1.Use the index on dept\n name to \ufb01nd all records pertaining to the Finance depart-\nment. Examine each such record to see whether salary = 80000.\n2.Use the index on salary to \ufb01nd all records pertaining to instructors with salary\nof $80,000. Examine each such record to see whether the department name is\n\u201cFinance\u201d.\n", "690": "662 Chapter 14 Indexing\n3.Use the index on dept\n name to \ufb01nd pointers to all records pertaining to the Fi-\nnance department. Also, use the index on salary to \ufb01nd pointers to all records\npertaining to instructors with a salary of $80,000. Take the intersection of these\ntwo sets of pointers. Those pointers that are in the intersection point to records\npertaining to instructors of the Fina nce department and with salary of $80,000.\nThe third strategy is the only one of the three that takes advantage of the existence of\nmultiple indices. However, even this strategy may be a poor choice if all of the following\nhold:\n\u2022There are many records pertaining to the Finance department.\n\u2022There are many records pertaining to instructors with a salary of $80,000.\n\u2022There are only a few records pertaining to both the Finance department and in-\nstructors with a salary of $80,000.\nIf these conditions hold, we must scan a lar ge number of pointers to produce a small\nresult. An index structure called a \u201cbitmap index\u201d can in some cases greatly speed up\nthe intersection operation used in the third strategy. Bitmap indices are outlined in\nSection 14.9.\n14.6.2 Indices on Multiple Keys\nAn alternative strategy for this case is to create and use an index on a composite search\nkey ( dept\n name ,salary )\u2014that is, the search key consisting of the department name\nconcatenated with the instructor salary.\nWe can use an ordered (B+-tree) index on the preceding composite search key to\nanswer e\ufb03ciently queries of the form\nselect ID\nfrom instructor\nwhere dept\n name = 'Finance' and salary = 80000;\nQueries such as the following query, which speci\ufb01es an equality condition on the \ufb01rst\nattribute of the search key ( dept\n name ) and a range on the second attribute of the search\nkey ( salary ), can also be handled e\ufb03ciently since they correspond to a range query on\nthe search attribute.\nselect ID\nfrom instructor\nwhere dept\n name = 'Finance' and salary <80000;\nW ec a ne v e nu s ea no r d e r e di n d e xo nt h es e a r c hk e y( dept\n name ,salary ) to answer the\nfollowing query on only one attribute e\ufb03ciently:\n", "691": "14.6 Multiple-Key Access 663\nselect ID\nfrom instructor\nwhere dept\n name = 'Finance';\nAn equality condition dept\n name = \u201cFinance\u201d is equivalent to a range query on the\nrange with lower end (Finance, \u2212\u221e) and upper end (Finance, +\u221e). Range queries on\njust the dept\n name attribute can be handled in a similar manner.\nThe use of an ordered-index structure on a composite search key, however, has a\nfew shortcomings. As an illustration, consider the query\nselect ID\nfrom instructor\nwhere dept\n name <'Finance' and salary <80000;\nWe can answer this query by using an ordered index on the search key ( dept\n name ,\nsalary ): For each value of dept\n name that is less than \u201cFinance\u201d in alphabetic order, the\nsystem locates records with a salary value of 80000. However, each record is likely to\nbe in a di\ufb00erent disk block, because of the ordering of records in the \ufb01le, leading to\nmany I/Ooperations.\nThe di\ufb00erence between this query and the previous two queries is that the condition\non the \ufb01rst attribute ( dept\n name ) is a comparison condition, rather than an equality\ncondition. The condition does not correspond to a range query on the search key.\nTo speed the processing of general composite search-key queries (which can involve\none or more comparison operations), we can use several special structures. We shall\nconsider bitmap indices in Section 14.9. There is another structure, called the R-tree ,\nthat can be used for this purpose. The R-tree is an extension of the B+-tree to handle\nindexing on multiple dimensions and is discussed in Section 14.10.1.\n14.6.3 Covering Indices\nCovering indices are indices that store the values of some attributes (other than the\nsearch-key attributes) along with the pointers to the record. Storing extra attribute val-\nues is useful with secondary indices, since they allow us to answer some queries using\njust the index, without even looking up the actual records.\nFor example, suppose that we have a nonclustering index on the IDattribute of the\ninstructor relation. If we store the value of the salary attribute along with the record\npointer, we can answer queries that require the salary (but not the other attribute, dept\nname ) without accessing the instructor record.\nThe same e\ufb00ect could be obtained by creating an index on the search key ( ID,\nsalary ) ,b u tac o v e r i n gi n d e xr e d u c e st h es i z eo ft h es e a r c hk e y ,a l l o w i n gal a r g e rf a n o u t\nin the nonleaf nodes, and potentially reducing the height of the index.\n", "692": "664 Chapter 14 Indexing\n14.7 Creation of Indices\nAlthough the SQL standard does not specify any speci\ufb01c syntax for creation of indices,\nmost databases support SQL commands to create and drop indices. As we saw in Sec-\ntion 4.6, indices can be created using the following syntax, which is supported by most\ndatabases.\ncreate index <index-name >on<relation-name >(<attribute-list >);\nThe attribute-list is the list of attributes of the relations that form the search key for the\nindex. Indices can be dropped using a command of the form\ndrop index <index-name >;\nFor example, to de\ufb01ne an index named dept\n index on the instructor relation with\ndept\n name a st h es e a r c hk e y ,w ew r i t e :\ncreate index dept\n index oninstructor (dept\n name );\nTo declare that an attribute or list of attributes is a candidate key, we can use the\nsyntax create unique index in place of create index above. Databases that support mul-\ntiple types of indices also allow the type of index to be speci\ufb01ed as part of the index\ncreation command. Refer to the manual of your database system to \ufb01nd out what index\ntypes are available, and the syntax for specifying the index type.\nWhen a user submits an SQL query that can bene\ufb01t from using an index, the SQL\nquery processor automatically uses the index.\nIndices can be very useful on attributes that participate in selection conditions or\njoin conditions of queries, since they can reduce the cost of queries signi\ufb01cantly. Con-\nsider a query that retrieves takes records for a particular student ID12345 (expressed\nin relational algebra as \u03c3ID=12345(takes )). If there were an index on the IDattribute of\ntakes , pointers to the required records could be obtained with only a few I/Oopera-\ntions. Since students typically only take a few tens of courses, even fetching the actual\nrecords would take only a few tens of I/Ooperations subsequently. In contrast, in the\nabsence of this index, the database system would be forced to read all takes records and\nselect those with matching IDvalues. Reading an entire relation can be very expensive\nif there are a large number of students.\nHowever, indices do have a cost, since they have to be updated whenever there is an\nupdate to the underlying relation. Creating too many indices would slow down update\nprocessing, since each update would have to also update all a\ufb00ected indices.\nSometimes performance problems are apparent during testing, for example, if a\nquery takes tens of seconds, it is clear that it is quite slow. However, suppose each\nquery takes 1 second to scan a large relation without an index, versus 10 milliseconds\nto retrieve the same records using an index. If testers run one query at a time, queries\n", "693": "14.8 Write-Optimized Index Structures 665\nrespond quickly, even without an index. However, suppose that the queries are part of\na registration system that is used by a thousand students in an hour, and the actions\nof each student require 10 such queries to be executed. The total execution time would\nthen be 10,000 seconds for queries submitted in 1 hour, that is, 3600 seconds. Students\nare then likely to \ufb01nd that the registration system is extremely slow, or even totally unre-\nsponsive. In contrast, if the required indices were present, the execution time required\nwould be 100 seconds for queries submitted in 1 hour, and the performance of the\nregistration system would be very good.\nIt is therefore important when building an application to \ufb01gure out which indices\nare important for performance and to create them before the application goes live.\nIf a relation is declared to have a primary key, most database systems automatically\ncreate an index on the primary key. Whenever a tuple is inserted into the relation, the\nindex can be used to check that the primary-key constraint is not violated (i.e., there\nare no duplicates on the primary-key value). Without the index on the primary key,\nwhenever a tuple is inserted, the entire relation has to be scanned to ensure that the\nprimary-key constraint is satis\ufb01ed.\nAlthough most database systems do not automatically create them, it is often\na good idea to create indices on foreign-key attributes, too. Most joins are between\nforeign-key and primary-key attributes, and queries containing such joins, where there\nis also a selection condition on the referenced table, are not uncommon. Consider a\nquery takes\u22c8\u03c3name=Shankar (student ), where the foreign-key attribute IDoftakes refer-\nences the primary-key attribute IDof student. Since very few students are likely to be\nnamed Shankar, the index on the foreign-key attribute takes .IDcan be used to e\ufb03ciently\nretrieve the takes tuples corresponding to these students.\nMany database systems provide tools that help database administrators track what\nqueries and updates are being executed on the system and recommend the creation\nof indices depending on the frequencies of the queries and updates. Such tools are\nreferred to as index tuning wizards or advisors.\nSome recent cloud-based database systems also support completely automated cre-\nation of indices whenever the system \ufb01nds that doing so would avoid repeated relation\nscans, without the intervention of a database administrator.\n14.8 Write-Optimized Index Structures\nO n eo ft h ed r a w b a c k so ft h eB+-tree index structure is that performance can be quite\npoor with random writes. Consider an index that is too large to \ufb01t in memory; since\nthe bulk of the space is at the leaf level, and memory sizes are quite large these days,\nwe assume for simplicity that higher levels of the index \ufb01t in memory.\nNow suppose writes or inserts are done in an order that does not match the sort\norder of the index. Then, each write/insert is likely to touch a di\ufb00erent leaf node; if the\nnumber of leaf nodes is signi\ufb01cantly larger than the bu\ufb00er size, most of these leaf ac-\nc e s s e sw o u l dr e q u i r ear a n d o mr e a do p e r a t i o n ,a sw e l la sas u b s e q u e n tw r i t eo p e r a t i o n\n", "694": "666 Chapter 14 Indexing\nto write the updated leaf page back to disk. On a system with a magnetic disk, with a\n10-millisecond access time, the index would support not more than 100 writes/inserts\nper second per disk; and this is an optimistic estimate, assuming that the seek takes\nthe bulk of the time, and the head has not moved between the read and the write of a\nleaf page. On a system with \ufb02ash based SSDs, random I/Ois much faster, but a page\nwrite still has a signi\ufb01cant cost since it (eventually) requires a page erase, which is an\nexpensive operation. Thus, the basic B+-tree structure is not ideal for applications that\nneed to support a very large number of random writes/inserts per second.\nSeveral alternative index structures have been proposed to handle workloads with\na high write/insert rate. The log-structured merge tree orLSM tree and its variants are\nwrite-optimized index structures that have seen very signi\ufb01cant adoption. The bu\ufb00er\ntree is an alternative approach, which can be used with a variety of search tree struc-\ntures. We outline these structures in the rest of this section.\n14.8.1 LSM Trees\nAnLSM tree consists of several B+-trees, starting with an in-memory tree, called L0,\nand on-disk trees L1,L2,\u2026,Lkfor some k,w h e r e kis called the level. Figure 14.26\ndepicts the structure of an LSM tree for k=3.\nAn index lookup is performed by using separate lookup operations on each of\nthe trees L0,\u2026,Lk, and merging the results of the lookups. (We assume for now that\nthere are only inserts, and no updates or deletes; index lookups in the presence of\nupdates/deletes are more complicated and are discussed later.)\nWhen a record is \ufb01rst inserted into an LSM tree, it is inserted into the in-memory\nB+-tree structure L0. A fairly large amount of memory space is allocated for this tree.\nThe tree grows as more inserts are processed, until it \ufb01lls the memory allocated to it.\nAt this point, we need to move data from the in-memory structure to a B+-tree on disk.\nL0\nL1\nL2\nL3Memory\nDisk\nFigure 14.26 Log-structured merge tree with three levels.\n", "695": "14.8 Write-Optimized Index Structures 667\nIf tree L1is empty, the entire in-memory tree L0is written to disk to create the\ninitial tree L1.H o w e v e r ,i f L1is not empty, the leaf level of L0is scanned in increasing\nkey order, and entries are merged with the leaf level entries of L1(also scanned in\nincreasing key order). The merged entries are used to create a new B+-tree, using the\nbottom-up build process. The new tree with the merged entries then replaces the old\nL1. In either case, after entries of L0have been moved to L1, all entries in L0as well\nas the old L1, if it existed, are deleted. Inserts can then be made to the now empty L0\nin-memory.\nNote that all entries in the leaf level of the old L1tree, including those in leaf nodes\nthat do not have any updates, are copied to the new tree instead of performing updates\non the existing L1tree node. This gives the following bene\ufb01ts.\n1.The leaves of the new tree are sequentially located, avoiding random I/Oduring\nsubsequent merges.\n2.The leaves are full, avoiding the overhead of partially occupied leaves that can\noccur with page splits.\nThere is, however, a cost to using the LSM structure as described above: the entire\ncontents of the tree are copied each time a set of entries from L0are copied into L1.\nOne of two techniques is used to reduce this cost:\n1.Multiple levels are used, with level Li+1trees having a maximum size that is k\ntimes the maximum size of level Litrees. Thus, each record is written at most k\ntimes at a particular level. The number of levels is proportional logk(I\u2215M)w h e r e\nIis the number of entries and Mis the number of entries that \ufb01t in the in-memory\ntree L0.\n2.Each level (other than L0) can have up to some number bof trees, instead of\njust 1 tree. When an L0tree is written to disk, a new L1tree is created instead\nof merging it with an existing L1tree. When there are bsuch L1trees, they are\nmerged into a single new L2tree. Similarly, when there are btrees at level Lithey\nare merged into a new Li+1tree.\nThis variant of the LSM tree is called a stepped-merge index . The stepped-\nmerge index decreases the insert cost signi\ufb01cantly compared to having only one\ntree per level, but it can result in an increase in query cost, since multiple trees\nmay need to be searched. Bitmap-based structures called Bloom \ufb01lters , described\nin Section 24.1, are used to reduce the number of lookups by e\ufb03ciently detecting\nthat a search key is not present in a particular tree. Bloom \ufb01lters occupy very\nlittle space, but they are quite e\ufb00ective at reducing query cost.\nDetails of all these variants of LSM trees can be found in Section 24.2.\nSo far we have only described inserts and lookups. Deletes are handled in an in-\nteresting manner. Instead of directly \ufb01nding an index entry and deleting it, deletion\n", "696": "668 Chapter 14 Indexing\nresults in insertion of a new deletion entry that indicates which index entry is to be\ndeleted. The process of inserting a deletion entry is identical to the process of inserting\nan o r m a li n d e xe n t r y .\nHowever, lookups have to carry out an extra step. As mentioned earlier, lookups\nretrieve entries from all the trees and merge them in sorted order of key value. If there\nis a deletion entry for some entry, both of them would have the same key value. Thus,\na lookup would \ufb01nd both the deletion entry and the original entry for that key, which\nis to be deleted. If a deletion entry is found, the to-be-deleted entry is \ufb01ltered out and\nnot returned as part of the lookup result.\nWhen trees are merged, if one of the trees contains an entry, and the other had\na matching deletion entry, the entries get matched up during the merge (both would\nhave the same key), and are both discarded.\nUpdates are handled in a manner similar to deletes, by inserting an update entry.\nLookups need to match update entries with the original entries and return the latest\nvalue. The update is actually applied during a merge, when one tree has an entry and\nanother has its matching update entry; the merge process would \ufb01nd a record and an\nupdate record with the same key, apply the update, and discard the update entry.\nLSM trees were initially designed to reduce the write and seek overheads of mag-\nnetic disks. Flash based SSDs have a relatively low overhead for random I/Ooperations\nsince they do not require seek, and thus the bene\ufb01t of avoiding random I/Othat LSM\ntree variants provide is not particularly important with SSDs.\nHowever, recall that \ufb02ash memory does not allow in-place update, and writing even\nas i n g l eb y t et oap a g er e q u i r e st h ew h o l ep a g et ob er e w r i t t e nt oan e wp h y s i c a ll o c a t i o n ;\nthe original location of the page needs to be erased eventually, which is a relatively\nexpensive operation. The reduction in number of writes using LSM tree variants, as\ncompared to traditional B+-trees, can provide substantial performance bene\ufb01ts when\nLSM trees are used with SSDs.\nAv a r i a n to ft h e LSM tree similar to the stepped-merge index, with multiple trees in\neach layer, was used in Google\u2019s BigTable system, as well as in Apache HBase, the open\nsource clone of BigTable. These systems are built on top of distributed \ufb01le systems that\nallow appends to \ufb01les but do not support updates to existing data. The fact that LSM\ntrees do not perform in-place update made LSM trees a very good \ufb01t for these systems.\nSubsequently, a large number of BigData storage systems such as Apache Cas-\nsandra, Apache AsterixDB, and MongoDB added support for LSM trees, with most\nimplementing versions with multiple trees in each layer. LSM trees are also supported\nin MySQL (using the MyRocks storage engine) and in the embedded database systems\nSQLite4 and LevelDB.\n14.8.2 Buffer Tree\nThe bu\ufb00er tree is an alternative to the log-structured merge tree approach. The key\nidea behind the bu\ufb00er tree is to associate a bu\ufb00er with each internal node of a B+-tree,\n", "697": "14.8 Write-Optimized Index Structures 669\nBu\ufb00erInternal node\np6 k5p5 k4p4 k3p3p2p1 k2 k1\nFigure 14.27 Structure of an internal node of a buffer tree.\nincluding the root node; this is depicted pictorially in Figure 14.27. We \ufb01rst outline how\ninserts and lookups are handled, and subsequently we outline how deletes and updates\nare handled.\nWhen an index record is inserted into the bu\ufb00er tree, instead of traversing the tree\nto the leaf, the index record is inserted into the bu\ufb00er of the root. If the bu\ufb00er becomes\nfull, each index record in the bu\ufb00er is pushed one level down the tree to the appropriate\nchild node. If the child node is an internal node, the index record is added to the child\nnode\u2019s bu\ufb00er; if that bu\ufb00er is full, all records in that bu\ufb00er are similarly pushed down.\nAll records in a bu\ufb00er are sorted on the search key before being pushed down. If the\nchild node is a leaf node, index records are inserted into the leaf in the usual manner. If\nthe insert results in an overfull leaf node, the node is split in the usual B+-tree manner,\nwith the split potentially propagating to parent nodes. Splitting of an overfull internal\nnode is done in the usual way, with the additional step of also splitting the bu\ufb00er; the\nbu\ufb00er entries are partitioned between the two split nodes based on their key values.\nLookups are done by traversing the B+-tree structure in the usual way, to \ufb01nd leaves\nthat contain records matching the lookup key. But there is one additional step: at each\ninternal node traversed by a lookup, the node\u2019s bu\ufb00er must be examined to see if there\nare any records matching the lookup key. Range lookups are done as in a normal B+-\ntree, but they must also examine the bu\ufb00ers of all internal nodes above any of the leaf\nnodes that are accessed.\nSuppose the bu\ufb00er at an internal node holds ktimes as many records as there are\nchild nodes. Then, on average, krecords would be pushed down at a time to each child\n(regardless of whether the child is an internal node or a leaf node). Sorting of records\nbefore they are pushed ensures that all these records are pushed down consecutively.\nThe bene\ufb01t of the bu\ufb00er-tree approach for inserts is that the cost of accessing the child\nnode from storage, and of writing the updated node back, is amortized (divided), on\naverage, between krecords. With su\ufb03ciently large k, the savings can be quite signi\ufb01cant\ncompared to inserts in a regular B+-tree.\nDeletes and updates can be processed in a manner similar to LSM trees, using\ndeletion entries or update entries. Alternatively, deletes and updates could be processed\nusing the normal B+-tree algorithms, at the risk of a higher I/Ocost per delete/update\nas compared to the cost when using deletion/update entries.\nBu\ufb00er trees provide better worst-case complexity bounds on the number of I/O\noperations than do LSM tree variants. In terms of read cost, bu\ufb00er trees are signi\ufb01cantly\nfaster than LSM trees. However, write operations on bu\ufb00er trees involve random I/O,\n", "698": "670 Chapter 14 Indexing\nrequiring more seeks, in contrast to sequential I/Ooperations with LSM tree variants.\nFor magnetic disk storage, the high cost of seeks results in bu\ufb00er trees performing\nworse than LSM trees on write-intensive workloads. LSM trees have thus found greater\nacceptance for write-intensive workloads with data stored on magnetic disk. However,\nsince random I/Ooperations are very e\ufb03cient on SSDs, and bu\ufb00er trees tend to perform\nfewer write operations overall compared to LSM trees, bu\ufb00er trees can provide better\nwrite performance on SSDs. Several index structures designed for \ufb02ash storage make\nuse of the bu\ufb00er concept introduced by bu\ufb00er trees.\nAnother bene\ufb01t of bu\ufb00er trees is that the key idea of associating bu\ufb00ers with inter-\nnal nodes, to reduce the number of writes, can be used with any type of tree-structured\nindex. For example, bu\ufb00ering has been used as a way of supporting bulk loading of spa-\ntial indices such as R-trees (which we study in Section 14.10.1), as well as other types\nof indices, for which sorting and bottom-up construction are not applicable.\nBu\ufb00er trees have been implemented as part of the Generalized Search Tree (GiST )\nindex structure in Postgre SQL. The GiST index allows user-de\ufb01ned code to be executed\nto implement search, update, and split operations on nodes and has been used to im-\nplement R-trees and other spatial index structures.\n14.9 Bitmap Indices\nBitmap indices are a specialized type of index designed for easy querying on multiple\nkeys, although each bitmap index is built on a single key. We describe key features of\nbitmap indices in this section but provide further details in Section 24.3.\nFor bitmap indices to be used, records in a relation must be numbered sequentially,\ns t a r t i n gf r o m ,s a y ,0 .G i v e nan u m b e r n, it must be easy to retrieve the record numbered\nn. This is particularly easy to achieve if records are \ufb01xed in size and allocated on con-\nsecutive blocks of a \ufb01le. The record number can then be translated easily into a block\nnumber and a number that identi\ufb01es the record within the block.\nConsider a relation with an attribute that can take on only one of a small number\n(e.g., 2 to 20) of values. For instance, consider a relation instructor\n info, which has (in\naddition to an IDattribute) an attribute gender , which can take only values m(male)\norf(female). Suppose the relation also has an attribute income\n level,w h i c hs t o r e st h e\nincome level, where income has been broken up into \ufb01ve levels: L1: 0\u20139999, L2: 10, 000\n\u201319, 999, L3: 20, 000\u201339, 999, L4: 40, 000\u201374, 999, and L5: 75, 000 \u2212\u221e . Here, the\nraw data can take on many values, but a data analyst has split the values into a small\nnumber of ranges to simplify analysis of the data. An instance of this relation is shown\no nt h el e f ts i d eo fF i g u r e1 4 . 2 8 .\nAbitmap is simply an array of bits. In its simplest form, a bitmap index on the\nattribute Aof relation rconsists of one bitmap for each value that Acan take. Each\nbitmap has as many bits as the number of records in the relation. The ith bit of the\nbitmap for value vjis set to 1 if the record numbered ihas the value vjfor attribute A.\nAll other bits of the bitmap are set to 0.\n", "699": "14.9 Bitmap Indices 671\nID income_level gender\n76766\n22222\n12121\n15151\n58583mm\nff\nfL1L1\nL2\nL4\nL3record\nnumber\n10\n2\n3\n4m\nfBitmaps for gender\n10010\n01101Bitmaps for\nincome_level\nL1\nL2\nL3\nL4\nL510100\n01000\n00001\n00010\n00000\nFigure 14.28 Bitmap indices on relation instructor\n info.\nIn our example, there is one bitmap for the value mand one for f.T h e ith bit of\nthe bitmap for mis set to 1 if the gender value of the record numbered iism. All other\nbits of the bitmap for mare set to 0. Similarly, the bitmap for fhas the value 1 for bits\ncorresponding to records with the value ffor the gender attribute; all other bits have the\nvalue 0. Figure 14.28 shows bitmap indices on the gender and income\n level attributes\nofinstructor\n inforelation, for the relation instance shown in the same \ufb01gure.\nWe now consider when bitmaps are useful. The simplest way of retrieving all\nrecords with value m(or value f) would be to simply read all records of the relation and\nselect those records with value m(orf, respectively). The bitmap index doesn\u2019t really\nhelp to speed up such a selection. While it would allow us to read only those records\nfor a speci\ufb01c gender, it is likely that every disk block for the \ufb01le would have to be read\nanyway.\nIn fact, bitmap indices are useful for selections mainly when there are selections\non multiple keys. Suppose we create a bitmap index on attribute income\n level,w h i c h\nwe described earlier, in addition to the bitmap index on gender .\nConsider now a query that selects women with income in the range 10,000 to\n19,999. This query can be expressed as\nselect *\nfrom instructor\n info\nwhere gender =' f 'and income\n level =' L 2 ' ;\nTo evaluate this selection, we fetch the bitmaps for gender value fand the bitmap for\nincome\n level value L2, and perform an intersection (logical-and) of the two bitmaps. In\nother words, we compute a new bitmap where bit ih a sv a l u e1i ft h e ith bit of the two\nbitmaps are both 1, and has a value 0 otherwise. In the example in Figure 14.28, the\nintersection of the bitmap for gender=\ud835\uddbf(01101) and the bitmap for income\n level=L2\n(01000) gives the bitmap 01000.\n", "700": "672 Chapter 14 Indexing\nSince the \ufb01rst attribute can take two values, and the second can take \ufb01ve values, we\nwould expect only about 1 in 10 records, on an average, to satisfy a combined condition\non the two attributes. If there are further conditions, the fraction of records satisfying\nall the conditions is likely to be quite small. The system can then compute the query\nresult by \ufb01nding all bits with value 1 in the intersection bitmap and retrieving the cor-\nresponding records. If the fraction is large, scanning the entire relation would remain\nthe cheaper alternative.\nMore detailed coverage of bitmap indices, including how to e\ufb03ciently implement\naggregate operations, how to speed up bitmap operations, and hybrid indices that com-\nbine B+-trees with bitmaps, can be found in Section 24.3.\n14.10 Indexing of Spatial and Temporal Data\nTraditional index structures, such as hash indices and B+-trees, are not suitable for\nindexing of spatial data, which are typically of two or more dimensions. Similarly,\nwhen tuples have temporal intervals associated with them, and queries may specify\ntime points or time intervals, the traditional index structures may result in poor perfor-\nmance.\n14.10.1 Indexing of Spatial Data\nIn this section we provide an overview of techniques for indexing spatial data. Further\ndetails can be found in Section 24.4. Spatial data refers to data referring to a point or\na region in two or higher dimensional space. For example, the location of restaurants,\nidenti\ufb01ed by a (latitude, longitude) pair, is a form of spatial data. Similarly, the spatial\nextent of a farm or a lake can be identi\ufb01ed by a polygon, with each corner identi\ufb01ed by\na (latitude, longitude) pair.\nThere are many forms of queries on spatial data, which need to be e\ufb03ciently sup-\nported using indices. A query that asks for restaurants at a precisely speci\ufb01ed (latitude,\nlongitude) pair can be answered by creating a B+-tree on the composite attribute (lati-\ntude, longitude). However, such a B+-tree index cannot e\ufb03ciently answer a query that\nasks for all restaurants that are within a 500-meter radius of a user\u2019s location, which\nis identi\ufb01ed by a (latitude, longitude) pair. Nor can such an index e\ufb03ciently answer a\nquery that asks for all restaurants that are within a rectangular region of interest. Both\nof these are forms of range queries , which retrieve objects within a speci\ufb01ed area. Nor\ncan such an index e\ufb03ciently answer a query that asks for the nearest restaurant to a\nspeci\ufb01ed location; such a query is an example of a nearest neighbor query.\nThe goal of spatial indexing is to support di\ufb00erent forms of spatial queries, with\nrange and nearest neighbor queries being of particular interest, since they are widely\nused.\nTo understand how to index spatial data consisting of two or more dimensions, we\nconsider \ufb01rst the indexing of points in one-dimensional data. Tree structures, such as\nbinary trees and B+-trees, operate by successively dividing space into smaller parts. For\n", "701": "14.10 Indexing of Spatial and Temporal Data 673\n31 3233\n2\nFigure 14.29 Division of space by a k-d tree.\ninstance, each internal node of a binary tree partitions a one-dimensional interval in\ntwo. Points that lie in the left partition go into the left subtree; points that lie in the right\npartition go into the right subtree. In a balanced binary tree, the partition is chosen so\nthat approximately one-half of the points stored in the subtree fall in each partition.\nSimilarly, each level of a B+-tree splits a one-dimensional interval into multiple parts.\nWe can use that intuition to create tree structures for two-dimensional space as well\nas in higher-dimensional spaces. A tree structure called a k-d tree was one of the early\nstructures used for indexing in multiple dimensions. Each level of a k-d tree partitions\nthe space into two. The partitioning is done along one dimension at the node at the top\nlevel of the tree, along another dimension in nodes at the next level, and so on, cycling\nthrough the dimensions. The partitioning proceeds in such a way that, at each node,\napproximately one-half of the points stored in the subtree fall on one side and one-\nhalf fall on the other. Partitioning stops when a node has less than a given maximum\nnumber of points.\nFigure 14.29 shows a set of points in two-dimensional space, and a k-d tree repre-\nsentation of the set of points, where the maximum number of points in a leaf node has\nbeen set at 1. Each line in the \ufb01gure (other than the outside box) corresponds to a node\nin the k-d tree. The numbering of the lines in the \ufb01gure indicates the level of the tree at\nwhich the corresponding node appears.\nRectangular range queries, which ask for points within a speci\ufb01ed rectangular re-\ngion, can be answered e\ufb03ciently using a k-d tree as follows: Such a query essentially\nspeci\ufb01es an interval on each dimension. For example, a range query may ask for all\npoints whose xdimension lies between 50 and 80, and ydimension lies between 40\nand 70. Recall that each internal node splits space on one dimension, and as in a B+-\n", "702": "674 Chapter 14 Indexing\ntree. Range search can be performed by the following recursive procedure, starting at\nthe root:\n1.Suppose the node is an internal node, and let it be split on a particular dimension,\nsayx,a tap o i n t xi. Entries in the left subtree have xvalues <xi, and those in the\nright subtree have xvalues \u2265xi. If the query range contains xi, search is recur-\nsively performed on both children. If the query range is to the left of xi,s e a r c hi s\nrecursively performed only on the left child, and otherwise it is performed only\non the right subtree.\n2.If the node is a leaf, all entries that are contained in the query range are retrieved.\nNearest neighbor search is more complicated, and we shall not describe it here, but\nnearest neighbor queries can also be answered quite e\ufb03ciently using k-d trees.\nThek-d-B tree extends the k-d tree to allow multiple child nodes for each internal\nnode, just as a B-tree extends a binary tree, to reduce the height of the tree. k-d-B trees\nare better suited for secondary storage than k-d trees. Range search as outlined above\ncan be easily extended to k-d-B trees, and nearest neighbor queries too can be answered\nquite e\ufb03ciently using k-d-B trees.\nThere are a number of alternative index structures for spatial data. Instead of di-\nviding the data one dimension at a time, quadtrees divide up a two-dimensional space\ninto four quadrants at each node of the tree. Details may be found in Section 24.4.1.\nIndexing of regions of space, such as line segments, rectangles, and other polygons,\npresents new problems. There are extensions of k-d trees and quadtrees for this task.\nA key idea is that if a line segment or polygon crosses a partitioning line, it is split\nalong the partitioning line and represented in each of the subtrees in which its pieces\noccur. Multiple occurrences of a line segme nt or polygon can result in ine\ufb03ciencies in\nstorage, as well as ine\ufb03ciencies in querying.\nA storage structure called an R-tree is useful for indexing of objects spanning re-\ngions of space, such as line segments, rectangles, and other polygons, in addition to\npoints. An R-tree is a balanced tree structure with the indexed objects stored in leaf\nnodes, much like a B+-tree. However, instead of a range of values, a rectangular bound-\ning box is associated with each tree node. The bounding box of a leaf node is the small-\nest rectangle parallel to the axes that contains all objects stored in the leaf node. The\nbounding box of internal nodes is, similarly, the smallest rectangle parallel to the axes\nthat contains the bounding boxes of its child nodes. The bounding box of an object\n(such as a polygon) is de\ufb01ned, similarly, as the smallest rectangle parallel to the axes\nthat contains the object.\nEach internal node stores the bounding boxes of the child nodes along with the\npointers to the child nodes. Each leaf node stores the indexed objects.\nFigure 14.30 shows an example of a set of rectangles (drawn with a solid line) and\nthe bounding boxes (drawn with a dashed line) of the nodes of an R-tree for the set of\nrectangles. Note that the bounding boxes are shown with extra space inside them, to\nmake them stand out pictorially. In reality, the boxes would be smaller and \ufb01t tightly\n", "703": "14.10 Indexing of Spatial and Temporal Data 675\nBB1 BB2 BB\nBCAE F H IA B\nC\nI\nEFH1\n23\nDG\nDG3\nFigure 14.30 An R-tree.\non the objects that they contain; that is, each side of a bounding box Bwould touch at\nleast one of the objects or bounding boxes that are contained in B.\nT h eR - t r e ei t s e l fi sa tt h er i g h ts i d eo fF i g u r e1 4 . 3 0 .T h e\ufb01 g u r er e f e r st ot h ec o o r -\ndinates of bounding box iasBBiin the \ufb01gure. More details about R-trees, including\ndetails of how to answer range queries using R-trees, may be found in Section 24.4.2.\nUnlike some alternative structures for storing polygons and line segments, such as\nR\u2217-trees and interval trees, R-trees store only one copy of each object, and we can ensure\neasily that each node is at least half full. However, querying may be slower than with\nsome of the alternatives, since multiple pat hs have to be searched. However, because of\ntheir better storage e\ufb03ciency and their similarity to B-trees, R-trees and their variants\nhave proved popular in database systems that support spatial data.\n14.10.2 Indexing Temporal Data\nTemporal data refers to data that has an associated time period, as discussed in Section\n7.10. The time period associated with a tuple indicates the period of time for which the\ntuple is valid. For example, a particular course identi\ufb01er may have its title changed at\nsome point of time. Thus, a course identi\ufb01er is associated with a title for a given time\ninterval, after which the same course identi\ufb01er is associated with a di\ufb00erent title. This\ncan be modeled by having two or more tuples in the course relation with the same course\nid,b u td i \ufb00 e r e n t titlevalues, each with its own valid time interval.\nAtime interval has a start time and an end time. Further a time interval indicates\nwhether the interval starts at the start time, or just after the start time, that is, whether\nthe interval is closed oropen at the start time. Similarly, the time interval indicates\nwhether it is closed or open at the end time. To represent the fact that a tuple is valid\ncurrently, until it is next updated, the end time is conceptually set to in\ufb01nity (which\ncan be represented by a suitably large time, such as midnight of 9999-12-31).\n", "704": "676 Chapter 14 Indexing\nIn general, the valid period for a particular fact may not consist of just one time\ninterval; for example, a student may be registered in a university one academic year,\ntake a leave of absence for the next year, and register again the following year. The\nvalid period for the student\u2019s registration at the university is clearly not a single time\ninterval. However, any valid period can be represented by multiple intervals; thus, a\ntuple with any valid period can be represented by multiple tuples, each of which has a\nvalid period that is a single time interval. We shall therefore only consider time intervals\nwhen modeling temporal data.\nSuppose we wish to retrieve the value of a tuple, given a value vfor an attribute a,\nand a point in time t1. We can create an index on the a,a n du s ei tt or e t r i e v ea l lt u p l e s\nwith value vfor attribute a. While such an index may be adequate if the number of time\nintervals for that search-key value is small, in general the index may retrieve a number\nof tuples whose time intervals do not include the time point t1.\nA better solution is to use a spatial index such as an R-tree, with the indexed tuple\ntreated as having two dimensions, one being the indexed attribute a, and the other\nbeing the time dimension. In this case, the tuple forms a line segment, with value vfor\ndimension a, and the valid time interval of the tuple as interval in the time dimension.\nO n ei s s u et h a tc o m p l i c a t e st h eu s eo fas p a t i a li n d e xs u c ha sa nR - t r e ei st h a tt h e\nend time interval may be in\ufb01nity (perhaps represented by a very large value), whereas\nspatial indices typically assume that bounding boxes are \ufb01nite, and may have poor\nperformance if bounding boxes are very large. This problem can be dealt with as follows:\n\u2022All current tuples (i.e., those with end time as in\ufb01nity, which is perhaps represented\nby a large time value) are stored in a separate index from those tuples that have a\nnon-in\ufb01nite end time. The index on current tuples can be a B+-tree index on ( a,\nstart\n time), where ais the indexed attribute and start\n time is the start time, while\nthe index for non-current tuples would be a spatial index such as an R-tree.\n\u2022Lookups for a key value vat a point in time tiwould need to search on both indices;\nthe search on the current-tuple index would be for tuples with a=v,a n d start\n ts\n\u2264ti, which can be done by a simple range query. Queries with a time range can be\nhandled similarly.\nInstead of using spatial indices that are designed for multidimensional data, one\ncan use specialized indices, such as the interval B+-tree, that are designed to index\nintervals in a single dimension, and provide better complexity guarantees than R-tree\nindices. However, most database implementations \ufb01nd it simpler to use R-tree indices\ninstead of implementing yet another type of index for time intervals.\nRecall that with temporal data, more than one tuple may have the same value for a\nprimary key, as long as the tuples with the same primary-key value have non-overlapping\ntime intervals. Temporal indices on the primary key attribute can be used to e\ufb03ciently\ndetermine if the temporal primary key constraint is violated when a new tuple is in-\nserted or the valid time interval of an existing tuple is updated.\n", "705": "14.11 Summary 677\n14.11 Summary\n\u2022Many queries reference only a small proportion of the records in a \ufb01le. To reduce\nthe overhead in searching for these records, we can construct indices for the \ufb01les\nthat store the database.\n\u2022There are two types of indices that we can use: dense indices and sparse indices.\nDense indices contain entries for every search-key value, whereas sparse indices\ncontain entries only for some search-key values.\n\u2022If the sort order of a search key matches the sort order of a relation, an index on\nthe search key is called a clustering index . The other indices are called nonclustering\norsecondary indices . Secondary indices improve the performance of queries that\nuse search keys other than the search key of the clustering index. However, they\nimpose an overhead on modi\ufb01cation of the database.\n\u2022Index-sequential \ufb01les are one of the oldest index schemes used in database systems.\nTo permit fast retrieval of records in search-key order, records are stored sequen-\ntially, and out-of-order records are chained together. To allow fast random access,\nwe use an index structure.\n\u2022The primary disadvantage of the index-sequential \ufb01le organization is that perfor-\nmance degrades as the \ufb01le grows. To overcome this de\ufb01ciency, we can use a B+-tree\nindex .\n\u2022AB+-tree index takes the form of a balanced tree, in which every path from the\nroot of the tree to a leaf of the tree is of the same length. The height of a B+-\ntree is proportional to the logarithm to the base Nof the number of records in the\nrelation, where each nonleaf node stores Npointers; the value of Nis often around\n50 or 100. B+-trees are much shorter than other balanced binary-tree structures\nsuch as AVL trees, and therefore require fewer disk accesses to locate records.\n\u2022Lookup on B+-trees is straightforward and e\ufb03cient. Insertion and deletion, how-\never, are somewhat more complicated, but still e\ufb03cient. The number of operations\nrequired for lookup, insertion, and deletion on B+-trees is proportional to the log-\narithm to the base Nof the number of records in the relation, where each nonleaf\nnode stores Npointers.\n\u2022We can use B+-trees for indexing a \ufb01le containing records, as well as to organize\nrecords into a \ufb01le.\n\u2022B-tree indices are similar to B+-tree indices. The primary advantage of a B-tree is\nthat the B-tree eliminates the redundant storage of search-key values. The major\ndisadvantages are overall complexity and reduced fanout for a given node size.\nSystem designers almost universally prefer B+-tree indices over B-tree indices in\npractice.\n", "706": "678 Chapter 14 Indexing\n\u2022Hashing is a widely used technique for building indices in main memory as well\nas in disk-based systems.\n\u2022Ordered indices such as B+-trees can be used for selections based on equality con-\nditions involving single attributes. When m ultiple attributes are involved in a selec-\ntion condition, we can intersect record identi\ufb01ers retrieved from multiple indices.\n\u2022The basic B+-tree structure is not ideal for applications that need to support a\nvery large number of random writes/inserts per second. Several alternative index\nstructures have been proposed to handle workloads with a high write/insert rate,\nincluding the log-structured merge tree and the bu\ufb00er tree.\n\u2022Bitmap indices provide a very compact representation for indexing attributes with\nvery few distinct values. Intersection operations are extremely fast on bitmaps,\nmaking them ideal for supporting queries on multiple attributes.\n\u2022R-trees are a multidimensional extension of B-trees; with variants such as R+-trees\nand R\u2217-trees, they have proved popular in spatial databases. Index structures that\npartition space in a regular fashion, such as quadtrees, help in processing spatial\njoin queries.\n\u2022There are a number of techniques for indexing temporal data, including the use of\nspatial index and the interval B+-tree specialized index.\nReview Terms\n\u2022Index type\n\u00b0Ordered indices\n\u00b0Hash indices\n\u2022Evaluation factors\n\u00b0Access types\n\u00b0Access time\n\u00b0Insertion time\n\u00b0Deletion time\n\u00b0Space overhead\n\u2022Search key\n\u2022Ordered indices\n\u00b0Ordered index\n\u00b0Clustering index\u00b0Primary indices;\n\u00b0Nonclustering indices\n\u00b0Secondary indices\n\u00b0Index-sequential \ufb01les\n\u2022Index entry\n\u2022Index record\n\u2022Dense index\n\u2022Sparse index\n\u2022Multilevel indices\n\u2022Nonunique search key\n\u2022Composite search key\n\u2022B+-tree index \ufb01les\n\u00b0Balanced tree\n\u00b0Leaf nodes\n", "707": "Practice Exercises 679\n\u00b0Nonleaf nodes\n\u00b0Internal nodes\n\u00b0Range queries\n\u00b0Node split\n\u00b0Node coalesce\n\u00b0Redistribute of pointers\n\u00b0Uniqui\ufb01er\n\u2022B+-tree extensions\n\u00b0Pre\ufb01x compression\n\u00b0Bulk loading\n\u00b0Bottom-up B+-tree construction\n\u2022B-tree indices\n\u2022Hash \ufb01le organization\n\u00b0Hash function\n\u00b0Bucket\n\u00b0Over\ufb02ow chaining\n\u00b0Closed addressing\n\u00b0Closed hashing\n\u00b0Bucket over\ufb02ow\n\u00b0Skew\n\u00b0Static hashing\u00b0Dynamic hashing\n\u2022Multiple-key access\n\u2022Covering indices\n\u2022Write-optimized index structure\n\u00b0Log-structured merge ( LSM)t r e e\n\u00b0Stepped-merge index\n\u00b0Bu\ufb00er tree\n\u2022Bitmap index\n\u2022Bitmap intersection\n\u2022Indexing of spatial data\n\u00b0Range queries\n\u00b0Nearest neighbor queries\n\u00b0k-d tree\n\u00b0k-d-B tree\n\u00b0Quadtrees\n\u00b0R-tree\n\u00b0Bounding box\n\u2022Temporal indices\n\u2022Time interval\n\u2022Closed interval\n\u2022Open interval\nPractice Exercises\n14.1 Indices speed query processing, but it is usually a bad idea to create indices on\nevery attribute, and every combination of attributes, that are potential search\nkeys. Explain why.\n14.2 Is it possible in general to have two clustering indices on the same relation for\ndi\ufb00erent search keys? Explain your answer.\n14.3 Construct a B+-tree for the following set of key values:\n(2, 3, 5, 7, 11, 17, 19, 23, 29, 31)\n", "708": "680 Chapter 14 Indexing\nAssume that the tree is initially empty and values are added in ascending order.\nConstruct B+-trees for the cases where the number of pointers that will \ufb01t in\none node is as follows:\na. Four\nb. Six\nc. Eight\n14.4 For each B+- t r e eo fE x e r c i s e1 4 . 3 ,s h o wt h ef o r mo ft h et r e ea f t e re a c ho ft h e\nfollowing series of operations:\na. Insert 9.\nb. Insert 10.\nc. Insert 8.\nd. Delete 23.\ne. Delete 19.\n14.5 Consider the modi\ufb01ed redistribution scheme for B+-trees described on page\n651. What is the expected height of the tree as a function of n?\n14.6 Give pseudocode for a B+-tree function findRangeIterator (), which is like the\nfunction findRange (), except that it returns an iterator object, as described\nin Section 14.3.2. Also give pseudocode for the iterator class, including the\nvariables in the iterator object, and the next() method.\n14.7 W h a tw o u l dt h eo c c u p a n c yo fe a c hl e a fn o d eo faB+- t r e eb ei fi n d e xe n t r i e s\nwere inserted in sorted order? Explain why.\n14.8 Suppose you have a relation rwith nrtuples on which a secondary B+-tree is\nto be constructed.\na. Give a formula for the cost of building the B+-tree index by inserting one\nrecord at a time. Assume each block will hold an average of fentries and\nthat all levels of the tree above the leaf are in memory.\nb. Assuming a random disk access takes 10 milliseconds, what is the cost\nof index construction on a relation with 10 million records?\nc. Write pseudocode for bottom-up construction of a B+-tree, which was\noutlined in Section 14.4.4. You can assume that a function to e\ufb03ciently\nsort a large \ufb01le is available.\n14.9 The leaf nodes of a B+-tree \ufb01le organization may lose sequentiality after a se-\nquence of inserts.\na. Explain why sequentiality may be lost.\n", "709": "Practice Exercises 681\nb. To minimize the number of seeks in a sequential scan, many databases\nallocate leaf pages in extents of nblocks, for some reasonably large n.\nWhen the \ufb01rst leaf of a B+-tree is allocated, only one block of an n-block\nunit is used, and the remaining pages are free. If a page splits, and its\nn-block unit has a free page, that space is used for the new page. If the\nn-block unit is full, another n-block unit is allocated, and the \ufb01rst n\u22152l e a f\npages are placed in one n-block unit and the remaining one in the second\nn-block unit. For simplicity, assume that there are no delete operations.\ni. What is the worst-case occupancy of allocated space, assuming no\ndelete operations, after the \ufb01rst n-block unit is full?\nii. Is it possible that leaf nodes allocated to an n-node block unit are not\nconsecutive, that is, is it possible that two leaf nodes are allocated\nto one n- n o d eb l o c k ,b u ta n o t h e rl e a fn o d ei nb e t w e e nt h et w oi s\nallocated to a di\ufb00erent n-node block?\niii. Under the reasonable assumption that bu\ufb00er space is su\ufb03cient to\nstore an n-page block, how many seeks would be required for a leaf-\nlevel scan of the B+-tree, in the worst case? Compare this number\nwith the worst case if leaf pages are allocated a block at a time.\niv. The technique of redistributing values to siblings to improve space\nutilization is likely to be more e\ufb03cient when used with the preceding\nallocation scheme for leaf blocks. Explain why.\n14.10 Suppose you are given a database schema and some queries that are executed\nfrequently. How would you use the above information to decide what indices\nto create?\n14.11 In write-optimized trees such as the LSM tree or the stepped-merge index, en-\ntries in one level are merged into the next level only when the level is full.\nSuggest how this policy can be changed to improve read performance during\nperiods when there are many reads but no updates.\n14.12 What trade o\ufb00s do bu\ufb00er trees pose as compared to LSM trees?\n14.13 Consider the instructor relation shown in Figure 14.1.\na. Construct a bitmap index on the attribute salary , dividing salary values\ninto four ranges: below 50,000, 50, 000 to below 60,000, 60,000 to below\n70,000, and 70,000 and above.\nb. Consider a query that requests all instructors in the Finance department\nwith salary of 80,000 or more. Outline the steps in answering the query,\nand show the \ufb01nal and intermediate bitmaps constructed to answer the\nquery.\n14.14 Suppose you have a relation containing the x,ycoordinates and names of\nrestaurants. Suppose also that the only queries that will be asked are of the\n", "710": "682 Chapter 14 Indexing\nfollowing form: The query speci\ufb01es a point and asks if there is a restaurant ex-\nactly at that point. Which type of index would be preferable, R-tree or B-tree?\nWhy?\n14.15 Suppose you have a spatial database that supports region queries with circular\nregions, but not nearest-neighbor queries. Describe an algorithm to \ufb01nd the\nnearest neighbor by making use of multiple region queries.\nExercises\n14.16 When is it preferable to use a dense index rather than a sparse index? Explain\nyour answer.\n14.17 What is the di\ufb00erence between a clustering index and a secondary index?\n14.18 For each B+-tree of Exercise 14.3, show the steps involved in the following\nqueries:\na. Find records with a search-key value of 11.\nb. Find records with a search-key value between 7 and 17, inclusive.\n14.19 The solution presented in Section 14.3.5 to deal with nonunique search keys\nadded an extra attribute to the search key. What e\ufb00ect could this change have\non the height of the B+-tree?\n14.20 Suppose there is a relation r(A,B,C), with a B+-tree index with search key\n(A,B).\na. What is the worst-case cost of \ufb01nding records satisfying 10 <A<50\nusing this index, in terms of the number of records retrieved n1and the\nheight hof the tree?\nb. What is the worst-case cost of \ufb01nding records satisfying 10 <A<50\u2227\n5<B<10 using this index, in terms of the number of records n2that\nsatisfy this selection, as well as n1and hde\ufb01ned above?\nc. Under what conditions on n1and n2would the index be an e\ufb03cient way\nof \ufb01nding records satisfying 10 <A<50\u22275<B<10?\n14.21 S u p p o s ey o uh a v et oc r e a t eaB+-tree index on a large number of names, where\nthe maximum size of a name may be quite large (say 40 characters) and the av-\nerage name is itself large (say 10 characters). Explain how pre\ufb01x compression\ncan be used to maximize the average fanout of nonleaf nodes.\n14.22 S u p p o s ear e l a t i o ni ss t o r e di naB+-tree \ufb01le organization. Suppose secondary\nindices store record identi\ufb01ers that are pointers to records on disk.\n", "711": "Further Reading 683\na. What would be the e\ufb00ect on the secondary indices if a node split hap-\npened in the \ufb01le organization?\nb. What would be the cost of updating all a\ufb00ected records in a secondary\nindex?\nc. How does using the search key of the \ufb01le organization as a logical record\nidenti\ufb01er solve this problem?\nd. What is the extra cost due to the use of such logical record identi\ufb01ers?\n14.23 What trade-o\ufb00s do write-optimized indices pose as compared to B+-tree in-\ndices?\n14.24 Anexistence bitmap has a bit for each record position, with the bit set to 1\nif the record exists, and 0 if there is no record at that position (for example,\nif the record were deleted). Show how to compute the existence bitmap from\nother bitmaps. Make sure that your technique works even in the presence of\nnull values by using a bitmap for the value null.\n14.25 Spatial indices that can index spatial intervals can conceptually be used to in-\ndex temporal data by treating valid time as a time interval. What is the problem\nwith doing so, and how is the problem solved?\n14.26 Some attributes of relations may contain sensitive data, and may be required\nto be stored in an encrypted fashion. How does data encryption a\ufb00ect index\nschemes? In particular, how might it a\ufb00ect schemes that attempt to store data\nin sorted order?\nFurther Reading\nB-tree indices were \ufb01rst introduced in [Bayer and McCreight (1972)] and [Bayer\n(1972)]. B+-trees are discussed in [Comer (1979)],[Bayer and Unterauer (1977)], and\n[Knuth (1973)]. [Gray and Reuter (1993)] provide a good description of issues in the\nimplementation of B+-trees.\nThe log-structured merge (LSM) tree is presented in [O\u2019Neil et al. (1996)], while\nthe stepped merge tree is presented in [Jagadish et al. (1997)]. The bu\ufb00er tree is\npresented in [Arge (2003)]. [Vitter (2001)] p rovides an extensive survey of external-\nmemory data structures and algorithms.\nBitmap indices are described in [O\u2019Neil and Quass (1997)]. They were \ufb01rst in-\ntroduced in the IBM Model 204 \ufb01le manager on the AS 400 platform. They provide\nvery large speedups on certain types of queries and are today implemented on most\ndatabase systems.\n[Samet (2006)] and [Shekhar and Chawla (2003)] provide textbook coverage of\nspatial data structures and spatial databases. [Bentley (1975)] describes the k-d tree,\n", "712": "684 Chapter 14 Indexing\nand [Robinson (1981)] describes the k-d-B tree. The R-tree was originally presented in\n[Guttman (1984)].\nBibliography\n[Arge (2003)] L. Arge, \u201cThe Bu\ufb00er Tree: A Technique for Designing Batched External Data\nStructures\u201d, Algorithmica , Volume 37, Number 1 (2003), pages 1\u201324.\n[Bayer (1972)] R. Bayer, \u201cSymmetric Binary B-trees: Data Structure and Maintenance Algo-\nrithms\u201d, Acta Informatica , Volume 1, Number 4 (1972), pages 290\u2013306.\n[Bayer and McCreight (1972)] R. Bayer and E. M. McCreight, \u201cOrganization and Mainte-\nnance of Large Ordered Indices\u201d, Acta Informatica , Volume 1, Number 3 (1972), pages 173\u2013\n189.\n[Bayer and Unterauer (1977)] R. Bayer and K. Unterauer, \u201cPre\ufb01x B-trees\u201d, ACM Transactions\non Database Systems , Volume 2, Number 1 (1977), pages 11\u201326.\n[Bentley (1975)] J. L. Bentley, \u201cMultidimensional Binary Search Trees Used for Associative\nSearching\u201d, Communications of the ACM , Volume 18, Number 9 (1975), pages 509\u2013517.\n[Comer (1979)] D. Comer, \u201cThe Ubiquitous B-tree\u201d, ACM Computing Surveys ,V o l u m e1 1 ,\nNumber 2 (1979), pages 121\u2013137.\n[Gray and Reuter (1993)] J. Gray and A. Reuter, Transaction Processing: Concepts and Tech-\nniques , Morgan Kaufmann (1993).\n[Guttman (1984)] A. Guttman, \u201cR-Trees: A Dynamic Index Structure for Spatial Searching\u201d,\nInProc. of the ACM SIGMOD Conf. on Management of Data (1984), pages 47\u201357.\n[Jagadish et al. (1997)] H. V. Jagadish, P. P. S. Narayan, S. Seshadri, S. Sudarshan, and\nR. Kanneganti, \u201cIncremental Organization for Data Recording and Warehousing\u201d, In Pro-\nceedings of the 23rd International Conference on Very Large Data Bases , VLDB \u201997 (1997),\npages 16\u201325.\n[Knuth (1973)] D. E. Knuth, The Art of Computer Programming, Volume 3 , Addison Wesley,\nSorting and Searching (1973).\n[O\u2019Neil and Quass (1997)] P. O\u2019Neil and D. Quass, \u201cImproved Query Performance with\nVariant Indexes\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data (1997), pages\n38\u201349.\n[O\u2019Neil et al. (1996)] P. O\u2019Neil, E. Cheng, D. Gawlick, and E. O\u2019Neil, \u201cThe Log-structured\nMerge-tree (LSM-tree)\u201d, Acta Inf. , Volume 33, Number 4 (1996), pages 351\u2013385.\n[Robinson (1981)] J. Robinson, \u201cThe k-d-B Tree: A Search Structure for Large Multidimen-\nsional Indexes\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data (1981), pages\n10\u201318.\n[Samet (2006)] H. Samet, Foundations of Multidimensional and Metric Data Structures ,M o r -\ngan Kaufmann (2006).\n", "713": "Further Reading 685\n[Shekhar and Chawla (2003)] S. Shekhar and S. Chawla, Spatial Databases: A TOUR ,P e a r -\nson (2003).\n[Vitter (2001)] J. S. Vitter, \u201cExternal Memory Algorithms and Data Structures: Dealing with\nMassive Data\u201d, ACM Computing Surveys , Volume 33, (2001), pages 209\u2013271.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "714": "", "715": "PART6\nQUERY PROCESSING AND\nOPTIMIZATION\nUser queries have to be executed on the database contents, which reside on storage\ndevices. It is usually convenient to break up queries into smaller operations, roughly\ncorresponding to the relational-algebra operations. Chapter 15 describes how queries\nare processed, presenting algorithms for implementing individual operations and then\noutlining how the operations are executed in synchrony to process a query. The algo-\nrithms covered include those that can work on data much larger than main-memory,\nas well as those that are optimized for in-memory data.\nThere are many alternative ways of processing a query, and these can have widely\nvarying costs. Query optimization refers to the process of \ufb01nding the lowest-cost\nmethod of evaluating a given query. Chapter 16 describes the process of query opti-\nmization, covering techniques for estimating query plan cost, and techniques for gen-\nerating alternative query plans and picking the lowest cost plans. The chapter also\ndescribes other optimization techniques, such as materialized views, for speeding up\nquery processing.\n687\n", "716": "", "717": "CHAPTER15\nQuery Processing\nQuery processing refers to the range of activities involved in extracting data from a\ndatabase. The activities include translation of queries in high-level database languages\ninto expressions that can be used at the physical level of the \ufb01le system, a variety of\nquery-optimizing transformations, and actual evaluation of queries.\n15.1 Overview\nThe steps involved in processing a query a ppear in Figure 15.1. The basic steps are:\n1.Parsing and translation.\n2.Optimization.\n3.Evaluation.\nBefore query processing can begin, the system must translate the query into a us-\nable form. A language such as SQL is suitable for human use, but it is ill suited to be\nthe system\u2019s internal representation of a query. A more useful internal representation\nis one based on the extended relational algebra.\nThus, the \ufb01rst action the system must take in query processing is to translate a given\nquery into its internal form. This translation process is similar to the work performed\nby the parser of a compiler. In generating the internal form of the query, the parser\nchecks the syntax of the user\u2019s query, veri\ufb01es that the relation names appearing in the\nquery are names of the relations in the database, and so on. The system constructs a\nparse-tree representation of the query, which it then translates into a relational-algebra\nexpression. If the query was expressed in terms of a view, the translation phase also\nreplaces all uses of the view by the relationa l-algebra expression that de\ufb01nes the view.1\nMost compiler texts cover parsing in detail.\n1For materialized views, the expression de\ufb01ning the view has already been evaluated and stored. Therefore, the stored\nrelation can be used, instead of uses of the view being repla ced by the expression de\ufb01ning the view. Recursive views are\nhandled di\ufb00erently, via a \ufb01xed-point procedure, as discussed in Section 5.4 and Section 27.4.7.\n689\n", "718": "690 Chapter 15 Query Processing\nquery\noutputqueryparser and\ntranslator\nevaluation enginerelational-algebra\nexpression\nexecution planoptimizer\ndata statistics\nabout data\nFigure 15.1 Steps in query processing.\nGiven a query, there are generally a variety of methods for computing the answer.\nF o re x a m p l e ,w eh a v es e e nt h a t ,i n SQL, a query could be expressed in several di\ufb00er-\nent ways. Each SQL query can itself be translated into a relational-algebra expression in\none of several ways. Furthermore, the relational-algebra representation of a query spec-\ni\ufb01es only partially how to evaluate a query; there are usually several ways to evaluate\nrelational-algebra expressions. As an illustration, consider the query:\nselect salary\nfrom instructor\nwhere salary <75000;\nThis query can be translated into either of the following relational-algebra expressions:\n\u2022\u03c3salary <75000 (\u03a0salary (instructor ))\n\u2022\u03a0salary (\u03c3salary <75000 (instructor ))\nFurther, we can execute each relational-algebra operation by one of several dif-\nferent algorithms. For example, to implement the preceding selection, we can search\nevery tuple in instructor to \ufb01nd tuples with salary less than 75000. If a B+-tree index is\navailable on the attribute salary , we can use the index instead to locate the tuples.\nTo specify fully how to evaluate a query, we need not only to provide the relational-\nalgebra expression, but also to annotate it with instructions specifying how to evaluate\neach operation. Annotations may state the algorithm to be used for a speci\ufb01c opera-\ntion or the particular index or indices to use. A relational-algebra operation annotated\n", "719": "15.1 Overview 691\nwith instructions on how to evaluate it is called an evaluation primitive .As e q u e n c eo f\nprimitive operations that can be used to evaluate a query is a query-execution plan or\nquery-evaluation plan . Figure 15.2 illustrates an evaluation plan for our example query,\nin which a particular index (denoted in the \ufb01gure as \u201cindex 1\u201d) is speci\ufb01ed for the se-\nlection operation. The query-execution engine takes a query-evaluation plan, executes\nthat plan, and returns the answers to the query.\nThe di\ufb00erent evaluation plans for a given query can have di\ufb00erent costs. We do\nnot expect users to write their queries in a way that suggests the most e\ufb03cient evalua-\ntion plan. Rather, it is the responsibility of the system to construct a query-evaluation\nplan that minimizes the cost of query evaluation; this task is called query optimization .\nChapter 16 describes query optimization in detail.\nOnce the query plan is chosen, the query is evaluated with that plan, and the result\nof the query is output.\nThe sequence of steps already described for processing a query is representative;\nnot all databases exactly follow those steps. For instance, instead of using the relational-\nalgebra representation, several databases use an annotated parse-tree representation\nbased on the structure of the given SQL q u e r y .H o w e v e r ,t h ec o n c e p t st h a tw ed e s c r i b e\nhere form the basis of query processing in databases.\nIn order to optimize a query, a query optimizer must know the cost of each opera-\ntion. Although the exact cost is hard to compute, since it depends on many parameters\nsuch as actual memory available to the operation, it is possible to get a rough estimate\nof execution cost for each operation.\nIn this chapter, we study how to evaluate individual operations in a query plan and\nhow to estimate their cost; we return to query optimization in Chapter 16. Section 15.2\noutlines how we measure the cost of a query. Section 15.3 through Section 15.6 cover\nthe evaluation of individual relational-algebra operations. Several operations may be\ngrouped together into a pipeline , in which each of the operations starts working on its\ninput tuples even as they are being generated by another operation. In Section 15.7, we\nexamine how to coordinate the execution of multiple operations in a query evaluation\nsalary\nsalary  < 75000; use index 1\ninstructor\u03c3\u03c0\nFigure 15.2 A query-evaluation plan.\n", "720": "692 Chapter 15 Query Processing\nplan, in particular, how to use pipelined operations to avoid writing intermediate results\nto disk.\n15.2 Measures of Query Cost\nThere are multiple possible evaluation plans for a query, and it is important to be able\nto compare the alternatives in terms of their (estimated) cost, and choose the best plan.\nTo do so, we must estimate the cost of individual operations and combine them to get\nthe cost of a query evaluation plan. Thus, as we study evaluation algorithms for each\noperation later in this chapter, we also outline how to estimate the cost of the operation.\nThe cost of query evaluation can be measured in terms of a number of di\ufb00erent\nresources, including disk accesses, CPU time to execute a query, and, in parallel and\ndistributed database systems, the cost of communication. (We discuss parallel and dis-\ntributed database systems in Chapter 21 through Chapter 23.)\nFor large databases resident on magnetic disk, the I/Ocost to access data from\ndisk usually dominates the other costs; thus, early cost models focused on the I/Ocost\nwhen estimating the cost of query operations. However, with \ufb02ash storage becoming\nlarger and less expensive, most organizational data today can be stored on solid-state\ndrives ( SSDs) in a cost e\ufb00ective manner. In addition, main memory sizes have increased\nsigni\ufb01cantly, and the cost of main memory has decreased enough in recent years that for\nmany organizations, organizational data can be stored cost-e\ufb00ectively in main memory\nfor querying, although it must of course be stored on \ufb02ash or magnetic storage to ensure\npersistence.\nWith data resident in-memory or on SSDs ,I/Ocost does not dominate the overall\ncost, and we must include CPU costs when computing the cost of query evaluation.\nWe do not include CPU costs in our model to simplify our presentation, but note that\nthey can be approximated by simple estimators. For example, the cost model used by\nPostgre SQL(as of 2018) includes (i) a CPU cost per tuple, (ii) a CPU cost for processing\neach index entry (in addition to the I/Ocost), and (iii) a CPU cost per operator or\nfunction (such as arithmetic operators, comparison operators, and related functions).\nThe database has default values for each of these costs, which are multiplied by the\nnumber of tuples processed, the number of index entries processed, and the number\nof operators and functions executed, respectively. The defaults can be changed as a\ncon\ufb01guration parameter.\nWe use the number of blocks transferred from storage and the number of random I/O\naccesses , each of which will require a disk seek on magnetic storage, as two important\nfactors in estimating the cost of a query-evaluation plan. If the disk subsystem takes an\naverage of tTseconds to transfer a block of data and has an average block-access time\n(disk seek time plus rotational latency) of tSseconds, then an operation that transfers\nbblocks and performs Srandom I/Oaccesses would take b\u2217tT+S\u2217tSseconds.\nThe values of tTandtSmust be calibrated for the disk system used. We summarize\nperformance data here; see Chapter 12 for full details on storage systems. Typical values\nfor high-end magnetic disks in the year 2018 would be tS=4 milliseconds and tT=0.1\n", "721": "15.2 Measures of Query Cost 693\nmilliseconds, assuming a 4-kilobyte block size and a transfer rate of 40 megabytes per\nsecond.2\nAlthough SSDs do not perform a physical seek operation, they have an overhead\nfor initiating an I/Ooperation; we refer to the latency from the time an I/Orequest\nis made to the time when the \ufb01rst byte of data is returned as tS.F o rm i d - r a n g e SSDs\nin 2018 using the SATA interface, tSis around 90 microseconds, while the transfer\ntime tTis about 10 microseconds for a 4-kilobyte block. Thus, SSDs can support about\n10,000 random 4-kilobyte reads per second, and they support 400 megabytes/second\nthroughput on sequential reads using the standard SATA interface. SSDsu s i n gt h e PCIe\n3.0x4 interface have smaller tS, of 20 to 60 microseconds, and much higher transfer\nrates of around 2 gigabytes/second, corresponding to tTof 2 microseconds, allowing\naround 50,000 to 15,000 random 4-kilobyte block reads per second, depending on the\nmodel.3\nFor data that are already present in main memory, reads happen at the unit of\ncache lines, instead of disk blocks. But assuming entire blocks of data are read, the\ntime to transfer tTfor a 4-kilobyte block is less than 1 microsecond for data in memory.\nThe latency to fetch data from memory, tS, is less than 100 nanoseconds.\nGiven the wide diversity of speeds of di\ufb00erent storage devices, database systems\nmust ideally perform test seeks and block transfers to estimate tSandtTfor speci\ufb01c\nsystems/storage devices, as part of the software installation process. Databases that do\nnot automatically infer these numbers often allow users to specify the numbers as part\nof con\ufb01guration \ufb01les.\nWe can re\ufb01ne our cost estimates further by distinguishing block reads from block\nwrites. Block writes are typically about twice as expensive as reads on magnetic disks,\nsince disk systems read sectors back after they are written to verify that the write was\nsuccessful. On PCIe \ufb02ash, write throughput may be about 50 percent less than read\nthroughput, but the di\ufb00erence is almost completely masked by the limited speed of\nSATA interfaces, leading to write throughput matching read throughput. However, the\nthroughput numbers do not re\ufb02ect the cost of erases that are required if blocks are\noverwritten. For simplicity, we ignore this detail.\nThe cost estimates we give do not include the cost of writing the \ufb01nal result of\nan operation back to disk. These are taken into account separately where required.\n2Storage device speci\ufb01cations often mention the transfer rate, and the number of random I/Ooperations that can be\ncarried out in 1 second. The values tTcan be computed as block size divided by transfer rate, while tScan be computed\nas (1\u2215N)\u2212tT,w h e r e Nis the number of random I/Ooperations per second that the device supports, since a random\nI/Ooperation performs a random I/Oaccess, followed by data transfer of 1 block.\n3The I/Ooperations per second number used here are for the case of sequential I/Orequests, usually denoted as QD-1\nin the SSDspeci\ufb01cations. SSDs can support multiple random requests in par allel, with 32 to 64 parallel requests being\ncommonly supported; an SSDwith SATA interface supports nearly 100,000 rand om 4-kilobyte block reads in a second\nif multiple requests are sent in parallel, while PCIe disks can support over 350,000 random 4-kilobyte block reads per\nsecond; these numbers are referred to as the QD-32 or QD -64 numbers depending on how many requests are sent in\nparallel. We do not explore parallel requests in our cost model, since we only consider se quential query processing\nalgorithms in this chapter. Shared-memory parallel query pr ocessing techniques, discussed in Section 22.6, can be used\nto exploit the parallel request capabilities of SSDs .\n", "722": "694 Chapter 15 Query Processing\nThe costs of all the algorithms that we consider depend on the size of the bu\ufb00er in\nmain memory. In the best case, if data \ufb01ts in the bu\ufb00er, the data can be read into\nthe bu\ufb00ers, and the disk does not need to be accessed again. In the worst case, we\nmay assume that the bu\ufb00er can hold only a few blocks of data\u2014approximately one\nblock per relation. However, with large main memories available today, such worst-case\nassumptions are overly pessimistic. In fact, a good deal of main memory is typically\navailable for processing a query, and our cost estimates use the amount of memory\navailable to an operator, M,a sap a r a m e t e r .I n Postgre SQL the total memory available\nto a query, called the e\ufb00ective cache size, is assumed by default to be 4 gigabytes, for\nthe purpose of cost estimation; if a query has several operators that run concurrently,\nthe available memory has to be divided amongst the operators.\nIn addition, although we assume that data must be read from disk initially, it is\npossible that a block that is accessed is already present in the in-memory bu\ufb00er. Again,\nfor simplicity, we ignore this e\ufb00ect; as a res ult, the actual disk-access cost during the\nexecution of a plan may be less than the estimated cost. To account (at least partially)\nfor bu\ufb00er residence, Postgre SQL uses the following \u201chack\u201d: the cost of a random page\nread is assumed to be 1/10thof the actual random page read cost, to model the situation\nthat 90% of reads are found to be resident in cache. Further, to model the situation that\ninternal nodes of B+-tree indices are traversed often, most database systems assume\nthat all internal nodes are present in the in-memory bu\ufb00er, and assume that a traversal\nof an index only incurs a single random I/Ocost for the leaf node.\nTheresponse time for a query-evaluation plan (that is, the wall-clock time required\nto execute the plan), assuming no other activity is going on in the computer, would\naccount for all these costs, and could be used as a measure of the cost of the plan.\nUnfortunately, the response time of a plan is very hard to estimate without actually\nexecuting the plan, for the following two reasons:\n1.The response time depends on the contents of the bu\ufb00er when the query begins\nexecution; this information is not available when the query is optimized and is\nhard to account for even if it were available.\n2.In a system with multiple disks, the response time depends on how accesses are\ndistributed among disks, which is hard to estimate without detailed knowledge\nof data layout on disk.\nInterestingly, a plan may get a better response time at the cost of extra resource con-\nsumption. For example, if a system has multiple disks, a plan Athat requires extra disk\nreads, but performs the reads in parallel across multiple disks may, \ufb01nish faster than\nanother plan Bthat has fewer disk reads, but performs reads from only one disk at a\ntime. However, if many instances of a query using plan Arun concurrently, the overall\nresponse time may actually be more than if the same instances are executed using plan\nB,s i n c ep l a n Agenerates more load on the disks.\nAs a result, instead of trying to minimize the response time, optimizers generally\ntry to minimize the total resource consumption of a query plan. Our model of estimating\n", "723": "15.3 Selection Operation 695\nthe total disk access time (including seek and data transfer) is an example of such a\nresource consumption\u2013based model of query cost.\n15.3 Selection Operation\nIn query processing, the \ufb01le scan is the lowest-level operator to access data. File scans\nare search algorithms that locate and retrieve records that ful\ufb01ll a selection condition.\nIn relational systems, a \ufb01le scan allows an entire relation to be read in those cases where\nthe relation is stored in a single, dedicated \ufb01le.\n15.3.1 Selections Using File Scans and Indices\nConsider a selection operation on a relation whose tuples are stored together in one\n\ufb01le. The most straightforward way of performing a selection is as follows:\n\u2022A1(linear search ). In a linear search, the system scans each \ufb01le block and tests\nall records to see whether they satisfy the selection condition. An initial seek is\nrequired to access the \ufb01rst block of the \ufb01le. In case blocks of the \ufb01le are not stored\ncontiguously, extra seeks may be required, but we ignore this e\ufb00ect for simplicity.\nAlthough it may be slower than other algorithms for implementing selection,\nthe linear-search algorithm can be applied to any \ufb01le, regardless of the ordering\nof the \ufb01le, or the availability of indices, or the nature of the selection operation.\nThe other algorithms that we shall study are not applicable in all cases, but when\napplicable they are generally faster than linear search.\nCost estimates for linear scan, as well as for other selection algorithms, are shown\nin Figure 15.3. In the \ufb01gure, we use hito represent the height of the B+-tree, and assume\nar a n d o m I/Ooperation is required for each node in the path from the root to a leaf.\nMost real-life optimizers assume that the internal nodes of the tree are present in the\nin-memory bu\ufb00er since they are frequently accessed, and usually less than 1 percent of\nthe nodes of a B+-tree are nonleaf nodes. The cost formulae can be correspondingly\nsimpli\ufb01ed, charging only one random I/Ocost for a traversal from the root to a leaf, by\nsetting hi=1.\nIndex structures are referred to as access paths , since they provide a path through\nwhich data can be located and accessed. In Chapter 14, we pointed out that it is e\ufb03cient\nto read the records of a \ufb01le in an order corresponding closely to physical order. Recall\nthat a clustering index (also referred to as a primary index ) is an index that allows the\nrecords of a \ufb01le to be read in an order that corresponds to the physical order in the\n\ufb01le. An index that is not a clustering index is called a secondary index or anonclustering\nindex .\n", "724": "696 Chapter 15 Query Processing\nSearch algorithms that use an index are referred to as index scans .W eu s et h e\nselection predicate to guide us in the choice of the index to use in processing the query.\nSearch algorithms that use an index are:\nAlgorithm\n Cost\n Reason\nA1\n Linear Search\n tS+br\u2217tT\n One initial seek plus brblock transfers,\nwhere brdenotes the number of blocks in\nthe \ufb01le.\nA1\n Linear Search,\nEquality on Key\nAverage case\ntS+(br\u22152)\u2217tT\nSince at most one record satis\ufb01es the con-\ndition, scan can be terminated as soon as\nthe required record is found. In the worst\ncase, brblock transfers are still required.\nA2\n Clustering\nB+-tree Index,\nEquality on Key\n(hi+1)\u2217\n(tT+tS)\n(Where hidenotes the height of the in-\ndex.) Index lookup traverses the height of\nthe tree plus one I/Oto fetch the record;\neach of these I/Ooperations requires a\nseek and a block transfer.\nA3\n Clustering\nB+-tree Index,\nEquality on\nNon-key\nhi\u2217(tT+tS)+\ntS+b\u2217tT\nOne seek for each level of the tree, one\nseek for the \ufb01rst block. Here bis the num-\nber of blocks containing records with the\nspeci\ufb01ed search key, all of which are read.\nThese blocks are leaf blocks assumed to be\nstored sequentially (since it is a clustering\nindex) and don\u2019t require additional seeks.\nA4\n Secondary\nB+-tree Index,\nEquality on Key\n(hi+1)\u2217\n(tT+tS)\nThis case is similar to clustering index.\nA4\n Secondary\nB+-tree Index,\nEquality on\nNon-key\n(hi+n)\u2217\n(tT+tS)\n(Where nis the number of records\nfetched.) Here, cost of index traversal is\nthe same as for A3, but each record may\nbe on a di\ufb00erent block, requiring a seek\nper record. Cost is potentially very high if\nnis large.\nA5\n Clustering\nB+-tree Index,\nComparison\nhi\u2217(tT+tS)+\ntS+b\u2217tT\nIdentical to the case of A3, equality on\nnon-key.\nA6\n Secondary\nB+-tree Index,\nComparison\n(hi+n)\u2217\n(tT+tS)\nIdentical to the case of A4, equality on\nnon-key.\nFigure 15.3 Cost estimates for selection algorithms.\n", "725": "15.3 Selection Operation 697\n\u2022A2(clustering index, equality on key ) .F o ra ne q u a l i t yc o m p a r i s o no nak e ya t -\ntribute with a clustering index, we can use the index to retrieve a single record that\nsatis\ufb01es the corresponding equality condition. Cost estimates are shown in Figure\n15.3. To model the common situation that the internal nodes of the index are in\nthe in-memory bu\ufb00er, hican be set to 1.\n\u2022A3(clustering index, equality on non-key ). We can retrieve multiple records by\nusing a clustering index when the selection condition speci\ufb01es an equality com-\nparison on a non-key attribute, A. The only di\ufb00erence from the previous case is\nthat multiple records may need to be fetched. However, the records must be stored\nconsecutively in the \ufb01le since the \ufb01le is sorted on the search key. Cost estimates\nare shown in Figure 15.3.\n\u2022A4(secondary index, equality ). Selections specifying an equality condition can\nuse a secondary index. This strategy can retrieve a single record if the equality\ncondition is on a key; multiple records may be retrieved if the indexing \ufb01eld is not\nak e y .\nIn the \ufb01rst case, only one record is retrieved. The cost in this case is the same\nas that for a clustering index (case A2).\nIn the second case, each record may be resident on a di\ufb00erent block, which may\nresult in one I/Ooperation per retrieved record, with each I/Ooperation requiring\na seek and a block transfer. The worst-case cost in this case is ( hi+n)\u2217(tS+tT),\nwhere nis the number of records fetched, if each record is in a di\ufb00erent disk block,\nand the block fetches are randomly ordered. The worst-case cost could become\neven worse than that of linear search if a large number of records are retrieved.\nIf the in-memory bu\ufb00er is large, the block containing the record may already\nbe in the bu\ufb00er. It is possible to construct an estimate of the average orexpected\ncost of the selection by taking into account the probability of the block containing\nthe record already being in the bu\ufb00er. For large bu\ufb00ers, that estimate will be much\nless than the worst-case estimate.\nIn certain algorithms, including A2, the use of a B+-tree \ufb01le organization can save\none access since records are stored at the leaf level of the tree.\nAs described in Section 14.4.2, when records are stored in a B+-tree \ufb01le organiza-\ntion or other \ufb01le organizations that may require relocation of records, secondary in-\ndices usually do not store pointers to the records.4Instead, secondary indices store the\nvalues of the attributes used as the search key in a B+-tree \ufb01le organization. Accessing\na record through such a secondary index is then more expensive: First the secondary\nindex is searched to \ufb01nd the B+-tree \ufb01le organization search-key values, then the B+-\ntree \ufb01le organization is looked up to \ufb01nd the records. The cost formulae described for\nsecondary indices have to be modi\ufb01ed appropriately if such indices are used.\n4Recall that if B+-tree \ufb01le organizations are used to store relations, records may be moved between blocks when leaf\nnodes are split or merged, and when records are redistributed.\n", "726": "698 Chapter 15 Query Processing\n15.3.2 Selections Involving Comparisons\nConsider a selection of the form \u03c3A\u2264v(r). We can implement the selection either by\nusing linear search or by using indices in one of the following ways:\n\u2022A5(clustering index, comparison ). A clustering ordered index (for example, a clus-\ntering B+-tree index) can be used when the selection condition is a comparison.\nFor comparison conditions of the form A>vorA\u2265v,ac l u s t e r i n gi n d e xo n A\ncan be used to direct the retrieval of tuples, as follows: For A\u2265v, we look up the\nvalue vin the index to \ufb01nd the \ufb01rst tuple in the \ufb01le that has a value of A\u2265v.A\ufb01 l e\nscan starting from that tuple up to the end of the \ufb01le returns all tuples that satisfy\nthe condition. For A>v, the \ufb01le scan starts with the \ufb01rst tuple such that A>v.\nThe cost estimate for this case is identical to that for case A3.\nFor comparisons of the form A<vorA\u2264v, an index lookup is not required.\nForA<v, we use a simple \ufb01le scan starting from the beginning of the \ufb01le, and\ncontinuing up to (but not including) the \ufb01rst tuple with attribute A=v.T h ec a s e\nofA\u2264vis similar, except that the scan continues up to (but not including) the\n\ufb01rst tuple with attribute A>v. In either case, the index is not useful.\n\u2022A6(secondary index, comparison ). We can use a secondary ordered index to guide\nretrieval for comparison conditions involving <,\u2264,\u2265,o r>. The lowest-level index\nblocks are scanned, either from the smallest value up to v(for<and\u2264), or from\nvup to the maximum value (for >and\u2265).\nThe secondary index provides pointers to the records, but to get the actual\nrecords we have to fetch the records by using the pointers. This step may require\nanI/Ooperation for each record fetched, since consecutive records may be on\ndi\ufb00erent disk blocks; as before, each I/Ooperation requires a disk seek and a block\ntransfer. If the number of retrieved records is large, using the secondary index may\nbe even more expensive than using linear search. Therefore, the secondary index\nshould be used only if very few records are selected.\nAs long as the number of matching tuples is known ahead of time, a query opti-\nmizer can choose between using a secondary index or using a linear scan based on the\ncost estimates. However, if the number of matching tuples is not known accurately at\ncompilation time, either choice may lead to bad performance, depending on the actual\nnumber of matching tuples.\nTo deal with the above situation, Postgre SQL uses a hybrid algorithm that it calls a\nbitmap index scan ,5when a secondary index is available, but the number of matching\nrecords is not known precisely. The bitmap index scan algorithm \ufb01rst creates a bitmap\nwith as many bits as the number of blocks in the relation, with all bits initialized to 0.\nThe algorithm then uses the secondary index to \ufb01nd index entries for matching tuples,\nbut instead of fetching the tuples immediately, it does the following. As each index\n5This algorithm should not be confused with a scan using a bitmap index.\n", "727": "15.3 Selection Operation 699\nentry is found, the algorithm gets the block number from the index entry, and sets the\ncorresponding bit in the bitmap to 1.\nOnce all index entries have been processed, the bitmap is scanned to \ufb01nd all blocks\nwhose bit is set to 1. These are exactly the blocks containing matching records. The\nrelation is then scanned linearly, but blocks whose bit is not set to 1 are skipped; only\nblocks whose bit is set to 1 are fetched, and then a scan within each block is used to\nretrieve all matching records in the block.\nIn the worst case, this algorithm is only slightly more expensive than linear scan,\nbut in the best case it is much cheaper than linear scan. Similarly, in the worst case it is\nonly slightly more expensive than using a secondary index scan to directly fetch tuples,\nbut in the best case it is much cheaper than a secondary index scan. Thus, this hybrid\nalgorithm ensures that performance is never much worse than the best plan for that\ndatabase instance.\nA variant of this algorithm collects all the index entries, and sorts them (using\nsorting algorithms which we study later in this chapter), and then performs a relation\nscan that skips blocks that do not have any matching entries. Using a bitmap as above\ncan be cheaper than sorting the index entries.\n15.3.3 Implementation of Complex Selections\nSo far, we have considered only simple selection conditions of the form Ao pB ,w h e r e\nopis an equality or comparison operation. We now consider more complex selection\npredicates.\n\u2022Conjunction: Aconjunctive selection is a selection of the form:\n\u03c3\u03b81\u2227\u03b82\u2227\u22ef\u2227\u03b8n(r)\n\u2022Disjunction: Adisjunctive selection is a selection of the form:\n\u03c3\u03b81\u2228\u03b82\u2228\u22ef\u2228\u03b8n(r)\nA disjunctive condition is satis\ufb01ed by the union of all records satisfying the indi-\nvidual, simple conditions \u03b8i.\n\u2022Negation: The result of a selection \u03c3\u00ac\u03b8(r)i st h es e to ft u p l e so f rfor which the\ncondition \u03b8evaluates to false. In the absence of nulls, this set is simply the set of\ntuples in rthat are not in \u03c3\u03b8(r).\nWe can implement a selection operation involving either a conjunction or a dis-\njunction of simple conditions by using one of the following algorithms:\n\u2022A7(conjunctive selection using one index ). We \ufb01rst determine whether an access\npath is available for an attribute in one of the simple conditions. If one is, one of the\n", "728": "700 Chapter 15 Query Processing\nselection algorithms A2 through A6 can retrieve records satisfying that condition.\nWe complete the operation by testing, in the memory bu\ufb00er, whether or not each\nretrieved record satis\ufb01es the remaining simple conditions.\nTo reduce the cost, we choose a \u03b8iand one of algorithms A1 through A6 for\nwhich the combination results in the least cost for \u03c3\u03b8i(r). The cost of algorithm\nA7 is given by the cost of the chosen algorithm.\n\u2022A8(conjunctive selection using composite index ). An appropriate composite index\n(that is, an index on multiple attributes) may be available for some conjunctive se-\nlections. If the selection speci\ufb01es an equality condition on two or more attributes,\nand a composite index exists on these combined attribute \ufb01elds, then the index\ncan be searched directly. The type of index determines which of algorithms A2,\nA3, or A4 will be used.\n\u2022A9(conjunctive selection by intersection of identi\ufb01ers ). Another alternative for im-\nplementing conjunctive selection operations involves the use of record pointers\nor record identi\ufb01ers. This algorithm requ ires indices with record pointers, on the\n\ufb01elds involved in the individual conditions. The algorithm scans each index for\npointers to tuples that satisfy an individual condition. The intersection of all the\nretrieved pointers is the set of pointers to tuples that satisfy the conjunctive condi-\ntion. The algorithm then uses the pointers to retrieve the actual records. If indices\nare not available on all the individual conditions, then the algorithm tests the re-\ntrieved records against the remaining conditions.\nThe cost of algorithm A9 is the sum of the costs of the individual index scans,\nplus the cost of retrieving the records in the intersection of the retrieved lists of\npointers. This cost can be reduced by sorting the list of pointers and retrieving\nrecords in the sorted order. Thereby, (1) all pointers to records in a block come\ntogether, hence all selected records in the block can be retrieved using a single I/O\noperation, and (2) blocks are read in sorted order, minimizing disk-arm movement.\nSection 15.4 describes sorting algorithms.\n\u2022A10(disjunctive selection by union of identi\ufb01ers ). If access paths are available on\nall the conditions of a disjunctive selection, each index is scanned for pointers to\ntuples that satisfy the individual condition. The union of all the retrieved pointers\nyields the set of pointers to all tuples that satisfy the disjunctive condition. We\nthen use the pointers to retrieve the actual records.\nHowever, if even one of the conditions does not have an access path, we have\nto perform a linear scan of the relation to \ufb01nd tuples that satisfy the condition.\nTherefore, if there is even one such condition in the disjunct, the most e\ufb03cient\naccess method is a linear scan, with the dis junctive condition tested on each tuple\nduring the scan.\nThe implementation of selections with negation conditions is left to you as an\nexercise (Practice Exercise 15.6).\n", "729": "15.4 Sorting 701\n15.4 Sorting\nSorting of data plays an important role in database systems for two reasons. First, SQL\nqueries can specify that the output be sorted. Second, and equally important for query\nprocessing, several of the relational operations, such as joins, can be implemented ef-\n\ufb01ciently if the input relations are \ufb01rst sorted. Thus, we discuss sorting here before\ndiscussing the join operation in Section 15.5.\nWe can sort a relation by building an index on the sort key and then using that\nindex to read the relation in sorted order. However, such a process orders the relation\nonly logically , through an index, rather than physically . Hence, the reading of tuples\nin the sorted order may lead to a disk access (disk seek plus block transfer) for each\nrecord, which can be very expensive, since the number of records can be much larger\nthan the number of blocks. For this reason, it may be desirable to order the records\nphysically.\nThe problem of sorting has been studied extensively, both for relations that \ufb01t\nentirely in main memory and for relations that are bigger than memory. In the \ufb01rst\ncase, standard sorting techniques such as quick-sort can be used. Here, we discuss how\nto handle the second case.\n15.4.1 External Sort-Merge Algorithm\nSorting of relations that do not \ufb01t in memory is called external sorting . The most com-\nmonly used technique for external sorting is the external sort\u2013merge algorithm. We\ndescribe the external sort\u2013merge algorithm next. Let Mdenote the number of blocks\nin the main memory bu\ufb00er available for sorting, that is, the number of disk blocks\nwhose contents can be bu\ufb00ered in available main memory.\n1.In the \ufb01rst stage, a number of sorted runs are created; each run is sorted but\ncontains only some of the records of the relation.\ni=0 ;\nrepeat\nread Mblocks of the relation, or the rest of the relation,\nwhichever is smaller;\nsort the in-memory part of the relation;\nwrite the sorted data to run \ufb01le Ri;\ni=i+1 ;\nuntil the end of the relation\n2.In the second stage, the runs are merged .S u p p o s e ,f o rn o w ,t h a tt h et o t a ln u m b e r\nof runs, N,i sl e s st h a n M, so that we can allocate one block to each run and have\nspace left to hold one block of output. The merge stage operates as follows:\n", "730": "702 Chapter 15 Query Processing\nread one block of each of the N\ufb01lesRiinto a bu\ufb00er block in memory;\nrepeat\nchoose the \ufb01rst tuple (in sort order) among all bu\ufb00er blocks;\nwrite the tuple to the output, and delete it from the bu\ufb00er block;\nifthe bu\ufb00er block of any run Riis empty and not end-of-\ufb01le( Ri)\nthen read the next block of Riinto the bu\ufb00er block;\nuntil all input bu\ufb00er blocks are empty\nThe output of the merge stage is the sorted relation. The output \ufb01le is bu\ufb00ered to\nreduce the number of disk write operations. The preceding merge operation is a gener-\nalization of the two-way merge used by the standard in-memory sort\u2013merge algorithm;\nit merges Nruns, so it is called an N-way merge .\nIn general, if the relation is much larger than memory, there may be Mor more\nruns generated in the \ufb01rst stage, and it is not possible to allocate a block for each run\nduring the merge stage. In this case, the merge operation proceeds in multiple passes.\nSince there is enough memory for M\u22121 input bu\ufb00er blocks, each merge can take M\u22121\nruns as input.\nThe initial pass functions in this way: It merges the \ufb01rst M\u22121 runs (as described\nin item 2 above) to get a single run for the next pass. Then, it merges the next M\u22121\nruns similarly, and so on, until it has processed all the initial runs. At this point, the\nnumber of runs has been reduced by a factor of M\u22121. If this reduced number of runs\nis still greater than or equal to M, another pass is made, with the runs created by the\n\ufb01rst pass as input. Each pass reduces the number of runs by a factor of M\u22121. The\npasses repeat as many times as required, until the number of runs is less than M; a \ufb01nal\npass then generates the sorted output.\nFigure 15.4 illustrates the steps of the external sort\u2013merge for an example relation.\nFor illustration purposes, we assume that only one tuple \ufb01ts in a block ( fr=1), and we\nassume that memory holds at most three blocks. During the merge stage, two blocks\nare used for input and one for output.\n15.4.2 Cost Analysis of External Sort-Merge\nWe compute the disk-access cost for the external sort\u2013merge in this way: Let\nbrdenote the number of blocks containing records of relation r. The \ufb01rst stage\nreads every block of the relation and writes them out again, giving a total of 2 brblock\ntransfers. The initial number of runs is \u2308br\u2215M\u2309. During the merge pass, reading in\neach run one block at a time leads to a large number of seeks; to reduce the number of\nseeks, a larger number of blocks, denoted bb, are read or written at a time, requiring bb\nbu\ufb00er blocks to be allocated to each input run and to the output run. Then, \u230aM\u2215bb\u230b\u22121\nr u n sc a nb em e r g e di ne a c hm e r g ep a s s ,d e c r e a s i n gt h en u m b e ro fr u n sb yaf a c t o ro f\n\u230aM\u2215bb\u230b\u22121. The total number of merge passes required is \u2308log\u230aM\u2215bb\u230b\u22121(br\u2215M)\u2309.E a c h\nof these passes reads every block of the relation once and writes it out once, with two\nexceptions. First, the \ufb01nal pass can produce the sorted output without writing its result\n", "731": "15.4 Sorting 703\ng\na   \nd   31\nc    33\nb   14\ne   16\nr   16\nd   21\nm    3\np     2\nd     7\na   14a    14\na    19\nb    14\nc    33\nd     7\nd    21\nd    31\ne    16\ng    24\nm    3\np     2\nr    16a    19\nb    14\nc    33\nd    31\ne    16\ng    24\na    14\nd     7\nd    21\nm    3\np     2\nr    16a   19\nd   31\ng   24\nb   14\nc   33\ne   16\nd   21\nm    3\nr    16\na    14\nd     7\np     2\ninitial\nrelation\ncreate\nrunsmerge\npass\u20131merge\npass\u20132runs runssorted\noutput24\n19\nFigure 15.4 External sorting using sort\u2013merge.\nto disk. Second, there may be runs that are not read in or written out during a pass\n\u2014for example, if there are \u230aM\u2215bb\u230bruns to be merged in a pass, \u230aM\u2215bb\u230b\u22121 are read\nin and merged, and one run is not accessed during the pass. Ignoring the (relatively\nsmall) savings due to the latter e\ufb00ect, the total number of block transfers for external\nsorting of the relation is:\nbr(2\u2308log\u230aM\u2215bb\u230b\u22121(br\u2215M)\u2309+1)\nApplying this equation to the example in Figure 15.4, with bbset to 1, we get a total of\n12\u2217(4+1)=60 block transfers, as you can verify from the \ufb01gure. Note that these\nabove numbers do not include the cost of writing out the \ufb01nal result.\nWe also need to add the disk-seek costs. Run generation requires seeks for reading\ndata for each of the runs as well as for writing the runs. Each merge pass requires\naround \u2308br\u2215bb\u2309seeks for reading data.6Although the output is written sequentially, if\nit is on the same disk as the input runs, the head may have moved away between writes\nof consecutive blocks. Thus we would have to add a total of 2 \u2308br\u2215bb\u2309seeks for each\nmerge pass, except the \ufb01nal pass (since we assume the \ufb01nal result is not written back\nto disk).\n6To be more precise, since we read each run separately and may get fewer than bbblocks when reading the end of a\nrun, we may require an extra seek for each run. We ignore this detail for simplicity.\n", "732": "704 Chapter 15 Query Processing\n2\u2308br\u2215M\u2309+\u2308br\u2215bb\u2309(2\u2308log\u230aM\u2215bb\u230b\u22121(br\u2215M)\u2309\u22121)\nApplying this equation to the example in Figure 15.4, we get a total of 8 +12\u2217(2\u2217\n2\u22121)=44 disk seeks if we set the number of bu\ufb00er blocks per run bbto 1.\n15.5 Join Operation\nIn this section, we study several algorithms for computing the join of relations, and we\nanalyze their respective costs.\nWe use the term equi-join to refer to a join of the form r\u22c8r.A=s.Bs,w h e r e AandB\nare attributes or sets of attributes of relations rands, respectively.\nWe use as a running example the expression:\nstudent \u22c8takes\nusing the same relation schemas that we used in Chapter 2. We assume the following\ninformation about the two relations:\n\u2022Number of records of student :nstudent=5000.\n\u2022Number of blocks of student :bstudent=100.\n\u2022Number of records of takes :ntakes=10, 000.\n\u2022Number of blocks of takes :btakes=400.\n15.5.1 Nested-Loop Join\nFigure 15.5 shows a simple algorithm to compute the theta join, r\u22c8\u03b8s,o ft w or e l a t i o n s\nrands. This algorithm is called the nested-loop join algorithm, since it basically consists\nof a pair of nested forloops. Relation ris called the outer relation and relation sthe\ninner relation of the join, since the loop for rencloses the loop for s. The algorithm\nuses the notation tr\u22c5ts,w h e r e trandtsare tuples; tr\u22c5tsdenotes the tuple constructed\nby concatenating the attribute values of tuples trandts.\nLike the linear \ufb01le-scan algorithm for selection, the nested-loop join algorithm re-\nquires no indices, and it can be used regardless of what the join condition is. Extending\nthe algorithm to compute the natural join is straightforward, since the natural join can\nbe expressed as a theta join followed by elimination of repeated attributes by a projec-\ntion. The only change required is an extra step of deleting repeated attributes from the\ntuple tr\u22c5ts, before adding it to the result.\nThe nested-loop join algorithm is expensive, since it examines every pair of tuples\nin the two relations. Consider the cost of the nested-loop join algorithm. The number\nof pairs of tuples to be considered is nr\u2217ns,w h e r e nrdenotes the number of tuples in\nr,a n d nsdenotes the number of tuples in s.F o re a c hr e c o r di n r,w eh a v et op e r f o r m\n", "733": "15.5 Join Operation 705\nfor each tuple trinrdo begin\nfor each tuple tsinsdo begin\ntest pair ( tr,ts) to see if they satisfy the join condition \u03b8\nif they do, add tr\u22c5tsto the result;\nend\nend\nFigure 15.5 Nested-loop join.\nac o m p l e t es c a no n s. In the worst case, the bu\ufb00er can hold only one block of each\nrelation, and a total of nr\u2217bs+brblock transfers would be required, where brandbs\ndenote the number of blocks containing tuples of rands, respectively. We need only\none seek for each scan on the inner relation ssince it is read sequentially, and a total\nofbrseeks to read r,l e a d i n gt oat o t a lo f nr+brseeks. In the best case, there is enough\nspace for both relations to \ufb01t simultaneously in memory, so each block would have to\nbe read only once; hence, only br+bsblock transfers would be required, along with\ntwo seeks.\nIf one of the relations \ufb01ts entirely in main memory, it is bene\ufb01cial to use that\nrelation as the inner relation, since the inner relation would then be read only once.\nTherefore, if sis small enough to \ufb01t in main memory, our strategy requires only a total\nbr+bsblock transfers and two seeks\u2014the same cost as that for the case where both\nrelations \ufb01t in memory.\nNow consider the natural join of student andtakes . Assume for now that we have\nno indices whatsoever on either relation, and that we are not willing to create any\nindex. We can use the nested loops to compute the join; assume that student is the\nouter relation and takes is the inner relation in the join. We will have to examine 5000\n\u22171 0 , 0 0 0=5 0 \u2217106pairs of tuples. In the worst case, the number of block transfers\nis 5000\u2217400+100=2,000,100, plus 5000 +100=5100 seeks. In the best-case\nscenario, however, we can read both relations only once and perform the computation.\nThis computation requires at most 100 +400=500 block transfers, plus two seeks\n\u2014a signi\ufb01cant improvement over the worst-case scenario. If we had used takes as the\nrelation for the outer loop and student for the inner loop, the worst-case cost of our\n\ufb01nal strategy would have been 10,000 \u2217100+400=1,000,400 block transfers, plus\n10,400 disk seeks. The number of block transfers is signi\ufb01cantly less, and although the\nnumber of seeks is higher, the overall cost is reduced, assuming tS=4 milliseconds\nandtT=0.1 milliseconds.\n15.5.2 Block Nested-Loop Join\nIf the bu\ufb00er is too small to hold either relation entirely in memory, we can still obtain\na major saving in block accesses if we process the relations on a per-block basis, rather\n", "734": "706 Chapter 15 Query Processing\nthan on a per-tuple basis. Figure 15.6 shows block nested-loop join , which is a variant of\nthe nested-loop join where every block of the inner relation is paired with every block\nof the outer relation. Within each pair of blocks, every tuple in one block is paired with\nevery tuple in the other block, to generate all pairs of tuples. As before, all pairs of\ntuples that satisfy the join condition are added to the result.\nThe primary di\ufb00erence in cost between the block nested-loop join and the basic\nnested-loop join is that, in the worst case, each block in the inner relation sis read only\nonce for each block in the outer relation, instead of once for each tuple in the outer\nrelation. Thus, in the worst case, there will be a total of br\u2217bs+brblock transfers,\nwhere brandbsdenote the number of blocks containing records of rands, respectively.\nEach scan of the inner relation requires one seek, and the scan of the outer relation\nrequires one seek per block, leading to a total of 2 \u2217brseeks. It is more e\ufb03cient to\nuse the smaller relation as the outer relation, in case neither of the relations \ufb01ts in\nmemory. In the best case, where the inner relation \ufb01ts in memory, there will be br+bs\nblock transfers and just two seeks (we would choose the smaller relation as the inner\nrelation in this case).\nNow return to our example of computing student \u22c8takes , using the block nested-\nloop join algorithm. In the worst case, we have to read each block of takes once for each\nblock of student . Thus, in the worst case, a total of 100 \u2217400+100=40,100 block\ntransfers plus 2 \u2217100=200 seeks are required. This cost is a signi\ufb01cant improvement\nover the 5000 \u2217400+100=2,000,100 block transfers plus 5100 seeks needed in the\nworst case for the basic nested-loop join. The best-case cost remains the same\u2014namely,\n100+400=500 block transfers and two seeks.\nThe performance of the nested-loop and block nested-loop procedures can be fur-\nther improved:\nfor each block Brofrdo begin\nfor each block Bsofsdo begin\nfor each tuple trinBrdo begin\nfor each tuple tsinBsdo begin\ntest pair ( tr,ts) to see if they satisfy the join condition\nif they do, add tr\u22c5tsto the result;\nend\nend\nend\nend\nFigure 15.6 Block nested-loop join.\n", "735": "15.5 Join Operation 707\n\u2022If the join attributes in a natural join or an equi-join form a key on the inner rela-\ntion, then for each outer relation tuple the inner loop can terminate as soon as the\n\ufb01rst match is found.\n\u2022In the block nested-loop algorithm, instead of using disk blocks as the blocking\nunit for the outer relation, we can use the biggest size that can \ufb01t in memory, while\nleaving enough space for the bu\ufb00ers of the inner relation and the output. In other\nwords, if memory has Mblocks, we read in M\u22122 blocks of the outer relation at\na time, and when we read each block of the inner relation we join it with all the\nM\u22122 blocks of the outer relation. This change reduces the number of scans of\nthe inner relation from brto\u2308br\u2215(M\u22122)\u2309,w h e r e bris the number of blocks of\nthe outer relation. The total cost is then \u2308br\u2215(M\u22122)\u2309\u2217bs+brblock transfers\nand 2 \u2308br\u2215(M\u22122)\u2309seeks.\n\u2022We can scan the inner loop alternately forward and backward. This scanning\nmethod orders the requests for disk blocks so that the data remaining in the bu\ufb00er\nfrom the previous scan can be reused, thus reducing the number of disk accesses\nneeded.\n\u2022If an index is available on the inner loop\u2019s join attribute, we can replace \ufb01le scans\nwith more e\ufb03cient index lookups. Section 15.5.3 describes this optimization.\n15.5.3 Indexed Nested-Loop Join\nIn a nested-loop join (Figure 15.5), if an index is available on the inner loop\u2019s join\nattribute, index lookups can replace \ufb01le scans. For each tuple trin the outer relation r,\nthe index is used to look up tuples in sthat will satisfy the join condition with tuple tr.\nThis join method is called an indexed nested-loop join ;i tc a nb eu s e dw i t he x i s t i n g\nindices, as well as with temporary indices created for the sole purpose of evaluating the\njoin.\nLooking up tuples in sthat will satisfy the join conditions with a given tuple tris\nessentially a selection on s. For example, consider student \u22c8takes . Suppose that we\nhave a student tuple with ID\u201c00128\u201d. Then, the relevant tuples in takes are those that\nsatisfy the selection \u201c ID= 00128\u201d.\nThe cost of an indexed nested-loop join can be computed as follows: For each tuple\nin the outer relation r, a lookup is performed on the index for s,a n dt h er e l e v a n tt u p l e s\nare retrieved. In the worst case, there is space in the bu\ufb00er for only one block of rand\none block of the index. Then, brI/Ooperations are needed to read relation r,w h e r e br\ndenotes the number of blocks containing records of r;e a c h I/Orequires a seek and a\nblock transfer, since the disk head may have moved in between each I/O.F o re a c ht u p l e\ninr, we perform an index lookup on s. Then, the cost of the join can be computed as\nbr(tT+tS)+nr\u2217c,w h e r e nris the number of records in relation r,a n d cis the cost of\na single selection on susing the join condition. We have seen in Section 15.3 how to\nestimate the cost of a single selection algorithm (possibly using indices); that estimate\ng i v e su st h ev a l u eo f c.\n", "736": "708 Chapter 15 Query Processing\nThe cost formula indicates that, if indices are available on both relations rands,i t\nis generally most e\ufb03cient to use the one with fewer tuples as the outer relation.\nFor example, consider an indexed nested-loop join of student \u22c8takes ,w i t h student\nas the outer relation. Suppose also that takes has a clustering B+-tree index on the join\nattribute ID, which contains 20 entries on average in each index node. Since takes has\n10,000 tuples, the height of the tree is 4, and one more access is needed to \ufb01nd the\nactual data. Since nstudent is 5000, the total cost is 100 +5000\u22175=25,100 disk\naccesses, each of which requires a seek and a block transfer. In contrast, as we saw\nbefore, 40,100 block transfers plus 200 seeks were needed for a block nested-loop join.\nAlthough the number of block transfers has been reduced, the seek cost has actually\nincreased, increasing the total cost since a seek is considerably more expensive than a\nblock transfer. However, if we had a selection on the student relation that reduces the\nnumber of rows signi\ufb01cantly, indexed nested-loop join could be signi\ufb01cantly faster than\nblock nested-loop join.\n15.5.4 Merge Join\nThe merge-join algorithm (also called the sort-merge-join algorithm) can be used to\ncompute natural joins and equi-joins. Let r(R)a n d s(S) be the relations whose natural\njoin is to be computed, and let R\u2229Sdenote their common attributes. Suppose that\nboth relations are sorted on the attributes R\u2229S. Then, their join can be computed by\na process much like the merge stage in the merge\u2013sort algorithm.\n15.5.4.1 Merge-Join Algorithm\nFigure 15.7 shows the merge-join algorithm. In the algorithm, JoinAttrs refers to the\nattributes in R\u2229S,a n d tr\u22c8ts,w h e r e trandtsare tuples that have the same values for\nJoinAttrs , denotes the concatenation of the attributes of the tuples, followed by project-\ning out repeated attributes. The merge-join algorithm associates one pointer with each\nrelation. These pointers point initially to the \ufb01rst tuple of the respective relations. As\nthe algorithm proceeds, the pointers move through the relation. A group of tuples of\none relation with the same value on the join attributes is read into Ss. The algorithm\nin Figure 15.7 requires that every set of tuples Ss\ufb01t in main memory; we discuss ex-\ntensions of the algorithm to avoid this requirement shortly. Then, the corresponding\ntuples (if any) of the other relation are read in and are processed as they are read.\nFigure 15.8 shows two relations that are sorted on their join attribute a1. It is\ninstructive to go through the steps of the merge-join algorithm on the relations shown\nin the \ufb01gure.\nThe merge-join algorithm of Figure 15.7 requires that each set Ssof all tuples with\nthe same value for the join attributes must \ufb01t in main memory. This requirement can\nusually be met, even if the relation sis large. If there are some join attribute values for\n", "737": "15.5 Join Operation 709\npr:= address of \ufb01rst tuple of r;\nps:= address of \ufb01rst tuple of s;\nwhile (ps\u2260nullandpr\u2260null) do\nbegin\nts:= tuple to which pspoints;\nSs:={ts};\nsetpst op o i n tt on e x tt u p l eo f s;\ndone :=false;\nwhile (notdone andps\u2260null) do\nbegin\nts\u2032:= tuple to which pspoints;\nif(ts\u2032[JoinAttrs ]=ts[JoinAttrs ])\nthen begin\nSs:=Ss\u222a{ts\u2032};\nsetpsto point to next tuple of s;\nend\nelsedone :=true;\nend\ntr:= tuple to which prpoints;\nwhile (pr\u2260nullandtr[JoinAttrs ]<ts[JoinAttrs ])do\nbegin\nsetprt op o i n tt on e x tt u p l eo f r;\ntr:= tuple to which prpoints;\nend\nwhile (pr\u2260nullandtr[JoinAttrs ]=ts[JoinAttrs ])do\nbegin\nfor each tsinSsdo\nbegin\naddts\u22c8trto result;\nend\nsetprt op o i n tt on e x tt u p l eo f r;\ntr:= tuple to which prpoints;\nend\nend.\nFigure 15.7 Merge join.\nwhich Ssis larger than available memory, a block nested-loop join can be performed\nfor such sets Ss, matching them with corresponding blocks of tuples in rwith the same\nvalues for the join attributes.\n", "738": "710 Chapter 15 Query Processing\na3\nb1\nd8\n13d\nf 7\nm 5\nq 6a    A\nb    G\nc     L\nd    N\nm   Ba1 a2 a1 a3\npr ps\nrs\nFigure 15.8 Sorted relations for merge join.\nIf either of the input relations randsis not sorted on the join attributes, they can be\nsorted \ufb01rst, and then the merge-join algorithm can be used. The merge-join algorithm\ncan also be easily extended from natural joins to the more general case of equi-joins.\n15.5.4.2 Cost Analysis\nOnce the relations are in sorted order, tuples with the same value on the join attributes\nare in consecutive order. Thereby, each tuple in the sorted order needs to be read only\nonce, and, as a result, each block is also read only once. Since it makes only a single\npass through both \ufb01les (assuming all sets Ss\ufb01t in memory), the merge-join method is\ne\ufb03cient; the number of block transfers is equal to the sum of the number of blocks in\nboth \ufb01les, br+bs.\nAssuming that bbbu\ufb00er blocks are allocated to each relation, the number of disk\nseeks required would be \u2308br\u2215bb\u2309+\u2308bs\u2215bb\u2309disk seeks. Since seeks are much more\nexpensive than data transfer, it makes sense to allocate multiple bu\ufb00er blocks to each\nrelation, provided extra memory is available. For example, with tT=0.1 milliseconds\nper 4-kilobyte block, and tS=4 milliseconds, the bu\ufb00er size is 400 blocks (or 1.6\nmegabytes), so the seek time would be 4 milliseconds for every 40 milliseconds of\ntransfer time; in other words, seek time would be just 10 percent of the transfer time.\nIf either of the input relations randsis not sorted on the join attributes, they must\nbe sorted \ufb01rst; the cost of sorting must then be added to the above costs. If some sets\nSsdo not \ufb01t in memory, the cost would increase slightly.\nSuppose the merge-join scheme is applied to our example of student \u22c8takes .\nT h ej o i na t t r i b u t eh e r ei s ID. Suppose that the relations are already sorted on the join\nattribute ID. In this case, the merge join takes a total of 400 +100=500 block transfers.\nIf we assume that in the worst case only one bu\ufb00er block is allocated to each input\nrelation (that is, bb=1), a total of 400 +100=500 seeks would also be required; in\nreality bbcan be set much higher since we need to bu\ufb00er blocks for only two relations,\nand the seek cost would be signi\ufb01cantly less.\n", "739": "15.5 Join Operation 711\nS u p p o s et h er e l a t i o n sa r en o ts o r t e d ,a n dt h em e m o r ys i z ei st h ew o r s tc a s e ,o n l y\nthree blocks. The cost is as follows:\n1.Using the formulae that we developed in Section 15.4, we can see that sorting\nrelation takes requires \u2308log3\u22121(400\u22153)\u2309=8 merge passes. Sorting of relation\ntakes then takes 400 \u2217(2\u2308log3\u22121(400\u22153)\u2309+1), or 6800, block transfers, with\n400 more transfers to write out the result. The number of seeks required is 2 \u2217\n\u2308400\u22153\u2309+400\u2217(2\u22178\u22121) or 6268 seeks for sorting, and 400 seeks for writing\nthe output, for a total of 6668 seeks, since only one bu\ufb00er block is available for\neach run.\n2.Similarly, sorting relation student takes \u2308log3\u22121(100\u22153)\u2309=6 merge passes and\n100\u2217(2\u2308log3\u22121(100\u22153)\u2309+1), or 1300, block transfers, with 100 more transfers\nto write it out. The number of seeks required for sorting student is 2\u2217\u2308100\u22153\u2309+\n100\u2217(2\u22176\u22121)=1168, and 100 seeks are required for writing the output, for\na total of 1268 seeks.\n3.Finally, merging the two relations takes 400 +100=500 block transfers and 500\nseeks.\nThus, the total cost is 9100 block transfers plus 8932 seeks if the relations are not\nsorted, and the memory size is just 3 blocks.\nWith a memory size of 25 blocks, and the relations not sorted, the cost of sorting\nfollowed by merge join would be as follows:\n1.Sorting the relation takes c a nb ed o n ew i t hj u s to n em e r g es t e pa n dt a k e sat o t a l\nof just 400 \u2217(2\u2308log24(400\u221525)\u2309+1) = 1200 block transfers. Similarly, sorting\nstudent takes 300 block transfers. Writing the sorted output to disk requires 400\n+100=500 block transfers, and the merge step requires 500 block transfers\nto read the data back. Adding up these costs gives a total cost of 2500 block\ntransfers.\n2.If we assume that only one bu\ufb00er block is allocated for each run, the number of\nseeks required in this case is 2 \u2217\u2308400\u221525\u2309+400+400=832 seeks for sorting\ntakes and writing the sorted output to disk, and similarly 2 \u2217\u2308100\u221525\u2309+100+\n100=208 for student , plus 400 +100 seeks for reading the sorted data in the\nmerge-join step. Adding up these costs gives a total cost of 1640 seeks.\nThe number of seeks can be signi\ufb01cantly reduced by setting aside more bu\ufb00er\nblocks for each run. For example, if 5 bu\ufb00er blocks are allocated for each run\nand for the output from merging the 4 runs of student , the cost is reduced to\n2\u2217\u2308100\u221525\u2309+\u2308100\u22155\u2309+\u2308100\u22155\u2309=48 seeks, from 208 seeks. If the merge-\njoin step sets aside 12 blocks each for bu\ufb00ering takes andstudent , the number\nof seeks for the merge-join step goes down to \u2308400\u221512\u2309+\u2308100\u221512\u2309=43, from\n500. The total number of seeks is then 251.\n", "740": "712 Chapter 15 Query Processing\nThus, the total cost is 2500 block transfers pl us 251 seeks if the relations are not sorted,\nand the memory size is 25 blocks.\n15.5.4.3 Hybrid Merge Join\nIt is possible to perform a variation of the merge-join operation on unsorted tuples, if\nsecondary indices exist on both join attributes. The algorithm scans the records through\nthe indices, resulting in their being retrieved in sorted order. This variation presents\na signi\ufb01cant drawback, however, since records may be scattered throughout the \ufb01le\nblocks. Hence, each tuple access could involve accessing a disk block, and that is costly.\nTo avoid this cost, we can use a hybrid merge-join technique that combines indices\nwith merge join. Suppose that one of the relations is sorted; the other is unsorted, but\nhas a secondary B+- t r e ei n d e xo nt h ej o i na t t r i b u t e s .T h e hybrid merge-join algorithm\nmerges the sorted relation with the leaf entries of the secondary B+-tree index. The\nresult \ufb01le contains tuples from the sorted relation and addresses for tuples of the un-\nsorted relation. The result \ufb01le is then sorted on the addresses of tuples of the unsorted\nrelation, allowing e\ufb03cient retrieval of the corresponding tuples, in physical storage or-\nder, to complete the join. Extensions of the technique to handle two unsorted relations\nare left as an exercise for you.\n15.5.5 Hash Join\nLike the merge-join algorithm, the hash-join algorithm can be used to implement natu-\nral joins and equi-joins. In the hash-join algorithm, a hash function his used to partition\ntuples of both relations. The basic idea is to partition the tuples of each of the relations\ninto sets that have the same hash value on the join attributes.\nWe assume that:\n\u2022his a hash function mapping JoinAttrs values to {0, 1,\u2026,nh},w h e r e JoinAttrs\ndenotes the common attributes of randsused in the natural join.\n\u2022r0,r1,\u2026,rnhdenote partitions of rtuples, each initially empty. Each tuple tr\u2208r\nis put in partition ri,w h e r e i=h(tr[JoinAttrs ]).\n\u2022s0,s1,...,snhdenote partitions of stuples, each initially empty. Each tuple ts\u2208sis\nput in partition si,w h e r e i=h(ts[JoinAttrs ]).\nThe hash function hshould have the \u201cgoodness\u201d properties of randomness and uni-\nformity that we discussed in Chapter 14. Figure 15.9 depicts the partitioning of the\nrelations.\n15.5.5.1 Basics\nThe idea behind the hash-join algorithm is this: Suppose that an rtuple and an stuple\nsatisfy the join condition; then, they have the same value for the join attributes. If that\nvalue is hashed to some value i,t h e rtuple has to be in riand the stuple in si. Therefore,\n", "741": "15.5 Join Operation 713\n0\n1\n2\n3\n40\n1\n2\n3\n4rs.\n.\n.\n..\n.\n.\n.\npartitions\nof rpartitions\nof s\nFigure 15.9 Hash partitioning of relations.\nrtuples in rineed only be compared with stuples in si; they do not need to be compared\nwith stuples in any other partition.\nFor example, if dis a tuple in student ,ca tuple in takes ,a n d ha hash function\non the IDattributes of the tuples, then dandcmust be tested only if h(c)=h(d). If\nh(c)\u2260h(d), then canddmust have di\ufb00erent values for ID.H o w e v e r ,i f h(c)=h(d), we\nmust test canddto see whether the values in their join attributes are the same, since\nit is possible that canddhave di\ufb00erent iids that have the same hash value.\nFigure 15.10 shows the details of the hash-join algorithm to compute the natural\njoin of relations rands. As in the merge-join algorithm, tr\u22c8tsdenotes the concatena-\ntion of the attributes of tuples trandts, followed by projecting out repeated attributes.\nAfter the partitioning of the relations, the rest of the hash-join code performs a sepa-\nrate indexed nested-loop join on each of the partition pairs i,f o ri=0,\u2026,nh.T od os o ,\nit \ufb01rst builds ah a s hi n d e xo ne a c h si,a n dt h e n probes (that is, looks up si) with tuples\nfrom ri.T h er e l a t i o n sis the build input ,a n d ris the probe input .\nThe hash index on siis built in memory, so there is no need to access the disk to\nretrieve the tuples. The hash function used to build this hash index must be di\ufb00erent\nfrom the hash function hused earlier, but it is still applied to only the join attributes. In\nthe course of the indexed nested-loop join, the system uses this hash index to retrieve\nrecords that match records in the probe input.\nThe build and probe phases require only a single pass through both the build and\nprobe inputs. It is straightforward to extend the hash-join algorithm to compute general\nequi-joins.\nThe value nhmust be chosen to be large enough such that, for each i,t h et u p l e si n\nthe partition siof the build relation, along with the hash index on the partition, \ufb01t in\nmemory. It is not necessary for the partitions of the probe relation to \ufb01t in memory. It is\n", "742": "714 Chapter 15 Query Processing\n/* Partition s*/\nfor each tuple tsinsdo begin\ni:=h(ts[JoinAttrs ]);\nHsi:=Hsi\u222a{ts};\nend\n/* Partition r*/\nfor each tuple trinrdo begin\ni:=h(tr[JoinAttrs ]);\nHri:=Hri\u222a{tr};\nend\n/* Perform join on each partition */\nfori:= 0tonhdo begin\nread Hsiand build an in-memory hash index on it;\nfor each tuple trinHrido begin\nprobe the hash index on Hsito locate all tuples ts\nsuch that ts[JoinAttrs ]=tr[JoinAttrs ];\nfor each matching tuple tsinHsido begin\naddtr\u22c8tsto the result;\nend\nend\nend\nFigure 15.10 Hash join.\nbest to use the smaller input relation as the build relation. If the size of the build relation\nisbsblocks, then, for each of the nhpartitions to be of size less than or equal to M,nh\nmust be at least \u2308bs\u2215M\u2309.M o r ep r e c i s e l ys t a t e d ,w eh a v et oa c c o u n tf o rt h ee x t r as p a c e\noccupied by the hash index on the partition as well, so nhshould be correspondingly\nlarger. For simplicity, we sometimes ignore the space requirement of the hash index in\nour analysis.\n15.5.5.2 Recursive Partitioning\nIf the value of nhis greater than or equal to the number of blocks of memory, the rela-\ntions cannot be partitioned in one pass, since there will not be enough bu\ufb00er blocks.\nInstead, partitioning has to be done in repeated passes. In one pass, the input can be\nsplit into at most as many partitions as there are blocks available for use as output\nbu\ufb00ers. Each bucket generated by one pass is separately read in and partitioned again\nin the next pass, to create smaller partitions. The hash function used in a pass is dif-\nferent from the one used in the previous pass. The system repeats this splitting of the\n", "743": "15.5 Join Operation 715\ninput until each partition of the build input \ufb01ts in memory. Such partitioning is called\nrecursive partitioning .\nA relation does not need recursive partitioning if M>nh+1, or equivalently\nM>(bs\u2215M)+1, which simpli\ufb01es (approximately) to M>\u221a\nbs. For example, consider\na memory size of 12 megabytes, divided into 4-kilobyte blocks; it would contain a total\nof 3-kilobyte (3072) blocks. We can use a memory of this size to partition relations\nof size up to 3-kilobyte \u22173-kilobyte blocks, which is 36 gigabytes. Similarly, a relation\nof size 1 gigabyte requires just over\u221a\n256K blocks, or 2 megabytes, to avoid recursive\npartitioning.\n15.5.5.3 Handling of Over\ufb02ows\nHash-table over\ufb02ow occurs in partition iof the build relation sif the hash index on si\nis larger than main memory. Hash-table over\ufb02ow can occur if there are many tuples in\nthe build relation with the same values for the join attributes, or if the hash function\ndoes not have the properties of randomness and uniformity. In either case, some of\nthe partitions will have more tuples than the average, whereas others will have fewer;\npartitioning is then said to be skewed .\nWe can handle a small amount of skew by increasing the number of partitions so\nthat the expected size of each partition (including the hash index on the partition) is\nsomewhat less than the size of memory. The number of partitions is therefore increased\nby a small value, called the fudge factor , that is usually about 20 percent of the number\nof hash partitions computed as described in Section 15.5.5.\nEven if, by using a fudge factor, we are conservative on the sizes of the partitions,\nover\ufb02ows can still occur. Hash-table over\ufb02ows can be handled by either over\ufb02ow reso-\nlution orover\ufb02ow avoidance .Over\ufb02ow resolution is performed during the build phase if\na hash-index over\ufb02ow is detected. Over\ufb02ow resolution proceeds in this way: If si,f o r\nanyi, is found to be too large, it is further partitioned into smaller partitions by using\na di\ufb00erent hash function. Similarly, riis also partitioned using the new hash function,\nand only tuples in the matching partitions need to be joined.\nIn contrast, over\ufb02ow avoidance performs the partitioning carefully, so that over\ufb02ows\nnever occur during the build phase. In over\ufb02ow avoidance, the build relation sis initially\npartitioned into many small partitions, and then some partitions are combined in such\na way that each combined partition \ufb01ts in memory. The probe relation ris partitioned\nin the same way as the combined partitions on s,b u tt h es i z e so f rido not matter.\nIf a large number of tuples in shave the same value for the join attributes, the\nresolution and avoidance techniques may fail on some partitions. In that case, instead\nof creating an in-memory hash index and usin g a nested-loop join to join the partitions,\nwe can use other join techniques, such as block nested-loop join, on those partitions.\n15.5.5.4 Cost of Hash Join\nWe now consider the cost of a hash join. Our analysis assumes that there is no hash-\ntable over\ufb02ow. First, consider the case where recursive partitioning is not required.\n", "744": "716 Chapter 15 Query Processing\n\u2022The partitioning of the two relations randscalls for a complete reading of both\nrelations and a subsequent writing back of them. This operation requires 2( br+bs)\nblock transfers, where brandbsdenote the number of blocks containing records\nof relations rands, respectively. The build and probe phases read each of the\npartitions once, calling for further br+bsblock transfers. The number of blocks\noccupied by partitions could be slightly more than br+bs, as a result of partially\n\ufb01lled blocks. Accessing such partially \ufb01lled blocks can add an overhead of at most\n2nhfor each of the relations, since each of the nhpartitions could have a partially\n\ufb01lled block that has to be written and read back. Thus, a hash join is estimated to\nrequire:\n3(br+bs)+4nh\nblock transfers. The overhead 4 nhis usually quite small compared to br+bsand\ncan be ignored.\n\u2022Assuming bbblocks are allocated for the input bu\ufb00er and each output bu\ufb00er, parti-\ntioning requires a total of 2( \u2308br\u2215bb\u2309+\u2308bs\u2215bb\u2309) seeks. The build and probe phases\nrequire only one seek for each of the nhpartitions of each relation, since each par-\ntition can be read sequentially. The hash join thus requires 2( \u2308br\u2215bb\u2309+\u2308bs\u2215bb\u2309)+\n2nhseeks.\nNow consider the case where recursive partitioning is required. Again we assume\nthatbbblocks are allocated for bu\ufb00ering each partition. Each pass then reduces the size\nof each of the partitions by an expected factor of \u230aM\u2215bb\u230b\u22121; and passes are repeated\nuntil each partition is of size at most Mblocks. The expected number of passes required\nfor partitioning sis therefore \u2308log\u230aM\u2215bb\u230b\u22121(bs\u2215M)\u2309.\n\u2022Since, in each pass, every block of sis read in and written out, the total number\nof block transfers for partitioning of sis 2bs\u2308log\u230aM\u2215bb\u230b\u22121(bs\u2215M)\u2309. The number of\npasses for partitioning of ris the same as the number of passes for partitioning of\ns, therefore the join is estimated to require\n2(br+bs)\u2308log\u230aM\u2215bb\u230b\u22121(bs\u2215M)\u2309+br+bs\nblock transfers.\n\u2022Ignoring the relatively small number of seeks during the build and probe phases,\nhash join with recursive partitioning requires\n2(\u2308br\u2215bb\u2309+\u2308bs\u2215bb\u2309)\u2308log\u230aM\u2215bb\u230b\u22121(bs\u2215M)\u2309\ndisk seeks.\nConsider, for example, the natural join takes\u22c8student . With a memory size of 20\nblocks, the student relation can be partitioned into \ufb01ve p artitions, each of size 20 blocks,\nwhich size will \ufb01t into memory. Only one pass is required for the partitioning. The\n", "745": "15.5 Join Operation 717\nrelation takes is similarly partitioned into \ufb01ve partitions, each of size 80. Ignoring the\ncost of writing partially \ufb01lled blocks, the cost is 3(100 +400)=1500 block transfers.\nThere is enough memory to allocate the bu\ufb00ers for the input and each of the \ufb01ve outputs\nduring partitioning (i.e, bb=3) leading to 2( \u2308100\u22153\u2309+\u2308400\u22153\u2309)=336 seeks.\nThe hash join can be improved if the main memory size is large. When the entire\nbuild input can be kept in main memory, nhcan be set to 0; then, the hash-join algorithm\nexecutes quickly, without partitioning the relations into temporary \ufb01les, regardless of\nthe probe input\u2019s size. The cost estimate goes down to br+bsblock transfers and two\nseeks.\nIndexed nested loops join can have a much lower cost than hash join in case the\nouter relation is small, and the index lookups fetch only a few tuples from the inner\n(indexed) relation. However, in case a secondary index is used, and the number of\ntuples in the outer relation is large, indexed nested loops join can have a very high cost,\nas compared to hash join. If the number of tuples in the outer relation is known at\nquery optimization time, the best join algorithm can be chosen at that time. However,\nin some cases, for example, when there is a selection condition on the outer input, the\noptimizer makes a decision based on an estimate that may potentially be imprecise. The\nnumber of tuples in the outer relation may be found only at runtime, for example, after\nexecuting selection. Some systems allow a dynamic choice between the two algorithms\nat run time, after \ufb01nding the number of tuples in the outer input.\n15.5.5.5 Hybrid Hash Join\nThehybrid hash-join algorithm performs another optimization; it is useful when mem-\nory sizes are relatively large but not all of the build relation \ufb01ts in memory. The parti-\ntioning phase of the hash-join algorithm needs a minimum of one block of memory as\na bu\ufb00er for each partition that is created, and one block of memory as an input bu\ufb00er.\nTo reduce the impact of seeks, a larger number of blocks would be used as a bu\ufb00er;\nletbbdenote the number of blocks used as a bu\ufb00er for the input and for each parti-\nt i o n .H e n c e ,at o t a lo f( nh+1)\u2217bbblocks of memory are needed for partitioning the\ntwo relations. If memory is larger than ( nh+1)\u2217bb,w ec a nu s et h er e s to fm e m o r y\n(M\u2212(nh+1)\u2217bbblocks) to bu\ufb00er the \ufb01rst partition of the build input (i.e, s0)s o\nthat it will not need to be written out and read back in. Further, the hash function is\ndesigned in such a way that the hash index on s0\ufb01ts in M\u2212(nh+1)\u2217bbblocks, in\norder that, at the end of partitioning of s,s0is completely in memory and a hash index\ncan be built on s0.\nWhen the system partitions r,i ta g a i nd o e sn o tw r i t et u p l e si n r0to disk; instead, as\nit generates them, the system uses them to probe the memory-resident hash index on\ns0, and to generate output tuples of the join. After they are used for probing, the tuples\ncan be discarded, so the partition r0does not occupy any memory space. Thus, a write\nand a read access have been saved for each block of both r0ands0. The system writes\nout tuples in the other partitions as usual and joins them later. The savings of hybrid\nhash join can be signi\ufb01cant if the build input is only slightly bigger than memory.\n", "746": "718 Chapter 15 Query Processing\nIf the size of the build relation is bs,nhis approximately equal to bs\u2215M.T h u s ,h y b r i d\nhash join is most useful if M>>(bs\u2215M)\u2217bb,o rM>>\u221a\nbs\u2217bb,w h e r et h en o t a t i o n\n>>denotes much larger than . For example, suppose the block size is 4 kilobytes, the\nbuild relation size is 5 gigabytes, and bbis 20. Then, the hybrid hash-join algorithm\nis useful if the size of memory is signi\ufb01cantly more than 20 megabytes; memory sizes\nof gigabytes or more are common on computers today. If we devote 1 gigabyte for the\njoin algorithm, s0would be nearly 1 gigabyte, and hybrid hash join would be nearly 20\npercent cheaper than hash join.\n15.5.6 Complex Joins\nNested-loop and block nested-loop joins can be used regardless of the join conditions.\nThe other join techniques are more e\ufb03cient than the nested-loop join and its variants,\nbut they can handle only simple join condition s, such as natural joins or equi-joins. We\ncan implement joins with complex join conditions, such as conjunctions and disjunc-\ntions, by using the e\ufb03cient join techniques, if we apply the techniques developed in\nSection 15.3.3 for handling complex selections.\nConsider the following join with a conjunctive condition:\nr\u22c8\u03b81\u2227\u03b82\u2227\u22ef\u2227\u03b8ns\nOne or more of the join techniques described earlier may be applicable for joins on the\nindividual conditions r\u22c8\u03b81s,r\u22c8\u03b82s,r\u22c8\u03b83s,a n ds oo n .W ec a nc o m p u t et h eo v e r a l l\njoin by \ufb01rst computing the result of one of these simpler joins r\u22c8\u03b8is; each pair of\ntuples in the intermediate result consists of one tuple from rand one from s.T h er e s u l t\nof the complete join consists of those tuples in the intermediate result that satisfy the\nremaining conditions:\n\u03b81\u2227\u22ef\u2227\u03b8i\u22121\u2227\u03b8i+1\u2227\u22ef\u2227\u03b8n\nThese conditions can be tested as tuples in r\u22c8\u03b8isare being generated.\nA join whose condition is disjunctive can be computed in this way. Consider:\nr\u22c8\u03b81\u2228\u03b82\u2228\u22ef\u2228\u03b8ns\nThe join can be computed as the union of the records in individual joins r\u22c8\u03b8is:\n(r\u22c8\u03b81s)\u222a(r\u22c8\u03b82s)\u222a\u22ef\u222a(r\u22c8\u03b8ns)\nSection 15.6 describes algorithms for computing the union of relations.\n", "747": "15.6 Other Operations 719\n15.5.7 Joins over Spatial Data\nThe join algorithms we have presented make no speci\ufb01c assumptions about the type of\ndata being joined, but they do assume the use of standard comparison operations such\nas equality, less than, or greater than, where the values are linearly ordered.\nSelection and join conditions on spatial data involve comparison operators that\ncheck if one region contains or overlaps another, or whether a region contains a partic-\nular point; and the regions may be multi-dimensional. Comparisons may pertain also\nto the distance between points, for example, \ufb01nding a set of points closest to a given\npoint in a two-dimensional space.\nMerge-join cannot be used with such comparison operations, since there is no sim-\nple sort order over spatial data in two or more dimensions. Partitioning of data based\non hashing is also not applicable, since there is no way to ensure that tuples that sat-\nisfy an overlap or containment predicate are hashed to the same value. Nested loops\njoin can always be used regardless of the complexity of the conditions, but can be very\nine\ufb03cient on large datasets.\nIndexed nested-loops join can however be used, if appropriate spatial indices are\navailable. In Section 14.10, we saw several types of indices for spatial data, including\nR-trees, k-d trees, k-d-B trees, and quadtrees. Additional details on those indices appear\nin Section 24.4. These index structures enable e\ufb03cient retrieval of spatial data based\non predicates such as contains, contained in, or overlaps, and can also be e\ufb00ectively\nused to \ufb01nd nearest neighbors.\nMost major database systems today incorporate support for indexing spatial data,\nand make use of them when processing queries using spatial comparison conditions.\n15.6 Other Operations\nOther relational operations and extended relational operations\u2014such as duplicate elim-\nination, projection, set operations, outer join, and aggregation\u2014can be implemented as\noutlined in Section 15.6.1 through Section 15.6.5.\n15.6.1 Duplicate Elimination\nWe can implement duplicate elimination easily by sorting. Identical tuples will appear\nadjacent to each other as a result of sorting, and all but one copy can be removed. With\nexternal sort\u2013merge, duplicates found while a run is being created can be removed\nbefore the run is written to disk, thereby reducing the number of block transfers. The\nremaining duplicates can be eliminated during merging, and the \ufb01nal sorted run has\nno duplicates. The worst-case cost estimate for duplicate elimination is the same as the\nworst-case cost estimate for sorting of the relation.\nWe can also implement duplicate elimination by hashing, as in the hash-join algo-\nrithm. First, the relation is partitioned on the basis of a hash function on the whole\ntuple. Then, each partition is read in, and an in-memory hash index is constructed.\n", "748": "720 Chapter 15 Query Processing\nWhile constructing the hash index, a tuple is inserted only if it is not already present.\nOtherwise, the tuple is discarded. After all tuples in the partition have been processed,\nthe tuples in the hash index are written to the result. The cost estimate is the same as\nthat for the cost of processing (partitioning and reading each partition) of the build\nrelation in a hash join.\nBecause of the relatively high cost of duplicate elimination, SQLrequires an explicit\nrequest by the user to remove duplicates; otherwise, the duplicates are retained.\n15.6.2 Projection\nWe can implement projection easily by performing projection on each tuple, which\ngives a relation that could have duplicate records, and then removing duplicate rec-\nords. Duplicates can be eliminated by the methods described in Section 15.6.1. If the at-\ntributes in the projection list include a key of the relation, no duplicates will exist; hence,\nduplicate elimination is not required. Generalized projection can be implemented in\nthe same way as projection.\n15.6.3 Set Operations\nWe can implement the union ,intersection ,a n d set-di\ufb00erence operations by \ufb01rst sorting\nboth relations, and then scanning once through each of the sorted relations to produce\nthe result. In r\u222as, when a concurrent scan of both relations reveals the same tuple in\nboth \ufb01les, only one of the tuples is retained. The result of r\u2229swill contain only those\ntuples that appear in both relations. We implement set di\ufb00erence ,r\u2212s, similarly, by\nretaining tuples in ronly if they are absent in s.\nFor all these operations, only one scan of the two sorted input relations is required,\nso the cost is br+bsblock transfers if the relations are sorted in the same order. As-\nsuming a worst case of one block bu\ufb00er for each relation, a total of br+bsdisk seeks\nwould be required in addition to br+bsblock transfers. The number of seeks can be\nreduced by allocating extra bu\ufb00er blocks.\nIf the relations are not sorted initially, the cost of sorting has to be included. Any\nsort order can be used in the evaluation of set operations, provided that both inputs\nhave that same sort order.\nHashing provides another way to implement these set operations. The \ufb01rst step in\neach case is to partition the two relations by the same hash function and thereby create\nthe partitions r0,r1,\u2026,rnhands0,s1,\u2026,snh. Depending on the operation, the system\nthen takes these steps on each partition i=0, 1,\u2026,nh:\n\u2022r\u222as\n1.Build an in-memory hash index on ri.\n2.Add the tuples in sito the hash index only if they are not already present.\n3.Add the tuples in the hash index to the result.\n", "749": "15.6 Other Operations 721\nNote 15.1 Answering Keyword Queries\nKeyword search on documents is widely used in the context of web search. In\nits simplest form, a keyword query provides a set of words K1,K2,\u2026,Kn,a n dt h e\ngoal is to \ufb01nd documents difrom a collection of documents Dsuch that dicon-\ntains all the keywords in the query. Real-life keyword search is more complicated,\nsince it requires ranking of documents based on various metrics such TF\u2013IDF and\nPageRank, as we saw earlier in Section 8.3.\nDocuments that contain a speci\ufb01ed keyword can be located e\ufb03ciently by using\nan index (often referred to as an inverted index ) that maps each keyword Kito a\nlistSiof identi\ufb01ers of the documents that contain Ki. The list is kept sorted. For\nexample, if documents d1,d9andd21contain the term \u201cSilberschatz\u201d, the inverted\nlist for the keyword Silberschatz would be \u201c d1;d9;d21\u201d. Compression techniques\na r eu s e dt or e d u c et h es i z eo ft h ei n v e r t e dl i s t s .AB+-tree index can be used to\nmap each keyword Kito its associated inverted list Si.\nTo answer a query with keyword K1,K2,\u2026,Kn, we retrieve the inverted list Si\nfor each keyword Ki, and then compute the intersection S1\u2229S2\u2229\u22ef\u2229Snto \ufb01nd\ndocuments that appear in all the lists. Since the lists are sorted, the intersection can\nbe e\ufb03ciently implemented by merging the lists using concurrent scans of all the\nlists. Many information-retrieval systems return documents that contain several,\neven if not all, of the keywords; the merge step can be easily modi\ufb01ed to output\ndocuments that contain at least kof the nkeywords.\nTo support ranking of keyword-query results, extra information can be stored\nin each inverted list, including the inverse document frequency of the term, and\nfor each document the PageRank, the term frequency of the term, as well as the\npositions within the document where the term occurs. This information can be\nused to compute scores that are then used to rank the documents. For example,\ndocuments where the keywords occur close to each other may receive a higher\nscore for keyword proximity than those where they occur farther from each other.\nThe keyword proximity score may be combined with the TF\u2013IDF score, and PageR-\nank to compute an overall score. Documents are then ranked on this score. Since\nmost web searches retrieve only the top few answers, search engines incorporate a\nnumber of optimizations that help to \ufb01nd the top few answers e\ufb03ciently, without\ncomputing the full list and then \ufb01nding the ranking. References providing further\ndetails may be found in the Further Reading section at the end of the chapter.\n\u2022r\u2229s\n1.Build an in-memory hash index on ri.\n2.For each tuple in si, probe the hash index and output the tuple to the result\nonly if it is already present in the hash index.\n", "750": "722 Chapter 15 Query Processing\n\u2022r\u2212s\n1.Build an in-memory hash index on ri.\n2.For each tuple in si, probe the hash index, and, if the tuple is present in the\nhash index, delete it from the hash index.\n3.Add the tuples remaining in the hash index to the result.\n15.6.4 Outer Join\nRecall the outer-join operations described in Section 4.1.3. For example, the natural left\nouter join takes\u27d5student contains the join of takes andstudent ,a n d ,i na d d i t i o n ,f o r\neach takes tuple tthat has no matching tuple in student (i.e, where IDis not in student ),\nthe following tuple t1is added to the result. For all attributes in the schema of takes ,\ntuple t1has the same values as tuple t. The remaining attributes (from the schema of\nstudent )o ft u p l e t1contain the value null.\nWe can implement the outer-join operations by using one of two strategies:\n1.Compute the corresponding join, and then add further tuples to the join result to\nget the outer-join result. Consider the left outer-join operation and two relations:\nr(R)a n d s(S). To evaluate r\u27d5\u03b8s,w e\ufb01 r s tc o m p u t e r\u22c8\u03b8sand save that result\nas temporary relation q1.N e x t ,w ec o m p u t e r\u2212\u03a0R(q1)t oo b t a i nt h o s et u p l e si n\nrthat do not participate in the theta join. We can use any of the algorithms for\ncomputing the joins, projection, and set di\ufb00erence described earlier to compute\nthe outer joins. We pad each of these tuples with null values for attributes from\ns,a n da d di tt o q1to get the result of the outer join.\nThe right outer-join operation r\u27d6\u03b8sis equivalent to s\u27d5\u03b8rand can therefore\nbe implemented in a symmetric fashion to the left outer join. We can implement\nthe full outer-join operation r\u27d7\u03b8sby computing the join r\u22c8sand then adding\nthe extra tuples of both the left and right outer-join operations, as before.\n2.Modify the join algorithms. It is easy to extend the nested-loop join algorithms\nto compute the left outer join: Tuples in the outer relation that do not match any\ntuple in the inner relation are written to the output after being padded with null\nvalues. However, it is hard to extend the nested-loop join to compute the full outer\njoin.\nNatural outer joins and outer joins with an equi-join condition can be com-\nputed by extensions of the merge-join and hash-join algorithms. Merge join can\nbe extended to compute the full outer join as follows: When the merge of the two\nrelations is being done, tuples in either relation that do not match any tuple in\nthe other relation can be padded with nulls and written to the output. Similarly,\nwe can extend merge join to compute the left and right outer joins by writing out\nnonmatching tuples (padded with nulls) from only one of the relations. Since the\nrelations are sorted, it is easy to detect whether or not a tuple matches any tuples\n", "751": "15.6 Other Operations 723\nfrom the other relation. For example, when a merge join of takes andstudent is\ndone, the tuples are read in sorted order of ID, and it is easy to check, for each\ntuple, whether there is a matching tuple in the other.\nThe cost estimates for implementing outer joins using the merge-join algo-\nrithm are the same as are those for the corresponding join. The only di\ufb00erence\nlies in the size of the result, and therefore in the block transfers for writing it out,\nwhich we did not count in our earlier cost estimates.\nThe extension of the hash-join algorithm to compute outer joins is left for you\nto do as an exercise (Exercise 15.21).\n15.6.5 Aggregation\nRecall the aggregation function (operator) , discussed in Section 3.7. For example, the\nfunction\nselect dept\nname ,avg(salary )\nfrom instructor\ngroup by dept\nname ;\ncomputes the average salary in each university department.\nThe aggregation operation can be implemented in the same way as duplicate elim-\nination. We use either sorting or hashing, just as we did for duplicate elimination, but\nbased on the grouping attributes ( dept\nname in the preceding example). However, in-\nstead of eliminating tuples with the same value for the grouping attribute, we gather\nthem into groups and apply the aggregation operations on each group to get the result.\nThe cost estimate for implementing the aggregation operation is the same as the\ncost of duplicate elimination for aggregate functions such as min,max,sum,count ,a n d\navg.\nInstead of gathering all the tuples in a group and then applying the aggregation\noperations, we can implement the aggregation operations sum,min,max,count ,a n d\navgon the \ufb02y as the groups are being constructed. For the case of sum,min,a n d max,\nwhen two tuples in the same group are found, the system replaces them with a single\ntuple containing the sum,min,o rmax, respectively, of the columns being aggregated.\nFor the count operation, it maintains a running count for each group for which a tuple\nhas been found. Finally, we implement the avgoperation by computing the sum and\nthe count values on the \ufb02y, and \ufb01nally dividing the sum by the count to get the average.\nIf all tuples of the result \ufb01t in memory, the sort-based and the hash-based imple-\nmentations do not need to write any tuples to disk. As the tuples are read in, they can\nbe inserted in a sorted tree structure or in a hash index. When we use on-the-\ufb02y ag-\ngregation techniques, only one tuple needs to be stored for each of the groups. Hence,\nthe sorted tree structure or hash index \ufb01ts in memory, and the aggregation can be pro-\ncessed with just brblock transfers (and 1 seek) instead of the 3 brtransfers (and a worst\ncase of up to 2 brseeks) that would be required otherwise.\n", "752": "724 Chapter 15 Query Processing\n15.7 Evaluation of Expressions\nSo far, we have studied how individual relational operations are carried out. Now we\nconsider how to evaluate an expression cont aining multiple operations. The obvious\nway to evaluate an expression is simply to evaluate one operation at a time, in an ap-\npropriate order. The result of each evaluation is materialized in a temporary relation\nfor subsequent use. A disadvantage to this approach is the need to construct the tem-\nporary relations, which (unless they are small) must be written to disk. An alternative\napproach is to evaluate several operations simultaneously in a pipeline , with the results\nof one operation passed on to the next, without the need to store a temporary relation.\nIn Section 15.7.1 and Section 15.7.2, we consider both the materialization approach\nand the pipelining approach. We shall see that the costs of these approaches can di\ufb00er\nsubstantially, but also that there are cases where only the materialization approach is\nfeasible.\n15.7.1 Materialization\nIt is easiest to understand intuitively how to evaluate an expression by looking at a\npictorial representation of the expression in an operator tree . Consider the expression:\n\u03a0name(\u03c3building=\u201cWatson\u201d (department )\u22c8instructor )\nin Figure 15.11.\nIf we apply the materialization approach, we start from the lowest-level operations\nin the expression (at the bottom of the tree). In our example, there is only one such\noperation: the selection operation on department . The inputs to the lowest-level oper-\nations are relations in the database. We execute these operations using the algorithms\nthat we studied earlier, and we store the results in temporary relations. We can use these\ntemporary relations to execute the operations at the next level up in the tree, where the\ninputs now are either temporary relations or relations stored in the database. In our\n\u03a0\n\u03c3name\nbuilding  = \u201cWatson\u201d\ndepartmentinstructo r\nFigure 15.11 Pictorial representation of an expression.\n", "753": "15.7 Evaluation of Expressions 725\nexample, the inputs to the join are the instructor relation and the temporary relation\ncreated by the selection on department . The join can now be evaluated, creating another\ntemporary relation.\nBy repeating the process, we will eventually evaluate the operation at the root of the\ntree, giving the \ufb01nal result of the expression. In our example, we get the \ufb01nal result by\nexecuting the projection operation at the root of the tree, using as input the temporary\nrelation created by the join.\nEvaluation as just described is called materialized evaluation ,s i n c et h er e s u l t so f\neach intermediate operation are created (materialized) and then are used for evaluation\nof the next-level operations.\nThe cost of a materialized evaluation is not simply the sum of the costs of the\noperations involved. When we computed the cost estimates of algorithms, we ignored\nthe cost of writing the result of the operation to disk. To compute the cost of evaluating\nan expression as done here, we have to add the costs of all the operations, as well as\nthe cost of writing the intermediate results to disk. We assume that the records of the\nresult accumulate in a bu\ufb00er, and, when the bu\ufb00er is full, they are written to disk. The\nnumber of blocks written out, br,c a nb ee s t i m a t e da s nr\u2215fr,w h e r e nris the estimated\nnumber of tuples in the result relation randfris the blocking factor of the result relation,\nthat is, the number of records of rthat will \ufb01t in a block. In addition to the transfer\ntime, some disk seeks may be required, since the disk head may have moved between\nsuccessive writes. The number of seeks can be estimated as \u2308br\u2215bb\u2309where bbis the size\nof the output bu\ufb00er (measured in blocks).\nDouble bu\ufb00ering (using two bu\ufb00ers, with one continuing execution of the algorithm\nwhile the other is being written out) allows the algorithm to execute more quickly by\nperforming CPU activity in parallel with I/Oactivity. The number of seeks can be re-\nduced by allocating extra blocks to the output bu\ufb00er and writing out multiple blocks\nat once.\n15.7.2 Pipelining\nWe can improve query-evaluation e\ufb03ciency by reducing the number of temporary \ufb01les\nthat are produced. We achieve this reduction by combining several relational operations\ninto a pipeline of operations, in which the results of one operation are passed along\nto the next operation in the pipeline. Evaluation as just described is called pipelined\nevaluation .\nFor example, consider the expression ( \u03a0a1,a2(r\u22c8s)). If materialization were ap-\nplied, evaluation would involve creating a temporary relation to hold the result of the\njoin and then reading back in the result to perform the projection. These operations\ncan be combined: When the join operation generates a tuple of its result, it passes that\ntuple immediately to the project operatio n for processing. By combining the join and\nthe projection, we avoid creating the intermediate result and instead create the \ufb01nal\nresult directly.\n", "754": "726 Chapter 15 Query Processing\nCreating a pipeline of operations can provide two bene\ufb01ts:\n1.It eliminates the cost of reading and writing temporary relations, reducing the\ncost of query evaluation. Note that the cost formulae that we saw earlier for each\noperation included the cost of reading the result from disk. If the input to an\noperator oiis pipelined from a preceding operator oj,t h ec o s to f oishould not\ninclude the cost of reading the input from disk; the cost formulae that we saw\nearlier can be modi\ufb01ed accordingly.\n2.It can start generating query results quickly, if the root operator of a query-\nevaluation plan is combined in a pipeline with its inputs. This can be quite useful\nif the results are displayed to a user as they are generated, since otherwise there\nmay be a long delay before the user sees any query results.\n15.7.2.1 Implementation of Pipelining\nWe can implement a pipeline by constructing a single, complex operation that com-\nbines the operations that constitute the pipeline. Although this approach may be feasi-\nble for some frequently occurring situations, it is desirable in general to reuse the code\nfor individual operations in the construction of a pipeline.\nIn the example of Figure 15.11, all three operations can be placed in a pipeline,\nwhich passes the results of the selection to the join as they are generated. In turn,\nit passes the results of the join to the projection as they are generated. The memory\nrequirements are low, since results of an operation are not stored for long. However,\nas a result of pipelining, the inputs to the operations are not available all at once for\nprocessing.\nPipelines can be executed in either of two ways:\n1.In ademand-driven pipeline , the system makes repeated requests for tuples from\nthe operation at the top of the pipeline. Each time that an operation receives\na request for tuples, it computes the next tuple (or tuples) to be returned and\nthen returns that tuple. If the inputs of the operation are not pipelined, the next\ntuple(s) to be returned can be computed from the input relations, while the sys-\ntem keeps track of what has been returned so far. If it has some pipelined inputs,\nthe operation also makes requests for tuples from its pipelined inputs. Using the\ntuples received from its pipelined inputs, the operation computes tuples for its\noutput and passes them up to its parent.\n2.In a producer-driven pipeline ,o p e r a t i o n sd on o tw a i tf o rr e q u e s t st op r o d u c et u -\nples, but instead generate the tuples eagerly . Each operation in a producer-driven\npipeline is modeled as a separate process or thread within the system that takes\na stream of tuples from its pipelined inputs and generates a stream of tuples for\nits output.\n", "755": "15.7 Evaluation of Expressions 727\nWe describe next how demand-driven and producer-driven pipelines can be imple-\nmented.\nEach operation in a demand-driven pipeline can be implemented as an iterator that\nprovides the following functions: open(),next(), and close(). After a call to open(), each\ncall to next() returns the next output tuple of the operation. The implementation of the\noperation in turn calls open() and next() on its inputs, to get its input tuples when\nrequired. The function close() tells an iterator that no more tuples are required. The\niterator maintains the state of its execution in between calls so that successive next()\nrequests receive successive result tuples.\nFor example, for an iterator implementing the select operation using linear search,\ntheopen() operation starts a \ufb01le scan, and the iterator\u2019s state records the point to which\nthe \ufb01le has been scanned. When the next() function is called, the \ufb01le scan continues\nfrom after the previous point; when the next tuple satisfying the selection is found by\nscanning the \ufb01le, the tuple is returned after storing the point where it was found in\nthe iterator state. A merge-join iterator\u2019s open() operation would open its inputs, and\nif they are not already sorted, it would also sort the inputs. On calls to next(), it would\nreturn the next pair of matching tuples. The state information would consist of up to\nwhere each input had been scanned. Details of the implementation of iterators are left\nfor you to complete in Practice Exercise 15.7.\nProducer-driven pipelines, on the other hand, are implemented in a di\ufb00erent man-\nner. For each pair of adjacent operations in a producer-driven pipeline, the system cre-\nates a bu\ufb00er to hold tuples being passed from one operation to the next. The processes\nor threads corresponding to di\ufb00erent operat ions execute concurrently. Each operation\nat the bottom of a pipeline continually gene rates output tuples, and puts them in its\noutput bu\ufb00er, until the bu\ufb00er is full. An operation at any other level of a pipeline gen-\nerates output tuples when it gets input tuples from lower down in the pipeline until its\noutput bu\ufb00er is full. Once the operation uses a tuple from a pipelined input, it removes\nthe tuple from its input bu\ufb00er. In either case, once the output bu\ufb00er is full, the opera-\ntion waits until its parent operation removes tuples from the bu\ufb00er so that the bu\ufb00er\nhas space for more tuples. At this point, the operation generates more tuples until the\nbu\ufb00er is full again. The operation repeats this process until all the output tuples have\nbeen generated.\nIt is necessary for the system to switch between operations only when an output\nbu\ufb00er is full or when an input bu\ufb00er is empty and more input tuples are needed to gen-\nerate any more output tuples. In a parallel-pr ocessing system, operations in a pipeline\nmay be run concurrently on distinct processors (see Section 22.5.1).\nUsing producer-driven pipelining can be thought of as pushing data up an oper-\nation tree from below, whereas using demand-driven pipelining can be thought of as\npulling data up an operation tree from the top. Whereas tuples are generated eagerly\nin producer-driven pipelining, they are generated lazily , on demand, in demand-driven\npipelining. Demand-driven pipelining is used more commonly than producer-driven\npipelining because it is easier to implement. However, producer-driven pipelining is\nvery useful in parallel processing systems. Producer-driven pipelining has also been\n", "756": "728 Chapter 15 Query Processing\nfound to be more e\ufb03cient than demand-driven pipelining on modern CPUs since it re-\nduces the number of function call invocations as compared to demand-driven pipelin-\ning. Producer-driven pipelining is increasingly used in systems that generate machine\ncode for high performance query evaluation.\n15.7.2.2 Evaluation Algorithms for Pipelining\nQuery plans can be annotated to mark edges that are pipelined; such edges are called\npipelined edges . In contrast, non-pipelined edges are referred to as blocking edges or\nmaterialized edges . The two operators connected by a pipelined edge must be executed\nconcurrently, since one consumes tuples as the other generates them. Since a plan can\nhave multiple pipelined edges, the set of all operators that are connected by pipelined\nedges must be executed concurrently. A query plan can be divided into subtrees such\nthat each subtree has only pipelined edges, and the edges between the subtrees are non-\npipelined. Each such subtree is called a pipeline stage . The query processor executes\nthe plan one pipeline stage at a time, and concurrently executes all the operators in a\nsingle pipeline stage.\nSome operations, such as sorting, are inherently blocking operations ,t h a ti s ,t h e y\nmay not be able to output any results until all tuples from their inputs have been exam-\nined.7But interestingly, blocking operators can consume tuples as they are generated,\nand can output tuples to their consumers as they are generated; such operations actu-\nally execute in two or more stages, and blocking actually happens between two stages\nof the operation.\nFor example, the external sort-merge operation actually has two steps: (i) run-\ngeneration, followed by (ii) merging. The run-generation step can accept tuples as they\nare generated by the input to the sort, and can thus be pipelined with the sort input.\nThe merge step, on the other hand, can send tuples to its consumer as they are gener-\nated, and can thus be pipelined with the consumer of the sort operation. But the merge\nstep can start only after the run-generation step has \ufb01nished. We can thus model the\nsort-merge operator as two sub-operators connected to each other by a non-pipelined\nedge, but each of the sub-operators can be connected by pipelined edges to their input\nand output respectively.\nOther operations, such as join, are not inherently blocking, but speci\ufb01c evaluation\nalgorithms may be blocking. For example, the indexed nested loops join algorithm can\noutput result tuples as it gets tuples for the outer relation. It is therefore pipelined on its\nouter (left-hand side) relation; however, it is blocking on its indexed (right-hand side)\ninput, since the index must be fully constructed before the indexed nested-loop join\nalgorithm can execute.\nThe hash-join algorithm is a blocking operation on both inputs, since it requires\nboth its inputs to be fully retrieved and partitioned before it outputs any tuples. How-\n7Blocking operations such as sorting may be able to output tuples early if the input is known to satisfy some special\nproperties such as being sorted, or partially sorted, already. However, in the absence of such information, blocking\noperations cannot output tuples early.\n", "757": "15.7 Evaluation of Expressions 729\n(a) Logical Query (b) Pipelined Planr\nsPart.\nPart.r\nsHJ-BP HA-IM \u03b3\nFigure 15.12 Query plan with pipelining.\never, hash-join partitions each of its inputs, and then performs multiple build-probe\nsteps, once per partition. Thus, the hash-join algorithm has 3 steps: (i) partitioning of\nthe \ufb01rst input, (ii) partitioning of the second input, and (iii) the build-probe step. The\npartitioning step for each input can accept tuples as they are generated by the input,\nand can thus be pipelined with its input. The build-probe step can output tuples to its\nconsumer as the tuples are generated, and can thus be pipelined with its consumer.\nBut the two partitioning steps are connected to the build-probe step by non-pipelined\nedges, since build-probe can start only after partitioning has been completed on both\ninputs.\nHybrid hash join can be viewed as partially pipelined on the probe relation, since\nit can output tuples from the \ufb01rst partition as tuples are received for the probe relation.\nHowever, tuples that are not in the \ufb01rst partition will be output only after the entire\npipelined input relation is received. Hybrid hash join thus provides fully pipelined eval-\nuation on its probe input if the build input \ufb01ts entirely in memory, or nearly pipelined\nevaluation if most of the build input \ufb01ts in memory.\nFigure 15.12a shows a query that joins two relations rands,a n dt h e np e r f o r m sa n\naggregation on the result; details of the join predicate, group by attributes and aggre-\ngation functions are omitted for simplicity. Figure 15.12b shows a pipelined plan for\nthe query using hash join and in-memory hash aggregation. Pipelined edges are shown\nusing a normal line, while blocking edges are shown using a bold line. Pipeline stages\nare enclosed in dashed boxes. Note that hash join has been split into three suboper-\nators. Two of suboperators, shown abbreviated to Part., partition randsrespectively.\nThe third, abbreviated to HJ-BP , performs the build and probe phase of the hash join.\nTheHA-IM operator is the in-memory hash aggregation operator. The edges from the\npartition operators to the HJ-BP operator are blocking edges, since the HJ-BP operator\ncan start execution only after the partition operators have completed execution. The\nedges from the relations (assumed to be scanned using a relation scan operator) to the\npartition operators are pipelined, as is the edge from the HJ-BP operator to the HA-IM\noperator. The resultant pipeline stages are shown enclosed in dashed boxes.\nIn general, for each materialized edge we need to add the cost of writing the data\nto disk, and the cost of the consumer operator should include the cost of reading the\ndata from disk. However, when a materialized edge is between suboperators of a single\n", "758": "730 Chapter 15 Query Processing\ndoner:=false;\ndones:=false;\nr:=\u2205;\ns:=\u2205;\nresult :=\u2205;\nwhile not doneror not donesdo\nbegin\nifqueue is empty, then wait until queue is not empty;\nt:= top entry in queue;\nift=Endrthen doner:=true\nelse if t=Endsthen dones:=true\nelse if tis from input r\nthen\nbegin\nr:=r\u222a{t};\nresult :=result\u222a({t}\u22c8s);\nend\nelse/*tis from input s*/\nbegin\ns:=s\u222a{t};\nresult :=result\u222a(r\u22c8{t});\nend\nend\nFigure 15.13 Double-pipelined join algorithm.\noperator, for example between run generation and merge, the materialization cost has\nalready been accounted for in the operators cost, and should not be added again.\nIn some applications, a join algorithm tha t is pipelined on both its inputs and its\noutput is desirable. If both inputs are sorted on the join attribute, and the join condition\nis an equi-join, merge join can be used, with both its inputs and its output pipelined.\nHowever, in the more common case that the two inputs that we desire to pipeline\ninto the join are not already sorted, another alternative is the double-pipelined join tech-\nnique, shown in Figure 15.13. The algorithm assumes that the input tuples for both\ninput relations, rands, are pipelined. Tuples made available for both relations are\nqueued for processing in a single queue. Special queue entries, called EndrandEnds,\nwhich serve as end-of-\ufb01le markers, are inserted in the queue after all tuples from rands\n(respectively) have been generated. For e\ufb03cient evaluation, appropriate indices should\nbe built on the relations rands. As tuples are added to rands,t h ei n d i c e sm u s tb ek e p t\n", "759": "15.8 Query Processing in Memory 731\nup to date. When hash indices are used on rands, the resultant algorithm is called the\ndouble-pipelined hash-join technique.\nThe double-pipelined join algorithm in Figure 15.13 assumes that both inputs \ufb01t in\nmemory. In case the two inputs are larger than memory, it is still possible to use the\ndouble-pipelined join technique as usual until available memory is full. When available\nmemory becomes full, randstuples that have arrived up to that point can be treated\nas being in partition r0ands0, respectively. Tuples for randsthat arrive subsequently\nare assigned to partitions r1ands1, respectively, which are written to disk, and are\nnot added to the in-memory index. However, tuples assigned to r1ands1are used to\nprobe s0andr0, respectively, before they are written to disk. Thus, the join of r1with\ns0,a n d s1with r0, is also carried out in a pipelined fashion. After randshave been\nfully processed, the join of r1tuples with s1tuples must be carried out to complete the\njoin; any of the join techniques we have seen earlier can be used to join r1with s1.\n15.7.3 Pipelines for Continuous-Stream Data\nPipelining is also applicable in situations where data are entered into the database\nin a continuous manner, as is the case, for example, for inputs from sensors that are\ncontinuously monitoring environmental data. Such data are called data streams ,a sw e\nsaw earlier in Section 10.5. Queries may be written over stream data in order to respond\nto data as they arrive. Such queries are called continuous queries .\nThe operations in a continuous query should be implemented using pipelined al-\ngorithms, so that results from the pipeline can be output without blocking. Producer-\ndriven pipelines (which we discussed earlier in Section 15.7.2.1) are the best suited for\ncontinuous query evaluation.\nMany such queries perform aggregation with windowing; tumbling windows which\ndivide time into \ufb01xed size intervals, such as 1 minute, or 1 hour, are commonly used.\nGrouping and aggregation is performed separately on each window, as tuples are re-\nceived; assuming memory size is large enough, an in-memory hash index is used to\nperform aggregation.\nThe result of aggregation on a window can be output once the system knows that no\nfurther tuples in that window will be received in future. If tuples are guaranteed to arrive\nsorted by timestamp, the arrival of a tuple of a following window indicates no more\ntuples will be received for an earlier window. If tuples may arrive out of order, streams\nmust carry punctuations that indicate that all future tuples will have a timestamp greater\nthan some speci\ufb01ed value. The arrival of a punctuation allows the output of aggregates\nof windows whose end-timestamp is less than or equal to the timestamp speci\ufb01ed by\nthe punctuation.\n15.8 Query Processing in Memory\nThe query processing algorithms that we have described so far focus on minimizing\nI/Ocost. In this section, we discuss extensions to the query processing techniques that\n", "760": "732 Chapter 15 Query Processing\nhelp minimize memory access costs by using cache-conscious query processing algo-\nrithms and query compilation. We then discuss query processing with column-oriented\nstorage. The algorithms we describe in this section give signi\ufb01cant bene\ufb01ts for memory\nresident data; they are also very useful with disk-resident data, since they can speed up\nprocessing once data has been brought into the in-memory bu\ufb00er.\n15.8.1 Cache-Conscious Algorithms\nWhen data is resident in memory, access is much faster than if data were resident on\nmagnetic disks, or even SSDs . However, it must be kept in mind that data already in\nCPU cache can be accessed as much as 100 times faster than data in memory. Modern\nCPUs have several levels of cache. Commonly used CPUs today have an L1 cache of size\naround 64 kilobytes, with a latency of about 1 nanosecond, an L2 cache of size around\n256 kilobytes, with a latency of around 5 nanoseconds, and an L3 cache of having a size\nof around 10 megabytes, with a latency of 10 to 15 nanoseconds. In contrast, reading\ndata in memory results in a latency of around 50 to 100 nanoseconds. For simplicity\nin the rest of this section we ignore the di\ufb00erence between the L1, L2 and L3 cache\nlevels, and assume that there is only a single cache level.\nAs we saw in Section 14.4.7, the speed di\ufb00erence between cache memory and main\nmemory, and the fact that data are transferred between main memory and cache in units\nof acache-line (typically about 64 bytes), results in a situation where the relationship\nbetween cache and main memory is not dissimilar to the relationship between main\nmemory and disk (although with smaller speed di\ufb00erences). But there is a di\ufb00erence:\nwhile the contents of the main memory bu\ufb00ers disk-based data are controlled by the\ndatabase system, CPU cache is controlled by the algorithms built into the computer\nhardware. Thus, the database system cannot directly control what is kept in cache.\nHowever, query processing algorithms can be designed in a way that the makes the\nbest use of cache, to optimize performance. Here are some ways this can be done:\n\u2022To sort a relation that is in-memory, we use the external merge-sort algorithm, with\nthe run size chosen such that the run \ufb01ts into the cache; assuming we focus on the\nL3 cache, each run should be a few megabytes in size. We then use an in-memory\nsorting algorithm on each run; since the run \ufb01ts in cache, cache misses are likely to\nbe minimal when the run is sorted. The sorted runs (all of which are in memory)\nare then merged. Merging is cache e\ufb03cient, since access to the runs is sequential:\nwhen a particular word is accessed from memory, the cache line that is fetched\nwill contain the words that would be accessed next from that run.\nTo sort a relation larger than memory, we can use external sort-merge with\nmuch larger run sizes, but use the in-memory merge-sort technique we just de-\nscribed to perform the in-memory sort of the large runs.\n\u2022Hash-join requires probing of an index on the build relation. If the build relation\n\ufb01ts in memory, an index could be built on the whole relation; however, cache hits\nduring probe can be maximized by partitioning the relations into smaller pieces\n", "761": "15.8 Query Processing in Memory 733\nsuch that each partition of the build-relation along with the index \ufb01ts in the cache.\nEach partition is processed separately, with a build and a probe phase; since the\nbuild partition and its index \ufb01t in cache, cache misses are minimized during the\nbuild as well as the probe phase.\nFor relations larger than memory, the \ufb01rst stage of hash-join should partition\nthe two relations such that for each partition, the partitions of the two relations\ntogether \ufb01t in memory. The technique just described can then be used to perform\nthe hash join on each of these partitions, after fetching the contents into memory.\n\u2022Attributes in a tuple can be arranged such that attributes that tend to be accessed\ntogether are laid out consecutively. For example, if a relation is often used for aggre-\ngation, those attributes used as group by attributes, and those that are aggregated\nupon, can be stored consecutively. As a result, if there is a cache miss on one at-\ntribute, the cache line that is fetched would contain attributes that are likely to be\nused immediately.\nCache-aware algorithms are of increasing importance in modern database systems,\nsince memory sizes are often large enough that much of the data is memory-resident.\nIn cases where the requisite data item is not in cache, there is a processing stall\nwhile the data item is retrieved from memory and loaded into cache. In order to con-\nt i n u et om a k eu s eo ft h ec o r et h a tm a d et h er e q u e s tr e s u l t i n gi nt h es t a l l ,t h eo p e r a t i n g\nsystem maintains multiple threads of execution on which a core may work. Parallel\nquery processing algorithms, which we study in Chapter 22 can use multiple threads\nrunning on a single CPU core; if one thread is stalled, another can start execution so\ntheCPU core is utilized better.\n15.8.2 Query Compilation\nWith data resident in memory, CPU cost becomes the bottleneck, and minimizing CPU\ncost can give signi\ufb01cant bene\ufb01ts. Traditional databases query processors act as inter-\npreters that execute a query plan. However, there is a signi\ufb01cant overhead due to inter-\npretation: for example, to access an attribute of a record, the query execution engine\nmay repeatedly look up the relation meta-data to \ufb01nd the o\ufb00set of the attribute within\nthe record, since the same code must work for all relations. There is also signi\ufb01cant\noverhead due to function calls that are performed for each record processed by an\noperation.\nTo avoid overhead due to interpretation, modern main-memory databases com-\npile query plans into machine code or intermediate level byte-code. For example, the\ncompiler can compute the o\ufb00set of an attribute at compile time, and generate code\nwhere the o\ufb00set is a constant. The compiler can also combine the code for multiple\nfunctions in a way that minimizes function calls. With these, and other related opti-\nmizations, compiled code has been found to execute faster, by up to a factor of 10, than\ninterpreted code.\n", "762": "734 Chapter 15 Query Processing\n15.8.3 Column-Oriented Storage\nIn Section 13.6, we saw that in data-analytic applications, only a few attributes of a\nlarge schema may be needed, and that in such cases, storing a relation by column\ninstead of by row may be advantageous. Selection operations on a single attribute (or\nsmall number of attributes) have signi\ufb01can tly lower cost in a column store since only\nthe relevant attributes need to be accessed. However, since accessing each attribute\nrequires its own data access, the cost of retrieving many attributes is higher and may\nincur additional seeks if data are stored on disk.\nBecause column stores permit e\ufb03cient a ccess to many values for a given attribute\nat once, they are well suited to exploit the vector-processing capabilities of modern\nprocessors. This capability allows certain operations (such as comparisons and aggre-\ngations) to be performed in a parallel on multiple attribute values. When compiling\nquery plans to machine code, the compiler can generate vector-processing instructions\nsupported by the processor.\n15.9 Summary\n\u2022The \ufb01rst action that the system must perform on a query is to translate the query\ninto its internal form, which (for relational database systems) is usually based on\nthe relational algebra. In the process of generating the internal form of the query,\nthe parser checks the syntax of the user\u2019s query, veri\ufb01es that the relation names\nappearing in the query are names of relations in the database, and so on. If the\nquery was expressed in terms of a view, the parser replaces all references to the\nview name with the relational-algebra expression to compute the view.\n\u2022Given a query, there are generally a variety of methods for computing the answer.\nIt is the responsibility of the query optimizer to transform the query as entered by\nthe user into an equivalent query that can be computed more e\ufb03ciently. Chapter\n16 covers query optimization.\n\u2022We can process simple selection operations by performing a linear scan or by\nmaking use of indices. We can handle complex selections by computing unions\nand intersections of the results of simple selections.\n\u2022We can sort relations larger than memory by the external sort\u2013merge algorithm.\n\u2022Queries involving a natural join may be processed in several ways, depending on\nthe availability of indices and the form of physical storage for the relations.\n\u00b0If the join result is almost as large as the Cartesian product of the two relations,\nablock nested-loop join strategy may be advantageous.\n\u00b0If indices are available, the indexed nested-loop join can be used.\n", "763": "Review Terms 735\n\u00b0If the relations are sorted, a merge join may be desirable. It may be advantageous\nto sort a relation prior to join computation (so as to allow use of the merge-join\nstrategy).\n\u00b0Thehash-join algorithm partitions the relations into several pieces, such that\neach piece of one of the relations \ufb01ts in memory. The partitioning is carried\nout with a hash function on the join attributes so that corresponding pairs of\npartitions can be joined independently.\n\u2022Duplicate elimination, projection, set operations (union, intersection, and di\ufb00er-\nence), and aggregation can be done by sorting or by hashing.\n\u2022Outer-join operations can be implemented by simple extensions of join algorithms.\n\u2022Hashing and sorting are dual, in the sense that any operation such as duplicate\nelimination, projection, aggregation, join, and outer join that can be implemented\nby hashing can also be implemented by sorting, and vice versa; that is, any opera-\ntion that can be implemented by sorting can also be implemented by hashing.\n\u2022An expression can be evaluated by means of materialization, where the system\ncomputes the result of each subexpression and stores it on disk and then uses it to\ncompute the result of the parent expression.\n\u2022Pipelining helps to avoid writing the results of many subexpressions to disk by\nusing the results in the parent expression even as they are being generated.\nReview Terms\n\u2022Query processing\n\u2022Evaluation primitive\n\u2022Query-execution plan\n\u2022Query-evaluation plan\n\u2022Query-execution engine\n\u2022Measures of query cost\n\u2022Sequential I/O\n\u2022Random I/O\n\u2022File scan\n\u2022Linear search\n\u2022Selections using indices\n\u2022Access paths\n\u2022Index scans\n\u2022Conjunctive selection\u2022Disjunctive selection\n\u2022Composite index\n\u2022Intersection of identi\ufb01ers\n\u2022External sorting\n\u2022External sort\u2013merge\n\u2022Runs\n\u2022N-way merge\n\u2022Equi-join\n\u2022Nested-loop join\n\u2022Block nested-loop join\n\u2022Indexed nested-loop join\n\u2022Merge join\n\u2022Sort-merge join\n\u2022Hybrid merge join\n", "764": "736 Chapter 15 Query Processing\n\u2022Hash-join\n\u00b0Build\n\u00b0Probe\n\u00b0Build input\n\u00b0Probe input\n\u00b0Recursive partitioning\n\u00b0Hash-table over\ufb02ow\n\u00b0Skew\n\u00b0Fudge factor\n\u00b0Over\ufb02ow resolution\n\u00b0Over\ufb02ow avoidance\n\u2022Hybrid hash-join\u2022Spatial join\n\u2022Operator tree\n\u2022Materialized evaluation\n\u2022Double bu\ufb00ering\n\u2022Pipelined evaluation\n\u00b0Demand-driven pipeline\n(lazy, pulling)\n\u00b0Producer-driven pipeline\n(eager, pushing)\n\u00b0Iterator\n\u00b0Pipeline stages\n\u2022Double-pipelined join\n\u2022Continuous query evaluation\nPractice Exercises\n15.1 Assume (for simplicity in this exercise) that only one tuple \ufb01ts in a block and\nmemory holds at most three blocks. Show the runs created on each pass of\nthe sort-merge algorithm when applied to sort the following tuples on the \ufb01rst\nattribute: (kangaroo, 17), (wallaby, 21), (emu, 1), (wombat, 13), (platypus,\n3), (lion, 8), (warthog, 4), (zebra, 11), (meerkat, 6), (hyena, 9), (hornbill, 2),\n(baboon, 12).\n15.2 Consider the bank database of Figure 15.14, where the primary keys are un-\nderlined, and the following SQL query:\nselect T.branch\n name\nfrom branch T ,branch S\nwhere T.assets >S.assets andS.branch\n city= \u201cBrooklyn\u201d\nWrite an e\ufb03cient relational-algebra expression that is equivalent to this query.\nJustify your choice.\n15.3 Let relations r1(A,B,C)a n d r2(C,D,E) have the following properties: r1has\n20,000 tuples, r2has 45,000 tuples, 25 tuples of r1\ufb01t on one block, and 30\ntuples of r2\ufb01t on one block. Estimate the number of block transfers and seeks\nrequired using each of the following join strategies for r1\u22c8r2:\na. Nested-loop join.\nb. Block nested-loop join.\n", "765": "Practice Exercises 737\nc. Merge join.\nd. Hash join.\n15.4 The indexed nested-loop join algorithm described in Section 15.5.3 can be\nine\ufb03cient if the index is a secondary index and there are multiple tuples with\nthe same value for the join attributes. Why is it ine\ufb03cient? Describe a way,\nusing sorting, to reduce the cost of retrieving tuples of the inner relation. Under\nwhat conditions would this algorithm be more e\ufb03cient than hybrid merge join?\n15.5 Letrandsbe relations with no indices, and assume that the relations are not\nsorted. Assuming in\ufb01nite memory, what is the lowest-cost way (in terms of I/O\noperations) to compute r\u22c8s? What is the amount of memory required for\nthis algorithm?\n15.6 Consider the bank database of Figure 15.14, where the primary keys are un-\nderlined. Suppose that a B+-tree index on branch\n cityis available on relation\nbranch , and that no other index is available. List di\ufb00erent ways to handle the\nfollowing selections that involve negation:\na.\u03c3\u00ac(branch\n city<\u201cBrooklyn\u201d) (branch )\nb.\u03c3\u00ac(branch\n city=\u201cBrooklyn\u201d) (branch )\nc.\u03c3\u00ac(branch\n city<\u201cBrooklyn\u201d \u2228assets <5000)(branch )\n15.7 Write pseudocode for an iterator that implements indexed nested-loop join,\nwhere the outer relation is pipelined. Your pseudocode must de\ufb01ne the stan-\ndard iterator functions open(),next(), and close(). Show what state information\nthe iterator must maintain between calls.\n15.8 Design sort-based and hash-based algorithms for computing the relational di-\nvision operation (see Practice Exercise 2.9 for a de\ufb01nition of the division op-\neration).\nbranch (branch\n name\n ,branch\n city, assets )\ncustomer (customer\n name\n ,customer\n street, customer\n city)\nloan (loan\n number\n ,branch\n name, amount )\nborrower (customer\n name\n ,loan\n number\n )\naccount (account\n number\n ,branch\n name, balance )\ndepositor (customer\n name\n ,account\n number\n )\nFigure 15.14 Bank database.\n", "766": "738 Chapter 15 Query Processing\n15.9 What is the e\ufb00ect on the cost of merging runs if the number of bu\ufb00er blocks\nper run is increased while overall memory available for bu\ufb00ering runs remains\n\ufb01xed?\n15.10 Consider the following extended relational-algebra operators. Describe how to\nimplement each operation using sorting and using hashing.\na.Semijoin (\u22c9\u03b8): The multiset semijoin operator r\u22c9\u03b8sis de\ufb01ned as follows:\nif a tuple riappears ntimes in r, it appears ntimes in the result of r\u22c9\u03b8\nif there is at least one tuple sjsuch that riandsjsatisfy predicate \u03b8;\notherwise ridoes not appear in the result.\nb.Anti-semijoin (\n\u22c9\u03b8): The multiset anti-semijoin operator r\n\u22c9\u03b8sis de\ufb01ned\nas follows: if a tuple riappears ntimes in r, it appears ntimes in the result\nofr\u22c9\u03b8if there does not exist any tuple sjinssuch that riandsjsatisfy\npredicate \u03b8;o t h e r w i s e ridoes not appear in the result.\n15.11 Suppose a query retrieves only the \ufb01rst Kresults of an operation and termi-\nnates after that. Which choice of demand-driven or producer-driven pipelining\n(with bu\ufb00ering) would be a good choice for such a query? Explain your an-\nswer.\n15.12 Current generation CPUsi n c l u d ea n instruction cache , which caches recently\nused instructions. A function call then has a signi\ufb01cant overhead because the\nset of instructions being executed changes, resulting in cache misses on the\ninstruction cache.\na. Explain why producer-driven pipelining with bu\ufb00ering is likely to result\nin a better instruction cache hit rate, as compared to demand-driven\npipelining.\nb. Explain why modifying demand-driven pipelining by generating multiple\nresults on one call to next(), and returning them together, can improve\nthe instruction cache hit rate.\n15.13 Suppose you want to \ufb01nd documents that contain at least kof a given set of n\nkeywords. Suppose also you have a keyword index that gives you a (sorted) list\nof identi\ufb01ers of documents that contain a speci\ufb01ed keyword. Give an e\ufb03cient\nalgorithm to \ufb01nd the desired set of documents.\n15.14 Suggest how a document containing a word (such as \u201cleopard\u201d) can be in-\ndexed such that it is e\ufb03ciently retrieved by queries using a more general con-\ncept (such as \u201ccarnivore\u201d or \u201cmammal\u201d). You can assume that the concept\nhierarchy is not very deep, so each concept has only a few generalizations (a\nconcept can, however, have a large number of specializations). You can also\nassume that you are provided with a function that returns the concept for each\nword in a document. Also suggest how a query using a specialized concept can\nretrieve documents using a more general concept.\n", "767": "Exercises 739\n15.15 Explain why the nested-loops join algorithm (see Section 15.5.1) would work\npoorly on a database stored in a column-oriented manner. Describe an alterna-\ntive algorithm that would work better, and explain why your solution is better.\n15.16 Consider the following queries. For each query, indicate if column-oriented\nstorage is likely to be bene\ufb01cial or not, and explain why.\na. Fetch ID,name anddept\nname of the student with ID 12345.\nb. Group the takes relation by yearandcourse\n id, and \ufb01nd the total number\nof students for each ( year,course\n id) combination.\nExercises\n15.17 Suppose you need to sort a relation of 40 gigabytes, with 4-kilobyte blocks,\nusing a memory size of 40 megabytes. Suppose the cost of a seek is 5 millisec-\nonds, while the disk transfer rate is 40 megabytes per second.\na. Find the cost of sorting the relation, in seconds, with bb=1a n dw i t h\nbb=100.\nb. In each case, how many merge passes are required?\nc. Suppose a \ufb02ash storage device is used instead of a disk, and it has a\nlatency of 20 microsecond and a transfer rate of 400 megabytes per sec-\nond. Recompute the cost of sorting the relation, in seconds, with bb=1\nand with bb=100, in this setting.\n15.18 Why is it not desirable to force users to make an explicit choice of a query-\nprocessing strategy? Are there cases in which it isd e s i r a b l ef o ru s e r st ob ea w a r e\nof the costs of competing query-processing strategies? Explain your answer.\n15.19 Design a variant of the hybrid merge-join algorithm for the case where both\nrelations are not physically sorted, but both have a sorted secondary index on\nthe join attributes.\n15.20 Estimate the number of block transfers and seeks required by your solution to\nExercise 15.19 for r1\u22c8r2,w h e r e r1andr2are as de\ufb01ned in Exercise 15.3.\n15.21 The hash-join algorithm as described in Section 15.5.5 computes the natural\njoin of two relations. Describe how to extend the hash-join algorithm to com-\npute the natural left outer join, the natural right outer join, and the natural full\nouter join. (Hint: Keep extra information with each tuple in the hash index to\nd e t e c tw h e t h e ra n yt u p l ei nt h ep r o b er e l a t i o nm a t c h e st h et u p l ei nt h eh a s h\nindex.) Try out your algorithm on the takes andstudent relations.\n", "768": "740 Chapter 15 Query Processing\n15.22 Suppose you have to computeA\u03b3sum(C)(r)a sw e l la sA,B\u03b3sum(C)(r). Describe how\nto compute these together using a single sorting of r.\n15.23 Write pseudocode for an iterator that implements a version of the sort\u2013merge\nalgorithm where the result of the \ufb01nal merge is pipelined to its consumers.\nYour pseudocode must de\ufb01ne the standard iterator functions open(),next(),\nandclose(). Show what state information the iterator must maintain between\ncalls.\n15.24 Explain how to split the hybrid hash-join operator into sub-operators to model\npipelining. Also explain how this split is di\ufb00erent from the split for a hash-join\noperator.\n15.25 Suppose you need to sort relation rusing sort\u2014merge and merge\u2014join the re-\nsult with an already sorted relation s.\na. Describe how the sort operator is broken into suboperators to model the\npipelining in this case.\nb. The same idea is applicable even if both inputs to the merge join are the\noutputs of sort\u2014merge operations. However, the available memory has to\nbe shared between the two merge operations (the merge\u2014join algorithm\nitself needs very little memory). What is the e\ufb00ect of having to share\nmemory on the cost of each sort-merge operation?\nFurther Reading\n[Graefe (1993)] presents an excellent survey of query-evaluation techniques. [Faerber\net al. (2017)] describe main-memory database implementation techniques, including\nquery processing techniques for main-memory databases, while [Kemper et al. (2012)]\ndescribes techniques for query processing with in-memory columnar data. [Samet\n(2006)] provides a textbook description of spatial data structures, while [Shekhar and\nChawla (2003)] provides a textbook description of spatial databases, including index-\ning and query processing techniques. Textbook descriptions of techniques for indexing\ndocuments, and e\ufb03ciently computing ranked answers to keyword queries may be found\nin [Manning et al. (2008)].\nBibliography\n[Faerber et al. (2017)] F. Faerber, A. Kemper, P.-A. Larson, J. Levandoski, T. Neumann, and\nA. Pavlo, \u201cMain Memory Database Systems\u201d, Foundations and Trends in Databases ,V o l u m e\n8, Number 1-2 (2017), pages 1\u2013130.\n[Graefe (1993)] G. Graefe, \u201cQuery Evaluation Techniques for Large Databases\u201d, ACM Com-\nputing Surveys , Volume 25, Number 2 (1993).\n", "769": "Further Reading 741\n[Kemper et al. (2012)] A. Kemper, T. Neumann, F. Funke, V. Leis, and H. M\u00a8 uhe, \u201cHyPer:\nAdapting Columnar Main-Memory Data Management for Transaction AND Query Process-\ning\u201d, IEEE Data Engineering Bulletin , Volume 35, Number 1 (2012), pages 46\u201351.\n[Manning et al. (2008)] C. D. Manning, P. Raghavan, and H. Sch\u00a8 utze, Introduction to Infor-\nmation Retrieval , Cambridge University Press (2008).\n[Samet (2006)] H. Samet, Foundations of Multidimensional and Metric Data Structures ,M o r -\ngan Kaufmann (2006).\n[Shekhar and Chawla (2003)] S. Shekhar and S. Chawla, Spatial Databases: A TOUR ,P e a r -\nson (2003).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "770": "", "771": "CHAPTER16\nQuery Optimization\nQuery optimization is the process of selecting the most e\ufb03cient query-evaluation plan\nfrom among the many strategies usually possi ble for processing a given query, especially\nif the query is complex. We do not expect users to write their queries so that they can be\nprocessed e\ufb03ciently. Rather, we expect the system to construct a query-evaluation plan\nthat minimizes the cost of query evaluation. This is where query optimization comes\ninto play.\nOne aspect of optimization occurs at the relational-algebra level, where the system\nattempts to \ufb01nd an expression that is equivalent to the given expression, but more\ne\ufb03cient to execute. Another aspect is selecting a detailed strategy for processing the\nquery, such as choosing the algorithm to use for executing an operation, choosing the\nspeci\ufb01c indices to use, and so on.\nThe di\ufb00erence in cost (in terms of evaluation time) between a good strategy and a\nbad strategy is often substantial and may be several orders of magnitude. Hence, it is\nworthwhile for the system to spend a substantial amount of time on the selection of a\ngood strategy for processing a query, even if the query is executed only once.\n16.1 Overview\nConsider the following relational-algebra expression, for the query \u201cFind the names of\nall instructors in the Music department together with the course title of all the courses\nthat the instructors teach.\u201d1\n\u03a0name ,title(\u03c3dept\nname=\u201cMusic\u201d (instructor \u22c8(teaches \u22c8\u03a0course\n id,title(course ))))\nThe subexpression instructor \u22c8teaches \u22c8\u03a0course\n id,title(course ) in the preceding ex-\npression can create a very large intermediate result. However, we are interested in only\na few tuples of this intermediate result, namely, those pertaining to instructors in the\n1Note that the projection of course on (course\n id,title) is required since course shares an attribute dept\nname with\ninstructor ; if we did not remove this attribute using the projection, the above expression using natural joins would\nreturn only courses from the Music department, even if some Music department instructors taught courses in other\ndepartments.\n743\n", "772": "744 Chapter 16 Query Optimization\ninstructor\ninstructor teaches teaches\ncourse course\u220fname, title\n\u220fcourse_id, title\u220fcourse_id, title\u220fname, title \u03c3dept_name = Music\n\u03c3dept_name = Music\n(a) Initial expression tree                            (b) Transformed expression tree\nFigure 16.1 Equivalent expressions.\nMusic department, and in only two of the nine attributes of this relation. Since we are\nconcerned with only those tuples in the instructor relation that pertain to the Music\ndepartment, we do not need to consider those tuples that do not have dept\nname =\n\u201cMusic\u201d. By reducing the number of tuples of the instructor relation that we need to\naccess, we reduce the size of the intermediate result. Our query is now represented by\nthe relational-algebra expression:\n\u03a0name ,title((\u03c3dept\nname=\u201cMusic\u201d (instructor ))\u22c8(teaches \u22c8\u03a0course\n id,title(course )))\nwhich is equivalent to our original algebra expression, but which generates smaller\nintermediate relations. Figure 16.1 depicts the initial and transformed expressions.\nAn evaluation plan de\ufb01nes exactly what algorithm should be used for each op-\neration and how the execution of the operations should be coordinated. Figure 16.2\nillustrates one possible evaluation plan for the expression from Figure 16.1(b). As we\nhave seen, several di\ufb00erent algorithms can be used for each relational operation, giving\nrise to alternative evaluation plans. In the \ufb01gure, hash join has been chosen for one\nof the join operations, while the other uses merge join, after sorting the relations on\nthe join attribute, which is ID. All edges are assumed to be pipelined, unless marked\nas materialized. With pipelined edges the output of the producer is sent directly to\nthe consumer, without being written out to disk; with materialized edges, on the other\nhand, the output is written to disk, and then read from the disk by the consumer. There\nare no materialized edges in the evaluation plan in Figure 16.2, although some of the\noperators, such as sort and hash join, can be represented using suboperators with ma-\nterialized edges between the suboperators, as we saw in Section 15.7.2.2.\nGiven a relational-algebra expression, it is the job of the query optimizer to come\nup with a query-evaluation plan that computes the same result as the given expression,\nand is the least costly way of generating the result (or, at least, is not much costlier than\nthe least costly way).\n", "773": "16.1 Overview 745\n\u03a0name, title\ninstructor\u03c3dept_name = Music\n                                   (use index 1)sortID\nteaches(sort to remove duplicates)\n(hash join)(merge join)\ncourse\u03a0course_id, titlesortID\nFigure 16.2 An evaluation plan.\nThe expression that we saw in Figure 16.1 may not necessarily lead to the least-\ncost evaluation plan for computing the result, since it still computes the join of the\nentire teaches relation with the course relation. The following expression gives the same\n\ufb01nal result, but generates smaller intermediate results, since it joins teaches with only\ninstructor tuples corresponding to the Music department, and then joins that result\nwith course .\n\u03a0name ,title((\u03c3dept\nname=\u201cMusic\u201d (instructor )\u22c8teaches )\u22c8\u03a0course\n id,title(course ))\nRegardless of the way the query is written, it is the job of the optimizer to \ufb01nd the\nleast-cost plan for the query.\nTo \ufb01nd the least costly query-evaluation plan, the optimizer needs to generate al-\nternative plans that produce the same result as the given expression and to choose the\nleast costly one. Generation of query-evaluation plans involves three steps: (1) gener-\nating expressions that are logically equivalent to the given expression, (2) annotating\nthe resultant expressions in alternative ways to generate alternative query-evaluation\nplans, and (3) estimating the cost of each evaluation plan, and choosing the one whose\nestimated cost is the least.\nSteps (1), (2), and (3) are interleaved in the query optimizer\u2014some expressions\nare generated and annotated to generate evaluation plans, then further expressions are\ngenerated and annotated, and so on. As evaluation plans are generated, their costs are\nestimated by using statistical information about the relations, such as relation sizes and\nindex depths.\nTo implement the \ufb01rst step, the query optimizer must generate expressions equiv-\nalent to a given expression. It does so by means of equivalence rules that specify how\nto transform an expression into a logically equivalent one. We describe these rules in\nSection 16.2.\nIn Section 16.3 we describe how to estimate statistics of the results of each opera-\ntion in a query plan. Using these statistics with the cost formulae in Chapter 15 allows\n", "774": "746 Chapter 16 Query Optimization\nNote 16.1 VIEWING QUERY EVALUATION PLANS\nMost database systems provide a way to view the evaluation plan chosen to execute\na given query. It is usually best to use the GUI provided with the database system\nto view evaluation plans. However, if you use a command line interface, many\ndatabases support variations of a command \u201c explain<query>\u201d, which displays\nthe execution plan chosen for the speci\ufb01ed query <query>. The exact syntax varies\nwith di\ufb00erent databases:\n\u2022Postgre SQL uses the syntax shown above.\n\u2022Oracle uses the syntax explain plan for . However, the command stores the\nresultant plan in a table called plan\n table, instead of displaying it. The query\n\u201cselect *from table (dbms\n xplan.display );\u201d displays the stored plan.\n\u2022DB2 follows a similar approach to Oracle, but requires the program db2exfmt\nto be executed to display the stored plan.\n\u2022SQL S erver requires the command set showplan\n text on to be executed before\nsubmitting the query; then, when a query is submitted, instead of executing\nthe query, the evaluation plan is displayed.\n\u2022MySQLuses the same explain<query>syntax as Postgre SQL,b u tt h eo u t p u ti s\na table whose contents are not easy to understand. However, executing show\nwarnings after the explain command displays the evaluation plan in a more\nhuman-readable format.\nThe estimated costs for the plan are also displayed along with the plan. It is\nworth noting that the costs are usually not in any externally meaningful unit, such\nas seconds or I/Ooperations, but rather in units of whatever cost model the opti-\nmizer uses. Some optimizers such as Postgre SQL display two cost-estimate num-\nbers; the \ufb01rst indicates the estimated cost for outputting the \ufb01rst result, and the\nsecond indicates the estimated cost for outputting all results.\nus to estimate the costs of individual operations. The individual costs are combined\nto determine the estimated cost of evaluating a given relational-algebra expression, as\noutlined in Section 15.7.\nIn Section 16.4, we describe how to choose a query-evaluation plan. We can choose\none based on the estimated cost of the plans. Since the cost is an estimate, the selected\nplan is not necessarily the least costly plan; however, as long as the estimates are good,\nthe plan is likely to be the least costly one, or not much more costly than it.\n", "775": "16.2 Transformation of Relational Expressions 747\nFinally, materialized views help to speed up processing of certain queries. In Sec-\ntion 16.5, we study how to \u201cmaintain\u201d materialized views\u2014that is, to keep them up-to-\ndate\u2014and how to perform query optimization with materialized views.\n16.2 Transformation of Relational Expressions\nA query can be expressed in several di\ufb00erent ways, with di\ufb00erent costs of evaluation. In\nthis section, rather than take the relational expression as given, we consider alternative,\nequivalent expressions.\nTwo relational-algebra expressions are said to be equivalent if, on every legal data-\nbase instance, the two expressions generate the same set of tuples. (Recall that a legal\ndatabase instance is one that satis\ufb01es all the integrity constraints speci\ufb01ed in the data-\nbase schema.) Note that the order of the tuples is irrelevant; the two expressions may\ngenerate the tuples in di\ufb00erent orders, but would be considered equivalent as long as\nthe set of tuples is the same.\nInSQL, the inputs and outputs are multisets of tuples, and the multiset version of\nthe relational algebra (described in Note 3.1 on page 80, Note 3.2 on page 97, and Note\n3.3 on page 108) is used for evaluating SQL queries. Two expressions in the multiset\nversion of the relational algebra are said to be equivalent if on every legal database the\ntwo expressions generate the same multiset of tuples. The discussion in this chapter\nis based on the relational algebra. We leave extensions to the multiset version of the\nrelational algebra to you as exercises.\n16.2.1 Equivalence Rules\nAnequivalence rule says that expressions of two forms are equivalent. We can replace\nan expression of the \ufb01rst form with an expression of the second form, or vice versa\u2014\nthat is, we can replace an expression of th e second form by an expression of the \ufb01rst\nform\u2014since the two expressions generate the same result on any valid database. The\noptimizer uses equivalence rules to transform expressions into other logically equiva-\nlent expressions.\nWe now describe several equivalence rules on relational-algebra expressions. Some\nof the equivalences listed appear in Figure 16.3. We use \u03b8,\u03b81,\u03b82, and so on to denote\npredicates, L1,L2,L3, and so on to denote lists of attributes, and E,E1,E2,a n ds oo n\nto denote relational-algebra expressions. A relation name ris simply a special case of\na relational-algebra expression and can be used wherever Eappears.\n1.Conjunctive selection operations can be deconstructed into a sequence of indi-\nvidual selections. This transformation is referred to as a cascade of \u03c3.\n\u03c3\u03b81\u2227\u03b82(E)\u2261\u03c3\u03b81(\u03c3\u03b82(E))\n2.Selection operations are commutative .\n\u03c3\u03b81(\u03c3\u03b82(E))\u2261\u03c3\u03b82(\u03c3\u03b81(E))\n", "776": "748 Chapter 16 Query Optimization\nE1E2\u03b8\nE2E1Rule 5\nE3\nE1E2E2E3E1Rule 6.a\nRule 7.a\nIf \u03b8 only has\nattributes from E1 \nE1E2 E1E2\u03c3\u03b8\n\u03c3\u03b8\u03b8\nFigure 16.3 Pictorial representation of equivalences.\n3.Only the \ufb01nal operations in a sequence of projection operations are needed; the\nothers can be omitted. This transformation can also be referred to as a cascade\nof\u03a0.\n\u03a0L1(\u03a0L2(\u2026(\u03a0Ln(E))\u2026))\u2261\u03a0L1(E)\nwhere L1\u2286L2\u2286\u2026\u2286Ln.\n4.Selections can be combined with Cartesian products and theta joins.\na.\u03c3\u03b8(E1\u00d7E2)\u2261E1\u22c8\u03b8E2\nThis expression is just the de\ufb01nition of the theta join.\nb.\u03c3\u03b81(E1\u22c8\u03b82E2)\u2261E1\u22c8\u03b81\u2227\u03b82E2\n5.Theta-join operations are commutative.\nE1\u22c8\u03b8E2\u2261E2\u22c8\u03b8E1\nRecall that the natural-join operator is simply a special case of the theta-join\noperator; hence, natural joins are also commutative.\nThe order of attributes di\ufb00ers between the left-hand side and right-hand side\nof the commutativity rule, so the equivalence does not hold if the order of at-\ntributes is taken into account. Since we use a version of relational algebra where\nevery attribute must have a name for it to be referenced, the order of attributes\n", "777": "16.2 Transformation of Relational Expressions 749\ndoes not actually matter, except when the result is \ufb01nally displayed. When the\norder does matter, a projection operation can be added to one of the sides of the\nequivalence to appropriately reorder attributes. However, for simplicity, we omit\nthe projection and ignore the attribute order in all our equivalence rules.\n6. a. Natural-join operations are associative .\n(E1\u22c8E2)\u22c8E3\u2261E1\u22c8(E2\u22c8E3)\nb. Theta joins are associative in the following manner:\n(E1\u22c8\u03b81E2)\u22c8\u03b82\u2227\u03b83E3\u2261E1\u22c8\u03b81\u2227\u03b83(E2\u22c8\u03b82E3)\nwhere\u03b82involves attributes from only E2andE3. Any of these conditions\nmay be empty; hence, it follows that the Cartesian product ( \u00d7)o p e r a t i o n\nis also associative. The commutativity and associativity of join operations\nare important for join reordering in query optimization.\n7.The selection operation distributes over the theta-join operation under the fol-\nlowing two conditions:\na. Selection distributes over the theta-join operation when all the attributes in\nselection condition \u03b81involve only the attributes of one of the expressions\n(say, E1) being joined.\n\u03c3\u03b81(E1\u22c8\u03b8E2)\u2261(\u03c3\u03b81(E1))\u22c8\u03b8E2\nb. Selection distributes over the theta-join operation when selection condition\n\u03b81involves only the attributes of E1and\u03b82involves only the attributes of\nE2.\n\u03c3\u03b81\u2227\u03b82(E1\u22c8\u03b8E2)\u2261(\u03c3\u03b81(E1))\u22c8\u03b8(\u03c3\u03b82(E2))\n8.The projection operation distributes over the theta-join operation under the fol-\nlowing conditions.\na. Let L1andL2be attributes of E1andE2, respectively. Suppose that the join\ncondition \u03b8involves only attributes in L1\u222aL2. Then,\n\u03a0L1\u222aL2(E1\u22c8\u03b8E2)\u2261(\u03a0L1(E1))\u22c8\u03b8(\u03a0L2(E2))\nb. Consider a join E1\u22c8\u03b8E2.L e t L1andL2be sets of attributes from E1and\nE2, respectively. Let L3be attributes of E1that are involved in join condition\n\u03b8,b u ta r en o ti n L1and let L4be attributes of E2that are involved in join\ncondition \u03b8, but are not in L2. Then,\n\u03a0L1\u222aL2(E1\u22c8\u03b8E2)\u2261\u03a0L1\u222aL2((\u03a0L1\u222aL3(E1))\u22c8\u03b8(\u03a0L2\u222aL4(E2)))\nSimilar equivalences hold for outer join operations \u27d5,\u27d6and\u27d7.\n", "778": "750 Chapter 16 Query Optimization\n9.The set operations union and intersection are commutative.\na.E1\u222aE2\u2261E2\u222aE1\nb.E1\u2229E2\u2261E2\u2229E1\nSet di\ufb00erence is not commutative.\n10. Set union and intersection are associative.\na. (E1\u222aE2)\u222aE3\u2261E1\u222a(E2\u222aE3)\nb. (E1\u2229E2)\u2229E3\u2261E1\u2229(E2\u2229E3)\n11. The selection operation distributes over the union, intersection, and set-\ndi\ufb00erence operations.\na.\u03c3\u03b8(E1\u222aE2)\u2261\u03c3\u03b8(E1)\u222a\u03c3\u03b8(E2)\nb.\u03c3\u03b8(E1\u2229E2)\u2261\u03c3\u03b8(E1)\u2229\u03c3\u03b8(E2)\nc.\u03c3\u03b8(E1\u2212E2)\u2261\u03c3\u03b8(E1)\u2212\u03c3\u03b8(E2)\nd.\u03c3\u03b8(E1\u2229E2)\u2261\u03c3\u03b8(E1)\u2229E2\ne.\u03c3\u03b8(E1\u2212E2)\u2261\u03c3\u03b8(E1)\u2212E2\nThe preceding equivalence does not hold if \u2212is replaced by \u222a.\n12. The projection operation distributes over the union operation\n\u03a0L(E1\u222aE2)\u2261(\u03a0L(E1))\u222a(\u03a0L(E2))\nprovided E1andE2have the same schema.\n13. Selection distributes over aggregation under the following conditions. Let Gbe\na set of group by attributes, and Aa set of aggregate expressions. When \u03b8only\ninvolves attributes in G, the following equivalence holds.\n\u03c3\u03b8(G\u03b3A(E)\u2261G\u03b3A(\u03c3\u03b8(E))\n14. a. Full outer join is commutative.\nE1\u27d7E2\u2261E2\u27d7E2\nb. Left and right outer join are not commutative. However, left outer join and\nright outer join can be exchanged as follows.\nE1\u27d5E2\u2261E2\u27d6E1\n15. Selection distributes over left and right outer join under some conditions. Specif-\nically, when the selection condition \u03b81involves only the attributes of one of the\nexpressions being joined, say E1, the following equivalences hold.\n", "779": "16.2 Transformation of Relational Expressions 751\na.\u03c3\u03b81(E1\u27d5\u03b8E2)\u2261(\u03c3\u03b81(E1))\u27d5\u03b8E2\nb.\u03c3\u03b81(E2\u27d6\u03b8E1)\u2261(E2\u27d6\u03b8(\u03c3\u03b81(E1)))\n16. Outer joins can be replaced by inner joins under some conditions. Speci\ufb01cally, if\n\u03b81has the property that it evaluates to false or unknown whenever the attributes\nofE2are null, then the following equivalences hold.\na.\u03c3\u03b81(E1\u27d5\u03b8E2)\u2261\u03c3\u03b81(E1\u22c8\u03b8E2)\nb.\u03c3\u03b81(E2\u27d6\u03b8E1)\u2261\u03c3\u03b81(E2\u22c8\u03b8E2)\nAp r e d i c a t e \u03b81satisfying the above property is said to be null rejecting onE2.F o r\nexample, if \u03b81is of the form A<4w h e r e Ais an attribute from E2,t h e n\u03b81would\nevaluate to unknown whenever Ais null, and as a result any tuples in E1\u27d5\u03b8E2\nthat are not in E1\u22c8\u03b8E2would be rejected by \u03c3\u03b81. We can therefore replace the\nouter join by an inner join (or vice versa).\nMore generally, the condition would hold if \u03b81is of the form \u03b81\n1\u2227\u03b82\n1\u2227\u2026\u2227\u03b8k\n1,\nand at least one of the terms \u03b8i\n1is of the form e1relop e2,w h e r e e1ande2are\narithmetic or string expressions involving at least one attribute from E2,a n d relop\nis any of<,\u2264,=,\u2265,>.\nThis is only a partial list of equivalences. More equivalences are discussed in the\nexercises.\nSome equivalences that hold for joins do not hold for outer joins. For example, the\nselection operation does not distribute over outer join when the conditions speci\ufb01ed\nin rule 15.a or rule 15.b do hold. To see this, we look at the expression:\n\u03c3year=2017(instructor\u27d5teaches )\nand consider the case of an instructor who teaches no courses at all, regardless of\nyear. In the above expression, the left outer join retains a tuple for each such instructor\nwith a null value for year. Then the selection operation removes those tuples since\nthe predicate null=2017 evaluates to unknown , and such instructors do not appear in\nthe result. However, if we push the selection operation down to teaches , the resulting\nexpression:\ninstructor\u27d5\u03c3year=2017(teaches )\nis syntactically correct since the selection predicate includes only attributes from\nteaches , but the result is di\ufb00erent. For an instructor that does not teach at all, the instruc-\ntortuple appears in the result of instructor\u27d5\u03c3year=2017(teaches ), but not in the result\nof\u03c3year=2017(instructor\u27d5teaches ). The following equivalence, however, does hold:\n\u03c3year=2017(instructor\u27d5teaches )\u2261\u03c3year=2017(instructor \u22c8teaches )\nAs another example, unlike inner joins, outer joins are not associative. We show\nthus using an example for the natural left outer join. Similar examples can be con-\n", "780": "752 Chapter 16 Query Optimization\nstructed for natural right and natural full outer join, as well as for the corresponding\ntheta-join versions of the outer join operations.\nLet relation r(A,B) be a relation consisting of the single tuple (1, 1), s(B,C)b ea\nrelation consisting of the single tuple (1, 1), and t(A,C)b ea ne m p t yr e l a t i o nw i t hn o\ntuples. We shall show that for this example,\n(r\u27d5s)\u27d5t\u2262r\u27d5(s\u27d5t)\nTo see this, note \ufb01rst that ( r\u27d5s) produces a result with schema ( A,B,C)h a v i n go n e\ntuple (1, 1, 1). Computing the left outer join of that result with relation tproduces a\nresult with schema ( A,B,C) having one tuple (1, 1, 1). Next, we look at the expression\nr\u27d5(s\u27d5t), and note that s\u27d5tproduces a result with schema ( A,B,C)h a v i n go n e\ntuple ( null,1 ,1 ) . C o m p u t i n g t h e l e f to u t e r j o i n o f rwith that result produces a result\nwith schema ( A,B,C) having one tuple (1, 1, null).\n16.2.2 Examples of Transformations\nWe now illustrate the use of the equivalence rules. We use our university example with\nthe relation schemas:\ninstructor (ID,name ,dept\nname ,salary )\nteaches (ID,course\n id,sec\nid,semester ,year)\ncourse (course\n id,title,dept\nname ,credits )\nIn our example in Section 16.1, the expression:\n\u03a0name ,title(\u03c3dept\nname=\u201cMusic\u201d (instructor \u22c8(teaches \u22c8\u03a0course\n id,title(course ))))\nwas transformed into the following expression:\n\u03a0name ,title((\u03c3dept\nname=\u201cMusic\u201d (instructor ))\u22c8(teaches \u22c8\u03a0course\n id,title(course )))\nwhich is equivalent to our original algebra expression but generates smaller intermedi-\nate relations. We can carry out this transformation by using rule 7.a. Remember that\nthe rule merely says that the two expressions are equivalent; it does not say that one is\nbetter than the other.\nMultiple equivalence rules can be used, one after the other, on a query or on parts\nof the query. As an illustration, suppose that we modify our original query to restrict\nattention to instructors who have taught a course in 2017. The new relational-algebra\nquery is:\n\u03a0name ,title(\u03c3dept\nname=\u201cMusic\u201d\u2227year=2017\n(instructor \u22c8(teaches \u22c8\u03a0course\n id,title(course ))))\n", "781": "16.2 Transformation of Relational Expressions 753\nWe cannot apply the selection predicate directly to the instructor relation, since the\npredicate involves attributes of both the instructor andteaches relations. However, we\ncan \ufb01rst apply rule 6.a (associativity of natural join) to transform the join instructor \u22c8\n(teaches \u22c8\u03a0course\n id,title(course )) into ( instructor \u22c8teaches )\u22c8\u03a0course\n id,title(course ):\n\u03a0name ,title(\u03c3dept\nname=\u201cMusic\u201d\u2227year=2017\n((instructor \u22c8teaches )\u22c8\u03a0course\n id,title(course )))\nThen, using rule 7.a, we can rewrite our query as:\n\u03a0name ,title((\u03c3dept\nname=\u201cMusic\u201d\u2227year=2017\n(instructor \u22c8teaches ))\u22c8\u03a0course\n id,title(course ))\nLet us examine the selection subexpressi on within this expression. Using rule 1,\nwe can break the selection into two selections to get the following subexpression:\n\u03c3dept\nname=\u201cMusic\u201d (\u03c3year=2017(instructor \u22c8teaches ))\nBoth of the preceding expressions select tuples with dept\n name=\u201cMusic\u201d and\ncourse\n id= 2017. However, the latter form of the expression provides a new opportunity\nto apply rule 7.a (\u201cperform selections ear ly\u201d), resulting in the subexpression:\n\u03c3dept\nname=\u201cMusic\u201d (instructor )\u22c8\u03c3year=2017(teaches )\nFigure 16.4 depicts the initial expression and the \ufb01nal expression after all these\ntransformations. We could equally well have used rule 7.b to get the \ufb01nal expression\ndirectly, without using rule 1 to break the selection into two selections. In fact, rule 7.b\ncan itself be derived from rules 1 and 7.a.\n(a) Initial expression tree (b) Tree after multiple transformations\u220fname, title\n\u220fcourse_id, title\u03c3dept_name = Music\n                                  year = 2017\ninstructor\nteaches\ncourse\u220fname, title\n\u220fcourse_id, title\nyear = 2017 \u03c3dept_name = Music\u03c3\ninstructor teaches course^\nFigure 16.4 Multiple transformations.\n", "782": "754 Chapter 16 Query Optimization\nA set of equivalence rules is said to be minimal if no rule can be derived from any\ncombination of the others. The preceding example illustrates that the set of equiva-\nlence rules in Section 16.2.1 is not minimal. An expression equivalent to the original\nexpression may be generated in di\ufb00erent ways; the number of di\ufb00erent ways of generat-\ning an expression increases when we use a non minimal set of equivalence rules. Query\noptimizers therefore use minimal sets of equivalence rules.\nNow consider the following form of our example query:\n\u03a0name ,title((\u03c3dept\nname=\u201cMusic\u201d (instructor )\u22c8teaches )\u22c8\u03a0course\n id,title(course ))\nWhen we compute the subexpression:\n(\u03c3dept\nname=\u201cMusic\u201d (instructor )\u22c8teaches )\nwe obtain a relation whose schema is:\n(ID,name ,dept\nname ,salary ,course\n id,sec\nid,semester ,year)\nWe can eliminate several attributes from the schema by pushing projections based on\nequivalence rules 8.a and 8.b. The only attrib utes that we must retain are those that ei-\nther appear in the result of the query or are needed to process subsequent operations.\nBy eliminating unneeded attributes, we reduce the number of columns of the interme-\ndiate result. Thus, we reduce the size of the intermediate result. In our example, the\nonly attributes we need from the join of instructor andteaches arename andcourse\n id.\nTherefore, we can modify the expression to:\n\u03a0name ,title((\u03a0name ,course\n id((\u03c3dept\nname=\u201cMusic\u201d (instructor ))\u22c8teaches ))\n\u22c8\u03a0course\n id,title(course ))\nThe projection \u03a0name ,course\n idreduces the size of the intermediate join results.\n16.2.3 Join Ordering\nA good ordering of join operations is important for reducing the size of temporary\nresults; hence, most query optimizers pay a lot of attention to the join order. As men-\ntioned in equivalence rule 6.a, the natural-join operation is associative. Thus, for all\nrelations r1,r2,a n d r3:\n(r1\u22c8r2)\u22c8r3\u2261r1\u22c8(r2\u22c8r3)\nAlthough these expressions are equivalent, the costs of computing them may di\ufb00er.\nConsider again the expression:\n\u03a0name ,title((\u03c3dept\nname=\u201cMusic\u201d (instructor ))\u22c8teaches \u22c8\u03a0course\n id,title(course ))\n", "783": "16.2 Transformation of Relational Expressions 755\nWe could choose to compute teaches \u22c8\u03a0course\n id,title(course ) \ufb01rst, and then to join the\nresult with:\n\u03c3dept\nname=\u201cMusic\u201d (instructor )\nHowever, teaches \u22c8\u03a0course\n id,title(course ) is likely to be a large relation, since it\ncontains one tuple for every course taught. In contrast:\n\u03c3dept\nname=\u201cMusic\u201d (instructor )\u22c8teaches\nis probably a small relation. To see that it is, we note that a university has fewer in-\nstructors than courses and, since a university has a large number of departments, it\nis likely that only a small fraction of the university instructors are associated with\nthe Music department. Thus, the preceding expression results in one tuple for each\ncourse taught by an instructor in the Music department. Therefore, the temporary\nrelation that we must store is smaller than it would have been had we computed\nteaches \u22c8\u03a0course\n id,title(course )\ufb01 r s t .\nThere are other options to consider for evaluating our query. We do not care about\nthe order in which attributes appear in a join, since it is easy to change the order before\ndisplaying the result. Thus, for all relations r1andr2:\nr1\u22c8r2\u2261r2\u22c8r1\nThat is, natural join is commutative (equivalence rule 5).\nUsing the associativity and commutativity of the natural join (rules 5 and 6), con-\nsider the following relational-algebra expression:\n(instructor \u22c8\u03a0course\n id,title(course ))\u22c8teaches\nNote that there are no attributes in common between \u03a0course\n id,title(course )a n d instruc-\ntor, so the join is just a Cartesian product. If there are atuples in instructor andbtuples\nin\u03a0course\n id,title(course ), this Cartesian product generates a\u2217btuples, one for every\npossible pair of instructor tuple and course (without regard for whether the instruc-\ntor taught the course). This Cartesian product would produce a very large temporary\nrelation. However, if the user had entered the preceding expression, we could use the\nassociativity and commutativity of the natural join to transform this expression to the\nmore e\ufb03cient expression:\n(instructor \u22c8teaches )\u22c8\u03a0course\n id,title(course )\n16.2.4 Enumeration of Equivalent Expressions\nQuery optimizers can use equivalence rules to systematically generate expressions\nequivalent to the given query expression. The cost of an expression is computed based\n", "784": "756 Chapter 16 Query Optimization\nprocedure genAllEquivalent( E)\nEQ={E}\nrepeat\nM a t c he a c he x p r e s s i o n EiinEQwith each equivalence rule Rj\nifany subexpression eiofEimatches one side of Rj\nCreate a new expression E\u2032which is identical to Ei,e x c e p tt h a t\neiis transformed to match the other side of Rj\nAdd E\u2032toEQif it is not already present in EQ\nuntil no new expression can be added to EQ\nFigure 16.5 Procedure to generate all equivalent expressions.\non statistics that are discussed in Section 16.3. Cost-based query optimizers, described\nin Section 16.4 compute the cost of each alternative and pick the least cost alternative.\nConceptually, enumeration of equivalent expressions can be done as outlined in\nFigure 16.5. The process proceeds as follows: Given a query expression E,t h es e to f\nequivalent expressions EQinitially contains only E. Now, each expression in EQis\nmatched with each equivalence rule. If a subexpression ejof any expression Ei\u2208EQ\n(as a special case, ejcould be Eiitself) matches one side of an equivalence rule, the\noptimizer generates a copy EkofEi,i nw h i c h ejis transformed to match the other side\nof the rule, and adds EktoEQ. This process continues until no more new expressions\ncan be generated. With a properly chosen set of equivalence rules, the set of equivalent\nexpressions is \ufb01nite, and the process can be guaranteed to terminate.\nFor example, given an expression r\u22c8(s\u22c8t), the commutativity rule can match\nthe subexpression ( s\u22c8t), and would create a new expression r\u22c8(t\u22c8s). The\ncommutativity rule also matches the join at the root of r\u22c8(s\u22c8t), and creates a\nnew expression ( s\u22c8t)\u22c8r. Associativity and commutativity rules can continue to\nbe applied to generate new expressions. But eventually applying any equivalence rule\nwill only generate expressions that were already generated earlier, and the process will\nterminate.\nThe preceding process is extremely costly both in space and in time, but optimizers\ncan greatly reduce both the space and time cost, using two key ideas.\n1.If we generate an expression E\u2032from an expression E1by using an equivalence\nrule on subexpression ei,t h e n E\u2032andE1have identical subexpressions except for\neiand its transformation. Even eiand its transformed version usually share many\nidentical subexpressions. Expression-r epresentation techniques that allow both\nexpressions to point to shared subexpre ssions can reduce the space requirement\nsigni\ufb01cantly.\n", "785": "16.3 Estimating Statistics of Expression Results 757\n2.It is not always necessary to generate every expression that can be generated\nwith the equivalence rules. If an optimizer takes cost estimates of evaluation into\naccount, it may be able to avoid examining some of the expressions, as we shall\nsee in Section 16.4. We can reduce the time required for optimization by using\ntechniques such as these.\nWith these and other techniques to reduce the optimization time, equivalence rules can\nbe used to enumerate alternative plans, whose costs can be computed; the lowest-cost\nplan amongst the alternatives is then chosen. We discuss e\ufb03cient implementation of\ncost-based query optimization based on equivalence rules in Section 16.4.2.\nSome query optimizers use equivalence rules in a heuristic manner. With such an\napproach, if the left-hand side of an equivalence rule matches a subtree in a query plan,\nthe subtree is rewritten to match the right-hand side of the rule. This process is repeated\ntill the query plan cannot be further rewritten. Rules must be carefully chosen such\nthat the cost decreases when a rule is applied, and rewriting must eventually terminate.\nAlthough this approach can be implemented to execute quite fast, there is no guarantee\nthat it will \ufb01nd the optimal plan.\nYet other query optimizers focus on join order selection, which is often a key factor\nin query cost. We discuss algorithms for join-order optimization in Section 16.4.1.\n16.3 Estimating Statistics of Expression Results\nThe cost of an operation depends on the size and other statistics of its inputs. Given\nan expression such as r\u22c8(s\u22c8t) to estimate the cost of joining rwith ( s\u22c8t), we\nn e e dt oh a v ee s t i m a t e so fs t a t i s t i c ss u c ha st h es i z eo f s\u22c8t.\nIn this section, we \ufb01rst list some statistics about database relations that are stored\nin database-system catalogs, and then show how to use the stored statistics to estimate\nstatistics on the results of various relational operations.\nGiven a query expression, we consider it as a tree; we can start from the bottom-\nlevel operations, and estimate their statistics, and continue the process on higher-level\noperations, till we reach the root of the tree. The size estimates that we compute as\npart of these statistics can be used to compute the cost of algorithms for individual\noperations in the tree, and these costs can be added up to \ufb01nd the cost of an entire\nquery plan, as we saw in Chapter 15.\nOne thing that will become clear later in this section is that the estimates are not\nvery accurate, since they are based on assumptions that may not hold exactly. A query-\nevaluation plan that has the lowest estimated execution cost may therefore not actually\nhave the lowest actual execution cost. However, real-world experience has shown that\neven if estimates are not precise, the plans with the lowest estimated costs usually have\nactual execution costs that are either the lowest actual execution costs or are close to\nthe lowest actual execution costs.\n", "786": "758 Chapter 16 Query Optimization\n16.3.1 Catalog Information\nThe database-system catalog stores the following statistical information about database\nrelations:\n\u2022nr, the number of tuples in the relation r.\n\u2022br, the number of blocks containing tuples of relation r.\n\u2022lr, the size of a tuple of relation rin bytes.\n\u2022fr, the blocking factor of relation r\u2014that is, the number of tuples of relation rthat\n\ufb01t into one block.\n\u2022V(A,r), the number of distinct values that appear in the relation rfor attribute A.\nThis value is the same as the size of \u03a0A(r). IfAis a key for relation r,V(A,r)i snr.\nThe last statistic, V(A,r), can also be maintained for sets of attributes, if desired, instead\nof just for individual attributes. Thus, given a set of attributes, \ue22d,V(\ue22d,r)i st h es i z eo f\n\u03a0\ue22d(r).\nIf we assume that the tuples of relation rare stored together physically in a \ufb01le, the\nfollowing equation holds:\nbr=\u2308nr\nfr\u2309\nStatistics about indices, such as the heights of B+-tree indices and number of leaf pages\nin the indices, are also maintained in the catalog.\nIf we wish to maintain accurate statistics, then every time a relation is modi\ufb01ed, we\nmust also update the statistics. This update incurs a substantial amount of overhead.\nTherefore, most systems do not update the statistics on every modi\ufb01cation. Instead,\nthey update the statistics during periods of light system load. As a result, the statistics\nused for choosing a query-processing strategy may not be completely accurate. How-\never, if not too many updates occur in the intervals between the updates of the statistics,\nthe statistics will be su\ufb03ciently accurate to provide a good estimation of the relative\ncosts of the di\ufb00erent plans.\nThe statistical information noted here is simpli\ufb01ed. Real-world optimizers often\nmaintain further statistical information to improve the accuracy of their cost estimates\nof evaluation plans. For instance, most databases store the distribution of values for\neach attribute as a histogram : in a histogram, the values for the attribute are divided\ninto a number of ranges, and with each range the histogram associates the number\nof tuples whose attribute value lies in that range. Figure 16.6 shows an example of a\nhistogram for an integer-valued attribute that takes values in the range 1 to 25.\nAs an example of a histogram, the range of values for an attribute ageof a relation\nperson could be divided into 0\u20149, 10\u201419, . . . , 90\u201499 (assuming a maximum age of 99).\nWith each range we store a count of the number of person tuples whose agevalues lie\nin that range.\n", "787": "16.3 Estimating Statistics of Expression Results 759\nvaluefrequency50\n40\n30\n20\n10\n1\u20135 6\u201310 11\u201315 16\u201320 21\u201325\nFigure 16.6 Example of histogram.\nThe histogram shown in Figure 16.6, is an equi-width histogram since it divides the\nrange of values into equal-sized ranges. In contrast, an equi-depth histogram adjusts the\nboundaries of the ranges such that each range has the same number of values. Thus,\nan equi-depth histogram merely stores the boundaries of partitions of the range, and\nneed not store the number of values. For example, the following could be the equidepth\nhistogram for the data whose equi-width histogram is shown in Figure 16.6:\n( 4 ,8 ,1 4 ,1 9 )\nThe histogram indicates that 1 \u22155th of the tuples have age less than 4, another 1 \u22155th\nhave age \u22654 but<8, and so on, with the last 1 \u22155th having age \u226519. Information about\nthe total number of tuples is also stored with the equi-width histogram. Equi-depth\nhistograms are preferred to equi-width histograms since they provide better estimates,\nand occupy less space.\nHistograms used in database systems can al so record the number of distinct values\nin each range, in addition to the number of tuples with attribute values in that range.\nIn our example, the histogram could store the number of distinct age values that lie in\neach range. Without such histogram information, an optimizer would have to assume\nthat the distribution of values is uniform; that is, each range has the same number of\ndistinct values.\nIn many database applications, some values are very frequent, compared to other\nvalues. To get better estimates for queries that specify these values, many databases\nstore a list of nmost frequent values for some n(say 5 or 10), along with the number of\ntimes each value appears. In our example, if ages 4, 7, 18, 19, and 23 are the \ufb01ve most\nfrequently occurring values, the database could store the number of persons having\neach of these ages. The histogram then only stores statistics for age values other than\nthese \ufb01ve values, since we have now have exact counts for these values.\n", "788": "760 Chapter 16 Query Optimization\nA histogram takes up only a little space, so histograms on several di\ufb00erent at-\nt r i b u t e sc a nb es t o r e di nt h es y s t e mc a t a l o g .\n16.3.2 Selection Size Estimation\nThe size estimate of the result of a selection operation depends on the selection predi-\ncate. We \ufb01rst consider a single equality predicate, then a single comparison predicate,\nand \ufb01nally combinations of predicates.\n\u2022\u03c3A=a(r): If ais a frequently occurring value for which the occurrence count is\navailable, we can use that value directly as the size estimate for the selection.\nOtherwise if there is no histogram available, we assume uniform distribution\nof values (i.e., each value appears with equal probability), the selection result is\nestimated to have nr\u2215V(A,r) tuples, assuming that the value aappears in attribute\nAof some record of r. The assumption that the value ain the selection appears in\nsome record is generally true, and cost estimates often make it implicitly. However,\nit is often not realistic to assume that each value appears with equal probability.\nThecourse\n idattribute in the takes relation is an example where the assumption\nis not valid. It is reasonable to expect that a popular undergraduate course will\nhave many more students than a smaller specialized graduate course. Therefore,\ncertain course\n idvalues appear with greater probability than do others. Despite the\nfact that the uniform-distribution assumption is often not correct, it is a reasonable\napproximation of reality in many cases, and it helps us to keep our presentation\nrelatively simple.\nIf a histogram is available on attribute A, we can locate the range that contains\nthe value a, and modify the above-mentioned estimate nr\u2215V(A,r)b yu s i n gt h e\nfrequency count for that range instead of nr, and the number of distinct values\nthat occurs in that range instead of V(A,r).\n\u2022\u03c3A\u2264v(r): Consider a selection of the form \u03c3A\u2264v(r). Suppose that the lowest and\nhighest values (min( A,r)a n dm a x ( A,r)) for the attribute are stored in the catalog.\nAssuming that values are uniformly distributed, we can estimate the number of\nrecords that will satisfy the condition A\u2264vas:\n\u00b00i fv<min( A,r)\n\u00b0nrifv\u2265max( A,r), and,\n\u00b0nr\u22c5v\u2212min( A,r)\nmax( A,r)\u2212min( A,r),o t h e r w i s e .\nIf a histogram is available on attribute A, we can get a more accurate estimate;\nwe leave the details as an exercise for you.\nIn some cases, such as when the query is part of a stored procedure, the value\nvmay not be available when the query is optimized. In such cases, we assume that\napproximately one-half the records will satisfy the comparison condition. That is,\n", "789": "16.3 Estimating Statistics of Expression Results 761\nNote 16.2 COMPUTING AND MAINTAINING STATISTICS\nConceptually, statistics on relations can be thought of as materialized views, which\nshould be automatically maintained when relations are modi\ufb01ed. Unfortunately,\nkeeping statistics up-to-date on every insert, delete or update to the database can be\nvery expensive. On the other hand, optimizers generally do not need exact statis-\ntics: an error of a few percent may result in a plan that is not quite optimal being\nchosen, but the alternative plan chosen is likely to have a cost which is within a\nfew percent of the optimal cost. Thus, it is acceptable to have statistics that are\napproximate.\nDatabase systems reduce the cost of generating and maintaining statistics, as\noutlined below, by exploiting the fact that statistics can be approximate.\n\u2022Statistics are often computed from a sample of the underlying data, instead\nof examining the entire collection of data. For example, a fairly accurate his-\ntogram can be computed from a sample of a few thousand tuples, even on a\nrelation that has millions, or hundreds of millions of records. However, the\nsample used must be a random sample ; a sample that is not random may have\nan excessive representation of one part o f the relation and can give misleading\nresults. For example, if we used a sample of instructors to compute a histogram\non salaries, if the sample has an overrepresentation of lower-paid instructors\nthe histogram would result in wrong estimates. Database systems today rou-\ntinely use random sampling to create statistics. See the bibliographical notes\nonline for references on sampling.\n\u2022Statistics are not maintained on every update to the database. In fact, some\ndatabase systems never update statistics automatically. They rely on database\nadministrators periodically running a command to update statistics. Oracle\nandPostgre SQL provide an SQL command called analyze that generates statis-\ntics on speci\ufb01ed relations, or on all relations. IBM DB2 supports an equivalent\ncommand called runstats . See the system manuals for details. You should be\naware that optimizers sometimes choose very bad plans due to incorrect statis-\ntics. Many database systems, such as IBM DB2 ,O r a c l e ,a n d SQL S erver, update\nstatistics automatically at certain points of time. For example, the system can\nkeep approximate track of how many tuples there are in a relation and re-\ncompute statistics if this number changes signi\ufb01cantly. Another approach is\nto compare estimated cardinalities of a relation scan with actual cardinalities\nwhen a query is executed, and if they di\ufb00er signi\ufb01cantly, initiate an update of\nstatistics for that relation.\n", "790": "762 Chapter 16 Query Optimization\nwe assume the result has nr\u22152 tuples; the estimate may be very inaccurate, but it\nis the best we can do without any further information.\n\u2022Complex selections:\n\u00b0Conjunction: Aconjunctive selection is a selection of the form:\n\u03c3\u03b81\u2227\u03b82\u2227\u22ef\u2227\u03b8n(r)\nWe can estimate the result size of such a selection: For each \u03b8i,w ee s t i m a t e\nthe size of the selection \u03c3\u03b8i(r), denoted by si, as described previously. Thus, the\nprobability that a tuple in the relation satis\ufb01es selection condition \u03b8iissi\u2215nr.\nThe preceding probability is called the selectivity of the selection \u03c3\u03b8i(r).\nAssuming that the conditions are independent of each other, the probability that\na tuple satis\ufb01es all the conditions is simply the product of all these probabilities.\nThus, we estimate the number of tuples in the full selection as:\nnr\u2217s1\u2217s2\u2217\u22ef\u2217sn\nnn\nr\n\u00b0Disjunction: Adisjunctive selection is a selection of the form:\n\u03c3\u03b81\u2228\u03b82\u2228\u22ef\u2228\u03b8n(r)\nA disjunctive condition is satis\ufb01ed by the union of all records satisfying the\nindividual, simple conditions \u03b8i.\nAs before, let si\u2215nrdenote the probability that a tuple satis\ufb01es condition \u03b8i.\nThe probability that the tuple will satisfy the disjunction is then 1 minus the\nprobability that it will satisfy none of the conditions:\n1\u2212(1\u2212s1\nnr)\u2217(1\u2212s2\nnr)\u2217\u22ef\u2217(1\u2212sn\nnr)\nMultiplying this value by nrgives us the estimated number of tuples that satisfy\nthe selection.\n\u00b0Negation: In the absence of nulls, the result of a selection \u03c3\u00ac\u03b8(r)i ss i m p l yt h e\ntuples of rthat are not in \u03c3\u03b8(r). We already know how to estimate the number\nof tuples in \u03c3\u03b8(r). The number of tuples in \u03c3\u00ac\u03b8(r) is therefore estimated to be\nnrminus the estimated number of tuples in \u03c3\u03b8(r).\nWe can account for nulls by estimating the number of tuples for which\nthe condition \u03b8would evaluate to unknown , and subtracting that number from\nthe above estimate, ignoring nulls. Estimating that number would require extra\nstatistics to be maintained in the catalog.\n16.3.3 Join Size Estimation\nIn this section, we see how to estimate the size of the result of a join.\n", "791": "16.3 Estimating Statistics of Expression Results 763\nThe Cartesian product r\u00d7scontains nr\u2217nstuples. Each tuple of r\u00d7soccupies\nlr+lsbytes, from which we can calculate the size of the Cartesian product.\nEstimating the size of a natural join is somewhat more complicated than estimating\nthe size of a selection or of a Cartesian product. Let r(R)a n d s(S)b er e l a t i o n s .\n\u2022IfR\u2229S=\u2205\u2014that is, the relations have no attribute in common\u2014then r\u22c8sis\nthe same as r\u00d7s, and we can use our estimation technique for Cartesian products.\n\u2022IfR\u2229Sis a key for R, then we know that a tuple of swill join with at most\none tuple from r. Therefore, the number of tuples in r\u22c8sis no greater than the\nnumber of tuples in s.T h ec a s ew h e r e R\u2229Sis a key for Sis symmetric to the\ncase just described. If R\u2229Sforms a foreign key of S, referencing R, the number\nof tuples in r\u22c8sis exactly the same as the number of tuples in s.\n\u2022The most di\ufb03cult case is when R\u2229Sis a key for neither RnorS.I nt h i sc a s e ,w e\nassume, as we did for selections, that each value appears with equal probability.\nConsider a tuple tofr, and assume R\u2229S={A}. We estimate that tuple tproduces\nns\nV(A,s)\ntuples in r\u22c8s, since this number is the average number of tuples in swith a given\nvalue for the attributes A. Considering all the tuples in r, we estimate that there\nare\nnr\u2217ns\nV(A,s)\ntuples in r\u22c8s. Observe that, if we reverse the roles of randsin the preceding\nestimate, we obtain an estimate of\nnr\u2217ns\nV(A,r)\ntuples in r\u22c8s.T h e s et w oe s t i m a t e sd i \ufb00 e ri f V(A,r)\u2260V(A,s). If this situation\noccurs, there are likely to be dangling tuples that do not participate in the join.\nThus, the lower of the two estimates is probably the more accurate one.\nThe preceding estimate of join size may be too high if the V(A,r)v a l u e sf o r\nattribute Ainrh a v ef e wv a l u e si nc o m m o nw i t ht h e V(A,s) values for attribute A\nins. However, this situation is unlikely to happen in the real world, since dangling\ntuples either do not exist or constitute only a small fraction of the tuples, in most\nreal-world relations.\nMore important, the preceding estimate depends on the assumption that each\nvalue appears with equal probability. More sophisticated techniques for size esti-\nmation have to be used if this assumption does not hold. For example, if we have\nhistograms on the join attributes of both relations, and both histograms have the\nsame ranges, then we can use the above estimation technique within each range,\n", "792": "764 Chapter 16 Query Optimization\nusing the number of rows with values in the range instead of nrorns,a n dt h e\nnumber of distinct values in that range, instead of V(A,r)o rV(A,s). We then add\nup the size estimates obtained for each range to get the overall size estimate. We\nleave the case where both relations have histograms on the join attribute, but the\nhistograms have di\ufb00erent ranges, as an exercise for you.\nWe can estimate the size of a theta join r\u22c8\u03b8sby rewriting the join as \u03c3\u03b8(r\u00d7s)\nand using the size estimates for Cartesian products along with the size estimates for\nselections, which we saw in Section 16.3.2.\nTo illustrate all these ways of estimating join sizes, consider the expression:\nstudent \u22c8takes\nAssume the following catalog information about the two relations:\n\u2022nstudent=5000.\n\u2022ntakes=10000.\n\u2022V(ID,takes )=2500, which implies that only half the students have taken any\ncourse (this is unrealistic, but we use it to show that our size estimates are correct\neven in this case), and on average, each student who has taken a course has taken\nfour courses.\nNote that since IDis a primary key of student ,V(ID,student )=nstudent=5000.\nThe attribute IDintakes is a foreign key on student , and null values do not occur in\ntakes .ID,s i n c e IDis part of the primary key of takes ;t h u s ,t h es i z eo f student \u22c8takes\nis exactly ntakes, which is 10000.\nWe now compute the size estimates for student \u22c8takes without using information\nabout foreign keys. Since V(ID,takes )=2500 and V(ID,student )=5000, the two\nestimates we get are 5000 \u221710000\u22152500=20000 and 5000 \u221710000\u22155000=10000,\na n dw ec h o o s et h el o w e ro n e .I nt h i sc a s e ,t h el o w e ro ft h e s ee s t i m a t e si st h es a m ea s\nthat which we computed earlier from information about foreign keys.\n16.3.4 Size Estimation for Other Operations\nNext we outline how to estimate the sizes of the results of other relational-algebra op-\nerations.\n\u2022Projection: The estimated size (number of records or number of tuples) of a pro-\njection of the form \u03a0A(r)i sV(A,r), since projection eliminates duplicates.\n\u2022Aggregation: T h es i z eo fG\u03b3A(r)i ss i m p l y V(G,r), since there is one tuple inG\u03b3A(r)\nfor each distinct value of G.\n\u2022Set operations: If the two inputs to a set operation are selections on the same rela-\ntion, we can rewrite the set operation as di sjunctions, conjunctions, or negations.\nFor example, \u03c3\u03b81(r)\u222a\u03c3\u03b82(r)c a nb er e w r i t t e na s \u03c3\u03b81\u2228\u03b82(r). Similarly, we can rewrite\n", "793": "16.3 Estimating Statistics of Expression Results 765\nintersections as conjunctions, and we can rewrite set di\ufb00erence by using negation,\nso long as the two relations participating in the set operations are selections on the\nsame relation. We can then use the estimates for selections involving conjunctions,\ndisjunctions, and negation in Section 16.3.2.\nIf the inputs are not selections on the same relation, we estimate the sizes this\nway: The estimated size of r\u222asis the sum of the sizes of rands.T h ee s t i m a t e d\nsize of r\u2229sis the minimum of the sizes of rands.T h ee s t i m a t e ds i z eo f r\u2212sis the\nsame size as r. All three estimates may be inaccurate, but provide upper bounds\non the sizes.\n\u2022Outer join: The estimated size of r\u27d5sis the size of r\u22c8sp l u st h es i z eo f r;t h a to f\nr\u27d6sis symmetric, while that of r\u27d7sis the size of r\u22c8splus the sizes of rand\ns. All three estimates may be inaccurate, but provide upper bounds on the sizes.\n16.3.5 Estimation of Number of Distinct Values\nThe size estimates discussed earlier depend on statistics such as histograms, or at a\nminimum, the number of distinct values for an attribute. While these statistics can be\nprecomputed and stored for relations in the database, we need to compute them for\nintermediate results. Note that estimation of the number of sizes and the number of\ndistinct values of attributes in an intermediate result Eihelps us estimate the sizes and\nnumber of distinct values of attributes in the next level intermediate results that use Ei.\nFor selections, the number of distinct val ues of an attribute (or set of attributes) A\nin the result of a selection, V(A,\u03c3\u03b8(r)), can be estimated in these ways:\n\u2022If the selection condition \u03b8forces Ato take on a speci\ufb01ed value (e.g., A=3),\nV(A,\u03c3\u03b8(r))=1.\n\u2022If\u03b8forces Ato take on one of a speci\ufb01ed set of values (e.g., ( A=1\u2228A=3\u2228A=4)),\nthen V(A,\u03c3\u03b8(r)) is set to the number of speci\ufb01ed values.\n\u2022If the selection condition \u03b8is of the form Ao pv ,w h e r e opis a comparison operator,\nV(A,\u03c3\u03b8(r)) is estimated to be V(A,r)\u2217s,w h e r e sis the selectivity of the selection.\n\u2022In all other cases of selections, we assume that the distribution of Avalues is inde-\npendent of the distribution of the values on which selection conditions are speci-\n\ufb01ed, and we use an approximate estimate of min( V(A,r),n\u03c3\u03b8(r)). A more accurate\nestimate can be derived for this case using probability theory, but the preceding\napproximation works fairly well.\nFor joins, the number of distinct values of an attribute (or set of attributes) Ain\nthe result of a join, V(A,r\u22c8s), can be estimated in these ways:\n", "794": "766 Chapter 16 Query Optimization\n\u2022If all attributes in Aare from r,V(A,r\u22c8s) is estimated as min( V(A,r),nr\u22c8s),\nand similarly if all attributes in Aare from s,V(A,r\u22c8s)i se s t i m a t e dt ob e\nmin( V(A,s),nr\u22c8s).\n\u2022IfAcontains attributes A1f r o m randA2f r o m s,t h e n V(A,r\u22c8s) is estimated as:\nmin( V(A1,r)\u2217V(A2\u2212A1,s),V(A1\u2212A2,r)\u2217V(A2,s),nr\u22c8s)\nNote that some attributes may be in A1a sw e l la si n A2, and A1\u2212A2a n d A2\u2212A1\ndenote, respectively, attributes in Athat are only from rand attributes in Athat are\nonly from s. Again, more accurate estimates can be derived by using probability\ntheory, but the above approximations work fairly well.\nThe estimates of distinct values are straightforward for projections: They are the\nsame in\u03a0A(r)a si n r. The same holds for grouping attributes of aggregation. For results\nofsum,count ,a n d average , we can assume, for simplicity, that all aggregate values\nare distinct. For min(A)a n d max(A), the number of distinct values can be estimated\nas min( V(A,r),V(G,r)), where Gdenotes the grouping attributes. We omit details of\nestimating distinct values for other operations.\n16.4 Choice of Evaluation Plans\nGeneration of expressions is only part of the query-optimization process, since each\noperation in the expression can be implemented with di\ufb00erent algorithms. An evalua-\ntion plan de\ufb01nes exactly what algorithm should be used for each operation, and how\nthe execution of the operations should be coordinated.\nGiven an evaluation plan, we can estimate its cost using statistics estimated by\nthe techniques in Section 16.3 coupled with cost estimates for various algorithms and\nevaluation methods described in Chapter 15.\nAcost-based optimizer explores the space of all query-evaluation plans that are\nequivalent to the given query, and chooses the one with the least estimated cost. We\nhave seen how equivalence rules can be used to generate equivalent plans. However,\ncost-based optimization with arbitrary equivalence rules is fairly complicated. We \ufb01rst\ncover a simpler version of cost-based optimization, which involves only join-order and\njoin algorithm selection, in Section 16.4.1. Then, in Section 16.4.2, we brie\ufb02y sketch\nhow a general-purpose optimizer based on equivalence rules can be built, without going\ninto details.\nExploring the space of all possible plans may be too expensive for complex queries.\nMost optimizers include heuristics to red uce the cost of query optimization, at the\npotential risk of not \ufb01nding the optimal plan. We study some such heuristics in Section\n16.4.3.\n", "795": "16.4 Choice of Evaluation Plans 767\n16.4.1 Cost-Based Join-Order Selection\nThe most common type of query in SQL consists of a join of a few relations, with join\npredicates and selections speci\ufb01ed in the where clause. In this section we consider the\nproblem of choosing the optimal join order for such a query.\nFor a complex join query, the number of di\ufb00erent query plans that are equivalent\nto the query can be large. As an illustration, consider the expression:\nr1\u22c8r2\u22c8\u22ef\u22c8 rn\nwhere the joins are expressed without any ordering. With n=3, there are 12 di\ufb00erent\njoin orderings:\nr1\u22c8(r2\u22c8r3) r1\u22c8(r3\u22c8r2)( r2\u22c8r3)\u22c8r1 (r3\u22c8r2)\u22c8r1\nr2\u22c8(r1\u22c8r3) r2\u22c8(r3\u22c8r1)( r1\u22c8r3)\u22c8r2 (r3\u22c8r1)\u22c8r2\nr3\u22c8(r1\u22c8r2) r3\u22c8(r2\u22c8r1)( r1\u22c8r2)\u22c8r3 (r2\u22c8r1)\u22c8r3\nIn general, with nrelations, there are (2( n\u22121))!\u2215(n\u22121)!di\ufb00erent join orders.\n(We leave the computation of this expression for you to do in Exercise 16.12.) For\njoins involving small numbers of relations, this number is acceptable; for example, with\nn=5, the number is 1680. However, as nincreases, this number rises quickly. With\nn=7, the number is 665,280; with n=10, the number is greater than 17 .6 billion!\nLuckily, it is not necessary to generate all the expressions equivalent to a given\nexpression. For example, suppose we want to \ufb01nd the best join order of the form:\n(r1\u22c8r2\u22c8r3)\u22c8r4\u22c8r5\nwhich represents all join orders where r1,r2,a n d r3are joined \ufb01rst (in some order), and\nthe result is joined (in some order) with r4andr5. There are 12 di\ufb00erent join orders\nfor computing r1\u22c8r2\u22c8r3, and 12 orders for computing the join of this result with\nr4andr5. Thus, there appear to be 144 join orders to examine. However, once we have\nfound the best join order for the subset of relations {r1,r2,r3},w ec a nu s et h a to r d e rf o r\nfurther joins with r4andr5, and we can ignore all costlier join orders of r1\u22c8r2\u22c8r3.\nThus, instead of 144 choices to examine, we need to examine only 12 + 12 choices.\nUsing this idea, we can develop a dynamic-programming algorithm for \ufb01nding opti-\nmal join orders. Dynamic-programming algorithms store results of computations and\nreuse them, a procedure that can reduce execution time greatly.\nWe now consider how to \ufb01nd the optimal join order for a set of nrelations S=\n{r1,r2,\u2026,rn}, where each relation may have selection conditions, and a set of join\nconditions between the relations riis provided. We assume that relations have unique\nnames.\nA recursive procedure implementing the dynamic-programming algorithm appears\nin Figure 16.7 and is invoked as FindBestPlan( S), where Sis the set of relations above.\nThe procedure applies selections on individual relations at the earliest possible point,\n", "796": "768 Chapter 16 Query Optimization\nprocedure FindBestPlan( S)\nif(bestplan [S].cost\u2260\u221e)/ *bestplan [S] already computed */\nreturn bestplan [S]\nif(Scontains only 1 relation)\nsetbestplan [S].plan andbestplan [S].costbased on the best way of\naccessing Susing selection conditions (if any) on S.\nelse for each non-empty subset S1o fSsuch that S1\u2260S\nP1 = FindBestPlan( S1)\nP2 = FindBestPlan( S\u2212S1)\nfor each algorithm Afor joining the results of P1a n d P2\n// For indexed-nested loops join, the outer relation could be P1o rP2.\n// Similarly for hash-join, the build relation could be P1o rP2.\n// We assume the alternatives are considered as separate algorithms.\n// We assume cost of Adoes not include cost of reading the inputs.\nifalgorithm Ais indexed nested loops\nLetPoandPidenote the outer and inner inputs of A\nifPihas a single relation ri,a n d rihas an index on the join\nattributes\nplan = \u201cexecute Po.plan; join results of Poandriusing A\u201d,\nwith any selection condition on Piperformed as\npart of the join condition\ncost = Po.cost+c o s to f A\nelse/* Cannot use indexed nested loops join */\ncost =\u221e\nelse\nplan = \u201cexecute P1.plan;e x e c u t e P2.plan;\njoin results of P1a n d P2u s i n g A\u201d\ncost = P1.cost+P2.cost+c o s to f A\nifcost<bestplan [S].cost\nbestplan [S].cost=c o s t\nbestplan [S].plan =p l a n\nreturn bestplan [S]\nFigure 16.7 Dynamic-programming algorithm for join-order optimization.\nthat is, when the relations are accessed. It is easiest to understand the procedure as-\nsuming that all joins are natural joins, although the procedure works unchanged with\nany join condition. With arbitrary join conditions, the join of two subexpressions is\nunderstood to include all join conditions that relate attributes from the two subexpres-\nsions.\n", "797": "16.4 Choice of Evaluation Plans 769\nThe procedure stores the evaluation plans it computes in an associative array\nbestplan , which is indexed by sets of relations. Each element of the associative array\ncontains two components: the cost of the best plan of S,a n dt h ep l a ni t s e l f .T h ev a l u e\nofbestplan [S].costis assumed to be initialized to \u221eifbestplan [S] has not yet been\ncomputed.\nThe procedure \ufb01rst checks if the best plan for computing the join of the given set\nof relations Shas been computed already (and stored in the associative array bestplan );\nif so, it returns the already computed plan.\nIfScontains only one relation, the best way of accessing S(taking selections on S,\nif any, into account) is recorded in bestplan . This may involve using an index to identify\ntuples, and then fetching the tuples (often referred to as an index scan ), or scanning the\nentire relation (often referred to as a relation scan ).2If there is any selection condition\nonS, other than those ensured by an index scan, a selection operation is added to the\nplan to ensure all selections on Sare satis\ufb01ed.\nOtherwise, if Scontains more than one relation, the procedure tries every way of\ndividing Sinto two disjoint subsets. For each division, the procedure recursively \ufb01nds\nthe best plans for each of the two subsets. It then considers all possible algorithms\nfor joining the results of the two subsets. Note that since indexed nested loops join can\npotentially use either input P1o rP2 as the inner input, we consider the two alternatives\nas two di\ufb00erent algorithms. The choice of build versus probe input also leads us to\nconsider the two choices for hash join as two di\ufb00erent algorithms.\nThe cost of each alternative is considered, and the least cost option chosen. The\njoin cost considered should not include the cost of reading the inputs, since we as-\nsume that the input is pipelined from the preceding operators, which could be a rela-\ntion/index scan, or a preceding join. Recall that some operators, such as hash join, can\nbe treated as having suboperators with a blocking (materialized) edge between them,\nbut with the input and output edges of the join being pipelined. The join cost formulae\nthat we saw in Chapter 15 can be used with appropriate modi\ufb01cations to ignore the\ncost of reading the input relations. Note that indexed nested loops join is treated di\ufb00er-\nently from other join techniques: the plan as well as the cost are di\ufb00erent in this case,\nsince we do not perform a relation/index scan of the inner input, and the index lookup\ncost is included in the cost of indexed nested loops join.\nThe procedure picks the cheapest plan from among all the alternatives for dividing\nSinto two sets and the algorithms for joining the results of the two sets. The cheapest\nplan and its cost are stored in the array bestplan and returned by the procedure. The\ntime complexity of the procedure can be shown to be O(3n) (see Practice Exercise\n16.13).\nThe order in which tuples are generated by the join of a set of relations is important\nfor \ufb01nding the best overall join order, since it can a\ufb00ect the cost of further joins. For\n2If an index contains all the attributes of a relation that are used in a query, it is possible to perform an index-only scan ,\nwhich retrieves the required attribute values from the index, without fetching actual tuples.\n", "798": "770 Chapter 16 Query Optimization\ninstance, if merge join is used, a potentially expensive sort operation is required on the\ninput, unless the input is already sorted on the join attribute.\nA particular sort order of the tuples is said to be an interesting sort order if it could\nbe useful for a later operation. For instance, generating the result of r1\u22c8r2\u22c8r3sorted\non the attributes common with r4orr5may be useful, but generating it sorted on the\nattributes common to only r1andr2is not useful. Using merge join for computing\nr1\u22c8r2\u22c8r3may be costlier than using some other join technique, but it may provide\nan output sorted in an interesting sort order.\nHence, it is not su\ufb03cient to \ufb01nd the best join order for each subset of the set of\nngiven relations. Instead, we have to \ufb01nd the best join order for each subset, for each\ninteresting sort order of the join result for that subset. The bestplan array can now\nbe indexed by [ S,o], where Sis a set of relations, and ois an interesting sort order.\nThe FindBestPlan function can then be modi\ufb01ed to take interesting sort orders into\nconsideration; we leave details as an exercise for you (see Practice Exercise 16.11).\nThe number of subsets of nrelations is 2n. The number of interesting sort orders\nis generally not large. Thus, about 2njoin expressions need to be stored. The dynamic-\nprogramming algorithm for \ufb01nding the best join order can be extended to handle sort\norders. Speci\ufb01cally, when considering sort-merge join, the cost of sorting has to be\nadded if an input (which may be a relation, or the result of a join operation) is not\nsorted on the join attribute, but is not added if it is sorted.\nThe cost of the extended algorithm depends on the number of interesting orders\nfor each subset of relations; since this number has been found to be small in practice,\nthe cost remains at O(3n). With n=10, this number is around 59,000, which is much\nbetter than the 17 .6 billion di\ufb00erent join orders. More important, the storage required\nis much less than before, since we need to store only one join order for each interesting\ns o r to r d e ro fe a c ho f1 0 2 4s u b s e t so f r1,\u2026,r10. Although both numbers still increase\nrapidly with n, commonly occurring joins usually have less than 10 relations and can\nbe handled easily.\nThe code shown in Figure 16.7 actually considers each possible way of dividing S\ninto two disjoint subsets twice, since each of the two subsets can play the role of S1.\nConsidering the division twice does not a\ufb00ect correctness, but wastes time. The code\ncan be optimized as follows: \ufb01nd the alphabetically smallest relation riinS1, and the\nalphabetically smallest relation rjinS\u2212S1, and execute the loop only if ri<rj.D o i n g\nso ensures that each division is considered only once.\nFurther, the code also considers all possible join orders, including those that con-\ntain Cartesian products; for example, if two relations r1andr3do not have any join\ncondition linking the two relations, the code will still consider S={r1,r3}, which will\nresult in a Cartesian product. It is possibl e to take join conditions into account, and\nmodify the code to only generate divisions that do not result in Cartesian products. This\noptimization can save a great deal of time for many queries. See the Further Reading\nsection at the end of the chapter for references providing more details on Cartesian-\nproduct-free join order enumeration.\n", "799": "16.4 Choice of Evaluation Plans 771\n16.4.2 Cost-Based Optimization with Equivalence Rules\nThe join-order optimization technique we just saw handles the most common class\nof queries, which perform an inner join of a set of relations. However, many queries\nuse other features, such as aggregation, outer join, and nested queries, which are not\naddressed by join-order selection, but can be handled by using equivalence rules.\nIn this section we outline how to create a general-purpose cost-based optimizer\nbased on equivalence rules. Equivalence rules can help explore alternatives with a wide\nvariety of operations, such as outer joins, aggregations, and set operations, as we have\nseen earlier. Equivalence rules can be added if required for further operations, such as\noperators that return the top-K results in sorted order.\nIn Section 16.2.4, we saw how an optimizer could systematically generate all ex-\npressions equivalent to the given query. The procedure for generating equivalent expres-\nsions can be modi\ufb01ed to generate all possible evaluation plans as follows: A new class\nof equivalence rules, called physical equivalence rules , is added that allows a logical op-\neration, such as a join, to be transformed to a physical operation, such as a hash join,\nor a nested-loops join. By adding such rules to the original set of equivalence rules, the\nprocedure can generate all possible evaluation plans. The cost estimation techniques\nwe have seen earlier can then be used to choose the optimal (i.e., the least-cost) plan.\nHowever, the procedure shown in Section 16.2.4 is very expensive, even if we do\nnot consider generation of evaluation plans. To make the approach work e\ufb03ciently\nrequires the following:\n1.A space-e\ufb03cient representation of expr essions that avoids making multiple\nc o p i e so ft h es a m es u b e x p r e s s i o n sw h e ne q u i v a l e n c er u l e sa r ea p p l i e d .\n2.E \ufb03cient techniques for detecting duplicate derivations of the same expression.\n3.A form of dynamic programming based on memoization ,w h i c hs t o r e st h eo p t i m a l\nquery evaluation plan for a subexpression when it is optimized for the \ufb01rst time;\nsubsequent requests to optimize the same subexpression are handled by returning\nthe already memorized plan.\n4.Techniques that avoid generating all possible equivalent plans by keeping track\nof the cheapest plan generated for any subexpression up to any point of time, and\npruning away any plan that is more expensive than the cheapest plan found so\nfar for that subexpression.\nThe details are more complex than we wish to deal with here. This approach was pi-\noneered by the Volcano research project, and the query optimizer of SQL S erver is\nbased on this approach. See the bibliographical notes for references containing further\ninformation.\n16.4.3 Heuristics in Optimization\nA drawback of cost-based optimization is the cost of optimization itself. Although the\ncost of query optimization can be reduced by clever algorithms, the number of di\ufb00erent\n", "800": "772 Chapter 16 Query Optimization\nevaluation plans for a query can be very large, and \ufb01nding the optimal plan from this\nset requires a lot of computational e\ufb00ort. Hence, optimizers use heuristics to reduce\nthe cost of optimization.\nAn example of a heuristic rule is the following rule for transforming relational-\nalgebra queries:\n\u2022Perform selection operations as early as possible.\nA heuristic optimizer would use this rule without \ufb01nding out whether the cost is re-\nduced by this transformation. In the \ufb01rst tra nsformation example in Section 16.2, the\nselection operation was pushed into a join.\nWe say that the preceding rule is a heuristic because it usually, but not always,\nhelps to reduce the cost. For an example of where it can result in an increase in cost,\nconsider an expression \u03c3\u03b8(r\u22c8s), where the condition \u03b8refers to only attributes in\ns. The selection can certainly be performed before the join. However, if ris extremely\nsmall compared to s, and if there is an index on the join attributes of s,b u tn oi n d e xo n\nthe attributes used by \u03b8, then it is probably a bad idea to perform the selection early.\nPerforming the selection early\u2014that is, directly on s\u2014would require doing a scan of all\ntuples in s. It is probably cheaper, in this case, to compute the join by using the index\nand then to reject tuples that fail the selection. (This case is speci\ufb01cally handled by the\ndynamic programming algorithm for join order optimization.)\nThe projection operation, like the selection operation, reduces the size of relations.\nThus, whenever we need to generate a temporary relation, it is advantageous to apply\nimmediately any projections that are possible. This advantage suggests a companion to\nthe \u201cperform selections early\u201d heuristic:\n\u2022Perform projections early.\nIt is usually better to perform selections earlier than projections, since selections have\nthe potential to reduce the sizes of relations greatly, and selections enable the use of\nindices to access tuples. An example similar to the one used for the selection heuristic\nshould convince you that this heuristic does not always reduce the cost.\nOptimizers based on join-order enumeration typically use heuristic transforma-\ntions to handle constructs other than joins, and applying the cost-based join-order\nselection algorithm to subexpressions involving only joins and selections. Details of\nsuch heuristics are for the most part speci\ufb01c to individual optimizers, and we do not\ncover them.\nMost practical query optimizers have further heuristics to reduce the cost of opti-\nmization. For example, many query optimizers, such as the System R optimizer,3do\nnot consider all join orders, but rather restrict the search to particular kinds of join or-\n3System R was one of the \ufb01rst implementations of SQL, and its optimizer pioneered the idea of cost-based join-order\noptimization.\n", "801": "16.4 Choice of Evaluation Plans 773\nr4 r 5 r3\nr1 r2r5\nr4\nr3\nr2 r1\n(a) Left-deep join tree (b) Non-left-deep join tree\nFigure 16.8 Left-deep join trees.\nders. The System R optimizer considers only those join orders where the right operand\nof each join is one of the initial relations r1,\u2026,rn.S u c hj o i no r d e r sa r ec a l l e d left-deep\njoin orders . Left-deep join orders are particularly convenient for pipelined evaluation,\nsince the right operand is a stored relation, and thus only one input to each join is\npipelined.\nFigure 16.8 illustrates the di\ufb00erence between left-deep join trees and non-left-deep\njoin trees. The time it takes to consider all left-deep join orders is O(n!), which is much\nless than the time to consider all join orders. With the use of dynamic-programming\noptimizations, the System R optimizer can \ufb01nd the best join order in time O(n2n).\nContrast this cost with the O(3n) time required to \ufb01nd the best overall join order. The\nSystem R optimizer uses heuristics to push selections and projections down the query\ntree.\nA heuristic approach to reduce the cost of join-order selection, which was originally\nused in some versions of Oracle, works roughly this way: For an n-way join, it considers\nnevaluation plans. Each plan uses a left-deep join order, starting with a di\ufb00erent one\nof the nrelations. The heuristic constructs the join order for each of the nevaluation\nplans by repeatedly selecting the \u201cbest\u201d relation to join next, on the basis of a ranking\nof the available access paths. Either nested-loop or sort-merge join is chosen for each\nof the joins, depending on the available access paths. Finally, the heuristic chooses one\nof the nevaluation plans in a heuristic manner, on the basis of minimizing the number\nof nested-loop joins that do not have an index available on the inner relation and on\nthe number of sort-merge joins.\nQuery-optimization approaches that apply heuristic plan choices for some parts\nof the query, with cost-based choice based on generation of alternative access plans\non other parts of the query, have been adopted in several systems. The approach used\nin System R and in its successor, the Starburst project, is a hierarchical procedure\nbased on the nested-block concept of SQL. The cost-based optimization techniques\ndescribed here are used for each block of the query separately. The optimizers in several\n", "802": "774 Chapter 16 Query Optimization\ndatabase products, such as IBM DB2 and Oracle, are based on the above approach, with\nextensions to handle other operations such as aggregation. For compound SQL queries\n(using the \u222a,\u2229,o r\u2212operation), the optimizer processes each component separately\nand combines the evaluation plans to form the overall evaluation plan.\nMost optimizers allow a cost budget to be speci\ufb01ed for query optimization. The\nsearch for the optimal plan is terminated when the optimization cost budget is exceeded,\nand the best plan found up to that point is returned. The budget itself may be set dynam-\nically; for example, if a cheap plan is found for a query, the budget may be reduced, on\nthe premise that there is no point spending a lot of time optimizing the query if the best\nplan found so far is already quite cheap. On the other hand, if the best plan found so\nfar is expensive, it makes sense to invest more time in optimization, which could result\nin a signi\ufb01cant reduction in execution time. To best exploit this idea, optimizers usually\n\ufb01rst apply cheap heuristics to \ufb01nd a plan and then start full cost-based optimization\nwith a budget based on the heuristically chosen plan.\nMany applications execute the same query repeatedly, but with di\ufb00erent values for\nthe constants. For example, a university application may repeatedly execute a query to\n\ufb01nd the courses for which a student has registered, but each time for a di\ufb00erent student\nwith a di\ufb00erent value for the student ID. As a heuristic, many optimizers optimize\na query once, with whatever values were provided for the constants when the query\nwas \ufb01rst submitted, and cache the query plan. Whenever the query is executed again,\nperhaps with new values for constants, the cached query plan is reused (using new\nvalues for the constants). The optimal plan for the new constants may di\ufb00er from the\noptimal plan for the initial values, but as a heuristic the cached plan is reused.4Caching\nand reuse of query plans is referred to as plan caching .\nEven with the use of heuristics, cost-ba sed query optimization imposes a substan-\ntial overhead on query processing. However, the added cost of cost-based query op-\ntimization is usually more than o\ufb00set by the saving at query-execution time, which is\ndominated by slow disk accesses. The di\ufb00erence in execution time between a good plan\nand a bad one may be huge, making query optimization essential. The achieved saving\nis magni\ufb01ed in those applications that run on a regular basis, where a query can be op-\ntimized once, and the selected query plan can be used each time the query is executed.\nTherefore, most commercial systems include relatively sophisticated optimizers. The\nbibliographical notes give references to de scriptions of the query optimizers of actual\ndatabase systems.\n16.4.4 Optimizing Nested Subqueries\nSQL conceptually treats nested subqueries in the where clause as functions that take\nparameters and return either a single value or a set of values (possibly an empty set).\nThe parameters are the variables from an outer-level query that are used in the nested\n4For the student registration query, the plan would almost certainly be the same for any student ID. But a query that\ntook a range of student IDs, and returned registration information for all student IDs in that range, would probably have\na di\ufb00erent optimal plan if the range were very small than if the range were large.\n", "803": "16.4 Choice of Evaluation Plans 775\nsubquery (these variables are called correlation variables ). For instance, suppose we\nhave the following query, to \ufb01nd the names of all instructors who taught a course in\n2019:\nselect name\nfrom instructor\nwhere exists (select *\nfrom teaches\nwhere instructor .ID=teaches .ID\nandteaches .year=2019);\nConceptually, the subquery can be viewed as a function that takes a parameter (here,\ninstructor .ID) and returns the set of all courses taught in 2019 by instructors (with the\nsame ID).\nSQLevaluates the overall query (conceptually) by computing the Cartesian product\nof the relations in the outer from clause and then testing the predicates in the where\nclause for each tuple in the product. In the preceding example, the predicate tests if\nthe result of the subquery evaluation is empty. In practice, the predicates in the where\nclause that can be used as join predicates, or as selection predicates are evaluated as\npart of the selections on relations or to perf orm joins that avoid Cartesian products.\nPredicates involving nested subqueries in the where clause are evaluated subsequently,\nsince they are usually expensive, by invoking the subquery as a function.\nThe technique of evaluating a nested subquery by invoking it as a function is called\ncorrelated evaluation . Correlated evaluation is not very e\ufb03cient, since the subquery is\nseparately evaluated for each tuple in the outer level query. A large number of random\ndisk I/Ooperations may result.\nSQL optimizers therefore attempt to transform nested subqueries into joins, where\npossible. E\ufb03cient join algorithms help avoid expensive random I/O. Where the trans-\nformation is not possible, the optimizer keeps the subqueries as separate expressions,\noptimizes them separately, and then evaluates them by correlated evaluation.\nAs an attempt at transforming a nested subquery into a join, the query in the pre-\nceding example can be rewritten in relational algebra as a join:\n\u03a0name(instructor \u22c8instructor.ID=teaches.ID\u2227teaches.year=2019teaches )\nUnfortunately, the above query is not quite correct, since the multiset versions of the\nrelational algebra operators are used in SQL implementations, and as a result an in-\nstructor who teaches multiple sections in 2019 will appear multiple times in the result\nof the relational algebra query, although that instructor would appear only once in the\nSQL query result. Using the set version of the relational algebra operators will not help\neither, since if there are two instructors with the same name who teach in 2019, the\nname would appear only once with the set version of relational algebra, but would ap-\npear twice in the SQL query result. (We note that the set version of relational algebra\n", "804": "776 Chapter 16 Query Optimization\nwould give the correct result if the query output contained the primary key of instructor ,\nnamely ID.)\nTo properly re\ufb02ect SQL semantics, the number of duplicates of a tuple in the result\nshould not change because of the rewriting. The semijoin operator of the relational al-\ngebra provides a solution to this problem. The multiset version of the semijoin operator\nr\u22c9\u03b8sis de\ufb01ned as follows: if a tuple riappears ntimes in r, it appears ntimes in the\nresult of r\u22c9\u03b8if there is at least one tuple sjsuch that riandsjtogether satisfy predicate\n\u03b8;o t h e r w i s e ridoes not appear in the result. The set version of the semijoin operator\nr\u22c9\u03b8scan be de\ufb01ned as \u03a0R(r\u22c8\u03b8s), where Ris the set of attributes in the schema\nofr. The multiset version of the semijoin operator outputs the same tuples, but the\nnumber of duplicates of each tuple riin the semijoin result is the same as the number\nof duplicates of riinr.\nThe preceding SQL query can be translated into the following equivalent relational\nalgebra using the multiset semijoin operator:\n\u03a0name(instructor \u22c9instructor.ID=teaches.ID\u2227teaches.year=2019teaches )\nThe above query in the multiset relational algebra gives the same result as the SQL\nquery, including the counts of duplicates. The query can equivalently be written as:\n\u03a0name(instructor \u22c9instructor.ID=teaches.ID(\u03c3teaches.year=2019(teaches )))\nThe following SQL query using the inclause is equivalent to the preceding SQL query\nusing the exists clause, and can be translated to the same relational algebra expression\nusing semijoin.\nselect name\nfrom instructor\nwhere instructor .IDin(select teaches .ID\nfrom teaches\nwhere teaches .year=2019);\nThe anti-semijoin is useful with not exists queries. The multiset anti-semijoin oper-\nator r\n\u22c9\u03b8sis de\ufb01ned as follows: if a tuple riappears ntimes in r, it appears ntimes\nin the result of r\u22c9\u03b8sif there does not exist any tuple sjinssuch that riandsjsatisfy\npredicate \u03b8;o t h e r w i s e ridoes not appear in the result. The anti-semijoin operator is\nalso known as the anti-join operator.\nConsider the SQL query:\nselect name\nfrom instructor\nwhere not exists (select *\nfrom teaches\nwhere instructor .ID=teaches .ID\nandteaches .year=2019);\n", "805": "16.4 Choice of Evaluation Plans 777\nThe preceding query can be translated into the following relational algebra using the\nanti-semijoin operator:\n\u03a0name(instructor\n \u22c9instructor.ID=teaches.ID(\u03c3teaches.year=2019(teaches )))\nIn general, a query of the form:\nselect A\nfrom r1,r2,..., rn\nwhere P1and exists (select *\nfrom s1,s2, ..., sm\nwhere P1\n2andP2\n2);\nwhere P1\n2are predicates that only reference the relations siin the subquery, and P2\n2\npredicates that also reference the relations rifrom the outer query, can be translated\nto:\n\u03a0A((\u03c3P1(r1\u00d7r2\u00d7\u2026\u00d7 rn))\u22c9P2\n2\u03c3P1\n2(s1\u00d7s2\u00d7\u2026\u00d7 sm))\nIfnot exists were used instead of exists , the semijoin should be replaced by anti-semijoin\nin the relational algebra query. If an inclause is used instead of exists ,t h er e l a t i o n a l\nalgebra query can be appropriately modi\ufb01ed by adding a corresponding predicate in\nthe semijoin predicate, as our earlier example illustrated.\nThe process of replacing a nested query by a query with a join, semijoin, or anti-\nsemijoin is called decorrelation . The semijoin and anti-semijoin operators can be e\ufb03-\nciently implemented using modi\ufb01cations of the join algorithms, as explored in Practice\nExercise 15.10.\nConsider the following query with aggregation in a scalar subquery, that \ufb01nds in-\nstructors who have taught more than one course section in 2019.\nselect name\nfrom instructor\nwhere 1<(select count (*)\nfrom teaches\nwhere instructor .ID=teaches .ID\nandteaches .year=2019);\nThe above query can be rewritten using a semijoin as follows:\n\u03a0name(instructor \u22c9(instructor.ID=TID)\u2227(1<cnt)(IDasTID\u03b3count (\u2217)ascnt(\u03c3year=2019(teaches )))\nObserve that the subquery has a predicate instructor .ID=teaches .ID, and aggregation\nwithout a group by clause. The decorrelated query has the predicate moved into the\nsemijoin condition, and the aggregation is now grouped by ID.T h ep r e d i c a t e1 <(sub-\nquery) has turned into a semijoin predicate. I ntuitively, the subquery performs a sep-\n", "806": "778 Chapter 16 Query Optimization\narate count for each instructor .ID;g r o u p i n gb y IDensures that counts are computed\nseparately for each ID.\nDecorrelation is clearly more complicated when the nested subquery uses aggre-\ngation, or when the nested subquery is used as a scalar subquery. In fact, decorrelation\nis not possible for certain cases of subqueries. For example, a subquery that is used\nas a scalar subquery is expected to return only one result; if it returns more than one\nresult, a runtime exception can occur, which is not possible with a decorrelated query.\nFurther, whether to decorrelate or not should ideally be done in a cost-based manner,\ndepending on whether decorrelation reduces the cost or not. Some query optimizers\nrepresent nested subqueries using extended relational-algebra constructs, and express\ntransformations of nested subqueries to semijoin, anti-semijoin, and so forth, as equiv-\nalence rules. We do not attempt to give algorithms for the general case, and instead\nrefer you to relevant items in the online bibliographical notes.\nOptimization of complex nested subqueries is a complicated task, as you can in-\nfer from the preceding discussion, and many optimizers do only a limited amount of\ndecorrelation. It better to avoid using comp lex nested subqueries, where possible, since\nwe cannot be sure that the query optimizer will succeed in converting them to a form\nthat can be evaluated e\ufb03ciently.\n16.5 Materialized Views\nWhen a view is de\ufb01ned, normally the database stores only the query de\ufb01ning the view.\nIn contrast, a materialized view is a view whose contents are computed and stored.\nMaterialized views constitute redundant data, in that their contents can be inferred\nfrom the view de\ufb01nition and the rest of the database contents. However, it is much\ncheaper in many cases to read the contents of a materialized view than to compute the\ncontents of the view by executing the query de\ufb01ning the view.\nMaterialized views are important for improving performance in some applications.\nConsider this view, which gives the total salary in each department:\ncreate view department\n total\n salary (dept\nname ,total\n salary )as\nselect dept\nname ,sum(salary )\nfrom instructor\ngroup by dept\nname ;\nSuppose the total salary amount at a department is required frequently. Computing the\nview requires reading every instructor tuple pertaining to a department and summing\nup the salary amounts, which can be time-consuming. In contrast, if the view de\ufb01nition\nof the total salary amount were materialized, the total salary amount could be found\nby looking up a single tuple in the materialized view.5\n5The di\ufb00erence may not be all that large for a medium-sized university, but in other settings the di\ufb00erence can be\nvery large. For example, if the materialized view computed total sales of each product, from a sales relation with tens\n", "807": "16.5 Materialized Views 779\n16.5.1 View Maintenance\nA problem with materialized views is that they must be kept up-to-date when the data\nused in the view de\ufb01nition changes. For instance, if the salary value of an instructor\nis updated, the materialized view will become inconsistent with the underlying data,\nand it must be updated. The task of keeping a materialized view up-to-date with the\nunderlying data is known as view maintenance .\nViews can be maintained by manually written code: That is, every piece of code\nthat updates the salary value can be modi\ufb01ed to also update the total salary amount for\nthe corresponding department. However, this approach is error prone, since it is easy\nto miss some places where the salary is updated, and the materialized view will then\nno longer match the underlying data.\nAnother option for maintaining materialized views is to de\ufb01ne triggers on insert,\ndelete, and update of each relation in the view de\ufb01nition. The triggers must modify\nthe contents of the materialized view, to take into account the change that caused the\ntrigger to \ufb01re. A simplistic way of doing so is to completely recompute the materialized\nview on every update.\nA better option is to modify only the a\ufb00ected parts of the materialized view, which\nis known as incremental view maintenance . We describe how to perform incremental\nview maintenance in Section 16.5.2.\nModern database systems provide more direct support for incremental view main-\ntenance. Database-system programmers no longer need to de\ufb01ne triggers for view main-\ntenance. Instead, once a view is declared to be materialized, the database system com-\nputes the contents of the view and incrementally updates the contents when the under-\nlying data change.\nMost database systems perform immediate view maintenance ; that is, incremental\nview maintenance is performed as soon as an update occurs, as part of the updat-\ning transaction. Some database systems also support deferred view maintenance ,w h e r e\nview maintenance is deferred to a later time; for example, updates may be collected\nthroughout a day, and materialized views may be updated at night. This approach re-\nduces the overhead on update transactions. However, materialized views with deferred\nview maintenance may not be consistent with the underlying relations on which they\nare de\ufb01ned.\n16.5.2 Incremental View Maintenance\nTo understand how to maintain materialized views incrementally, we start o\ufb00 by con-\nsidering individual operations, and then we see how to handle a complete expression.\nThe changes to a relation that can cause a materialized view to become out-of-date\nare inserts, deletes, and updates. To simplify our description, we replace updates to a\ntuple by deletion of the tuple followed by insertion of the updated tuple. Thus, we need\nof millions of tuples, the di\ufb00erence between computing t he aggregate from the underlying data and looking up the\nmaterialized view can be many orders of magnitude.\n", "808": "780 Chapter 16 Query Optimization\nto consider only inserts and deletes. The changes (inserts and deletes) to a relation or\nexpression are referred to as its di\ufb00erential .\n16.5.2.1 Join Operation\nConsider the materialized view v=r\u22c8s. Suppose we modify rby inserting a set of\ntuples denoted by ir. If the old value of ris denoted by rold,a n dt h en e wv a l u eo f rby\nrnew,rnew=rold\u222air.N o w ,t h eo l dv a l u eo ft h ev i e w , vold,i sg i v e nb y rold\u22c8s,a n dt h e\nnew value vnewis given by rnew\u22c8s.W ec a nr e w r i t e rnew\u22c8sas (rold\u222air)\u22c8s,w h i c h\nwe can again rewrite as ( rold\u22c8s)\u222a(ir\u22c8s). In other words:\nvnew=vold\u222a(ir\u22c8s)\nThus, to update the materialized view v, we simply need to add the tuples ir\u22c8sto the\nold contents of the materialized view. Inserts to sare handled in an exactly symmetric\nfashion.\nNow suppose ris modi\ufb01ed by deleting a set of tuples denoted by dr.U s i n gt h es a m e\nreasoning as above, we get:\nvnew=vold\u2212(dr\u22c8s)\nDeletes on sare handled in an exactly symmetric fashion.\n16.5.2.2 Selection and Projection Operations\nConsider a view v=\u03c3\u03b8(r). If we modify rby inserting a set of tuples ir, the new value\nofvcan be computed as:\nvnew=vold\u222a\u03c3\u03b8(ir)\nSimilarly, if ris modi\ufb01ed by deleting a set of tuples dr, the new value of vcan be com-\nputed as:\nvnew=vold\u2212\u03c3\u03b8(dr)\nProjection is a more di\ufb03cult operation with which to deal. Consider a materialized\nview v=\u03a0A(r). Suppose the relation ris on the schema R=(A,B), and rcontains two\ntuples ( a,2 )a n d( a,3 ) .T h e n , \u03a0A(r) has a single tuple ( a). If we delete the tuple ( a,2 )\nfrom r,w ec a n n o td e l e t et h et u p l e( a)f r o m\u03a0A(r): If we did so, the result would be an\nempty relation, whereas in reality \u03a0A(r) still has a single tuple ( a). The reason is that\nthe same tuple ( a) is derived in two ways, and deleting one tuple from rremoves only\none of the ways of deriving ( a); the other is still present.\nThis reason also gives us the intuition for a solution: For each tuple in a projection\nsuch as\u03a0A(r), we will keep a count of how many times it was derived.\n", "809": "16.5 Materialized Views 781\nWhen a set of tuples dris deleted from r,f o re a c ht u p l e tindrwe do the following:\nLett.Adenote the projection of ton the attribute A. We \ufb01nd ( t.A) in the materialized\nview and decrease the count stored with it by 1. If the count becomes 0, ( t.A) is deleted\nfrom the materialized view.\nHandling insertions is relatively straightforward. When a set of tuples iris inserted\nintor,f o re a c ht u p l e tinirwe do the following: If ( t.A) is already present in the ma-\nterialized view, we increase the count stored with it by 1. If not, we add ( t.A)t ot h e\nmaterialized view, with the count set to 1.\n16.5.2.3 Aggregation Operations\nAggregation operations proceed somewhat like projections. The aggregate operations\ninSQL arecount, sum, avg, min, andmax:\n\u2022count : Consider a materialized view v=G\u03b3count (B)(r), which computes the count\nof the attribute B,a f t e rg r o u p i n g rby attribute G.\nWhen a set of tuples iris inserted into r,f o re a c ht u p l e tinirwe do the following:\nWe look for the group t.Gin the materialized view. If it is not present, we add\n(t.G, 1) to the materialized view. If the group t.Gis present, we add 1 to the count\nof the group.\nWhen a set of tuples dris deleted from r,f o re a c ht u p l e tindrwe do the\nfollowing: We look for the group t.Gin the materialized view and subtract 1 from\nthe count for the group. If the count becomes 0, we delete the tuple for the group\nt.Gfrom the materialized view.\n\u2022sum: Consider a materialized view v=G\u03b3sum(B)(r).\nWhen a set of tuples iris inserted into r,f o re a c ht u p l e tinirwe do the following:\nWe look for the group t.Gin the materialized view. If it is not present, we add\n(t.G,t.B) to the materialized view; in addition, we store a count of 1 associated\nwith ( t.G,t.B), just as we did for projection. If the group t.Gis present, we add\nthe value of t.Bto the aggregate value for the group and add 1 to the count of the\ngroup.\nWhen a set of tuples dris deleted from r,f o re a c ht u p l e tindrwe do the\nfollowing: We look for the group t.Gin the materialized view and subtract t.B\nf r o mt h ea g g r e g a t ev a l u ef o rt h eg r o u p .W ea l s os u b t r a c t1f r o mt h ec o u n tf o rt h e\ngroup, and if the count becomes 0, we delete the tuple for the group t.Gfrom the\nmaterialized view.\nWithout keeping the extra count value, we would not be able to distinguish a\ncase where the sum for a group is 0 from the case where the last tuple in a group\nis deleted.\n\u2022avg: Consider a materialized view v=G\u03b3avg(B)(r).\nDirectly updating the average on an insert or delete is not possible, since it depends\nnot only on the old average and the tuple being inserted/deleted, but also on the\nnumber of tuples in the group.\n", "810": "782 Chapter 16 Query Optimization\nInstead, to handle the case of avg, we maintain the sum andcount aggregate\nvalues as described earlier and compute the average as the sum divided by the\ncount.\n\u2022min, max : Consider a materialized view v=G\u03b3min(B)(r). (The case of maxis exactly\nequivalent.)\nHandling insertions on ris straightforward, similar to the case of sum.M a i n -\ntaining the aggregate values minandmax on deletions may be more expensive. For\nexample, if the tuple tcorresponding to the minimum value for a group is deleted\nfrom r, we have to look at the other tuples of rthat are in the same group to \ufb01nd\nthe new minimum value. It is a good idea to create an ordered index on ( G,B)\nsince it would help us to \ufb01nd the new minimum value for a group very e\ufb03ciently.\n16.5.2.4 Other Operations\nThe set operation intersection is maintained as follows: Given materialized view v=\nr\u2229s, when a tuple is inserted in rwe check if it is present in s,a n di fs ow ea d di t\ntov. If a tuple is deleted from r, we delete it from the intersection if it is present. The\nother set operations, union andset di\ufb00erence , are handled in a similar fashion; we leave\ndetails to you.\nOuter joins are handled in much the same way as joins, but with some extra work.\nIn the case of deletion from rwe have to handle tuples in sthat no longer match any\ntuple in r. In the case of insertion to r,w eh a v et oh a n d l et u p l e si n sthat did not match\nany tuple in r. Again we leave details to you.\n16.5.2.5 Handling Expressions\nSo far we have seen how to update incrementally the result of a single operation. To\nhandle an entire expression, we can derive expressions for computing the incremental\nchange to the result of each subexpression, s tarting from the smallest subexpressions.\nFor example, suppose we wish to incrementally update a materialized view E1\u22c8E2\nwhen a set of tuples iris inserted into relation r. Let us assume ris used in E1alone.\nSuppose the set of tuples to be inserted into E1is given by expression D1. Then the\nexpression D1\u22c8E2gives the set of tuples to be inserted into E1\u22c8E2.\nSee the online bibliographical notes for further details on incremental view main-\ntenance with expressions.\n16.5.3 Query Optimization and Materialized Views\nQuery optimization can be performed by treating materialized views just like regular\nrelations. However, materialized views o\ufb00er further opportunities for optimization:\n\u2022Rewriting queries to use materialized views:\nSuppose a materialized view v=r\u22c8sis available, and a user submits a query\nr\u22c8s\u22c8t. Rewriting the query as v\u22c8tmay provide a more e\ufb03cient query plan\n", "811": "16.6 Advanced Topics in Query Optimization 783\nthan optimizing the query as submitted. Thus, it is the job of the query optimizer\nto recognize when a materialized view can be used to speed up a query.\n\u2022Replacing a use of a materialized view with the view de\ufb01nition:\nSuppose a materialized view v=r\u22c8sis available, but without any index on\nit, and a user submits a query \u03c3A=10(v). Suppose also that shas an index on the\ncommon attribute B,a n d rhas an index on attribute A. The best plan for this query\nmay be to replace vwith r\u22c8s, which can lead to the query plan \u03c3A=10(r)\u22c8s;t h e\nselection and join can be performed e\ufb03ciently by using the indices on r.Aands.B,\nrespectively. In contrast, evaluating the selection directly on vmay require a full\nscan of v,w h i c hm a yb em o r ee x p e n s i v e .\nThe online bibliographical notes give pointers to research showing how to perform\nquery optimization e\ufb03ciently with materialized views.\n16.5.4 Materialized View and Index Selection\nAnother related optimization problem is that of materialized view selection ,n a m e l y ,\n\u201cWhat is the best set of views to materialize?\u201d This decision must be made on the basis\nof the system workload , which is a sequence of queries and updates that re\ufb02ects the\ntypical load on the system. One simple criterion would be to select a set of materialized\nviews that minimizes the overall execution time of the workload of queries and updates,\nincluding the time taken to maintain the materialized views. Database administrators\nusually modify this criterion to take into account the importance of di\ufb00erent queries\nand updates: Fast response may be required for some queries and updates, but a slow\nresponse may be acceptable for others.\nIndices are just like materialized views, in that they too are derived data, can speed\nup queries, and may slow down updates. Thus, the problem of index selection is closely\nrelated to that of materialized view selection, although it is simpler. We examine index\nand materialized view selection in more detail in Section 25.1.4.1 and Section 25.1.4.2.\nMost database systems provide tools to help the database administrator with index\nand materialized view selection. These tools examine the history of queries and updates\nand suggest indices and views to be materialized. The Microsoft SQL Server Database\nTuning Assistant, the IBM DB2 Design Advisor, and the Oracle SQL Tuning Wizard\nare examples of such tools.\n16.6 Advanced Topics in Query Optimization\nThere are a number of opportunities for optimizing queries, beyond those we have seen\nso far. We examine a few of these in this section.\n", "812": "784 Chapter 16 Query Optimization\n16.6.1 Top- KOptimization\nMany queries fetch results sorted on some attributes, and require only the top Kre-\nsults for some K. Sometimes the bound Kis speci\ufb01ed explicitly. For example, some\ndatabases support a limit Kclause which results in only the top Kresults being re-\nturned by the query. Other databases support alternative ways of specifying similar\nlimits. In other cases, the query may not specify such a limit, but the optimizer may\nallow a hint to be speci\ufb01ed, indicating that only the top Kresults of the query are likely\nto be retrieved, even if the query generates more results.\nWhen Kis small, a query optimization plan that generates the entire set of re-\nsults, then sorts and generates the top K, is very ine\ufb03cient since it discards most of\nthe intermediate results that it computes. Several techniques have been proposed to\noptimize such top-K queries . One approach is to use pipelined plans that can generate\nthe results in sorted order. Another approach is to estimate what is the highest value\non the sorted attributes that will appear in the top- Koutput, and introduce selection\npredicates that eliminate larger values. If extra tuples beyond the top- Kare generated\nthey are discarded, and if too few tuples are generated then the selection condition is\nchanged and the query is re-executed. See the bibliographical notes for references to\nwork on top- Koptimization.\n16.6.2 Join Minimization\nWhen queries are generated through views, sometimes more relations are joined than\nare needed for computation of the query. For example, a view vmay include the join of\ninstructor anddepartment ,b u tau s eo ft h ev i e w vmay use only attributes from instruc-\ntor. The join attribute dept\nname ofinstructor is a foreign key referencing department .\nAssuming that instructor .dept\nname has been declared not null ,t h ej o i nw i t h department\ncan be dropped, with no impact on the query. For under the above assumption, the join\nwith department does not eliminate any tuples from instructor , nor does it result in extra\ncopies of any instructor tuple.\nDropping a relation from a join as above is an example of join minimization. In fact,\njoin minimization can be performed in other situations as well. See the bibliographical\nnotes for references on join minimization.\n16.6.3 Optimization of Updates\nUpdate queries often involve subqueries in the setandwhere clauses, which must also\nbe taken into account in optimizing the update. Updates that involve a selection on the\nupdated column (e.g., give a 10 percent salary raise to all employees whose salary is \u2265\n$100,000) must be handled carefully. If the update is done while the selection is being\nevaluated by an index scan, an updated tuple may be reinserted in the index ahead of\nthe scan and seen again by the scan; the same employee tuple may then get incorrectly\nupdated multiple times (an in\ufb01nite number of times, in this case). A similar problem\nalso arises with updates involving subqueries whose result is a\ufb00ected by the update.\n", "813": "16.6 Advanced Topics in Query Optimization 785\nThe problem of an update a\ufb00ecting the execution of a query associated with the up-\ndate is known as the Halloween problem (named so because it was \ufb01rst recognized on a\nHalloween day, at IBM). The problem can be avoided by executing the queries de\ufb01ning\nthe update \ufb01rst, creating a list of a\ufb00ected tuples, and updating the tuples and indices as\nthe last step. However, breaking up the execution plan in such a fashion increases the\nexecution cost. Update plans can be optimized by checking if the Halloween problem\ncan occur, and if it cannot occur, updates can be performed while the query is being\nprocessed, reducing the update overheads. For example, the Halloween problem can-\nnot occur if the update does not a\ufb00ect index attributes. Even if it does, if the updates\ndecrease the value while the index is scanned in increasing order, updated tuples will\nnot be encountered again during the scan. In such cases, the index can be updated even\nwhile the query is being executed, reducing the overall cost.\nUpdate queries that result in a large number of updates can also be optimized by\ncollecting the updates as a batch and then applying the batch of updates separately to\neach a\ufb00ected index. When applying the batch of updates to an index, the batch is \ufb01rst\nsorted in the index order for that index; such sorting can greatly reduce the amount of\nrandom I/Orequired for updating indices.\nSuch optimizations of updates are implemented in most database systems. See the\nbibliographical notes for references to such optimization.\n16.6.4 Multiquery Optimization and Shared Scans\nWhen a batch of queries are submitted together, a query optimizer can potentially\nexploit common subexpressions between the di\ufb00erent queries, evaluating them once\nand reusing them where required. Complex queries may in fact have subexpressions\nrepeated in di\ufb00erent parts of the query, which can be similarly exploited to reduce\nquery evaluation cost. Such optimization is known as multiquery optimization .\nCommon subexpression elimination optimizes subexpressions shared by di\ufb00erent\nexpressions in a program by computing and storing the result and reusing it wherever\nthe subexpression occurs. Common subexpression elimination is a standard optimiza-\ntion applied on arithmetic expressions by programming-language compilers. Exploiting\ncommon subexpressions among evaluation plans chosen for each of a batch of queries\nis just as useful in database query evaluation, and is implemented by some databases.\nHowever, multiquery optimization can do even better in some cases: A query typically\nhas more than one evaluation plan, and a judiciously chosen set of query evaluation\nplans for the queries may provide for a greater sharing and lesser cost than that a\ufb00orded\nby choosing the lowest cost evaluation plan for each query. More details on multiquery\noptimization may be found in references cited in the bibliographical notes.\nSharing of relation scans between queries is another limited form of multiquery op-\ntimization that is implemented in some databases. The shared-scan optimization works\nas follows: Instead of reading the relation repeatedly from disk, once for each query\nthat needs to scan a relation, data are read once from disk, and pipelined to each of\n", "814": "786 Chapter 16 Query Optimization\nthe queries. The shared-scan optimization is particularly useful when multiple queries\nperform a scan on a single large relation (typically a \u201cfact table\u201d).\n16.6.5 Parametric Query Optimization\nPlan caching, which we saw in Section 16.4.3, is used as a heuristic in many databases.\nRecall that with plan caching, if a query is invoked with some constants, the plan cho-\nsen by the optimizer is cached and reused if the query is submitted again, even if the\nconstants in the query are di\ufb00erent. For example, suppose a query takes a department\nname as a parameter and retrieves all courses of the department. With plan caching, a\nplan chosen when the query is executed for the \ufb01rst time, say for the Music department,\nis reused if the query is executed for any other department.\nSuch reuse of plans by plan caching is reasonable if the optimal query plan is\nnot signi\ufb01cantly a\ufb00ected by the exact value of the constants in the query. However, if\nthe plan is a\ufb00ected by the value of the constants, parametric query optimization is an\nalternative.\nInparametric query optimization , a query is optimized without being provided spe-\nci\ufb01c values for its parameters\u2014for example, dept\nname in the preceding example. The\noptimizer then outputs several plans, each optimal for a di\ufb00erent parameter value. A\nplan would be output by the optimizer only i f it is optimal for some possible value of\nthe parameters. The set of alternative plans output by the optimizer are stored. When\na query is submitted with speci\ufb01c values for its parameters, instead of performing a full\noptimization, the cheapest plan from the set of alternative plans computed earlier is\nused. Finding the cheapest such plan usually takes much less time than reoptimization.\nSee the bibliographical notes for references on parametric query optimization.\n16.6.6 Adaptive Query Processing\nAs we noted earlier, query optimization is based on estimates that are at best approxi-\nmations. Thus, it is possible at times for the optimizer to choose a plan that turns out\nto perform very badly. Adaptive operators that choose the speci\ufb01c operator at execu-\ntion time provide a partial solution to this problem. For example, SQL S erver supports\nan adaptive join algorithm that checks the size of its outer input, and chooses either\nnested loops join, or hash join depending on the size of the outer input.\nMany systems also include the ability to monitor the behavior of a plan during\nquery execution, and adapt the plan accordingly. For example, suppose the statistics\ncollected by the system during early stages of the plan\u2019s execution (or the execution\nof subparts of the plan) are found to di\ufb00er substantially from the optimizers estimates\nto such an extent that it is clear that the chosen plan is suboptimal. Then an adaptive\ns y s t e mm a ya b o r tt h ee x e c u t i o n ,c h o o s ean e wq u e r ye x e c u t i o np l a nu s i n gt h es t a t i s -\ntics collected during the initial execution, and restart execution using the new plan;\nthe statistics collected during the execution of the old plan ensure the old plan is not\nselected again. Further, the system must avoid repeated aborts and restarts; ideally, the\nsystem should ensure that the overall cost of query evaluation is close to that with the\n", "815": "16.7 Summary 787\nplan that would be chosen if the optimizer had exact statistics. The speci\ufb01c criteria and\nmechanisms for such adaptive query processing are complex, and are referenced in the\nbibliographic notes available online.\n16.7 Summary\n\u2022Given a query, there are generally a variety of methods for computing the answer.\nIt is the responsibility of the system to transform the query as entered by the user\ninto an equivalent query that can be computed more e\ufb03ciently. The process of\n\ufb01nding a good strategy for processing a query is called query optimization .\n\u2022The evaluation of complex queries involves many accesses to disk. Since the trans-\nfer of data from disk is slow relative to the speed of main memory and the CPU\nof the computer system, it is worthwhile to allocate a considerable amount of pro-\ncessing to choose a method that minimizes disk accesses.\n\u2022There are a number of equivalence rules that we can use to transform an expression\ninto an equivalent one. We use these rules to generate systematically all expressions\nequivalent to the given query.\n\u2022Each relational-algebra expression represents a particular sequence of operations.\nThe \ufb01rst step in selecting a query-processing strategy is to \ufb01nd a relational-algebra\nexpression that is equivalent to the given expression and is estimated to cost less\nto execute.\n\u2022The strategy that the database system chooses for evaluating an operation depends\non the size of each relation and on the distribution of values within columns. So\nthat they can base the strategy choice on reliable information, database systems\nmay store statistics for each relation r. These statistics include:\n\u00b0The number of tuples in the relation r.\n\u00b0The size of a record (tuple) of relation rin bytes.\n\u00b0The number of distinct values that appear in the relation rfor a particular\nattribute.\n\u2022Most database systems use histograms to store the number of values for an at-\ntribute within each of several ranges of values. Histograms are often computed\nusing sampling.\n\u2022These statistics allow us to estimate the sizes of the results of various operations, as\nwell as the cost of executing the operations. Statistical information about relations\nis particularly useful when several indices are available to assist in the processing of\na query. The presence of these structures has a signi\ufb01cant in\ufb02uence on the choice\nof a query-processing strategy.\n", "816": "788 Chapter 16 Query Optimization\n\u2022Alternative evaluation plans for each expression can be generated by equivalence\nrules, and the cheapest plan across all expressions can be chosen. Several opti-\nmization techniques are available to reduce the number of alternative expressions\nand plans that need to be generated.\n\u2022We use heuristics to reduce the number of plans considered, and thereby to reduce\nthe cost of optimization. Heuristic rules for transforming relational-algebra queries\ninclude \u201cPerform selection operations as early as possible,\u201d \u201cPerform projections\nearly,\u201d and \u201cAvoid Cartesian products.\u201d\n\u2022Materialized views can be used to speed up query processing. Incremental view\nmaintenance is needed to e\ufb03ciently update materialized views when the underly-\ning relations are modi\ufb01ed. The di\ufb00erential of an operation can be computed by\nmeans of algebraic expressions involving di\ufb00erentials of the inputs of the opera-\ntion. Other issues related to materialized views include how to optimize queries by\nmaking use of available materialized views, and how to select views to be materi-\nalized.\n\u2022A number of advanced optimization techniques have been proposed, such as top-\nKoptimization, join minimization, optimi zation of updates, multiquery optimiza-\ntion, and parametric query optimization.\nReview Terms\n\u2022Query optimization\n\u2022Transformation of expressions\n\u2022Equivalence of expressions\n\u2022Equivalence rules\n\u00b0Join commutativity\n\u00b0Join associativity\n\u2022Minimal set of equivalence rules\n\u2022Enumeration of equivalent\nexpressions\n\u2022Statistics estimation\n\u2022Catalog information\n\u2022Size estimation\n\u00b0Selection\n\u00b0Selectivity\n\u00b0Join\u2022Histograms\n\u2022Distinct value estimation\n\u2022Random sample\n\u2022Choice of evaluation plans\n\u2022Interaction of evaluation\ntechniques\n\u2022Cost-based optimization\n\u2022Join-order optimization\n\u00b0Dynamic-programming\nalgorithm\n\u00b0Left-deep join order\n\u00b0Interesting sort order\n\u2022Heuristic optimization\n\u2022Plan caching\n\u2022Access-plan selection\n", "817": "Practice Exercises 789\n\u2022Correlated evaluation\n\u2022Decorrelation\n\u2022Semijoin\n\u2022Anti-semijoin\n\u2022Materialized views\n\u2022Materialized view maintenance\n\u00b0Recomputation\n\u00b0Incremental maintenance\n\u00b0Insertion\u00b0Deletion\n\u00b0Updates\n\u2022Query optimization with\nmaterialized views\n\u2022Index selection\n\u2022Materialized view selection\n\u2022Top-Koptimization\n\u2022Join minimization\n\u2022Halloween problem\n\u2022Multiquery optimization\nPractice Exercises\n16.1 Download the university database schema and the large university dataset from\ndbbook.com . Create the university schema on your favorite database, and load\nthe large university dataset. Use the explain feature described in Note 16.1 on\npage 746 to view the plan chosen by the database, in di\ufb00erent cases as detailed\nbelow.\na. Write a query with an equality condition on student .name (which does\nnot have an index), and view the plan chosen.\nb. Create an index on the attribute student .name , and view the plan chosen\nfor the above query.\nc. Create simple queries joining two relations, or three relations, and view\nthe plans chosen.\nd. Create a query that computes an aggregate with grouping, and view the\nplan chosen.\ne. Create an SQL query whose chosen plan uses a semijoin operation.\nf. Create an SQL query that uses a not in clause, with a subquery using\naggregation. Observe what plan is chosen.\ng. Create a query for which the chosen plan uses correlated evaluation (the\nway correlated evaluation is represented varies by database, but most\ndatabases would show a \ufb01lter or a project operator with a subplan or\nsubquery).\nh. Create an SQL update query that updates a single row in a relation. View\nthe plan chosen for the update query.\n", "818": "790 Chapter 16 Query Optimization\ni. Create an SQL update query that updates a large number of rows in a re-\nlation, using a subquery to compute the new value. View the plan chosen\nfor the update query.\n16.2 Show that the following equivalences hold. Explain how you can apply them\nto improve the e\ufb03ciency of certain queries:\na.E1\u22c8\u03b8(E2\u2212E3)\u2261(E1\u22c8\u03b8E2\u2212E1\u22c8\u03b8E3).\nb.\u03c3\u03b8(A\u03b3F(E))\u2261A\u03b3F(\u03c3\u03b8(E)), where \u03b8uses only attributes from A.\nc.\u03c3\u03b8(E1\u27d5E2)\u2261\u03c3\u03b8(E1)\u27d5E2,w h e r e\u03b8uses only attributes from E1.\n16.3 For each of the following pairs of expressions, give instances of relations that\nshow the expressions are not equivalent.\na.\u03a0A(r\u2212s)a n d\u03a0A(r)\u2212\u03a0A(s).\nb.\u03c3B<4(A\u03b3max(B)asB(r)) andA\u03b3max(B)asB(\u03c3B<4(r)).\nc. In the preceding expressions, if both occurrences of max were replaced\nbymin, would the expressions be equivalent?\nd. ( r\u27d6s)\u27d6tandr\u27d6(s\u27d6t)\nIn other words, the natural right outer join is not associative.\ne.\u03c3\u03b8(E1\u27d5E2)a n d E1\u27d5\u03c3\u03b8(E2), where\u03b8uses only attributes from E2.\n16.4 SQL allows relations with duplicates (Chapter 3), and the multiset version of\nthe relational algebra is de\ufb01ned in Note 3.1 on page 80, Note 3.2 on page 97,\nand Note 3.3 on page 108. Check which of the equivalence rules 1 through 7.b\nhold for the multiset version of the relational algebra.\n16.5 Consider the relations r1(A,B,C),r2(C,D,E), and r3(E,F), with primary keys\nA,C,a n d E, respectively. Assume that r1has 1000 tuples, r2has 1500 tuples,\nandr3has 750 tuples. Estimate the size of r1\u22c8r2\u22c8r3, and give an e\ufb03cient\nstrategy for computing the join.\n16.6 Consider the relations r1(A,B,C),r2(C,D,E), and r3(E,F)o fP r a c t i c eE x e r -\ncise 16.5. Assume that there are no primary keys, except the entire schema.\nLetV(C,r1) be 900, V(C,r2) be 1100, V(E,r2) be 50, and V(E,r3) be 100.\nAssume that r1has 1000 tuples, r2has 1500 tuples, and r3has 750 tuples. Es-\ntimate the size of r1\u22c8r2\u22c8r3and give an e\ufb03cient strategy for computing\nthe join.\n16.7 Suppose that a B+-tree index on building is available on relation department\nand that no other index is available. What would be the best way to handle the\nfollowing selections that involve negation?\na.\u03c3\u00ac(building<\u201cWatson\u201d) (department )\n", "819": "Practice Exercises 791\nb.\u03c3\u00ac(building=\u201cWatson\u201d) (department )\nc.\u03c3\u00ac(building<\u201cWatson\u201d \u2228budget<50000) (department )\n16.8 Consider the query:\nselect *\nfrom r,s\nwhere upper( r.A) = upper( s.A);\nwhere \u201cupper\u201d is a function that returns its input argument with all lowercase\nletters replaced by the corresponding uppercase letters.\na. Find out what plan is generated for this query on the database system\nyou use.\nb. Some database systems would use a (block) nested-loop join for this\nquery, which can be very ine\ufb03cient. Brie\ufb02y explain how hash-join or\nmerge-join can be used for this query.\n16.9 Give conditions under which the following expressions are equivalent:\nA,B\u03b3agg(C)(E1\u22c8E2)a n d(A\u03b3agg(C)(E1))\u22c8E2\nwhere aggdenotes any aggregation operation. How can the above conditions\nbe relaxed if aggis one of minormax?\n16.10 Consider the issue of interesting orders in optimization. Suppose you are given\na query that computes the natural join of a set of relations S. Given a subset\nS1o fS, what are the interesting orders of S1?\n16.11 Modify the FindBestPlan( S) function to create a function FindBestPlan( S,O),\nwhere Ois a desired sort order for S, and which considers interesting sort\norders. A nullorder indicates that the order is not relevant. Hints : An algorithm\nAmay give the desired order O; if not a sort operation may need to be added\nto get the desired order. If Ais a merge-join, FindBestPlan must be invoked on\nthe two inputs with the desired orders for the inputs.\n16.12 Show that, with nrelations, there are (2( n\u22121))!\u2215(n\u22121)!di\ufb00erent join orders.\nHint: Acomplete binary tree is one where every internal node has exactly two\nchildren. Use the fact that the number of di\ufb00erent complete binary trees with\nnleaf nodes is:\n1\nn(\n2(n\u22121)\n(n\u22121))\nIf you wish, you can derive the formula for the number of complete binary trees\nwith nnodes from the formula for the number of binary trees with nnodes.\nThe number of binary trees with nnodes is:\n1\nn+1(\n2n\nn)\n", "820": "792 Chapter 16 Query Optimization\nThis number is known as the Catalan number , and its derivation can be found\nin any standard textbook on data structures or algorithms.\n16.13 Show that the lowest-cost join order can be computed in time O(3n). Assume\nthat you can store and look up information about a set of relations (such as\nthe optimal join order for the set, and the cost of that join order) in constant\ntime. (If you \ufb01nd this exercise di\ufb03cult, at least show the looser time bound of\nO(22n).)\n16.14 Show that, if only left-deep join trees are considered, as in the System R opti-\nmizer, the time taken to \ufb01nd the most e\ufb03cient join order is around n2n. Assume\nthat there is only one interesting sort order.\n16.15 Consider the bank database of Figure 16.9, where the primary keys are under-\nlined. Construct the following SQL queries for this relational database.\na. Write a nested query on the relation account to \ufb01nd, for each branch\nwith name starting with B, all accounts with the maximum balance at\nthe branch.\nb. Rewrite the preceding query withou t using a nested subquery; in other\nwords, decorrelate the query, but in SQL.\nc. Give a relational algebra expression using semijoin equivalent to the\nquery.\nd. Give a procedure (similar to that described in Section 16.4.4) for decor-\nrelating such queries.\nExercises\n16.16 Suppose that a B+-tree index on ( dept\nname ,building ) is available on relation\ndepartment . What would be the best way to handle the following selection?\n\u03c3(building<\u201cWatson\u201d) \u2227(budget<55000)\u2227(dept\nname=\u201cMusic\u201d) (department )\nbranch (branch\n name\n ,branch\n city, assets )\ncustomer (customer\n name\n ,customer\n street, customer\n city)\nloan (loan\n number\n ,branch\n name, amount )\nborrower (customer\n name\n ,loan\n number\n )\naccount (account\n number\n ,branch\n name, balance )\ndepositor (customer\n name\n ,account\n number\n )\nFigure 16.9 Banking database.\n", "821": "Exercises 793\n16.17 Show how to derive the following equivalences by a sequence of transforma-\ntions using the equivalence rules in Section 16.2.1.\na.\u03c3\u03b81\u2227\u03b82\u2227\u03b83(E)\u2261\u03c3\u03b81(\u03c3\u03b82(\u03c3\u03b83(E)))\nb.\u03c3\u03b81\u2227\u03b82(E1\u22c8\u03b83E2)\u2261\u03c3\u03b81(E1\u22c8\u03b83(\u03c3\u03b82(E2))), where \u03b82involves only\nattributes from E2\n16.18 Consider the two expressions \u03c3\u03b8(E1\u27d5E2)a n d\u03c3\u03b8(E1\u22c8E2).\na. Show using an example that the two expressions are not equivalent in\ngeneral.\nb. Give a simple condition on the predicate \u03b8, which if satis\ufb01ed will ensure\nthat the two expressions are equivalent.\n16.19 A set of equivalence rules is said to be complete if, whenever two expressions\nare equivalent, one can be derived from the other by a sequence of uses of the\nequivalence rules. Is the set of equivalence rules that we considered in Section\n16.2.1 complete? Hint: Consider the equivalence \u03c33=5(r)\u2261{}.\n16.20 Explain how to use a histogram to estim ate the size of a selection of the form\n\u03c3A\u2264v(r).\n16.21 Suppose two relations randshave histograms on attributes r.Aands.A,r e s p e c -\ntively, but with di\ufb00erent ranges. Suggest how to use the histograms to estimate\nt h es i z eo f r\u22c8s. Hint: Split the ranges of each histogram further.\n16.22 Consider the query\nselect A,B\nfrom r\nwhere r.B<some (select B\nfrom s\nwhere s.A=r.A)\nShow how to decorrelate this query using the multiset version of the semi join\noperation.\n16.23 Describe how to incrementally maintain the results of the following operations\non both insertions and deletions:\na. Union and set di\ufb00erence.\nb. Left outer join.\n16.24 Give an example of an expression de\ufb01ning a materialized view and two situa-\ntions (sets of statistics for the input relations and the di\ufb00erentials) such that\nincremental view maintenance is better than recomputation in one situation,\nand recomputation is better in the other situation.\n", "822": "794 Chapter 16 Query Optimization\n16.25 Suppose you want to get answers to r\u22c8ssorted on an attribute of r,a n d\nwant only the top Kanswers for some relatively small K. Give a good way of\nevaluating the query:\na . W h e nt h ej o i ni so naf o r e i g nk e yo f rreferencing s,w h e r et h ef o r e i g n\nkey attribute is declared to be not null.\nb. When the join is not on a foreign key.\n16.26 Consider a relation r(A,B,C), with an index on attribute A. Give an example\nof a query that can be answered by using the index only, without looking at the\ntuples in the relation. (Query plans that use only the index, without accessing\nthe actual relation, are called index-only plans.)\n16.27 Suppose you have an update query U. Give a simple su\ufb03cient condition on\nUthat will ensure that the Halloween problem cannot occur, regardless of the\nexecution plan chosen or the indices that exist.\nFurther Reading\nThe seminal work of [Selinger et al. (1979)] describes access-path selection in the\nSystem R optimizer, which was one of the earliest relational-query optimizers. Query\nprocessing in Starburst, described in [Haas et al. (1989)], forms the basis for query\noptimization in IBM DB2 .\n[Graefe and McKenna (1993)] describes Volcano, an equivalence-rule\u2013based\nquery optimizer that, along with its successor Cascades ([Graefe (1995)]), forms the\nbasis of query optimization in Microsoft SQL S erver. [Moerkotte (2014)] provides ex-\ntensive textbook coverage of query optimization, including optimizations of the dy-\nnamic programming algorithm for join order optimization to avoid considering Carte-\nsian products. Avoiding generation of plans with Cartesian products can result in sub-\ns t a n t i a lr e d u c t i o ni no p t i m i z a t i o nc o s tf o rc o m m o nq u e r i e s .\nThe bibliographic notes for this chapter, available online, provides references to\nresearch on a variety of optimization techniques, including optimization of queries\nwith aggregates, with outer joins, nested s ubqueries, top-K queries, join minimization,\noptimization of update queries, materialized view maintenance and view matching,\nindex and materialized view selection, parametric query optimization, and multiquery\noptimization.\nBibliography\n[Graefe (1995)] G. Graefe, \u201cThe Cascades Framework for Query Optimization\u201d, Data Engi-\nneering Bulletin , Volume 18, Number 3 (1995), pages 19\u201329.\n", "823": "Further Reading 795\n[Graefe and McKenna (1993)] G. Graefe and W. McKenna, \u201cThe Volcano Optimizer Gen-\nerator\u201d, In Proc. of the International Conf. on Data Engineering (1993), pages 209\u2013218.\n[Haas et al. (1989)] L. M. Haas, J. C. Freytag, G. M. Lohman, and H. Pirahesh, \u201cExtensible\nQuery Processing in Starburst\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data\n(1989), pages 377\u2013388.\n[Moerkotte (2014)] G. Moerkotte, Building Query Compilers , available online at http://pi3.\ninformatik.uni-mannheim.de/\u223cmoer/querycompiler.pdf , retrieved 13 Dec 2018 (2014).\n[Selinger et al. (1979)] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and\nT. G. Price, \u201cAccess Path Selection in a Relational Database System\u201d, In Proc. of the ACM\nSIGMOD Conf. on Management of Data (1979), pages 23\u201334.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "824": "", "825": "PART7\nTRANSACTION\nMANAGEMENT\nThe term transaction refers to a collection of operations that form a single logical unit\nof work. For instance, transfer of money from one account to another is a transaction\nconsisting of two updates, one to each account.\nIt is important that either all actions of a transaction be executed completely, or,\nin case of some failure, partial e\ufb00ects of each incomplete transaction be undone. This\nproperty is called atomicity . Further, once a transaction is successfully executed, its\ne\ufb00ects must persist in the database\u2014a system failure should not result in the database\nforgetting about a transaction that successfully completed. This property is called dura-\nbility .\nIn a database system where multiple transactions are executing concurrently, if\nupdates to shared data are not controlled, there is potential for transactions to see in-\nconsistent intermediate states created by updates of other transactions. Such a situation\ncan result in erroneous updates to data stored in the database. Thus, database systems\nmust provide mechanisms to isolate transactions from the e\ufb00ects of other concurrently\nexecuting transactions. This property is called isolation .\nChapter 17 describes the concept of a transaction in detail, including the proper-\nties of atomicity, durability, isolation, and other properties provided by the transaction\nabstraction. In particular, the chapter makes precise the notion of isolation by means\nof a concept called serializability.\nChapter 18 describes several concurrency-control techniques that help implement\nthe isolation property. Chapter 19 describes the recovery management component of\na database, which implements the atomicity and durability properties.\nTaken as a whole, the transaction-management component of a database system al-\nlows application developers to focus on the implementation of individual transactions,\nignoring the issues of concurrency and fault tolerance.\n797\n", "826": "", "827": "CHAPTER17\nTransactions\nOften, a collection of several operations on the database appears to be a single unit from\nthe point of view of the database user. For example, a transfer of funds from a checking\naccount to a savings account is a single operation from the customer\u2019s standpoint;\nwithin the database system, however, it consists of several operations. It is essential\nthat all these operations occur, or that, in case of a failure, none occur. It would be\nunacceptable if the checking account were debited but the savings account not credited.\nCollections of operations that form a single logical unit of work are called trans-\nactions. A database system must ensure proper execution of transactions despite fail-\nures\u2014either the entire transaction executes, or none of it does. Furthermore, it must\nmanage concurrent execution of transactions in a way that avoids the introduction of\ninconsistency. In our funds-transfer example, a transaction computing the customer\u2019s\ntotal balance might see the checking-account balance before it is debited by the funds-\ntransfer transaction, but see the savings balance after it is credited. As a result, it would\nobtain an incorrect result.\nThis chapter introduces the basic concepts of transaction processing. Details on\nconcurrent transaction processing and recovery from failures are in Chapter 18 and\nChapter 19, respectively.\n17.1 Transaction Concept\nAtransaction is aunitof program execution that accesses and possibly updates various\ndata items. Usually, a transaction is initiated by a user program written in a high-level\ndata-manipulation language (typically SQL), or programming language (e.g., C++ or\nJava), with embedded database accesses in JDBC orODBC . A transaction is delimited\nby statements (or function calls) of the form begin transaction andend transaction .T h e\ntransaction consists of all operations executed between the begin transaction and end\ntransaction .\nThis collection of steps must appear to the user as a single, indivisible unit. Since\na transaction is indivisible, it either executes in its entirety or not at all. Thus, if a\n799\n", "828": "800 Chapter 17 Transactions\ntransaction begins to execute but fails for whatever reason, any changes to the database\nthat the transaction may have made must be undone. This requirement holds regardless\nof whether the transaction itself failed (e.g., if it divided by zero), the operating system\ncrashed, or the computer itself stopped operating. As we shall see, ensuring that this\nrequirement is met is di\ufb03cult since some changes to the database may still be stored\nonly in the main-memory variables of the transaction, while others may have been\nwritten to the database and stored on disk. This \u201call-or-none\u201d property is referred to as\natomicity .\nFurthermore, since a transaction is a single unit, its actions cannot appear to be\nseparated by other database operations not part of the transaction. While we wish to\npresent this user-level impression of transactions, we know that reality is quite di\ufb00er-\nent. Even a single SQL statement involves many separate accesses to the database, and\na transaction may consist of several SQL statements. Therefore, the database system\nmust take special actions to ensure that transactions operate properly without interfer-\nence from concurrently executing database statements. This property is referred to as\nisolation .\nEven if the system ensures correct execution of a transaction, this serves little pur-\npose if the system subsequently crashes and, as a result, the system \u201cforgets\u201d about the\ntransaction. Thus, a transaction\u2019s actions must persist across crashes. This property is\nreferred to as durability .\nBecause of the above three properties, transactions are an ideal way of structuring\ninteraction with a database. This leads us to impose a requirement on transactions\nthemselves. A transaction must preserve database consistency\u2014if a transaction is run\natomically in isolation starting from a consistent database, the database must again\nbe consistent at the end of the transaction. This consistency requirement goes beyond\nthe data-integrity constraints we have seen earlier (such as primary-key constraints,\nreferential integrity, check constraints, and the like). Rather, transactions are expected\nto go beyond that to ensure preservation of those application-dependent consistency\nconstraints that are too complex to state using the SQL constructs for data integrity.\nHow this is done is the responsibility of the programmer who codes a transaction. This\nproperty is referred to as consistency .\nTo restate the above more concisely, we require that the database system maintain\nthe following properties of the transactions:\n\u2022Atomicity . Either all operations of the transaction are re\ufb02ected properly in the\ndatabase, or none are.\n\u2022Consistency . Execution of a transaction in isolation (i.e., with no other transaction\nexecuting concurrently) preserves the consistency of the database.\n\u2022Isolation . Even though multiple transactions may execute concurrently, the system\nguarantees that, for every pair of transactions Tiand Tj, it appears to Tithat either\nTj\ufb01nished execution before Tistarted or Tjstarted execution after Ti\ufb01nished.\n", "829": "17.2 A Simple Transaction Model 801\nThus, each transaction is unaware of other transactions executing concurrently in\nthe system.\n\u2022Durability . After a transaction completes successfully, the changes it has made to\nthe database persist, even if there are system failures.\nThese properties are often called the ACID properties ; the acronym is derived from the\n\ufb01rst letter of each of the four properties.\nAs we shall see later, ensuring the isolation property may have a signi\ufb01cant ad-\nverse e\ufb00ect on system performance. For this reason, some applications compromise\non the isolation property. We shall study these compromises after \ufb01rst studying the\nstrict enforcement of the ACID properties.\n17.2 A Simple Transaction Model\nBecause SQL is a powerful and complex language, we begin our study of transactions\nwith a simple database language that focuses on when data are moved from disk to\nmain memory and from main memory to disk. In doing this, we ignore SQL insert\nand delete operations and defer considering them until Section 18.4. The only actual\noperations on the data are restricted in our simple language to arithmetic operations.\nLater we shall discuss transactions in a realistic, SQL-based context with a richer set\nof operations. The data items in our simpli\ufb01ed model contain a single data value (a\nnumber in our examples). Each data item is identi\ufb01ed by a name (typically a single\nletter in our examples, that is, A,B,C,e t c . ) .\nWe shall illustrate the transaction concept using a simple bank application consist-\ning of several accounts and a set of transactions that access and update those accounts.\nTransactions access data using two operations:\n\u2022read (X), which transfers the data item Xfrom the database to a variable, also\ncalled X, in a bu\ufb00er in main memory belonging to the transaction that executed\ntheread operation.\n\u2022write (X), which transfers the value in the variable Xin the main-memory bu\ufb00er\nof the transaction that executed the write to the data item Xin the database.\nIt is important to know if a change to a data item appears only in main memory\nor if it has been written to the database on disk. In a real database system, the write\noperation does not necessarily result in the immediate update of the data on the disk;\nthewrite operation may be temporarily stored elsewhere and executed on the disk\nlater. For now, however, we shall assume that the write operation updates the database\nimmediately. We discuss storage issues furt her in Section 17.3 and discuss the issue of\nwhen database data in main memory are written to the database on disk in Chapter\n19.\n", "830": "802 Chapter 17 Transactions\nLet Tibe a transaction that transfers $50 from account Ato account B. This trans-\naction can be de\ufb01ned as:\nTi:read (A);\nA:=A\u221250;\nwrite (A);\nread (B);\nB:=B+ 50;\nwrite (B).\nLet us now consider each of the ACID properties. (For ease of presentation, we consider\nthem in an order di\ufb00erent from the order A-C-I-D .)\n\u2022Consistency : The consistency requirement here is that the sum of Aand Bbe un-\nchanged by the execution of the transaction. Without the consistency requirement,\nmoney could be created or destroyed by the transaction! It can be veri\ufb01ed eas-\nily that, if the database is consistent before an execution of the transaction, the\ndatabase remains consistent after the execution of the transaction.\nEnsuring consistency for an individual transaction is the responsibility of the\napplication programmer who codes the transaction. This task may be facilitated\nby automatic testing of integrity constraints, as we discussed in Section 4.4.\n\u2022Atomicity : Suppose that, just before the execution of transaction Ti,t h ev a l u e so f\naccounts Aand Bare $1000 and $2000, respectively. Now suppose that, during the\nexecution of transaction Ti, a failure occurs that prevents Tifrom completing its ex-\necution successfully. Further, suppose that the failure happened after the write (A)\noperation but before the write (B) operation. In this case, the values of accounts\nAand Bre\ufb02ected in the database are $950 and $2000. The system destroyed $50\nas a result of this failure. In particular, we note that the sum A+Bis no longer\npreserved.\nThus, because of the failure, the state of the system no longer re\ufb02ects a real\nstate of the world that the database is supposed to capture. We term such a state\naninconsistent state . We must ensure that such inconsistencies are not visible in\na database system. Note, however, that the system must at some point be in an\ninconsistent state. Even if transaction Tiis executed to completion, there exists\na point at which the value of account Ais $950 and the value of account Bis\n$2000, which is clearly an inconsistent state. This state, however, is eventually\nreplaced by the consistent state where the value of account Ais $950, and the value\nof account Bis $2050. Thus, if the transaction never started or was guaranteed\nto complete, such an inconsistent state would not be visible except during the\nexecution of the transaction. That is the reason for the atomicity requirement: If\nthe atomicity property is present, all actions of the transaction are re\ufb02ected in the\ndatabase, or none are.\n", "831": "17.2 A Simple Transaction Model 803\nThe basic idea behind ensuring atomicity is this: The database system keeps\ntrack (on disk) of the old values of any data on which a transaction performs a\nwrite. This information is written to a \ufb01le called the log.I ft h et r a n s a c t i o nd o e s\nnot complete its execution, the database system restores the old values from the log\nto make it appear as though the transaction never executed. We discuss these ideas\nfurther in Section 17.4. Ensuring atomicity is the responsibility of the database sys-\ntem; speci\ufb01cally, it is handled by a component of the database called the recovery\nsystem , which we describe in detail in Chapter 19.\n\u2022Durability : Once the execution of the transaction completes successfully, and the\nuser who initiated the transaction has been noti\ufb01ed that the transfer of funds has\ntaken place, it must be the case that no system failure can result in a loss of data\ncorresponding to this transfer of funds. The durability property guarantees that,\nonce a transaction completes successfully, all the updates that it carried out on the\ndatabase persist, even if there is a system failure after the transaction completes\nexecution.\nWe assume for now that a failure of the computer system may result in loss of\ndata in main memory, but data written to disk are never lost. Protection against\nloss of data on disk is discussed in Chapter 19. We can guarantee durability by\nensuring that either:\n1.The updates carried out by the transaction have been written to disk before\nthe transaction completes.\n2.Information about the updates carried out by the transaction is written to\ndisk, and such information is su\ufb03cient to enable the database to reconstruct\nthe updates when the database system is restarted after the failure.\nThe recovery system of the database, described in Chapter 19, is responsible for\nensuring durability, in addition to ensuring atomicity.\n\u2022Isolation : Even if the consistency and atomicity properties are ensured for each\ntransaction, if several transactions are executed concurrently, their operations may\ninterleave in some undesirable way, resulting in an inconsistent state.\nFor example, as we saw earlier, the database is temporarily inconsistent while\nthe transaction to transfer funds from AtoBis executing, with the deducted total\nwritten to Aand the increased total yet to be written to B. If a second concurrently\nrunning transaction reads Aand Bat this intermediate point and computes A+B,\nit will observe an inconsistent value. Furthermore, if this second transaction then\nperforms updates on Aand Bbased on the inconsistent values that it read, the\ndatabase may be left in an inconsistent state even after both transactions have\ncompleted.\nA way to avoid the problem of concurrently executing transactions is to execute\ntransactions serially\u2014that is, one after the other. However, concurrent execution\nof transactions provides signi\ufb01cant performance bene\ufb01ts, as we shall see in Section\n", "832": "804 Chapter 17 Transactions\n17.5. Other solutions have therefore been developed; they allow multiple transac-\ntions to execute concurrently.\nWe discuss the problems caused by concurrently executing transactions in Sec-\ntion 17.5. The isolation property of a transaction ensures that the concurrent ex-\necution of transactions results in a system state that is equivalent to a state that\ncould have been obtained had these transactions executed one at a time in some\norder. We shall discuss the principles of isolation further in Section 17.6. Ensuring\nthe isolation property is the responsibility of a component of the database system\ncalled the concurrency-control system , which we discuss in Chapter 18.\n17.3 Storage Structure\nTo understand how to ensure the atomicity and durability properties of a transaction,\nwe must gain a better understanding of how the various data items in the database may\nbe stored and accessed.\nIn Chapter 12, we saw that storage media can be distinguished by their relative\nspeed, capacity, and resilience to failure, and classi\ufb01ed as volatile storage or non-volatile\nstorage. We review these terms and introduce another class of storage, called stable\nstorage.\n\u2022Volatile storage . Information residing in volatile storage does not usually survive\nsystem crashes. Examples of such storage are main memory and cache memory.\nAccess to volatile storage is extremely fast, both because of the speed of the mem-\nory access itself and because it is possible to access any data item in volatile storage\ndirectly.\n\u2022Non-volatile storage . Information residing in non-volatile storage survives system\ncrashes. Examples of non-volatile storage include secondary storage devices such\nas magnetic disk and \ufb02ash storage, used for online storage, and tertiary storage\ndevices such as optical media and magnetic tapes, used for archival storage. At the\ncurrent state of technology, non-volatile storage is slower than volatile storage, par-\nticularly for random access. Both secondary and tertiary storage devices, however,\nare susceptible to failures that may result in loss of information.\n\u2022Stable storage . Information residing in stable storage is never lost ( never should\nbe taken with a grain of salt, since theoretically never cannot be guaranteed\u2014for\nexample, it is possible, although extremely unlikely, that a black hole may envelop\nthe earth and permanently destroy all data!). Although stable storage is theoreti-\ncally impossible to obtain, it can be closely approximated by techniques that make\ndata loss extremely unlikely. To implement stable storage, we replicate the informa-\ntion in several non-volatile storage media (usually disk) with independent failure\nmodes. Updates must be done with care to ensure that a failure during an update\nto stable storage does not cause a loss of information. Section 19.2.1 discusses\nstable-storage implementation.\n", "833": "17.4 Transaction Atomicity and Durability 805\nThe distinctions among the various storage types can be less clear in practice than in\nour presentation. For example, certain systems, for example some RAID controllers,\nprovide battery backup, so that some main memory can survive system crashes and\npower failures.\nFor a transaction to be durable, its changes need to be written to stable storage.\nSimilarly, for a transaction to be atomic, log records need to be written to stable storage\nbefore any changes are made to the database on disk. The degree to which a system\nensures durability and atomicity depends on how stable its implementation of stable\nstorage really is. In some cases, a single copy on disk is considered su\ufb03cient, but ap-\nplications whose data are highly valuable and whose transactions are highly important\nrequire multiple copies, or, in other words, a closer approximation of the idealized\nconcept of stable storage.\n17.4 Transaction Atomicity and Durability\nAs we noted earlier, a transaction may not always complete its execution successfully.\nSuch a transaction is termed aborted .I fw ea r et oe n s u r et h ea t o m i c i t yp r o p e r t y ,a n\naborted transaction must have no e\ufb00ect on the state of the database. Thus, any changes\nthat the aborted transaction made to the database must be undone. Once the changes\ncaused by an aborted transaction have been undone, we say that the transaction has\nbeen rolled back . It is part of the responsibility of the recovery scheme to manage trans-\naction aborts. This is done typically by maintaining a log. Each database modi\ufb01cation\nmade by a transaction is \ufb01rst recorded in the log. We record the identi\ufb01er of the trans-\naction performing the modi\ufb01cation, the identi\ufb01er of the data item being modi\ufb01ed, and\nboth the old value (prior to modi\ufb01cation) and the new value (after modi\ufb01cation) of\nthe data item. Only then is the database itself modi\ufb01ed. Maintaining a log provides\nthe possibility of redoing a modi\ufb01cation to ensure atomicity and durability as well as\nthe possibility of undoing a modi\ufb01cation to ensure atomicity in case of a failure during\ntransaction execution. Details of log-based recovery are discussed in Chapter 19.\nA transaction that completes its execution successfully is said to be committed .A\ncommitted transaction that has performed updates transforms the database into a new\nconsistent state, which must persist even if there is a system failure.\nOnce a transaction has committed, we cannot undo its e\ufb00ects by aborting it. The\nonly way to undo the e\ufb00ects of a committed transaction is to execute a compensating\ntransaction . For instance, if a transaction added $20 to an account, the compensating\ntransaction would subtract $20 from the account. However, it is not always possible to\ncreate such a compensating transaction. Therefore, the responsibility of writing and ex-\necuting a compensating transaction is left to the user and is not handled by the database\nsystem.\nWe need to be more precise about what we mean by successful completion of a\ntransaction. We therefore establish a simple abstract transaction model. A transaction\nmust be in one of the following states:\n", "834": "806 Chapter 17 Transactions\n\u2022Active , the initial state; the transaction stays in this state while it is executing.\n\u2022Partially committed , after the \ufb01nal statement has been executed.\n\u2022Failed , after the discovery that normal execution can no longer proceed.\n\u2022Aborted , after the transaction has been rolled back and the database has been\nrestored to its state prior to the start of the transaction.\n\u2022Committed , after successful completion.\nThe state diagram corresponding to a transaction appears in Figure 17.1. We say\nthat a transaction has committed only if it has entered the committed state. Similarly,\nwe say that a transaction has aborted only if it has entered the aborted state. A trans-\naction is said to have terminated if it has either committed or aborted.\nA transaction starts in the active state. When it \ufb01nishes its \ufb01nal statement, it enters\nthe partially committed state. At this point, the transaction has completed its execution,\nbut it is still possible that it may have to be aborted, since the actual output may still\nbe temporarily residing in main memory, and thus a hardware failure may preclude its\nsuccessful completion.\nThe database system then writes out enough information to disk that, even in the\nevent of a failure, the updates performed by the transaction can be re-created when the\nsystem restarts after the failure. When the last of this information is written out, the\ntransaction enters the committed state.\nAs mentioned earlier, we assume for now that failures do not result in loss of data\non disk. Chapter 19 discusses techniques to deal with loss of data on disk.\nA transaction enters the failed state after the system determines that the transac-\ntion can no longer proceed with its normal execution (e.g., because of hardware or\nlogical errors). Such a transaction must be rolled back. Then, it enters the aborted\nstate. At this point, the system has two options:\nactive\nfailedpartially\ncommittedcommitted\naborted\nFigure 17.1 State diagram of a transaction.\n", "835": "17.5 Transaction Isolation 807\n\u2022It can restart the transaction, but only if the transaction was aborted as a result of\nsome hardware or software error that was not created through the internal logic\nof the transaction. A restarted transaction is considered to be a new transaction.\n\u2022It can killthe transaction. It usually does so because of some internal logical error\nthat can be corrected only by rewriting the application program, or because the\ninput was bad, or because the desired data were not found in the database.\nWe must be cautious when dealing with observable external writes ,s u c ha sw r i t e s\nto a user\u2019s screen, or sending email. Once such a write has occurred, it cannot be\nerased, since it may have been seen external to the database system. Most systems\nallow such writes to take place only after the transaction has entered the committed\nstate. One way to implement such a scheme is for the database system to store any value\nassociated with such external writes temporarily in a special relation in the database,\nand to perform the actual writes only after the transaction enters the committed state. If\nthe system should fail after the transaction has entered the committed state, but before\nit could complete the external writes, the database system will carry out the external\nwrites (using the data in non-volatile storage) when the system is restarted.\nHandling external writes can be more complicated in some situations. For example,\nsuppose the external action is that of dispensing cash at an automated teller machine,\nand the system fails just before the cash is actually dispensed (we assume that cash\ncan be dispensed atomically). It makes no sense to dispense cash when the system is\nrestarted, since the user may have left the machine. In such a case a compensating trans-\naction, such as depositing the cash back into the user\u2019s account, needs to be executed\nwhen the system is restarted.\nAs another example, consider a user making a booking over the web. It is possi-\nble that the database system or the application server crashes just after the booking\ntransaction commits. It is also possible that the network connection to the user is lost\njust after the booking transaction commits. In either case, even though the transaction\nhas committed, the external write has not taken place. To handle such situations, the\napplication must be designed such that when the user connects to the web application\nagain, she will be able to see whether her transaction had succeeded or not.\nFor certain applications, it may be desirable to allow active transactions to display\ndata to users, particularly for long-duration transactions that run for minutes or hours.\nUnfortunately, we cannot allow such output of observable data unless we are willing to\ncompromise transaction atomicity.\n17.5 Transaction Isolation\nTransaction-processing systems usually allo w multiple transactions to run concurrently.\nAllowing multiple transactions to update data concurrently causes several complica-\ntions with consistency of the data, as we saw earlier. Ensuring consistency in spite of\n", "836": "808 Chapter 17 Transactions\nNote 17.1 TRENDS IN CONCURRENCY\nSeveral current trends in the \ufb01eld of computing are giving rise to an increase in the\namount of concurrency possible. As database systems exploit this concurrency to\nincrease overall system performance, there will necessarily be an increasing num-\nber of transactions run concurrently.\nEarly computers had only one processor. Therefore, there was never any real\nconcurrency in the computer. The only concurrency was apparent concurrency\ncreated by the operating system as it shared the processor among several distinct\ntasks or processes. Modern computers are likely to have many processors. Each\nprocessor is referred to as a core; a single processor chip may contain several cores,\nand several such chips may be connected together in a single system, which all\nshare a common system memory. Further, parallel database systems may contain\nmultiple such systems. Parallel database architectures are discussed in Chapter 20.\nThe parallelism provided by multiple processors and cores is used for two pur-\nposes. One is to execute di\ufb00erent parts of a single long running query in parallel,\nto speed up query execution. The other is to allow a large number of queries (often\nmuch smaller queries) to execute concurrently, for example to support a very large\nnumber of concurrent users. Chapter 21 through Chapter 23 describe algorithms\nfor building parallel database systems.\nconcurrent execution of transactions requires extra work; it is far easier to insist that\ntransactions run serially \u2014that is, one at a time, each starting only after the previous\none has completed. However, there are two good reasons for allowing concurrency:\n\u2022Improved throughput and resource utilization . A transaction consists of many steps.\nSome involve I/Oactivity; others involve CPU activity. The CPU and the disks in\na computer system can operate in parallel. Therefore, I/Oactivity can be done\nin parallel with processing at the CPU. The parallelism of the CPU and the I/O\nsystem can therefore be exploited to run multiple transactions in parallel. While\na read or write on behalf of one transaction is in progress on one disk, another\ntransaction can be running in the CPU, while another disk may be executing a\nread or write on behalf of a third transaction. All of this increases the throughput\nof the system\u2014that is, the number of transactions executed in a given amount of\ntime. Correspondingly, the processor and disk utilization also increase; in other\nwords, the processor and disk spend less time idle, or not performing any useful\nwork.\n\u2022Reduced waiting time . There may be a mix of transactions running on a system,\nsome short and some long. If transactions run serially, a short transaction may\nhave to wait for a preceding long transaction to complete, which can lead to un-\n", "837": "17.5 Transaction Isolation 809\npredictable delays in running a transaction. If the transactions are operating on\ndi\ufb00erent parts of the database, it is better to let them run concurrently, sharing\ntheCPU cycles and disk accesses among them. Concurrent execution reduces the\nunpredictable delays in running transactions. Moreover, it also reduces the average\nresponse time : the average time for a transaction to be completed after it has been\nsubmitted.\nThe motivation for using concurrent execution in a database is essentially the same\nas the motivation for using multiprogramming in an operating system.\nWhen several transactions run concurrently, the isolation property may be vio-\nlated, resulting in database consistency being destroyed despite the correctness of each\nindividual transaction. In this section, we present the concept of schedules to help\nidentify those executions that are guaranteed to ensure the isolation property and thus\ndatabase consistency.\nThe database system must control the interaction among the concurrent trans-\nactions to prevent them from destroying the consistency of the database. It does\nso through a variety of mechanisms called concurrency-control schemes .W es t u d y\nconcurrency-control schemes in Chapter 18; for now, we focus on the concept of cor-\nrect concurrent execution.\nConsider again the simpli\ufb01ed banking system of Section 17.1, which has several\naccounts, and a set of transactions that access and update those accounts. Let T1and\nT2be two transactions that transfer funds from one account to another. Transaction T1\ntransfers $50 from account Ato account B. It is de\ufb01ned as:\nT1:read (A);\nA:=A\u221250;\nwrite (A);\nread (B);\nB:=B+ 50;\nwrite (B).\nTransaction T2transfers 10 percent of the balance from account Ato account B.I ti s\nde\ufb01ned as:\nT2:read (A);\ntemp :=A*0 . 1 ;\nA:=A\u2212temp;\nwrite (A);\nread (B);\nB:=B+temp;\nwrite (B).\n", "838": "810 Chapter 17 Transactions\nT1\n T2\nread (A)\nA:=A\u221250\nwrite (A)\nread (B)\nB:=B+50\nwrite (B)\ncommit\nread (A)\ntemp :=A\u22170.1\nA:=A\u2212temp\nwrite (A)\nread (B)\nB:=B+temp\nwrite (B)\ncommit\nFigure 17.2 Schedule 1\u2014a serial schedule in which T1is followed by T2.\nSuppose the current values of accounts Aand Bare $1000 and $2000, respectively.\nSuppose also that the two transactions are executed one at a time in the order T1fol-\nlowed by T2. This execution sequence appears in Figure 17.2. In the \ufb01gure, the sequence\nof instruction steps is in chronological order from top to bottom, with instructions of\nT1appearing in the left column and instructions of T2appearing in the right column.\nThe \ufb01nal values of accounts Aand B, after the execution in Figure 17.2 takes place, are\n$855 and $2145, respectively. Thus, the total amount of money in accounts Aand B\u2014\nthat is, the sum A+B\u2014is preserved after the execution of both transactions.\nSimilarly, if the transactions are executed one at a time in the order T2followed\nbyT1, then the corresponding execution sequence is that of Figure 17.3. Again, as\nexpected, the sum A+Bis preserved, and the \ufb01nal values of accounts Aand Bare $850\nand $2150, respectively.\nThe execution sequences just described are called schedules . They represent the\nchronological order in which instructions are executed in the system. Clearly, a sched-\nule for a set of transactions must consist of all instructions of those transactions and\nthey must preserve the order in which the instructions appear in each individual trans-\naction. For example, in transaction T1, the instruction write (A) must appear before\nthe instruction read (B), in any valid schedule. Note that we include in our schedules\nthecommit operation to indicate that the transaction has entered the committed state.\nIn the following discussion, we shall refer to the \ufb01rst execution sequence ( T1followed\nbyT2) as schedule 1, and to the second execution sequence ( T2followed by T1)a s\nschedule 2.\n", "839": "17.5 Transaction Isolation 811\nT1\n T2\nread (A)\ntemp :=A\u22170.1\nA:=A\u2212temp\nwrite (A)\nread (B)\nB:=B+temp\nwrite (B)\ncommit\nread (A)\nA:=A\u221250\nwrite (A)\nread (B)\nB:=B+50\nwrite (B)\ncommit\nFigure 17.3 Schedule 2\u2014a serial schedule in which T2is followed by T1.\nThese schedules are serial : Each serial schedule consists of a sequence of instruc-\ntions from various transactions, where the instructions belonging to one single trans-\naction appear together in that schedule. Recalling a well-known formula from combi-\nnatorics, we note that, for a set of ntransactions, there exist nfactorial ( n!) di\ufb00erent\nvalid serial schedules.\nWhen the database system executes several transactions concurrently, the corre-\nsponding schedule no longer needs to be serial. If two transactions are running concur-\nrently, the operating system may execute one transaction for a little while, then perform\na context switch, execute the second transaction for some time, and then switch back\nto the \ufb01rst transaction for some time, and so on. With multiple transactions, the CPU\ntime is shared among all the transactions.\nSeveral execution sequences are possible, since the various instructions from both\ntransactions may now be interleaved. In general, it is not possible to predict exactly\nhow many instructions of a transaction will be executed before the CPU switches to\nanother transaction.1\nReturning to our previous example, suppose that the two transactions are executed\nconcurrently. One possible schedule appears i n Figure 17.4. After this execution takes\nplace, we arrive at the same state as the one in which the transactions are executed\nserially in the order T1followed by T2.T h es u m A+Bis indeed preserved.\n1The number of possible schedules for a set of ntransactions is very large. There are n! di\ufb00erent serial schedules.\nConsidering all the possible ways that steps of transactions might be interleaved, the total number of possible schedules\nis much larger than n!.\n", "840": "812 Chapter 17 Transactions\nT1\n T2\nread (A)\nA:=A\u221250\nwrite (A)\nread (A)\ntemp :=A\u22170.1\nA:=A\u2212temp\nwrite (A)\nread (B)\nB:=B+50\nwrite (B)\ncommit\nread (B)\nB:=B+temp\nwrite (B)\ncommit\nFigure 17.4 Schedule 3\u2014a concurrent schedule equivalent to schedule 1.\nNot all concurrent executions result in a correct state. To illustrate, consider the\nschedule of Figure 17.5. After the execution of this schedule, we arrive at a state where\nthe \ufb01nal values of accounts Aand Bare $950 and $2100, respectively. This \ufb01nal state is\naninconsistent state , since we have gained $50 in the process of the concurrent execu-\ntion. Indeed, the sum A+Bis not preserved by the execution of the two transactions.\nIf control of concurrent execution is left entirely to the operating system, many\npossible schedules, including ones that leave the database in an inconsistent state, such\nas the one just described, are possible. It is the job of the database system to ensure\nthat any schedule that is executed will leave the database in a consistent state. The\nconcurrency-control component of the database system carries out this task.\nWe can ensure consistency of the database under concurrent execution by making\nsure that any schedule that is executed has the same e\ufb00ect as a schedule that could\nhave occurred without any concurrent execution. That is, the schedule should, in some\nsense, be equivalent to a serial schedule. Such schedules are called serializable sched-\nules.\n17.6 Serializability\nBefore we can consider how the concurrency-control component of the database sys-\ntem can ensure serializability, we consider how to determine when a schedule is serial-\nizable. Certainly, serial schedules are serializable, but if steps of multiple transactions\nare interleaved, it is harder to determine whether a schedule is serializable. Since trans-\n", "841": "17.6 Serializability 813\nT1\n T2\nread (A)\nA:=A\u221250\nread (A)\ntemp :=A\u22170.1\nA:=A\u2212temp\nwrite (A)\nread (B)\nwrite (A)\nread (B)\nB:=B+50\nwrite (B)\ncommit\nB:=B+temp\nwrite (B)\ncommit\nFigure 17.5 Schedule 4\u2014a concurrent schedule resulting in an inconsistent state.\nactions are programs, it is di\ufb03cult to determine exactly what operations a transaction\nperforms and how operations of various transactions interact. For this reason, we shall\nnot consider the various types of operations that a transaction can perform on a data\nitem, but instead consider only two operations: read andwrite .W ea s s u m et h a t ,b e -\ntween a read (Q) instruction and a write (Q) instruction on a data item Q,at r a n s a c t i o n\nmay perform an arbitrary sequence of operations on the copy of Qthat is residing in\nthe local bu\ufb00er of the transaction. In this model, the only signi\ufb01cant operations of a\ntransaction, from a scheduling point of view, are its read andwrite instructions. Com-\nmit operations, though relevant, are not considered until Section 17.7. We therefore\nmay show only read andwrite instructions in schedules, as we do for schedule 3 in\nFigure 17.6.\nIn this section, we discuss di\ufb00erent forms of schedule equivalence but focus on a\nparticular form called con\ufb02ict serializability .\nLet us consider a schedule Sin which there are two consecutive instructions, I\nand J,o ft r a n s a c t i o n s Tiand Tj, respectively ( i\u2260j). If Iand Jrefer to di\ufb00erent data\nitems, then we can swap Iand Jwithout a\ufb00ecting the results of any instruction in the\nschedule. However, if Iand Jrefer to the same data item Q, then the order of the two\nsteps may matter. Since we are dealing with only read andwrite instructions, there are\nfour cases that we need to consider:\n1.I=read (Q),J=read (Q). The order of Iand Jdoes not matter, since the same\nvalue of Qis read by Tiand Tj, regardless of the order.\n", "842": "814 Chapter 17 Transactions\nT1\n T2\nread (A)\nwrite (A)\nread (A)\nwrite (A)\nread (B)\nwrite (B)\nread (B)\nwrite (B)\nFigure 17.6 Schedule 3\u2014showing only the read and write instructions.\n2. I=read (Q),J=write (Q). If Icomes before J,t h e n Tidoes not read the value\nofQthat is written by Tjin instruction J.I fJcomes before I,t h e n Tireads the\nvalue of Qthat is written by Tj.T h u s ,t h eo r d e ro f Iand Jmatters.\n3. I=write (Q),J=read (Q). The order of Iand Jmatters for reasons similar to\nthose of the previous case.\n4. I=write (Q),J=write (Q). Since both instructions are write operations, the\norder of these instructions does not a\ufb00ect either TiorTj.H o w e v e r ,t h ev a l u e\nobtained by the next read (Q) instruction of Sis a\ufb00ected, since the result of only\nthe latter of the two write instructions is preserved in the database. If there is no\nother write (Q)i n s t r u c t i o na f t e r Iand JinS, then the order of Iand Jdirectly\na\ufb00ects the \ufb01nal value of Qin the database state that results from schedule S.\nThus, only in the case where both Iand Jareread instructions does the relative order\nof their execution not matter.\nWe say that Iand Jcon\ufb02ict if they are operations by di\ufb00erent transactions on the\nsame data item, and at least one of these instructions is a write operation.\nTo illustrate the concept of con\ufb02icting instructions, we consider schedule 3 in Fig-\nure 17.6. The write (A) instruction of T1con\ufb02icts with the read (A) instruction of T2.\nHowever, the write (A) instruction of T2does not con\ufb02ict with the read (B) instruction\nofT1because the two instructions access di\ufb00erent data items.\nLet Iand Jbe consecutive instructions of a schedule S.I fIand Jare instructions\nof di\ufb00erent transactions and Iand Jdo not con\ufb02ict, then we can swap the order of I\nand Jto produce a new schedule S\u2032.Sis equivalent to S\u2032, since all instructions appear\nin the same order in both schedules except for Iand J, whose order does not matter.\nSince the write (A) instruction of T2in schedule 3 of Figure 17.6 does not con-\n\ufb02ict with the read (B) instruction of T1, we can swap these instructions to generate an\nequivalent schedule, schedule 5, in Figure 17.7. Regardless of the initial system state,\nschedules 3 and 5 both produce the same \ufb01nal system state.\n", "843": "17.6 Serializability 815\nT1\n T2\nread (A)\nwrite (A)\nread (A)\nread (B)\nwrite (A)\nwrite (B)\nread (B)\nwrite (B)\nFigure 17.7 Schedule 5\u2014schedule 3 after swapping of a pair of instructions.\nWe continue to swap noncon\ufb02icting instructions:\n\u2022Swap the read (B) instruction of T1with the read (A) instruction of T2.\n\u2022Swap the write (B) instruction of T1with the write (A) instruction of T2.\n\u2022Swap the write (B) instruction of T1with the read (A) instruction of T2.\nThe \ufb01nal result of these swaps, schedule 6 of Figure 17.8, is a serial schedule. Note\nt h a ts c h e d u l e6i se x a c t l yt h es a m ea ss c h e d u l e1 ,b u ti ts h o w so n l yt h e read andwrite\ninstructions. Thus, we have shown that schedule 3 is equivalent to a serial schedule.\nThis equivalence implies that, regardless of the initial system state, schedule 3 produces\nthe same \ufb01nal state as some serial schedule.\nIf a schedule Scan be transformed into a schedule S\u2032by a series of swaps of non-\ncon\ufb02icting instructions, we say that Sand S\u2032arecon\ufb02ict equivalent .2\nT1\n T2\nread (A)\nwrite (A)\nread (B)\nwrite (B)\nread (A)\nwrite (A)\nread (B)\nwrite (B)\nFigure 17.8 Schedule 6\u2014a serial schedule that is equivalent to schedule 3.\n2We use the term con\ufb02ict equivalent to distinguish the way we have just de\ufb01ned equivalence from other de\ufb01nitions that\nwe shall discuss later on in this section.\n", "844": "816 Chapter 17 Transactions\nT3\n T4\nread (Q)\nwrite (Q)\nwrite (Q)\nFigure 17.9 Schedule 7.\nNot all serial schedules are con\ufb02ict equivalent to each other. For example, sched-\nules 1 and 2 are not con\ufb02ict equivalent.\nThe concept of con\ufb02ict equivalence leads to the concept of con\ufb02ict serializability.\nWe say that a schedule Siscon\ufb02ict serializable if it is con\ufb02ict equivalent to a serial\nschedule. Thus, schedule 3 is con\ufb02ict seria lizable, since it is con\ufb02ict equivalent to the\nserial schedule 1.\nFinally, consider schedule 7 of Figure 17.9; it consists of only the signi\ufb01cant op-\nerations (that is, the read andwrite )o ft r a n s a c t i o n s T3and T4.T h i ss c h e d u l ei sn o t\ncon\ufb02ict serializable, since it is not equivalent to either the serial schedule <T3,T4>or\nthe serial schedule <T4,T3>.\nWe now present a simple and e\ufb03cient method for determining the con\ufb02ict serial-\nizability of a schedule. Consider a schedule S. We construct a directed graph, called\naprecedence graph ,f r o m S. This graph consists of a pair G=(V, E), where Vis a set\nof vertices and Eis a set of edges. The set of vertices consists of all the transactions\nparticipating in the schedule. The set of edges consists of all edges Ti\u2192Tjfor which\none of three conditions holds:\n1.Tiexecutes write (Q)b e f o r e Tjexecutes read (Q).\n2. Tiexecutes read (Q)b e f o r e Tjexecutes write (Q).\n3. Tiexecutes write (Q)b e f o r e Tjexecutes write (Q).\nIf an edge Ti\u2192Tjexists in the precedence graph, then, in any serial schedule S\u2032equiv-\nalent to S,Timust appear before Tj.\nFor example, the precedence graph for schedule 1 in Figure 17.10a contains the\nsingle edge T1\u2192T2, since all the instructions of T1a r ee x e c u t e db e f o r et h e\ufb01 r s ti n -\nstruction of T2is executed. Similarly, Figure 17.10b shows the precedence graph for\n(a) (b)T1 T2 T2 T1\nFigure 17.10 Precedence graph for (a) schedule 1 and (b) schedule 2.\n", "845": "17.6 Serializability 817\nT1 T2\nFigure 17.11 Precedence graph for schedule 4.\nschedule 2 with the single edge T2\u2192T1, since all the instructions of T2are executed\nbefore the \ufb01rst instruction of T1is executed.\nThe precedence graph for schedule 4 appears in Figure 17.11. It contains the edge\nT1\u2192T2because T1executes read (A)b e f o r e T2executes write (A). It also contains the\nedge T2\u2192T1because T2executes read (B)b e f o r e T1executes write (B).\nIf the precedence graph for Shas a cycle, then schedule Sis not con\ufb02ict serializ-\nable. If the graph contains no cycles, then the schedule Sis con\ufb02ict serializable.\nAserializability order of the transactions can be obtained by \ufb01nding a linear order\nconsistent with the partial order of the precedence graph. This process is called topo-\nlogical sorting . There are, in general, several possible linear orders that can be obtained\nthrough a topological sort. For example, the graph of Figure 17.12a has the two accept-\nable linear orderings shown in Figure 17.12b and Figure 17.12c.\nThus, to test for con\ufb02ict serializability, we need to construct the precedence graph\nand to invoke a cycle-detection algorithm. Cycle-detection algorithms can be found\nin standard textbooks on algorithms. Cycle-detection algorithms, such as those based\non depth-\ufb01rst search, require on the order of n2operations, where nis the number of\nvertices in the graph (that is, the number of transactions).3\nReturning to our previous examples, note that the precedence graphs for schedules\n1 and 2 (Figure 17.10) indeed do not contain cycles. The precedence graph for sched-\nule 4 (Figure 17.11), on the other hand, contains a cycle, indicating that this schedule\nis not con\ufb02ict serializable.\nI ti sp o s s i b l et oh a v et w os c h e d u l e st h a tp r o d u c et h es a m eo u t c o m eb u tt h a ta r e\nnot con\ufb02ict equivalent. For example, consider transaction T5,w h i c ht r a n s f e r s$ 1 0f r o m\naccount Bto account A. Let schedule 8 be as de\ufb01ned in Figure 17.13. We claim that\nschedule 8 is not con\ufb02ict equivalent to the serial schedule <T1,T5>,s i n c e ,i ns c h e d -\nule 8, the write (B) instruction of T5con\ufb02icts with the read (B) instruction of T1.T h i s\ncreates an edge T5\u2192T1in the precedence graph. Similarly, we see that the write (A)\ninstruction of T1con\ufb02icts with the read instruction of T5,c r e a t i n ga ne d g e T1\u2192T5.\nThis shows that the precedence graph has a cycle and that schedule 8 is not serializable.\nHowever, the \ufb01nal values of accounts Aand Bafter the execution of either schedule 8\nor the serial schedule <T1,T5>are the same\u2014$960 and $2040, respectively.\n3If instead we measure complexity in terms of the number of edges, which corresponds to the number of actual con\ufb02icts\nbetween active transactions, then depth-\ufb01rst-based cycle detection is linear.\n", "846": "818 Chapter 17 Transactions\n(b) (c)(a)TmTk\nTk\nTkTjTi\nTmTjTi\nTmTi\nTj\nFigure 17.12 Illustration of topological sorting.\nWe can see from this example that there are less-stringent de\ufb01nitions of schedule\nequivalence than con\ufb02ict equivalence. For the system to determine that schedule 8\nproduces the same outcome as the serial schedule <T1,T5>,i tm u s ta n a l y z et h ec o m -\nputation performed by T1and T5, rather than just the read andwrite operations. In\ngeneral, such analysis is hard to implement and is computationally expensive. In our\nexample, the \ufb01nal result is the same as that of a serial schedule because of the math-\nematical fact that the increment and decrement operations are commutative. While\nthis may be easy to see in our simple example, the general case is not so easy since a\ntransaction may be expressed as a complex SQL statement, a Java program with JDBC\ncalls, etc.\nHowever, there are other de\ufb01nitions of schedule equivalence based purely on the\nread andwrite operations. One such de\ufb01nition is view equivalence , a de\ufb01nition that\nleads to the concept of view serializability . View serializability is not used in practice\ndue to its high degree of computational complexity.4We therefore defer discussion of\n4Testing for view serializability has been proven to be NP-complete, which means that it is virtually certain that no\ne\ufb03cient test for view serializability exists.\n", "847": "17.7 Transaction Isolation and Atomicity 819\nT1\n T5\nread (A)\nA:=A\u221250\nwrite (A)\nread (B)\nB:=B\u221210\nwrite (B)\nread (B)\nB:=B+50\nwrite (B)\nread (A)\nA:=A+10\nwrite (A)\nFigure 17.13 Schedule 8.\nview serializability to Chapter 18, but, for completeness, note here that the example of\nschedule 8 is not view serializable.\n17.7 Transaction Isolation and Atomicity\nSo far, we have studied schedules while assuming implicitly that there are no transaction\nfailures. We now address the e\ufb00ect of transaction failures during concurrent execution.\nIf a transaction Tifails, for whatever reason, we need to undo the e\ufb00ect of this\ntransaction to ensure the atomicity property of the transaction. In a system that allows\nconcurrent execution, the atomicity property requires that any transaction Tjthat is\ndependent on Ti(i.e., Tjhas read data written by Ti) is also aborted. To achieve this,\nwe need to place restrictions on the types of schedules permitted in the system.\nIn the following two subsections, we address the issue of what schedules are accept-\nable from the viewpoint of recovery from transaction failure. We describe in Chapter\n18 how to ensure that only such acceptable schedules are generated.\n17.7.1 Recoverable Schedules\nConsider the partial schedule 9 in Figure 17.14, in which T7is a transaction that per-\nforms only one instruction: read (A). We call this a partial schedule because we have\nnot included a commit orabort operation for T6.N o t i c et h a t T7commits immediately\nafter executing the read (A) instruction. Thus, T7commits while T6is still in the ac-\ntive state. Now suppose that T6fails before it commits. T7has read the value of data\nitem Awritten by T6. Therefore, we say that T7isdependent onT6.B e c a u s eo ft h i s ,w e\nmust abort T7to ensure atomicity. However, T7has already committed and cannot be\n", "848": "820 Chapter 17 Transactions\nT6\n T7\nread (A)\nwrite (A)\nread (A)\ncommit\nread (B)\nFigure 17.14 Schedule 9, a nonrecoverable schedule.\naborted. Thus, we have a situation where it is impossible to recover correctly from the\nfailure of T6.\nSchedule 9 is an example of a nonrecoverable schedule. A recoverable schedule is one\nwhere, for each pair of transactions Tiand Tjsuch that Tjreads a data item previously\nwritten by Ti, the commit operation of Tiappears before the commit operation of Tj.\nFor the example of schedule 9 to be recoverable, T7would have to delay committing\nuntil after T6commits.\n17.7.2 Cascadeless Schedules\nEven if a schedule is recoverable, to recover correctly from the failure of a transac-\ntion Ti, we may have to roll back several transactions. Such situations occur if transac-\ntions have read data written by Ti. As an illustration, consider the partial schedule of\nFigure 17.15. Transaction T8writes a value of Athat is read by transaction T9.T r a n s a c -\ntion T9writes a value of Athat is read by transaction T10.S u p p o s et h a t ,a tt h i sp o i n t ,\nT8fails. T8must be rolled back. Since T9is dependent on T8,T9must be rolled back.\nSince T10is dependent on T9,T10must be rolled back. This phenomenon, in which a\nsingle transaction failure leads to a series of transaction rollbacks, is called cascading\nrollback .\nT8\n T9\n T10\nread (A)\nread (B)\nwrite (A)\nread (A)\nwrite (A)\nread (A)\nabort\nFigure 17.15 Schedule 10.\n", "849": "17.8 Transaction Isolation Levels 821\nCascading rollback is undesirable, since it leads to the undoing of a signi\ufb01cant\namount of work. It is desirable to restrict the schedules to those where cascading roll-\nbacks cannot occur. Such schedules are called cascadeless schedules. Formally, a cas-\ncadeless schedule is one where, for each pair of transactions Tiand Tjsuch that Tjreads\nad a t ai t e mp r e v i o u s l yw r i t t e nb y Ti, the commit operation of Tiappears before the read\noperation of Tj. It is easy to verify that every cascadeless schedule is also recoverable.\n17.8 Transaction Isolation Levels\nSerializability is a useful concept because it allows programmers to ignore issues related\nto concurrency when they code transactions. If every transaction has the property that\nit maintains database consistency if executed alone, then serializability ensures that\nconcurrent executions maintain consistency. However, the protocols required to ensure\nserializability may allow too little concurrency for certain applications. In these cases,\nweaker levels of consistency are used. The use of weaker levels of consistency places\nadditional burdens on programmers for ensuring database correctness.\nThe SQL standard also allows a transaction to specify that it may be executed in\nsuch a way that it becomes nonserializable with respect to other transactions. For in-\nstance, a transaction may operate at the isolation level of read uncommitted ,w h i c h\npermits the transaction to read a data item even if it was written by a transaction that\nhas not been committed. SQL provides such features for the bene\ufb01t of long transac-\ntions whose results do not need to be precise. If these transactions were to execute in\na serializable fashion, they could interfere with other transactions, causing the others\u2019\nexecution to be delayed.\nThe isolation levels speci\ufb01ed by the SQL standard are as follows:\n\u2022Serializable usually ensures serializable execution. However, as we shall explain\nshortly, some database systems implement this isolation level in a manner that\nmay, in certain cases, allow nonserializable executions.\n\u2022Repeatable read allows only committed data to be read and further requires that,\nbetween two reads of a data item by a transaction, no other transaction is allowed\nto update it. However, the transaction may not be serializable with respect to other\ntransactions. For instance, when it is searching for data satisfying some conditions,\na transaction may \ufb01nd some of the data inserted by a committed transaction, but\nmay not \ufb01nd other data inserted by the same transaction.\n\u2022Read committed allows only committed data to be read, but does not require re-\npeatable reads. For instance, between two reads of a data item by the transaction,\nanother transaction may have updated the data item and committed.\n\u2022Read uncommitted allows uncommitted data to be read. It is the lowest isolation\nlevel allowed by SQL.\n", "850": "822 Chapter 17 Transactions\nAll the isolation levels above additionally disallow dirty writes , that is, they disallow\nwrites to a data item that has already been written by another transaction that has not\nyet committed or aborted.\nMany database systems run, by default, at the read-committed isolation level. In\nSQL, it is possible to set the isolation level explicitly, rather than accepting the system\u2019s\ndefault setting. For example, the statement\nset transaction isolation level serializable\nsets the isolation level to serializable; any of the other isolation levels may be speci\ufb01ed\ninstead. The preceding syntax is supported by Oracle, Postgre SQL,a n d SQL S erver;\nOracle uses the syntax\nalter session set isolation\n level=serializable\nwhile DB2 uses the syntax \u201c change isolation level \u201d with its own abbreviations for iso-\nlation levels. Changing of the isolation level must be done as the \ufb01rst statement of a\ntransaction.\nB yd e f a u l t ,m o s td a t a b a s e sc o m m i ti n d i v i d u a ls t a t e m e n t sa ss o o na st h e ya r ee x e -\ncuted. Such automatic commit of individual statements must be turned o\ufb00 to allow mul-\ntiple statements to run as a single transaction. The command start transaction ensures\nthat subsequent SQL statements, until a subsequent commit orrollback , are executed\nas a single transaction. As expected, the commit operation commits the preceding SQL\nstatements, while rollback rolls back the preceding SQL statements. ( SQL S erver uses\nbegin transaction in place of start transaction , while Oracle and Postgre SQL treat begin\nas identical to start transaction .)\nAPIss u c ha s JDBC and ODBC provide functions to turn o\ufb00 automatic commit. In\nJDBC thesetAutoCommit method of the Connection interface (which we saw earlier\nin Section 5.1.1.8) can be used to turn automatic commit o\ufb00 by invoking setAutoCom-\nmit(false) ,o ro nb yi n v o k i n g setAutoCommit(true) .F u r t h e r ,i n JDBC the method set-\nTransactionIsolation(int level) of the Connection interface can be invoked with any\none of\n\u2022Connection.TRANSACTION\n SERIALIZABLE ,\n\u2022Connection.TRANSACTION\n REPEATABLE\n READ ,\n\u2022Connection.TRANSACTION\n READ\n COMMITTED ,o r\n\u2022Connection.TRANSACTION\n READ\n UNCOMMITTED\nto set the transaction isolation level correspondingly.\nAn application designer may decide to accept a weaker isolation level in order to\nimprove system performance. As we shall see in Section 17.9 and Chapter 18, ensuring\nserializability may force a transaction to wait for other transactions or, in some cases,\nto abort because the transaction can no longer be executed as part of a serializable ex-\necution. While it may seem shortsighted to risk database consistency for performance,\n", "851": "17.9 Implementation of Isolation Levels 823\nthis trade-o\ufb00 makes sense if we can be sure that the inconsistency that may occur is\nnot relevant to the application.\nThere are many means of implementing isolation levels. As long as the implemen-\ntation ensures serializability, the designer of a database application or a user of an\napplication does not need to know the details of such implementations, except per-\nhaps for dealing with performance issues. Unfortunately, even if the isolation level is\nset to serializable , some database systems actually implement a weaker level of isola-\ntion, which does not rule out every possible n onserializable execution; we revisit this\nissue in Section 17.9. If weaker levels of isolation are used, either explicitly or implic-\nitly, the application designer has to be aware of some details of the implementation, to\navoid or minimize the chance of inconsistency due to lack of serializability.\n17.9 Implementation of Isolation Levels\nSo far, we have seen what properties a schedule must have if it is to leave the database\nin a consistent state and allow transaction failures to be handled in a safe manner.\nThere are various concurrency-control policies that we can use to ensure that, even\nwhen multiple transactions are executed concurrently, only acceptable schedules are\ngenerated, regardless of how the operating system time-shares resources (such as CPU\ntime) among the transactions.\nAs a trivial example of a concurrency-control policy, consider this: A transaction\nacquires a lock on the entire database before it starts and releases the lock after it\nhas committed. While a transaction holds a lock, no other transaction is allowed to\nacquire the lock, and all must therefore wait for the lock to be released. As a result of\nthe locking policy, only one transaction can execute at a time. Therefore, only serial\nschedules are generated. These are trivially serializable, and it is easy to verify that they\nare recoverable and cascadeless as well.\nA concurrency-control policy such as this one leads to poor performance, since it\nforces transactions to wait for preceding transactions to \ufb01nish before they can start. In\nother words, it provides a poor degree of concurrency (indeed, no concurrency at all).\nAs we saw in Section 17.5, concurrent execution has substantial performance bene\ufb01ts.\nThe goal of concurrency-control policies is to provide a high degree of concurrency,\nwhile ensuring that all schedules that can be generated are con\ufb02ict or view serializable,\nrecoverable, and cascadeless.\nHere we provide an overview of how some of most important concurrency-control\nmechanisms work, and we defer the details to Chapter 18.\n17.9.1 Locking\nInstead of locking the entire database, a transaction could instead lock only those data\nitems that it accesses. Under such a policy, the transaction must hold locks long enough\nto ensure serializability, but for a period short enough not to harm performance exces-\n", "852": "824 Chapter 17 Transactions\nNote 17.2 SERIALIZABILITY IN THE REAL WORLD\nSerializable schedules are the ideal way to ensure consistency, but in our day-to-\nday lives, we don\u2019t impose such stringent requirements. A web site o\ufb00ering goods\nfor sale may list an item as being in stock, yet by the time a user selects the item\nand goes through the checkout process, that item might no longer be available.\nViewed from a database perspective, this would be a nonrepeatable read.\nAs another example, consider seat selection for air travel. Assume that a trav-\neler has already booked an itinerary and now is selecting seats for each \ufb02ight. Many\nairline web sites allow the user to step through the various \ufb02ights and choose a seat,\nafter which the user is asked to con\ufb01rm the selection. It could be that other trav-\nelers are selecting seats or changing their seat selections for the same \ufb02ights at\nthe same time. The seat availability that the traveler was shown is thus actually\nchanging, but the traveler is shown a snapshot of the seat availability as of when\nthe traveler started the seat selection process.\nEven if two travelers are selecting seats at the same time, most likely they will\nselect di\ufb00erent seats, and if so there would be no real con\ufb02ict. However, the trans-\nactions are not serializable, since each traveler has read data that was subsequently\nupdated by the other traveler, leading to a cycle in the precedence graph. If two\ntravelers performing seat selection conc urrently actually selected the same seat,\no n eo ft h e mw o u l dn o tb ea b l et og e tt h es e a tt h e ys e l e c t e d ;h o w e v e r ,t h es i t u a t i o n\ncould be easily resolved by asking the traveler to perform the selection again, with\nupdated seat availability information.\nIt is possible to enforce serializability by allowing only one traveler to do seat\nselection for a particular \ufb02ight at a time. However, doing so could cause signi\ufb01cant\ndelays as travelers would have to wait for their \ufb02ight to become available for seat\nselection; in particular a traveler who takes a long time to make a choice could\ncause serious problems for other travelers. Instead, any such transaction is typically\nbroken up into a part that requires user interaction and a part that runs exclusively\non the database. In the example above, the database transaction would check if\nthe seats chosen by the user are still available, and if so update the seat selection\nin the database. Serializability is ensured only for the transactions that run on the\ndatabase, without user interaction.\nsively. Complicating matters are SQL statements where the data items accessed depend\non a where clause, which we discuss in Section 17.10. In Chapter 18, we present the\ntwo-phase locking protocol, a simple, widely used technique that ensures serializability.\nStated simply, two-phase locking requires a transaction to have two phases, one where\nit acquires locks but does not release any, and a second phase where the transaction re-\nleases locks but does not acquire any. (In practice, locks are usually released only when\nthe transaction completes its execution and has been either committed or aborted.)\n", "853": "17.9 Implementation of Isolation Levels 825\nFurther improvements to locking result if we have two kinds of locks: shared and\nexclusive. Shared locks are used for data that the transaction reads and exclusive locks\nare used for those it writes. Many transactions can hold shared locks on the same data\nitem at the same time, but a transaction is allowed an exclusive lock on a data item\nonly if no other transaction holds any lock (regardless of whether shared or exclusive)\non the data item. This use of two modes of locks along with two-phase locking allows\nconcurrent reading of data while still ensuring serializability.\n17.9.2 Timestamps\nAnother category of techniques for the implementation of isolation assigns each trans-\naction a timestamp , typically when it begins. For each data item, the system keeps two\ntimestamps. The read timestamp of a data item holds the largest (that is, the most re-\ncent) timestamp of those transactions that read the data item. The write timestamp of\na data item holds the timestamp of the transaction that wrote the current value of the\ndata item. Timestamps are used to ensure that transactions access each data item in or-\nder of the transactions\u2019 timestamps if their accesses con\ufb02ict. When this is not possible,\no\ufb00ending transactions are aborted and restarted with a new timestamp.\n17.9.3 Multiple Versions and Snapshot Isolation\nBy maintaining more than one version of a data item, it is possible to allow a trans-\naction to read an old version of a data item rather than a newer version written by an\nuncommitted transaction or by a transaction that should come later in the serializa-\ntion order. There are a variety of multiversion concurrency-control techniques. One in\nparticular, called snapshot isolation , is widely used in practice.\nIn snapshot isolation, we can imagine that each transaction is given its own version,\nor snapshot, of the database when it begins.5It reads data from this private version and\nis thus isolated from the updates made by other transactions. If the transaction updates\nthe database, that update appears only in its own version, not in the actual database\nitself. Information about these updates is saved so that the updates can be applied to\nthe \u201creal\u201d database if the transaction commits.\nWhen a transaction Tenters the partially committed state, it then proceeds to\nthe committed state only if no other concurrent transaction has modi\ufb01ed data that T\nintends to update. Transactions that, as a result, cannot commit abort instead.\nSnapshot isolation ensures that attempts to read data never need to wait (unlike\nlocking). Read-only transactions cannot be aborted; only those that modify data run a\nslight risk of aborting. Since each transaction reads its own version or snapshot of the\ndatabase, reading data does not cause subsequent update attempts by other transactions\nto wait (unlike locking). Since most transactions are read-only (and most others read\nmore data than they update), this is often a major source of performance improvement\nas compared to locking.\n5In reality, the entire database is not copied. Multiple versions are kept only of those data items that are changed.\n", "854": "826 Chapter 17 Transactions\nThe problem with snapshot isolation is that, paradoxically, it provides too much\nisolation. Consider two transactions Tand T\u2032. In a serializable execution, either Tsees\nall the updates made by T\u2032orT\u2032sees all the updates made by T,b e c a u s eo n em u s t\nfollow the other in the serialization order. Under snapshot isolation, there are cases\nwhere neither transaction sees the updates of the other. This is a situation that cannot\noccur in a serializable execution. In many (indeed, most) cases, the data accesses by\nthe two transactions do not con\ufb02ict and there is no problem. However, if Treads some\ndata item that T\u2032updates and T\u2032reads some data item that Tupdates, it is possible\nthat both transactions fail to read the update made by the other. The result, as we shall\nsee in Chapter 18, may be an inconsistent database state that, of course, could not be\nobtained in any serializable execution.\nOracle, Postgre SQL,a n d SQL Server o\ufb00er the option of snapshot isolation. Oracle\nand Postgre SQL versions prior to Postgre SQL 9.1 implement the serializable isolation\nlevel using snapshot isolation. As a result, their implementation of serializability can,\nin exceptional circumstances, result in a nonserializable execution being allowed. SQL\nServer instead includes an additional isolation level beyond the standard ones, called\nsnapshot , to o\ufb00er the option of snapshot isolation. Postgre SQL versions subsequent\nto 9.1 implement a form of concurrency control called serializable snapshot isolation,\nwhich provides the bene\ufb01ts of snapshot isolation while ensuring serializability.\n17.10 Transactions as SQL Statements\nIn Section 4.3, we presented the SQL syntax for specifying the beginning and end of\ntransactions. Now that we have seen some of the issues in ensuring the ACID proper-\nties for transactions, we are ready to consider how those properties are ensured when\ntransactions are speci\ufb01ed as a sequence of SQL statements rather than the restricted\nmodel of simple reads and writes that we considered up to this point.\nIn our simple model, we assumed a set of data items exists. While our simple\nmodel allowed data-item values to be changed, it did not allow data items to be created\nor deleted. In SQL,h o w e v e r , insert statements create new data and delete statements\ndelete data. These two statements are, in e\ufb00ect, write operations, since they change the\ndatabase, but their interactions with the actions of other transactions are di\ufb00erent from\nwhat we saw in our simple model. As an example, consider how insertion or deletion\nwould con\ufb02ict with the following SQL query, which \ufb01nds all instructors who earn more\nthan $90,000:\nselect ID,name\nfrom instructor\nwhere salary >90000;\nUsing our sample instructor relation (Section A.3), we \ufb01nd that only Einstein and\nBrandt satisfy the condition. Now assume that around the same time we are running our\nquery, another user inserts a new instructor named \u201cJames\u201d whose salary is $100,000.\n", "855": "17.10 Transactions as SQL Statements 827\ninsert into instructor values ('11111', 'James', 'Marketing', 1 00000);\nThe result of our query depends on whether this insert comes before or after our query\nis run. In a concurrent execution of these transactions, it is intuitively clear that they\ncon\ufb02ict, but this is a con\ufb02ict that may not be captured by our simple model. This situ-\nation is referred to as the phantom phenomenon because a con\ufb02ict may exist on \u201cphan-\ntom\u201d data.\nOur simple model of transactions required that operations operate on a speci\ufb01c\ndata item given as an argument to the operation. In our simple model, we can look at the\nreadandwrite steps to see which data items are referenced. But in an SQLstatement, the\nspeci\ufb01c data items (tuples) referenced may be determined by a where clause predicate.\nSo the same transaction, if run more than once, might reference di\ufb00erent data items\neach time it is run if the values in the database change between runs. In our example,\nthe 'James' tuple is referenced only if our query comes after the insertion. Let Tdenote\nthe query and let T\u2032denote the insert. If T\u2032comes \ufb01rst, then there is an edge T\u2032\u2192T\nin the precedence graph. However, in the case where the query Tcomes \ufb01rst, there\nis no edge in the precedence graph between Tand T\u2032despite the actual con\ufb02ict on\nphantom data that forces Tto be serialized before T\u2032.\nThe above-mentioned problem demonstrates that it is not su\ufb03cient for concur-\nrency control to consider only the tuples that are accessed by a transaction; the in-\nformation used to \ufb01nd the tuples that are accessed by the transaction must also be\nconsidered for the purpose of concurrency control. The information used to \ufb01nd tu-\nples could be updated by an insertion or deletion, or in the case of an index, even by an\nupdate to a search-key attribute. For example, if locking is used for concurrency control,\nthe data structures that track the tuples in a relation, as well as index structures, must\nbe appropriately locked. However, such locking can lead to poor concurrency in some\nsituations; index-locking protocols that maximize concurrency, while ensuring serial-\nizability in spite of inserts, deletes, and predicates in queries, are discussed in Section\n18.4.3.\nLet us consider again the query:\nselect ID,name\nfrom instructor\nwhere salary>90000;\nand the following SQL update:\nupdate instructor\nsetsalary =salary *0 . 9\nwhere name=\u2019Wu\u2019;\nWe now face an interesting situation in det ermining whether our query con\ufb02icts with\nthe update statement. If our query reads the entire instructor relation, then it reads the\n", "856": "828 Chapter 17 Transactions\ntuple with Wu\u2019s data and con\ufb02icts with the update. However, if an index were available\nthat allowed our query direct access to those tuples with salary>90000, then our query\nwould not have accessed Wu\u2019s data at all because Wu\u2019s salary is initially $90,000 in our\nexample instructor relation and reduces to $81,000 after the update.\nHowever, using the above approach, it would appear that the existence of a con\ufb02ict\ndepends on a low-level query processing decision by the system that is unrelated to a\nuser-level view of the meaning of the two SQL statements! An alternative approach to\nconcurrency control treats an insert, delete, or update as con\ufb02icting with a predicate\non a relation, if it could a\ufb00ect the set of tuples selected by a predicate. In our example\nquery above, the predicate is \u201c salary >90000\u201d, and an update of Wu\u2019s salary from\n$90,000 to a value greater than $90,000, or an up date of Einstein\u2019s salary from a value\ngreater than $90,000 to a value less than or equal to $90,000, would con\ufb02ict with this\npredicate. Locking based on this idea is called predicate locking ; predicate locking is\noften implemented using locks on index nodes as we see in Section 18.4.3.\n17.11 Summary\n\u2022Atransaction is a unitof program execution that accesses and possibly updates\nvarious data items. Understanding the concept of a transaction is critical for un-\nderstanding and implementing updates of data in a database in such a way that\nconcurrent executions and failures of various forms do not result in the database\nbecoming inconsistent.\n\u2022Transactions are required to have the ACID properties: atomicity, consistency, iso-\nlation, and durability.\n\u00b0Atomicity ensures that either all the e\ufb00ects of a transaction are re\ufb02ected in the\ndatabase, or none are; a failure cannot leave the database in a state where a\ntransaction is partially executed.\n\u00b0Consistency ensures that, if the database is initially consistent, the execution\nof the transaction (by itself) leaves the database in a consistent state.\n\u00b0Isolation ensures that concurrently executing transactions are isolated from\none another, so that each has the impression that no other transaction is exe-\ncuting concurrently with it.\n\u00b0Durability ensures that, once a transaction has been committed, that transac-\ntion\u2019s updates do not get lost, even if there is a system failure.\n\u2022Concurrent execution of transactions improves throughput of transactions and\nsystem utilization and also reduces the waiting time of transactions.\n\u2022The various types of storage in a computer are volatile storage, non-volatile storage,\nand stable storage. Data in volatile storage, such as in RAM ,a r el o s tw h e nt h e\ncomputer crashes. Data in non-volatile storage, such as disk, are not lost when\n", "857": "17.11 Summary 829\nthe computer crashes but may occasionally be lost because of failures such as disk\ncrashes. Data in stable storage are never lost.\n\u2022Stable storage that must be accessible online is approximated with mirrored disks,\nor other forms of RAID , which provide redundant data storage. O\ufb04ine, or archival,\nstable storage may consist of multiple tape copies of data stored in physically se-\ncure locations.\n\u2022When several transactions execute concurrently on the database, the consistency\nof data may no longer be preserved. It is therefore necessary for the system to\ncontrol the interaction among the concurrent transactions.\n\u00b0Since a transaction is a unit that preserves consistency, a serial execution of\ntransactions guarantees that consistency is preserved.\n\u00b0Aschedule captures the key actions of transactions that a\ufb00ect concurrent ex-\necution, such as read andwrite operations, while abstracting away internal\ndetails of the execution of the transaction.\n\u00b0We require that any schedule produced by concurrent processing of a set of\ntransactions will have an e\ufb00ect equivalent to a schedule produced when these\ntransactions are run serially in some order.\n\u00b0A system that guarantees this property is said to ensure serializability .\n\u00b0There are several di\ufb00erent notions of equivalence leading to the concepts of\ncon\ufb02ict serializability and view serializability .\n\u2022Serializability of schedules generated by concurrently executing transactions can\nbe ensured through one of a variety of mechanisms called concurrency-control poli-\ncies.\n\u2022We can test a given schedule for con\ufb02ict serializability by constructing a prece-\ndence graph for the schedule and by searching for the absence of cycles in the\ngraph. However, there are more e\ufb03cient concurrency-control policies for ensur-\ning serializability.\n\u2022S c h e d u l e sm u s tb er e c o v e r a b l e ,t om a k es u r et h a ti ft r a n s a c t i o n asees the e\ufb00ects\nof transaction b,a n d bthen aborts, then aalso gets aborted.\n\u2022Schedules should preferably be cascadeless, so that the abort of a transaction does\nnot result in cascading aborts of other transactions. Cascadelessness is ensured by\nallowing transactions to only read committed data.\n\u2022The concurrency-control management component of the database is responsible\nfor handling the concurrency-control policies. Techniques include locking, times-\ntamp ordering, and snapshot isolation. Chapter 18 describes concurrency-control\npolicies.\n", "858": "830 Chapter 17 Transactions\n\u2022Database systems o\ufb00er isolation levels weaker than serializability to allow less re-\nstriction of concurrency and thus improved performance. This introduces a risk\nof inconsistency that some applications \ufb01nd acceptable.\n\u2022Ensuring correct concurrent execution in the presence of SQL update ,insert ,a n d\ndelete operations requires additional care due to the phantom phenomenon.\nReview Terms\n\u2022Transaction\n\u2022ACID properties\n\u00b0Atomicity\n\u00b0Consistency\n\u00b0Isolation\n\u00b0Durability\n\u2022Inconsistent state\n\u2022Storage types\n\u00b0Volatile storage\n\u00b0Non-volatile storage\n\u00b0Stable storage\n\u2022Concurrency-control system\n\u2022Recovery system\n\u2022Transaction state\n\u00b0Active\n\u00b0Partially committed\n\u00b0Failed\n\u00b0Aborted\n\u00b0Committed\n\u00b0Terminated\n\u2022compensating transaction\n\u2022Transaction\n\u00b0Restart\n\u00b0Kill\u2022Observable external writes\n\u2022Concurrent executions\n\u2022Serial execution\n\u2022Schedules\n\u2022Con\ufb02ict of operations\n\u2022Con\ufb02ict equivalence\n\u2022Con\ufb02ict serializability\n\u2022Serializability testing\n\u2022Precedence graph\n\u2022Serializability order\n\u2022Recoverable schedules\n\u2022Cascading rollback\n\u2022Cascadeless schedules\n\u2022Isolation levels\n\u00b0Serializable\n\u00b0Repeatable read\n\u00b0Read committed\n\u00b0Read uncommitted\n\u2022Dirty writes\n\u2022Automatic commit\n\u2022Concurrency control\n\u2022Locking\n\u2022Timestamp ordering\n\u2022Snapshot isolation\n\u2022Phantom phenomenon\n\u2022Predicate locking\n", "859": "Practice Exercises 831\nPractice Exercises\n17.1 Suppose that there is a database system that never fails. Is a recovery manager\nrequired for this system?\n17.2 Consider a \ufb01le system such as the one on your favorite operating system.\na. What are the steps involved in the creation and deletion of \ufb01les and in\nwriting data to a \ufb01le?\nb. Explain how the issues of atomicity and durability are relevant to the\ncreation and deletion of \ufb01les and to writing data to \ufb01les.\n17.3 Database-system implementers have paid much more attention to the ACID\nproperties than have \ufb01le-system implementers. Why might this be the case?\n17.4 What class or classes of storage can be used to ensure durability? Why?\n17.5 Since every con\ufb02ict-serializable schedule is view serializable, why do we em-\nphasize con\ufb02ict serializability rather than view serializability?\n17.6 Consider the precedence graph of Figure 17.16. Is the corresponding schedule\ncon\ufb02ict serializable? Explain your answer.\n17.7 What is a cascadeless schedule? Why is cascadelessness of schedules desir-\nable? Are there any circumstances under which it would be desirable to allow\nnoncascadeless schedules? Explain your answer.\n17.8 The lost update anomaly is said to occur if a transaction Tjreads a data item,\nthen another transaction Tkwrites the data item (possibly based on a previous\nread), after which Tjwrites the data item. The update performed by Tkhas\nb e e nl o s t ,s i n c et h eu p d a t ed o n eb y Tjignored the value written by Tk.\nT1\nT4\nT5T3T2\nFigure 17.16 Precedence graph for Practice Exercise 17.6.\n", "860": "832 Chapter 17 Transactions\na. Give an example of a schedule showing the lost update anomaly.\nb. Give an example schedule to show that the lost update anomaly is possi-\nble with the read committed isolation level.\nc. Explain why the lost update anomaly is not possible with the repeatable\nread isolation level.\n17.9 Consider a database for a bank where the database system uses snapshot iso-\nlation. Describe a particular scenario in which a nonserializable execution oc-\ncurs that would present a problem for the bank.\n17.10 Consider a database for an airline where the database system uses snapshot\nisolation. Describe a particular scenario in which a nonserializable execution\noccurs, but the airline may be willing to accept it in order to gain better overall\nperformance.\n17.11 The de\ufb01nition of a schedule assumes that operations can be totally ordered\nby time. Consider a database system that runs on a system with multiple pro-\ncessors, where it is not always possible to establish an exact ordering between\noperations that executed on di\ufb00erent processors. However, operations on a\ndata item can be totally ordered.\nDoes this situation cause any problem for the de\ufb01nition of con\ufb02ict serializ-\nability? Explain your answer.\nExercises\n17.12 List the ACID properties. Explain the usefulness of each.\n17.13 During its execution, a transaction passes through several states, until it \ufb01nally\ncommits or aborts. List all possible sequences of states through which a trans-\naction may pass. Explain why each state transition may occur.\n17.14 Explain the distinction between the terms serial schedule and serializable sched-\nule.\n17.15 Consider the following two transactions:\nT13:read (A);\nread (B);\nifA=0then B:=B+1 ;\nwrite (B).\nT14:read (B);\nread (A);\nifB=0then A:=A+1 ;\nwrite (A).\n", "861": "Exercises 833\nLet the consistency requirement be A=0\u2228B=0, with A=B=0a s\nthe initial values.\na. Show that every serial execution involving these two transactions pre-\nserves the consistency of the database.\nb. Show a concurrent execution of T13and T14that produces a nonserializ-\nable schedule.\nc. Is there a concurrent execution of T13and T14that produces a serializable\nschedule?\n17.16 Give an example of a serializable schedule with two transactions such that the\norder in which the transactions commit is di\ufb00erent from the serialization order.\n17.17 What is a recoverable schedule? Why is recoverability of schedules desirable?\nAre there any circumstances under which it would be desirable to allow non-\nrecoverable schedules? Explain your answer.\n17.18 Why do database systems support concurrent execution of transactions, de-\nspite the extra e\ufb00ort needed to ensure that concurrent execution does not cause\nany problems?\n17.19 Explain why the read-committed isolation level ensures that schedules are\ncascade-free.\n17.20 For each of the following isolation levels, give an example of a schedule that\nrespects the speci\ufb01ed level of isolation but is not serializable:\na. Read uncommitted\nb. Read committed\nc. Repeatable read\n17.21 Suppose that in addition to the operations read andwrite , we allow an opera-\ntionpred\n read (r,P), which reads all tuples in relation rthat satisfy predicate\nP.\na. Give an example of a schedule using the pred\n read operation that ex-\nhibits the phantom phenomenon and is nonserializable as a result.\nb. Give an example of a schedule where one transaction uses the\npred\n read operation on relation rand another concurrent transaction\ndeletes a tuple from r, but the schedule does not exhibit a phantom con-\n\ufb02ict. (To do so, you have to give the schema of relation rand show the\nattribute values of the deleted tuple.)\n", "862": "834 Chapter 17 Transactions\nFurther Reading\n[Gray and Reuter (1993)] provides detailed textbook coverage of transaction-\nprocessing concepts, techniques, and impleme ntation details, including concurrency\ncontrol and recovery issues. [Bernstein and Newcomer (2009)] provides textbook cov-\nerage of various aspects of transaction processing.\nThe concept of serializability was formalized by [Eswaran et al. (1976)] in connec-\ntion with work on concurrency control for System R.\nReferences covering speci\ufb01c aspects of transaction processing, such as concur-\nrency control and recovery, are cited in Chapter 18 and Chapter 19.\nBibliography\n[Bernstein and Newcomer (2009)] P. A. Bernstein and E. Newcomer, Principles of Transaction\nProcessing , 2nd edition, Morgan Kaufmann (2009).\n[Eswaran et al. (1976)] K. P. Eswaran, J. N. Gray, R. A. Lorie, and I. L. Traiger, \u201cThe Notions\nof Consistency and Predicate Locks in a Database System\u201d, Communications of the ACM ,\nVolume 19, Number 11 (1976), pages 624\u2013633.\n[Gray and Reuter (1993)] J. Gray and A. Reuter, Transaction Processing: Concepts and Tech-\nniques , Morgan Kaufmann (1993).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "863": "CHAPTER18\nConcurrency Control\nWe saw in Chapter 17 that one of the fundamental properties of a transaction is iso-\nlation. When several transactions execute concurrently in the database, however, the\nisolation property may no longer be preserved. To ensure that it is, the system must\ncontrol the interaction among the concurrent transactions; this control is achieved\nthrough one of a variety of mechanisms called concurrency-control schemes. In this\nchapter, we consider the management of concurrently executing transactions, and we\nignore failures. In Chapter 19, we shall see how the system can recover from failures.\nAs we shall see, there are a variety of concurrency-control schemes. No one scheme\nis clearly the best; each one has advantages. In practice, the most frequently used\nschemes are two-phase locking and snapshot isolation .\n18.1 Lock-Based Protocols\nOne way to ensure isolation is to require that data items be accessed in a mutually exclu-\nsive manner; that is, while one transaction is accessing a data item, no other transaction\ncan modify that data item. The most common method used to implement this require-\nment is to allow a transaction to access a data item only if it is currently holding a lock\non that item. We introduced the concept of locking in Section 17.9.\n18.1.1 Locks\nThere are various modes in which a data item may be locked. In this section, we restrict\nour attention to two modes:\n1.Shared .I fat r a n s a c t i o n Tihas obtained a shared-mode lock (denoted by S) on\nitem Q,t h e n Tican read, but cannot write, Q.\n2.Exclusive .I fat r a n s a c t i o n Tihas obtained an exclusive-mode lock (denoted by X)\non item Q,t h e n Tic a nb o t hr e a da n dw r i t e Q.\nWe require that every transaction request a lock in an appropriate mode on data\nitem Q, depending on the types of operations that it will perform on Q.T h et r a n s a c t i o n\n835\n", "864": "836 Chapter 18 Concurrency Control\nS X\nS true false\nX false false\nFigure 18.1 Lock-compatibility matrix comp .\nmakes the request to the concurrency-control manager. The transaction can proceed\nwith the operation only after the concurrency-control manager grants the lock to the\ntransaction. The use of these two lock modes allows multiple transactions to read a\ndata item but limits write access to just one transaction at a time.\nTo state this more generally, given a set of lock modes, we can de\ufb01ne a compatibility\nfunction on them as follows: Let Aand Brepresent arbitrary lock modes. Suppose that\nat r a n s a c t i o n Tirequests a lock of mode Aon item Qon which transaction Tj(Ti\n\u2260Tj) currently holds a lock of mode B.I ft r a n s a c t i o n Tican be granted a lock on\nQimmediately, in spite of the presence of the mode Block, then we say mode Ais\ncompatible with mode B. Such a function can be represented conveniently by a matrix.\nThe compatibility relation between the two modes of locking discussed in this section\nappears in the matrix comp of Figure 18.1. An element comp (A, B) of the matrix has\nthe value trueif and only if mode Ais compatible with mode B.\nNote that shared mode is compatible with shared mode, but not with exclusive\nmode. At any time, several shared-mode locks can be held simultaneously (by di\ufb00erent\ntransactions) on a particular data item. A s ubsequent exclusive-mode lock request has\nto wait until the currently held shared-mode locks are released.\nA transaction requests a shared lock on data item Qby executing the lock-S (Q)\ninstruction. Similarly, a transaction requests an exclusive lock through the lock-X (Q)\ninstruction. A transaction can unlock a data item Qby the unlock (Q) instruction.\nTo access a data item, transaction Timust \ufb01rst lock that item. If the data item is al-\nready locked by another transaction in an incompatible mode, the concurrency-control\nmanager will not grant the lock until all incompatible locks held by other transactions\nhave been released. Thus, Tiis made to waituntil all incompatible locks held by other\ntransactions have been released.\nTransaction Timay unlock a data item that it had locked at some earlier point.\nNote that a transaction must hold a lock on a data item as long as it accesses that\nitem. Moreover, it is not necessarily desirable for a transaction to unlock a data item\nimmediately after its \ufb01nal access of that data item, since serializability may not be\nensured.\nAs an illustration, consider again the banking example that we introduced in Chap-\nter 17. Let Aand Bbe two accounts that are accessed by transactions T1and T2.T r a n s -\naction T1transfers $50 from account Bto account A(Figure 18.2). Transaction T2\ndisplays the total amount of money in accounts Aand B\u2014that is, the sum A+B(Figure\n18.3).\n", "865": "18.1 Lock-Based Protocols 837\nT1:lock-X (B);\nread (B);\nB:=B\u221250;\nwrite (B);\nunlock (B);\nlock-X (A);\nread (A);\nA:=A+ 50;\nwrite (A);\nunlock (A).\nFigure 18.2 Transaction T1.\nSuppose that the values of accounts Aand Bare $100 and $200, respectively. If\nthese two transactions are executed serially, either in the order T1,T2or the order T2,\nT1, then transaction T2will display the value $300. If, however, these transactions are\nexecuted concurrently, then schedule 1, in Figure 18.4, is possible. In this case, trans-\naction T2displays $250, which is incorrect. The reason for this mistake is that the\ntransaction T1unlocked data item Btoo early, as a result of which T2saw an inconsis-\ntent state.\nThe schedule shows the actions executed by the transactions, as well as the points\nat which the concurrency-control manager grants the locks. The transaction making\na lock request cannot execute its next action until the concurrency-control manager\ng r a n t st h el o c k .H e n c e ,t h el o c km u s tb eg r a n t e di nt h ei n t e r v a lo ft i m eb e t w e e nt h e\nlock-request operation and the following action of the transaction. Exactly when within\nthis interval the lock is granted is not important; we can safely assume that the lock is\ngranted just before the following action of the transaction. We shall therefore drop the\nT2:lock-S (A);\nread (A);\nunlock (A);\nlock-S (B);\nread (B);\nunlock (B);\ndisplay (A+B).\nFigure 18.3 Transaction T2.\n", "866": "838 Chapter 18 Concurrency Control\nT1\n T2\n concurrency-control manager\nlock-X (B)\ngrant-X (B, T1)\nread (B)\nB:=B\u221250\nwrite (B)\nunlock (B)\nlock-S (A)\ngrant-S (A, T2)\nread (A)\nunlock (A)\nlock-S (B)\ngrant-S (B, T2)\nread (B)\nunlock (B)\ndisplay (A+B )\nlock-X (A)\ngrant-X (A, T1)\nread (A)\nA:=A+5 0\nwrite (A)\nunlock (A)\nFigure 18.4 Schedule 1.\ncolumn depicting the actions of the concurrency-control manager from all schedules\ndepicted in the rest of the chapter. We let you infer when locks are granted.\nSuppose now that unlocking is delayed to the end of the transaction. Transaction\nT3corresponds to T1with unlocking delayed (Figure 18.5). Transaction T4corresponds\ntoT2with unlocking delayed (Figure 18.6).\nYou should verify that the sequence of reads and writes in schedule 1, which lead to\nan incorrect total of $250 being displayed, is no longer possible with T3and T4.O t h e r\nschedules are possible. T4will not print out an inconsistent result in any of them; we\nshall see why later.\nUnfortunately, locking can lead to an undesirable situation. Consider the partial\nschedule of Figure 18.7 for T3and T4.S i n c e T3is holding an exclusive-mode lock on\nBand T4is requesting a shared-mode lock on B,T4is waiting for T3to unlock B.\nSimilarly, since T4is holding a shared-mode lock on Aand T3is requesting an exclusive-\nmode lock on A,T3is waiting for T4to unlock A. Thus, we have arrived at a state\nwhere neither of these transactions can ever proceed with its normal execution. This\nsituation is called deadlock . When deadlock occurs, the system must roll back one of\n", "867": "18.1 Lock-Based Protocols 839\nT3:lock-X (B);\nread (B);\nB:=B\u221250;\nwrite (B);\nlock-X (A);\nread (A);\nA:=A+ 50;\nwrite (A);\nunlock (B);\nunlock (A).\nFigure 18.5 Transaction T3(transaction T1with unlocking delayed).\nthe two transactions. Once a transaction has been rolled back, the data items that were\nlocked by that transaction are unlocked. These data items are then available to the\nother transaction, which can continue with i ts execution. We shall return to the issue\nof deadlock handling in Section 18.2.\nIf we do not use locking, or if we unlock data items too soon after reading or writing\nthem, we may get inconsistent states. On the other hand, if we do not unlock a data\nitem before requesting a lock on another data item, deadlocks may occur. There are\nways to avoid deadlock in some situations, as we shall see in Section 18.1.5. However,\nin general, deadlocks are a necessary evil associated with locking, if we want to avoid\ninconsistent states. Deadlocks are de\ufb01nitely preferable to inconsistent states, since they\ncan be handled by rolling back transactions, whereas inconsistent states may lead to\nreal-world problems that cannot be handled by the database system.\nWe shall require that each transaction in the system follow a set of rules, called a\nlocking protocol , indicating when a transaction may lock and unlock each of the data\nitems. Locking protocols restrict the numb er of possible schedules. The set of all such\nT4:lock-S (A);\nread (A);\nlock-S (B);\nread (B);\ndisplay (A+B);\nunlock (A);\nunlock (B).\nFigure 18.6 Transaction T4(transaction T2with unlocking delayed).\n", "868": "840 Chapter 18 Concurrency Control\nT3\n T4\nlock-X (B)\nread (B)\nB:=B\u221250\nwrite (B)\nlock-S (A)\nread (A)\nlock-S (B)\nlock-X (A)\nFigure 18.7 Schedule 2.\nschedules is a proper subset of all possibl e serializable schedules. We shall present\nseveral locking protocols that allow only con\ufb02ict-serializable schedules, and thereby\nensure isolation. Before doing so, we introduce some terminology.\nLet {T0,T1,\u2026,Tn}be a set of transactions participating in a schedule S.W es a y\nthat Tiprecedes TjinS,w r i t t e n Ti\u2192Tj,i ft h e r ee x i s t sad a t ai t e m Qsuch that Tihas\nheld lock mode AonQ,a n d Tjhas held lock mode BonQlater, and comp (A,B)=f a l s e .\nIfTi\u2192Tj, then that precedence implies that in any equivalent serial schedule, Timust\nappear before Tj. Observe that this graph is similar to the precedence graph that we\nused in Section 17.6 to test for con\ufb02ict serializability. Con\ufb02icts between instructions\ncorrespond to noncompatibility of lock modes.\nWe say that a schedule Sislegal under a given locking protocol if Sis a possible\nschedule for a set of transactions that follows the rules of the locking protocol. We say\nthat a locking protocol ensures con\ufb02ict serializability if and only if all legal schedules\nare con\ufb02ict serializable; in other words, for all legal schedules the associated \u2192relation\nis acyclic.\n18.1.2 Granting of Locks\nWhen a transaction requests a lock on a data item in a particular mode, and no other\ntransaction has a lock on the same data item in a con\ufb02icting mode, the lock can be\ngranted. However, care must be taken to avoid the following scenario. Suppose a trans-\naction T2has a shared-mode lock on a data item, and another transaction T1requests\nan exclusive-mode lock on the data item. T1has to wait for T2to release the shared-\nmode lock. Meanwhile, a transaction T3may request a shared-mode lock on the same\ndata item. The lock request is compatible with the lock granted to T2,s o T3may be\ngranted the shared-mode lock. At this point T2may release the lock, but still T1has\nto wait for T3to \ufb01nish. But again, there may be a new transaction T4that requests a\nshared-mode lock on the same data item, and is granted the lock before T3releases it.\nIn fact, it is possible that there is a sequence of transactions that each requests a shared-\nmode lock on the data item, and each transaction releases the lock a short while after it\n", "869": "18.1 Lock-Based Protocols 841\nis granted, but T1never gets the exclusive-mode lock on the data item. The transaction\nT1may never make progress, and is said to be starved .\nWe can avoid starvation of transactions by granting locks in the following manner:\nWhen a transaction Tir e q u e s t sal o c ko nad a t ai t e m Qin a particular mode M,t h e\nconcurrency-control manager grants the lock provided that:\n\u2022There is no other transaction holding a lock on Qin a mode that con\ufb02icts with M.\n\u2022There is no other transaction that is waiting for a lock on Qand that made its lock\nrequest before Ti.\nThus, a lock request will never get blocked by a lock request that is made later.\n18.1.3 The Two-Phase Locking Protocol\nOne protocol that ensures serializability is the two-phase locking protocol .T h i sp r o t o c o l\nrequires that each transaction issue lock and unlock requests in two phases:\n1. Growing phase . A transaction may obtain locks, but may not release any lock.\n2. Shrinking phase . A transaction may release locks, but may not obtain any new\nlocks.\nInitially, a transaction is in the growing phase. The transaction acquires locks as needed.\nOnce the transaction releases a lock, it enters the shrinking phase, and it can issue no\nmore lock requests.\nFor example, transactions T3and T4are two phase. On the other hand, transactions\nT1and T2are not two phase. Note that the unlock instructions do not need to appear\nat the end of the transaction. For example, in the case of transaction T3,w ec o u l d\nmove the unlock (B) instruction to just after the lock-X (A) instruction and still retain\nthe two-phase locking property.\nWe can show that the two-phase locking protocol ensures con\ufb02ict serializability.\nConsider any transaction. The point in the schedule where the transaction has obtained\nits \ufb01nal lock (the end of its growing phase) is called the lock point of the transaction.\nNow, transactions can be ordered according to their lock points\u2014this ordering is, in\nfact, a serializability ordering for the transactions. We leave the proof as an exercise for\ny o ut od o( s e eP r a c t i c eE x e r c i s e1 8 . 1 ) .\nTwo-phase locking does notensure freedom from deadlock. Observe that transac-\ntions T3and T4are two phase, but, in schedule 2 (Figure 18.7), they are deadlocked.\nRecall from Section 17.7.2 that, in addition to being serializable, schedules should\nbe cascadeless. Cascading rollback may occur under two-phase locking. As an illus-\ntration, consider the partial schedule of Figure 18.8. Each transaction observes the\ntwo-phase locking protocol, but the failure of T5after the read (A) step of T7leads to\ncascading rollback of T6and T7.\n", "870": "842 Chapter 18 Concurrency Control\nT5\n T6\n T7\nlock-X (A)\nread (A)\nlock-S (B)\nread (B)\nwrite (A)\nunlock (A)\nlock-X (A)\nread (A)\nwrite (A)\nunlock (A)\nlock-S (A)\nread (A)\nFigure 18.8 Partial schedule under two-phase locking.\nCascading rollbacks can be avoided by a modi\ufb01cation of two-phase locking called\nthestrict two-phase locking protocol .T h i sp r o t o c o lr e q u i r e sn o to n l yt h a tl o c k i n gb et w o\nphase, but also that all exclusive-mode locks taken by a transaction be held until that\ntransaction commits. This requirement ensures that any data written by an uncommit-\nted transaction are locked in exclusive mode until the transaction commits, preventing\nany other transaction from reading the data.\nAnother variant of two-phase locking is the rigorous two-phase locking protocol ,\nwhich requires that all locks be held until th e transaction commits. We can easily verify\nthat, with rigorous two-phase locking, tr ansactions can be serialized in the order in\nwhich they commit.\nConsider the following two transactions, for which we have shown only some of\nthe signi\ufb01cant read andwrite operations:\nT8:read (a1);\nread (a2);\n...\nread (an);\nwrite (a1).\nT9:read (a1);\nread (a2);\ndisplay (a1+a2).\nIf we employ the two-phase locking protocol, then T8must lock a1in exclusive\nmode. Therefore, any concurrent execution of both transactions amounts to a serial\nexecution. Notice, however, that T8needs an exclusive lock on a1only at the end of\n", "871": "18.1 Lock-Based Protocols 843\nT8\n T9\nlock-S (a1)\nlock-S (a1)\nlock-S (a2)\nlock-S (a2)\nlock-S (a3)\nlock-S (a4)\nunlock (a1)\nunlock (a2)\nlock-S (an)\nupgrade (a1)\nFigure 18.9 Incomplete schedule with a lock conversion.\nits execution, when it writes a1.T h u s ,i f T8could initially lock a1in shared mode, and\nthen could later change the lock to exclusi ve mode, we could get more concurrency,\nsince T8and T9could access a1and a2simultaneously.\nThis observation leads us to a re\ufb01nement of the basic two-phase locking protocol,\nin which lock conversions are allowed. We shall provide a mechanism for upgrading a\nshared lock to an exclusive lock, and downgrading an exclusive lock to a shared lock. We\ndenote conversion from shared to exclusive modes by upgrade ,a n df r o me x c l u s i v et o\nshared by downgrade . Lock conversion cannot be allowed arbitrarily. Rather, upgrading\ncan take place in only the growing phase, whereas downgrading can take place in only\nthe shrinking phase.\nReturning to our example, transactions T8and T9can run concurrently under the\nre\ufb01ned two-phase locking protocol, as shown in the incomplete schedule of Figure 18.9,\nwhere only some of the locking instructions are shown.\nNote that a transaction attempting to upgrade a lock on an item Qmay be forced\nto wait. This enforced wait occurs if Qis currently locked by another transaction in\nshared mode.\nJust like the basic two-phase locking protocol, two-phase locking with lock conver-\nsion generates only con\ufb02ict-serializable schedules, and transactions can be serialized\nby their lock points. Further, if exclusive locks are held until the end of the transaction,\nthe schedules are cascadeless.\nFor a set of transactions, there may be con\ufb02ict-serializable schedules that can-\nnot be obtained through the two-phase locking protocol. However, to obtain con\ufb02ict-\nserializable schedules through non-two-phase locking protocols, we need either to have\nadditional information about the transactions or to impose some structure or ordering\no nt h es e to fd a t ai t e m si nt h ed a t a b a s e .W es h a l ls e ee x a m p l e sw h e nw ec o n s i d e ro t h e r\nlocking protocols later in this chapter.\nStrict two-phase locking and rigorous two-phase locking (with lock conversions)\nare used extensively in commercial database systems.\n", "872": "844 Chapter 18 Concurrency Control\nA simple but widely used scheme automatically generates the appropriate lock and\nunlock instructions for a transaction, on the basis of read and write requests from the\ntransaction:\n\u2022When a transaction Tiissues a read (Q) operation, the system issues a lock-S (Q)\ninstruction followed by the read (Q) instruction.\n\u2022When Tiissues a write (Q) operation, the system checks to see whether Tialready\nholds a shared lock on Q. If it does, then the system issues an upgrade (Q)i n -\nstruction, followed by the write (Q) instruction. Otherwise, the system issues a\nlock-X (Q) instruction, followed by the write (Q) instruction.\n\u2022All locks obtained by a transaction are unlocked after that transaction commits or\naborts.\n18.1.4 Implementation of Locking\nAlock manager can be implemented as a process that receives messages from trans-\nactions and sends messages in reply. The lock-manager process replies to lock-request\nmessages with lock-grant messages, or with messages requesting rollback of the trans-\naction (in case of deadlocks). Unlock messages require only an acknowledgment in\nresponse, but may result in a grant message to another waiting transaction.\nThe lock manager uses this data structure: For each data item that is currently\nlocked, it maintains a linked list of records, one for each request, in the order in which\nthe requests arrived. It uses a hash table, indexed on the name of a data item, to \ufb01nd\nthe linked list (if any) for a data item; this table is called the lock table .E a c hr e c o r do f\nthe linked list for a data item notes which transaction made the request, and what lock\nmode it requested. The record also notes if the request has currently been granted.\nFigure 18.10 shows an example of a lock table. The table contains locks for \ufb01ve\ndi\ufb00erent data items, I4, I7, I23, I44, and I912. The lock table uses over\ufb02ow chaining,\nso there is a linked list of data items for each entry in the lock table. There is also a\nlist of transactions that have been granted locks, or are waiting for locks, for each of\nthe data items. Granted locks are the rectangles \ufb01lled in a darker shade, while waiting\nrequests are the rectangles \ufb01lled in a lighter shade. We have omitted the lock mode to\nkeep the \ufb01gure simple. It can be seen, for example, that T23 has been granted locks on\nI912 and I7 and is waiting for a lock on I4.\nAlthough the \ufb01gure does not show it, the lock table should also maintain an index\non transaction identi\ufb01ers so that it is possible to determine e\ufb03ciently the set of locks\nheld by a given transaction.\nThe lock manager processes requests this way:\n\u2022When a lock request message arrives, it adds a record to the end of the linked list\nfor the data item, if the linked list is present. Otherwise it creates a new linked list,\ncontaining only the record for the request.\n", "873": "18.1 Lock-Based Protocols 845\ngranted\nwaiting\nT8I44T1 T23I4T23I7 I23\nT23 T1 T8 T2\nI912\nFigure 18.10 Lock table.\nIt always grants a lock request on a data item that is not currently locked. But\nif the transaction requests a lock on an item on which a lock is currently held,\nthe lock manager grants the request only if it is compatible with the locks that are\ncurrently held, and all earlier requests have been granted already. Otherwise the\nrequest has to wait.\n\u2022When the lock manager receives an unlock message from a transaction, it deletes\nthe record for that data item in the linked list corresponding to that transaction. It\ntests the record that follows, if any, as described in the previous paragraph, to see\nif that request can now be granted. If it can, the lock manager grants that request\nand processes the record following it, if any, similarly, and so on.\n\u2022If a transaction aborts, the lock manager deletes any waiting request made by the\ntransaction. Once the database system has taken appropriate actions to undo the\ntransaction (see Section 19.3), it releases all locks held by the aborted transaction.\n", "874": "846 Chapter 18 Concurrency Control\nThis algorithm guarantees freedom from starvation for lock requests, since a re-\nquest can never be granted while a request received earlier is waiting to be granted.\nWe study how to detect and handle deadlocks later, in Section 18.2.2. Section 20.3.1\ndescribes an alternative implementation\u2014one that uses shared memory instead of mes-\nsage passing for lock request/grant.\n18.1.5 Graph-Based Protocols\nAs noted in Section 18.1.3, if we wish to develop protocols that are not two phase, we\nneed additional information on how each transaction will access the database. There\nare various models that can give us the additional information, each di\ufb00ering in the\namount of information provided. The simplest model requires that we have prior knowl-\nedge about the order in which the database items will be accessed. Given such infor-\nmation, it is possible to construct locking protocols that are not two phase, but that,\nnevertheless, ensure con\ufb02ict serializability.\nTo acquire such prior knowledge, we impose a partial ordering \u2192on the set\nD={d1,d2,\u2026,dh}of all data items. If di\u2192dj, then any transaction accessing both\ndiand djmust access dibefore accessing dj. This partial ordering may be the result of\neither the logical or the physical organization of the data, or it may be imposed solely\nfor the purpose of concurrency control.\nThe partial ordering implies that the set Dmay now be viewed as a directed acyclic\ngraph, called a database graph . In this section, for the sake of simplicity, we will restrict\nour attention to only those graphs that are rooted trees. We shall present a simple\nprotocol, called the tree protocol , which is restricted to employ only exclusive locks.\nReferences to other, more complex, graph-based locking protocols are in the online\nbibliographical notes.\nIn the tree protocol , the only lock instruction allowed is lock-X . Each transaction\nTican lock a data item at most once, and must observe the following rules:\n1.The \ufb01rst lock by Timay be on any data item.\n2.Subsequently, a data item Qcan be locked by Tionly if the parent of Qis currently\nlocked by Ti.\n3.Data items may be unlocked at any time.\n4.A data item that has been locked and unlocked by Ticannot subsequently be\nrelocked by Ti.\nAll schedules that are legal under the tree protocol are con\ufb02ict serializable.\nTo illustrate this protocol, consider the database graph of Figure 18.11. The follow-\ning four transactions follow the tree protocol on this graph. We show only the lock and\nunlock instructions:\n", "875": "18.1 Lock-Based Protocols 847\nA\nC B\nF\nE\nIH\nJD\nG\nFigure 18.11 Tree-structured database graph.\nT10:lock-X (B);lock-X (E);lock-X (D);unlock (B);unlock (E);lock-X (G);\nunlock (D);unlock (G).\nT11:lock-X (D);lock-X (H);unlock (D);unlock (H).\nT12:lock-X (B);lock-X (E);unlock (E);unlock (B).\nT13:lock-X (D);lock-X (H);unlock (D);unlock (H).\nO n ep o s s i b l es c h e d u l ei nw h i c ht h e s ef o u rt r a n s a c t i o n sp a r t i c i p a t e da p p e a r si n\nFigure 18.12. Note that, during its execution, transaction T10holds locks on two disjoint\nsubtrees.\nObserve that the schedule of Figure 18.12 is con\ufb02ict serializable. It can be shown\nnot only that the tree protocol ensures con\ufb02ict serializability, but also that this protocol\nensures freedom from deadlock.\nThe tree protocol in Figure 18.12 does not ensure recoverability and cascadeless-\nness. To ensure recoverability and cascadelessness, the protocol can be modi\ufb01ed to\nnot permit release of exclusive locks until the end of the transaction. Holding exclu-\nsive locks until the end of the transaction reduces concurrency. Here is an alternative\nthat improves concurrency, but ensures only recoverability: For each data item with\nan uncommitted write, we record which transaction performed the last write to the\ndata item. Whenever a transaction Tiperforms a read of an uncommitted data item,\nwe record a commit dependency ofTion the transaction that performed the last write\nto the data item. Transaction Tiis then not permitted to commit until the commit of all\ntransactions on which it has a commit dependency. If any of these transactions aborts,\nTimust also be aborted.\n", "876": "848 Chapter 18 Concurrency Control\nT10\n T11\n T12\n T13\nlock-X (B)\nlock-X (D)\nlock-X (H)\nunlock (D)\nlock-X (E)\nlock-X (D)\nunlock (B)\nunlock (E)\nlock-X (B)\nlock-X (E)\nunlock (H)\nlock-X (G)\nunlock (D)\nlock-X (D)\nlock-X (H)\nunlock (D)\nunlock (H)\nunlock (E)\nunlock (B)\nunlock (G)\nFigure 18.12 Serializable schedule under the tree protocol.\nThe tree-locking protocol has an advantage over the two-phase locking protocol\nin that, unlike two-phase locking, it is deadlock-free, so no rollbacks are required. The\ntree-locking protocol has another advantage over the two-phase locking protocol in that\nunlocking may occur earlier. Earlier unlocking may lead to shorter waiting times and\nto an increase in concurrency.\nHowever, the protocol has the disadvantage that, in some cases, a transaction may\nhave to lock data items that it does not access. For example, a transaction that needs\nto access data items Aand Jin the database graph of Figure 18.11 must lock not only\nAand J, but also data items B,D,a n d H. This additional locking results in increased\nlocking overhead, the possibility of additional waiting time, and a potential decrease\nin concurrency. Further, without prior knowledge of what data items will need to be\nlocked, transactions will have to lock the root of the tree, and that can reduce concur-\nrency greatly.\nFor a set of transactions, there may be con\ufb02ict-serializable schedules that cannot\nbe obtained through the tree protocol. Indeed, there are schedules possible under the\ntwo-phase locking protocol that are not possi b l eu n d e rt h et r e ep r o t o c o l ,a n dv i c ev e r s a .\nExamples of such schedules are explored in the exercises.\n", "877": "18.2 Deadlock Handling 849\n18.2 Deadlock Handling\nA system is in a deadlock state if there exists a set of transactions such that every\ntransaction in the set is waiting for another transaction in the set. More precisely, there\nexists a set of waiting transactions {T0,T1,\u2026,Tn}such that T0is waiting for a data\nitem that T1holds, and T1is waiting for a data item that T2holds, and \u2026,a n d Tn\u22121\nis waiting for a data item that Tnholds, and Tnis waiting for a data item that T0holds.\nNone of the transactions can make progress in such a situation.\nThe only remedy to this undesirable situation is for the system to invoke some\ndrastic action, such as rolling back some of the transactions involved in the deadlock.\nRollback of a transaction may be partial: That is, a transaction may be rolled back to\nthe point where it obtained a lock whose release resolves the deadlock.\nThere are two principal methods for dealing with the deadlock problem. We can\nuse a deadlock prevention protocol to ensure that the system will never enter a deadlock\nstate. Alternatively, we can allow the system to enter a deadlock state, and then try to\nrecover by using a deadlock detection anddeadlock recovery scheme. As we shall see,\nboth methods may result in transaction rollback. Prevention is commonly used if the\nprobability that the system would enter a deadlock state is relatively high; otherwise,\ndetection and recovery are more e\ufb03cient.\nNote that a detection and recovery scheme requires overhead that includes not\nonly the run-time cost of maintaining the necessary information and of executing the\ndetection algorithm, but also the potential losses inherent in recovery from a deadlock.\n18.2.1 Deadlock Prevention\nThere are two approaches to deadlock prevention. One approach ensures that no cyclic\nwaits can occur by ordering the requests for locks, or requiring all locks to be acquired\ntogether. The other approach is closer to deadlock recovery, and it performs transaction\nrollback instead of waiting for a lock whenever the wait could potentially result in a\ndeadlock.\nThe simplest scheme under the \ufb01rst approach requires that each transaction locks\nall its data items before it begins execution. Moreover, either all are locked in one step\nor none are locked. There are two main disadvantages to this protocol: (1) it is often\nhard to predict, before the transaction begins, what data items need to be locked; (2)\ndata-item utilization may be very low, since many of the data items may be locked but\nunused for a long time.\nAnother approach for preventing deadlocks is to impose an ordering of all data\nitems and to require that a transaction lock data items only in a sequence consistent\nwith the ordering. We have seen one such scheme in the tree protocol, which uses a\npartial ordering of data items.\nA variation of this approach is to use a total order of data items, in conjunction with\ntwo-phase locking. Once a transaction has locked a particular item, it cannot request\nlocks on items that precede that item in the ordering. This scheme is easy to implement,\n", "878": "850 Chapter 18 Concurrency Control\nas long as the set of data items accessed by a transaction is known when the transaction\nstarts execution. There is no need to change the underlying concurrency-control system\nif two-phase locking is used: All that is needed is to ensure that locks are requested in\nthe right order.\nThe second approach for preventing deadlocks is to use preemption and transac-\ntion rollbacks. In preemption, when a transaction Tjrequests a lock that transaction\nTiholds, the lock granted to Timay be preempted by rolling back of Ti, and granting\nof the lock to Tj. To control the preemption, we assign a unique timestamp, based on\na counter or on the system clock, to each transaction when it begins. The system uses\nthese timestamps only to decide whether a transaction should wait or roll back. Lock-\ning is still used for concurrency control. If a transaction is rolled back, it retains its\noldtimestamp when restarted. Two di\ufb00erent deadlock-prevention schemes using times-\ntamps have been proposed:\n1.Thewait\u2013die scheme is a nonpreemptive technique. When transaction Tirequests\nad a t ai t e mc u r r e n t l yh e l db y Tj,Tiis allowed to wait only if it has a timestamp\nsmaller than that of Tj(i.e., Tiis older than Tj). Otherwise, Tiis rolled back (dies).\nFor example, suppose that transactions T14,T15,a n d T16have timestamps 5,\n10, and 15, respectively. If T14requests a data item held by T15,t h e n T14will wait.\nIfT16requests a data item held by T15,t h e n T16will be rolled back.\n2.Thewound\u2013wait scheme is a preemptive technique. It is a counterpart to the wait\u2013\ndie scheme. When transaction Tirequests a data item currently held by Tj,Tiis\nallowed to wait only if it has a timestamp larger than that of Tj(i.e., Tiis younger\nthan Tj). Otherwise, Tjis rolled back ( Tjiswounded byTi).\nReturning to our example, with transactions T14,T15,a n d T16,i fT14requests\nad a t ai t e mh e l db y T15, then the data item will be preempted from T15,a n d T15\nwill be rolled back. If T16requests a data item held by T15,t h e n T16will wait.\nThe major problem with both of these schemes is that unnecessary rollbacks may\noccur.\nAnother simple approach to deadlock prevention is based on lock timeouts .I nt h i s\napproach, a transaction that has requested a lock waits for at most a speci\ufb01ed amount\nof time. If the lock has not been granted within that time, the transaction is said to time\nout, and it rolls itself back and restarts. If there was in fact a deadlock, one or more\ntransactions involved in the deadlock will time out and roll back, allowing the others to\nproceed. This scheme falls somewhere between deadlock prevention, where a deadlock\nwill never occur, and deadlock detection and recovery, which Section 18.2.2 discusses.\nThe timeout scheme is particularly easy to implement, and it works well if transac-\ntions are short and if long waits are likely to be due to deadlocks. However, in general\nit is hard to decide how long a transaction must wait before timing out. Too long a wait\nresults in unnecessary delays once a deadlock has occurred. Too short a wait results\nin transaction rollback even when there is no deadlock, leading to wasted resources.\n", "879": "18.2 Deadlock Handling 851\nStarvation is also a possibility with this scheme. Hence, the timeout-based scheme has\nlimited applicability.\n18.2.2 Deadlock Detection and Recovery\nIf a system does not employ some protocol that ensures deadlock freedom, then a\ndetection and recovery scheme must be used. An algorithm that examines the state\nof the system is invoked periodically to determine whether a deadlock has occurred.\nIf one has, then the system must attempt to recover from the deadlock. To do so, the\nsystem must:\n\u2022Maintain information about the current allocation of data items to transactions,\nas well as any outstanding data item requests.\n\u2022Provide an algorithm that uses this information to determine whether the system\nhas entered a deadlock state.\n\u2022Recover from the deadlock when the detection algorithm determines that a dead-\nlock exists.\nIn this section, we elaborate on these issues.\n18.2.2.1 Deadlock Detection\nDeadlocks can be described precisely in terms of a directed graph called a wait-for\ngraph . This graph consists of a pair G=(V,E), where Vis a set of vertices and Eis\na set of edges. The set of vertices consists of all the transactions in the system. Each\nelement in the set Eof edges is an ordered pair Ti\u2192Tj.I fTi\u2192Tjis in E, then there\nis a directed edge from transaction TitoTj, implying that transaction Tiis waiting for\ntransaction Tjto release a data item that it needs.\nWhen transaction Tirequests a data item currently being held by transaction Tj,\nthen the edge Ti\u2192 Tjis inserted in the wait-for graph. This edge is removed only\nwhen transaction Tjis no longer holding a data item needed by transaction Ti.\nA deadlock exists in the system if and only if the wait-for graph contains a cycle.\nEach transaction involved in the cycle is said to be deadlocked. To detect deadlocks, the\nsystem needs to maintain the wait-for graph, and periodically to invoke an algorithm\nthat searches for a cycle in the graph.\nTo illustrate these concepts, consider the wait-for graph in Figure 18.13, which\ndepicts the following situation:\n\u2022Transaction T17is waiting for transactions T18and T19.\n\u2022Transaction T19is waiting for transaction T18.\n\u2022Transaction T18is waiting for transaction T20.\n", "880": "852 Chapter 18 Concurrency Control\nT18 T20\nT17\nT19\nFigure 18.13 Wait-for graph with no cycle.\nSince the graph has no cycle, the system is not in a deadlock state.\nSuppose now that transaction T20is requesting an item held by T19.T h ee d g e T20\u2192\nT19is added to the wait-for graph, resulting in the new system state in Figure 18.14. This\ntime, the graph contains the cycle:\nT18\u2192T20\u2192T19\u2192T18\nimplying that transactions T18,T19,a n d T20are all deadlocked.\nConsequently, the question arises: When should we invoke the detection algo-\nrithm? The answer depends on two factors:\n1.How often does a deadlock occur?\n2.How many transactions will be a\ufb00ected by the deadlock?\nIf deadlocks occur frequently, then the detection algorithm should be invoked more\nfrequently. Data items allocated to deadlocked transactions will be unavailable to other\ntransactions until the deadlock can be broken. In addition, the number of cycles in the\ngraph may also grow. In the worst case, we would invoke the detection algorithm every\ntime a request for allocation could not be granted immediately.\nT18 T20\nT17\nT19\nFigure 18.14 Wait-for graph with a cycle.\n", "881": "18.3 Multiple Granularity 853\n18.2.2.2 Recovery from Deadlock\nWhen a detection algorithm determines that a deadlock exists, the system must recover\nfrom the deadlock. The most common solution is to roll back one or more transactions\nto break the deadlock. Three actions need to be taken:\n1.Selection of a victim . Given a set of deadlocked transactions, we must determine\nwhich transaction (or transactions) to roll back to break the deadlock. We should\nroll back those transactions that will incur the minimum cost. Unfortunately, the\nterm minimum cost is not a precise one. Many factors may determine the cost of\na rollback, including:\na. How long the transaction has computed, and how much longer the trans-\naction will compute before it completes its designated task.\nb. How many data items the transaction has used.\nc. How many more data items the transaction needs for it to complete.\nd. How many transactions will be involved in the rollback.\n2.Rollback . Once we have decided that a particular transaction must be rolled back,\nwe must determine how far this transaction should be rolled back.\nThe simplest solution is a total rollback : Abort the transaction and then restart\nit. However, it is more e\ufb00ective to roll back the transaction only as far as necessary\nto break the deadlock. Such partial rollback requires the system to maintain ad-\nditional information about the state of all the running transactions. Speci\ufb01cally,\nthe sequence of lock requests/grants and updates performed by the transaction\nneeds to be recorded. The deadlock detection mechanism should decide which\nlocks the selected transaction needs to release in order to break the deadlock. The\nselected transaction must be rolled back to the point where it obtained the \ufb01rst\nof these locks, undoing all actions it took after that point. The recovery mech-\nanism must be capable of performing such partial rollbacks. Furthermore, the\ntransactions must be capable of resuming execution after a partial rollback. See\nthe online bibliographical notes for relevant references.\n3.Starvation . In a system where the selection of victims is based primarily on cost\nfactors, it may happen that the same transaction is always picked as a victim.\nAs a result, this transaction never completes its designated task, thus there is\nstarvation . We must ensure that a transaction can be picked as a victim only\na (small) \ufb01nite number of times. The most common solution is to include the\nnumber of rollbacks in the cost factor.\n18.3 Multiple Granularity\nIn the concurrency-control schemes described thus far, we have used each individual\ndata item as the unit on which synchronization is performed.\n", "882": "854 Chapter 18 Concurrency Control\nThere are circumstances, however, where it would be advantageous to group several\ndata items, and to treat them as one individual synchronization unit. For example, if a\ntransaction Tineeds to access an entire relation, and a locking protocol is used to lock\ntuples, then Timust lock each tuple in the relation. Clearly, acquiring many such locks\nis time-consuming; even worse, the lock table may become very large and no longer \ufb01t\nin memory. It would be better if Ticould issue a single lock request to lock the entire\nrelation. On the other hand, if transaction Tjneeds to access only a few tuples, it should\nnot be required to lock the entire relation, since otherwise concurrency is lost.\nWhat is needed is a mechanism to allow the system to de\ufb01ne multiple levels of\ngranularity . This is done by allowing data items to be of various sizes and de\ufb01ning a\nhierarchy of data granularities, where the small granularities are nested within larger\nones. Such a hierarchy can be represented graphically as a tree. Note that the tree that\nwe describe here is signi\ufb01cantly di\ufb00erent from that used by the tree protocol (Section\n18.1.5). A nonleaf node of the multiple-granularity tree represents the data associated\nwith its descendants. In the tree protocol, each node is an independent data item.\nAs an illustration, consider the tree of Figure 18.15, which consists of four levels\nof nodes. The highest level represents the entire database. Below it are nodes of type\narea; the database consists of exactly these areas. Each area in turn has nodes of type\n\ufb01leas its children. Each area contains exactly those \ufb01les that are its child nodes. No \ufb01le\nis in more than one area. Finally, each \ufb01le has nodes of type record . As before, the \ufb01le\nconsists of exactly those records that are its child nodes, and no record can be present\nin more than one \ufb01le.\nEach node in the tree can be locked individually. As we did in the two-phase locking\nprotocol, we shall use shared andexclusive lock modes. When a transaction locks a\nnode, in either shared or exclusive mode, the transaction also has implicitly locked all\nthe descendants of that node in the same lock mode. For example, if transaction Ti\ngets an explicit lock on \ufb01le Fcof Figure 18.15, in exclusive mode, then it has an implicit\nra1ra2ranrb1rbkrc1rcmFa Fb FcA1 A2DB\n. . .                             . . .                                 . . .           \nFigure 18.15 Granularity hierarchy.\n", "883": "18.3 Multiple Granularity 855\nlockin exclusive mode on all the records belonging to that \ufb01le. It does not need to lock\nthe individual records of Fcexplicitly.\nSuppose that transaction Tjwishes to lock record rb6of \ufb01le Fb.S i n c e Tihas locked\nFbexplicitly, it follows that rb6is also locked (implicitly). But, when Tjissues a lock\nrequest for rb6,rb6is not explicitly locked! How does the system determine whether Tj\ncan lock rb6?Tjm u s tt r a v e r s et h et r e ef r o mt h er o o tt or e c o r d rb6.I fa n yn o d ei nt h a t\npath is locked in an incompatible mode, then Tjmust be delayed.\nSuppose now that transaction Tkwishes to lock the entire database. To do so, it\nsimply must lock the root of the hierarchy. Note, however, that Tkshould not succeed\nin locking the root node, since Tiis currently holding a lock on part of the tree (speci\ufb01-\ncally, on \ufb01le Fb). But how does the system determine if the root node can be locked? One\npossibility is for it to search the entire tree. This solution, however, defeats the whole\npurpose of the multiple-granularity locking scheme. A more e\ufb03cient way to gain this\nknowledge is to introduce a new class of lock modes, called intention lock modes .I fa\nnode is locked in an intention mode, explicit locking is done at a lower level of the tree\n(that is, at a \ufb01ner granularity). Intention locks are put on all the ancestors of a node\nbefore that node is locked explicitly. Thus, a transaction does not need to search the\nentire tree to determine whether it can lock a node successfully. A transaction wish-\ning to lock a node\u2014say, Q\u2014must traverse a path in the tree from the root to Q. While\ntraversing the tree, the transaction locks the various nodes in an intention mode.\nThere is an intention mode associated with shared mode, and there is one with\nexclusive mode. If a node is locked in intention-shared (IS) mode , explicit locking is\nbeing done at a lower level of the tree, but with only shared-mode locks. Similarly, if\na node is locked in intention-exclusive (IX) mode , then explicit locking is being done\nat a lower level, with exclusive-mode or shared-mode locks. Finally, if a node is locked\ninshared and intention-exclusive (SIX) mode , the subtree rooted by that node is locked\nexplicitly in shared mode, and that explicit locking is being done at a lower level with\nexclusive-mode locks. The compatibility function for these lock modes is shown in\nFigure 18.16.\nIS IX S SIX X\nIS\nIX\nS\nSIX\nXtrue\ntrue\ntrue\ntrue\nfalsetrue\ntrue\nfalse\nfalse\nfalsetrue\nfalse\ntrue\nfalse\nfalsetrue\nfalse\nfalse\nfalse\nfalsefalse\nfalse\nfalse\nfalse\nfalse\nFigure 18.16 Compatibility matrix.\n", "884": "856 Chapter 18 Concurrency Control\nThemultiple-granularity locking protocol uses these lock modes to ensure serializ-\nability. It requires that a transaction Tithat attempts to lock a node Qmust follow these\nrules:\n\u2022Transaction Timust observe the lock-compatibility function of Figure 18.16.\n\u2022Transaction Timust lock the root of the tree \ufb01rst and can lock it in any mode.\n\u2022Transaction Tican lock a node Qin S or ISmode only if Ticurrently has the parent\nofQlocked in either IXorISmode.\n\u2022Transaction Tican lock a node Qin X, SIX,o rIXmode only if Ticurrently has the\nparent of Qlocked in either IXorSIXmode.\n\u2022Transaction Tican lock a node only if Tihas not previously unlocked any node\n(i.e., Tiis two phase).\n\u2022Transaction Tican unlock a node Qonly if Ticurrently has none of the children\nofQlocked.\nObserve that the multiple-granularity protocol requires that locks be acquired in top-\ndown (root-to-leaf) order, whereas locks must be released in bottom-up (leaf-to-root)\norder. Deadlock is possible in the multiple-granularity protocol, as it is in the two-phase\nlocking protocol.\nAs an illustration of the protocol, consider the tree of Figure 18.15 and these trans-\nactions:\n\u2022Suppose that transaction T21reads record ra2in \ufb01le Fa. Then, T21needs to lock\nthe database, area A1,a n d FainISmode (and in that order), and \ufb01nally to lock ra2in S mode.\n\u2022Suppose that transaction T22modi\ufb01es record ra9in \ufb01le Fa. Then, T22needs to lock\nthe database, area A1, and \ufb01le Fa(and in that order) in IXmode, and \ufb01nally to lock\nra9in X mode.\n\u2022Suppose that transaction T23reads all the records in \ufb01le Fa. Then, T23needs to\nlock the database and area A1(and in that order) in ISmode, and \ufb01nally to lock\nFain S mode.\n\u2022Suppose that transaction T24reads the entire database. It can do so after locking\nthe database in S mode.\nWe note that transactions T21,T23,a n d T24can access the database concurrently. Trans-\naction T22can execute concurrently with T21, but not with either T23orT24.\nThis protocol enhances concurrency and reduces lock overhead. It is particularly\nuseful in applications that include a mix of:\n\u2022Short transactions that access only a few data items.\n\u2022Long transactions that produce reports from an entire \ufb01le or set of \ufb01les.\n", "885": "18.4 Insert Operations, Delete Operations, and Predicate Reads 857\nThe number of locks that an SQL query may need to acquire can usually be esti-\nmated based on the relation scan operations performed by a query. A relation scan, for\nexample, would acquire a lock at a relation level, while an index scan that is expected to\nfetch only a few records may acquire an intention lock at the relation level and regular\nlocks at the tuple level. In case the a transac tion acquires a large number of tuple locks,\nthe lock table may become overfull. To deal with this situation, the lock manager may\nperform lock escalation , replacing many lower level locks by a single higher level lock;\nin our example, a single relation lock could replace a large number of tuple locks.\n18.4 Insert Operations, Delete Operations, and Predicate Reads\nU n t i ln o w ,w eh a v er e s t r i c t e do u ra t t e n t i o nt o read andwrite operations. This restric-\ntion limits transactions to data items alread y in the database. Some transactions require\nnot only access to existing data items, but also the ability to create new data items. Oth-\ners require the ability to delete data items. To examine how such transactions a\ufb00ect\nconcurrency control, we introduce these additional operations:\n\u2022delete (Q) deletes data item Qfrom the database.\n\u2022insert (Q) inserts a new data item Qinto the database and assigns Qan initial value.\nAn attempt by a transaction Tito perform a read (Q)o p e r a t i o na f t e r Qhas been deleted\nresults in a logical error in Ti. Likewise, an attempt by a transaction Tito perform a\nread (Q)o p e r a t i o nb e f o r e Qhas been inserted results in a logical error in Ti.I ti sa l s o\na logical error to attempt to delete a nonexistent data item.\n18.4.1 Deletion\nTo understand how the presence of delete instructions a\ufb00ects concurrency control, we\nmust decide when a delete instruction con\ufb02icts with another instruction. Let Iiand\nIjbe instructions of Tiand Tj, respectively, that appear in schedule Sin consecutive\norder. Let Ii=delete (Q). We consider several instructions Ij.\n\u2022Ij=read (Q).Iiand Ijcon\ufb02ict. If Iicomes before Ij,Tjwill have a logical error. If\nIjcomes before Ii,Tjcan execute the read operation successfully.\n\u2022Ij=write (Q).Iiand Ijcon\ufb02ict. If Iicomes before Ij,Tjwill have a logical error. If\nIjcomes before Ii,Tjcan execute the write operation successfully.\n\u2022Ij=delete (Q).Iiand Ijcon\ufb02ict. If Iicomes before Ij,Tjwill have a logical error. If\nIjcomes before Ii,Tiwill have a logical error.\n\u2022Ij=insert (Q).Iiand Ijcon\ufb02ict. Suppose that data item Qdid not exist prior to\nthe execution of Iiand Ij. Then, if Iicomes before Ij, a logical error results for Ti.\n", "886": "858 Chapter 18 Concurrency Control\nIfIjcomes before Ii, then no logical error results. Likewise, if Qexisted prior to\nthe execution of Iiand Ij, then a logical error results if Ijcomes before Ii,b u tn o t\notherwise.\nWe can conclude the following:\n\u2022Under the two-phase locking protocol, an exclusive lock is required on a data item\nbefore that item can be deleted.\n\u2022Under the timestamp-ordering protocol, a test similar to that for a write must be\nperformed. Suppose that transaction Tiissues delete (Q).\n\u00b0If TS( Ti)<R-timestamp( Q), then the value of Qthat Tiwas to delete has al-\nready been read by a transaction Tjwith TS( Tj)>TS(Ti). Hence, the delete\noperation is rejected, and Tiis rolled back.\n\u00b0If TS( Ti)<W-timestamp( Q), then a transaction Tjwith TS( Tj)>TS(Ti)h a s\nwritten Q.H e n c e ,t h i s delete operation is rejected, and Tiis rolled back.\n\u00b0Otherwise, the delete is executed.\n18.4.2 Insertion\nWe have already seen that an insert (Q) operation con\ufb02icts with a delete (Q)o p e r a t i o n .\nSimilarly, insert (Q) con\ufb02icts with a read (Q) operation or a write (Q)o p e r a t i o n ;n o\nread orwrite can be performed on a data item before it exists.\nSince an insert (Q) assigns a value to data item Q,a ninsert is treated similarly to a\nwrite for concurrency-control purposes:\n\u2022Under the two-phase locking protocol, if Tiperforms an insert (Q)o p e r a t i o n , Tiis\ngiven an exclusive lock on the newly created data item Q.\n\u2022Under the timestamp-ordering protocol, if Tiperforms an insert (Q)o p e r a t i o n ,t h e\nvalues R-timestamp( Q)a n dW - t i m e s t a m p ( Q)a r es e tt oT S ( Ti).\n18.4.3 Predicate Reads and The Phantom Phenomenon\nConsider transaction T30that executes the following SQL query on the university\ndatabase:\nselect count (*)\nfrom instructor\nwhere dept\n name =' P h y s i c s ';\nTransaction T30requires access to all tuples of the instructor relation pertaining to the\nPhysics department.\n", "887": "18.4 Insert Operations, Delete Operations, and Predicate Reads 859\nLet T31be a transaction that executes the following SQL insertion:\ninsert into instructor\nvalues (11111, 'Feynman', 'Physics', 9 4000);\nLet Sbe a schedule involving T30and T31. We expect there to be potential for a\ncon\ufb02ict for the following reasons:\n\u2022IfT30uses the tuple newly inserted by T31in computing count (*), then T30reads\nav a l u ew r i t t e nb y T31. Thus, in a serial schedule equivalent to S,T31must come\nbefore T30.\n\u2022IfT30does not use the tuple newly inserted by T31in computing count (*), then in\na serial schedule equivalent to S,T30must come before T31.\nThe second of these two cases is curious. T30and T31do not access any tuple in com-\nmon, yet they con\ufb02ict with each other! In e\ufb00ect, T30and T31con\ufb02ict on a phantom\ntuple. If concurrency control is performed at the tuple granularity, this con\ufb02ict would\ngo undetected. As a result, the system could fail to prevent a nonserializable schedule.\nThis problem is an instance of the phantom phenomenon .\nPhantom phenomena can occur not just with inserts, but also with updates. Con-\nsider the situation we saw in Section 17.10, where a transaction Tiused an index to\n\ufb01nd only tuples with dept\n name = \u201cPhysics\u201d, and as a result did not read any tuples\nwith other department names. If another transaction Tjupdates one of these tuples,\nchanging its department name to Physics, a problem similar to the above problem oc-\ncurs: even though Tiand Tjhave not accessed any tuples in common, they do con\ufb02ict\nwith each other. This problem too is an instance of the phantom phenomenon. In gen-\neral, the phantom phenomenon is rooted in predicate reads that con\ufb02ict with inserts\nor updates that result in new/updated tuples that satisfy the predicate.\nWe can prevent these problems by allowing transaction T30to prevent other trans-\nactions from creating new tuples in the instructor relation with dept\n name =\u201c P h y s i c s \u201d ,\nand from updating the department name of an existing instructor tuple to Physics.\nTo \ufb01nd all instructor tuples with dept\n name =\u201c P h y s i c s \u201d , T30must search either the\nwhole instructor relation, or at least an index on the relation. Up to now, we have as-\nsumed implicitly that the only data items accessed by a transaction are tuples. However,\nT30is an example of a transaction that reads information about what tuples are in a\nrelation, and T31is an example of a transaction that updates that information.\nClearly, it is not su\ufb03cient merely to lock the tuples that are accessed; the informa-\ntion used to \ufb01nd the tuples that are accessed by the transaction must also be locked.\nLocking of information used to \ufb01nd tuples can be implemented by associating a\ndata item with the relation; the data item represents the information used to \ufb01nd the\ntuples in the relation. Transactions, such as T30, that read the information about what\ntuples are in a relation would then have to lock the data item corresponding to the\n", "888": "860 Chapter 18 Concurrency Control\nrelation in shared mode. Transactions, such as T31, that update the information about\nwhat tuples are in a relation would have to lock the data item in exclusive mode. Thus,\nT30and T31would con\ufb02ict on a real data item, rather than on a phantom. Similarly,\ntransactions that use an index to retrieve tuples must lock the index itself.\nDo not confuse the locking of an entire relation, as in multiple-granularity locking,\nwith the locking of the data item corresponding to the relation. By locking the data item,\na transaction only prevents other transactions from updating information about what\ntuples are in the relation. Locking is still required on tuples. A transaction that directly\naccesses a tuple can be granted a lock on the tuples even when another transaction has\nan exclusive lock on the data item corresponding to the relation itself.\nThe major disadvantage of locking a data item corresponding to the relation, or\nlocking an entire index, is the low degree of concurrency\u2014 two transactions that insert\ndi\ufb00erent tuples into a relation are prevented from executing concurrently.\nAb e t t e rs o l u t i o ni sa n index-locking technique that avoids locking the whole index.\nAny transaction that inserts a tuple into a relation must insert information into every\nindex maintained on the relation. We eliminate the phantom phenomenon by imposing\na locking protocol for indices. For simplicity we shall consider only B+-tree indices.\nAs we saw in Chapter 14, every search-key value is associated with an index leaf\nnode. A query will usually use one or more indices to access a relation. An insert\nmust insert the new tuple in all indices on the relation. In our example, we assume\nthat there is an index on instructor for attribute dept\n name . Then, T31must modify the\nleaf containing the key \u201cPhysics\u201d. If T30reads the same leaf node to locate all tuples\npertaining to the Physics department, then T30and T31con\ufb02ict on that leaf node.\nTheindex-locking protocol takes advantage of the availability of indices on a rela-\ntion, by turning instances of the phantom phenomenon into con\ufb02icts on locks on index\nleaf nodes. The protocol operates as follows:\n\u2022Every relation must have at least one index.\n\u2022At r a n s a c t i o n Tican access tuples of a relation only after \ufb01rst \ufb01nding them through\none or more of the indices on the relation. For the purpose of the index-locking\nprotocol, a relation scan is treated as a scan through all the leaves of one of the\nindices.\n\u2022At r a n s a c t i o n Tithat performs a lookup (whether a range lookup or a point\nlookup) must acquire a shared lock on all the index leaf nodes that it accesses.\n\u2022At r a n s a c t i o n Timay not insert, delete, or update a tuple tiin a relation rwithout\nupdating all indices on r. The transaction must obtain exclusive locks on all index\nleaf nodes that are a\ufb00ected by the insertion, deletion, or update. For insertion\nand deletion, the leaf nodes a\ufb00ected are those that contain (after insertion) or\ncontained (before deletion) the search-key value of the tuple. For updates, the leaf\nnodes a\ufb00ected are those that (before the modi\ufb01cation) contained the old value of\nthe search key, and nodes that (after the modi\ufb01cation) contain the new value of\nthe search key.\n", "889": "18.5 Timestamp-Based Protocols 861\n\u2022Locks are obtained on tuples as usual.\n\u2022T h er u l e so ft h et w o - p h a s el o c k i n gp r o t o c o lm u s tb eo b s e r v e d .\nNote that the index-locking protocol does not address concurrency control on inter-\nnal nodes of an index; techniques for concurrency control on indices, which minimize\nlock con\ufb02icts, are presented in Section 18.10.2.\nLocking an index leaf node prevents any update to the node, even if the update\ndid not actually con\ufb02ict with the predicate. A variant called key-value locking, which\nminimizes such false lock con\ufb02icts, is presented in Section 18.10.2 as part of index\nconcurrency control.\nAs noted in Section 17.10, it would appear that the existence of a con\ufb02ict between\ntransactions depends on a low-level query-processing decision by the system that is\nunrelated to a user-level view of the meaning of the two transactions. An alternative\napproach to concurrency control acquires s hared locks on predicates in a query, such\nas the predicate \u201c salary >90000\u201d on the instructor relation. Inserts and deletes of the\nrelation must then be checked to see if they satisfy the predicate; if they do, there is a\nlock con\ufb02ict, forcing the insert or delete to wait till the predicate lock is released. For\nupdates, both the initial value and the \ufb01nal value of the tuple must be checked against\nthe predicate. Such con\ufb02icting inserts, deletes, and updates a\ufb00ect the set of tuples se-\nlected by the predicate, and they cannot be allowed to execute concurrently with the\nquery that acquired the (shared) predicate lock. We call this protocol predicate lock-\ning;1predicate locking is not used in practice since it is more expensive to implement\nthan the index-locking protocol and does not give signi\ufb01cant additional bene\ufb01ts.\n18.5 Timestamp-Based Protocols\nThe locking protocols that we have described thus far determine the order between ev-\nery pair of con\ufb02icting transactions at execution time by the \ufb01rst lock that both members\nof the pair request that involves incompatible modes. Another method for determining\nthe serializability order is to select an ordering among transactions in advance. The\nmost common method for doing so is to use a timestamp-ordering scheme.\n18.5.1 Timestamps\nWith each transaction Tiin the system, we associate a unique \ufb01xed timestamp, denoted\nby TS( Ti). This timestamp is assigned by the database system before the transaction\nTistarts execution. If a transaction Tihas been assigned timestamp TS( Ti), and a new\ntransaction Tjenters the system, then TS( Ti)<TS(Tj) .T h e r ea r et w os i m p l em e t h o d s\nfor implementing this scheme:\n1The term predicate locking was used for a version of the protocol that used shared and exclusive locks on predicates,\nand was thus more complicated. The version we present here, with only shared locks on predicates, is also referred to\nasprecision locking .\n", "890": "862 Chapter 18 Concurrency Control\n1.U s et h ev a l u eo ft h e system clock as the timestamp; that is, a transaction\u2019s time-\nstamp is equal to the value of the clock when the transaction enters the system.\n2.Use a logical counter that is incremented after a new timestamp has been as-\nsigned; that is, a transaction\u2019s timestamp is equal to the value of the counter\nwhen the transaction enters the system.\nThe timestamps of the transactions determine the serializability order. Thus, if\nTS(Ti)<TS(Tj), then the system must ensure that the produced schedule is equivalent\nto a serial schedule in which transaction Tiappears before transaction Tj.\nTo implement this scheme, we associate with each data item Qtwo timestamp\nvalues:\n1. W-timestamp (Q) denotes the largest timestamp of any transaction that executed\nwrite (Q) successfully.\n2. R-timestamp (Q) denotes the largest timestamp of any transaction that executed\nread (Q) successfully.\nThese timestamps are updated whenever a new read (Q)o r write (Q) instruction is\nexecuted.\n18.5.2 The Timestamp-Ordering Protocol\nThetimestamp-ordering protocol ensures that any con\ufb02icting read andwrite operations\nare executed in timestamp order. This protocol operates as follows:\n\u2022Suppose that transaction Tiissues read (Q).\n\u00b0If TS( Ti)<W-timestamp( Q), then Tineeds to read a value of Qthat was already\noverwritten. Hence, the read operation is rejected, and Tiis rolled back.\n\u00b0If TS( Ti)\u2265W-timestamp( Q), then the read operation is executed, and R-\ntimestamp( Q) is set to the maximum of R-timestamp( Q)a n dT S ( Ti).\n\u2022Suppose that transaction Tiissues write (Q).\n\u00b0If TS( Ti)<R-timestamp( Q), then the value of Qthat Tiis producing was\nneeded previously, and the system assumed that that value would never be pro-\nduced. Hence, the system rejects the write operation and rolls Tiback.\n\u00b0If TS( Ti)<W-timestamp( Q), then Tiis attempting to write an obsolete value\nofQ. Hence, the system rejects this write operation and rolls Tiback.\n\u00b0Otherwise, the system executes the write operation and sets W-time-\nstamp( Q)t oT S ( Ti).\n", "891": "18.5 Timestamp-Based Protocols 863\nIf a transaction Tiis rolled back by the concurrency-control scheme as result of issuance\nof either a read orwrite operation, the system assigns it a new timestamp and restarts\nit.\nTo illustrate this protocol, we consider transactions T25and T26.T r a n s a c t i o n T25\ndisplays the contents of accounts Aand B:\nT25:read (B);\nread (A);\ndisplay (A+B).\nTransaction T26transfers $50 from account Bto account A, and then displays the con-\ntents of both:\nT26:read (B);\nB:=B\u221250;\nwrite (B);\nread (A);\nA:=A+ 50;\nwrite (A);\ndisplay (A+B).\nIn presenting schedules under the timestamp protocol, we shall assume that a transac-\ntion is assigned a timestamp immediately before its \ufb01rst instruction. Thus, in schedule\n3o fF i g u r e1 8 . 1 7 ,T S ( T25)<TS(T26), and the schedule is possible under the timestamp\nprotocol.\nWe note that the preceding execution can also be produced by the two-phase lock-\ning protocol. There are, however, schedules that are possible under the two-phase lock-\ning protocol, but are not possible under the timestamp protocol, and vice versa (see\nExercise 18.27).\nThe timestamp-ordering protocol ensures con\ufb02ict serializability. This is because\ncon\ufb02icting operations are processed in timestamp order.\nThe protocol ensures freedom from deadlock, since no transaction ever waits. How-\never, there is a possibility of starvation of long transactions if a sequence of con\ufb02ict-\ning short transactions causes repeated restarting of the long transaction. If a transac-\ntion is su\ufb00ering from repeated restarts, co n\ufb02icting transactions need to be temporarily\nblocked to enable the transaction to \ufb01nish.\nThe protocol can generate schedules that are not recoverable. However, it can be\nextended to make the schedules recoverable, in one of several ways:\n\u2022Recoverability and cascadelessness can be ensured by performing all writes to-\ngether at the end of the transaction. The writes must be atomic in the following\nsense: While the writes are in progress, no transaction is permitted to access any\nof the data items that have been written.\n", "892": "864 Chapter 18 Concurrency Control\nT25\n T26\nread (B)\nread (B)\nB:=B\u221250\nwrite (B)\nread (A)\nread (A)\ndisplay (A+B )\nA:=A+50\nwrite (A)\ndisplay (A+B )\nFigure 18.17 Schedule 3.\n\u2022Recoverability and cascadelessness can also be guaranteed by using a limited form\nof locking, whereby reads of uncommitted items are postponed until the transac-\ntion that updated the item commits (see Exercise 18.28).\n\u2022Recoverability alone can be ensured by tracking uncommitted writes and allowing\nat r a n s a c t i o n Tit oc o m m i to n l ya f t e rt h ec o m m i to fa n yt r a n s a c t i o nt h a tw r o t ea\nvalue that Tiread. Commit dependencies, outlined in Section 18.1.5, can be used\nfor this purpose.\nIf the timestamp-ordering protocol is applied only to tuples, the protocol would be\nvulnerable to the phantom problems that we saw in Section 17.10 and Section 18.4.3.\nTo avoid this problem, the timestamp-ordering protocol could be applied to all\ndata that is read by a transaction, including relation metadata and index data. In the\ncontext of locking-based concurrency control, the index-locking protocol, described in\nSection 18.4.3, is a more e\ufb03cient alternative for avoiding the phantom problem; recall\nthat the index-locking protocol obtains locks on index nodes, in addition to obtaining\nlocks on tuples. The timestamp-ordering protocol can be similarly modi\ufb01ed to treat\neach index node as a data item, with associated read and write timestamps, and to\napply the timestamp-ordering tests on these data items, too. This extended version of\nthe timestamp-ordering protocol avoids phantom problems and ensures serializability\neven with predicate reads.\n18.5.3 Thomas\u2019 Write Rule\nWe now present a modi\ufb01cation to the timestamp-ordering protocol that allows greater\npotential concurrency than does the protocol of Section 18.5.2. Let us consider sched-\nule 4 of Figure 18.18 and apply the timestamp-ordering protocol. Since T27starts before\nT28, we shall assume that TS( T27)<TS(T28). The read (Q) operation of T27succeeds,\nas does the write (Q) operation of T28.W h e n T27attempts its write (Q)o p e r a t i o n ,\n", "893": "18.5 Timestamp-Based Protocols 865\nT27\n T28\nread (Q)\nwrite (Q)\nwrite (Q)\nFigure 18.18 Schedule 4.\nwe \ufb01nd that TS( T27)<W-timestamp( Q), since W-timestamp( Q)=T S ( T28). Thus, the\nwrite (Q)b y T27is rejected and transaction T27must be rolled back.\nAlthough the rollback of T27is required by the timestamp-ordering protocol, it\nis unnecessary. Since T28has already written Q, the value that T27is attempting to\nwrite is one that will never need to be read. Any transaction Tiwith TS( Ti)<TS(T28)\nthat attempts a read (Q) will be rolled back, since TS( Ti)<W-timestamp( Q). Any\ntransaction Tjwith TS( Tj)>TS(T28)m u s tr e a dt h ev a l u eo f Qwritten by T28,r a t h e r\nthan the value that T27is attempting to write.\nThis observation leads to a modi\ufb01ed version of the timestamp-ordering protocol in\nwhich obsolete write operations can be ignored under certain circumstances. The pro-\ntocol rules for read operations remain unchanged. The protocol rules for write opera-\ntions, however, are slightly di\ufb00erent from the timestamp-ordering protocol of Section\n18.5.2.\nThe modi\ufb01cation to the timestamp-ordering protocol, called Thomas\u2019 write rule ,i s\nthis: Suppose that transaction Tiissues write (Q).\n1.If TS( Ti)<R-timestamp( Q), then the value of Qthat Tiis producing was previ-\nously needed, and it had been assumed that the value would never be produced.\nHence, the system rejects the write operation and rolls Tiback.\n2.If TS( Ti)<W-timestamp( Q), then Tiis attempting to write an obsolete value of\nQ.H e n c e ,t h i s write operation can be ignored.\n3.Otherwise, the system executes the write operation and sets W-timestamp( Q)t o\nTS(Ti).\nThe di\ufb00erence between these rules and those of Section 18.5.2 lies in the sec-\nond rule. The timestamp-ordering protocol requires that Tibe rolled back if Tiissues\nwrite (Q)a n dT S ( Ti)<W-timestamp( Q). However, here, in those cases where TS( Ti)\n\u2265R-timestamp( Q), we ignore the obsolete write .\nBy ignoring the write, Thomas\u2019 write rule allows schedules that are not con\ufb02ict seri-\nalizable but are nevertheless correct. Those non-con\ufb02ict-serializable schedules allowed\nsatisfy the de\ufb01nition of view serializable schedules (see Note 18.1 on page 867). Thomas\u2019\nwrite rule makes use of view serializability by, in e\ufb00ect, deleting obsolete write opera-\ntions from the transactions that issue them. This modi\ufb01cation of transactions makes it\npossible to generate serializable schedules that would not be possible under the other\n", "894": "866 Chapter 18 Concurrency Control\nprotocols presented in this chapter. For example, schedule 4 of Figure 18.18 is not con-\n\ufb02ict serializable and, thus, is not possible under the two-phase locking protocol, the tree\nprotocol, or the timestamp-ordering pr otocol. Under Thomas\u2019 write rule, the write (Q)\noperation of T27would be ignored. The result is a schedule that is view equivalent to the\nserial schedule <T27,T28>.\n18.6 Validation-Based Protocols\nIn cases where a majority of transactions are read-only transactions, the rate of con\ufb02icts\namong transactions may be low. Thus, many of these transactions, if executed without\nthe supervision of a concurrency-control scheme, would nevertheless leave the system\nin a consistent state. A concurrency-control scheme imposes overhead of code execu-\ntion and possible delay of transactions. It may be better to use an alternative scheme\nthat imposes less overhead. A di\ufb03culty in reducing the overhead is that we do not know\nin advance which transactions will be involved in a con\ufb02ict. To gain that knowledge,\nwe need a scheme for monitoring the system.\nThevalidation protocol requires that each transaction Tiexecutes in two or three\ndi\ufb00erent phases in its lifetime, depending on whether it is a read-only or an update\ntransaction. The phases are, in order:\n1. Read phase . During this phase, the system executes transaction Ti.I tr e a d st h e\nvalues of the various data items and stores them in variables local to Ti.I tp e r -\nforms all write operations on temporary local variables, without updates of the\nactual database.\n2. Validation phase . The validation test (described below) is applied to transaction\nTi. This determines whether Tiis allowed to proceed to the write phase without\ncausing a violation of serializability. If a t ransaction fails the validation test, the\nsystem aborts the transaction.\n3. Write phase . If the validation test succeeds for transaction Ti, the temporary local\nvariables that hold the results of any write operations performed by Tiare copied\nto the database. Read-only transactions omit this phase.\nEach transaction must go through the phases in the order shown. However, phases of\nconcurrently executing transactions can be interleaved.\nTo perform the validation test, we need to know when the various phases of trans-\nactions took place. We shall, therefore, associate three di\ufb00erent timestamps with each\ntransaction Ti:\n1.StartTS (Ti), the time when Tistarted its execution.\n2.ValidationTS (Ti) ,t h et i m ew h e n Ti\ufb01nished its read phase and started its valida-\ntion phase.\n3.FinishTS (Ti) ,t h et i m ew h e n Ti\ufb01nished its write phase.\n", "895": "18.6 Validation-Based Protocols 867\nNote 18.1 VIEW SERIALIZABILITY\nThere is another form of equivalence that is less stringent than con\ufb02ict equivalence,\nbut that, like con\ufb02ict equivalence, is based on only the read andwrite operations\nof transactions.\nConsider two schedules Sand S\u2032,w h e r et h es a m es e to ft r a n s a c t i o n sp a r t i c -\nipates in both schedules. The schedules Sand S\u2032are said to be view equivalent if\nthree conditions are met:\n1.For each data item Q,i ft r a n s a c t i o n Tireads the initial value of Qin schedule\nS, then transaction Timust, in schedule S\u2032, also read the initial value of Q.\n2.For each data item Q,i ft r a n s a c t i o n Tiexecutes read (Q)i ns c h e d u l e S,a n di f\nthat value was produced by a write (Q) operation executed by transaction Tj,\nthen the read (Q) operation of transaction Timust, in schedule S\u2032,a l s or e a d\nthe value of Qthat was produced by the same write (Q) operation of trans-\naction Tj.\n3.For each data item Q, the transaction (if any) that performs the \ufb01nal\nwrite (Q) operation in schedule Smust perform the \ufb01nal write (Q)i ns c h e d -\nuleS\u2032.\nConditions 1 and 2 ensure that each transaction reads the same values in both\nschedules and, therefore, performs the same computation. Condition 3, coupled\nwith conditions 1 and 2, ensures that both schedules result in the same \ufb01nal system\nstate.\nThe concept of view equivalence leads to the concept of view serializability. We\nsay that a schedule Sisview serializable if it is view equivalent to a serial schedule.\nAs an illustration, suppose that we augment schedule 4 with transaction T29\nand obtain the following view serializable (schedule 5):\nT27 T28 T29\nread ( Q)\nwrite ( Q)write ( Q)\nwrite ( Q)\nIndeed, schedule 5 is view equivalent to the serial schedule <T27,T28,T29>,s i n c e\nthe one read (Q) instruction reads the initial value of Qin both schedules and T29\nperforms the \ufb01nal write of Qin both schedules.\nEvery con\ufb02ict-serializable schedule is also view serializable, but there are view-\nserializable schedules that are not con\ufb02ict serializable. Indeed, schedule 5 is not\ncon\ufb02ict serializable, since every pair of consecutive instructions con\ufb02icts, and,\nthus, no swapping of instructions is possible.\n", "896": "868 Chapter 18 Concurrency Control\nNote 18.1 VIEW SERIALIZABILITY (Cont.)\nObserve that, in schedule 5, transactions T28and T29perform write (Q)o p -\nerations without having performed a read (Q) operation. Writes of this sort are\ncalled blind writes . Blind writes appear in any view-serializable schedule that is not\ncon\ufb02ict serializable.\nWe determine the serializability order by the timestamp-ordering technique, using\nthe value of the timestamp ValidationTS (Ti). Thus, the value TS( Ti)=ValidationTS (Ti)\nand, if TS( Tj)<TS(Tk), then any produced schedule must be equivalent to a serial\nschedule in which transaction Tjappears before transaction Tk.\nThe validation test for transaction Tirequires that, for all transactions Tkwith\nTS(Tk)<TS(Ti), one of the following two conditions must hold:\n1.FinishTS (Tk)<StartTS (Ti). Since Tkcompletes its execution before Tistarted,\nthe serializability order is indeed maintained.\n2.The set of data items written by Tkdoes not intersect with the set of data items\nread by Ti,a n d Tkcompletes its write phase before Tistarts its validation phase\n(StartTS (Ti)<FinishTS (Tk)<ValidationTS (Ti)). This condition ensures that the\nwrites of Tkand Tido not overlap. Since the writes of Tkdo not a\ufb00ect the read\nofTi, and since Ticannot a\ufb00ect the read of Tk, the serializability order is indeed\nmaintained.\nAs an illustration, consider again transactions T25and T26. Suppose that TS( T25)\n<TS(T26). Then, the validation phase succeeds in the schedule 6 in Figure 18.19. Note\nthat the writes to the actual variables are performed only after the validation phase of\nT26.T h u s , T25reads the old values of Band A, and this schedule is serializable.\nThe validation scheme automatically guards against cascading rollbacks, since the\nactual writes take place only after the transaction issuing the write has committed.\nHowever, there is a possibility of starvation of long transactions, due to a sequence of\ncon\ufb02icting short transactions that cause repeated restarts of the long transaction. To\navoid starvation, con\ufb02icting transactions must be temporarily blocked to enable the\nlong transaction to \ufb01nish.\nNote also that the validation conditions result in a transaction Tonly being val-\nidated again the set of transactions Tithat \ufb01nished after Tstarted, and, further, are\nserialized before T. Transactions that \ufb01nished before Tstarted can be ignored in the\nvalidation tests. Transactions Tithat are serialized after T(that is, they have Valida-\ntionTS (Ti)>ValidationTS (T) )c a na l s ob ei g n o r e d ;w h e ns u c hat r a n s a c t i o n Tiis vali-\ndated, it would be validated against TifT\ufb01nished after Tistarted.\n", "897": "18.7 Multiversion Schemes 869\nT25\n T26\nread (B)\nread (B)\nB:=B\u221250\nread (A)\nA:=A+50\nread (A)\n<validate >\ndisplay (A+B )\n<validate >\nwrite (B)\nwrite (A)\nFigure 18.19 Schedule 6, a schedule produced by using validation.\nThis validation scheme is called the optimistic concurrency-control scheme since\ntransactions execute optimistically, assuming they will be able to \ufb01nish execution and\nvalidate at the end. In contrast, locking and timestamp ordering are pessimistic in that\nthey force a wait or a rollback whenever a con\ufb02ict is detected, even though there is a\nchance that the schedule may be con\ufb02ict serializable.\nIt is possible to use TS( Ti)=StartTS (Ti)i n s t e a do f ValidationTS (Ti)w i t h o u ta f -\nfecting serializability. However, doing so may result in a transaction Tientering the\nvalidation phase before a transaction Tjthat has TS( Tj)<TS(Ti). Then, the validation\nofTiwould have to wait for Tjto complete, so its read and write sets are completely\nknown. Using ValidationTS avoids this problem.\n18.7 Multiversion Schemes\nThe concurrency-control schemes discussed thus far ensure serializability by either de-\nlaying an operation or aborting the transaction that issued the operation. For example,\naread operation may be delayed because the appropriate value has not been written\nyet; or it may be rejected (that is, the issuing transaction must be aborted) because\nthe value that it was supposed to read has already been overwritten. These di\ufb03culties\ncould be avoided if old copies of each data item were kept in a system.\nInmultiversion concurrency-control schemes, each write (Q)o p e r a t i o nc r e a t e sa\nnew version ofQ. When a transaction issues a read (Q) operation, the concurrency-\ncontrol manager selects one of the versions of Qto be read. The concurrency-control\nscheme must ensure that the version to be read is selected in a manner that ensures\nserializability. It is also crucial, for performance reasons, that a transaction be able to\ndetermine easily and quickly which version of the data item should be read.\n", "898": "870 Chapter 18 Concurrency Control\n18.7.1 Multiversion Timestamp Ordering\nThe timestamp-ordering protocol can be extended to a multiversion protocol. With\neach transaction Tiin the system, we associate a unique static timestamp, denoted\nby TS( Ti). The database system assigns this timestamp before the transaction starts\nexecution, as described in Section 18.5.\nWith each data item Q, a sequence of versions <Q1,Q2,\u2026,Qm>is associated.\nEach version Qkcontains three data \ufb01elds:\n1.Content is the value of version Qk.\n2.W-timestamp (Qk) is the timestamp of the transaction that created version Qk.\n3.R-timestamp (Qk) is the largest timestamp of any transaction that successfully read\nversion Qk.\nAt r a n s a c t i o n \u2014 s a y , Ti\u2014creates a new version Qkof data item Qby issuing a write (Q)\noperation. The content \ufb01eld of the version holds the value written by Ti.T h es y s t e m\ninitializes the W-timestamp and R-timestamp to TS( Ti). It updates the R-timestamp\nvalue of Qkwhenever a transaction Tjreads the content of Qkand R-timestamp( Qk)<\nTS(Tj).\nThe multiversion timestamp-ordering scheme presented next ensures serializabil-\nity. The scheme operates as follows: Suppose that transaction Tiissues a read (Q)o r\nwrite (Q)o p e r a t i o n .L e t Qkdenote the version of Qwhose write timestamp is the largest\nwrite timestamp less than or equal to TS( Ti).\n1.If transaction Tiissues a read (Q), then the value returned is the content of ver-\nsion Qk.\n2.If transaction Tiissues write (Q), and if TS( Ti)<R-timestamp( Qk), then\nthe system rolls back transaction Ti. On the other hand, if TS( Ti)=W-\ntimestamp( Qk), the system overwrites the contents of Qk;o t h e r w i s e( i fT S ( Ti)\n>R-timestamp( Qk)), it creates a new version of Q.\nThe justi\ufb01cation for rule 1 is clear. A transaction reads the most recent version\nthat comes before it in time. The second rule forces a transaction to abort if it is \u201ctoo\nlate\u201d in doing a write. More precisely, if Tiattempts to write a version that some other\ntransaction would have read, then we cannot allow that write to succeed.\nThevalid interval of a version QiofQwith W-timestamp tis de\ufb01ned as follows: if\nQiis the latest version of Q, the interval is [ t,\u221e] ;o t h e r w i s el e tt h en e x tv e r s i o no f Q\nhave timestamp s; then the valid interval is [ t,s). You can easily verify that reads by\na transaction with timestamp tireturn the content of the version whose valid interval\ncontains ti.\nVersions that are no longer needed are removed according to the following rule:\nSuppose that there are two versions, Qkand Qj, of a data item, and that both versions\n", "899": "18.7 Multiversion Schemes 871\nhave a W-timestamp less than the timestamp of the oldest transaction in the system.\nThen, the older of the two versions Qkand Qjwill not be used again, and can be deleted.\nThe multiversion timestamp-ordering scheme has the desirable property that a read\nrequest never fails and is never made to wait. In typical database systems, where reading\nis a more frequent operation than is writing, this advantage may be of major practical\nsigni\ufb01cance.\nThe scheme, however, su\ufb00ers from two undesirable properties. First, the reading of\na data item also requires the updating of the R-timestamp \ufb01eld, resulting in two potential\ndisk accesses, rather than one. Second, the con\ufb02icts between transactions are resolved\nthrough rollbacks, rather than through waits. This alternative may be expensive. Section\n18.7.2 describes an algorithm to alleviate this problem.\nThis multiversion timestamp-ordering scheme does not ensure recoverability and\ncascadelessness. It can be extended in the same manner as the basic timestamp-\nordering scheme to make it recoverable and cascadeless.\n18.7.2 Multiversion Two-Phase Locking\nThemultiversion two-phase locking protocol attempts to combine the advantages of mul-\ntiversion concurrency control with the advantages of two-phase locking. This protocol\ndi\ufb00erentiates between read-only transactions andupdate transactions .\nUpdate transactions perform rigorous two-phase locking; that is, they hold all locks\nup to the end of the transaction. Thus, they can be serialized according to their commit\norder. Each version of a data item has a single timestamp. The timestamp in this case\nis not a real clock-based timestamp, but rather is a counter, which we will call the ts-\ncounter , that is incremented during commit processing.\nThe database system assigns read-only transactions a timestamp by reading the\ncurrent value of ts-counter before they start execution; they follow the multiversion\ntimestamp-ordering protocol for performing reads. Thus, when a read-only transaction\nTiissues a read (Q), the value returned is the contents of the version whose timestamp\nis the largest timestamp less than or equal to TS( Ti).\nWhen an update transaction reads an item, it gets a shared lock on the item and\nreads the latest version of that item. When an update transaction wants to write an\nitem, it \ufb01rst gets an exclusive lock on the item and then creates a new version of the\ndata item. The write is performed on the new version, and the timestamp of the new\nversion is initially set to a value \u221e, a value greater than that of any possible timestamp.\nWhen the update transaction Ticompletes its actions, it carries out commit pro-\ncessing; only one update transaction is allowed to perform commit processing at a time.\nFirst, Tisets the timestamp on every version it has created to 1 more than the value of\nts-counter ; then, Tiincrements ts-counter by 1, and commits.\nRead-only transactions see the old value of ts-counter until Tihas successfully\ncommitted. As a result, read-only transactions that start after Ticommits will see the\nvalues updated by Ti, whereas those that start before Ticommits will see the value\nbefore the updates by Ti. In either case, read-only transactions never need to wait for\n", "900": "872 Chapter 18 Concurrency Control\nNote 18.2 MULTIVERSIONING AND DATABASE IMPLEMENTATION\nConsider a database system that implements a primary key constraint by ensuring\nthat only one tuple exists for any value of the primary key attribute. The creation\nof a second version of the record with the same primary key would appear to be a\nviolation of the primary key constraint. How ever, it is logically not a violation, since\nthe two versions do not coexist at any time in the database. Therefore, primary\nconstraint enforcement must be modi\ufb01ed to allow multiple records with the same\nprimary key, as long as they are di\ufb00erent versions of the same record.\nNext, consider the issue of deletion of tuples. This can be implemented by\ncreating a new version of the tuple, with timestamps created as usual, but with a\nspecial marker denoting that the tuple has been deleted. Transactions that read\nsuch a tuple simply skip it, since it has been deleted.\nFurther, consider the issue of enforcing foreign-key dependencies. Consider\nt h ec a s eo far e l a t i o n rwhose attribute r.Bis a foreign-key referencing attribute\ns.Bof relation s. In general, deletion of a tuple tsinsor update of a primary key\nattribute of tuple tsinscauses a foreign-key violation if there is an rtuple trsuch that\ntr.B=ts.B. With multiversioning, if the timestamp of the transaction performing\nthe deletion/update is tsi, the corresponding condition for violation is the existence\nof such a tuple version tr, with the additional condition that the valid interval of tr\ncontains tsi.\nFinally, consider the case of an index on attribute r.Bof relation r.I ft h e r ea r e\nmultiple versions of a record tiwith the same value for B, the index could point to\nthe latest version of the record, and the latest version could have pointers to earlier\nversions. However, if an update was made to attribute ti.B, the index would need\nto contain separate entries for di\ufb00erent versions of record ti;o n ee n t r yf o rt h eo l d\nvalue of ti.Band another for the new value of ti.B. When old versions of a record\nare deleted, any entry in the index for the old version must also be deleted.\nlocks. Multiversion two-phase locking also ensures that schedules are recoverable and\ncascadeless.\nVersions are deleted in a manner like that of multiversion timestamp ordering.\nSuppose there are two versions, Qkand Qj, of a data item, and that both versions have\na timestamp less than or equal to the timestamp of the oldest read-only transaction in\nthe system. Then, the older of the two versions Qkand Qjwill not be used again and it\ncan be deleted.\n18.8 Snapshot Isolation\nSnapshot isolation is a particular type of concurrency-control scheme that has\ngained wide acceptance in commercial and op en-source systems, including Oracle,\n", "901": "18.8 Snapshot Isolation 873\nPostgre SQL,a n d SQL Server. We introduced snapshot isolation in Section 17.9.3. Here,\nwe take a more detailed look into how it works.\nConceptually, snapshot isolation involves giving a transaction a \u201csnapshot\u201d of the\ndatabase at the time when it begins its execution. It then operates on that snapshot\nin complete isolation from concurrent transactions. The data values in the snapshot\nconsist only of values written by committed transactions. This isolation is ideal for\nread-only transactions since they never wait and are never aborted by the concurrency\nmanager.\nTransactions that update the database potentially have con\ufb02icts with other transac-\ntions that update the database. Updates performed by a transaction must be validated\nbefore the transaction is allowed to commit. We describe how validation is performed,\nlater in this section. Updates are kept in the transaction\u2019s private workspace until the\ntransaction is validated, at which point the updates are written to the database.\nWhen a transaction Tis allowed to commit, the transition of Tto the committed\nstate and the writing of all of the updates made by Tto the database must be concep-\ntually done as an atomic action so that any snapshot created for another transaction\neither includes all updates by transaction Tor none of them.\n18.8.1 Multiversioning in Snapshot Isolation\nTo implement snapshot isolation, transactions are given two timestamps. The \ufb01rst\ntimestamp, StartTS (Ti), is the time at which transaction Tistarted. The second times-\ntamp, CommitTS (Ti) is the time when the transaction Tirequested validation.\nNote that timestamps can be wall clock time, as long as no two transactions are\ngiven the same timestamp, but they are usually assigned from a counter that is incre-\nmented every time a transaction enters its validation phase.\nSnapshot isolation is based on multiversioning, and each transaction that updates\na data item creates a version of the data item. Versions have only one timestamp, which\nis the write timestamp, indicating when the version was created. The timestamp of a\nversion created by transaction Tiis set to CommitTS (Ti). (Since updates to the database\nare also only made after validation of the transaction Ti,CommitTS (Ti) is available\nwhen a version is created.)2\nWhen a transaction Tireads a data item, the latest version of the data item whose\ntimestamp is \u2264StartTS (Ti)i sr e t u r n e dt o Ti.T h u s , Tidoes not see the updates of\nany transactions that committed after Tistarted, while it does see the updates of all\ntransactions that commit before it started. As a result, Tie\ufb00ectively sees a snapshot of\nthe database as of the time it started.3\n2Many implementations create versions even before the trans action starts validation; since the version timestamp is\nnot available at this point, the timestamp is set to in\ufb01nity initially, and is updated to the correct value at the time of\nvalidation. Further optimizations are used in actua l implementations, but we ignore them for simplicity.\n3To e\ufb03ciently \ufb01nd the correct version of a data item for a given timestamp, many implementations store not only\nthe timestamp when a version was created, but also the timestamp when the next version was created, which can\nbe considered an invalidation timestamp for that version; the version is valid between the creation and invalidation\ntimestamps. The current version of a data item has the invalidation timestamp set to in\ufb01nity.\n", "902": "874 Chapter 18 Concurrency Control\n18.8.2 Validation Steps for Update Transactions\nDeciding whether or not to allow an update transaction to commit requires some care.\nPotentially, two transactions running concurrently might both update the same data\nitem. Since these two transactions operate in isolation using their own private snap-\nshots, neither transaction sees the update made by the other. If both transactions are\nallowed to write to the database, the \ufb01rst update written will be overwritten by the\nsecond. The result is a lost update . This must be prevented. There are two variants of\nsnapshot isolation, both of which prevent lost updates. They are called \ufb01rst committer\nwins and \ufb01rst updater wins . Both approaches are based on testing the transaction against\nconcurrent transactions.\nAt r a n s a c t i o n Tjis said to be concurrent with a given transaction Tiif it was active\nor partially committed at any point from the start of Tup to the point when validation\nofTistarted. Formally, Tjis concurrent with Tiif either\nStartTS (Tj)\u2264StartTS (Ti)\u2264CommitTS (Tj), or\nStartTS (Ti)\u2264StartTS (Tj)\u2264CommitTS (Ti).\nUnder \ufb01rst committer wins ,w h e nat r a n s a c t i o n Tistarts validation, the following\nactions are performed as part of validation, after its CommitTS is assigned. (We assume\nfor simplicity that only one transaction performs validation at a time, although real\nimplementations do support concurrent validation.)\n\u2022A test is made to see if any transaction that was concurrent with Thas already\nwritten an update to the database for some data item that Tintends to write.\nThis can be done by checking for each data item dthat Tiintends to write, whether\nthere is a version of the data item dwhose timestamp is between StartTS (Ti)a n d\nCommitTS (Ti).4\n\u2022If any such data item is found, then Tiaborts.\n\u2022If no such data item is found, then Tcommits and its updates are written to the\ndatabase.\nThis approach is called \u201c\ufb01rst committer wins\u201d because if transactions con\ufb02ict, the \ufb01rst\none to be tested using the above rule succeeds in writing its updates, while the subse-\nquent ones are forced to abort. Details of how to implement these tests are addressed\nin Exercise 18.15.\nUnder \ufb01rst updater wins , the system uses a locking mechanism that applies only to\nupdates (reads are una\ufb00ected by this, since they do not obtain locks). When a trans-\naction Tiattempts to update a data item, it requests a write lock on that data item. If\nthe lock is not held by a concurrent transaction, the following steps are taken after the\nlock is acquired:\n\u2022If the item has been updated by any concurrent transaction, then Tiaborts.\n4There are alternative implementations, based on k eeping track of read and write sets for transactions.\n", "903": "18.8 Snapshot Isolation 875\n\u2022Otherwise Timay proceed with its execution, including possibly committing.\nIf, however, some other concurrent transaction Tjalready holds a write lock on that\ndata item, then Ticannot proceed, and the following rules are followed:\n\u2022Tiwaits until Tjaborts or commits.\n\u00b0IfTjaborts, then the lock is released and Tic a no b t a i nt h el o c k .A f t e rt h el o c k\nis acquired, the check for an update by a concurrent transaction is performed\nas described earlier: Tiaborts if a concurrent transaction had updated the data\nitem, and it proceeds with its execution otherwise.\n\u00b0IfTjcommits, then Timust abort.\nLocks are released when the transaction commits or aborts.\nThis approach is called \u201c\ufb01rst updater wins\u201d because if transactions con\ufb02ict, the \ufb01rst\none to obtain the lock is the one that is permitted to commit and perform its update.\nThose that attempt the update later abort unless the \ufb01rst updater subsequently aborts\nfor some other reason. (As an alternative to waiting to see if the \ufb01rst updater Tjaborts,\na subsequent updater Tican be aborted as soon as it \ufb01nds that the write lock it wishes\nto obtain is held by Tj.)\n18.8.3 Serializability Issues and Solutions\nSnapshot isolation is attractive in practice because transactions that read a lot of data\n(typically for data analysis) do not interfere with shorter update transactions (typi-\ncally used for transaction processing). With two-phase locking, such long read-only\ntransactions would block update transactions for long periods of time, which is often\nunacceptable.\nIt is worth noting that integrity constraints that are enforced by the database, such\nas primary-key and foreign-key constraints, cannot be checked on a snapshot; other-\nwise it would be possible for two concurrent transactions to insert two tuples with the\nsame primary key value, or for a transaction to insert a foreign key value that is con-\ncurrently deleted from the referenced table. This problem is handled by checking these\nconstraints on the current state of the database, rather than on the snapshot, as part\nof validation at the time of commit.\nEven with the above \ufb01x, there is still a serious problem with the snapshot isolation\nscheme as we have presented it and as it is implemented in practice: snapshot isolation\ndoes notensure serializability!\nNext we give examples of possible nonserializable executions under snapshot iso-\nlation. We then outline the serializable snapshot isolation technique that is supported\nby some databases, which extends the snapshot isolation technique to ensure serializ-\nability. Snapshot isolation implementations that do not support serializable snapshot\nisolation often support SQL extensions that allow the programmer to ensure serializ-\nability even with snapshot isolation; we study these extensions at the end of the section.\n", "904": "876 Chapter 18 Concurrency Control\nTi\n Tj\nread (A)\nread (B)\nread (A)\nread (B)\nA=B\nB=A\nwrite (A)\nwrite (B)\nFigure 18.20 Nonserializable schedule under snapshot isolation.\n\u2022Consider the transaction schedule shown in Figure 18.20. Two concurrent trans-\nactions Tiand Tjboth read data items Aand B.Tisets A=Band writes A, while Tj\nsets B=Aand writes B.S i n c e Tiand Tjare concurrent, under snapshot isolation\nneither transaction sees the update by the other in its snapshot. But, since they\nupdate di\ufb00erent data items, both are allowed to commit regardless of whether the\nsystem uses the \ufb01rst-update-wins policy or the \ufb01rst-committer-wins policy.\nHowever, the execution is not serializable, since it results in swapping of the\nvalues of Aand B, whereas any serializable schedule would set both Aand Bto the\nsame value: either the initial value of Ao rt h ei n i t i a lv a l u eo f B, depending on the\norder of Tiand Tj.\nIt can be easily seen that the precedence graph has a cycle. There is an edge\nin the precedence graph from TitoTjbecause Tireads the value of Athat existed\nbefore Tjwrites A. There is also an edge in the precedence graph from TjtoTi\nbecause Tjreads the value of Bthat existed before Tiwrites B.S i n c et h e r ei sa\ncycle in the precedence graph, the result is a nonserializable schedule.\nThis situation, where each of a pair of transactions has read a data item that is\nwritten by the other, but the set of data items written by the two transactions do\nnot have any data item in common, is referred to as write skew .\n\u2022As another example of write skew, consider a banking scenario. Suppose that the\nbank enforces the integrity constraint that the sum of the balances in the checking\nand the savings account of a customer must not be negative. Suppose the checking\nand savings balances for a customer are $100 and $200, respectively. Suppose that\ntransaction T36withdraws $200 from the checking account, after verifying the in-\ntegrity constraint by reading both balances. Suppose that concurrently transaction\nT37withdraws $200 from the savings account, again after verifying the integrity\nconstraint. Since each of the transactions checks the integrity constraint on its\nown snapshot, if they run concurrently each will believe that the sum of the bal-\nances after the withdrawal is $100, and therefore its withdrawal does not violate\n", "905": "18.8 Snapshot Isolation 877\nthe constraint. Since the two transactions update di\ufb00erent data items, they do not\nhave any update con\ufb02ict, and under snapshot isolation both of them can commit.\nUnfortunately, in the \ufb01nal state after both T36and T37have committed, the sum\nof the balances is $100, violating the integrity constraint. Such a violation could\nnever have occurred in any serial execution of T36and T37.\n\u2022Many \ufb01nancial applications create consecutive sequence numbers, for example to\nnumber bills, by taking the maximum c urrent bill number and adding 1 to the\nvalue to get a new bill number. If two such transactions run concurrently, each\nwould see the same set of bills in its snapshot, and each would create a new bill\nwith the same number. Both transactions pass the validation tests for snapshot\nisolation, since they do not update any tuple in common. However, the execution\nis not serializable; the resultant database state cannot be obtained by any serial\nexecution of the two transactions. Creating two bills with the same number could\nhave serious legal implications.\nThe above problem is in fact an example of the phantom phenomenon, which\nwe saw in Section 18.4.3, since the insert performed by each transaction con\ufb02icts\nwith the read performed by the other transaction to \ufb01nd the maximum bill number,\nbut the con\ufb02ict is not detected by snapshot isolation.5\nThe problems listed above seem to indicate that the snapshot isolation technique\nis vulnerable to many serializability problems and should never be used. However, se-\nrializability problems are relatively rare for two reasons:\n1.The fact that the database must check integrity constraints at the time of com-\nmit, and not on a snapshot, helps avoid inconsistencies in many situations. For\nexample, in the \ufb01nancial application example that we saw earlier, the bill number\nwould likely have been declared as a primary key. The database system would\ndetect the primary key violation outside the snapshot and roll back one of the\ntwo transactions.\nIt was shown that primary key constraints ensured that all transactions in a\npopular transaction processing benchmark, TPC-C ,w e r ef r e ef r o mn o n s e r i a l i z -\nability problems, when executed under snapshot isolation. This was viewed as an\nindication that such problems are rare. However, they do occur occasionally, and\nwhen they occur they must be dealt with.6\n2.In many applications that are vulnerable to serializability problems, such as skew\nwrites, on some data items, the transactions con\ufb02ict on other data items, ensuring\n5The SQLstandard uses the term phantom problem to refer to nonrepeatable predicate reads, leading some to claim that\nsnapshot isolation avoids the phantom problem; however, such a claim is not valid under our de\ufb01nition of phantom\ncon\ufb02ict.\n6For example, the problem of duplicate bill numbers actually occurred several times in a \ufb01nancial application in I.I.T.\nBombay, where (for reasons too complex to discuss here) the bill number was not a primary key, and it was detected\nby \ufb01nancial auditors.\n", "906": "878 Chapter 18 Concurrency Control\nsuch transactions cannot execute concurrently; as a result, the execution of such\ntransactions under snapshot isolation remains serializable.\nNonserializable may nevertheless occur with snapshot isolation. The impact of\nnonserializable execution due to snapshot isolation is not very severe for many applica-\ntions. For example, consider a university application that implements enrollment limits\nfor a course by counting the current enrollment before allowing registration. Snapshot\nisolation could allow the class enrollment limit to be exceeded. However, this may hap-\npen very rarely, and if it does, having one extra student in a class is usually not a major\nproblem. The fact that snapshot isolation allows long read transactions to execute with-\nout blocking updaters is a large enough bene\ufb01t for many such applications to live with\noccasional glitches.\nNonserializability may not be acceptable for many other applications, such as \ufb01-\nnancial applications. There are several possible solutions.\n\u2022A modi\ufb01ed form of snapshot isolation, called serializable snapshot isolation, can\nbe used if it is supported by the database system. This technique extends the snap-\nshot isolation technique in a way that ensures serializability.\n\u2022Some systems allow di\ufb00erent transactions to run under di\ufb00erent isolation levels,\nwhich can be used to avoid the serializability problems mentioned above.\n\u2022Some systems that support snapshot isolation provide a way for SQL programmers\nto create arti\ufb01cial con\ufb02icts, using a for update clause in SQL,w h i c hc a nb eu s e dt o\nensure serializability.\nWe brie\ufb02y outline each of these solutions below.\nSince version 9.1, Postgre SQL implements a technique called serializable snapshot\nisolation, which ensures serializability; in addition, Postgre SQL versions from 9.1 on-\nwards include an index-locking-based technique to provide protection against phantom\nproblems.\nThe intuition behind the serializable snapshot isolation (SSI) protocol is as follows:\nSuppose we track all con\ufb02icts (i.e., write-write, read-write, and write-read con\ufb02icts)\nbetween transactions. Recall from Section 17.6 that we can construct a transaction\nprecedence graph which has a directed edge from T1toT2if transactions T1and T2\nhave con\ufb02icting operations on a tuple, with T1\u2019s action preceding T2\u2019s action. As we saw\nin Section 17.6, one way to ensure serializability is to look for cycles in the transaction\nprecedence graph and roll back transactions if a cycle is found.\nThe key reason for loss of serializability with snapshot isolation is that read-write\ncon\ufb02icts, where a transaction T1writes a version of an object, and a transaction T2sub-\nsequently reads an earlier version of the object, are not tracked by snapshot isolation.\nThis con\ufb02ict can be represented by a read-write con\ufb02ict edge from T2toT1.\nIt has been shown that in all cases where snapshot isolation allows nonserializable\nschedules, there must be a transaction that has both an incoming read-write con\ufb02ict\n", "907": "18.8 Snapshot Isolation 879\nedge and an outgoing read-write con\ufb02ict edge (all other cases of cycles in the con\ufb02ict\ngraph are caught by the snapshot isolation rules). Thus, serializable snapshot isolation\nimplementations track all read-write con\ufb02icts between concurrent transactions to de-\ntect if a transaction has both an incoming and an outgoing read-write con\ufb02ict edge. If\nsuch a situation is detected, one of the transactions involved in the read-write con\ufb02icts\nis rolled back. This check is signi\ufb01cantly cheaper than tracking all con\ufb02icts and looking\nfor cycles, although it may result in some unnecessary rollbacks.\nIt is also worth mentioning that the technique used by Postgre SQL to prevent phan-\ntoms uses index locking, but the locks are not held in a two-phase manner. Instead, they\nare used to detect potential con\ufb02icts between concurrent transactions and must be re-\ntained for some time even after a transaction commits, to allow checks against other\nconcurrent transactions. The index-locking technique used by Postgre SQL also does\nnot result in any deadlocks.\nSQL Server o\ufb00ers the option of allowing some transactions to run under snapshot\nisolation, while allowing others to run under the serializable isolation level. Running\nlong read-only transactions under the snapshot isolation level while running update\ntransactions under the serializable isolation level ensures that the read-only transaction\ndoes not block updaters, while also ensuring that the above anomalies cannot occur.\nIn Oracle versions till at least Oracle 12c (to the best of our knowledge), and in\nPostgre SQL versions prior to 9.1, the serializable isolation level actually implements\nsnapshot isolation. As a result, even with the isolation level set to serializable, it is\npossible that the database permits some schedules that are not serializable.\nIf an application has to run under snapshot isolation, on several of these databases\nan application developer can guard against certain snapshot anomalies by appending\nafor update clause to the SQL select query as illustrated below:\nselect *\nfrom instructor\nwhere ID= 22222\nfor update ;\nAdding the for update clause causes the system to treat data that are read as if they\nhad been updated for purposes of concurrency control. In our \ufb01rst example of write\nskew shown in Figure 18.20, if the for update clause were appended to the select queries\nthat read the values of Aand B, only one of the two concurrent transactions would be\nallowed to commit since it appears that both transactions have updated both Aand B.\nFormal methods exist (see the online bibliographical notes) to determine whether\na given mix of transactions runs the risk of nonserializable execution under snapshot\nisolation and to decide on what con\ufb02icts to introduce (using the for update clause, for\nexample) to ensure serializability. Such methods can work only if we know in advance\nwhat transactions are being executed. In some applications, all transactions are from a\npredetermined set of transactions, making this analysis possible. However, if the appli-\ncation allows unrestricted, ad hoc transactions, then no such analysis is possible.\n", "908": "880 Chapter 18 Concurrency Control\n18.9 Weak Levels of Consistency in Practice\nIn Section 17.8, we discussed the isolation levels speci\ufb01ed by the SQL standard: seri-\nalizable, repeatable read, read committed, and read uncommitted. In this section, we\n\ufb01rst brie\ufb02y outline some older terminology relating to consistency levels weaker than\nserializability and relate it to the SQL standard levels. We then discuss the issue of con-\ncurrency control for transactions that involve user interaction, an issue that we brie\ufb02y\ndiscussed in Section 17.8.\n18.9.1 Degree-Two Consistency\nThe purpose of degree-two consistency is to avoid cascading aborts without necessarily\nensuring serializability. The locking protocol for degree-two consistency uses the same\ntwo lock modes that we used for the two-phase locking protocol: shared (S) and exclu-\nsive (X). A transaction must hold the appropriate lock mode when it accesses a data\nitem, but two-phase behavior is not required.\nIn contrast to the situation in two-phase locking, S-locks may be released at any\ntime, and locks may be acquired at any time. Exclusive locks, however, cannot be re-\nleased until the transaction either commits or aborts. Serializability is not ensured by\nthis protocol. Indeed, a transaction may read the same data item twice and obtain dif-\nferent results. In Figure 18.21, T32reads the value of Qbefore that value is written by\nT33, and again after it is written by T33.\nReads are not repeatable, but since exclusive locks are held until transaction com-\nmit, no transaction can read an uncommitted value. Thus, degree-two consistency is\none particular implementation of the read-committed isolation level.\nIt is interesting to note that with degree-two consistency, a transaction that is scan-\nning an index may potentially see two versions of a record that was updated while\nthe scan was in progress and may also potentially see neither version! For example,\nT32\n T33\nlock-S (Q)\nread (Q)\nunlock (Q)\nlock-X (Q)\nread (Q)\nwrite (Q)\nunlock (Q)\nlock-S (Q)\nread (Q)\nunlock (Q)\nFigure 18.21 Nonserializable schedule with degree-two consistency.\n", "909": "18.9 Weak Levels of Consistency in Practice 881\nconsider a relation r(A,B,C), with primary key A, with an index on attribute B.N o w\nconsider a query that is scanning the relation rusing the index on attribute B,u s i n g\ndegree-two consistency. Suppose there is a concurrent update to a tuple t1\u2208rthat\nupdates attribute t1.Bfrom v1tov2. Such an update requires deletion of an entry corre-\nsponding to value v1from the index and insertion of a new entry corresponding to v2.\nNow, the scan of rcould possibly scan the index node corresponding to v1after the old\ntuple is deleted there but visit the index node corresponding to v2before the updated\ntuple is inserted in that node. Then, the scan would completely miss the tuple, even\nthough it should have seen either the old value or the new value of t1.F u r t h e r ,as c a n\nusing degree-two consistency could possibly visit the node corresponding to v1before\nthe delete, and the node corresponding to v2after the insert, and thereby see two ver-\nsions of t1, one from before the update and one from after the update. (This problem\nwould not arise if the scan and the update both used two-phase locking.)\n18.9.2 Cursor Stability\nCursor stability is a form of degree-two consistency designed for programs that iterate\nover tuples of a relation by using cursors. Instead of locking the entire relation, cursor\nstability ensures that:\n\u2022The tuple that is currently being processed by the iteration is locked in shared\nmode. Once the tuple is processed, the lock on the tuple can be released.\n\u2022Any modi\ufb01ed tuples are locked in exclusive mode until the transaction commits.\nThese rules ensure that degree-two consistency is obtained. But locking is not done\nin a two-phase manner, and serializability is not guaranteed. Cursor stability is used\nin practice on heavily accessed relations as a means of increasing concurrency and\nimproving system performance. Applications that use cursor stability must be coded\nin a way that ensures database consistency despite the possibility of nonserializable\nschedules. Thus, the use of cursor stability is limited to specialized situations with\nsimple consistency constraints.\nWhen supported by the database, snapshot isolation is a better alternative to\ndegree-two consistency as well as cursor stability, since it o\ufb00ers a similar or even better\nlevel of concurrency while reducing the risk of nonserializable executions.\n18.9.3 Concurrency Control Across User Interactions\nConcurrency-control protocols usually consider transactions that do not involve user\ninteraction. Consider the airline seat selection example from Section 17.8, which in-\nvolved user interaction. Suppose we treat all the steps from when the seat availability is\ninitially shown to the user, until the seat selection is con\ufb01rmed, as a single transaction.\nIf two-phase locking is used, the entire set of seats on a \ufb02ight would be locked in\nshared mode until the user has completed the seat selection, and no other transaction\nwould be able to update the seat allocation information in this period. Such locking\n", "910": "882 Chapter 18 Concurrency Control\nwould be a very bad idea since a user may take a long time to make a selection, or even\njust abandon the transaction without explicitly cancelling it. Timestamp protocols or\nvalidation could be used instead, which avoid the problem of locking, but both these\nprotocols would abort the transaction for a user Aif any other user Bhas updated the\nseat allocation information, even if the seat selected by Bdoes not con\ufb02ict with the\nseat selected by user A. Snapshot isolation is a good option in this situation, since it\nwould not abort the transaction of user Aas long as Bd i dn o ts e l e c tt h es a m es e a ta s\nA.\nHowever, snapshot isolation requires the database to remember information about\nupdates performed by a transaction even after it has committed, as long as any other\nconcurrent transaction is still active, which can be problematic for long-duration trans-\nactions.\nAnother option is to split a transaction that involves user interaction into two or\nmore transactions, such that no transaction spans a user interaction. If our seat se-\nlection transaction is split thus, the \ufb01rst tr ansaction would read the seat availability,\nwhile the second transaction would complete the allocation of the selected seat. If the\nsecond transaction is written carelessly, it could assign the selected seat to the user,\nwithout checking if the seat was meanwhile assigned to some other user, resulting in\na lost-update problem. To avoid the problem, as we outlined in Section 17.8, the sec-\nond transaction should perform the seat allocation only if the seat was not meanwhile\nassigned to some other user.\nThe above idea has been generalized in an alternative concurrency control scheme,\nwhich uses version numbers stored in tuples to avoid lost updates. The schema of each\nrelation is altered by adding an extra version\n number attribute, which is initialized to 0\nwhen the tuple is created. When a transaction reads (for the \ufb01rst time) a tuple that it\nintends to update, it remembers the version number of that tuple. The read is performed\nas a stand-alone transaction on the database, and hence any locks that may be obtained\nare released immediately. Updates are done locally and copied to the database as part\nof commit processing, using the following steps which are executed atomically (i.e., as\npart of a single database transaction):\n\u2022For each updated tuple, the transaction checks if the current version number is the\nsame as the version number of the tuple when it was \ufb01rst read by the transaction.\n1.If the version numbers match, the update is performed on the tuple in the\ndatabase, and its version number is incremented by 1.\n2.If the version numbers do not match, the transaction is aborted, rolling back\nall the updates it performed.\n\u2022If the version number check succeeds for all updated tuples, the transaction com-\nmits. It is worth noting that a timestamp could be used instead of the version\nnumber without impacting the scheme in any way.\n", "911": "18.10 Advanced Topics in Concurrency Control 883\nObserve the close similarity between the preceding scheme and snapshot isolation.\nThe version number check implements the \ufb01rst-committer-wins rule used in snapshot\nisolation, and it can be used even if the transaction was active for a very long time.\nHowever, unlike snapshot isolation, the reads performed by a transaction may not cor-\nrespond to a snapshot of the database; and unlike the validation-based protocol, reads\nperformed by the transaction are not validated.\nWe refer to the above scheme as optimistic concurrency control without read valida-\ntion. Optimistic concurrency control without read validation provides a weak level of\nserializability, and it does not ensure serializability. A variant of this scheme uses ver-\nsion numbers to validate reads at the time of commit, in addition to validating writes,\nto ensure that the tuples read by the transaction were not updated subsequent to the\ninitial read; this scheme is equivalent to the optimistic concurrency-control scheme\nwhich we saw earlier.\nThis scheme has been widely used by application developers to handle transac-\ntions that involve user interaction. An attractive feature of the scheme is that it can\nbe implemented easily on top of a database system. The validation and update steps\nperformed as part of commit processing are then executed as a single transaction in\nthe database, using the concurrency-control scheme of the database to ensure atomic-\nity for commit processing. The scheme is also used by the Hibernate object-relational\nmapping system (Section 9.6.2), and other o bject-relational mapping systems, where\nit is referred to as optimistic concurrency control (even though reads are not validated\nby default). Hibernate and other object-relational mapping systems therefore perform\nthe version number checks transparently as part of commit processing. (Transactions\nthat involve user interaction are called conversations in Hibernate to di\ufb00erentiate them\nfrom regular transactions; validation using version numbers is particularly useful for\nsuch transactions.)\nApplication developers must, however, be aware of the potential for non-\nserializable execution, and they must restrict their usage of the scheme to applications\nwhere non-serializability does not cause serious problems.\n18.10 Advanced Topics in Concurrency Control\nInstead of using two-phase locking, special -purpose concurrency control techniques\ncan be used for index structures, resulting in improved concurrency. When using main-\nmemory databases, conversely, index concurrency control can be simpli\ufb01ed. Further,\nconcurrency control actions often become bottlenecks in main-memory databases, and\ntechniques such as latch-free data structures have been designed to reduce concurrency\ncontrol overheads. Instead of detecting con\ufb02icts at the level of reads and writes, it is pos-\nsible to consider operations, such as increm ent of a counter, as basic operations, and\nperform concurrency control on the basis of con\ufb02icts between operations. Certain ap-\nplications require guarantees on transaction completion time. Specialized concurrency\ncontrol techniques have been developed for such applications.\n", "912": "884 Chapter 18 Concurrency Control\n18.10.1 Online Index Creation\nWhen we are dealing with large volumes of data (ranging in the terabytes), operations\nsuch as creating an index can take a long time\u2014perhaps hours or even days. When\nthe operation \ufb01nishes, the index contents must be consistent with the contents of the\nrelation, and all further updates to the relation must maintain the index.\nOne way of ensuring that the data and the index are consistent is to block all up-\ndates to the relation while the index is created, for example by getting a shared lock on\nthe relation. After the index is created, and the relation metadata are updated to re\ufb02ect\nthe existence of the index locks can be released. Subsequent update transactions will\n\ufb01nd the index, and carry out index maintenance as part of the transaction.\nHowever, the above approach would make the system unavailable for updates to\nthe relation for a very long time, which is unacceptable. Instead, most database systems\nsupport online index creation , which allows relation updates to occur even as the index\nis being created. Online index creation can be carried out as follows:\n1.Index creation gets a snapshot of the relation and uses it to create the index; mean-\nwhile, the system logs all updates to the relation that happen after the snapshot\nis created.\n2.When the index on the snapshot data is complete, it is not yet ready for use, since\nsubsequent updates are missing. At this point, the log of updates to the relation is\nused to update the index. But while the index update is being carried out, further\nupdates may be happening on the relation.\n3.The index update then obtains a shared lock on the relation to prevent further\nupdates and applies all remaining updates to the index. At this point, the index\nis consistent with the contents of the relation. The relation metadata are then\nupdated to indicate the existence of the new index. Subsequently all locks are\nreleased.\nAny transaction that executes after this will see the existence of the index; if the\ntransaction updates the relation, it will also update the index.\nCreation of materialized views that are maintained immediately, as part of the\ntransaction that updates any of the relations used in the view, can also bene\ufb01t from on-\nline construction techniques that are similar to online index construction. The query\nde\ufb01ning the view is executed on a snapshot of the participating relations, and subse-\nquent updates are logged. The updates are applied to the materialized view, with a \ufb01nal\nphase of locking and catching up similar to the case of online index creation.\nSchema changes such as adding or deleting attributes or constraints can also have\na signi\ufb01cant impact if relations are locked while the schema change is implemented on\nall tuples.\n\u2022For adding or deleting attributes, a version number can be kept with each tuple,\nand tuples can be updated in the background, or whenever they are accessed; the\nversion number is used to determine if the schema change has already been applied\n", "913": "18.10 Advanced Topics in Concurrency Control 885\nto the tuple, and the schema change is applied to the tuple if it has not already been\napplied.\n\u2022Adding of constraints requires that existing data must be checked to ensure that\nthe constraint is satis\ufb01ed. For example, adding a primary or unique key constraint\non an attribute IDrequires checking of existing tuples to ensure that no two tuples\nhave the same IDvalue. Online addition of such constraints is done in a manner\nsimilar to online index construction, by checking the constraints on a relation\nsnapshot, while keeping a log of updates that occur after the snapshot. The updates\nin the log must then be checked to ensure that they do not violate the constraint. In\na \ufb01nal catch-up phase, the constraint is checked on any remaining updates in the\nlog and added to the relation metadata while holding a shared lock on the relation.\n18.10.2 Concurrency in Index Structures\nIt is possible to treat access to index structures like any other database structure and to\napply the concurrency-control techniques discussed earlier. However, since indices are\naccessed frequently, they would become a point of great lock contention, leading to a\nlow degree of concurrency. Luckily, indices do not have to be treated like other database\nstructures; it is desirable to release index locks early, in a non-two-phase manner, to\nmaximize concurrency. In fact, it is perfectly acceptable for a transaction to perform\na lookup on an index twice and to \ufb01nd that the structure of the index has changed\nin between, as long as the index lookup returns the correct set of tuples. Informally,\nit is acceptable to have nonserializable concurrent access to an index, as long as the\naccuracy of the index is maintained; we formalize this notion next.\nOperation serializability for index operations is de\ufb01ned as follows: A concurrent\nexecution of index operations on an index is said to be serializable if there is a se-\nrialization order of the operations that is consistent with the results that each index\noperation in the concurrent execution sees, as well as with the \ufb01nal state of the index\nafter all the operations have been executed. Index concurrency control techniques must\nensure that any concurrent execution of index operations is serializable.\nWe outline two techniques for managing concurrent access to B+-trees as well as\nan index-concurrency control technique to prevent the phantom phenomenon. The on-\nline bibliographical notes reference other techniques for B+-trees as well as techniques\nfor other index structures. The techniques that we present for concurrency control on\nB+-trees are based on locking, but neither two-phase locking nor the tree protocol is\nemployed. The algorithms for lookup, insertion, and deletion are those used in Chapter\n14, with only minor modi\ufb01cations.\nThe \ufb01rst technique is called the crabbing protocol :\n\u2022When searching for a key value, the crabbing protocol \ufb01rst locks the root node in\nshared mode. When traversing down the tree, it acquires a shared lock on the child\nnode to be traversed further. After acquiring the lock on the child node, it releases\nthe lock on the parent node. It repeats this process until it reaches a leaf node.\n", "914": "886 Chapter 18 Concurrency Control\n\u2022When inserting or deleting a key value, the crabbing protocol takes these actions:\n\u00b0It follows the same protocol as for searching until it reaches the desired leaf\nnode. Up to this point, it obtains (and releases) only shared locks.\n\u00b0It locks the leaf node in exclusive mode and inserts or deletes the key value.\n\u00b0If it needs to split a node or coalesce it with its siblings, or redistribute key\nvalues between siblings, the crabbing protocol locks the parent of the node in\nexclusive mode. After performing these actions, it releases the locks on the\nnode and siblings.\nIf the parent requires splitting, coale scing, or redistribution of key values,\nthe protocol retains the lock on the parent, and splitting, coalescing, or redistri-\nbution propagates further in the same manner. Otherwise, it releases the lock\non the parent.\nThe protocol gets its name from the way in which crabs advance by moving side-\nways, moving the legs on one side, then the legs on the other, and so on alternately.\nThe progress of locking while the protocol both goes down the tree and goes back up\n(in case of splits, coalescing, or redistribution) proceeds in a similar crab-like manner.\nOnce a particular operation releases a lock on a node, other operations can access\nthat node. There is a possibility of deadlocks between search operations coming down\nthe tree, and splits, coalescing, or redistribution propagating up the tree. The system\ncan easily handle such deadlocks by restarting the search operation from the root, after\nreleasing the locks held by the operation.\nLocks that are held for a short duration, instead of being held in a two-phase man-\nner, are often referred to as latches . Latches are used internally in databases to achieve\nmutual exclusion on shared data structures. In the above case, locks are held in a way\nthat does not ensure mutual exclusion during an insert or delete operation, yet the\nresultant execution of index operations is serializable.\nThe second technique achieves even more concurrency, avoiding even holding the\nlock on one node while acquiring the lock on another node; thereby, deadlocks are\navoided, and concurrency is increased. This technique uses a modi\ufb01ed version of B+-\ntrees called B-link trees ; B-link trees require that every node (including internal nodes,\nnot just the leaves) maintain a pointer to its right sibling. This pointer is required be-\ncause a lookup that occurs while a node is being split may have to search not only that\nnode but also that node\u2019s right.\nUnlike the crabbing protocol, the B-link-tree locking protocol holds locks on only\none internal node at a time. The protocol releases the lock on the current internal\nnode before requesting a lock on a child node (when traversing downwards), or on a\nparent node (while traversing upwards during a split or merge). Doing so can result in\nanomalies: for example, between the time the lock on a node is released and the lock\non a parent is requested, a concurrent insert or delete on a sibling may cause a split\nor merge on the parent, and the original parent node may no longer be a parent of the\n", "915": "18.10 Advanced Topics in Concurrency Control 887\nchild node when it is locked. The protocol detects and handles such situations, ensuring\noperation serializability while avoiding deadlocks between operations and increasing\nconcurrency compared to the crabbing protocol.\nThe phantom phenomenon, where con\ufb02icts between a predicate read and an insert\nor update are not detected, can allow nonserializable executions to occur. The index-\nlocking technique, which we saw in Section 18.4.3, prevents the phantom phenomenon\nby locking index leaf nodes in a two-phase manner. Instead of locking an entire index\nleaf node, some index concurrency-control schemes use key-value locking on individual\nkey values, allowing other key values to be inserted or deleted from the same leaf. Key-\nvalue locking thus provides increased concurrency.\nUsing key-value locking na \u00a8\u0131vely, however, would allow the phantom phenomenon\nto occur; to prevent the phantom phenomenon, the next-key locking technique is used.\nIn this technique, every index lookup must lock not only the keys found within the\nr a n g e( o rt h es i n g l ek e y ,i nc a s eo fap o i n tl o o k u p )b u ta l s ot h en e x t - k e yv a l u e \u2014 t h a ti s ,\nthe key value just greater than the last key value that was within the range. Also, every\ninsert must lock not only the value that is inserted, but also the next-key value. Thus, if\na transaction attempts to insert a value that was within the range of the index lookup\nof another transaction, the two transactions would con\ufb02ict on the key value next to\nthe inserted key value. Similarly, deletes must also lock the next-key value to the value\nbeing deleted to ensure that con\ufb02icts with subsequent range lookups of other queries\nare detected.\n18.10.3 Concurrency Control in Main-Memory Databases\nWith data stored on hard disk, the cost of I/Ooperations often dominates the cost of\ntransaction processing. When disk I/Ois the bottleneck cost in a system, there is little\nbene\ufb01t from optimizing other smaller costs, such as the cost of concurrency control.\nHowever, in a main-memory database, with disk I/Ono longer the bottleneck, systems\nbene\ufb01t from reducing other costs, such as query processing costs, as we saw in Section\n15.8; we now consider how to reduce the cost of concurrency control in main-memory\ndatabases.\nAs we saw in Section 18.10.2, concurrency-control techniques for operations on\ndisk-based index structures acquire locks on individual nodes, to increase the potential\nfor concurrent access to the index. However, such locking comes at the increased cost\nof acquiring the locks. In a main-memory database, where data are in memory, index\noperations take very little time for execution. Thus, it may be acceptable to perform\nlocking at a coarse granularity: for example, the entire index could be locked using a\nsingle latch (i.e., short duration lock), the operation performed, and the latch released.\nThe reduced overhead of locking has been found to make up for the slightly reduced\nconcurrency, and to improve overall performance.\nThere is another way to improve performance with in-memory indices, using\natomic instructions to carry out index updates without acquiring any latches at all.\n", "916": "888 Chapter 18 Concurrency Control\ninsert (value ,head ){\nnode =new node\nnode\u2212>value =value\nnode\u2212>next =head\nhead =node\n}\nFigure 18.22 Insertion code that is unsafe with concurrent inserts.\nData structures implementations that support concurrent operations without requir-\ning latches are called latch-free data structure implementations.\nConsider a linked list, where each node has a value value and a next pointer, and\nthe head of the linked list is stored in the variable head. The function insert () shown in\nFigure 18.22 would work correctly to insert a node at the head of the list, if there are\nno concurrent invocations of the code for the same list.7\nHowever, if two processes execute the insert () function concurrently on the same\nlist, it is possible that both of them would read the same value of variable head,a n d\nthen both would update the variable after that. The \ufb01nal result would contain one of\nthe two nodes being inserted, while the other node being inserted would be lost.\nOne way of preventing such a problem is to get an exclusive latch (short term lock)\non the linked list, perform the insert () function, and then release the latch. The insert ()\nfunction can be modi\ufb01ed to acquire and release a latch on the list.\nAn alternative implementation, which is faster in practice, is to use an atomic\ncompare-and-swap () instruction, abbreviated to CAS, which works as follows: The in-\nstruction CAS( var,oldval ,newval ) takes three arguments: a variable varand two values,\noldval and newval . The instruction does the following atomically: check if the value of\nvaris equal to oldval ,a n di fs o ,s e t vartonewval , and return success. If the value is\nnot equal, it returns failure. The instruction is supported by most modern processor\narchitectures, and it executes very quickly.\nThe function insert\n latchfree (), shown in Figure 18.23 is a modi\ufb01cation of insert ()\nthat works correctly even with concurrent inserts on the same list, without obtaining\nany latches. With this code, if two processes concurrently read the old value of head ,\nand then both execute the CAS instruction, one of them will \ufb01nd the CAS instruction\nreturning success, while the other one will \ufb01nd it returning failure since the value of\nhead changes between the time it is read and when the CAS instruction is executed.\nThe repeat loop then retries the insert using the new value of head, until it succeeds.\nFunction delete\n latchfree (), shown in Figure 18.23, similarly implements deletion\nfrom the head of the list using the compare and swap instruction, without requiring\nlatches. (In this case, the list is used as a stack, since deletion occurs at the head of\n7We assume all parameters are passed by reference.\n", "917": "18.10 Advanced Topics in Concurrency Control 889\ninsert\n latchfree (head,value ){\nnode =new node\nnode\u2212>value =value\nrepeat\noldhead =head\nnode\u2212>next =oldhead\nresult =C A S ( head,oldhead ,node)\nuntil (result==success)\n}\ndelete\n latchfree (head ){\n/* This function is not quite safe; see explanation in text. */\nrepeat\noldhead =head\nnewhead =oldhead\u2212>next\nresult =C A S ( head,oldhead ,newhead )\nuntil (result==success)\n}\nFigure 18.23 Latch-free insertion and deletion on a list.\nthe list.) However, it has a problem: it does not work correctly in some rare cases. The\nproblem can occur when a process P1 is performing a delete, with node n1 at the head\nof the list, and concurrently a second process P2 deletes the \ufb01rst two elements, n1a n d\nn2, and then reinserts n1 at the head of the list, with some other element, say n3a s\nthe next element. If P1r e a d n1b e f o r e P2 deleted it, but performs the CAS after P2\nhas reinserted n1, the CAS operation of P1 will succeed, but set the head of the list\nto point to n2, which has been deleted, leaving the list in an inconsistent state. This\nproblem is known as the ABA problem .\nOne solution is to keep a counter along with each pointer, which is incremented\nevery time the pointer is updated. The CAS instruction is applied on the (pointer,\ncounter) pair; most CAS implementations on 64 bit processors support such a double\ncompare-and-swap on 128 bits. The ABA problem can then be avoided since although\nthe reinsert of n1 would result in the head pointing to n1 ,t h ec o u n t e rw o u l db ed i \ufb00 e r -\nent, resulting in the CAS operation of P1 failing. See the online solutions to Practice\nExercise 18.16 for more details of the ABA problem and the above solution. With such\na modi\ufb01cation, both inserts and deletes can be executed concurrently without acquir-\ning latches. There are other solutions that do not require a double compare-and-swap,\nb u ta r em o r ec o m p l i c a t e d .\nDeletion from the tail of the list (to implement a queue) as well as more complex\ndata structures such as hash indices and search trees can also be implemented in a latch-\n", "918": "890 Chapter 18 Concurrency Control\nfree manner. It is best to use latch-free dat a structure implementations (more often\nreferred to as lock-free data structure implementations) that are provided by standard\nlibraries, such as the Boost library for C++, or the ConcurrentLinkedQueue class in\nJava; do not build your own, since you may introduce bugs due to \u201c race conditions \u201d\nbetween concurrent accesses, that can be very hard to detect or debug.\nSince today\u2019s multiprocessor CPUsh a v eal a r g en u m b e ro fc o r e s ,l a t c h - f r e ei m p l e -\nmentations have been found to signi\ufb01cantly outperform implementations that obtain\nlatches, in the context of in-memory indices and other in-memory data structures\n18.10.4 Long-Duration Transactions\nThe transaction concept developed initially in the context of data-processing applica-\ntions, in which most transactions are noninteractive and of short duration. Serious\nproblems arise when this concept is applied to database systems that involve human\ninteraction. Such transactions have these key properties:\n\u2022Long duration . Once a human interacts with an active transaction, that transaction\nbecomes a long-duration transaction from the perspective of the computer, since\nhuman response time is slow relative to computer speed. Furthermore, in design\napplications, the human activity may involve hours, days, or an even longer period.\nThus, transactions may be of long duration in human terms, as well as in machine\nterms.\n\u2022Exposure of uncommitted data . Data generated and displayed to a user by a long-\nduration transaction are uncommitted, since the transaction may abort. Thus,\nusers\u2014and, as a result, other transactions\u2014may be forced to read uncommitted\ndata. If several users are cooperating on a project, user transactions may need to\nexchange data prior to transaction commit.\n\u2022Subtasks . An interactive transaction may consist of a set of subtasks initiated by\nthe user. The user may wish to abort a subtask without necessarily causing the\nentire transaction to abort.\n\u2022Recoverability . It is unacceptable to abort a long-duration interactive transaction\nbecause of a system crash. The active transaction must be recovered to a state that\nexisted shortly before the crash so that relatively little human work is lost.\n\u2022Performance . Good performance in an interactive transaction system is de\ufb01ned as\nfast response time. This de\ufb01nition is in contrast to that in a noninteractive system,\nin which high throughput (number of transactions per second) is the goal. Systems\nwith high throughput make e\ufb03cient use of system resources. However, in the case\nof interactive transactions, the most costly resource is the user. If the e\ufb03ciency and\nsatisfaction of the user are to be optimi zed, response time should be fast (from a\nhuman perspective). In those cases where a task takes a long time, response time\n", "919": "18.10 Advanced Topics in Concurrency Control 891\nT1\n T2\nread (A)\nA:=A\u221250\nwrite (A)\nread (B)\nB:=B\u221210\nwrite (B)\nread (B)\nB:=B+50\nwrite (B)\nread (A)\nA:=A+10\nwrite (A)\nFigure 18.24 A non-conflict-serializable schedule.\nshould be predictable (i.e., the variance in response times should be low) so that\nusers can manage their time well.\nSnapshot isolation, described in Section 18.8, can provide a partial solution to\nthese issues, as can the optimistic concurrency control without read validation protocol\ndescribed in Section 18.9.3. The latter pr otocol was in fact designed speci\ufb01cally to\ndeal with long-duration transactions that involve user interaction. Although it does\nnot guarantee serializability, optimistic concurrency control without read validation is\nquite widely used.\nHowever, when transactions are of long duration, con\ufb02icting updates are more\nlikely, resulting in additional waits or aborts. These considerations are the basis for the\nalternative concepts of correctness of concurrent executions and transaction recovery\nthat we consider in the remainder of this section.\n18.10.5 Concurrency Control with Operations\nConsider a bank database consisting of two accounts Aand B, with the consistency\nrequirement that the sum A+Bbe preserved. Consider the schedule of Figure 18.24.\nAlthough the schedule is not con\ufb02ict serializable, it nevertheless preserves the sum of\nA+B . It also illustrates two important points about the concept of correctness without\nserializability.\n1.Correctness depends on the speci\ufb01c consistency constraints for the database.\n2.Correctness depends on the properties of operations performed by each transac-\ntion.\n", "920": "892 Chapter 18 Concurrency Control\nWhile two-phase locking ensures serializability, it can result in poor concurrency\nin case a large number of transactions con\ufb02ict on a particular data item. Timestamp\nand validation protocols also have similar problems in this case.\nConcurrency can be increased by treating some operations besides read andwrite\nas fundamental low-level operations and to extend concurrency control to deal with\nthem.\nConsider the case of materialized view maintenance, which we saw in Section\n16.5.1. Suppose there is a relation sales(date, custID, itemID, amount ), and a materi-\nalized view daily\n sales\n total(date, total\n amount ), that records total sales on each day.\nEvery sales transaction must update the materialized view as part of the transaction\nif immediate view maintenance is used. With a high volume of sales, and every trans-\naction updating the same record in the daily\n sales\n total relation, the degree of concur-\nrency will be quite low if two-phase locking is used on the materialized view.\nA better way to perform concurrency control for the materialized view is as follows:\nObserve that each transaction increments a record in the daily\n sales\n total relation by\nsome value but does not need to see the value. It would make sense to have an operation\nincrement (v,n), that adds a value nto a variable vwithout making the value of vvisible\nto the transaction; we shall see shortly how this is implemented. In our sales example,\na transaction that inserts a sales tuple with amount ninvokes the increment operation\nwith the \ufb01rst argument being the total\n amount value of the appropriate tuple in the\nmaterialized view daily\n sales\n total, and the second argument being the value n.\nTheincrement operation does not lock the variable in a two-phase manner; how-\never, individual operations should be executed serially on the variable. Thus, if two\nincrement operations are initiated concurrently on the same variable, one must \ufb01n-\nish before the other is allowed to start. This can be ensured by acquiring an exclusive\nlatch (lock) on the variable vbefore starting the operation and releasing the latch after\nthe operation has \ufb01nished its updates. Increment operations can also be implemented\nusing compare-and-swap operations, without getting latches.\nTwo transactions that invoke the increment operation should be allowed to execute\nconcurrently to avoid concurrency control bottlenecks. In fact, increment operations\nexecuted by two transactions do not con\ufb02ict with each other, since the \ufb01nal result is\nt h es a m er e g a r d l e s so ft h eo r d e ri nw h i c ht h eo p e r a t i o n sw e r ee x e c u t e d .I fo n eo ft h e\ntransactions rolls back, the increment (v,n)o p e r a t i o nm u s tb er o l l e db a c kb ye x e c u t -\ning an operation increment (v,\u2212n), which adds a negative of the original value; this\noperation is referred to as a compensating operation .\nHowever, if a transaction Twishes to read the materialized view, it clearly con\ufb02icts\nwith any concurrent transaction that has performed an increment operation; the value\nthat Treads depends on whether the other transaction is serialized before or after T.\nWe can de\ufb01ne a locking protocol to handle the preceding situation by de\ufb01ning an\nincrement lock . The increment lock is compatible with itself but is not compatible with\nshared and exclusive locks. Figure 18.25 shows a lock-compatibility matrix for three\nlock modes: share mode, exclusive mode, and increment mode.\n", "921": "18.10 Advanced Topics in Concurrency Control 893\nS X I\nS\nX\nItrue\nfalse false\nfalse falsefalsefalse false\ntrue\nFigure 18.25 Lock-compatibility matrix with increment lock mode.\nAs another example of special-purpose concurrency control for operations, con-\nsider an insert operation on a B+-tree index which releases locks early, as we saw in\nSection 18.10.2. In this case, there is no special lock mode, but holding locks on leaf\nnodes in a two-phase manner (or using next-key locking) as we saw in Section 18.10.2\nensures serializability. The insert operation may have modi\ufb01ed several nodes of the B+-\ntree index. Other transactions may have read and updated these nodes further while\nprocessing other operations. To roll back the insertion, we would have to delete the\nrecord inserted by Ti; deletion is the compensating action for insertion. The result is a\ncorrect, consistent B+-tree, but not necessarily one with exactly the same structure as\nthe one we had before Tistarted.\nWhile operation locking can be done in a way that ensures serializability, in some\ncases it may even be used in a way that does not guarantee serializability, but where\nviolations may be acceptable. Consider the case of concert tickets, where every transac-\ntion needs to access and update the total ticket sales. We can have an operation incre-\nment\n conditional (v,n) which increments vbyn, provided the resultant value would be\n\u22650; the operation returns a status of success in case the resultant value is \u22650a n dr e -\nturns failure otherwise. Consider a transaction Tiexecuted to purchase tickets. To book\nthree tickets, where variable avail\n tickets indicates the number of available tickets, the\ntransaction can execute increment\n conditional (avail\n tickets ,\u22123). A return value of\nsuccess indicates that there were enough tickets available, and decrements the avail-\nable tickets, while failure indicates i nsu\ufb03cient availability of tickets.\nIf the variable avail\n tickets is locked in a two-phase manner, concurrency would be\nvery poor, with customers being forced to wait for bookings while an earlier transaction\ncommits, even when there are many tickets available. Concurrency can be greatly in-\ncreased by executing the increment\n conditional operation, without holding any locks\nonavail\n tickets in a two-phase manner; instead, an exclusive lock is obtained on the\nvariable, the operation is performed, and the lock is then released.\nThe transaction Tialso needs to carry out other steps, such as collecting the pay-\nment; if one of the subsequent steps, such as payment, fails, the increment operation\nmust be rolled back by executing a compensating operation; if the original operation\nadded\u2212ntoavail\n tickets , the compensating operation adds +ntoavail\n tickets .\nIt may appear that two increment\n conditional operations are compatible with\neach other, similar to the increment operation that we saw earlier. But that is not\n", "922": "894 Chapter 18 Concurrency Control\nthe case. Consider two concurrent transactions to purchase a single ticket, and assume\nthat there is only one ticket left. The order in which the operations are executed has an\nobvious impact on which one succeeds and which one fails. Nevertheless, many real-\nworld applications allow operations that hold short-term locks while they execute and\nrelease them at the end of the operation to increase concurrency, even at the cost of\nloss of serializability in some situations.\n18.10.6 Real-Time Transaction Systems\nIn certain applications, the constraints include deadlines by which a task must be com-\npleted. Examples of such applications include plant management, tra\ufb03c control, and\nscheduling. When deadlines are included, correctness of an execution is no longer\nsolely an issue of database consistency. Rather, we are concerned with how many dead-\nlines are missed, and by how much time they are missed. Deadlines are characterized\nas follows:\n\u2022Hard deadline .S e r i o u sp r o b l e m s ,s u c ha ss y s t e mc r a s h ,m a yo c c u ri fat a s ki sn o t\ncompleted by its deadline.\n\u2022Firm deadline . The task has zero value if it is completed after the deadline.\n\u2022Soft deadlines . The task has diminishing value if it is completed after the deadline,\nwith the value approaching zero as the degree of lateness increases.\nSystems with deadlines are called real-time systems .\nTransaction management in real-time systems must take deadlines into account.\nIf the concurrency-control protocol determines that a transaction Timust wait, it may\ncause Tito miss the deadline. In such cases, it may be preferable to pre-empt the trans-\naction holding the lock, and to allow Tito proceed. Pre-emption must be used with\ncare, however, because the time lost by the pre-empted transaction (due to rollback\nand restart) may cause the pre-empted transaction to miss its deadline. Unfortunately,\nit is di\ufb03cult to determine whether rollback or waiting is preferable in a given situation.\nDue to the unpredictable nature of delays when reading data from disk, main-\nmemory databases are often used if real-time constraints have to be met. However, even\nif data are resident in main memory, variances in execution time arise from lock waits,\ntransaction aborts, and so on. Researchers have devoted considerable e\ufb00ort to concur-\nrency control for real-time databases. They have extended locking protocols to provide\nhigher priority for transactions with early deadlines. They have found that optimistic\nconcurrency protocols perform well in real-time databases; that is, these protocols re-\nsult in fewer missed deadlines than even the extended locking protocols. The online\nbibliographical notes provide references to research in the area of real-time databases.\n18.11 Summary\n\u2022When several transactions execute concurrently in the database, the consistency\nof data may no longer be preserved. It is necessary for the system to control the in-\n", "923": "18.11 Summary 895\nteraction among the concurrent transactions, and this control is achieved through\no n eo fav a r i e t yo fm e c h a n i s m sc a l l e d concurrency-control schemes.\n\u2022To ensure serializability, we can use various concurrency-control schemes. All\nthese schemes either delay an operation or abort the transaction that issued the\noperation. The most common ones are locking protocols, timestamp-ordering\nschemes, validation techniques, and multiversion schemes.\n\u2022A locking protocol is a set of rules that state when a transaction may lock and\nunlock each of the data items in the database.\n\u2022The two-phase locking protocol allows a transaction to lock a new data item only\nif that transaction has not yet unlocked any data item. The protocol ensures serial-\nizability, but not deadlock freedom. In the absence of information concerning the\nmanner in which data items are accessed, the two-phase locking protocol is both\nnecessary and su\ufb03cient for ensuring serializability.\n\u2022The strict two-phase locking protocol permits release of exclusive locks only at\nthe end of transaction, in order to ensure recoverability and cascadelessness of\nthe resulting schedules. The rigorous two-phase locking protocol releases all locks\nonly at the end of the transaction.\n\u2022Various locking protocols do not guard against deadlocks. One way to prevent\ndeadlock is to use an ordering of data items and to request locks in a sequence\nconsistent with the ordering.\n\u2022Another way to prevent deadlock is to use preemption and transaction rollbacks.\nTo control the preemption, we assign a unique timestamp to each transaction. The\nsystem uses these timestamps to decide whether a transaction should wait or roll\nback. The wound\u2013wait scheme is a preemptive scheme.\n\u2022If deadlocks are not prevented, the system must deal with them by using a deadlock\ndetection and recovery scheme. To do so, the system constructs a wait-for graph.\nA system is in a deadlock state if and only if the wait-for graph contains a cycle.\nWhen the deadlock detection algorithm determines that a deadlock exists, the\nsystem rolls back one or more transactions to break the deadlock.\n\u2022There are circumstances where it would be advantageous to group several data\nitems and to treat them as one aggregate data item for purposes of working, re-\nsulting in multiple levels of granularity. We allow data items of various sizes, and\nwe de\ufb01ne a hierarchy of data items where the small items are nested within larger\nones. Such a hierarchy can be represented graphically as a tree. In such multi-\ngranularity locking protocols, locks are ac quired in root-to-leaf order; they are re-\nleased in leaf-to-root order. Intention lock modes are used at higher levels to get\nbetter concurrency, without a\ufb00ecting serializability.\n", "924": "896 Chapter 18 Concurrency Control\n\u2022A timestamp-ordering scheme ensures serializability by selecting an ordering in\nadvance between every pair of transactions. A unique \ufb01xed timestamp is associated\nwith each transaction in the system. The timestamps of the transactions determine\nthe serializability order. Thus, if the timestamp of transaction Tiis smaller than the\ntimestamp of transaction Tj, then the scheme ensures that the produced schedule\nis equivalent to a serial schedule in which transaction Tiappears before transaction\nTj. It does so by rolling back a transaction whenever such an order is violated.\n\u2022A validation scheme is an appropriate concurrency-control method in cases where\na majority of transactions are read-only transactions, and thus the rate of con\ufb02icts\namong these transactions is low. A unique \ufb01xed timestamp is associated with each\ntransaction in the system. The serializability order is determined by the timestamp\nof the transaction. A transaction in this scheme is never delayed. It must, however,\npass a validation test to complete. If it does not pass the validation test, the system\nrolls it back to its initial state.\n\u2022A multiversion concurrency-control scheme is based on the creation of a new ver-\nsion of a data item for each transaction that writes that item. When a read opera-\ntion is issued, the system selects one of the versions to be read. The concurrency-\ncontrol scheme ensures that the version to be read is selected in a manner that\nensures serializability by using timestamps. A read operation always succeeds.\n\u00b0In multiversion timestamp ordering, a write operation may result in the rollback\nof the transaction.\n\u00b0In multiversion two-phase locking, write operations may result in a lock wait\nor, possibly, in deadlock.\n\u2022Snapshot isolation is a multiversion concurrency-control protocol based on valida-\ntion, which, unlike multiversion two-phase locking, does not require transactions\nto be declared as read-only or update. Snapshot isolation does not guarantee se-\nrializability but is nevertheless supported by many database systems. Serializable\nsnapshot isolation is an extension of snapshot isolation which guarantees serializ-\nability.\n\u2022Adelete operation may be performed only if the transaction deleting the tuple has\na ne x c l u s i v el o c ko nt h et u p l et ob ed e l e t e d .At r a n s a c t i o nt h a ti n s e r t san e wt u p l e\ninto the database is given an exclusive lock on the tuple.\n\u2022Insertions can lead to the phantom phenomenon, in which an insertion logically\ncon\ufb02icts with a query even though the two transactions may access no tuple in\ncommon. Such con\ufb02ict cannot be detected if locking is done only on tuples ac-\ncessed by the transactions. Locking is required on the data used to \ufb01nd the tuples\nin the relation. The index-locking technique solves this problem by requiring locks\non certain index nodes. These locks ensure that all con\ufb02icting transactions con\ufb02ict\no nar e a ld a t ai t e m ,r a t h e rt h a no nap h a n t o m .\n", "925": "Review Terms 897\n\u2022Weak levels of consistency are used in some applications where consistency\nof query results is not critical, and using serializability would result in queries\nadversely a\ufb00ecting transaction processing. Degree-two consistency is one such\nweaker level of consistency; cursor stability is a special case of degree-two con-\nsistency and is widely used.\n\u2022Concurrency control is a challenging task for transactions that span user interac-\ntions. Applications often implement a scheme based on validation of writes using\nversion numbers stored in tuples; this scheme provides a weak level of serializabil-\nity and can be implemented at the application level without modi\ufb01cations to the\ndatabase.\n\u2022Special concurrency-control techniques can be developed for special data struc-\ntures. Often, special techniques are applied in B+-trees to allow greater concur-\nrency. These techniques allow nonserializable access to the B+-tree, but they en-\nsure that the B+-tree structure is correct, and they ensure that accesses to the\ndatabase itself are serializable. Latch-free data structures are used to implement\nhigh-performance indices and other data structures in main-memory databases.\nReview Terms\n\u2022Concurrency control\n\u2022Lock types\n\u00b0Shared-mode (S) lock\n\u00b0Exclusive-mode (X) lock\n\u2022Lock\n\u00b0Compatibility\n\u00b0Request\n\u00b0Wait\n\u00b0Grant\n\u2022Deadlock\n\u2022Starvation\n\u2022Locking protocol\n\u2022Legal schedule\n\u2022Two-phase locking protocol\n\u00b0Growing phase\n\u00b0Shrinking phase\n\u00b0Lock point\u00b0Strict two-phase locking\n\u00b0Rigorous two-phase locking\n\u2022Lock conversion\n\u00b0Upgrade\n\u00b0Downgrade\n\u2022Graph-based protocols\n\u00b0Tree protocol\n\u00b0Commit dependency\n\u2022Deadlock handling\n\u00b0Prevention\n\u00b0Detection\n\u00b0Recovery\n\u2022Deadlock prevention\n\u00b0Ordered locking\n\u00b0Preemption of locks\n\u00b0Wait\u2013die scheme\n", "926": "898 Chapter 18 Concurrency Control\n\u00b0Wound\u2013wait scheme\n\u00b0Timeout-based schemes\n\u2022Deadlock detection\n\u00b0Wait-for graph\n\u2022Deadlock recovery\n\u00b0Total rollback\n\u00b0Partial rollback\n\u2022Multiple granularity\n\u00b0Explicit locks\n\u00b0Implicit locks\n\u00b0Intention locks\n\u2022Intention lock modes\n\u00b0Intention-shared (IS)\n\u00b0Intention-exclusive (IX)\n\u00b0Shared and intention-\nexclusive (SIX)\n\u2022Multiple-granularity locking\nprotocol\n\u2022Timestamp\n\u00b0System clock\n\u00b0Logical counter\n\u00b0W-timestamp( Q)\n\u00b0R-timestamp( Q)\n\u2022Timestamp-ordering protocol\n\u00b0Thomas\u2019 write rule\n\u2022Validation-based protocols\n\u00b0Read phase\n\u00b0Validation phase\u00b0Write phase\n\u00b0Validation test\n\u2022Multiversion timestamp ordering\n\u2022Multiversion two-phase locking\n\u00b0Read-only transactions\n\u00b0Update transactions\n\u2022Snapshot isolation\n\u00b0Lost update\n\u00b0First committer wins\n\u00b0First updater wins\n\u00b0Write skew\n\u00b0Select for update\n\u2022Insert and delete operations\n\u2022Phantom phenomenon\n\u2022Index-locking protocol\n\u2022Predicate locking\n\u2022Weak levels of consistency\n\u00b0Degree-two consistency\n\u00b0Cursor stability\n\u2022Optimistic concurrency control with-\nout read validation\n\u2022Conversations\n\u2022Concurrency in indices\n\u00b0Crabbing protocol\n\u00b0B-link trees\n\u00b0B-link-tree locking protocol\n\u00b0Next-key locking\n\u2022Latch-free data structures\n\u2022Compare-and-swap ( CAS)i n s t r u c t i o n\n", "927": "Practice Exercises 899\nPractice Exercises\n18.1 Show that the two-phase locking protocol ensures con\ufb02ict serializability and\nthat transactions can be serialized according to their lock points.\n18.2 Consider the following two transactions:\nT34:read (A);\nread (B);\nifA=0then B:=B+1 ;\nwrite (B).\nT35:read (B);\nread (A);\nifB=0then A:=A+1 ;\nwrite (A).\nAdd lock and unlock instructions to transactions T31and T32so that they ob-\nserve the two-phase locking protocol. Can the execution of these transactions\nresult in a deadlock?\n18.3 What bene\ufb01t does rigorous two-phase locking provide? How does it compare\nwith other forms of two-phase locking?\n18.4 Consider a database organized in the form of a rooted tree. Suppose that we\ninsert a dummy vertex between each pair of vertices. Show that, if we follow\nthe tree protocol on the new tree, we get better concurrency than if we follow\nt h et r e ep r o t o c o lo nt h eo r i g i n a lt r e e .\n18.5 Show by example that there are schedules possible under the tree protocol that\nare not possible under the two-phase locking protocol, and vice versa.\n18.6 Locking is not done explicitly in persistent programming languages. Rather,\nobjects (or the corresponding pages) must be locked when the objects are ac-\ncessed. Most modern operating systems allow the user to set access protections\n(no access, read, write) on pages, and memory access that violate the access\nprotections result in a protection violation (see the Unix mprotect command,\nfor example). Describe how the access-protection mechanism can be used for\npage-level locking in a persistent programming language.\n18.7 Consider a database system that includes an atomic increment operation, in\naddition to the read andwrite operations. Let Vbe the value of data item X.\nThe operation\nincrement (X)b y C\n", "928": "900 Chapter 18 Concurrency Control\nsets the value of XtoV+Cin an atomic step. The value of Xis not available\nto the transaction unless the latter executes a read (X).\nAssume that increment operations lock the item in increment mode using the\ncompatibility matrix in Figure 18.25.\na. Show that, if all transactions lock the data that they access in the corre-\nsponding mode, then two-phase locking ensures serializability.\nb. Show that the inclusion of increment mode locks allows for increased\nconcurrency.\n18.8 In timestamp ordering, W-timestamp (Q) denotes the largest timestamp of any\ntransaction that executed write (Q) successfully. Suppose that, instead, we de-\n\ufb01ned it to be the timestamp of the most recent transaction to execute write (Q)\nsuccessfully. Would this change in wording make any di\ufb00erence? Explain your\nanswer.\n18.9 Use of multiple-granularity locking may require more or fewer locks than an\nequivalent system with a single lock granularity. Provide examples of both sit-\nuations, and compare the relative amount of concurrency allowed.\n18.10 For each of the following protocols, describe aspects of practical applications\nthat would lead you to suggest using the protocol, and aspects that would sug-\ngest not using the protocol:\n\u2022Two-phase locking\n\u2022Two-phase locking with multiple-granularity locking.\n\u2022T h et r e ep r o t o c o l\n\u2022Timestamp ordering\n\u2022Validation\n\u2022Multiversion timestamp ordering\n\u2022Multiversion two-phase locking\n18.11 Explain why the following technique for transaction execution may provide\nbetter performance than just using strict two-phase locking: First execute the\ntransaction without acquiring any locks and without performing any writes\nto the database as in the validation-based techniques, but unlike the validation\ntechniques do not perform either validation or writes on the database. Instead,\nrerun the transaction using strict two-phase locking. (Hint: Consider waits for\ndisk I/O.)\n18.12 Consider the timestamp-ordering protocol, and two transactions, one that\nwrites two data items pand q, and another that reads the same two data items.\n", "929": "Practice Exercises 901\nGive a schedule whereby the timestamp test for a write operation fails and\ncauses the \ufb01rst transaction to be restarted, in turn causing a cascading abort\nof the other transaction. Show how this could result in starvation of both trans-\nactions. (Such a situation, where two or more processes carry out actions, but\nare unable to complete their task because of interaction with the other pro-\ncesses, is called a livelock .)\n18.13 Devise a timestamp-based protocol that avoids the phantom phenomenon.\n18.14 Suppose that we use the tree protocol of Section 18.1.5 to manage concurrent\naccess to a B+-tree. Since a split may occur on an insert that a\ufb00ects the root, it\na p p e a r st h a ta ni n s e r to p e r a t i o nc a n n o tr e l e a s ea n yl o c k su n t i li th a sc o m p l e t e d\nthe entire operation. Under what circumstances is it possible to release a lock\nearlier?\n18.15 The snapshot isolation protocol uses a validation step which, before perform-\ning a write of a data item by transaction T, checks if a transaction concurrent\nwith Thas already written the data item.\na. A straightforward implementation uses a start timestamp and a commit\ntimestamp for each transaction, in addition to an update set ,t h a t ,i st h e\nset of data items updated by the transaction. Explain how to perform\nvalidation for the \ufb01rst-committer-wins scheme by using the transaction\ntimestamps along with the update sets. You may assume that validation\nand other commit processing steps are executed serially, that is, for one\ntransaction at a time,\nb. Explain how the validation step can be implemented as part of commit\nprocessing for the \ufb01rst-committer-wins scheme, using a modi\ufb01cation of\nthe above scheme, where instead of using update sets, each data item\nhas a write timestamp associated with it. Again, you may assume that\nvalidation and other commit processing steps are executed serially.\nc. The \ufb01rst-updater-wins scheme can be implemented using timestamps as\ndescribed above, except that validation is done immediately after acquir-\ning an exclusive lock, instead of being done at commit time.\ni. Explain how to assign write timestamps to data items to implement\nthe \ufb01rst-updater-wins scheme.\nii. Show that as a result of locking, if the validation is repeated at com-\nmit time the result would not change.\niii. Explain why there is no need to perform validation and other commit\nprocessing steps serially in this case.\n18.16 Consider functions insert\n latchfree () and delete\n latchfree (), shown in Figure\n18.23.\n", "930": "902 Chapter 18 Concurrency Control\na. Explain how the ABA problem can occur if a deleted node is reinserted.\nb. Suppose that adjacent to head we store a counter cnt. Also suppose that\nDCAS(( head,cnt ), (oldhead, oldcnt ), (newhead, newcnt )) atomically per-\nforms a compare-and-swap on the 128 bit value ( head,cnt ). Modify the in-\nsert\n latchfree () and delete\n latchfree () to use the DCAS operation to avoid\nthe ABA problem.\nc. Since most processors use only 48 bits of a 64 bit address to actually\naddress memory, explain how the other 16 bits can be used to implement\na counter, in case the DCAS operation is not supported.\nExercises\n18.17 What bene\ufb01t does strict two-phase locking provide? What disadvantages re-\nsult?\n18.18 Most implementations of database systems use strict two-phase locking. Sug-\ngest three reasons for the popularity of this protocol.\n18.19 Consider a variant of the tree protocol called the forest protocol. The database\nis organized as a forest of rooted trees. Each transaction Timust follow the\nfollowing rules:\n\u2022The \ufb01rst lock in each tree may be on any data item.\n\u2022The second, and all subsequent, locks in a tree may be requested only if\nthe parent of the requested node is currently locked.\n\u2022Data items may be unlocked at any time.\n\u2022Ad a t ai t e mm a yn o tb er e l o c k e db y Tiafter it has been unlocked by Ti.\nShow that the forest protocol does notensure serializability.\n18.20 Under what conditions is it less expensive to avoid deadlock than to allow\ndeadlocks to occur and then to detect them?\n18.21 If deadlock is avoided by deadlock-avoidance schemes, is starvation still possi-\nble? Explain your answer.\n18.22 In multiple-granularity locking, what is the di\ufb00erence between implicit and\nexplicit locking?\n18.23 Although SIXmode is useful in multiple-granularity locking, an exclusive and\nintention-shared ( XIS) mode is of no use. Why is it useless?\n18.24 The multiple-granularity protocol rules specify that a transaction Tican lock a\nnode Qin S or ISmode only if Ticurrently has the parent of Qlocked in either\n", "931": "Exercises 903\nIXorISmode. Given that SIXand S locks are stronger than IXorISlocks,\nwhy does the protocol not allow locking a node in S or ISmode if the parent\nis locked in either SIXor S mode?\n18.25 Suppose the lock hierarchy for a database consists of database, relations, and\ntuples.\na. If a transaction needs to read a lot of tuples from a relation r,w h a tl o c k s\nshould it acquire?\nb. Now suppose the transaction wants to update a few of the tuples in r\nafter reading a lot of tuples. What locks should it acquire?\nc. If at run-time the transaction \ufb01nds that it needs to actually update a very\nlarge number of tuples (after acquiring locks assuming only a few tuples\nwould be updated). What problems would this cause to the lock table,\nand what could the database do to avoid the problem?\n18.26 When a transaction is rolled-back under timestamp ordering, it is assigned a\nnew timestamp. Why can it not simply keep its old timestamp?\n18.27 Show that there are schedules that are possible under the two-phase locking\nprotocol but not possible under the timestamp protocol, and vice versa.\n18.28 Under a modi\ufb01ed version of the timestamp protocol, we require that a commit\nbit be tested to see whether a read request must wait. Explain how the com-\nmit bit can prevent cascading abort. Why is this test not necessary for write\nrequests?\n18.29 As discussed in Exercise 18.15, snapshot isolation can be implemented using\na form of timestamp validation. However, unlike the multiversion timestamp-\nordering scheme, which guarantees serializability, snapshot isolation does not\nguarantee serializability. Explain the key di\ufb00erence between the protocols that\nresults in this di\ufb00erence.\n18.30 Outline the key similarities and di\ufb00erences between the timestamp-based im-\nplementation of the \ufb01rst-committer-wins version of snapshot isolation, de-\nscribed in Exercise 18.15, and the optimistic-concurrency control-without-read-\nvalidation scheme, described in Section 18.9.3.\n18.31 Consider a relation r(A,B,C) and a transaction Tthat does the following: \ufb01nd\nthe maximum Avalue in r,a n di n s e r tan e wt u p l ei n rwhose Avalue is 1+the\nmaximum Avalue. Assume that an index is used to \ufb01nd the maximum Avalue.\na. Suppose that the transaction locks each tuple it reads in S mode, and\nthe tuple it creates in X mode, and performs no other locking. Now sup-\npose two instances of Tare run concurrently. Explain how the resultant\nexecution could be non-serializable.\n", "932": "904 Chapter 18 Concurrency Control\nb. Now suppose that r.Ais declared as a primary key. Can the above non-\nserializable execution occur in this case? Explain why or why not.\n18.32 Explain the phantom phenomenon. Why may this phenomenon lead to an\nincorrect concurrent execution despite the use of the two-phase locking proto-\ncol?\n18.33 Explain the reason for the use of degree-two consistency. What disadvantages\ndoes this approach have?\n18.34 Give example schedules to show that with key-value locking, if lookup, insert,\nor delete does not lock the next-key value, the phantom phenomenon could go\nundetected.\n18.35 Many transactions update a common item (e.g., the cash balance at a branch)\nand private items (e.g., individual account balances). Explain how you can in-\ncrease concurrency (and throughput) by ordering the operations of the trans-\naction.\n18.36 Consider the following locking protocol: All items are numbered, and once\nan item is unlocked, only higher-numbered items may be locked. Locks may\nbe released at any time. Only X-locks are used. Show by an example that this\nprotocol does not guarantee serializability.\nFurther Reading\n[Gray and Reuter (1993)] provides detailed textbook coverage of transaction-\nprocessing concepts, including concurrenc y-control concepts and implementation de-\ntails. [Bernstein and Newcomer (2009)] provides textbook coverage of various aspects\nof transaction processing including concurrency control.\nThe two-phase locking protocol was introduced by [Eswaran et al. (1976)]. The\nlocking protocol for multiple-granularity data items is from [Gray et al. (1975)]. The\ntimestamp-based concurrency-control scheme is from [Reed (1983)]. The validation\nconcurrency-control scheme is from [Kung and Robinson (1981)]. Multiversion times-\ntamp order was introduced in [Reed (1983)]. A multiversion tree-locking algorithm\nappears in [Silberschatz (1982)].\nDegree-two consistency was introduced in [Gray et al. (1975)]. The levels of con-\nsistency\u2014or isolation\u2014o\ufb00ered in SQL are explained and critiqued in [Berenson et al.\n(1995)]; the snapshot isolation technique was also introduced in the same paper. Seri-\nalizable snapshot-isolation was introduced by [Cahill et al. (2009)]; [Ports and Grittner\n(2012)] describes the implementation o f serializable snapshot isolation in Postgre SQL.\nConcurrency in B+-trees was studied by [Bayer and Schkolnick (1977)] and [John-\nson and Shasha (1993)]. The crabbing and B-link tree techniques were introduced by\n[Kung and Lehman (1980)] and [Lehman and Yao (1981)]. The technique of key-value\nlocking used in ARIES provides for very high concurrency on B+-tree access and is de-\n", "933": "Further Reading 905\nscribed in [Mohan (1990)] and [Mohan and Narang (1992)]. [Faerber et al. (2017)]\nprovide a survey of main-memory databases, including coverage of concurrency con-\ntrol in main-memory databases. The ABA problem with latch-free data structures as\nwell as solutions for the problem are discussed in [Dechev et al. (2010)].\nBibliography\n[Bayer and Schkolnick (1977)] R. Bayer and M. Schkolnick, \u201cConcurrency of Operating on\nB-trees\u201d, Acta Informatica , Volume 9, Number 1 (1977), pages 1\u201321.\n[Berenson et al. (1995)] H. Berenson, P. Bernstein, J. Gray, J. Melton, E. O\u2019Neil, and\nP. O\u2019Neil, \u201cA Critique of ANSI SQL Isolation Levels\u201d, In Proc. of the ACM SIGMOD Conf.\non Management of Data (1995), pages 1\u201310.\n[Bernstein and Newcomer (2009)] P. A. Bernstein and E. Newcomer, Principles of Transaction\nProcessing , 2nd edition, Morgan Kaufmann (2009).\n[Cahill et al. (2009)] M. J. Cahill, U. R\u00a8 ohm, and A. D. Fekete, \u201cSerializable isolation for\nsnapshot databases\u201d, ACM Transactions on Database Systems , Volume 34, Number 4 (2009),\npages 20:1\u201320:42.\n[Dechev et al. (2010)] D. Dechev, P. Pirkelbauer, and B. Stroustrup, \u201cUnderstanding and Ef-\nfectively Preventing the ABA Problem in Descriptor-Based Lock-Free Designs\u201d, In IEEE\nInt\u2019l Symp. on Object/Component/Service-Orien ted Real-Time Distributed Computing, (ISORC)\n(2010), pages 185\u2013192.\n[Eswaran et al. (1976)] K. P. Eswaran, J. N. Gray, R. A. Lorie, and I. L. Traiger, \u201cThe Notions\nof Consistency and Predicate Locks in a Database System\u201d, Communications of the ACM ,\nVolume 19, Number 11 (1976), pages 624\u2013633.\n[Faerber et al. (2017)] F. Faerber, A. Kemper, P.-A. Larson, J. Levandoski, T. Neumann, and\nA. Pavlo, \u201cMain Memory Database Systems\u201d, Foundations and Trends in Databases ,V o l u m e\n8, Number 1-2 (2017), pages 1\u2013130.\n[Gray and Reuter (1993)] J. Gray and A. Reuter, Transaction Processing: Concepts and Tech-\nniques , Morgan Kaufmann (1993).\n[Gray et al. (1975)] J. Gray, R. A. Lorie, and G. R. Putzolu, \u201cGranularity of Locks and De-\ngrees of Consistency in a Shared Data Base\u201d, In Proc. of the International Conf. on Very Large\nDatabases (1975), pages 428\u2013451.\n[Johnson and Shasha (1993)] T. Johnson and D. Shasha, \u201cThe Performance of Concurrent\nB-Tree Algorithms\u201d, ACM Transactions on Database Systems , Volume 18, Number 1 (1993),\npages 51\u2013101.\n[Kung and Lehman (1980)] H. T. Kung and P. L. Lehman, \u201cConcurrent Manipulation of Bi-\nnary Search Trees\u201d, ACM Transactions on Database Systems , Volume 5, Number 3 (1980),\npages 339\u2013353.\n", "934": "906 Chapter 18 Concurrency Control\n[Kung and Robinson (1981)] H. T. Kung and J. T. Robinson, \u201cOptimistic Concurrency Con-\ntrol\u201d, ACM Transactions on Database Systems , Volume 6, Number 2 (1981), pages 312\u2013326.\n[Lehman and Yao (1981)] P. L. Lehman and S. B. Yao, \u201cE\ufb03cient Locking for Concurrent\nOperations on B-trees\u201d, ACM Transactions on Database Systems , Volume 6, Number 4 (1981),\npages 650\u2013670.\n[Mohan (1990)] C. Mohan, \u201cARIES/KVL: A Key-Value Locking Method for Concurrency\nControl of Multiaction Transactions Operations on B-Tree indexes\u201d, In Proc. of the Interna-\ntional Conf. on Very Large Databases (1990), pages 392\u2013405.\n[Mohan and Narang (1992)] C. Mohan and I. Narang, \u201cE\ufb03cient Locking and Caching of\nData in the Multisystem Shared Disks Transaction Environment\u201d, In Proc. of the International\nConf. on Extending Database Technology (1992), pages 453\u2013468.\n[Ports and Grittner (2012)] D. R. K. Ports and K. Grittner, \u201cSerializable Snapshot Isolation\nin PostgreSQL\u201d, Proceedings of the VLDB Endowment , Volume 5, Number 12 (2012), pages\n1850\u20131861.\n[Reed (1983)] D. Reed, \u201cImplementing Atomic Actions on Decentralized Data\u201d, Transac-\ntions on Computer Systems , Volume 1, Number 1 (1983), pages 3\u201323.\n[Silberschatz (1982)] A. Silberschatz, \u201cA Multi-Version Concurrency Control Scheme With\nNo Rollbacks\u201d, In Proc. of the ACM Symposium on Principles of Distributed Computing (1982),\npages 216\u2013223.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "935": "CHAPTER19\nRecovery System\nA computer system, like any other device, is subject to failure from a variety of causes:\ndisk crash, power outage, software error, a \ufb01re in the machine room, even sabotage.\nIn any failure, information may be lost. Therefore, the database system must take ac-\ntions in advance to ensure that the atomicity and durability properties of transactions,\nintroduced in Chapter 17, are preserved. An integral part of a database system is a re-\ncovery scheme that can restore the database to the consistent state that existed before\nthe failure.\nThe recovery scheme must also support high availability , that is, the database\nshould be usable for a very high percentage of time. To support high availability in\nthe face of machine failure (as also planned machine shutdowns for hardware/software\nupgrades and maintenance), the recovery scheme must support the ability to keep a\nbackup copy of the database synchronized with the current contents of the primary\ncopy of the database. If the machine with the primary copy fails, transaction process-\ning can continue on the backup copy.\n19.1Failure Classification\nThere are various types of failure that may occur in a system, each of which needs to be\ndealt with in a di\ufb00erent manner. In this chapter, we shall consider only the following\ntypes of failure:\n\u2022Transaction failure . There are two types of errors that may cause a transaction to\nfail:\n\u00b0Logical error . The transaction can no longer continue with its normal execution\nbecause of some internal condition, such as bad input, data not found, over\ufb02ow,\nor resource limit exceeded.\n\u00b0System error . The system has entered an undesirable state (e.g., deadlock), as\na result of which a transaction cannot continue with its normal execution. The\ntransaction, however, can be reexecuted at a later time.\n907\n", "936": "908 Chapter 19 Recovery System\n\u2022System crash . There is a hardware malfunction, or a bug in the database software\nor the operating system, that causes the loss of the content of volatile storage and\nbrings transaction processing to a halt. The content of non-volatile storage remains\nintact and is not corrupted.\nThe assumption that hardware errors and bugs in the software bring the system\nto a halt, but do not corrupt the non-volatile storage contents, is known as the\nfail-stop assumption . Well-designed systems have numerous internal checks, at the\nhardware and the software level, that bring the system to a halt when there is an\nerror. Hence, the fail-stop assumption is a reasonable one.\n\u2022Disk failure . A disk block loses its content as a result of either a head crash or fail-\nure during a data-transfer operation. Copies of the data on other disks, or archival\nbackups on tertiary media, such as DVD or tapes, are used to recover from the\nfailure.\nTo determine how the system should recover from failures, we need to identify the\nfailure modes of those devices used for storing data. Next, we must consider how these\nfailure modes a\ufb00ect the contents of the database. We can then propose algorithms\nto ensure database consistency and transaction atomicity despite failures. These algo-\nrithms, known as recovery algorithms, have two parts:\n1.Actions taken during normal transaction processing to ensure that enough infor-\nmation exists to allow recovery from failures.\n2.Actions taken after a failure to recover the database contents to a state that en-\nsures database consistency, transaction atomicity, and durability.\n19.2 Storage\nAs we saw in Chapter 13, the various data items in the database may be stored and\naccessed in a number of di\ufb00erent storage media. In Section 17.3, we saw that storage\nmedia can be distinguished by their relative speed, capacity, and resilience against fail-\nure. We identi\ufb01ed three categories of storage:\n1. Volatile storage\n2. Non-Volatile storage\n3. Stable storage\nStable storage or, more accurately, an approximation thereof, plays a critical role in\nrecovery algorithms.\n", "937": "19.2 Storage 909\n19.2.1 Stable-Storage Implementation\nTo implement stable storage, we need to replicate the needed information in several\nnon-volatile storage media (usually disk) with independent failure modes and to update\nthe information in a controlled manner to ensure that failure during data transfer does\nnot damage the needed information.\nRecall (from Chapter 12) that RAID systems guarantee that the failure of a single\ndisk (even during data transfer) will not result in loss of data. The simplest and fastest\nform of RAID is the mirrored disk, which keeps two copies of each block on separate\ndisks. Other forms of RAID o\ufb00er lower costs, but at the expense of lower performance.\nRAID systems, however, cannot guard against data loss due to disasters such as \ufb01res\nor \ufb02ooding. Many systems store archival backups of tapes o\ufb00-site to guard against such\ndisasters. However, since tapes cannot be carried o\ufb00-site continually, updates since the\nmost recent time that tapes were carried o\ufb00-site could be lost in such a disaster. More\nsecure systems keep a copy of each block of stable storage at a remote site, writing it\nout over a computer network, in addition to storing the block on a local disk system.\nSince the blocks are output to a remote system as and when they are output to local\nstorage, once an output operation is complete, the output is not lost, even in the event\nof a disaster such as a \ufb01re or \ufb02ood. We study such remote backup systems in Section\n19.7.\nIn the remainder of this section, we discuss how storage media can be protected\nfrom failure during data transfer. Block transfer between memory and disk storage can\nresult in:\n\u2022Successful completion . The transferred information arrived safely at its destination.\n\u2022Partial failure . A failure occurred in the midst of transfer, and the destination block\nhas incorrect information.\n\u2022Total failure . The failure occurred su\ufb03ciently early during the transfer that the\ndestination block remains intact.\nWe require that, if a data-transfer failure occurs, the system detects it and invokes a\nrecovery procedure to restore the block to a consistent state. To do so, the system must\nmaintain two physical blocks for each logical database block; in the case of mirrored\ndisks, both blocks are at the same location; in the case of remote backup, one of the\nblocks is local, whereas the other is at a remote site. An output operation is executed\nas follows:\n1.Write the information onto the \ufb01rst physical block.\n2.When the \ufb01rst write completes successfully, write the same information onto the\nsecond physical block.\n3.The output is completed only after the second write completes successfully.\n", "938": "910 Chapter 19 Recovery System\nIf the system fails while blocks are being written, it is possible that the two copies\nof a block could be inconsistent with each other. During recovery, for each block, the\nsystem would need to examine two copies of the blocks. If both are the same and no\ndetectable error exists, then no further actions are necessary. (Recall that errors in a\ndisk block, such as a partial write to the block, are detected by storing a checksum with\neach block.) If the system detects an error in one block, then it replaces its content\nwith the content of the other block. If both blocks contain no detectable error, but\nthey di\ufb00er in content, then the system can either replace the content of the \ufb01rst block\nwith the value of the second, or replace the content of the second block with the value\nof the \ufb01rst. Either way, the recovery procedure ensures that a write to stable storage\neither succeeds completely (i.e., updates all copies) or results in no change.\nThe requirement of comparing every corresponding pair of blocks during recovery\nis expensive to meet. We can reduce the cost greatly by keeping track of block writes that\nare in progress, using a small amount of non-volatile RAM . On recovery, only blocks\nfor which writes were in progress need to be compared.\nThe protocols for writing out a block to a remote site are similar to the protocols\nfor writing blocks to a mirrored disk system, which we examined in Chapter 12, and\nparticularly in Practice Exercise 12.6.\nWe can extend this procedure easily to allow the use of an arbitrarily large number\nof copies of each block of stable storage. Although a large number of copies reduces\nthe probability of a failure to even lower than two copies do, it is usually reasonable to\nsimulate stable storage with only two copies.\n19.2.2 Data Access\nAs we saw in Chapter 12, the database system resides permanently on non-volatile\nstorage (usually disks), and only parts of the database are in memory at any time.\n(In main-memory databases, the entire database resides in memory, but a copy still\nresides on non-volatile storage so data can survive the loss of main-memory contents.)\nThe database is partitioned into \ufb01xed-length storage units called blocks .B l o c k sa r et h e\nunits of data transfer to and from disk and may contain several data items. We shall\nassume that no data item spans two or more blocks. This assumption is realistic for\nmost data-processing applications, such as a bank or a university.\nTransactions input information from the disk into main memory and then output\nthe information back onto the disk. The input and output operations are done in block\nunits. The blocks residing on the disk are referred to as physical blocks ;t h eb l o c k sr e s i d -\ning temporarily in main memory are referred to as bu\ufb00er blocks . The area of memory\nwhere blocks reside temporarily is called the disk bu\ufb00er .\nBlock movements between disk and main memory are initiated through the follow-\ning two operations:\n1.input (B)t r a n s f e r st h ep h y s i c a lb l o c k Bto main memory.\n", "939": "19.2 Storage 911\n2.output (B) transfers the bu\ufb00er block Bto the disk and replaces the appropriate\nphysical block there.\nFigure 19.1 illustrates this scheme.\nConceptually, each transaction Tihas a private work area in which copies of data\nitems accessed and updated by Tiare kept. The system creates this work area when\nthe transaction is initiated; the system removes it when the transaction either commits\nor aborts. Each data item Xkept in the work area of transaction Tiis denoted by xi.\nTransaction Tiinteracts with the database system by transferring data to and from its\nwork area to the system bu\ufb00er. We transfer data by these two operations:\n1.read (X) assigns the value of data item Xto the local variable xi.I te x e c u t e st h i s\noperation as follows:\na. If block BXon which Xresides is not in main memory, it issues input (BX).\nb. It assigns to xithe value of Xfrom the bu\ufb00er block.\n2.write (X) assigns the value of local variable xito data item Xin the bu\ufb00er block.\nIt executes this operation as follows:\na. If block BXon which Xresides is not in main memory, it issues input (BX).\nb. It assigns the value of xitoXin bu\ufb00er BX.\nNote that both operations may require the transfer of a block from disk to main mem-\nory. They do not, however, speci\ufb01cally require the transfer of a block from main mem-\nory to disk.\nA bu\ufb00er block is eventually written out to the disk either because the bu\ufb00er man-\nager needs the memory space for other purposes or because the database system wishes\nA\nBinput( A)\noutput( B)\nB\nmain memorydisk\nFigure 19.1 Block storage operations.\n", "940": "912 Chapter 19 Recovery System\nto re\ufb02ect the change to Bon the disk. We shall say that the database system performs\naforce-output of bu\ufb00er Bif it issues an output (B).\nWhen a transaction needs to access a data item Xfor the \ufb01rst time, it must execute\nread (X). The transaction then performs all updates to Xonxi.A ta n yp o i n td u r i n gi t s\nexecution a transaction may execute write (X) to re\ufb02ect the change to Xin the database\nitself; write (X) must certainly be done after the \ufb01nal write to xi.\nTheoutput (BX)o p e r a t i o nf o rt h eb u \ufb00 e rb l o c k BXon which Xr e s i d e sd o e sn o tn e e d\nto take e\ufb00ect immediately after write (X) is executed, since the block BXmay contain\nother data items that are still being accessed. Thus, the actual output may take place\nlater. Notice that, if the system crashes after the write (X)o p e r a t i o nw a se x e c u t e db u t\nbefore output (BX) was executed, the new value of Xis never written to disk and, thus,\nis lost. As we shall see shortly, the database system executes extra actions to ensure\nthat updates performed by committed transactions are not lost even if there is a system\ncrash.\n19.3 Recovery and Atomicity\nConsider again our simpli\ufb01ed banking system and a transaction Tithat transfers $50\nfrom account Ato account B, with initial values of Aand Bbeing $1000 and $2000,\nrespectively. Suppose that a system crash has occurred during the execution of Ti,a f t e r\noutput (BA) has taken place, but before output (BB) was executed, where BAand BB\ndenote the bu\ufb00er blocks on which Aand Breside. Since the memory contents were\nl o s t ,w ed on o tk n o wt h ef a t eo ft h et r a n s a c t i o n .\nWhen the system restarts, the value of Awould be $950, while that of Bwould be\n$2000, which is clearly inconsistent with the atomicity requirement for transaction Ti.\nUnfortunately, there is no way to \ufb01nd out by examining the database state what blocks\nhad been output and what had not before the crash. It is possible that the transaction\ncompleted, updating the database on stable storage from an initial state with the values\nofAand Bbeing $1000 and $1950; it is also possible that the transaction did not a\ufb00ect\nthe stable storage at all, and the values of Aand Bwere $950 and $2000 initially; or\nthat the updated Bwas output but not the updated A; or that the updated Awas output\nbut the updated Bwas not.\nOur goal is to perform either all or no database modi\ufb01cations made by Ti.H o w -\never, if Tiperformed multiple database modi\ufb01cations, several output operations may\nbe required, and a failure may occur after some of these modi\ufb01cations have been made,\nbut before all of them are made.\nTo achieve our goal of atomicity, we must \ufb01rst output to stable storage informa-\ntion describing the modi\ufb01cations, without m odifying the database itself. As we shall\nsee, this information can help us ensure that all modi\ufb01cations performed by commit-\nted transactions are re\ufb02ected in the database (perhaps during the course of recovery\nactions after a crash). We also need to stor e information about the old value of any\nitem updated by a modi\ufb01cation in case the transaction performing the modi\ufb01cation\n", "941": "19.3 Recovery and Atomicity 913\nfails (aborts). This information can help us undo the modi\ufb01cations made by the failed\ntransaction.\nThe most commonly used technique for recovery is based on log records, and we\nstudy log-based recovery in detail in this chapter. An alternative, called shadow copying,\nis used by text editors but is not used in database systems; this approach is summarized\nin Note 19.1 on page 914.\n19.3.1 Log Records\nThe most widely used structure for recording database modi\ufb01cations is the log.T h el o g\nis a sequence of log records , recording all the update activities in the database.\nThere are several types of log records. An update log record describes a single data-\nbase write. It has these \ufb01elds:\n\u2022Transaction identi\ufb01er , which is the unique identi\ufb01er of the transaction that per-\nformed the write operation.\n\u2022Data-item identi\ufb01er , which is the unique identi\ufb01er of the data item written. Typi-\ncally, it is the location on disk of the data item, consisting of the block identi\ufb01er\nof the block on which the data item resides and an o\ufb00set within the block.\n\u2022Old value , which is the value of the data item prior to the write.\n\u2022New value , which is the value that the data item will have after the write.\nWe represent an update log record as <Ti,Xj,V1,V2>, indicating that transaction Ti\nhas performed a write on data item Xj.Xjhad value V1before the write and has value\nV2after the write. Other special log records exist to record signi\ufb01cant events during\ntransaction processing, such as the start of a transaction and the commit or abort of a\ntransaction. Among the types of log records are:\n\u2022<Tistart>.T r a n s a c t i o n Tihas started.\n\u2022<Ticommit >.T r a n s a c t i o n Tihas committed.\n\u2022<Tiabort>.T r a n s a c t i o n Tihas aborted.\nWe shall introduce several other types of log records later.\nWhenever a transaction performs a write, it is essential that the log record for that\nwrite be created and added to the log, before the database is modi\ufb01ed. Once a log\nrecord exists, we can output the modi\ufb01cation to the database if that is desirable. Also,\nwe have the ability to undo a modi\ufb01cation that has already been output to the database.\nWe undo it by using the old-value \ufb01eld in log records.\nFor log records to be useful for recovery from system and disk failures, the log must\nreside in stable storage. For now, we assume that every log record is written to the end\n", "942": "914 Chapter 19 Recovery System\nNote 19.1 SHADOW COPIES AND SHADOW PAGING\nIn the shadow-copy scheme, a transaction that wants to update the database \ufb01rst\ncreates a complete copy of the database. All updates are done on the new database\ncopy, leaving the original copy, the shadow copy, untouched. If at any point the\ntransaction has to be aborted, the system merely deletes the new copy. The old\ncopy of the database has not been a\ufb00ected. The current copy of the database is\nidenti\ufb01ed by a pointer, called db-pointer, which is stored on disk.\nIf the transaction partially commits (i.e., executes its \ufb01nal statement) it is com-\nmitted as follows: First, the operating system is asked to make sure that all pages\nof the new copy of the database have been written out to disk. (Unix systems use\nthefsync command for this purpose.) After the operating system has written all\nthe pages to disk, the database system updates the pointer db-pointer to point to\nthe new copy of the database; the new copy then becomes the current copy of the\ndatabase. The old copy of the database is then deleted. The transaction is said to\nhave been committed at the point where the updated db-pointer is written to disk.\nThe implementation actually depends on the write to db-pointer being atomic;\nthat is, either all its bytes are written or none of its bytes are written. Disk systems\nprovide atomic updates to entire blocks, or at least to a disk sector. In other words,\nthe disk system guarantees that it will update db-pointer atomically, as long as we\nmake sure that db-pointer lies entirely in a single sector, which we can ensure by\nstoring db-pointer at the beginning of a block.\nShadow-copy schemes are commonly used by text editors (saving the \ufb01le is\nequivalent to transaction commit, while quitting without saving the \ufb01le is equiva-\nlent to transaction abort). Shadow copying can be used for small databases, but\ncopying a large database would be extremely expensive. A variant of shadow copy-\ning, called shadow paging , reduces copying as follows: the scheme uses a page ta-\nble containing pointers to all pages; the page table itself and all updated pages are\ncopied to a new location. Any page which is not updated by a transaction is not\ncopied, but instead the new page table just stores a pointer to the original page.\nWhen a transaction commits, it atomically updates the pointer to the page table,\nw h i c ha c t sa sd b - p o i n t e rt op o i n tt ot h en e wc o p y .\nShadow paging unfortunately does not work well with concurrent transactions\nand is not widely used in databases.\nof the log on stable storage as soon as it is created. In Section 19.5, we shall see when\nit is safe to relax this requirement so as to reduce the overhead imposed by logging.\nObserve that the log contains a complete record of all database activity. As a result,\nthe volume of data stored in the log may become unreasonably large. In Section 19.3.6,\nwe shall show when it is safe to erase log information.\n", "943": "19.3 Recovery and Atomicity 915\n19.3.2 Database Modification\nAs we noted earlier, a transaction creates a log record prior to modifying the database.\nThe log records allow the system to undo changes made by a transaction in the event\nthat the transaction must be aborted; they allow the system also to redo changes made\nby a transaction if the transaction has committed but the system crashed before those\nchanges could be stored in the database on disk. In order for us to understand the role\nof these log records in recovery, we need to consider the steps a transaction takes in\nmodifying a data item:\n1.The transaction performs some computations in its own private part of main\nmemory.\n2.The transaction modi\ufb01es the data block in the disk bu\ufb00er in main memory hold-\ning the data item.\n3.The database system executes the output operation that writes the data block to\ndisk.\nWe say a transaction modi\ufb01es the database if it performs an update on a disk bu\ufb00er,\nor on the disk itself; updates to the private part of main memory do not count as\ndatabase modi\ufb01cations. If a transaction does not modify the database until it has com-\nmitted, it is said to use the deferred-modi\ufb01cation technique. If database modi\ufb01cations\noccur while the transaction is still active, the transaction is said to use the immediate-\nmodi\ufb01cation technique. Deferred modi\ufb01cation has the overhead that transactions need\nto make local copies of all updated data items; further, if a transaction reads a data\nitem that it has updated, it must read the value from its local copy.\nThe recovery algorithms we describe in this chapter support immediate modi\ufb01ca-\ntion. As described, they work correctly even with deferred modi\ufb01cation, but they can be\noptimized to reduce overhead when used with deferred modi\ufb01cation; we leave details\nas an exercise.\nA recovery algorithm must take into account a variety of factors, including:\n\u2022The possibility that a transaction may have committed although some of its\ndatabase modi\ufb01cations exist only in the disk bu\ufb00er in main memory and not in\nthe database on disk.\n\u2022The possibility that a transaction may have modi\ufb01ed the database while in the\nactive state and, as a result of a subsequent failure, may need to abort.\nBecause all database modi\ufb01cations must be preceded by the creation of a log\nrecord, the system has available both the old value prior to the modi\ufb01cation of the\ndata item and the new value that is to be written for the data item. This allows the\nsystem to perform undo and redo operations as appropriate.\n", "944": "916 Chapter 19 Recovery System\n\u2022Theundo o p e r a t i o nu s i n gal o gr e c o r ds e t st h ed a t ai t e ms p e c i \ufb01 e di nt h el o gr e c o r d\nto the old value contained in the log record.\n\u2022Theredo operation using a log record sets the data item speci\ufb01ed in the log record\nto the new value contained in the log record.\n19.3.3 Concurrency Control and Recovery\nIf the concurrency control scheme allows a data item Xthat has been modi\ufb01ed by a\ntransaction T1to be further modi\ufb01ed by another transaction T2before T1commits,\nthen undoing the e\ufb00ects of T1by restoring the old value of X(before T1updated X)\nw o u l da l s ou n d ot h ee \ufb00 e c t so f T2. To avoid such situations, recovery algorithms usually\nrequire that if a data item has been modi\ufb01ed by a transaction, no other transaction can\nmodify the data item until the \ufb01rst transaction commits or aborts.\nThis requirement can be ensured by acquiring an exclusive lock on any updated\ndata item and holding the lock until the transaction commits; in other words, by using\nstrict two-phase locking. Snapshot isolation and validation-based concurrency-control\ntechniques also acquire exclusive locks on data items at the time of validation, before\nmodifying the data items, and hold the locks until the transaction is committed; as a\nresult the above requirement is satis\ufb01ed even by these concurrency control protocols.\nWe discuss in Section 19.8 how the above requirement can be relaxed in certain\ncases.\nWhen either snapshot isolation or validation is used for concurrency control,\ndatabase updates of a transaction are (conceptually) deferred until the transaction is\npartially committed; the deferred-modi\ufb01cation technique is a natural \ufb01t with these con-\ncurrency control schemes. However, it is worth noting that some implementations of\nsnapshot isolation use immediate modi\ufb01cation but provide a logical snapshot on de-\nmand: when a transaction needs to read an item that a concurrent transaction has up-\ndated, a copy of the (already updated) item is made, and updates made by concurrent\ntransactions are rolled back on the copy of the item. Similarly, immediate modi\ufb01cation\nof the database is a natural \ufb01t with two-phase locking, but deferred modi\ufb01cation can\nalso be used with two-phase locking.\n<T0  start>\n<T0 ,  A,  1000,  950>\n<T0 ,  B,  2000,  2050>\n<T0  commit>\n<T1  start>\n<T1 ,  C,  700,  600>\n<T1  commit>\nFigure 19.2 Portion of the system log corresponding to T0andT1.\n", "945": "19.3 Recovery and Atomicity 917\n19.3.4 Transaction Commit\nWe say that a transaction has committed when its commit log record, which is the\nlast log record of the transaction, has been output to stable storage; at that point all\nearlier log records have already been output to stable storage. Thus, there is enough\ninformation in the log to ensure that even if there is a system crash, the updates of the\ntransaction can be redone. If a system crash occurs before a log record <Ticommit >\nis output to stable storage, transaction Tiwill be rolled back. Thus, the output of the\nblock containing the commit log record is the single atomic action that results in a\ntransaction getting committed.1\nWith most log-based recovery techniques, including the ones we describe in this\nchapter, blocks containing the data items modi\ufb01ed by a transaction do not have to be\noutput to stable storage when the transaction commits but can be output some time\nlater. We discuss this issue further in Section 19.5.2.\n19.3.5 Using the Log to Redo and Undo Transactions\nWe now provide an overview of how the log can be used to recover from a system crash\nand to roll back transactions during normal operation. However, we postpone details\nof the procedures for failure recovery and rollback to Section 19.4.\nConsider our simpli\ufb01ed banking system. Let T0be a transaction that transfers $50\nfrom account Ato account B:\nT0:read (A);\nA:=A\u221250;\nwrite (A);\nread (B);\nB:=B+ 50;\nwrite (B).\nLet T1be a transaction that withdraws $100 from account C:\nT1:read (C);\nC:=C\u2212100;\nwrite (C).\nThe portion of the log containing the relevant information concerning these two trans-\nactions appears in Figure 19.2.\nFigure 19.3 shows one possible order in which the actual outputs took place in\nboth the database system and the log as a result of the execution of T0and T1.2\n1The output of a block can be made atomic by techniques for dea ling with data-transfer failure, as described in Section\n19.2.1.\n2Notice that this order could not be obtained using the deferred-modi\ufb01cation technique, because the database is mod-\ni\ufb01ed by T0before it commits, and likewise for T1.\n", "946": "918 Chapter 19 Recovery System\nLog Database\nA = 950\nB = 2050\nC = 600<T0  start>\n<T0 ,  A,  1000,  950 >\n<T0 ,  B,  2000,  2050 >\n<T0  commit>\n<T1  start>\n<T1 ,  C,  700,  600 >\n<T1  commit>\nFigure 19.3 State of system log and database corresponding to T0andT1.\nUsing the log, the system can handle any failure that does not result in the loss of\ninformation in non-volatile storage. The recovery scheme uses two recovery procedures.\nBoth these procedures make use of the log to \ufb01nd the set of data items updated by each\ntransaction Tiand their respective old and new values.\n\u2022redo(Ti). The procedure sets the value of all data items updated by transaction Ti\nto the new values. The order in which updates are carried out by redo is impor-\ntant; when recovering from a system crash, if updates to a particular data item are\napplied in an order di\ufb00erent from the order in which they were applied originally,\nthe \ufb01nal state of that data item will have a wrong value. Most recovery algorithms,\nincluding the one we describe in Section 19.4, do not perform redo of each trans-\naction separately; instead they perform a single scan of the log, during which redo\nactions are performed for each log recor d as it is encountered. This approach en-\nsures the order of updates is preserved, and it is more e\ufb03cient since the log needs\nto be read only once overall, instead of once per transaction.\n\u2022undo (Ti). The procedure restores the value of all data items updated by transaction\nTito the old values. In the recovery scheme that we describe in Section 19.4:\n\u00b0The undo operation not only restores the data items to their old value, but\nalso writes log records to record the updates performed as part of the undo\nprocess. These log records are special redo-only log records, since they do not\nneed to contain the old value of the updated data item; note that when such\nlog records are used during undo, the \u201cold value\u201d is actually the value written\nby the transaction that is being rolled back, and the \u201cnew value\u201d is the original\nvalue that is being restored by the undo operation.\nAs with the redo procedure, the order in which undo operations are per-\nformed is important; again we postpone details to Section 19.4.\n", "947": "19.3 Recovery and Atomicity 919\n\u00b0When the undo operation for transaction Ticompletes, it writes a <Tiabort>\nlog record, indicating that the undo has completed.\nAs we shall see in Section 19.4, the undo (Ti) procedure is executed only\nonce for a transaction, if the transaction is rolled back during normal process-\ning or if on recovering from a system crash, neither a commit nor an abort\nrecord is found for transaction Ti. As a result, every transaction will eventually\nhave either a commit or an abort record in the log.\nAfter a system crash has occurred, the system consults the log to determine which\ntransactions need to be redone and which need to be undone so as to ensure atomicity.\n\u2022Transaction Tineeds to be undone if the log contains the record <Tistart>but\ndoes not contain either the record <Ticommit >or the record <Tiabort>.\n\u2022Transaction Tineeds to be redone if the log contains the record <Tistart>and\neither the record <Ticommit >or the record <Tiabort>. It may seem strange to\nredo Tiif the record <Tiabort>is in the log. To see why this works, note that if <Ti\nabort>is in the log, so are the redo-only records written by the undo operation.\nThus, the end result will be to undo Ti\u2019s modi\ufb01cations in this case. This slight\nredundancy simpli\ufb01es the recovery algorithm and enables faster overall recovery\ntime.\nAs an illustration, return to our banking example, with transaction T0and T1ex-\necuted one after the other in the order T0followed by T1. Suppose that the system\ncrashes before the completion of the transactions. We shall consider three cases. The\nstate of the logs for each of these cases appears in Figure 19.4.\nFirst, let us assume that the crash occurs just after the log record for the step:\nwrite (B)\nof transaction T0has been written to stable storage (Figure 19.4a). When the system\ncomes back up, it \ufb01nds the record <T0start>in the log, but no corresponding <T0\ncommit >or<T0abort>record. Thus, transaction T0must be undone, so an undo (T0)\nis performed. As a result, the values in accounts Aand B(on the disk) are restored to\n$1000 and $2000, respectively.\nNext, let us assume that the crash comes just after the log record for the step:\nwrite (C)\nof transaction T1has been written to stable storage (Figure 19.4b). When the system\ncomes back up, two recovery actions need to be taken. The operation undo (T1)m u s t\nbe performed, since the record <T1start>appears in the log, but there is no record\n<T1commit >or<T1abort>.T h eo p e r a t i o n redo(T0) must be performed, since the\nlog contains both the record <T0start>and the record <T0commit >.A tt h ee n do f\n", "948": "920 Chapter 19 Recovery System\n<T0  start>\n<T0 , A, 1000, 950 > \n<T0 , B, 2000, 2050 > <T0  start>\n<T0 ,  A, 1000, 950 > \n<T0 ,  B, 2000, 2050 >  \n<T0  commit>\n<T1  start>\n<T1 , C, 700, 600 >  <T0  start>\n<T0 , A, 1000, 950 >\n<T0 ,  B,  2000, 2050 >\n<T0  commit>\n<T1  start>\n<T1 ,  C, 700, 600 >\n<T1  commit>\n(a) (b) (c)\nFigure 19.4 The same log, shown at three different times.\nthe entire recovery procedure, the values of accounts A,B,a n d Care $950, $2050, and\n$700, respectively.\nFinally, let us assume that the crash occurs just after the log record:\n<T1commit >\nhas been written to stable storage (Figure 19.4c). When the system comes back up,\nboth T0and T1need to be redone, since the records <T0start>and<T0commit >\nappear in the log, as do the records <T1start>and<T1commit >.A f t e rt h es y s t e m\nperforms the recovery procedures redo(T0)a n d redo(T1), the values in accounts A,B,\nand Care $950, $2050, and $600, respectively.\n19.3.6 Checkpoints\nWhen a system crash occurs, we must consult the log to determine those transactions\nthat need to be redone and those that need to be undone. In principle, we need to\nsearch the entire log to determine this information. There are two major di\ufb03culties\nwith this approach:\n1.The search process is time-consuming.\n2.Most of the transactions that, according to our algorithm, need to be redone have\nalready written their updates into the database. Although redoing them will cause\nno harm, it will nevertheless cause recovery to take longer.\nTo reduce these types of overhead, we introduce checkpoints .\nWe describe below a simple checkpoint scheme that (a) does not permit any up-\ndates to be performed while the checkpoint operation is in progress, and (b) outputs\nall modi\ufb01ed bu\ufb00er blocks to disk when the checkpoint is performed. We discuss later\nhow to modify the checkpointing and recovery procedures to provide more \ufb02exibility\nby relaxing both these requirements.\nA checkpoint is performed as follows:\n", "949": "19.3 Recovery and Atomicity 921\n1.Output onto stable storage all log records currently residing in main memory.\n2.Output to the disk all modi\ufb01ed bu\ufb00er blocks.\n3.Output onto stable storage a log record of the form <checkpoint L>,w h e r e Lis\na list of transactions active at the time of the checkpoint.\nTransactions are not allowed to perform any update actions, such as writing to a\nbu\ufb00er block or writing a log record, while a checkpoint is in progress. We discuss how\nthis requirement can be enforced in Section 19.5.2.\nT h ep r e s e n c eo fa <checkpoint L>record in the log allows the system to stream-\nline its recovery procedure. Consider a transaction Tithat completed prior to the check-\npoint. For such a transaction, the <Ticommit >record (or <Tiabort>record) ap-\npears in the log before the <checkpoint >record. Any database modi\ufb01cations made\nbyTimust have been written to the database either prior to the checkpoint or as part\nof the checkpoint itself. Thus, at recovery time, there is no need to perform a redo\noperation on Ti.\nAfter a system crash has occurred, the system examines the log to \ufb01nd the last\n<checkpoint L>record (this can be done by searching the log backward, from the\nend of the log, until the \ufb01rst <checkpoint L>record is found).\nThe redo orundo operations need to be applied only to transactions in L,a n dt o\nall transactions that started execution after the <checkpoint L>record was written to\nthe log. Let us denote this set of transactions as T.\n\u2022For all transactions TkinTthat have no <Tkcommit >record or <Tkabort>\nrecord in the log, execute undo (Tk).\n\u2022For all transactions TkinTsuch that either the record <Tkcommit >or the record\n<Tkabort>appears in the log, execute redo(Tk).\nNote that we need only examine the part of the log starting with the last checkpoint\nlog record to \ufb01nd the set of transactions Tand to \ufb01nd out whether a commit orabort\nrecord occurs in the log for each transaction in T.\nAs an illustration, consider the set of transactions {T0,T1,\u2026,T100}. Suppose that\nthe most recent checkpoint took place during the execution of transaction T67and\nT69, while T68and all transactions with subscripts lower than 67 completed before the\ncheckpoint. Thus, only transactions T67,T69,\u2026,T100need to be considered during\nthe recovery scheme. Each of them needs to be redone if it has completed (i.e., either\ncommitted or aborted); otherwise, it was incomplete and needs to be undone.\nConsider the set of transactions Lin a checkpoint log record. For each transaction\nTiinL, log records of the transaction that occur prior to the checkpoint log record\nmay be needed to undo the transaction, in case it does not commit. However, all log\nrecords prior to the earliest of the <Tistart>log records, among transactions TiinL,\n", "950": "922 Chapter 19 Recovery System\nare not needed once the checkpoint has completed. These log records can be erased\nwhenever the database system needs to reclaim the space occupied by these records.\nThe requirement that transactions must not perform any updates to bu\ufb00er blocks\nor to the log during checkpointing can be bothersome, since transaction processing\nhas to halt while a checkpoint is in progress. A fuzzy checkpoint is a checkpoint where\ntransactions are allowed to perform updates even while bu\ufb00er blocks are being writ-\nten out. Section 19.5.4 describes fuzzy-checkpointing schemes. Later in Section 19.9\nwe describe a checkpoint scheme that is not only fuzzy, but does not even require all\nmodi\ufb01ed bu\ufb00er blocks to be output to disk at the time of the checkpoint.\n19.4 Recovery Algorithm\nUntil now, in discussing recovery, we have identi\ufb01ed transactions that need to be re-\ndone and those that need to be undone, but we have not given a precise algorithm for\nperforming these actions. We are now ready to present the full recovery algorithm using\nlog records for recovery from transaction failure and a combination of the most recent\ncheckpoint and log records to recover from a system crash.\nThe recovery algorithm described in this section requires that a data item that\nhas been updated by an uncommitted transaction cannot be modi\ufb01ed by any other\ntransaction, until the \ufb01rst transaction has either committed or aborted. Recall that this\nrestriction was discussed in Section 19.3.3.\n19.4.1 Transaction Rollback\nFirst consider transaction rollback during normal operation (i.e., not during recovery\nfrom a system crash). Rollback of a transaction Tiis performed as follows:\n1.The log is scanned backward, and for each log record of Tiof the form\n<Ti,Xj,V1,V2>that is found:\na. The value V1is written to data item Xj,a n d\nb. A special redo-only log record <Ti,Xj,V1>is written to the log, where\nV1is the value being restored to data item Xjduring the rollback. These\nlog records are sometimes called compensation log records .S u c hr e c o r d s\ndo not need undo information, since we never need to undo such an undo\noperation. We shall explain later how they are used.\n2.Once the log record <Tistart>is found, the backward scan is stopped, and a log\nrecord <Tiabort>is written to the log.\nObserve that every update action performed by the transaction or on behalf of the\ntransaction, including actions taken to restore data items to their old value, have now\nbeen recorded in the log. In Section 19.4.2 we shall see why this is a good idea.\n", "951": "19.4 Recovery Algorithm 923\n19.4.2 Recovery After a System Crash\nRecovery actions, when the database system is restarted after a crash, take place in two\nphases:\n1.In the redo phase , the system replays updates of alltransactions by scanning the\nlog forward from the last checkpoint. The log records that are replayed include\nlog records for transactions that were rolled back before system crash, and those\nthat had not committed when the system crash occurred.\nThis phase also determines all transactions that were incomplete at the time of\nthe crash, and must therefore be rolled back. Such incomplete transactions would\neither have been active at the time of the checkpoint, and thus would appear in the\ntransaction list in the checkpoint record, or would have started later; further, such\nincomplete transactions would have neither a <Tiabort>nor a<Ticommit >\nrecord in the log.\nThe speci\ufb01c steps taken while scanning the log are as follows:\na. The list of transactions to be rolled back, undo-list, is initially set to the list\nLin the<checkpoint L>log record.\nb. Whenever a normal log record of the form <Ti,Xj,V1,V2>,o rar e d o -\nonly log record of the form <Ti,Xj,V2>is encountered, the operation is\nredone; that is, the value V2is written to data item Xj.\nc. Whenever a log record of the form <Tistart>is found, Tiis added to\nundo-list.\nd. Whenever a log record of the form <Tiabort>or<Ticommit >is found,\nTiis removed from undo-list.\nAt the end of the redo phase, undo-list contains the list of all transactions that\nare incomplete, that is, they neither committed nor completed rollback before\nthe crash.\n2.In the undo phase , the system rolls back all transactions in the undo-list. It per-\nforms rollback by scanning the log backward from the end.\na. Whenever it \ufb01nds a log record belonging to a transaction in the undo-list, it\nperforms undo actions just as if the log record had been found during the\nrollback of a failed transaction.\nb. When the system \ufb01nds a <Tistart>log record for a transaction Tiin undo-\nlist, it writes a <Tiabort>log record to the log and removes Tifrom undo-\nlist.\nc. The undo phase terminates once undo-list becomes empty, that is, the sys-\ntem has found <Tistart>log records for all transactions that were initially\nin undo-list.\n", "952": "924 Chapter 19 Recovery System\nAfter the undo phase of recovery terminates, normal transaction processing can\nresume.\nObserve that the redo phase replays every log record since the most recent check-\npoint record. In other words, this phase of restart recovery repeats all the update actions\nthat were executed after the checkpoint, and whose log records reached the stable log.\nThe actions include actions of incomplete transactions and the actions carried out to\nroll back failed transactions. The actions are repeated in the same order in which they\nwere originally carried out; hence, this process is called repeating history . Although it\nmay appear wasteful, repeating history even for failed transactions simpli\ufb01es recovery\nschemes.\nFigure 19.5 shows an example of actions logged during normal operation and ac-\ntions performed during failure recovery. In the log shown in the \ufb01gure, transaction T1\nhad committed, and transaction T0had been completely rolled back, before the system\nc r a s h e d .O b s e r v eh o wt h ev a l u eo fd a t ai t e m Bis restored during the rollback of T0.\nObserve also the checkpoint record, with the list of active transactions containing T0\nand T1.\nWhen recovering from a crash, in the redo phase, the system performs a redo of\nall operations after the last checkpoint record. In this phase, the list undo-list initially\ncontains T0and T1;T1is removed \ufb01rst when its commit log record is found, while T2\nis added when its start log record is found. Transaction T0is removed from undo-list\nwhen its abort log record is found, leaving only T2in undo-list. The undo phase scans\nthe log backwards from the end, and when it \ufb01nds a log record of T2updating A,t h e\nold value of Ais restored, and a redo-only log record is written to the log. When the\nolder\nLog records\nadded during\nrecovery\nnewer<T0 start>\nT0 rollback\n(during normal\noperation)\nbeginsStart log records\nfound for all\ntransactions in\nundo list\nT2 is incomplete\nat crash\nT2 rolled back\nin undo passUndo list: T2 T0 rollback\ncomplete<T1 start>\n<T2 start>\n<T0 abort>\n<T2 abort><T1 commit><checkpoint { T0, T1}><T0, B, 2000, 2050>\n<T1, C, 700, 600>\n<T2, A, 500, 400>\n<T2, A, 500><T0, B, 2000>Beginning of log\nRedo Pass\nUndo PassEnd of log\nat crash!\nFigure 19.5 Example of logged actions and actions during recovery.\n", "953": "19.4 Recovery Algorithm 925\nstart record for T2is found, an abort record is added for T2. Since undo-list contains\nno more transactions, the undo phase terminates, completing recovery.\n19.4.3 Optimizing Commit Processing\nCommitting a transaction requires that its log records have been forced to disk. If a sep-\narate log \ufb02ush is done for each transaction, each commit incurs a signi\ufb01cant log write\noverhead. The rate of transaction commit can be increased using the group-commit\ntechnique. With this technique, instead of attempting to force the log as soon as a\ntransaction completes, the system waits until several transactions have completed, or\na certain period of time has passed since a transaction completed execution. It then\ncommits the group of transactions that are waiting, together. Blocks written to the log\non stable storage would contain records of several transactions. By careful choice of\ngroup size and maximum waiting time, the system can ensure that blocks are full when\nthey are written to stable storage without making transactions wait excessively. This\ntechnique results, on average, in fewer output operations per committed transaction.\nIf logging is done to hard disk, writing a block of data can take about 5 to 10\nmilliseconds. As a result, without group commit, at most 100 to 200 transactions can\nbe committed per second. If records of 10 transactions \ufb01t in a disk block, group commit\nwill allow 1000 to 2000 transactions to be committed per second.\nIf logging is done to \ufb02ash, writing a block can take about 100 microseconds, allow-\ning 10,000 transactions to be committed per second without group commit. If records\nof 10 transactions \ufb01t in a disk block, group commit will allow 100,000 transactions to\nbe committed per second on \ufb02ash. A further bene\ufb01t of group commit with \ufb02ash is that\nit minimizes the number of times the same page is written, which in turn minimizes the\nnumber of erase operations, which can be expensive. (Recall that \ufb02ash storage systems\nremap logical pages to a pre-erased physical page, avoiding delay at the time a page\nis written, but the erase operation must be performed eventually as part of garbage\ncollection of old versions of pages.)\nAlthough group commit reduces the overhead imposed by logging, it results in a\nslight delay in commit of transactions that perform updates. When the rate of commits\nis low, the delay may not be worth the bene\ufb01t, but with high rates of transaction commit,\nthe overall delay in commit is actually reduced by using group commit.\nIn addition to optimizations done at the database, programmers can also take some\nsteps to improve transaction commit performance. For example, consider an applica-\ntion that loads data into a database. If the application performs each insert as a separate\ntransaction, the number of inserts that can be performed per second is limited by the\nnumber of blocks writes that can be performed per second. If the application waits for\none insert to \ufb01nish before starting the next one, group commit does not o\ufb00er any bene-\n\ufb01ts and in fact may slow the system down. However, in such a case, performance can be\nsigni\ufb01cantly improved by performing a batch of inserts as a single transaction. The log\nrecords corresponding to multiple inserts are then written together in one page. The\nnumber of inserts that can be performed per second then increases correspondingly.\n", "954": "926 Chapter 19 Recovery System\n19.5 Buffer Management\nIn this section, we consider several subtle details that are essential to the implementa-\ntion of a crash-recovery scheme that ensures data consistency and imposes a minimal\namount of overhead on interactions with the database.\n19.5.1 Log-Record Buffering\nSo far, we have assumed that every log record is output to stable storage at the time it\nis created. This assumption imposes a high overhead on system execution for several\nreasons: Typically, output to stable storage is in units of blocks. In most cases, a log\nrecord is much smaller than a block. Thus, the output of each log record translates to a\nmuch larger output at the physical level. Furthermore, as we saw in Section 19.2.1, the\noutput of a block to stable storage may involve several output operations at the physical\nlevel.\nThe cost of outputting a block to stable storage is su\ufb03ciently high that it is desirable\nto output multiple log records at once. To do so, we write log records to a log bu\ufb00er\nin main memory, where they stay temporarily until they are output to stable storage.\nMultiple log records can be gathered in the log bu\ufb00er and output to stable storage in a\nsingle output operation. The order of log records in the stable storage must be exactly\nthe same as the order in which they were written to the log bu\ufb00er.\nAs a result of log bu\ufb00ering, a log record may reside in only main memory (volatile\nstorage) for a considerable time before it is output to stable storage. Since such log\nrecords are lost if the system crashes, we must impose additional requirements on the\nrecovery techniques to ensure transaction atomicity:\n\u2022Transaction Tienters the commit state after the <Ticommit >log record has been\noutput to stable storage.\n\u2022Before the <Ticommit >log record can be output to stable storage, all log records\npertaining to transaction Timust have been output to stable storage.\n\u2022Before a block of data in main memory can be output to the database (in non-\nvolatile storage), all log records pertaining to data in that block must have been\noutput to stable storage.\nThis rule is called the write-ahead logging (WAL) r u l e .( S t r i c t l ys p e a k i n g ,t h e\nWAL rule requires only that the undo information in the log has been output to sta-\nble storage, and it permits the redo information to be written later. The di\ufb00erence\nis relevant in systems where undo information and redo information are stored in\nseparate log records.)\nThe three rules state situations in which certain log records must have been output\nto stable storage. There is no problem resulting from the output of log records earlier\nthan necessary. Thus, when the system \ufb01nds it necessary to output a log record to\n", "955": "19.5 Bu\ufb00er Management 927\nstable storage, it outputs an entire block of log records, if there are enough log records\nin main memory to \ufb01ll a block. If there are insu\ufb03cient log records to \ufb01ll the block, all\nlog records in main memory are combined into a partially full block and are output to\nstable storage.\nWriting the bu\ufb00ered log to disk is sometimes referred to as a log force .\n19.5.2 Database Buffering\nIn Section 19.2.2, we described the use of a two-level storage hierarchy. The system\nstores the database in non-volatile storage (disk), and brings blocks of data into main\nmemory as needed. Since main memory is typically much smaller than the entire\ndatabase, it may be necessary to overwrite a block B1in main memory when another\nblock B2needs to be brought into memory. If B1has been modi\ufb01ed, B1must be output\nprior to the input of B2. As discussed in Section 13.5.1 this storage hierarchy is similar\nto the standard operating-system concept of virtual memory .\nOne might expect that transactions would force-output all modi\ufb01ed blocks to disk\nwhen they commit. Such a policy is called the force policy. The alternative, the no-force\npolicy, allows a transaction to commit even if it has modi\ufb01ed some blocks that have\nnot yet been written back to disk. All the recovery algorithms described in this chapter\nwork correctly even with the no-force policy. The no-force policy allows faster commit\nof transactions; moreover it allows multiple updates to accumulate on a block before it\nis output to stable storage, which can reduce the number of output operations greatly\nfor frequently updated blocks. As a result, the standard approach taken by most systems\nis the no-force policy.\nSimilarly, one might expect that blocks modi\ufb01ed by a transaction that is still active\nshould not be written to disk. This policy is called the no-steal policy. The alternative,\nthesteal policy, allows the system to write modi\ufb01ed blocks to disk even if the transac-\ntions that made those modi\ufb01cations have not all committed. As long as the write-ahead\nlogging rule is followed, all the recovery algorithms we study in the chapter work cor-\nrectly even with the steal policy. Further, the no-steal policy does not work with trans-\nactions that perform a large number of updates, since the bu\ufb00er may get \ufb01lled with\nupdated pages that cannot be evicted to disk, and the transaction cannot then proceed.\nAs a result, the standard approach taken by most systems is the steal policy.\nTo illustrate the need for the write-ahead logging requirement, consider our banking\nexample with transactions T0and T1. Suppose that the state of the log is:\n<T0start>\n<T0,A, 1000, 950 >\nand that transaction T0issues a read (B). Assume that the block on which Bresides is\nnot in main memory and that main memory is full. Suppose that the block on which A\nr e s i d e si sc h o s e nt ob eo u t p u tt od i s k .I ft h es y s t e mo u t p u t st h i sb l o c kt od i s ka n dt h e na\ncrash occurs, the values in the database for accounts A,B,a n d Care $950, $2000, and\n", "956": "928 Chapter 19 Recovery System\n$700, respectively. This database state is inconsistent. However, because of the WAL\nrequirements, the log record:\n<T0,A, 1000, 950 >\nmust be output to stable storage prior to output of the block on which Aresides. The\nsystem can use the log record during recovery to bring the database back to a consistent\nstate.\nWhen a block B1is to be output to disk, all log records pertaining to data in B1\nmust be output to stable storage before B1is output. It is important that no writes to\nthe block B1be in progress while the block is being output, since such a write could\nviolate the write-ahead logging rule. We can ensure that there are no writes in progress\nby using a special means of locking:\n\u2022Before a transaction performs a write on a data item, it acquires an exclusive lock\non the block in which the data item resides. The lock is released immediately after\nthe update has been performed.\n\u2022The following sequence of actions is taken when a block is to be output:\n\u00b0Obtain an exclusive lock on the block, to ensure that no transaction is perform-\ning a write on the block.\n\u00b0Output log records to stable storage until all log records pertaining to block B1\nhave been output.\n\u00b0Output block B1to disk.\n\u00b0Release the lock once the block output has completed.\nLocks on bu\ufb00er blocks are unrelated to locks used for concurrency control of trans-\nactions, and releasing them in a non-two-phase manner does not have any implications\non transaction serializability. These locks, and other similar locks that are held for a\nshort duration, are often referred to as latches .\nLocks on bu\ufb00er blocks can also be used to ensure that bu\ufb00er blocks are not up-\ndated, and log records are not generated, while a checkpoint is in progress. This re-\nstriction may be enforced by acquiring exclusive locks on all bu\ufb00er blocks, as well as\nan exclusive lock on the log, before the checkpoint operation is performed. These locks\ncan be released as soon as the checkpoint operation has completed.\nDatabase systems usually have a process that continually cycles through the bu\ufb00er\nblocks, outputting modi\ufb01ed bu\ufb00er blocks ba ck to disk. The above locking protocol must\nof course be followed when the blocks are output. As a result of continuous output of\nmodi\ufb01ed blocks, the number of dirty blocks in the bu\ufb00er, that is, blocks that have been\nmodi\ufb01ed in the bu\ufb00er but have not been subsequently output, is minimized. Thus, the\nnumber of blocks that have to be output during a checkpoint is minimized; further,\n", "957": "19.5 Bu\ufb00er Management 929\nwhen a block needs to be evicted from the bu\ufb00er, it is likely that there will be a non-\ndirty block available for eviction, allowing the input to proceed immediately instead of\nwaiting for an output to complete.\n19.5.3 Operating System Role in Buffer Management\nWe can manage the database bu\ufb00er by using one of two approaches:\n1.The database system reserves part of main memory to serve as a bu\ufb00er that it,\nrather than the operating system, manages. The database system manages data-\nblock transfer in accordance with the requirements in Section 19.5.2.\nThis approach has the drawback of limiting \ufb02exibility in the use of main mem-\nory. The bu\ufb00er must be kept small enough that other applications have su\ufb03cient\nmain memory available for their needs. However, even when the other applica-\ntions are not running, the database will not be able to make use of all the available\nmemory. Likewise, non-database applications may not use that part of main mem-\nory reserved for the database bu\ufb00er, even if some of the pages in the database\nbu\ufb00er are not being used.\n2.The database system implements its bu\ufb00er within the virtual memory provided\nby the operating system. Since the operating system knows about the memory\nrequirements of all processes in the system, ideally it should be in charge of\ndeciding what bu\ufb00er blocks must be force-output to disk, and when. But, to en-\nsure the write-ahead logging requirements in Section 19.5.1, the operating system\nshould not write out the database bu\ufb00er pages itself, but instead should request\nthe database system to force-output the bu\ufb00er blocks. The database system in\nturn would force-output the bu\ufb00er blocks to the database, after writing relevant\nlog records to stable storage.\nUnfortunately, almost all current-generation operating systems retain com-\nplete control of virtual memory. The operating system reserves space on disk for\nstoring virtual-memory pages that are not currently in main memory; this space\nis called swap space . If the operating system decides to output a block Bx,t h a t\nblock is output to the swap space on disk, and there is no way for the database\nsystem to get control of the output of bu\ufb00er blocks.\nTherefore, if the database bu\ufb00er is in virtual memory, transfers between\ndatabase \ufb01les and the bu\ufb00er in virtual memory must be managed by the database\nsystem, which enforces the write-ahead logging requirements that we discussed.\nThis approach may result in extra output of data to disk. If a block Bxis output\nby the operating system, that block is not output to the database. Instead, it is\noutput to the swap space for the operating system\u2019s virtual memory. When the\ndatabase system needs to output Bx, the operating system may need \ufb01rst to input\nBxfrom its swap space. Thus, instead of a single output of Bx,t h e r em a yb et w o\noutputs of Bx(one by the operating system, and one by the database system) and\none extra input of Bx.\n", "958": "930 Chapter 19 Recovery System\nAlthough both approaches su\ufb00er from some drawbacks, one or the other must be\nchosen unless the operating system is designed to support the requirements of database\nlogging.\n19.5.4 Fuzzy Checkpointing\nThe checkpointing technique described in Section 19.3.6 requires that all updates to the\ndatabase be temporarily suspended while the checkpoint is in progress. If the number\nof pages in the bu\ufb00er is large, a checkpoint may take a long time to \ufb01nish, which can\nresult in an unacceptable interruption in processing of transactions.\nTo avoid such interruptions, the checkpointing technique can be modi\ufb01ed to permit\nupdates to start once the checkpoint record has been written, but before the modi\ufb01ed\nbu\ufb00er blocks are written to disk. The checkpoint thus generated is a fuzzy checkpoint .\nSince pages are output to disk only after the checkpoint record has been written, it\nis possible that the system could crash before all pages are written. Thus, a checkpoint\non disk may be incomplete. One way to deal with incomplete checkpoints is this: The\nlocation in the log of the checkpoint record of the last completed checkpoint is stored in\na \ufb01xed position, last-checkpoint, on disk. The system does not update this information\nwhen it writes the checkpoint r e c o r d .I n s t e a d ,b e f o r ei tw r i t e st h e checkpoint record,\nit creates a list of all modi\ufb01ed bu\ufb00er blocks. The last-checkpoint information is updated\nonly after all bu\ufb00er blocks in the list of modi\ufb01ed bu\ufb00er blocks have been output to disk.\nEven with fuzzy checkpointing, a bu\ufb00er block must not be updated while it is being\noutput to disk, although other bu\ufb00er blocks may be updated concurrently. The write-\nahead log protocol must be followed so that (undo) log records pertaining to a block\nare on stable storage before the block is output.\n19.6 Failure with Loss of Non-Volatile Storage\nUntil now, we have considered only the case where a failure results in the loss of infor-\nmation residing in volatile storage while the content of the non-volatile storage remains\nintact. Although failures in which the content of non-volatile storage is lost are rare,\nwe nevertheless need to be prepared to deal with this type of failure. In this section, we\ndiscuss only disk storage. Our discussions apply as well to other non-volatile storage\ntypes.\nThe basic scheme is to dump the entire contents of the database to stable storage\nperiodically\u2014say, once per day. For example, we may dump the database to one or\nmore magnetic tapes. If a failure occurs that results in the loss of physical database\nblocks, the system uses the most recent dump in restoring the database to a previous\nconsistent state. Once this restoration has been accomplished, the system uses the log\nto bring the database system to the most recent consistent state.\nOne approach to database dumping requires that no transaction may be active\nduring the dump procedure, and it uses a procedure similar to checkpointing:\n", "959": "19.7 High Availability Using Remote Backup Systems 931\n1.Output all log records currently residing in main memory onto stable storage.\n2.Output all bu\ufb00er blocks onto the disk.\n3.Copy the contents of the database to stable storage.\n4.Output a log record <dump>o n t ot h es t a b l es t o r a g e .\nSteps 1, 2, and 4 correspond to the three steps used for checkpoints in Section 19.3.6.\nTo recover from the loss of non-volatile storage, the system restores the database\nto disk by using the most recent dump. Then, it consults the log and redoes all the\nactions since the most recent dump occurred. Notice that no undo operations need to\nbe executed.\nIn case of a partial failure of non-volatile storage, such as the failure of a single block\nor a few blocks, only those blocks need to be restored, and redo actions performed only\nfor those blocks.\nA dump of the database contents is also referred to as an archival dump ,s i n c ew e\ncan archive the dumps and use them later to examine old states of the database. Dumps\nof a database and checkpointing of bu\ufb00ers are similar.\nMost database systems also support an SQL dump ,w h i c hw r i t e so u t SQL DDL\nstatements and SQLinsert statements to a \ufb01le, which can then be reexecuted to re-create\nthe database. Such dumps are useful when migrating data to a di\ufb00erent instance of the\ndatabase, or to a di\ufb00erent version of the database software, since the physical locations\nand layout may be di\ufb00erent in the other database instance or database software version.\nT h es i m p l ed u m pp r o c e d u r ed e s c r i b e dh e r ei sc o s t l yf o rt h ef o l l o w i n gt w or e a s o n s .\nFirst, the entire database must be copied to stable storage, resulting in considerable data\ntransfer. Second, since transaction processing is halted during the dump procedure,\nCPU cycles are wasted. Fuzzy dump schemes have been developed that allow transac-\ntions to be active while the dump is in progress. They are similar to fuzzy-checkpointing\nschemes; see the bibliographical notes for more details.\n19.7 High Availability Using Remote Backup Systems\nTraditional transaction-processing systems are centralized or client\u2013server systems.\nSuch systems are vulnerable to environmental disasters such as \ufb01re, \ufb02ooding, or earth-\nquakes. Today\u2019s applications need transaction-processing systems that can function in\nspite of system failures or environmental disasters. Such systems must provide high\navailability ; that is, the time for which the system is unusable must be extremely short.\nWe can achieve high availability by performing transaction processing at one site,\ncalled the primary site , and having a remote backup site where all the data from the pri-\nmary site are replicated. The remote backup site is sometimes also called the secondary\nsite. The remote site must be kept synchronized with the primary site as updates are\nperformed at the primary. We achieve synchronization by sending all log records from\nt h ep r i m a r ys i t et ot h er e m o t eb a c k u ps i t e .T h er e m o t eb a c k u ps i t em u s tb ep h y s i c a l l y\n", "960": "932 Chapter 19 Recovery System\nlog\nrecordsbackup network primary\nFigure 19.6 Architecture of remote backup system.\nseparated from the primary\u2014for example, we can locate it in a di\ufb00erent state\u2014so that a\ndisaster such as a \ufb01re, \ufb02ood or an earthquake at the primary does not also damage the\nremote backup site.3Figure 19.6 shows the architecture of a remote backup system.\nWhen the primary site fails, the remote backup site takes over processing. First,\nhowever, it performs recovery, using its (perhaps outdated) copy of the data from the\nprimary and the log records received from the primary. In e\ufb00ect, the remote backup\nsite is performing recovery actions that would have been performed at the primary site\nwhen the latter recovered. Standard recovery algorithms, with minor modi\ufb01cations, can\nbe used for recovery at the remote backup site. Once recovery has been performed, the\nremote backup site starts processing transactions.\nAvailability is greatly increased over a single-site system, since the system can re-\ncover even if all data at the primary site are lost.\nSeveral issues must be addressed in designing a remote backup system:\n\u2022Detection of failure . It is important for the remote backup system to detect when\nthe primary has failed. Failure of communication lines can fool the remote backup\ninto believing that the primary has failed. To avoid this problem, we maintain sev-\neral communication links with independent modes of failure between the primary\nand the remote backup. For example, several independent network connections,\nincluding perhaps a modem connection over a telephone line, may be used. These\nconnections may be backed up via manual intervention by operators, who can\ncommunicate over the telephone system.\n\u2022Transfer of control . When the primary fails, the backup site takes over processing\nand becomes the new primary. The decision to transfer control can be done man-\nually or can be automated using software provided by database system vendors.\nQueries must now be sent to the new primary. To do so automatically, many sys-\ntems assign the IPaddress of the old primary to the new primary. Existing database\nconnections will fail, but when an application tries to reopen a connection it gets\nconnected to the new primary. Some systems instead use a high availability proxy\n3Since earthquakes can cause damage over a wide area, the backup is generally required to be in a di\ufb00erent seismic\nzone.\n", "961": "19.7 High Availability Using Remote Backup Systems 933\nmachine. Application clients do not conne ct to the database directly, but connect\nthrough the proxy. The proxy transparently routes application requests to the cur-\nrent primary. (There can be more than one machine acting as proxy at the same\ntime, to deal with a situation where a proxy machine fails; requests can be routed\nthrough any active proxy machine.)\nWhen the original primary site recovers, it can either play the role of remote\nbackup or it can take over the role of primary site again. In either case, the old\nprimary must receive a log of updates carried out by the backup site while the old\nprimary was down. The old primary must catch up with the updates in the log by\napplying them locally. The old primary can then act as a remote backup site. If\ncontrol must be transferred back, the new primary (which is the old backup site)\ncan pretend to have failed, resulting in the old primary taking over.\n\u2022Time to recover . If the log at the remote backup grows large, recovery will take a\nlong time. The remote backup site can periodically process the redo log records\nthat it has received and can perform a checkpoint so that earlier parts of the log\ncan be deleted. The delay before the remote backup takes over can be signi\ufb01cantly\nreduced as a result.\nAhot-spare con\ufb01guration can make takeover by the backup site almost instanta-\nneous. In this con\ufb01guration, the remote backup site continually processes redo log\nrecords as they arrive, applying the updates locally. As soon as the failure of the\nprimary is detected, the backup site completes recovery by rolling back incomplete\ntransactions; it is then ready to process new transactions.\n\u2022Time to commit . To ensure that the updates of a committed transaction are durable,\na transaction must not be declared committed until its log records have reached\nthe backup site. This delay can result in a longer wait to commit a transaction,\nand some systems therefore permit lower degrees of durability. The degrees of\ndurability can be classi\ufb01ed as follows:\n\u00b0One-safe . A transaction commits as soon as its commit log record is written to\nstable storage at the primary site.\nThe problem with this scheme is that the updates of a committed transac-\ntion may not have made it to the backup site when the backup site takes over\nprocessing. Thus, the updates may appear to be lost. When the primary site\nrecovers, the lost updates cannot be merged in directly, since the updates may\ncon\ufb02ict with later updates performed at the backup site. Thus, human inter-\nvention may be required to bring the database to a consistent state.\n\u00b0Two-very-safe . A transaction commits as soon as its commit log record is written\nto stable storage at the primary and the backup site.\nThe problem with this scheme is that transaction processing cannot proceed\nif either the primary or the backup site is down. Thus, availability is actually\nless than in the single-site case, although the probability of data loss is much\nless.\n", "962": "934 Chapter 19 Recovery System\n\u00b0Two-safe . This scheme is the same as two-very-safe if both primary and backup\nsites are active. If only the primary is active, the transaction is allowed to com-\nmit as soon as its commit log record is written to stable storage at the primary\nsite.\nThis scheme provides better availability than does two-very-safe, while avoid-\ning the problem of lost transactions faced by the one-safe scheme. It results in\na slower commit than the one-safe scheme, but the bene\ufb01ts generally outweigh\nthe cost.\nMost database systems today provide support for replication to a backup copy,\nalong with support for hot spares and quick switchover from the primary to the backup.\nMany database systems also allow replication to more than one backup; such a feature\ncan be used to provide a local backup to deal with machine failures, along with a remote\nbackup to deal with disasters.\nAlthough update transactions cannot be executed at a backup server, many\ndatabase systems allow read-only queries to be executed at backup servers. The load\nat the primary can be reduced by executing at least some of the read-only transactions\nat the backup. Snapshot-isolation can be used at the backup server to give readers a\ntransaction consistent view of the data, while ensuring that updates are never blocked\nfrom being applied at the backup.\nRemote backup is also supported at the level of \ufb01le systems, typically by network\n\ufb01le system or NAS implementations, as well as at the disk level, typically by storage\narea network ( SAN) implementations. Remote backups are kept synchronized with the\nprimary by ensuring that all block writes performed at the primary are also replicated\nat the backup. File-system level and disk level backups can be used to replicate the\ndatabase data as well as log \ufb01les. If the primary fails, the backup system can recover\nusing its replica of the data and log \ufb01les. However, to ensure that recovery will work\ncorrectly at the backup site, the \ufb01le system level replication must be done in a way\nthat ensures that the write-ahead logging ( WAL ) rule continues to hold. To do so, if\nthe database forces a block to disk and then performs some other update actions at\nthe primary, the block must also be forced to disk at the backup, before subsequent\nupdates are performed at the backup system.\nAn alternative way of achieving high availability is to use a distributed database ,\nwith data replicated at more than one site. Transactions are then required to update all\nreplicas of any data item that they update. We study distributed databases, including\nreplication, in Chapter 23. When properly implemented, distributed databases can pro-\nvide a higher level of availability than remote backup systems, but are more complex\nand expensive to implement and maintain.\nEnd-users typically interact with applicat ions, rather than directly with database.\nTo ensure availability of an application, as well as to support handling of a large number\nof requests per second, applications may run on multiple application servers. Requests\nfrom clients are load-balanced across the servers. The load-balancer ensures that all\nrequests from a particular client are sent to a single application server, as long as the\n", "963": "19.8 Early Lock Release and Logical Undo Operations 935\napplication server is functional. If an application server fails, client requests are routed\nto other application servers, so users can c ontinue to use the application. Although\nusers may notice a small interruption, application servers can ensure that a user is not\nforced to login again, by sharing session information across application servers.\n19.8 Early Lock Release and Logical Undo Operations\nAny index used in processing a transaction, such as a B+-tree, can be treated as normal\ndata, but to increase concurrency, we can use the B+-tree concurrency-control algo-\nrithm described in Section 18.10.2 to allow locks to be released early, in a non-two-phase\nmanner. As a result of early lock release, it is possible that a value in a B+-tree node\nis updated by one transaction T1, which inserts an entry ( V1,R1), and subsequently\nby another transaction T2, which inserts an entry ( V2,R2) in the same node, moving\nthe entry ( V1,R1) even before T1completes execution.4At this point, we cannot undo\ntransaction T1by replacing the contents of the node with the old value prior to T1per-\nforming its insert, since that would also undo the insert performed by T2;t r a n s a c t i o n\nT2may still commit (or may have already committed). In this example, the only way to\nundo the e\ufb00ect of insertion of ( V1,R1) is to execute a corresponding delete operation.\nIn the rest of this section, we see how to extend the recovery algorithm of Section\n19.4 to support early lock release.\n19.8.1 Logical Operations\nThe insertion and deletion operations are examples of a class of operations that require\nlogical undo operations since they release locks early; we call such operations logical\noperations . Such early lock release is important not only for indices, but also for oper-\nations on other system data structures that are accessed and updated very frequently;\nexamples include data structures that track the blocks containing records of a relation,\nthe free space in a block, and the free blocks in a database. If locks were not released\nearly after performing operations on such data structures, transactions would tend to\nrun serially, a\ufb00ecting system performance.\nThe theory of con\ufb02ict serializability has been extended to operations, based on\nwhat operations con\ufb02ict with what other operations. For example, two insert opera-\ntions on a B+-tree do not con\ufb02ict if they insert di\ufb00erent key values, even if they both\nupdate overlapping areas of the same index page. However, insert and delete opera-\ntions con\ufb02ict with other insert and delete operations, as well as with read operations,\nif they use the same key value. See the bibliographical notes for references to more\ninformation on this topic.\nOperations acquire lower-level locks while they execute but release them when they\ncomplete; the corresponding transaction must however retain a higher-level lock in a\n4Recall that an entry consists of a key value and a record identi\ufb01er, or a key value and a record in the case of the leaf\nlevel of a B+-tree \ufb01le organization.\n", "964": "936 Chapter 19 Recovery System\ntwo-phase manner to prevent concurrent transactions from executing con\ufb02icting ac-\ntions. For example, while an insert operation is being performed on a B+-tree page,\na short-term lock is obtained on the page, allowing entries in the page to be shifted\nduring the insert; the short-term lock is released as soon as the page has been updated.\nSuch early lock release allows a second insert to execute on the same page. However,\neach transaction must obtain a lock on the key values being inserted or deleted and\nretain it in a two-phase manner, to prevent a concurrent transaction from executing a\ncon\ufb02icting read, insert, or delete operation on the same key value.\nOnce the lower-level lock is released, the operation cannot be undone by using the\nold values of updated data items and must instead be undone by executing a compen-\nsating operation; such an operation is called a logical undo operation .I ti si m p o r t a n t\nthat the lower-level locks acquired during an operation are su\ufb03cient to perform a sub-\nsequent logical undo of the operation, for reasons explained later in Section 19.8.4.\n19.8.2 Logical Undo Log Records\nTo allow logical undo of operations, before an operation is performed to modify an\nindex, the transaction creates a log record <Ti,Oj,operation-begin >,w h e r e Ojis a\nunique identi\ufb01er for the operation instance.5While the system is executing the oper-\nation, it creates update log records in the normal fashion for all updates performed\nb yt h eo p e r a t i o n .T h u s ,t h eu s u a lo l d - v a l u ea n dn e w - v a l u ei n f o r m a t i o ni sw r i t t e no u t\nas usual for each update performed by the operation; the old-value information is re-\nquired in case the transaction needs to be rolled back before the operation completes.\nWhen the operation \ufb01nishes, it writes an operation-end log record of the form <Ti,Oj,\noperation-end ,U>,w h e r et h e Udenotes undo information.\nFor example, if the operation inserted an entry in a B+-tree, the undo information\nUwould indicate that a deletion operation is to be performed and would identify the\nB+-tree and what entry to delete from the tree. Such logging of information about op-\nerations is called logical logging . In contrast, logging of old-value and new-value infor-\nmation is called physical logging , and the corresponding log records are called physical\nlog records .\nNote that in the above scheme, logical logging is used only for undo, not for redo;\nredo operations are performed exclusively using physical log record. This is because the\nstate of the database after a system failure may re\ufb02ect some updates of an operation and\nnot other operations, depending on what bu\ufb00er blocks had been written to disk before\nthe failure. Data structures such as B+-trees would not be in a consistent state, and\nneither logical redo nor logical undo operations can be performed on an inconsistent\ndata structure. To perform logical redo or undo, the database state on disk must be\noperation consistent , that is, it should not have partial e\ufb00ects of any operation. However,\nas we shall see, the physical redo processing in the redo phase of the recovery scheme,\nalong with undo processing using physical log records, ensures that the parts of the\n5The position in the log of the operation-begin log record can be used as the unique identi\ufb01er.\n", "965": "19.8 Early Lock Release and Logical Undo Operations 937\ndatabase accessed by a logical undo operation are in an operation consistent state\nbefore the logical undo operation is performed.\nAn operation is said to be idempotent if executing it several times in a row gives the\nsame result as executing it once. Operations such as inserting an entry into a B+-tree\nmay not be idempotent, and the recovery algorithm must therefore make sure that an\noperation that has already been performed is not performed again. On the other hand,\na physical log record is idempotent, since the corresponding data item would have the\nsame value regardless of whether the logged update is executed one or multiple times.\n19.8.3 Transaction Rollback with Logical Undo\nWhen rolling back a transaction Ti, the log is scanned backwards, and log records\ncorresponding to Tiare processed as follows:\n1.Physical log records encountered during the scan are handled as described earlier,\nexcept those that are skipped as described shortly. Incomplete logical operations\nare undone using the physical log records generated by the operation.\n2.Completed logical operations, identi\ufb01ed by operation-end records, are rolled\nback di\ufb00erently. Whenever the system \ufb01nds a log record <Ti,Oj,operation-end ,\nU>, it takes special actions:\na. It rolls back the operation by using the undo information Uin the log\nrecord. It logs the updates performed during the rollback of the operation\njust like updates performed when the operation was \ufb01rst executed.\nAt the end of the operation rollback, instead of generating a log record\n<Ti,Oj,operation-end ,U>, the database system generates a log record\n<Ti,Oj,operation-abort >.\nb. As the backward scan of the log continues, the system skips all log records\nof transaction Tiuntil it \ufb01nds the log record <Ti,Oj,operation-begin >.\nAfter it \ufb01nds the operation-begin log record, it processes log records of\ntransaction Tiin the normal manner again.\nObserve that the system logs physical undo information for the updates per-\nformed during rollback, instead of using redo-only compensation log records.\nThis is because a crash may occur while a logical undo is in progress, and on re-\ncovery the system has to complete the logical undo; to do so, restart recovery will\nundo the partial e\ufb00ects of the earlier undo, using the physical undo information,\nand then perform the logical undo again.\nObserve also that skipping over physical log records when the operation-end\nlog record is found during rollback ensures that the old values in the physical log\nrecord are not used for rollback once the operation completes.\n", "966": "938Chapter 19 Recovery System\n3.If the system \ufb01nds a record <Ti,Oj,operation-abort >, it skips all preceding re-\ncords (including the operation-end record for Oj) until it \ufb01nds the record <Ti,Oj,\noperation-begin >.\nAnoperation-abort log record would be found only if a transaction that is\nbeing rolled back had been partially rolled back earlier. Recall that logical op-\nerations may not be idempotent, and hence a logical undo operation must not\nbe performed multiple times. These pr eceding log records must be skipped to\nprevent multiple rollback of the same operation in case there had been a crash\nduring an earlier rollback and the transaction had already been partly rolled back.\n4.As before, when the <Tistart>log record has been found, the transaction roll-\nback is complete, and the system adds a record <Tiabort>to the log.\nIf a failure occurs while a logical operation is in progress, the operation-end log\nrecord for the operation will not be found when the transaction is rolled back. However,\nfor every update performed by the operation, undo information\u2014in the form of the old\nvalue in the physical log records\u2014is available in the log. The physical log records will\nbe used to roll back the incomplete operation.\nNow suppose an operation undo was in progress when the system crash occurred,\nwhich could happen if a transaction was being rolled back when the crash occurred.\n<T0, C, 700, 600> \n<T1, C, 600, 400>\n<T0, C, 400, 500> \n<T0, B, 2000><T0 start>If T0 aborts before \noperation O 1 ends, undo of \nupdate to C will be physical\nT0 has completed operation O 1 \non C, releases lower-level \nlock; physical undo cannot be \ndone anymore, logical undo \nwill add 100 to C\nT1 can update C since T 0 has \nreleased lower-level lock on C\nLogical undo of O 1 adds 100 \non CT1 releases lower-level lock\non C \nO1 undo completeT  \ndecides \nto abort<T1 start>\n<T0, abort> \n<T1, commit><T0, B, 2000, 2050>\n<T0, O1, operation-begin> \n<T0, O2, operation-begin>\n<T0, O1, operation-abort><T1, O2, operation-end, (C, +200)><T0, O1, operation-end, (C, +100)>Beginning of log\n0\nFigure 19.7 Transaction rollback with logical undo operations.\n", "967": "19.8 Early Lock Release and Logical Undo Operations 939\nThen the physical log records written during operation undo would be found, and the\npartial operation undo would itself be undone using these physical log records. Con-\ntinuing in the backward scan of the log, the original operation\u2019s operation-end record\nwould then be found, and the operation undo would be executed again. Rolling back\nthe partial e\ufb00ects of the earlier undo operation using the physical log records brings\nthe database to a consistent state, allowing the logical undo operation to be executed\nagain.\nFigure 19.7 shows an example of a log generated by two transactions, which add or\nsubtract a value from a data item. Early lock release on the data item Cby transaction\nT0after operation O1completes allows transaction T1to update the data item using\nO2,e v e nb e f o r e T0completes, but necessitates logical undo. The logical undo operation\nneeds to add or subtract a value from the data item instead of restoring an old value to\nthe data item.\nThe annotations on the \ufb01gure indicate that before an operation completes, rollback\ncan perform physical undo; after the operation completes and releases lower-level locks,\nthe undo must be performed by subtracting or adding a value, instead of restoring the\nold value. In the example in the \ufb01gure, T0rolls back operation O1by adding 100 to C;\non the other hand, for data item B, which was not subject to early lock release, undo is\nperformed physically. Observe that T1, which had performed an update on C,c o m m i t s ,\nand its update O2, which added 200 to Cand was performed before the undo of O1,\nhas persisted even though O1has been undone.\nFigure 19.8 shows an example of recovery from a crash with logical undo logging.\nIn this example, operation T1was active and executing operation O4at the time of\ncheckpoint. In the redo pass, the actions of O4that are after the checkpoint log record\nare redone. At the time of crash, operation O5was being executed by T2,b u tt h eo p e r -\nation was not complete. The undo-list contains T1and T2at the end of the redo pass.\nDuring the undo pass, the undo of operation O5is carried out using the old value in\nthe physical log record, setting Cto 400; this operation is logged using a redo-only log\nrecord. The start record of T2is encountered next, resulting in the addition of <T2\nabort>to the log and removal of T2from undo-list.\nThe next log record encountered is the operation-end record of O4;l o g i c a lu n d o\nis performed for this operation by adding 300 to C, which is logged physically, and\nanoperation-abort log record is added for O4. The physical log records that were\npart of O4are skipped until the operation-begin log record for O4is encountered.\nIn this example, there are no other intervening log records, but in general log records\nfrom other transactions may be found before we reach the operation-begin log record;\nsuch log records should of course not be skipped (unless they are part of a completed\noperation for the corresponding transaction and the algorithm skips those records).\nAfter the operation-begin log record is found for O4, a physical log record is found\nforT1, which is rolled back physically. Finally the start log record for T1is found; this\nresults in <T1abort>being added to the log and T1being deleted from undo-list. At\nthis point undo-list is empty, and the undo phase is complete.\n", "968": "940 Chapter 19 Recovery System\n<T0 start>\nRecords\nadded\nduring\nrecoveryStart log records\nfound for all\ntransactions in\nundo list\nUpdate of C was part of O 5, undone\nphysically during recovery since\nO5 did not complete\nLogical undo of O 4 adds 300 to C<T1 start>\n<T2 start><T0 commit><T0, B, 2000, 2050>\n<T1, B, 2050, 2100>\n<T1, C, 700, 400>\n<T2, C, 400, 300>\nUndo list: T1, T2 \n<T2, C, 400>\n<T1, C, 400, 700>\n<T1, B, 2050>\n<T1 abort><T2 abort><T1, O4, operation-begin>\n<T1, O4, operation-end (C, +300)>\n<T2, O5, operation-begin>\n<T1, O4, operation-abort><checkpoint { T1}>Beginning of log\nRedo Pass\nUndo PassEnd of\nlog at\ncrash!\nFigure 19.8 Failure recovery actions with logical undo operations.\n19.8.4 Concurrency Issues in Logical Undo\nAs mentioned earlier, it is important that the lower-level locks acquired during an op-\neration are su\ufb03cient to perform a subsequent logical undo of the operation; otherwise\nconcurrent operations that execute during normal processing may cause problems in\nthe undo phase. For example, suppose the logical undo of operation O1of transaction\nT1can con\ufb02ict at the data item level with a concurrent operation O2of transaction T2,\nand O1completes while O2does not. Assume also that neither transaction had commit-\nted when the system crashed. The physical update log records of O2may appear before\nand after the operation-end record for O1, and during recovery updates done during\nthe logical undo of O1may get fully or partially overwritten by old values during the\nphysical undo of O2. This problem cannot occur if O1had obtained all the lower-level\nlocks required for the logical undo of O1, since then there cannot be such a concurrent\nO2.\nIf both the original operation and its logical undo operation access a single page\n(such operations are called physiological op erations and are discussed in Section 19.9),\nthe locking requirement above is met easily. Otherwise the details of the speci\ufb01c opera-\ntion need to be considered when deciding on what lower-level locks need to be obtained.\nFor example, update operations on a B+-tree could obtain a short-term lock on the root,\n", "969": "19.9 ARIES 941\nto ensure that operations execute serially. See the bibliographical notes for references\non B+-tree concurrency control and recovery exploiting logical undo logging. See the\nbibliographical notes also for references to an alternative approach, called multilevel\nrecovery, which relaxes this locking requirement.\n19.9 ARIES\nThe state of the art in recovery methods is best illustrated by the ARIES recovery\nmethod. The recovery technique that we described in Section 19.4, along with the log-\nical undo logging techniques described in Section 19.8, are modeled after ARIES ,b u t\nthey have been simpli\ufb01ed signi\ufb01cantly to bring out key concepts and make them easier\nto understand. In contrast, ARIES uses a number of techniques to reduce the time taken\nfor recovery and to reduce the overhead of checkpointing. In particular, ARIES is able\nto avoid redoing many logged operations that have already been applied and to reduce\nthe amount of information logged. The price paid is greater complexity; the bene\ufb01ts\nare worth the price.\nThe four major di\ufb00erences between ARIES and the recovery algorithm presented\nearlier are that ARIES :\n1.Uses a log sequence number (LSN) to identify log records and stores LSNsi n\ndatabase pages to identify which operations have been applied to a database page.\n2.Supports physiological redo operations, which are physical in that the a\ufb00ected\npage is physically identi\ufb01ed but can be logical within the page.\nFor instance, the deletion of a record from a page may result in many other\nrecords in the page being shifted, if a slotted page structure (Section 13.2.2) is\nused. With physical redo logging, all bytes of the page a\ufb00ected by the shifting of\nrecords must be logged. With physiological logging, the deletion operation can\nbe logged, resulting in a much smaller log record. Redo of the deletion operation\nwould delete the record and shift other records as required.\n3.Uses a dirty page table to minimize unnecessary redos during recovery. As men-\ntioned earlier, dirty pages are those that have been updated in memory, and the\ndisk version is not up-to-date.\n4.Uses a fuzzy-checkpointing scheme that records only information about dirty\npages and associated information and does not even require writing of dirty pages\nto disk. It \ufb02ushes dirty pages in the background, continuously, instead of writing\nthem during checkpoints.\nIn the rest of this section, we provide an overview of ARIES . The bibliographical notes\nlist some references that provide a complete description of ARIES .\n", "970": "942 Chapter 19 Recovery System\n19.9.1 Data Structures\nEach log record in ARIES has a log sequence number ( LSN) that uniquely identi\ufb01es\nthe record. The number is conceptually just a logical identi\ufb01er whose value is greater\nfor log records that occur later in the log. In practice, the LSN is generated in such a\nway that it can also be used to locate the log record on disk. Typically, ARIES splits a\nlog into multiple log \ufb01les, each of which has a \ufb01le number. When a log \ufb01le grows to\nsome limit, ARIES appends further log records to a new log \ufb01le; the new log \ufb01le has a\n\ufb01le number that is higher by 1 than the previous log \ufb01le. The LSN then consists of a\n\ufb01le number and an o\ufb00set within the \ufb01le.\nEach page also maintains an identi\ufb01er called the Page LSN. Whenever an update\noperation (whether physical or physiological) occurs on a page, the operation stores\ntheLSN of its log record in the Page LSN \ufb01eld of the page. During the redo phase of\nrecovery, any log records with LSN less than or equal to the Page LSN of a page should\nnot be executed on the page, since their actions are already re\ufb02ected on the page. In\ncombination with a scheme for recording Page LSNs as part of checkpointing, which we\npresent later, ARIES can avoid even reading many pages for which logged operations\nare already re\ufb02ected on disk. Thereby, recovery time is reduced signi\ufb01cantly.\nThe Page LSN is essential for ensuring idempotence in the presence of physiological\nredo operations, since reapplying a physiological redo that has already been applied to\nap a g ec o u l dc a u s ei n c o r r e c tc h a n g e st oap a g e .\nPages should not be \ufb02ushed to disk while an update is in progress, since physio-\nlogical operations cannot be redone on the pa rtially updated state of the page on disk.\nTherefore, ARIES uses latches on bu\ufb00er pages to prevent them from being written to\ndisk while they are being updated. It releases the bu\ufb00er page latch only after the update\nis completed and the log record for the update has been written to the log.\nEach log record also contains the LSN of the previous log record of the same trans-\naction. This value, stored in the Prev LSN \ufb01eld, permits log records of a transaction to\nbe fetched backward, without reading the whole log. There are special redo-only log\nrecords generated during transaction rollback, called compensation log records (CLRs)\ninARIES . These serve the same purpose as the redo-only log records in our earlier re-\ncovery scheme. In addition, CLRss e r v et h er o l eo ft h e operation-abort log records in\nour scheme. The CLRs have an extra \ufb01eld, called the UndoNext LSN,t h a tr e c o r d st h e\nLSN of the log that needs to be undone next, when the transaction is being rolled back.\nT h i s\ufb01 e l ds e r v e st h es a m ep u r p o s ea st h eo p e r a t i o ni d e n t i \ufb01 e ri nt h e operation-abort\nlog record in our earlier recovery scheme, which helps to skip over log records that\nhave already been rolled back.\nTheDirtyPageTable contains a list of pages that have been updated in the database\nbu\ufb00er. For each page, it stores the Page LSN and a \ufb01eld called the Rec LSN, which helps\nidentify log records that have been applied already to the version of the page on disk.\nWhen a page is inserted into the DirtyPageTable (when it is \ufb01rst modi\ufb01ed in the bu\ufb00er\npool), the value of Rec LSN is set to the current end of log. Whenever the page is \ufb02ushed\nto disk, the page is removed from the DirtyPageTable.\n", "971": "19.9 ARIES 943\nAcheckpoint log record contains the DirtyPageTable and a list of active transac-\ntions. For each transaction, the checkpoint log record also notes Last LSN,t h e LSN of\nthe last log record written by the transaction. A \ufb01xed position on disk also notes the\nLSN of the last (complete) checkpoint log record.\nFigure 19.9 illustrates some of the data structures used in ARIES .T h el o gr e c o r d s\nshown in the \ufb01gure are pre\ufb01xed by their LSN; these may not be explicitly stored, but\ninferred from the position in the log, in an actual implementation. The data item iden-\nti\ufb01er in a log record is shown in two parts, for example, 4894.1; the \ufb01rst identi\ufb01es the\npage, and the second part identi\ufb01es a record within the page (we assume a slotted page\nrecord organization within a page). Note that the log is shown with the newest records\non top, since older log records, which are on disk, are shown lower in the \ufb01gure.\nEach page (whether in the bu\ufb00er or on disk) has an associated Page LSN \ufb01eld. You\ncan verify that the LSN for the last log record that updated page 4894 is 7567. By com-\nparing Page LSNs for the pages in the bu\ufb00er with the Page LSNs for the corresponding\npages in stable storage, you can observe that the DirtyPageTable contains entries for\nall pages in the bu\ufb00er that have been modi\ufb01ed since they were fetched from stable stor-\nage. The Rec LSN entry in the DirtyPageTable re\ufb02ects the LSN at the end of the log\n7567\n75652345\n7567: <T 145,4894.1, 40, 60>\n7566: <T 143, commit>\n(PrevLSN and UndoNextLSN\n\ufb01elds not shown)PageID PageLSN RecLSN\nPage 9923 Page 4894\nPage 7200\nDatabase Buffer\nStable data Stable logDirty Page Table\nLog Buffer\nPage 4894\nPage 9923Page 72004566\n23454404\n7563: <T 145begin>7564: <T 145,4894.1, 20, 40>7565: <T 143,7200.2, 60, 80>4894\n72007567\n75657564\n7565\nFigure 19.9 Data structures used in ARIES .\n", "972": "944 Chapter 19 Recovery System\nwhen the page was added to DirtyPageTable and would be greater than or equal to the\nPage LSN for that page on stable storage.\n19.9.2 Recovery Algorithm\nARIES recovers from a system crash in three passes.\n\u2022Analysis pass : This pass determines which transactions to undo, which pages were\ndirty at the time of the crash, and the LSN from which the redo pass should start.\n\u2022Redo pass : This pass starts from a position determined during analysis and per-\nforms a redo, repeating history, to bring the database to a state it was in before the\ncrash.\n\u2022Undo pass : This pass rolls back all transactions that were incomplete at the time\nof crash.\n19.9.2.1 Analysis Pass\nThe analysis pass \ufb01nds the last complete checkpoint log record and reads in the Dirty-\nPageTable from this record. It then sets Redo LSN to the minimum of the Rec LSNso f\nthe pages in the DirtyPageTable. If there are no dirty pages, it sets Redo LSN to the LSN\nof the checkpoint log record. The redo pass starts its scan of the log from Redo LSN. All\nthe log records earlier than this point have already been applied to the database pages\non disk. The analysis pass initially sets the list of transactions to be undone, undo-list,\nto the list of transactions in the checkpoint log record. The analysis pass also reads\nfrom the checkpoint log record the LSNs of the last log record for each transaction in\nundo-list.\nThe analysis pass continues scanning forward from the checkpoint. Whenever it\n\ufb01nds a log record for a transaction not in the undo-list, it adds the transaction to undo-\nlist. Whenever it \ufb01nds a transaction end log record, it deletes the transaction from\nundo-list. All transactions left in undo-list at the end of analysis have to be rolled back\nlater, in the undo pass. The analysis pass also keeps track of the last record of each\ntransaction in undo-list, which is used in the undo pass.\nThe analysis pass also updates DirtyPageTable whenever it \ufb01nds a log record for\nan update on a page. If the page is not in DirtyPageTable, the analysis pass adds it to\nDirtyPageTable and sets the Rec LSN of the page to the LSN of the log record.\n19.9.2.2 Redo Pass\nThe redo pass repeats history by replaying every action that is not already re\ufb02ected in\nt h ep a g eo nd i s k .T h er e d op a s ss c a n st h el o gf o r w a r df r o mR e d o LSN. Whenever it\n\ufb01nds an update log record, it takes this action:\n", "973": "19.9 ARIES 945\n\u2022If the page is not in DirtyPageTable or if the LSN of the update log record is less\nthan the Rec LSN of the page in DirtyPageTable, then the redo pass skips the log\nrecord.\n\u2022Otherwise the redo pass fetches the page from disk, and if the Page LSN is less than\ntheLSN of the log record, it redoes the log record.\nNote that if either of the tests is negative, then the e\ufb00ects of the log record have\nalready appeared on the page; otherwise the e\ufb00ects of the log record are not re\ufb02ected on\nthe page. Since ARIES allows non-idempotent physiological log records, a log record\nshould not be redone if its e\ufb00ect is already re\ufb02ected on the page. If the \ufb01rst test is\nnegative, it is not even necessary to fetch the page from disk to check its Page LSN.\n19.9.2.3 Undo Pass and Transaction Rollback\nThe undo pass is relatively straightforward. It performs a single backward scan of the\nlog, undoing all transactions in undo-list. The undo pass examines only log records of\ntransactions in undo-list; the last LSN recorded during the analysis pass is used to \ufb01nd\nthe last log record for each transaction in undo-list.\nWhenever an update log record is found, it is used to perform an undo (whether\nfor transaction rollback during normal processing, or during the restart undo pass).\nThe undo pass generates a CLR containing the undo action performed (which must\nbe physiological). It sets the UndoNext LSN of the CLR to the Prev LSN value of the\nupdate log record.\nIf aCLR is found, its UndoNext LSN value indicates the LSN of the next log record\nto be undone for that transaction; later log records for that transaction have already\nbeen rolled back. For log records other than CLRs, the Prev LSN \ufb01eld of the log record\nindicates the LSN of the next log record to be undone for that transaction. The next\nlog record to be processed at each stop in the undo pass is the maximum, across all\ntransactions in undo-list, of next log record LSN.\nFigure 19.10 illustrates the recovery actions performed by ARIES on an example\nlog. We assume that the last completed checkpoint pointer on disk points to the check-\npoint log record with LSN 7568. The Prev LSN values in the log records are shown using\narrows in the \ufb01gure, while the UndoNext LSN value is shown using a dashed arrow for\nthe one compensation log record, with LSN 7565, in the \ufb01gure. The analysis pass would\nstart from LSN 7568, and when it is complete, Redo LSN would be 7564. Thus, the redo\npass must start at the log record with LSN 7564. Note that this LSN is less than the\nLSN of the checkpoint log record, since the ARIES checkpointing algorithm does not\n\ufb02ush modi\ufb01ed pages to stable storage. The DirtyPageTable at the end of analysis would\ninclude pages 4894, 7200 from the checkpoint log record, and 2390 which is updated\nby the log record with LSN 7570. At the end of the analysis pass, the list of transactions\nto be undone consists of only T145in this example.\nThe redo pass for the preceding example starts from LSN 7564 and performs redo\nof log records whose pages appear in DirtyPageTable. The undo pass needs to undo\n", "974": "946 Chapter 19 Recovery System\nnewer\nolderCLR\nUndoNextLSNPrevLSN\npointers End of log at crash\nAnalysis\npass\nRedo\npassUndo\npass7571: <T 146 commit>\n7570: <T 146, 2390.4, 50, 90>\n7567: <T 145,4894.1, 40, 60>\n7565: <T 143,7200.2, 60>\n7564: <T 145,4894.1, 20, 40>\n7563: <T 145 begin>\n7562: <T 143, 7200.2, 60, 80>7566: <T 143 commit>7569: <T 146 begin>\n7568: checkpoint\nTxn lastLSN\n7567 T145\n4894\n72007567\n75657564\n7565PageID PageLSN RecLSN\nFigure 19.10 Recovery actions in ARIES .\nonly transaction T145, and hence it starts from its Last LSN value 7567 and continues\nbackwards until the record <T145start>is found at LSN 7563.\n19.9.3 Other Features\nAmong other key features that ARIES provides are:\n\u2022Nested top actions :ARIES allows the logging of operations that should not be un-\ndone even if a transaction gets rolled back; for example, if a transaction allocates a\npage to a relation, even if the transaction is rolled back, the page allocation should\nnot be undone since other transactions may have stored records in the page. Such\noperations that should not be undone are called nested top actions. Such opera-\nt i o n sc a nb em o d e l e da so p e r a t i o n sw h o s eu n d oa c t i o nd o e sn o t h i n g .I n ARIES ,\nsuch operations are implemented by creating a dummy CLR whose UndoNext LSN\nis set such that transaction rollback skips the log records generated by the opera-\ntion.\n\u2022Recovery independence : Some pages can be recovered independently from others\nso that they can be used even while other pages are being recovered. If some pages\n", "975": "19.10 Recovery in Main-Memory Databases 947\nof a disk fail, they can be recovered without stopping transaction processing on\nother pages.\n\u2022Savepoints : Transactions can record savepoints and can be rolled back partially up\nto a savepoint. This can be quite useful for d eadlock handling, since transactions\ncan be rolled back up to a point that permits release of required locks and then\nrestarted from that point.\nProgrammers can also use savepoints to undo a transaction partially, and then\ncontinue execution; this approach can be useful to handle certain kinds of errors\ndetected during the transaction execution.\n\u2022Fine-grained locking :T h e ARIES recovery algorithm can be used with index\nconcurrency-control algorithms that permit tuple-level locking on indices, instead\nof page-level locking, which improves concurrency signi\ufb01cantly.\n\u2022Recovery optimizations : The DirtyPageTable can be used to prefetch pages during\nredo, instead of fetching a page only when the system \ufb01nds a log record to be\napplied to the page. Out-of-order redo is also possible: Redo can be postponed on a\npage being fetched from disk and performed when the page is fetched. Meanwhile,\nother log records can continue to be processed.\nIn summary, the ARIES algorithm is a state-of-the-art recovery algorithm, incor-\nporating a variety of optimizations designed to improve concurrency, reduce logging\noverhead, and reduce recovery time.\n19.10 Recovery in Main-Memory Databases\nMain-memory databases support fast querying and updates, since main memory sup-\nports very fast random access. However, the contents of main memory are lost on\nsystem failure, as well as on system shutdown. Thus, data must be additionally stored\non persistent or stable storage to allow recovery of data when the system comes back\nup.\nTraditional recovery algorithms can be used with main-memory databases. Log\nrecords for updates have to be output to stable storage. On recovery, the database has\nto be reloaded from disk and log records applied to restore the database state. Data\nblocks that have been modi\ufb01ed by committed transactions still have to be written to\ndisk, and checkpoints have to be performed, so that the amount of log that has to be\nreplayed at recovery time is reduced.\nHowever, some optimizations are possible with main-memory databases.\n\u2022With main-memory databases, indices can b e rebuilt very quickly after the under-\nlying relation is brought into memory and recovery has been performed on the\nrelation. Thus, many systems do not perform any redo logging actions for index\nupdates. Undo logging to support transaction abort is still required, but such undo\n", "976": "948 Chapter 19 Recovery System\nNote 19.2 NON-VOLATILE RAM\nSome newly launched non-volatile storage systems support direct access to indi-\nvidual words, instead of requiring that an entire page must be read or written. Such\nnon-volatile RAM systems, also called storage class memory (SCM ), support very\nfast random access, with latency and bandwidth comparable to RAM access. The\ncontents of such non-volatile RAM survive power failures, like \ufb02ash, but o\ufb00er direct\naccess, like RAM . In terms of capacity and cost per megabyte, current generation\nnon-volatile storage lies between RAM and \ufb02ash storage.\nRecovery techniques have been specialized to deal with NVRAM storage. In\nparticular, redo logging can be avoided, although undo logging may be used to\ndeal with transaction aborts. Issues such as atomic updates to NVRAM have to be\ntaken into consideration when designing such recovery techniques.\nlog records can be kept in memory, and they need not be written to the log on\nstable storage.\n\u2022Several main-memory databases reduce logging overhead by performing only redo\nlogging. Checkpoints are taken periodically, either ensuring that uncommitted\ndata are not written to disk or avoiding in-place updates of records by creating\nmultiple versions of records. Recovery consists of reloading the checkpoint and\nthen performing redo operations. (Record versions created by uncommitted trans-\nactions must be garbage collected eventually.)\n\u2022Fast recovery is crucial for main-memory databases, since the entire database has\nto be loaded and recovery actions performed before any transaction processing\ncan be done.\nSeveral main-memory databases therefore perform recovery in parallel using\nmultiple cores, to minimize recovery time. To do so, data and log records may be\npartitioned, with log records of a partition a\ufb00ecting only data in the corresponding\ndata partition. Each core is then responsible for performing recovery operations\nfor a particular partition, and it can perform recovery operations in parallel with\nother cores.\n19.11 Summary\n\u2022A computer system, like any other mechanical or electrical device, is subject to\nfailure. There are a variety of causes of such failure, including disk crash, power\nfailure, and software errors. In each of these cases, information concerning the\ndatabase system is lost.\n", "977": "19.11 Summary 949\n\u2022In addition to system failures, transactions may also fail for various reasons, such\nas violation of integrity constraints or deadlocks.\n\u2022An integral part of a database system is a recovery scheme that is responsible\nfor the detection of failures and for the restoration of the database to a state that\nexisted before the occurrence of the failure.\n\u2022The various types of storage in a computer are volatile storage, non-volatile storage,\nand stable storage. Data in volatile storage, such as in RAM ,a r el o s tw h e nt h e\ncomputer crashes. Data in non-volatile storage, such as disk, are not lost when\nthe computer crashes but may occasionally be lost because of failures such as disk\ncrashes. Data in stable storage are never lost.\n\u2022Stable storage that must be accessible online is approximated with mirrored disks,\nor other forms of RAID , which provide redundant data storage. O\ufb04ine, or archival,\nstable storage may consist of multiple tape copies of data stored in a physically\nsecure location.\n\u2022In case of failure, the state of the database system may no longer be consistent;\nthat is, it may not re\ufb02ect a state of the world that the database is supposed to\ncapture. To preserve consistency, we require that each transaction be atomic. It is\nthe responsibility of the recovery scheme to ensure the atomicity and durability\nproperty.\n\u2022In log-based schemes, all updates are recorded on a log, which must be kept in\nstable storage. A transaction is considered to have committed when its last log\nrecord, which is the commit log record for the transaction, has been output to\nstable storage.\n\u2022Log records contain old values and new values for all updated data items. The new\nvalues are used in case the updates need to be redone after a system crash. The old\nvalues are used to roll back the updates of the transaction if the transaction aborts\nduring normal operation, as well as to roll back the updates of the transaction in\ncase the system crashed before the transaction committed.\n\u2022In the deferred-modi\ufb01cations scheme, during the execution of a transaction, all the\nwrite operations are deferred until the transaction has been committed, at which\ntime the system uses the information on the log associated with the transaction in\nexecuting the deferred writes. With deferred modi\ufb01cation, log records do not need\nto contain old values of updated data items.\n\u2022To reduce the overhead of searching the log and redoing transactions, we can use\ncheckpointing techniques.\n\u2022Modern recovery algorithms are based on the concept of repeating history,\nwhereby all actions taken during normal operation (since the last completed check-\npoint) are replayed during the redo pass of recovery. Repeating history restores\n", "978": "950 Chapter 19 Recovery System\nthe system state to what it was at the time the last log record was output to sta-\nble storage before the system crashed. Undo is then performed from this state by\nexecuting an undo pass that processes log records of incomplete transactions in\nreverse order.\n\u2022Undo of an incomplete transaction writes out special redo-only log records and an\nabort log record. After that, the transaction can be considered to have completed,\nand it will not be undone again.\n\u2022Transaction processing is based on a storage model in which main memory holds\na log bu\ufb00er, a database bu\ufb00er, and a system bu\ufb00er. The system bu\ufb00er holds pages\nof system object code and local work areas of transactions.\n\u2022E\ufb03cient implementation of a recovery scheme requires that the number of writes\nto the database and to stable storage be minimized. Log records may be kept in\nvolatile log bu\ufb00er initially, but they must be written to stable storage when one of\nthe following conditions occurs:\n\u00b0Before the <Ticommit >log record may be output to stable storage, all log\nrecords pertaining to transaction Timust have been output to stable storage.\n\u00b0Before a block of data in main memory is output to the database (in non-volatile\nstorage), all log records pertaining to data in that block must have been output\nto stable storage.\n\u2022Remote backup systems provide a high degree of availability, allowing transaction\nprocessing to continue even if the primary site is destroyed by a \ufb01re, \ufb02ood, or\nearthquake. Data and log records from a primary site are continually backed up\nto a remote backup site. If the primary site fails, the remote backup site takes over\ntransaction processing, after executing certain recovery actions.\n\u2022Modern recovery techniques support high-concurrency locking techniques, such\nas those used for B+-tree concurrency control. These techniques allow early re-\nlease of lower-level locks obtained by operations such as inserts or deletes, which\nallows other such operations to be performed by other transactions. After lower-\nlevel locks are released, physical undo is not possible, and instead logical undo,\nsuch as a deletion to undo an insertion, is required. Transactions retain higher-\nlevel locks that ensure that concurrent transactions cannot perform actions that\ncould make logical undo of an operation impossible.\n\u2022To recover from failures that result in the loss of non-volatile storage, we must\ndump the entire contents of the database onto stable storage periodically\u2014say,\nonce per day. If a failure occurs that results in the loss of physical database blocks,\nwe use the most recent dump in restoring the database to a previous consistent\nstate. Once this restoration has been accomplished, we use the log to bring the\ndatabase system to the most recent consistent state.\n", "979": "Review Terms 951\n\u2022The ARIES recovery scheme is a state-of-the-art scheme that supports a number of\nfeatures to provide greater concurrency, reduce logging overheads, and minimize\nrecovery time. It is also based on repeating history, and it allows logical undo\noperations. The scheme \ufb02ushes pages on a continuous basis and does not need to\n\ufb02ush all pages at the time of a checkpoint. It uses log sequence numbers (LSNs)\nto implement a variety of optimizations that reduce the time taken for recovery.\nReview Terms\n\u2022Recovery scheme\n\u2022Failure classi\ufb01cation\n\u00b0Transaction failure\n\u00b0Logical error\n\u00b0System error\n\u00b0System crash\n\u00b0Data-transfer failure\n\u2022Fail-stop assumption\n\u2022Disk failure\n\u2022Storage types\n\u00b0Volatile storage\n\u00b0Non-Volatile storage\n\u00b0Stable storage\n\u2022Blocks\n\u00b0Physical blocks\n\u00b0Bu\ufb00er blocks\n\u2022Disk bu\ufb00er\n\u2022Force-output\n\u2022Log-based recovery\n\u2022Log\n\u2022Log records\n\u2022Update log record\n\u2022Deferred modi\ufb01cation\n\u2022Immediate modi\ufb01cation\u2022Uncommitted modi\ufb01cations\n\u2022Checkpoints\n\u2022Recovery algorithm\n\u2022Restart recovery\n\u2022Transaction rollback\n\u2022Physical undo\n\u2022Physical logging\n\u2022Transaction rollback\n\u2022Restart recovery\n\u2022Redo phase\n\u2022Undo phase\n\u2022Repeating history\n\u2022Bu\ufb00er management\n\u2022Log-record bu\ufb00ering\n\u2022Write-ahead logging ( WAL )\n\u2022Log force\n\u2022Database bu\ufb00ering\n\u2022Latches\n\u2022Operating system and bu\ufb00er\nmanagement\n\u2022Fuzzy checkpointing\n\u2022High availability\n\u2022Remote backup systems\n\u00b0Primary site\n\u00b0Remote backup site\n\u00b0Secondary site\n", "980": "952 Chapter 19 Recovery System\n\u2022Detection of failure\n\u2022Transfer of control\n\u2022Time to recover\n\u2022Hot-spare con\ufb01guration\n\u2022Time to commit\n\u00b0One-safe\n\u00b0Two-very-safe\n\u00b0Two-safe\n\u2022Early lock release\n\u2022Logical operations\n\u2022Logical logging\n\u2022Logical undo\n\u2022Loss of non-volatile storage\n\u2022Archival dump\u2022Fuzzy dump\n\u2022ARIES\n\u00b0Log sequence number ( LSN)\n\u00b0Page LSN\n\u00b0Physiological redo\n\u00b0Compensation log record\n(CLR)\n\u00b0DirtyPageTable\n\u00b0Checkpoint log record\n\u00b0Analysis pass\n\u00b0Redo pass\n\u00b0Undo pass\nPractice Exercises\n19.1 Explain why log records for transactions on the undo-list must be processed in\nreverse order, whereas redo is performed in a forward direction.\n19.2 Explain the purpose of the checkpoint mechanism. How often should check-\npoints be performed? How does the frequency of checkpoints a\ufb00ect:\n\u2022System performance when no failure occurs?\n\u2022T h et i m ei tt a k e st or e c o v e rf r o mas y s t e mc r a s h ?\n\u2022The time it takes to recover from a media (disk) failure?\n19.3 Some database systems allow the administrator to choose between two forms\nof logging: normal logging , used to recover from system crashes, and archival\nlogging , used to recover from media (disk) failure. When can a log record be\ndeleted, in each of these cases, using the recovery algorithm of Section 19.4?\n19.4 Describe how to modify the recovery algorithm of Section 19.4 to implement\nsavepoints and to perform rollback to a savepoint. (Savepoints are described\nin Section 19.9.3.)\n19.5 Suppose the deferred modi\ufb01cation technique is used in a database.\na. Is the old value part of an update log record required any more? Why or\nwhy not?\n", "981": "Practice Exercises 953\nb. If old values are not stored in update log records, transaction undo is\nclearly not feasible. How would the redo phase of recovery have to be\nmodi\ufb01ed as a result?\nc. Deferred modi\ufb01cation can be implemented by keeping updated data\nitems in local memory of transactions and reading data items that have\nnot been updated directly from the database bu\ufb00er. Suggest how to e\ufb03-\nciently implement a data item read, ensuring that a transaction sees its\nown updates.\nd. What problem would arise with the above technique if transactions per-\nform a large number of updates?\n19.6 The shadow-paging scheme requires the page table to be copied. Suppose the\npage table is represented as a B+-tree.\na. Suggest how to share as many nodes as possible between the new copy\nand the shadow copy of the B+-tree, assuming that updates are made\nonly to leaf entries, with no insertions or deletions.\nb. Even with the above optimization, logging is much cheaper than a\nshadow copy scheme, for transactions that perform small updates. Ex-\nplain why.\n19.7 Suppose we (incorrectly) modify the recovery algorithm of Section 19.4 to\nnote log actions taken during transaction rollback. When recovering from a\nsystem crash, transactions that were rolled back earlier would then be included\nin undo-list and rolled back again. Give an example to show how actions taken\nduring the undo phase of recovery could result in an incorrect database state.\n(Hint: Consider a data item updated by an aborted transaction and then up-\ndated by a transaction that commits.)\n19.8 Disk space allocated to a \ufb01le as a result of a transaction should not be released\neven if the transaction is rolled back. Explain why, and explain how ARIES\nensures that such actions are not rolled back.\n19.9 Suppose a transaction deletes a record, and the free space generated thus is\nallocated to a record inserted by another transaction, even before the \ufb01rst trans-\naction commits.\na. What problem can occur if the \ufb01rst transaction needs to be rolled back?\nb. Would this problem be an issue if page-level locking is used instead of\ntuple-level locking?\nc. Suggest how to solve this problem while supporting tuple-level locking,\nby logging post-commit actions in special log records, and executing\n", "982": "954 Chapter 19 Recovery System\nthem after commit. Make sure your scheme ensures that such actions\nare performed exactly once.\n19.10 Explain the reasons why recovery of interactive transactions is more di\ufb03cult\nto deal with than is recovery of batch transactions. Is there a simple way to deal\nwith this di\ufb03culty? (Hint: Consider an automatic teller machine transaction\nin which cash is withdrawn.)\n19.11 Sometimes a transaction has to be undone after it has committed because it\nwas erroneously executed\u2014for example, because of erroneous input by a bank\nteller.\na. Give an example to show that using the normal transaction undo mech-\nanism to undo such a transaction could lead to an inconsistent state.\nb. One way to handle this situation is to bring the whole database to a state\nprior to the commit of the erroneous transaction (called point-in-time re-\ncovery). Transactions that committed later have their e\ufb00ects rolled back\nwith this scheme.\nSuggest a modi\ufb01cation to the recovery algorithm of Section 19.4 to\nimplement point-in-time recovery using database dumps.\nc. Later nonerroneous transactions can be reexecuted logically, if the up-\ndates are available in the form of SQL but cannot be reexecuted using\ntheir log records. Why?\n19.12 The recovery techniques that we described assume that blocks are written\natomically to disk. However, a block may be partially written when power fails,\nwith some sectors written, and others not yet written.\na. What problems can partial block writes cause?\nb. Partial block writes can be detected using techniques similar to those\nused to validate sector reads. Explain how.\nc. Explain how RAID 1 can be used to recover from a partially written\nblock, restoring the block to either its old value or to its new value.\n19.13 The Oracle database system uses undo log records to provide a snapshot view\nof the database under snapshot isolation. The snapshot view seen by transac-\ntion Tire\ufb02ects updates of all transactions that had committed when Tistarted\nand the updates of Ti; updates of all other transactions are not visible to Ti.\nDescribe a scheme for bu\ufb00er handling whereby transactions are given a\nsnapshot view of pages in the bu\ufb00er. Include details of how to use the log to\ngenerate the snapshot view. You can assume that operations as well as their\nundo actions a\ufb00ect only one page.\n", "983": "Exercises 955\nExercises\n19.14 Explain the di\ufb00erence between the three storage types\u2014volatile, nonvolatile,\nand stable\u2014in terms of I/Ocost.\n19.15 Stable storage cannot be implemented.\na. Explain why it cannot be.\nb. Explain how database systems deal with this problem.\n19.16 Explain how the database may become inconsistent if some log records per-\ntaining to a block are not output to stable storage before the block is output to\ndisk.\n19.17 Outline the drawbacks of the no-steal and force bu\ufb00er management policies.\n19.18 Suppose two-phase locking is used, but exclusive locks are released early, that\nis, locking is not done in a strict two-phase manner. Give an example to show\nwhy transaction rollback can result in a wrong \ufb01nal state, when using the log-\nbased recovery algorithm.\n19.19 Physiological redo logging can reduce logging overheads signi\ufb01cantly, espe-\ncially with a slotted page record organization. Explain why.\n19.20 Explain why logical undo logging is used widely, whereas logical redo logging\n(other than physiological redo logging) is rarely used.\n19.21 Consider the log in Figure 19.5. Suppose there is a crash just before the log\nrecord <T0abort>is written out. Explain what would happen during recovery.\n19.22 Suppose there is a transaction that has been running for a very long time but\nhas performed very few updates.\na. What e\ufb00ect would the transaction have on recovery time with the recov-\nery algorithm of Section 19.4, and with the ARIES recovery algorithm?\nb. What e\ufb00ect would the transaction have on deletion of old log records?\n19.23 Consider the log in Figure 19.7. Suppose there is a crash during recovery, just\nbefore the operation abort log record is written for operation O1. Explain what\nwill happen when the system recovers again.\n19.24 Compare log-based recovery with the shadow-copy scheme in terms of their\noverheads for the case when data are being added to newly allocated disk pages\n(in other words, there is no old value to be restored in case the transaction\naborts).\n19.25 In the ARIES recovery algorithm:\n", "984": "956 Chapter 19 Recovery System\na. If at the beginning of the analysis pass, a page is not in the checkpoint\ndirty page table, will we need to apply any redo records to it? Why?\nb. What is Rec LSN, and how is it used to minimize unnecessary redos?\n19.26 Explain the di\ufb00erence between a system crash and a \u201cdisaster.\u201d\n19.27 For each of the following requirements, identify the best choice of degree of\ndurability in a remote backup system:\na. Data loss must be avoided, but some loss of availability may be tolerated.\nb. Transaction commit must be accomplished quickly, even at the cost of\nloss of some committed transactions in a disaster.\nc. A high degree of availability and durability is required, but a longer run-\nning time for the transaction commit protocol is acceptable.\nFurther Reading\n[Gray and Reuter (1993)] is an excellent textbook source of information about recov-\nery, including interesting implementation and historical details. [Bernstein and Good-\nman (1981)] is an early textbook source of information on concurrency control and\nrecovery. [Faerber et al. (2017)] provide an overview of main-memory databases, in-\ncluding recovery techniques.\nAn overview of the recovery scheme of System R is presented by [Gray (1978)]\n(which also includes extensive coverage of concurrency control and other aspects of\nSystem R), and [Gray et al. (1981)]. A comprehensive presentation of the principles\nof recovery is o\ufb00ered by [Haerder and Reuter (1983)]. The ARIES recovery method is\ndescribed in [Mohan et al. (1992)]. Many databases support high-availability features;\nmore details may be found in their online manuals.\nBibliography\n[Bayer et al. (1978)] R. Bayer, R. M. Graham, and G. Seegmuller, editors, Operating Systems:\nAn Advanced Course ,v o l u m e6 0o f Lecture Notes in Computer Science , Springer Verlag (1978).\n[Bernstein and Goodman (1981)] P. A. Bernstein and N. Goodman, \u201cConcurrency Control\nin Distributed Database Systems\u201d, ACM Computing Surveys , Volume 13, Number 2 (1981),\npages 185\u2013221.\n[Faerber et al. (2017)] F. Faerber, A. Kemper, P.-A. Larson, J. Levandoski, T. Neumann, and\nA. Pavlo, \u201cMain Memory Database Systems\u201d, Foundations and Trends in Databases ,V o l u m e\n8, Number 1-2 (2017), pages 1\u2013130.\n[Gray (1978)] J. Gray. \u201cNotes on Data Base Operating System\u201d, In [Bayer et al. (1978)] ,\npages 393\u2013481. Springer Verlag (1978).\n", "985": "Further Reading 957\n[Gray and Reuter (1993)] J. Gray and A. Reuter, Transaction Processing: Concepts and Tech-\nniques , Morgan Kaufmann (1993).\n[Gray et al. (1981)] J. Gray, P. R. McJones, and M. Blasgen, \u201cThe Recovery Manager of the\nSystem R Database Manager\u201d, ACM Computing Surveys , Volume 13, Number 2 (1981), pages\n223\u2013242.\n[Haerder and Reuter (1983)] T. Haerder and A. Reuter, \u201cPrinciples of Transaction-Oriented\nDatabase Recovery\u201d, ACM Computing Surveys , Volume 15, Number 4 (1983), pages 287\u2013318.\n[Mohan et al. (1992)] C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz,\n\u201cARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial\nRollbacks Using Write-Ahead Logging\u201d, ACM Transactions on Database Systems ,V o l u m e1 7 ,\nNumber 1 (1992), pages 94\u2013162.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "986": "", "987": "PART8\nPARALLEL AND\nDISTRIBUTED DATABASES\nDatabase systems can be centralized, where one server machine executes operations\non the database. Database systems can also be designed to exploit parallel computer\narchitectures. Distributed databases span multiple geographically separated machines.\nChapter 20 \ufb01rst outlines the architectures of database systems running on server\nsystems, which are used in centralized and client\u2013server architectures. The chapter then\noutlines parallel computer architectures, and parallel database architectures designed\nfor di\ufb00erent types of parallel computers. Finally, the chapter outlines architectural is-\nsues in building a distributed database system.\nChapter 21 discusses techniques for data storage and indexing in parallel and dis-\ntributed database systems. These include data partitioning and replication. Key-value\nstores, which o\ufb00er some but not all features of a full database system, are discussed\nalong with their bene\ufb01ts and drawbacks.\nChapter 22 discusses algorithms for query processing in parallel and distributed\ndatabase systems. This chapter focuses on query processing in decision-support sys-\ntems. Such systems need to execute queries on very large amounts of data, and parallel\nprocessing of the query across multiple node s is critical for processing queries within\nacceptable response times. The chapter covers parallel sort and join, pipelining, the\nimplementation of MapReduce systems, and parallel stream processing.\nChapter 23 discusses how to carry out transaction processing in parallel and dis-\ntributed database systems. In addition to supporting concurrency control and recovery,\nthe system must deal with issues pertaining to replication of data and with failures that\ninvolve some, but not all, nodes. The chapter covers atomic commit protocols and con-\nsensus protocols designed for distributed da tabases, distributed concurrency control,\nreplica consistency, and trade-o\ufb00s of consistency for the sake of performance and avail-\nability.\n959\n", "988": "", "989": "CHAPTER20\nDatabase-System Architectures\nThe architecture of a database system is greatly in\ufb02uenced by the underlying computer\nsystem on which it runs, in particular by such aspects as processor and memory archi-\ntecture, and networking, as well as by requirements of parallelism and distribution. In\nthis chapter, we provide a high-level view of database architectures, with a focus on how\nthey are in\ufb02uenced by the underlying hardware, as well as by requirements of parallel\nand distributed processing.\n20.1 Overview\nThe earliest databases were built to run on a single physical machine supporting multi-\ntasking; such centralized database systems are still widely used. An enterprise-scale ap-\nplication that runs on a centralized database system today may have from tens to thou-\nsands of users and database sizes ranging from megabytes to hundreds of gigabytes.\nParallel database systems were developed, starting in the late 1980s to execute tasks\nin parallel on a large number of machines. These were developed to handle high-end\nenterprise applications whose requirements in terms of transaction processing perfor-\nmance, time to process decision support queries, and storage capacity could not be met\nby centralized databases. These databases were designed to run in parallel on hundreds\nof machines. Today, the growth of parallel databases is driven not just by enterprise ap-\nplications, but even more so by web-scale applications, which may have millions to even\nhundreds of millions of users and may need to deal with many petabytes of data.\nParallel data storage systems are designed primarily to store and retrieve data based\non keys. Unlike parallel databases, data storage systems typically provide very limited\nsupport for transactions, and they lack support for declarative querying. On the other\nhand, such systems can be run in parallel on very large numbers of machines (thou-\nsands to tens of thousands), a scale that most parallel databases cannot handle.\nFurther, data are often generated and stored on di\ufb00erent database systems, and\nthere is a need to execute queries and update transactions across multiple databases.\nThis need led to the development of distributed database systems . Techniques developed\nfor fault tolerance in the context of distributed databases today also play a key role in\n961\n", "990": "962 Chapter 20 Database-System Architectures\nensuring the extremely high reliability and availability of massively parallel database\nand data storage systems.\nWe study the architecture of database systems in this chapter, starting with the\ntraditional centralized architectures and covering parallel and distributed database ar-\nchitectures. We cover issues of parallel and distributed data storage and indexing in\nChapter 21. Chapter 22 covers issues of parallel and distributed query processing, while\nChapter 23 covers issues of parallel and distributed transaction processing.\n20.2 Centralized Database Systems\nCentralized database systems are those that run on a single computer system. Such\ndatabase systems span a range from single-user database systems running on mobile\ndevices or personal computers to high-performance database systems running on a\nserver with multiple CPU cores and disks and a large amount of main memory that can\nbe accessed by any of the CPU cores. Centralized database systems are widely used for\nenterprise-scale applications.\nWe distinguish two ways in which computers are used: as single-user systems and\nas multiuser systems. Smartphones and personal computers fall into the \ufb01rst category.\nAt y p i c a l single-user system is a system used by a single person, usually with only one\nprocessor (usually with multiple cores), and one or two disks.1At y p i c a l multiuser\nsystem , on the other hand, has multiple disks, a large amount of memory, and multiple\nprocessors. Such systems serve a large number of users who are connected to the system\nremotely, and they are called server systems .\nDatabase systems designed for single-user systems usually do not provide many of\nthe facilities that a multiuser database provides. In particular, they may support very\nsimple concurrency control schemes, since highly concurrent access to the database\nis very unlikely. Provisions for crash recovery in such systems may also be either very\nbasic (e.g., making a copy of data before updating it), or even absent in some cases.\nSuch systems may not support SQL a n dm a yi n s t e a dp r o v i d ea n APIfor data access.\nSuch database systems are referred to as embedded databases , since they are usually\ndesigned to be linked to a single application program and are accessible only from that\napplication.\nIn contrast, multiuser database systems support the full transactional features that\nwe have studied earlier. Such databases are usually designed as servers ,w h i c hs e r v i c e\nrequests received from application programs; the requests could be in the form of SQL\nqueries, or they could be requests for retrieving, storing, or updating data speci\ufb01ed\nusing an API.\nMost general-purpose computer systems in use today have a few multicore proces-\nsors (typically one to four), with each multi core processor having a few cores (typically\n1Recall that we use the term diskto refer to hard disks as well as solid-state drives.\n", "991": "20.3 Server System Architectures 963\n4 to 8). Main memory is shared across all the processors. Parallelism with such a small\nnumber of cores, and with shared memory, is referred to as coarse-grained parallelism .\nOperating systems that run on single-processor systems support multitasking, al-\nlowing multiple processes to run on the same processor in a time-shared manner. Ac-\ntions of di\ufb00erent processes may thus be interleaved. Databases designed for single-\nprocessor machines have long been designed to allow multiple processes or threads\nto access shared database structures concurrently. Thus, many of the issues in han-\ndling multiple processes running truly in parallel, such as concurrent access to data,\nare already addressed by databases designed for single-processor machines. As a result,\ndatabase systems designed for time-shared single-processor machines could be adapted\nrelatively easily to run on coarse-grained parallel systems.\nDatabases running on coarse-grained parallel machines traditionally did not at-\ntempt to partition a single query among the processors; instead, they ran each query\non a single processor, allowing multiple queries to run concurrently. Thus, such sys-\ntems support a higher throughput; that is, they allow a greater number of transactions\nto run per second, although individual transactions do not run any faster. In recent\nyears, with even mobile phones supporting multiple cores, such systems are evolving\nto support parallel processing of individual queries.\nIn contrast, machines with \ufb01ne-grained parallelism have a large number of proces-\nsors, and database systems running on such machines attempt to parallelize single tasks\n(queries, for example) submitted by users.\nParallelism has emerged as a critical issue in the design of software systems in gen-\neral, and in particular in the design of database systems. As a result, parallel database\nsystems, which once were specialized systems running on specially designed hardware,\nare increasingly becoming the norm. We study the architecture of parallel database\nsystems in Section 20.4.\n20.3 Server System Architectures\nServer systems can be broadly categorized as transaction servers and data servers.\n\u2022Transaction-server systems, also called query-server systems, provide an interface\nto which clients can send requests to perform an action, in response to which they\nexecute the action and send back results to the client. Usually, client machines ship\ntransactions to the server systems, where those transactions are executed, and re-\nsults are shipped back to clients that are in charge of displaying the data. Requests\nmay be speci\ufb01ed through the use of SQL or through a specialized application pro-\ngram interface.\n\u2022Data-server systems allow clients to interact with the servers by making requests\nto read or update data, in units such as \ufb01les, pages, or objects. For example, \ufb01le\nservers provide a \ufb01le-system interface where clients can create, update, read, and\n", "992": "964 Chapter 20 Database-System Architectures\ndelete \ufb01les. Data servers for database systems o\ufb00er much more functionality; they\nsupport units of data\u2014such as pages, tuples, or objects\u2014that are smaller than a \ufb01le.\nThey provide indexing facilities for data, and they provide transaction facilities so\nthat the data are never left in an inconsistent state if a client machine or process\nfails.\nOf these, the transaction-server architecture is by far the more widely used architecture,\nalthough parallel data servers are widely used to handle tra\ufb03c at web scale. We shall\nelaborate on the transaction-server and data-server architectures in Section 20.3.1 and\nSection 20.3.2.\n20.3.1 Transaction-Server Architecture\nA typical transaction-server system today consists of multiple processes accessing data\nin shared memory, as in Figure 20.1. The processes that form part of the database\nsystem include:\nlock \nmanager\nprocesslock table log bu\ufb00ershared\nmemory \ndatabase\n writer\nprocesslog writer\nprocesscheckpoint\nprocessprocess\nmonitor\nprocessserver\nprocessserver\nprocessuser\nprocessuser\nprocess\nserver\nprocessuser\nprocess\nODBC JDBC\nlog disks data disksquery plan cachebu\ufb00er pool\nFigure 20.1 Shared memory and process structure.\n", "993": "20.3 Server System Architectures 965\n\u2022Server processes : These are processes that receive user queries (transactions), exe-\ncute them, and send the results back. The queries may be submitted to the server\nprocesses from a user interface, or from a user process running embedded SQL,o r\nviaJDBC ,ODBC , or similar protocols. Some database systems use a separate pro-\ncess for each user session, and a few use a single database process for all user ses-\nsions, but with multiple threads so that multiple queries can execute concurrently.\n(Athread is similar to a process, but multiple threads execute as part of the same\nprocess, and all threads within a process run in the same virtual-memory space.\nMultiple threads within a process can execute concurrently.) Many database sys-\ntems use a hybrid architecture, with multiple processes, each one running multiple\nthreads.\n\u2022Lock manager process : This process implements lock manager functionality, which\nincludes lock grant, lock release, and deadlock detection.\n\u2022Database writer process : There are one or more processes that output modi\ufb01ed\nbu\ufb00er blocks back to disk on a continuous basis.\n\u2022Log writer process : This process outputs log records from the log record bu\ufb00er to\nstable storage. Server processes simply add log records to the log record bu\ufb00er in\nshared memory, and if a log force is required, they request the log writer process\nto output log records (recall that a log force causes the log contents in memory to\nbe output to stable storage).\n\u2022Checkpoint process : This process performs periodic checkpoints.\n\u2022Process monitor process : This process monitors other processes, and if any of them\nfails, it takes recovery actions for the process, such as aborting any transaction\nbeing executed by the failed process and then restarting the process.\nThe shared memory contains all shared data, such as:\n\u2022Bu\ufb00er pool.\n\u2022Lock table.\n\u2022Log bu\ufb00er, containing log records waiting to be output to the log on stable storage.\n\u2022Cached query plans, which can be reused if the same query is submitted again.\nAll database processes can access the data in shared memory. Since multiple processes\nmay read or perform updates on data structures in shared memory, there must be a\nmechanism to ensure mutual exclusion , that is, to ensure that a data structure is modi-\n\ufb01ed by at most one process at a time, and no process is reading a data structure while\nit is being written by other processes.\nSuch mutual exclusion can be implemented by means of operating system func-\ntions called semaphores. Alternative implementations, with less overhead, use one of\n", "994": "966 Chapter 20 Database-System Architectures\nNote 20.1 ATOMIC INSTRUCTIONS\n1.The instruction test-and-set (M) performs the following two actions atomi-\ncally: (i) test, that is, read the value of memory location M, and then (ii) set\nit to 1; the test-and-set instruction returns the value that it read in step (i).\nSuppose a memory location Mrepresenting an exclusive lock is initially\nset to 0. A process that wishes to get the lock executes the test-and-set ( M).\nIf it is the only process executing the instruction on M, the value that is read\nand returned would be 0, indicating to the process that it has acquired the\nlock, and Mwould be set to 1. When the process is done using the lock, it\nreleases the lock by setting Mback to 0.\nIf a second process executes test-and-set ( M) before the lock is released,\nthe value returned would be 1, indicating that some other process already\nhas the lock. The process could repeat the execution of test-and-set on M\nperiodically, until it gets a return value of 0, indicating that it has acquired\nthe lock after it was released by another process.\nNow, if two processes execute test-and-set ( M) concurrently, one of them\nwould see a return value of 0, while the other would see a return value of 1;\nthis is because the read operation and the set operation are executed together,\natomically. The \ufb01rst process to read the value would also set it to 1, and the\nsecond process would \ufb01nd that Mis already set to 1. Thus, only one process\nacquires the lock, ensuring mutual execution.\n2.The compare-and-swap instruction is another atomic instruction similar to\nthe test-and-set instruction, but it takes the following operands: ( M,Vo,Vn),\nwhere Mis a memory location, and value Voand Vnare two values (referred\nto as the old and new values). The instruction does the following atomically:\nit compares the value at Mwith Vo, and if it matches, it updates the value\ntoVnand returns success. If the values do not match, it does not update M,\nand it returns failure.\nSimilar to the case of test-and-set, we have a memory location Mrepre-\nsenting a lock, which is initially set to 0. A process that wants to acquire\nthe lock executes compare-and-swap ( M,0 ,id)w h e r e idcan be any nonzero\nvalue and is typically the process identi\ufb01er. If no process has the lock, the\ncompare-and-swap operation returns success, after storing the process iden-\nti\ufb01er in M; otherwise, the operation returns failure.\nA bene\ufb01t of compare-and-swap over the test-and-set implementation is\nthat it is easy to \ufb01nd out which process has acquired the lock by just reading\nthe content of M, if the process identi\ufb01er is used as Vn.\n", "995": "20.3 Server System Architectures 967\ntheatomic instructions , test-and-set, or compare-and-swap, which are supported by the\ncomputer hardware. See Note 20.1 on page 966 for details of these instructions. All\nmultiprocessor systems today support either the test-and-set or the compare-and-swap\natomic instructions. Further details on these instructions may be found in operating\nsystems textbooks.\nNote that the atomic instructions can be us ed for mutual exclusion, which is equiv-\nalent to supporting exclusive locks, but they do not directly support shared locks.\nThus, they cannot be used directly to implement general-purpose locking in databases.\nAtomic instructions are, however, used to implement short-duration locks, also known\nas latches, which are used for mutual exclusion in databases.\nTo avoid the overhead of message passing, in many database systems, server pro-\ncesses implement locking by directly updating the lock table (which is in shared mem-\nory) instead of sending lock request messages to a lock manager process. (The lock\ntable is shown in Figure 18.10.)\nSince multiple server processes may access the lock table in shared memory con-\ncurrently, processes must ensure mutual exclusion on access to the lock table. This is\ntypically done by acquiring a mutex (also referred to as a latch) on the lock table, using\nthe test-and-set or compare-and-swap instructions on a memory location representing\nal o c ko nt h el o c kt a b l e .\nA transaction that wants to acquire a lock by directly updating the lock table in\nshared memory executes the following steps.\n1.Acquire a mutex (latch) on the lock table.\n2.Check if the requested lock can be allocated, using the procedure we saw in Sec-\ntion 18.1.4. If it can, update the lock table to indicate the lock is allocated. Oth-\nerwise, update the lock table to indicate that the lock request is in the queue for\nthat lock.\n3.Release the mutex on the lock table.\nIf a lock cannot be obtained immediately because of a lock con\ufb02ict, the transaction\nmay periodically read the lock table to check if the lock has been allocated to it due to\na lock release, which is described next.\nLock release is done as follows:\n1.Acquire a mutex on the lock table\n2.Remove the entry in the lock table for the lock being released.\n3.If there are any other lock requests pending for the data item that can now be\nallocated to the lock, the lock table is updated to mark those requests as allocated.\nThe rules on which lock requests may be granted are as described in Section\n18.1.4.\n4.Release the mutex on the lock table.\n", "996": "968 Chapter 20 Database-System Architectures\nTo avoid repeated checks on the lock table (an example of the phenomenon of\nbusy waiting ), operating system semaphores can be used by the lock request code to\nwait for a lock grant noti\ufb01cation. The lock release code must then use the semaphore\nmechanism to notify waiting transactions that their locks have been granted.\nEven if the system handles lock requests through shared memory, it still uses the\nl o c km a n a g e rp r o c e s sf o rd e a d l o c kd e t e c t i o n .\n20.3.2 Data Servers and Data Storage Systems\nData-server systems were originally developed to support data access from object-\noriented databases; object-oriented databases allow programmers to use a program-\nming language that allows creation, retrieval, and update of persistent objects.\nMany of the target applications of object-oriented databases, such as computer-\naided design (CAD) systems, required extensive computation on the retrieved data.\nFor example, the CAD system may store a model of a computer chip or a building, and\nit may perform computations such as simulations on the retrieved model, which may\nbe expensive in terms of CPU time.\nIf all the computation were done at the server, the server would be overloaded.\nInstead, in such an environment, it makes sense to store data on a separate data server\nmachine, fetch data to client machines when needed, perform all processing at the\nclient machines, and then to store new or updated data back on the data server ma-\nchine. Thus, the processing power of client machines can be used to carry out the\ncomputation, while the server needs only to store and fetch data, without performing\nany computation.\nMore recently, a number of parallel data storage systems have been developed for\nhandling very high volumes of data and transactions. Such systems do not necessarily\nsupport SQL,b u ti n s t e a dp r o v i d e APIs for storing, retrieving, and updating data items.\nData items stored in such systems they could be tuples, or could be objects represented\nin formats such as JSON orXML , or they could even be \ufb01les or documents.\nWe use the term data item to refer to tuples, objects, \ufb01les, and documents. We also\nuse the terms data server and data storage system interchangeably.\nData servers support communication of entire data items; in the case of very large\ndata items, they may also support communication of only speci\ufb01ed parts of the data\nitem, for instance, speci\ufb01ed blocks, instead of requiring that the entire data item be\nfetched or stored.\nData servers in earlier generations of storage systems supported a concept called\npage shipping , where the unit of communication is a database page that may potentially\ncontain multiple data items. Page shipping is not used today, since storage systems do\nnot expose the underlying storage layout to clients.\n20.3.3 Caching at Clients\nThe time cost of communication between a client application and a server (whether\na transaction server, or a data server) is high compared to that of a local memory\n", "997": "20.3 Server System Architectures 969\nreference (milliseconds, versus less than 100 nanoseconds). The time to send a message\no v e ran e t w o r k ,a n dg e tar e s p o n s eb a c k ,c a l l e dt h e network round-trip time ,o r network\nlatency , can be nearly a millisecond even if the data server is in the same location as\nthe client.\nAs a result, applications running at the clients adopt several optimization strategies\nto reduce the e\ufb00ects of network latency. The same strategies can also be useful in par-\nallel database systems, where some of the data required for processing a query may be\nstored on a di\ufb00erent machine from where it is consumed. The optimization strategies\ninclude the following:\n\u2022Prefetching . If the unit of communication is a single small item, the overhead of\nmessage passing is high compared to the amount of data transmitted. In particu-\nlar, network latency can cause signi\ufb01cant delays if a transaction makes repeated\nrequests for data items across a network.\nThus, when an item is requested, it may make sense to also send other items\nthat are likely to be used in the near future. Fetching items even before they are\nrequested is called prefetching .\n\u2022Data caching . Data that are shipped to a client on behalf of a transaction can be\ncached at the client within the scope of a single transaction. Data can be cached\neven after the transaction completes, allowing successive transactions at the same\nclient to make use of the cached data.\nHowever, cache coherency is an issue: Even if a transaction \ufb01nds cached data, it\nmust make sure that those data are up to date, since they may have been updated,\nor even deleted, by a di\ufb00erent client after they were cached. Thus, a message must\nstill be exchanged with the server to check validity of the data and to acquire a\nlock on the data, unless the application is willing to live with potentially stale data.\nFurther, new tuples may have been inserted after a transaction caches data, which\nmay not be in the cache. The transaction may have to contact the server to \ufb01nd\nsuch tuples.\n\u2022Lock caching . If the usage of data is mostly partitioned among the clients, with\nclients rarely requesting data that are also requested by other clients, locks can\nalso be cached at the client machine. Suppose that a client \ufb01nds a data item in\nthe cache, and that it also \ufb01nds the lock required for an access to the data item\nin the cache. Then, the access can proceed without any communication with the\nserver. However, the server must keep track of cached locks; if a client requests\na lock from the server, the server must call back all con\ufb02icting locks on the data\nitem from any other client machines that have cached the locks. The task becomes\nmore complicated when machine failures are taken into account.\n\u2022Adaptive lock granularity . If a transaction requires locks on multiple data items,\ndiscovered in the course of a transaction, and each lock acquisition requires a\nround trip to a data server, the transaction may waste a good deal of time on\n", "998": "970 Chapter 20 Database-System Architectures\njust lock acquisition. In such a situation, multi-granularity locking can be used to\na v o i dm u l t i p l er e q u e s t s .F o re x a m p l e ,i fm u l t i p l ed a t ai t e m sa r es t o r e di nap a g e ,a\nsingle page lock (which is at a coarser granularity) can avoid the need to acquire\nmultiple data-item locks (which are at a \ufb01ner granularity). This strategy works well\nif there is very little lock contention, but with higher contention, acquiring a coarse\ngranularity lock can a\ufb00ect concurrency signi\ufb01cantly.\nLock de-escalation , is a way of adaptively decreasing the lock granularity if there\nis higher contention. Lock de-escalation is initiated by the data server sending a\nrequest to the client to de-escalate a lock, and the client responds by acquiring\n\ufb01ner-granularity locks and then releasing the coarser-granularity lock.\nWhen switching to a \ufb01ner granularity, if some of the locks were for cached data\nitems that are not currently locked by any transaction at a client, the data item can\nbe removed from the cache instead of acquiring a \ufb01ner-granularity lock on it.\n20.4 Parallel Systems\nParallel systems improve processing and I/Ospeeds by using a large number of com-\nputers in parallel. Parallel machines are becoming increasingly common, making the\nstudy of parallel database systems correspondingly more important.\nIn parallel processing, many operations are performed simultaneously, as opposed\nto serial processing, in which the computati onal steps are performed sequentially. A\ncoarse-grain parallel machine consists of a small number of powerful processors; a mas-\nsively parallel or\ufb01ne-grain parallel machine uses thousands of smaller processors. Vir-\ntually all high-end server machines today o\ufb00er some degree of coarse-grain parallelism,\nwith up to two or four processors each of which may have 20 to 40 cores.\nMassively parallel computers can be distinguished from the coarse-grain parallel\nmachines by the much larger degree of parallelism that they support. It is not practical\nto share memory between a large number of processors. As a result, massively parallel\ncomputers are typically built using a large number of computers, each of which has its\nown memory, and often, its own set of disks. Each such computer is referred to as a\nnode in the system.\nParallel systems at the scale of hundreds to thousands of nodes or more are housed\nin a data center , which is a facility that houses a large number of servers. Data centers\nprovide high-speed network connectivity within the data center, as well as to the outside\nworld. The numbers and sizes of data centers have grown tremendously in the last\ndecade, and modern data centers may have several hundred thousand servers.\n20.4.1 Motivation for Parallel Databases\nThe transaction requirements of organizations have grown with the increasing use of\ncomputers. Moreover, the growth of the World Wide Web has created many sites with\nmillions of viewers, and the increasing amounts of data collected from these viewers\nhas produced extremely large databases at many companies.\n", "999": "20.4 Parallel Systems 971\nThe driving force behind parallel database systems is the demands of applications\nthat have to query extremely large databases (of the order of petabytes\u2014that is, 1000\nterabytes, or equivalently, 1015bytes) or that have to process an extremely large number\nof transactions per second (of the order of thousands of transactions per second).\nCentralized and client\u2013server database systems are not powerful enough to handle\nsuch applications.\nWeb-scale applications today often require hundreds to thousands of nodes (and\nin some cases, tens of thousands of nodes) to handle the vast number of users on the\nweb.\nOrganizations are using these increasingly large volumes of data\u2014such as data\nabout what items people buy, what web links users click on, and when people make\ntelephone calls\u2014to plan their activities and pricing. Queries used for such purposes\nare called decision-support queries , and the data requirements for such queries may run\ninto terabytes. Single-node systems are not capable of handling such large volumes of\ndata at the required rates.\nThe set-oriented nature of database queries naturally lends itself to parallelization.\nA number of commercial and research systems have demonstrated the power and scal-\nability of parallel query processing.\nAs the cost of computing systems has reduced signi\ufb01cantly over the years, parallel\nmachines have become common and relatively inexpensive. Individual computers have\nthemselves become parallel machines using multicore architectures. Parallel databases\nare thus quite a\ufb00ordable even for small organizations.\nParallel database systems which can support hundreds of nodes have been avail-\nable commercially for several decades, but the number of such products has seen a\nsigni\ufb01cant increase since the mid 2000s. Open-source platforms for parallel data stor-\nage such as the Hadoop File System (HDFS), and HBase, and for query processing,\nsuch as Hadoop Map-Reduce and Hive (among many others), have also seen extensive\nadoption.\nIt is worth noting that application programs are typically built such that they can\nbe executed in parallel on a number of application servers, which communicate over\na network with a database server, which may itself be a parallel system. The parallel\narchitectures described in this section can be used not only for data storage and query\nprocessing in the database but also for parallel processing of application programs.\n20.4.2 Measures of Performance for Parallel Systems\nThere are two main measures of performance of a database system: (1) throughput ,t h e\nnumber of tasks that can be completed in a given time interval, and (2) response time ,\nthe amount of time it takes to complete a single task from the time it is submitted. A\nsystem that processes a large number of small transactions can improve throughput by\nprocessing many transactions in parallel. A system that processes large transactions\ncan improve response time as well as throughput by performing subtasks of each trans-\naction in parallel.\n", "1000": "972 Chapter 20 Database-System Architectures\nlinear speedup\nsublinear speedup\nresourcesspeed\nFigure 20.2 Speedup with increasing resources.\nParallel processing within a computer system allows database-system activities to\nbe speeded up, allowing faster response to transactions, as well as more transactions to\nbe executed per second. Queries can be processed in a way that exploits the parallelism\no\ufb00ered by the underlying computer system.\nTwo important issues in studying parallelism are speedup and scaleup. Running a\ngiven task in less time by increasing the degree of parallelism is called speedup .H a n -\ndling larger tasks by increasing the degree of parallelism is called scaleup .\nConsider a database application running on a parallel system with a certain num-\nber of processors and disks. Now suppose that we increase the size of the system by\nincreasing the number of processors, disks, and other components of the system. The\ngoal is to process the task in time inversely proportional to the number of processors\nand disks allocated. Suppose that the execution time of a task on the larger machine\nisTL, and that the execution time of the same task on the smaller machine is TS.T h e\nspeedup due to parallelism is de\ufb01ned as TS\u2215TL. The parallel system is said to demon-\nstrate linear speedup if the speedup is Nwhen the larger system has Ntimes the re-\nsources (processors, disk, and so on) of the smaller system. If the speedup is less than\nN, the system is said to demonstrate sublinear speedup . Figure 20.2 illustrates linear\nand sublinear speedup.2\nScaleup relates to the ability to process larger tasks in the same amount of time\nby providing more resources. Let Qbe a task, and let QNbe a task that is Ntimes\nbigger than Q. Suppose that the execution time of task Qon a given machine MSis\nTS, and the execution time of task QNon a parallel machine MLthat is Ntimes larger\nthan MSisTL. The scaleup is then de\ufb01ned as TS\u2215TL. The parallel system MLis said\nto demonstrate linear scaleup on task QifTL=TS.I fTL>TS, the system is said\n2In some cases, a parallel system may provide superlinear speedup, that is, an Ntimes larger system may provide\nspeedup greater than N. This could happen, for example, because data that did not \ufb01t in the main memory of a smaller\nsystem do \ufb01t in the main memory of a larger system, avoiding disk I/O. Similarly, data may \ufb01t in the cache of a larger\nsystem, reducing memory accesses compared to a smalle r system, which could lead to superlinear speedup.\n", "1001": "20.4 Parallel Systems 973\nlinear scaleup\nsublinear scaleup\nproblem sizeTS\nTL\nFigure 20.3 Scaleup with increasing problem size and resources.\nto demonstrate sublinear scaleup . Figure 20.3 illustrates linear and sublinear scaleups\n(where the resources increase in proportion to problem size). There are two kinds of\nscaleup that are relevant in parallel database systems, depending on how the size of the\ntask is measured:\n\u2022Inbatch scaleup , the size of the database increases, and the tasks are large jobs\nwhose runtime depends on the size of the database. An example of such a task is\na scan of a relation whose size is proportional to the size of the database. Thus,\nt h es i z eo ft h ed a t a b a s ei st h em e a s u r eo ft h es i z eo ft h ep r o b l e m .B a t c hs c a l e u p\nalso applies in scienti\ufb01c applications, such as executing a weather simulation at\nanN-times \ufb01ner resolution,3or performing the simulation for an N-times longer\nperiod of time.\n\u2022Intransaction scaleup , the rate at which transactions are submitted to the database\nincreases, and the size of the database increases proportionally to the transaction\nrate. This kind of scaleup is what is relevant in transaction-processing systems\nwhere the transactions are small updates\u2014for example, a deposit or withdrawal\nfrom an account\u2014and transaction rates grow as more accounts are created. Such\ntransaction processing is especially well adapted for parallel execution, since trans-\nactions can run concurrently and independently on separate nodes, and each trans-\naction takes roughly the same amount of time, even if the database grows.\nScaleup is usually the more important metric for measuring the e\ufb03ciency of par-\nallel database systems. The goal of parallelism in database systems is usually to make\nsure that the database system can continue to perform at an acceptable speed, even as\nthe size of the database and the number of transactions increases. Increasing the ca-\n3For example, a weather simulation that divides the atmos phere in a particular region into cubes of side 200 meters\nmay need to be modi\ufb01ed to use a \ufb01ner resolution, with cube s of side 100 meters; the number of cubes would thus be\nscaled up by a factor of 8.\n", "1002": "974 Chapter 20 Database-System Architectures\npacity of the system by increasing the parallelism provides a smoother path for growth\nfor an enterprise than does replacing a centralized system with a faster machine (even\nassuming that such a machine exists). However, we must also look at absolute perfor-\nmance numbers when using scaleup measures; a machine that scales up linearly may\nperform worse than a machine that scales less than linearly, simply because the latter\nmachine is much faster to start o\ufb00 with.\nA number of factors work against e\ufb03cient parallel operation and can diminish both\nspeedup and scaleup.\n\u2022Sequential computation . Many tasks have some components that can bene\ufb01t from\nparallel processing, and some components that have to be executed sequentially.\nConsider a task that takes time Tto run sequentially. Suppose the fraction of the\ntotal execution time that can bene\ufb01t from parallelization is p,a n dt h a tp a r ti se x e -\ncuted by nnodes in parallel. Then the total time taken would be (1 \u2212p)T+(p\u2215n)T,\nand the speedup would be1\n(1\u2212p)+(p\u2215n). (This formula is referred to as Amdahl\u2019s law .)\nIf the fraction pis, say9\n10, then the maximum speedup possible, even with very large\nn,w o u l db e1 0 .\nNow consider scaleup, where the problem size increases. If the time taken\nby the sequential part of a task increases along with the problem size, scaleup\nwill be similarly limited. Suppose fraction pof the execution time of a problem\nbene\ufb01ts from increasing resources, while fraction (1 \u2212p) is sequential and does not\nbene\ufb01t from increasing resources. Then the scaleup with ntimes more resources\non a problem that is ntimes larger will be1\nn(1\u2212p)+p. (This formula is referred to\nasGustafson\u2019s law .) However, if the time taken by the sequential part does not\nincrease with problem size, its impact on scaleup will be less as the problem sizes.\nStart-up costs . There is a start-up cost associated with initiating a single process.\nIn a parallel operation consisting of thousands of processes, the start-up time may\novershadow the actual processing time, a\ufb00ecting speedup adversely.\n\u2022Interference . Since processes executing in a parallel system often access shared\nresources, a slowdown may result from the interference of each new process as it\ncompetes with existing processes for commonly held resources, such as a system\nbus, or shared disks, or even locks. Both speedup and scaleup are a\ufb00ected by this\nphenomenon.\n\u2022Skew . By breaking down a single task into a number of parallel steps, we reduce the\nsize of the average step. Nonetheless, the service time for the single slowest step\nwill determine the service time for the task as a whole. It is often di\ufb03cult to divide\na task into exactly equal-sized parts, and the way that the sizes are distributed is\ntherefore skewed . For example, if a task of size 100 is divided into 10 parts, and the\ndivision is skewed, there may be some tasks of size less than 10 and some tasks of\nsize more than 10; if even one task happens to be of size 20, the speedup obtained\nby running the tasks in parallel is only 5, instead of 10 as we would have hoped.\n", "1003": "20.4 Parallel Systems 975\n. . .\n. . .\n. . .core\nswitches\nswitchesaggregation\nswitchestop-of-rack(a) bus\n(c) mesh (d) hypercube\n(e) tree-like topology(b) ring\nFigure 20.4 Interconnection networks.\n20.4.3 Interconnection Networks\nParallel systems consist of a set of components (processors, memory, and disks) that\ncan communicate with each other via an interconnection network . Figure 20.4 shows\nseveral commonly used types of interconnection networks:\n\u2022Bus. All the system components can send data on and receive data from a single\ncommunication bus. This type of interconnection is shown in Figure 20.4a. Bus\ninterconnects were used in earlier days to connect multiple nodes in a network,\nbut they are no longer used for this task. However, bus interconnections are still\nused for connecting multiple CPUs and memory units within a single node, and\nthey work well for small numbers of processors. However, they do not scale well\n", "1004": "976 Chapter 20 Database-System Architectures\nwith increasing parallelism, since the bus can handle communication from only\none component at a time; with increasing numbers of CPUs and memory banks in\na node, other interconnection mechanisms such as ring or mesh interconnections\nare now used even within a single node.\n\u2022Ring. The components are nodes arranged in a ring (circle), and each node is\nconnected to its two adjacent nodes in the ring, as shown in Figure 20.4b. Unlike\na bus, each link can transmit data concurrently with other links in the ring, leading\nto better scalability. However, to transmit data from one node to another node on\nthe ring may require a large number of hops; speci\ufb01cally, up to n\u22152h o p sm a y\nbe needed on a ring with nnodes, assuming communication can be done in either\ndirection on the ring. Furthermore, the transmission delay increases if the number\nof nodes in the ring is increased.\n\u2022Mesh . The components are nodes in a grid, and each component connects to all its\nadjacent components in the grid. In a two-dimensional mesh, each node connects\nto (up to) four adjacent nodes, while in a three-dimensional mesh, each node con-\nnects to (up to) six adjacent nodes. Figure 20.4c shows a two-dimensional mesh.\nNodes that are not directly connected can communicate with one another by rout-\ning messages via a sequence of intermediate nodes that are directly connected to\none another. The number of communication links grows as the number of com-\nponents grows, and the communication capacity of a mesh therefore scales better\nwith increasing parallelism.\nMesh interconnects are used to connect multiple cores in a processor, or pro-\ncessors in a single server, to each other; each processor core has direct access to\na bank of memory connected to the processor core, but the system transparently\nfetches data from other memory banks by sending messages over the mesh inter-\nconnects.\nHowever, mesh interconnects are no longer used for interconnecting nodes,\nsince the number of hops required to transmit data increases signi\ufb01cantly with the\nnumber of nodes (the number of hops required to transmit data from one node to\nanother node in a mesh is proportional in the worst case to the square root of the\nnumber of nodes). Parallel systems today have very large numbers of nodes, and\nmesh interconnects would thus be impractically slow.\n\u2022Hypercube . The components are numbered in binary, and a component is con-\nnected to another if the binary representations of their numbers di\ufb00er in exactly\none bit. Thus, each of the ncomponents is connected to log( n) other components.\nFigure 20.4d shows a hypercube with eight nodes. In a hypercube interconnection,\na message from a component can reach any other component by going through\nat most log( n) links. In contrast, in a mesh architecture a component may be\n2(\u221a\nn\u22121) links away from some of the other components (or\u221a\nnlinks away,\nif the mesh interconnection wraps around at the edges of the grid). Thus commu-\nnication delays in a hypercube are signi\ufb01cantly lower than in a mesh.\n", "1005": "20.4 Parallel Systems 977\nHypercubes have been used to interconnect nodes in massively parallel com-\nputers in earlier days, but they are no longer commonly used.\n\u2022Tree-like . Server systems in a data center are typically mounted in racks, with each\nrack holding up to about 40 nodes. Multiple racks are used to build systems with\nlarger numbers of nodes. A key issue is how to interconnect such nodes.\nTo connect nodes within a rack, there is typically a network switch mounted\na tt h et o po ft h er a c k ;4 8p o r ts w i t c h e sa r ec o m m o n l yu s e d ,s oas i n g l es w i t c hc a n\nbe used to connect all the servers in a rack. Current-generation network switches\ntypically support a bandwidth of 1 to 10 gigabits per second (Gbps) simultaneously\nfrom/to each of the servers connected to the switch, although more expensive\nnetwork interconnects with 40 to 100 Gbps bandwidths are available.\nMultiple top-of-rack switches (also referred to as edge switches )c a ni nt u r nb e\nconnected to another switch, called an aggregation switch , allowing interconnec-\ntion between racks. If there are a large number of racks, the racks may be divided\ninto groups, with one aggregation switch connecting a group of racks, and all the\naggregation switches in turn connected to a core switch . Such an architecture is a\ntree topology with three tiers. The core switch at the top of the tree also provides\nconnectivity to outside networks.\nA problem with this basic tree structure, which is frequently used in local-area\nnetworks within organizations, is that t he available bandwidth between racks is\noften not su\ufb03cient if multiple machines in a rack try to communicate signi\ufb01cant\namounts of data with machines from other racks. Typically, the interconnects of\nthe aggregation switches support higher bandwidths of 10 to 40 Gbps, although\ninterconnects of 100 Gbps are available. Interconnects of even higher capacity can\nbe created by using multiple interconnects in parallel. However, even such high-\nspeed links can be saturated if a large enough number of servers in a rack attempt\nto communicate at their full connection bandwidth to servers in other racks.\nTo avoid the bandwidth bottleneck of a tr ee structure, data centers typically\nconnect each top-of-rack (edge) switch to multiple aggregation switches. Each ag-\ngregation switch in turn is linked to a number of core switches at the next layer.\nSuch an interconnection topology is called a tree-like topology ; Figure 20.4e shows\na tree-like topology with three tiers. The tree-like topology is also referred to as a\nfat-tree topology , although originally the fat-tree topology referred to a tree topol-\nogy where edges higher in the tree have a higher bandwidth than edges lower in\nthe tree.\nThe bene\ufb01t of the tree-like architecture is that each top-of-rack switch can route\nits messages through any of the aggregation switches that it is connected to, in-\ncreasing the inter-rack bandwidth greatly as compared to the tree topology. Simi-\nlarly, each aggregation switch can communicate with another aggregation switch\nvia any of the core switches that it is connected to, increasing the bandwidth avail-\nable between the aggregation switches. Further, even if an aggregation or edge\nswitch fails, there are alternative paths through other switches. With appropriate\n", "1006": "978 Chapter 20 Database-System Architectures\nrouting algorithms, the network can continue functioning even if a switch fails,\nmaking the network fault-tolerant , at least to failures of one or a few switches.\nA tree-like architecture with three tiers can handle a cluster of tens of thou-\nsands of machines. Although a tree-like topology improves the inter-rack band-\nwidth greatly compared to a tree topology, parallel processing applications, in-\ncluding parallel storage and parallel database systems, perform best if they are\ndesigned in a way that reduces inter-rack tra\ufb03c.\nThe tree-like topology and variants of it are widely used in data centers today.\nThe complex interconnection networks in a data center are referred to as a data\ncenter fabric .\nWhile network topologies are very important for scalability, a key to network per-\nformance is network technology used for individual links. The popular technologies\ninclude:\n\u2022Ethernet : The dominant technology for network connections today is the Ether-\nnet technology. Ethernet standards have evolved over time, and the predominant\nversions used today are 1-gigabit Ethernet and 10-gigabit Ethernet, which support\nbandwidths of 1 and 10 gigabits per second respectively. Forty-gigabit Ethernet and\n100-gigabit Ethernet technologies are also available at a higher cost and are seeing\nincreasing usage. Ethernet protocols can be used over cheaper copper cables for\nshort distances, and over optical \ufb01ber for longer distances.\n\u2022Fiber channel : The Fiber Channel Protocol standard was designed for high-speed\ninterconnection between storage systems and computers, and it is predominantly\nused to implement storage area networks (described in Section 20.4.6). The di\ufb00er-\nent versions of the standard have supported increasing bandwidth over the years,\nwith 16 gigabits per second available as of 2011, and 32 and 128 gigabits per second\nsupported from 2016.\n\u2022In\ufb01niband : The In\ufb01niband standard was designed for interconnections with a data\ncenter; it was speci\ufb01cally designed for high-performance computing applications\nwhich need not just very high bandwidth, but also very low latency. The In\ufb01niband\nstandard has evolved, with link speeds of 8-gigabits per second available by 2007\nand 24-gigabits per second available by 2014. Multiple links can be aggregated to\ngive a bandwidth of 120 to 290-gigabits per second.\nThe latency associated with message delivery is as important as bandwidth for\nmany applications. A key bene\ufb01t of In\ufb01niband is that it supports latencies as low\nas 0.7 to 0.5 microseconds. In contrast, Ethernet latencies can be up to hundreds\nof microseconds in an unoptimized local-area network, while latency-optimized\nEthernet implementations still have latencies of several microseconds.\nOne of the important techniques used to reduce latency is to allow applications to\nsend and receive messages by directly interfacing with the hardware, bypassing the oper-\nating system. With the standard implementations of the networking stack, applications\n", "1007": "20.4 Parallel Systems 979\nP\nPM\nP\nP\nP\nM M MP\nP\nP\nP\nPP\nP\nP\nP\nPP\nP\nP\nP\nP(a) shared memory\nP\nP\nP\nP\n(c) shared nothing (d) hierarchicalP M\nP\nP\nP\nP\n(b) shared diskP M\nP M\nP M\nM\nM\nMM P\nM\nM\nFigure 20.5 Parallel database architectures.\nsend messages to the operating system, which in turn interfaces with the hardware,\nwhich in turn delivers the message to the other computer, where again the hardware\ninterfaces with the operating system, which then interfaces with the application to de-\nliver the message. Support for direct access to the network interface, bypassing the\noperating system, reduces the communication latency signi\ufb01cantly.\nAnother approach to reducing latency is to use remote direct memory access\n(RDMA ), a technology which allows a process on one node to directly read or write\nto memory on another node, without explicit message passing. Hardware support en-\nsures that RDMA can transfer data at very high rates with very low latency. RDMA\nimplementations can use In\ufb01niband, Ethernet, or other networking technologies for\nphysical communication between nodes.\n20.4.4 Parallel Database Architectures\nThere are several architectural models for parallel machines. Among the most promi-\nnent ones are those in Figure 20.5 (in the \ufb01gure, M denotes memory, P denotes a\nprocessor, and disks are shown as cylinders):\n\u2022Shared memory . All the processors share a common memory (Figure 20.5a).\n", "1008": "980 Chapter 20 Database-System Architectures\n\u2022Shared disk . A set of nodes that share a common set of disks; each node has its\nown processor and memory (Figure 20.5b). Shared-disk systems are sometimes\ncalled clusters .\n\u2022Shared nothing . A set of nodes that share neither a common memory nor common\ndisk (Figure 20.5c).\n\u2022Hierarchical . This model is a hybrid of the preceding three architectures (Figure\n20.5d). This model is the most widely used model today.\nIn Section 20.4.5 through Section 20.4.8, we elaborate on each of these models.\nNote that the interconnection networks are shown in an abstract manner in Figure\n20.5. Do not interpret the interconnection networks shown in the \ufb01gures as necessarily\nbeing a bus; in fact other interconnection networks are used in practice. For example,\nmesh networks are used within a processor, and tree-like networks are often used to\ninterconnect nodes.\n20.4.5 Shared Memory\nIn a shared-memory architecture, the processors have access to a common memory,\ntypically through an interconnection network. Disks are also shared by the processors.\nThe bene\ufb01t of shared memory is extremely e\ufb03cient communication between processes\n\u2014data in shared memory can be accessed by any process without being moved with\nsoftware. A process can send messages to other processes much faster by using memory\nwrites (which usually take less than a microsecond) than by sending a message through\na communication mechanism.\nMulticore processors with 4 to 8 cores are now common not just in desktop com-\nputers, but even in mobile phones. High-end processing systems such as Intel\u2019s Xeon\nprocessor have up to 28 cores per CPU,w i t hu pt o8 CPUs on a board, while the Xeon\nPhi coprocessor systems contain around 72 cores, as of 2018, and these numbers have\nbeen increasing steadily. The reason for the increasing number of cores is that the sizes\nof features such as logic gates in integrated circuits has been decreasing steadily, allow-\ning more gates to be packed in a single chip. The number of transistors that can be\naccommodated on a given area of silicon has been doubling approximately every 1 1/2\nto 2 years.4\nSince the number of gates required for a processor core has not increased corre-\ns p o n d i n g l y ,i tm a k e ss e n s et oh a v em u l t i p l ep r o c e s s o r so nas i n g l ec h i p .T om a i n t a i na\ndistinction between on-chip multiproce ssors and traditional processors, the term core\nis used for an on-chip processor. Thus, we say that a machine has a multicore processor.\n4Gordon Moore, cofounder of Intel, predicted such an exponential growth in the number of transistors back in the\n1960s; his prediction is popularly known as Moore\u2019s law , even though, technically, it is not a law, but rather an observa-\ntion and a prediction. In earlier decades, processor speeds a lso increased along with the decrease in the feature sizes,\nbut that trend ended in the mid-2000s since processor clock fr equencies beyond a few gigahertz could not be attained\nwithout unreasonable increase in power consumption and heat generation. Moores\u2019s law is sometimes erroneously\ninterpreted to have predicted exponential increases in processor speeds.\n", "1009": "20.4 Parallel Systems 981\nAll the cores on a single processor typically access a shared memory. Further, a\nsystem can have multiple processors which can share memory. Another e\ufb00ect of the\nincreasing number of gates has been the steady increase in the size of main memory as\nwell as a decrease in cost, per-byte, of main memory.\nGiven the availability of multicore processors at a low cost, as well as the concur-\nrent availability of very large amounts of memory at a low cost, shared-memory parallel\nprocessing has become increasingly important in recent years.\n20.4.5.1 Shared-Memory Architectures\nIn earlier generation architectures, processors were connected to memory via a bus,\nwith all processor cores and memory banks sharing a single bus. A downside of shared-\nmemory accessed via a common bus is that the bus or the interconnection network\nbecomes a bottleneck, since it is shared by all processors. Adding more processors\ndoes not help after a point, since the processors will spend most of their time waiting\nfor their turn on the bus to access memory.\nAs a result, modern shared-memory architectures associate memory directly with\nprocessors; each processor has locally connected memory, which can be accessed very\nquickly; however, each processor can also access memory associated with other proces-\nsors; a fast interprocessor communication network ensures that data are fetched with\nrelatively low overhead. Since there is a di\ufb00erence in memory access speed depend-\ning on which part of memory is accessed, such an architecture is often referred to as\nnon-uniform memory architecture (NUMA ).\nFigure 20.6 shows a conceptual architecture of a modern shared-memory system\nwith multiple processors; note that each processor has a bank of memory directly con-\nnected to it, and the processors are linked by a fast interconnect system; processors are\nalso connected to I/Ocontrollers which interface with external storage.\nI/O\nController\nI/O\nControllerCPU CPUCPU CPU Memory\nController Memory\nController Memory\nController Memory\nControllerMemory Memory\nFigure 20.6 Architecture of a modern shared-memory system.\n", "1010": "982 Chapter 20 Database-System Architectures\nBecause shared-memory architectures require specialized high-speed interconnects\nbetween cores and between processors, the number of cores/processors that can be\ninterconnected in a shared-memory system is relatively small. As a result, the scalability\nof shared-memory parallelism is limited to at most a few hundred cores.\nProcessor architectures include cache memory, since access to cache memory is\nmuch faster than access to main memory (cache can be accessed in a few nanoseconds\ncompared to nearly a hundred nanoseconds for main memory). Large cache memory\nis particularly important in shared-memory architectures, since a large cache can help\nminimize the number of accesses to shared memory.\nIf an instruction needs to access a data item that is not in cache, it must be fetched\nfrom main memory. Because main memory is much slower than processors, a signi\ufb01-\ncant amount of potential processing speed may be lost while a core waits for data from\nmain memory. These waits are referred to as cache misses .\nMany processor architectures support a feature called hyper-threading ,o rhardware\nthreads , where a single physical core appears as two or more logical cores or threads.\nDi\ufb00erent processes could be mapped to di\ufb00erent logical cores. Only one of the logical\ncores corresponding to a single physical cor e can actually execute at any time. But the\nmotivation for logical cores is that if the code running on one logical core blocks on a\ncache miss, waiting for data to be fetched from memory, the hardware of the physical\ncore can start execution of one of the other logical cores instead of idling while waiting\nfor data to be fetched from memory.\nA typical multicore processor has multiple levels of cache, with the L1 cache being\nfastest to access, but also the smallest; lower cache levels such as L2 and L3 are slower\n(although still much faster than main memory) but considerably larger than the L1\ncache. Lower cache levels are usually shared between multiple cores on a single pro-\ncessor. In the cache architecture shown in Figure 20.7, the L1 and L2 caches are local\nto each of the 4 cores, while the L3 cache is shared by all cores of the processor. data\nare read into, or written from, cache in units of a cache line ,w h i c ht y p i c a l l yc o n s i s t s\nof 64 consecutive bytes.\nCore 0 Core 1 Core 2 Core 3\nL1 Cache L1 Cache L1 Cache\nL2 Cache L2 Cache L2 CacheL1 Cache\nL2 Cache\nShared L3 Cache\nFigure 20.7 Multilevel cache system.\n", "1011": "20.4 Parallel Systems 983\n20.4.5.2 Cache Coherency\nCache coherency is an issue whenever there are multiple cores or processors, each with\nits own cache. An update done on one core may not be seen by another core, if the\nlocal cache on the second core contains an old value of the a\ufb00ected memory location.\nThus, whenever an update occurs to a memory location, copies of the content of that\nmemory location that are cached on other caches must be invalidated.\nSuch invalidation is done lazily in many processor architectures; that is, there may\nbe some time lag between a write to a cache and the dispatch of invalidation mes-\nsages to other caches; in addition there may be a further lag in processing invalidation\nmessages that are received at a cache. (Requiring immediate invalidation to be done\nalways can cause a signi\ufb01cant performance penalty, and thus it is not done in current-\ngeneration systems.) Thus, it is quite possi ble for a write to happen on one processor,\nand a subsequent read on another processor may not see the updated value.\nSuch a lack of cache coherency can cause problems if a process expects to see an\nupdated memory location but does not. Modern processors therefore support memory\nbarrier instructions, which ensure certain orderings between load/store operations be-\nfore the barrier and those after the barrier. For example, the store barrier instruction\n(sfence ) on the x86 architecture forces the processor to wait until invalidation mes-\nsages are sent to all caches for all updates done prior to the instruction, before any fur-\nther load/store operations are issued. Similarly, the load barrier instruction ( lfence )e n -\nsures all received invalidation messages have been applied before any further load/store\noperations are issued. The mfence instruction does both of these tasks.\nMemory barrier instructions must be used with interprocess synchronization pro-\ntocols to ensure that the protocols execute correctly. Without the use of memory bar-\nriers, if the caches are not \u201cstrongly\u201d coherent, the following scenario can happen.\nConsider a situation where a process P1u p d a t e sm e m o r yl o c a t i o n A\ufb01rst, then loca-\ntion B; a concurrent process P2 running on a di\ufb00erent core or processor reads B\ufb01rst\nand then reads A. With a coherent cache, if P2 sees the updated value of B,i tm u s ta l s o\nsee the updated value of A. However, in the absence of cache coherence, the writes\nmay be propagated out of order, and P2 may thus see the updated value of Bbut the\nold value of A. While many architectures disallow out-of-order propagation of writes,\nthere are other subtle errors that can occur due to lack of cache coherency. However,\nexecuting sfence instructions after each of these writes and lfence before each of the\nreads will always ensure that reads see a cache coherent state. As a result, in the above\nexample, the updated value of Bwill be seen only if the updated value of Ais seen.\nIt is worth noting that programs do not need to include any extra code to deal\nwith cache coherency, as long as they acquire locks before accessing data, and release\nlocks only after performing updates, since l ock acquire and release functions typically\ninclude the required memory barrier instructions. Speci\ufb01cally, an sfence instruction is\nexecuted as part of the lock release code, before the data item is actually unlocked. Sim-\nilarly an lfence is executed right after locking a data item, as part of the lock acquisition\n", "1012": "984 Chapter 20 Database-System Architectures\nfunction, and is thus executed before the item is read. Thus, the reader is guaranteed\nto see the most recent value written to the data item.\nSynchronization primitives supported in a variety of languages also internally ex-\necute memory barrier instructions; as a result, programmers who use these primitives\nneed not be concerned about lack of cache coherency.\nIt is also interesting to note that many processor architectures use a form of\nhardware-level shared and exclusive locking of memory locations to ensure cache co-\nherency. A widely used protocol, called the MESI protocol, can be understood as fol-\nlows: Locking is done at the level of cache lines, containing multiple memory locations,\ninstead of supporting locks on individual memory locations, since cache lines are the\nunits of cache access. Locking is implemented in the hardware, rather than in software,\nto provide the required high performance.\nThe MESI protocol keeps track of the state of each cache line, which can be Modi-\n\ufb01ed (updated after exclusive locking), Exclusive locked (locked but not yet modi\ufb01ed, or\nalready written back to memory), Share locked, or Invalid. A read of a memory location\nautomatically acquires a shared lock on the cache line containing that location, while\na memory write gets an exclusive lock on the cache line before performing the write.\nIn contrast to database locks, memory lock requests do not wait; instead they immedi-\nately revoke con\ufb02icting locks. Thus, an exclusive lock request automatically invalidates\nall cached copies of the cache line and revokes all shared locks on the cache line. Sym-\nmetrically, a shared lock request causes any existing exclusive lock to be revoked and\nthen fetches the latest copy of the memory location into cache.\nIn principle, it is possible to ensure \u201cstrong\u201d cache coherency with such a locking-\nbased cache coherence protocol, making memory barrier instructions redundant. How-\never, many implementations include some optimizations that speed up processing, such\nas allowing delayed delivery of invalidation messages, at the cost of not guaranteeing\ncache coherence. As a result, memory barrier instructions are required on many pro-\ncessor architectures to ensure cache coherency.\n20.4.6 Shared Disk\nIn the shared-disk model, each node has its own processors and memory, but all nodes\ncan access all disks directly via an interconnection network. There are two advantages\nof this architecture over a shared-memory architecture. First, a shared-disk system can\nscale to a larger number of processors than a shared-memory system. Second, it o\ufb00ers\nac h e a pw a yt op r o v i d ead e g r e eo f fault tolerance : If a node fails, the other nodes can\ntake over its tasks, since the database is resident on disks that are accessible from all\nnodes.\nWe can make the disk subsystem itself fault tolerant by using a RAID architecture,\nas described in Chapter 12, allowing the system to function even if individual disks fail.\nT h ep r e s e n c eo fal a r g en u m b e ro fs t o r a g ed e v i c e si na RAID system also provides some\ndegree of I/Oparallelism.\n", "1013": "20.4 Parallel Systems 985\nstorage area\n    networknode\nnode\nnode\nnode\nnodestorage arraystorage array\nFigure 20.8 Storage-area network.\nAstorage-area network (SAN) is a high-speed local-area network designed to con-\nnect large banks of storage devices (disks) to nodes that use the data (see Figure 20.8).\nThe storage devices physically consist of an array of multiple disks but provide a view\nof a logical disk, or set of disks, that hides the details of the underlying disks. For ex-\nample, a logical disk may be much larger than any of the physical disks, and a logical\ndisk\u2019s size can be increased by adding more physical disks. The processing nodes can\naccess disks as if they are local disks, even though they are physically separate.\nStorage-area networks are usually built with redundancy, such as multiple paths\nbetween nodes, so if a component such as a link or a connection to the network fails,\nthe network continues to function.\nStorage-area networks are well suited for building shared-disk systems. The shared-\ndisk architecture with storage-area networ ks has found acceptance in applications that\ndo not need a very high degree of parallelism but do require high availability.\nCompared to shared-memory systems, shared-disk systems can scale to a larger\nnumber of processors, but communication across nodes is slower (up to a few millisec-\nonds in the absence of special-purpose hardware for communication), since it has to\ngo through a communication network.\nOne limitation of shared-disk systems is that the bandwidth of the network connec-\ntion to storage in a shared-disk system is usually less than the bandwidth available to\naccess local storage. Thus, storage access can become a bottleneck, limiting scalability.\n20.4.7 Shared Nothing\nIn a shared-nothing system, each node consists of a processor, memory, and one or\nmore disks. The nodes communicate by a high-speed interconnection network. A node\n", "1014": "986 Chapter 20 Database-System Architectures\nfunctions as the server for the data on the disk or disks that the node owns. Since\nlocal disk references are serviced by local disks at each node, the shared-nothing model\novercomes the disadvantage of requiring all I/Oto go through a single interconnection\nnetwork.\nMoreover, the interconnection networks for shared-nothing systems, such as the\ntree-like interconnection network, are usually designed to be scalable, so their trans-\nmission capacity increases as more nodes are added. Consequently, shared-nothing\narchitectures are more scalable and can easily support a very large number of nodes.\nThe main drawbacks of shared-nothing systems are the costs of communication\nand of nonlocal disk access, which are higher than in a shared-memory or shared-disk\narchitecture since sending data involves software interaction at both ends.\nDue to their high scalability, shared-nothing architectures are widely used to deal\nwith very large data volumes, supporting scalability to thousands of nodes, or in ex-\ntreme cases, even to tens of thousands of nodes.\n20.4.8 Hierarchical\nThe hierarchical architecture combines the characteristics of shared-memory, shared-\ndisk, and shared-nothing architectures. At the top level, the system consists of nodes\nthat are connected by an interconnection network and do not share disks or memory\nwith one another. Thus, the top level is a shared-nothing architecture. Each node of the\nsystem could actually be a shared-memory system with a few processors. Alternatively,\neach node could be a shared-disk system, and each of the systems sharing a set of disks\ncould be a shared-memory system. Thus, a system could be built as a hierarchy, with\nshared-memory architecture with a few processors at the base, and a shared-nothing\narchitecture at the top, with possibly a shared-disk architecture in the middle. Figure\n20.5d illustrates a hierarchical architecture with shared-memory nodes connected to-\ngether in a shared-nothing architecture.\nParallel database systems today typically run on a hierarchical architecture, where\neach node supports shared-memory parallelis m, with multiple nodes interconnected in\na shared-nothing manner.\n20.5 Distributed Systems\nIn a distributed database system , the database is stored on nodes located at geographi-\ncally separated sites. The nodes in a distributed system communicate with one another\nthrough various communication media, such as high-speed private networks or the in-\nternet. They do not share main memory or disks. The general structure of a distributed\nsystem appears in Figure 20.9.\nThe main di\ufb00erences between shared-nothing parallel databases and distributed\ndatabases include the following:\n", "1015": "20.5 Distributed Systems 987\nsite A site C\nsite Bcommunication\nvia networknetwork\nFigure 20.9 A distributed system.\n\u2022Distributed databases have sites that are geographically separated. As a result, the\nnetwork connections have lower bandwidth, higher latency, and greater probability\nof failures, as compared to networks within a single data center.\nSystems built on distributed databases therefore need to be aware of network la-\ntency, and failures, as well as of physical data location. We discuss these issues\nlater in this section. In particular, it is often desirable to keep a copy of the data at\na data center close to the end user.\n\u2022Parallel database systems address the problem of node failure. However, some fail-\nures, particularly those due to earthquakes, \ufb01res, or other natural disasters, may\na\ufb00ect an entire data center, causing failure of a large number of nodes. Distributed\ndatabase systems need to continue working even in the event of failure of an en-\ntire data center, to ensure high availability. This requires replication of data across\ngeographically separated data centers, to ensure that a common natural disaster\ndoes not a\ufb00ect all the data centers. Replication and other techniques to ensure\nhigh availability are similar in both parallel and distributed databases, although\nimplementation details may di\ufb00er.\n\u2022Distributed databases may be separately administered, with each site retaining\nsome degree of autonomy of operation. Such databases are often the result of\nthe integration of existing databases to allow queries and transactions to cross\ndatabase boundaries. However, distributed databases that are built for providing\ngeographic distribution, versus those built by integrating existing databases, may\nbe centrally administered.\n\u2022Nodes in a distributed database tend to vary more in size and function, whereas\nparallel databases tend to have nodes that are of similar capacity.\n", "1016": "988 Chapter 20 Database-System Architectures\n\u2022In a distributed database system, we di\ufb00erentiate between local and global trans-\nactions. A local transaction is one that accesses data only from nodes where the\ntransaction was initiated. A global transaction , on the other hand, is one that ei-\nther accesses data in a node di\ufb00erent from the one at which the transaction was\ninitiated, or accesses data in several di\ufb00erent nodes.\nWeb-scale applications today run on data management systems that combine sup-\nport for parallelism and distribution. Parallelism is used within a data center to handle\nhigh loads, while distribution across data centers is used to ensure high availability\neven in the event of natural disasters. At the lower end of functionality, such systems\nmay be distributed data storage systems that support only limited functionality such\nas storage and retrieval of data by key, and they may not support schemas, query lan-\nguages, or transactions; all such higher-level functionality has to be managed by the\napplications. At the higher end of functionality, there are distributed database systems\nthat support schemas, query language, and transactions. However, one characteristic\nof such systems is that they are centrally administered.\nIn contrast, distributed databases that are built by integrating existing database\nsystems have somewhat di\ufb00erent characteristics.\n\u2022Sharing data. The major advantage in building a distributed database system is the\nprovision of an environment where users at one site may be able to access the data\nresiding at other sites. For instance, in a distributed university system, where each\ncampus stores data related to that campus, it is possible for a user in one campus\nto access data in another campus. Without this capability, the transfer of student\nrecords from one campus to another campus would have to rely on some external\nmechanism.\n\u2022Autonomy. The primary advantage of sharing data by means of data distribution\nis that each site can retain a degree of control over data that are stored locally.\nIn a centralized system, the database administrator of the central site controls the\ndatabase. In a distributed system, there is a global database administrator respon-\nsible for the entire system. A part of these responsibilities is delegated to the local\ndatabase administrator for each site. Depending on the design of the distributed\ndatabase system, each administrator may have a di\ufb00erent degree of local autonomy .\nIn a homogeneous distributed database system, nodes share a common global\nschema (although some relations may be stored only at some nodes), all nodes run\nthe same distributed database-management software, and the nodes actively cooperate\nin processing transactions and queries.\nHowever, in many cases a distributed database has to be constructed by linking\ntogether multiple already-existing database systems, each with its own schema and pos-\nsibly running di\ufb00erent database-management software. The sites may not be aware of\none another, and they may provide only limited facilities for cooperation in query and\ntransaction processing. Such systems are sometimes called federated database systems\norheterogeneous distributed database systems .\n", "1017": "20.6 Transaction Processing in Parallel and Distributed Systems 989\nNodes in a distributed database communicate over wide-area networks (WAN ). Al-\nthough wide-area networks have bandwidth much greater than local-area networks, the\nbandwidth is usually shared by multiple user s/applications and is expensive relative to\nlocal-area network bandwidth. Thus, applications that communicate across wide-area\nnetworks usually have a lower bandwidth.\nCommunication in a WAN must also contend with signi\ufb01cant latency : a message\nmay take up to a few hundred milliseconds to be delivered across the world, both due\nto speed-of-light delays, and due to queuing delays at a number of routers in the path of\nthe message. Latency in a wide-area setting is a fundamental problem that cannot be\nreduced beyond a point. Thus, applications whose data and computing resources are\ndistributed geographically have to be carefully designed to ensure that latency does not\na\ufb00ect system performance excessively.\nWide-area networks also have to contend with network-link failures, a problem that\nis relatively rare in local-area networks. In particular, network-link failures may result in\ntwo sites that are both alive having no way to communicate with each other, a situation\nreferred to as a network partition .5In the event of a partition, it may not be possible for\na user or an application to access required data. Thus, network partitioning a\ufb00ects the\navailability of a system. Tradeo\ufb00s between availability and consistency of data in the\nevent of network partitions are discussed in Section 23.4.\n20.6 Transaction Processing in Parallel and Distributed Systems\nAtomicity of transactions is an important issue in building a parallel and distributed\ndatabase system. If a transaction runs across two nodes, unless the system designers\nare careful, it may commit at one node and abort at another, leading to an inconsistent\nstate. Transaction commit protocols ensure such a situation cannot arise. The two-phase\ncommit protocol (2PC) is the most widely used of these protocols.\nThe 2PCprotocol is described in detail in Sec tion 23.2.1, but the key ideas are as\nfollows: The basic idea behind 2PC is for each node to execute the transaction until\nit enters the partially committed state, and then leave the commit decision to a single\ncoordinator node; the transaction is said to be in the ready state at a node at this point.\nThe coordinator decides to commit the transaction only if the transaction reaches the\nready state at every node where it executed; otherwise (e.g., if the transaction aborts\nat any node), the coordinator decides to abort the transaction. Every node where the\ntransaction executed must follow the decision of the coordinator. If a node fails when\na transaction is in ready state, when the node recovers from failure it should be in a\nposition to either commit or abort the transaction, depending on the decision of the\ncoordinator.\n5Do not confuse the term network partitioning with the term data partitioning ; data partitioning refers to dividing up of\ndata items into partitions, which may be stored at di\ufb00erent nodes.\n", "1018": "990 Chapter 20 Database-System Architectures\nConcurrency control is another issue in parallel and distributed databases. Since\na transaction may access data items at several nodes, transaction managers at several\nnodes may need to coordinate to implement concurrency control. If locking is used,\nlocking can be performed locally at the nodes containing accessed data items, but\nthere is also a possibility of deadlock involving transactions originating at multiple\nnodes. Therefore deadlock detection needs to be carried out across multiple nodes.\nFailures are more common in distributed systems since not only may computers fail,\nbut communication links may also fail. Replication of data items, which is the key to\nthe continued functioning of distributed databases when failures occur, further compli-\ncates concurrency control. We describe concurrency-control techniques for distributed\ndatabases in Section 23.3 (which describes techniques based on locking) and Section\n23.3.4 (which describes techniques based on timestamps).\nThe standard transaction models, based on multiple actions carried out by a single\nprogram unit, are often inappropriate for carrying out tasks that cross the boundaries\nof databases that cannot or will not cooperate to implement protocols such as 2PC.\nAlternative approaches, based on persistent messaging for communication, are generally\nused for such tasks; persistent mess aging is discussed in Section 23.2.3.\nWhen the tasks to be carried out are complex, involving multiple databases and/or\nmultiple interactions with humans, coordina tion of the tasks and ensuring transaction\nproperties for the tasks become more complicated. Work\ufb02ow management systems are\nsystems designed to help with carrying out such tasks.\n20.7 Cloud-Based Services\nTraditionally, enterprises purchased and ran servers that execute the database as well\nas the applications. There is a high cost to maintaining servers, including setting up\nserver room infrastructure dealing with all kinds of failures such as air conditioning\nand power failures, not to mention failures of CPUs, disks, and other components of\nthe servers. Further, if there is a sudden increase in demand, it is very di\ufb03cult to add\ninfrastructure to service the demand, and if demand falls, the infrastructure may lie\nidle.\nIn contrast, in the cloud computing model, applications of an enterprise are ex-\necuted on an infrastructure that is managed by another company, typically at a data\ncenter that hosts a large number of machines used by many di\ufb00erent enterprises/users.\nThe service provider may provide not just hardware, but also support platforms such\nas databases, and application software.\nA variety of vendors o\ufb00er cloud services; these include major vendors such as\nAmazon, Microsoft, IBM, and Google, and a number of smaller vendors. One of the\npioneers of cloud services, Amazon, originally built a large computing infrastructure\npurely for its internal use; then, seeing a business opportunity, it o\ufb00ered computing\ninfrastructure as a service to other users. Cloud services became very popular within\njust a few years.\n", "1019": "20.7 Cloud-Based Services 991\nCloud Clients\nWeb browsers, mobile apps, ...\nSoftware-as-a-Service\nEnterprise applications, email, \nshared documents, ... \nPlatform-as-a-Service\nData storage, Database, \nApplication server, ...\nInfrastructure-as-a-Service\nContainers \nVirtual Machines \nServers                 Storageinternet\nFigure 20.10 Cloud service models.\n20.7.1 Cloud Service Models\nThere are several ways in which cloud computing can be utilized, which are summa-\nrized in Figure 20.10. These include infrastructure-as-a-service, platform-as-a-service,\nand software-as-a-service models.\n\u2022In the infrastructure-as-a-service model, an enterprise rents computing facilities;\nfor example, an enterprise may rent one or more physical machines, along with\ndisk storage space.\nMore frequently, cloud computing providers provide an abstraction of a virtual\nmachine (VM), which appears to the user to be a real machine. These machines\nare not \u201creal\u201d machines, but rather are simulated by software that allows a single\n", "1020": "992 Chapter 20 Database-System Architectures\nreal computer to simulate several independent computers. Containers are a lower\ncost alternative to VMs and are described later in this section. Multiple VMsc a n\nrun on a single server machine, and multiple containers can run on a single VM\nor server.\nBy running a very large data center with many machines, cloud-service\nproviders can exploit economies of scale and deliver computing power at much\nlower cost than an enterprise can do using its own infrastructure.\nAnother major advantage of cloud computing is that the cloud-service provider\nusually has a large number of machines, with spare capacity, and thus an enterprise\ncan rent more (virtual) machines as needed to meet demand and release them at\ntimes of light load. The ability to expand or contract capacity at short notice is\noften referred to as elasticity .\nThe above bene\ufb01ts of on-demand elastic provisioning of server systems have\nled to the widespread adoption of infrastructure-as-service platforms, especially by\ncompanies that anticipate rapid growth in their computing usage. However, due to\nthe potential security risks of storing data outside the enterprise, the use of cloud\ncomputing is still limited in high-security enterprise needs, such as banking.\nIn the infrastructure-as-service model, the client enterprise runs its own soft-\nware, including database systems, on virtual machines provided by the cloud-\nservice provider; the client has to install the database system and deal with main-\ntenance issues such as backup and restore.\n\u2022In the platform-as-a-service model, the service provider not only provides comput-\ning infrastructure, but it also deploys and manages platforms, such as data storage,\ndatabases, and application servers, that are used by application software. The client\nhas to install and maintain application software, such as enterprise resource plan-\nning (ERP) systems, which run on such platform-provided services as application\nservers, database services, or data storage services.\n\u00b0Cloud-based data storage platforms provide a service that applications can use\nto store and retrieve data. The service provider takes care of provisioning su\ufb03-\ncient amount of storage and computing power to support the load on the data\nstorage platform. Such storage systems could support \ufb01les, which are typically\nlarge, ranging in size from a few megabytes to thousands of megabytes, support-\ning millions of such \ufb01les. Or such storage systems could support data items,\nwhich are typically small, ranging from hundreds of bytes to a few megabytes,\nbut supporting billions of such data items. Such distributed \ufb01le systems and\ndata storage systems are discussed in Section 21.6 and Section 21.7. Database\napplications using cloud-based storage may run on the same cloud (i.e., the\nsame set of machines), or on another cloud.\nOne of the main attractions of cloud-based storage is that it can be used by\npaying a fee without worrying about purchasing, maintaining, and managing\nthe computer systems on which such a service runs. Further, if there is an\n", "1021": "20.7 Cloud-Based Services 993\nincrease in demand, the number of servers on which the service runs can be\nincreased by paying a larger fee, without having to actually purchase and deploy\nmore servers. The service provider would of course have to deploy extra servers,\nbut they bene\ufb01t from economies of scale; the cost of deployment, and especially\nthe time to deployment, are greatly reduced compared to what they would be\nif the end-users did it on their own.\nThe fees for cloud-based data storage are typically based on the amount of\ndata stored, and amount of data input to, and the amount of data output from,\nthe data storage system.\n\u00b0Database-as-a-service platforms provide a database that can be accessed and\nqueried by clients. Unlike storage services, database-as-a-service platforms pro-\nvide database functionality such as querying using SQL or other query lan-\nguages, which data storage systems do not provide. Early o\ufb00erings of database-\nas-a-service only supported databases that run on a single node, although the\nnode itself can have a substantial number of processors, memory, and storage.\nMore recently, parallel database systems are being o\ufb00ered as a service on the\ncloud.\n\u2022In the software-as-a-service model, the service provider provides the application\nsoftware as a service. The client does not need to deal with issues such as software\ninstallation or upgrades; these tasks are left to the service provider. The client can\ndirectly use interfaces provided by the software-as-a-service provider, such as web\ninterfaces, or mobile app interfaces that provide a front end, with the application\nsoftware acting as the back end.\nThe concept of virtual machines was developed in the 1960s to allow an expensive\nmainframe computer to be shared concurrently by users running di\ufb00erent operating\nsystems. Although computers are now much cheaper, there is still a cost associated with\nsupplying electrical power to the computers and maintaining them; virtual machines\nallow this cost to be shared by multiple concurrent users. Virtual machines also ease\nthe task of moving services to new machines: a virtual machine can be shut down\non one physical server and restarted on another physical server with very little delay\nor downtime. This feature is particularly important for quick recovery in the event of\nhardware failure or upgrade.\nAlthough multiple virtual machines can run on a single real machine, each VM\nhas a high overhead, since it runs an entire operating system internally. When a single\norganization wishes to run a number of services, if it creates a separate VMfor each\nservice, the overhead can be very high. If multiple applications are run on one machine\n(orVM), two problems often arise: (1) applications con\ufb02ict on network ports by each\ntrying to listen to the same network port, and (2) applications require di\ufb00erent versions\nof shared libraries, causing con\ufb02icts.\nContainers solve both these problems; applications run in a container, which has\nits own IPaddress and its own set of shared libraries. Each application can consist of\n", "1022": "994 Chapter 20 Database-System Architectures\na) Multiple applications on a\n    single machineb) Each application running on\n    its own VM, with multiple VMs\n    running in a machinec) Each application running in\n    its own container, with multiple\n    containers running in a machineApp App App\nLibraries LibrariesLibraries Libraries\nOS Kernel OS KernelApp\nApp App\nLibraries\nOS Kernel Hypervisor\nOS KernelApp\nAppApp\nApp\nLibraries Libraries\nFigure 20.11 Application deployment alternatives.\nmultiple processes, all running within the same container. The cost of using containers\nto run applications is much less than the alternative of running each application in\nits own VM, since many containers can share the same operating system kernel. Each\ncontainer appears to have its own \ufb01le system, but the \ufb01les are all stored in a common\nunderlying \ufb01le system across all containers. Processes within a container can interact\nwith each other through the \ufb01le system as well as interprocess communication, but\nthey can interact with processes from other containers only via network connections.\nFigure 20.11 depicts the di\ufb00erent deployme nt alternatives for a set of applications.\nFigure 20.11a shows the alternative of multiple applications running in a single ma-\nchine, sharing libraries and operating-system kernel. Figure 20.11b shows the alterna-\ntive of running each application in its own VM,w i t hm u l t i p l e VMs running on a single\nmachine. The di\ufb00erent VMs running on a single real machine are managed by a soft-\nware layer called the hypervisor . Figure 20.11c shows the alternative of using contain-\ners, with each container having its own libraries, and multiple containers running on a\nsingle machine. Since containers have lower overheads, a single machine can support\nmore containers than VMs.\nContainers provide low-cost support for elasticity, since more containers can be\ndeployed very quickly on existing virtual machines, instead of starting up fresh virtual\nmachines.\nMany applications today are built as a collection of multiple services ,e a c ho fw h i c h\nruns as a separate process, o\ufb00ering a network API; that is, the functions provided by\nthe service are invoked by creating a network connection to the process and sending a\nservice request over the network connection. Such an application architecture, which\nbuilds an application as a collection of small services, is called a microservices architec-\nture. Containers \ufb01t the microservices architecture very well, since they provide a very\nlow overhead mechanism to execute processes supporting each service.\n", "1023": "20.8 Summary 995\nDocker is a very widely used container platform, while Kubernetes is a very popular\nplatform that provides not only containers, but also a microservices platform. Kuber-\nnetes allows applications to specify declaratively their container needs, and it auto-\nmatically deploys and links multiple contai ners to execute the application. It can also\nmanage a number of pods, which allow multiple containers to share storage (\ufb01le sys-\ntem) and network ( IPaddress) while allowing containers to retain their own copies of\nshared libraries. Furthermore, it can manage elasticity by controlling the deployment\nof additional containers when required. Kube rnetes can support application scalability\nby load-balancing APIrequests across a collection of containers that all run copies of\nthe same application. Users of the APIdo not need to know what IPaddresses (each\ncorresponding to a container) the service is running on, and they can instead connect\nto a single IPaddress. The load balancer distributes the requests from the common IP\naddress to a set of containers (each with its own IPaddress) running the service.\n20.7.2 Benefits and Limitations of Cloud Services\nMany enterprises are \ufb01nding the model of cloud computing and services bene\ufb01cial.\nThe cloud model saves client enterprises the need to maintain a large system-support\nsta\ufb00 and allows new enterprises to begin operation without having to make a large, up-\nfront capital investment in computing systems. Further, as the needs of the enterprise\ngrow, more resources (computing and storage) can be added as required; the cloud-\ncomputing vendor generally has very large clusters of computers, making it easy for\nthe vendor to allocate resources on demand.\nUsers of cloud computing must be willing to accept that their data are held by\nanother organization. This may present a variety of risks in terms of security and legal\nliability. If the cloud vendor su\ufb00ers a security breach, client data may be divulged,\ncausing the client to face legal challenges from its customers. Yet the client has no\ndirect control over cloud-vendor security. These issues become more complex if the\ncloud vendor chooses to store data (or replicas of data) in a foreign country. Various\nlegal jurisdictions di\ufb00er in their privacy laws. So, for example, if a German company\u2019s\ndata are replicated on a server in New York, then the privacy laws of the United States\nmay apply instead of or in addition to those of Germany or the European Union. The\ncloud vendor might be required to release client data to the U.S. government even\nthough the client never knew that its data would be stored in a location under U.S.\njurisdiction. Speci\ufb01c cloud vendors o\ufb00er their clients varying degrees of control over\nhow their data are distributed geographically and replicated.\nDespite the drawbacks, the bene\ufb01ts of cloud services are great enough that there\nis a rapidly growing market for such services.\n20.8 Summary\n\u2022Centralized database systems run entirely on a single computer. Database systems\ndesigned for multiuser systems need to support the full set of transaction features.\n", "1024": "996 Chapter 20 Database-System Architectures\nSuch systems are usually designed as servers that accept requests from applications\nviaSQL or their own APIs.\n\u2022Parallelism with a small number of cores is referred to as coarse-grained paral-\nlelism. Parallelism with a large number of processors is referred to as \ufb01ne-grained\nparallelism.\n\u2022Transaction servers have multiple processes, possibly running on multiple proces-\nsors. So that these processes have access to common data, such as the database\nbu\ufb00er, systems store such data in shared memory. In addition to processes that\nhandle queries, there are system processes that carry out tasks such as lock and\nlog management and checkpointing.\n\u2022Access to shared memory is controlled by a mutual-exclusion mechanism based\non machine-level atomic instructions (test-and-set or compare-and-swap).\n\u2022Data-server systems supply raw data to clients. Such systems strive to minimize\ncommunication between clients and servers by caching data and locks at the\nclients. Parallel database systems use similar optimizations.\n\u2022Parallel database systems consist of multiple processors and multiple disks con-\nnected by a fast interconnection network. Speedup measures how much we can in-\ncrease processing speed by increasing parallelism for a single transaction. Scaleup\nmeasures how well we can handle an increased number of transactions by increas-\ning parallelism. Interference, skew, and start-up costs act as barriers to getting ideal\nspeedup and scaleup.\n\u2022The components of a parallel system are connected via several possible types of\ninterconnection networks: bus, ring, mesh, hypercube, or a tree-like topology.\n\u2022Parallel database architectures include the shared-memory, shared-disk, shared-\nnothing, and hierarchical architectures. These architectures have di\ufb00erent trade-\no\ufb00s of scalability versus communication speed.\n\u2022Modern shared-memory architectures associate some memory with each proces-\nsor, resulting in a non-uniform memory architecture ( NUMA ). Since each proces-\nsor has its own cache, there is a problem of ensuring cache coherency, that is,\nconsistency of data across the caches of multiple processors.\n\u2022Storage-area networks are a special type of local-area network designed to provide\nfast interconnection between large banks of storage devices and multiple comput-\ners.\n\u2022A distributed database system is a collection of partially independent database sys-\ntems that (ideally) share a common schema and coordinate processing of transac-\ntions that access nonlocal data.\n", "1025": "Review Terms 997\n\u2022Cloud services may be provided at a variety of levels: The infrastructure-as-a-\nservice model provides clients with a virtual machine on which clients install their\nown software. The platform-as-a-service model provides data-storage, database,\nand application servers in addition to virtual machines, but the client needs to in-\nstall and maintain application software. The software-as-a-service model provides\nthat application software plus the underlying platform.\n\u2022Organizations using cloud services have to consider a wide variety of technical,\neconomic, and legal issues in order to ensure the privacy and security of data\nand adequate performance despite the likelihood of data being stored at a remote\nlocation.\nReview Terms\n\u2022Centralized Database Systems\n\u00b0Single-user system\n\u00b0Multiuser system\n\u00b0Server systems\n\u00b0Embedded databases\n\u00b0Servers\n\u00b0Coarse-grained parallelism\n\u00b0Fine-grained parallelism\n\u2022Server System Architectures\n\u00b0Transaction-server\n\u00b0Query-server\n\u00b0Data-server systems\n\u00b0Server processes\n\u2022Mutual exclusion\n\u2022Atomic instructions\n\u2022Data caching\n\u2022Parallel Systems\n\u00b0Coarse-grain parallel machine\n\u00b0Massively parallel machine\n\u00b0Fine-grain parallel machine\u00b0Data center\n\u2022Decision-support queries\n\u2022Measure of performance\n\u00b0Throughput\n\u00b0Response time\n\u00b0Linear speedup\n\u00b0Sublinear speedup\n\u00b0Linear scaleup\n\u00b0Sublinear scaleup\n\u2022Sequential computation\n\u2022Amdahl\u2019s law\n\u2022Start-up costs\n\u2022Interconnection network\n\u00b0Bus\n\u00b0Ring\n\u00b0Mesh\n\u00b0Hypercube\n\u00b0Tree-like\n\u00b0Edge switches\n\u2022Aggregation switch\n", "1026": "998 Chapter 20 Database-System Architectures\n\u2022Ethernet\n\u2022Fiber channel\n\u2022In\ufb01niband\n\u2022Remote direct memory access\n(RDMA )\n\u2022Parallel Database Architectures\n\u00b0Shared memory\n\u00b0Shared disk\n\u00b0Clusters\n\u00b0Shared nothing\n\u00b0Hierarchical\n\u2022Moore\u2019s law\n\u2022NUMA\n\u2022Cache misses\n\u2022Hyper-threading\n\u2022Hardware threads\n\u2022Cache\u2022Shared-disk\n\u2022Fault tolerance\n\u2022Storage-area network ( SAN)\n\u2022Distributed database system\n\u2022Local autonomy\n\u2022Homogeneous distributed database\n\u2022Federated database systems\n\u2022Heterogeneous distributed database\nsystems\n\u2022Latency\n\u2022Network partition\n\u2022Availability\n\u2022Cloud computing\n\u2022Infrastructure-as-a-service\n\u2022Platform-as-a-service\n\u2022Cloud-based data storage\n\u2022Database-as-a-service\n\u2022Software-as-a-service\n\u2022Microservices architecture\nPractice Exercises\n20.1 Is a multiuser system necessarily a parallel system? Why or why not?\n20.2 Atomic instructions such as compare-and-swap and test-and-set also execute a\nmemory fence as part of the instruction on many architectures. Explain what\nis the motivation for executing the memory fence, from the viewpoint of data\nin shared memory that is protected by a mutex implemented by the atomic\ninstruction. Also explain what a process should do before releasing a mutex.\n20.3 Instead of storing shared structures in shared memory, an alternative archi-\ntecture would be to store them in the local memory of a special process and\naccess the shared data by interprocess communication with the process. What\nwould be the drawback of such an architecture?\n20.4 Explain the distinction between a latch and a lock as used for transactional\nconcurrency control.\n20.5 Suppose a transaction is written in C with embedded SQL,a n da b o u t8 0p e r -\nc e n to ft h et i m ei ss p e n ti nt h e SQL code, with the remaining 20 percent spent\n", "1027": "Exercises 999\nin C code. How much speedup can one hope to attain if parallelism is used\nonly for the SQL code? Explain.\n20.6 Consider a pair of processes in a shared memory system such that process\nAupdates a data structure, and then sets a \ufb02ag to indicate that the update is\ncompleted. Process Bmonitors the \ufb02ag, and starts processing the data struc-\nture only after it \ufb01nds the \ufb02ag is set.\nExplain the problems that could arise in a memory architecture where\nwrites may be reordered, and explain how the sfence andlfence instructions\ncan be used to ensure the problem does not occur.\n20.7 In a shared-memory architecture, why might the time to access a memory lo-\ncation vary depending on the memory location being accessed?\n20.8 Most operating systems for parallel machines (i) allocate memory in a local\nmemory area when a process requests memory, and (ii) avoid moving a pro-\ncess from one core to another. Why are these optimizations important with a\nNUMA architecture?\n20.9 Some database operations such as joins can see a signi\ufb01cant di\ufb00erence in\nspeed when data (e.g., one of the relations involved in a join) \ufb01ts in mem-\nory as compared to the situation where the data do not \ufb01t in memory. Show\nhow this fact can explain the phenomenon of superlinear speedup ,w h e r ea n\napplication sees a speedup greater than the amount of resources allocated to\nit.\n20.10 What is the key distinction between homogeneous and federated distributed\ndatabase systems?\n20.11 Why might a client choose to subscribe only to the basic infrastructure-as-a-\nservice model rather than to the services o\ufb00ered by other cloud service mod-\nels?\n20.12 Why do cloud-computing services support traditional database systems best by\nusing a virtual machine, instead of running directly on the service provider\u2019s\nactual machine, assuming that data is on external storage?\nExercises\n20.13 Consider a bank that has a collection of sites, each running a database system.\nSuppose the only way the databases interact is by electronic transfer of money\nbetween themselves, using persistent messaging. Would such a system qualify\nas a distributed database? Why?\n20.14 Assume that a growing enterprise has outgrown its current computer system\nand is purchasing a new parallel computer. If the growth has resulted in many\nmore transactions per unit time, but the length of individual transactions has\n", "1028": "1000 Chapter 20 Database-System Architectures\nnot changed, what measure is most relevant\u2014speedup, batch scaleup, or trans-\naction scaleup? Why?\n20.15 Database systems are typically impleme nted as a set of processes (or threads)\naccessing shared memory.\na. How is access to the shared-memory area controlled?\nb. Is two-phase locking appropriate for serializing access to the data struc-\ntures in shared memory? Explain your answer.\n20.16 Is it wise to allow a user process to access the shared-memory area of a database\nsystem? Explain your answer.\n20.17 What are the factors that can work against linear scale up in a transaction\nprocessing system? Which of the factors are likely to be the most important in\neach of the following architectures: shared-memory, shared disk, and shared\nnothing?\n20.18 Memory systems today are divided into multiple modules, each of which can\nbe serving a separate request at a given time, in contrast to earlier architec-\ntures where there was a single interface to memory. What impact has such a\nmemory architecture have on the number of processors that can be supported\nin a shared-memory system?\n20.19 Assume we have data items d1,d2,\u2026,dnwith each diprotected by a lock stored\nin memory location Mi.\na. Describe the implementation of lock-X (di)a n d unlock (di)v i at h eu s e\nof the test-and-set instruction.\nb. Describe the implementation of lock-X (di)a n d unlock (di)v i at h eu s e\nof the compare-and-swap instruction.\n20.20 I nas h a r e d - n o t h i n gs y s t e md a t aa c c e s sf r o mar e m o t en o d ec a nb ed o n eb y\nremote procedure calls, or by sending messages. But remote direct memory\naccess ( RDMA ) provides a much faster mechanism for such data access. Ex-\nplain why.\n20.21 Suppose that a major database vendor o\ufb00ers its database system (e.g., Oracle,\nSQL S erver DB2) as a cloud service. Where would this \ufb01t among the cloud-\nservice models? Why?\n20.22 If an enterprise uses its own ERP application on a cloud service under the\nplatform-as-a-service model, what restrictions would there be on when that\nenterprise may upgrade the ERP system to a new version?\n", "1029": "Further Reading 1001\nFurther Reading\n[Hennessy et al. (2017)] provides an excellent introduction to the area of computer ar-\nchitecture, including the topics of shared-memory architectures and cache coherency,\nparallel computing architectures, and cloud computing, which we covered in this chap-\nter. [Gray and Reuter (1993)] provides the cl assic textbook description of transac-\ntion processing, including the architecture of client\u2013server and distributed systems.\n[Ozsu and Valduriez (2010)] provides textbook coverage of distributed database sys-\ntems. [Abts and Felderman (2012)] provides an overview of data center networking.\nBibliography\n[Abts and Felderman (2012)] D. Abts and B. Felderman, \u201cA Guided Tour of Datacenter Net-\nworking\u201d, Communications of the ACM , Volume 55, Number 6 (2012), pages 44\u201351.\n[Gray and Reuter (1993)] J. Gray and A. Reuter, Transaction Processing: Concepts and Tech-\nniques , Morgan Kaufmann (1993).\n[Hennessy et al. (2017)] J. L. Hennessy, D. A. Patterson, and D. Goldberg, Computer Archi-\ntecture: A Quantitative Approach , 6th edition, Morgan Kaufmann (2017).\n[Ozsu and Valduriez (2010)] T. Ozsu and P. Valduriez, Principles of Distributed Database Sys-\ntems, 3nd edition, Prentice Hall (2010).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "1030": "", "1031": "CHAPTER21\nParallel and Distributed Storage\nAs we discussed in Chapter 20, parallelism is used to provide speedup, where queries\nare executed faster because more resources, such as processors and disks, are provided.\nParallelism is also used to provide scaleup, where increasing workloads are handled\nwithout increased response time, via an increase in the degree of parallelism.\nIn this chapter, we discuss techniques for data storage and indexing in parallel\ndatabase systems.\n21.1 Overview\nWe \ufb01rst describe, in Section 21.2 and Section 21.3, how to partition data amongst multi-\nple nodes. We then discuss, in Section 21.4, replication of data and parallel indexing (in\nSection 21.5). Our description focuses on shared-nothing parallel database systems, but\nthe techniques we describe are also applicable to distributed database systems, where\ndata are stored in a geographically distributed manner.\nFile systems that run on a large number of nodes, called distributed \ufb01le systems,\nare a widely used way to store data in a parallel system. We discuss distributed \ufb01le\nsystems in Section 21.6.\nIn recent years parallel data storage and indexing techniques have been extensively\nused for storage of nonrelational data, including unstructured text data, and semi-\nstructured data in XML, JSON, or other formats. Such data are often stored in parallel\nkey-value stores , which store data items with an associated key. The techniques we de-\nscribe for parallel storage of relational data can also be used for key-value stores which\nare discussed in Section 21.7. We use the term data storage system to refer to both\nkey-value stores, and the data storage and access layer of parallel database systems.\nQuery processing in parallel and distributed databases is discussed in Chapter 22,\nwhile and transaction processing in parallel and distributed databases is discussed in\nChapter 23.\n1003\n", "1032": "1004 Chapter 21 Parallel and Distributed Storage\n21.2 Data Partitioning\nIn its simplest form, I/Oparallelism refers to reducing the time required to retrieve data\nfrom disk by partitioning the data over multiple disks.1\nAt the lowest level, RAID systems allow blocks to be partitioned across multiple\ndisks, allowing them to be accessed in parallel. Blocks are usually allocated to di\ufb00erent\nd i s k si nar o u n d - r o b i nf a s h i o n ,a sw es a wi nS e c t i o n1 2 . 5 .F o re x a m p l e ,i ft h e r ea r e\nndisks numbered 0 to n\u22121, round-robin allocation assigns block ito disk imod n.\nHowever, the block-level partitioning techniques supported by RAID systems do not\no\ufb00er any control in terms of which tuples of a relation are stored on which disk or\nnode. Therefore, parallel database systems typically do not use block-level partitioning\nand instead perform partitioning at the level of tuples.\nIn systems with multiple nodes (computers), each with multiple disks, partitioning\ncan potentially be speci\ufb01ed to the level of individual disks. However, parallel database\nsystems typically focus on partitioning data across nodes and leave it to the operating\nsystem on each node to decide on assigning blocks to disks within the node.\nIn a parallel storage system, the tuples of a relation are partitioned (divided) among\nmany nodes, so that each tuple resides on one node; such partitioning is referred to as\nhorizontal partitioning . Several partitioning strategies have been proposed for horizon-\ntal partitioning, which we study next.\nWe note that vertical partitioning , discussed in Section 13.6 in the context of colum-\nnar storage, is orthogonal to horizontal partitioning. (As an example of vertical parti-\ntioning, a relation r(A,B,C,D)w h e r e Ais a primary key, may be vertically partitioned\ninto r(A,B)a n d r(A,C,D), if many queries require Bvalues, while Cand Dvalues are\nlarge in size and not required for many queries.) Once tuples are horizontally parti-\ntioned, they may be stored in a vertically partitioned manner at each node.\nWe also note that several database vendors use the term partitioning to denote the\npartitioning of tuples of a relation rinto multiple physical relations r1,r2,\u2026,rn,w h e r e\nall the physical relations riare stored in a single node. The relation ris not stored, but\ntreated as a view de\ufb01ned by the query r1\u222ar2\u222a\u2026\u222a rn.S u c h intra-node partitioning of a\nrelation is typically used to ensure that frequently accessed tuples are stored separately\nfrom infrequently accessed tuples and is di\ufb00erent from horizontal partitioning across\nnodes. Intra-node partitioning is described in more detail in Section 25.1.4.3.\nIn the rest of this chapter, as well as in subsequent chapters, we use the term par-\ntitioning to refer to horizontal partitioning across multiple nodes.\n21.2.1 Partitioning Strategies\nWe present three basic data-partitioning strategies for partitioning tuples. Assume that\nthere are nnodes, N1,N1,\u2026,Nn, across which the data are to be partitioned.\n1As in earlier chapters, we use the term diskto refer to persistent storage devices, such as magnetic hard disks and\nsolid-state drives.\n", "1033": "21.2 Data Partitioning 1005\nNode 1\nNode 2NodeRange partitioning\nvectorRange associated\nwith the node\nNode 3\nNode 415\n40\n75[\u2013\u221e, 15) \n[15, 40)\n[40, 75)\n[ 75, + \u221e]\nFigure 21.1 Example of range partitioning vector.\n\u2022Round-robin . This strategy scans the relation in any order and sends the ith tuple\nfetched during the scan to node number N((i\u22121)mod n )+1. The round-robin scheme\nensures an even distribution of tuples across nodes; that is, each node has approx-\nimately the same number of tuples as the others.\n\u2022Hash partitioning . This declustering strategy designates one or more attributes\nfrom the given relation\u2019s schema as the partitioning attributes. A hash function\nis chosen whose range is {1, 2,\u2026,n}. Each tuple of the original relation is hashed\non the partitioning attributes. If the hash function returns i, then the tuple is placed\non node Ni.2\n\u2022Range partitioning . This strategy distributes tuples by assigning contiguous\nattribute-value ranges to each node. It chooses a partitioning attribute, A,a n da\npartitioning vector [v1,v2,\u2026,vn\u22121], such that, if i<j,t h e n vi<vj. The relation is\npartitioned as follows: Consider a tuple tsuch that t[A]=x.I fx<v1,t h e n tgoes\non node N1.I fx\u2265vn\u22121,t h e n tgoes on node Nn.I fvi\u2264x<vi+1,t h e n tgoes on\nnode Ni+1.\nFigure 21.1 shows an example of a range partitioning vector. In the example in the\n\ufb01gure, values less than 15 are mapped to Node 1. Values in the range [15, 40), i.e.,\nvalues \u226515 but <40. are mapped to Node 2; Values in the range [40, 75), i.e., values\n\u226540 but <75, are mapped to Node 3, while values >7 5a r em a p p e dt oN o d e4 .\nWe now consider how partitioning is maintained when a relation is updated.\n1.When a tuple is inserted into a relation, it is sent to the appropriate node based\non the partitioning strategy.\n2Hash-function design is discussed in Section 24.5.1.1.\n", "1034": "1006 Chapter 21 Parallel and Distributed Storage\n2.If a tuple is deleted, its location is \ufb01rst found based on the value of its partitioning\nattribute (for round-robin, all partitions are searched). The tuple is then deleted\nfrom wherever it is located.\n3.If a tuple is updated, its location is not a\ufb00ected if either round-robin partitioning\nis used or if the update does not a\ufb00ect a partitioning attribute.\nHowever, if range partitioning or hash partitioning is used, and the update a\ufb00ects\na partitioning attribute, the location of the tuple may be a\ufb00ected. In this case:\na. The original tuple is deleted from the original location, and\nb. The updated tuple is inserted and sent to the appropriate node based on\nthe partitioning strategy used.\n21.2.2 Comparison of Partitioning Techniques\nOnce a relation has been partitioned among several nodes, we can retrieve it in parallel,\nusing all the nodes. Similarly, when a relation is being partitioned, it can be written\nto multiple nodes in parallel. Thus, the transfer rates for reading or writing an entire\nrelation are much faster with I/Oparallelism than without it. However, reading an entire\nrelation, or scanning a relation , is only one kind of access to data. Access to data can\nbe classi\ufb01ed as follows:\n1.Scanning the entire relation.\n2.Locating a tuple associatively (e.g., employee\n name = \u201cCampbell\u201d); these queries,\ncalled point queries , seek tuples that have a speci\ufb01ed value for a speci\ufb01c attribute.\n3.Locating all tuples for which the value of a given attribute lies within a speci\ufb01ed\nrange (e.g., 10000 <salary <20000); these queries are called range queries .\nThe di\ufb00erent partitioning techniques support these types of access at di\ufb00erent levels\nof e\ufb03ciency:\n\u2022Round-robin. The scheme is ideally suited for applications that wish to read the\nentire relation sequentially for each query. With this scheme, both point queries\nand range queries are complicated to process, since each of the nnodes must be\nused for the search.\n\u2022Hash partitioning. This scheme is best suited for point queries based on the parti-\ntioning attribute. For example, if a relation is partitioned on the telephone\n number\nattribute, then we can answer the query \u201cFind the record of the employee with\ntelephone\n number = 555-3333\u201d by applying the partitioning hash function to 555-\n3333 and then searching that node. Directing a query to a single node saves the\nstart-up cost of initiating a query on multiple nodes and leaves the other nodes free\nto process other queries.\n", "1035": "21.3 Dealing with Skew in Partitioning 1007\nHash partitioning is also useful for sequential scans of the entire relation. If\nthe hash function is a good randomizing function, and the partitioning attributes\nform a key of the relation, then the number of tuples in each of the nodes is ap-\nproximately the same, without much variance. Hence, the time taken to scan the\nrelation is approximately 1 \u2215nof the time required to scan the relation in a single\nnode system.\nThe scheme, however, is not well suited for point queries on nonpartition-\ning attributes. Hash-based partitioning is also not well suited for answering range\nqueries, since, typically, hash functions do not preserve proximity within a range.\nTherefore, all the nodes need to be scanned for range queries to be answered.\n\u2022Range partitioning. This scheme is well suited for point and range queries on the\npartitioning attribute. For point queries, we can consult the partitioning vector to\nl o c a t et h en o d ew h e r et h et u p l er e s i d e s .F o rr a n g eq u e r i e s ,w ec o n s u l tt h ep a r t i t i o n -\ning vector to \ufb01nd the range of nodes on which the tuples may reside. In both cases,\nthe search narrows to exactly those nodes that might have any tuples of interest.\nAn advantage of this feature is that, if there are only a few tuples in the queried\nrange, then the query is typically sent to one node, as opposed to all the nodes.\nSince other nodes can be used to answer other queries, range partitioning results\nin higher throughput while maintaining good response time. On the other hand, if\nthere are many tuples in the queried range (as there are when the queried range is\na larger fraction of the domain of the relation), many tuples have to be retrieved\nfrom a few nodes, resulting in an I/Obottleneck (hot spot) at those nodes. In this\nexample of execution skew , all processing occurs in one\u2014or only a few\u2014partitions.\nIn contrast, hash partitioning and round-robin partitioning would engage all the\nnodes for such queries, giving a faster response time for approximately the same\nthroughput.\nThe type of partitioning also a\ufb00ects other relational operations, such as joins, as\nwe shall see in Section 22.3 and Section 22.4.1.\nThus, the choice of partitioning technique also depends on the operations that\nneed to be executed. In general, hash partitioning or range partitioning are preferred\nto round-robin partitioning.\nPartitioning is important for large relations. Large databases that bene\ufb01t from par-\nallel storage often have some small relations. Partitioning is not a good idea for such\nsmall relations, since each node would end up with just a few tuples. Partitioning is\nworthwhile only if each node would contain at least a few disk blocks worth of data.\nSmall relations are best left unpartitioned, while medium-sized relations could be par-\ntitioned across some of the nodes, rather than across all the nodes, in a large system.\n21.3 Dealing with Skew in Partitioning\nWhen a relation is partitioned (by a technique other than round-robin), there may be\na skew in the distribution of tuples, with a high percentage of tuples placed in some\n", "1036": "1008 Chapter 21 Parallel and Distributed Storage\npartitions and fewer tuples in other partitions. Such an imbalance in the distribution\nof data is called data distribution skew . Data distribution skew may be caused by one\nof two factors.\n\u2022Attribute-value skew , which refers to the fact that some values appear in the parti-\ntioning attributes of many tuples. All the tuples with the same value for the parti-\ntioning attribute end up in the same partition, resulting in skew.\n\u2022Partition skew , which refers to the fact that there may be load imbalance in the\npartitioning, even when there is no attribute skew.\nAttribute-value skew can result in skewed partitioning regardless of whether range\npartitioning or hash partitioning is used. If the partition vector is not chosen carefully,\nrange partitioning may result in partition skew. Partition skew is less likely with hash\npartitioning if a good hash function is chosen.\nAs Section 20.4.2 noted, even a small skew can result in a signi\ufb01cant decrease in\nperformance. Skew becomes an increasing problem with a higher degree of parallelism.\nFor example, if a relation of 1000 tuples is divided into 10 parts, and the division is\nskewed, then there may be some partitions of size less than 100 and some partitions of\nsize more than 100; if even one partition happens to be of size 200, the speedup that\nwe would obtain by accessing the partitions in parallel is only 5, instead of the 10 for\nwhich we would have hoped. If the same relation has to be partitioned into 100 parts,\na partition will have 10 tuples on an average. If even one partition has 40 tuples (which\nis possible, given the large number of parti tions) the speedup that we would obtain by\naccessing them in parallel would be 25, rather than 100. Thus, we see that the loss of\nspeedup due to skew increases with parallelism.\nIn addition to skew in the distribution of tuples, there may be execution skew even\nif there is no skew in the distribution of tuples, if queries tend to access some partitions\nmore often than others. For example, suppose a relation is partitioned by the timestamp\nof the tuples, and most queries refer to recent tuples; then, even if all partitions contain\nthe same number of tuples, the partition containing recent tuples would experience a\nsigni\ufb01cantly higher load.\nIn the rest of this section, we consider several approaches to handling skew.\n21.3.1 Balanced Range-Partitioning Vectors\nData distribution skew in range partitioning can be avoided by choosing a balanced\nrange-partitioning vector , which evenly distributes tuples across all nodes.\nA balanced range-partitioning vector can be constructed by sorting, as follows:\nThe relation is \ufb01rst sorted on the partitioning attributes. The relation is then scanned in\nsorted order. After every 1 \u2215nof the relation has been read, the value of the partitioning\nattribute of the next tuple is added to the partition vector. Here, ndenotes the number\nof partitions to be constructed.\n", "1037": "21.3 Dealing with Skew in Partitioning 1009\nThe main disadvantage of this method is the extra I/Ooverhead incurred in doing\nthe initial sort. The I/Ooverhead for constructing balanced range-partitioning vectors\ncan be reduced by using a precomputed frequency table, or histogram , of the attribute\nvalues for each attribute of each relation. Figure 21.2 shows an example of a histogram\nfor an integer-valued attribute that takes values in the range 1 to 25. It is straightfor-\nward to construct a balanced range-partitioning function given a histogram on the par-\ntitioning attributes. A histogram takes up only a little space, so histograms on several\ndi\ufb00erent attributes can be stored in the system catalog. If the histogram is not stored,\nit can be computed approximately by sampling the relation, using only tuples from a\nrandomly chosen subset of the disk blocks of the relation. Using a random sample al-\nlows the histogram to be constructed in much less time than it would take to sort the\nrelation.\nThe preceding approach for creating range-partitioning vectors addresses data-\ndistribution skew; extensions to handle execution skew are left as an exercise for the\nreader (Exercise 21.3).\nA drawback of the above approach is that it is static : the partitioning is decided\nat some point and is not automatically updated as tuples are inserted, deleted, or up-\ndated. The partitioning vectors can be recomputed, and the data repartitioned, when-\never the system detects skew in data distribution. However, the cost of repartitioning\ncan be quite large, and doing it periodically would introduce a high load which can\na\ufb00ect normal processing. Dynamic techniques for avoiding skew, which can adapt in a\ncontinuous and less disruptive fashion, are discussed in Section 21.3.2 and in Section\n21.3.3.\n21.3.2 Virtual Node Partitioning\nAnother approach to minimizing the e\ufb00ect of skew is to use virtual nodes .I nt h e virtual\nnodes approach, we pretend there are several times as many virtual nodes as the number\nvaluefrequency50\n40\n30\n20\n10\n1\u20135 6\u201310 11\u201315 16\u201320 21\u201325 \nFigure 21.2 Example of histogram.\n", "1038": "1010 Chapter 21 Parallel and Distributed Storage\nof real nodes. Any of the partitioning techniques described earlier can be used, but to\nmap tuples and work to virtual nodes instead of to real nodes.3\nVirtual nodes, in turn, are mapped to real nodes. One way to map virtual nodes to\nreal nodes is round-robin allocation; thus, if there are nreal nodes numbered 1 to n,\nvirtual node iis mapped to real node (( i\u22121)modn )+1. The idea is that even if one\nrange had many more tuples than the others because of skew, these tuples would get\nsplit across multiple virtual nodes ranges. Round-robin allocation of virtual nodes to\nreal nodes would distribute the extra work among multiple real nodes, so that one node\ndoes not have to bear all the burden.\nA more sophisticated way of doing the mapping is by tracking the number of tuples\nin each virtual node, and the load (e.g., the number of accesses per second) on each\nvirtual node. Virtual nodes are then mapped to real nodes in a way that balances the\nnumber of stored tuples as well as the load across the real nodes. Thus, data-distribution\nskew and execution skew can be minimized.\nThe system must then record this mapping and use it to route accesses to the\ncorrect real node. If virtual nodes are numbered by consecutive integers, this mapping\ncan be stored as an array virtual\n to\nreal\n map[] ,w i t h mentries, where there are mvirtual\nnodes; the ith element of this array stores the real node to which virtual node iis\nmapped.\nYet another bene\ufb01t of the virtual node approach is that it allows elasticity of storage ,\nthat is, as the load on the system increases it is possible to add more resources (nodes)\nto the system to handle the load. When a new node is added, some of the virtual nodes\naremigrated to the new real node, which can be done without a\ufb00ecting any of the other\nvirtual nodes. If the amount of data mapped to each virtual node is small, the migration\nof a virtual node from one node to another can be done relatively fast, minimizing\ndisruption.\n21.3.3 Dynamic Repartitioning\nWhile the virtual-node approach can reduce skew with range partitioning as well as\nhash partitioning, it does not work very well if the data distribution changes over time,\nresulting in some virtual nodes having a very large number of tuples, or a very high\nexecution load. For example, if partitioning was done by timestamps of records, the\nlast timestamp range would get an increasing number of records, as more records are\ninserted, while other ranges would not get any new records. Thus, even if the initial\npartitioning is balanced, it could become increasingly skewed over time.\nSkew can be dealt with by recomputing the partitioning scheme entirely. However,\nrepartitioning the data based on the new partitioning scheme would, in general, be a\nvery expensive operation. In the preceding example, we would end up moving a signi\ufb01-\ncant number of records from each partition to a partition that precedes it in timestamp\n3The virtual node approach is also called the virtual processor approach, a term used in earlier editions of this book;\nsince the term virtual processor is now commonly used in a di\ufb00erent sense in the context of virtual machines, we now\nuse the term virtual node .\n", "1039": "21.3 Dealing with Skew in Partitioning 1011\norder. When dealing with large amounts of data, such repartitioning would be unrea-\nsonably expensive.\nDynamic repartitioning can be done in an e\ufb03cient manner by instead exploiting\nthe virtual node scheme. The basic idea is to split a virtual node into two virtual nodes\nwhen it has too many tuples, or too much load; the idea is very similar to a B+-tree node\nbeing split into two nodes when it is overfull. One of the newly created virtual nodes\ncan then be migrated to a di\ufb00erent node to rebalance the data stored at each node, or\nthe load at each node.\nConsidering the preceding example, if the virtual node corresponding to a range\nof timestamps 2017-01-01 to MaxDate were to become overfull, the partition could be\nsplit into two partitions. For example, if half the tuples in this range have timestamps\nless than 2018-01-01, one partition would have timestamps from 2017-01-01 to less than\n2018-01-01, and the other would have tuples with timestamps from 2018-01-01 to Max-\nDate. To rebalance the number of tuples in a real node, we would just need to move\none of the virtual nodes to a new real node.\nDynamic repartitioning in this way is very widely used in parallel databases and\nparallel data storage systems today. In data storage systems, the term table refers to a\ncollection of data items. Tables are partitioned into multiple tablets . The number of\ntablets into which a table is divided is much larger than the number of real nodes in\nthe system; thus tablets correspond to virtual nodes.\nThe system needs to maintain a partition table , which provides a mapping from the\npartitioning key ranges to a tablet identi\ufb01er, as well as the real node on which the tablet\ndata reside. Figure 21.3 shows an example of a partition table, where the partition key\nis a date. Tablet0 stores records with key value <2012-01-01. Tablet1 stores records\nwith key values \u22652012-01-01, but <2013-01-01. Tablet2 stores records with key values\n\u22652013-01-01, but <2014-01-01, and so on. Finally, Tablet6 stores values \u22652017-01-01.\nRead requests must specify a value for the partitioning attribute, which is used to\nidentify the tablet which could contain a record with that key value; a request that does\nnot specify a value for the partitioning attribute would have to be sent to all tablets.\nA read request is processed by using the partitioning key value vto identify the tablet\nValue\n Tablet ID\n Node ID\n2012-01-01\n Tablet0\n Node0\n2013-01-01\n Tablet1\n Node1\n2014-01-01\n Tablet2\n Node2\n2015-01-01\n Tablet3\n Node2\n2016-01-01\n Tablet4\n Node0\n2017-01-01\n Tablet5\n Node1\nMaxDate\n Tablet6\n Node1\nFigure 21.3 Example of a partition table.\n", "1040": "1012 Chapter 21 Parallel and Distributed Storage\nwhose range of keys contains v, and then sending the request to the real node where\nthe tablet resides. The request can be handled e\ufb03ciently at that node by maintaining,\nfor each tablet, an index on the partitioning key attribute.\nWrite, insert, and delete requests are processed similarly, by routing the requests\nto the correct tablet and real node, using the mechanism described above for reads.\nThe above scheme allows tablets to be split if they become too big; the key range\ncorresponding to the tablet is split into two, with a newly created tablet getting half the\nkey range. Records whose key range is mapped to the new tablet are then moved from\nthe original tablet to the new tablet. The partition table is updated to re\ufb02ect the split,\nso requests are then correctly directed to the appropriate tablet.\nIf a real node gets overloaded, either due to a large number of requests or due to too\nmuch data at the node, some of the tablets from the node can be moved to a di\ufb00erent\nreal node that has a lower load. Tablets can also be moved similarly in case one of the\nreal nodes has a large amount of data, while another real node has less data. Finally, if\na new real node joins a system, some tables can be moved from existing nodes to the\nnew node. Whenever a tablet is moved to a di\ufb00erent real node, the partition table is\nupdated; subsequent requests will then be sent to the correct real node.\nFigure 21.4 shows the partition table from Figure 21.3 after Tablet6, which had\nvalues \u22652017-01-01, has been split into two: Tablet6 now has values \u22652017-01-01, but\n<2018-01-01, while the new tablet, Tablet7, has values \u22652018-01-01. Such a split could\nbe caused by a large number of inserts into Tablet6, making it very large; the split\nrebalances the sizes of the tablets.\nNote also that Tablet1, which was in Node1, has now been moved to Node0 in\nFigure 21.4. Such a tablet move could be because Node1 is overloaded due to excessive\ndata, or due to a high number of requests.\nMost parallel data storage systems store the partition table at a master node. How-\never, to support a large number of requests each second, the partition table is usually\nreplicated, either to all client nodes that access data or to multiple routers .R o u t e r s\naccept read/write requests from clients and forward the requests to the appropriate\nValue\n Tablet ID\n Node ID\n2012-01-01\n Tablet0\n Node0\n2013-01-01\n Tablet1\n Node0\n2014-01-01\n Tablet2\n Node2\n2015-01-01\n Tablet3\n Node2\n2016-01-01\n Tablet4\n Node0\n2017-01-01\n Tablet5\n Node1\n2018-01-01\n Tablet6\n Node1\nMaxDate\n Tablet7\n Node1\nFigure 21.4 Example partition table after tablet split and tablet move.\n", "1041": "21.4 Replication 1013\nreal node containing the tablet/virtual nodes based on the key values speci\ufb01ed in the\nrequest.\nAn alternative fully distributed approach is supported by a hash based partitioning\nscheme called consistent hashing . In the consistent hashing approach, keys are hashed\nto a large space, such as 32 bit integers. Further, node (or virtual node) identi\ufb01ers are\nalso hashed to the same space. A key kicould be logically mapped to the node njwhose\nhash value h(nj) is the highest value among all nodes satisfying h(nj)<h(ki). But to\nensure that every key is assigned to a node, hash values are treated as lying on a cycle\nsimilar to the face of a clock, where the maximum hash value maxhash is immediately\nfollowed by 0. Then, key kiis then logically mapped to the node njwhose hash value\nh(nj) is the closest among all nodes, when we move anti-clockwise in the circle from\nh(ki).\nDistributed hash tables based on this idea have been developed where there is no\nneed for either a master node or a router; instead each participating node keeps track\nof a few other peer nodes, and routing is implemented in a cooperative manner. New\nnodes can join the system, and integrate themselves by following speci\ufb01ed protocols\nin a completely peer-to-peer manner, without the need for a master. See the Further\nReading section at the end of the chapter for references providing further details.\n21.4 Replication\nWith a large number of nodes, the probability that at least one node will malfunction in\na parallel system is signi\ufb01cantly greater than in a single-node system. A poorly designed\nparallel system will stop functioning if any node fails. Assuming that the probability of\nfailure of a single node is small, the probability of failure of the system goes up linearly\nwith the number of nodes. For example, if a single node would fail once every 5 years,\na system with 100 nodes would have a failure every 18 days.\nParallel data storage systems must, therefore, be resilient to failure of nodes. Not\nonly should data not be lost in the event of a node failure, but also, the system should\ncontinue to be available , that is, continue to function, even during such a failure.\nTo ensure tuples are not lost on node failure, tuples are replicated across at least\ntwo nodes, and often three nodes. If a node fails, the tuples that it stored can still be\naccessed from the other nodes where the tuples are replicated.4\nTracking the replicas at the level of individual tuples would result in signi\ufb01cant\noverhead in terms of storage and query processing. Instead, replication is done at the\nlevel of partitions (tablets, nodes, or virtual n odes). That is, each partition is replicated;\nthe locations of the partition replicas are recorded as part of the partition table.\nFigure 21.5 shows a partition table with re plication of tablets. Each tablet is repli-\ncated in two nodes.\n4Caching also results in replication of data, but with th e aim of speeding up access. Since data may be evicted from\ncache at any time, caching does not ensure availability in the event of failure.\n", "1042": "1014 Chapter 21 Parallel and Distributed Storage\nValue\n Tablet ID\n Node ID\n2012-01-01\n Tablet0\n Node0,Node1\n2013-01-01\n Tablet1\n Node0,Node2\n2014-01-01\n Tablet2\n Node2,Node0\n2015-01-01\n Tablet3\n Node2,Node1\n2016-01-01\n Tablet4\n Node0,Node1\n2017-01-01\n Tablet5\n Node1,Node0\n2018-01-01\n Tablet6\n Node1,Node2\nMaxDate\n Tablet7\n Node1,Node2\nFigure 21.5 Partition table of Figure 21.4 with replication.\nThe database system keeps track of failed nodes; requests for data stored at a failed\nnode are automatically routed to the backup nodes that store a replica of the data.\nIssues of how to handle the case where one or more replicas are stored at a currently\nfailed node are addressed brie\ufb02y in Section 21.4.2, and in more detail later, in Section\n23.4.\n21.4.1 Location of Replicas\nReplication to two nodes provides protection from data loss/unavailability in the event\nof single node failure, while replication to three nodes provides protection even in the\nevent of two node failures. If all nodes where a partition is replicated fail, obviously\nthere is no way to prevent data loss/unavailability. Systems that use low-cost commodity\nmachines for data storage typically use three-way replication, while systems that use\nmore reliable machines typically use two-way replication.\nThere are multiple possible failure modes in a parallel system. A single node could\nfail due to some internal fault. Further, it is possible for all the nodes in a rack to fail\nif there is some problem with the rack such as, for example, failure of power supply to\nthe entire rack, or failure of the network switches in a rack, making all the nodes in the\nrack inaccessible. Further, there is a possibility of failure of an entire data center, for\nexample, due to \ufb01re, \ufb02ooding, or a large-scale power failure.\nThe location of the nodes where the replicas of a partition are stored must, there-\nfore, be chosen carefully, to maximize the probability of at least one copy being acces-\nsible even during a failure. Such replication can be within a data center or across data\ncenters.\n\u2022Replication within a data center : Since single node failures are the most common\nfailure mode, partitions are often replicated to another node.\nWith the tree-like interconnection topology commonly used in data center net-\nworks (described in Section 20.4.3) network bandwidth within a rack is much\n", "1043": "21.4 Replication 1015\nhigher than the network bandwidth between racks. As a result, replication to an-\nother node within the same rack as the \ufb01rst node reduces network demand on the\nnetwork between racks. But to deal with the possibility of a rack failure, partitions\nare also replicated to a node on a di\ufb00erent rack.\n\u2022Replication across data centers : To deal with the possibility of failure of an en-\ntire data center, partitions may also be replicated at one or more geographically\nseparated data centers. Geographic separation is important to deal with disasters\nsuch as earthquakes or storms that may shut down all data centers in a geographic\nregion.\nFor many web applications, round-trip delays across a long-distance network\ncan a\ufb00ect performance signi\ufb01cantly, a problem that is increasing with the use\nof Ajax applications that require multiple rounds of communication between the\nbrowser and the application. To deal with this problem, users are connected with\napplication servers that are closest to them geographically, and data replication is\ndone in such a way that one of the replicas is in the same data center as (or at\nleast, geographically close to) the application server.\nSuppose all the partitions at a node N1are replicated at a single node N2,a n d N1\nfails. Then, node N2will have to handle all the requests that would originally have gone\ntoN1,a sw e l la sr e q u e s t sr o u t e dt on o d e N2.A sar e s u l t ,n o d e N2would have to perform\ntwice as much work as other nodes in the system, resulting in execution skew during\nfailure of node N1.\nTo avoid this problem, the replicas of partitions residing at a node, say N1,a r e\nspread across multiple other nodes. For example, consider a system with 10 nodes and\ntwo-way replication. Suppose node N1had one of the replicas of partitions p1through\np9. Then, the other replica of partitions p1c o u l db es t o r e do n N2,o fp2onN3,a n ds o\non to p9onN10. Then in the event of failure of N1,n o d e s N2through N10would share\nthe extra work equally, instead of burdening a single node with all the extra work.\n21.4.2 Updates and Consistency of Replicas\nSince each partition is replicated, updates made to tuples in a partition must be per-\nformed on all the replicas of the partition. For data that is never updated after it has\nbeen created, reads can be performed at any of the replicas, since all of them will have\nthe same value. If a storage system ensures that all replicas are exclusive-locked and\nupdated atomically (using, for example, the two-phase commit protocol which we will\nsee in Section 23.2.1), reads of a tuple can be performed (after obtaining a shared lock)\nat any of the replicas and will see the most recent version of the tuple.\nIf data are updated, and replicas are not updated atomically, di\ufb00erent replicas may\ntemporarily have di\ufb00erent values. Thus, a read may see a di\ufb00erent value depending on\nwhich replica it accesses. Most applications require that read requests for a tuple must\n", "1044": "1016 Chapter 21 Parallel and Distributed Storage\nreceive the most recent version of the tuple; updates that are based on reading an older\nversion could result in a lost update problem .\nOne way of ensuring that reads get the latest value is to treat one of the replicas\nof each partition as a master replica . All updates are sent to the master replica and are\nthen propagated to other replicas. Reads are also sent to the master replica, so that\nreads get the latest version of any data item even if updates have not yet been applied\nto the other replicas.\nIf a master replica fails, a new master is assigned for that partition. It is important\nto ensure that every update operation performed by the old master has also been seen\nby the new master. Further, the old master may have updated some of the replicas,\nbut not all, before it failed; the new master must complete the task of updating all the\nreplicas. We discuss details in Section 23.6.2.\nIt is important to know which node is the (current) master for each partition. This\ninformation can be stored along with the partition table. Speci\ufb01cally, the partition table\nmust record, in addition to the range of key values assigned to that partition, where the\nreplicas of the partition are stored, and further which replica is currently the master.\nThree solutions are commonly used to update replicas.\n\u2022The two-phase commit (2PC) protocol, which ensures that multiple updates per-\nformed by a transaction are applied atomically across multiple sites, is described\nin Section 23.2. This protocol can be used with replicas to ensure that an update\nis performed atomically on all replicas of a tuple.\nWe assume for now that all replicas are available and can be updated. Issues of\nhow to allow two-phase commit to continue execution in the presence of failures,\nwhen some replicas may not be reachable, are discussed in Section 23.4.\n\u2022Persistent messaging systems, described i n Section 23.2.3, which guarantee that a\nmessage is delivered once it is sent. Persistent messaging systems can be used to\nupdate replicas as follows: An update to a tuple is registered as a persistent mes-\nsage, sent to all replicas of the tuple. Once the message is recorded, the persistent\nmessaging system ensures it will be delivered to all replicas. Thus, all replicas will\nget the update, eventually; the property is known as eventual consistency of replicas.\nHowever, there may be a delay in message delivery, and during that time some\nreplicas may have applied an update while others have not. To ensure that reads get\na consistent version, reads are performed only at a master replica, where updates\nare made \ufb01rst. (If the site with a master replica of a tuple has failed, another replica\ncan take over as the master replica after ensuring all pending persistent messages\nwith updates have been applied.) Details are presented in Section 23.6.2.\n\u2022Protocols called consensus protocols , that allow updates of replicas to proceed even\nin the face of failures, when some replicas may not be reachable, can be used to\nmanage update of replicas. Unlike the preceding protocols, consensus protocols\ncan work even without a designated master replica. We study consensus protocols\nin Section 23.8.\n", "1045": "21.5 Parallel Indexing 1017\n21.5 Parallel Indexing\nIndices in a parallel data storage system can be divided into two kinds: local and global\nindices. In the following discussion, when virtual node partitioning is used, the term\nnode should be understood to mean virtual node (or equivalently, tablet).\n\u2022Alocal index is an index built on tuples stored in a particular node; typically, such\nan index would be built on all the partitions of a given relation. The index contents\nare stored on the same node as the data.\n\u2022Aglobal index is an index built on data stored across multiple nodes; a global index\ncan be used to e\ufb03ciently \ufb01nd matching tuples, regardless of where the tuples are\nstored.\nWhile the contents of a global index could be stored at a single central location,\nsuch a scheme would result in poor scalability; as a result, the contents of a global\nindex should partitioned across multiple nodes.\nAglobal primary index on a relation is a global index on the attributes on which\nthe tuples of the relation are partitioned. A global index on partitioning attribute Kis\nconstructed by merely creating local indices on Kon each partition.\nA query that is intended to retrieve tuples with a speci\ufb01c key value k1f o r Kcan be\nanswered by \ufb01rst \ufb01nding which partition could hold the key value k1, and then using\nthe local index in that partition to \ufb01nd the required tuples.\nFor example, suppose the student relation is partitioned on the attribute ID,a n da\nglobal index is to be constructed on the attribute ID. All that is required is to construct\nal o c a li n d e xo n IDon each partition. Figure 21.6(a) shows a global primary index on\nthestudent relation, on attribute ID; the local indices are not shown explicitly in the\n\ufb01gure.\nA query that is intended to retrieve tuples with a speci\ufb01c value for ID,s a y5 5 7 ,c a n\nbe answered by \ufb01rst using the partitioning function on IDto \ufb01rst \ufb01nd which partition\ncould contain the speci\ufb01ed IDvalue 557; the query is then sent to the corresponding\nnode, which uses the local index on IDto locate the required tuples.\nNote that IDis the primary key for the relation student ; however, the above scheme\nwould work even if the partitioning attribute were not the primary key. The scheme\ncan be extended to handle range queries, provided the partitioning function is itself\nbased on ranges of values; a partitioning scheme based on hashing cannot support\nrange queries.\nAglobal secondary index on a relation is a global index whose index attributes do\nnot match the attributes on which the tuples are partitioned.\nSuppose the partitioning attributes are Kp, while the index attributes are Ki,w i t h\nKp\u2260Ki. One approach for answering a selection query on attributes Kiis as follows: If\na local index is created on Kion each partition of the relation, the query is sent to each\n", "1046": "1018 Chapter 21 Parallel and Distributed Storage\n(b) Secondary index on name231\n543\n765Partition TableZhang Comp. Sci.\nComp. Sci.102\nShankar 32\nBrandt History 80 199001\n123Tablet 1\nChavez Finance 110\nPeltier 56\nLevy Physics 46 456231\n445Tablet 2\nPhysics\nWilliams   54\nSanchez 38\nSnow Physics   0 705543\n557 MusicTablet 3\nComp. Sci.\nBrown   58\nAoi 6\nBourakis Elec. Eng.   98 987\nTanaka\n(a) Primary index on IDBiology   120 989765\n766 Elec. Eng.Tablet 4\nComp. Sci.BrownAoi\nBourakis\n199 Brandt\n765766\n987Tablet 6\nChavez\nLevy\nPeltier 445\nSanchez 557231\n456Tablet 7\nShankar\nSnow\nTanaka 989\nWilliams 543123\n705Tablet 8\nZhang 001Partition Table\nChavez\nShankar\nFigure 21.6 Global primary and secondary indices on student relation\npartition, and the local index is used to \ufb01nd matching tuples. Such an approach using\nlocal indices is very ine\ufb03cient if the number of partitions is large, since every partition\nhas to be queried, even if only one or a few partitions contain matching tuples.\nWe now illustrate an e\ufb03cient scheme for constructing a global secondary index by\nusing an example. Consider again the student relation partitioned on the attribute ID,\nand suppose a global index is to be constructed on the attribute name .As i m p l ew a y\nto construct such an index is to create a set of ( name ,ID) tuples, with one tuple per\nstudent tuple; let us call this set of tuples index\n name .N o w ,t h e index\n name tuples are\npartitioned on the attribute name .Al o c a li n d e xo n name is then constructed on each\npartition of index\n name . In addition, a global index is created on the ID,w h i c hi st h e\npartitioning attribute. Figure 21.6(b) shows a secondary index on the student relation\non attribute name ; local indices are not explicitly shown in the \ufb01gure.\nNow, a query that needs to retrieve students with a given name can be handled by\n\ufb01rst examining the partition function of index\n name to \ufb01nd which partition could store\nindex\n name tuples with the given name; the query is then sent to that partition, which\nuses the local index on name to \ufb01nd the corresponding IDvalue. Next, the global index\non the IDv a l u ei su s e dt o\ufb01 n dt h er e q u i r e dt u p l e .\nNote that in the example above, the partitioning attribute IDdoes not have du-\nplicates; hence it su\ufb03ces to add only the index key name and the attribute IDto the\nindex\n name relation. Otherwise, further attributes would have to be added to ensure\ntuples can be uniquely identi\ufb01ed, as described next.\nIn general, given a relation r, which is partitioned on a set of attributes Kp,i fw e\nwish to create a global secondary index on a set of attributes Ki, we create a new relation\nrs\ni, containing the following attributes:\n", "1047": "21.6 Distributed File Systems 1019\n1.Ki,a n d Kp\n2.If the partitioning attributes Kphave duplicates, we would have to add further\nattributes Ku,s u c ht h a t Kp\u222aKuis a key for the relation being indexed.\nThe relation rs\niis partitioned on Ki, and a local index is created on Ki.I na d d i t i o n ,a\nlocal index on attributes ( Kp,Ku) is created on each partition of relation r.\nWe now consider how to use a global secondary index to answer a query. Consider\na query that speci\ufb01es a particular value vforKi. The query is processed as follows:\n1.The relevant partition of rs\nifor the value vis found using the partitioning function\nonKi.\n2.Use the local index on Kiat the above partition to \ufb01nd tuples of rs\nithat have the\nspeci\ufb01ed value vforKi.\n3.The tuples in the preceding result are partitioned based on the Kpvalue and sent\nto the corresponding nodes.\n4.At each node, the tuples received from the preceding step are used along with\nthe local index on ron attributes Kp\u222aKu, to \ufb01nd matching rtuples.\nNote that relation rs\niis basically a materialized view de\ufb01ned as rs\ni=\u03a0Ki,Kp,Ku(r).\nWhenever ris modi\ufb01ed by inserts, deletions, or updates to tuples, the materialized view\nrs\nimust be correspondingly updated.\nNote also that updates to a tuple of rat some node may result in updates to tuples\nofrs\niat other nodes. For example, in Figure 21.6, if the name of ID001 is updated from\nZhang to Yang, the tuple (Zhang, 001) at Tablet8 will be updated to (Yang, 001); since\nboth Zhang and Yang belong in the same partition of the secondary index, no other\npartition is a\ufb00ected. On the other hand, if the name is updated from Zhang to Bolin,\nthe tuple (Zhang, 001) will be deleted from Tablet8 and a new entry (Bolin, 001) added\nto Tablet6.\nPerforming the above updates to the secondary index as part of the same transac-\ntion that updates the base relation requires updates to be committed atomically across\nmultiple nodes. Two-phase commit, discussed in Section 23.2, can be used for this\ntask. Alternatives based on persistent messaging can also be used as described in Sec-\ntion 23.2.3, provided it is acceptable for the secondary index to be somewhat out of\ndate.\n21.6 Distributed File Systems\nAdistributed \ufb01le system stores \ufb01les across a large collection of machines while giving a\nsingle-\ufb01le-system view to clients. As with any \ufb01le system, there is a system of \ufb01le names\nand directories, which clients can use to identify and access \ufb01les. Clients do not need\nto bother about where the \ufb01les are stored.\n", "1048": "1020 Chapter 21 Parallel and Distributed Storage\nThe goal of \ufb01rst-generation distributed \ufb01le systems was to allow client machines to\naccess \ufb01les stored on one or more \ufb01le servers. In contrast, later-generation distributed\n\ufb01le systems, which we focus on, address distribution of \ufb01le blocks across a very large\nnumber of nodes. Such distributed \ufb01le systems can store very large amounts of data and\nsupport very large numbers of concurrent clients. A landmark system in this context\nwas the Google File System (GFS), developed in the early 2000s, which saw widespread\nuse within Google. The open-source Hadoop File System (HDFS) is based on the GFS\narchitecture and is now very widely used.\nDistributed \ufb01le systems are generally designed to e\ufb03ciently store large \ufb01les whose\nsizes range from tens of megabytes to hundreds of gigabytes or more. However, they\nare designed to store moderate numbers of such \ufb01les, of the order of millions; they are\ntypically not designed to stores billions of di\ufb00erent \ufb01les. In contrast, the parallel data\nstorage systems we have seen earlier are designed to store very large numbers (billions\nor more) of data items, whose size can range from small (tens of bytes) to medium (a\nfew megabytes).\nAs in parallel data storage systems, the data in a distributed \ufb01le system are stored\nacross a number of nodes. Since \ufb01les can be much larger than data items in a data\nstorage system, \ufb01les are broken up into multiple blocks. The blocks of a single \ufb01le can\nbe partitioned across multiple machines. Further, each \ufb01le block is replicated across\nmultiple (typically three) machines, so tha t a machine failure does not result in the \ufb01le\nbecoming inaccessible.\nFile systems typically support two kinds of metadata :\n1.A directory system, which allows a hierarchical organization of \ufb01les into directo-\nries and subdirectories, and\n2.A mapping from a \ufb01le name to the sequence of identi\ufb01ers of blocks that store\nthe actual data in each \ufb01le.\nIn the case of a centralized \ufb01le system, the block identi\ufb01ers help locate blocks in a\nstorage device such as a disk. In the case of a distributed \ufb01le system, in addition to\nproviding a block identi\ufb01er, the \ufb01le system must provide the location (node identi\ufb01er)\nwhere the block is stored; in fact, due to replication, the \ufb01le system provides a set of\nnode identi\ufb01ers along with each block identi\ufb01er.\nIn the rest of this section, we describe the organization of the Hadoop File System\n(HDFS), which is shown in Figure 21.7; the architecture of HDFS is derived from that\nof the Google File System (GFS). The nodes (machines) which store data blocks in\nHDFS are called datanodes . Blocks have an associated ID, and datanodes map the\nblock ID to a location in their local \ufb01le system where the block is stored.\nThe \ufb01le system metadata too can be partitioned across many nodes, but unless\ncarefully architected, this could lead to bad performance. GFS and HDFS took a sim-\npler and more pragmatic approach of storing the \ufb01le system metadata at a single node,\ncalled the namenode in HDFS.\n", "1049": "21.6 Distributed File Systems 1021\nNameNode\nRack 1 Rack 2ClientMetadata Ops\nDataNodes\nBlocksBlock Read\nBlock Write\nReplicationMetadata (name, replicas, ...)\nBackupNode\nMetadata (name, replicas, ...)\nClient\nFigure 21.7 Hadoop Distributed File System (HDFS) architecture\nSince all metadata reads have to go to the namenode, if a disk access were required\nto satisfy a metadata read, the number of requests that could be satis\ufb01ed per second\nwould be very small. To ensure acceptable performance, HDFS namenodes cache the\nentire metadata in memory; the size of memory then becomes a limiting factor in the\nnumber of \ufb01les and blocks that the \ufb01le system can manage. To reduce the memory size,\nHDFS uses very large block sizes (typically 64 MB) to reduce the number of blocks\nthat the namenode must track for each \ufb01le. Despite this, the limited amount of main\nmemory on most machines constrains namenodes to support only a limited number of\n", "1050": "1022 Chapter 21 Parallel and Distributed Storage\n\ufb01les (of the order of millions). However, with main-memory sizes of many gigabytes,\nand block sizes of tens of megabytes, an HDFS system can comfortably handle many\npetabytes of data.\nIn any system with a large number of datanodes, datanode failures are a frequent\noccurrence. To deal with datanode failures, data blocks must be replicated to multiple\ndatanodes. If a datanode fails, the block can still be read from one of the other datan-\nodes that stores a replica of the block. Replication to three datanodes is widely used to\nprovide high availability, without paying too high a storage overhead.\nWe now consider how a \ufb01le open and read request is satis\ufb01ed with HDFS. First,\nthe client contacts the namenode, with the name of the \ufb01le. The namenode \ufb01nds the\nlist of IDs of blocks containing the \ufb01le data and returns to the client the list of block\nIDs, along with the set of nodes that contain replicas of each of the blocks. The client\nthen contacts any one of the replicas for each block of the \ufb01le, sending it the ID of the\nblock, to retrieve the block. In case that particular replica does not respond, the client\ncan contact any of the other replicas.\nTo satisfy a write request, the client \ufb01rst contacts the namenode, which allocates\nblocks, and decides which datanodes should store replicas of each block. The metadata\nare recorded at the namenode and sent back to the client. The client then writes the\nblock to all the replicas. As an optimization to reduce network tra\ufb03c, HDFS implemen-\ntations may choose to store two replicas in the same rack; in that case, the block write is\nperformed to one replica, which then copies the data to the second replica on the same\nrack. When all the replicas have processed the write of a block, an acknowledgment is\nsent to the client.\nReplication introduces the problem of consistency of data across the replicas in\ncase the \ufb01le is updated. As an example, suppose one of the replicas of a data block\nis updated, but due to system failure, another replica does not get updated; then the\nsystem could end up with inconsistent states across the replicas. And what value is read\nwould depend on which replica is accessed, which is not an acceptable situation.\nWhile data storage systems in general need to deal with consistency, using tech-\nniques that we study in Chapter 23, some distributed \ufb01le systems such as HDFS take\na di\ufb00erent approach: namely, not allowing updates. In other words, a \ufb01le can be ap-\npended to, but data that are written cannot be updated. As each block of the \ufb01le is\nwritten, the block is copied to the replicas. The \ufb01le cannot be read until it is closed ,\nthat is, all data have been written to the \ufb01le, and the blocks have been written suc-\ncessfully at all their replicas. The model of writing data to a \ufb01le is sometimes referred\nto as write-once-read-many access model. Others such as GFS allow updates and de-\ntect certain inconsistent states caused by failures while writing to replicas; however,\ntransactional (atomic) updates are not supported.\nThe restriction that \ufb01les cannot be updated, but can only be appended to, is not\na problem for many applications of a distributed \ufb01le system. Applications that require\nupdates should use a data-storage system that supports updates instead of using a dis-\ntributed \ufb01le system.\n", "1051": "21.7 Parallel Key-Value Stores 1023\n21.7 Parallel Key-Value Stores\nMany Web applications need to store very large numbers (many billions) of relatively\nsmall records (of size ranging from a few kilobytes to a few megabytes). Storage would\nhave to be distributed across thousands of nodes. Storing such records as \ufb01les in a\ndistributed \ufb01le system is infeasible, since \ufb01le systems are not designed to store such\nlarge numbers of small \ufb01les. Ideally, a massively parallel relational database should\nbe used to store such data. But the parallel relational databases available in the early\n2000s were not designed to work at a massive scale; nor did they support the ability to\neasily add more nodes to the system without causing signi\ufb01cant disruption to ongoing\nactivities.\nA number of parallel key-value storage systems were developed to meet the needs of\nsuch web applications. A key-value store provides a way to store or update a data item\n(value) with an associated key and to retrieve the data item with a given key. Some\nkey-value stores treat the data items as uninterpreted sequences of bytes, while others\nallow a schema to be associated with the data item. If the system supports de\ufb01nition of\na schema for data items, it is possible for the system to create and maintain secondary\nindices on speci\ufb01ed attributes of data items.\nKey-value stores support two very basic functions on tables: put(table, key, value) ,\nused to store values, with an associated key, in a table, and get(table, key) ,w h i c hr e -\ntrieves the stored value associated with the speci\ufb01ed key. In addition, they may support\nother functions, such as range queries on key values, using get(table, key1, key2) .\nFurther, many key-value stores support some form of \ufb02exible schema.\n\u2022Some allow column names to be speci\ufb01ed as part of a schema de\ufb01nition, similar\nto relational data stores.\n\u2022Others allow columns to be added to, or deleted from, individual tuples; such key-\nvalue stores are sometimes referred to as wide-column stores . Such key-value stores\nsupport functions such as put(table, key, columname, value) ,t os t o r eav a l u ei n\na speci\ufb01c column of a row identi\ufb01ed by the key (creating the column if it does not\nalready exist), and get(table, key, columname) , which retrieves the value for a\nspeci\ufb01c column of a speci\ufb01c row identi\ufb01ed by the key. Further, delete(table, key,\ncolumname) deletes a speci\ufb01c column from a row.\n\u2022Yet other key-value stores allow the value stored with a key to have a complex\nstructure, typically based on JSON ; they are sometimes referred to as document\nstores .\nThe ability to specify a (partial) schema of the stored value allows the key-value store\nto evaluate selection predicates at the data store; some stores also use the schema to\nsupport secondary indices.\nWe use the term key-value store to include all the above types of data stores; how-\never, some people use the term key-value store to refer more speci\ufb01cally to those that\n", "1052": "1024 Chapter 21 Parallel and Distributed Storage\ndo not support any form of schema and treat the value as an uninterpreted sequence\nof bytes.\nParallel key-value stores typically support elasticity ,w h e r e b yt h en u m b e ro fn o d e s\ncan be increased or decreased incrementally, depending on demand. As nodes are\nadded, tablets can be moved to the new nodes. To reduce the number of nodes, tablets\ncan be moved away from some nodes, which can then be removed from the system.\nWidely used parallel key-value stores that support \ufb02exible columns (also known as\nwide-column stores) include Bigtable from Google, Apache HBase, Apache Cassan-\ndra (originally developed at Facebook), and Microsoft Azure Table Storage from Mi-\ncrosoft, among others. Key-value stores that support a schema include Megastore and\nSpanner from Google, and Sherpa/ PNUTS from Yahoo!. Key-value stores that support\nsemi-structured data (also known as document-stores) include Couchbase, DynamoDB\nfrom Amazon, and MongoDB, among others. Redis and Memcached are parallel in-\nmemory key-value stores which are widely used for caching data.\nKey-value stores are not full-\ufb02edged databases, since they do not provide many of\nthe features that are viewed as standard on database systems today. Features that key-\nvalue stores typically do not support include declarative querying (using SQL or any\nother declarative query language), support for transactions, and support for e\ufb03cient\nretrieval of records based on selections on nonkey attributes (traditional databases\nsupport such retrieval using secondary indices). In fact, they typically do not support\nprimary-key constraints for attributes other than the key, and do not support foreign-key\nconstraints.\n21.7.1 Data Representation\nAs an example of data management needs of web applications, consider the pro\ufb01le of\na user, which needs to be accessible to a number of di\ufb00erent applications that are run\nby an organization. The pro\ufb01le contains a variety of attributes, and there are frequent\nadditions to the attributes stored in the pro\ufb01le. Some attributes may contain complex\ndata. A simple relational representation is often not su\ufb03cient for such complex data.\nMany key-value stores support the JavaScript Object Notation (JSON )r e p r e s e n t a -\ntion, which has found increasing acceptance for representing complex data ( JSON is\ncovered in Section 8.1.2). The JSON representation provides \ufb02exibility in the set of\nattributes that a record contains, as well as the types of these attributes. Yet others,\nsuch as Bigtable, de\ufb01ne their own data model for complex data, including support for\nrecords with a very large number of optional columns.\nIn Bigtable, a record is not stored as a single value but is instead split into compo-\nnent attributes that are stored separately. Thus, the key for an attribute value conceptu-\nally consists of (record-identi\ufb01er, attribute-name). Each attribute value is just a string\nas far as Bigtable is concerned. To fetch all attributes of a record, a range query, or\nmore precisely a pre\ufb01x-match query consisting of just the record identi\ufb01er, is used. The\nget() function returns the attribute names along with the values. For e\ufb03cient retrieval\n", "1053": "21.7 Parallel Key-Value Stores 1025\nof all attributes of a record, the storage system stores entries sorted by the key, so all\nattribute values of a particular record are clustered together.\nIn fact, the record identi\ufb01er can itself be structured hierarchically, although to\nBigtable itself the record identi\ufb01er is just a string. For example, an application that\nstores pages retrieved from a web crawl could map a URL of the form:\nwww.cs.yale.edu/people/silberschatz.html\nto the record identi\ufb01er:\nedu.yale.cs.www/people/silberschatz.html\nso that pages are clustered in a useful order.\nData-storage systems often allow multiple versions of data items to be stored. Ver-\nsions are often identi\ufb01ed by timestamp, but they may be alternatively identi\ufb01ed by an\ninteger value that is incremented whenever a new version of a data item is created.\nReads can specify the required version of a data item, or they can pick the version with\nthe highest version number. In Bigtable, for example, a key actually consists of three\nparts: (record-identi\ufb01er, attribute-name, timestamp).\nSome key-value stores support columnar storage o fr o w s ,w i t he a c hc o l u m no fa\nrow stored separately, with the row key and the column value stored for each row. Such\na representation allows a scan to e\ufb03ciently retrieve a speci\ufb01ed column of all rows,\nwithout having to retrieve other columns from storage. In contrast, if rows are stored\nin the usual manner, with all column values stored with the row, a sequential scan of\nthe storage would fetch columns that are not required, reducing performance.\nFurther, some key-value stores support the notion of a column family ,w h i c hg r o u p s\nsets of columns into a column family. For a given row, all the columns in a speci\ufb01c col-\numn family are stored together, but columns from other column families are stored\nseparately. If a set of columns are often retrieved together, storing them as a column\nfamily may allow more e\ufb03cient retrieval, as compared to either columnar storage where\nthese are stored and retrieved separately, or a row storage, which could result in retriev-\ning unneeded columns from storage.\n21.7.2 Storing and Retrieving Data\nIn this section, we use the term tablet to refer to partitions, as discussed in Section\n21.3.3. We also use the term tablet server to refer to the node that acts as the server for\na particular tablet; all requests related to a tablet are sent to the tablet server for that\ntablet.5. The tablet server would be one of the nodes that has a replica of the tablet and\nplays the role of master replica as discussed in Section 21.4.1.6\n5HBase uses the terms region andregion server in place of the terms tablet andtablet server\n6In BigTable and HBase, replication is handled by the underly ing distributed \ufb01le system; tablet data are stored in \ufb01les,\nand one of the nodes containing a replica of the tablet \ufb01les is chosen as the tablet server.\n", "1054": "1026 Chapter 21 Parallel and Distributed Storage\nWe use the term master to refer to a site that stores a master copy of the partition\ninformation, including, for each tablet, the key ranges for the tablet, the sites storing\nthe replicas of the tablet, and the current tablet server for that tablet.7The master is\nalso responsible for tracking the health of tablet servers; in case a tablet server node\nfails, the master assigns one of the other nodes that contains a replica of the tablet to\nact as the new tablet server for that tablet. The master is also responsible for reassigning\ntablets to balance the load in the system if some node is overloaded or if a new node is\nadded to the system.\nFor each request coming into the system, the tablet corresponding to the key must\nbe identi\ufb01ed, and the request routed to the tablet server. If a single master site were\nresponsible for this task, it would get overloaded. Instead, the routing task is parallelized\nin one of two ways:\n\u2022By replicating the partition information to the client sites; the key-value store API\nused by clients looks up the partition information copy stored at the client to de-\ncide where to route a request. This approach is used in Bigtable and HBase.\n\u2022By replicating the partition information to a set of router sites, which route requests\nto the site with the appropriate tablet. Requests can be sent to any one of the router\nsites, which forward the request to the cor rect tablet master. This approach is used,\nfor example, in the PNUTS system.\nSince there may be a gap between actually splitting or moving a tablet and updating\nthe partition information at a router (or client), the partition information may be out\nof date when the routing decision is made. When the request reaches the identi\ufb01ed\ntablet master node, the node detects that the tablet has been split, or that the site no\nlonger stores a (master) replica of the tablet. In such a case, the request is returned to\nthe router with an indication that the routing was incorrect; the router then retrieves\nup-to-date tablet mapping information from the master and reroutes the request to the\ncorrect destination.\nFigure 21.8 depicts the architecture of a cloud data-storage system, based loosely\non the PNUTS architecture. Other systems provide similar functionality, although their\narchitecture may vary. For example, Bigtable/HBase do not have separate routers; the\npartitioning and tablet-server mapping information is stored in the Google File Sys-\ntem/HDFS, and clients read the information from the \ufb01le system and decide where to\nsend their requests.\n21.7.2.1 Geographically Distributed Storage\nSeveral key-value stores support replication of data to geographically distributed loca-\ntions; some of these also support partitioning of data across geographically distributed\nlocations, allowing di\ufb00erent partitions to be replicated in di\ufb00erent sets of locations.\n7The term tablet controller is used by PNUTS to refer to the master site.\n", "1055": "21.7 Parallel Key-Value Stores 1027\nRoutersRequests Requests Requests\nTablets\nTablet serversTablet\ncontrollerMaster copy of\npartition table/\ntablet mapping\nFigure 21.8 Architecture of a cloud data storage system.\nOne of the key motivations for geographic distribution is fault tolerance, which\nallows the system to continue functioning even if an entire data center fails due to a\ndisaster such as a \ufb01re or an earthquake; in fact, earthquakes could cause all data centers\nin a region to fail. A second key motivation is to allow a copy of the data to reside at a\ngeographic region close to the user; requiring data to be fetched from across the world\ncould result in latencies of hundreds of milliseconds.\nA key performance issue with geographical replication of data is that the latency\nacross geographical regions is much higher than the latency within a data center. Some\nkey-value stores nevertheless support geographically distributed replication, requiring\ntransactions to wait for con\ufb01rmation of updates from remote locations. Other key-\nvalue stores support asynchronous replication of updates to remote locations, allowing\na transaction to commit without waiting for con\ufb01rmation of updates from a remote\nlocation. There is, however, a risk of loss of updates in case of failure before the updates\nare replicated. Some key-value stores allow the application to choose whether to wait\nfor con\ufb01rmation from remote locations or to commit as soon as updates are performed\nlocally.\nKey-value stores that support geographic replication include Apache Cassandra,\nMegastore and Spanner from Google, Windows Azure storage from Microsoft, and\nPNUTS/Sherpa from Yahoo!, among others.\n", "1056": "1028 Chapter 21 Parallel and Distributed Storage\n21.7.2.2 Index Structure\nThe records in each tablet in a key-value store are indexed on the key; range queries\ncan be e\ufb03ciently supported by storing records clustered on the key. A B+-tree \ufb01le orga-\nnization is a good option, since it supports indexing with clustered storage of records.\nThe widely used key-value stores BigTable and HBase are built on top of distributed\n\ufb01le systems in which \ufb01les are immutable ; that is, \ufb01les cannot be updated once they are\ncreated. Thus B+- t r e ei n d i c e so r\ufb01 l eo r g a n i z a t i o nc a n n o tb es t o r e di ni m m u t a b l e\ufb01 l e s ,\nsince B+-trees require updates, which cannot be done on an immutable \ufb01le.\nI n s t e a d ,t h eB i g T a b l ea n dH B a s es y s t e m su s et h e stepped-merge variant of the log\nstructured merge tree (LSM tree), which we saw in Section 14.8.1, and is described in\nmore detail in Section 24.2. The LSM tree does not perform updates on existing trees,\nbut instead creates new trees either using new data or by merging existing trees. Thus,\nit is an ideal \ufb01t for use on top of distributed \ufb01le systems that only support immutable\n\ufb01les. As an extra bene\ufb01t, the LSM tree supports clustered storage of records, and can\nsupport very high insert and update rates, which has been found very useful in many\napplications of key-value stores. Several key-value stores, such as Apache Cassandra\nand the WiredTiger storage structure used by MongoDB, use the LSM tree structure.\n21.7.3 Support for Transactions\nMost key-value stores o\ufb00er limited support for transactions. For example, key-value\nstores typically support atomic updates on a single data item and ensure that updates\non the data item are serialized, that is, run one after the other. Serializability at the level\nof individual operations is thus trivially satis\ufb01ed, since the operations are run serially.\nNote that serializability at the level of transactions is not guaranteed by serial execution\nof updates on individual data items, since a transaction may access more than one data\nitem.\nSome key-value stores, such as Google\u2019s MegaStore and Spanner, provide full sup-\nport for ACID transactions across multiple nodes. However, most key-value stores do\nnot support transactions across multiple data items.\nSome key-value stores provide a test-and-set operation that can help applications\nimplement limited forms of concurrency control, as we see next.\n21.7.3.1 Concurrency Control\nSome key-value stores, such as the Megastore and Spanner systems from Google, sup-\nport concurrency control via locking. Issues in distributed concurrency control are\ndiscussed in Chapter 23. Spanner also supports versioning and database snapshots\nbased on timestamps. Details of the multiversion concurrency control technique im-\nplemented in Spanner are di scussed in Section 23.5.1.\nHowever, most of the other key-value stores, such as Bigtable, PNUTS /Sherpa, and\nMongoDB, support atomic operations on single data items (which may have multiple\ncolumns, or may be JSON documents in MongoDB).\n", "1057": "21.7 Parallel Key-Value Stores 1029\nSome key-value stores, such as HBase and PNUTS , provide an atomic test-and-set\nfunction, which allows an update to a data item to be conditional on the current version\nof the data item being the same as a speci\ufb01ed version number; the check (test) and the\nupdate (set) are performed atomically. This feature can be used to implement a limited\nform of validation-based concurrency control, as discussed in Section 23.3.7.\nSome data stores support atomic increment operations on data items and atomic\nexecution of stored procedures. For example, HBase supports the incrementColum-\nnValue() operation, which atomically reads and increments a column value, and a\ncheckAndPut() which atomically checks a condition on a data item and updates it\nonly if the check succeeds. HBase also supports atomic execution of stored proce-\ndures, which are called \u201ccoprocessors\u201d in HBase terminology. These procedures run\non a single data item and are executed atomically.\n21.7.3.2 Atomic Commit\nBigTable, HBase, and PNUTS support atomic commit of multiple updates to a single\nrow; however, none of these systems supports atomic updates across di\ufb00erent rows.\nAs one of the results of the above limitation, none of these systems supports sec-\nondary indices; updates to a data item would require updates to the secondary index,\nw h i c hc a n n o tb ed o n ea t o m i c a l l y .\nSome systems, such as PNUTS , support secondary indices or materialized views\nwith deferred updates; updates to a data item result in updates to the secondary index\nor materialized view being added to a messaging service to be delivered to the node\nwhere the update needs to be applied. These updates are guaranteed to be delivered\nand applied subsequently; however, until they are applied, the secondary index may be\ninconsistent with the underlying data. View maintenance is also supported by PNUTS\nin the same deferred fashion. There is no transactional guarantee on the updates of\nsuch secondary indices or materialized views, and only a best-e\ufb00ort guarantee in terms\nof when the updates reach their destination. Consistency issues with deferred mainte-\nnance are discusse d in Section 23.6.3.\nIn contrast, the Megastore and Spanner systems developed by Google support\natomic commit for transactions spanning multiple data items, which can be spread\nacross multiple nodes. These systems use two-phase commit (discussed in Section 23.2)\nto ensure atomic commit across multiple nodes.\n21.7.3.3 Dealing with Failures\nIf a tablet server node fails, another node that has a copy of the tablet should be assigned\nthe task of serving the tablet. The master node is responsible for detecting node failures\nand reassigning tablet servers.\nWhen a new node takes over as tablet server, it must recover the state of the tablet.\nTo ensure that updates to the tablet survive node failures, updates to a tablet are logged,\nand the log is itself replicated. When a site fails, the tablets at the site are assigned to\n", "1058": "1030 Chapter 21 Parallel and Distributed Storage\nother sites; the new master site of each tablet is responsible for performing recovery\nactions using the log to bring its copy of the tablet to an up-to-date state, after which\nupdates and reads can be performed on the tablet.\nIn Bigtable, as an example, mapping information is stored in an index structure,\nand the index, as well as the actual tablet data, are stored in the \ufb01le system. Tablet data\nupdates are not \ufb02ushed immediately, but log data are. The \ufb01le system ensures that the\n\ufb01le system data are replicated and will be available even in the face of failure of a few\nnodes in the cluster. Thus, when a tablet is reassigned, the new server for that tablet\nhas access to up-to-date log data.\nYahoo!\u2019s Sherpa/ PNUTS system, on the other hand, explicitly replicates tablets to\nmultiple nodes in a cluster and uses a persistent messaging system to implement the\nlog. The persistent messaging system replicates log records at multiple sites to ensure\navailability in the event of a failure. When a new node takes over as the tablet server,\nit must apply any pending log records that were generated by the earlier tablet server\nbefore taking over as the tablet server.\nTo ensure availability in the face of failures, data must be replicated. As noted in\nSection 21.4.2, a key issue with replication is the task of keeping the replicas consis-\ntent with each other. Di\ufb00erent systems implement atomic update of replicas in di\ufb00er-\nent fashions. Google BigTable and Apache HBase use replication features provided by\nan underlying \ufb01le system (GFS for BigTable, and HDFS for HBase), instead of im-\nplementing replication on their own. Interestingly, neither GFS nor HDFS supports\natomic updates of all replicas of a \ufb01le; instead they support appends to \ufb01les, which are\ncopied to all replicas of the \ufb01le blocks. An append is successful only when it has been\napplied to all replicas. System failures can result in appends that are applied to only\nsome replicas; such incomplete appends are detected using sequence numbers and are\ncleaned up when they are detected.\nSome systems such as PNUTS use a persistent messaging service to log updates;\nthe messaging service guarantees that updates will be delivered to all replicas. Other\nsystems, such as Google\u2019s Megastore and Spanner, use a technique called distributed\nconsensus to implement consistent replication, as we discuss in Section 23.8. Such\nsystems require a majority of replicas to be available to perform an update. Other sys-\ntems, such as Apache Cassandra and MongoDB, allow the user control over how many\nreplicas must be available to perform an update. Setting the value low could result in\ncon\ufb02icting updates, which must be resolved later. We discuss these issues in Section\n23.6.\n21.7.4 Managing Without Declarative Queries\nKey-value stores do not provide any query processing facility, such as SQL language\nsupport, or even lower-level primitives such as joins. Many applications that use key-\nvalue stores can manage without query language support. The primary mode of data\naccess in such applications is to store data with an associated key and to retrieve data\n", "1059": "21.7 Parallel Key-Value Stores 1031\nwith that key. In the user pro\ufb01le example, the key for user-pro\ufb01le data would be the\nuser\u2019s identi\ufb01er.\nThere are applications that require joins but implement the joins either in appli-\ncation code or by a form of view materialization. For example, in a social-networking\napplication, each user should be shown new posts from all her friends, which concep-\ntually requires a join.\nOne approach to computing the join is to implement it in the application code,\nby \ufb01rst \ufb01nding the set of friends of a given user, and then querying the data object\nrepresenting each friend, to \ufb01nd their recent posts.\nAn alternative approach is as follows: Whenever a user makes a post, for each\nfriend of the user a message is sent to, the data object representing that friend and the\ndata associated with the friend are updated with a summary of the new post. When\nthat user checks for updates, all required data are available in one place and can be\nretrieved quickly.\nBoth approaches can be used without any underlying support for joins. There are\ntrade-o\ufb00s between the two alternatives such as higher cost at query time for the \ufb01rst\nalternative versus higher storage cost and higher cost at the time of writes for the second\nalternative.\n21.7.5 Performance Optimizations\nWhen using a data storage system, the physical location of data are decided by the stor-\nage system and hidden from the client. When storing multiple relations that need to be\njoined, partitioning each independently may be suboptimal in terms of communication\ncost. For example, if the join of two relations is computed frequently, it may be best if\nthey are partitioned in exactly the same way, on their join attributes. As we will see in\nSection 22.7.4, doing so would allow the join to be computed in parallel at each storage\nsite, without data transfer.\nTo support such scenarios, some data storage systems allow the schema designer\nto specify that tuples of one relation should be stored in the same partitions as tuples\nof another relation that they reference, typically using a foreign key. A typical use of\nthis functionality is to store all tuples related to a particular entity together in the same\npartition; the set of such tuples is called an entity group .\nFurther, many data storage systems, such as HBase, support stored functions or\nstored procedures . Stored functions/procedures allow clients to invoke a function on\na tuple (or an entity group) and instead of the tuples being fetched and executed lo-\ncally, the function is executed at the partition where the tuple is stored. Stored func-\ntions/procedures are particularly useful if the stored tuples are large, while the func-\ntion/procedure results are small, reducing data transfer.\nMany data storage systems provide features such as support for automatically delet-\ning old versions of data items after some period of time, or even deleting data items\nthat are older than some speci\ufb01ed period.\n", "1060": "1032 Chapter 21 Parallel and Distributed Storage\n21.8 Summary\n\u2022Parallel databases have gained signi\ufb01cant commercial acceptance in the past 20\nyears.\n\u2022Data storage and indexing are two important aspects of parallel database systems.\n\u2022Data partitioning involves the distrib ution of data among multiple nodes. In I/O\nparallelism, relations are partitioned among available disks so that they can be re-\ntrieved faster. Three commonly used partitioning techniques are round-robin par-\ntitioning, hash partitioning, and range partitioning.\n\u2022Skew is a major problem, especially with increasing degrees of parallelism. Bal-\nanced partitioning vectors, using histog rams, and virtual node partitioning are\namong the techniques used to reduce skew.\n\u2022Parallel data storage systems must be resilient to failure of nodes. To ensure that\ndata are not lost on node failure, tuples are replicated across at least two nodes,\nand often three nodes. If a node fails, the tuples that it stored can still be accessed\nfrom the other nodes where the tuples are replicated.\n\u2022Indices in a parallel data storage system can be divided into two kinds: local and\nglobal. A local index is an index built on tuples stored in a particular node; The\nindex contents are stored on the same node as the data. A global index is an index\nbuilt on data stored across multiple nodes.\n\u2022A distributed \ufb01le system stores \ufb01les across a large collection of machines, while\ngiving a single-\ufb01le-system view to clients. As with any \ufb01le system, there is a system\nof \ufb01le names and directories, which clients can use to identify and access \ufb01les.\nClients do not need to bother about where the \ufb01les are stored.\n\u2022Web applications need to store very large numbers (many billions) of relatively\nsmall records (of size ranging from a few kilobytes to a few megabytes). A number\nof parallel key-value storage systems were developed to meet the needs of such\nweb applications. A key-value store provides a way to store or update a data item\n(value) with an associated key, and to retrieve the data item with a given key.\nReview Terms\n\u2022Key-value stores\n\u2022Data storage system\n\u2022I/Oparallelism\n\u2022Data partitioning\n\u2022Horizontal partitioning\u2022Partitioning strategies\n\u00b0Round-robin\n\u00b0Hash partitioning\n\u00b0Range partitioning\n\u2022Partitioning vector\n", "1061": "Practice Exercises 1033\n\u2022Point queries\n\u2022Range queries\n\u2022Skew\n\u00b0Execution skew\n\u00b0Data distribution skew\n\u00b0Attribute-value skew\n\u00b0Partition skew\n\u00b0Execution skew\n\u2022Handling of skew\n\u00b0Balanced range-partitioning\nvector\n\u00b0Histogram\n\u00b0Virtual nodes\n\u2022Elasticity of storage\n\u2022Table\n\u2022Tablets\n\u2022Partition table\n\u2022Master node\u2022Routers\n\u2022Consistent hashing\n\u2022Distributed hash tables\n\u2022Replication\n\u00b0Replication within a data center\n\u00b0Replication across data center\n\u00b0Master replicas\n\u00b0Consistency of replicas\n\u2022Eventual consistency\n\u2022Global primary index\n\u2022Global secondary index\n\u2022Distributed \ufb01le system\n\u2022Write-once-read-many access model\n\u2022Key-value store\n\u2022Wide-column stores\n\u2022Document stores\n\u2022Column family\n\u2022Tablet server\nPractice Exercises\n21.1 In a range selection on a range-partitioned attribute, it is possible that only\none disk may need to be accessed. Describe the bene\ufb01ts and drawbacks of this\nproperty.\n21.2 Recall that histograms are used for constructing load-balanced range parti-\ntions.\na. Suppose you have a histogram where values are between 1 and 100, and\nare partitioned into 1 0 ranges, 1\u20131 0, 1 1\u201320, \u2026, 9 1\u20131 00, with frequen-\ncies 15, 5, 20, 10, 10, 5, 5, 20, 5, and 5, respectively. Give a load-balanced\nrange partitioning function to divide the values into \ufb01ve partitions.\nb. Write an algorithm for computing a balanced range partition with ppar-\ntitions, given a histogram of frequency distributions containing nranges.\n21.3 Histograms are traditionally construc ted on the values of a speci\ufb01c attribute\n(or set of attributes) of a relation. Such histograms are good for avoiding data\n", "1062": "1034 Chapter 21 Parallel and Distributed Storage\ndistribution skew but are not very useful for avoiding execution skew. Explain\nwhy.\nNow suppose you have a workload of queries that perform point lookups.\nExplain how you can use the queries in the workload to come up with a parti-\ntioning scheme that avoids execution skew.\n21.4 Replication:\na. Give two reasons for replicating data across geographically distributed\ndata centers.\nb. Centralized databases support replication using log records. How is\nthe replication in centralized databases di\ufb00erent from that in paral-\nlel/distributed databases?\n21.5 Parallel indices:\na. Secondary indices in a centralized database store the record identi\ufb01er.\nA global secondary index too could potentially store a partition num-\nber holding the record, and a record identi\ufb01er within the partition. Why\nwould this be a bad idea?\nb. Global secondary indices are implemented in a way similar to local sec-\nondary indices that are used when records are stored in a B+-tree \ufb01le\norganization. Explain the similarities between the two scenarios that re-\nsult in a similar implementation of the secondary indices.\n21.6 Parallel database systems store replicas of each data item (or partition) on\nmore than one node.\na. Why is it a good idea to distribute the copies of the data items allocated\nto a node across multiple other nodes, instead of storing all the copies\nin the same node (or set of nodes).\nb. What are the bene\ufb01ts and drawbacks of using RAID storage instead of\nstoring an extra copy of each data item?\n21.7 Partitioning and replication.\na. Explain why range-partitioning give s better control on tablet sizes than\nhash partitioning. List an analogy between this case and the case of B+-\ntree indices versus hash indices.\nb. Some systems \ufb01rst perform hashing on the key, and then use range par-\ntitioning on the hash values. What could be a motivation for this choice,\nand what are its drawbacks as compared to performing range partition\ndirection on the key?\nc. It is possible to horizontally partition data, and then perform vertical\npartitioning locally at each node. It is also possible to do the converse,\n", "1063": "Practice Exercises 1035\nwhere vertical partitioning is done \ufb01rst, and then each partition is then\nhorizontally partitioned independently. What are are the bene\ufb01ts of the\n\ufb01rst option over the second one?\n21.8 In order to send a request to the master replica of a data item, a node must\nknow which replica is the master for that data item.\na. Suppose that between the time the node identi\ufb01es which node is the\nmaster replica for a data item, and the time the request reaches the iden-\nti\ufb01ed node, the mastership has changed, and a di\ufb00erent node is now the\nm a s t e r .H o wc a ns u c has i t u a t i o nb ed e a l tw i t h ?\nb. While the master replica could be chosen on a per-partition basis, some\nsystems support a per-record master replica , where the records of a par-\ntition (or tablet) are replicated at some set of nodes, but each record\u2019s\nmaster replica can be on any of the nodes from within this set of nodes,\nindependent of the master replica of other records. List two bene\ufb01ts of\nkeeping track of master on a per-record basis.\nc. Suggest how to keep track of the master replica for each record, when\nthere are a large number of records.\nExercises\n21.9 For each of the three partitioning techniques, namely, round-robin, hash par-\ntitioning, and range partitioning, give an example of a query for which that\npartitioning technique would provide the fastest response.\n21.10 What factors could result in skew when a relation is partitioned on one of its\nattributes by:\na. Hash partitioning?\nb. Range partitioning?\nIn each case, what can be done to reduce the skew?\n21.11 What is the motivation for storing related records together in a key-value store?\nExplain the idea using the notion of an entity group.\n21.12 Why is it easier for a distributed \ufb01le system such as GFS orHDFS to support\nreplication than it is for a key-value store?\n21.13 Joins can be expensive in a key-value store, and di\ufb03cult to express if the system\ndoes not support SQL or a similar declarative query language. What can an\napplication developer do to e\ufb03ciently get results of join or aggregate queries\nin such a setting?\n", "1064": "1036 Chapter 21 Parallel and Distributed Storage\nTools\nA wide variety of open-source Big Data tools are available, in addition to some com-\nmercial tools. In addition, a number of these tools are available on cloud platforms.\nGoogle File System ( GFS) was an early generation parallel \ufb01le system. Apache HDFS\n(hadoop.apache.org ) is a widely used distributed \ufb01le system implementation mod-\neled after GFS.HDFS by itself does not de\ufb01ne any internal format for \ufb01les, but Hadoop\nimplementations today support several optimized \ufb01le formats such as Sequence \ufb01les\n(which allow binary data), Avro (which supports semi-structured schemas) and Par-\nquet and Orc (which support columnar data representation). Hosted cloud storage\nsystems include the Amazon S3 storage system ( aws.amazon.com/s3 ) and Google\nCloud Storage ( cloud.google.com/storage ).\nGoogle\u2019s Bigtable was an early generation parallel data storage system, ar-\nchitected as a layer on top of GFS. Amazon\u2019s Dynamo is an early generation\nparallel key-value store which is based on the idea of consistent hashing, de-\nveloped initially for peer-to-peer data storage. Both are available hosted on the\ncloud as Google Bigtable ( cloud.google.com/bigtable ) and Amazon DynamoDB\n(aws.amazon.com/dynamodb ). Google Spanner ( cloud.google.com/spanner )i s\na hosted storage system that provides extensive transactional support. Apache\nHBase ( hbase.apache.org ) is a widely used open-source data storage sys-\ntem which is based on Bigtable and is implemented as a layer on top of\nHDFS . Apache Cassandra ( cassandra.apache.org ) which was developed at Face-\nbook, Voldemort ( www.project-voldemort.com ) developed at LinkedIn, MongoDB\n(www.mongodb.com ), CouchDB ( couchdb.apache.org )a n dR i a k( basho.com )a r e\nall open-source key-value stores. MongoDB and CouchDB use the JSON format for\nstoring data. Aerospike ( www.aerospike.com )i sa no p e n - s o u r c ed a t as t o r a g es y s t e m\noptimized for Flash storage. There are many other open-source parallel data storage\nsystems available today.\nCommercial parallel database systems include Teradata, Teradata Aster Data, IBM\nNetezza, and Pivotal Greenplum. IBM Netezza, Pivotal Greenplum, and Teradata Aster\nData all use Postgre SQL as the underlying database, running independently on each\nnode; each of these systems builds a layer on top, to partition data, and parallelize\nquery processing across the nodes.\nFurther Reading\nIn the late 1970s and early 1980s, as the relational model gained reasonably sound foot-\ning, people recognized that relational operators are highly parallelizable and have good\ndata\ufb02ow properties. Several research projects, including GAMMA ([DeWitt (1990)]),\nXPRS ([Stonebraker et al. (1988)]), and Volca no ([Graefe (1990)]) were launched to\ninvestigate the practicality of parallel storage of data and parallel execution of queries.\nTeradata was one of the \ufb01rst commercial shared-nothing parallel database systems\ndesigned for decision support systems, and it continues to have a large market share.\n", "1065": "Further Reading 1037\nTeradata supports partitioning and replication of data to deal with node failures. The\nRed Brick Warehouse was another early parallel database system designed for decision\nsupport (Red Brick was bought by Informix, and later IBM).\nInformation on the Google \ufb01le system can be found in [Ghemawat et al. (2003)],\nwhile the Google Bigtable system is described in [Chang et al. (2008)]. The Yahoo!\nPNUTS system is described in [Cooper et al. (2008)], while Google Megastore and\nGoogle Spanner are described in [Baker et al. (2011)] and [Corbett et al. (2013)] re-\nspectively. Consistent hashing is described in [Karger et al. (1997)], while Dynamo,\nwhich is based on consistent hashing, is described in [DeCandia et al. (2007)].\nBibliography\n[Baker et al. (2011)] J. Baker, C. Bond, J. C. Corbett, J. J. Furman, A. Khorlin, J. Larson,\nJ.-M. Leon, Y. Li, A. Lloyd, and V. Yushprakh, \u201cMegastore: Providing Scalable, Highly Avail-\nable Storage for Interactive Services\u201d, In Proceedings of the Conference on Innovative Data\nsystem Research (CIDR) (2011), pages 223\u2013234.\n[Chang et al. (2008)] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Bur-\nrows, T. Chandra, A. Fikes, and R. E. Gruber, \u201cBigtable: A Distributed Storage System for\nStructured Data\u201d, ACM Trans. Comput. Syst. , Volume 26, Number 2 (2008).\n[Cooper et al. (2008)] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bo-\nhannon, H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni, \u201cPNUTS: Yahoo!\u2019s Hosted\nData Serving Platform\u201d, Proceedings of the VLDB Endowment , Volume 1, Number 2 (2008),\npages 1277\u20131288.\n[Corbett et al. (2013)] J. C. Corbett et al., \u201cSpanner: Google\u2019s Globally Distributed\nDatabase\u201d, ACM Trans. on Computer Systems , Volume 31, Number 3 (2013).\n[DeCandia et al. (2007)] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Laksh-\nman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels, \u201cDynamo: Amazons Highly\nAvailable Key-value Store\u201d, In Proc. of the ACM Symposium on Operating System Principles\n(2007), pages 205\u2013220.\n[DeWitt (1990)] D. DeWitt, \u201cThe Gamma Database Machine Project\u201d, IEEE Transactions\non Knowledge and Data Engineering , Volume 2, Number 1 (1990), pages 44\u201362.\n[Ghemawat et al. (2003)] S. Ghemawat, H. Gobio\ufb00, and S.-T. Leung, \u201cThe Google File Sys-\ntem\u201d, Proc. of the ACM Symposium on Operating System Principles (2003).\n[Graefe (1990)] G. Graefe, \u201cEncapsulation of Parallelism in the Volcano Query Processing\nSystem\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data (1990), pages 102\u2013111.\n[Karger et al. (1997)] D. Karger, E. Lehman, T. Leighton, R. Panigrahy, M. Levine, and\nD. Lewin, \u201cConsistent Hashing and Random Trees: Distributed Caching Protocols for Re-\nlieving Hot Spots on the World Wide Web\u201d, In Proc. of the ACM Symposium on Theory of\nComputing (1997), pages 654\u2013663.\n", "1066": "1038 Chapter 21 Parallel and Distributed Storage\n[Stonebraker et al. (1988)] M. Stonebraker, R. H. Katz, D. A. Patterson, and J. K. Ouster-\nhout, \u201cThe Design of XPRS\u201d, In Proc. of the International Conf. on Very Large Databases\n(1988), pages 318\u2013330.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "1067": "CHAPTER22\nParallel and Distributed Query\nProcessing\nIn this chapter, we discuss algorithms for query processing in parallel database sys-\ntems. We assume that the queries are read only , and our focus is on query processing in\ndecision support systems. Such systems need to execute queries on very large amounts\nof data, and parallel processing of the query across multiple nodes is critical for pro-\ncessing queries within acceptable response times.\nOur focus in the early parts of this chapter is on relational query processing. How-\never, later in the chapter, we examine issues in parallel processing of queries expressed\nin models other than the relational model.\nTransaction processing systems execute large numbers of queries that perform up-\ndates, but each query a\ufb00ects only a small number of tuples. Parallel execution is key\nto handle large transaction processing loads; however, this topic is covered in Chapter\n23.\n22.1 Overview\nParallel processing can be exploited in two distinct ways in a database system. One\napproach is interquery parallelism , which refers to the execution of multiple queries\nin parallel with each other, across multiple nodes. The second approach is intraquery\nparallelism , which refers to the processing of di\ufb00erent parts of the execution of a single\nquery, in parallel across multiple nodes.\nInterquery parallelism is essential for transaction processing systems. Transaction\nthroughput can be increased by this form of parallelism. However, the response times\nof individual transactions are no faster than they would be if the transactions were run\nin isolation. Thus, the primary use of interquery parallelism is to scale up a transaction-\nprocessing system to support a larger numbe r of transactions per second. Transaction\nprocessing systems are considered in Chapter 23.\nIn contrast, intraquery parallelism is essential for speeding up long-running queries,\nand it is the focus of this chapter.\n1039\n", "1068": "1040 Chapter 22 Parallel and Distributed Query Processing\nExecution of a single query involves execution of multiple operations, such as se-\nlects, joins, or aggregate operations. The key to exploiting large-scale parallelism is to\nprocess each operation in parallel, across multiple nodes. Such parallelism is referred to\nasintraoperation parallelism . Since the number of tuples in a relation can be large, the\ndegree of intraoperation parallelism is also potentially very large; thus, intraoperation\nparallelism is natural in a database system.\nTo illustrate the parallel evaluation of a query, consider a query that requires a rela-\ntion to be sorted. Suppose that the relation has been partitioned across multiple disks\nby range partitioning on some attribute, and the sort is requested on the partitioning\nattribute. The sort operation can be implemented by sorting each partition in parallel,\nthen concatenating the sorted partitions to get the \ufb01nal sorted relation. Thus, we can\nparallelize a query by parallelizing individual operations.\nThere is another source of parallelism in evaluating a query: The operator tree for\na query can contain multiple operations. We can parallelize the evaluation of the op-\nerator tree by evaluating in parallel some of the operations that do not depend on one\nanother. Further, as Chapter 15 mentions, we may be able to pipeline the output of\none operation to another operation. The two operations can be executed in parallel\non separate nodes, one generating output that is consumed by the other, even as it is\ngenerated. Both these forms of parallelism are examples of interoperation parallelism ,\nwhich allows di\ufb00erent operators of a query to be executed in parallel.\nIn summary, the execution of a single query can be parallelized in two di\ufb00erent\nways:\n\u2022Intraoperation parallelism , which we consider in detail in the next few sections,\nwhere we study parallel implementations of common relational operations such\nas sort, join, aggregate and other operations.\n\u2022Interoperation parallelism , which we consider in detail in Section 22.5.1.\nThe two forms of parallelism are complementary and can be used simultaneously\non a query. Since the number of operations in a typical query is small, compared to the\nnumber of tuples processed by each operation, intraoperation parallelism can scale bet-\nter with increasing parallelism. However, interoperation parallelism is also important,\nespecially in shared memory systems with multiple cores.\nTo simplify the presentation of the algorithms, we assume a shared nothing ar-\nchitecture with nnodes, N1,N2,\u2026,Nn. Each node may have one or more disks, but\ntypically the number of such disks is small. We do not address how to partition the\ndata between the disks at a node; RAID organizations can be used with these disks to\nexploit parallelism at the storage level, rather than at the query processing level.\nThe choice of algorithms for parallelizing query evaluation depends on the ma-\nchine architecture. Rather than present algorithms for each architecture separately, we\nuse a shared-nothing architecture in our description. Thus, we explicitly describe when\ndata have to be transferred from one node to another.\n", "1069": "22.2 Parallel Sort 1041\nWe can simulate this model easily by using the other architectures, since transfer of\ndata can be done via shared memory in a shared-memory architecture, and via shared\ndisks in a shared-disk architecture. Hence, algorithms for shared-nothing architectures\ncan be used on the other architectures too. In Section 22.6, we discuss how some of\nthe algorithms can be further optimized for shared-memory systems.\nCurrent-generation parallel systems are typically based on a hybrid architecture,\nwhere each computer has multiple cores with a shared memory, and there are multiple\ncomputers organized in a shared-nothing fashion. For the purpose of our discussion,\nwith such an architecture, each core can be considered a node in a shared-nothing\nsystem. Optimizations to exploit the fact that some of the cores share memory with\nother cores can be performed as discussed in Section 22.6.\n22.2 Parallel Sort\nSuppose that we wish to sort a relation rthat resides on nnodes N1,N2,\u2026,Nn.I ft h e\nrelation has been range-partitioned on the attributes on which it is to be sorted, we can\nsort each partition separately and concatenate the results to get the full sorted relation.\nSince the tuples are partitioned on nnodes, the time required for reading the entire\nrelation is reduced by a factor of nby the parallel access.\nIf relation rhas been partitioned in any other way, we can sort it in one of two\nways:\n1.We can range-partition ron the sort attributes, and then sort each partition sep-\narately.\n2.We can use a parallel version of the external sort-merge algorithm.\n22.2.1 Range-Partitioning Sort\nRange-partitioning sort , shown pictorially in Figure 22.1a, works in two steps: \ufb01rst\nrange-partitioning the relation, then sorting each partition separately. When we sort\nby range-partitioning the relation, it is not necessary to range-partition the relation on\nthe same set of nodes as those on which that relation is stored. Suppose that we choose\nnodes N1,N2,\u2026,Nmto sort the relation. There are two steps involved in this operation:\n1.Redistribute the tuples in the relation, using a range-partition strategy, so that all\ntuples that lie within the ith range are sent to node ni, which stores the relation\ntemporarily on its local disk.\nTo implement range partitioning, in parallel every node reads the tuples from\nits disk and sends each tuple to its destination node based on the partition func-\ntion. Each node N1,N2,\u2026,Nmalso receives tuples belonging to its partition and\nstores them locally. This step requires disk I/Oand network communication.\n", "1070": "1042 Chapter 22 Parallel and Distributed Query Processing\nr1\nr2\nr3Local Sort\nLocal Sort\nLocal Sort\nLocal Sort\n1. Range Partition 2. Local SortMerge\nMerge\nMerge\nMerge\n1. Local Sort 2. Range Partition and Merge\n(a) Range Partitioning Sort (b) Parallel External Sort-Mergernr1\nr2\nr3\nrnr1\nr2\nr3\nrLocal Sort\nLocal Sort\nLocal Sort\nLocal Sort\u2018\n\u2018\n\u2018\n\u2018 \u2018 mr1\nr2\nr3\nr\u2018\n\u2018\n\u2018\nm\nFigure 22.1 Parallel sorting algorithms.\n2.Each of the nodes sorts its partition of the relation locally, without interaction\nwith the other nodes. Each node executes the same operation\u2014namely, sorting\u2014\non a di\ufb00erent data set. (Execution of the same operation in parallel on di\ufb00erent\nsets of data are called data parallelism .)\nThe \ufb01nal merge operation is trivial, because the range partitioning in the \ufb01rst\nphase ensures that, for 1 \u2264i<j\u2264m,t h ek e yv a l u e si nn o d e Niare all less than\nthe key values in Nj.\nWe must do range partitioning with a balanced range-partition vector so that each\npartition will have approximately the same number of tuples. We saw how to create such\npartition vectors in Section 21.3.1. Virtual node partitioning, as discussed in Section\n21.3.2, can also be used to reduce skew. Recall that there are several times as many\nvirtual nodes as real nodes, and virtual node partitioning creates a partition for each\nvirtual node. Virtual nodes are then mapped to real nodes; doing so in a round-robin\nf a s h i o nt e n d st os p r e a d sv i r t u a ln o d e sa c r o s sr e a ln o d e si naw a yt h a tr e d u c e st h ed e g r e e\nof skew at real nodes.\n22.2.2 Parallel External Sort-Merge\nParallel external sort-merge , shown pictorially in Figure 22.1b, is an alternative to range\npartitioning sort. Suppose that a relation has already been partitioned among nodes\nN1,N2,\u2026,Nn(it does not matter how the relation has been partitioned). Parallel ex-\nternal sort-merge then works this way:\n1.Each node Nisorts the data available at Ni.\n2.The system then merges the sorted runs on each node to get the \ufb01nal sorted\noutput.\n", "1071": "22.3 Parallel Join 1043\nThe merging of the sorted runs in step 2 can be parallelized by this sequence of\nactions:\n1.The system range-partitions the sorted partitions at each node Ni(all by the same\npartition vector) across the nodes N1,N2,\u2026,Nm.I ts e n d st h et u p l e si ns o r t e d\norder, so each node receives the tuples as sorted streams.\n2.Each node Niperforms a merge on the streams of tuples as they are received to\nget a single sorted run.\n3.The system concatenates the sorted runs on nodes N1,N2,\u2026,Nmto get the \ufb01nal\nresult.\nAs described, this sequence of actions results in an interesting form of execution skew ,\nsince at \ufb01rst every node sends all tuples of partition 1 to N1, then every node sends all\ntuples of partition 2 to N2, and so on. Thus, while sending happens in parallel, receiving\ntuples becomes sequential: First only N1receives tuples, then only N2receives tuples,\nand so on. To avoid this problem, the sorted sequence of tuples Si,jfrom any node i\ndestined to any other node jis broken up into multiple blocks. Each node Nisends\nthe \ufb01rst block of tuples from Si,jnode Nj,f o re a c h j; it then sends the second block of\ntuples to each node Nj,a n ds oo n ,u n t i la l lb l o c k sh a v eb e e ns e n t .A sar e s u l t ,a l ln o d e s\nreceive data in parallel. (Note that tuples are sent in blocks, rather than individually,\nto reduce network overheads.)\n22.3 Parallel Join\nParallel join algorithms attempt to divide the tuples of the input relations over several\nnodes. Each node then computes part of the join locally. Then, the system collects the\nresults from each node to produce the \ufb01nal result. How exactly to divide the tuples\ndepends on the join algorithm, as we see next.\n22.3.1 Partitioned Join\nFor certain kinds of joins, it is possible to partition the two input relations across the\nnodes and to compute the join locally at each node. The partitioned join technique\ncan be used for inner joins, where the j oin condition is an equi-join (e.g., r\u22c8r.A=s.Bs);\nthe relations randsare partitioned by the same partitioning function on their join\nattributes. The idea of partitioning is exactly the same as that behind the partitioning\nstep of hash join. Partitioned join can also be used for outer joins, as we shall see\nshortly.\nSuppose that we are using mnodes to perform the join, and that the relations to\nbe joined are rands.Partitioned join then works this way: The system partitions the\nrelations randseach into mpartitions, denoted r1,r2,\u2026,rmands1,s2,\u2026,sm.I na\npartitioned join, however, there are two di\ufb00erent ways of partitioning rands:\n", "1072": "1044 Chapter 22 Parallel and Distributed Query Processing\n\u2022Range partitioning on the join attributes.\n\u2022Hash partitioning on the join attributes.\nIn either case, the same partitioning function must be used for both relations. For\nrange partitioning, the same partition vector must be used for both relations. For hash\npartitioning, the same hash function must be used on both relations. Figure 22.2 depicts\nthe partitioning in a partitioned parallel join.\nThe partitioned join algorithm \ufb01rst partitions one of the relations by scanning its\ntuples and sending them to the appropriate n ode based on the partition function and\nthe join attribute values of each tuple. Speci\ufb01cally, each node Nireads in the tuples of\none of the relations, say r, from local disk, computes for each tuple tthe partition rj\nto which tbelongs, and sends the tuple tto node Nj. Each node also simultaneously\nreceives tuples that are sent to it and stores them on its local disk (this can be done by\nhaving separate threads for sending and receiving data). The process is repeated for all\ntuples from the other relation, s.\nOnce both relations are partitioned, we can use any join technique locally at each\nnode Nito compute the join of riandsi. Thus, we can use partitioning to parallelize\nany join technique.\nPartitioned join can be used not only for inner joins, but also for all three forms\nof outer join (left, right and full outer join). Each node computes the corresponding\nouter join locally, after partitioning is done on the join attributes. Further, since natural\njoin can be expressed as an equijoin followed by a projection, natural joins can also be\ncomputed using partitioned join.\nIf one or both of the relations randsare already partitioned on the join attributes\n(by either hash partitioning or range partitioning), the work needed for partitioning\nis reduced greatly. If the relations are not p artitioned or are partitioned on attributes\nother than the join attributes, then the tuples need to be repartitioned.\nStep 1: Partition r          Step 2: Partition  s\nStep 3: Each node Ni computes ri      si\u0384 \u0384ss 3 s 2 s 1 \n n rm \u0384\u0384 sm s3 \u0384s2 \u0384s1 \u0384\nr3 \u0384r2 \u0384r1 \u0384 r1\nr2\nr3\nrn\nFigure 22.2 Partitioned parallel join.\n", "1073": "22.3 Parallel Join 1045\nWe now consider issues speci\ufb01c to the join technique used locally at each node Ni.\nThe local join operation can be optimized by performing some initial steps on tuples\nas they arrive at a node, instead of \ufb01rst storing the tuples to disk and then reading\nthem back to perform these initial steps. These optimizations, which we describe be-\nlow, are also used in nonparallel query processing, when results of an earlier operation\nare pipelined into a subsequent operation; thus, they are not speci\ufb01c to parallel query\nprocessing.\n\u2022If we use hash join locally, the resultant parallel join technique is called partitioned\nparallel hash join .\nRecall that hash join \ufb01rst partitions both input relations into smaller pieces\nsuch that each partition of the smaller relation (the build relation) \ufb01ts into mem-\nory. Thus, to implement hash join, the partitions riand sireceived by node Ni\nmust be repartitioned using a hash function, say h1(). If the partitioning of rands\nacross the nodes was done by using a hash function h0(), the system must ensure\nthat h1() is di\ufb00erent from h0 ( ) .L e tt h er e s u l t a n tp a r t i t i o n sa tn o d e Niberi,jand\nsi,jforj=1\u2026ni,w h e r e nidenotes the number of local partitions at node Ni.\nNote that the tuples can be repartitioned based on the hash function used for\nthe local hash join as they arrive and written out to the appropriate partitions,\navoiding the need to write the tuples to disk and read them back in.\nRecall also that hash join then loads each partition of the build relation into\nmemory, builds an in-memory index on the join attributes, and \ufb01nally probes the\nin-memory index using each tuple of the other relation, called the probe relation.\nAssume that relation sis chosen as the build relation. Then each partition si,jis\nloaded in memory, with an index built on the join attributes, and the index is\nprobed with each tuple of ri,j.\nHybrid hash join (described in Section 15.5.5.5) can be used in case the parti-\ntions of one of the relations are small enough that a signi\ufb01cant part of the partition\n\ufb01ts in memory at each node. In this case, the smaller relation, say s,w h i c hi su s e d\nas the build relation, should be partitioned \ufb01rst, followed by the larger relation, say\nr, which is used as the probe relation. Recall that with hybrid hash join, the tuples\nin the partition s0of the build relation sare retained in memory, and an in-memory\nindex is built on these tuples. When the probe relation tuples arrive at the node,\nthey are also repartitioned; tuples in the r0partition are used directly to probe the\nindex on the s0tuples, instead of being written out to disk and read back in.\n\u2022If we use merge join locally, the resultant technique is called partitioned parallel\nmerge join . Each of the partitions siandrimust be sorted, and merged locally, at\nnode Ni.\nThe \ufb01rst step of sorting, namely, run generation, can directly consume incom-\ning tuples to generate runs, avoiding a write to disk before run generation.\n\u2022If we use nested-loops or indexed nested-l oops join locally, the resultant technique\nis called partitioned parallel nested-loop join orpartitioned parallel indexed nested-\n", "1074": "1046 Chapter 22 Parallel and Distributed Query Processing\nloops join .E a c hn o d e Niperforms a nested-loops (or indexed nested-loops) join\nonsiandri.\n22.3.2 Fragment-and-Replicate Join\nPartitioning is not applicable to all types of joins. For instance, if the join condition is\nan inequality, such as r\u22c8r.a<s.bs, it is possible that all tuples in rjoin with some tuple\nins(and vice versa). Thus, there may be no nontrivial way of partitioning randsso\nthat tuples in partition rijoin with only tuples in partition si.\nWe can parallelize such joins by using a technique called fragment-and-replicate .\nWe \ufb01rst consider a special case of fragment-and-replicate\u2014 asymmetric fragment-and-\nreplicate join \u2014 w h i c hw o r k sa sf o l l o w s :\n1.The system partitions one of the relations\u2014say, r. Any partitioning technique can\nbe used on r, including round-robin partitioning.\n2.The system replicates the other relation, s, across all the nodes.\n3.Node Nithen locally computes the join of riwith all of s, using any join technique.\nThe asymmetric fragment-and-replicate scheme appears in Figure 22.3a. If ris already\nstored by partitioning, there is no need to partition it further in step 1. All that is\nrequired is to replicate sacross all nodes.\nThe asymmetric fragment-and-replicate join technique is also referred to as broad-\ncast join . It is a very useful technique, even for equi-joins, if one of the relations, say\ns, is small, and the other relation, say r, is large, since replicating the small relation s\nacross all nodes may be cheaper than repartitioning the large relation r.\nThe general case of fragment-and-replicate join (also called the symmetric fragment-\nand-replicate join appears in Figure 22.3b; it works this way: The system partitions\nrelation rintonpartitions, r1,r2,\u2026,rn, and partitions sintompartitions, s1,s2,\u2026,sm.\nAs before, any partitioning technique may be used on rand on s.T h ev a l u e so f mand\nndo not need to be equal, but they must be chosen so that there are at least m\u2217n\nnodes. Asymmetric fragment-and-replicate is simply a special case of general fragment-\nand-replicate, where m=1. Fragment-and-replicate reduces the sizes of the relations\nat each node, compared to asymmetric fragment-and-replicate.\nLet the nodes be N1,1,N1,2,\u2026,N1,m,N2,1,\u2026,Nn,m.N o d e Ni,jcomputes the join of\nriwith sj. To ensure that each node Ni,jgets all tuples of riandsj, the system replicates\nrito nodes Ni,1,Ni,2,\u2026,Ni,m(which form a row in Figure 22.3b), and replicates sito\nnodes N1,i,N2,i,\u2026,Nn,i(which form a column in Figure 22.3b). Any join technique\nc a nb eu s e da te a c hn o d e Ni,j.\nFragment-and-replicate works with any join condition, since every tuple in rcan\nbe tested with every tuple in s. Thus, it can be used where partitioning cannot be used.\nHowever, note that each tuple in ris replicated mtimes, and each tuple in sis replicated\nntimes.\n", "1075": "22.3 Parallel Join 1047\nr1 N1,1s1 s2 s3s\ns4 sm\nr2\nrr3\nr4\nrnNn,m ...N1 r1\nN2 r2\nrs\nN3 r2\nN4 r3\n......N2,1\nN3,1N1,2\nN2,2\nN3,2N1,3\nN2,3N1,4. . .\n......\n.\n.\n.\n.\n(a) Asymmetric\nfragment and replicate(b) Fragment and replicate\nFigure 22.3 Fragment-and-replicate schemes.\nFragment-and-replicate join has a higher cost than partitioning, since it involves\nreplication of both relations, and is therefore used only if the join does not involve\nequi-join conditions. Asymmetric fragment- and-replicate, on the other hand, is useful\neven for equi-join conditions, if one of the relations is small, as discussed earlier.\nNote that asymmetric fragment-and-replicate join can be used to compute the left\nouter join operation r\u27d5\u03b8sifsis replicated, by simply computing the left outer join\nlocally at each node. There is no restriction on the join condition \u03b8.\nHowever, r\u27d5\u03b8scannot be computed locally if sis fragmented and ris replicated,\nsince an rtuple may have no matching tuple in partition si, but may have a matching\ntuple in partition sj,j\u2260i. Thus, a decision on whether or not to output the rtuple with\nnull values for sattributes cannot be made locally at node Ni.F o rt h es a m er e a s o n ,\nasymmetric fragment-and-replicate cannot be used to compute the full outer join op-\neration, and symmetric fragment-and-replicate cannot be used to compute any of the\nouter join operations.\n22.3.3 Handling Skew in Parallel Joins\nSkew presents a special problem for parallel join techniques. If one of the nodes has a\nmuch heavier load than other nodes, the parallel join operation will take much longer\nto \ufb01nish, with many idle nodes waiting for the heavily loaded node to \ufb01nish its task.\n", "1076": "1048 Chapter 22 Parallel and Distributed Query Processing\nWhen partitioning data for storage, to minimize skew in storage we use a balanced\npartitioning vector that ensures all nodes get the same number of tuples. For parallel\njoins, we need to instead balance the execution time of join operations across all nodes.\nHash partitioning using any good hash function usually works quite well at balancing\nthe load across nodes, unless some join attribute values occur very frequently. Range\npartitioning, on the other hand, is more vulnerable to join skew, unless the ranges are\ncarefully chosen to balance the load.1\nVirtual-node partitioning with, say, round-robin distribution of virtual nodes to real\nnodes, can help in reducing skew at the level of real nodes even if there is skew at the\nlevel of virtual nodes, since the skewed virtual nodes tend to get spread over multiple\nreal nodes.\nThe preceding techniques are examples of join skew avoidance . Virtual-node parti-\ntioning, in particular, is very e\ufb00ective at skew avoidance in most cases.\nHowever, there are cases with high skew, for example where some join attribute\nvalues are very frequent in both input relations, leading to a large join result size. In\nsuch cases, there could be signi\ufb01cant join skew, even with virtual-node partitioning.\nDynamic handling of join skew is an alternative to skew avoidance. A dynamic ap-\nproach can be used to detect and handle skew in such situations. Virtual node partition-\ning is used, and the system then monitors the join progress at each real node. Each real\nnode schedules one virtual node at a time. Suppose that some real node has completed\njoin processing for all virtual nodes assigned to it, and is thus idle, while some other\nreal node has multiple virtual nodes waiting to be processed. Then, the idle node can\nget a copy of the data corresponding to one of the virtual nodes at the busy node and\nprocess the join for that virtual node. This process can be repeated whenever there is\nan idle real node, as long as some real node has virtual nodes waiting to be processed.\nThis technique is an example of work stealing , where a processor that is idle takes\nwork that is in the queue of another processor that is busy. Work stealing is inexpensive\nin a shared-memory system, since all data can be accessed quickly from the shared\nmemory, as discussed further in Section 22.6. In a shared-nothing environment, data\nmovement may be required to move a task from one processor to another, but it is often\nworth paying the overhead to reduce the completion time of a task.\n22.4 Other Operations\nIn this section, we discuss parallel processing of other relational operations, as well as\nparallel processing in the MapReduce framework.\n1Cost estimation should be done using histograms on join attr ibutes. A heuristic approximation is to estimate the join\ncost at each node Nias the sum of the sizes of riandsi, and choose range partitioning vectors to balance the sum of\nthe sizes.\n", "1077": "22.4 Other Operations 1049\n22.4.1 Other Relational Operations\nThe evaluation of other relational operations also can be parallelized:\n\u2022Selection . Let the selection be \u03c3\u03b8(r). Consider \ufb01rst the case where \u03b8is of the form\nai=v,w h e r e aiis an attribute and vis a value. If the relation ris partitioned on\nai, the selection proceeds at a single node. If \u03b8is of the form l\u2264ai\u2264u\u2014that is,\n\u03b8is a range selection\u2014and the relation has been range-partitioned on ai, then the\nselection proceeds at each node whose partition overlaps with the speci\ufb01ed range\nof values. In all other cases, the selection proceeds in parallel at all the nodes.\n\u2022Duplicate elimination . Duplicates can be eliminated by sorting; either of the paral-\nlel sort techniques can be used, optimized to eliminate duplicates as soon as they\nappear during sorting. We can also parallelize duplicate elimination by partition-\ning the tuples (by either range or hash partitioning) and eliminating duplicates\nlocally at each node.\n\u2022Projection . Projection without duplicate elimination can be performed as tuples\nare read in from disk in parallel. If duplicates are to be eliminated, either of the\ntechniques just described can be used.\n\u2022Aggregation . Consider an aggregation operation. We can parallelize the operation\nby partitioning the relation on the grouping attributes, and then computing the\naggregate values locally at each node. Either hash partitioning or range partitioning\ncan be used. If the relation is already partitioned on the grouping attributes, the\n\ufb01rst step can be skipped.\nWe can reduce the cost of transferring tuples during partitioning by partly\ncomputing aggregate values before partitioning, at least for the commonly used\naggregate functions. Consider an aggregation operation on a relation r,u s i n gt h e\nsum aggregate function on attribute B, with grouping on attribute A.T h es y s t e m\ncan perform the sum aggregation at each node Nion those rtuples stored at Ni.\nThis computation results in tuples with partial sums at each node; the result at\nNihas one tuple for each Avalue present in rtuples stored at Ni,w i t ht h es u m\nof the Bvalues of those tuples. The system then partitions the result of the local\naggregation on the grouping attribute Aand performs the aggregation again (on\ntuples with the partial sums) at each node Nito get the \ufb01nal result.\nAs a result of this optimization, which is called partial aggregation , fewer tuples\nneed to be sent to other nodes during partitioning. This idea can be extended easily\nto the minandmax aggregate functions. Extensions to the count andavgaggregate\nfunctions are left for you to do in Exercise 22.2.\nSkew handling for aggregation is easier than skew handling for joins, since the cost\nof aggregation is directly proportional to the input size. Usually, all that needs to be\ndone is to use a good hash function to ensure the group-by attribute values are evenly\ndistributed amongst the participating nodes. However, in some extreme cases, a few\n", "1078": "1050 Chapter 22 Parallel and Distributed Query Processing\nvalues occur very frequently in the group-by attributes, and hashing can lead to uneven\ndistribution of values. When applicable, partial aggregation is very e\ufb00ective in avoiding\nskew in such situations. However, when partial aggregation is not applicable, skew can\noccur with aggregation.\nDynamic detection and handling of such skew can be done in some such cases: in\ncase a node is found to be overloaded, some of the key values that are not yet processed\nby the node can be reassigned to another node, to balance the load. Such reassignment\nis greatly simpli\ufb01ed if virtual-node partitioning is used; in that case, if a real node is\nfound to be overloaded, some virtual nodes assigned to the overloaded real node, but\nnot yet processed, are identi\ufb01ed, and reassigned to other real nodes.\nMore information on skew handling for join and other operators may be found in\nthe Further Reading section at the end of the chapter.\n22.4.2 Map and Reduce Operations\nRecall the MapReduce paradigm, described in Section 10.3, which is designed to ease\nthe writing of parallel data processing programs.\nRecall that the map() function provided by the programmer is invoked on each\ninput record and emits zero or more output data items, which are then passed on to the\nreduce() function. Each data item output by a map() function consists of a record ( key,\nvalue ); we shall call the key as the intermediate key . In general, a map() function can\nemit multiple such records and since there are many input records, there are potentially\nmany output records overall.\nThe MapReduce system takes all the records emitted by the map() functions, and\ngroups them such that all records with a particular intermediate key are gathered to-\ngether. The reduce() function provided by the programmer is then invoked for each\nintermediate key and iterates over a collection of all values associated with that key.\nNote that the map function can be thought of as a generalization of the project op-\neration: both process a single record at a time, but for a given input record the project\noperation generates a single output record, whereas the map function can output mul-\ntiple records (including, as a special case, 0 records). Unlike the project operation, the\noutput of a map function is usually intended to become the input of a reduce func-\ntion; hence, the output of a map function has an associated key that serves as a group\nby attribute. Recall that the reduce function takes as input a collection of values and\noutputs a result; with most of the reduce functions commonly in use, the result is an\naggregate computed on the input values, and the reduce function is then essentially a\nuser-de\ufb01ned aggregation function.\nMapReduce systems are designed for parallel processing of data. A key require-\nment for parallel processing is the ability to parallelize \ufb01le input and output across\nmultiple machines; otherwise, the single machine storing the data will become a bot-\ntleneck. Parallelization of \ufb01le input and output can be done by using a distributed \ufb01le\nsystem, such as the Hadoop File System (HDFS ), discussed in Section 21.6, or by using\na parallel/distributed storage system, discussed in Section 21.7. Recall that in such sys-\n", "1079": "22.4 Other Operations 1051\nProgram\nMasterUser\nPart 1\nPart 2\nPart 3\nPart 4\nPart p\nreadMap n Reduce mReduce 1Reduce 1\nMap 2Map 1 File 1\nFile 2\nFile m\nInput \ufb01le\npartitionsIntermediate\n\ufb01lesOutput \ufb01leslocal\nwriteassign\nmapassign\nreduce\nRemote\nRead, Sortwritecopy copy copy\nFigure 22.4 Parallel processing of MapReduce job.\ntems, data are replicated (copied) across several (typically 3) machines, so that even if\na few of the machines fail, the data are available from other machines that have copies\nof the data in the failed machine.\nConceptually, the map and reduce operations are parallelized in the same way\nthat the relational operations project and aggregation are parallelized. Each node in\nthe system has a number of concurrently executing workers , which are processes that\nexecute map and reduce functions. The number of workers on one machine is often set\nto match the number of processor cores on the machine.\nParallel processing of MapReduce jobs is shown schematically in Figure 22.4. As\nshown in the \ufb01gure, MapReduce systems split the input data into multiple pieces; the\njob of processing one such piece is called a task. Splitting can be done in units of \ufb01les,\nand large \ufb01les can be split into multiple parts. Tasks correspond to virtual nodes in\nour terminology, while workers correspond to real nodes. Note that with a multicore\nprocessor (as is standard today), MapReduce systems typically allocate one worker per\ncore.\nMapReduce systems also have a scheduler, which assigns tasks to workers.2When-\never a worker completes a task, it is assigned a new task, until all tasks have been\nassigned.\nA key step between the map and reduce operations is the repartitioning of records\noutput by the map step; these records are repartitioned based on their intermediate\n(reduce) key, such that all records with a particular key are assigned to the same reducer\ntask. This could be done either by range-partitioning on the reduce key or by computing\na hash function on the reduce key. In either case, the records are divided into multiple\n2The scheduler is run on a dedicated node called the master node; the nodes that perform map() and reduce() tasks\nare called slave nodes in the Hadoop MapReduce terminology.\n", "1080": "1052 Chapter 22 Parallel and Distributed Query Processing\npartitions, each of which is called a reduce task. A scheduler assigns reduce tasks to\nworkers.\nThis step is identical to the repartitioning done for parallelizing the relational ag-\ngregation operation, with records partitioned into a number of virtual nodes based on\ntheir group-by key.\nT op r o c e s st h er e c o r d si nap a r t i c u l a r reduce task, the records are sorted (or\ngrouped) by the reduce key, so that all records with the same reduce-key value are\nbrought together, and then the reduce() is executed on each group of reduce-key val-\nues.\nThe reduce tasks are executed in parallel by the workers. When a worker completes\na reduce task, another task is assigned to it, until all reduce tasks have been completed.\nA reduce task may have multiple di\ufb00erent reduce key values, but a particular call to the\nreduce() function is for a single reduce key; thus, the reduce() function is called for\neach key in the reduce task.\nTasks correspond to virtual nodes in the virtual-node partitioning scheme. There\nare far more tasks than there are nodes, and tasks are divided among the nodes. As\ndiscussed in Section 22.3.3, virtual-node pa rtitioning reduces skew. Also note that as\ndiscussed in Section 22.4.1, skew can be red uced by partial aggregation, which corre-\nsponds to combiners in the MapReduce framework.\nFurther, MapReduce implementations typically also carry out dynamic detection\nand handling of skew, as discussed in Section 22.4.1.\nMost MapReduce implementations include techniques to ensure that processing\ncan be continued even if some nodes fail during query execution. Details are discussed\nfurther in Section 22.5.4.\n22.5 Parallel Evaluation of Query Plans\nAs discussed in Section 22.1, there are two types of parallelism: intraoperation and in-\nteroperation. Until now in this chapter, we have focused on intraoperation parallelism.\nIn this section, we consider execution plans fo r queries containing multiple operations.\nWe \ufb01rst consider how to exploit interope rator parallelism. We then consider a\nmodel of parallel query execution which breaks parallel query processing into two types\nof steps: partitioning of data using the exchange operator ,a n de x e c u t i o no fo p e r a t i o n s\non purely local data, without any data exchange. This model is surprisingly powerful\nand is widely used in parallel database implementations.\n22.5.1 Interoperation Parallelism\nThere are two forms of interoperation parallelism: pipelined parallelism and indepen-\ndent parallelism. We \ufb01rst describe these forms of parallelism, assuming each operator\nruns on a single node without intraoperation parallelism.\n", "1081": "22.5 Parallel Evaluation of Query Plans 1053\nWe then describe a model for parallel execution based on the exchange operator,\nin Section 22.5.2. Finally, in Section 22. 5.3, we describe how a complete plan can be\nexecuted, combining all the forms of parallelism.\n22.5.1.1 Pipelined Parallelism\nRecall from Section 15.7.2 that in pipelining, the output tuples of one operation, A,\nare consumed by a second operation, B, even before the \ufb01rst operation has produced\nthe entire set of tuples in its output. The major advantage of pipelined execution in a\nsequential evaluation is that we can carry out a sequence of such operations without\nwriting any of the intermediate results to disk.\nParallel systems use pipelining primarily for the same reason that sequential sys-\ntems do. However, pipelines are a source of parallelism as well, since it is possible to\nrun operations AandBsimultaneously on di\ufb00erent nodes (or di\ufb00erent cores of a sin-\ngle node), so that Bconsumes tuples in parallel with Aproducing them. This form of\nparallelism is called pipelined parallelism .\nPipelined parallelism is useful with a small number of nodes, but it does not scale\nup well. First, pipeline chains generally do not attain su\ufb03cient length to provide a high\ndegree of parallelism. Second, it is not possible to pipeline relational operators that\ndo not produce output until all inputs have been accessed, such as the set-di\ufb00erence\noperation. Third, only marginal speedup is obtained for the frequent cases in which\none operator\u2019s execution cost is much higher than are those of the others.\nAll things considered, when the degree of parallelism is high, pipelining is a less\nimportant source of parallelism than partitioning. The real reason for using pipelining\nwith parallel query processing is the same reason that pipelining is used with sequen-\ntial query processing: namely, that pipelined executions can avoid writing intermediate\nresults to disk.\nPipelining in centralized databases was discussed in Section 15.7.2; as mentioned\nthere, pipelining can be done using a demand-driven, or pull, model of computation,\nor using a producer-driven, or push, model of computation. The pull model is widely\nused in centralized database systems.\nHowever, the push model is greatly preferred in parallel database systems, since,\nunlike the pull model, the push model allows both the producer and consumer to exe-\ncute in parallel.\nUnlike the pull model, the push model requires a bu\ufb00er that can hold multiple\ntuples, between the producer and consumer; without such a bu\ufb00er, the producer would\nstall as soon as it generates one tuple. Figure 22.5 shows a producer and consumer with\na bu\ufb00er in-between. If the producer and consumer are on the same node, as shown\nin Figure 22.5a, the bu\ufb00er can be in shared memory. However, if the producer and\nconsumer are in di\ufb00erent nodes, as shown in Figure 22.5b, there will be two bu\ufb00ers:\none at the producer node to collect tuples as they are produced, and another at the\nconsumer node to collect them as they are sent across the network.\n", "1082": "1054 Chapter 22 Parallel and Distributed Query Processing\nbu\ufb00er\nnetworkproducer\nproducerconsumer\nconsumer(a)  Producer-consumer in shared memory\n(b) Producer-consumer across a networkbu\ufb00erbu\ufb00er\nFigure 22.5 Producer and consumer with buffer.\nWhen sending tuples across a network, it makes sense to collect multiple tuples\na n ds e n dt h e ma sas i n g l e batch , rather than send tuples one at a time, since there is\nusually a very signi\ufb01cant overhead per message. Batching greatly reduces this overhead.\nIf the producer and consumer are on the same node and can communicate via a\nshared memory bu\ufb00er, mutual exclusion needs to be ensured when inserting tuples into,\nor fetching tuples from, the bu\ufb00er. Mutual exclusion protocols have some overhead,\nwhich can be reduced by inserting/retrieving a batch of tuples at a time, instead of one\ntuple at a time.\nNote that with the pull model, either the producer or the consumer, but not both,\ncan be executing at a given time; while this avoids the contention on the shared bu\ufb00er\nthat arises with the use of the push model, it also prevents the producer and consumer\nfrom running concurrently.\n22.5.1.2 Independent Parallelism\nOperations in a query expression that do not depend on one another can be executed\nin parallel. This form of parallelism is called independent parallelism .\nConsider the join r1\u22c8r2\u22c8r3\u22c8r4. One possible plan is to compute intermedi-\nate result t1\u2190r1\u22c8r2in parallel with intermediate result t2\u2190r3\u22c8r4. Neither of\nthese computations depends on each other, and hence they can be parallelized by inde-\npendent parallelism. In other words, the execution of these two joins can be scheduled\nin parallel.\n", "1083": "22.5 Parallel Evaluation of Query Plans 1055\nWhen these two computations complete, we can compute:\nt1\u22c8t2\nNote that computation of the above join depends on the results of the \ufb01rst two joins,\nhence it cannot be done using independent parallelism.\nLike pipelined parallelism, independent parallelism does not provide a high degree\nof parallelism and is less useful in a highly parallel system, although it is useful with a\nlower degree of parallelism.\n22.5.2 The Exchange Operator Model\nThe Volcano parallel database popularized a model of parallelization called the\nexchange-operator model. The exchange operation repartitions data in a speci\ufb01ed way;\ndata interchange between nodes is done only by the exchange operator. All other oper-\nations work on local data, just as they would in a centralized database system; the data\nmay be available locally either because it is already present, or because of the execution\nof a preceding exchange operator.\nThe exchange operator has two components: a scheme for partitioning outgoing\ndata, applied at each source node, and a scheme for merging incoming data, applied at\neach destination node. The operator is shown pictorially in Figure 22.6, with the parti-\ntioning scheme denoted as \u201cPartition,\u201d and the merging scheme denoted as \u201cMerge.\u201d\nThe exchange operator can partition data in one of several ways:\n1.By hash partitioning on a speci\ufb01ed set of attributes.\n2.By range partitioning on a speci\ufb01ed set of attributes.\n3.By replicating the input data at all nodes, referred to as broadcasting .\n4.By sending all data to a single node.\nMerge\nMerge\nMerge\nMerger1\nr2\nr3\nrnPartition\nPartition\nPartition\nPartition \u2018r1\nr2\nr3\nrm\u2018\n\u2018\n\u2018\nFigure 22.6 The exchange operator used for repartitioning.\n", "1084": "1056 Chapter 22 Parallel and Distributed Query Processing\nBroadcasting data to all nodes is required for operations such as the asymmetric\nfragment-and-replicate join. Sending all data to a single node is usually done as a \ufb01-\nnal step of parallel query processing, to get partitioned results together at a single site.\nNote also that the input to the exchange operator can be at a single site (referred to\nasunpartitioned ), or already partitioned across multiple sites. Repartitioning of already\npartitioned data results in each destination node receiving data from multiple source\nnodes, as shown in Figure 22.6.\nEach destination node merges the data items received from the source nodes. This\nmerge step can store data in the order received (which may be nondeterministic, since\nit depends on the speeds of the machines and unpredictable network delays); such\nmerging is called random merge .\nOn the other hand, if the input data from each source is sorted, the merge step can\nexploit the sort order by performing an ordered merge . Suppose, for example, nodes\nN1,\u2026,Nm\ufb01rst sort a relation locally, and then repartition the sorted relation using\nrange partitioning. Each node performs an ordered merge operation on the tuples that\nit receives, to generate a sorted output locally.\nThus, the exchange operator performs the partitioning of data at the source nodes,\nas well the merging of data at the destination nodes.\nAll the parallel operator implementations we have seen so far can be modeled as a\nsequence of exchange operations, and local operators, at each node, that are completely\nunaware of parallelism.\n\u2022Range partitioning sort: can be implemented by an exchange operation that per-\nforms range partitioning, with random merge at the destination nodes, followed\nby a local sort operation at each destination node.\n\u2022Parallel external sort-merge: can be implemented by local sorting at the source\nnodes, followed by an exchange operation that performs range partitioning, along\nwith ordered merging.\n\u2022Partitioned join: can be implemented by an exchange operation that performs the\ndesired partitioning, followed by local join at each node.\n\u2022Asymmetric fragment-and-replicate join: can be implemented by an exchange op-\neration that performs broadcast \u201cpartitioning\u201d of the smaller relation, followed by\na local join at each node.\n\u2022Symmetric fragment-and-replicate join: can be implemented by an exchange oper-\nation that partitions, and partially broadcasts each partition, followed by a local\njoin at each node.\n\u2022Aggregation: can be implemented by an exchange operation that performs hash-\npartitioning on the grouping attributes, followed by a local aggregation operation\nat each node. The partial-aggregation optimization simply requires an extra local\naggregation operation at each node, before the exchange operation.\n", "1085": "22.5 Parallel Evaluation of Query Plans 1057\nOther relational operations can be implemented similarly, by a sequence of local oper-\nations running in parallel at each node, interspersed with exchange operations.\nAs noted earlier, parallel execution where data are partitioned, and operations are\nexecuted locally at each node, is referred to as data parallelism . The use of the exchange\noperator model to implement data parallel execution has the major bene\ufb01t of allowing\nexisting database query engines to be used at each of the local nodes, without any\nsigni\ufb01cant code changes. As a result, the exchange-operator model of parallel execution\nis widely used in parallel database systems.\nThere are, however, some operator implementations that can bene\ufb01t from being\naware of the parallel nature of the system they are running on. For example, an indexed\nnested-loops join where the inner relation is indexed on a parallel data-store would\nrequire remote access for each index lookup; the index lookup operation is thus aware\nof the parallel nature of the underlying system. Similarly, in a shared-memory system it\nmay make sense to have a hash table or index in shared-memory, which is accessed by\nmultiple processors (this approach is discussed brie\ufb02y in Section 22.6); the operations\nrunning on each processor are then aware of the parallel nature of the system.\nAs we discussed in Section 22.5.1.1, while the demand-driven (or pull) iterator\nmodel for pipelined execution of operators is widely used in centralized database en-\ngines, the push model is preferred for parallel execution of operators in a pipeline.\nThe exchange operator can be used to implement the push model between nodes\nin a parallel system, while allowing existing implementations of local operators to run\nusing the pull model. To do so, at each source node of an exchange operator, the op-\nerator can pull multiple tuples from its input and create a batch of tuples destined for\neach destination node. The input may be computed by a local operation, whose imple-\nmentation can use the demand-driven iterator model.\nThe exchange operator then sends batches of tuples to the destination nodes, where\nthey are merged and kept in a bu\ufb00er. The local operations can then consume the tuples\nin a demand-driven manner.\n22.5.3 Putting It All Together\nFigure 22.7 shows a query, along with a sequential and two alternative parallel query\nexecution plans. The query, shown in Figure 22.7a, computes a join of two relations,\nrands, and then computes an aggregate on the join result. Assume for concreteness\nthat the query isr.C,s.D\u03b3sum( s.E)(r\u22c8r.A=s.Bs).\nThe sequential plan, shown in Figure 22.7b, uses a hash join (denoted as \u201cHJ\u201d in\nthe \ufb01gure), which executes in three separate stages. The \ufb01rst stage partitions the \ufb01rst\ninput ( r) locally on r.A; the second stage partitions the second input ( s) locally on s.B;\nand the third stage computes the join of each of the corresponding partitions of rands.\nThe aggregate is computed using in-memory hash-aggregation, denoted by the operator\nHA; we assume that the number of groups is small enough that the hash table \ufb01ts in\nmemory.\n", "1086": "1058 Chapter 22 Parallel and Distributed Query Processing\n(a) Logical Query\n(c) Parallel Plan\n(d) Parallel Plan with Partial Aggregation(b) Sequential Planr\nsLoc.\nPart.\nLoc.\nPart.r\nsHJ HA\nr\nsE1\nE2Loc.\nPart.\nLoc.\nPart.\nLoc.\nPart.\nLoc.\nPart.E2E1r\nsE HJ\nHJHA 3E4 Result\nResultE   :  partition on  r.A \nE   :  partition on  (r.C,s.D ) 1 E   :  partition on  s.B 2 \n3 E   :  collect results  4 \nE   :  partition on r.A \nE   :  partition on ( r.C,s.D ) 1 E   :  partition on  s.B 2 \n3 E   :  collect results  4 HA1HA2E3E4\u03b3\nFigure 22.7 Parallel query execution plans.\nThe dashed boxes in the \ufb01gure show which steps run in a pipelined fashion. In the\nsequential plan, the read of the relation ris pipelined to the \ufb01rst partitioning stage of\nthe sequential hash join; similarly, the read of relations sis pipelined to the second\npartitioning stage of the hash join. The third stage of the hash join pipelines its output\ntuples to the hash aggregation operator.\nThe parallel query evaluation plan, shown in Figure 22.7c, starts with randsal-\nready partitioned, but not on t he required join attributes.3The plan, therefore, uses the\nexchange operation E1to repartition rusing attribute r.A; similarly, exchange operator\nE2repartitions susing s.B. Each node then uses hash join locally to compute the join\n3Note the multiple boxes indicating a relation is stored in mul tiple nodes; similarly, multiple circles indicate that an\noperation is executed in parallel on multiple nodes.\n", "1087": "22.5 Parallel Evaluation of Query Plans 1059\nof its partition of rands. Note that these partitions are not assumed to \ufb01t in memory,\nso they must be further partitioned by the \ufb01rst two stages of hash join; this local par-\ntitioning step is denoted as \u201cLoc. Part.\u201d in the \ufb01gure. The dashed boxes indicate that\nthe output of the exchange operator can be pipelined to the local partitioning step. As\nin the sequential plan, there are two pipelined stages, one each for rands.N o t et h a t\nexchange of tuples across nodes is done only by the exchange operator, and all other\nedges denote tuple \ufb02ows within each node.\nSubsequently, the hash join algorithm is executed in parallel at all participating\nnodes, and its output pipelined to the exchange operator E3. This exchange operator\nrepartitions its input on the pair of attributes ( r.C,s.D), which are the grouping at-\ntributes of the subsequent aggregation. At the receiving end of the exchange operator,\ntuples are pipelined to the hash aggregation operator. Note that the above steps all run\ntogether as a single pipelined stage, even though there is an exchange operator as part\nof the stage. Note that the local operators computing hash join and hash aggregate\nn e e dn o tb ea w a r eo ft h ep a r a l l e le x e c u t i o n .\nThe results of the aggregates are then collected together at a central location by\nthe \ufb01nal exchange operator E4, to create the \ufb01nal result relation.\nFigure 22.7d shows an alternative plan th at performs partial aggregation on the\nresults of the hash join, before partitioning the results. The partial aggregation is com-\nputed locally at each node by the operator HA1. Since no tuple is output by the partial\naggregation operator until all its input is consumed, the pipelined stage contains only\nthe local hash join and hash aggregation operators. The subsequent exchange operator\nE3which partitions its input on ( r.C,s.D) is part of a subsequent pipelined stage along\nwith the hash aggregation operation HA2which computes the \ufb01nal aggregate values.\nAs before, the exchange operator E4collects the results at a centralized location.\nThe above example shows how pipelined execution can be performed across nodes,\nas well as within nodes, and further how it can be done along with intra-operator parallel\nexecution. The example also shows that some pipelined stages depend on the output\nof earlier pipelined stages; therefore their execution can start only after the previous\nstep \ufb01nishes. On the other hand, the initial exchange and partitioning of randsoccur\nin pipelined stages that are independent of each other; such independent stages can be\nscheduled concurrently, that is, at the same time, if desired.\nTo execute a parallel plan such as the one in our example, the di\ufb00erent pipelined\nstages have to be scheduled for execution, in an order that ensures inter-stage depen-\ndencies are met. When executing a particular stage, the system must decide how many\nnodes an operation should be executed on. T hese decisions are usually made as part\nof the scheduling phase, before query execution starts.\n22.5.4 Fault Tolerance in Query Plans\nParallel processing of queries across a moderate number of nodes, for example, hun-\ndreds of nodes, can be done without worrying about fault tolerance. If a fault occurs,\nthe query is rerun, after removing any failed nodes from the system (replication of data\n", "1088": "1060 Chapter 22 Parallel and Distributed Query Processing\nat the storage layer ensures that data continues to be available even in the event of a\nfailure). However, this simple solution does not work well when operating at the scale\nof thousands or tens of thousands of nodes: if a query runs for several hours, there is a\nsigni\ufb01cant chance that there will be a failure while the query is being executed. If the\nquery is restarted, there is a signi\ufb01cant chance of another failure while it is executing,\nwhich is obviously an undesirable situation.\nTo deal with this problem, the query processing system should ideally just be able\nto redo the actions of a failed node, without redoing the rest of the computation.\nImplementations of MapReduce that are designed to work at a massively parallel\nscale can be made fault tolerant as follows:\n1.Each map operation executed at each node writes its output to local \ufb01les.\n2.The next operation, which is a reduce operation, executes at each node; the op-\neration execution at a node reads data from the \ufb01les stored at multiple nodes,\ncollects the data, and starts processing the data only after it has got all its re-\nquired data.\n3.Thereduce operation writes its output to a distributed \ufb01le system (or distributed\nstorage system) that replicates data, so that the data would be available even in\nthe event of a failure.\nLet us now examine the reason why things are done as above. First, if a particular\nmap node fails, the work done at that node can be redone at a backup node; the work\ndone at other map n o d e si sn o ta \ufb00 e c t e d .W o r ki sn o tc a r r i e do u tb y reduce nodes\nuntil all the required data has been fetched; the failure of a map node just means the\nreduce nodes fetch data from the backup map nodes. There is certainly a delay while\nthe backup node does its work, but there is no need to repeat the entire computation.\nFurther, once a reduce node has \ufb01nished its work, its output goes to replicated\nstorage to ensure it is not lost even if a data storage node fails. This means that if a\nreduce node fails before it completes its work, it will have to be reexecuted at a backup\nnode; other reduce nodes are not a\ufb00ected. Once a reduce node has \ufb01nished its work,\nthere is no need to reexecute it.\nNote that it is possible to store the output of a map node in a replicated storage\nsystem. However, this increases the execution cost signi\ufb01cantly, and hence map output\nis stored in local storage, even at the risk of having to reexecute the work done by a map\nnode in case it fails before all the reduce nodes have fetched the data that they require\nfrom that map node.\nIt is also worth noting that sometimes nodes do not completely fail, but run very\nslowly; such nodes are called straggler nodes. Even a single straggler node can delay all\nthe nodes in the next step (if there is a following step), or delay task completion (if it is in\nthe last step). Straggler nodes can be dealt with by treating them similar to failed nodes,\nand reexecuting their tasks on other nodes (the original task on the straggler node can\nalso be allowed to continue, in case it \ufb01nishes \ufb01rst). Such reexecution to deal with\n", "1089": "22.6 Query Processing on Shared-Memory Architectures 1061\nstragglers has been found to signi\ufb01cantly improve time to completion of MapReduce\ntasks.\nWhile the above scheme for fault tolerance is quite e\ufb00ective, there is an overhead\nthat must be noted: a reduce stage cannot perform any work until the previous map\nstage has \ufb01nished;4and if multiple map andreduce steps are executed, the next map\nstage cannot perform any work until the preceding reduce stage has \ufb01nished. In par-\nticular, this means that pipelining of data between stages cannot be supported; data\nare always materialized before it is sent to the next stage. Materialization carries a\nsigni\ufb01cant overhead, which can slow down computation.\nApache Spark uses an abstraction called Resilient Distributed Datasets (RDD s) to\nimplement fault tolerance. As we have seen in Section 10.4.2, RDD sc a nb ev i e w e d\nas collections, and Spark supports algebraic operations that take RDD sa si n p u t ,a n d\ngenerate RDD sa so u t p u t .S p a r kk e e p st r a c ko ft h eo p e r a t i o n su s e dt oc r e a t ea n RDD .\nIn case of failures that result in loss of an RDD , the operations used to create the RDD\ncan be reexecuted to regenerate the RDD .H o w e v e r ,t h i sm a yb et i m e - c o n s u m i n g ,s o\nSpark also supports replication to reduce the chance of data loss, as well as storing of\nlocal copies of data when a shu\ufb04e (exchange) step is executed, to allow reexecution to\nbe restricted to computation that was performed on failed nodes.\nThere has been a good deal of research on how to allow pipelining of data, while not\nrequiring query execution to restart from the beginning in case of a single failure. Such\nschemes typically require nodes to track what data they have received from each source\nnode. In the event of a source node failure, the work of the source node is redone on a\nbackup node, which can result in some tuples that were received earlier being received\nagain. Tracking the data received earlier is important to ensure duplicate tuples are\ndetected and eliminated by the receiving node. The above ideas can also be used to\nimplement fault tolerance for other algebraic operations, such as joins. In particular, if\nwe use the exchange operator with data parallelism, fault tolerance can be implemented\nas an extension of the exchange operator.\nReferences to more information on fault tolerant pipelining based on extensions\nof the exchange operator, as well as on fault tolerance schemes used in MapReduce\nand in Apache Spark, may be found in the Further Reading section at the end of the\nchapter.\n22.6 Query Processing on Shared-Memory Architectures\nParallel algorithms designed for shared-nothing architectures can be used in shared-\nmemory architectures. Each processor can be treated as having its own partition of\nmemory, and we can ignore the fact that the processors have a common shared-\n4Once a map node \ufb01nishes its tasks, redistribution of results from that node to the reduce nodes can start even if\nother map nodes are still active; but the actual computation at the reduce node cannot start until all map tasks have\ncompleted and all map results redistributed.\n", "1090": "1062 Chapter 22 Parallel and Distributed Query Processing\nmemory. However, execution can be optimized signi\ufb01cantly by exploiting the fast access\nto shared-memory from any of the processors.\nBefore we study optimizations that exploit shared-memory, we note that while\nmany large-scale systems can execute on a single shared-memory system, the largest-\nscale systems today are typically implemented using a hierarchical architecture, with\na shared-nothing architecture at the outer level, but with each node having a shared-\nmemory architecture locally, as discussed in Section 20.4.8. The techniques we have\nstudied so far for storing, indexing, and querying data in shared-nothing architectures\nare used to divide up storage, indexing, and query processing tasks among the di\ufb00erent\nnodes in the system. Each node is a shared-memory parallel system, which uses parallel\nquery processing techniques to execute the query processing tasks assigned to it. The\noptimizations we describe in this section can thus be used locally, at each node.\nParallel processing in a shared memory system is typically done by using threads,\nrather than separate processes. A thread is an execution stream that shares its entire\nmemory5with other threads. Multiple threads can be started up, and the operating\nsystem schedules threads on available processors.\nWe list below some optimizations that can be applied when parallel algorithms\nthat we saw earlier are executed in a shared memory system.\n1.If we use asymmetric fragment-and-replicate join, the smaller relation need not\nbe replicated to each processor. Instead, only one copy needs to be stored in\nshared memory, which can be accessed by all the processors. This optimization is\nparticularly useful if there are a large number of processors in the shared-memory\nsystem.\n2.Skew is a signi\ufb01cant problem in parallel systems, and it becomes worse as the\nnumber of processors grows. Handing o\ufb00 work from an overloaded node to an\nunderloaded node is expensive in a shared-nothing system since it involves net-\nwork tra\ufb03c. In contrast, in a shared memory system, data assigned to a processor\ncan be easily accessed from another processor.\nTo address skew in a shared-memory system, a good option is to use virtual-\nnode partitioning, which allows work to be redistributed in order to balance load.\nSuch redistribution could be done when a processor is found to be overloaded.\nAlternatively, whenever a processor \ufb01nds that it has \ufb01nished processing all the\nvirtual nodes assigned to it, it can \ufb01nd other processors that still have virtual\nnodes left to be processed, and take over some of those tasks; as mentioned in\nSection 22.3.3, this approach is called work stealing . Note that such an approach\nto avoiding skew would be much more expensive in a shared-nothing environment\nsince a signi\ufb01cant amount of data movement would be involved, unlike in the\nshared-memory case.\n5Technically, in operating-system terminology, its address space.\n", "1091": "22.6 Query Processing on Shared-Memory Architectures 1063\n3.Hash join can be executed in two distinct ways.\na. The \ufb01rst option is to partition both relations to each processor and then\ncompute the joins of the partitions, in a manner similar to shared-nothing\nhash join. Each partition must be small enough that the hash index on a\nbuild-relation partition \ufb01ts in the part of shared memory allocated to each\nprocessor.\nb. The second option is to partition the relations into fewer pieces, such that\nthe hash index on a build-relation partition \ufb01ts into common shared mem-\nory, rather than a fraction of the shared memory. The construction of the\nin-memory index, as well as probing of the index, must now be done in\nparallel by all the processors.\nParallelizing the probe phase is relatively easy, since each processor can\nwork on some partition of the probe relation. In fact it makes sense to use\nthe virtual node approach and partition the probe relation into many small\npieces (sometimes called \u201cmorsels\u201d), and have processors process a morsel\nat a time. When a processor is done with a morsel, it \ufb01nds an unprocessed\nmorsel and works on it, until there are no morsels left to be processed.\nParallelizing the construction of the shared hash index is more com-\nplicated, since multiple processors may attempt to update the same part\nof the hash index. Using locks is an option, but there are overheads due\nto locking. Techniques based on lock-free data structures can be used to\nconstruct the hash index in parallel.\nReferences to more details on how to parallelize join implementations in shared mem-\nory may be found in the Further Reading section at the end of the chapter.\nAlgorithms designed for shared-memory systems must take into account the fact\nthat in today\u2019s processors, memory is divided into multiple memory banks, with each\nbank directly linked to some processor. The cost of accessing memory from a given\nprocessor is less if the memory is directly linked to the processor, and is more if it is\nlinked to a di\ufb00erent processor. Such memory systems are said to have a Non-Uniform\nMemory Access orNUMA architecture.\nTo get the best performance, algorithms must be NUMA-aware ;t h a ti s ,t h e ym u s tb e\ndesigned to ensure that data accessed by a thread running on a particular processor is,\nas far as possible, stored in memory local to that processor. Operating systems support\nthis task in two ways:\n1.Each thread is scheduled, as far as possible, on the same processor core, every\ntime it is executed.\n2.When a thread requests memory from the operating system memory manager,\nthe operating system allocates memory that is local to that processor core.\nNote that the techniques for making the best use of shared memory are comple-\nmentary to techniques that make the best use of processor caches, including cache-\n", "1092": "1064 Chapter 22 Parallel and Distributed Query Processing\nconscious index structures (which we saw in Section 14.4.7) and cache-conscious algo-\nrithms for processing relational operators.\nBut in addition, since each processor core has its own cache, it is possible for a\ncache to have an old value that was subsequently updated on another processor core.\nThus, query processing algorithms that update shared data structures should be care-\nful to ensure that there are no bugs due to the use of outdated values, and due to race\nconditions on updating the same memory location from two processor cores. Lock-\ning and fence instructions to ensure cache consistency (Section 20.4.5) are used in\ncombination to implement updates to shared data structures.\nThe form of parallelism we have studied so far allows each processor to execute its\nown code independently of other processors. However, some parallel systems support\na di\ufb00erent form of parallelism, called Single Instruction Multiple Data (SIMD ). With\nSIMD parallelism, the same instruction is executed on each of multiple data items,\nwhich are typically elements of an array. SIMD architectures became widely used in\ngraphics processing units ( GPUs), which were initially used for speeding up processing\nof computer graphics tasks. However, more recently, GPU chips have been used for\nparallelizing a variety of other tasks, one of which is parallel processing of relational\noperations using the SIMD support provided by GPUs. Intel\u2019s Xeon Phi coprocessor\nsupports not only multiple cores in a single chip, but also several SIMD instructions that\ncan operate in parallel on multiple words. There has been a good deal of research on\nhow to process relational operations in parallel on such SIMD architectures; references\nto more information on this topic may be found in the bibliographic notes for this\nchapter, available online.\n22.7 Query Optimization for Parallel Execution\nQuery optimizers for parallel query evaluation are more complicated than query op-\ntimizers for sequential query evaluation. First, the space of plan alternatives can be\nmuch larger than that for sequential plans. In particular, in a parallel setting, we need\nto take into account the di\ufb00erent possible ways of partitioning the inputs and interme-\ndiate results, since di\ufb00erent partitioning schemes can lead to di\ufb00erent query execution\ncosts, which is not an issue for a sequential plan.\nSecond, the cost models are more complicated, since the cost of partitioning must\nbe taken into account, and issues such as skew and resource contention must be ad-\ndressed.\n22.7.1 Parallel Query Plan Space\nAs we have seen in Section 15.1, a sequential query plan can be expressed as an alge-\nbraic expression tree, where each node is a ph ysical operator, such as a sort operator,\nah a s hj o i no p e r a t o r ,am e r g e - j o i no p e r a t o r ,e t c .S u c hap l a nm a yb ef u r t h e ra n n o t a t e d\nwith instructions on what operations are to be pipelined and what intermediate results\nare to be materialized, as we saw in Section 15.7.\n", "1093": "22.7 Query Optimization for Parallel Execution 1065\nIn addition to the above, a parallel query plan must specify\n\u2022How to parallelize each operation, including deciding what algorithm to use, and\nhow to partition the inputs and intermediate results. Exchange operators can be\nused to partition inputs as well as intermediate results and \ufb01nal results.\n\u2022How the plan is to be scheduled ;s p e c i \ufb01 c a l l y :\n\u00b0How many nodes to use for each operation.\n\u00b0What operations to pipeline within the same node, or across di\ufb00erent nodes.\n\u00b0What operations to execute sequentially, one after the other.\n\u00b0What operations to execute independently in parallel.\nAs an example of the partitioning decision, a join r\u22c8r.A=s.A\u2227r.B=s.Bscan be paral-\nlelized by partitioning randson the attributes r.Aands.Aalone, or on the attributes\nr.Bands.Balone, or on ( r.A,r.B)a n d( s.A,s.B). The last option is likely to be the best\nif we consider only this join, since it minimizes the chances of skew which can arise if\nthe cardinalities of r.A,r.B,s.Aors.Bare low.\nBut consider now the queryr.A\u03b3sum(s.C)(r\u22c8r.A=s.A\u2227r.B=s.Bs). If we perform the join\nby partitioning on ( r.A,r.B)( a n d( s.A,s.B)), we would then need to repartition the join\nresult by r.Ato compute the aggregate. On the other hand, if we performed the join\nby partitioning on randsonr.Aands.Arespectively, both the join and the aggregate\ncan be computed without any repartitioning, which could reduce the cost. This option\nis particularly likely to be a good option if r.Aands.Ahave high cardinality and few\nduplicates, since the chance of skew is less in this case.\nThus, the optimizer has to consider a larger space of alternatives, taking partition-\ning into account. References with more details about how to implement query optimiza-\ntion for parallel query processing systems, taking partitioning alternatives into account,\nmay be found in the Further Reading section at the end of the chapter.\n22.7.2 Cost of Parallel Query Evaluation\nThe cost of a sequential query plan is typically estimated based on the total resource\nconsumption of the plan, adding up the CPU and I/Ocosts of the operators in a query\nplan. The resource consumption cost model can also be used in a parallel system, addi-\ntionally taking into account the network cost, and adding it along with the other costs.\nAs discussed in Section 15.2, even in a sequential system, the resource consumption\ncost model does not guarantee minimization of the execution time of an individual\nquery. There are other cost models that are better at modeling the time to complete a\nquery; however, the resource consumption cost model has the bene\ufb01t of reducing the\ncost of query optimization, and is thus widely used. We return to the issue of other cost\nmodels later in the section.\n", "1094": "1066 Chapter 22 Parallel and Distributed Query Processing\nWe now study how the cost of a parallel query plan can be estimated based on\nthe resource consumption model. If a query plan is data parallel, then each operation,\nother than the exchange operation, runs locally; the cost of such operations can be esti-\nmated using techniques we have seen earlier in Chapter 15, if we assume that the input\nrelations are uniformly partitioned across nnodes, with each node receiving 1 \u2215nth of\nthe overall input.\nThe cost of the exchange operation can be estimated based on the network topol-\nogy, the amount of data transferred, and the network bandwidth; as before it is assumed\nthat each node is equally loaded during the exchange operation. The cost of a query\nplan under the resource-consumption model can then be found by adding up the costs\nof the individual operations.\nHowever, in a parallel system, two plans with the same resource consumption may\nhave signi\ufb01cantly di\ufb00erent time to completion. A response-time cost model is a cost\nmodel that attempts to better estimate the time to completion of a query. If a particular\noperation is able to perform I/Ooperations in parallel with CPU execution, the response\ntime would be better modeled as max(CPU cost, I/Ocost), rather than the resource\nconsumption cost model of ( CPU cost+I/Ocost). Similarly, if two operations o1and\no2are in a pipeline on a single node, and their CPU and I/Ocosts are c1,io1andc2,io2\nrespectively, then their response time cost would be max(c1+c2,io1+io2). Similarly,\nif operations o1ando2are executed sequentially, then their response time cost would\nbemax(c1,io1)+max(c2,io2).\nWhen executing operations in parallel across multiple nodes, the response time\ncost model would have to take into account:\n\u2022Start-up costs for initiating an operation at multiple nodes.\n\u2022Skew in the distribution of work among the nodes, with some nodes getting a larger\nnumber of tuples than others, and thus taking longer to complete. It is the time\nto completion of the slowest node that determines the time to completion of the\noperation.\nThus, any skew in the distribution of the work across nodes greatly a\ufb00ects performance.\nEstimating the time to completion of the slowest node due to skew is not an easy\ntask since it is highly data dependent. However, statistics such as number of distinct\nvalues of partitioning attributes, histograms on the distribution of values of partitioning\nattributes, and counts of most frequent values can be used to estimate the potential for\nskew. Partitioning algorithms that can detect and minimize the e\ufb00ect of skew, such as\nthose discussed in Section 21.3, are important to minimize skew.\n22.7.3 Choosing a Parallel Query Plan\nThe number of parallel evaluation plans from which to choose is much larger than the\nnumber of sequential evaluation plans. Optimization of parallel queries by consider-\ning all alternatives is therefore much more expensive than optimization of sequential\n", "1095": "22.7 Query Optimization for Parallel Execution 1067\nqueries. Hence, we usually adopt heuristic approaches to reduce the number of parallel\nexecution plans that we have to consider. We describe two popular approaches here.\n1.A simple approach is to choose the most e\ufb03cient sequential evaluation plan, and\nthen to choose the optimal way to parallelize the operations in that evaluation\nplan. When choosing a sequential plan, the optimizer may use a basic sequential\ncost model; or it may use a simple cost model that takes some aspects of paral-\nlel execution into account but does not consider issues such as partitioning or\nscheduling. This option allows an existing sequential query optimizer to be used\nwith minimal changes for the \ufb01rst step.\nNext, the optimizer decides how to create an optimal parallel plan correspond-\ning to the chosen sequential plan. At this point, choices of what partitioning\ntechniques to use and how to schedule operators can be made in a cost-based\nmanner.\nThe chosen sequential plan may not be optimal in the parallel context, since\nthe exact cost formulae for parallel execution were not used when choosing it;\nnevertheless, the approach works reasonably well in many cases.\n2.A more principled approach is to \ufb01nd the best plan, assuming that each operation\nis executed in parallel across all the nodes (operations with very small inputs may\nbe executed on fewer nodes). Scheduling of independent operations in parallel\non di\ufb00erent nodes is not considered at this stage.\nPartitioning of inputs and intermediate results is taken into consideration\nwhen estimating the cost of a query plan. Existing techniques for query optimiza-\ntion have been extended by considering partitioning as a physical property, in\naddition to physical properties such as sort orders that are already taken into ac-\ncount when choosing a sequential query plan. Just as sort operators are added\nto a query plan to get a desired sort order, exchange operators are added to get\nthe desired partitioning property. The cost model used in practice is typically\na resource consumption model, which we saw earlier. Although response-time\ncost models o\ufb00er better estimates of query execution time, the cost of query opti-\nmization is higher when using a response-time cost model compared to the cost\nof optimization when using a resource-consumption cost model. References pro-\nviding more information on the response-time cost model may be found in the\nFurther Reading section at the end of the chapter.\nYet another dimension of optimization is the design of physical-storage organiza-\ntion to speed up queries. For example, a relation can be stored partitioned on any of\nseveral di\ufb00erent attributes, and it may even be replicated and replicas can be stored\npartitioned on di\ufb00erent attributes. For example, a relation r(A,B,C) could be stored\npartitioned on A, and a replica could be partitioned on B. The query optimizer chooses\nthe replica that is best suited for the query. The optimal physical organization di\ufb00ers\n", "1096": "1068 Chapter 22 Parallel and Distributed Query Processing\nfor di\ufb00erent queries. The database administrator must choose a physical organization\nthat appears to be good for the expected mix of database queries.\n22.7.4 Colocation of Data\nEven with parallel data storage and parallel processing of operations, the execution\ntime of some queries can be too slow for the needs of some applications. In particular,\nqueries that access small amounts of data stored at multiple nodes may run quite slowly\nwhen executed in parallel, as compared to the execution of the same query on a single\nnode, if all the data were available at that node. There are many applications that need\nsuch queries to return answers with very low latency.\nAn important technique to speed up such queries is to colocate data that a query\naccesses in a single node. For example, suppose an application needs to access student\ninformation, along with information about courses taken by the student. Then, the\nstudent relation may be partitioned on the ID,a n dt h e takes relation also partitioned in\nexactly the same manner on ID.T u p l e si nt h e course relation, which is a small relation,\nmay be replicated to all nodes. With such a partitioning, any query involving these three\nrelations that retrieves data for a single IDcan be answered locally at a single node. The\nquery processing engine just detects which node is responsible for that IDand sends\nthe query to that node, where it is executed locally.\nColocation of tuples from di\ufb00erent relations is supported by many data storage\nsystems. If the data storage system does not natively support colocation, an alternative\nis to create an object containing related tuples that share a key (e.g., student andtakes\nrecords corresponding to a particular ID), and store it in the data storage system with\nthe associated key ( ID,i no u re x a m p l e ) .\nThe colocation technique, however, does not work directly if di\ufb00erent queries need\na relation partitioned in di\ufb00erent ways. For example, if the goal is to \ufb01nd all students\nwho have taken a particular course section, the takes relation needs to be partitioned on\nthe section information ( course\n id,year,semester ,sec\nid) instead of being partitioned\nonID.\nA simple technique to handle this situation is to allow multiple copies of a rela-\ntion, partitioned on di\ufb00erent attributes. These copies can be modeled as indices on the\nrelation, partitioned on di\ufb00erent attributes; when tuples in the relation are updated,\nso are the copies, to keep them consistent. In our example, one copy of takes can be\npartitioned on IDto be colocated with the student partitions, while another copy is par-\ntitioned on ( course\n id,year,semester ,sec\nid) to be colocated with the section relation.\nColocation helps optimize queries that compute joins between two relations; it can\nextend to three or more relations if either the remaining relations are replicated, or if all\nrelations share a common set of join attributes. In the latter case, all tuples that would\njoin together can be colocated by partitioning on the common join attributes. In either\ncase, the join can be computed locally at a single node, if the query only wants the\nresults for a speci\ufb01c value of the join attribute, as we saw earlier. However, not all join\n", "1097": "22.7 Query Optimization for Parallel Execution 1069\nqueries are amenable to evaluation at a single node by using colocation. Materialized\nviews, which we discuss next, o\ufb00er a more general alternative.\n22.7.5 Parallel Maintenance of Materialized Views\nMaterialized views, which we saw in Section 4.2.3 for speeding up queries in centralized\ndatabases, can also be used to speed up queries in parallel databases. Materialized\nviews need to be maintained when the database is updated, as we saw in Section 16.5.\nMaterialized views in a parallel database can have a very large amount of data, and\nmust, therefore, be stored partitioned across multiple nodes.\nAs in the centralized case, materialized views in a parallel database speed up query\nanswering at the cost of the overhead of view maintenance at the time of processing\nupdates.\nWe consider \ufb01rst a very simple case of materialized views. It is often useful to store\nan extra copy of a relation, partitioned on di\ufb00erent attributes, to speed up answering of\nsome queries. Such a repartitioning can be considered a very simple case of a material-\nized view; view maintenance for such a view is straightforward, just requiring updates\nto be sent to the appropriate partition.\nIndices can be considered a form of materialized views. Recall from Section 21.5\nhow parallel indices are maintained. Consider the case of maintenance of a parallel\nsecondary index on an attribute Bof a relation r(A,B,C), with primary key A.T h es e c -\nondary index would be sorted on attribute Band would include unique key A; assume it\nalso includes attribute C. Suppose now that attribute Bof a tuple ( a1,b1,c1) is updated\nfrom b1t o b2. This update results in two updates to the secondary index: delete the\nindex entry with key ( b1,a1,c1), and add an entry ( b2,a1,c1). Since the secondary in-\ndex is itself partitioned, these two updates need to be sent to the appropriate partition,\nbased on the unique key attributes ( B,A).\nIn some cases, materialized view maintenance can be done by partitioning fol-\nlowed by local view maintenance. Consider a view that groups takes tuples by ( course\nid,year,semester ,sec\nid), and then counts the number of students who have taken that\ncourse section. Such a materialized view would be stored partitioned on the grouping\nattributes ( course\n id,year,semester ,sec\nid). It can be computed by maintaining a copy\nof the takes relation partitioned on ( course\n id,year,semester ,sec\nid), and materializing\nthe aggregates locally at each node. When there is an update, say an insert or delete to\nthetakes relation, that update must be propagated to the appropriate node based on\nthe partitioning chosen above. The materialized aggregate can be maintained locally at\neach node, as updates are received for the set of local tuples of the takes relation.\nFor more complex views, materialized view maintenance cannot be done by a single\nstep of partitioning and local view maintenance. We consider a more general approach\nnext.\nFirst, consider an operator o, whose result is materialized, and an update (insert or\ndelete) to one of the input relations of othat requires maintenance of the materialized\nresult of o. Suppose the execution of operator ois parallelized using the exchange op-\n", "1098": "1070 Chapter 22 Parallel and Distributed Query Processing\nerator model (Section 22.5.2) where the inputs are partitioned, and then operators are\nexecuted at each node on data made available locally by (re)partitioning. To support\nmaterialized view maintenance of the result of o, we materialize the output of oat each\nnode and additionally materialize (store) the input partitions sent to the node when\nthe materialized view result is initially computed.\nNow, when there is an update (insert or delete) to an input to o,w es e n dt h eu p d a t e\nto the appropriate node using the same partition function used during the initial com-\nputation of o. Consider a node that has received such an update. Now, the maintenance\nof the locally materialized result at the node can be done using standard (nonparallel)\nview maintenance techniques using only locally available data.\nNote that as we saw in Section 22.5.2, a variety of operations can be parallelized\nusing the exchange operator model, and hence the preceding scheme provides a parallel\nview maintenance technique for all such operators.\nNext, consider a query with multiple operators. Such a query can be parallelized\nusing the exchange operator model. The exchange operators repartition data between\nnodes, and each node computing (possibly multiple) operations using data made avail-\nable locally by the exchange operators, as we saw in Section 22.5.2.\nWe can materialize the inputs and results at each node. Now, when there is a change\nto an input at a node, we use standard view maintenance techniques locally to \ufb01nd the\nchange to the materialized result, say v,a tt h a tn o d e .I ft h a tr e s u l t vis the \ufb01nal output of\nthe query, we are done. Otherwise, there must be an exchange operator above it; we use\nt h ee x c h a n g eo p e r a t o rt or o u t et h eu p d a t e s( i n s e r t so rd e l e t e )t o vto the next operator.\nThat operator in turn computes the change to its result, and propagates it further if\nrequired, until we get to the root of the original materialized view.\nThe issue of consistency of materialized views in the face of concurrent updates to\nthe underlying relations is addressed in Section 23.6.3.\n22.8 Parallel Processing of Streaming Data\nWe saw several applications of streaming data in Section 10.5. Many of the streaming\ndata applications that we saw in that section, such as network monitoring or stock\nmarket applications, have very high rates of tuple arrival. Incoming tuples cannot be\nprocessed by a single computer, and parallel processing is essential for such systems.\nStreaming data systems apply a variety of operations on incoming data. We now see\nhow some of these operations can be executed in parallel.\nParallelism is essential at all stages of query processing, starting with the entry of\ntuples from the sources. Thus, a parallel stream processing system needs to support a\nlarge number of entry points for data.\nFor example, a system that is monitoring queries posed on a search engine such\nas Google or Bing search has to keep up with a very high rate of queries. Search en-\ngines have a large number of machines across which user queries are distributed and\nexecuted. Each of these machines becomes a source for the query stream. The stream\n", "1099": "22.8 Parallel Processing of Streaming Data 1071\nprocessing system must have multiple entry points for the data, which receive data from\nthe original sources and route them within the stream processing system.\nProcessing of data must be done by routing tuples from producers to consumers.\nWe discuss routing of tuples in Section 22.8.1. Parallel processing of stream operations\nis discussed in Section 22.8.2, while fault t olerance is discussed in Section 22.8.3.\nIt is also worth noting that many applications that perform real-time analytics on\nstreaming data also need to store the data and analyze it in other ways subsequently.\nThus, many systems duplicate incoming data streams, sending one copy to a storage\nsystem for subsequent analysis and sending the other copy to a streaming data system;\nsuch an architecture is called the lambda architecture: the Greek symbol \u03bbis used to\npictorially denote that incoming data are forked into two copies, sent to two di\ufb00erent\nsystems.\nWhile the lambda architecture allows streaming systems to be built quickly, it also\nresults in duplication of e\ufb00ort: programmers need to write code to store and query the\ndata in the format/language supported by a database, as well as to query the data in\nthe language supported by a streaming data system. More recently, there have been\ne\ufb00orts to perform stream processing as well as query processing on stored data within\nthe same system to avoid this duplication.\n22.8.1 Routing of Tuples\nSince processing of data typically involves m ultiple operators, routing of data to oper-\nators is an important task. We \ufb01rst consider the logical structure of such routing, and\naddress the physical structure, which takes parallel processing into account, later.\nThelogical routing of tuples is done by creating a directed acyclic graph (DAG) with\noperators as nodes. Edges between nodes de\ufb01ne the \ufb02ow of tuples. Each tuple output by\nan operator is sent along all the out-edges of the operator, to the consuming operators.\nEach operator receives tuples from all its in-edges. Figure 22.8a depicts the logical\nPublish-Subscribe\n          SystemData\nSink\nData\nSink\nData\nSinkData\nSink\nData\nSink\nData\nSink  Data\nSource\n  Data\nSource\n  Data\nSource\n  Data\nSourceOp Op\nOp OpOp\nOp OpOp  Data\nSource\n  Data\nSource\n  Data\nSource\n  Data\nSource\n(a) DAG representation of streaming data \ufb02ow (b) Publish-subscribe representation of streaming data \ufb02ow\nFigure 22.8 Routing of streams using DAG and publish-subscribe representations.\n", "1100": "1072 Chapter 22 Parallel and Distributed Query Processing\nrouting of stream tuples through a DAG structure. Operation nodes are denoted as\n\u201cOp\u201d nodes in the \ufb01gure. The entry points to the stream processing system are the data\nsource nodes of the DAG; these nodes consume tuples from the stream sources and\ninject them into the stream processing system. The exit points of the stream processing\nsystem are data sink nodes; tuples exiting the system through a data sink may be stored\nin a data store or \ufb01le system or may be output in some other manner.\nOne way of implementing a stream processing system is by specifying the graph\nas part of the system con\ufb01guration, which is read when the system starts processing\ntuples and is then used to route tuples. The Apache Storm stream processing system\nis an example of a system that uses a con\ufb01guration \ufb01le to de\ufb01ne the graph, which is\ncalled a topology in the Storm system. (Data source nodes are called spouts in the Storm\nsystem, while operator nodes are called bolts, and edges connect these nodes.)\nAn alternative way of creating such a routing graph is by using publish-subscribe\nsystems. A publish-subscribe system allows publication of documents or other forms\nof data, with an associated topic. Subscribers correspondingly subscribe to speci\ufb01ed\ntopics. Whenever a document is published to a particular topic, a copy of the document\nis sent to all subscribers who have subscribed to that topic. Publish-subscribe systems\nare also referred to as pub-sub systems for short.\nWhen a publish-subscribe system is used for routing tuples in a stream processing\nsystem, tuples are considered documents, and each tuple is tagged with a topic. The\nentry points to the system conceptually \u201cpublish\u201d tuples, each with an associated topic.\nOperators subscribe to one or more topics; the system routes all tuples with a speci\ufb01c\ntopic to all subscribers of that topic. Opera tors can also publish their outputs back to\nthe publish-subscribe system, with an associated topic.\nA major bene\ufb01t of the publish-subscribe approach is that operators can be added\nto the system, or removed from it, with relative ease. Figure 22.8b depicts the routing of\ntuples using a publish-subscribe representation. Each data source is assigned a unique\ntopic name; the output of each operator is also assigned a unique topic name. Each\noperator subscribes to the topics of its inputs and publishes to the topics corresponding\nto its output. Data sources publish to their associated topic, while data sinks subscribe\nto the topics of the operators whose output goes to the sink.\nThe Apache Kafka system uses the publish-subscribe model to manage routing\nof tuples in streams. In the Kafka system, tuples published for a topic are retained\nfor a speci\ufb01ed period of time (called the retention period), even if there is currently no\nsubscriber for the topic. Subscribers usually process tuples at the earliest possible time,\nbut in case processing is delayed or temporarily stopped due to failures, the tuples are\nstill available for processing until the retention time expires.\nMany streaming data systems, such as Google\u2019s Millwheel, and the Muppet stream\nprocessing system, use the term stream in place of the term topic.I ns u c hs y s t e m s ,\nstreams are assigned names; operators can publish tuples to a stream, or subscribe to\na stream, based on the name.\nWe now consider the physical routing of tuples. Regardless of the model used above,\neach logical operator must have multiple physical instances running in parallel on dif-\n", "1101": "22.8 Parallel Processing of Streaming Data 1073\nferent nodes. Incoming tuples for a logical operator must be routed to the appropriate\nphysical instance(s) of the operator. A partitioning function is used to determine which\ntuple goes to which instance of the operator.\nIn the context of a publish-subscribe system, each topic can be thought of as a\nseparate logical operator that accepts tuples and passes them on to all subscribers of\nthe topic. Since there may be a very large number of tuples for a given topic, they must\nbe processed in parallel across multiple nodes in a parallel publish-subscribe system.\nIn the Kafka system, for example, a topic is divided into multiple partitions, called\natopic-partition ; each tuple for that topic is sent to only one of the topic-partitions.\nKafka allows a partition key to be attached to each tuple, and ensures that tuples with\nthe same key are delivered to the same partition.\nTo allow processing by consumers, Kafka allows consumer operators register with\na speci\ufb01ed \u201cconsumer group.\u201d The consumer group corresponds to a logical operator,\nwhile the individual consumers correspond to physical instances of the logical operator\nthat run in parallel. Each tuple of a topic is sent to only one consumer in the consumer\ngroup. More precisely, all tuples in a particular topic-partition are sent to a single con-\nsumer in a consumer group; however, tuples from multiple partitions may be sent to the\nsame consumer, leading to a many-to-one relationship from partitions to consumers.\nKafka is used in many streaming data processing implementations for routing\ntuples. Kafka Streams provides a client lib rary supporting algebraic operations on\nstreams, which can be used to build streaming applications on top of the Kafka publish-\nsubscribe system.\n22.8.2 Parallel Processing of Stream Operations\nFor standard relational operations, the techniques that we have seen for parallel evalu-\nation of the operations can be used with streaming data.\nSome of these, such as selection and projection operations, can be done in parallel\non di\ufb00erent tuples. Others, such as grouping, have to bring all tuples of a group together\nto one machine.6When grouping is done with aggregation, optimizations such as pre-\naggregation can be used to reduce the data transferred, but information about the tuples\nin a group must still be delivered to a single machine.\nWindowing is an important operation in streaming data systems. Recall from Sec-\ntion 10.5.2.1 that incoming data are divided into windows, typically based on times-\ntamps (windows can also be de\ufb01ned based on the number of tuples). Windowing is\noften combined with grouping/aggregation, with aggregates computed on groups of tu-\nples within a window. The use of windows ensures that once the system can determine\nthat new tuples will no longer belong to a particular window, aggregates for that win-\ndow can be output. For example, suppose a window is based on time, say with each 5\nminutes de\ufb01ning a window; once the system determines that future tuples will have a\n6When grouping is combined with windowing (Section 10.5. 2.1), a group contains all tuples in a window that have the\nsame values for their grouping attributes.\n", "1102": "1074 Chapter 22 Parallel and Distributed Query Processing\ntimestamp larger than the end of a particular window, aggregates on that window can\nbe output. Unlike grouping, windows can overlap each other.\nWhen windowing and grouping are used together to compute aggregates, if there\nare overlapping windows, it is best to partition on just the grouping attributes. Oth-\nerwise, tuples which belong to multiple windows would have to be sent to multiple\nwindows, an overhead that is avoided by partitioning on only the grouping attributes.\nMany streaming systems allow users to create their own operators. It is important\nto be able to parallelize user-de\ufb01ned operators by allowing multiple instances of the\noperator to run concurrently. Such systems typically require each tuple to have an as-\nsociated key, and all tuples with a particular key are sent to a particular instance of the\noperator. Tuples with di\ufb00erent keys can be sent to di\ufb00erent operator instances, allowing\nparallel processing.\nStream operations often need to store state. For example, a windowing operator\nmay need to retain all tuples that it has seen in a particular window, as long as the\nwindow is active. Or, it may need to store aggregates computed at some resolution (say\nper minute) to later compute coarser resolution aggregates (say per hour). There are\nmany other reasons for operators to store state. User-de\ufb01ned operators often de\ufb01ne\nstate internal to the operator (local variables), which needs to be stored.\nSuch state may be stored locally, at each node where a copy of the operator is\nexecuted. Alternatively, it may be stored centrally in a parallel data-storage system. The\nstore-locally alternative has a lower cost but a higher risk of losing state information\non failure, as compared to storing state in a parallel data-storage system. This aspect is\ndiscussed further in Section 22.8.3.\n22.8.3 Fault Tolerance with Streaming Data\nFault tolerance when querying stored data can be achieved by reexecuting the query, or\nparts of the query, as we have seen in Section 22.5.4. However, such an approach to fault\ntolerance does not work well in a streaming setting for multiple reasons. First, many\nstreaming data applications are latency sensitive, and delays in delivering results due\nto restarts are not desirable. Second, streaming systems provide a continuous stream\nof outputs. In the event of a failure, reexecuting the entire system or parts of it could\npotentially lead to duplicate copies of output tuples, which is not acceptable for many\napplications.\nThus, streaming data systems need to provide guarantees about delivery of output\ntuples, which can be one of: at-least once, at-most once, and exactly-once. The at-least-\nonce semantics guarantees that each tuple is output at least once, but allows duplicate\ndelivery during recovery from failures. The at-most-once semantics guarantees that each\ntuple is delivered at most once, without duplicates, but some tuples may be lost in\nthe event of a failure. The exactly-once semantics guarantees that each tuple will be\ndelivered exactly once, regardless of failures. This is the model that most applications\nrequire, although some applications may not care about duplicates and may accept\nat-least-once semantics.\n", "1103": "22.8 Parallel Processing of Streaming Data 1075\nTo ensure such semantics, streaming systems must track what tuples have been\nprocessed at each operator and what tuples have been output. Duplicates can be de-\ntected by comparing against tuples output earlier and removed. (This can be done only\nif the system guarantees the absence of duplicates during normal processing, since\notherwise the semantics of the streaming query may be a\ufb00ected by removal of genuine\nduplicates.)\nOne way to implement fault tolerance is to support it in the subsystem that routes\ntuples between operators. For example, in Kafka, tuples are published to topics, and\neach topic-partition can be stored in two or more nodes so that even if one of the nodes\nfails, the other one is available. Further, the tuples are stored on disk in each of the\nnodes so that they are not lost on power failure or system restart. Thus, the streaming\ndata system can use this underlying fault tolerance and high availability mechanism to\nimplement fault-tolerance and high availability at a higher level of the system.\nIn such a system, if an operator was executing on a failed node, it can be restarted\non another node. The system must also (at least periodically) record up to what point\neach input stream had been consumed by the operator. The operator must be restarted\nand each input stream replayed from a point such that the operator can correctly output\nall tuples that it had not already output before the failure. This is relatively easy for\noperators without any state; operators without any state need to do extra work to restore\nthe state that existed before failure. For example, a window operator needs to start from\na point in the stream corresponding to the start of a window and replay tuples from\nthat point.\nIf the window is very large, restarting from a very old point in the stream would\nbe very ine\ufb03cient. Instead, the operator may checkpoint its state periodically, along\nwith points in the input stream up to which processing has been done. In the event of\na failure, the latest checkpoint may be restored, and only input stream tuples that were\nprocessed since the last checkpoint need to be replayed.\nThe same approach can be for other operators that have state information; the\nstate can be checkpointed periodically, and replay starts from the last checkpoint. The\ncheckpointed state may be stored locally; however, this means that until the node re-\ncovers, stream processing cannot proceed. As an alternative, the state may be stored\nin a distributed \ufb01le system or in a parallel data-storage system. Such systems replicate\ndata to ensure high availability even in the event of failures. Thus, if a node has failed,\nits functions can be restarted on a di\ufb00erent node, starting with the last checkpoint, and\nreplaying the stream contents.\nIf the underlying system does not implement fault tolerance, operators can im-\nplement their own fault-tolerance mechanisms to avoid tuple loss. For example, each\noperator may store all tuples that it has output; a tuple can be discarded only after the\noperator knows that no consumer will need the tuple, even in the event of a failure.\nFurther, streaming systems must often guarantee low latency, even in the event\nof failures. To do so, some streaming systems have replicas of each operator, running\nconcurrently. If one replica fails, the output can be fetched from the other replica.\nThe system must make sure that duplicate tuples from the replicas are not output to\n", "1104": "1076 Chapter 22 Parallel and Distributed Query Processing\nconsumers. In such systems, one copy of an operator is treated as a primary copy, and\nthe other copy as a hot-spare replica (recall that we discussed hot-spares in Section\n19.7).\nWhat we have described above is a high-level view of how streaming data systems\nimplement fault tolerance. References to more information on how to implement fault\ntolerance in streaming systems may be found in the Further Reading section at the end\nof the chapter.\n22.9 Distributed Query Processing\nThe need for distributed query processing originally arose when organizations needed\nto execute queries across multiple databases that were often geographically distributed.\nHowever, today the same need arises because organizations have data stored in multiple\ndi\ufb00erent databases and data storage systems, and they need to execute queries that\naccess multiples of these databases and data storage systems.\n22.9.1 Data Integration from Multiple Data Sources\nDi\ufb00erent parts of an enterprise may use di\ufb00erent databases, either because of a legacy\nof how they were automated, or because of mergers of companies. Migrating an entire\norganization to a common system may be an expensive and time-consuming operation.\nAn alternative is to keep data in individual databases, but to provide users with a logical\nview of integrated data. The local database systems may employ di\ufb00erent logical mod-\nels, di\ufb00erent data-de\ufb01nition and data-manipulation languages, and may di\ufb00er in their\nconcurrency-control and transaction-management mechanisms. Some of the sources\nof data may not be full-\ufb02edged database systems but may instead be data storage sys-\ntems, or even just \ufb01les in a \ufb01le system. Yet another possibility is that the data source\nmay be on the cloud and accessed as a web service. Queries may need access to data\nstored across multiple databases and data sources.\nManipulation of information located in multiple databases and other data sources\nrequires an additional software layer on top of existing database systems. This layer\ncreates the illusion of logical database integration without requiring physical database\nintegration and is sometimes called a federated database system .\nDatabase integration can be done in several di\ufb00erent ways:\n\u2022The federated database approach creates a common schema, called a global\nschema , for data from all the databases/data sources; each database has its own\nlocal schema . The task of creating a uni\ufb01ed global schema from multiple local\nschemas is referred to as schema integration .\nUsers can issue queries against the global schema. A query on a global schema\nmust be translated into queries on the local schemas at each of the sites where\nt h eq u e r yh a st ob ee x e c u t e d .T h eq u e r yr e s u l t sh a v et ob et r a n s l a t e db a c ki n t ot h e\nglobal schema and combined to get the \ufb01nal result.\n", "1105": "22.9 Distributed Query Processing 1077\nIn general, updates to the common schema also need to be mapped to updates\nto the individual databases; systems that support a common schema and queries,\nbut not updates, against the schema are sometimes referred to as mediator systems.\n\u2022The data virtualization approach allows applications to access data from multiple\ndatabases/data sources, but it does not try to enforce a common schema. Users\nhave to be aware of the di\ufb00erent schemas used in di\ufb00erent databases, but they do\nnot need to worry about which data are stored on which database system, or about\nhow to combine information from multiple databases.\n\u2022The external data approach allows database administrators to provide schema in-\nformation about data that are stored in other databases, along with other informa-\ntion, such as connection and authorization information needed to access the data.\nData stored in external sources that can be accessed from a database are referred\nto as external data .Foreign tables are views de\ufb01ned in a database whose actual\ndata are stored in an external data source. Such tables can be read as well as up-\ndated, depending on what operations the external data source supports. Updates\non foreign tables, if supported, must be translated into updates on the external\ndata source.\nUnlike the earlier-mentioned approaches, the goal here is not to create a full-\n\ufb02edged distributed database, but merely to facilitate access to data from other\ndata sources. The SQL Management of External Data ( SQL MED ) component\nof the SQL standard de\ufb01nes standards for accessing external data sources from a\ndatabase.\nI fad a t as o u r c ei sad a t a b a s et h a ts u p p o r t s SQL, its data can be easily accessed\nusing ODBC orJDBC connections. Data in parallel data storage systems that do not\nsupport SQL, such as HBase, can be accessed using the APImethods that they provide.\nAwrapper provides a view of data stored at a data source, in a desired schema. For\nexample, if the system has a global schema, and the local database schema is di\ufb00erent\nfrom the global schema, a wrapper can provide a view of the data in the global schema.\nWrappers can even be used to provide a relational view of nonrelational data sources,\nsuch as web services, \ufb02at \ufb01les (e.g., web logs), and directory systems.\nWrappers can also translate queries on the global schema into queries on the lo-\ncal schema, and translate results back into the global schema. Wrappers may be pro-\nvided by individual sites, they may be written as part of the federated database system.\nMany relational databases today support wrappers that provide a relational view of\nd a t as t o r e di n\ufb01 l es y s t e m s ;s u c hw r a p p e r sa r es p e c i \ufb01 ct ot h et y p eo fd a t as t o r e di nt h e\n\ufb01les.\nIf the goal of data integration is solely to run decision support queries, data ware-\nhouses , which we saw in Section 11.2, are a widely used alternative to database inte-\ngration. Data warehouses import data from multiple sources into a single (possibly\nparallel) system, with a centralized schema. Data are typically imported periodically,\n", "1106": "1078 Chapter 22 Parallel and Distributed Query Processing\nfor example, once in a day or once in a few hours, although continuous import is also\nincreasingly used. Raw data imported from the data sources are typically processed\nand cleaned before being stored in the data warehouse.\nHowever, with the scale at which data are generated by some applications, creating\nand maintaining such a warehouse can be expensive. Furthermore, queries cannot ac-\ncess the most recent data, since there is a delay between updates on the source database\nand import of the updates to the data warehouse. On the other hand, query processing\ncan be done more e\ufb03ciently in a data warehouse; further, queries at a data warehouse\ndo not a\ufb00ect the performance of other queries and transactions at the data source.\nWhether to use a data warehouse architecture, or to directly access data from the data\nsources in response to individual queries, is a decision that each enterprise has to make\nbased on its needs.\nThe term data lake is used to refer to an architecture where data are stored in\nmultiple data storage systems and in di\ufb00erent formats, including in \ufb01le systems, but\ncan be queried from a single system. Data lakes are viewed as an alternative to data\nwarehouses, since they do not require up-front e\ufb00ort to preprocess data, although they\ndo require more e\ufb00ort when creating queries.\n22.9.2 Schema and Data Integration\nThe \ufb01rst task in providing a uni\ufb01ed view of data lies in creating a uni\ufb01ed conceptual\nschema, a task that is referred to as schema integration . Each local system provides its\nown conceptual schema. The database system must integrate these separate schemas\ninto one common schema. Schema integration is a complicated task, mainly because\nof the semantic heterogeneity. The same attribute names may appear in di\ufb00erent local\ndatabases but with di\ufb00erent meanings.\nSchema integration requires the creation of a global schema , which provides a uni-\n\ufb01ed view of data in di\ufb00erent databases. Schema integration also requires a way to de\ufb01ne\nhow data are mapped from the local schema representation at each database, to the\nglobal schema. This step can be done by de\ufb01ning views at each site which, transform\ndata from the local schema to the global schema. Data in the global schema is then\ntreated as the union of the global views at the individual site. This approach is called\ntheglobal-as-view (GAV) approach.\nConsider an example with two sites which store student information in two di\ufb00er-\nent ways:\n\u2022Site s1which uses the relation student1 (ID,name ,dept\n name ), and the relation\nstudentCreds (ID,tot\ncred).\n\u2022Sites2which uses the relation student2 (ID,name ,tot\ncred), and the relation stu-\ndentDept (ID,dept\n name ).\nLet the global schema chosen be student (ID,name ,dept\n name ,tot\ncred).\n", "1107": "22.9 Distributed Query Processing 1079\nThen, the global schema view at site s1would be de\ufb01ned as the view:\ncreate view student\n s1(ID,name ,dept\n name ,tot\ncred)as\nselect ID,name ,dept\n name ,tot\ncred\nfrom student1 ,studentCreds\nwhere student1 .ID=studentCreds .ID;\nWhile the global schema view at site s2would be de\ufb01ned as the view:\ncreate view student\n s2(ID,name ,dept\n name ,tot\ncred)as\nselect ID,name ,dept\n name ,tot\ncred\nfrom student2 ,studentDept\nwhere student2 .ID=studentDept .ID;\nFinally, the global schema student would be de\ufb01ned as the union of student\n s1and\nstudent\n s2.\nNote that with the above view de\ufb01nition, a query on the global schema relation\nstudent can be easily translated into queries on the local schema relations at the sites\ns1ands2. It is harder to translate updates on the global schema into updates on the\nlocal schema, since there may not be a uniqu e way to do so as discussed in Section 4.2.\nThere are more complex mapping schemes that are designed to deal with dupli-\ncation of information across sites and to allow translation of updates on the global\nschema into updates on the local schema. The local-as-view (LAV) approach, which\nde\ufb01nes local data in each site as a view on a conceptual uni\ufb01ed global relation, is one\nsuch approach.\nConsider for example a situation where the student relation is partitioned between\ntwo sites based on the dept\n name attribute, with all students in the \u201cComp. Sci.\u201d depart-\nment at site s3 and all students in other departments in site s4. This can be speci\ufb01ed\nusing the local-as-view approach by de\ufb01ning the relations student\n s3and student\n s4,\nwhich are actually stored at the sites s3ands4, as equivalent to views de\ufb01ned on the\nglobal relation student .\ncreate view student\n s3as\nselect\u2217\nfrom student\nwhere student .dept\n name='Comp. Sci.';\nand\ncreate view student\n s4as\nselect\u2217\nfrom student\nwhere student .dept\n name!='Comp. Sci.';\n", "1108": "1080 Chapter 22 Parallel and Distributed Query Processing\nWith this extra information, the query optimizer can \ufb01gure out that a query that seeks\nto retrieve students in the Comp. Sci. department need only be executed at site s3and\nn e e dn o tb ee x e c u t e da ts i t e s4. More information on schema integration may be found\nin the bibliographic notes for this chapter, available online.\nThe second task in providing a uni\ufb01ed view of data from multiple sources lies in\ndealing with di\ufb00erences in data types and values. For example, the data types used\nin one system may not be supported by other systems, and translation between types\nmay not be simple. Even for identical data types, problems may arise from the phys-\nical representation of data: One system may use 8-bit ASCII , while another may use\n16-bit Unicode; \ufb02oating-point representations may di\ufb00er; integers may be represented\ninbig-endian orlittle-endian form. At the semantic level, an integer value for length\nmay be inches in one system and millimeters in another; when integrating the data, a\nsingle representation must be used, and values converted to the chosen representation.\nMapping between types can be done as part of the view de\ufb01nitions that translate data\nbetween the local schemas and the global schema.\nA deeper problem is that the same conceptual entity may have di\ufb00erent names in\ndi\ufb00erent systems. For example, a system based in the United States may refer to the\ncity \u201cCologne,\u201d whereas one in Germany refers to it as \u201cK \u00a8oln.\u201d One approach to deal\nwith this problem is to have a globally unique naming system, and map values to the\nunique names as part of the view de\ufb01nitions used for schema mappings. For example,\nthe International Standards Organization has a unique code for country names, and for\nstates/provinces within the countries. The GeoNames database ( www.geonames.org )\nprovides unique names for several million locations such as cities, geographical fea-\ntures, roads, buildings, and so forth.\nWhen such standard naming systems are not available, some systems allow the\nspeci\ufb01cation of name equivalences; for example, such a system could allow a user to\nsay that \u201cCologne\u201d is the same as \u201cK \u00a8oln\u201d. This approach is used in the Linked Data\nproject, which supports the integration of a very large number of databases that use\ntheRDF representation of data (the RDF representation is described in Section 8.1.4).\nHowever, querying is more complicated in such a scenario.\nIn our description of the view de\ufb01nitions above, we assumed that data are stored in\nlocal databases, and the view de\ufb01nitions are used to provide a global view of the data,\nwithout actually materializing the data in the global schema. However, such views can\nalso be used to materialize the data in the global schema, which can then be stored in\na data warehouse. In the latter case, updates on underlying data must be propagated to\nthe data warehouse.\n22.9.3 Query Processing Across Multiple Data Sources\nA naive way to execute a query that accesses data from multiple data sources is to\nfetch all required data to one database, which then executes the query. But suppose, for\nexample, that the query has a selection condition that is satis\ufb01ed by only one or a few\nrecords out of a large relation. If the data source allows the selection to be performed at\n", "1109": "22.9 Distributed Query Processing 1081\nthe data source, it makes no sense to retrieve the entire relation; instead, the selection\noperation should be performed at the data source, while other operations, if any, may\nbe performed at the database where the query was issued.\nIn general, di\ufb00erent data sources may support di\ufb00erent query capabilities. For ex-\nample, if the source is a data storage system, it may support selections on key attributes\nonly. Web data sources may restrict which \ufb01elds selections are allowed on and may ad-\nditionally require that selections be present on certain \ufb01elds. On the other hand, if the\nsource is a database that supports SQL, operations such as join or aggregation could\nbe performed at the source and only the result brought over to the database that issues\nthe query. In general, queries may have to be broken up and performed partly at the\ndata source and partly at the site issuing the query.\nThe cost of processing a query that accesses multiple data sources depends on\nthe local execution costs, as well as on the data transfer cost. If the network is a low\nbandwidth wide-area network, particular attention must be paid to minimizing data\ntransfer.\nIn this section, we study issues in distributed query processing and optimization.\n22.9.3.1 Join Locations and Join Ordering\nConsider the following relational-algebra expression:\nr1\u22c8r2\u22c8r3\nAssume that r1is stored at site S1,r2atS2,a n d r3atS3.L e t SId e n o t et h es i t ea tw h i c h\nthe query was issued. The system needs to produce the result at site SI.A m o n gt h e\npossible strategies for processing this query are these:\n\u2022Ship copies of all three relations to site SI. Using the techniques of Chapter 16,\nchoose a strategy for processing the entire query locally at site SI.\n\u2022Ship a copy of the r1relation to site S2,a n dc o m p u t e temp1=r1\u22c8r2atS2.S h i p\ntemp1from S2toS3,a n dc o m p u t e temp2=temp1\u22c8r3atS3. Ship the result temp2\ntoSI.\n\u2022Devise strategies similar to the previous one, with the roles of S1,S2,S3exchanged.\nThere are several other possible strategies.\nNo one strategy is always the best one. Among the factors that must be considered\nare the volume of data being shipped, the cost of transmitting a block of data between a\npair of sites, and the relative speed of processi ng at each site. Consider the \ufb01rst strategy.\nSuppose indices present at S2andS3a r eu s e f u lf o rc o m p u t i n gt h ej o i n .I fw es h i pa l l\nthree relations to SI, we would need to either re-create these indices at SIor use a\ndi\ufb00erent, possibly more expensive, join strategy. Re-creation of indices entails extra\nprocessing overhead and extra disk accesses. There are many variants of the second\nstrategy, which process joins in di\ufb00erent orders.\n", "1110": "1082 Chapter 22 Parallel and Distributed Query Processing\nThe cost of each of the strategies depends on the sizes of the intermediate results,\nthe network transmission costs, and the costs of processing at each node. The query\noptimizer needs to choose the best strategy, based on cost estimates.\n22.9.3.2 Semijoin Strategy\nSuppose that we wish to evaluate the expression r1\u22c8r2,w h e r e r1andr2are stored at\nsites S1andS2, respectively. Let the schemas of r1andr2beR1andR2. Suppose that\nwe wish to obtain the result at S1. If there are many tuples of r2that do not join with\nany tuple of r1, then shipping r2toS1entails shipping tuples that fail to contribute to\nthe result. We want to remove such tuples before shipping data to S1,p a r t i c u l a r l yi f\nnetwork costs are high.\nA possible strategy to accomplish all this is:\n1.Compute temp1\u2190\u03a0R1\u2229R2(r1)a tS1.\n2.Ship temp1from S1toS2.\n3.Compute temp2\u2190r2\u22c8temp1atS2.\n4.Ship temp2from S2toS1.\n5.Compute r1\u22c8temp2atS1. The resulting relation is the same as r1\u22c8r2.\nBefore considering the e\ufb03ciency of this strategy, let us verify that the strategy computes\nthe correct answer. In step 3, temp2has the result of r2\u22c8\u03a0R1\u2229R2(r1). In step 5, we\ncompute:\nr1\u22c8r2\u22c8\u03a0R1\u2229R2(r1)\nS i n c ej o i ni sa s s o c i a t i v ea n dc o m m u t a t i v e ,w ec a nr e w r i t et h i se x p r e s s i o na s :\n(r1\u22c8\u03a0R1\u2229R2(r1))\u22c8r2\nSince r1\u22c8\u03a0(R1\u2229R2)(r1)=r1,t h ee x p r e s s i o ni s ,i n d e e d ,e q u a lt o r1\u22c8r2,t h e\nexpression we are trying to evaluate.\nThis strategy is called a semijoin strategy , after the semijoin operator of the rela-\ntional algebra, denoted \u22c9, which we saw in Section 16.4.4. The natural semijoin ofr1\nwith r2, denoted r1\u22c9r2, is de\ufb01ned as:\nr1\u22c9r2\u225d\u03a0R1(r1\u22c8r2)\nThus, r1\u22c9r2selects those tuples of relation r1that contributed to r1\u22c8r2.I ns t e p\n3,temp2=r2\u22c9r1. The semijoin operation is easily extended to theta-joins. The theta\nsemijoin ofr1with r2, denoted r1\u22c9\u03b8r2, is de\ufb01ned as:\nr1\u22c9\u03b8r2\u225d\u03a0R1(r1\u22c8\u03b8r2)\n", "1111": "22.9 Distributed Query Processing 1083\nFor joins of several relations, the semijoin strategy can be extended to a series of\nsemijoin steps. It is the job of the query optimizer to choose the best strategy based on\ncost estimates.\nThis strategy is particularly advantageous when relatively few tuples of r2contribute\nto the join. This situation is likely to occur if r1is the result of a relational-algebra ex-\npression involving selection. In such a case, temp2,t h a ti s , r2\u22c9r1, may have signi\ufb01cantly\nfewer tuples than r2. The cost savings of the strategy result from having to ship only\nr2\u22c9r1, rather than all of r2,t oS1.\nSome additional cost is incurred in shipping temp1,t h a ti s\u03a0R1\u2229R2(r1)t oS2.I fa\nsu\ufb03ciently small fraction of tuples in r2contribute to the join, the overhead of shipping\ntemp1will be dominated by the savings of shipping only a fraction of the tuples in r2.\nThe overhead of sending temp1tuples from s1tos2can be reduced as follows. For the\npurpose of optimization of join processing, the semijoin operation can be implemented\nin a manner that overapproximates the true semijoin result. That is, the result should\ncontain all the tuples in the actual semijoin result, but it may contain a few extra tuples.\nThe extra tuples will get eliminated later by the join operation.\nAn e\ufb03cient overapproximation of the semijoin result can be computed by using a\nprobabilistic data structure called a Bloom \ufb01lter , which uses bitmaps. Bloom \ufb01lters are\ndescribed in more detail in Section 24.1. To implement r2\u22c9r1, a Bloom \ufb01lter with a\nbitmap bof size m, initialized with all bits set to 0 is used. The join attributes of each\ntuple of r1are hashed to a value in the range 0 \u2026(m\u22121), and the corresponding bit\nofbis set to 1. The bitmap b, which is much smaller than the relation r1,c a nn o wb e\nsent to the site containing r2. There, the same hash function is computed on the join\nattributes of each tuple of r2. If the corresponding bit is set to 1 in b,t h a t r2tuple is\naccepted (added to the result relation), and otherwise it is rejected.\nNote that it is possible for di\ufb00erent join attribute values, say v1andv2to have the\nsame hash value; even if r1has a tuple with value v1,b u td o e sn o th a v ea n yt u p l ew i t h\nvalue v2, the result of the above procedure may include r2tuples whose join attribute\nvalue is v2. Such a situation is referred to as a false positive .H o w e v e r ,i f v1is present in\nr1, the technique will never reject a tuple in r2that has join attribute value v1,w h i c hi s\nimportant for correctness.\nThe result relation computed above, which is a superset of or equal to r2\u22c9r1,i s\nsent to site s1.T h ej o i n r1\u22c8result is then computed at site s1to get the required join\nresult. False positives may result in extra tuples in result that are not present in r2\u22c9r1,\nbut such tuples would be eliminated by the join.\nTo keep the probability of false positives low, the number of bits in the Bloom\n\ufb01lter is usually set to a few times the estimated number of distinct join attribute values.\nFurther, it is possible to use kindependent hash functions, for some k>1, to identify\nkbit positions for a given value, and set all of them to 1 when creating the bitmap.\nWhen querying it with a given value, the same khash functions are used to identify k\nbit locations, and the value is determined to be absent if even one of the kbits has a 0\nvalue. For example, if the bitmap has 10 nbits, where nis the number of distinct join\n", "1112": "1084 Chapter 22 Parallel and Distributed Query Processing\nattribute values, and k=7 hash functions are used, the false positive rate would be\nabout 1%.\n22.9.3.3 Distributed Query Optimization\nSeveral extensions need to be made to existing query optimization techniques in order\nto optimize distributed query plans.\nThe \ufb01rst extension is to record the location of data as a physical property of the\ndata; recall that optimizers already deal with other physical properties such as the sort\norder of results. Just as the sort operation is used to create di\ufb00erent sort orders, an\nexchange operation is used to transfer data between di\ufb00erent sites.\nThe second extension is to track where an operator is executed; optimizers already\nconsider di\ufb00erent algorithms, such as hash join or merge join, for a given logical oper-\nator, in this case, the join operator. The optimizer is extended to additionally consider\nalternative sites for execution of each algorithm. Note that to execute an operator at a\ngiven site, its inputs must satisfy the physical property of being located at that site.\nThe third extension is to consider semijoin operations to reduce data transfer costs.\nSemijoin operations can be introduced as logical transformation rules; however, if done\nnaively, the search space increases greatly, making this approach infeasible. Optimiza-\ntion cost can be reduced by restricting, as a heuristic, semijoins to be applied only on\ndatabase tables, and never on intermediate join results.\nA fourth extension is to use schema information to restrict the set of nodes at\nwhich a query needs to be executed. Recall from Section 22.9.2 that the local-as-view\napproach can be used to specify that a relation is partitioned in a particular way. In the\nexample we saw there, site s3contains all student tuples with dept\n name being Comp.\nSci., while s4contains all the other student tuples. Suppose a query has a selection\n\u201cdept\n name ='Comp. Sci.'\u201d on student ; then, the optimizer should recognize that there\nis no need to involve site s4when executing this query.\nAs another example, if the student data at site s5is a replica of the data at site s3,\nthen the optimizer can choose to execute the query at either of the sites, depending on\nwhich is cheaper; there is no need to execute the query at both sites.\n22.9.4 Distributed Directory Systems\nA directory is a listing of information about some class of objects such as persons.\nDirectories can be used to \ufb01nd information about a speci\ufb01c object, or in the reverse\ndirection to \ufb01nd objects that meet a certain requirement. Several directory access proto-\ncolshave been developed to provide a standardized way of accessing data in a directory.\nA very widely used distributed directory system is the internet Domain Name Ser-\nvice(DNS) system, which provides a standardized way to map domain names (such\nasdb-book.com orwww.cs.yale.edu ,t ot h e IPaddresses of the machines. (Although\nusers see only the domain names, the underlying network routes messages based on\nIPaddresses, and hence a way to convert domain names to IPaddresses is critical for\n", "1113": "22.9 Distributed Query Processing 1085\nthe functioning of the internet.) The Lightweight Directory Access Protocol (LDAP )i s\nanother very widely used protocol designed for storing organizational data.\nData stored in directories can be represented in the relational model, stored in a\nrelational database, and accessed through standard protocols such as JDBC orODBC .\nThe question then is, why come up with a specialized protocol for accessing directory\ninformation? There are several reasons.\n\u2022First, directory access protocols are si mpli\ufb01ed protocols that cater to a limited\ntype of access to data. They evolved in parallel with the database access protocol\nstandards.\n\u2022Second, directory systems were designed to support a hierarchical naming system\nfor objects, similar to \ufb01le system directory names. Such a naming system is im-\nportant in many applications. For example, all computers whose names end in\nyale.edu belong to Yale, while those whose names end in iitb.ac.in belong to IIT\nBombay. Within the yale.edu domain, there are subdomains such as cs.yale.edu ,\nwhich corresponds to the CS department in Yale, and math.yale.edu which cor-\nresponds to the Math department at Yale.\n\u2022Third, and most important from a distributed systems perspective, the data in a\ndistributed directory system are stored and controlled in a distributed, hierarchical,\nmanner.\nFor example, a DNS server at Yale would store information about names of\ncomputers at Yale, along with associated information such as their IPaddresses.\nSimilarly, DNS servers at Lehigh and IIT Bombay would store information about\ncomputers in their respective domains. The DNS servers store information in a\nhierarchical fashion; for example, the information provided by the Yale DNS server\nmay be stored in a distributed fashion across subdomains at Yale, such as the CS\nand Math DNS servers.\nDistributed directory systems automatically forward queries submitted at a site\nto the site where the required information is actually stored, to give a uni\ufb01ed view\nof data to users and applications.\nFurther, distributed directory implementations typically support replication to\nensure the availability of data even if some nodes have failed.\nAnother example of usage of directory systems is for organization data. Such sys-\ntems store information about employees, such as the employee identi\ufb01er, name, email,\norganization unit (such as department), room number, phone number, and (encrypted)\npassword of each employee. The schema of such organizational data are standardized\nas part of the Lightweight Directory Access Protocol ( LDAP ). Directory systems based\non the LDAP protocol are widely used to authenticate users, using the encrypted pass-\nwords stored for each user. (More information about the LDAP data representation\nmay be found in Section 25.5.)\n", "1114": "1086 Chapter 22 Parallel and Distributed Query Processing\nAlthough distributed data storage across organizational units was important at one\ntime, such directory systems are often centralized these days. In fact, several direc-\ntory implementations use relational databases to store data, instead of creating special-\npurpose storage systems. However, the fact that the data representation and protocol to\naccess the data are standardized has meant that these protocols continue to be widely\nused.\n22.10 Summary\n\u2022Current generation parallel systems are typically based on a hybrid architecture,\nwhere each computer has multiple cores with a shared memory, and there are\nmultiple computers organized in a shared-nothing fashion.\n\u2022Parallel processing in a database system can be exploited in two distinct ways.\n\u00b0Interquery parallelism\u2014the execution of multiple queries in parallel with each\nother, across multiple nodes.\n\u00b0Intraquery parallelism\u2014the processing of di\ufb00erent parts of the execution of a\nsingle query, in parallel across multiple nodes.\n\u2022Interquery parallelism is essential for transaction-processing systems, while intra-\nquery parallelism is essential for speeding up long-running queries.\n\u2022Execution of a single query involves execution of multiple relational operations.\nThe key to exploiting large-scale parallelism is to process each operation in parallel,\nacross multiple nodes (referred to as intraoperation parallelism).\n\u2022The operations that are the most amenable to parallelism are: sort, selection, du-\nplicate elimination, projection, and aggregation.\n\u2022Range-partitioning sort works in two steps: \ufb01rst range-partitioning the relation,\nthen sorting each partition separately.\n\u2022Parallel external sort-merge works in two steps: \ufb01rst, each node Nisorts the data\navailable at node Ni, then the system merges the sorted runs on each node to get\nthe \ufb01nal sorted output.\n\u2022Parallel join algorithms divide the tuples of the input relations over several nodes.\nEach node then computes part of the join locally. Then the system collects the\nresults from each node to produce the \ufb01nal result.\n\u2022Skew is a major problem, especially with increasing degrees of parallelism. Bal-\nanced partitioning vectors, using histog rams, and virtual node partitioning are\namong the techniques used to reduce skew.\n", "1115": "22.10 Summary 1087\n\u2022The MapReduce paradigm is designed to ease the writing of parallel data process-\ning programs using imperative programming languages that may not be express-\nible using SQL. Fault-tolerant implementations of the MapReduce paradigm are\nimportant for a variety of very large scale data processing tasks. Extensions of the\nMapReduce model based on algebraic operations is increasingly important. The\nHive SQL system uses a MapReduce system as its underlying execution engine,\ncompiling SQL queries to MapReduce code.\n\u2022There are two forms of interoperation parallelism: pipelined parallelism and in-\ndependent parallelism. Pipelined parallelism is usually implemented using a push\nmodel, with bu\ufb00ers between operations.\n\u2022The exchange operation repartitions data in a speci\ufb01ed way. Parallel query plans\ncan be created in such a way that data interchange between nodes is done only by\nthe exchange operator, while all other operations work on local data, just as they\nwould in a centralized database system.\n\u2022In the event of failure, parallel query execution plans could be restarted. However,\nin very large systems where there is a signi\ufb01cant chance of failure during the exe-\ncution of a query, fault tolerance techniques are important to ensure that queries\ncomplete execution without restarting, despite failures.\n\u2022Parallel algorithms designed for shared-nothing architectures can be used in\nshared-memory architectures. Each processor can be treated as having its own par-\ntition of memory, and we can ignore the fact that the processors have a common\nshared memory. However, execution can be optimized signi\ufb01cantly by exploiting\nthe fast access to shared memory from any of the processors.\n\u2022Query optimization for parallel execution can be done using the traditional re-\nsource consumption cost model, or using the response time cost model. Partition-\ning of tables must be taken into account when choosing a plan, to minimize data\nexchange which is often a signi\ufb01cant factor in query execution cost. Materialized\nviews can be important in parallel environments since they can signi\ufb01cantly reduce\nquery execution cost.\n\u2022There are many streaming data applications today that require high performance\nprocessing, which can only be achieved by parallel processing of streaming data.\nIncoming tuples and results of operations need to be routed to other operations.\nThe publish-subscribe model, implemented for example in Apache Kafka, has\nproven quite useful for such routing. Fault-tolerant processing of streaming data\nwith exactly-once semantics for processing tuples is important in many applica-\ntions. Persistence provided by publish-subscribe systems helps in this regard.\n\u2022Integration of schema and data from multiple databases is needed for many data\nprocessing tasks. The external data approach allows external data to be queries in\na database as if it is locally resident. The gl obal-as-view and local-as-view architec-\n", "1116": "1088 Chapter 22 Parallel and Distributed Query Processing\ntures allows rewriting of queries from a global schema to individual local schemas.\nThe semijoin strategy using Bloom \ufb01lters can be useful to reduce data movement\nfor joins in a distributed database. Distributed directory systems are a type of dis-\ntributed database designed for distributed storage and querying of directories.\nReview Terms\n\u2022Interquery parallelism\n\u2022Intraquery parallelism\n\u00b0Intraoperation parallelism\n\u00b0Interoperation parallelism\n\u2022Range-partitioning sort\n\u2022Data parallelism\n\u2022Parallel external sort-merge\n\u2022Partitioned join\n\u2022Partitioned parallel hash join\n\u2022Partitioned parallel merge join\n\u2022Partitioned parallel nested-loop join\n\u2022Partitioned parallel indexed nested-\nloops join\n\u2022Asymmetric fragment-and-replicate\njoin\n\u2022Broadcast join\n\u2022Fragment-and-replicate join\n\u2022Symmetric fragment-and-replicate\njoin\n\u2022Join skew avoidance\n\u2022Dynamic handling of join skew\n\u2022Work stealing\n\u2022Parallel selection\n\u2022Parallel duplicate elimination\n\u2022Parallel projection\n\u2022Parallel aggregation\n\u2022Partial aggregation\n\u2022Intermediate key\n\u2022Pipelined parallelism\u2022Independent parallelism\n\u2022Exchange operator\n\u2022Unpartitioned\n\u2022Random merge\n\u2022Ordered merge\n\u2022Parallel query execution plan\n\u2022Query processing in shared memory\n\u2022Thread\n\u2022Single Instruction Multiple Data\n(SIMD )\n\u2022Response-time cost model\n\u2022Parallel view maintenance\n\u2022Streaming data\n\u2022Lambda architecture\n\u2022Routing of streams\n\u2022Publish-subscribe\n\u2022Topic-partition\n\u2022At-least-once semantics\n\u2022At-most-once semantics\n\u2022Exactly-once semantics\n\u2022Federated database system\n\u2022Global schema\n\u2022Local schema\n\u2022Schema integration\n\u2022Mediator\n\u2022Data virtualization\n\u2022External data\n\u2022Foreign tables\n", "1117": "Practice Exercises 1089\n\u2022Data lake\n\u2022Global-as-view (GAV)\n\u2022Local-as-view (LAV)\n\u2022Linked Data\n\u2022Semijoin strategy\n\u2022Semijoin\u2022Theta semijoin\n\u2022Bloom \ufb01lter\n\u2022False positive\n\u2022Directory access protocols\n\u2022Domain Name Service (DNS)\n\u2022Lightweight Directory Access Proto-\ncol (LDAP)\nPractice Exercises\n22.1 What form of parallelism (interquery, interoperation, or intraoperation) is\nlikely to be the most important for each of the following tasks?\na. Increasing the throughput of a system with many small queries\nb. Increasing the throughput of a system with a few large queries when the\nnumber of disks and processors is large\n22.2 Describe how partial aggregation can be implemented for the count and avg\naggregate functions to reduce data transfer.\n22.3 With pipelined parallelism, it is often a good idea to perform several operations\nin a pipeline on a single processor, even when many processors are available.\na. Explain why.\nb. Would the arguments you advanced in part ahold if the machine has a\nshared-memory architecture? Explain why or why not.\nc. Would the arguments in part ahold with independent parallelism? (That\nis, are there cases where, even if the operations are not pipelined and\nthere are many processors available, it is still a good idea to perform\nseveral operations on the same processor?)\n22.4 Consider join processing using symmetric fragment and replicate with range\npartitioning. How can you optimize the evaluation if the join condition is of\nthe form\u2223r.A\u2212s.B\u2223\u2264k,w h e r e kis a small constant? Here, \u2223x\u2223denotes\nthe absolute value of x. A join with such a join condition is called a band join .\n22.5 Suppose relation ris stored partitioned and indexed on A,a n d sis stored par-\ntitioned and indexed on B. Consider the query:\nr.C\u03b3count( s.D)((\u03c3A>5(r))\u22c8r.B=s.Bs)\na. Give a parallel query plan using the exchange operator, for computing\nthe subtree of the query involving only the select and join operators.\n", "1118": "1090 Chapter 22 Parallel and Distributed Query Processing\nb. Now extend the above to compute the aggregate. Make sure to use pre-\naggregation to minimize the data transfer.\nc. Skew during aggregation is a serious problem. Explain how pre-\naggregation as above can also signi\ufb01cantly reduce the e\ufb00ect of skew dur-\ning aggregation.\n22.6 Suppose relation ris stored partitioned and indexed on A,a n d sis stored parti-\ntioned and indexed on B.C o n s i d e rt h ej o i n r\u22c8r.B=s.Bs. Suppose sis relatively\nsmall, but not small enough to make asymmetric fragment-and-replicate join\nthe best choice, and ris large, with most rtuples not matching any stuple. A\nhash-join can be performed but with a semijoin \ufb01lter used to reduce the data\ntransfer. Explain how semijoin \ufb01ltering using Bloom \ufb01lters would work in this\nparallel join setting.\n22.7 Suppose you want to compute r\u27d5r.A=s.As.\na. Suppose sis a small relation, while ris stored partitioned on r.B.G i v e\nan e\ufb03cient parallel algorithm for computing the left outer join.\nb. Now suppose that ris a small relation, and sis a large relation, stored\npartitioned on attribute s.B. Give an e\ufb03cient parallel algorithm for com-\nputing the above left outer join.\n22.8 Suppose you want to computeA,B\u03b3sum(C)on a relation swhich is stored par-\ntitioned on s.B. Explain how you would do it e\ufb03ciently, minimizing/avoiding\nrepartitioning, if the number of distinct s.Bvalues is large, and the distribution\nof number of tuples with each s.Bvalue is relatively uniform.\n22.9 MapReduce implementations provide fault tolerance, where you can reexecute\nonly failed mappers or reducers. By default, a partitioned parallel join execu-\ntion would have to be rerun completely in case of even one node failure. It is\npossible to modify a parallel partitioned join execution to add fault tolerance\nin a manner similar to MapReduce, so failure of a node does not require full\nreexecution of the query, but only actions related to that node. Explain what\nneeds to be done at the time of partitioning at the sending node and receiving\nnode to do this.\n22.10 If a parallel data-store is used to store two relations randsand we need to join\nrands, it may be useful to maintain the join as a materialized view. What are\nthe bene\ufb01ts and overheads in terms of overall throughput, use of space, and\nresponse time to user queries?\n22.11 Explain how each of the following join algorithms can be implemented using\nthe MapReduce framework:\na. Broadcast join (also known as asymmetric fragment-and-replicate join).\n", "1119": "Practice Exercises 1091\nb. Indexed nested loop join, where the inner relation is stored in a parallel\ndata-store.\nc. Partitioned join.\nExercises\n22.12 Can partitioned join be used for r\u22c8r.A<s.A\u2227r.B=s.Bs? Explain your answer.\n22.13 Describe a good way to parallelize each of the following:\na. The di\ufb00erence operation\nb. Aggregation by the count operation\nc. Aggregation by the count distinct operation\nd. Aggregation by the avgoperation\ne. Left outer join, if the join condition involves only equality\nf. Left outer join, if the join condition involves comparisons other than\nequality\ng. Full outer join, if the join condition involves comparisons other than\nequality\n22.14 Suppose you wish to handle a workload consisting of a large number of small\ntransactions by using shared-nothing parallelism.\na. Is intraquery parallelism required in such a situation? If not, why, and\nwhat form of parallelism is appropriate?\nb. What form of skew would be of signi\ufb01cance with such a workload?\nc. Suppose most transactions accessed one account record, which includes\nanaccount\n typeattribute, and an associated account\n type\n master record,\nwhich provides information about the account type. How would you par-\ntition and/or replicate data to speed up transactions? You may assume\nthat the account\n type\n master relation is rarely updated.\n22.15 What is the motivation for work-stealing with virtual nodes in a shared-memory\nsetting? Why might work-stealing not be as e\ufb03cient in a shared-nothing set-\nting?\n22.16 The attribute on which a relation is partitioned can have a signi\ufb01cant impact\non the cost of a query.\na. Given a workload of SQL queries on a single relation, what attributes\nwould be candidates for partitioning?\n", "1120": "1092 Chapter 22 Parallel and Distributed Query Processing\nb. How would you choose between the alternative partitioning techniques,\nbased on the workload?\nc. Is it possible to partition a relation on more than one attribute? Explain\nyour answer.\n22.17 Consider system that is processing a stream of tuples for a relation rwith\nattributes ( A,B,C,timestamp ) Suppose the goal of a parallel stream processing\nsystem is to compute the number of tuples for each Avalue in each 5 minute\nwindow (based on the timestamp of the tuple). What would be the topic and\nthe topic partitions? Explain why.\nTools\nA wide variety of open-source tools are available, in addition to some commercial tools,\nfor parallel query processing. A number of these tools are also available on hosted cloud\nplatforms.\nTeradata was one of the \ufb01rst commercial parallel database systems, and it continues\nto have a large market share. The Red Brick Warehouse was another early parallel\ndatabase system; Red Brick was acquired by Informix, which was itself acquired by\nIBM. Other commercial parallel database systems include Teradata Aster Data, IBM\nNetezza, and Pivotal Greenplum. IBM Netezza, Pivotal Greenplum, and Teradata Aster\nData all use Postgre SQL as the underlying database, running independently on each\nnode; each of these systems builds a layer on top, to partition data, and parallelize\nquery processing across the nodes.\nThe open source Hadoop platform ( hadoop.apache.org ) includes the HDFS dis-\ntributed \ufb01le system and the Hadoop MapReduce platform. Systems that support SQL\non a MapReduce platform include Apache Hive ( hive.apache.org ), which originated\nat Facebook, Apache Impala ( impala.apache.org ), which originated at Cloudera,\nand Apache HAWQ (hawq.apache.org ), which originated at Pivotal. Apache Spark\n(spark.apache.org ), which originated at the Univ. of California, Berkeley, and Apache\nTez ( tez.apache.org ) are parallel execution frameworks that support a variety of op-\nerators beyond the basic map and reduce operators; and Hive SQL queries can be exe-\ncuted on both these platforms. Other parallel execution frameworks include Apache Pig\n(pig.apache.org ), which originated at Yahoo!, the Asterix system ( asterix.ics.uci.edu ),\nwhich originated at the University of California, Irvine, the Apache Flink system\n(flink.apache.org ), which originated as the Stratosphere project at the Technical Uni-\nversity, Berlin, Humboldt University and the Hasso-Plattner Institute). Apache Spark\nand Apache Flink also support libraries for parallel machine learning.\nThese systems can access data stored in multiple di\ufb00erent data formats, such as\n\ufb01les in di\ufb00erent formats in HDFS , or objects in a storage system such as HBase. Hadoop\n\ufb01le formats were initially textual \ufb01les, but today Hadoop implementations support sev-\neral optimized \ufb01le formats such as Sequence \ufb01les (which allow binary data), Avro\n", "1121": "Further Reading 1093\n(which supports semi-structured schemas) and Parquet (which supports columnar data\nrepresentation).\nApache Kafka ( kafka.apache.org ) is widely used for routing tu-\nples in streaming data systems. Systems designed for query process-\ning on streaming data include Apache Storm ( storm.apache.org ),\nKafka Streams ( kafka.apache.org/documentation/streams/ )a n dH e r o n\n(apache.github.io/incubator-heron/ ), developed by Twitter. Apache Flink\n(flink.apache.org ), Spark Streaming ( spark.apache.org/streaming/ ), the streaming\ncomponent of Apache Spark, and Apache Apex ( apex.apache.org ) support analytics\non streaming data along with analytics on stored data.\nMany of the above systems are also o\ufb00ered as cloud-based services, as part of the\ncloud services o\ufb00ered by Amazon AWS, Google Cloud, Microsoft Azure, and other\nsimilar platforms.\nFurther Reading\nEarly work on parallel database systems include GAMMA ([DeWitt (1990)]), XPRS\n([Stonebraker et al. (1989)]) and Volcano ([G raefe (1990)]). [Graefe (1993)] presents\nan excellent survey of query processing, including parallel processing of queries. The\nexchange-operator model was advocated by [Graefe (1990)] and [Graefe and McKenna\n(1993)]. Skew handling in parallel joins is described by [DeWitt et al. (1992)].\n[Ganguly et al. (1992)] describe query-optimization techniques based on the\nresponse-time cost model for parallel query execution, while [Zhou et al. (2010)] de-\nscribe how to extend a query optimizer to account for partitioning properties and par-\nallel plans. View maintenance in parallel data storage systems is described by [Agrawal\net al. (2009)].\nA fault-tolerant implementation of the MapReduce framework at Google, which\nlead to the widespread use of the MapReduce paradigm, is described by [Dean and\nGhemawat (2010)]. [Kwon et al. (2013)] provide an overview of skew handling in the\nHadoop MapReduce framework. [Herodotou and Babu (2013)] describe how to opti-\nmize a number of parameters for query execution in the MapReduce framework. An\noverview of the Apache Spark system is provided by [Zaharia et al. (2016)], while [Za-\nharia et al. (2012)] describe Resilient Distributed Datasets, a fault-tolerant abstraction\nwhich formed a basis for the Spark implementation. Extensions of the exchange op-\nerator to support fault-tolerance, are described by [Shah et al. (2004)], with a focus\non fault-tolerant continuous queries. Fault-tolerant stream processing in the Google\nMillWheel system is described in [Akidau et al. (2013)].\nThe morsel-driven approach to parallel query evaluation in shared-memory sys-\ntems with multi-core processors is described in [Leis et al. (2014)]. [Kersten et al.\n(2018)] provides a comparison of vectorwise query processing using optimizations\nsuch as SIMD instructions, with producer-driven pipelining.\n", "1122": "1094 Chapter 22 Parallel and Distributed Query Processing\n[Carbone et al. (2015)] describe stream and batch processing in Apache Flink.\n[Ozsu and Valduriez (2010)] provides textbook coverage of distributed database sys-\ntems.\nBibliography\n[Agrawal et al. (2009)] P. Agrawal, A. Silberstein, B. F. Cooper, U. Srivastava, and R. Ra-\nmakrishnan, \u201cAsynchronous view maintenance for VLSD databases\u201d, In Proc. of the ACM\nSIGMOD Conf. on Management of Data (2009), pages 179\u2013192.\n[Akidau et al. (2013)] T. Akidau, A. Balikov, K. Bekiro\u02d8 glu, S. Chernyak, J. Haberman,\nR. Lax, S. McVeety, D. Mills, P. Nordstrom, an d S. Whittle, \u201cMillWheel: Fault-tolerant\nStream Processing at Internet Scale\u201d, Proceedings of the VLDB Endowment ,V o l u m e6 ,N u m -\nber 11 (2013), pages 1033\u20131044.\n[Carbone et al. (2015)] P. Carbone, A. Katsifodimos, S. Ewen, V. Markl, S. Haridi, and\nK. Tzoumas, \u201cApache Flink: Stream and Batch Processing in a Single Engine\u201d, IEEE Data\nEng. Bull. , Volume 38, Number 4 (2015), pages 28\u201338.\n[Dean and Ghemawat (2010)] J. Dean and S. Ghemawat, \u201cMapReduce: a \ufb02exible data pro-\ncessing tool\u201d, Communications of the ACM , Volume 53, Number 1 (2010), pages 72\u201377.\n[DeWitt (1990)] D. DeWitt, \u201cThe Gamma Database Machine Project\u201d, IEEE Transactions\non Knowledge and Data Engineering , Volume 2, Number 1 (1990), pages 44\u201362.\n[DeWitt et al. (1992)] D. DeWitt, J. Naughton, D. Schneider, and S. Seshadri, \u201cPractical\nSkew Handling in Parallel Joins\u201d, In Proc. of the International Conf. on Very Large Databases\n(1992), pages 27\u201340.\n[Ganguly et al. (1992)] S. Ganguly, W. Hasan, and R. Krishnamurthy, \u201cQuery Optimization\nfor Parallel Execution\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data (1992),\npages 9\u201318.\n[Graefe (1990)] G. Graefe, \u201cEncapsulation of Parallelism in the Volcano Query Processing\nSystem\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data (1990), pages 102\u2013111.\n[Graefe (1993)] G. Graefe, \u201cQuery Evaluation Techniques for Large Databases\u201d, ACM Com-\nputing Surveys , Volume 25, Number 2 (1993).\n[Graefe and McKenna (1993)] G. Graefe and W. McKenna, \u201cThe Volcano Optimizer Gen-\nerator\u201d, In Proc. of the International Conf. on Data Engineering (1993), pages 209\u2013218.\n[Herodotou and Babu (2013)] H. Herodotou and S. Babu, \u201cA What-if Engine for Cost-based\nMapReduce Optimization\u201d, IEEE Data Eng. Bull. , Volume 36, Number 1 (2013), pages 5\u201314.\n[Kersten et al. (2018)] T. Kersten, V. Leis, A. Kemper, T. Neumann, A. Pavlo, and P. A.\nBoncz, \u201cEverything You Always Wanted to Know About Compiled and Vectorized Queries\nBut Were Afraid to Ask\u201d, Proceedings of the VLDB Endowment , Volume 11, Number 13 (2018),\npages 2209\u20132222.\n", "1123": "Further Reading 1095\n[Kwon et al. (2013)] Y. Kwon, K. Ren, M. Balazinska, and B. Howe, \u201cManaging Skew in\nHadoop\u201d, IEEE Data Eng. Bull. , Volume 36, Number 1 (2013), pages 24\u201333.\n[Leis et al. (2014)] V. Leis, P. A. Boncz, A. Kemper, and T. Neumann, \u201cMorsel-driven par-\nallelism: a NUMA-aware query evaluation framework for the many-core age\u201d, In Proc. of the\nACM SIGMOD Conf. on Management of Data (2014), pages 743\u2013754.\n[Ozsu and Valduriez (2010)] T. Ozsu and P. Valduriez, Principles of Distributed Database Sys-\ntems, 3rd edition, Prentice Hall (2010).\n[Shah et al. (2004)] M. A. Shah, J. M. Hellerstein, and E. A. Brewer, \u201cHighly-Available,\nFault-Tolerant, Parallel Data\ufb02ows\u201d, In Proc. of the ACM SIGMOD Conf. on Management of\nData (2004), pages 827\u2013838.\n[Stonebraker et al. (1989)] M. Stonebraker, P. Aoki, and M. Seltzer, \u201cParallelism in XPRS\u201d,\nInProc. of the ACM SIGMOD Conf. on Management of Data (1989).\n[Zaharia et al. (2012)] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly,\nM. J. Franklin, S. Shenker, and I. Stoica, \u201cResilient Distributed Datasets: A Fault-Tolerant\nAbstraction for In-Memory Cluster Computing\u201d, In Procs. USENIX Symposium on Networked\nSystems Design and Implementation, NSDI (2012), pages 15\u201328.\n[Zaharia et al. (2016)] M. Zaharia, R. S. Xin, P. Wendell, T. Das, M. Armbrust, A. Dave,\nX. Meng, J. Rosen, S. Venkataraman, M. J. Franklin, A. Ghodsi, J. Gonzalez, S. Shenker,\nand I. Stoica, \u201cApache Spark: a uni\ufb01ed engine for big data processing\u201d, Communications of\nthe ACM , Volume 59, Number 11 (2016), pages 56\u201365.\n[Zhou et al. (2010)] J. Zhou, P. Larson, and R. Chaiken, \u201cIncorporating partitioning and par-\nallel plans into the SCOPE optimizer\u201d, In Proc. of the International Conf. on Data Engineering\n(2010), pages 1060\u20131071.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "1124": "", "1125": "CHAPTER23\nParallel and Distributed\nTransaction Processing\nWe studied transaction processing in centralized databases earlier, covering concur-\nrency control in Chapter 18 and recovery in Chapter 19. In this chapter, we study how\nto carry out transaction processing in parallel and distributed databases. In addition\nto supporting concurrency control and recovery, transaction processing in parallel and\ndistributed databases must also deal with issues due to replication of data, and of fail-\nures of some nodes.\nBoth parallel and distributed databases have multiple nodes, which can fail in-\ndependently. The main di\ufb00erence between parallel and distributed databases from the\nview point of transaction processing is that the latency of remote access is much higher,\nand bandwidth lower, in a distributed database than in a parallel database where all\nnodes are in a single data center. Failures such as network partitioning and message\ndelays are much less likely within a data center than across geographically distributed\nsites, but nevertheless they can occur; transaction processing must be done correctly\neven if they do occur.\nThus, most techniques for transaction processing are common to both parallel and\ndistributed databases. In the few cases where there is a di\ufb00erence, we explicitly point\nout the di\ufb00erence. And as a result, in this chapter, whenever we say that a technique\nis applicable to distributed databases, it should be interpreted to mean that it is appli-\ncable to distributed databases as well as to parallel databases, unless we explicitly say\notherwise.\nIn Section 23.1, we outline a model for transaction processing in a distributed\ndatabase. In Section 23.2, we describe how to implement atomic transactions in a dis-\ntributed database by using special commit protocols.\nIn Section 23.3 we describe how to extend traditional concurrency control tech-\nniques to distributed databases. Section 23. 4 describes concurrency control techniques\nfor the case where data items are replicated, while Section 23.5 describes further ex-\ntensions including how multiversion concurrency control techniques can be extended\nto deal with distributed databases, and concurrency control can be implemented with\n1097\n", "1126": "1098 Chapter 23 Parallel and Distributed Transaction Processing\nheterogeneous distributed databases. Replication with weak degrees of consistency is\ndiscussed in Section 23.6.\nMost techniques for dealing with distributed data require the use of coordinators to\nensure consistent and e\ufb03cient transaction processing. In Section 23.7 we discuss how\ncoordinators can be chosen in a distributed fashion, robust to failures. Finally, Section\n23.8 describes the distributed consensus problem, outlines solutions for the problem,\nand then discusses how these solutions can be used to implement fault-tolerant services\nby means of replication of a log.\n23.1 Distributed Transactions\nAccess to the various data items in a distributed system is usually accomplished through\ntransactions, which must preserve the ACID properties (Section 17.1). There are two\ntypes of transaction that we need to consider. The local transactions are those that\naccess and update data in only one local database; the global transactions are those that\naccess and update data in several local databases. Ensuring the ACID properties of the\nlocal transactions can be done as described in Chapter 17, Chapter 18, and Chapter\n19. However, for global transactions, this task is much more complicated, since several\nnodes may be participating in the execution of the transaction. The failure of one of\nthese nodes, or the failure of a communication link connecting these nodes, may result\nin erroneous computations.\nIn this section, we study the system structure of a distributed database and its\npossible failure modes. In later s ections, we discuss how to ensure ACID properties are\nsatis\ufb01ed in a distributed database, despite failures. We reemphasize that these failure\nmodes occur with parallel databases as well, and the techniques we describe are equally\napplicable to parallel databases.\n23.1.1 System Structure\nWe now consider a system structure with multiple nodes, each of which can fail inde-\npendently of the others. We note that the nodes may be within a single data center,\ncorresponding to a parallel database system, or geographically distributed, in a dis-\ntributed database system. The system structure is similar in either case; the problems\nwith respect to transaction isolation and atomicity are the same in both cases, as are\nthe solutions.\nWe note that the system structure we consider here is not applicable to a shared-\nmemory parallel database system whose components do not have independent modes\nof failures. In such systems either the whole system is up, or the whole system is down.\nFurther, there is usually only one transaction log used for recovery. Concurrency con-\ntrol and recovery techniques that are designed for centralized database systems can be\nused in such systems, and are preferable to techniques described in this chapter.\nEach node has its own local transaction manager, whose function is to ensure the\nACID properties of those transactions that execute at that node. The various trans-\n", "1127": "23.1 Distributed Transactions 1099\nTM1 TMn\ncomputer 1 computer nTC1 TCntransaction\ncoordinator\ntransaction\nmanager\nFigure 23.1 System architecture.\naction managers cooperate to execute global transactions. To understand how such a\nmanager can be implemented, consider an abstract model of a transaction system, in\nwhich each node contains two subsystems:\n\u2022Thetransaction manager manages the execution of those transactions (or subtrans-\nactions) that access data stored in the node. Note that each such transaction may\nbe either a local transaction (i.e., a transaction that executes at only that node) or\npart of a global transaction (i.e., a transaction that executes at several nodes).\n\u2022The transaction coordinator coordinates the execution of the various transactions\n(both local and global) initiated at that node.\nThe overall system architecture appears in Figure 23.1.\nThe structure of a transaction manager is similar in many respects to the structure\nof a centralized system. Each transaction manager is responsible for:\n\u2022Maintaining a log for recovery purposes.\n\u2022Participating in an appropriate concurrency-control scheme to coordinate the con-\ncurrent execution of the transactions executing at that node.\nAs we shall see, we need to modify both the recovery and concurrency schemes to\naccommodate the distributed execution of transactions.\nThe transaction coordinator subsystem is not needed in the centralized environ-\nment, since a transaction accesses data at only a single node. A transaction coordinator,\nas its name implies, is responsible for coordinating the execution of all the transactions\ninitiated at that node. For each such transaction, the coordinator is responsible for:\n", "1128": "1100 Chapter 23 Parallel and Distributed Transaction Processing\n\u2022Starting the execution of the transaction.\n\u2022Breaking the transaction into a number of subtransactions and distributing these\nsubtransactions to the appropriate nodes for execution.\n\u2022Coordinating the termination of the transaction, which may result in the transac-\ntion being committed at all nodes or aborted at all nodes.\n23.1.2 System Failure Modes\nA distributed system may su\ufb00er from the same types of failure that a centralized sys-\ntem does (e.g., software errors, hardware errors, or disk crashes). There are, however,\nadditional types of failure with which we need to deal in a distributed environment.\nThe basic failure types are:\n\u2022Failure of a node.\n\u2022Loss of messages.\n\u2022Failure of a communication link.\n\u2022Network partition.\nThe loss or corruption of messages is always a possibility in a distributed system.\nThe system uses transmission-control protocols, such as TCP/IP ,t oh a n d l es u c he r r o r s .\nInformation about such protocols may be found in standard textbooks on networking.\nHowever, if two nodes Aand Bare not directly connected, messages from one to the\nother must be routed through a sequence of communication links. If a communication\nlink fails, messages that would have been transmitted across the link must be rerouted.\nIn some cases, it is possible to \ufb01nd another route through the network, so that the\nmessages are able to reach their destination. In other nodes, a failure may result in\nthere being no connection between some pairs of nodes. A system is partitioned if it\nhas been split into two (or more) subsystems, called partitions , that lack any connection\nbetween them. Note that, under this de\ufb01nition, a partition may consist of a single node.\n23.2 Commit Protocols\nIf we are to ensure atomicity, all the nodes in which a transaction Texecuted must\nagree on the \ufb01nal outcome of the execution. Tmust either commit at all nodes, or it\nmust abort at all nodes. To ensure this property, the transaction coordinator of Tmust\nexecute a commit protocol.\nAmong the simplest and most widely used commit protocols is the two-phase com-\nmit protocol (2PC), which is described in Section 23.2.1.\n", "1129": "23.2 Commit Protocols 1101\n23.2.1 Two-Phase Commit\nWe \ufb01rst describe how the two-phase commit protocol ( 2PC)o p e r a t e sd u r i n gn o r m a l\noperation, then describe how it handles failures and \ufb01nally how it carries out recovery\nand concurrency control.\nConsider a transaction Tinitiated at node Ni, where the transaction coordinator\nisCi.\n23.2.1.1 The Commit Protocol\nWhen Tcompletes its execution\u2014that is, when all the nodes at which Thas executed\ninform Cithat Thas completed\u2014 Cistarts the 2PCprotocol.\n\u2022Phase 1 .Ciadds the record <prepare T>to the log and forces the log onto stable\nstorage. It then sends a prepare Tmessage to all nodes at which Texecuted.\nOn receiving such a message, the transaction manager at that node determines\nwhether it is willing to commit its portion of T. If the answer is no, it adds a\nrecord <noT>to the log and then responds by sending an abort Tmessage to\nCi. If the answer is yes, it adds a record <ready T>to the log and forces the log\n(with all the log records corresponding to T) onto stable storage. The transaction\nmanager then replies with a ready Tmessage to Ci.\n\u2022Phase 2 .W h e n Cireceives ready responses to the prepare Tmessage from all\nthe nodes, or when it receives an abort Tmessage from at least one participant\nnode, Cican determine whether the transaction Tcan be committed or aborted.\nTransaction Tcan be committed if Cireceived a ready Tmessage from all the\nparticipating nodes. Otherwise, transaction Tmust be aborted. Depending on the\nverdict, either a record <commit T>or a record <abort T>is added to the log\nand the log is forced onto stable storage. At this point, the fate of the transaction\nhas been sealed.\nFollowing this point, the coordinator sends either a commit Tor an abort T\nmessage to all participating nodes. When a node receives that message, it records\nthe result (either <commit T>or<abort T>) in its log, and correspondingly\neither commits or aborts the transaction.\nSince nodes may fail, the coordinator cannot wait inde\ufb01nitely for responses\nfrom all the nodes. Instead, when a prespeci\ufb01ed interval of time has elapsed since\ntheprepare Tmessage was sent out, if any node has not responded to the coor-\ndinator, the coordinator can decide to abort the transaction; the steps described\nfor aborting the transaction must be followed, just as if a node had sent an abort\nmessage for the transaction.\nFigure 23.2 shows an instance of successful execution of 2PCfor a transaction T,\nwith two nodes, N1and N2, that are both willing to commit transaction T.I fa n yo ft h e\n", "1130": "1102 Chapter 23 Parallel and Distributed Transaction Processing\nCoordinator Node N 1 Node N 2\nForce log record \n   < prepare T > \nSend message\n      prepare T\nForce log record \n    < commit T > \nSend message\n       commit T\nForce log record \n    < commit T >Force log record \n    < commit T >Force log record \n    < ready T > \nSend message\n       ready TForce log record \n    < ready T > \nSend message\n      ready T\nFigure 23.2 Successful execution of 2PC.\nnodes sends a noTmessage, the coordinator will send an abort Tmessage to all the\nnodes, which will then abort the transaction.\nAn o d ea tw h i c h Texecuted can unconditionally abort Tat any time before it sends\nthe message ready Tto the coordinator. Once the <ready T>log record is written,\nthe transaction Tis said to be in the ready state at the node. The ready Tmessage\nis, in e\ufb00ect, a promise by a node to follow the coordinator\u2019s order to commit Tor to\nabort T. To make such a promise, the needed information must \ufb01rst be stored in stable\nstorage. Otherwise, if the node crashes after sending ready T, it may be unable to make\ngood on its promise. Further, locks acquired by the transaction must continue to be\nheld until the transaction completes, even if there is an intervening node failure, as we\nshall see in Section 23.2.1.3.\n", "1131": "23.2 Commit Protocols 1103\nSince unanimity is required to commit a transaction, the fate of Tis sealed as soon\nas at least one node responds abort T. Since the coordinator node Niis one of the\nnodes at which Texecuted, the coordinator can decide unilaterally to abort T.T h e\n\ufb01nal verdict regarding Tis determined at the time that the coordinator writes that\nverdict (commit or abort) to the log and forces that verdict to stable storage.\nIn some implementations of the 2PC protocol, a node sends an acknowledge T\nmessage to the coordinator at the end of the second phase of the protocol. When the\ncoordinator receives the acknowledge Tmessage from all the nodes, it adds the record\n<complete T>to the log. Until this step, the coordinator cannot forget about the\ncommit or abort decision on T, since a node may ask for the decision. (A node that\nhas not received a commit orabort for transaction T, perhaps due to a network failure\nor temporary node failure, may send such a request to the coordinator.) After this step,\nthe coordinator can discard information about transaction T.\n23.2.1.2 Handling of Failures\nThe 2PCprotocol responds in di\ufb00erent ways to various types of failure:\n\u2022Failure of a participating node . If the coordinator Cidetects that a node has failed,\nit takes these actions: If the node fails before responding with a ready Tmessage\ntoCi, the coordinator assumes that it responded with an abort Tmessage. If the\nnode fails after the coordinator has received the ready Tmessage from the node,\nthe coordinator executes the rest of the commit protocol in the normal fashion,\nignoring the failure of the node.\nWhen a participating node Nkrecovers from a failure, it must examine its log\nto determine the fate of those transactions that were in the midst of execution\nwhen the failure occurred. Let Tbe one such transaction. We consider each of the\npossible cases:\n\u00b0The log contains a <commit T>r e c o r d .I nt h i sc a s e ,t h en o d ee x e c u t e s\nredo(T).\n\u00b0The log contains an <abort T>r e c o r d .I nt h i sc a s e ,t h en o d ee x e c u t e s\nundo (T).\n\u00b0The log contains a <ready T>r e c o r d .I nt h i sc a s e ,t h en o d em u s tc o n s u l t Cito\ndetermine the fate of T.I fCiis up, it noti\ufb01es Nkregarding whether Tcommitted\nor aborted. In the former case, it executes redo(T); in the latter case, it executes\nundo (T). If Ciis down, Nkmust try to \ufb01nd the fate of Tfrom other nodes. It\ndoes so by sending a querystatus Tmessage to all the nodes in the system.\nOn receiving such a message, a node must consult its log to determine whether\nThas executed there, and if Thas, whether Tcommitted or aborted. It then\nnoti\ufb01es Nkabout this outcome. If no node has the appropriate information\n(i.e., whether Tcommitted or aborted), then Nkcan neither abort nor commit\nT. The decision concerning Tis postponed until Nkcan obtain the needed\n", "1132": "1104 Chapter 23 Parallel and Distributed Transaction Processing\ninformation. Thus, Nkmust periodically resend the querystatus message to\nthe other nodes. It continues to do so until a node that contains the needed\ninformation has recovered. Note that the node at which Ciresides always has\nthe needed information.\n\u00b0The log contains no control records ( abort, commit, ready )c o n c e r n i n g T.\nThus, we know that Nkfailed before responding to the prepare Tmessage\nfrom Ci. Since the failure of Nkprecludes the sending of such a response, by\nour algorithm Cimust abort T.H e n c e , Nkmust execute undo (T).\n\u2022Failure of the coordinator . If the coordinator fails in the midst of the execution of\nthe commit protocol for transaction T, then the participating nodes must decide\nthe fate of T. We shall see that, in certain cases, the participating nodes cannot\ndecide whether to commit or abort T, and therefore these nodes must wait for the\nrecovery of the failed coordinator.\n\u00b0If an active node contains a <commit T>record in its log, then Tmust be\ncommitted.\n\u00b0If an active node contains an <abort T>record in its log, then Tmust be\naborted.\n\u00b0If some active node does notcontain a <ready T>record in its log, then the\nfailed coordinator Cicannot have decided to commit T,b e c a u s ean o d et h a t\ndoes not have a <ready T>record in its log cannot have sent a ready Tmes-\nsage to Ci. However, the coordinator may have decided to abort T,b u tn o tt o\ncommit T. Rather than wait for Cito recover, it is preferable to abort T.\n\u00b0If none of the preceding cases holds, then all active nodes must have a <ready\nT>record in their logs, but no additional control records (such as <abort T>\nor<commit T>). Since the coordinator has failed, it is impossible to determine\nwhether a decision has been made, and if one has, what that decision is, until\nthe coordinator recovers. Thus, the active nodes must wait for Cito recover.\nSince the fate of Tremains in doubt, Tmay continue to hold system re-\nsources. For example, if locking is used, Tmay hold locks on data at active\nnodes. Such a situation is undesirable, because it may be hours or days before\nCiis again active. During this time, other transactions may be forced to wait\nforT. As a result, data items may be unavailable not only on the failed node\n(Ci), but on active nodes as well. This situation is called the blocking problem ,\nbecause Tis blocked pending the recovery of node Ci.\n\u2022Network partition . When a network partition occurs, two possibilities exist:\n1.The coordinator and all its participants remain in one partition. In this case,\nthe failure has no e\ufb00ect on the commit protocol.\n", "1133": "23.2 Commit Protocols 1105\n2.The coordinator and its participants belong to several partitions. From the\nviewpoint of the nodes in one of the partitions, it appears that the nodes in\nother partitions have failed. Nodes that are not in the partition containing\nthe coordinator simply execute the protocol to deal with the failure of the\ncoordinator. The coordinator and the nodes that are in the same partition as\nthe coordinator follow the usual commit protocol, assuming that the nodes\nin the other partitions have failed.\nThus, the major disadvantage of the 2PCprotocol is that coordinator failure may result\nin blocking, where a decision either to commit or to abort Tm a yh a v et ob ep o s t p o n e d\nuntil Cirecovers. We discuss how to remove this limitation shortly, in Section 23.2.2.\n23.2.1.3 Recovery and Concurrency Control\nWhen a failed node restarts, we can perform recovery by using, for example, the recov-\nery algorithm described in Section 19.4. To deal with distributed commit protocols, the\nrecovery procedure must treat in-doubt transactions specially; in-doubt transactions are\ntransactions for which a <ready T>log record is found, but neither a <commit T>\nlog record nor an <abort T>log record is found. The recovering node must determine\nthe commit\u2013abort status of such transactions by contacting other nodes, as described\nin Section 23.2.1.2.\nIf recovery is done as just described, however, normal transaction processing at\nthe node cannot begin until all in-doubt transactions have been committed or rolled\nback. Finding the status of in-doubt transactions can be slow, since multiple nodes\nmay have to be contacted. Further, if the coordinator has failed, and no other node\nhas information about the commit\u2013abort status of an incomplete transaction, recovery\npotentially could become blocked if 2PC is used. As a result, the node performing\nrestart recovery may remain unusable for a long period.\nTo circumvent this problem, recovery algorithms typically provide support for not-\ning lock information in the log. (We are assuming here that locking is used for con-\ncurrency control.) Instead of writing a <ready T>log record, the algorithm writes a\n<ready T,L>log record, where Lis a list of all write locks held by the transaction\nTwhen the log record is written. At recovery time, after performing local recovery ac-\ntions, for every in-doubt transaction T, all the write locks noted in the <ready T,L>\nlog record (read from the log) are reacquired.\nAfter lock reacquisition is complete for all in-doubt transactions, transaction pro-\ncessing can start at the node, even before the commit\u2013abort status of the in-doubt\ntransactions is determined. The commit or rollback of in-doubt transactions proceeds\nconcurrently with the execution of new transactions. Thus, node recovery is faster and\nnever gets blocked. Note that new transactions that have a lock con\ufb02ict with any write\nlocks held by in-doubt transactions will be unable to make progress until the con\ufb02icting\nin-doubt transactions have been committed or rolled back.\n", "1134": "1106 Chapter 23 Parallel and Distributed Transaction Processing\n23.2.2 Avoiding Blocking During Commit\nThe blocking problem of 2PCis a serious concern for system designers, since the failure\nof a coordinator node could lead to blocking of a transaction that has acquired locks\non a frequently used data item, which in turn prevents other transactions that need to\nacquire a con\ufb02icting lock from completing their execution.\nBy involving multiple nodes in the commit decision step of 2PC, it is possible to\navoid blocking as long as a majority of the nodes involved in the commit decision are\nalive and can communicate with each other. This is done by using the idea of fault-\ntolerant distributed consensus. Details of di stributed consensus are discussed in detail\nlater, in Section 23.8, but we outline the problem and sketch a solution approach below.\nThe distributed consensus problem is as follows: A set of nnodes need to agree on\na decision; in this case, whether or not to commit a particular transaction. The inputs\nto make the decision are provided to all the nodes, and then each node votes on the\ndecision; in the case of 2PC, the decision is on whether or not to commit a transaction.\nThe key goal of protocols for achieving distributed consensus is that the decision should\nbe made in such a way that all nodes will \u201clearn\u201d the same value for the decision (i.e.,\nall nodes will learn that the transaction is to be committed, or all nodes will learn that\nthe transaction is to be aborted), even if some nodes fail during the execution of the\nprotocol, or there are network partitions. Further, the distributed consensus protocol\nshould not block, as long as a majority of the nodes participating remain alive and can\ncommunicate with each other.\nThere are several protocols for distributed consensus, two of which are widely used\ntoday (Paxos and Raft). We study distributed consensus in Section 23.8. A key idea\nbehind these protocols is the idea of a vote, which succeeds only if a majority of the\nparticipating nodes agree on a particular decision.\nGiven an implementation of distributed consensus, the blocking problem due to\ncoordinator failure can be avoided as follows: Instead of the coordinator locally decid-\ning to commit or abort a transaction, it initiates the distributed consensus protocol,\nrequesting that the value \u201ccommitted\u201d or \u201c aborted\u201d be assigned to the transaction T.\nThe request is sent to all the nodes participating in the distributed consensus, and the\nconsensus protocol is then executed by those nodes. Since the protocol is fault tolerant,\nit will succeed even if some nodes fail, as long as a majority of the nodes are up and\nremain connected. The transaction can be declared committed by the coordinator only\nafter the consensus protocol completes successfully.\nThere are two possible failure scenarios:\n\u2022The coordinator fails at any stage before informing all participating nodes of the com-\nmit or abort status of a transaction T .\nIn this case, a new coordinator is chosen (we will see how to do so in Section\n23.7). The new coordinator checks with the nodes participating in the distributed\nconsensus to see if a decision was made, and if so informs the 2PC participants\n", "1135": "23.2 Commit Protocols 1107\nof the decision. A majority of the nodes participating in consensus must respond,\nto check if a decision was made or not; the protocol will not block as long as the\nfailed/disconnected nodes are in a minority.\nIf no decision was made earlier for transaction T, the new coordinator again\nchecks with the 2PCparticipants to check if they are ready to commit or wish to\nabort the transaction, and follows the usual coordinator protocol based on their\nresponses. As before, if no response is received from a participant, the new coor-\ndinator may choose to abort T.\n\u2022The distributed consensus protocol fails to reach a decision.\nFailure of the protocol can occur due to the failure of some participating nodes.\nIt could also occur because of con\ufb02icting requests, none of which gets a majority of\n\u201cvotes\u201d during the consensus protocol. For 2PC, the request normally comes from\na single coordinator, so such a con\ufb02ict is unlikely. However, con\ufb02icting requests\ncan arise in rare cases if a coordinator fails after sending out a commit message, but\nits commit message is delivered late; meanwhile, a new 2PCcoordinator makes an\nabort decision since it could not reach some participating nodes. Even with such a\ncon\ufb02ict, the distributed consensus protocol guarantees that only one of the commit\nor abort requests can succeed, even in the presence of failures. But if some nodes\nare down, and neither the commit nor the abort request gets a majority vote from\nnodes participating in the distributed consensus, it is possible for the protocol to\nfail to reach a decision.\nRegardless of the reason, if the distributed consensus protocol fails to reach a\ndecision, the new coordinator just re-initiates the protocol.\nNote that in the event of a network partition, a node that gets disconnected from\nthe majority of the nodes participating in consensus may not learn about a decision,\neven if a decision was successfully made. Thus, transactions running at such a node\nmay be blocked.\nFailure of 2PC participants could make data unavailable, in the absence of repli-\ncation. Distributed consensus can also be used to keep replicas of a data item in a\nconsistent state, as we explain later in Section 23.8.4.\nThe idea of distributed consensus to make 2PCnonblocking was proposed in the\n1980s; it is used, for example, in the Google Spanner distributed database system.\nThe three-phase commit (3PC) protocol is an extension of the two-phase commit\nprotocol that avoids the blocking problem under certain assumptions. One variant of\nthe protocol avoids blocking as long as network partitions do not occur, but it may lead\nto inconsistent decisions in the event of a ne twork partition. Extensions of the protocol\nthat work safely under network partitioning were developed subsequently. The idea\nbehind these extensions is similar to the majority voting idea of distributed consensus,\nbut the protocols are speci\ufb01cally tailored for the task of atomic commit.\n", "1136": "1108 Chapter 23 Parallel and Distributed Transaction Processing\n23.2.3 Alternative Models of Transaction Processing\nWith two-phase commit, participating nod es agree to let the coordinator decide the\nfate of a transaction, and are forced to wait for the decision of the coordinator, while\nholding locks on updated data items. While such loss of autonomy may be acceptable\nwithin an organization, no organization would be willing to force its computers to wait,\npotentially for a long time, while a computer at another organization makes the deci-\nsion.\nIn this section, we describe how to use persistent messaging to avoid the problem\nof distributed commit. To understand persistent messaging, consider how one might\ntransfer funds between two di\ufb00erent banks, each with its own computer. One approach\nis to have a transaction span the two nodes and use two-phase commit to ensure atomic-\nity. However, the transaction may have to update the total bank balance, and blocking\ncould have a serious impact on all other transactions at each bank, since almost all\ntransactions at the bank would update the total bank balance.\nIn contrast, consider how funds transfer by a bank check occurs. The bank \ufb01rst\ndeducts the amount of the check from the available balance and prints out a check.\nThe check is then physically transferred to the other bank where it is deposited. After\nverifying the check, the bank increases the local balance by the amount of the check.\nThe check constitutes a message sent between the two banks. So that funds are not\nlost or incorrectly increased, the check must not be lost and must not be duplicated\nand deposited more than once. When the bank computers are connected by a network,\npersistent messages provide the same service as the check (but much faster).\nPersistent messages are messages that are guaranteed to be delivered to the re-\ncipient exactly once (neither less nor more), regardless of failures, if the transaction\nsending the message commits, and are guaranteed to not be delivered if the transac-\ntion aborts. Database recovery techniques are used to implement persistent messaging\non top of the normal network channels, as we shall see shortly. In contrast, regular\nmessages may be lost or may even be delivered multiple times in some situations.\nError handling is more complicated with persistent messaging than with two-phase\ncommit. For instance, if the account where the check is to be deposited has been closed,\nthe check must be sent back to the originating account and credited back there. Both\nnodes must, therefore, be provided with error-handling code, along with code to han-\ndle the persistent messages. In contrast, with two-phase commit, the error would be\ndetected by the transaction, which would then never deduct the amount in the \ufb01rst\nplace.\nThe types of exception conditions that may arise depend on the application, so it\nis not possible for the database system to handle exceptions automatically. The applica-\ntion programs that send and receive persistent messages must include code to handle\nexception conditions and bring the system back to a consistent state. For instance, it\nis not acceptable to just lose the money being transferred if the receiving account has\nbeen closed; the money must be credited back to the originating account, and if that is\nnot possible for some reason, humans must be a lerted to resolve the situation manually.\n", "1137": "23.2 Commit Protocols 1109\nMessage Delivery Process\nMonitor messages_to_send  relation \nSend any new messages to recipient \nAlso periodically resend old messages\nWhen Acknowledgment received from recipient, \nfor a message, delete messageMessage Receiving Process\nOn receiving message, execute transaction to \nadd message to received_messages  relation,\nif not already present\nAfter transaction commits, send \nAcknowledgementmessages_to_send received _messagesPerform database updates\nWrite message to messages_to_send  relationAtomic Transaction at Sending Node Atomic Transaction at Receiving Node\nProcess any unprocessed message in\nreceived_messages\nMark message as processed\nFigure 23.3 Implementation of persistent messaging.\nThere are many applications where the bene\ufb01t of eliminating blocking is well worth\nthe extra e\ufb00ort to implement systems that use persistent messages. In fact, few organi-\nzations would agree to support two-phase commit for transactions originating outside\nthe organization, since failures could result in blocking of access to local data. Persis-\ntent messaging therefore plays an important role in carrying out transactions that cross\norganizational boundaries.\nWe now consider the implementation of persistent messaging. Persistent messag-\ning can be implemented on top of an unreliable messaging infrastructure, which may\nlose messages or deliver them multiple times. Figure 23.3 shows a summary of the\nimplementation, which is described in detail next.\n\u2022Sending node protocol . When a transaction wishes to send a persistent message,\nit writes a record containing the message in a special relation messages\n to\nsend,\ninstead of directly sending out the message. The message is also given a unique\nmessage identi\ufb01er. Note that this relation acts as a message outbox .\nAmessage delivery process monitors the relation, and when a new message is\nfound, it sends the message to its destination. The usual database concurrency-\ncontrol mechanisms ensure that the system process reads the message only after\nthe transaction that wrote the message commits; if the transaction aborts, the usual\nrecovery mechanism would delete the message from the relation.\nThe message delivery process deletes a message from the relation only after it\nreceives an acknowledgment from the destination node. If it receives no acknowl-\nedgment from the destination node, after some time it sends the message again.\nIt repeats this until an acknowledgment is received. In case of permanent failures,\n", "1138": "1110 Chapter 23 Parallel and Distributed Transaction Processing\nthe system will decide, after some period of time, that the message is undeliver-\nable. Exception handling code provided by the application is then invoked to deal\nwith the failure.\nWriting the message to a relation and processing it only after the transaction has\ncommitted ensures that the message will be delivered if and only if the transaction\ncommits. Repeatedly sending it guarantees it will be delivered even if there are\n(temporary) system or network failures.\n\u2022Receiving node protocol . When a node receives a persistent message, it runs a trans-\naction that adds the message to a special received\n messages relation, provided it is\nnot already present in the relation (the unique message identi\ufb01er allows duplicates\nto be detected). The relation has an attribute to indicate if the message has been\nprocessed, which is set to false when the message is inserted in the relation. Note\nthat this relation acts as a message inbox .\nAfter the transaction commits, or if the message was already present in the\nrelation, the receiving node sends an acknowledgment back to the sending node.\nNote that sending the acknowledgment before the transaction commits is not\nsafe, since a system failure may then result in loss of the message. Checking\nwhether the message has been received earlier is essential to avoid multiple de-\nliveries of the message.\n\u2022Processing of message . Received messages must be processed to carry out the ac-\ntions speci\ufb01ed in the message. A process at the receiving node monitors the re-\nceived\n messages relation to check for messages that have not been processed. When\nit \ufb01nds such a message, the message is processed, and as part of the same transac-\ntion that processes the message, the processed \ufb02ag is set to true. This ensures that\na message is processed exactly once after it is received.\n\u2022Deletion of old messages . In many messaging systems, it is possible for messages to\nget delayed arbitrarily, although such delays are very unlikely. Therefore, to be safe,\nthe message must never be deleted from the received\n messages relation. Deleting it\ncould result in a duplicate delivery not being detected. But as a result, the received\nmessages relation may grow inde\ufb01nitely. To deal with this problem, each message\nis given a timestamp, and if the timestamp of a received message is older than some\ncuto\ufb00, the message is discarded. All messages recorded in the received\n messages\nrelation that are older than the cuto\ufb00 can be deleted.\nWork\ufb02ows provide a general model of distributed transaction processing involving\nmultiple nodes and possibly human processing of certain steps, and they are supported\nby application software used by enterprises. For instance, as we saw in Section 9.6.1,\nwhen a bank receives a loan application, there are many steps it must take, including\ncontacting external credit-checking agencies, before approving or rejecting a loan appli-\ncation. The steps, together, form a work\ufb02ow. Persistent messaging forms the underlying\nbasis for supporting work\ufb02ows in a distributed environment.\n", "1139": "23.3 Concurrency Control in Distributed Databases 1111\n23.3 Concurrency Control in Distributed Databases\nWe now consider how the concurrency-control schemes discussed in Chapter 18 can\nbe modi\ufb01ed so that they can be used in a distributed environment. We assume that each\nnode participates in the execution of a commit protocol to ensure global transaction\natomicity.\nIn this section, we assume that data items are not replicated, and we do not consider\nmultiversion techniques. We discuss how to handle replicas later, in Section 23.4, and\nwe discuss distributed multiversion concur rency control techniques in Section 23.5.\n23.3.1 Locking Protocols\nThe various locking protocols described in Chapter 18 can be used in a distributed\nenvironment. We discuss implementation issues in this section.\n23.3.1.1 Single Lock-Manager Approach\nIn the single lock-manager approach, the system maintains a single lock manager that\nresides in a single chosen node\u2014say Ni. All lock and unlock requests are made at node\nNi. When a transaction needs to lock a data item, it sends a lock request to Ni.T h e\nlock manager determines whether the lock can be granted immediately. If the lock can\nbe granted, the lock manager sends a message to that e\ufb00ect to the node at which the\nlock request was initiated. Otherwise, the request is delayed until it can be granted, at\nwhich time a message is sent to the node at which the lock request was initiated. The\ntransaction can read the data item from anyone of the nodes at which a replica of the\ndata item resides. In the case of a write, all the nodes where a replica of the data item\nresides must be involved in the writing.\nT h es c h e m eh a st h e s ea d v a n t a g e s :\n\u2022Simple implementation . This scheme requires two messages for handling lock re-\nquests and one message for handling unlock requests.\n\u2022Simple deadlock handling . Since all lock and unlock requests are made at one node,\nthe deadlock-handling algorithms discussed in Chapter 18 can be applied directly.\nThe disadvantages of the scheme are:\n\u2022Bottleneck . The node Nibecomes a bottleneck, since all requests must be processed\nthere.\n\u2022Vulnerability . If the node Nifails, the concurrency controller is lost. Either pro-\ncessing must stop, or a recovery scheme must be used so that a backup node can\ntake over lock management from Ni, as described in Section 23.7.\n", "1140": "1112 Chapter 23 Parallel and Distributed Transaction Processing\n23.3.1.2 Distributed Lock Manager\nA compromise between the advantages and disadvantages can be achieved through the\ndistributed-lock-manager approach, in which the lock-manager function is distributed\nover several nodes.\nEach node maintains a local lock manager whose function is to administer the\nlock and unlock requests for those data items that are stored in that node. When a\ntransaction wishes to lock a data item Qthat resides at node Ni, a message is sent to\nthe lock manager at node Nirequesting a lock (in a particular lock mode). If data item\nQis locked in an incompatible mode, then the request is delayed until it can be granted.\nOnce it has determined that the lock request can be granted, the lock manager sends\na message back to the initiator indicating that it has granted the lock request.\nThe distributed-lock-manager scheme has the advantage of simple implementation,\nand it reduces the degree to which the coordinator is a bottleneck. It has a reasonably\nlow overhead, requiring two message transfers for handling lock requests, and one mes-\nsage transfer for handling unlock requests. However, deadlock handling is more com-\nplex, since the lock and unlock requests are no longer made at a single node: There\nmay be internode deadlocks even when there is no deadlock within a single node. The\ndeadlock-handling algorithms discussed in Chapter 18 must be modi\ufb01ed, as we shall\ndiscuss in Section 23.3.2, to detect global deadlocks.\n23.3.2 Deadlock Handling\nThe deadlock-prevention and deadlock-detection algorithms in Chapter 18 can be used\nin a distributed system, with some modi\ufb01cations.\nConsider \ufb01rst the deadlock-prevention techniques, which we saw in Section 18.2.1.\n\u2022Techniques for deadlock prevention based on lock ordering can be used in a dis-\ntributed system, with no changes at all. These techniques prevent cyclic lock waits;\nthe fact that locks may be obtained at di\ufb00erent nodes has no e\ufb00ect on prevention\nof cyclic lock waits.\n\u2022Techniques based on preemption and transaction rollback can also be used un-\nchanged in a distributed system. In particular, the wait-die technique is used in\nseveral distributed systems. Recall that this technique allows older transactions to\nwait for locks held by younger transactions, but if a younger transaction needs to\nwait for a lock held by an older transaction, the younger transaction is rolled back.\nThe transaction that is rolled back may subsequently be executed again; recall that\nit retains its original start time; if it is treated as a new transaction, it could be\nrolled back repeatedly, and starve, even as other transactions make progress and\ncomplete.\n\u2022Timeout-based schemes, too, work without any changes in a distributed system.\n", "1141": "23.3 Concurrency Control in Distributed Databases 1113\nT2 T4 T1 T2\nT5 T3 T3\nsite S1 site S2\nFigure 23.4 Local wait-for graphs.\nDeadlock-prevention techniques may result in unnecessary waiting and rollback\nwhen used in a distributed system, just as in a centralized system,\nWe now consider deadlock-detection techniques that allow deadlocks to occur and\ndetect them if they do. The main problem in a distributed system is deciding how to\nmaintain the wait-for graph. Common techniques for dealing with this issue require\nthat each node keep a local wait-for graph . The nodes of the graph correspond to all the\ntransactions (local as well as nonlocal) tha t are currently either holding or requesting\nany of the items local to that node. For example, Figure 23.4 depicts a system consisting\nof two nodes, each maintaining its local wait-for graph. Note that transactions T2and\nT3appear in both graphs, indicating that the transactions have requested items at both\nnodes.\nThese local wait-for graphs are constructed in the usual manner for local transac-\ntions and data items. When a transaction Tion node N1needs a resource in node N2,\nit sends a request message to node N2. If the resource is held by transaction Tj,t h e\nsystem inserts an edge Ti\u2192Tjin the local wait-for graph of node N2.\nIf any local wait-for graph has a cycle, a deadlock has occurred. On the other hand,\nthe fact that there are no cycles in any of the local wait-for graphs does not mean\nthat there are no deadlocks. To illustrate this problem, we consider the local wait-for\ngraphs of Figure 23.4. Each wait-for graph is acyclic; nevertheless, a deadlock exists in\nthe system because the union of the local wait-for graphs contains a cycle. This graph\nappears in Figure 23.5.\nT1 T4\nT5T2\nT3\nFigure 23.5 Global wait-for graph for Figure 23.4.\n", "1142": "1114 Chapter 23 Parallel and Distributed Transaction Processing\nIn the centralized deadlock detection approach, the system constructs and main-\ntains a global wait-for graph (the union of all the local graphs) in a single node: the\ndeadlock-detection coordinator. Since there is communication delay in the system, we\nmust distinguish between two types of wait-for graphs. The realgraph describes the real\nbut unknown state of the system at any instance in time, as would be seen by an omni-\nscient observer. The constructed graph is an approximation generated by the controller\nduring the execution of the controller\u2019s algorithm. Obviously, the controller must gen-\nerate the constructed graph in such a way that, whenever the detection algorithm is\ninvoked, the reported results are correct. Correct means in this case that, if a deadlock\nexists, it is reported promptly, and if the system reports a deadlock, it is indeed in a\ndeadlock state.\nThe global wait-for graph can be reconstructed or updated under these conditions:\n\u2022Whenever a new edge is inserted in or removed from one of the local wait-for\ngraphs.\n\u2022Periodically, when a number of changes have occurred in a local wait-for graph.\n\u2022Whenever the coordinator needs to invoke the cycle-detection algorithm.\nWhen the coordinator invokes the deadlock-detection algorithm, it searches its\nglobal graph. If it \ufb01nds a cycle, it selects a victim to be rolled back. The coordinator\nmust notify all the nodes that a particular transaction has been selected as the victim.\nT h en o d e s ,i nt u r n ,r o l lb a c kt h ev i c t i mt r a n s a c t i o n .\nThis scheme may produce unnecessary rollbacks if:\n\u2022False cycles exist in the global wait-for graph. As an illustration, consider a snap-\nshot of the system represented by the local wait-for graphs of Figure 23.6. Suppose\nthat T2releases the resource that it is holding in node N1, resulting in the deletion\nof the edge T1\u2192T2inN1.T r a n s a c t i o n T2then requests a resource held by T3\nat node N2, resulting in the addition of the edge T2\u2192 T3inN2.I ft h e insert\nT2\u2192T3message from N2arrives before the remove T1\u2192T2message from N1,\nthe coordinator may discover the false cycle T1\u2192T2\u2192T3after the insert (but\nbefore the remove ). Deadlock recovery may be initiated, although no deadlock\nhas occurred.\nNote that the false-cycle situation could not occur under two-phase locking.\nThe likelihood of false cycles is usually su\ufb03ciently low that they do not cause a\nserious performance problem.\n\u2022Adeadlock has indeed occurred and a victim has been picked, while one of the\ntransactions was aborted for reasons unrelated to the deadlock. For example, sup-\npose that node N1in Figure 23.4 decides to abort T2. At the same time, the coor-\ndinator has discovered a cycle and has picked T3as a victim. Both T2and T3are\nnow rolled back, although only T2needed to be rolled back.\n", "1143": "23.3 Concurrency Control in Distributed Databases 1115\nT1\nT2\nT2T1\nT3\nS2\nT1\nT3\ncoordinatorS1\nFigure 23.6 False cycles in the global wait-for graph.\nDeadlock detection can be done in a distributed manner, with several nodes taking\non parts of the task, instead of it being done at a single node. However, such algorithms\nare more complicated and more expensive. See the bibliographical notes for references\nto such algorithms.\n23.3.3 Leases\nOne of the issues with using locking in a distributed system is that a node holding a lock\nmay fail, and not release the lock. The locked data item could thus become (logically)\ninaccessible, until the failed node recovers and releases the lock, or the lock is released\nby another node on behalf of the failed node.\nIf an exclusive lock has been obtained on a data item, and the transaction is in the\nprepared state, the lock cannot be released until a commit/abort decision is made for\nthe transaction. However, in many other cases it is acceptable for a lock that has been\ngranted earlier to be revoked subsequently. In such cases, the concept of a lease can be\nvery useful.\nAlease is a lock that is granted for a speci\ufb01c period of time. If the process that\nacquires a lease needs to continue holding the lock beyond the speci\ufb01ed period, it can\nrenew the lease. A lease renewal request is sent to the lock manager, which extends the\nlease and responds with an acknowledgment as long as the renewal request comes in\ntime. However, if the time expires, and the process does not renew the lease, the lease\nis said to expire , and the lock is released. Thus, any lease acquired by a node that either\nfails, or gets disconnected from the lock manager, is automatically released when the\n", "1144": "1116 Chapter 23 Parallel and Distributed Transaction Processing\nlease expires. The node that holds a lease regularly compares the current lease expiry\ntime with its local clock to determine if it still has the lease or the lease has expired.\nOne of the uses of leases is to ensure that there is only one coordinator for a proto-\ncol in a distributed system. A node that wants to act as coordinator requests an exclusive\nlease on a data item associated with the protocol. If it gets the lease, it can act as co-\nordinator until the lease expires; as long as it is active, it requests lease renewal before\nthe lease expires, and it continues to be the coordinator as long as the lock manager\npermits the lease renewal.\nIf a node N1acting as coordinator dies after the expiry of the lease period, the\nlease automatically expires, and another node N2that requests the lease can acquire\nit and become the coordinator. In most protocols it is important that there should be\nonly one coordinator at a given time. The lease mechanism guarantees this, as long as\nclocks are synchronized. However, if the coordinator\u2019s clock runs slower than the lock\nmanager\u2019s clock, a situation can arise where the coordinator thinks it still has the lease,\nwhile the lock manager thinks the lease has expired. While clocks cannot be exactly\nsynchronized, in practice the inaccuracy is not very high. The lock manager waits for\nsome extra wait time after the lease expiry time to account for clock inaccuracies before\nit actually treats the lease as expired.\nA node that checks the local clock and decides it still has a lease may then take a\nsubsequent action as coordinator. It is possible that the lease may have expired between\nwhen the clock was checked and when the subsequent action took place, which could\nresult in the action taking place after the node is no longer the coordinator. Further,\neven if the action took place while the node had a valid lease, a message sent by the\nnode may be delivered after a delay, by which time the node may have lost its lease.\nWhile it is possible for the network to deliver a message arbitrarily late, the system can\ndecide on a maximum message delay time, and any message that is older is ignored by\nthe recipient; messages have timestamps set by the sender, which are used to detect if\na message needs to be ignored.\nT h et i m eg a p sd u et ot h ea b o v et w oi s s u e sc a nb et a k e ni n t oa c c o u n tb yc h e c k i n g\nthat the lease expiry is at least some time t\u2032into the future before initiating an action,\nwhere t\u2032is a bound on how long the action will take after the lease time check, including\nthe maximum message delay.\nWe have assumed here that while coordinators may fail, the lock manager that\nissues leases is able to tolerate faults. We s tudy in Section 23.8.4 how to build a fault-\ntolerant lock manager; we note that the techniques described in that section are general\npurpose and can be used to implement fault-tolerant versions of any deterministic pro-\ncess, modeled as a \u201cstate machine.\u201d\n23.3.4 Distributed Timestamp-Based Protocols\nThe principal idea behind the timestamp-based concurrency control protocols in Sec-\ntion 18.5 is that each transaction is given a unique timestamp that the system uses in\n", "1145": "23.3 Concurrency Control in Distributed Databases 1117\nsite\nidenti\ufb01er\nglobal unique\nidenti\ufb01erlocal unique\ntimestamp\nFigure 23.7 Generation of unique timestamps.\ndeciding the serialization order. Our \ufb01rst task, then, in generalizing the centralized\nscheme to a distributed scheme is to develop a scheme for generating unique times-\ntamps. We then discuss how the timestamp-based protocols can be used in a distributed\nsetting.\n23.3.5 Generation of Timestamps\nThere are two primary methods for generating unique timestamps, one centralized and\none distributed. In the centralized scheme, a single node distributes the timestamps.\nThe node can use a logical counter or its own local clock for this purpose. While this\nscheme is easy to implement, failure of the node would potentially block all transaction\nprocessing in the system.\nIn the distributed scheme, each node generates a unique local timestamp by using\neither a logical counter or the local clock. We obtain the unique global timestamp by\nconcatenating the unique local timestamp with the node identi\ufb01er, which also must be\nunique (Figure 23.7). If a node has multiple threads running on it (as is almost always\nthe case today), a thread identi\ufb01er is concatenated with the node identi\ufb01er, to make the\ntimestamp unique. Further, we assume that consecutive calls to get the local timestamp\nwithin a node/thread will return di\ufb00erent timestamps; if this is not guaranteed by the\nlocal clock, the returned local timestamp value may need to be incremented, to ensure\ntwo calls do not get the same local timestamp.\nT h eo r d e ro fc o n c a t e n a t i o ni si m p o r t a n t !W eu s et h en o d ei d e n t i \ufb01 e ri nt h el e a s t\nsigni\ufb01cant position to ensure that the global timestamps generated in one node are not\nalways greater than those generated in another node.\nWe may still have a problem if one node generates local timestamps at a rate faster\nthan that of the other nodes. In such a case, the fast node\u2019s logical counter will be larger\nthan that of other nodes. Therefore, all timestamps generated by the fast node will be\nlarger than those generated by other nodes. What we need is a mechanism to ensure\nthat local timestamps are generated fairly across the system. There are two solution\napproaches for this problem.\n1.Keep the clocks synchronized by using a network time protocol, which is a stan-\ndard feature in computers today. The protocol periodically communicates with a\n", "1146": "1118 Chapter 23 Parallel and Distributed Transaction Processing\nserver to \ufb01nd the current time. If the local time is ahead of the time returned by\nthe server, the local clock is slowed down, whereas if the local time is behind the\ntime returned by the server it is speeded up, to bring it back in synchronization\nwith the time at the server. Since all nodes are approximately synchronized with\nthe server, they are also approximately synchronized with each other.\n2.We de\ufb01ne within each node Nialogical clock (LCi), which generates the unique\nlocal timestamp. The logical clock can be implemented as a counter that is in-\ncremented after a new local timestamp is generated. To ensure that the various\nlogical clocks are synchronized, we require that a node Niadvance its logical\nclock whenever a transaction Tiwith timestamp <x,y>visits that node and x\nis greater than the current value of LCi. In this case, node Niadvances its logical\nclock to the value x+1 .A sl o n ga sm e s s a g e sa r ee x c h a n g e dr e g u l a r l y ,t h el o g i c a l\nclocks will be approximately synchronized.\n23.3.6 Distributed Timestamp Ordering\nThe timestamp ordering protocol can be easily extended to a parallel or distributed\ndatabase setting. Each transaction is assigned a globally unique timestamp at the node\nwhere it originates. Requests sent to other nodes include the transaction timestamp.\nEach node keeps track of the read and write timestamps of the data items at that node.\nWhenever an operation is received by a node, it does the timestamp checks that we saw\nin Section 18.5.2, locally, without any need to communicate with other nodes.\nTimestamps must be reasonably synchronized across nodes; otherwise, the follow-\ning problem can occur. Suppose one node has a time signi\ufb01cantly lagging the others,\nand a transaction T1gets its timestamp at that node n1. Suppose the transaction T1\nf a i l sat i m e s t a m pt e s to nad a t ai t e m dibecause dihas been updated by a transaction T2\nwith a higher timestamp; T1would be restarted with a new timestamp, but if the time\nat node n1is not synchronized, the new timestamp may still be old enough to cause\nthe timestamp test to fail, and T1would be restarted repeatedly until the time at n1\nadvances ahead of the timestamp of T2.\nNote that as in the centralized case, if a transaction Tireads an uncommitted value\nwritten by another transaction Tj,Ticannot commit until Tjcommits. This can be en-\nsured either by making reads wait for uncommitted writes to be committed, which can\nbe implemented using locking, or by introducing commit dependencies, as discussed\nin Section 18.5. The waiting time can be exacerbated by the time required to perform\n2PC, if the transaction performs updates at more than one node. While a transaction Ti\nis in the prepared state, its writes are not committed, so any transaction with a higher\ntimestamp that reads an item written by Tiwould be forced to wait.\nWe also note that the multiversion timestamp ordering protocol can be used locally\nat each node, without any need to communicate with other nodes, similar to the case\nof the timestamp ordering protocol.\n", "1147": "23.3 Concurrency Control in Distributed Databases 1119\n23.3.7 Distributed Validation\nWe now consider the validation-based protocol (also called the optimistic concurrency\ncontrol protocol) that we saw in Section 18.6. The protocol is based on three times-\ntamps:\n\u2022The start timestamp StartTS( Ti).\n\u2022The validation timestamp, TS( Ti), which is used as the serialization order.\n\u2022The \ufb01nish timestamp FinishTS( Ti) which identi\ufb01es when the writes of a transac-\ntion have completed.\nWhile we saw a serial version of the validation protocol in Section 18.6, where only\none transaction can perform validation at a ti me, there are extensions to the protocol to\nallow validations of multiple transactions to occur concurrently, within a single system.\nWe now consider how to adapt the protocol to a distributed setting.\n1.Validation is done locally at each node, with timestamps assigned as described\nbelow.\n2.In a distributed setting, the validation timestamp TS( Ti) can be assigned at any\nof the nodes, but the same timestamp TS( Ti) must be used at all nodes where\nvalidation is to be performed. Transactions must be serializable based on their\ntimestamps TS( Ti).\n3.The validation test for a transaction Tilooks at all transactions Tjwith TS( Tj)<\nTS(Ti), to check if Tjeither \ufb01nished before Tistarted, or has no con\ufb02icts with\nTi. The assumption is that once a particular transaction enters the validation\nphase, no transaction with a lower timestamp can enter the validation phase. The\nassumption can be ensured in a centralized system by assigning the timestamps\nin a critical section, but cannot be ensured in a distributed setting.\nA key problem in the distributed setting is that a transaction Tjmay enter the\nvalidation phase after a transaction Ti, but with TS( Tj)<TS(Ti). It is too late for\nTito be validated against Tj. However, this problem can be easily \ufb01xed by rolling\nback any transaction if, when it starts validation at a node, a transaction with a\nlater timestamp had already started validation at that node.\n4.The start and \ufb01nish timestamps are used to identify transactions Tjwhose writes\nwould de\ufb01nitely have been seen by a transaction Ti. These timestamps must\nbe assigned locally at each node, and must satisfy StartTS( Ti)\u2264TS(Ti)\u2264\nFinishTS( Ti). Each node uses these timestamps to perform validation locally.\n5.When used in conjunction with 2PC, a transaction must \ufb01rst be validated and\nthen enter the prepared state. Writes cannot be committed at the database until\n", "1148": "1120 Chapter 23 Parallel and Distributed Transaction Processing\nthe transaction enters the committed state in 2PC. Suppose a transaction Tjreads\nan item updated by a transaction Tithat is in the prepared state and is allowed\nto proceed using the old value of the data item (since the value generated by Ti\nhas not yet been written to the database). Then, when transaction Tjattempts to\nvalidate, it will be serialized after Tiand will surely fail validation if Ticommits.\nThus, the read by Tjmay as well be held until Ticommits and \ufb01nishes its writes.\nThe above behavior is the same as what would happen with locking, with write\nlocks acquired at the time of validation.\nAlthough full implementations of validation-based protocols are not widely used\nin distributed settings, optimistic concurrency control without read validation, which\nwe saw in Section 18.9.3, is widely used in distributed settings. Recall that the scheme\ndepends on storing a version number with each data item, a feature that is supported\nby many key-value stores.1Version numbers are incremented each time the data item\nis updated.\nValidation is performed at the time of writing the data item, which can be done\nusing a test-and-set function based on version numbers, that is supported by some key-\nvalue stores. This function allows an update to a data item to be conditional on the\ncurrent version of the data item being the same as a speci\ufb01ed version number. If the\ncurrent version number of the data item is more recent than the speci\ufb01ed version num-\nber, the update is not performed. For example, a transaction that read version 7 of a\ndata item can perform a write, conditional on the version still being at 7. If the item has\nbeen updated meanwhile, the current version would not match, and the write would\nfail; however, if the version number is still 7, the write would be performed successfully,\nand the version number incremented to 8.\nThe test-and-set function can thus be used by applications to implement the lim-\nited form of validation-based concurrency control, discussed in Section 18.9.3, at the\nlevel of individual data items. Thereby, a transaction could read a value from a data\nitem, perform computation locally, and update the data item at the end, as long as the\nvalue it read has not changed subsequently. This approach does not guarantee overall\nserializability, but it does prevent the lost-update anomaly.\nHBase supports the test-and-set operation based on comparing values (similar to\nthe hardware test-and-set operation), which is called checkAndPut() .I n s t e a do fc o m -\nparing to a system-generated version number, the checkAndPut() invocation can pass\nin a column and a value; the update is performed only if the row has the speci\ufb01ed\nvalue for the speci\ufb01ed column. The check and the update are performed atomically.\nAv a r i a n t , checkAndMutate() , allows multiple modi\ufb01cations to a row, such as adding\nor updating a column, deleting a column, or incrementing a column, after checking a\ncondition, as a single atomic operation.\n1Note that this is not the same as multiversioning, since only one version needs to be stored.\n", "1149": "23.4 Replication 1121\n23.4 Replication\nOne of the goals in using distributed databases is high availability ; that is, the database\nmust function almost all the time. In particular, since failures are more likely in large\ndistributed systems, a distributed databas e must continue functioning even when there\nare various types of failures. The ability to continue functioning even during failures is\nreferred to as robustness .\nFor a distributed system to be robust, data must be replicated, allowing the data to\nbe accessible even if a node containing a replica of the data fails.\nThe database system must keep track of the locations of the replicas of each data\nitem in the database catalog. Replication can be at the level of individual data items, in\nwhich the catalog will have one entry for each data item, recording the nodes where it is\nreplicated. Alternatively, replication can be done at the level of partitions of a relation,\nwith an entire partition replicated at two or more nodes. The catalog would then have\none entry for each partition, resulting in considerably lower overhead than having one\nentry for each data item.\nIn this section we \ufb01rst discuss (in Section 23.4.1) issues with consistency of values\nbetween replicas. We then discuss (in Section 23.4.2) how to extend concurrency con-\ntrol techniques to deal with replicas, ignoring the issue of failures. Further extensions\nof the techniques to handle failures but modifying how reads and writes are executed\nare described in Section 23.4.3.\n23.4.1 Consistency of Replicas\nGiven that a data item (or partition) is replicated, the system should ideally ensure that\nthe copies have the same value. Practically, given that some nodes may be disconnected\nor may have failed, it is impossible to ensure that all copies have the same value. Instead,\nt h es y s t e mm u s te n s u r et h a te v e ni fs o m er e p l i c a sd on o th a v et h el a t e s tv a l u e ,r e a d so f\na data item get to see the latest value that was written.\nMore formally, the implementations of read and write operations on the replicas\nof a data item must follow a protocol that ensures the following property, called lin-\nearizability : Given a set of read and write operations on a data item,\n1.there must be a linear ordering of the operations such that each read in the or-\ndering should see the value written by the most recent write preceding the read\n(or the initial value if there is no such write), and\n2.if an operation o1\ufb01nishes before an operation o2begins (based on external time),\nthen o1must precede o2in the linear order.\nNote that linearizability only addresses what happens to a single data item, and it is\northogonal to serializability.\n", "1150": "1122 Chapter 23 Parallel and Distributed Transaction Processing\nWe \ufb01rst consider approaches that write all copies of a data item and discuss limi-\ntations of this approach; in particular, to ensure availability during failure, failed nodes\nneed to be removed from the set of replicas, which can be quite tricky as we will see.\nIt is not possible, in general, to di\ufb00erentiate between node failure and network\npartition. The system can usually detect that a failure has occurred, but it may not be\nable to identify the type of failure. For example, suppose that node N1is not able to\ncommunicate with N2.I tc o u l db et h a t N2has failed. However, another possibility is\nthat the link between N1and N2has failed, resulting in network partition. The problem\nis partly addressed by using multiple links between nodes, so that even if one link fails\nthe nodes will remain connected. However, multiple link failures can still occur, so there\nare situations where we cannot be sure whether a node failure or network partition has\noccurred.\nThere are protocols for data access that can continue working even if some nodes\nhave failed, without any explicit actions to deal with the failures, as we shall see in\nSection 23.4.3.1. These protocols are based on ensuring a majority of nodes are writ-\nten/read. With such protocols, actions to detect failed nodes and remove them from\nthe system can be done in the background, and (re)integration of new or recovered\nnodes into the system can also be done without disrupting processing.\nAlthough traditional database systems place a premium on consistency, there are\nmany applications today that value availability more than consistency. The design of\nreplication protocols is di\ufb00erent for such systems and is discussed in Section 23.6.\nIn particular, one such alternative that is widely used for maintaining replicated\ndata is to perform the update on a primary copy of the data item, and allow the trans-\naction to commit without updating the other copies. However, the update is subse-\nquently propagated to the other copies. Such propagation of updates, referred to as\nasynchronous replication orlazy propagation of updates , is discussed in Section 23.6.2.\nOne drawback of asynchronous replication is that replicas may be out of date for\nsome time following each update. Another drawback is that if the primary copy fails\nafter a transaction commits, but before the updates were propagated to the replicas, the\nupdates of the committed transaction may not be visible to subsequent transactions,\nleading to an inconsistency.\nOn the other hand, a major bene\ufb01t of asynchronous replication is that exclusive\nlocks can be released as soon as the transaction commits on the primary copy. In\ncontrast, if other replicas have to be updated before the transaction commits, there\nmay be a signi\ufb01cant delay in committing the transaction. In particular, if data is geo-\ngraphically replicated to ensure availability despite failure of an entire data center, the\nnetwork round-trip time to a remote data center could range from tens of milliseconds\nto nearby locations, up to hundreds of milliseconds for data centers that are on the\nother side of the world. If a transaction were to hold a lock on a data item for this\nduration, the number of transactions that can update that data item would be limited\nto approximately 10 to 100 transactions per second. For certain applications, for exam-\nple, user data in a web application, 10 to 100 transactions per second for a single data\nitem is quite su\ufb03cient. However, for applications where some data items are updated\n", "1151": "23.4 Replication 1123\nby a large number of transactions each second, holding locks for such a long time is\nnot acceptable. Asynchronous replication may be preferred in such cases.\n23.4.2 Concurrency Control with Replicas\nWe discuss several alternative ways of dealing with locking in the presence of replica-\ntion of data items, in Section 23.4.2.1 to Section 23.4.2.4.\nIn this section, we assume updates are done on all replicas of a data item. If any\nnode containing a replica of a data item has failed, or is disconnected from the other\nnodes, that replica cannot be updated. We discuss how to perform reads and updates\nin the presence of failures later, in Section 23.4.3.\n23.4.2.1 Primary Copy\nWhen a system uses data replication, we can choose one of the replicas of a data item as\ntheprimary copy . For each data item Q,t h ep r i m a r yc o p yo f Qmust reside in precisely\none node, which we call the primary node ofQ.\nWhen a transaction needs to lock a data item Q, it requests a lock at the primary\nnode of Q. As before, the response to the request is delayed until it can be granted. The\nprimary copy enables concurrency control for replicated data to be handled like that\nfor unreplicated data. This similarity allows for a simple implementation. However, if\nthe primary node of Qfails, lock information for Qwould be lost, and Qwould be\ninaccessible, even though other nodes containing a replica may be accessible.\n23.4.2.2 Majority Protocol\nThe majority protocol works this way: If data item Qis replicated in ndi\ufb00erent nodes,\nthen a lock-request message must be sent to more than one-half of the nnodes in which\nQis stored. Each lock manager determines whether the lock can be granted immedi-\nately (as far as it is concerned). As before, the response is delayed until the request can\nbe granted. The transaction does not operate on Quntil it has successfully obtained a\nlock on a majority of the replicas of Q.\nWe assume for now that writes are performed on all replicas, requiring all nodes\ncontaining replicas to be available. However, the major bene\ufb01t of the majority protocol\nis that it can be extended to deal with node failures, as we shall see in Section 23.4.3.1.\nThe protocol also deals with replicated data in a decentralized manner, thus avoiding\nthe drawbacks of central control. However, it su\ufb00ers from these disadvantages:\n\u2022Implementation . The majority protocol is more complicated to implement than are\nthe previous schemes. It requires at least 2( n\u22152+1) messages for handling lock\nrequests and at least ( n\u22152+1) messages for handling unlock requests.\n\u2022Deadlock handling . In addition to the problem of global deadlocks due to the use\nof a distributed-lock-manager approach, it is possible for a deadlock to occur even\nif only one data item is being locked. As an illustration, consider a system with\nfour nodes and full replication. Suppose that transactions T1and T2wish to lock\n", "1152": "1124 Chapter 23 Parallel and Distributed Transaction Processing\ndata item Qin exclusive mode. Transaction T1may succeed in locking Qat nodes\nN1and N3, while transaction T2may succeed in locking Qat nodes N2and N4.\nEach then must wait to acquire the third lock; hence, a deadlock has occurred.\nLuckily, we can avoid such deadlocks with relative ease by requiring all nodes to\nrequest locks on the replicas of a data item in the same predetermined order.\n23.4.2.3 Biased Protocol\nThe biased protocol is another approach to handling replication. The di\ufb00erence from\nthe majority protocol is that requests for shared locks are given more favorable treat-\nment than requests for exclusive locks.\n\u2022Shared locks . When a transaction needs to lock data item Q, it simply requests a\nlock on Qfrom the lock manager at one node that contains a replica of Q.\n\u2022Exclusive locks .W h e nat r a n s a c t i o nn e e d st ol o c kd a t ai t e m Q,i tr e q u e s t sal o c k\nonQfrom the lock manager at all nodes that contain a replica of Q.\nAs before, the response to the request is delayed until it can be granted.\nThe biased scheme has the advantage of imposing less overhead on read operations\nthan does the majority protocol. This savings is especially signi\ufb01cant in common cases\nin which the frequency of read is much greater than the frequency of write .H o w e v e r ,\nthe additional overhead on writes is a disadvantage. Furthermore, the biased protocol\nshares the majority protocol\u2019s disadvantage of complexity in handling deadlock.\n23.4.2.4 Quorum Consensus Protocol\nThe quorum consensus protocol is a generalization of the majority protocol. The quo-\nrum consensus protocol assigns each node a nonnegative weight. It assigns read and\nwrite operations on an item xtwo integers, called read quorum Qrand write quorum\nQw, that must satisfy the following condition, where Sis the total weight of all nodes\nat which xresides:\nQr+Qw>Sand2\u2217Qw>S\nTo execute a read operation, enough replicas must be locked that their total weight\nis at least Qr. To execute a write operation, enough replicas must be locked so that their\nt o t a lw e i g h ti sa tl e a s t Qw.\nA bene\ufb01t of the quorum consensus approach is that it can permit the cost of either\nread or write locking to be selectively reduced by appropriately de\ufb01ning the read and\nwrite quorums. For instance, with a small read quorum, reads need to obtain fewer\nlocks, but the write quorum will be higher, hence writes need to obtain more locks.\nAlso, if higher weights are given to some nodes (e.g., those less likely to fail), fewer\n", "1153": "23.4 Replication 1125\nnodes need to be accessed for acquiring locks. In fact, by setting weights and quorums\nappropriately, the quorum consensus protocol can simulate the majority protocol and\nthe biased protocols.\nLike the majority protocol, quorum consensus can be extended to work even in the\npresence of node failures, as we shall see in Section 23.4.3.1.\n23.4.3 Dealing with Failures\nConsider the following protocol to deal with replicated data. Writes must be success-\nfully performed at all replicas of a data item. Reads may read from any replica. When\ncoupled with two-phase locking, such a protocol will ensure that reads will see the value\nwritten by the most recent write to the same data item. This protocol is also called the\nread one, write all copies protocol since all replicas must be written, and any replica\ncan be read.\nThe problem with this protocol lies in what to do if some node is unavailable. To\nallow work to proceed in the event of failures, it may appear that we can use a \u201cread one,\nwrite all available\u201d protocol. In this approach, a read operation proceeds as in the read\none, write all scheme; any available replica can be read, and a read lock is obtained at\nthat replica. A write operation is shipped to all replicas, and write locks are acquired on\nall the replicas. If a node is down, the transaction manager proceeds without waiting for\nthe node to recover. While this approach appears very attractive, it does not guarantee\nconsistency of writes and reads. For example, a temporary communication failure may\ncause a node to appear to be unavailable, resulting in a write not being performed, but\nwhen the link is restored, the node is not aware that it has to perform some reintegration\nactions to catch up on writes it has lost. Further, if the network partitions, each partition\nmay proceed to update the same data item, believing that nodes in the other partitions\nare all dead.\n23.4.3.1 Robustness Using the Majority-Based Protocol\nThe majority-based approach to distributed concurrency control in Section 23.4.2.2\ncan be modi\ufb01ed to work in spite of failures. In this approach, each data object stores\nwith it a version number to detect when it was last written. Whenever a transaction\nwrites an object it also updates the version number in this way:\n\u2022If data object ais replicated in ndi\ufb00erent nodes, then a lock-request message must\nbe sent to more than one-half of the nnodes at which ais stored. The transaction\ndoes not operate on auntil it has successfully obtained a lock on a majority of the\nreplicas of a.\nUpdates to the replicas can be committed atomically using 2PC. (We assume\nfor now that all replicas that were accessible stay accessible until commit, but we\n", "1154": "1126 Chapter 23 Parallel and Distributed Transaction Processing\nrelax this requirement later in this section, where we also discuss alternatives to\n2PC.)\n\u2022Read operations look at all replicas on which a lock has been obtained and read\nthe value from the replica that has the highest version number. (Optionally, they\nmay also write this value back to replicas with lower version numbers.) Writes read\nall the replicas just like reads to \ufb01nd the highest version number (this step would\nnormally have been performed earlier in the transaction by a read, and the result\ncan be reused). The new version number is one more than the highest version\nnumber. The write operation writes all the replicas on which it has obtained locks\nand sets the version number at all the replicas to the new version number.\nFailures (whether network partitions or node failures) can be tolerated as long as (1)\nthe nodes available at commit contain a majority of replicas of all the objects written\nto and (2) during reads, a majority of replicas are read to \ufb01nd the version numbers.\nIf these requirements are violated, the transaction must be aborted. As long as the\nrequirements are satis\ufb01ed, the two-phase commit protocol can be used, as usual, on\nthe nodes that are available.\nIn this scheme, reintegration is trivial; nothing needs to be done. This is because\nwrites would have updated a majority of the replicas, while reads will read a majority\nof the replicas and \ufb01nd at least one replica that has the latest version.\nHowever, the majority protocol using version numbers has some limitations, which\ncan be avoided by using extensions or by using alternative protocols.\n1.The \ufb01rst problem is how to deal with the failure of participants during an execu-\ntion of the two-phase commit protocol.\nThis problem can be dealt with by an extension of the two-phase commit\nprotocol that allows commit to happen even if some replicas are unavailable, as\nlong as a majority of replicas of a partition con\ufb01rm that they are in prepared\nstate. When participants recover or get reconnected, or otherwise discover that\nthey do not have the latest updates, they need to query other nodes to catch up on\nmissing updates. References that provide details of such solutions may be found\nin the bibliographic notes for this chapter, available online.\n2.The second problem is how to deal with the failure of the coordinator during\nan execution of two-phase commit protocol, which could lead to the blocking\nproblem. Consensus protocols, which we study in Section 23.8, provide a robust\nway of implementing two-phase commit without the risk of blocking even if the\ncoordinator fails, as long as a majority of the nodes are up and connected, as we\nwill see in Section 23.8.5.\n3.The third problem is that reads pay a higher price, having to contact a majority\nof the copies. We study approaches to reducing the read overhead in Section\n23.4.3.2.\n", "1155": "23.4 Replication 1127\n23.4.3.2 Reducing Read Cost\nOne approach to dealing with this problem is to use the idea of read and write quorums\nfrom the quorum consensus protocol; reads can read from a smaller read quorum, while\nwrites have to successfully write to a larger write quorum. There is no change to the\nversion numbering technique described earlier. The drawback of this approach is that\na higher write quorum increases the chance of blocking of update transactions, due to\nfailure or disconnection of nodes. As a special case of quorum consensus, we give unit\nweights to all nodes, set the read quorum to 1, and set the write quorum to n(all nodes).\nThis corresponds to the read-any-write-all protocol we saw earlier. There is no need to\nuse version numbers with this protocol. However, if even a single node containing a\ndata item fails, no write to the item can proceed, since the write quorum will not be\navailable.\nA second approach is to use the primary copy technique for concurrency control\nand force all updates to go through the primary copy. Reads can be satis\ufb01ed by ac-\ncessing only one node, in contrast to the majority or quorum protocols. However, an\nissue with this approach is how to handle failures. If the primary copy node fails, and\nanother node is assigned to act as the primary copy, it must ensure that it has the latest\nversion of all data items. Subsequently, reads can be done at the primary copy, without\nhaving to read data from other copies.\nThis approach requires that there be at most one node that can act as primary copy\nat a time, even in the event of network partitions. This can be ensured using leases as\nwe saw earlier in Section 23.7. Furthermore, this approach requires an e\ufb03cient way\nfor the new coordinator to ensure that it has the latest version of all data items. This\ncan be done by having a log at each node and ensuring the logs are consistent with\neach other. This problem is by itself a nontrivial process, but it can be solved using\ndistributed consensus protocols which we st udy in Section 23.8. Distributed consensus\ninternally uses a majority scheme to ensure consistency of the logs. But it turns out that\nif distributed consensus is used to keep logs synchronized, there is no need for version\nnumbering.\nIn fact, consensus protocols provide a way of implementing fault-tolerant replica-\ntion of data, as we see later in Section 23.8.4. Many fault-tolerant storage system imple-\nmentations today are built using fault-toler ant replication of data based on consensus\nprotocols.\nThere is a variant of the primary copy scheme, called the chain replication protocol,\nwhere the replicas are ordered. Each update is sent to the \ufb01rst replica, which records\nit locally and forwards it to the next replica, and so on. The update is completed when\nthe last (tail) replica receives the update. Reads must be executed at the tail replica, to\nensure that only updates that have been fully replicated are read. If a node in a replica\nchain fails, recon\ufb01guration is required to update the chain; further, the system must\nensure that any incomplete updates are completed before processing further updates.\nOptimized versions of the chain replication scheme are used in several storage systems.\n", "1156": "1128 Chapter 23 Parallel and Distributed Transaction Processing\nReferences providing more details of the chain replication protocol may be found in\nthe Further Reading section at the end of the chapter.\n23.4.4 Reconfiguration and Reintegration\nWhile nodes do fail, in most cases nodes recover soon, and the protocols described\nearlier can ensure that they will catch up with any updates that they missed.\nHowever, in some cases a node may fail permanently. The system must then be\nrecon\ufb01gured to remove failed nodes, and to allow other nodes to take over the tasks\nassigned to the failed node. Further, the database catalog must be updated to remove\nthe failed node from the list of replicas of all data items (or relation partitions) that\nwere replicated at that node.\nAs discussed earlier, a network failure may result in a node appearing to have failed,\neven if it has not actually failed. It is safe to remove such a node from the list of replicas;\nreads will no longer be routed to the node even though it may be accessible, but that\nwill not cause any consistency problems.\nIf a failed node that was removed from the system eventually recovers, it must\nbereintegrated into the system. When a failed node recovers, if it had replicas of any\npartition or data item, it must obtain the current values of these data items it stores.\nThe database recovery log at a live site can be used to \ufb01nd and perform all updates that\nhappened when the node was down,\nReintegration of a node is more complicated than it may seem to be at \ufb01rst glance,\ns i n c et h e r em a yb eu p d a t e st ot h ed a t ai t e m sp r o c e s s e dd u r i n gt h et i m et h a tt h en o d e\nis recovering. The database recovery log at a live site is used for catching up with the\nlatest values for all data items at the node. Once it has caught up with the current value\nof all data items, the node should be added back into the list of replicas for the relevant\npartitions/data items, so it will receive all future updates. Locks are obtained on the\npartitions/data items, updates up to that point are applied from the log, and the node is\nadded to the list of replicas for the partitions or data items, before releasing the locks.\nSubsequent updates will be applied directly to the node, since it will be in the list of\nreplicas.\nReintegration is much easier with the majority-based protocols in Section 23.4.3.1,\nsince the protocol can tolerate nodes with out-of-date data. In this case, a node can be\nreintegrated even before catching up on updates, and the node can catch up with missed\nupdates subsequently.\nRecon\ufb01guration depends on nodes having an up-to-date version of the catalog that\nrecords what table partitions (or data items) are replicated at what nodes; thus infor-\nmation must be consistent across all nodes in a system. The replication information\nin the catalog could be stored centrally, and consulted on each access, but such a de-\nsign would not be scalable since the central node would be consulted very frequently\nand would get overloaded. To avoid such a bottleneck, the catalog itself needs to be\npartitioned, and it may be replicated, for example, using the majority protocol.\n", "1157": "23.5 Extended Concurrency Control Protocols 1129\n23.5 Extended Concurrency Control Protocols\nIn this section, we describe further extensions to distributed concurrency control proto-\ncols. We \ufb01rst consider multiversion 2PL a n dh o wi tc a nb ee x t e n d e dt og e tg l o b a l l yc o n -\nsistent timestamps, in Section 23.5.1. Extensions of snapshot isolation to distributed\nsettings are described in Section 23.5.2. Issues in concurrency control in heteroge-\nneous distributed databases, where each node may have its own concurrency control\ntechnique, are descri bed in Section 23.5.3.\n23.5.1 Multiversion 2PLand Globally Consistent Timestamps\nThe multiversion two-phase locking ( MV2PL ) protocol, described in Section 18.7.2,\ncombines the bene\ufb01ts of lock-free read-only transactions with the serializability guar-\nantees of two-phase locking. Read-only transactions see a snapshot at a point in time,\nwhile update transactions use two-phase locking but create new versions of each\ndata item that they update. Recall that with this protocol, each transaction Tigets a\nunique timestamp CommitTS( Ti) (which could be a counter, instead of actual time)\na tt h et i m eo fc o m m i t .T h et r a n s a c t i o ns e t st h et i m e s t a m po fa l li t e m st h a ti tu p -\ndates to CommitTS( Ti). Only one transaction performs commit at a point in time;\nthis guarantees that once Ticommits, a read-only transaction Tjwhose StartTS( Tj)\nis set to CommitTS( Ti) will see committed values of all versions with timestamp \u2264\nCommitTS( Ti).\nMV2PL can be extended to work in a distributed setting by having a central coordi-\nnator, that assigns start and commit timestamps and ensures that only one transaction\ncan perform commit at a point in time. However, the use of a central coordinator limits\nscalability in a massively parallel data storage system.\nThe Google Spanner data storage system pioneered a version of the MV2PL system\nthat is scalable and uses timestamps based on real clock time. We study the Spanner\nMV2PL implementation in the rest of this section.\nSuppose every node has a perfectly accurate clock, and that commit processing can\nhappen instantly with no delay between initiation of commit and its completion. Then,\nwhen a transaction wants to commit, it gets a commit timestamp by just reading the\nclock at any one node at any time after getting all locks, but before releasing any lock.\nAll data item versions created by the transaction use this commit timestamp. Transac-\ntions can be serialized by this commit timestamp. Read-only transactions simply read\nthe clock when they start and use it to get a snapshot of the database as of their start\ntime.\nIf the clocks are perfectly accurate, and commit processing is instantaneous, this\nprotocol can be used to implement MV2PL without any central coordination, making\nit very scalable.\nUnfortunately, in the real world, the above assumptions do not hold, which can\nlead to the following problems:\n", "1158": "1130 Chapter 23 Parallel and Distributed Transaction Processing\n1.Clocks are never perfectly accurate, and the clock at each node may be a little\nfast or a little slow compared to other clocks.\nThus, it is possible to have the following situation. Two update transactions\nT1and T2, both write a data item x,w i t h T1writing it \ufb01rst, followed by T2but\nT2may end up with a lower commit timestamp because it got the timestamp at\na di\ufb00erent node than T1. This situation is not consistent with the serialization\nordering of T1and T2, and it cannot happen with MV2PL in a centralized setting.\n2.Commit processing takes time, which can cause read-only transactions to miss\nupdates if the protocol is not carefully designed. Consider the following situation.\nAr e a d - o n l yt r a n s a c t i o n T1with start timestamp t1reads data item xat node N1,i t\nis possible that soon after the read, another transaction T2with CommitTS( T2)\u2264\nt1(which got the commit timestamp at a di\ufb00erent node N2) may still perform a\nwrite on x. Then, T1should have read the value written by T2,b u td i dn o ts e ei t .\nTo deal with the \ufb01rst problem, namely, the lack of clock synchronization, Spanner\nuses the following techniques.\n\u2022Spanner has a few atomic clocks that are very accurate at each data center and uses\nthe time they provide, along with time information from the Global Positioning\nsystem (GPS) satellites, which provides very accurate time information, to get a\nvery good estimate of time at each node. We use the term true time to refer to the\ntime that would have been given by an absolutely accurate clock.\nEach node periodically communicates with time servers to synchronize its\nclock; if the clock has gone faster it is (logically) slowed down, and if it is slower,\nit is moved forward to the time from the server. In between synchronizations the\nlocal clock continues to tick, advancing the local time. A clock that ticks slower\nor faster than the correct rate results in local time at the node that is progressively\nbehind or ahead of the true time.\n\u2022The second key technique is to measure local clock drift each time the node syn-\nchronizes with a time server and to use it to estimate the rate at which the lo-\ncal clock loses or gains time. Using this information, the Spanner system main-\ntains a value \u03f5such that if the local clock time is t\u2032, the true time tis bounded by\nt\u2032\u2212\u03f5\u2264t\u2264t\u2032+\u03f5. The Spanner system is able to keep the uncertainty value \u03f5to\nless than 10 msec typically. The TrueTime APIused by Spanner allows the system\nto get the current time value, along with an upper bound on the uncertainty in the\ntime value.\n\u2022The next piece of the solution is an idea called commit wait . The idea is as follows:\nAfter all locks have been acquired at all nodes, the local time t\u2032is read at a coordi-\nn a t o rn o d e .W ew o u l dl i k et ou s et h et r u et i m ea st i m e s t a m p ,b u tw ed o n \u2019 th a v et h e\nexact value. Instead, the highest p ossible value of true time, namely, t\u2032+\u03f5,i su s e d\nas a commit timestamp tc. The transaction then waits, while holding locks , until it\n", "1159": "23.5 Extended Concurrency Control Protocols 1131\nis sure that the true time tis\u2265tc; this just requires waiting for a time interval 2 \u03f5,\ncalculated as described earlier.\nWhat the commit wait guarantees is that if a transaction T1has a commit\ntimestamp tc, at the true time tcall locks were held by T1.\n\u2022Given the above, if a version xto fad a t ai t e m xhas a timestamp t,w ec a ns a yt h a t\nthat was indeed the value of xat true time t. This allows us to de\ufb01ne a snapshot of\nthe database at a time t, containing the latest versions of all data items as of time t.\nA database system is said to provide external consistency if the serialization order\nis consistent with the real-world time ordering in which the transactions commit.\nSpanner guarantees external consistency by ensuring that the timestamps used to\nde\ufb01ne the transaction serialization order correspond to the true time when the\ntransactions commit.\n\u2022One remaining issue is that transaction co mmit processing takes time (particularly\nso when 2PCis used). While a transaction with commit timestamp tis committing,\nar e a do f xby a read-only transaction with timestamp t1\u2265tmay not see the version\nxt, either because the timestamp has not yet been propagated to the node with the\ndata item x, or the transaction is in prepared state.\nTo deal with this problem, reads that ask for a snapshot as of time t1are made\nto wait until the system is sure that no transactions with timestamp \u2264t1are still\nin the process of committing. If a transaction with timestamp t\u2264t1is currently in\nthe prepared phase of 2PC, and we are not sure whether it will commit or abort, a\nread with timestamp t1would have to wait until we know the \ufb01nal commit status\nof the transaction.\nRead-only transactions can be given a somewhat earlier timestamp, to guaran-\ntee that they will not have to wait; the trade-o\ufb00 here is that to avoid waiting, the\ntransaction may not see the latest version of some data items.\n23.5.2 Distributed Snapshot Isolation\nSince snapshot isolation is widely used, extending it to work in a distributed setting is of\nsigni\ufb01cant practical importance. Recall from Section 18.8 that while snapshot isolation\ndoes not guarantee serializability, it avoids a number of concurrency anomalies.\nIf each node implements snapshot isolation independently, the resultant schedules\ncan have anomalies that cannot occur in a centralized system. For example, suppose\ntwo transactions, T1and T2r u nc o n c u r r e n t l yo nn o d e N1,w h e r e T1writes xand T2\nreads x;t h u s T2would not see updates made by T1tox. Suppose also that T1updates a\ndata item yat node N2, and commits, and subsequently T2reads yat node N2.T h e n T2\nwould see the value of yupdated by T1atN2,b u tn o ts e e T1\u2019s update to xatN1.S u c ha\nsituation could never occur when using snapshot isolation at a single node. Thus, just\ndepending on local enforcement of snapshot isolation at each node is not su\ufb03cient to\nenforce snapshot isolation across nodes.\n", "1160": "1132 Chapter 23 Parallel and Distributed Transaction Processing\nSeveral alternative distributed snapshot isolation protocols have been proposed\nin the literature. Since the protocols are somewhat complicated, we omit details, but\nreferences with more details may be found in the bibliographic notes for this chap-\nter, available online. Some of these protocols allow local transactions at each node to\nexecute without any global coordination step; an extra cost is paid only by global trans-\nactions, that is, transactions that execute at more than one node. These protocols have\nbeen prototyped on several databases/data storage systems, such as SAP HANA and\nHBase.\nThere has been some work on extending distributed snapshot isolation protocols to\nmake them serializable. Approaches explored include adding timestamp checks similar\nto timestamp ordering, creating a transaction dependency graph at a central server, and\nchecking for cycles in the graph, among other approaches.\n23.5.3 Concurrency Control in Federated Database Systems\nRecall from Section 20.5 that in many cases a distributed database has to be con-\nstructed by linking together multiple already-existing database systems, each with its\nown schema and possibly running di\ufb00erent database-management software. Recall that\nsuch systems are called federated database systems orheterogeneous distributed database\nsystems , and they consist of a layer of software on top of the existing database systems.\nTransactions in a federated database may be classi\ufb01ed as follows:\n1.Local transactions . These transactions are executed by each local database system\noutside of the federated database system\u2019s control.\n2.Global transactions . These transactions are executed under the control of the\nfederated database system.\nThe federated database system is aware of the fact that local transactions may run at\nthe local nodes, but it is not aware of what speci\ufb01c transactions are being executed, or\nof what data they may access.\nEnsuring the local autonomy of each database system requires that no changes be\nmade to its software. A database system at one node thus is not able to communicate\ndirectly with one at any other node to synchronize the execution of a global transaction\nactive at several nodes.\nSince the federated database system has no control over the execution of local\ntransactions, each local system must use a concurrency-control scheme (e.g., two-phase\nlocking or timestamping) to ensure that its schedule is serializable. In addition, in the\ncase of locking, the local system must be able to guard against the possibility of local\ndeadlocks.\nThe guarantee of local serializability is not su\ufb03cient to ensure global serializability.\nAs an illustration, consider two global transactions T1and T2, each of which accesses\nand updates two data items, Aand B, located at nodes N1and N2, respectively. Suppose\nthat the local schedules are serializable. It is still possible to have a situation where, at\n", "1161": "23.6 Replication with Weak Degrees of Consistency 1133\nnode N1,T2follows T1,w h e r e a s ,a t N2,T1follows T2, resulting in a nonserializable\nglobal schedule. Indeed, even if there is no concurrency among global transactions\n(i.e., a global transaction is submitted only after the previous one commits or aborts),\nlocal serializability is not su\ufb03cient to ensure gl obal serializability (see Practice Exercise\n23.11).\nDepending on the implementation of the local database systems, a global transac-\ntion may not be able to control the precise loc king behavior of its local subtransactions.\nThus, even if all local database systems follow two-phase locking, it may be possible\nonly to ensure that each local transaction follows the rules of the protocol. For exam-\nple, one local database system may commit its subtransaction and release locks, while\nthe subtransaction at another local system is still executing. If the local systems permit\ncontrol of locking behavior and all systems follow two-phase locking, then the feder-\nated database system can ensure that global transactions lock in a two-phase manner\nand the lock points of con\ufb02icting transactions would then de\ufb01ne their global serializa-\ntion order. If di\ufb00erent local systems follow di\ufb00erent concurrency-control mechanisms,\nhowever, this straightforward sort of global control does not work.\nThere are many protocols for ensuring consistency despite the concurrent execu-\ntion of global and local transactions in federated database systems. Some are based\non imposing su\ufb03cient conditions to ensure global serializability. Others ensure only\na form of consistency weaker than serializability but achieve this consistency by less\nrestrictive means.\nThere are several schemes to ensure global serializability in an environment where\nupdate transactions as well as read-only transactions can execute. Several of these\nschemes are based on the idea of a ticket . A special data item called a ticket is cre-\nated in each local database system. Every global transaction that accesses data at a\nnode must write the ticket at that node. This requirement ensures that global trans-\nactions con\ufb02ict directly at every node they visit. Furthermore, the global transaction\nmanager can control the order in which global transactions are serialized, by control-\nling the order in which the tickets are accessed. References to such schemes appear in\nthe bibliographic notes for this chapter, available online.\n23.6 Replication with Weak Degrees of Consistency\nThe replication protocols we have seen so far guarantee consistency, even if there are\nnode and network failures. However, these protocols have a nontrivial cost, and further\nthey may block if a signi\ufb01cant number of nodes fail or get disconnected due to a network\npartition. Further, in the case of a network partition, a node that is not in the majority\npartition would not only be unable to perform writes, but it would also be unable to\nperform even reads.\n", "1162": "1134 Chapter 23 Parallel and Distributed Transaction Processing\nMany applications wish to have higher availability, even at the cost of consistency.\nWe study the trade-o\ufb00s between consistency and availability in this section.\n23.6.1 Trading Off Consistency for Availability\nThe protocols we have seen so far require a (weighted) majority of nodes be in a par-\ntition for updates to proceed. Nodes that are in a minority partition cannot process\nupdates; if a network failure results in more than two partitions, no partition may have\na majority of nodes. Under such a situation, the system would be completely unavail-\nable for updates, and depending on the read-quorum, may even become unavailable for\nreads. The write-all-available protocol which we saw earlier provides availability but not\nconsistency.\nIdeally, we would like to have consistency and availability, even in the face of par-\ntitions. Unfortunately, this is not possible, a fact that is crystallized in the so-called\nCAP theorem , which states that any distributed database can have at most two of the\nfollowing three properties:\n\u2022Consistency.\n\u2022Availability.\n\u2022Partition-tolerance.\nThe proof of the CAP theorem uses the following de\ufb01nition of consistency, with repli-\ncated data: an execution of a set of operations (reads and writes) on replicated data\nis said to be consistent if its result is the same as if the operations were executed on\na single node, in a sequential order that is consistent with the ordering of operations\nissued by each process (transaction). The notion of consistency is similar to atomicity\nof transactions, but with each operation treated as a transaction, and is weaker than\nthe atomicity property of transactions.\nIn any large-scale distributed system, partitions cannot be prevented, and as a re-\nsult, either availability or consistency has to be sacri\ufb01ced. The schemes we have seen\nearlier sacri\ufb01ce availability for consistency in the face of partitions.\nConsider a web-based social-networking system that replicates its data on three\nservers, and a network partition occurs that prevents the servers from communicating\nwith each other. Since none of the partitions has a majority, it would not be possible to\nexecute updates on any of the partitions. If one of these servers is in the same partition\nas a user, the user actually has access to data, but would be unable to update the data,\nsince another user may be concurrently updating the same object in another partition,\nwhich could potentially lead to inconsistency. Inconsistency is not as great a risk in a\nsocial-networking system as in a banking database. A designer of such a system may\ndecide that a user who can access the system should be allowed to perform updates on\nwhatever replicas are accessible, even at the risk of inconsistency.\n", "1163": "23.6 Replication with Weak Degrees of Consistency 1135\nIn contrast to systems such as ban king databases that require the ACID properties,\nsystems such as the social-networking system mentioned above are said to require the\nBASE properties:\n\u2022Basically available.\n\u2022Soft state.\n\u2022Eventually consistent.\nThe primary requirement is availability, even at the cost of consistency. Updates should\nbe allowed, even in the event of partitioning, following, for example, the write-all-\navailable protocol (which is similar to multi master replication described in Section\n23.6). Soft state refers to the property that the state of the database may not be precisely\nde\ufb01ned, with each replica possibly having a somewhat di\ufb00erent state due to partition-\ning of the network. Eventually consistent is the requirement that once a partitioning is\nresolved, eventually all replicas will become consistent with each other.\nThis last step requires that inconsistent copies of data items be identi\ufb01ed; if one is\nan earlier version of the other, the earlier version can be replaced by the later version.\nIt is possible, however, that the two copies were the result of independent updates to\na common base copy. A scheme for detecting such inconsistent updates, called the\nversion-vector scheme, is described in Section 23.6.4.\nRestoring consistency in the face of inconsistent updates requires that the updates\nbe merged in some way that is meaningful to the application. We discuss possible so-\nlutions for resolution of con\ufb02ict ing updates, in Section 23.6.5.\nIn general, no system designer wants to deal with the possibility of inconsistent\nupdates and the resultant problems of detection and resolution. Where possible, the\nsystem should be kept consistent. Inconsistent updates are allowed only when a node\nis disconnected from the network, in applications that can tolerate inconsistency.\nSome key-value stores such as Apache Cassandra and MongoDB allow an applica-\ntion to specify how many replicas need to be accessible to carry out a write operation\nor a read operation. As long as a majority of replicas are accessible, there is no problem\nwith consistency for writes. However, if the application sets the required number at less\nthan a majority, and many replicas are inaccessible, updates are allowed to go ahead;\nthere is, however, a risk of inconsistent updates, which must be resolved later.\nFor applications where inconsistency can cause signi\ufb01cant problems, or is harder\nto resolve, system designers prefer to build fa ult-tolerant systems using replication and\ndistributed consensus that avoid inconsistencies, even at the cost of potential non-\navailability.\n23.6.2 Asynchronous Replication\nMany relational database systems support replication with weak consistency, which\ncan take one of several forms.\n", "1164": "1136 Chapter 23 Parallel and Distributed Transaction Processing\nWith asynchronous replication the database allows updates at a primary node (also\nreferred to as the master node) and propagates updates to replicas at other nodes sub-\nsequently; the transaction that performs the update can commit once the update is\nperformed at the primary, even before rep licas are updated. Propagation of updates\nafter commit is also referred to as lazy propagation .I nc o n t r a s t ,t h et e r m synchronous\nreplication refers to the case where updates are propagated to other replicas as part of\nas i n g l et r a n s a c t i o n .\nWith asynchronous replication, the system must guarantee that once the trans-\naction commits at the primary, the updates are eventually propagated to all replicas,\neven if there are system failures in between. Later in this section, we shall see how this\nproperty is guaranteed using persistent messaging.\nSince propagation of updates is done asynchronously, a read at a replica may not\nget the latest version of a data item. Asynchronous propagation of updates is commonly\nused to allow update transactions to commit quickly, even at the cost of consistency.\nA system designer may choose to use replicas only for fault tolerance. However, if the\nreplica is available on a local machine, or another machine that can be accessed with\nlow latency, it may be much cheaper to read the data item at the replica instead of\nreading it from the primary, as long as the application is willing to accept potentially\nstale data values.\nData storage systems based on asynchronous replication may allow data items to\nhave versions, with associated timestamps. A transaction may then request a version\nwith required freshness properties, for example not more than 10 minutes old. If a local\nreplica has a version of the data item satisfying the freshness criterion, it can be used;\notherwise, the read may have to be sent to the primary node.\nConsider, for example, an airline reservation site that shows the prices of multiple\n\ufb02ight options. Prices may vary frequently, and the system does not guarantee that a\nuser will actually be able to book a ticket at the price shown initially. Thus, it is quite\nacceptable to show a price that is a few minutes old. Asynchronous replication is a\ngood solution for this application: price data can be replicated to a large number of\nservers, which share the load of user queries; and price data are updated at a primary\nnode and replicated asynchronously to all other replicas.\nMultiversion concurrency control schemes can be used to give a transaction-\nconsistent snapshot of the database to read-only transactions that execute at a replica;\nthat is, the transaction should see all updates of all transactions up to some transaction\nin the serialization order and should not see any updates of transactions later in the\nserialization order. The multiversion 2PLscheme, described in Section 23.5.1, can be\nextended to allow a read-only transaction to access a replica that may not have up-to-\ndate versions of some data items, but still get a transaction-consistent snapshot view\nof the database. To do so, replicas must be aware of what is the latest timestamp tsafe\nsuch that they have received all updates with commit timestamp before t.A n yr e a do f\na snapshot with timestamp t<tsafecan be processed by that replica. Such a scheme is\nused in the Google Spanner database,\n", "1165": "23.6 Replication with Weak Degrees of Consistency 1137\nAsynchronous replication is used in traditional (centralized) databases to create\none or more replicas of the database, on which large queries can be executed, without\ninterfering with transactions running on a primary node. Such replication is referred\ntomaster-slave replication , since the replicas cannot perform any updates on their own\nbut must only perform updates that the master node asks them to perform.\nIn such systems, asynchronous propagation of updates is typically done in a con-\ntinuous fashion to minimize delays until an update is seen at a replica. However, in\ndata warehouses, updates may be propagated periodically\u2014every night, for example\u2014\nso that update propagation does not interfere with query processing.\nSome database systems support multimaster replication (also called update-\nanywhere replication ); updates are permitted at any replica of a data item and are propa-\ngated to all replicas either synchronously, using two-phase commit, or asynchronously.\nAsynchronous replication is also used in some distributed storage systems. Such\nsystems partition data, as we have seen earlier, but replicate each partition. There is a\nprimary node for each partition, and updates are typically sent to the primary node,\nwhich commits the updates locally, and propagates them asynchronously to the other\nreplicas of the partition. Some systems such as PNUTS even allow each data item in\na partition to specify which node should act as the primary node for that data item;\nthat node is responsible for committing updates to the data item, and propagating the\nupdate to the other replicas. The motivation is to allow a node that is geographically\nclose to a user to act as the primary node for data items corresponding to that user.\nIn any system supporting asynchronous propagation of updates, it is important\nthat once an update is committed at the primary, it must de\ufb01nitely be delivered to the\nother replicas. If there are multiple updates at a primary node, they must be delivered\nin the same order to the replicas; out-of-order delivery can cause an earlier update to\narrive late and overwrite a later update.\nPersistent messaging , which we saw in Section 23.2.3, provides guaranteed deliv-\nery of messages and is widely used for asynchronous replication. The implementation\ntechniques for persistent messages described in Section 23.2.3 can be easily modi\ufb01ed to\nensure that messages are delivered in the order in which they were sent. With persistent\nmessaging, each primary node needs to be aware of the location of all the replicas.\nPublish-subscribe systems , which we saw in Section 22.8.1, o\ufb00er a more \ufb02exible\nway of ensuring reliable message delivery. Re call that publish-subscribe systems allow\nmessages to be published with an associated topic, and subscribers can subscribe to any\ndesired topic. To implement asynchronous replication, a topic is created corresponding\nto each partition. All replicas of a partition subscribe to the topic corresponding to the\npartition. Any update (including inserts, deletes, and data item updates) to a partition\nis published as a message with the topic corr esponding to the partition. The publish-\nsubscribe system ensures that once such a message is published, it will be delivered to\nall subscribers in the order in which it was published.\nPublish-subscribe systems designed for parallel systems, such as the Apache Kafka\nsystem, or the Yahoo Message Bus service used for asynchronous replication in the\nPNUTS distributed data storage system, allow a large number of topics, and use mul-\n", "1166": "1138 Chapter 23 Parallel and Distributed Transaction Processing\ntiple servers to handle messages to di\ufb00erent topics in parallel. Thus, asynchronous\nreplication can be made scalable.\nFault tolerance is an issue with asynchronous propagation of updates. If a primary\nnode fails, a new node must take over as primary; this can be done either using an\nelection algorithm, as we saw earlier or by having a master node (which is itself chosen\nby election) decide which node takes over the job of a failed primary node.\nConsider what happens if a primary copy records an update but fails before the\nupdate is sent to the replicas. The new primary node has no way of \ufb01nding out what\nwas the last update committed at the primary copy. It can either wait for the primary to\nrecover, which is unacceptable, or it can proceed without knowing what updates were\ncommitted just before failure. In the latter case, there is a risk that a transaction on the\nnew primary may read an old value of a data item or perform an update that con\ufb02icts\nwith an earlier update on the old primary.\nTo reduce the chance of such problems, some systems replicate the log records\nof the primary node to a backup node and allow the transaction to commit at the\nprimary only after the log record has been successfully replicated at the backup node;\nif the primary node fails, the backup node takes over as the primary. Recall that this\nis the two-safe protocol from Section 19.7. This protocol is resilient to failure of one\nnode, but not to the failure of two nodes.\nIf an application is built on top of a storage system using asynchronous replication,\napplications may potentially see some anomalous behaviors such as a read not seeing\nthe e\ufb00ect of an earlier write done by the same application, or a later read seeing an\nearlier version of a data item than an earlier read, if di\ufb00erent reads and writes are sent\nto di\ufb00erent replicas. While such anomalies cannot be completely prevented in the event\nof failures, they can be avoided during normal operation by taking some precautions.\nFor example, if read and write requests for a data item from a particular node are always\nsent to the same replica, the application will see any writes it has performed, and if two\nreads are performed on the same data item, the later read will see a version at least\nas new as the earlier read. This property is guaranteed if a primary replica is used to\nperform all actions on a data item.\n23.6.3 Asynchronous View Maintenance\nIndices and materialized views are forms of data derived from underlying data, and\ncan thus be viewed as forms of replicated data. Just like replicas, indices and materi-\nalized views could be updated (maintained) as part of each transaction that updates\nthe underlying data; doing so would ensure consistency of the derived data with the\nunderlying data.\nHowever, many systems prefer to perform index and view maintenance in an asyn-\nchronous manner, to reduce the overhead on transactions that update the underlying\ndata. As a result, the indices and materialized views could be out of date. Any transac-\n", "1167": "23.6 Replication with Weak Degrees of Consistency 1139\ntion that uses such indices or materialized views must be aware that these structures\nmay be out of date.\nWe now consider how to maintain indices and materialized views in the face of\nconcurrent updates.\n\u2022The \ufb01rst requirement for view maintenance is for the subsystem that performs\nmaintenance to receive information about updates to the underlying data in such\na way that each update is delivered exactly once, despite failures.\nPublish-subscribe systems are a good match for the \ufb01rst requirement above.\nAll updates to any underlying relation are published to the pub-sub system with\nthe relation name as the topic; the view maintenance subsystem subscribes to the\ntopics corresponding to its underlying relations and received all relevant updates.\nAs we saw in Section 22.8.1, we can have t opics corresponding to each tablet of\na stored relation. For a nonmaterialized intermediate relation that is partitioned,\nwe can have a topic corresponding to each partition.\n\u2022The second requirement is for the subsystem to update the derived data in such\na way that the derived data will be consistent with the underlying data, despite\nconcurrent updates to the underlying data.\nSince the underlying data may receive further updates as an earlier update\nis being processed, no asynchronous view maintenance technique can guarantee\nthat the view state is consistent with the state of the underlying data at all times.\nHowever, the consistency requirement can be formalized as follows: if there are\nno updates to the underlying data for a su\ufb03cient amount of time, asynchronous\nmaintenance must ensure that the derived data is consistent with the underlying\ndata; such a requirement is known as an eventual consistency requirement.\nThe technique for parallel maintenance of materialized views which we saw in\nSection 22.7.5 uses the exchange operator model to send updates to nodes and\nallows view maintenance to be done locally. Techniques designed for view main-\ntenance in a centralized setting can be used at each node, on locally materialized\ninput data. Recall from Section 16.5.1 that view maintenance may be deferred,\nthat is, it may be done after the transaction commits. Techniques for deferred view\nmaintenance in a centralized setting already need to deal with concurrent updates;\nsuch techniques can be used locally at each node.\n\u2022A third requirement is for reads to get a consistent view of data. In general, a query\nthat reads data from multiple nodes may not observe the updates of a transaction\nTon node N1, but may see the updates that Tperformed on node N2, thus seeing\na transactionally inconsistent view of data. Systems that use asynchronous repli-\ncation typically do not support transactionally consistent views of the database.\nFurther, scans of the database may not see an operation-consistent view of the\ndatabase. (Recall the notion of operation consistency from Section 18.9, which\nrequires that any operation should not see a database state that re\ufb02ects only some\nof the updates of another operation. In Section 18.9 we saw an example of a scan\n", "1168": "1140 Chapter 23 Parallel and Distributed Transaction Processing\nusing an index that could see two versions, or neither version, of a record updated\nby a concurrent transaction, if the relation scan does not follow two-phase locking.\nA similar problem occurs with asynchronous propagation of updates, even if both\nthe relation scan and the update transaction follow two-phase locking.\nFor example, consider a relation r(A,B,C), with primary key A, which is parti-\ntioned on attribute B. Now consider a query that is scanning the relation r. Suppose\nthere is a concurrent update to a tuple t1\u2208r, which updates attribute t1.Bfrom v1\ntov2. Such an update requires deletion of the old tuple from the partition corre-\nsponding to value v1, and insertion of the new tuple in the partition corresponding\ntov2. These updates are propagated asynchronously.\nNow, the scan of rcould possibly scan the node corresponding to v1after\nthe old tuple is deleted there but visit the node corresponding to v2before the\nasynchronous propagation inserts the updated tuple in that node. Then, the scan\nwould completely miss the tuple, even though it should have seen either the old\nvalue or the new value of t1. Further, the scan could visit the node corresponding\ntov1before the delete is propagated to that node, and the node corresponding to\nv2after the insert is propagated to that node, and thereby see two versions of t1,\none from before the update and one from after the update. Neither case would\nbe possible with two-phase locking, if updates are propagated synchronously to all\ncopies.\nIf a multiversion concurrency control technique is used, where data items have\ntimestamps, snapshot reads are a good way to get a consistent scan of a relation;\nthe snapshot timestamp should be set a su\ufb03ciently old value that all updates as of\nthat timestamp have reached all replicas.\n23.6.4 Detecting Inconsistent Updates\nMany applications developed for such high availability are designed to continue func-\ntioning locally even when the node running the application is disconnected from the\nother nodes.\nAs an example, when data are replicated, and the network gets partitioned, if a sys-\ntem chooses to trade o\ufb00 consistency to get availability, updates may be done concur-\nrently at multiple replicas. Such con\ufb02icting updates need to be detected and resolved.\nWhen a connection is re-established, the a pplication needs to communicate with a stor-\nage system to send any updates done locally and fetch updates performed elsewhere.\nThere is a potential for con\ufb02icting updates from di\ufb00erent nodes. For example, node N1\nmay update a locally cached copy of a data item while it is disconnected; concurrently\nanother node may have updated the data item on the storage system, or may have up-\ndated its own local copy of the data item. Such con\ufb02icting updates must be detected,\nand resolved.\nAs another example, consider an application on a mobile device that supports\no\ufb04ine updates (i.e., permits updates even if the mobile device is not connected to the\n", "1169": "23.6 Replication with Weak Degrees of Consistency 1141\nnetwork). To give the user a seamless usage experience, such applications perform the\nupdates on a locally cached copy, and then apply the update to the data store when the\ndevice goes back online. If the same data item may be updated from multiple devices,\nthe problem of con\ufb02icting updates arises here, too. The schemes described below can\nbe used in this context too, with nodes understood to also refer to mobile devices.\nA mechanism for detecting con\ufb02icting updates is described in this section. How to\nresolve con\ufb02icting updates once they are detected is application dependent, and there\nis no general technique for doing so. However, some commonly used approaches are\ndiscussed in Section 23.6.5.\nFor data items updated by only one node, it is a simple matter to propagate the\nupdates when the node gets reconnected to the storage system. If the node only caches\nread-only copies of data that may be updated by other nodes, the cached data may be-\ncome inconsistent. When the node gets reconnected, it can be sent invalidation reports\nthat inform it of out-of-date cache entries.\nHowever, if updates can occur at more than one node, detecting con\ufb02icting up-\nd a t e si sm o r ed i \ufb03 c u l t .S c h e m e sb a s e do n version numbering allow updates of shared\ndata from multiple nodes. These schemes do not guarantee that the updates will be\nconsistent. Rather, they guarantee that, if two nodes independently update the same\nversion of a data item, the clash will be detected eventually, when the nodes exchange\ninformation either directly or through a common node.\nThe version-vector scheme detects inconsistencies when replicas of a data item are\nindependently updated. This scheme allows copies of a data item to be stored at mul-\ntiple nodes.\nThe basic idea is for each node ito store, with its copy of each data item d,aversion\nvector \u2014that is, a set of version numbers {V[j]}, with one entry for each other node j\non which the data item could potentially be updated. When a node iupdates a data\nitem d, it increments the version number V[i]b yo n e .\nFor example, suppose a data item is replicated at nodes N1,N2and N3.I ft h ei t e m\nis initially created at N1, the version vector could be [1, 0, 0]. If it is then replicated\natN2,a n dt h e nu p d a t e da tn o d e N2, the resultant version vector would be [1, 1, 0].\nSuppose now that this version of the data item is replicated to N3, and then both N2\nand N3concurrently update the data item. Then, the version vector of the data item at\nN2would be [1, 2, 0], while the version vector at N3would be [1, 1, 1].\nWhenever two nodes iand jconnect with each other, they exchange updated data\nitems, so that both obtain new versions of the data items. However, before exchanging\ndata items, the nodes have to discover whether the copies are consistent:\n1.If the version vectors Viand Vjof the copy of the data item at nodes iand jare\nthe same\u2014that is, for each k,Vi[k]=Vj[k]\u2014then the copies of data item dare\nidentical.\n2.If, for each k,Vi[k]\u2264Vj[k] and the version vectors are not identical, then the\ncopy of data item dat node iis older than the one at node j.T h a ti s ,t h ec o p yo f\n", "1170": "1142 Chapter 23 Parallel and Distributed Transaction Processing\ndata item dat node jwas obtained by one or more modi\ufb01cations of the copy of\nthe data item at node i.N o d e ireplaces its copy of d,a sw e l la si t sc o p yo ft h e\nversion vector for d, with the copies from node j.\nIn our example above, if N1had the vector [1, 0, 0] for a data item, while N2\nhad the vector [1, 1, 0], then the version at N2is newer than the version at N1.\n3.If there are a pair of values kand msuch that Vi[k]<Vj[k]a n d Vi[m]>Vj[m],\nthen the copies are inconsistent ;t h a ti s ,t h ec o p yo f daticontains updates per-\nformed by node kthat have not been propagated to node j, and, similarly, the\ncopy of datjcontains updates performed by node mthat have not been propa-\ngated to node i. Then, the copies of dare inconsistent, since two or more updates\nhave been performed on dindependently.\nIn our example, after the concurrent updates at N2and N3,t h et w ov e r s i o n\nvectors show the updates are inconsistent. Let V2and V3denote the version vec-\ntors at N2and N3.T h e n V2[2]=2 while V3[2]=1, whereas V2[3]=0, while\nV3[3]=1.\nManual intervention may be required to merge the updates. After merging the\nupdates (perhaps manually), the version vectors are merged, by setting V[k]t o\nthe maximum of Vi[k]a n d Vj[k]f o re a c h k. The node lthat performs the write\nthen increments V[l] by 1 and then writes the data item and its version vector V.\nThe version-vector scheme was initially designed to deal with failures in distrib-\nuted \ufb01le systems. The scheme gained importance because mobile devices often store\ncopies of data that are also present on server systems. The scheme is also widely used\nin distributed storage systems that allow updates to happen even if a node is not in a\nmajority partition.\nThe version-vector scheme cannot solve the problem of how to reconcile incon-\nsistent copies of data detected by the scheme. We discuss reconciliation in Section\n23.6.5.\nThe version-vector scheme works well for detecting inconsistent updates to a sin-\ngle data item. However, if a storage system has a very large number of replicated items,\n\ufb01nding which items have been inconsistently updated can be quite expensive if done\nnaively. In Section 23.6.6 we study a data structure called a Merkle tree that can e\ufb03-\nciently detect di\ufb00erences between sets of data items.\n23.6.5 Resolving Conflicting Updates\nDetection of con\ufb02icting updates may happen when a read operation fetches copies of\na data item from multiple replicas or when the system executes a background process\nthat compares data item versions.\nAt that point, con\ufb02icting updates on the same data item need to be resolved ,t o\ncreate a single common version. Resolution of con\ufb02icting updates is also referred to as\nreconciliation .\n", "1171": "23.6 Replication with Weak Degrees of Consistency 1143\nThere is no technique for resolution so that can be used across all applications. We\ndiscuss some techniques that have been used in several commonly used applications.\nMany applications can perform reconciliation automatically by executing on each\nnode all update operations that had been performed on other nodes during a period\nof disconnection. This solution requires that the system keep track of operations, for\nexample, adding an item to a shopping cart, or deleting an item from a shopping cart.\nThis solution works if operations commute \u2014that is, they generate the same result, re-\ngardless of the order in which they are executed. The addition of items to a shopping\ncart clearly commutes. Deletions do not commute with additions in general, which\nshould be clear if you consider what happens if an addition of an item is exchanged\nwith a delete of the same item. However, as long as deletion always operates only on\nitems already present in the cart, this problem does not arise.\nAs another example, many banks allow customers to withdraw money from an\nATM even if it is temporarily disconnected from the bank network. When the ATM\ngets reconnected, the withdrawal operation is applied to the account. Again, if there\nare multiple withdrawals, they may get merged in an order di\ufb00erent from the order in\nwhich they happened in the real world, but the end result (balance) is the same. Note\nthat since the operation already took place in the physical world, it cannot be rejected\nbecause of a negative balance; the fact that an account has a negative balance has to\nbe dealt with separately.\nThere are other application-speci\ufb01c solutions for resolving con\ufb02icting updates. In\nthe worst case, however, a system may need to alert humans to the con\ufb02icting updates,\nand let the humans decide how to resolve the con\ufb02ict.\nDealing with such inconsistency automatically, and assisting users in resolving in-\nconsistencies that cannot be handled automatically, remains an area of research.\n23.6.6 Detecting Differences Between Collections Using Merkle Tree\nThe Merkle tree (also known as hash tree ) is a data structure that allows e\ufb03cient detec-\ntion of di\ufb00erences between sets of data items that may be stored at di\ufb00erent replicas.\n(To avoid confusion between tree nodes and system nodes, we shall refer to the latter\nas replicas in this section.)\nDetecting items that have inconsistent values across replicas due to weak consis-\ntency is merely one of motivations for Merkle trees. Another motivation is performing\nsanity checks of replicas that are synchronously updated, and should be consistent, but\nmay be inconsistent due to bugs or other failures. We consider below a binary version\nof the Merkle tree.\nWe assume that each data item has a key and a value; in case we are considering\ncollections that do not have an explicit key, the data item value itself can be used as a\nkey.\nEach data item key kiis hashed by a function h1() to get a hash value with nbits,\nwhere nis chosen such that 2nis within a small factor of the number of data items.\nEach data item value viis hashed by another function h2() to get a hash value (which\n", "1172": "1144 Chapter 23 Parallel and Distributed Transaction Processing\nis typically much longer than nbits). Finally, we assume a hash function h3() which\ntakes as input a collection of hash values and returns a hash value computed from the\ncollection (this hash function must be computed in a way that does not depend on the\ninput order of the hash values, which can be done, for example, by sorting the collection\nbefore computing the hash function).\nEach node of a Merkle tree has associated with it an identi\ufb01er and stores a hash\nvalue. Each leaf of the tree can be identi\ufb01ed by an n-bit binary number. For a given\nleaf identi\ufb01ed by number k, consider the set of all data items iwhose key kiis such\nthat h1(ki)=k. Then, the hash value vkstored at leaf kis computed by applying h2()\non each of the data item values vi, and then applying h3() on the resultant collection\nof hash values. The system also maintains an index that can retrieve all the data items\nwith a given hash value computed by function h2().\nFigure 23.8 shows an example of a Merkle tree on 8 data items. The hash value of\nthese data items on h1are shown on the left. Note that if for an item ij,h1(ij)=k,t h e n\nthe data item ijis associated with the leaf with identi\ufb01er k.\nEach internal node of the Merkle tree is identi\ufb01ed by a hash value that is jbits long\ni ft h en o d ei sa td e p t h j;l e a v e sa r ea td e p t h n, and the root at depth 0. The internal\nnode identi\ufb01ed by a number khas as children nodes identi\ufb01ed by 2 kand 2 k+1. The\nhash value stored vkat node kis computed by applying h3() to the hash value stored at\nnodes 2 kand 2 k+1.\nNow, suppose this Merkle tree is constructed on the data at two replicas (the repli-\ncas may be whole database replicas, or replicas of a partition of the database). If all\nitems at the two replicas are identical, the stored hash values at the root nodes will also\nbe identical.\nAs long as h2() computes a long enough hash value, and is suitably chosen, it is\nvery unlikely that h2(v1)=h2(v2)i fv1\u2260v2, and similarly for h3(). The SHA1 hash\nfunction with a 160-bit hash value is an example of a hash function that satis\ufb01es this\nHash values of\ndata items\nh  (i  )=00\nh  (i  )=01\nh  (i  )=11\nh  (i  )=00\nh  (i  )=10\nh  (i  )=11\nh  (i  )=101 \n1\n1\n1\n1\n1\n1 1\n 2\n 3\n 4\n 5\n 6\n 7\nh  (h  (i  ), h  (i  )) 21 33 4 h  (h  (i  )) 2 2 3 h  (h  (i  ), h  (i  )) 2 533 7 h  (h  (i  ), h  (i  )) 23 33 6Merkle Tree\nNode identi\ufb01er shown above node, and has value shown inside node, \nv  denotes stored hash value in node i00 11 10 0101\nh  (v    , v    ) 2      0100   h  (v   , v  ) 2    10\nh  (v    , v  )  2       11 10\ni\nFigure 23.8 Example of Merkle tree.\n", "1173": "23.6 Replication with Weak Degrees of Consistency 1145\nrequirement. Thus, we can assume that if two nodes have the same stored hash values,\nall the data items under the two nodes are identical.\nIf, in fact, there is a di\ufb00erence in the value of any items at the two replicas, or if\nan item is present at one replica but not at the other, the stored hash values at the root\nwill be di\ufb00erent, with high probability.\nThen, the stored hash values at each of the children are compared with the hash\nvalues at the corresponding child in the other tree. Search traverses down each child\nwhose hash value di\ufb00ers, until a leaf is reached. The traversal is done in parallel on\nboth trees and requires communication to send tree node contents from one replica to\nthe other.\nAt the leaf, if the hash values di\ufb00er, the list of data item keys associated with the\nleaves, and the corresponding data item values are compared across the two trees, to\n\ufb01nd data items whose values di\ufb00er as well as data items that are present in one of the\ntrees but not in the other.\nOne such traversal takes time at most logarithmic in the number of leaf nodes of\nthe tree; since the number of leaf nodes is chosen to be close to the number of data\nitems, the traversal time is also logarithmic in the number of data items. This cost is\npaid at most once for each data item that di\ufb00ers between the two replicas. Furthermore,\na path to a leaf is traversed only if there is, in fact, a di\ufb00erence at the leaf.\nThus, the overall cost for \ufb01nding di\ufb00erences between two (potentially very large)\nsets is O(mlog2N), where mis the number of data items that di\ufb00er and Nis the total\nnumber of data items. Wider trees can be used to reduce the number of nodes encoun-\ntered in a traversal, which would be logKNif each node has Kchildren, at the cost of\nmore data being transferred for each node. Wider trees are preferred if network latency\nis high compared to the network bandwidth.\nMerkle trees have many applications; they can be used to \ufb01nd the di\ufb00erence in\ncontents of two databases that are almost identical without transferring large amounts\nof data. Such inconsistencies can occur due to the use of protocols that only guarantee\nweak consistency. They could also occur because of message or network failures that\nresult in di\ufb00erences in replicas, even if consensus or other protocols that guarantee\nconsistent reads are used.\nThe original use of Merkle trees was for veri\ufb01cation of the contents of a collection\nthat may have potentially been corrupted by malicious users. Here, the Merkle tree leaf\nnodes must store the hash values of all data items that map to it, or a tree variant that\nonly stores one data item at a leaf may be used. Further, the stored hash value at the\nroot is digitally signed, meaning its contents cannot be modi\ufb01ed by a malicious user\nwho does not have the private key used for signing the value.\nTo check an entire relation, the hash values can be recomputed from the leaves\nupwards, and the recomputed hash value at the root can be compared with the digitally\nsigned hash value stored at the root.\nTo check consistency of a single data item, its hash value is recomputed; and then\nso is the hash value for its leaf node ni, using existing hash values for other data items\n", "1174": "1146 Chapter 23 Parallel and Distributed Transaction Processing\nthat hash to the same leaf. Next consider the parent node njof node niin the tree. The\nhash value of njis computed using the recomputed hash value of niand the already\nstored hash values of other children of nj. This process is continued upward until the\nroot of the tree. If the recomputed hash value at the root matches the signed hash value\nstored with the root, the contents of the data item can be determined to be uncorrupted.\nThe above technique works for detecting corruption since with suitably chosen\nhash functions, it is very hard for a malicious user to create replacement values for\ndata items in a way that the recomputed hash value is identical to the signed hash\nvalue stored at the root.\n23.7 Coordinator Selection\nSeveral of the algorithms that we have presented require the use of a coordinator. If\nthe coordinator fails because of a failure of the node at which it resides, the system\ncan continue execution by restarting a new coordinator on another node. One way\nto continue execution is by maintaining a backup to the coordinator that is ready to\nassume responsibility if the coordinator fails. Another way is to \u201celect\u201d a coordinator\nfrom among the nodes that are alive. We outline these options in this section. We\nthen brie\ufb02y describe fault-tolerant distributed services that have been developed to help\ndevelopers of distributed applications perform these tasks.\n23.7.1 Backup Coordinator\nAbackup coordinator is a node that, in addition to other tasks, maintains enough infor-\nmation locally to allow it to assume the role of coordinator with minimal disruption to\nthe distributed system. All messages directed to the coordinator are received by both\nthe coordinator and its backup. The backup coordinator executes the same algorithms\nand maintains the same internal state information (such as, for a concurrency coordi-\nnator, the lock table) as does the actual coordinator. The only di\ufb00erence in function\nbetween the coordinator and its backup is that the backup does not take any action\nthat a\ufb00ects other nodes. Such actions are left to the actual coordinator.\nIn the event that the backup coordinator detects the failure of the actual coordi-\nnator, it assumes the role of coordinator. Since the backup has all the information\navailable to it that the failed coordinator had, processing can continue without inter-\nruption.\nThe prime advantage of the backup approach is the ability to continue processing\nimmediately. If a backup were not ready to assume the coordinator\u2019s responsibility,\na newly appointed coordinator would have to seek information from all nodes in the\nsystem so that it could execute the coordination tasks. Frequently, the only source\nof some of the requisite information is the failed coordinator. In this case, it may be\nnecessary to abort several (or all) active transactions and to restart them under the\ncontrol of the new coordinator.\n", "1175": "23.7 Coordinator Selection 1147\nThus, the backup-coordinator approach avoids a substantial amount of delay while\nthe distributed system recovers from a coordinator failure. The disadvantage is the over-\nhead of duplicate execution of the coordinator\u2019s tasks. Furthermore, a coordinator and\nits backup need to communicate regularly to ensure that their activities are synchro-\nnized.\nIn short, the backup-coordinator approach incurs overhead during normal process-\ning to allow fast recovery from a coordinator failure.\n23.7.2 Election of Coordinator\nIn the absence of a designated backup coordinator, or in order to handle multiple fail-\nures, a new coordinator may be chosen dynamically by nodes that are live.\nOne possible approach is to have a designated node choose a new coordinator,\nwhen the current coordinator has failed. However, this raises the question of what to\ndo if the node that chooses replacement coordinators itself fails.\nIf we have a fault-tolerant lock manager, a very e\ufb00ective way of choosing a new\ncoordinator for a task is to use lock leases . The current coordinator has a lock lease on\na data item associated with the task. If the coordinator fails, the lease will expire. If a\nparticipant determines that the coordinator may have failed, it attempts to get a lock\nlease for the task. Note that multiple participants may attempt to get a lease, but the\nlock manager ensures that only one of them can get the lease. The participant that gets\nthe lease becomes the new coordinator. As di scussed in Section 23.3.3, this ensures\nthat only one node that can be the coordinator at a given time. Lock leases are widely\nused to ensure that a single node gets chosen as coordinator. However, observe that\nthere is an underlying assumption of a fault-tolerant lock manager.\nA participant determines that the coordinator may have failed if it is unable to\ncommunicate with the coordinator. Participants send periodic heart-beat messages to\nthe coordinator and wait for an acknowledgment; if the acknowledgment is not received\nwithin a certain time, the coordinator is assumed to have failed.\nNote that the participant cannot de\ufb01nitively distinguish a situation where the co-\nordinator is dead from a situation where the network link between the node and the\ncoordinator is cut. Thus, the system should be able to work correctly even if the current\ncoordinator is alive, but another participant determines that the coordinator is dead.\nLock leases ensure that at most one node can be the coordinator at any time; once a\ncoordinator dies, another node can become the coordinator. However, lock leases work\nonly if a fault-tolerant lock manager is available.\nThis raises the question of how to implement such a lock manager. We return later,\nin Section 23.8.4, to the question of how to implement a fault-tolerant lock manager.\nBut it turns out that to do so e\ufb03ciently, we need to have a coordinator. And, lock leases\ncannot be used to choose the coordinator for the lock manager! The problem of how\nto choose a coordinator without depending on a lock manager is solved by election\nalgorithms , which enable the participating nodes to choose a new coordinator in a\ndecentralized manner.\n", "1176": "1148 Chapter 23 Parallel and Distributed Transaction Processing\nSuppose the goal is to elect a coordinator just once. Then, each node that wishes\nto become the coordinator proposes itself as a candidate to all the other nodes; such\nnodes are called proposers . The participating nodes then vote on which node among the\ncandidates is to be chosen. If a majority of the participating nodes (called acceptors )\nvote for a particular candidate, it is chosen. A subset of nodes called learners ask the\nacceptor nodes for their vote and determine if a majority have voted for a particular\ncandidate.\nThe problem with the above idea is that if there are multiple candidates, none of\nthem may get a majority of votes. The question is what to do in such a situation. There\nare at least two approaches that have been proposed:\n\u2022Nodes are given unique numbers; if more than one candidate proposes itself, ac-\nceptors choose the highest-numbered candidate. Even then votes may be split with\nno majority decision, due to delayed or missing messages; in such a case, the elec-\ntion is run again. But if a node N1that was a candidate \ufb01nds that a higher-numbered\nnode N2has proposed itself as a candidate, then N1withdraws from the next round\nof the election. The highest-numbered candidate will win the election. The bully\nalgorithm for election is based on this idea.\nThere are some subtle details due to the possibility that the highest-numbered\ncandidate in one round may fail during a subsequent round, leading to there being\nno candidates at all! If a proposer observes that no coordinator was selected in\na round where it withdrew itself as a candidate, it proposes itself as a candidate\nagain in the next round.\nNote also that the election has multiple rounds; each round has a number,\nand a candidate attaches a round number with the proposal. The round number is\nchosen to be the maximum round that it has seen, plus 1. A node can give a vote\nto only one candidate in a particular round, but it may change its vote in the next\nround.\n\u2022The second approach is based on randomized retry ,w h i c hw o r k sa sf o l l o w s :I ft h e r e\nis no majority decision in a particular round, all participants wait for a randomly\nchosen amount of time; if by that time a coordinator has been chosen by a majority\nof nodes, it is accepted as a coordinator. Otherwise, after the timeout, the node\nproposes itself as a candidate. As long as the timeouts are chosen properly (large\nenough compared to network latency) with high likelihood only one node proposes\nitself at a particular time and will get votes from a majority of nodes in a particular\nround.\nIf no candidate gets a majority vote in a round, the process is repeated. With\nvery high probability, after a few rounds, one of the candidates gets a majority and\nis thus chosen as coordinator.\nThe randomized-retry approach was popularized by the Raft consensus algo-\nrithm, and it is easier to reason about it and show not just correctness, but also\nbounds on the expected time for an election round to succeed in choosing a coor-\ndinator, as compared to the node- numbering-based approach.\n", "1177": "23.7 Coordinator Selection 1149\nNote that the above description assumed that choosing a coordinator is a one-time\nactivity. However, the chosen coordinator may fail, requiring a fresh election algorithm.\nThe notion of a term is used to deal with this situation. As mentioned above, each time\na node proposes itself as a coordinator, it associates the proposal with a round number,\nwhich is 1 more than the highest round number it has seen earlier, after ensuring that\nin the previous round no coordinator was chosen, or the chosen coordinator has subse-\nquently failed. The round number is henceforth referred to as a term. When the election\nsucceeds, the chosen coordinator is the coor dinator for the corresponding term. If the\nelection fails, the corresponding term does not have any coordinator chosen, but the\nelection should succeed in a subsequent term.\nNote also that there are subtle issues that arise since a node nmay be disconnected\nfrom the network for a while, and it may get reconnected without ever realizing that it\nwas disconnected. In the interim, the coordinator may have changed. In particular, if\nthe node nwas the coordinator, it may continue to think it is the coordinator, and some\nother node, say N1, which was also disconnected may think that nis still coordinator.\nHowever, if a coordinator was successfully elected, the majority of the nodes agree that\nsome other node, say N2, is the coordinator.\nIn general, it is possible for more than one node to think that is the coordinator at\nthe same time, although at most one of them can have the majority vote at that point\nin time.\nTo avoid this problem, each coordinator can be given a lease for a speci\ufb01ed period.\nThe coordinator can extend the lease by requesting an extension from other nodes and\ngetting con\ufb01rmation from a majority of the nodes. But if the coordinator is discon-\nnected from a majority of the nodes, it cannot renew its lease, and the lease expires.\nA node can vote for a new coordinator only if the last lease time that it con\ufb01rmed to\nthe earlier coordinator has expired. Since a new coordinator needs a majority vote, it\nc a n n o tg e tt h ev o t eu n t i lt h el e a s et i m eo ft h ep r e v i o u sc o o r d i n a t o rh a se x p i r e d .\nHowever, even if leases are used to ensure that two nodes cannot be coordinators\nat the same time, delayed messages can result in a node getting a message from an old\ncoordinator after a new one has been elected.\nTo deal with this problem, the current term of the sender is included with each\nmessage exchanged in the system. Note that when a node nis elected as coordinator,\nit has an associated term t; participant nodes that learn that nis the coordinator are\naware of the current term t. A node may receive a message with an old term either\nbecause an old coordinator did not realize it has been replaced or because of message\ndelivery delay; the latter problem can occur even if leases or other mechanisms ensure\nthat only one node can be the coordinator at a time. In either case, a node that receives\nastale message , that is, one with a term older than the current term of the node, it can\nignore the message. If a node receives a message with a higher number, it is behind\nthe rest of the system, and it needs to \ufb01nd out the current term and coordinator by\ncontacting other nodes.\nSome protocols do not require the coordinator to store any state information; in\nsuch cases, the new coordinator can take over without any further actions. However,\n", "1178": "1150 Chapter 23 Parallel and Distributed Transaction Processing\nother protocols require coordinators to retain state information. In such cases, the new\ncoordinator has to reconstruct the state information from persistent data and recovery\nlogs created by the previous coordinator. Such logs, in turn, need to be replicated to\nmultiple nodes so that the loss of a node does not result in the loss of access to the\nrecovery data. We shall see how to ensure availability by means of data replication in\nsubsequent sections.\n23.7.2.1 Distributed Coordination Services\nThere are a very large number of distributed applications that are in daily use today.\nInstead of each one having to implement its own mechanism for electing coordinators\n(among other tasks), it makes sense to develop a fault-tolerant coordination service\nthat can be used by multiple distributed applications.\nThe ZooKeeper service is one such very widely used fault-tolerant distributed co-\nordination service. The Chubby service developed earlier at Google is another such\nservice, which is widely used for applications developed by Google. These services\ninternally use consensus protocols to implement fault tolerance; we study consensus\nprotocols in Section 23.8.\nThese services provide a \ufb01le-system-like API, which supports the following features,\namong others:\n\u2022Store (small amounts of) data in \ufb01les, with a hierarchical namespace. A typical\nuse for such storage is to store con\ufb01guration information that can be used to start\nup a distributed application, or for new nodes to join a distributed application by\n\ufb01nding out which node is currently the coordinator.\n\u2022Create and delete \ufb01les , which can be used to implement locking. For example, to\nget a lock, a process can attempt to create a \ufb01le with a name corresponding to the\nlock. If another process has already created the \ufb01le, the coordination service will\nreturn an error, so the process knows it could not get the lock.\nFor example, a node that acts as a master for a tablet in a key-value store would\nget a lock on a \ufb01le whose name is the identi\ufb01er of the tablet. This ensures that two\nnodes cannot be masters for the tablet at the same time.\nIf an overall application master detects that a tablet master has died, it could re-\nlease the lock. If the service supports lock leases, this could happen automatically,\nif the tablet master does not renew its lease.\n\u2022Watch for changes on a \ufb01le , which can be used by a process to check if a lock has\nbeen released, or to be informed about other changes in the system that require\naction by the process.\n23.8 Consensus in Distributed Systems\nIn this section we \ufb01rst describe the consensus problem in a distributed system, that is,\nhow a set of nodes agree on a decision in a fault-tolerant way. Distributed consensus is\n", "1179": "23.8 Consensus in Distributed Systems 1151\na key building block for protocols that update replicated data in a fault-tolerant manner.\nWe outline two consensus protocols, Paxos and Raft. We then describe replicated state\nmachines, which can be used to make services, such as data storage systems and lock\nmanagers, fault tolerant. We end the section by describing how consensus can be used\nto make two-phase commit nonblocking.\n23.8.1 Problem Overview\nSoftware systems need to make decisions, such as the coordinator\u2019s decision on\nwhether to commit or abort a transaction when using 2PC, or a decision on which\nnode is to act as coordinator, in case a current coordinator fails.\nIf the decision is made by a single node, such as the commit/abort decision made\nby a coordinator node in 2PC, the system may block in case the node fails, since other\nnodes have no way of determining what decision was made. Thus, to ensure fault tol-\nerance, multiple nodes must participate in t he decision protocol; even if some of these\nnodes fail, the protocol must be able to reach a decision. A single node may make a\nproposal for a decision, but it must involve the other nodes to reach a decision in a\nfault-tolerant manner.\nThe most basic form of the distributed consensus problem is thus as follows: a set of\nnnodes (referred to as participants ) need to agree on a decision by executing a protocol\nsuch that:\n\u2022All participants must \u201clearn\u201d the same value for the decision even if some nodes\nfail during the execution of the protocol, or messages are lost, or there are network\npartitions.\n\u2022The protocol should not block, and must terminate, as long as some majority of\nthe nodes participating remain alive and can communicate with each other.\nAny real system cannot just make a single decision once, but needs to make a series\nof decisions. A good abstract of the process of making multiple consensus decisions\nis to treat each decision as adding a record to a log . Each node has a copy of the log,\nand records are appended to the log at each node. There can potentially be con\ufb02icts\non what record is added at what point in a log. The multiple consensus protocol viewed\nfrom this perspective needs to ensure that the log is uniquely de\ufb01ned.\nMost consensus protocols allow temporary divergence of logs across nodes while\nthe protocol is being executed; that is, the same log position at di\ufb00erent nodes may have\ndi\ufb00erent records, and the end of the log may be di\ufb00erent at di\ufb00erent nodes. Shared-\nlog consensus protocols keep track of an index into the log such that any entry before\nthat index has de\ufb01nitely been agreed upon. Any entries after that index may be in the\nprocess of being agreed upon, or may be entries from failed attempts at consensus.\nHowever, the protocols subsequently bring the inconsistent parts of the log logs back\nin synchronization. To do this, log records at some nodes may be deleted after being\ninserted; such log records are viewed as not yet committed and cannot be used to make\n", "1180": "1152 Chapter 23 Parallel and Distributed Transaction Processing\ndecisions. Only log records in the pre\ufb01x of the log that are in the committed pre\ufb01x may\nbe used to make decisions.\nSeveral protocols have been proposed for distributed consensus. Of these, the\nPaxos family of protocols is one of the most popular, and it has been implemented\nin many systems. While the basic Paxos protocol is intuitively easy to understand at a\nhigh level, there are a number of details in its implementation that are rather compli-\ncated, and particularly so in the multiple co nsensus version. To address this issue, the\nRaft consensus protocol was developed, with ease of understanding and implementa-\ntion being key goals, and it has been adopted by many systems. We outline the intuition\nbehind these protocols in this section.\nA key idea behind distributed consensus protocols is the idea of voting to make a\ndecision; a particular decision succeeds onl y if a majority of the participating nodes\nvote for it. Note that if two or more di\ufb00erent values are proposed for a particular deci-\nsion, at most one of them can be voted for by a majority; thus, it is not possible for two\ndi\ufb00erent values to be chosen. Even if some nodes fail, if a majority of the participants\nvote for a value, it gets chosen, thus making the voting fault tolerant as long as a ma-\njority of the participants are up and connected to each other. There is, however, a risk\nthat votes may get split between the proposed values, and some nodes may not vote if\nthey fail; as a result, no value may be decided on. In such a case the voting procedure\nhas to be executed again.\nWhile the above intuition is easy enough to understand, there are many details that\nmake the protocols nontrivial. We study some of these issues in the following sections.\nWe note that although we study some of the features of the Paxos and Raft con-\nsensus protocols, we omit a number of details that are needed for correct operation to\nkeep our description concise.\nWe also note that a number of other consensus protocols have been proposed, and\nsome of them are widely used, such as the Zab protocol which is part of the ZooKeeper\ndistributed coordination service.\n23.8.2 The Paxos Consensus Protocol\nThe basic Paxos protocol for making a single decision has the following participants.\n1.One or more nodes that can propose a value for the decision; such nodes are\ncalled proposers .\n2.One or more nodes that act as acceptors . An acceptor may get proposals with\ndi\ufb00erent values from di\ufb00erent proposers and must choose (vote for) only one of\nthe values.\nNote that failure of an acceptor does not cause a problem, as long as a majority\nof the acceptors are live and reachable. Failure or disconnection of a majority\nwould block the consensus protocol.\n", "1181": "23.8 Consensus in Distributed Systems 1153\n3.A set of nodes, called learners , query the acceptors to \ufb01nd what value each ac-\nceptor voted for in a particular round. (Acceptors could also send the value they\naccepted to the learners, without waiting for a query from the learner.)\nNote that the same node can play the roles of proposer, acceptor, and learner.\nIf a majority of the acceptors voted for a particular value, that value is the chosen\n(consensus) value for that decision. But there are two problems:\n1.It is possible for votes to be split among multiple proposals, and no proposal is\naccepted by a majority of the acceptors.\nI fa n yp r o p o s e dv a l u ei st og e tam a j o r i t y ,a tl e a s ts o m ea c c e p t o r sm u s tc h a n g e\ntheir decision. Thus, we must allow another round of decision making, where ac-\nceptors may choose a new value. This may need to be repeated as long as required\nuntil one value wins a majority vote.\n2.Even if a majority of nodes do accept a value, it is possible that some of these\nnodes die or get disconnected after accepting a value, but before any learner\n\ufb01nds out about their acceptance, and the remaining acceptors of that value do\nnot constitute a majority.\nIf this is treated as the failure of a round, and a di\ufb00erent value is chosen in a\nsubsequent round, we have a problem. In particular, a learner that learned about\nthe earlier majority would conclude that a particular value was chosen, while\nanother learner could conclude that a di\ufb00erent value was chosen, which is not\nacceptable.\nNote also that acceptors must log their decision so when they recover they know what\ndecision they made earlier.\nThe \ufb01rst problem above, namely, split votes, does not a\ufb00ect correctness, but it\na\ufb00ects performance. To avoid this problem, Paxos makes use of a coordinator node.\nProposers send a proposal to the coordinator, which picks one of the proposed values\nand follows the preceding steps to get a majority vote. If proposals come from only\none coordinator, there is no con\ufb02ict, and the lone proposed value gets a majority vote\n(modulo network and node failures).\nNote that if the coordinator dies or is unreachable, a new coordinator can be\nelected, using techniques we saw earlier in Section 23.7, and the new coordinator can\nthen do the same job as the earlier coordinator. Coordinators have no local state, so\nthe new one can take over without any recovery steps.\nThe second problem, namely, di\ufb00erent values getting majorities in di\ufb00erent rounds,\nis a serious problem and must be avoided by the consensus protocol. To do so, Paxos\nuses the following steps:\n1.Each proposal in Paxos has a number; di\ufb00erent proposals must have di\ufb00erent\nnumbers.\n", "1182": "1154 Chapter 23 Parallel and Distributed Transaction Processing\n2.In phase 1a of the protocol, a proposer sends a prepare message to acceptors,\nwith its proposal number n.\n3.In phase 1b of the protocol, an acceptor that receives a prepare message with\nnumber nchecks if it has already responded to a message with a number higher\nthan n. If so, it ignores the message. Otherwise, it remembers the number nand\nresponds with the highest proposal number m<nthat it has already accepted,\nalong with the corresponding value v; if it has not accepted any value earlier, it\nindicates so in its response. (Note that responding is di\ufb00erent from accepting.)\n4.In phase 2a, the proposer checks if it got a response from a majority of the ac-\nceptors. If it does, it chooses a value vas follows: If none of the acceptors has\nalready accepted any value, the proposer may use whatever value it intended to\npropose. If at least one of the acceptors responded that it accepted a value vwith\nsome number m, the proposer chooses the value vthat has the highest associated\nnumber m(note that mmust be <n).\nThe proposer now sends an accept request with the chosen value vand number\nn.\n5.In phase 2b, when an acceptor gets an accept request with value vand number\nn, it checks if it has responded to a prepare message with number n1>n;i f\nso it ignores the accept request. Otherwise, it accepts the proposed value vwith\nnumber n.\nThe above protocol is quite clever, since it ensures the following: if a majority of\nacceptors accepted a value v(with any number n), then even if there are further pro-\nposals with number n1>n, the value proposed will be value v.I n t u i t i v e l y ,t h er e a s o n\nis that a value can be accepted with number nonly if a majority of nodes respond to\na prepare message with number n; let us call this set of acceptors P. Suppose a value v\nhad been accepted earlier by a majority of nodes with number m; call this set of nodes\nA.T h e n Aand Pmust have a node in common, and the common node will respond\nwith value vand number m.\nNote that some other proposal with a number p>nmay have been made earlier,\nbut if it had been accepted by even one node, then a majority of nodes would have\nresponded to the proposal with number p, and thus will not respond to the proposal\nwith number n. Thus, if a proposal with value vis accepted by a majority of nodes, we\ncan be sure that any further proposal will be for the already chosen value v.\nNote that if a learner \ufb01nds that no proposal was accepted by a majority of nodes,\nit can ask any proposer to issue a fresh proposal. If a value vhad been accepted by a\nmajority of nodes, it would be found and accepted again, and the learner would now\nlearn about the value. If no value was accepted by a majority of nodes earlier, the new\nproposal could be accepted.\nThe above algorithm is for a single decision. Paxos has been extended to allow a\nseries of decisions; the extended algorithm is called Multi-Paxos. Real implementations\n", "1183": "23.8 Consensus in Distributed Systems 1155\nalso need to deal with other issues, such as how to add a node to the set of acceptors,\nor to remove a node from the set of acceptors if it is down for a long time, without\na\ufb00ecting the correctness of the protocol. References with more details about Paxos\nand Multi-Paxos may be found in the bibliographic notes for this chapter, available\nonline.\n23.8.3 The Raft Consensus Protocol\nThere are several consensus protocols whose goal is to maintain a log, to which records\ncan be appended in a fault-tolerant manner. Each node participating in such a protocol\nhas a replica of the log. Log-based protocols s implify the handling of multiple decisions.\nThe Raft consensus protocol is an example of such a protocol, and it was designed to\nbe (relatively) easy to understand.\nA key goal of log-based protocols is to keep the log replicas in sync by presenting a\nlogical view of appending records atomically to all copies of the log. In fact, atomically\nappending the same entry to all replicas is not possible, due to failures. Recall that\nfailure modes may include a node being temporarily disconnected and missing some\nupdates, without ever realizing it was disconnected. Further, a log append may be done\nat just a few nodes, and the append process may fail subsequently, leaving other replicas\nwithout the record. Thus, ensuring all copies of the log are identical at all times is\nimpossible. Such protocols must ensure the following:\n\u2022Even if a log replica is temporarily inconsistent with another, the protocol will\nbring it back in sync eventually by deleting and replacing log records on some\ncopies.\n\u2022A log entry will not be treated as committed until the algorithm guarantees that it\nwill never be deleted.\nProtocols such as Raft that are based on log replication can allow each node to\nrun a \u201cstate machine,\u201d with log entries used as commands to the state machine; state\nmachines are described in Section 23.8.4.\nThe Raft algorithm is based on having a coordinator, which is called a leader in Raft\nterminology. The other participating nodes are called followers . Since leaders may die\nand need to be replaced, time is divided into terms, which are identi\ufb01ed by integers.\nEach term has a unique leader, although some terms may not have any associated\nleader. Later terms have higher identi\ufb01ers than earlier terms.\nLeaders are elected in Raft using the randomized-retry algorithm outlined in Sec-\ntion 23.7.2. Recall that the randomized-retry algorithm already incorporates the notion\nof a term. A node that votes for a leader does so for a speci\ufb01c term. Nodes keep track\nof the currentTerm based on messages from leaders or requests for votes.\nNote that a leader N1may get temporarily disconnected, but get reconnected after\nother nodes \ufb01nd the leader cannot be reached, and elect a new leader N2.N o d e N1\n", "1184": "1156 Chapter 23 Parallel and Distributed Transaction Processing\nlog index 12 3 4 5 67\nleader    1\nx      2    1\nz      2    1\nx      3     2\nx      4     3\nx      1     3\ny      6    3\nz      4\nfollower 1    1\nx      2    1\nz      2    1\nx      3     2\nx      4     3\nx      1 \nfollower 2    1\nx      2    1\nz      2    1\nx      3     2\nx      4     3\nx      1     3\ny      6    3\nz      4\nfollower 3    1\nx      2    1\nz      2    1\nx      3 \nfollower 4    1\nx      2    1\nz      2    1\nx      3     2\nx      4     3\nx      1    3\ny      6\ncommitted entries< < << < <\n< < << < < < << < < < <\n< << < < < < < <\nFigure 23.9 Example of Raft logs.\ndoes not know that there is a new leader and may continue to execute the actions of a\nleader. The protocol should be robust to such situations.\nFigure 23.9 shows an example of Raft logs at a leader and four followers. Note that\nthe log index denotes the position of a particular record in a log. The number at the\ntop of each log record is the term in which the log record was created, while the part\nbelow it shows the log entry, assumed here to record assignments to di\ufb00erent variables.\nAny node that wishes to append a record to the replicated log sends a log append\nrequest to the current leader. The leader adds its term as a \ufb01eld of the log records and\nappends the record to its log; it then sends an AppendEntries remote procedure call to\nthe other nodes; the call contains several parameters, including these:\n\u2022term: the term of the current leader.\n\u2022previousLogEntryPosition : the position in the log of the preceding log entry.\n\u2022previousLogEntryTerm : the term associated with the preceding log entry.\n\u2022logEntries : an array of log records, allowing the call to append multiple log records\nat the same time.\n\u2022leaderCommitIndex : an index such that all log records at that index or earlier are\ncommitted. Recall that a log entry is not considered committed until a leader has\ncon\ufb01rmed that a majority of nodes have accepted that log entry. The leader keeps,\nin leaderCommitIndex, a position in the log such that all log records at that index\nand earlier are committed; this value is sent along with the AppendEntries call so\nthat the nodes learn which log records have been committed.\n", "1185": "23.8 Consensus in Distributed Systems 1157\nIf a majority of the nodes respond to the call with a return value true, the leader\ncan report successful log append (along with the position in the log) to the node that\ninitiated the log append. We will shortly see what happens if a majority do not respond\nwith true.\nEach follower that receives an AppendEntries message does the following:\n1.If the term in the message is less than currentTerm, then Return false.\n2.If the log does not contain an entry at a previous log entry position, whose term\nmatches the term in the message, then Return false.\n3.If there is an existing entry at the log position that is di\ufb00erent from the \ufb01rst log\nrecord in the AppendEntries message, the existing entry and all subsequent log\nentries are deleted.\n4.Any log records in the logEntries parameter that are not already in the log are\nappended to the log.\n5.The follower also keeps track of a local commitIndex to track which records\nare committed. If the leaderCommitIndex >commitIndex, set commitIndex =\nmin(leaderCommitIndex, index of last entry in log).\n6.Return true.\nNote that the last step keeps track of the last committed log record. It is possible that\nthe leader\u2019s log is ahead of the local log, so commitIndex cannot be blindly set to\nleaderCommitIndex, and it may need to be set to the local end of log if the leaderCom-\nmitIndex is ahead of the local end of log.\nFigure 23.9 shows that di\ufb00erent followers may have di\ufb00erent log states, since some\nAppendEntries messages may not have reached those nodes. The part of the log up to\nentry 6 is present at a majority of nodes (namely, the leader, follower 2 and follower 4).\nOn receipt of a true response to the AppendEntries call for the log record at position\n6 from these followers, the leader can set leaderCommitIndex to 6.\nIt is possible for a node N1to be a leader in some term, and on temporary discon-\nnection it may get replaced by a new leader N2in the next term. N1may not realize\nthat there is a new leader for some time and may send appendEntry messages to other\nnodes. However, a majority of the other nodes will know about the new leader, and\nwould have a term higher than that of N1. Thus, these nodes would return false, and\ninclude their current term in the response. Node N1would then realize that there is a\nleader with a new term; it then switches from the role of leader to that of follower.\nThe protocol must deal with the fact that some nodes may have outdated logs. Note\nthat in step 2 of the follower protocol, the follower returns false if its log is outdated. In\nsuch a case, the leader will retry an AppendEntries, sending it all log records from an\neven earlier point in the log. This may happen several times, until the leader sends log\n", "1186": "1158 Chapter 23 Parallel and Distributed Transaction Processing\nrecords from a point that is already in the follower log. At this point, the AppendEntries\ncommand would succeed.\nA key remaining problem is to ensure that if a leader dies, and another one takes\nover, the log is brought to a consistent state. Note that the leader may have appended\nsome log entries locally and replicated some of them to some other nodes, but the\nnew leader may or may not have all these records. To deal with this situation, the Raft\nprotocol includes steps to ensure the following:\n1.The protocol ensures that any node elected as leader has all the committed log\nentries. To do so, any candidate must contact a majority of the nodes and send\ninformation about its log state when seeking a vote. A node will vote for a can-\ndidate in the election only if it \ufb01nds that the candidate\u2019s log state is at least as\nup-to-date as its own; note that the de\ufb01nition of \u201cat least as up-to-date\u201d is a little\ncomplicated since it involves term identi\ufb01ers in log records, and we omit details.\nSince the above check is done by a majority of the nodes that voted for the new\nleader, any committed entry would certainly be present in the log of the newly\nelected leader.\n2.The protocol then forces all other nodes to replicate the leader\u2019s log.\nNote that the \ufb01rst step above does not actually \ufb01nd up to what log record is\ncommitted. Some of the log records at the new leader may not have been com-\nmitted earlier, but may get committed when the new leader\u2019s log is replicated in\nthis step.\nThere is also a subtle detail in that the new leader cannot count the number of\nreplicas with a particular record from an earlier term, and declare it committed if\nit is at a majority of the nodes. Intuitively, the problem is because of the de\ufb01nition\nof \u201cat least as up-to-date\u201d and the possibility that a leader may fail, recover, and\nbe elected as leader again. We omit details, but note that the way this problem\nis solved is for the new leader to replicate a new log record in its current term;\nwhen that log record is determined to be at a majority of the replicas, it and all\nearlier log records can be declared to be committed.\nIt should be clear that although the protocol, like Paxos, seems simple at a high\nlevel, there are many subtle details that need to be taken care of to ensure consistency\neven in the face of multiple failures and restarts. There are further details to be taken\ncare of, including how to change the cluster membership , that is, the set of nodes that\nform the system, while the system is running (doing so carelessly can result in incon-\nsistencies). Details of the above steps, including proofs of correctness, may be found\nin references in the bibliographic notes for this chapter, available online.\n23.8.4 Fault-Tolerant Services Using Replicated State Machines\nA key requirement in many systems is for a service to be made fault tolerant. A lock\nmanager is an example of such a service, as is a key-value storage system.\n", "1187": "23.8 Consensus in Distributed Systems 1159\nLeader declares log record committed after it is replicated at a majority of nodes. Update of state machine at each \nreplica happens only after log record has been committed.x     2 x      2 z     2   x     3  y     4   y     7Consensus \nModuleConsensus \nModuleConsensus \nModulex   3 \ny   7 \nz   3x   3 \ny   7 \nz   3x   3 \ny   7 \nz   3\nLog Log LogClient\ny     7\nleader follower follower< < < < < z     2 x     3  y     4 y     7 < < < < < x     2 z     2 x     3  y     4 y     7 < < < < <<\nFigure 23.10 Replicated state machine.\nA very powerful approach to making services fault tolerant is to model them as\n\u201cstate machines\u201d and then use the idea of replicated state machines that we describe\nnext.\nAstate machine receives inputs and has a stored state; it makes state transitions\non each input and may output some results along with the state transition. A replicated\nstate machine is a state machine that is replicated on multiple nodes to make it fault\ntolerant. Intuitively, even if one of the nodes fails, the state and output can be obtained\nfrom any of the nodes that are alive, provided all the state machines are in a consistent\nstate. The key to ensuring that the state machine replicas are consistent is to (a) require\nthe state machines to be deterministic, and (b) ensure that all replicas get exactly the\nsame input in the same order.\nTo ensure that all replicas get exactly the same input in the same order, we just\nappend the inputs to a replicated log, using, for example, techniques we saw earlier in\nSection 23.8.3. As soon as a log entry is determined to be committed, it can be given\nas input to the state machine, which can then process it.\nFigure 23.10 depicts a replicated state machine based on a replicated log. When a\nclient issues a command, such as y\u21907 in the \ufb01gure, the command is sent to the leader,\nwhere the command is appended to the log. The leader then replicates the command\nto the logs at the followers. Once a majority have con\ufb01rmed that the command has\nbeen replicated in their logs, the leader declares the command committed and applies\nthe command to its state machine. It also informs the followers of the commit, and the\nfollowers then apply the command to their state machine.\nIn the example in Figure 23.10, the state machine merely records the value of the\nupdated variable; but in general, the state machine may execute any other actions. The\nactions are, however, required to be deterministic, so all state machines are in exactly\n", "1188": "1160 Chapter 23 Parallel and Distributed Transaction Processing\nthe same state when they have executed the same set of commands; the order of exe-\ncution of commands will be the same since commands are executed in the log order.\nCommands such as lock request must return a status to the caller. The status can\nbe returned from any one of the replicas where the command is performed. Most im-\nplementations return the status from the leader node, since the request is sent to the\nleader, and the leader is also the \ufb01rst node to know when a log record has been com-\nmitted (replicated to a majority of the nodes).\nWe now consider two applications that can be made fault-tolerant using the repli-\ncated state machine concept.\nWe \ufb01rst consider how to implement a fault-tolerant lock manager .Al o c km a n a g e r\ngets commands, namely, lock requests and releases, and maintains a state (lock table).\nIt also gives output (lock grants or rollback requests on deadlock) on processing inputs\n(lock requests or releases). Lock manager s can easily be coded to be deterministic,\nthat is, given the same input, the state and output will be the same even if the code is\nexecuted again on a di\ufb00erent node.\nThus, we can take a centralized implementation of a lock manager and run it on\neach node. Lock requests and releases are appended to a replicated log using, for ex-\nample, the Raft protocol. Once a log entry is committed, the corresponding command\n(lock request or release) can be processed, in order, by the lock manager code at each\nreplica. Even if some of the replicas fail, the other replicas can continue processing as\nlong as a majority are up and connected.\nNow consider the issue of implementing a fault-tolerant key-value store .As i n g l e -\nnode storage system can be modeled as a state machine that supports put() andget()\noperations. The storage system is treated as a state machine, and the state machine is\nrun on multiple nodes.\nTheput() operations are appended to the log using a consensus protocol and are\nprocessed when the consensus protocol declares the corresponding log records to be\ncommitted (i.e., replicated to a majority of the nodes).\nIf the consensus protocol uses leaders, get() operations need not be logged, and\nneed to be executed only on the leader. To ensure that a get() operation sees the most\nrecent put() on the same data item, all put() operations on the same data item that\nprecede the get() o p e r a t i o ni nt h el o gm u s tb ec o m m i t t e db e f o r et h e get() operation\nis processed. (If a consensus protocol does not use a leader, get() operations can also\nbe logged and executed by at least one of the replicas which returns the value to the\ncaller.)\nGoogle\u2019s Spanner is an example of a system that uses the replicated state machine\napproach to creating a fault-tolerant implementation of a key-value storage system and\na lock manager.\nTo ensure scalability, Spanner breaks up data into partitions, each of which has a\nsubset of the data. Each partition has its data replicated across multiple nodes. Each\nnode runs two state machines: one for the key-value storage system, and one for the\nlock manager. The set of replicas for a particular partition are called a Paxos group ;o n e\n", "1189": "23.8 Consensus in Distributed Systems 1161\nof the nodes in a Paxos group acts as the Paxos group leader. Lock manager operations,\nas well as key-value store operations for a particular partition, are initiated at the Paxos\ngroup leader for that partition. The operations are appended to a log, which is replicated\nto the other nodes in the Paxos group using the Paxos consensus protocol.2Requests\nare applied in order at each member of the P axos group, once they are committed.\nAs an optimization, get() operations are not logged, and executed only at the leader\nas described earlier. As a further optimization, Spanner allows reads to run as of a\nparticular point in time, allowing reads to be executed at any replica of the partition (in\nother words, any other member of the Paxos group) that is su\ufb03ciently up to date, based\non the multiversion two-phase locking protocol described earlier in Section 23.5.1.\n23.8.5 Two-Phase Commit Using Consensus\nGiven a consensus protocol implementation, we can use it to create a non-blocking two-\nphase commit implementation. The idea is simple: instead of a coordinator recording\nits commit or abort decision locally, it uses a consensus protocol to record its decision\nin a replicated log. Even if the coordinator subsequently fails, other participants in the\nconsensus protocol know about the decision, so the blocking problem is avoided.\nIn case the coordinator fails before making a decision for a transaction, a new\ncoordinator can \ufb01rst check the log to see if a decision was made earlier, and if not\nit can make a commit/abort decision and use the consensus protocol to record the\ndecision.\nFor example, in the Spanner system developed by Google, a transaction may span\nmultiple partitions. Two-phase commit is initiated by a client and coordinated by the\nPaxos group leader at one of the partitions where the transaction executed. All other\npartitions where an update was performed acts as a participant in the two-phase commit\nprotocol. Prepare and commit messages are sent to the Paxos group leader node of each\nof the partitions; recall that two-phase commit participants as well as coordinators\nrecord decisions in their local logs. These decisions are recorded by each leader, using\nconsensus involving all the other nodes in its Paxos group.\nIf a Paxos group member other than the leader dies, the leader can continue pro-\ncessing the two-phase commit steps, as long as a majority of the group nodes are up and\nconnected. If a Paxos group leader fails, one of the other group members takes over as\nthe group leader. Note that all the state information required to continue commit pro-\ncessing is available to the new leader. Log records written during commit processing\nare available since the log is replicated. Also, recall from Section 23.8.4 that Spanner\nmakes the lock manager fault tolerant by using the replicated state machine concept.\nThus, a consistent replica of the lock table is also available with the new leader. Thus,\nthe two-phase commit steps of both the coordinator and the participants can continue\nto be executed even if some nodes fail.\n2The Multi-Paxos version of Paxos is used, but we shall just refer to it as Paxos for simplicity.\n", "1190": "1162 Chapter 23 Parallel and Distributed Transaction Processing\n23.9 Summary\n\u2022A distributed database system consists of a collection of sites or nodes, each of\nwhich maintains a local database system. Each node is able to process local trans-\nactions: those transactions that access data in only that single node. In addition, a\nnode may participate in the execution of global transactions: those transactions\nthat access data in several nodes. Transaction managers at each node manage\naccess to local data, while the transaction coordinator coordinates execution of\nglobal transactions across multiple nodes.\n\u2022A distributed system may su\ufb00er from the same types of failure that can a\ufb04ict a\ncentralized system. There are, however, additional failures with which we need to\ndeal in a distributed environment, including the failure of a node, the failure of a\nlink, loss of a message, and network partition. Each of these problems needs to be\nconsidered in the design of a distributed recovery scheme.\n\u2022To ensure atomicity, all the nodes in which a transaction Texecuted must agree\non the \ufb01nal outcome of the execution. Teither commits at all nodes or aborts at\nall nodes. To ensure this property, the transaction coordinator of Tmust execute a\ncommit protocol. The most widely used commit protocol is the two-phase commit\nprotocol.\n\u2022The two-phase commit protocol may lead to blocking, the situation in which the\nfate of a transaction cannot be determined until a failed node (the coordinator)\nrecovers. We can use distributed consensus protocols, or the three-phase commit\nprotocol, to reduce the risk of blocking.\n\u2022Persistent messaging provides an alternative model for handling distributed trans-\nactions. The model breaks a single transaction into parts that are executed at di\ufb00er-\nent databases. Persistent messages (which are guaranteed to be delivered exactly\no n c e ,r e g a r d l e s so ff a i l u r e s ) ,a r es e n tt or e m o t en o d e st or e q u e s ta c t i o n st ob e\ntaken there. While persistent messaging avoids the blocking problem, application\ndevelopers have to write code to handle various types of failures.\n\u2022The various concurrency-control schemes used in a centralized system can be mod-\ni\ufb01ed for use in a distributed environment. In the case of locking protocols, the only\nchange that needs to be incorporated is in the way that the lock manager is imple-\nmented. Centralized lock managers are vulnerable to overloading and to failures.\nDeadlock detection in a distributed-lock-manager environment requires coopera-\ntion between multiple nodes, since there may be global deadlocks even when there\nare no local deadlocks.\n\u2022The timestamp ordering and validation based protocols can also be extended to\nwork in a distributed setting. Timestamps used to order transactions need to be\nmade globally unique.\n", "1191": "23.9 Summary 1163\n\u2022Protocols for handling replicated data must ensure consistency of data. Lineariz-\nability is a key property that ensures that concurrent reads and writes to replicas\nof a single data item can be serialized.\n\u2022Protocols for handling replicated data include the primary copy, majority, biased,\nand quorum consensus protocols. These have di\ufb00erent trade-o\ufb00s in terms of cost\nand ability to work in the presence of failures.\n\u2022The majority protocol can be extended by using version numbers to permit trans-\naction processing to proceed even in the presence of failures. While the protocol\nhas a signi\ufb01cant overhead, it works regardless of the type of failure. Less-expensive\nprotocols are available to deal with node failures, but they assume network parti-\ntioning does not occur.\n\u2022To provide high availability, a distributed database must detect failures, recon\ufb01gure\nitself so that computation may continue, and recover when a processor or a link\nis repaired. The task is greatly complicated by the fact that it is hard to distinguish\nbetween network partitions and node failures.\n\u2022Globally consistent and unique timestamps are key to extending multiversion two-\nphase locking and snapshot isolation to a distributed setting.\n\u2022TheCAP theorem indicates that one cannot have consistency and availability in the\nface of network partitions. Many systems tradeo\ufb00 consistency to get higher avail-\nability. The goal then becomes eventual cons istency, rather than ensuring consis-\ntency at all times. Detecting inconsistency of replicas can be done by using version\nvector schemes and Merkle trees.\n\u2022Many database systems support asynchronous replication, where updates are prop-\nagated to replicas outside the scope of the transaction that performed the update.\nSuch facilities must be used with great care, since they may result in nonserializ-\nable executions.\n\u2022Some of the distributed algorithms require the use of a coordinator. To provide\nhigh availability, the system must maintain a backup copy that is ready to assume\nresponsibility if the coordinator fails. Another approach is to choose the new coor-\ndinator after the coordinator has failed. The algorithms that determine which node\nshould act as a coordinator are called election algorithms. Distributed coordina-\ntion services such as ZooKeeper support coor dinator selection in a fault-tolerant\nmanner.\n\u2022Distributed consensus algorithms allow consistent updates of replicas, even in the\npresence of failures, without requiring th e presence of a coordinator. Coordina-\ntors may still be used for e\ufb03ciency, but failure of a coordinator does not a\ufb00ect\ncorrectness of the protocols. Paxos and Raft are widely used consensus protocols.\nReplicated state machines, which are implemented using consensus algorithms,\ncan be used to build a variety of fault-tolerant services.\n", "1192": "1164 Chapter 23 Parallel and Distributed Transaction Processing\nReview Terms\n\u2022Distributed transactions\n\u00b0Local transactions\n\u00b0Global transactions\n\u2022Transaction manager\n\u2022Transaction coordinator\n\u2022System failure modes\n\u2022Network partition\n\u2022Commit protocols\n\u2022Two-phase commit protocol ( 2PC)\n\u00b0Ready state\n\u00b0In-doubt transactions\n\u00b0Blocking problem\n\u2022Distributed consensus\n\u2022Three-phase commit protocol\n(3PC)\n\u2022Persistent messaging\n\u2022Concurrency control\n\u2022Single lock manager\n\u2022Distributed lock manager\n\u2022Deadlock handling\n\u00b0Local wait-for graph\n\u00b0Global wait-for graph\n\u00b0False cycles\n\u2022Lock leases\n\u2022Timestamping\n\u2022Replicated data\n\u2022Linearizability\n\u2022Protocols for replicas\n\u00b0Primary copy\n\u00b0Majority protocol\u00b0Biased protocol\n\u00b0Quorum consensus protocol\n\u2022Robustness\n\u00b0Majority-based approach\n\u00b0Read one, write all\n\u00b0Read one, write all available\n\u00b0Node/Site reintegration\n\u2022External consistency\n\u2022Commit wait\n\u2022CAP theorem\n\u2022BASE properties\n\u2022Asynchronous replication\n\u2022Lazy propagation\n\u2022Master\u2013slave replication\n\u2022Multimaster (update-anywhere)\nreplication\n\u2022Asynchronous view maintenance\n\u2022Eventual consistency\n\u2022Version-vector scheme\n\u2022Merkle tree\n\u2022Coordinator selection\n\u2022Backup coordinator\n\u2022Election algorithms\n\u2022Bully algorithm\n\u2022Term\n\u2022Distributed consensus protocol\n\u2022Paxos\n\u00b0Proposers\n\u00b0Acceptors\n\u00b0Learners\n\u2022Raft\n", "1193": "Practice Exercises 1165\n\u2022Leaders\n\u2022Followers\n\u2022Replicated state machine\u2022Fault tolerant lock manager\n\u2022Non-blocking two-phase commit\nPractice Exercises\n23.1 What are the key di\ufb00erences between a local-area network and a wide-area\nnetwork, that a\ufb00ect the design of a distributed database?\n23.2 To build a highly available distributed system, you must know what kinds of\nfailures can occur.\na. List possible types of failure in a distributed system.\nb. Which items in your list from part a are also applicable to a centralized\nsystem?\n23.3 Consider a failure that occurs during 2PCfor a transaction. For each possible\nfailure that you listed in Exercise 23.2a, explain how 2PCensures transaction\natomicity despite the failure.\n23.4 Consider a distributed system with two sites, Aand B.C a ns i t e Adistinguish\namong the following?\n\u2022Bgoes down.\n\u2022The link between Aand Bgoes down.\n\u2022Bis extremely overloaded and response time is 100 times longer than nor-\nmal.\nWhat implications does your answer have for recovery in distributed systems?\n23.5 The persistent messaging scheme described in this chapter depends on time-\nstamps. A drawback is that they can discard received messages only if they are\ntoo old, and may need to keep track of a large number of received messages.\nSuggest an alternative scheme based on sequence numbers instead of time-\nstamps, that can discard messages more rapidly.\n23.6 Explain the di\ufb00erence between data replication in a distributed system and the\nmaintenance of a remote backup site.\n23.7 Give an example where lazy replication can lead to an inconsistent database\nstate even when updates get an exclusive lock on the primary (master) copy if\ndata were read from a node other than the master.\n23.8 Consider the following deadlock-detection algorithm. When transaction Ti,a t\nsite S1, requests a resource from Tj,a ts i t e S3, a request message with time-\n", "1194": "1166 Chapter 23 Parallel and Distributed Transaction Processing\nstamp nis sent. The edge ( Ti,Tj,n) is inserted in the local wait-for graph of\nS1.T h ee d g e( Ti,Tj,n) is inserted in the local wait-for graph of S3only if Tj\nhas received the request message and cannot immediately grant the requested\nresource. A request from TitoTjin the same site is handled in the usual man-\nner; no timestamps are associated with the edge ( Ti,Tj). A central coordinator\ninvokes the detection algorithm by sending an initiating message to each site\nin the system.\nOn receiving this message, a site sends its local wait-for graph to the co-\nordinator. Note that such a graph contains all the local information that the\nsite has about the state of the real graph. The wait-for graph re\ufb02ects an instan-\ntaneous state of the site, but it is not synchronized with respect to any other\nsite.\nWhen the controller has received a reply from each site, it constructs a\ngraph as follows:\n\u2022The graph contains a vertex for every transaction in the system.\n\u2022The graph has an edge ( Ti,Tj)i fa n do n l yi f :\n\u00b0There is an edge ( Ti,Tj)i no n eo ft h ew a i t - f o rg r a p h s .\n\u00b0An edge ( Ti,Tj,n)( f o rs o m e n) appears in more than one wait-for\ngraph.\nShow that, if there is a cycle in the constructed graph, then the system is in a\ndeadlock state, and that, if there is no cycle in the constructed graph, then the\nsystem was not in a deadlock state when the execution of the algorithm began.\n23.9 Consider the chain-replication protocol, described in Section 23.4.3.2, which\nis a variant of the primary-copy protocol.\na. If locking is used for concurrency control, what is the earliest point when\nap r o c e s sc a nr e l e a s ea ne x c l u s i v el o c ka f t e ru p d a t i n gad a t ai t e m ?\nb. While each data item could have its own chain, give two reasons it would\nbe preferable to have a chain de\ufb01ned at a higher level, such as for each\npartition or tablet.\nc. How can consensus protocols be used to ensure that the chain is\nuniquely determined at any point in time?\n23.10 If the primary copy scheme is used for replication, and the primary gets dis-\nconnected from the rest of the system, a new node may get elected as primary.\nBut the old primary may not realize it has got disconnected, and may get re-\nconnected subsequently without realizing that there is a new primary.\na. What problems can arise if the old primary does not realize that a new\none has taken over?\nb. How can leases be used to avoid these problems?\n", "1195": "Exercises 1167\nc. Would such a situation, where a participant node gets disconnected and\nthen reconnected without realizing it was disconnected, cause any prob-\nlem with the majority or quorum protocols?\n23.11 Consider a federated database system in which it is guaranteed that at most\none global transaction is active at any time, and every local site ensures local\nserializability.\na. Suggest ways in which the federated database system can ensure that\nthere is at most one active global transaction at any time.\nb. Show by example that it is possible for a nonserializable global schedule\nto result despite the assumptions.\n23.12 Consider a federated database system in which every local site ensures local\nserializability, and all global transactions are read only.\na. Show by example that nonserializable executions may result in such a\nsystem.\nb. Show how you could use a ticket scheme to ensure global serializability.\n23.13 S u p p o s ey o uh a v eal a r g er e l a t i o n r(A,B,C) and a materialized view\nv=A\u03b3sum( B)(r). View maintenance can be performed as part of each trans-\naction that updates r, on a parallel/distributed storage system that supports\ntransactions across multiple nodes. Suppose the system uses two-phase com-\nmit along with a consensus protocol such as Paxos, across geographically dis-\ntributed data centers.\na. Explain why it is not a good idea to perform view maintenance as part of\nthe update transaction, if some values of attribute Aare \u201chot\u201d at certain\npoints in time, that is, many updates pertain to those values of A.\nb. Explain how operation locking (if s upported) could solve this problem.\nc. Explain the tradeo\ufb00s of using asynchronous view maintenance in this\ncontext.\nExercises\n23.14 What characteristics of an application make it easy to scale the application\nby using a key-value store, and what characteristics rule out deployment on\nkey-value stores?\n23.15 Give an example where the read one, write all available approach leads to an\nerroneous state.\n", "1196": "1168 Chapter 23 Parallel and Distributed Transaction Processing\n23.16 In the majority protocol, what should the reader do if it \ufb01nds di\ufb00erent values\nfrom di\ufb00erent copies, to (a) decide what is the correct value, and (b) to bring\nthe copies back to consistency? If the reader does not bother to bring the\ncopies back to consistency, would it a\ufb00ect correctness of the protocol?\n23.17 If we apply a distributed version of the multiple-granularity protocol of Chap-\nter 18 to a distributed database, the site responsible for the root of the DAG\nmay become a bottleneck. Suppose we modify that protocol as follows:\n\u2022Only intention-mode locks are allowed on the root.\n\u2022All transactions are given the strongest intention-mode lock (IX) on the\nroot automatically.\nShow that these modi\ufb01cations alleviate this problem without allowing any non-\nserializable schedules.\n23.18 Discuss the advantages and disadvantages of the two methods that we pre-\nsented in Section 23.3.4 for generating globally unique timestamps.\n23.19 Spanner provides read-only transactions a snapshot view of data, using multi-\nversion two-phase locking.\na. In the centralized multi-version 2PL scheme, read-only transactions\nnever wait. But in Spanner, reads may have to wait. Explain why.\nb. Using an older timestamp for the snapshot can reduce waits, but has\nsome drawbacks. Explain why, and what the drawbacks are.\n23.20 M e r k l et r e e sc a nb em a d es h o r ta n df a t( l i k eB+-trees) or thin and tall (like\nbinary search trees). Which option would be better if you are comparing data\nacross two sites that are geographically separated, and why?\n23.21 Why is the notion of term important when an election is used to choose a co-\nordinator? What are the analogies between elections with terms and elections\nused in a democracy?\n23.22 For correct execution of a replicated state machine, the actions must be deter-\nministic. What could happen if an action is non-deterministic?\nFurther Reading\nTextbook coverage of distributed transacti on processing, including concurrency con-\ntrol and the two-phase and three-phase commit protocols, is provided by [Bernstein\nand Goodman (1981)] and [Bernstein and Ne wcomer (2009)]. Textbook discussions\nof distributed databases are o\ufb00ered by [Ozsu and Valduriez (2010)]. A collection of\npapers on data management on cloud systems is in [Ooi and Parthasarathy (2009)].\n", "1197": "Further Reading 1169\nThe implementation of the transaction concept in a distributed database is pre-\nsented by [Gray (1981)] and [Traiger et al. (1982)]. The 2PCprotocol was developed\nby [Lampson and Sturgis (1976)]. The three-phase commit protocol is from [Skeen\n(1981)]. Techniques for non-blocking two- phase commit based on consensus, called\nPaxos Commit, are described in [Gray and Lamport (2004)].\nChain replication was initially proposed by [van Renesse and Schneider (2004)]\nand an optimized version of was proposed by [Terrace and Freedman (2009)].\nDistributed optimistic concurrency control is described in [Agrawal et al. (1987)],\nwhile distributed snapshot isolation is described in [Binnig et al. (2014)] and [Schenkel\net al. (1999)]. The externally consistent distributed multi-version 2PL scheme used in\nSpanner is described in [Corbett et al. (2013)].\nThe CAP theorem was conjectured by [Brewer (2000)], and was formalized and\nproved by [Gilbert and Lynch (2002)]. [Cooper et al. (2008)] describe Yahoo!\u2019s\nPNUTS system, including its support for asynchronous maintenance of replicas us-\ning a publish-subscribe system. Parallel view maintenance is described in [Chen et al.\n(2004)] and [Zhang et al. (2004)], while asynchronous view maintenance is described\nin [Agrawal et al. (2009)]. Transaction processing in federated database systems is\ndiscussed in [Mehrotra et al. (2001)].\nPaxos is described in [Lamport (1998)]; Paxos is based on features from several\nearlier protocols, reference in [Lamport (1998)]. Google\u2019s Chubby lock service, which\nis based on Paxos, is described by [Burrows (2006)]. The widely used ZooKeeper sys-\ntem for distributed coordination is described in [Hunt et al. (2010)], and the consensus\nprotocol (also known as atomic broadcast protocol) used in ZooKeeper is described\nin [Junqueira et al. (2011)]. The Raft consensus protocol is described in [Ongaro and\nOusterhout (2014)].\nBibliography\n[Agrawal et al. (1987)] D. Agrawal, A. Bernstein, P. Gupta, and S. Sengupta, \u201cDistributed\noptimistic concurrency control with reduced rollback\u201d, Distributed Computing ,V o l u m e2 ,\nNumber 1 (1987), pages 45\u201359.\n[Agrawal et al. (2009)] P. Agrawal, A. Silberstein, B. F. Cooper, U. Srivastava, and R. Ra-\nmakrishnan, \u201cAsynchronous view maintenance for VLSD databases\u201d, In Proc. of the ACM\nSIGMOD Conf. on Management of Data (2009), pages 179\u2013192.\n[Bernstein and Goodman (1981)] P. A. Bernstein and N. Goodman, \u201cConcurrency Control\nin Distributed Database Systems\u201d, ACM Computing Surveys , Volume 13, Number 2 (1981),\npages 185\u2013221.\n[Bernstein and Newcomer (2009)] P. A. Bernstein and E. Newcomer, Principles of Transaction\nProcessing , 2nd edition, Morgan Kaufmann (2009).\n[Binnig et al. (2014)] C. Binnig, S. Hildenbrand, F. F\u00c3\u00a4rber, D. Kossmann, J. Lee, and\nN. May, \u201cDistributed snapshot isolation: global transactions pay globally, local transactions\n", "1198": "1170 Chapter 23 Parallel and Distributed Transaction Processing\npay locally\u201d, VLDB Journal , Volume 23, Number 6 (2014), pages 987\u20131011.\n[Brewer (2000)] E. A. Brewer, \u201cTowards Robust Distributed Systems (Abstract)\u201d, In Proc. of\nthe ACM Symposium on Principles of Distributed Computing (2000), page 7.\n[Burrows (2006)] M. Burrows, \u201cThe Chubby Lock Service for Loosely-Coupled Distributed\nSystems\u201d, In Symp. on Operating Systems Design and Implementation (OSDI) (2006), pages\n335\u2013350.\n[Chen et al. (2004)] S. Chen, B. Liu, and E. A. Rundensteiner, \u201cMultiversion-based view\nmaintenance over distributed data sources\u201d, ACM Transactions on Database Systems ,V o l u m e\n29, Number 4 (2004), pages 675\u2013709.\n[Cooper et al. (2008)] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bo-\nhannon, H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni, \u201cPNUTS: Yahoo!\u2019s Hosted\nData Serving Platform\u201d, Proceedings of the VLDB Endowment , Volume 1, Number 2 (2008),\npages 1277\u20131288.\n[Corbett et al. (2013)] J. C. Corbett et al., \u201cSpanner: Google\u2019s Globally Distributed\nDatabase\u201d, ACM Trans. on Computer Systems , Volume 31, Number 3 (2013).\n[Gilbert and Lynch (2002)] S. Gilbert and N. Lynch, \u201cBrewer\u2019s Conjecture and the Feasibil-\nity of Consistent, Available, Partition-Tolerant Web Services\u201d, SIGACT News , Volume 33,\nNumber 2 (2002), pages 51\u201359.\n[Gray (1981)] J. Gray, \u201cThe Transaction Concept: Virtues and Limitations\u201d, In Proc. of the\nInternational Conf. on Very Large Databases (1981), pages 144\u2013154.\n[Gray and Lamport (2004)] J. Gray and L. Lamport, \u201cConsensus on transaction commit\u201d,\nACM Transactions on Database Systems , Volume 31, Number 1 (2004), pages 133\u2013160.\n[Hunt et al. (2010)] P. Hunt, M. Konar, F. Junqueira, and B. Reed, \u201cZooKeeper: Wait-free\nCoordination for Internet-scale Systems\u201d, In USENIX Annual Technical Conference (USENIX\nATC) (2010), pages 11\u201311.\n[Junqueira et al. (2011)] F. P. Junqueira, B. C. Reed, and M. Sera\ufb01ni, \u201cZab: High-perfor-\nmance broadcast for primary-backup systems\u201d, In IEEE/IFIP 41st International Conference\no nD e p e n d a b l eS y s t e m sN e t w o r k s( D S N ) (2011), pages 245\u2013256.\n[Lamport (1998)] L. Lamport, \u201cThe Part-Time Parliament\u201d, ACM Trans. Comput. Syst. ,V o l -\nume 16, Number 2 (1998), pages 133\u2013169.\n[Lampson and Sturgis (1976)] B. Lampson and H. Sturgis, \u201cCrash Recovery in a Distributed\nData Storage System\u201d, Technical report, Comp uter Science Laboratory, Xerox Palo Alto\nResearch Center, Palo Alto (1976).\n[Mehrotra et al. (2001)] S .M e h r o t r a ,R .R a s t o g i ,Y .B r e i t b a r t ,H .F .K o r t h ,a n dA .S i l b e r -\nschatz, \u201cOvercoming Heterogeneity and Autonomy in Multidatabase Systems.\u201d, Inf. Com-\nput., Volume 167, Number 2 (2001), pages 137\u2013172.\n[Ongaro and Ousterhout (2014)] D. Ongaro and J. K. Ousterhout, \u201cIn Search of an Under-\nstandable Consensus Algorithm\u201d, In USENIX Annual Technical Conference (USENIX ATC)\n(2014), pages 305\u2013319.\n", "1199": "Further Reading 1171\n[Ooi and Parthasarathy (2009)] B. C. Ooi and S. Parthasarathy, \u201cSpecial Issue on Data Man-\nagement on Cloud Computing Platforms\u201d, IEEE Data Engineering Bulletin , Volume 32, Num-\nber 1 (2009).\n[Ozsu and Valduriez (2010)] T. Ozsu and P. Valduriez, Principles of Distributed Database Sys-\ntems, 3rd edition, Prentice Hall (2010).\n[Schenkel et al. (1999)] R. Schenkel, G. Weikum, N. Weisenberg, and X. Wu, \u201cFederated\nTransaction Management with Snapshot Isolation\u201d, In Eight International Workshop on Foun-\ndations of Models and Languages for Data and Objects, Transactions and Database Dynamics\n(1999), pages 1\u201325.\n[Skeen (1981)] D. Skeen, \u201cNon-blocking Commit Protocols\u201d, In Proc. of the ACM SIGMOD\nConf. on Management of Data (1981), pages 133\u2013142.\n[Terrace and Freedman (2009)] J. Terrace and M. J. Freedman, \u201cObject Storage on CRAQ:\nHigh-Throughput Chain Replication for Read-Mostly Workloads\u201d, In USENIX Annual Tech-\nnical Conference (USENIX ATC) (2009).\n[Traiger et al. (1982)] I. L. Traiger, J. N. Gray, C. A. Galtieri, and B. G. Lindsay, \u201cTransac-\ntions and Consistency in Distributed Database Management Systems\u201d, ACM Transactions on\nDatabase Systems , Volume 7, Number 3 (1982), pages 323\u2013342.\n[van Renesse and Schneider (2004)] R. van Renesse and F. B. Schneider, \u201cChain Replication\nfor Supporting High Throughput and Availability\u201d, In Symp. on Operating Systems Design and\nImplementation (OSDI) (2004), pages 91\u2013104.\n[Zhang et al. (2004)] X. Zhang, L. Ding, and E. A. Rundensteiner, \u201cParallel multisource\nview maintenance\u201d, VLDB Journal , Volume 13, Number 1 (2004), pages 22\u201348.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "1200": "", "1201": "PART9\nADVANCED TOPICS\nChapter 24 provides further details about the index structures we covered in Chapter\n14. In particular, this chapter provides detailed coverage of the LSM tree and its vari-\nants, bitmap indices, and spatial indexing, all of which were covered in brief in Chapter\n14. The chapter also provides detailed coverage of dynamic hashing techniques.\nChapter 25 discusses a number of tasks involved in application development. Ap-\nplications can be made to run signi\ufb01cantly faster by performance tuning, which con-\nsists of \ufb01nding and eliminating bottlenecks and adding appropriate hardware such as\nmemory or disks. Application performance is evaluated using benchmarks, which are\nstandardized sets of tasks that help to characterize the performance of database sys-\ntems. Another important aspect of application development is testing, which requires\nthe generation of database states and test inputs, followed by checking that the ac-\ntual outputs of a query or a program on the test input match the expected outputs.\nLastly, standards are very important for application development. A variety of stan-\ndards have been proposed that a\ufb00ect database-application development. We outline\nseveral of these standards in this chapter.\nChapter 26 covers blockchain technology from a database perspective. This chap-\nter identi\ufb01es the ways in which blockchain databases di\ufb00er from the traditional\ndatabases covered elsewhere in this text and shows how these distinguishing features\nare implemented. Although blockchain systems are often associated with Bitcoin, this\nchapter goes beyond Bitcoin-style algorithms and implementation to focus on alterna-\ntives that are more suited to an enterprise database environment.\n1173\n", "1202": "", "1203": "CHAPTER24\nAdvanced Indexing Techniques\nWe studied the concept of indexing, as well as a number of di\ufb00erent index structures in\nChapter 14. While some index structures, such as B+-trees, were covered in detail, oth-\ners such as hashing, write-optimized indices, bitmap indices, and spatial indices were\nonly brie\ufb02y outlined in Chapter 14. In this chapter we provide further details of these\nindex structures. We provide detailed coverage of the LSM tree and its variants. We\nthen provide a detailed description of bitmap indices. Next, we provide more detailed\ncoverage of spatial indexing, covering quad trees and R-trees in more detail. Finally, we\ncover hashing, with detailed coverage of dynamic hashing techniques.\n24.1 Bloom Filter\nABloom \ufb01lter is a probabilistic data structure that can check for membership of a\nvalue in a set using very little space, but at a small risk of overestimating the set of\nelements that are in the set. A Bloom \ufb01lter is basically a bitmap. If the set has nvalues,\nthe associated bitmap has a few times n(typically 10 n) bits; the Bloom \ufb01lter also has\nassociated with it several hash functions. We assume initially that there is only one\nhash function h().\nThe bits in the bitmap are all initially set to 0; subsequently, each value in the set\nis read, and the hash function h(v) is computed on the element v, with the range of the\nfunction being 1 to 10 n.T h eb i ta tp o s i t i o n h(v) is then set to 1. This is repeated for\nevery element v. To check if a particular value vis present in the set, the hash function\nh(v)i sc o m p u t e d .I fb i t h(v) in the Bloom \ufb01lter is equal to 0, we can infer that vcannot\npossibly be in the set. However, if bit h(v)i se q u a lt o1 , vmay be present in the set.\nNote that with some probability, the bit h(v)m a yb e1e v e ni f vis not present, if some\nother value v\u2032, present in the set has h(v\u2032)=h(v). Thus, a lookup for vresults in a false\npositive .\nTo reduce the chance of false positives, Bloom \ufb01lters use kindependent hash func-\ntions hi(),i=1..k,f o rs o m e k>1; for each value vin the set, bits corresponding to\nhi(v),i=1..kare all set to 1 in the bitmap. When querying the Bloom \ufb01lter with a\n1175\n", "1204": "1176 Chapter 24 Advanced Indexing Techniques\ngiven value vthe same khash functions are used to identify kbit locations; the value v\nis determined to be absent if even one of these bits has a 0 value. Otherwise the value\nis judged to be potentially present. For example, if the bitmap has 10 nbits, where nis\nthe number of values in the set, and k=7 hash functions are used, the false positive\nrate would be about 1%.\n24.2 Log-Structured Merge Tree and Variants\nAs we saw in Section 14.8, B+-tree indices are not e\ufb03cient for workloads with a very\nhigh number of writes, and alternative index structures have been proposed to han-\ndle such workloads. We saw a brief description of two such index structures, the log-\nstructured merge tree orLSM tree and its variants, in Section 14.8.1, and the bu\ufb00er tree ,\nin Section 14.8.2. In this section we provide further details of the LSM tree and its vari-\nants. To help with the discussions, we repeat some of the basic material we presented\nin Section 14.8.1.\nThe key idea of the log-structured merge tree (LSM tree)i st or e p l a c er a n d o m I/O\noperations during tree inserts, updates, and deletes with a smaller number of sequen-\ntialI/Ooperations. Our initial description focuses on index inserts and lookups; we\ndescribe how to handle updates and deletes later in the section.\nAnLSM tree consists of several B+-trees, starting with an in-memory tree, called\nL0, and on-disk trees L1,L2,\u2026,Lkfor some k,w h e r e kis called the level. Figure 24.1\ndepicts the structure of an LSM tree for k=3.\nAn index lookup is performed by using separate lookup operations on each of the\ntrees L0,\u2026,Lk, and merging the results of the lookups. (We assume here that there\nare no updates or deletes; we will discus s how to perform lookups in the presence of\nupdates/deletes later.)\nL0\nL1\nL2\nL3Memory\nDisk\nFigure 24.1 Log-structured merge tree with three levels.\n", "1205": "24.2 Log-Structured Merge Tree and Variants 1177\n24.2.1 Insertion into LSM Trees\nWhen a record is \ufb01rst inserted into an LSM tree, it is inserted into the in-memory B+-\ntree structure L0. A fairly large amount of memory space is allocated for this tree. As the\ntree grows to \ufb01ll the memory allocated to it, we need to move data from the in-memory\nstructure to a B+-tree on disk.\nIf tree L1is empty, the entire in-memory tree L0is written to disk to create the\ninitial tree L1.H o w e v e r ,i f L1is not empty, the leaf level of L0is scanned in increasing\nkey order, and entries are merged with the leaf level entries of L1(also scanned in\nincreasing key order). The merged entries are used to create a new B+-tree using the\nbottom-up build process. The new tree with the merged entries then replaces the old L1.\nIn either case, after entries of L0have been moved to L1, all entries in L0are deleted.\nInserts can then be made to the now empty L0.\nNote that all entries in the leaf level of the old L1tree, including those in leaf nodes\nthat do not have any updates, are copied to the new tree instead of being inserted into\nthe existing L1tree node. This gives the following bene\ufb01ts:\n\u2022The leaves of the new tree are sequentially located, avoiding random I/Oduring\nsubsequent merges.\n\u2022The leaves are full, avoiding the overhead of partially occupied leaves that can\noccur with page splits.\nThere is, however, a cost to using the LSM structure: the entire contents of the tree are\ncopied each time a set of entries from L0are copied into L1.\nIf the tree structure is implemented on top of a distributed \ufb01le system (Section\n21.6), copying data to a new tree is often unavoidable, since most distributed \ufb01le sys-\ntems do not support updates to an already created block.\nTo ensure we get a bene\ufb01t for cases where the index size on disk is much bigger\nthan the in-memory index, the maximum size of L1is chosen as ktimes the target size\nofL0,f o rs o m e k. Similarly, the maximum size of each Li+1is set to ktimes the target\nsize of Li.O n c eap a r t i c u l a r Lireaches its maximum size, its entries are merged into the\nnext component Li+1.W h e n Li+1reaches its target size, its entries are in turn merged\nintoLi+2,a n ds oo n .\nNote that if each leaf of Lihasmentries, m\u2215kentries would map to a single leaf\nnode of Li+1.T h ev a l u eo f kis chosen to ensure that m\u2215kis some reasonable number,\nsay 10. Let Md e n o t et h es i z eo f L0. Then, the size of a tree at level LiiskiM.T h e\ntotal number of levels ris thus roughly logk(I\u2215M)w h e r e Iis the total size of the index\nentries.\nLet us now consider the number of I/Ooperations required with a multiple-level\nLSM tree. At each Li,m\u2215kinserts are performed using only one I/Ooperation. On\nthe other hand, each entry gets inserted once at each level Li. Thus, the total number\nofI/Ooperations for each insert is ( k\u2215m)logk(I\u2215M). Thus, as long as the number of\n", "1206": "1178 Chapter 24 Advanced Indexing Techniques\nlevels r=logk(I\u2215M)i sl e s st h a n m\u2215k, the overall number of I/Ooperations per insert\nis reduced by using an LSM tree as compared to direct insertion into a B+-tree.\nIf, for example, r=10 and k=10, the in-memory index needs to be greater than\n1% of the total index size to get a bene\ufb01t in terms of the number of I/Ooperations\nrequired for inserts. (As before, the bene\ufb01t of reduced seeks is available even if the L0\nis signi\ufb01cantly smaller.)\nIf the number of levels is greater than m\u2215k, even if there is no bene\ufb01t in terms\nof number of I/Ooperations, there can still be savings since sequential I/Ois used\ninstead of random I/O. In Section 24.2.4 we describe a variant of the LSM tree which\nfurther reduces the overhead on write operations, at the cost of adding overhead on\nread operations.\nOne way to avoid creating large LSM trees with many levels is to range partition the\nrelation and create separate LSM trees on each partition. Such partitioning is natural\nin a parallel environment, as we saw earlier in Section 21.2. In particular, in such en-\nvironments, a partition can be dynamically repartitioned into smaller pieces whenever\nit becomes too large, as we saw in Section 21.3.3. With such repartitioning, the size of\neach LSM tree can be kept small enough to avoid having a large number of levels. There\nis a price for such partitioning: each partition requires its own L0tree in memory. As a\nresult, although it can be used in a centralized setting, the partitioning approach best\n\ufb01ts a parallel environment where resources such as processing nodes can be added as\nthe load increases.\n24.2.2 Rolling Merges\nWe assumed for simplicity that when a particular level is full, its entries are entirely\nmerged with the next level. This would result in more I/Oload during merges with\nan unused I/Ocapacity between merges. To avoid this problem, merging is done on a\ncontinuous basis; this is called rolling merge.\nWith rolling merge ,af e wp a g e so f Liare merged into corresponding pages of Li+1\nat a time, and removed from Li. This is done whenever Libecomes close to its target\nsize, and it results in Lishrinking a bit to return to its target size. When Ligrows again,\nthe rolling merge restarts from a point at the leaf level of Lijust after where the earlier\nrolling merge stopped, so the scan is sequential. When the end of the Litree is reached,\nthe scan starts again at the beginning of the tree. Such a merge is called a rolling merge\nsince records are moved from one level to another on a continuous basis.\nThe number of leaves merged at a time is kept high enough to ensure that the seek\ntime is small compared to the time to transfer data from and to disk.\n24.2.3 Handling Deletes and Updates\nSo far we have only described inserts and lookups. Deletes are handled in an interesting\nmanner. Instead of directly \ufb01nding an index entry and deleting it, deletion results in\ninsertion of a new deletion entry that indicates which index entry is to be deleted. The\n", "1207": "24.2 Log-Structured Merge Tree and Variants 1179\nL01\nL 11\nL 21L0k\nL 1k\nL 2kL0 Memory\nDisk\nFigure 24.2 stepped-merge index\nprocess of inserting a deletion entry is identical to the process of inserting a normal\nindex entry.\nHowever, lookups have to carry out an extra step. As mentioned earlier, lookups\nretrieve entries from all the trees and merge them in sorted order of key value. If there\nis a deletion entry for some entry, both of them would have the same key value. Thus,\na lookup would \ufb01nd both the deletion entry and the original entry for that key, which\nis to be deleted. If a deletion entry is found, the to-be-deleted entry should be \ufb01ltered\nout and not returned as part of the lookup result.\nWhen trees are merged, if one of the trees contains an entry, and the other had\na matching deletion entry, the entries get matched up during the merge (both would\nhave the same key), and are both discarded.\nUpdates are handled in a manner similar to deletes, by inserting an update entry.\nLookups need to match update entries with the original entries and return the latest\nvalue. The update is actually applied during a merge, when one tree has an entry and\nanother has its matching update entry; the update is applied during the merge, and the\nupdate entry is discarded.\n24.2.4 The Stepped-Merge Index\nWe now consider a variant of the LSM tree, which has multiple trees at each level\ninstead of one tree per level and performs inserts in a slightly di\ufb00erent manner. This\nstructure is shown in Figure 24.2. We call the structure a stepped-merge index , following\nthe terminology in an early paper that introduced it. In the developer community, the\nbasic LSM tree, the stepped-merge index, and several other variants are all referred to as\nLSM trees. We use the terms stepped-merge index andbasic LSM treeto clearly identify\nwhich variant we are referring to.\n", "1208": "1180 Chapter 24 Advanced Indexing Techniques\n24.2.4.1 Insertion Algorithm\nIn the stepped-merge index, incoming data are initially stored in memory, in an L0tree,\nin a manner similar to the LSM tree. However, when the tree reaches its maximum size,\ninstead of merging it into an L1tree, the in-memory L0tree is written to disk. When\nthe in-memory tree again reaches its maximum size, it is again written to disk. Thus,\nwe may have multiple L0trees on disk, which we shall refer to as L1\n0,L2\n0and so forth.\nEach of the Li\n0trees is a B+-tree and can be written to disk using only sequential I/O\noperations.\nIf this process is repeated, after a while we would have a large number of trees,\neach as large as memory, stored on disk. Lookups would then have to pay a high price,\nsince they would have to search through each of the tree structures, incurring separate\nI/Oc o s t so ne a c hs e a r c h .\nTo limit the overhead on lookups, once the number of on-disk trees at a level Li\nreaches some limit k, all the trees at a level are merged together into one combined\nnew tree structure at the next level Li+1. The leaves of the trees at level Liare read\nsequentially, and the keys merged in sorted order, and the level Li+1tree is constructed\nusing standard techniques for bottom-up construction of B+-trees. As before, the merge\noperation avoids random I/Ooperations, since it reads the individual tree structures\nsequentially and writes the resultant merged tree also sequentially.\nOnce a set of trees are merged into a single new tree, future queries can search the\nmerged tree; the original trees can then be deleted (after ensuring any ongoing searches\nhave completed).\nThe bene\ufb01t of the stepped-merge index scheme as compared to the basic LSM tree\nis that index entries are written out only once per level. With the basic LSM tree, each\ntime a tree at level Liis merged into a tree at level Li+1, the entire contents of the Li+1\ntree is read and written back to disk. Thus, on average, each record is read and written\nback k\u22152 times at each level of an LSM tree for a total of klogk(I\u2215M)I/Ooperations. In\ncontrast, with stepped-merge index, each record is written to disk once per layer, and\nread again when merging into the next layer, for a total of approximately 2 logk(I\u2215M)\nI/Ooperations. Thus, the stepped-merge index incurs signi\ufb01cantly less cost for updates.\nThe total number of bytes written (across all levels) on account of inserting an\nentry, divided by the size of entry, is called the write ampli\ufb01cation .T oc a l c u l a t et h e\nwrite ampli\ufb01cation of the LSM trees and the stepped-merge index, we can modify the\nabove formulae for I/Ooperations by ignoring the reads. For a B+-tree where each leaf\ngets on average only one update before it is written back, the write ampli\ufb01cation would\nbe the size of the page divided by the size of the index entry.\nFor a B+-tree, if a page has 100 entries, the write ampli\ufb01cation would be 100. With\nk=5, and I=100M,w ew o u l dh a v e log5(100)=3 levels. The write ampli\ufb01cation of\nanLSM tree would then be 5 \u22152\u00d73=7.5. The write ampli\ufb01cation of the stepped-merge\nindex would be 3. With k=10, the tree would have log10(100)=2 levels, leading to a\nwrite ampli\ufb01cation of 2 for stepped-merge index, and 10 for an LSM tree.\n", "1209": "24.2 Log-Structured Merge Tree and Variants 1181\nNote that like the basic LSM tree, the stepped-merge index also requires no ran-\ndom I/Ooperations during insertion, in contrast to a B+-tree insertion. Thus, the per-\nformance of B+-trees would be worse than what the write ampli\ufb01cation number above\nindicates.\nMerging can be optimized as follows: While merging ktrees at a particular level\nLi,i n t oal e v e l Li+1tree, trees at levels Lj,j<ic a na l s ob em e r g e di na tt h es a m et i m e .\nEntries in these trees can thus entirely skip one or more levels of the stepped-merge\nindex. Further, if the system has idle capacity, trees at a level Lican be merged even\nif there are fewer than ktrees at that level. In a situation where there is a long period\nof time with very few inserts, and the system load is light, trees across all levels could\npotentially get merged into a single tree at some level r.\n24.2.4.2 Lookup Operations Using Bloom Filters\nLookup operations in stepped-merge index have to separately search each of the trees.\nThus, compared to the basic LSM scheme, the stepped-merge index increases the bur-\nden on lookups, since in the worst case lookups need to access ktrees at each level,\nleading to a total of k\u2217logk(I\u2215M) tree lookups, instead of logk(I\u2215M) tree lookups in\nthe worst case with a basic LSM tree.\nFor workloads with a signi\ufb01cant fraction of reads, this overhead can be unaccept-\nable. For example, with the stepped-merge index with I=100Mandk=5, a single\nlookup requires 15 tree traversals, while the LSM tree would require 3 tree traversals.\nNote that for the common case where each key value occurs in only one tree, only one\nof the traversals would \ufb01nd a given search key, while all the other traversals would fail\nto \ufb01nd the key.\nTo reduce the cost of point lookups (i.e., lookups of a given key value), most sys-\ntems use a Bloom \ufb01lter to check if a tree can possibly contain the given key value. One\nBloom \ufb01lter is associated with each tree, and it is built on the set of key values in the\ntree. To check if a particular tree may contain a search key v,t h ek e y vis looked up in\nthe Bloom \ufb01lter. If the Bloom \ufb01lter indicates that the key value is absent, it is de\ufb01nitely\nnot present in the tree, and lookup can skip that tree. Otherwise, the key value may be\npresent in the tree, which must be looked up.\nA Bloom \ufb01lter with 10 nbits, where the tree has nelements, and using 7 hash\nfunctions would give a false positive rate of 1 percent. Thus, for a lookup on a key that\nis present in the index, on average just slightly more than one tree would be accessed.\nThus, lookup performance would be only slightly worse than on a regular B+-tree.\nThe Bloom \ufb01lter check thus works very well for point lookups, allowing a signi\ufb01cant\nfraction of the trees to be skipped, as long as su\ufb03cient memory is available to store all\nthe Bloom \ufb01lters in memory. With Ikey values in the index, approximately 10 Ibits of\nmemory will be required. To reduce the main memory overhead, some of the Bloom\n\ufb01lters may be stored on \ufb02ash storage.\nNote that for range lookups, the Bloom \ufb01lter optimization cannot be used, since\nthere is no unique hash value. Instead, all trees must be accessed separately.\n", "1210": "1182 Chapter 24 Advanced Indexing Techniques\n24.2.5 LSM Trees For Flash Storage\nLSM trees were initially designed to reduce the write and seek overheads of hard disks.\nFlash disks have a relatively low overhead for random I/Ooperations since they do\nnot require seek, and thus the bene\ufb01t of avoiding random I/Othat LSM tree variants\nprovide is not particularly important with \ufb02ash disks.\nHowever, recall that \ufb02ash memory does not allow in-place update, and writing even\nas i n g l eb y t et oap a g er e q u i r e st h ew h o l ep a g et ob er e w r i t t e nt oan e wp h y s i c a ll o c a t i o n ;\nthe original location of the page needs to be erased eventually, which is a relatively\nexpensive operation. The reduction in write ampli\ufb01cation using LSM tree variants, as\ncompared to traditional B+-trees, can provide substantial performance bene\ufb01ts when\nLSM trees are used with \ufb02ash storage.\n24.3 Bitmap Indices\nAs we saw in Section 14.9, a bitmap index is a specialized type of index designed for\neasy querying on multiple keys. Bitmaps work best for attributes that take only a small\nnumber of distinct values.\nFor bitmap indices to be used, records in a relation must be numbered sequentially,\ns t a r t i n gf r o m ,s a y ,0 .G i v e nan u m b e r n, it must be easy to retrieve the record numbered\nn. This is particularly easy to achieve if records are \ufb01xed in size and allocated on con-\nsecutive blocks of a \ufb01le. The record number can then be translated easily into a block\nnumber and a number that identi\ufb01es the record within the block.\nRecall that column-oriented storage , described in Section 13.6, stores attributes in\narrays, allowing e\ufb03cient access of the attribute of the ith record, for any given i.B i t m a p\nindices are thus particularly useful with columnar storage.\nWe use as a running example a relation instructor\n info, which has an attribute gen-\nder, which can take only values m(male) or f(female), and an attribute income\n level,\nwhere income has been broken up into 5 levels: L1: 0-9999, L2: 10, 000-19, 999, L3:\n20, 000-39, 999, L4: 40, 000-74, 999, and L5: 75, 000 \u2212\u221e.\n24.3.1 Bitmap Index Structure\nAs we saw in Section 14.9, a bitmap is simply an array of bits. In its simplest form, a\nbitmap index on the attribute Aof relation rconsists of one bitmap for each value that\nAcan take. Each bitmap has as many bits as the number of records in the relation. The\nith bit of the bitmap for value vjis set to 1 if the record numbered ihas the value vjfor\nattribute A. All other bits of the bitmap are set to 0.\nIn our example, there is one bitmap for the value mand one for f.T h e ith bit of\nthe bitmap for mis set to 1 if the gender value of the record numbered iism. All other\nbits of the bitmap for mare set to 0. Similarly, the bitmap for fhas the value 1 for bits\ncorresponding to records with the value ffor the gender attribute; all other bits have\n", "1211": "24.3 Bitmap Indices 1183\nID income_level gender\n76766\n22222\n12121\n15151\n58583mm\nff\nfL1L1\nL2\nL4\nL3record\nnumber\n10\n2\n3\n4m\nfBitmaps for gender\n10010\n01101Bitmaps for\nincome_level\nL1\nL2\nL3\nL4\nL510100\n01000\n00001\n00010\n00000\nFigure 24.3 Bitmap indices on relation instructor\n info.\nthe value 0. Figure 24.3 shows an example of bitmap indices on a relation instructor\ninfo\nWe now consider when bitmaps are useful. The simplest way of retrieving all\nrecords with value m(or value f) would be to simply read all records of the relation and\nselect those records with value m(orf, respectively). The bitmap index doesn\u2019t really\nhelp to speed up such a selection. While it would allow us to read only those records\nfor a speci\ufb01c gender, it is likely that every disk block for the \ufb01le would have to be read\nanyway.\nIn fact, bitmap indices are useful for selections mainly when there are selections\non multiple keys. Suppose we create a bitmap index on attribute income\n level,w h i c h\nwe described earlier, in addition to the bitmap index on gender .\nConsider now a query that selects women with income in the range $10,000 to\n$19, 999. This query can be expressed as\nselect *\nfrom instructor\n info\nwhere gender =' f 'andincome\n level =' L 2 ' ;\nTo evaluate this selection, we fetch the bitmaps for gender value fand the bitmap for\nincome\n level value L2a n dp e r f o r ma n intersection (logical-and) of the two bitmaps. In\nother words, we compute a new bitmap where bit ih a sv a l u e1i ft h e ith bit of the two\nbitmaps are both 1 and has a value 0 otherwise. In the example in Figure 24.3, the\nintersection of the bitmap for gender=\ud835\uddbf(01101) and the bitmap for income\n level=L2\n(01000) gives the bitmap 01000.\nSince the \ufb01rst attribute can take two values, and the second can take \ufb01ve values, we\nwould expect only about 1 in 10 records, on an average, to satisfy a combined condition\non the two attributes. If there are further conditions, the fraction of records satisfying\nall the conditions is likely to be quite small. The system can then compute the query\nresult by \ufb01nding all bits with value 1 in the intersection bitmap and retrieving the cor-\n", "1212": "1184 Chapter 24 Advanced Indexing Techniques\nresponding records. If the fraction is large, scanning the entire relation would remain\nthe cheaper alternative.\nAnother important use of bitmaps is to count the number of tuples satisfying a\ngiven selection. Such queries are important for data analysis. For instance, if we wish\nto \ufb01nd out how many women have an income level L2, we compute the intersection of\nthe two bitmaps and then count the number of bits that are 1 in the intersection bitmap.\nWe can thus get the desired result from the bitmap index, without even accessing the\nrelation.\nBitmap indices are generally quite small c ompared to the actual relation size. Rec-\nords are typically at least tens of bytes to hundreds of bytes long, whereas a single bit\nrepresents the record in a bitmap. Thus, the space occupied by a single bitmap is usually\nless than 1 percent of the space occupied by the relation. For instance, if the record\nsize for a given relation is 100 bytes, then the space occupied by a single bitmap will\nbe1\n8of 1 percent of the space occupied by the relation. If an attribute Aof the relation\ncan take on only one of eight values, a bitmap index on attribute Awould consist of\neight bitmaps, which together occupy only 1 percent of the size of the relation.\nDeletion of records creates gaps in the sequence of records, since shifting records\n(or record numbers) to \ufb01ll gaps would be extremely expensive. To recognize deleted\nrecords, we can store an existence bitmap ,i nw h i c hb i t iis 0 if record idoes not exist and\n1 otherwise. We shall see the need for existence bitmaps in Section 24.3.2. Insertion of\nrecords should not a\ufb00ect the sequence numbering of other records. Therefore, we can\ndo insertion either by appending records to the end of the \ufb01le or by replacing deleted\nrecords.\n24.3.2 Efficient Implementation of Bitmap Operations\nWe can compute the intersection of two bitmaps easily by using a forloop: the ith\niteration of the loop computes the andof the ith bits of the two bitmaps. We can speed\nup computation of the intersection greatly by using bit-wise andinstructions supported\nby most computer instruction sets. A word usually consists of 32 or 64 bits, depending\non the architecture of the computer. A bit-wise andinstruction takes two words as\ninput and outputs a word where each bit is the logical andof the bits in corresponding\npositions of the input words. What is important to note is that a single bit-wise and\ninstruction can compute the intersection of 32 or 64 bits at once .\nIf a relation had 1 million records, each bitmap would contain 1 million bits, or\nequivalently 128 kilobytes. Only 31,250 instructions are needed to compute the inter-\nsection of two bitmaps for our relation, assuming a 32-bit word length. Thus, computing\nbitmap intersections is an extremely fast operation.\nJust as bitmap intersection is useful for computing the andof two conditions,\nbitmap union is useful for computing the orof two conditions. The procedure for\nbitmap union is exactly the same as for intersection, except we use bit-wise orinstruc-\ntions instead of bit-wise andinstructions.\n", "1213": "24.3 Bitmap Indices 1185\nThe complement operation can be used to compute a predicate involving the nega-\ntion of a condition, such as not(income-level =L1). The complement of a bitmap is\ngenerated by complementing every bit of the bitmap (the complement of 1 is 0 and the\ncomplement of 0 is 1). It may appear that not(income\n level =L1) can be implemented\nby just computing the complement of the bitmap for income level L1. If some records\nhave been deleted, however, just computing the complement of a bitmap is not su\ufb03-\ncient. Bits corresponding to such records would be 0 in the original bitmap but would\nbecome 1 in the complement, although the records don\u2019t exist. A similar problem also\narises when the value of an attribute is null.F o ri n s t a n c e ,i ft h ev a l u eo f income\n level\nis null, the bit would be 0 in the original bitmap for value L1 and 1 in the complement\nbitmap.\nTo make sure that the bits corresponding to deleted records are set to 0 in the\nresult, the complement bitmap must be intersected with the existence bitmap to turn\no\ufb00 the bits for deleted records. Similarly, to handle null values, the complement bitmap\nmust also be intersected with the complement of the bitmap for the value null.1\nCounting the number of bits that are 1 in a bitmap can be done quickly by a clever\ntechnique. We can maintain an array with 256 entries, where the ith entry stores the\nnumber of bits that are 1 in the binary representation of i. Set the total count initially\nto 0. We take each byte of the bitmap, use it to index into this array, and add the\nstored count to the total count. The number of addition operations is1\n8of the number\nof tuples, and thus the counting process is very e\ufb03cient. A large array (using 216=\n65,536 entries), indexed by pairs of bytes, would give even higher speedup, but at a\nhigher storage cost.\n24.3.3 Bitmaps and B+-Trees\nBitmaps can be combined with regular B+-tree indices for relations where a few at-\ntribute values are extremely common, and other values also occur, but much less fre-\nquently. In a B+-tree index leaf, for each value we would normally maintain a list of all\nrecords with that value for the indexed attribute. Each element of the list would be a\nrecord identi\ufb01er, consisting of at least 32 bits, and usually more. For a value that occurs\nin many records, we store a bitmap instead of a list of records.\nSuppose a particular value vioccurs in1\n16of the records of a relation. Let Nbe\nthe number of records in the relation, and assume that a record has a 64-bit number\nidentifying it. The bitmap needs only 1 bit per record, or Nbits in total. In contrast, the\nlist representation requires 64 bits per record where the value occurs, or 64 \u2217N\u221516=\n4Nbits. Thus, a bitmap is preferable for representing the list of records for value vi.\nIn our example (with a 64-bit record identi\ufb01er), if fewer than 1 in 64 records have a\nparticular value, the list representation is preferable for identifying records with that\n1Handling predicates such as is unknown would cause further complications, which would in general require use of an\nextra bitmap to track which operation results are unknown.\n", "1214": "1186 Chapter 24 Advanced Indexing Techniques\nvalue, since it uses fewer bits than the bitmap representation. If more than 1 in 64\nrecords have that value, the bitmap representation is preferable.\nThus, bitmaps can be used as a compressed storage mechanism at the leaf nodes\nof B+-trees for those values that occur very frequently.\n24.4 Indexing of Spatial Data\nAs we saw in Section 14.10.1, indices are required for e\ufb03cient access to spatial data,\nand such indices must e\ufb03ciently support queries such as range and nearest neighbor\nqueries. We also gave a brief overview of k-d trees, quadtrees, and R-trees; we also brie\ufb02y\ndescribed how to answer range queries using k-d trees. In this section we provide further\ndetails of quadtrees and R-trees.\nAs mentioned in Section 14.10.1, in addition to indexing of points, spatial indices\nmust also support indexing of regions of space such as line segments, rectangles, and\nother polygons. There are extensions of k-d trees and quadtrees for this task. However,\na line segment or polygon may cross a partitioning line. If it does, it has to be split and\nrepresented in each of the subtrees in which its pieces occur. Multiple occurrences of a\nline segment or polygon caused by such splits can result in ine\ufb03ciencies in storage, as\nwell as ine\ufb03ciencies in querying. R-trees were developed to support e\ufb03cient indexing\nof such structures.\n24.4.1 Quadtrees\nAn alternative representation for two-dimensional data are a quadtree .A ne x a m p l eo f\nthe division of space by a quadtree appears in Figure 24.4. Each node of a quadtree is\nFigure 24.4 Division of space by a quadtree.\n", "1215": "24.4 Indexing of Spatial Data 1187\nassociated with a rectangular region of space. The top node is associated with the entire\ntarget space. Each nonleaf node in a quadtree divides its region into four equal-sized\nquadrants, and correspondingly each such node has four child nodes corresponding to\nthe four quadrants. Leaf nodes have between zero and some \ufb01xed maximum number\nof points. Correspondingly, if the region corresponding to a node has more than the\nmaximum number of points, child nodes are created for that node. In the example in\nFigure 24.4, the maximum number of points in a leaf node is set to 1.\nThis type of quadtree is called a PRquadtree , to indicate it stores points, and that\nthe division of space is divided based on regions, rather than on the actual set of points\nstored. We can use region quadtrees to store array (raster) information. A node in a\nregion quadtree is a leaf node if all the array values in the region that it covers are the\nsame. Otherwise, it is subdivided further into four children of equal area and is therefore\nan internal node. Each node in the region quadtree corresponds to a subarray of values.\nThe subarrays corresponding to leaves either contain just a single array element or have\nmultiple array elements, all of which have the same value.\n24.4.2 R-Trees\nA storage structure called an R-tree is useful for indexing of objects such as points,\nline segments, rectangles, and other polygons. An R-tree is a balanced tree structure\nwith the indexed objects stored in leaf nodes, much like a B+-tree. However, instead of\na range of values, a rectangular bounding box is associated with each tree node. The\nbounding box of a leaf node is the smallest rectangle parallel to the axes that contains\nall objects stored in the leaf node. The bounding box of internal nodes is, similarly,\nthe smallest rectangle parallel to the axes that contains the bounding boxes of its child\nnodes. The bounding box of an object (such as a polygon) is de\ufb01ned, similarly, as the\nsmallest rectangle parallel to the axes that contains the object.\nEach internal node stores the bounding boxes of the child nodes along with the\npointers to the child nodes. Each leaf node stores the indexed objects and may option-\nally store the bounding boxes of the objects; the bounding boxes help speed up checks\nfor overlaps of the rectangle with the indexed objects\u2014if a query rectangle does not\noverlap with the bounding box of an object, it cannot overlap with the object, either.\n(If the indexed objects are rectangles, there is no need to store bounding boxes, since\nthey are identical to the rectangles.)\nFigure 24.5 shows an example of a set of rectangles (drawn with a solid line) and\nthe bounding boxes (drawn with a dashed line) of the nodes of an R-tree for the set of\nrectangles. Note that the bounding boxes are shown with extra space inside them, to\nmake them stand out pictorially. In reality, the boxes would be smaller and \ufb01t tightly\non the objects that they contain; that is, each side of a bounding box Bwould touch at\nleast one of the objects or bounding boxes that are contained in B.\nT h eR - t r e ei t s e l fi sa tt h er i g h ts i d eo fF i g u r e2 4 . 5 .T h e\ufb01 g u r er e f e r st ot h ec o o r d i -\nnates of bounding box iasBBiin the \ufb01gure.\n", "1216": "1188 Chapter 24 Advanced Indexing Techniques\nBB1 BB2 BB\nBCAE F H IA B\nC\nI\nEFH1\n23\nDG\nDG3\nFigure 24.5 An R-tree.\nWe shall now see how to implement search, insert, and delete operations on an\nR-tree.\n\u2022Search . As the \ufb01gure shows, the bounding boxes associated with sibling nodes\nmay overlap; in B+-trees, k-d trees, and quadtrees, in contrast, the ranges do not\noverlap. A search for objects containing a point therefore has to follow allchild\nnodes whose associated bounding boxes contain the point; as a result, multiple\npaths may have to be searched. Similarly, a query to \ufb01nd all objects that intersect a\ngiven object has to go down every node where the associated rectangle intersects\nthe given object.\n\u2022Insert . When we insert an object into an R-tree, we select a leaf node to hold the\nobject. Ideally we should pick a leaf node that has space to hold a new entry, and\nwhose bounding box contains the bounding box of the object. However, such a\nnode may not exist; even if it did, \ufb01nding the node may be very expensive, since it\nis not possible to \ufb01nd it by a single traversal down from the root. At each internal\nnode we may \ufb01nd multiple children whose bounding boxes contain the bounding\nbox of the object, and each of these children needs to be explored. Therefore, as\na heuristic, in a traversal from the root, if any of the child nodes has a bounding\nbox containing the bounding box of the object, the R-tree algorithm chooses one\nof them arbitrarily. If none of the children satisfy this condition, the algorithm\nchooses a child node whose bounding box has the maximum overlap with the\nbounding box of the object for continuing the traversal.\nOnce the leaf node has been reached, if the node is already full, the algorithm\nperforms node splitting (and propagates splitting upward if required) in a manner\nvery similar to B+-tree insertion. Just as with B+-tree insertion, the R-tree insertion\nalgorithm ensures that the tree remains ba lanced. Additionally, it ensures that the\n", "1217": "24.4 Indexing of Spatial Data 1189\nbounding boxes of leaf nodes, as well as internal nodes, remain consistent; that\nis, bounding boxes of leaves contain all the bounding boxes of the objects stored\nat the leaf, while the bounding boxes for internal nodes contain all the bounding\nboxes of the children nodes.\nThe main di\ufb00erence of the insertion procedure from the B+-tree insertion\nprocedure lies in how the node is split. In a B+-tree, it is possible to \ufb01nd a value\nsuch that half the entries are less than the midpoint and half are greater than the\nvalue. This property does not generalize beyond one dimension; that is, for more\nthan one dimension, it is not always possible to split the entries into two sets so\nthat their bounding boxes do not overlap. Instead, as a heuristic, the set of entries\nScan be split into two disjoint sets S1andS2so that the bounding boxes of S1and\nS2have the minimum total area; another heuristic would be to split the entries into\ntwo sets S1andS2in such a way that S1andS2have minimum overlap. The two\nnodes resulting from the split would contain the entries in S1andS2, respectively.\nThe cost of \ufb01nding splits with minimum total area or overlap can itself be large,\nso cheaper heuristics, such as the quadratic split heuristic, are used. (The heuristic\ngets its name from the fact that it takes time quadratic in the number of entries.)\nThequadratic split heuristic works this way: First, it picks a pair of entries a\nandbfrom Ssuch that putting them in the same node would result in a bounding\nbox with the maximum wasted space; that is, the area of the minimum bounding\nbox of aandbminus the sum of the areas of aandbis the largest. The heuristic\nplaces the entries aandbin sets S1andS2, respectively.\nIt then iteratively adds the remaining entries, one entry per iteration, to one of\nthe two sets S1orS2. At each iteration, for each remaining entry e,l e tie,1denote\nthe increase in the size of the bounding box of S1ifeis added to S1and let ie,2\ndenote the corresponding increase for S2. In each iteration, the heuristic chooses\none of the entries with the maximum di\ufb00erence of ie,1andie,2and adds it to S1if\nie,1is less than ie,2,a n dt o S2otherwise. That is, an entry with \u201cmaximum prefer-\nence\u201d for S1orS2is chosen at each iteration. The iteration stops when all entries\nhave been assigned, or when one of the sets S1orS2has enough entries that all\nremaining entries have to be added to the other set so the nodes constructed from\nS1andS2both have the required minimum occupancy. The heuristic then adds all\nunassigned entries to the set with fewer entries.\n\u2022Deletion . Deletion can be performed like a B+-tree deletion, borrowing entries from\nsibling nodes, or merging sibling nodes if a node becomes underfull. An alternative\napproach redistributes all the entries of underfull nodes to sibling nodes, with the\naim of improving the clustering of entries in the R-tree.\nSee the bibliographical references for more details on insertion and deletion operations\non R-trees, as well as on variants of R-trees, called R\u2217-trees or R+-trees.\nThe storage e\ufb03ciency of R-trees is better than that of k-d trees or quadtrees, since\nan object is stored only once, and we can ensure easily that each node is at least half\n", "1218": "1190 Chapter 24 Advanced Indexing Techniques\nfull. However, querying may be slower, since multiple paths have to be searched. Spatial\njoins are simpler with quadtrees than with R-trees, since all quadtrees on a region are\npartitioned in the same manner. However, because of their better storage e\ufb03ciency and\ntheir similarity to B-trees, R-trees and their variants have proved popular in database\nsystems that support spatial data.\n24.5 Hash Indices\nWe described the concepts of hashing and hash indices in Section 14.5. We provide\nfurther details in this section.\n24.5.1 Static Hashing\nAs in Section 14.5, let Kdenote the set of all search-key values, and let Bdenote the\nset of all bucket addresses. A hash function his a function from KtoB.L e t hdenote a\nhash function. Recall that in a hash index, buckets contain index entries, with pointers\nto records, while in a hash \ufb01le organization, actual records are stored in the buckets. All\nthe other details remain the same, so we do not explicitly di\ufb00erentiate between these\ntwo versions henceforth. We use the term hash index to denote hash \ufb01le organizations\nas well as secondary hash indices.\nFigure 24.6 shows a secondary hash index on the instructor \ufb01le, for the search key\nID. The hash function in the \ufb01gure computes the sum of the digits of the IDmodulo\n8. The hash index has eight buckets, each of size 2 (realistic indices would have much\nlarger bucket sizes). One of the buckets has three keys mapped to it, so it has an over\ufb02ow\nbucket. In this example, IDis a primary key for instructor , so each search key has only\none associated pointer. In general, multip le pointers can be associated with each key.\nHash indices can e\ufb03ciently answer point queries , which retrieve records with a\nspeci\ufb01ed value for a search key. However, they cannot e\ufb03ciently answer range queries ,\nwhich retrieve all records whose search key value lies in a range ( lb,ub). The di\ufb03culty\narises because a good hash function assigns values randomly to buckets. Thus, there is\nno simple notion of \u201cnext bucket in sorted order.\u201d The reason we cannot chain buckets\ntogether in sorted order on Aiis that each bucket is assigned many search-key values.\nSince values are scattered randomly by the hash function, the values in the speci\ufb01ed\nrange are likely to be scattered across many or all of the buckets. Therefore, we have to\nread all the buckets to \ufb01nd the required search keys.\nRecall that deletion is done as follows: If the search-key value of the record to be\ndeleted is Ki,w ec o m p u t e h(Ki), then search the corresponding bucket for that record,\nand delete the record from the bucket. Deletion of a record is e\ufb03cient if there are not\nmany records with a given key value. However, in the case of a hash index on a key\nw i t hm a n yd u p l i c a t e s ,al a r g en u m b e ro fe n t r i e sw i t ht h es a m ek e yv a l u em a yh a v et o\nbe scanned to \ufb01nd the entry for the record that is to be deleted. The complexity can in\nthe worst case be linear in the number of records.\n", "1219": "24.5 Hash Indices 1191\nbucket 0\nbucket 1\nbucket 2\nbucket 3\nbucket 4\nbucket 5\nbucket 676766\n45565\n76543\n10101\n15151\n3345658583\n8382122222\n98345\nbucket 7\n12121\n3234376766 Crick\n76543 Singh\n32343 El Said\n58583 Cali\ufb01eri\n15151 Mozart\n22222 Einstein\n33465 Gold10101 Srinivasan\n45565 Katz\n83821 Brandt\n98345 Kim\n12121 WuBiology\nPhysicsFinance\nHistory\nHistory\nMusic\nPhysicsComp. Sci.\nComp. Sci.\nComp. Sci.\nElec. Eng.\nFinance72000\n80000\n60000\n62000\n40000\n95000\n8700065000\n75000\n92000\n80000\n90000\nFigure 24.6 Hash index on search key IDofinstructor file.\nRecall also that with static hashing, the set of buckets is \ufb01xed at the time the index\nis created. If the relation grows far beyond the expected size, hash indices would be\nquite ine\ufb03cient due to long over\ufb02ow chains. We could rebuild the hash index using a\nlarger number of buckets. Such rebuilding can be triggered when the number of records\nexceeds the estimated number by some margin, and the index is rebuilt with a number\nof buckets that is a multiple of the original number of buckets (say by a factor of 1.5 to\n2). Such rebuilding is in fact done in many systems with in-memory hash indices.\nHowever, doing so can cause signi\ufb01cant disruption to normal processing with large\nrelations, since a large number of records have to be reindexed; the disruption is par-\nticularly marked with disk-resident data. In this section we discuss dynamic hashing\ntechniques that allow hash indices to grow gradually, without causing disruption.\n", "1220": "1192 Chapter 24 Advanced Indexing Techniques\n24.5.1.1 Hash Functions\nThe worst possible hash function maps all search-key values to the same bucket. Such\na function is undesirable because all the records have to be kept in the same bucket. A\nlookup has to examine every such record to \ufb01nd the one desired. An ideal hash function\ndistributes the stored keys uniformly across all the buckets, so that every bucket has the\nsame number of records.\nSince we do not know at design time precisely which search-key values will be\nstored in the \ufb01le, we want to choose a hash function that assigns search-key values to\nbuckets in such a way that the distribution has these qualities:\n\u2022The distribution is uniform . That is, the hash function assigns each bucket the same\nnumber of search-key values from the set of allpossible search-key values.\n\u2022The distribution is random . That is, in the average case, each bucket will have nearly\nthe same number of values assigned to it, regardless of the actual distribution of\nsearch-key values. More precisely, the hash value will not be correlated to any\nexternally visible ordering on the search-key values, such as alphabetic ordering\nor ordering by the length of the search keys; the hash function will appear to be\nrandom.\nAs an illustration of these principles, let us choose a hash function for the instructor\n\ufb01le using the search key dept\nname . The hash function that we choose must have the\ndesirable properties not only on the example instructor \ufb01le that we have been using, but\nalso on an instructor \ufb01le of realistic size for a large university with many departments.\nAssume that we decide to have 26 buckets, and we de\ufb01ne a hash function that\nmaps names beginning with the ith letter of the alphabet to the ith bucket. This hash\nfunction has the virtue of simplicity, but it fails to provide a uniform distribution, since\nwe expect more names to begin with such letters as B and R than Q and X, for example.\nNow suppose that we want a hash function on the search key salary . Suppose that\nthe minimum salary is $30,000 and the maximum salary is $130,000, and we use a hash\nfunction that divides the values int o 10 ranges, $30,000\u2013$40,000, $40,001\u2013$50,000,\nand so on. The distribution of search-key values is uniform (since each bucket has\nt h es a m en u m b e ro fd i \ufb00 e r e n t salary values) but is not random. Records with salaries\nbetween $60,001 and $70,000 are far more common than are records with salaries\nbetween $30,001 and $40,000. As a result, the distribution of records is not uniform\n\u2014some buckets receive more records than others do. If the function has a random\ndistribution, even if there are such correlations in the search keys, the randomness\nof the distribution will make it very likely that all buckets will have roughly the same\nnumber of records, as long as each search key occurs in only a small fraction of the\nrecords. (If a single search key occurs in a large fraction of the records, the bucket\ncontaining it is likely to have more records than other buckets, regardless of the hash\nfunction used.)\n", "1221": "24.5 Hash Indices 1193\nbucket 1\nbucket 2\nbucket 3bucket 4\nbucket 5\nbucket 6\nbucket 74556515151 Mozart Music 4000080000Wu 12121 Finance 90000\n76543 Finance Singh\n10101 Comp. Sci. Srinivasan\nKatz Comp. Sci. 75000\n9200065000 32343\n58583El Said\nCali\ufb01eriHistory\nHistory80000\n60000\nEinstein\nGold\nKim22222\n33456\n98345Physics\nPhysics\nElec. Eng.95000\n87000\n80000Brandt 83821 Comp. Sci.76766 Crick Biology 72000bucket 0\nFigure 24.7 Hash organization of instructor file, with dept\nname as the key.\nTypical hash functions perform computation on the internal binary machine rep-\nresentation of characters in the search key. A simple hash function of this type \ufb01rst\ncomputes the sum of the binary representations of the characters of a key, then returns\nthe sum modulo the number of buckets.\nFigure 24.7 shows the application of such a scheme, with eight buckets, to the\ninstructor \ufb01le, under the assumption that the ith letter in the alphabet is represented by\nthe integer i.\nThe following hash function is a better alternative for hashing strings. Let sbe a\nstring of length n,a n dl e t s[i]d e n o t et h e ith byte of the string. The hash function is\nde\ufb01ned as:\ns[0]\u221731(n\u22121)+s[1]\u221731(n\u22122)+\u22ef+s[n\u22121]\nThe function can be implemented e\ufb03ciently by setting the hash result initially to 0 and\niterating from the \ufb01rst to the last character of the string, at each step multiplying the\nhash value by 31 and then adding the next character (treated as an integer). The above\nexpression would appear to result in a very large number, but it is actually computed\nwith \ufb01xed-size positive integers; the result of each multiplication and addition is thus\nautomatically computed modulo the largest possible integer value plus 1. The result of\nthe above function modulo the number of buckets can then be used for indexing.\n", "1222": "1194 Chapter 24 Advanced Indexing Techniques\nHash functions require careful design. A bad hash function may result in lookup\ntaking time proportional to the number of search keys in the \ufb01le. A well-designed func-\ntion gives an average-case lookup time that is a (small) constant, independent of the\nnumber of search keys in the \ufb01le.\n24.5.1.2 Handling of Bucket Over\ufb02ows\nSo far, we have assumed that, when a record is inserted, the bucket to which it is mapped\nhas space to store the record. If the bucket does not have enough space, a bucket over-\n\ufb02owis said to occur. Bucket over\ufb02ow can occur for several reasons, as we outlined in\nSection 14.5.\n\u2022Insu\ufb03cient buckets . The number of buckets, which we denote nB,m u s tb ec h o s e n\nsuch that nB>nr\u2215fr,w h e r e nrdenotes the total number of records that will be\nstored and frdenotes the number of records that will \ufb01t in a bucket. This designa-\ntion assumes that the total number of records is known when the hash function is\nchosen.\n\u2022Skew . Some buckets are assigned more records than are others, so a bucket may\nover\ufb02ow even when other buckets still have space. This situation is called bucket\nskew. Skew can occur for two reasons:\n1.Multiple records may have the same search key.\n2.The chosen hash function may result in nonuniform distribution of search\nkeys.\nSo that the probability of bucket over\ufb02ow is reduced, the number of buckets is\nchosen to be ( nr\u2215fr)\u2217(1+d), where dis a fudge factor, typically around 0 .2. Some\nspace is wasted: About 20 percent of the space in the buckets will be empty. But the\nbene\ufb01t is that the probability of over\ufb02ow is reduced.\nDespite allocation of a few more buckets than required, bucket over\ufb02ow can still\noccur. As we saw in Section 14.5, we handle bucket over\ufb02ow by using over\ufb02ow buckets.\nWe must also change the lookup algorithm slightly to handle over\ufb02ow chaining, to look\nat the over\ufb02ow buckets in addition to the main bucket.\nThe form of hash structure that we have just described is called closed address-\ning(or, less commonly, closed hashing ). Under an alternative approach called open\naddressing (or, less commonly, open hashing ), the set of buckets is \ufb01xed, and there\nare no over\ufb02ow chains. Instead, if a bucket is full, the system inserts records in some\nother bucket in the initial set of buckets B. One policy is to use the next bucket (in\ncyclic order) that has space; this policy is called linear probing . Other policies, such as\ncomputing further hash functions, are also used. Open addressing has been used to con-\nstruct symbol tables for compilers and assemblers, but closed addressing is preferable\nfor database systems. The reason is that deletion under open addressing is troublesome.\nUsually, compilers and assemblers perform only lookup and insertion operations on\n", "1223": "24.5 Hash Indices 1195\ntheir symbol tables. However, in a database system, it is important to be able to handle\ndeletion as well as insertion. Thus, open addressing is of only minor importance in\ndatabase implementation.\nAn important drawback to the form of hashing that we have described is that\nwe must choose the hash function when we implement the system, and it cannot be\nchanged easily thereafter if the \ufb01le being indexed grows or shrinks. Since the function h\nmaps search-key values to a \ufb01xed set Bof bucket addresses, we waste space if Bis made\nlarge to handle future growth of the \ufb01le. If Bis too small, the buckets contain records\nof many di\ufb00erent search-key values, and bucket over\ufb02ows can occur. As the \ufb01le grows,\nperformance su\ufb00ers. We study in Section 24.5.2 how the number of buckets and the\nhash function can be changed dynamically.\n24.5.2 Dynamic Hashing\nAs we have seen, the need to \ufb01x the set Bof bucket addresses presents a serious problem\nwith the static hashing technique of the pre vious section. Most databases grow larger\no v e rt i m e .I fw ea r et ou s es t a t i ch a s h i n gf o rs u c had a t a b a s e ,w eh a v et h r e ec l a s s e so f\noptions:\n1.Choose a hash function based on the current \ufb01le size. This option will result in\nperformance degradation as the database grows.\n2.Choose a hash function based on the anticipated size of the \ufb01le at some point in\nthe future. Although performance degradation is avoided, a signi\ufb01cant amount\nof space may be wasted initially.\n3.Periodically reorganize the hash structure in response to \ufb01le growth. Such a reor-\nganization involves choosing a new hash function, recomputing the hash function\non every record in the \ufb01le, and generating new bucket assignments. This reorga-\nnization is a massive, time-consuming operation. Furthermore, it is necessary to\nforbid access to the \ufb01le during reorganization.\nSeveral dynamic hashing techniques allow the hash function to be modi\ufb01ed dy-\nnamically to accommodate the growth or shrin kage of the database. In this section we\ndescribe one form of dynamic hashing, called extendable hashing . The bibliographical\nnotes provide references to other forms of dynamic hashing.\n24.5.2.1 Data Structure\nExtendable hashing copes with changes in database size by splitting and coalescing\nbuckets as the database grows and shrinks. As a result, space e\ufb03ciency is retained.\nMoreover, since the reorganization is performed on only one bucket at a time, the\nresulting performance overhead is acceptably low.\n", "1224": "1196 Chapter 24 Advanced Indexing Techniques\nii1\ni2\ni3bucket 1\nbucket 2\nbucket 300..\n01..\n10..\n11..\nbucket address tablehash pre\ufb01x\u2026\u2026\nFigure 24.8 General extendable hash structure.\nWith extendable hashing, we choose a hash function hwith the desirable properties\nof uniformity and randomness. However, this hash function generates values over a\nrelatively large range\u2014namely, b-bit binary integers. A typical value for bis 32.\nWe do not create a bucket for each hash value. Indeed, 232is over 4 billion, and\nthat many buckets is unreasonable for all but the largest databases. Instead, we create\nbuckets on demand, as records are inserted into the \ufb01le. We do not use the entire bbits\nof the hash value initially. At any point, we use ibits, where 0 \u2264i\u2264b.T h e s e ibits\nare used as an o\ufb00set into an additional table of bucket addresses. The value of igrows\nand shrinks with the size of the database.\nFigure 24.8 shows a general extendable hash structure. The iappearing above the\nbucket address table in the \ufb01gure indicates that ibits of the hash value h(K)a r er e q u i r e d\nto determine the correct bucket for K. This number will change as the \ufb01le grows. Al-\nthough ibits are required to \ufb01nd the correct entry in the bucket address table, several\nconsecutive table entries may point to the same bucket. All such entries will have a com-\nmon hash pre\ufb01x, but the length of this pre\ufb01x may be less than i. Therefore, we associate\nwith each bucket an integer giving the length of the common hash pre\ufb01x. In Figure 24.8\nthe integer associated with bucket jis shown as ij. The number of bucket-address-table\nentries that point to bucket jis\n2(i\u2212ij)\n", "1225": "24.5 Hash Indices 1197\n24.5.2.2 Queries and Updates\nWe now see how to perform lookup, insertion, and deletion on an extendable hash\nstructure.\nTo locate the bucket containing search-key value Kl, the system takes the \ufb01rst i\nhigh-order bits of h(Kl), looks at the corresponding table entry for this bit string, and\nfollows the bucket pointer in the table entry.\nTo insert a record with search-key value Kl,t h es y s t e mf o l l o w st h es a m ep r o c e d u r e\nfor lookup as before, ending up in some bucket\u2014say, j. If there is room in the bucket,\nthe system inserts the record in the bucket. If, on the other hand, the bucket is full, it\nmust split the bucket and redistribute the current records, plus the new one. To split\nthe bucket, the system must \ufb01rst determine from the hash value whether it needs to\nincrease the number of bits that it uses.\n\u2022Ifi=ij, only one entry in the bucket address table points to bucket j. Therefore,\nthe system needs to increase the size of the bucket address table so that it can\ninclude pointers to the two buckets that result from splitting bucket j.I td o e ss o\nby considering an additional bit of the hash value. It increments the value of iby\n1, thus doubling the size of the bucket address table. It replaces each entry with\ntwo entries, both of which contain the same pointer as the original entry. Now two\nentries in the bucket address table point to bucket j. The system allocates a new\nbucket (bucket z) and sets the second entry to point to the new bucket. It sets ij\nandiztoi. Next, it rehashes each record in bucket jand, depending on the \ufb01rst i\nbits (remember the system has added 1 to i), either keeps it in bucket jor allocates\nit to the newly created bucket.\nThe system now reattempts the insertion of the new record. Usually, the attempt\nwill succeed. However, if all the records in bucket j, as well as the new record, have\nthe same hash-value pre\ufb01x, it will be necessary to split a bucket again, since all the\nrecords in bucket jand the new record are assigned to the same bucket. If the hash\nfunction has been chosen carefully, it is unlikely that a single insertion will require\nthat a bucket be split more than once, unless there are a large number of records\nwith the same search key. If all the records in bucket jhave the same search-key\nvalue, no amount of splitting will help. In such cases, over\ufb02ow buckets are used to\nstore the records, as in static hashing.\n\u2022Ifi>ij, then more than one entry in the bucket address table points to bucket j.\nThus, the system can split bucket jwithout increasing the size of the bucket address\ntable. Observe that all the entries that point to bucket jcorrespond to hash pre\ufb01xes\nthat have the same value on the leftmost ijbits. The system allocates a new bucket\n(bucket z), and sets ijandizto the value that results from adding 1 to the original\nijvalue. Next, the system needs to adjust the entries in the bucket address table\nthat previously pointed to bucket j. (Note that with the new value for ij,n o ta l l\nthe entries correspond to hash pre\ufb01xes that have the same value on the leftmost ij\nbits.) The system leaves the \ufb01rst half of the entries as they were (pointing to bucket\n", "1226": "1198 Chapter 24 Advanced Indexing Techniques\ndept_name h(dept_name )\nBiology\nComp. Sci.\nElec. Eng.\nFinance\nHistory\nMusic\nPhysics0010 1101 1111 1011 0010 1100 0011 0000\n1111 0001 0010 0100 1001 0011 0110 1101\n0100 0011 1010 1100 1100 0110 1101 1111\n1010 0011 1010 0000 1100 0110 1001 1111\n1100 0111 1110 1101 1011 1111 0011 1010\n0011 0101 1010 0110 1100 1001 1110 1011\n1001 1000 0011 1111 1001 1100 0000 0001\nFigure 24.9 Hash function for dept\nname .\nj), and sets all the remaining entries to point to the newly created bucket (bucket\nz). Next, as in the previous case, the system rehashes each record in bucket j,a n d\nallocates it either to bucket jor to the newly created bucket z.\nThe system then reattempts the insert. In the unlikely case that it again fails, it\napplies one of the two cases, i=ijori>ij, as appropriate.\nNote that, in both cases, the system needs to recompute the hash function on only the\nrecords in bucket j.\nTo delete a record with search-key value Kl, the system follows the same procedure\nfor lookup as before, ending up in some bucket\u2014say, j. It removes both the search key\nfrom the bucket and the record from the \ufb01le. The bucket, too, is removed if it becomes\nempty. Note that, at this point, several buckets can be coalesced, and the size of the\nbucket address table can be cut in half. The procedure for deciding on which buckets\ncan be coalesced and how to coalesce buckets is left to you to do as an exercise. The\nconditions under which the bucket address table can be reduced in size are also left to\nyou as an exercise. Unlike coalescing of buckets, changing the size of the bucket address\ntable is a rather expensive operation if the table is large. Therefore it may be worthwhile\nto reduce the bucket-address-table size only if the number of buckets reduces greatly.\nTo illustrate the operation of insertion, we use the instructor \ufb01le and assume that\nthe search key is dept\nname with the 32-bit hash values as appear in Figure 24.9. Assume\nthat, initially, the \ufb01le is empty, as in Figure 24.10. We insert the records one by one. To\n00\nbucket 1bucket address tablehash pre\ufb01x\nFigure 24.10 Initial extendable hash structure.\n", "1227": "24.5 Hash Indices 1199\n11\nbucket address tablehash pre\ufb01x\n115151 Music 40000\n10101\n12121Srinivasan 65000\nWu 90000Mozart\nComp. Sci.\nFinance\nFigure 24.11 Hash structure after three insertions.\nillustrate all the features of extendable hashing in a small structure, we shall make the\nunrealistic assumption that a bucket can hold only two records.\nWe insert the record (10101, Srinivasan, Comp. Sci., 65000). The bucket address\ntable contains a pointer to the one bucket, and the system inserts the record. Next, we\ninsert the record (12121, Wu, Finance, 90000). The system also places this record in\nthe one bucket of our structure.\nWhen we attempt to insert the next record (15151, Mozart, Music, 40000), we \ufb01nd\nthat the bucket is full. Since i=i0, we need to increase the number of bits that we\nuse from the hash value. We now use 1 bit, allowing us 21=2 buckets. This increase\nin the number of bits necessitates doubling the size of the bucket address table to two\nentries. The system splits the bucket, placing in the new bucket those records whose\nsearch key has a hash value beginning with 1, and leaving in the original bucket the\nother records. Figure 24.11 shows the state of our structure after the split.\n2\n21\n2bucket address tablehash pre\ufb01x\n15151 Music 40000 Mozart\n12121 Finance 90000 Wu\n10101 Comp. Sci. 65000 Srinivasan22222 Einstein Physics 95000\nFigure 24.12 Hash structure after four insertions.\n", "1228": "1200 Chapter 24 Advanced Indexing Techniques\n3\n31\n3\nbucket address tablehash pre\ufb01x\n222222\n33456Physics 95000\nPhysics 87000Music 15151 40000 Mozart\nEinstein\nGold\n12121 Wu 90000 Finance\n10101\n32343Srinivasan\nEl SaidComp. Sci.\nHistory 6000065000\nFigure 24.13 Hash structure after six insertions.\n3\n31\n3\nbucket address tablehash pre\ufb01x\n322222\n33456Physics 95000\nPhysics 87000Music 15151 40000 Mozart\nEinstein\nGold\n12121 Wu 90000 Finance\n10101\n45565Srinivasan\nKatzComp. Sci.\nComp. Sci. 750006500032343 El Said History 600003\nFigure 24.14 Hash structure after seven insertions.\n", "1229": "24.5 Hash Indices 1201\nNext, we insert (22222, Einstein, Physics, 95000). Since the \ufb01rst bit of h(Physics)\nis 1, we must insert this record into the bucket pointed to by the \u201c1\u201d entry in the bucket\naddress table. Once again, we \ufb01nd the bucket full and i=i1. We increase the number\nof bits that we use from the hash to 2. This increase in the number of bits necessitates\ndoubling the size of the bucket address table to four entries, as in Figure 24.12. Since\nthe bucket of Figure 24.11 for hash pre\ufb01x 0 was not split, the two entries of the bucket\naddress table of 00 and 01 both point to this bucket.\nFor each record in the bucket of Figure 24.11 for hash pre\ufb01x 1 (the bucket being\nsplit), the system examines the \ufb01rst two bits of the hash value to determine which\nbucket of the new structure should hold it.\nNext, we insert (32343, El Said, History, 60000), which goes in the same bucket\nas Comp. Sci. The following insertion of (33456, Gold, Physics, 87000) results in a\nbucket over\ufb02ow, leading to an increase in the number of bits and a doubling of the size\nof the bucket address table (see Figure 24.13).\nThe insertion of (45565, Katz, Comp. Sci., 75000) leads to another bucket over-\n\ufb02ow; this over\ufb02ow, however, can be handled without increasing the number of bits,\nsince the bucket in question has two pointers pointing to it (see Figure 24.14).\nNext, we insert the records of \u201cCali\ufb01eri\u201d, \u201cSingh\u201d, and \u201cCrick\u201d without any bucket\nover\ufb02ow. The insertion of the third Comp. Sci. record (83821, Brandt, Comp. Sci.,\n3\nbucket address tablehash pre\ufb01x 31\n3\n322222\n33456Physics 95000\nPhysics 87000Music\nBiology15151 40000\n72000Mozart\nEinstein\nGold\n12121 Wu 90000 Finance\n10101\n45565Srinivasan\nKatzComp. Sci.\nComp. Sci. 7500065000Crick 76766\nSingh 76543 Finance\n92000 Comp. Sci. Brandt 8382132343\n58583El Said\nCali\ufb01eriHistory\nHistory60000\n6200080000\n3\nFigure 24.15 Hash structure after 11 insertions.\n", "1230": "1202 Chapter 24 Advanced Indexing Techniques\n32\n3\n3\nbucket address tablehash pre\ufb01x\n322222\n33456Physics 95000\nPhysics 87000Music 15151 40000 Mozart\nEinstein\nGold\n12121 Wu 90000 Finance\n10101\n45565Srinivasan\nKatzComp. Sci.\nComp. Sci. 7500065000Crick Biology 72000 76766\nSingh 76543 Finance80000 Elec. Eng. Kim 98345\n92000 Comp. Sci. Brandt 8382132343\n58583El Said\nCali\ufb01eriHistory\nHistory60000\n620002\n80000\n3\nFigure 24.16 Extendable hash structure for the instructor file.\n92000), however, leads to another over\ufb02ow. This over\ufb02ow cannot be handled by in-\ncreasing the number of bits, since there are three records with exactly the same hash\nvalue. Hence the system uses an over\ufb02ow bucket, as in Figure 24.15. We continue in this\nmanner until we have inserted all the instructor records of Figure 14.1. The resulting\nstructure appears in Figure 24.16.\n24.5.2.3 Static Hashing versus Dynamic Hashing\nWe now examine the advantages and disadvantages of extendable hashing, compared\nwith static hashing. The main advantage of extendable hashing is that performance\ndoes not degrade as the \ufb01le grows. Furthermore, there is minimal space overhead.\nAlthough the bucket address table incurs additional overhead, it contains one pointer\nfor each hash value for the current pre\ufb01x length. This table is thus small. The main\nspace saving of extendable hashing over other forms of hashing is that no buckets need\nto be reserved for future growth; rather, buckets can be allocated dynamically.\nA disadvantage of extendable hashing is that lookup involves an additional level of\nindirection, since the system must access the bucket address table before accessing the\n", "1231": "24.6 Summary 1203\nbucket itself. This extra reference has only a minor e\ufb00ect on performance. Although\nthe hash structures that we discussed in Section 24.5.1 do not have this extra level of\nindirection, they lose their minor performance advantage as they become full. A fur-\nther disadvantage of extendable hashing is the cost of periodic doubling of the bucket\naddress table.\nThe bibliographical notes also provide references to another form of dynamic hash-\ning called linear hashing , which avoids the extra level of indirection associated with\nextendable hashing, at the possible cost of more over\ufb02ow buckets.\n24.5.3 Comparison of Ordered Indexing and Hashing\nWe have seen several ordered-indexing schemes and several hashing schemes. We can\norganize \ufb01les of records as ordered \ufb01les by using index-sequential organization or B+-\ntree organizations. Alternatively, we can organize the \ufb01les by using hashing. Finally, we\ncan organize them as heap \ufb01les, where the records are not ordered in any particular\nway.\nEach scheme has advantages in certain situations. A database-system implementor\ncould provide many schemes, leaving the \ufb01nal decision of which schemes to use to the\ndatabase designer. However, such an approach requires the implementor to write more\ncode, adding both to the cost of the system and to the space that the system occupies.\nMost database systems support B+-trees for indexing disk-based data, and many\ndatabases also support B+-tree \ufb01le organization. However, most databases do not sup-\nport hash \ufb01le organizations or hash indices for disk-based data. One of the important\nreasons is the fact that many applications bene\ufb01t from support for range queries. A\nsecond reason is the fact that B+-tree indices handle relation size increases gracefully,\nvia a series of node splits, each of which is of low cost, in contrast to the relatively\nhigh cost of doubling of the bucket address table, which extendable hashing requires.\nAnother reason for preferring B+- t r e e si st h ef a c tt h a tB+-trees give good worst-case\nbounds for deletion operations with duplicate keys, unlike hash indices.\nHowever, hash indices are used for in-memory indexing, if range queries are not\ncommon. In particular, they are widely used for creating temporary in-memory indices\nwhile processing join operations using the hash-join technique, as we see in Section\n15.5.5.\n24.6 Summary\n\u2022T h ek e yi d e ao ft h el o gs t r u c t u r e dm e r g et r e ei st or e p l a c er a n d o m I/Ooperations\nduring tree inserts, updates, and deletes with a smaller number of sequential I/O\noperations.\n\u2022Bitmap indices are specialized indices designed for easy querying on multiple keys.\nBitmaps work best for attributes that take only a small number of distinct values.\n", "1232": "1204 Chapter 24 Advanced Indexing Techniques\n\u2022Abitmap is an array of bits. In its simplest form, a bitmap index on the attribute\nAof relation rconsists of one bitmap for each value that Acan take. Each bitmap\nh a sa sm a n yb i t sa st h en u m b e ro fr e c o r d si nt h er e l a t i o n .\n\u2022Bitmap indices are useful for selections m ainly when there are selections on mul-\ntiple keys.\n\u2022An important use of bitmaps is to count the number of tuples satisfying a given\nselection. Such queries are important for data analysis.\n\u2022Indices are required for e\ufb03cient access to spatial data and must e\ufb03ciently support\nqueries such as range and nearest neighbor queries.\n\u2022Aquadtree is an alternative representation for two-dimensional data where the\nspace is divided by a quadtree. Each node of a quadtree is associated with a rect-\nangular region of space.\n\u2022AnR-tree is a storage structure that is useful for indexing of objects such as points,\nline segments, rectangles, and other polygons. An R-tree is a balanced tree struc-\nture with the indexed objects stored in leaf nodes, much like a B+-tree. However,\ni n s t e a do far a n g eo fv a l u e s ,ar e c t a n g u l a r bounding box is associated with each\ntree node.\n\u2022Static hashing uses hash functions in which the set of bucket addresses is \ufb01xed.\nSuch hash functions cannot easily accommodate databases that grow signi\ufb01cantly\nlarger over time.\n\u2022Dynamic hashing techniques allow the hash function to be modi\ufb01ed. One example\nisextendable hashing , which copes with changes in database size by splitting and\ncoalescing buckets as the database grows and shrinks.\nReview Terms\n\u2022Log-structured merge tree (LSM tree)\n\u2022Rolling merge\n\u2022Deletion entry\n\u2022Stepped-merge index\n\u2022Write ampli\ufb01cation\n\u2022Bloom \ufb01lter\n\u2022Bitmap\n\u2022Bitmap index\n\u2022Existence bitmap\n\u2022Quadtree\u2022Region quadtrees\n\u2022R-tree\n\u2022Bounding box\n\u2022Quadratic split\n\u2022Bucket over\ufb02ow\n\u2022Skew\n\u2022Closed addressing\n\u2022Closed hashing\n\u2022Open addressing\n\u2022Open hashing\n", "1233": "Practice Exercises 1205\n\u2022Dynamic hashing\n\u2022Extendable hashing\u2022Linear hashing\nPractice Exercises\n24.1 Both LSM trees and bu\ufb00er trees (described in Section 14.8.2) o\ufb00er bene\ufb01ts to\nwrite-intensive workloads, compared to normal B+-trees, and bu\ufb00er trees o\ufb00er\npotentially better lookup performance. Yet LSM trees are more frequently used\nin Big Data settings. What is the most important reason for this preference?\n24.2 Consider the optimized technique for counting the number of bits that are set\nin a bitmap. What are the tradeo\ufb00s in choosing a smaller versus a larger array\nsize, keeping cache size in mind?\n24.3 Suppose you want to store line segments in an R-tree. If a line segment is not\nparallel to the axes, the bounding box for it can be large, containing a large\nempty area.\n\u2022Describe the e\ufb00ect on performance of having large bounding boxes on\nqueries that ask for line segments intersecting a given region.\n\u2022Brie\ufb02y describe a technique to improve performance for such queries and\ngive an example of its bene\ufb01t. Hint: You can divide segments into smaller\npieces.\n24.4 Give a search algorithm on an R-tree for e\ufb03ciently \ufb01nding the nearest neighbor\nto a given query point.\n24.5 Give a recursive procedure to e\ufb03ciently compute the spatial join of two rela-\ntions with R-tree indices. (Hint: Use bounding boxes to check if leaf entries\nunder a pair of internal nodes may intersect.)\n24.6 Suppose that we are using extendable hashing on a \ufb01le that contains records\nwith the following search-key values:\n2, 3, 5, 7, 11, 17, 19, 23, 29, 31\nS h o wt h ee x t e n d a b l eh a s hs t r u c t u r ef o rt h i s\ufb01 l ei ft h eh a s hf u n c t i o ni s h(x)=x\nmod 8 and buckets can hold three records.\n24.7 Show how the extendable hash structure of Exercise 24.6 changes as the result\nof each of the following steps:\na. Delete 11.\nb. Delete 31.\nc. Insert 1.\n", "1234": "1206 Chapter 24 Advanced Indexing Techniques\nd. Insert 15.\n24.8 Give pseudocode for deletion of entries fromAVi an extendable hash structure,\nincluding details of when and how to coalesce buckets. Do not bother about\nreducing the size of the bucket address table.\n24.9 Suggest an e\ufb03cient way to test if the bucket address table in extendable hashing\ncan be reduced in size by storing an extra count with the bucket address table.\nGive details of how the count should be maintained when buckets are split,\ncoalesced, or deleted. ( Note: Reducing the size of the bucket address table is\nan expensive operation, and subsequent inserts may cause the table to grow\nagain. Therefore, it is best not to reduce the size as soon as it is possible to\ndo so, but instead do it only if the number of index entries becomes small\ncompared to the bucket-address-table size.)\nExercises\n24.10 The stepped merge variant of the LSM tree allows multiple trees per level. What\nare the tradeo\ufb00s in having more trees per level?\n24.11 Suppose you want to use the idea of a quadtree for data in three dimensions.\nHow would the resultant data structure (called an octtree ) divide up space?\n24.12 Explain the distinction between closed and open hashing. Discuss the relative\nmerits of each technique in database applications.\n24.13 What are the causes of bucket over\ufb02ow in a hash \ufb01le organization? What can\nbe done to reduce the occurrence of bucket over\ufb02ows?\n24.14 Why is a hash structure not the best choice for a search key on which range\nqueries are likely?\n24.15 Our description of static hashing assumes that a large contiguous stretch of\ndisk blocks can be allocated to a static hash table. Suppose you can allocate\nonly Ccontiguous blocks. Suggest how to implement the hash table, if it can\nbe much larger than Cblocks. Access to a block should still be e\ufb03cient.\nFurther Reading\nThe log-structured merge (LSM) tree is presented in [O\u2019Neil et al. (1996)], while the\nstepped merge tree is presented in [Jagadish et al. (1997)]. [Vitter (2001)] provides an\nextensive survey of external-memory data structures and algorithms.\nBitmap indices are described in [O\u2019Neil and Quass (1997)]. They were \ufb01rst intro-\nduced in the IBM Model 204 \ufb01le manager on the AS 400 platform. They provide very\n", "1235": "Further Reading 1207\nlarge speedups on certain types of queries and are today implemented in most database\nsystems.\n[Samet (2006)] provides a textbook coverage of spatial data structures. [Bentley\n(1975)] describes the k-d tree, and [Robinson (1981)] describes the k-d-B tree. The\nR-tree was originally presented in [Guttman (1984)].\nDiscussions of the basic data structures in hashing can be found in [Cormen et al.\n(2009)]. Extendable hashing was introduced by [Fagin et al. (1979)]. Linear hashing\nwas introduced by [Litwin (1978)] and [Litwin (1980)].\nBibliography\n[Bentley (1975)] J. L. Bentley, \u201cMultidimensional Binary Search Trees Used for Associative\nSearching\u201d, Communications of the ACM , Volume 18, Number 9 (1975), pages 509\u2013517.\n[Cormen et al. (2009)] T. Cormen, C. Leiserson, R. Rivest, and C. Stein, Introduction to Al-\ngorithms , 3rd edition, MIT Press (2009).\n[Fagin et al. (1979)] R. Fagin, J. Nievergelt, N. Pippenger, and H. R. Strong, \u201cExtendible\nHashing \u2014 A Fast Access Method for Dynamic Files\u201d, ACM Transactions on Database Sys-\ntems, Volume 4, Number 3 (1979), pages 315\u2013344.\n[Guttman (1984)] A. Guttman, \u201cR-Trees: A Dynamic Index Structure for Spatial Searching\u201d,\nInProc. of the ACM SIGMOD Conf. on Management of Data (1984), pages 47\u201357.\n[Jagadish et al. (1997)] H. V. Jagadish, P. P. S. Narayan, S. Seshadri, S. Sudarshan, and\nR. Kanneganti, \u201cIncremental Organization for Data Recording and Warehousing\u201d, In Pro-\nceedings of the 23rd International Conference on Very Large Data Bases , VLDB \u201997 (1997),\npages 16\u201325.\n[Litwin (1978)] W. Litwin, \u201cVirtual Hashing: A Dynamically Changing Hashing\u201d, In Proc. of\nthe International Conf. on Very Large Databases (1978), pages 517\u2013523.\n[Litwin (1980)] W. Litwin, \u201cLinear Hashing: A New Tool for File and Table Addressing\u201d, In\nProc. of the International Conf. on Very Large Databases (1980), pages 212\u2013223.\n[O\u2019Neil and Quass (1997)] P. O\u2019Neil and D. Quass, \u201cImproved Query Performance with\nVariant Indexes\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data (1997), pages\n38\u201349.\n[O\u2019Neil et al. (1996)] P. O\u2019Neil, E. Cheng, D. Gawlick, and E. O\u2019Neil, \u201cThe Log-structured\nMerge-tree (LSM-tree)\u201d, Acta Inf. , Volume 33, Number 4 (1996), pages 351\u2013385.\n[Robinson (1981)] J. Robinson, \u201cThe k-d-B Tree: A Search Structure for Large Multidimen-\nsional Indexes\u201d, In Proc. of the ACM SIGMOD Conf. on Management of Data (1981), pages\n10\u201318.\n[Samet (2006)] H. Samet, Foundations of Multidimensional and Metric Data Structures ,M o r -\ngan Kaufmann (2006).\n", "1236": "1208 Chapter 24 Advanced Indexing Techniques\n[Vitter (2001)] J. S. Vitter, \u201cExternal Memory Algorithms and Data Structures: Dealing with\nMassive Data\u201d, ACM Computing Surveys , Volume 33, (2001), pages 209\u2013271.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "1237": "CHAPTER25\nAdvanced Application\nDevelopment\nThere are a number of tasks in application development. We saw in Chapter 6 to Chap-\nter 9 how to design and build an application. One of the aspects of application design\nis the performance one expects out of the application. In fact, it is common to \ufb01nd\nthat once an application has been built, it runs slower than the designers wanted or\nhandles fewer transactions per second than they required. An application that takes an\nexcessive amount of time to perform request ed actions can cause user dissatisfaction\nat best and be completely unusable at worst.\nApplications can be made to run signi\ufb01cantly faster by performance tuning, which\nconsists of \ufb01nding and eliminating bottlenecks and adding appropriate hardware such\nas memory or disks. There are many things an application developer can do to tune the\napplication, and there are things that a database-system administrator can do to speed\nup processing for an application.\nBenchmarks are standardized sets of tasks that help to characterize the perfor-\nmance of database systems. They are useful to get a rough idea of the hardware and\nsoftware requirements of an application, even before the application is built.\nApplications must be tested as they are being developed. Testing requires gener-\nation of database states and test inputs, and verifying that the outputs match the ex-\npected outputs. We discuss issues in application testing. Legacy systems are application\nsystems that are outdated and usually based on older-generation technology. However,\nthey are often at the core of organizations and run mission-critical applications. We\noutline issues in interfacing with and issues in migrating away from legacy systems,\nreplacing them with more modern systems.\nStandards are very important for application development, especially in the age of\nthe internet, since applications need to co mmunicate with each other to perform use-\nful tasks. A variety of standards have been proposed that a\ufb00ect database-application\ndevelopment, which we outline in this chapter. Organizations often store information\nabout users in directory systems. Applications often use such directory systems to au-\nthenticate users and to get basic information about users, such as user categories (e.g.,\n1209\n", "1238": "1210 Chapter 25 Advanced Application Development\nstudent, instructor, and so on). We brie\ufb02y describe the architecture of directory sys-\ntems.\n25.1 Performance Tuning\nTuning the performance of a system involves adjusting various parameters and design\nchoices to improve its performance for a speci\ufb01c application. Various aspects of a\ndatabase-system design\u2014ranging from high-level aspects such as the schema and trans-\naction design to database parameters such as bu\ufb00er sizes, down to hardware issues such\nas number of disks\u2014a\ufb00ect the performance of an application. Each of these aspects\ncan be adjusted so that performance is improved.\n25.1.1 Motivation for Tuning\nApplications sometimes exhibit poor performance, with queries taking a long time to\ncomplete, leading to users being unable to carry out tasks that they need to do. We\ndescribe a few real-world examples that we have seen, including their causes and how\ntuning \ufb01xed the problems.\nIn one of the applications, we found that users were experiencing long delays and\ntime-outs in the web applications. On monitoring the database, we found that the CPU\nusage was very high, with negligible disk and network usage. Further analysis of queries\nrunning on the database showed that a simple lookup query on a large relation was us-\ning a full relation scan, which was quite expensive. Adding an index to the attribute\nused in the lookup drastically reduced the execution time of the query and a key per-\nformance problem vanished immediately.\nIn a second application, we found that a query had very poor performance. Examin-\ning the query, we found that the programmer had written an unnecessarily complicated\nquery, with several nested subqueries, and the optimizer produced a bad plan for the\nquery, as we realized after observing the query plan. To \ufb01x the problem, we rewrote the\nquery using joins instead of nested subqueries, that is, we decorrelated the query; this\nchange greatly reduced the execution time.\nIn a third application, we found that the application fetched a large number of\nrows from a query, and issued another database query for each row that it fetched.\nThis resulted in a large number of separate queries being sent to the database, resulting\nin poor performance. It is possible to replace such a large number of queries with a\nsingle query that fetches all required data, as we see later in this section. Such a change\nimproved the performance of the application by an order of magnitude.\nIn a fourth application, we found that while the application performed \ufb01ne un-\nder light load during testing, it completely stopped working when subjected to heavy\nload when it was used by actual users. In this case, we found that in some of the in-\nterfaces, programmers had forgotten to close JDBC connections. Databases typically\nsupport only a limited number of JDBC connections, and once that limit was reached,\nthe application was unable to connect to the database, and thus it stopped working.\n", "1239": "25.1 Performance Tuning 1211\nEnsuring that connections were closed \ufb01xed this problem. While this was technically\na bug \ufb01x, not a tuning action, we thought it is a good idea to highlight this problem\nsince we have found many applications have this problem. Connection pooling, which\nkeeps database connections open for use by subsequent transactions, is a related appli-\ncation tuning optimization, since it avoids the cost of repeated opening and closing of\ndatabase connections.\nIt is also worth pointing out that in several cases above the performance problems\ndid not show up during testing, either because the test database was much smaller\nthan the actual database size or because the testing was done with a much lighter load\n(number of concurrent users) than the load on the live system. It is important that\nperformance testing be done on realistic database sizes, with realistic load, so problems\nshow up during testing, rather than on a live system.\n25.1.2 Location of Bottlenecks\nThe performance of most systems (at least before they are tuned) is usually limited\nprimarily by the performance of one or a few components, called bottlenecks .F o ri n -\nstance, a program may spend 80 percent of its time in a small loop deep in the code,\nand the remaining 20 percent of the time on the rest of the code; the small loop then is\na bottleneck. Improving the performance of a component that is not a bottleneck does\nlittle to improve the overall speed of the system; in the example, improving the speed\nof the rest of the code cannot lead to more than a 20 percent improvement overall,\nwhereas improving the speed of the bottleneck loop could result in an improvement of\nnearly 80 percent overall, in the best case.\nHence, when tuning a system, we must \ufb01rst try to discover what the bottlenecks\nare and then eliminate them by improving the performance of system components\ncausing the bottlenecks. When one bottleneck is removed, it may turn out that another\ncomponent becomes the bottleneck. In a well-balanced system, no single component\nis the bottleneck. If the system contains bottlenecks, components that are not part of\nthe bottleneck are underutilized, and could perhaps have been replaced by cheaper\ncomponents with lower performance.\nFor simple programs, the time spent in each region of the code determines the\noverall execution time. However, database systems are much more complex, and query\nexecution involves not only CPU time, but also disk I/Oand network communication. A\n\ufb01rst step in diagnosing problems to use monitoring tools provided by operating systems\nto \ufb01nd the usage level of the CPU, disks, and network links.\nIt is also important to monitor the database itself, to \ufb01nd out what is happening\nin the database system. For example, most databases provide ways to \ufb01nd out which\nqueries (or query templates, where the same query is executed repeatedly with di\ufb00erent\nconstants) are taking up the maximum resources, such as CPU,d i s k I/O,o rn e t w o r k\ncapacity. In addition to hardware resource bottlenecks, poor performance in a database\nsystem may potentially be due to contention on locks, where transactions wait in lock\n", "1240": "1212 Chapter 25 Advanced Application Development\nNote 25.1 DATABASE PERFORMANCE MONITORING TOOLS\nMost database systems provide view relations that can be queried to monitor\ndatabase system performance. For example, Postgre SQL provides view relations\npg\nstat\n statements andpgpgrowlocks to monitor resource usage of SQL state-\nments and lock contention respectively. MySQL supports a command show pro-\ncessinfo that can be used to monitor what transactions are currently executing and\ntheir resource usage. Microsoft SQL S erver provides stored procedures sp\nmonitor ,\nsp\nwho,a n d sp\nlock to monitor system resource usage. The Oracle Database SQL\nTuning Guide, available online, provides details of similar views in Oracle.\nqueues for a long time. Again, most databases provide mechanisms to monitor lock\ncontention.\nMonitoring tools can help detect where the bottleneck lies (such as CPU,I/O,o r\nlocks), and to locate the queries that are causing the maximum performance prob-\nlems. In this chapter, we discuss a number of techniques that can be used to \ufb01x per-\nformance problems, such as adding required indices or materialized views, rewriting\nqueries, rewriting applications, or adding hardware to improve performance.\nTo understand the performance of database systems better, it is very useful to\nmodel database systems as queueing systems . A transaction requests various services\nfrom the database system, starting from entry into a server process, disk reads during\nexecution, CPU cycles, and locks for concurrency control. Each of these services has\na queue associated with it, and small transactions may spend most of their time wait-\ning in queues\u2014especially in disk I/Oqueues\u2014instead of executing code. Figure 25.1\nillustrates some of the queues in a database system. Note that each lockable item has a\nseparate queue in the concurrency control manager. The database system may have a\nsingle queue at the disk manager or may have separate queues for di\ufb00erent disks in case\nthe disks are directly controlled by the database. The transaction queue is used by the\ndatabase system to control the admission of new queries when the number of requests\nexceeds the number of concurrent query execution tasks that the database allows.\nAs a result of the numerous queues in the database, bottlenecks in a database sys-\ntem typically show up in the form of long queues for a particular service, or, equiva-\nlently, in high utilizations for a particular service. If requests are spaced exactly uni-\nformly, and the time to service a request is less than or equal to the time before the\nnext request arrives, then each request will \ufb01nd the resource idle and can therefore\nstart execution immediately without waiting. Unfortunately, the arrival of requests in a\ndatabase system is never so uniform and is often random.\nIf a resource, such as a disk, has a low utilization, then when a request is made,\nthe resource is likely to be idle, in which case the waiting time for the request will\nbe 0. Assuming uniformly randomly distributed arrivals, the length of the queue (and\n", "1241": "25.1 Performance Tuning 1213\nconcurrency-control \nmanager\ndisk managerCPU manager\ntransaction\nmanager\ntransaction\nqueuetransaction\nsource\nbu\ufb00er\nmanagerlock\ngrantlock\nrequest\npage\nreplypage\nrequest\npage\nreplypage\nrequest\n\u2026\u2026\nFigure 25.1 Queues in a database system.\ncorrespondingly the waiting time) goes up exponentially with utilization; as utilization\napproaches 100 percent, the queue length increases sharply, resulting in excessively\nlong waiting times. The utilization of a resource should be kept low enough that queue\nlength is short. As a rule of the thumb, utilizations of around 70 percent are consid-\nered to be good, and utilizations above 90 percent are considered excessive, since they\nwill result in signi\ufb01cant delays. To learn more about the theory of queueing systems,\ngenerally referred to as queueing theory , you can consult the references cited in the\nbibliographical notes.\n25.1.3 Tuning Levels\nTuning is typically done in the context of applications, and can be done at the database\nsystem layer, or outside the database system.\nTuning at layers above the database is application dependent, and is not our focus,\nbut we mention a few such techniques. Pro\ufb01ling application code to \ufb01nd code blocks\nthat have a heavy CPU consumption, and rewriting them to reduce CPU load is an\noption for CPU intensive applications. Application servers often have numerous pa-\nrameters that can be tuned to improve performance, or to ensure that the application\ndoes not run out of memory. Multiple application servers that work in parallel are often\n", "1242": "1214 Chapter 25 Advanced Application Development\nused to handle higher workloads. A load balancer is used to route requests to one of the\napplication servers; to ensure session continuity, requests from a particular source are\nalways routed to the same application server. Connection pooling (described in Section\n9.7.1) is another widely technique to reduce the overhead of database connection cre-\nation. Web application interfaces may be tuned to improve responsiveness, for example\nby replacing legacy web interfaces by ones based on JavaScript and Ajax (described in\nSection 9.5.1.3).\nReturning to database tuning, database administrators and application developers\ncan tune a database system at three levels.\nThe highest level of database tuning, which is under the control of application\ndevelopers, includes the schema and queries. The developer can tune the design of the\nschema, the indices that are created, and the transactions that are executed to improve\nperformance. Tuning at this level is comparatively system independent.\nThe second level consists of the database-system parameters, such as bu\ufb00er size\nand checkpointing intervals. The exact set of database-system parameters that can be\ntuned depends on the speci\ufb01c database system. Most database-system manuals provide\ninformation on what database-system parameters can be adjusted, and how you should\nchoose values for the parameters. Well-designed database systems perform as much\ntuning as possible automatically, freeing th e user or database administrator from the\nburden. For instance, in many database systems the bu\ufb00er size is \ufb01xed but tunable. If\nthe system automatically adjusts the bu\ufb00er size by observing indicators such as page-\nfault rates, then the database administrator will not have to worry about tuning the\nbu\ufb00er size.\nThe lowest level is at the hardware level. Options for tuning systems at this level\ninclude replacing hard disks with solid-state drives (which use \ufb02ash storage), adding\nmore disks or using a RAID system if disk I/Ois a bottleneck, adding more memory if\nthe disk bu\ufb00er size is a bottleneck, or moving to a system with more processors if CPU\nusage is a bottleneck.\nThe three levels of tuning interact with one another; we must consider them to-\ngether when tuning a system. For example, tuning at a higher level may result in the\nhardware bottleneck changing from the disk system to the CPU, or vice versa. Tuning\nof queries and the physical schema is usually the \ufb01rst step to improving performance.\nTuning of database system parameters, in case the database system does automate this\ntask, can also be done in parallel. If performance is still poor, tuning of logical schema\nand tuning of hardware are the next logical steps.\n25.1.4 Tuning of Physical Schema\nTuning of the physical schema, such as indices and materialized views, is the least\ndisruptive mode of tuning, since it does not a\ufb00ect application code in any way. We now\nstudy di\ufb00erent aspects of tuning of the physical schema.\n", "1243": "25.1 Performance Tuning 1215\n25.1.4.1 Tuning of Indices\nWe can tune the indices in a database system to improve performance. If queries are the\nbottleneck, we can often speed them up by creating appropriate indices on relations. If\nupdates are the bottleneck, there may be too many indices, which have to be updated\nwhen the relations are updated. Removing indices may speed up certain updates.\nThe choice of the type of index also is important. Some database systems support\ndi\ufb00erent kinds of indices, such as hash indices, B+-tree indices, and write-optimized\nindices such as LSM trees (Section 24.2). If range queries are common, B+-tree indices\nare preferable to hash indices. If the system has a very high write load, but a relatively\nlow read load, write-optimized LSM tree indices may be preferable to B+-tree indices.\nWhether to make an index a clustered index is another tunable parameter. Only\none index on a relation can be made clustered, by storing the relation sorted on the\nindex attributes. Generally, the index that bene\ufb01ts the greatest number of queries and\nupdates should be made clustered.\nTo help identify what indices to create, and which index (if any) on each relation\nshould be clustered, most commercial database systems provide tuning wizards ;t h e s e\nare described in more detail in Section 25.1.4.4. These tools use the past history of\nqueries and updates (called the workload ) to estimate the e\ufb00ects of various indices on\nthe execution time of the queries and updates in the workload. Recommendations on\nwhat indices to create are based on these estimates.\n25.1.4.2 Using Materialized Views\nMaintaining materialized views can greatly speed up certain types of queries, in par-\nticular aggregate queries. Recall the example from Section 16.5 where the total salary\nfor each department (obtained by summing the salary of each instructor in the depart-\nment) is required frequently. As we saw in that section, creating a materialized view\nstoring the total salary for each department can greatly speed up such queries.\nMaterialized views should be used with care, however, since there is not only space\noverhead for storing them but, more important, there is also time overhead for main-\ntaining materialized views. In the case of immediate view maintenance ,i ft h eu p d a t e s\nof a transaction a\ufb00ect the materialized view, the materialized view must be updated as\npart of the same transaction. The transaction may therefore run slower. In the case of\ndeferred view maintenance , the materialized view is updated later; until it is updated,\nthe materialized view may be inconsistent with the database relations. For instance, the\nmaterialized view may be brought up to date when a query uses the view, or periodically.\nUsing deferred maintenance reduces the burden on update transactions.\nThe database administrator is responsible for the selection of materialized views\nand for view-maintenance policies. The database administrator can make the selection\nmanually by examining the types of queries in the workload and \ufb01nding out which\nqueries need to run faster and which updates/queries may be executed more slowly.\nFrom the examination, the database administrator may choose an appropriate set of\n", "1244": "1216 Chapter 25 Advanced Application Development\nmaterialized views. For instance, the administrator may \ufb01nd that a certain aggregate\nis used frequently, and choose to materialize it, or may \ufb01nd that a particular join is\ncomputed frequently, and choose to materialize it.\nHowever, manual choice is tedious for even moderately large sets of query types,\nand making a good choice may be di\ufb03cult, since it requires understanding the costs\nof di\ufb00erent alternatives; only the query optimizer can estimate the costs with reason-\nable accuracy without actually executing the query. Thus, a good set of views may be\nfound only by trial and error\u2014that is, by materializing one or more views, running\nthe workload, and measuring the time taken to run the queries in the workload. The\nadministrator repeats the process until a set of views is found that gives acceptable\nperformance.\nA better alternative is to provide support for selecting materialized views within the\ndatabase system itself, integrated with the query optimizer. This approach is described\nin more detail in Section 25.1.4.4.\n25.1.4.3 Horizontal Partitioning of Relation Schema\nHorizontal partitioning of relations is widely used for parallel and distributed storage\nand query processing. However, it can also be used in a centralized system to improve\nqueries and updates by breaking up the tuples of a relation into partitions.\nFor example, suppose that a database stores a large relation that has a dateattribute,\nand most operations work on data inserted within the past few months. Suppose now\nthat the relation is partitioned on the date attribute, with one partition for each ( year,\nmonth ) combination. Then, queries that contain a selection on date,s u c ha s date='2018-\n06-01', need access only partitions that could possibly contain such tuples, skipping all\nother partitions.\nMore importantly, indices could be created independently on each partition. Sup-\npose an index is created on an attribute ID, with a separate index on each partition. A\nquery that speci\ufb01es selection on ID, along with a date or a date range, need look up the\nindex on only those partitions that match the speci\ufb01ed date or date range. Since each\npartition is smaller than the whole relation, the indices too are smaller, speeding up\nindex lookup. Index insertion is also much faster, since the index size is much smaller\nthan an index on the entire relation. And most importantly, even as the total data size\ngrows, the partition size never grows beyond some limit, ensuring that the performance\nof such queries does not degrade with time.\nThere is a cost to such partitioning: queries that do not contain a selection on\nthe partitioning attribute need to individually access each of the partitions, potentially\nslowing down such queries signi\ufb01cantly. If such queries are rare, the bene\ufb01ts of parti-\ntioning outweigh the costs, making them an attractive technique for optimization.\nEven if the database does not support partitioning internally, it is possible to re-\nplace a relation rby multiple physical relations r1,r2,\u2026,rn, and the original relation r\nis de\ufb01ned by the view r=r1\u222ar2\u222a\u2026\u222a rn. Suppose that the database optimizer knows\n", "1245": "25.1 Performance Tuning 1217\nthe predicate de\ufb01ning each ri(in our example, the date range corresponding to each\nri). Then the optimizer can replace a query on rthat includes a selection on the parti-\ntioning attribute ( date, in our example), with a query on the only relevant ris. Indices\nw o u l dh a v et ob ec r e a t e ds e p a r a t e l yo ne a c ho ft h e ris.\n25.1.4.4 Automated Tuning of Physical Design\nMost commercial database systems today provide tools to help the database adminis-\ntrator with index and materialized view selection and other tasks related to physical\ndatabase design such as how to partition data in a parallel database system.\nThese tools examine the workload (the history of queries and updates) and sug-\ngest indices and views to be materialized. The database administrator may specify the\nimportance of speeding up di\ufb00erent queries, which the tool takes into account when\nselecting views to materialize. Often tuning must be done before the application is fully\ndeveloped, and the actual database contents may be small on the development database\nbut are expected to be much larger on a produc tion database. Thus, some tuning tools\nalso allow the database administrator to specify information about the expected size of\nthe database and related statistics.\nMicrosoft\u2019s Database Tuning Assistant, for example, allows the user to ask \u201cwhat\nif\u201d questions, whereby the user can pick a view, and the optimizer then estimates the\ne\ufb00ect of materializing the view on the total cost of the workload and on the individual\ncosts of di\ufb00erent types of queries and updates in the workload.\nThe automatic selection of indices and materialized views is usually implemented\nby enumerating di\ufb00erent alternatives and using the query optimizer to estimate the\ncosts and bene\ufb01ts of selecting each alternative by using the workload. Since the number\nof design alternatives and the potential workload may be extremely large, the selection\ntechniques must be designed carefully.\nThe \ufb01rst step is to generate a workload. This is usually done by recording all the\nqueries and updates that are executed during some time period. Next, the selection\ntools perform workload compression , that is, create a representation of the workload\nusing a small number of updates and queries. For example, updates of the same form\ncan be represented by a single update with a weight corresponding to how many times\nthe update occurred. Queries of the same form can be similarly replaced by a repre-\nsentative with appropriate weight. After this, queries that are very infrequent and do\nnot have a high cost may be discarded from consideration. The most expensive queries\nmay be chosen to be addressed \ufb01rst. Such workload compression is essential for large\nworkloads.\nWith the help of the optimizer, the tool would come up with a set of indices and\nmaterialized views that could help the queries and updates in the compressed workload.\nDi\ufb00erent combinations of these indices and materialized views can be tried out to \ufb01nd\nthe best combination. However, an exhaustive approach would be totally impractical,\nsince the number of potential indices and materialized views is already large, and each\n", "1246": "1218 Chapter 25 Advanced Application Development\nNote 25.2 TUNING TOOLS\nTuning tools, such as the Database Engine Tuning Advisor provided by SQL S erver\nand the SQL Tuning Advisor of Oracle, provide recommendations such as what\nindices or materialized views to add, or how to partition a relation, to improve\nperformance. These recommendations can then be accepted and implemented by\na database administrator.\nAuto Tuning in Microsoft Azure SQL can automatically create and drop in-\ndices to improve query performance. A risk with automatically changing the phys-\nical schema is that some queries may perform poorly. For example, an optimizer\nmay choose a plan using a newly created index, assuming, based on wrong esti-\nmates of cost, that the new plan is cheaper than the plan used before the index was\ncreated. In reality, the query may run slower using the new plan, which may a\ufb00ect\nusers. The \u201cforce last good plan\u201d feature can monitor query performance after any\nchange such as addition of an index, and if performance is worse, it can force the\ndatabase to use the old plan before the change (as long as it is still valid).\nOracle also provides auto tuning support, for example recommending if an\nindex should be added, or monitoring the use of a query to decide if it should be\noptimized for fetching only a few rows or for fetching all rows (the best plan may\nb ev e r yd i \ufb00 e r e n ti fo n l yt h e\ufb01 r s tf e wr o w sa r ef e t c h e do ri fa l lr o w sa r ef e t c h e d ) .\nsubset of these is a potential design alternative, leading to an exponential number of\nalternatives. Heuristics are used to reduce the space of alternatives, that is, to reduce\nthe number of combinations considered.\nGreedy heuristics for index and materialized view selection operate as follows:\nThey estimate the bene\ufb01ts of materializing di\ufb00erent indices or views (using the op-\ntimizer\u2019s cost estimation functionality as a subroutine). They then choose the index or\nview that gives either the maximum bene\ufb01t or the maximum bene\ufb01t per unit space (i.e.,\nbene\ufb01t divided by the space required to store the index or view). The cost of maintain-\ning the index or view must be taken into account when computing the bene\ufb01t. Once the\nheuristic has selected an index or view, the bene\ufb01ts of other indices or views may have\nchanged, so the heuristic recomputes these and chooses the next best index or view for\nmaterialization. The process continues until either the available disk space for storing\nindices or materialized views is exhausted or the cost of maintaining the remaining\ncandidates is more than the bene\ufb01t to queries that could use the indices or views.\nReal-world index and materialized-view selection tools usually incorporate some\nelements of greedy selection but use other techniques to get better results. They also\nsupport other aspects of physical database design, such as deciding how to partition a\nrelation in a parallel database, or what physical storage mechanism to use for a relation.\n", "1247": "25.1 Performance Tuning 1219\n25.1.5 Tuning of Queries\nThe performance of an application can often be signi\ufb01cantly improved by rewriting\nqueries or by changing how the application issues queries to the database.\n25.1.5.1 Tuning of Query Plans\nIn the past, optimizers on many database systems were not particularly good, so how\na query was written would have a big in\ufb02uence on how it was executed, and therefore\non the performance. Today\u2019s advanced optimizers can transform even badly written\nqueries and execute them e\ufb03ciently, so the need for tuning individual queries is less\nimportant than it used to be. However, sometimes query optimizers choose bad plans\nfor one of several reasons, which we describe next.\nBefore checking if something needs to be tuned in the plan for a query, it is useful\nto \ufb01nd out what plan is being used for the query. Most databases support an explain\ncommand, which allows you to see what plan is being used for a query. The explain\ncommand also shows the statistics that the optimizer used or computed for di\ufb00erent\nparts of the query plan, and estimates of the costs of each part of a query plan. Vari-\nants of the explain command also execute the query and get actual tuple counts and\nexecution time for di\ufb00erent parts of the query plan.\nIncorrect statistics are often the reason for the choice of a bad plan. For example,\nif the optimizer thinks that the relations involved in a join have very few tuples, it may\nchoose nested loops join, which would be very ine\ufb03cient if the relations actually have\na large number of tuples.\nIdeally, database statistics should be updated whenever relations are updated. How-\never, doing so adds unacceptable overhead to update queries. Instead, databases either\nperiodically update statistics or leave it to the system administrator to issue a com-\nmand to update statistics. Some databases, such as Postgre SQL and MySQL support\na command called analyze ,1which can be used to recompute statistics. For example,\nanalyze instructor would recompute statistics for the instructor relation, while ana-\nlyze with no arguments would recompute statistics for all relations in Postgre SQL.I t\nis highly recommended to run this command after loading data into the database, or\nafter making a signi\ufb01cant number of inserts or deletes on a relation.\nSome databases such as Oracle and Microsoft SQL Server keep track of inserts\nand deletes to relations, and they update statistics whenever the relation size changes\nby a signi\ufb01cant fraction, making execution of the analyze command unnecessary.\nAnother reason for poor performance of queries is the lack of required indices. As\nwe saw earlier, the choice of indices can be done as part of the tuning of the physical\nschema, but examining a query helps us understand what indices may be useful to speed\nup that query.\nIndices are particularly important for queries that fetch only a few rows from a large\nrelation, based on a predicate. For example, a query that \ufb01nds students in a department\n1The command is called analyze table i nt h ec a s eo f MySQL.\n", "1248": "1220 Chapter 25 Advanced Application Development\nmay bene\ufb01t from an index on the student relation on the attribute dept\n name .I n d i c e s\non join attributes are often very useful. For example, if the above query also included\naj o i no f student with takes on the attribute takes .ID,a ni n d e xo n takes .IDcould be\nuseful.\nNote that databases typically create indices on primary-key attributes, which can be\nused for selections as well as joins. For example, in our university schema, the primary-\nkey index on takes hasIDas its \ufb01rst attribute and may thus be useful for the above\njoin.\nComplex queries containing nested subqueries are not optimized very well by many\noptimizers. We saw techniques for nested su bquery decorrelation in Section 16.4.4. If a\nsubquery is not decorrelated, it gets executed repeatedly, potentially resulting in a great\ndeal of random I/O. In contrast, decorrelation allows e\ufb03cient set-oriented operations\nsuch as joins to be used, minimizing random I/O. Most database query optimizers\nincorporate some forms of decorrelation, but some can handle only very simple nested\nsubqueries. The execution plan chosen by the optimizer can be found as described in\nChapter 16. If the optimizer has not succeeded in decorrelating a nested subquery, the\nquery can be decorrelated by rewriting it manually.\n25.1.5.2 Improving Set Orientation\nWhen SQL queries are executed from an application program, it is often the case that\na query is executed frequently, but with di\ufb00erent values for a parameter. Each call has\nan overhead of communication with the server, in addition to processing overheads at\nthe server.\nFor example, consider a program that steps through each department, invoking an\nembedded SQL query to \ufb01nd the total salary of all instructors in the department:\nselect sum (salary )\nfrom instructor\nwhere dept\n name =?\nIf the instructor relation does not have a clustered index on dept\n name ,e a c hs u c h\nquery will result in a scan of the relation. Even if there is such an index, a random I/O\noperation will be required for each dept\n name value.\nInstead, we can use a single SQL query to \ufb01nd total salary expenses of each depart-\nment:\nselect dept\n name ,sum(salary )\nfrom instructor\ngroup by dept\n name ;\nThis query can be evaluated with a single scan of the instructor relation, avoiding ran-\ndom I/Ofor each department. The results can be fetched to the client side using a single\nround of communication, and the client program can then step through the results to\n\ufb01nd the aggregate for each department. Combining multiple SQL queries into a single\n", "1249": "25.1 Performance Tuning 1221\nPreparedStatement pStmt = conn.prepareStatement(\n\"insert into instructor values(?,?,?,?)\");\npStmt.setString(1, \"88877\");\npStmt.setString(2, \"Perry\");\npStmt.setInt(3, \"Finance\");\npStmt.setInt(4, 125000);\npStmt.addBatch( );\npStmt.setString(1, \"88878\");\npStmt.setString(2, \"Thierry\");\npStmt.setInt(3, \"Physics\");\npStmt.setInt(4, 100000);\npStmt.addBatch( );\npStmt.executeBatch( );\nFigure 25.2 Batch update in JDBC .\nSQL query as above can reduce execution costs greatly in many cases\u2014for example, if\ntheinstructor relation is very large and has a large number of departments.\nThe JDBC API also provides a feature called batch update that allows a number of\ninserts to be performed using a single communication with the database. Figure 25.2\nillustrates the use of this feature. The code shown in the \ufb01gure requires only one round\nof communication with the database, when the executeBatch() method is executed, in\ncontrast to similar code without the batch update feature that we saw in Figure 5.2. In\nthe absence of batch update, as many rounds of communication with the database are\nrequired as there are instructo rs to be inserted. The batch update feature also enables\nthe database to process a batch of inserts at once, which can potentially be done much\nmore e\ufb03ciently than a series of single record inserts.\nAnother technique used widely in client-server systems to reduce the cost of com-\nmunication and SQL compilation is to use stored procedures, where queries are stored\nat the server in the form of procedures, which may be precompiled. Clients can invoke\nthese stored procedures rather than communicate a series of queries.\n25.1.5.3 Tuning of Bulk Loads and Updates\nWhen loading a large volume of data into a database (called a bulk load operation),\nperformance is usually very poor if the inserts are carried out as separate SQL insert\nstatements. One reason is the overhead of parsing each SQL query; a more important\nreason is that performing integrity constraint checks and index updates separately for\neach inserted tuple results in a large number of random I/Ooperations. If the inserts\nwere done as a large batch, integrity-constraint checking and index update can be done\n", "1250": "1222 Chapter 25 Advanced Application Development\nin a much more set-oriented fashion, reducing overheads greatly; performance improve-\nments of an order of magnitude or more are not uncommon.\nTo support bulk load operations, most database systems provide a bulk import util-\nity and a corresponding bulk export utility. The bulk-import utility reads data from a\n\ufb01le and performs integrity constraint checking as well as index maintenance in a very\ne\ufb03cient manner. Common input and output \ufb01le formats supported by such bulk im-\nport/export utilities include text \ufb01les with characters such as commas or tabs separating\nattribute values, with each record in a line of its own (such \ufb01le formats are referred to\nascomma-separated values ortab-separated values formats). Database-speci\ufb01c binary\nf o r m a t sa sw e l la s XML formats are also supported by bulk import/export utilities. The\nnames of the bulk import/export utilities di\ufb00er by database. In Postgre SQL, the utilities\nare called pg\ndump andpg\nrestore (Postgre SQL also provides an SQL command copy,\nwhich provides similar functionality). The bulk import/export utility in Oracle is called\nSQL*L oader , the utility in DB2 is called load, and the utility in SQL S erver is called bcp\n(SQL S erver also provides an SQL command called bulk insert ).\nWe now consider the case of tuning of bulk updates. Suppose we have a relation\nfunds\n received (dept\n name ,amount ) that stores funds received (say, by electronic funds\ntransfer) for each of a set of departments. Suppose now that we want to add the amounts\nto the balances of the corresponding department budgets. In order to use the SQL\nupdate statement to carry out this task, we have to perform a look up on the funds\nreceived relation for each tuple in the department relation. We can use subqueries in\nthe update clause to carry out this task, as follows: We assume for simplicity that the\nrelation funds\n received contains at most one tuple for each department.\nupdate department setbudget =budget +\n(select amount\nfrom funds\n received\nwhere funds\n received.dept\n name =department .dept\n name )\nwhere exists (\nselect *\nfrom funds\n received\nwhere funds\n received.dept\n name =department .dept\n name );\nNote that the condition in the where clause of the update ensures that only accounts\nwith corresponding tuples in funds\n received are updated, while the subquery within the\nsetclause computes the amount to be added to each such department.\nThere are many applications that require updates such as that illustrated above.\nTypically, there is a table, which we shall call the master table , and updates to the master\ntable are received as a batch. Now the master table has to be correspondingly updated.\nSQL:2003 introduced a special construct, called the merge construct, to simplify the\ntask of performing such merging of information. For example, the preceding update\ncan be expressed using merge as follows:\n", "1251": "25.1 Performance Tuning 1223\nmerge into department asA\nusing (select *\nfrom funds\n received )asF\non(A.dept\n name =F.dept\n name )\nwhen matched then\nupdate set budget =budget +F.amount ;\nWhen a record from the subquery in the using clause matches a record in the depart-\nment relation, the when matched clause is executed, which can execute an update on\nthe relation; in this case, the matching record in the department relation is updated as\nshown.\nThemerge statement can also have a when not matched then clause, which permits\ninsertion of new records into the relation. In the preceding example, when there is no\nmatching department for a funds\n received tuple, the insertion action could create a new\ndepartment record (with a null building ) using the following clause:\nwhen not matched then\ninsert values (F.dept\n name ,null,F.budget )\nAlthough not very meaningful in this example,2thewhen not matched then clause can\nbe quite useful in other cases. For example, suppose the local relation is a copy of\na master relation, and we receive updated as well as newly inserted records from the\nmaster relation. The merge statement can update matched records (these would be\nupdated old records) and insert records that are not matched (these would be new\nrecords).\nNot all SQL implementations support the merge statement currently; see the re-\nspective system manuals for further details.\n25.1.6 Tuning of the Logical Schema\nPerformance of queries can sometimes be improved by tuning of the logical schema.\nFor example, within the constraints of the chosen normal form, it is possible to parti-\ntion relations vertically. Consider the course relation, with the schema:\ncourse (course\n id\n,title,dept\n name ,credits )\nfor which course\n idis a key. Within the constraints of the normal forms ( BCNF and\n3NF), we can partition the course relation into two relations:\ncourse\n credit (course\n id\n,credits )\ncourse\n title\ndept(course\n id\n,title,dept\n name )\n2A better action here would have been to insert these records into an error relation, but that cannot be done with the\nmerge statement.\n", "1252": "1224 Chapter 25 Advanced Application Development\nThe two representations are logically equivalent, since course\n idis a key, but they have\ndi\ufb00erent performance characteristics.\nIf most accesses to course information look at only the course\n idandcredits ,t h e n\nthey can be run against the course\n credit relation, and access is likely to be somewhat\nfaster, since the titleanddept\n name attributes are not fetched. For the same reason,\nmore tuples of course\n credit will \ufb01t in the bu\ufb00er than corresponding tuples of course ,\nagain leading to faster performance. This e\ufb00ect would be particularly marked if the title\nanddept\n name attributes were large. Hence, a schema consisting of course\n credit and\ncourse\n title\ndeptwould be preferable to a schema consisting of the course relation in this\ncase.\nOn the other hand, if most accesses to course information require both dept\n name\nandcredits ,u s i n gt h e course relation would be preferable, since the cost of the join of\ncourse\n credit andcourse\n title\ndeptwould be avoided. Also, the storage overhead would\nbe lower, since there would be only one relation, and the attribute course\n idwould not\nbe replicated.\nThecolumn store approach to storing data are based on vertical partitioning but\ntakes it to the limit by storing each attribute (column) of the relation in a separate\n\ufb01le, as we saw in Section 13.6. Note that in a column store it is not necessary to re-\npeat the primary-key attribute since the ithrow can be reconstructed by taking the ith\nentry for each desired column. Column stores have been shown to perform well for\nseveral data-warehouse applications by reducing I/O, improving cache performance,\nenabling greater gains from data compression, and allowing e\ufb00ective use of CPU vector-\nprocessing capabilities.\nAnother trick to improve performance is to store a denormalized relation ,s u c ha s\naj o i no f instructor anddepartment , where the information about dept\n name ,building ,\nandbudget is repeated for every instructor. More e\ufb00ort has to be expended to make\nsure the relation is consistent whenever an update is carried out. However, a query\nthat fetches the names of the instructors and the associated buildings will be speeded\nup, since the join of instructor anddepartment will have been precomputed. If such a\nquery is executed frequently, and has to be performed as e\ufb03ciently as possible, the\ndenormalized relation could be bene\ufb01cial.\nMaterialized views can provide the bene\ufb01ts that denormalized relations provide,\nat the cost of some extra storage. A major advantage to materialized views over denor-\nmalized relations is that maintaining consistency of redundant data becomes the job\nof the database system, not the programmer. Thus, materialized views are preferable,\nwhenever they are supported by the database system.\nAnother approach to speed up the computation of the join without materializing\nit is to cluster records that would match in the join on the same disk page. We saw such\nclustered \ufb01le organizations in Section 13.3.3.\n25.1.7 Tuning of Concurrent Transactions\nConcurrent execution of di\ufb00erent types of transactions can sometimes lead to poor\nperformance because of contention on lock s. We \ufb01rst consider the case of read-write\n", "1253": "25.1 Performance Tuning 1225\ncontention, which is more common, and then consider the case of write-write con-\ntention.\nAs an example of read-write contention , consider the following situation on a bank-\ning database. During the day, numerous small update transactions are executed almost\ncontinuously. Suppose that a large query that computes statistics on branches is run at\nt h es a m et i m e .I ft h eq u e r yp e r f o r m sas c a no nar e l a t i o n ,i tm a yb l o c ko u ta l lu p d a t e s\non the relation while it runs, and that can have a disastrous e\ufb00ect on the performance\nof the system.\nSeveral database systems\u2014Oracle, Postgre SQL, and Microsoft SQL S erver, for ex-\nample\u2014 support snapshot isolation, whereby queries are executed on a snapshot of the\ndata, and updates can go on concurrently. (Snapshot isolation is described in detail in\nSection 18.8.) Snapshot isolation should be used, if available, for large queries, to avoid\nlock contention in the above situation. In SQL S erver, executing the statement\nset transaction isolation level snapshot\nat the beginning of a transaction results in snapshot isolation being used for that trans-\naction. In Oracle and Postgre SQL,u s i n gt h ek e y w o r d serializable in place of the key-\nword snapshot in the above command has the same e\ufb00ect, since these systems actually\nuse snapshot isolation (serializable snapshot isolation, in the case of Postgre SQL ver-\nsion 9.1 onwards) when the isolation level is set to serializable.\nIf snapshot isolation is not available, an alternative option is to execute large queries\nat times when updates are few or nonexistent. However, for databases supporting web\nsites, there may be no such quiet period for updates.\nAnother alternative is to use weaker levels of consistency, such as the read commit-\ntedisolation level, whereby evaluation of the query has a minimal impact on concurrent\nupdates, but the query results are not guaranteed to be consistent. The application se-\nmantics determine whether approximate (inconsistent) answers are acceptable.\nWe now consider the case of write-write contention . Data items that are updated\nvery frequently can result in poor performance with locking, with many transactions\nwaiting for locks on those data items. Such data items are called update hot spots .\nUpdate hot spots can cause problems even with snapshot isolation, causing frequent\ntransaction aborts due to write-validation f ailures. A commonly occurring situation that\nresults in an update hot spot is as follows: transactions need to assign unique identi\ufb01ers\nto data items being inserted into the database, and to do so they read and increment\na sequence counter stored in a tuple in the database. If inserts are frequent, and the\nsequence counter is locked in a two-phase manner, the tuple containing the sequence\ncounter becomes a hot spot.\nOne way to improve concurrency is to release the lock on the sequence counter\nimmediately after it is read and incremented; however, after doing so, even if the trans-\naction aborts, the update to the sequence counter should not be rolled back. To under-\nstand why, suppose T1increments the sequence counter, and then T2increments the\nsequence counter before T1commits; if T1then aborts, rolling back its update, either\n", "1254": "1226 Chapter 25 Advanced Application Development\nby restoring the counter to the original value or by decrementing the counter, will result\nin the sequence value used by T2getting reused by a subsequent transaction.\nMost databases provide a special construct for creating sequence counters that im-\nplement early, non-two-phase lock release, coupled with special-case treatment of undo\nlogging so that updates to the counter are not rolled back if the transaction aborts. The\nSQL standard allows a sequence counter to be created using the command:\ncreate sequence counter1 ;\nIn the above command, counter1 i st h en a m eo ft h es e q u e n c e ;m u l t i p l es e q u e n c e sc a n\nbe created with di\ufb00erent names. The syntax to get a value from the sequence is not\nstandardized; in Oracle, counter1.nextval would return the next value of the sequence,\nafter incrementing it, while the function call nextval (\u2019counter1 \u2019) would have the same\ne\ufb00ect in Postgre SQL,a n d DB2 uses the syntax nextval for counter1 .\nThe SQL standard provides an alternative to using an explicit sequence counter,\nwhich is useful when the goal is to give unique identi\ufb01ers to tuples inserted into a re-\nlation. To do so, the keyword identity can be added to the declaration of an integer\nattribute of a relation (usually this attribute would also be declared as the primary\nkey). If the value for that attribute is left unspeci\ufb01ed in an insert statement for that\nrelation, a unique new value is created automatically for each newly inserted tuple. A\nnon-two-phase locked sequence counter is used internally to implement the identity dec-\nlaration, with the counter incremented each time a tuple is inserted. Several databases,\nincluding DB2 and SQL S erver support the identity declaration, although the syntax\nvaries. Postgre SQL supports a data type called serial ,w h i c hp r o v i d e st h es a m ee \ufb00 e c t ;\nthePostgre SQL type serial is implemented by transparently creating a non-two-phase\nlocked sequence.\nIt is worth noting that since the acquisition of a sequence number by a transaction\ncannot be rolled back if the transaction aborts (for reasons discussed earlier), transac-\ntion aborts may result in g a p si nt h es e q u e n c en u m b e r s in tuples inserted in the database.\nFor example, there may be tuples with identi\ufb01er value 1001 and 1003, but no tuple with\nvalue 1002, if the transaction that acqui red the sequence number 1002 did not com-\nmit. Such gaps are not acceptable in some applications; for example, some \ufb01nancial\napplications require that there be no gaps in bill or receipt numbers. Database pro-\nvided sequences and automatically incremented attributes should not be used for such\napplications, since they can result in gaps. A sequence counter stored in normal tuples,\nwhich is locked in a two-phase manner, would not be susceptible to such gaps since a\ntransaction abort would restore the sequence counter value, and the next transaction\nwould get the same sequence number, avoiding a gap.\nLong update transactions can cause performance problems with system logs and\ncan increase the time taken to recover from system crashes. If a transaction performs\nmany updates, the system log may become full even before the transaction completes,\nin which case the transaction will have to be rolled back. If an update transaction runs\nfor a long time (even with few updates), it may block deletion of old parts of the log,\n", "1255": "25.1 Performance Tuning 1227\nif the logging system is not well designed. Again, this blocking could lead to the log\ngetting \ufb01lled up.\nTo avoid such problems, many database systems impose strict limits on the num-\nber of updates that a single transaction can carry out. Even if the system does not\nimpose such limits, it is often helpful to break up a large update transaction into a set\nof smaller update transactions where possible. For example, a transaction that gives a\nraise to every employee in a large corporation could be split up into a series of small\ntransactions, each of which updates a small range of employee-ids. Such transactions\nare called minibatch transactions . However, minibatch transactions must be used with\ncare. First, if there are concurrent updates on the set of employees, the result of the\nset of smaller transactions may not be equivalent to that of the single large transaction.\nSecond, if there is a failure, the salaries of some of the employees would have been\nincreased by committed transactions, but salaries of other employees would not. To\navoid this problem, as soon as the system recovers from failure, we must execute the\ntransactions remaining in the batch.\nLong transactions, whether read-only or update, can also result in the lock table\nbecoming full. If a single query scans a large relation, the query optimizer would ensure\nthat a relation lock is obtained instead of acquiring a large number of tuple locks.\nHowever, if a transaction executes a large number of small queries or updates, it may\nacquire a large number of locks, resulting in the lock table becoming full.\nTo avoid this problem, some databases provide for automatic lock escalation ;w i t h\nthis technique, if a transaction has acquired a large number of tuple locks, tuple locks\nare upgraded to page locks, or even full relation locks. Recall that with multiple-\ngranularity locking (Section 18.3), once a coarser-level lock is obtained, there is no\nneed to record \ufb01ner-level locks, so tuple lock entries can be removed from the lock\ntable, freeing up space. On databases that do not support lock escalation, it is possible\nfor the transaction to explicitly acquire a relation lock, thereby avoiding the acquisition\nof tuple locks.\n25.1.8 Tuning of Hardware\nHardware bottlenecks could include memory, I/O,CPU and network capacity. We focus\non memory and I/Otuning in this section. The availability of processors with a large\nnumber of CPU cores, and support for multiple CPUs on a single machine allows system\ndesigners to choose the CPU model and number of CPUst om e e tt h e CPU requirements\nof the application at an acceptable cost. How to tune or choose between CPU and\nnetwork interconnect options is a topic outside the domain of database tuning.\nEven in a well-designed transaction processing system, each transaction usually has\nto do at least a few I/Ooperations, if the data required by the transaction are on disk.\nAn important factor in tuning a transaction processing system is to make sure that the\ndisk subsystem can handle the rate at which I/Ooperations are required. For instance,\nconsider a hard disk that supports an access time of about 10 milliseconds, and average\ntransfer rate of 25 to 100 megabytes per second (a fairly typical disk today). Such a disk\n", "1256": "1228 Chapter 25 Advanced Application Development\nwould support a little under 100 random-access I/Ooperations of 4 kilobytes each per\nsecond. If each transaction requires just two I/Ooperations, a single disk would support\nat most 50 transactions per second.\nAn obvious way to improve performance is to replace a hard disk with a solid-state\ndrive ( SSD), since a single SSDcan support tens of thousands of random I/Ooperations\nper second. A drawback of using SSDs is that they cost a lot more than hard disks for\na given storage capacity. Another way to support more transactions per second is to\nincrease the number of disks. If the system needs to support ntransactions per second,\neach performing two I/Ooperations, data must be striped (or otherwise partitioned)\nacross at least n\u221550 hard disks (ignoring skew), or n\u22155000 SSDs ,i ft h e SSD supports\n10,000 random I/Ooperations per second.\nNotice here that the limiting factor is not the capacity of the disk, but the speed at\nwhich random data can be accessed (limited in a hard disk by the speed at which the\ndisk arm can move). The number of I/Ooperations per transaction can be reduced by\nstoring more data in memory. If all data are in memory, there will be no disk I/Oexcept\nfor writes. Keeping frequently used data in memory reduces the number of disk I/Os\nand is worth the extra cost of memory. Keeping very infrequently used data in memory\nwould be a waste, since memory is much more expensive than disk.\nThe question is, for a given amount of money available for spending on disks or\nmemory, what is the best way to spend the money to achieve the maximum number of\ntransactions per second? A reduction of one I/Oper second saves:\n(price per disk drive )\u2215(access per second per disk )\nThus, if a particular page is accessed once in mseconds, the saving due to keeping\nit in memory is1\nmtimes the above value. Storing a page in memory costs:\n(price per megabyte of memory )\u2215(pages per megabyte of memory )\nThus, the break-even point is:\n1\nm\u2217price per disk drive\naccess per second per disk=price per megabyte of memory\npages per megabyte of memory\nWe can rearrange the equation and substitute current values for each of the above\nparameters to get a value for m; if a page is accessed more frequently than once in m\nseconds, it is worth buying enough memory to store it.\nAs of 2018, hard-disk technology and memory and disk prices (which we assume\nto be about $50 for a 1-terabyte disk and $80 for 16-gigabytes of memory) give a value\nofmaround 4 hours for 4-kilobytes pages that are randomly accessed; that is, if a page\non hard disk is accessed at least once in 4 hours, it makes sense to purchase enough\nmemory to cache it in memory. Note that if we use larger pages, the time decreases;\nfor example, a page size of 16-kilobytes will lead to a value of mof 1 hour instead of 4\nhours.\n", "1257": "25.1 Performance Tuning 1229\nWith disk and memory cost and speeds as of the 1980/1990s, the corresponding\nvalue was 5 minutes with 4-kilobytes pages. Thus, a widely used rule of thumb, called\nthe\ufb01 v em i n u t er u l e , which said that data should be cached in memory if it is accessed\nmore frequently than once in 5 minutes.\nWith SSD technology and prices as of 2018 (which we assume to be around $500\nfor a 800 gigabytes SSD, which supports 67,000 random reads and 20,000 random\nwrites per second), if we make the same comparison between keeping a page in memory\nversus fetching it from SSD, the time comes to around 7 minutes with 4-kilobyte pages.\nThat is, if a page on SSDis accessed more frequently than once in 7 minutes, it is worth\npurchasing enough memory to cache it in memory.\nFor data that are sequentially accessed, signi\ufb01cantly more pages can be read per\nsecond. Assuming 1 megabyte of data are read at a time, the breakeven point for hard\ndisk currently is about 2.5 minutes. Thus, sequentially accessed data on hard disk\nshould be cached in memory if they are used at least once in 2.5 minutes. For SSDs ,t h e\nbreakeven point is much smaller, at 1.6 seconds. In other words, there is little bene\ufb01t\nin caching sequentially accessed data in memory unless it is very frequently accessed.\nT h ea b o v er u l e so ft h u m bt a k eo n l yt h en u m b e ro f I/Ooperations per second into\naccount and do not consider factors such as response time. Some applications need\nto keep even infrequently used data in memory to support response times that are less\nthan or comparable to disk-access time.\nSince SSD storage is more expensive than disk, one way to get faster random I/O\nfor frequently used data, while paying less for storing less frequently used data, is to\nuse the \ufb02ash-as-bu\ufb00er approach. In this approach, \ufb02ash storage is used as a persistent\nbu\ufb00er, with each block having a permanent location on disk, but stored in \ufb02ash instead\nof being written to disk as long as it is frequently used. When \ufb02ash storage is full, a\nblock that is not frequently used is evicted and \ufb02ushed back to disk if it was updated\nafter being read from disk. Disk subsystems that provide hard disks along with SSDs\nthat act as bu\ufb00ers are commercially available. A rule of thumb for deciding how much\nSSDstorage to purchase is that a 4-kilobyte page should be kept on SSD, instead of hard\ndisk, if it is accessed more frequently than once in a day (the computation is similar\nto the case of caching in main memory versus fetching from disk/ SSD). Note that in\nsuch a setup, the database system cannot control what data reside in which part of the\nstorage.\nIf the storage system allows direct access to SSDs as well as hard disks, the database\nadministrator can control the mapping of relations or indices to disks and allocate\nfrequently used relations/indices to \ufb02ash storage. The tablespace feature, supported by\nmost database systems, can be used to control the mapping by creating a tablespace on\n\ufb02ash storage and assigning desired relations and indices to that tablespace. Controlling\nthe mapping at a \ufb01ner level of granularity than a relation, however, requires changes to\nthe database-system code.\nAnother aspect of tuning is whether to use RAID 1o r RAID 5. The answer depends\non how frequently the data are updated, since RAID 5i sm u c hs l o w e rt h a n RAID 1o n\n", "1258": "1230 Chapter 25 Advanced Application Development\nrandom writes: RAID 5 requires 2 reads and 2 writes to execute a single random write\nrequest. If an application performs rrandom reads and wrandom writes per second to\nsupport a particular throughput rate, a RAID 5 implementation would require r+4w\nI/Ooperations per second, whereas a RAID 1 implementation would require r+2wI/O\noperations per second. We can then calculate the number of disks required to support\nthe required I/Ooperations per second by dividing the result of the calculation by 100\nI/Ooperations per second (for current-generation disks). For many applications, rand\nware large enough that the ( r+w)\u2215100 disks can easily hold two copies of all the\ndata. For such applications, if RAID 1 is used, the required number of disks is actually\nless than the required number of disks if RAID 5i su s e d !T h u s , RAID 5i su s e f u lo n l y\nwhen the data storage requirements are very large, but the update rates, and particularly\nrandom update rates, are small.\n25.1.9 Performance Simulation\nTo test the performance of a database system even before it is installed, we can create\na performance-simulation model of the database system. Each service shown in Figure\n25.1, such as the CPU, each disk, the bu\ufb00er, and the concurrency control, is modeled\nin the simulation. Instead of modeling details of a service, the simulation model may\ncapture only some aspects of each service, such as the service time \u2014that is, the time\ntaken to \ufb01nish processing a request once processing has begun. Thus, the simulation\ncan model a disk access from just the average disk-access time.\nSince requests for a service generally have to wait their turn, each service has an\nassociated queue in the simulation model. A transaction consists of a series of requests.\nThe requests are queued up as they arrive and are serviced according to the policy for\nthat service, such as \ufb01rst come, \ufb01rst served. The models for services such as CPU and\nthe disks conceptually operate in parallel, to account for the fact that these subsystems\noperate in parallel in a real system.\nOnce the simulation model for transaction processing is built, the system adminis-\ntrator can run a number of experiments on it. The administrator can use experiments\nwith simulated transactions arriving at di\ufb00erent rates to \ufb01nd how the system would\nbehave under various load conditions. The administrator could run other experiments\nthat vary the service times for each service to \ufb01nd out how sensitive the performance\nis to each of them. System parameters, too, can be varied, so that performance tuning\nc a nb ed o n eo nt h es i m u l a t i o nm o d e l .\n25.2 Performance Benchmarks\nAs database servers become more standardized, the di\ufb00erentiating factor among the\nproducts of di\ufb00erent vendors is those products\u2019 performance. Performance benchmarks\nare suites of tasks that are used to quantify the performance of software systems.\n", "1259": "25.2 Performance Benchmarks 1231\n25.2.1 Suites of Tasks\nSince most software systems, such as databases, are complex, there is a good deal of\nvariation in their implementation by di\ufb00erent vendors. As a result, there is a signi\ufb01cant\namount of variation in their performance on di\ufb00erent tasks. One system may be the\nmost e\ufb03cient on a particular task; another may be the most e\ufb03cient on a di\ufb00erent task.\nHence, a single task is usually insu\ufb03cient to quantify the performance of the system.\nInstead, the performance of a system is measured by suites of standardized tasks, called\nperformance benchmarks .\nCombining the performance numbers from multiple tasks must be done with care.\nSuppose that we have two tasks, T1andT2, and that we measure the throughput of a\nsystem as the number of transactions of each type that run in a given amount of time\u2014\nsay, 1 second. Suppose that system A runs T1at 99 transactions per second and T2at\n1 transaction per second. Similarly, let system B run both T1andT2at 50 transactions\nper second. Suppose also that a workload has an equal mixture of the two types of\ntransactions.\nIf we took the average of the two pairs of numbers (i.e., 99 and 1, versus 50 and 50),\nit might appear that the two systems have equal performance. However, it is wrong to\ntake the averages in this fashion\u2014if we ran 50 transactions of each type, system Awould\ntake about 50 .5 seconds to \ufb01nish, whereas system Bwould \ufb01nish in just 2 seconds!\nThe example shows that a simple measure of performance is misleading if there is\nmore than one type of transaction. The right way to average out the numbers is to take\nthetime to completion for the workload, rather than the average throughput for each\ntransaction type. We can then compute system performance accurately in transactions\nper second for a speci\ufb01ed workload. Thus, system A takes 50 .5\u2215100, which is 0 .505\nseconds per transaction, whereas system B takes 0 .02 seconds per transaction, on av-\nerage. In terms of throughput, system A runs at an average of 1 .98 transactions per\nsecond, whereas system B runs at 50 transactions per second. Assuming that transac-\ntions of all the types are equally likely, the correct way to average out the throughputs\non di\ufb00erent transaction types is to take the harmonic mean of the throughputs. The\nharmonic mean of nthroughputs t1,t2,\u2026,tnis de\ufb01ned as:\nn\n1\nt1+1\nt2+\u22ef+1\ntn\nFor our example, the harmonic mean for the throughputs in system A is 1 .98. For\nsystem B, it is 50. Thus, system B is approximately 25 times faster than system A on a\nworkload consisting of an equal mixture of the two example types of transactions.\n25.2.2 Database-Application Classes\nOnline transaction processing (OLTP )a n d decision support , including online analytical\nprocessing (OLAP ), are two broad classes of applications handled by database systems.\nThese two classes of tasks have di\ufb00erent requirements. High concurrency and clever\n", "1260": "1232 Chapter 25 Advanced Application Development\ntechniques to speed up commit processing are required for supporting a high rate of\nupdate transactions. On the other hand, good query-evaluation algorithms and query\noptimization are required for decision suppo rt. The architecture of some database sys-\ntems has been tuned to transaction processing; that of others, such as the Teradata\nseries of parallel database systems, has been tuned to decision support. Other vendors\ntry to strike a balance between the two tasks.\nApplications usually have a mixture of transaction-processing and decision-support\nrequirements. Hence, which database system is best for an application depends on what\nmix of the two requirements the application has.\nSuppose that we have throughput numbers for the two classes of applications sepa-\nrately, and the application at hand has a mix of transactions in the two classes. We must\nbe careful even about taking the harmonic mean of the throughput numbers because\nofinterference between the transactions. For example, a long-running decision-support\ntransaction may acquire a number of locks, which may prevent all progress of update\ntransactions. The harmonic mean of throughputs should be used only if the transac-\ntions do not interfere with one another.\n25.2.3 The TPC Benchmarks\nTheTransaction Processing Performance Council (TPC) has de\ufb01ned a series of bench-\nmark standards for database systems.\nThe TPC benchmarks are de\ufb01ned in great detail. They de\ufb01ne the set of relations\nand the sizes of the tuples. They de\ufb01ne the number of tuples in the relations not as\na \ufb01xed number, but rather as a multiple of the number of claimed transactions per\nsecond, to re\ufb02ect that a larger rate of transaction execution is likely to be correlated\nwith a larger number of accounts. The performance metric is throughput, expressed\nastransactions per second (TPS). When its performance is measured, the system must\nprovide a response time within certain bounds, so that a high throughput cannot be\nobtained at the cost of very long response times. Further, for business applications,\ncost is of great importance. Hence, the TPC benchmark also measures performance\nin terms of price per TPS. A large system may have a high number of transactions per\nsecond, but it may be expensive (i.e., have a high price per TPS). Moreover, a company\ncannot claim TPC benchmark numbers for its systems without an external audit that\nensures that the system faithfully follows the de\ufb01nition of the benchmark, including\nfull support for the ACID properties of transactions.\nThe \ufb01rst in the series was the TPC-A benchmark , which was de\ufb01ned in 1989. This\nbenchmark simulates a typical bank application by a single type of transaction that\nmodels cash withdrawal and deposit at a bank teller. The transaction updates several\nrelations\u2014such as the bank balance, the teller\u2019s balance, and the customer\u2019s balance\u2014\nand adds a record to an audit-trail relation. The benchmark also incorporates communi-\ncation with terminals, to model the end-to-end performance of the system realistically.\nThe TPC-B benchmark was designed to test the core performance of the database sys-\ntem, along with the operating system on which the system runs. It removes the parts\n", "1261": "25.2 Performance Benchmarks 1233\nof the TPC-A benchmark that deal with users, communication, and terminals, to focus\non the backend database server. Neither TPC-A norTPC-B is in use today.\nThe TPC-C benchmark was designed to model a more complex system than the\nTPC-A benchmark. The TPC-C benchmark concentrates on the main activities in an\norder-entry environment, such as entering and delivering orders, recording payments,\nchecking status of orders, and monitoring levels of stock. The TPC-C benchmark is still\nwidely used for benchmarking online transaction processing ( OLTP )s y s t e m s .\nT h em o r er e c e n t TPC-E benchmark is also aimed at OLTP systems but is based on\na model of a brokerage \ufb01rm, with customers who interact with the \ufb01rm and generate\ntransactions. The \ufb01rm in turn interacts with \ufb01nancial markets to execute transactions.\nThe TPC-D benchmark was designed to test the performance of database systems\non decision-support queries. Decision-support systems are becoming increasingly im-\nportant today. The TPC-A ,TPC-B ,a n d TPC-C benchmarks measure performance on\ntransaction-processing workloads and should not be used as a measure of performance\non decision-support queries. The DinTPC-D stands for decision support .T h e TPC-D\nbenchmark schema models a sales/distribution application, with parts, suppliers, cus-\ntomers, and orders, along with some auxiliary information. The sizes of the relations are\nde\ufb01ned as a ratio, and database size is the total size of all the relations, expressed in giga-\nbytes. TPC-D at scale factor 1 represents the TPC-D benchmark on a 1-gigabyte database,\nwhile scale factor 10 represents a 10-gigabyte database. The benchmark workload con-\nsists of a set of 17 SQL queries modeling common tasks executed on decision-support\nsystems. Some of the queries make use of complex SQL features, such as aggregation\nand nested queries.\nThe benchmark\u2019s users soon realized that the various TPC-D queries could be sig-\nni\ufb01cantly speeded up by using materialized views and other redundant information.\nThere are applications, such as periodic reporting tasks, where the queries are known\nin advance, and materialized views can be selected carefully to speed up the queries. It\nis necessary, however, to account for the overhead of maintaining materialized views.\nThe TPC-H benchmark (where \u02dd represents ad hoc) is a re\ufb01nement of the TPC-D\nbenchmark. The schema is the same, but there are 22 queries, of which 16 are from\nTPC-D . In addition, there are two updates, a s et of inserts, and a set of deletes. TPC-\nHprohibits materialized views and other redundant information and permits indices\nonly on primary and foreign keys. This benchmark models ad hoc querying where the\nqueries are not known beforehand, so it is not possible to create appropriate material-\nized views ahead of time. A variant, TPC-R (where Rstands for \u201creporting\u201d), which is no\nlonger in use, allowed the use of materialized views and other redundant information.\nThe TPC-DSbenchmark is a follow-up to the TPC-H benchmark and models the\ndecision-support functions of a retail product supplier, including information about\ncustomers, orders, and products, and with multiple sales channels such as retail stores\nand online sales. It has two subparts of the schema, corresponding to ad hoc querying\nand reporting, similar to TPC-H andTPC-R . There is a query workload, as well as a data\nmaintenance workload.\n", "1262": "1234 Chapter 25 Advanced Application Development\nTPC-H and TPC-DS measure performance in this way: The power test runs the\nqueries and updates one at a time sequentially, and 3600 seconds divided by the geomet-\nric mean of the execution times of the queries (in seconds) gives a measure of queries\nper hour. The throughput test runs multiple streams in parallel, with each stream exe-\ncuting all 22 queries. There is also a parallel update stream. Here the total time for the\nentire run is used to compute the number of queries per hour.\nThecomposite query per hour metric , which is the overall metric, is then obtained\nas the square root of the product of the power and throughput metrics. A composite\nprice/performance metric is de\ufb01ned by dividing the system price by the composite met-\nric.\nThere are several other TPC benchmarks, such as a data integration benchmark\n(TPC-DI), benchmarks for big data systems based on Hadoop/Spark ( TPCx-HS ), and\nfor back-end processing of internet-of-things data ( TPCx-IoT ).\n25.3 Other Issues in Application Development\nIn this section, we discuss two issues in applic ation development: testing of applications\nand migration of applications.\n25.3.1 Testing Applications\nT e s t i n go fp r o g r a m si n v o l v e sd e s i g n i n ga test suite , that is, a collection of test cases.\nTesting is not a one-time process, since programs evolve continuously, and bugs may\nappear as an unintended consequence of a change in the program; such a bug is referred\nto as program regression . Thus, after every change to a program, the program must be\ntested again. It is usually infeasible to have a human perform tests after every change to\na program. Instead, expected test outputs are stored with each test case in a test suite.\nRegression testing involves running the program on each test case in a test suite and\nchecking that the program generates the expected test output.\nIn the context of database applications, a test case consists of two parts: a database\nstate and an input to a speci\ufb01c interface of the application.\nSQLqueries can have subtle bugs that can be di\ufb03cult to catch. For example, a query\nmay execute a join when it should have performed an outer join (i.e., r\u22c8s,w h e ni t\nshould have actually performed r\u27d5s). The di\ufb00erence between these two queries would\nbe found only if the test database had an rtuple with no matching stuple. Thus, it is\nimportant to create test databases that can catch commonly occurring errors. Such\nerrors are referred to as mutations , since they are usually small changes to a query\n(or program). A test case that produces di\ufb00erent outputs on an intended query and a\nmutant of the query is said to kill the mutant . A test suite should have test cases that\nkill (most) commonly occurring mutants.\nIf a test case performs an update on the database, to check that it executed properly\none must verify that the contents of the database match the expected contents. Thus,\n", "1263": "25.3 Other Issues in Application Development 1235\nthe expected output consists not only of data displayed on the user\u2019s screen, but also\n(updates to) the database state.\nSince the database state can be rather large, multiple test cases would share a com-\nmon database state. Testing is complicated by the fact that if a test case performs an\nupdate on the database, the results of other test cases run subsequently on the same\ndatabase may not match the expected results. The other test cases would then be erro-\nneously reported as having failed. To avoid this problem, whenever a test case performs\nan update, the database state must be restored to its original state after running the test.\nTesting can also be used to ensure that an application meets performance require-\nments. To carry out such performance testing , the test database must be of the same\nsize as the real database would be. In some cases, there is already existing data on\nwhich performance testing can be carried out. In other cases, a test database of the\nrequired size must be generated; there are several tools available for generating such\ntest databases. These tools ensure that the generated data satisfy constraints such as\nprimary- and foreign-key constraints. They may additionally generate data that look\nmeaningful, for example, by populating a name attribute using meaningful names in-\nstead of random strings. Some tools also allow data distributions to be speci\ufb01ed; for\nexample, a university database may require a distribution with most students in the\nrange of 18 to 25 years and most faculty in the range of 25 to 65 years.\nEven if there is an existing database, organizations usually do not want to reveal\nsensitive data to an external organization that may be carrying out the performance\ntests. In such a situation, a copy of the real database may be made, and the values\nin the copy may be modi\ufb01ed in such a way that any sensitive data, such as credit-card\nnumbers, social security numbers, or dates of birth, are obfuscated . Obfuscation is done\nin most cases by replacing a real value with a randomly generated value (taking care to\nalso update all references to that value, in case the value is a primary key). On the other\nhand, if the application execution depends on the value, such as the date of birth in an\napplication that performs di\ufb00erent actions based on the date of birth, obfuscation may\nmake small random changes in the value instead of replacing it completely.\n25.3.2 Application Migration\nLegacy systems are older-generation application systems that are still in use despite\nbeing obsolete. They continue in use due to the cost and risk in replacing them. For\nexample, many organizations developed applications in-house, but they may decide to\nreplace them with a commercial product. In some cases, a legacy system may use old\ntechnology that is incompatible with current-generation standards and systems. Some\nlegacy systems in operation today are several decades old and are based on technologies\nsuch as databases that use the network or hierarchical data models, or use Cobol and\n\ufb01le systems without a database. Such systems may still contain valuable data and may\nsupport critical applications.\nReplacing legacy applications with new applications is often costly in terms of both\ntime and money, since they are often very large, consisting of millions of lines of code\n", "1264": "1236 Chapter 25 Advanced Application Development\ndeveloped by teams of programmers, often over several decades. They contain large\namounts of data that must be ported to the new application, which may use a com-\npletely di\ufb00erent schema. Switchover from an old to a new application involves retrain-\ning large numbers of sta\ufb00. Switchover must usually be done without any disruption,\nwith data entered in the old system available through the new system as well.\nMany organizations attempt to avoid replacing legacy systems and instead try to\ninteroperate them with newer systems. One approach used to interoperate between\nrelational databases and legacy databases is to build a layer, called a wrapper , on top of\nthe legacy systems that can make the legacy system appear to be a relational database.\nThe wrapper may provide support for ODBC or other interconnection standards such\nasOLE-DB , which can be used to query and update the legacy system. The wrapper is\nresponsible for converting relational queries and updates into queries and updates on\nthe legacy system.\nWhen an organization decides to replace a legacy system with a new system, it may\nfollow a process called reverse engineering , which consists of going over the code of the\nlegacy system to come up with schema designs in the required data model (such as an\nE-Rmodel or an object-oriented data model). Reverse engineering also examines the\ncode to \ufb01nd out what procedures and processes were implemented, in order to get a\nhigh-level model of the system. Reverse engineering is needed because legacy systems\nusually do not have high-level documentation of their schema and overall system design.\nWhen coming up with a new system, developers review the design so that it can be\nimproved rather than just reimplemented as is. Extensive coding is required to support\nall the functionality (such as user interface and reporting systems) that was provided\nby the legacy system. The overall process is called re-engineering .\nWhen a new system has been built and tested, the system must be populated with\ndata from the legacy system, and all further activities must be carried out on the new\nsystem. However, abruptly transitioning to a new system, which is called the big-bang\napproach , carries several risks. First, users may not be familiar with the interfaces of the\nnew system. Second, there may be bugs or performance problems in the new system\nthat were not discovered when it was tested. Such problems may lead to great losses\nfor companies, since their ability to carry out critical transactions such as sales and\npurchases may be severely a\ufb00ected. In some extreme cases the new system has even\nbeen abandoned, and the legacy system reused, after an attempted switchover failed.\nAn alternative approach, called the chicken-little approach , incrementally re-\nplaces the functionality of the legacy system. For example, the new user inter-\nfaces may be used with the old system in the back end, or vice versa. Another\noption is to use the new system only for some functionality that can be decou-\npled from the legacy system. In either case, the legacy and new systems coex-\nist for some time. There is therefore a need for developing and using wrappers\non the legacy system to provide required functionality to interoperate with the new\nsystem. This approach therefore has a higher development cost.\n", "1265": "25.4 Standardization 1237\n25.4 Standardization\nStandards de\ufb01ne the interface of a software system. For example, standards de\ufb01ne the\nsyntax and semantics of a programming language, or the functions in an application-\nprogram interface, or even a data model (such as the object-oriented database stan-\ndards). Today, database systems are complex, and they are often made up of multiple\nindependently created parts that need to interact. For example, client programs may\nbe created independently of backend systems, but the two must be able to interact with\neach other. A company that has multiple heterogeneous database systems may need to\nexchange data between the databases. Given such a scenario, standards play an impor-\ntant role.\nFormal standards are those developed by a standards organization or by industry\ngroups through a public process. Dominant products sometimes become de facto stan-\ndards , in that they become generally accepted as standards without any formal process\nof recognition. Some formal standards, like many aspects of the SQL-92 and SQL:1999\nstandards, are anticipatory standards that lead the marketplace; they de\ufb01ne features\nthat vendors then implement in products. In other cases, the standards, or parts of\nthe standards, are reactionary standards , in that they attempt to standardize features\nthat some vendors have already implemented, and that may even have become de facto\nstandards. SQL-89 was in many ways reactionary, since it standardized features, such\nas integrity checking, that were already present in the IBM SAA SQL standard and in\nother databases.\nFormal standards committees are typically composed of representatives of the ven-\ndors and of members from user groups and standards organizations such as the Inter-\nnational Organization for Standardization ( ISO) or the American National Standards\nInstitute ( ANSI ), or professional bodies, such as the Institute of Electrical and Electron-\nics Engineers ( IEEE ). Formal standards committees meet periodically, and members\npresent proposals for features to be added to or modi\ufb01ed in the standard. After a (usu-\nally extended) period of discussion, modi\ufb01cations to the proposal, and public review,\nmembers vote on whether to accept or reject a feature. Some time after a standard has\nbeen de\ufb01ned and implemented, its shortcomings become clear and new requirements\nbecome apparent. The process of updating the standard then begins, and a new version\nof the standard is usually released after a few years. This cycle usually repeats every few\nyears, until eventually (perhaps many years later) the standard becomes technologically\nirrelevant or loses its user base.\nThis section gives a very high-level overview of di\ufb00erent standards, concentrating\non the goals of the standard. Detailed descriptions of the standards mentioned in this\nsection appear in the bibliographic notes for this chapter, available online.\n25.4.1 SQL Standards\nSince SQL is the most widely used query language, much work has been done on stan-\ndardizing it. ANSI and ISO, with the various database vendors, have played a leading\n", "1266": "1238 Chapter 25 Advanced Application Development\nrole in this work. The SQL-86 standard was the initial version. The IBM Systems Appli-\ncation Architecture ( SAA)s t a n d a r df o r SQL was released in 1987. As people identi\ufb01ed\nthe need for more features, updated versions of the formal SQL standard were devel-\noped, called SQL-89 and SQL-92 .\nThe SQL:1999 version of the SQL standard added a variety of features to SQL.W e\nhave seen many of these features in earlier chapters.\nSubsequent versions of the SQL standard include the following:\n\u2022SQL:2003 , which is a minor extension of the SQL:1999 standard. Some features\nsuch as the SQL:1999 OLAP features (Section 11.3.3) were speci\ufb01ed as an amend-\nment to the earlier version of the SQL:1999 standard, instead of waiting for the\nrelease of SQL:2003 .\n\u2022SQL:2006 , which added several features related to XML .\n\u2022SQL:2008 , which introduced only minor extensions to the SQL language such as\nextensions to the merge clause.\n\u2022SQL:2011 , which added a number of temporal extensions to SQL, including the\nability to associate time periods with tuples , optionally using existing columns as\nstart and end times, and primary key de\ufb01nitions based on the time periods. The\nextensions support deletes and updates with associated periods; such deletes and\nupdates may result in modi\ufb01cation of the time period of existing tuples, along with\ndeletes or inserts of new tuples. A number of operators related to time periods,\nsuch as overlaps andcontains ,w e r ea l s oi n t r o d u c e di n SQL:2011 .\nIn addition, the standard provided a number of other features, such as further\nextensions to the merge construct, extensions to the window constructs that were\nintroduced in earlier versions of SQL, and extensions to limit the number of results\nfetched by a query, using a fetch clause.\n\u2022SQL:2016 , which added a number of features related to JSON support, and support\nfor the aggregate operation listagg , which concatenates attributes from a group of\ntuples into a string.\nIt is worth mentioning that most of the new features are supported by only a few\ndatabase systems, and conversely most database systems support a number of features\nthat are not part of the standard.\n25.4.2 Database Connectivity Standards\nThe ODBC standard is a widely used standard for communication between client appli-\ncations and database systems and de\ufb01nes APIsi ns e v e r a ll a n g u a g e s .T h e JDBC standard\nfor communication between Java applications and databases was modeled on ODBC\nand provides similar functionality.\nODBC is based on the SQL Call Level Interface (CLI) standards developed by the\nX/Open industry consortium and the SQL Access Group, but it has several extensions.\n", "1267": "25.4 Standardization 1239\nThe ODBC API de\ufb01nes a CLI,a n SQL syntax de\ufb01nition, and rules about permissible\nsequences of CLIcalls. The standard also de\ufb01nes conformance levels for the CLIand\ntheSQL syntax. For example, the core level of the CLIhas commands to connect to a\ndatabase, to prepare and execute SQL statements, to get back results or status values,\nand to manage transactions. The next level of conformance (level 1) requires support\nfor catalog information retrieval and some other features over and above the core-level\nCLI; level 2 requires further features, such as the ability to send and retrieve arrays of\nparameter values and to retrieve more detailed catalog information.\nODBC allows a client to connect simultaneously to multiple data sources and to\nswitch among them, but transactions on each are independent; ODBC does not support\ntwo-phase commit.\nA distributed system provides a more general environment than a client\u2013\nserver system. The X/Open consortium has also developed the X/Open XAstandards\nfor interoperation of databases. These standards de\ufb01ne transaction-management primi-\ntives (such as transaction begin, commit, abort, and prepare-to-commit) that compliant\ndatabases should provide; a transaction manager can invoke these primitives to imple-\nment distributed transactions by two-phase commit. The XAstandards are independent\nof the data model and of the speci\ufb01c interfaces between clients and databases to ex-\nchange data. Thus, we can use the XAprotocols to implement a distributed transaction\nsystem in which a single transaction can access relational as well as object-oriented\ndatabases, yet the transaction manager ensures global consistency via two-phase com-\nmit.\nThere are many data sources that are not relational databases, and in fact may\nnot be databases at all. Examples are \ufb02at \ufb01les and email stores. Microsoft\u2019s OLE-DB\nis a C++ APIwith goals similar to ODBC , but for nondatabase data sources that may\nprovide only limited querying and update facilities. Just like ODBC ,OLE-DB provides\nconstructs for connecting to a data source, starting a session, executing commands,\nand getting back results in the form of a rowset, which is a set of result rows.\nTheActiveX Data Objects (ADO )a n d ADO.NET APIs, created by Microsoft, provide\nan interface to access data from not only relational databases, but also some other types\nof data sources, such as OLE-DB data sources.\n25.4.3 Object Database Standards\nStandards in the area of object-oriented databases ( OODB ) have so far been driven\nprimarily by OODB vendors. The Object Database Management Group (ODMG )w a sa\ngroup formed by OODB vendors to standardize the data model and language interfaces\ntoOODB s.ODMG is no longer active. JDO is a standard for adding persistence to Java.\nThere were several other attempts to standardize object databases and related\nobject-based technologies such as services. However, most were not widely adopted,\nand they are rarely used anymore.\nObject-relational mapping technologies, which store data in relational databases at\nthe back end but provide programmers with an object-based APIto access and manip-\n", "1268": "1240 Chapter 25 Advanced Application Development\nulate data, have proven quite popular. Systems that support object-relational mapping\ninclude Hibernate, which supports Java, and the data layer of the popular Django Web\nframework, which is based on the Python programming language. However, there are\nno widely accepted formal standards in this area.\n25.5 Distributed Directory Systems\nConsider an organization that wishes to make data about its employees available to\na variety of people in the organization; examples of the kinds of data include name,\ndesignation, employee-id, address, email address, phone number, fax number, and so\non. Such data are often shared via directories, which allow users to browse and search\nfor desired information.\nIn general, a directory is a listing of information about some class of objects such\nas persons. Directories can be used to \ufb01nd information about a speci\ufb01c object, or in\nthe reverse direction to \ufb01nd objects that meet a certain requirement.\nA major application of directories today is to authenticate users: applications can\ncollect authentication information such as passwords from users and authenticate them\nusing the directory. Details about the user category (e.g., is the user a student or an\ninstructor), as well as authorizations that a user has been given, may also be shared\nthrough a directory. Multiple applicatio ns in an organization can then authenticate\nusers using a common directory service and user category and authorization informa-\ntion from the directory to provide users only with data that they are authorized to see.\nDirectories can be used for storing other types of information, much like \ufb01le sys-\ntem directories. For instance, web browsers can store personal bookmarks and other\nbrowser settings in a directory system. A user can thus access the same settings from\nmultiple locations, such as at home and at work, without having to share a \ufb01le system.\n25.5.1 Directory Access Protocols\nDirectory information can be made available through web interfaces, as many organi-\nzations, and phone companies in particular, do. Such interfaces are good for humans.\nHowever, programs too need to access directory information.\nSeveral directory access protocols have been developed to provide a standardized\nway of accessing data in a directory. The most widely used among them today is the\nLightweight Directory Access Protocol (LDAP ).\nAll the types of data in our examples can be stored without much trouble in a\ndatabase system and accessed through protocols such as JDBC orODBC .T h eq u e s t i o n\nthen is, why come up with a specialized protocol for accessing directory information?\nThere are at least two answers to the question.\n\u2022First, directory access protocols are si mpli\ufb01ed protocols that cater to a limited\ntype of access to data. They evolved in parallel with the database access protocols.\n", "1269": "25.5 Distributed Directory Systems 1241\n\u2022Second, and more important, directory systems provide a simple mechanism to\nname objects in a hierarchical fashion, similar to \ufb01le system directory names,\nwhich can be used in a distributed directory system to specify what information is\nstored in each of the directory servers. For example, a particular directory server\nmay store information for Bell Laboratories employees in Murray Hill, while an-\nother may store information for Bell Labor atories employees in Bangalore, giving\nboth sites autonomy in controlling their local data. The directory access protocol\ncan be used to obtain data from both directories across a network. More impor-\ntant, the directory system can be set up to automatically forward queries made at\none site to the other site, without user intervention.\nFor these reasons, several organizations have directory systems to make organiza-\ntional information available online through a directory access protocol. Information\nin an organizational directory can be used for a variety of purposes, such as to \ufb01nd\naddresses, phone numbers, or email addresses of people, to \ufb01nd which departments\npeople are in, and to track department hierarchies.\nAs may be expected, several directory implementations \ufb01nd it bene\ufb01cial to use\nrelational databases to store data instead of creating special-purpose storage systems.\n25.5.2 LDAP: Lightweight Directory Access Protocol\nIn general a directory system is implemented as one or more servers, which service mul-\ntiple clients. Clients use the APIde\ufb01ned by the directory system to communicate with\nthe directory servers. Directory access protocols also de\ufb01ne a data model and access\ncontrol. The X.500 directory access protocol , de\ufb01ned by the International Organization\nfor Standardization ( ISO), is a standard for accessing directory information. However,\nthe protocol is rather complex and is not widely used. The Lightweight Directory Access\nProtocol (LDAP ) provides many of the X.500 features, but with less complexity, and is\nwidely used. In addition to several open-sour ce implementations, the Microsoft Active\nDirectory system, which is based on LDAP , is used in a large number of organizations.\nIn the rest of this section, we shall outline the data model and access protocol\ndetails of LDAP .\n25.5.2.1 LDAP Data Model\nInLDAP , directories store entries , which are similar to objects. Each entry must have a\ndistinguished name (DN), which uniquely identi\ufb01es the entry. A DNis in turn made up\nof a sequence of relative distinguished names (RDNs). For example, an entry may have\nthe following distinguished name:\ncn=Silberschatz, ou=Computer Science, o=Yale University, c= USA\nAs you can see, the distinguished name in this example is a combination of a name and\n(organizational) address, starting with a person\u2019s name, then giving the organizational\nunit ( ou), the organization ( o), and country ( c) .T h eo r d e ro ft h ec o m p o n e n t so fa\ndistinguished name re\ufb02ects the normal postal address order, rather than the reverse\n", "1270": "1242 Chapter 25 Advanced Application Development\norder used in specifying path names for \ufb01les. The set of RDN sf o ra DNis de\ufb01ned by\nthe schema of the directory system.\nEntries can also have attributes. LDAP provides binary, string, and time types,\nand additionally the types telfor telephone numbers, and PostalAddress for addresses\n(lines separated by a \u201c$\u201d character). Unlike those in the relational model, attributes\nare multivalued by default, so it is possible to store multiple telephone numbers or\naddresses for an entry.\nLDAP allows the de\ufb01nition of object classes with attribute names and types. Inher-\nitance can be used in de\ufb01ning object classes. Moreover, entries can be speci\ufb01ed to be\nof one or more object classes. It is not necessary that there be a single most-speci\ufb01c\nobject class to which an entry belongs.\nEntries are organized into a directory information tree (DIT), according to their dis-\ntinguished names. Entries at the leaf level of the tree usually represent speci\ufb01c objects.\nEntries that are internal nodes represent objects such as organizational units, organi-\nzations, or countries. The children of a node have a DNcontaining all the RDN so ft h e\nparent, and one or more additional RDN s. For instance, an internal node may have a\nDNc=USA, and all entries below it have the value USAfor the RDN c.\nThe entire distinguished name need not be stored in an entry. The system can\ngenerate the distinguished name of an entry by traversing up the DITfrom the entry,\ncollecting the RDN=value components to create the full distinguished name.\nEntries may have more than one distinguished name\u2014for example, an entry for a\nperson in more than one organization. To deal with such cases, the leaf level of a DIT\ncan be an alias that points to an entry in another branch of the tree.\n25.5.2.2 Data Manipulation\nUnlike SQL,LDAP does not de\ufb01ne either a data-de\ufb01nition language or a data-\nmanipulation language. However, LDAP de\ufb01nes a network protocol for carrying out\ndata de\ufb01nition and manipulation. Users of LDAP can either use an application-\nprogramming interface or use tools provided by various vendors to perform data de\ufb01ni-\ntion and manipulation. LDAP also de\ufb01nes a \ufb01le format called LDAP Data Interchange\nFormat (LDIF ) that can be used for storing and exchanging information.\nThe querying mechanism in LDAP is very simple, consisting of just selections and\nprojections, without any join. A query must specify the following:\n\u2022A base\u2014that is, a node within a DIT\u2014by giving its distinguished name (the path\nfrom the root to the node).\n\u2022A search condition, which can be a Boolean combination of conditions on in-\ndividual attributes. Equality, matching by wild-card characters, and approximate\nequality (the exact de\ufb01nition of approximate equality is system dependent) are\nsupported.\n\u2022A scope, which can be just the base, the base and its children, or the entire subtree\nbeneath the base.\n", "1271": "25.6 Summary 1243\n\u2022Attributes to return.\n\u2022Limits on number of results and resource consumption.\nThe query can also specify whether to automatically dereference aliases; if alias deref-\nerences are turned o\ufb00, alias entries can be returned as answers.\nWe omit further details of query support in LDAP but note that LDAP implemen-\ntations support an APIfor querying/updating LDAP data and may additionally support\nweb services for querying LDAP data.\n25.5.2.3 Distributed Directory Trees\nInformation about an organization may be split into multiple DITs, each of which stores\ninformation about some entries. The su\ufb03x of a DITis a sequence of RDN=value pairs\nthat identify what information the DITstores; the pairs are concatenated to the rest of\nthe distinguished name generated by traversing from the entry to the root. For instance,\nthe su\ufb03x of a DITmay be o=Nokia, c= USA, while another may have the su\ufb03x o=Nokia,\nc=India .T h e DITs may be organizationally and geographically separated.\nAn o d ei na DITmay contain a referral to another node in another DIT;f o ri n s t a n c e ,\nthe organizational unit Bell Labs under o=Nokia, c= USAmay have its own DIT,i nw h i c h\ncase the DITforo=Nokia, c= USAwould have a node ou=BellLabsrepresenting a referral\nto the DITforBellLabs.\nReferrals are the key component that help organize a distributed collection of di-\nrectories into an integrated system. When a server gets a query on a DIT,i tm a yr e t u r n\na referral to the client, which then issues a query on the referenced DIT. Access to the\nreferenced DITis transparent, proceeding without the user\u2019s knowledge. Alternatively,\nthe server itself may issue the query to the referred DITand return the results along\nwith locally computed results.\nThe hierarchical naming mechanism used by LDAP helps break up control of in-\nformation across parts of an organization. The referral facility then helps integrate all\nthe directories in an organization into a single virtual directory.\nAlthough it is not an LDAP requirement, organizations often choose to break up in-\nformation either by geography (for instance, an organization may maintain a directory\nfor each site where the organization has a large presence) or by organizational structure\n(for instance, each organizational unit, such as department, maintains its own direc-\ntory). Many LDAP implementations support master\u2013slave and multimaster replication\nofDITs.\n25.6 Summary\n\u2022Tuning of the database-system parameters, as well as the higher-level database de-\nsign\u2014such as the schema, indices, and transactions\u2014is important for good perfor-\nmance. Tuning is best done by identifying bottlenecks and eliminating them.\n", "1272": "1244 Chapter 25 Advanced Application Development\n\u2022Database tuning can be done at the level of schema and queries, at the level of\ndatabase system parameters, and at the level of hardware. Database systems usually\nhave a variety of tunable parameters, such as bu\ufb00er sizes.\n\u2022The right choice of indices and materialized views, and the use of horizontal par-\ntitioning can provide signi\ufb01cant performance bene\ufb01ts. Tools for automated tuning\nbased on workload history can help signi\ufb01cantly in such tuning. The set of indices\nand materialized views can be appropriately chosen to minimize overall cost. Ver-\ntical partitioning, and columnar storage can lead to signi\ufb01cant bene\ufb01ts in online\nanalytical processing systems.\n\u2022Transactions can be tuned to minimize lock contention; snapshot isolation and\nsequence numbering facilities supporting early lock release are useful tools for\nreducing read-write and write-write contention.\n\u2022Hardware tuning includes choice of memory size, the use of SSDs versus magnetic\nhard disks, and increasingly, the number of CPU cores.\n\u2022Performance benchmarks play an important role in comparisons of database sys-\ntems, especially as systems become more standards compliant. The TPC bench-\nmark suites are widely used, and the di\ufb00erent TPC benchmarks are useful for com-\nparing the performance of databases under di\ufb00erent workloads.\n\u2022Applications need to be tested extensively as they are developed and before they\nare deployed. Testing is used to catch errors as well as to ensure that performance\ngoals are met.\n\u2022Legacy systems are systems based on older-generation technologies such as nonre-\nlational databases or even directly on \ufb01le systems. Interfacing legacy systems with\nnew-generation systems is often important when they run mission-critical systems.\nMigrating from legacy systems to new-generation systems must be done carefully\nto avoid disruptions, which can be very expensive.\n\u2022Standards are important because of the complexity of database systems and their\nneed for interoperation. Formal standards exist for SQL. De facto standards, such\nasODBC and JDBC , and standards adopted by industry groups have played an\nimportant role in the growth of client\u2013server database systems.\n\u2022Distributed directory systems have played an important role in many applications,\nand can be viewed as distributed databases. LDAP is widely used for authentication\nand for tracking employee information in organizations.\nReview Terms\n\u2022Performance tuning\n\u2022Bottlenecks\u2022Queueing systems\n\u2022Tuning of physical schema\n", "1273": "Practice Exercises 1245\n\u2022Tuning of indices\n\u2022Materialized views\n\u2022Immediate view maintenance\n\u2022Deferred view maintenance\n\u2022Tuning of physical design\n\u2022Workload\n\u2022Tuning of queries\n\u2022Set orientation\n\u2022Batch update ( JDBC )\n\u2022Bulk load\n\u2022Bulk update\n\u2022Merge statement\n\u2022Tuning of logical schema\n\u2022Tunable parameters\n\u2022Tuning of concurrent transactions\n\u2022Sequences\n\u2022Minibatch transactions\n\u2022Tuning of hardware\n\u2022Five minute rule\n\u2022Performance simulation\n\u2022Performance benchmarks\n\u2022Service time\n\u2022Throughput\n\u2022Database-application classes\n\u2022OLTP\n\u2022Decision support\u2022The TPC benchmarks\n\u00b0TPC-C\n\u00b0TPC-D\n\u00b0TPC-E\n\u00b0TPC-H\n\u00b0TPC-DS\n\u2022Regression testing\n\u2022Killing mutants\n\u2022Application migration\n\u2022Legacy systems\n\u2022Reverse engineering\n\u2022Re-engineering\n\u2022Standardization\n\u00b0Formal standards\n\u00b0De facto standards\n\u00b0Anticipatory standards\n\u00b0Reactionary standards\n\u2022Database connectivity standards\n\u2022X/Open XAstandards\n\u2022Object database standards\n\u2022XML-based standards\n\u2022LDAP\n\u2022Directory information tree\n\u2022Distributed directory trees\nPractice Exercises\n25.1 Find out all performance information your favorite database system provides.\nLook for at least the following: what queries are currently executing or exe-\ncuted recently, what resources each of them consumed ( CPU and I/O), what\nfraction of page requests resulted in bu\ufb00 er misses (for each query, if available),\nand what locks have a high degree of contention. Also get information about\nCPU,I/Oand network utilization, including the number of open network con-\nnections using your operating system utilities.\n", "1274": "1246 Chapter 25 Advanced Application Development\n25.2 Many applications need to generate sequence numbers for each transaction.\na. If a sequence counter is locked in two-phase manner, it can become a\nconcurrency bottleneck. Explain why this may be the case.\nb. Many database systems support built-in sequence counters that are not\nlocked in two-phase manner; when a transaction requests a sequence\nnumber, the counter is locked, incremented and unlocked.\ni. Explain how such counters can improve concurrency.\nii. Explain why there may be gaps in the sequence numbers belonging\nto the \ufb01nal set of committed transactions.\n25.3 Suppose you are given a relation r(a,b,c).\na. Give an example of a situation under which the performance of equal-\nity selection queries on attribute acan be greatly a\ufb00ected by how ris\nclustered.\nb. Suppose you also had range selection queries on attribute b.C a ny o u\ncluster rin such a way that the equality selection queries on r.aand the\nrange selection queries on r.bcan both be answered e\ufb03ciently? Explain\nyour answer.\nc. If clustering as above is not possible, suggest how both types of queries\ncan be executed e\ufb03ciently by choosing appropriate indices.\n25.4 When a large number of records are inserted into a relation in a short period\nof time, it is often recommended that all indices be dropped, and recreated\nafter the inserts have been completed.\na. What is the motivation for this recommendation?\nb. Dropping and recreation of indices can be avoided by bulk-updating of\nthe indices. Suggest how this could be done e\ufb03ciently for B+-tree indices.\nc. If the indices were write-optimized indices such as LSM trees, would this\nadvice be meaningful?\n25.5 Suppose that a database application does not appear to have a single bottle-\nneck; that is, CPU and disk utilization are both high, and all database queues\nare roughly balanced. Does that mean the application cannot be tuned further?\nExplain your answer.\n25.6 Suppose a system runs three types of transactions. Transactions of type A run\nat the rate of 50 per second, transactions of type B run at 100 per second, and\ntransactions of type C run at 200 per second. Suppose the mix of transactions\nhas 25 percent of type A, 25 percent of type B, and 50 percent of type C.\n", "1275": "Exercises 1247\na. What is the average transaction throughput of the system, assuming\nthere is no interference between the transactions?\nb. What factors may result in interference between the transactions of dif-\nferent types, leading to the calculated throughput being incorrect?\n25.7 Suppose an application programmer was supposed to write a query\nselect *\nfrom rnatural left outer join s;\non relations r(A,B)a n d s(B,C), but instead wrote the query\nselect *\nfrom rnatural join s;\na. Give sample data for randson which both queries would give the same\nresult.\nb. Give sample data for randswhere the two queries would give di\ufb00erent\nresults, thereby exposing the error in the query,\n25.8 List some bene\ufb01ts and drawbacks of an anticipatory standard compared to a\nreactionary standard.\n25.9 Describe how LDAP can be used to provide multiple hierarchical views of data,\nwithout replicating the base-level data.\nExercises\n25.10 Database tunning:\na. What are the three broad levels at which a database system can be tuned\nto improve performance?\nb. Give two examples of how tuning can be done for each of the levels.\n25.11 When carrying out performance tuning, should you try to tune your hardware\n(by adding disks or memory) \ufb01rst, or should you try to tune your transactions\n(by adding indices or materialized views) \ufb01rst. Explain your answer.\n25.12 Suppose that your application has transactions that each access and update\na single tuple in a very large relation stored in a B+-tree \ufb01le organization. As-\nsume that all internal nodes of the B+-tree are in memory, but only a very small\nfraction of the leaf pages can \ufb01t in memory. Explain how to calculate the min-\nimum number of disks required to support a workload of 1000 transactions\n", "1276": "1248 Chapter 25 Advanced Application Development\nper second. Also calculate the required number of disks, using values for disk\np a r a m e t e r sg i v e ni nS e c t i o n1 2 . 3 .\n25.13 What is the motivation for splitting a long transaction into a series of small\nones? What problems could arise as a result, and how can these problems be\naverted?\n25.14 Suppose the price of memory falls by half, and the speed of disk access (num-\nber of accesses per second) doubles, while all other factors remain the same.\nWhat would be the e\ufb00ect of this change on the 5-minute and 1-minute rule?\n25.15 List at least four features of the TPC benchmarks that help make them realistic\nand dependable measures.\n25.16 Why was the TPC-D benchmark replaced by the TPC-H and TPC-R bench-\nmarks?\n25.17 Explain what application characteristics would help you decide which of TPC-\nC,TPC-H ,o rTPC-R best models the application.\n25.18 Given that the LDAP functionality can be implemented on top of a database\nsystem, what is the need for the LDAP standard?\nFurther Reading\n[Harchol-Balte (2013)] provides textbook coverage of queuing theory from a computer\nscience perspective.\nInformation about tuning support in IBM DB2, Oracle and Microsoft SQL S erver\nmay be found in their respective manuals online, as well as in numerous books. [Shasha\nand Bonnet (2002)] provides detailed coverage of database tuning principles. [O\u2019Neil\nand O\u2019Neil (2000)] provides a very good textbook coverage of performance measure-\nment and tuning. The 5-minute and 1-minute rules are described in [Gray and Graefe\n(1997)], [Graefe (2008)], and [Appuswamy et al. (2017)].\nAn early proposal for a database-system benchmark (the Wisconsin benchmark)\nwas made by [Bitton et al. (1983)]. The TPC-A ,TPC-B ,a n d TPC-C benchmarks are\ndescribed in [Gray (1991)]. An online version of all the TPC benchmark descrip-\ntions, as well as benchmark results, is available on the World Wide Web at the URL\nwww.tpc.org ; the site also contains up-to-date information about new benchmark pro-\nposals.\nThe XData system ( www.cse.iitb.ac.in/infolab/xdata ) provides tools for generat-\ni n gt e s td a t at oc a t c he r r o r si n SQL queries, as well as for grading student SQL queries.\nA number of standards documents, including several parts\nof the SQL standard, can be found on the ISO/IEC website\n(standards.iso.org/ittf/PubliclyAvailableStandards/index.html ). Information\nabout ODBC ,OLE-DB ,ADO ,a n d ADO.NET can be found on the web site\n", "1277": "Further Reading 1249\nwww.microsoft.com/data . A wealth of information on XML -based standards\nand tools is available online on the web site www.w3c.org .\nBibliography\n[Appuswamy et al. (2017)] R. Appuswamy, R. Borovica, G. Graefe, and A. Ailamaki, \u201cThe\nFive minute Rule Thirty Years Later and its Impact on the Storage Hierarchy\u201d, In Proceedings\nof the 7th International Workshop on Accelerating Analytics and Data Management Systems\nUsing Modern Processor and Storage Architectures (2017).\n[Bitton et al. (1983)] D. Bitton, D. J. DeWitt, and C. Turby\ufb01ll, \u201cBenchmarking Database\nSystems: A Systematic Approach\u201d, In Proc. of the International Conf. on Very Large Databases\n(1983), pages 8\u201319.\n[Graefe (2008)] G. Graefe, \u201cThe Five-Minute Rule 20 Years Later: and How Flash Memory\nChanges the Rules\u201d, ACM Queue , Volume 6, Number 4 (2008), pages 40\u201352.\n[Gray (1991)] J. Gray, The Benchmark Handbook for Database and Transaction Processing\nSystems , 2nd edition, Morgan Kaufmann (1991).\n[Gray and Graefe (1997)] J. Gray and G. Graefe, \u201cThe Five-Minute Rule Ten Years Later,\nand Other Computer Storage Rules of Thumb\u201d, SIGMOD Record , Volume 26, Number 4\n(1997), pages 63\u201368.\n[Harchol-Balte (2013)] M. Harchol-Balte, Performance Modeling and Design of Computer Sys-\ntems: Queueing Theory in Action , Cambridge University Press (2013).\n[O\u2019Neil and O\u2019Neil (2000)] P. O\u2019Neil and E. O\u2019Neil, Database: Principles, Programming, Per-\nformance , 2nd edition, Morgan Kaufmann (2000).\n[Shasha and Bonnet (2002)] D. Shasha and P. Bonnet, Database Tuning: Principles, Experi-\nments, and Troubleshooting Techniques , Morgan Kaufmann (2002).\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "1278": "", "1279": "CHAPTER26\nBlockchain Databases\nAt the most basic level, a blockchain provides an alternative data format for storing a\ndatabase, and its paradigm for transaction processing enables a high level of decentral-\nization.\nA major application of blockchain technology is in the creation of digital ledgers .\nA ledger in the \ufb01nancial world is a book of \ufb01nancial accounts, that keeps track of trans-\nactions. For example, each time you deposit or withdraw money from your account, an\nentry is added to a ledger maintained by the bank. Since the ledger is maintained by\nthe bank, a customer of the bank implicitly trusts the bank to not cheat by adding unau-\nthorized transactions to the ledger, such as an unauthorized withdrawal, or modifying\nthe ledger by deleting transactions such as a deposit.\nBlockchain-based distributed ledgers maintain a ledger cooperatively among sev-\neral parties, in such a way that each transaction is digitally signed as proof of authen-\nticity, and further, the ledger is maintained in such a way that once entries are added,\nthey cannot be deleted or modi\ufb01ed by one party, without detection by others.\nBlockchains form a key foundation of Bitcoin and other cryptocurrencies. Al-\nthough much of the technology underlying blockchains was initially developed in the\n1980s and 1990s, blockchain technology gained widespread popular attention in the\n2010s as a result of boom (and subsequent bus t) in Bitcoin and other cryptocurrencies.\nHowever, beyond the many cryptocurrency schemes, blockchains can provide a\nsecure data-storage and data-processing foundation for business applications, without\nrequiring complete trust in any one party. For example, consider a large corporation\nand its suppliers, all of whom maintain data about where products and components\nare located at any time as part of the manufacturing process. Even if the organiza-\ntions are presumed trustworthy, there may a situation where one of them has a strong\nincentive to cheat and rewrite the record. A blockchain can help protect from such\nfraudulent updates. Ownership documents, such as real-estate deeds, are another ex-\nample of the potential for blockchain use. Criminals may commit real-estate fraud by\ncreating fake ownership deeds, which could allow them to sell a property that they\ndo not own, or could allow the same property to be sold multiple times by an actual\nowner. Blockchains can help verify the authenticity of digitally represented ownership\ndocuments; blockchains can also ensure that once an owner has sold a property, the\n1251\n", "1280": "1252 Chapter 26 Blockchain Databases\nowner cannot sell it again to another person without getting detected. The security\nprovided by the blockchain data structure makes it possible to allow the public to view\nthese real-estate records without putting them at risk. We describe other applications\nfor blockchains later in the chapter.\nIn this chapter, we shall look at blockchain from a database perspective. We shall\nidentify the ways in which blockchain databases di\ufb00er from the traditional databases\nwe have studied elsewhere in this book and show how these distinguishing features are\nimplemented. We shall consider alternatives to Bitcoin-style algorithms and implemen-\ntation that are more suited to an enterprise database environment. With this database-\noriented focus, we shall not consider the \ufb01nancial implications of cryptocurrencies, nor\nthe issues of managing one\u2019s holding of such currencies via a cryptocurrency wallet or\nexchange.\n26.1 Overview\nBefore we study blockchains in detail, we \ufb01rst give an overview of cryptocurrencies,\nwhich have driven the development and usage of blockchains. We note, however, that\nblockchains have many uses beyond cryptocurrencies.\nTraditional currencies, also known as \u201c\ufb01at currencies\u201d are typically issued by a cen-\ntral bank of a country, and guaranteed by the government of that country. Currency\nnotes are at one level just a piece of paper; the only reason they are of value is that the\ngovernment that issues the currency guarantees the value of the currency, and users\ntrust the government. Today, although \ufb01nancial holdings continue to be denominated\nin terms of a currency, most of the \ufb01nancial holdings are not physically present as cur-\nrency notes; they are merely entries in the ledger of a bank or other \ufb01nancial institution.\nUsers of the currency are forced to trust the organization that maintains the ledger.\nAcryptocurrency is a currency created purely online, and recorded in a way that\ndoes not require any one organization (or country) to be totally trusted. This term arises\nfrom the fact that any such scheme has to based on encryption technologies. Since any\ndigital information can be copied easily, unlike currency notes, any cryptocurrency\nscheme must be able to prevent \u201cdouble spending\u201d of money. To solve this problem,\ncryptocurrencies use ledgers to record transactions. Further, the ledgers are stored a\nsecure, distributed infrastructure, with no r equirement to trust any one party. These two\nkey concepts, decentralization and trustlessness, are fundamental to cryptocurrencies.\nCryptocurrenies typically aim, like regular currency, and unlike credit card or debit\ncard transactions, to provide transaction anonymity, to preserve the privacy of users\nof the currency. However, since cryptocurrency blockchains are public data analytics\nmay be used to compromise or limit anonymity.\nBitcoin, which was the \ufb01rst successful cryptocurrency, emerged with the publi-\ncation of a paper by Satoshi Nakamoto1in 2008 and the subsequent publication of\nthe open-source Bitcoin code in 2009. The ideas in the original bitcoin paper solved\n1Satoshi Nokamoto is a pseudonym for a person or group that anonymously created Bitcoin.\n", "1281": "26.1 Overview 1253\na number of problems, and thereby allowed cryptocurrencies, which had earlier been\nconsidered impractical, to become a reality.\nHowever, the underlying concepts and algorithms in many cases go back decades\nin their development. The brilliance of Nakamoto\u2019s work was a combination of innova-\ntion and well-architected use of prior research. The successes of Bitcoin prove the value\nof this contribution, but the target\u2014an anon ymous, trustless, fully distributed concur-\nrency\u2014drove many technical decisions in directions that work less well in a database\nsetting. The Further Reading section at the end of the chapter cites key historical pa-\npers in the development of these ideas.\nAt its most basic level, a blockchain is a linked list of blocks of data that can be\nthought of as constituting a log of updates to data. What makes blockchain technology\ninteresting is that blockchains can be managed in a distributed manner in such a way\nthat they are highly tamper resistant, and cannot be easily modi\ufb01ed or manipulated by\nany one participant, except by appending digitally signed records to the blockchain.\nIn a business setting, trustless distributed control is valuable, but absolute\nanonymity runs counter to both principles of accounting and regulatory requirements.\nThis leads to two main scenarios for the use of blockchains. Bitcoin\u2019s blockchain is\nreferred to as a public blockchain , since it allows any site to join and participate in the\ntasks of maintaining the blockchain. In contrast, most enterprise blockchains are more\nrestricted and referred to as permissioned blockchains . In a permissioned blockchain,\nparticipation is not open to the public. Access is granted by a permissioning authority,\nwhich may be an enterprise, a consortium of enterprises, or a government agency.\nBitcoin introduced a number of ideas that made public blockchains practical, but\nthese have a signi\ufb01cant cost in terms of CPU power (and thereby, electrical power)\nneeded to run the blockchain, as well as latencies in processing transactions. By re-\nlaxing Bitcoin\u2019s strong assumptions about tr ustlessness and anonymity, it is possible to\novercome many of the ine\ufb03ciencies and high latencies of the Bitcoin model and design\nblockchains that further the goals of enterprise data management.\nIn this chapter, we begin by looking at the classic blockchain structure as used in\nBitcoin and use that to introduce the key distinguishing properties of a blockchain.\nAchieving many of these properties relies upon one-way cryptographic hash functions.\nThese hash functions are quite di\ufb00erent from those used in Chapter 24 as a means of\nindexing databases. Cryptographic hash functions need to have some speci\ufb01c mathe-\nmatical properties such as the following: given a data value xand a hash function h,i t\nmust be relatively easy to compute h(x) but virtually impossible to \ufb01nd xgiven h(x).\nWhen a blockchain is stored distributed across multiple systems, an important\nissue is to ensure that the participating systems agree on what are the contents of the\nblockchain, and what gets added to it at each step. When participants trust each other,\nbut may be vulnerable to failure, consensus techniques that we studied earlier in Section\n23.8 can be used to ensure that all participants agree on the contents of a log (and a\nblockchain is, at its core, a log). However, reaching agreement on what data get added\nto a blockchain is much more challenging when participants in the blockchain do not\ntrust each other and have no centralized control. The basic consensus algorithms are\n", "1282": "1254 Chapter 26 Blockchain Databases\nnot applicable in such a setting. For example, an attacker could create a large number of\nsystems, each of which joins the blockchain as a participant; the attacker could thereby\ncontrol a majority of the participating systems. Any decision based on a majority can\nthen be controlled by the attacker, who can force decisions that can tamper with the\ncontents of the blockchain. The tamper resistance property of the blockchain would\nthen be compromised.\nWe begin by describing the energy-intensive approach of Bitcoin, but we then con-\nsider a variety of alternative, more e\ufb03cient approaches used in other cryptocurrencies.\nFinally, we consider approaches based on Byzantine-consensus algorithms, which are\nconsensus algorithms that are resistant to some fraction of the participating nodes not\njust failing, but also lying and attempting to disrupt consensus. Byzantine consensus is\nwell-suited to an enterprise blockchain environment, and can be used if the blockchain\nispermissioned , that is, some organization controls who can have permission to ac-\ncess the blockchain. Byzantine consensus is an old problem and solutions have been\naround for many years. However, the special constraints of blockchain databases have\nled to some newer approaches to this problem. References to more details on Byzan-\ntine consensus techniques may be found in the Further Reading section at the end of\nthe chapter.\nBlockchain databases store more than just currency-based debit-credit transac-\ntions. Like any database, they may store a variety of types of data about the enterprise.\nA traditional blockchain data organization makes it di\ufb03cult to retrieve such data e\ufb03-\nciently, but pairing a blockchain with a traditional database or building the blockchain\non top of a database can enable faster query processing. We shall explore a variety of\nmeans of speeding up not only queries but also update transactions, both within the\nblockchain itself and by performing certain operations \u201co\ufb00 chain\u201d and adding them in\nbulk to the blockchain at a later time.\nAfter covering blockchain algorithms, we shall explore (in Section 26.8) some of\nthe most promising applications of blockchain databases.\n26.2 Blockchain Properties\nAt its most basic level, a blockchain is a linked list of blocks of data. A distinguishing\nfeature of the blockchain data structure is that the pointers in the linked list include\nnot only the identi\ufb01er of the next older block, but also a hash of that older block. This\nstructure is shown in Figure 26.1. The initial block, or genesis block ,i ss h o w na sb l o c k\n0 in the \ufb01gure. It is set up by the creator of the blockchain. Each time a block is added\nto the chain, it includes the pair of values (poi nter-to-previous-block, hash-of-previous-\nblock). As a result, any change made to block is easily detected by comparing a hash\nof that block to the hash value contained in the next block in the chain. The hash value\nin the next block could be changed, but then the block after that would also have to be\nchanged, and so on.\n", "1283": "26.2 Blockchain Properties 1255\ndatah(block n - 1)\ndatah(block 0)\nblock 1 block n block 0\ngenesis blockdata.  .  .\nFigure 26.1 Blockchain data structure.\nThis hash-validated pointer format in a blockchain makes tampering with a\nblockchain hard. To make tampering virtually impossible, it is necessary to ensure that\nany tampering with the blockchain is easily detected and that the correct version of the\nblockchain is easily determined. To achieve this, the hash function must have certain\nmathematical properties that we shall discuss shortly. Further, the chain itself must be\nreplicated and distributed among many independent nodes so that no single node or\nsmall group of nodes can tamper with the blockchain. Since the blockchain is repli-\ncated across multiple nodes, a distributed consensus algorithm needs to be used to\nmaintain agreement regarding the correct current state of the blockchain. In this way,\neven if some nodes try to tamper with the blockchain contents, as long as a majority\nare honest, making decisions based on a majority vote can ensure the integrity of the\nblockchain.\nThe above approach works if the set of nodes that participates in the blockchain is\ncontrolled in some fashion that makes it di\ufb03cult for an adversary to control a major-\nity of the nodes. However, such control goes against the goal of not have any central\ncontrol, and is viewed as unacceptable in public blockchains such as Bitcoin, which are\nbased on public blockchains in which the number of participating nodes may change\ncontinuously. Any computer may download the blockchain and attempt to add blocks\n(the code for implementing blockchains is available in open source). As a result, a\nmajority-based approach can be overwhelmed by an adversary who sets up a large num-\nber of low-cost computers as nodes. Such an attack is called a Sybil attack .\nThe way in which consensus is achieved among independent nodes varies among\nblockchains. The variations address trade-o\ufb00s between performance (latency and\nthroughput) and robustness to adversarial attacks on the consensus mechanism, in-\ncluding Sybil attacks. When we addressed distributed consensus in Chapter 23, we\nassumed that a single organization controlled the entire distributed system, and so the\nconsensus algorithm had to tolerate only possible failures of nodes or the network that\nwere fail-stop, where participants do not behave in an adversarial manner.\nIn a typical blockchain application, the chain is shared among multiple indepen-\ndent organizations. In the extreme case, for example Bitcoin, anyone can set up a node\nand participate, possibly for nefarious purposes. This implies that the types of failure\nthat may occur are not just cases where a device or system stops working, but also cases\n", "1284": "1256 Chapter 26 Blockchain Databases\nwhere a system remains operational but behaves in an adversarial manner. In most en-\nterprise settings, the blockchain is permissioned , providing some control over the set of\nparticipants, but still without direct controls to prevent malicious behavior.\nA node participating in a blockchain fully needs to participate in the consensus\nmechanism and maintain its own replica of the blockchain. Such a node is called a full\nnode. In some applications, there is a need for low-cost nodes that submit updates to\nthe blockchain, but do not have the storage or computational power to participate in\nthe consensus process. Such a node is called a light node .\nWe discuss blockchain consensus algorithms in detail in Section 26.4. Blockchain\nconsensus algorithms can be placed into one of several broad categories:\n\u2022Proof of work: Proof of work, which is described in detail in Section 26.4.1, pro-\nvides a solution to Sybil attacks by making it very expensive for an attacker to\ncontrol a majority of the nodes. Speci\ufb01cally, the nodes agree that the next block\non the blockchain will be added by the \ufb01rst node to solve a certain hard mathemat-\nical problem. This is referred to as mining a block. Proof-of-work algorithms are\nrobust to adversarial behavior as long as the adversary does not control more than\nhalf the computing power in the entire network. To ensure this requirement, the\nproblems are made intentionally hard, and require a lot of computational e\ufb00ort.\nThus, robustness comes at the price of a huge amount of otherwise useless com-\nputation along with the price of electricity needed to carry out the computation.\n\u2022Proof of stake: Proof of stake, which is described in Section 26.4.2, provides an-\nother solution to Sybil attacks. Here, the nodes agree to select the next node to\nadd a block to the blockchain based on an amount of the blockchain\u2019s currency\nowned or held in reserve by a node.\n\u2022Byzantine consensus: Byzantine consensus does not solve the problem of Sybil at-\ntacks, but can be used in non-public blockchains, where entry of nodes to the sys-\ntem can be controlled. While some nodes may behave maliciously, it is assumed\nthat a substantial majority are honest. In Byzantine consensus, described in Sec-\ntion 26.4.3, the next node to add a block to the blockchain is decided by an algo-\nrithm from the class of algorithms referred to as Byzantine-consensus algorithms.\nLike the basic consensus algorithms we described earlier in Section 23.8, these\nalgorithms achieve agreement by message passing, but unlike those algorithms,\nthese algorithms can tolerate some number of nodes being malicious by either dis-\nrupting consensus or trying to cause an incorrect consensus to be reached. This\napproach requires signi\ufb01cantly more messages to be exchanged than in the case of\nthe basic consensus algorithms of Section 26.4.3, but this is a worthwhile trade-o\ufb00\nfor the ability for a system to work correctly in the presence of a certain number\nof malicious nodes.\n\u2022Other approaches: There are several other less widely used consensus mechanisms,\nsome of which are variants of the preceding mechanisms. These include proof of\n", "1285": "26.2 Blockchain Properties 1257\nactivity, proof of burn, proof of capacity, and proof of elapsed time. See the Further\nReading section at the end of the chapter for details.\nAnother way to damage a blockchain besides attempting to alter blocks is to add\na new block to a block other than the most recent one. This is called a fork.F o r k i n g\nmay occur due to malicious activity, but there are two sources of nonmalicious forks:\n1.Two distinct nodes may add a new block after the most recent block, but they do\nit so close together in time that both are added successfully, thus creating a forked\nchain. These accidental forks are resolved by a protocol rule that nodes always\nattempt to add blocks to the end of the longest chain. This probabilistically limits\nthese accidental forks to a short length. The blocks on the shorter forks are said\nto be orphaned , and the contents of those blocks will get inserted on the real\nchain later if those contents are not already there.\n2.A majority of blockchain users may agree to fork the blockchain in order to\nchange some aspect of the blockchain protocol or data structure. This is a rare\nevent and one that, when it has occurred in major blockchains, has caused major\ncontroversy. Such a fork is said to be a soft fork if prior blocks are not invalidated\nby the fork. That is, the old version of the blockchain software will recognize\nblocks from the new version as valid. This permits a gradual transition from the\nold version of the blockchain software to the new version. In a hard fork ,t h eo l d\nversion of the blockchain software will deem blocks from the new version to be\ninvalid. After a hard fork, if the old version of the blockchain software remains\nin use, it will lead to a separate blockchain with di\ufb00erent contents.\nBecause of the possibility of orphaned blocks, it may be necessary to wait for several\nadditional blocks to be added before it is safe to assume s block will not be orphaned.\nNote 26.1 on page 1258 presents a few examples of notable blockchain forks.\nSo far, we have not said much about the actual data in the blocks. The contents\nof blocks vary by application domain. In a cryptocurrency application, the most com-\nmon data found in blocks are basic currency-transfer transactions. Since any node can\nadd a block, there needs to be a way to ensure that transactions entered are in fact\ngenuine. This is achieved via a technique called a digital signature that allows a user to\n\u201csign\u201d a transaction and allows every node to verify that signature. This prevents fake\ntransactions from being added to the chain and prevents participants in the transaction\nfrom subsequently denying their involvement in the transaction. This latter property is\nreferred to as irrefutability .\nTransactions are broadcast to all nodes participating in the blockchain; when a\nnode adds a block to the chain, the block contains all transactions received by the\nnode that have not already been added to the chain.\nThe users who submit transactions may be known to the blockchain administrator\nin a permissioned blockchain, but in a public blockchain like Bitcoin, there is no direct\n", "1286": "1258 Chapter 26 Blockchain Databases\nNote 26.1 Blockchain Fork Examples\nThere have been several notable forks of major blockchains. We list a few here.\n\u2022Hard fork: Bitcoin/Bitcoin Cash: Bitcoin\u2019s built-in block-size limit was an ac-\nknowledged problem in the Bitcoin community but agreeing on a solution\nproved controversial. A hard fork in August 2017 created a new cryptocur-\nrency, Bitcoin Cash, with a larger block-size limit. Holders of Bitcoin at the\ntime of the fork received an equal amount of Bitcoin Cash, and thus could\nspend both.\n\u2022Soft fork: Bitcoin SegWit: SegWit (short for segregated witness) moves certain\ntransaction-signature data (referred to as witness data) outside the block. This\nallows more transactions per block while retaining the existing block size limit.\nThe relocated witness data are needed only for transaction validation. SegWit\nwas introduced in August 2017 via a soft fork. This was a soft fork because the\nold blocks were recognized as valid and nodes not yet upgraded were able to\nretain a high degree of compatibility.\n\u2022Hard fork: Ethereum/Ethereum Classic: This fork arose from the failure of\na crowd-funded venture-capital operation running as a smart contract in the\nEthereum blockchain. Its code contained a design \ufb02aw that enabled a hack in\n2016 that stole ether valued in the tens of millions of U.S. dollars. A controver-\nsial hard fork refunded the stolen funds, but opponents of the fork, believing\nin the inviolabilty of blockchain immutab ility, retained the original blockchain\nand created Ethereum Classic.\nconnection between a user IDand any real-world entity. This anonymity property is a\nkey feature of Bitcoin, but its value is diminished because of the possibility to tie a user\nIDto some o\ufb00-chain activity, thereby de-anonymizing the user. De-anonymization can\noccur if the user enters into a transaction with a user whose user IDhas already been\nde-anonymized. De-anonymization can occur also via data mining on the blockchain\ndata and correlating on-chain activity by a speci\ufb01c user IDwith the \u201creal-world\u201d activity\nof a speci\ufb01c individual.\nFinally, a feature of blockchains is the ability to store executable code, referred\nto as a smart contract . A smart contract can implement complex transactions, take\naction at some point in the future based on s peci\ufb01ed conditions, and, more generally,\nencode a complex agreement among a set of users. Blockchains di\ufb00er not only in the\nlanguage used for smart contracts but also in the power of the language used. Many\nare Turing complete, but some (notably, Bi tcoin) have more limited power. We discuss\nsmart contracts, including how and when their code is executed, in Section 26.6.\n", "1287": "26.3 Achieving Blockchain Properties via Cryptographic Hash Functions 1259\nWe summarize this discussion by listing a set of properties of blockchains:2\n\u2022Decentralization: In a public blockchain, control of the blockchain is by majority\nconsensus with no central controlling authority. In a permissioned blockchain,\nthe degree of central control is limited, typically only to access authorization and\nidentity management. All other actions happen in a decentralized manner.\n\u2022Tamper Resistance: Without gaining control over a majority of the blockchain net-\nwork, it is infeasible to change the contents of blocks.\n\u2022Irrefutability: Activity by a user on a blockchain is signed cryptographically by the\nuser. These signatures can be validated easily by anyone and thus prove that the\nuser indeed is responsible for the transaction.\n\u2022Anonymity: Users of a blockchain have user IDs that are not tied directly to any\npersonally identifying information, though anonymity may be compromised indi-\nrectly. Permissioned blockchains may o\ufb00er only limited anonymity or none at all.\n26.3 Achieving Blockchain Properties via Cryptographic Hash\nFunctions\nIn this section, we focus on the use of cryptographic hash functions to ensure some\nof the properties of blockchains. We begin with a discussion of special types of hash\nfunction for which it is infeasible to compute the inverse function or \ufb01nd hash colli-\nsions. We show how these concepts extend to public-key encryption, which we \ufb01rst\nsaw in Section 9.9. We then show how cryptographic hash functions can be used to\nensure the anonymity, irrefutability, and tamper-resistance properties. We show how\nhash functions are used in mining algorithms later in Section 26.4.1.\n26.3.1 Properties of Cryptographic Hash Functions\nIn Section 14.5, hash functions were used as a means of accessing data. Here, we use\nhash functions for a very di\ufb00erent set of purposes, and as a result, we shall need hash\nfunctions with additional properties beyond those discussed earlier.\nAh a s hf u n c t i o n htakes input from some (large) domain of values and generates\nas its output a \ufb01xed-length bit string. Typically, the cardinality of the domain is much\nlarger than the cardinality of the range. Furthermore, the hash function must have a\nuniform distribution , that is, each range value must be equally probable given random\ninput. A hash function hiscollision resistant if it is infeasible to \ufb01nd two distinct values\nxandysuch that h(x)=h(y). By infeasible , we mean that there is strong mathematical\n2These properties pertain to blockchains, but not to most cryptocurrency exchanges. Most exchanges hold not only\ncustomers\u2019 data but also their keys, which means that a hack against the exchange\u2019s database can result in theft of\nusers\u2019 private keys.\n", "1288": "1260 Chapter 26 Blockchain Databases\nevidence, if not an actual proof, that there is no way to \ufb01nd two distinct values xandy\nsuch that h(x)=h(y) that is any better than random guessing.\nThe current standard choice of a cryptographic hash function is called SHA-256,\na function that generates output 256 bits in length. This means that given a value x,\nthe chance that a randomly chosen ywill hash to the same value to which xhashes is\n1\u22152256. This means that even using the fastest computers, the probability of a successful\nguess is e\ufb00ectively zero.3\nThe collision-resistance property contributes to the tamper resistance of a\nblockchain in a very important way. Suppose an adversary wishes to modify a block\nB. Since the next-newer block after Bcontains not only a pointer to Bbut also the hash\nofB,a n ym o d i \ufb01 c a t i o nt o Bmust be such that the hash of Bremains unchanged after\nthe modi\ufb01cation in order to avoid having to modify also that next-newer block. Finding\nsuch a modi\ufb01cation is infeasible if the hash function has the collision-resistance prop-\nerty, and, therefore, any attempt to tamper with a block requires changing all newer\nblocks in the chain.\nA second important property that we require of a cryptographic hash function is\nirreversibility , which means that given only h(x), it is infeasible to \ufb01nd x.T h et e r m\nirreversible comes from the property that, given x,i ti se a s yt oc o m p u t e h(x), but given\nonly h(x), it is infeasible to \ufb01nd h\u22121(h(x)). The next section shows how this concept is\napplied to blockchains.4\n26.3.2 Public-Key Encryption, Digital Signatures, and Irrefutability\nSection 9.9 described two categories of encryption methods: private-key encryption,\nwhere users share a secret key, and public-key encryption, where each user has two\nkeys, a public key and a private key. The main problem with private-key encryption\nis that users must \ufb01nd a way at the outset to share the secret private key. Public-key\nencryption allows users who have never met to communicate securely. This property\nof public-key encryption is essential to blockchain applications that serve arbitrarily\nlarge communities of users worldwide.\nEach user Uihas a public key Eiand a private key Di. A message encrypted using Ei\ncan be decrypted only with the key Di, and, symmetrically, a message encrypted using\nDican be decrypted only with the key Ei,I fu s e r u1wishes to send a secure message xto\nU2,U1encrypts xusing the public key E2of user U2.O n l y U2has the key D2to decrypt\nthe result. For this to work, the speci\ufb01c function used must have the irreversibility\nproperty so that given a public key Eiit is infeasible to compute the inverse function,\n32256is larger than 1077. If a computer could make one guess per cycle it would take more than 1067seconds to have\na 50 percent chance of guessing correctly. That translates to more than 1059years. To put that in context, astronomers\npredict that the sun will have grown in size to envelop Earth within 1010years.\n4This property has long been used for storing passwords. Rather than storing user passwords in clear text, leaving them\nsusceptible to being stolen, hashes are kept instead. Then, when a user logs in and enters a password, the hash of that\npassword is computed and compared to the stored value. Were a n attacker to steal the hashes, that attacker would still\nlack the actual passwords, and, if the hash function in use has the irreversibility property, then it is infeasible for the\nhacker to reverse-engineer the user passwords.\n", "1289": "26.3 Achieving Blockchain Properties via Cryptographic Hash Functions 1261\nthat is, to \ufb01nd Di. This creates a mechanism for users who have never met to share\nsecret messages.\nSuppose now that instead of seeking to send a secret message, user U1wishes to\n\u201csign\u201d a document x.U s e r U1can encrypt xusing the private key D1. Since this key is\nprivate, no one besides U1could have computed that value, but anyone can verify the\nsignature by decrypting using the public key of U1,t h a ti s , E1. This provides a public\nproof that user U1has signed document x.\nIn blockchain applications, the concept of a digital signature is used to validate\ntransactions. Observe that the linkage of blocks in the blockchain, using a pointer and\nthe hash of block to which the pointer points, means that a user can sign an entire\nchain simply by signing the hash of the newest block in the chain. See the Further\nReading section at the end of the chapter for references to the mathematics of public-\nkey encryption.\n26.3.3 Simple Blockchain Transactions\nIn our discussion of database transactions in Chapter 17, we described a transaction as\na sequence of steps that read and/or write data values from the database. That concept\nof a transaction is based on a data model where there is a single store of data values\nthat are accessed by transactions. A blockchain, in its simplest form, is more closely an\nanalog of a database log in that it records the actual transactions and not just \ufb01nal data\nvalues. That analogy breaks down, however, in most blockchains, because transactions\nare either fully independent or depend explicitly on each other. The model we describe\nhere corresponds to simple Bitcoin transactions.\nAs an example, consider two users, AandB, and assume Awishes to pay B10 units\nof some currency. If this were a traditional banking application with a \ufb01at currency such\nas the U.S. dollar, the transaction implementing this transfer would read A\u2019s account\nbalance, decrement it by 10, and write that value to the database, and then read B\u2019s\nbalance, add 10, and write that value to the database. In a blockchain-based system,\nthis transaction is speci\ufb01ed in a di\ufb00erent manner.\nRather than referencing data items, a Bitcoin-style blockchain transaction refer-\nences users and other transactions. Users are referenced by their user ID.U s e r Awould\nlocate a transaction or set of transactions from past history T1,T2,\u2026,Tnthat paid Aa\nt o t a lo fa tl e a s t1 0u n i t so ft h ec u r r e n c y . Awould then create a transaction Tthat takes\nthe output (i.e., the amount paid to A) by those transactions as input, and as its output\npays 10 units of the currency to Band the remainder back to Aas the \u201cchange.\u201d The\noriginal transactions T1,T2,\u2026,Tnare then treated as having been spent.\nThus, each transaction indicates how much money has been paid to whom; the\ncurrency balance of a user Ais de\ufb01ned by a set of unspent transactions that have\npaid money to A. Assuming Ais honest, those transactions\u2019 outputs (i.e., the output\nofT1,T2,\u2026,Tn) would not have been spent already by Ain a previous transaction. If\nAwere indeed dishonest and Tattempted to spend the output of some T1as e c o n d\ntime, Twould be a double-spend transaction. Double-spend transactions and other in-\n", "1290": "1262 Chapter 26 Blockchain Databases\nvalid transactions are detected in the mining process that we discuss in Section 26.4,\nby keeping track of all unspent transactions and verifying that each transaction Tithat\nis input to Tis unspent when Tis executed. After Tis executed, each such Tiis treated\nas spent.\nEthereum uses a di\ufb00erent and more powerful model, where the blockchain main-\ntains state (including current balance) for each account in the system. Transactions\nupdate the state, and can transfer funds from one account to another. The model used\nin Ethereum is discussed in Section 26.5.\nA Bitcoin-style transaction Tspeci\ufb01es:\n\u2022The input transactions T1,T2,\u2026,Tn.\n\u2022The set of users being paid and the amount to be paid to each, which in our example\nis 10 units to Band the remainder to A.5\n\u2022A\u2019s signature of the transaction, to prove that Ain fact authorized this transaction.\n\u2022A more complex transaction might include executable code as part of its speci\ufb01-\ncation, but we shall defer that to Section 26.6.\n\u2022Data to be stored in the blockchain; the data must be under some size, which is\nblockchain dependent.\nThe transaction model described here is quite distinct from that of a traditional\ndatabase system in a variety of ways, including:\n\u2022Existing data items are not modi\ufb01ed. Instead, transactions add new information.\nAs a result, not only the current state but also the history leading to the current\nstate are fully visible.\n\u2022Con\ufb02icts in transaction ordering are prevented. If con\ufb02icts occur, the transaction\ncausing a con\ufb02ict is detected and deemed invalid as part of the process of adding\na block to the chain, described in Section 26.4.\n\u2022Although the blockchain is a distributed system, a transaction is created locally.\nIt becomes part of the permanent, shared blockchain only through the mining\nprocess. This is, in e\ufb00ect, a form of deferred transaction commit.\n\u2022Dependencies of one transaction upon another are stated explicitly in a transaction\nsince a transaction lists those transactions whose outputs it uses as input. If we\nview this in terms of the precedence graph introduced in Chapter 17, our example\nwould include precedence-graph edges T1\u2192T,T2\u2192T,\u2026,Tn\u2192T.\n\u2022There is no explicit concurrency control. Much of the need for concurrency control\nis eliminated by the maintenance of a complete history and the direct sequencing\n5In a real system, there may also be a payout to the miner of the transaction, that is, the node that adds the block to\nthe blockchain, as we discuss in Section 26.4.\n", "1291": "26.4 Consensus 1263\nof transactions. Thus, there is no contention for the current value of any database\ndata item.\nThis Bitcoin-based example is not the only way blockchain systems manage transaction\nordering. We shall see another example when we consider smart contracts in Section\n26.6.\nThe fact that data may be stored in the blockchain makes the blockchain more\nthan just a tamper-resistant transaction log. It allows for the representation of any sort\nof information that might be stored in a traditional database. In Section 26.5.2, we\nshall see how this capability, particularly in blockchains with a concept of blockchain\nstate, makes the blockchain a true database.\n26.4 Consensus\nBecause the blockchain is replicated at all participating nodes, each time a new block\nis added, all nodes must eventually agree \ufb01rst on which node may propose a new block\nand then agree on the actual block itself.\nIn a traditional distributed database system, the consensus process is simpli\ufb01ed\nby the fact that all participants are part of one controlling organization. Therefore,\nthe distributed system can implement global concurrency control and enforce two-\nphase commit to decide on transaction commit or abort. In a blockchain, there may\nbe no controlling organization, as is the case for a public blockchain like Bitcoin. In\nthe case of a permissioned blockchain, there may be a desire to have a high degree\nof decentralized control in all matters except the actual permissioning of participants,\nwhich is managed by the organization controlling the permissioned blockchain.\nWhen transactions are created, they are broadcast to the blockchain network.\nNodes may collect a set of transactions to place in a new block to be added to the chain.\nThe consensus mechanisms used in blockchains fall roughly into two categories:\n1.Those where the nodes reach agreement on one node to add the next block. These\ntypically use Byzantine consensus (Section 26.4.3).\n2.Those where the blockchain is allowed temporarily to forkby allowing multiple\nnodes to create a block following the last block in the chain. In this approach,\nnodes attempt to add blocks to the longest linear subchain. Those blocks not on\nthat longest chain are orphaned and not considered part of the blockchain. To\navoid a massive number of forks being created, this approach limits the rate at\nwhich blocks may be added so that the expected length of orphaned branches is\nshort. These typically use proof-of-work (Section 26.4.1) or proof-of-stake (Sec-\ntion 26.4.2).\nA node that adds a block to the chain must \ufb01rst check that block of transactions. This\nentails checking that:\n", "1292": "1264 Chapter 26 Blockchain Databases\n\u2022Each transaction is well-formed.\n\u2022The transaction is not double-spending by using as input (i.e., spending) currency\nunits that have been used already by a prior transaction. To do so, each node must\ntrack the set of all unspent currency units (transactions), and look up this set for\neach transaction Tto ensure that all the currency units that are inputs to Tare\nunspent.\n\u2022The transaction is correctly signed by the submitting user.\nWhen a node is selected to add a block to the chain, that block is propagated to all\nnodes, and each checks the block for validity before adding it to its local copy of the\nchain.\nWe next need to consider the question of why any node would want to use its\nresources for mining, that is to carry out the work needed to append blocks to the\nchain. Mining is a service to the blockchain network as a whole, and so miners are\npaid (in the currency of the blockchain) for their e\ufb00orts. There are two sources of\npayment to miners:\n1.A fee paid by the system in new coins in the currency of the blockchain.\n2.A fee included by the submitter of the transaction. In this case the output of the\ntransaction includes an additional output representing a payment to the miner\nof the block containing the transaction. Users are incented to include fees since\nsuch fees incent miners to include their transactions preferentially in new blocks.\nThe exact means of paying miners varies among blockchains.\nIn this section, we look at various ways to achieve consensus. We begin by assuming\na public blockchain and describe consensus based on two approaches: proof-of-work and\nproof-of-stake . We then consider permissioned blockchains that in many cases choose\nto use a consensus mechanism based on Byzantine consensus .\n26.4.1 Proof of Work\nProof-of-work consensus is designed for public blockchains in which the number of par-\nticipating nodes is changing continuously. Any computer may download the blockchain\nand attempt to add blocks. As a result, a majority-based approach can be overwhelmed\nby an adversary who sets up a large number of low-cost computers as nodes. As men-\ntioned earlier, such an attack is called a Sybil attack . Instead, proof-of-work requires a\nnode to solve a computationally hard, but not infeasible, mathematical problem. An\nattacker cannot overwhelm a blockchain network simply by adding inexpensive nodes.\nRather, the attacker would need to have access to computation capacity that forms a\nmajority of the network\u2019s total computation capacity, a task that is much more di\ufb03cult\nand costly than launching a Sybil attack.\n", "1293": "26.4 Consensus 1265\nThe computationally hard problem is based on the concept of cryptographic hash-\ning. A node that wishes to mine a block Bas the next block needs to \ufb01nd a value, called\nanonce , that, when concatenated to Band the hash of the previous block, hashes to a\nvalue less than a preset target value speci\ufb01ed for the blockchain. The nonce is typically a\n32-bit value. If the target is set very low, say to 4, and assuming the usual 256-bit hash, a\nminer would have only a 1 \u22152254chance of succeeding for a single choice for the nonce.\nIf the target were set very high, say to 2255, the miner would have a 50 percent chance\nof success. Blockchain implementations are designed to vary the target so as to control\nthe rate of mining of blocks across the whole system. This variability allows the system\nto adjust as computation power increases whether due to hardware advances or due to\nadditional nodes joining the network. The target times vary for di\ufb00erent blockchains.\nBitcoin targets having some node in the system successfully mine a block every 10\nminutes. Ethereum targeted a mining time of 10 to 15 seconds with its proof-of-work\nmechanism. As of late 2018, Ethereum is moving to a proof-of-stake mechanism and\nis expected to target a slightly faster rate. While faster may appear to be better, note\nthat if mining occurs at a faster rate than the time it takes to propagate a new block\nthroughout the network, the probability of forks and orphaned blocks increases.\nNow that we have seen how proof-of-work mining works, let us recall the properties\nwe stated about cryptographic hash functions. If there were an e\ufb03cient algorithm for\n\ufb01nding a nonce that results in a hash less than the target, miners might \ufb01nd nonces too\nquickly. Therefore, the hash function must ensure that there is no better way to \ufb01nd a\nnonce than simply trying each possible nonce value in turn. This leads us to require one\nadditional property for cryptographic hash functions, the puzzle-friendliness property.\nThis property requires that given a value k,f o ra n y n-bit value yit is infeasible to \ufb01nd\nav a l u e xsuch that h(x\u2016k)=yin time signi\ufb01cantly less that 2n,w h e r e \u2016denotes\nconcatenation of bit strings.\nProof-of-work mining is controversial. On the positive side, for a large network,\nit would be highly costly for an adversary to obtain enough computational power to\ndominate mining. However, on the negative side, the amount of energy used in mining\nis huge. Estimates as this chapter is being written suggest that Bitcoin mining worldwide\nconsumes about 1 percent of the power consumed by the United States, or more than\nthe entire consumption of several nations, for example Ireland. The large amount of\ncomputation needed has created incentives to design special-purpose computing chips\nfor mining and incentives to locate large mining installations near sources of cheap\npower sources.\nThese concerns are causing a movement to alternatives, such as proof-of-stake,\nwhich we discuss next. These concerns have led also to interest in alternative forms\nof proof-of-work that, for example, require having a large amount of main memory in\norder quickly to \ufb01nd a nonce. Memory-intensive schemes retain the cost barrier of\nproof-of-work while reducing the energy waste. They are a subject of current research.\nFurthermore, we shall see that for enterprise permissioned-blockchain applications,\nmuch less costly means of consensus are possible.\n", "1294": "1266 Chapter 26 Blockchain Databases\nIn practice, a group of users may unite to form a mining pool , which is a consortium\nthat works together to mine blocks and then shares the proceeds among its members.\n26.4.2 Proof of Stake\nThe concept of proof-of-stake is to allow nodes holding a large stake in the currency of\nthe blockchain to be chosen preferentially to add blocks. This rule cannot be applied\nabsolutely, since then a single largest stakeholder would control the chain. Instead,\nthe probability of mining success, using proof-of-work, is made higher for nodes in\nproportion to their stake. By adjusting both the stake requirements and the mining\nd i \ufb03 c u l t y ,i tr e m a i n sp o s s i b l et oc o n t r o lt h er a t ea tw h i c hb l o c k sa r em i n e d .\nThere are a wide variety of proof-of-stake schemes. They may include measurement\nnot only of overall stake, but also the total time a stake has been held. They may require\nthat the stake or some fraction of it be held inactive for some period of time in the\nfuture.\nProperly tuning a proof-of-stake mechanism is di\ufb03cult. Not only are there more\nparameters to consider than in proof-of-work, but also one must guard against a situa-\ntion where there is too little cost penalty for an adversary to add blocks to a fork other\nthan the longest one.\n26.4.3 Byzantine Consensus\nAn important alternative to work- or stake-based consensus is message-based consen-\nsus. Message-based consensus is widely used in distributed database systems. As we\nnoted earlier, the basic consensus protocols do not work for blockchain consensus be-\ncause it cannot be assumed that there are no malicious nodes.\nMessage-based systems aim to achieve consensus via a majority vote. Such systems\nare vulnerable to a Sybil attack. In an enterprise permissioned blockchain, in which\nusers have to be granted permission to participate, Sybil attacks are not possible since\nthe permissioning authority can easily deny permission when a malicious user attempts\nto add an excessive number of nodes. However, even in this setting, one cannot assume\nevery user is totally honest.\nFor example, consider a supply-chain blockchain in which all suppliers enter data\non the chain pertaining to each item being supplied either to another supplier or the\nultimate manufacturer of an end-user product. Some supplier might choose to falsify\ndata for its own advantage, but, when a fraud investigation begins, that supplier may\nthen seek to fork the blockchain to cover-up its fraud. Thus, even absent the possibility\nof Sybil attacks, there remains the possibility of adversarial behavior. It is di\ufb03cult to\nanticipate every possible form of adversarial behavior.\nFor this reason, we model this situation using the concept of Byzantine failure in\nwhich it is assumed that a \u201cfailed\u201d node can behave in an arbitrary manner, and the net-\nwork of non-failed nodes must be robust to all such misbehavior, including misbehavior\nthat takes exactly the needed set of steps to sabotage the network. The assumption of\n", "1295": "26.5 Data Management in a Blockchain 1267\nByzantine failure is quite di\ufb00erent from the assumption made by consensus protocols,\nwhere the only type of failure considered is the absence of function, that is, the only\nway a node or network link fails to stop working and thus do nothing. This is referred\nto as a fail-stop model and precludes any malicious behavior.\nIn Section 23.8, we discussed distributed consensus protocols, notably Paxos and\nRaft. These protocols depend on the fail-stop assumption, but allow agreement using\nmajority rule (in contrast, 2PCrequires unanimity of agreement). For Byzantine con-\nsensus, we must seek a form of majority rule that overcomes not only the failure of a\nminority of nodes, but also the possible malic ious behavior of that minority. For exam-\nple, a malicious node n1may tell node n2that it desires to commit a transaction, but\ntelln3that it desires to abort the transaction. As one might expect, achieving consensus\nin the face of such malicious nodes requires a higher cost in the number of messages\nsent to achieve agreement, but in a blockchain, that higher cost is acceptable since it\ncan be much lower than the cost of proof-of-work or proof-of-stake mining.\nThe development of Byzantine consensus algorithms began in the early 1980s; see\nthe Further Reading section at the end of the chapter for references. There has been\nmuch theoretical work relating the number of rounds of messaging, the total number of\nmessages sent, and the fraction of the nodes that can be malicious without causing the\nprotocol to fail. Early work made assumptions about network behavior, such as the time\nit takes to deliver a message or that the network behaves in a highly synchronous man-\nner. Modern Byzantine consensus algorithms are based on real-world assumptions and\nincorporate cryptographic signatures to guard against forged messages. The degree of\nsynchronization is reduced, but truly asychronous fault-tolerant consensus is provably\nimpossible. One widely used approach, called Practical Byzantine Fault Tolerance ,t o l -\nerates malicious failure of up to \u230an\u22121\n3\u230bnodes and is viewed as providing an acceptable\nlevel of performance. Other protocols are referenced in the Further Reading section at\nthe end of the chapter.\n26.5 Data Management in a Blockchain\nUntil now we have not been concerned about the e\ufb03ciency of looking up information\nin a blockchain. While individual users can track their unspent transactions, that is not\nsu\ufb03cient to validate a block. Each node needs to be able to check each transaction in\na block to see if it was already spent. In principle, that could be done by searching the\nentire blockchain, but that is far too costly since it could involve searching backwards\nto the very \ufb01rst block in the chain. In this section, we shall consider data structures to\nmake such lookups e\ufb03cient.\nFurthermore, not every blockchain uses a transaction model in which transac-\ntion inputs are restricted to be the direct output of other transactions. Some, notably\nEthereum, allow for the maintenance of a state for each user (account, in Ethereum\nparlance) that holds the account balance (in the Ethereum currency, Ether) and some\n", "1296": "1268 Chapter 26 Blockchain Databases\nassociated storage. This transaction and data model comes closer to that of a database\nsystem. Simply storing this information in a database, however, would not preserve the\nblockchain properties we listed in Section 26.2. In this section, we consider this richer\nmodel and how it can be represented physically either via specialized data structures\nor with the help of database system concepts.\n26.5.1 Efficient Lookup in a Blockchain\nAs we noted earlier, in order to validate a Bitcoin-style transaction, a node needs to\ncheck three items:\n1.The transaction is syntactically well formed (proper data format, sum of inputs\nequals sum of outputs, and so on). This is relatively straightforward.\n2.The transaction is signed by the user submitting it. This is a matter of ensuring\nthat the signature, which should have been produced by the user submitting the\ntransaction using her or his private key, can be decrypted with that user\u2019s public\nkey to obtain the transaction itself. This is not a highly costly step.\n3.The transaction\u2019s inputs have not been spent already. This entails looking up\neach individual input transaction in the blockchain. These transactions could be\nanywhere in the blockchain since they can be arbitrarily old. Without a good\nmeans of performing this lookup, this step would be prohibitively costly.\nTo test for an input transaction having been used already, it is necessary to be able to\ncheck that transaction did not appear earlier as input to another transaction. Thus, it\nsu\ufb03ces for each node to maintain an index on all unspent transactions. Entries in this\nindex point to the location of the corresponding transaction in the blockchain, allowing\nthe details of the input transaction to be validated.\nBitcoin, like many other blockchains, facilitates lookup and validation by storing\ntransactions within a block in a Merkle tree, which we discussed in Section 23.6.6. In\nthat section, we noted that a Merkle tree enables the e\ufb03cient veri\ufb01cation of a collection\n(transactions, in the case of a blockchain) that may have been corrupted by a malicious\nuser. In a blockchain, there are optimizatio n st ot h eM e r k l et r e ep o s s i b l e ,s u c ha st r u n -\ncating the tree to remove subtrees consisting solely of spent transactions. This reduces\nsigni\ufb01cantly the space requirements for nodes to store the full blockchain. Space is a\nmajor consideration since major blockchain s grow faster that one gigabyte per month,\na rate likely to increase as blockchain applications grow.\nThe Merkle-tree structure is particularly useful for light nodes (i.e., nodes that do\nnot store the entire blockchain) since they need to retain only the root hash of the\ntree for veri\ufb01cation. A full node can then provide any needed data to the light node\nby providing those data plus the hashes needed for the light node to verify that the\nprovided data are consistent with its stored hash value (see Section 23.6.6).\n", "1297": "26.6 Smart Contracts 1269\n26.5.2 Maintaining Blockchain State\nThe simple blockchain transaction model o f Section 26.3.3 showed how a basic Bitcoin\ntransaction works. There are more complex transactions possible in Bitcoin, but they\nfollow the same pattern of a set of input transactions and a set of payments to users.\nIn this section, we look at the model used by certain other blockchains, notably\nEthereum, that maintain a state that holds the balance in each account. Transactions\nmove currency units ( ether in Ethereum) among accounts. Since transactions are se-\nrialized into blocks by miners, there is no need for concurrency-control protocols like\nthose of Chapter 18. Each block contains a sequence of transactions but also contains\nthe state as it existed after execution of transactions in the block. It would be wasteful\nto replicate the entire state in each block since the modest number of transactions in\none block are likely to change a relatively small fraction of the overall state. This creates\na need for a data structure allowing better use of storage.\nRecall that transactions within a block are stored in a Merkle tree. State is stored\nsimilarly. This would appear to o\ufb00er the possibility of saving space by allowing pointers\n(plus the associated hash) back to earlier blocks for those parts of the state that are\nunchanged. The only challenge here is that it must be possible not only to change tree\nnodes, but also to insert and delete them. A variant of the Merkle-tree data structure,\ncalled a Merkle-Patricia tree , is used for this purpose in some blockchains, including\nEthereum. This data structure allows for e\ufb03cient key-based search in the tree. Instead\nof actually deleting and inserting tree nodes, a new tree root is created and the tree\nitself structured so as to reference (and thus reuse) subtrees of prior trees. Those prior\ntrees are immutable, so rather than making new parent pointer (which we can\u2019t do),\na leaf-to-root path is generated by reversing a root-to-leaf path that is easily obtained\nin the Merkle-Patricia tree structure. Details of this data structure can be found in the\nreferences in the Further Reading section at the end of the chapter.\nCorda, Hyperledger Fabric, and BigchainDB are examples of blockchains that use\na database to store state and allow querying of that state. Fabric and Bigchain DBuse\nNoSQL databases. Corda uses an embedded- SQL database. In contrast, Ethereum state\nis stored in a key-value store.\n26.6 Smart Contracts\nSo far, we have focused on simple funds-transfer transactions. Actual blockchain trans-\nactions can be more complex because they may include executable code. Blockchains\ndi\ufb00er not only in the supported language(s) for such code, but also, and more im-\nportantly, in the power of those languages. Some blockchains o\ufb00er Turing-complete\nlanguages, that is, languages that can express all possible computations. Others o\ufb00er\nmore limited languages.\n26.6.1 Languages and Transactions\nBitcoin uses a language of limited power that is suitable for de\ufb01ning many standard\ntypes of conditional funds-transfer transactions. Key to this capability is its multisig\n", "1298": "1270 Chapter 26 Blockchain Databases\ninstruction, which requires mofnspeci\ufb01ed users to approve the transfer. This enables\nescrow transactions in which a trusted third party resolves any dispute between the two\nparties to the actual transfer. It also enables grouping several transactions between two\nusers into one larger transaction without having to submit each component transaction\nseparately to the blockchain. Because adding transactions to the blockchain has a time\ndelay and a cost in transaction fees, this feature is quite important. This concept has\nbeen extended in o\ufb00-chain processing systems, which we discuss in Section 26.7.\nEthereum as well as most blockchains targeting enterprise applications include a\nlanguage that is Turing complete. Many use familiar programming languages or vari-\nants based heavily on such languages. This would seem like an obvious advantage over\nless-powerful languages, but it comes at some risk. Whereas it is impossible to write\nan in\ufb01nite loop in Bitcoin\u2019s language, it is possible to do so in any Turing-complete\nlanguage. A malicious user could submit a transaction that encodes an in\ufb01nite loop,\nthereby consuming an arbitrarily large amount of resources for any node attempting\nto include that transaction in a newly mined block. Testing code for termination, the\nhalting problem, is a provably unsolvable problem in the general case. Even if the ma-\nlicious user avoids an in\ufb01nite loop, that user could submit code that runs for an excep-\ntionally long time, again consuming miner resources. The solution to this problem is\nthat users submitting a transaction agree to pay the miner for code execution, with an\nupper bound placed on the payment. This limits the amount of total execution to some\nbounded amount of time.\nThe decentralized nature of a blockchain leads to an incentive system for users to\nconvince miners to include their transaction and thus execute their code. Ethereum\u2019s\nsolution is based on the concept of gas, so named as to provide an analogy to auto-\nmobile fuel. Each instruction consumes a \ufb01xed amount of gas. Gas consumption in a\ntransaction is governed by three parameters:\n1.Gas price: the amount of ether the user is o\ufb00ering to pay the miner for one unit\nof gas.\n2.Transaction gas limit: the upper bound on transaction gas consumption. Trans-\nactions that exceed their gas limit are aborted. The miner keeps the payment, but\nthe transaction actions are never committed to the blockchain.\n3.Block gas limit: a limit in the blockchain system itself on the sum over all trans-\nactions in a block of their transaction gas limits.\nA user who sets a gas price too low may face a long wait to \ufb01nd a miner willing to\ninclude the transaction. Setting the gas price too high results in the user overpaying.\nAnother hard choice is that of the gas limit. It is hard to set the limit to the pre-\ncise amount of gas that the contract will use. A user who sets the limit too low risks\ntransaction failure, while a user who, feari ng \u201crunning out of gas,\u201d sets the transaction\ngas limit excessively high may \ufb01nd that miners are unwilling to include the transaction\nbecause it consumes too large a fraction of the block gas limit. The result of this is an\n", "1299": "26.6 Smart Contracts 1271\ninteresting problem for transaction designers in optimizing for both cost and speed of\nmining.\nIn a Bitcoin-style transaction, transaction ordering is explicit. In a state-based\nblockchain like Ethereum, there is no explicit concept of input transactions. However,\nthere may be important reasons why a smart contract may wish to enforce a transaction\norder. For transactions coming from the same account, Ethereum forces those trans-\nactions to be mined in the order in which the account created them by means of an\naccount nonce associated with the transaction. An account nonce is merely a sequence\nnumber associated with each transaction from an account, and the set of transactions\nfrom an account must have consecutive sequence numbers. Two transactions from an\naccount cannot have the same sequence number, and a transaction is accepted only\nafter the transaction with the previous sequence number has been accepted, thus pre-\nventing any cheating in transaction ordering. If the transactions to be ordered are from\ndi\ufb00erent accounts, they need to be designed such that the second transaction in the\nordering would fail to validate until after the \ufb01rst transaction is processed.\nT h ef a c tt h a tm i n e r sm u s tr u nt h es m a r t - c o n t r a c tc o d eo ft r a n s a c t i o n st h e yw i s h\nto include in a block, and that all full nodes must run the code of all transactions in\nmined blocks, regardless of which node mined the block, leads to a concern about\nsecurity. Code is run in a safe manner, usually on a virtual machine designed in the\nstyle of the Java virtual machine. Ethereum has its own virtual machine, called the\nEVM . Hyperledger executes code in Docker containers.\n26.6.2 External Input\nA smart contract may be de\ufb01ned in terms of external events. As a simple example,\nconsider a crop-insurance smart contract for a farmer that pays the farmer an amount\nof money dependent on the amount of rainfall in the growing season. Since the amount\nof rainfall in any future season is not known when the smart contract is written, that\nvalue cannot be hard-coded. Instead, input must be taken from an external source that\nis trusted by all parties to the smart contract. Such an external source is called an\noracle .6\nOracles are essential to smart contracts in many business applications. The fact that\nthe oracle must be trusted is a compromise on the general trustlessness of a blockchain\nenvironment. However, this is not a serious compromise in the sense that only the\nparties to a contract need to agree on any oracles used and, once that agreement is\nmade, the agreement is coded into the smart contract and is immutable from that\npoint forward.\nCorruption of an oracle after it is coded into an operating smart contract is a real\nproblem. This issue could be left as an externality for the legal system but ideally, a\nprocess for settlement of future disputes would be coded into the contract in a variety\nof ways. For example, parties to the contract could be required to send the contract\n6This term is rooted in ancient Greek culture and bears no relationship to the company by the same name.\n", "1300": "1272 Chapter 26 Blockchain Databases\ncerti\ufb01cation messages periodically, and code could be written de\ufb01ning actions to be\ntaken in case a party fails to recertify its approval of the oracle.\nDirect external output from a smart contract is problematic since such output\nwould have to occur during its execution and thus before the corresponding transaction\nis added to the blockchain. Ethereum, for example, deals with this by allowing a smart\ncontract to emit events that are then logged in the blockchain. The public visibility of\nthe blockchain then allows the actions of the smart contract to trigger activity external\nto the blockchain.\n26.6.3 Autonomous Smart Contracts\nIn many blockchains, including Ethereum, smart contracts can be deployed as indepen-\ndent entities. Such smart contracts have their own account, balance, and storage. This\nallows users (or other smart contracts) to use services provided by a smart contract\nand to send or receive currency from a smart contract.\nDepending on how a speci\ufb01c smart contract is coded, a user may be able, by design,\nto control the smart contract by sending it messages (transactions). A smart contract\nmay be coded so that it operates inde\ufb01nitely and autonomously. Such a contract is\nreferred to as a distributed autonomous organization (DAO ).7DAO s, once established,\nare di\ufb03cult to control and manage. There is no way to install bug \ufb01xes. In addition, there\nare many unanswered questions about legal and regulatory matters. However, the ability\nto create these entities that communicate, store data, and do business independent of\nany user is one of the most powerful features of the blockchain concept. In an enterprise\nsetting, smart contracts operate under some form of control by an organization or a\nconsortium.\nA smart contract may be used to create a currency on top of another currency.\nEthereum often serves as the base blockchain as this allows the rich existing ecosystem\nfor Ethereum to be leveraged to provide underlying infrastructure. Such higher-level\ncurrency units are called tokens, and the process of creating such currencies is referred\nto as an initial coin o\ufb00ering (ICO). An important added bene\ufb01t of using an existing\nblockchain as the basis for a token is that it is then possible to reuse key elements of the\nuser infrastructure, most importantly the wallet software users need to store tokens. The\nERC-20 Ethereum standard for tokens is widely used. More recent standards, including\nERC-223, ERC-621, ERC-721, ERC-777 ,a n d ERC-827 , are discussed in the references in\nthe Further Reading section at the end of the chapter.\nT h er e l a t i v ee a s eo fc r e a t i n ga n ICOhas made it an important method of funding\nnew ventures, but this has also led to several scams, resulting in attempts by govern-\nments to regulate this fundraising methodology.\nBeyond fundraising, an important application of smart contracts is to create inde-\npendent, autonomous service providers whose operation is controlled not by humans\n7The general use of DAO is distinct from a speci\ufb01c distributed autonomous organization called \u201cThe DAO\u201d. The DAO\nwas a crowdfunded venture-capital operation that failed d ue to a bug that enabled a major theft of funds (see Note 26.1\non page 1258).\n", "1301": "26.6 Smart Contracts 1273\nbut by source code, often open-source. In this way, trustless services that do not require\ntheir users to trust any person or organization can be created. As we noted earlier, a\nfully autonomous contract cannot be stopped or modi\ufb01ed. Thus, bugs last forever, and\nthe contract can continue as long as it can raise enough currency to support its oper-\nation (i.e., for Ethereum, earn enough ether to pay for gas). These risks suggest that\nsome compromise on the concept of trustlessness may make sense in smart-contract\ndesign, such as giving the contract creator the ability to send a self-destruct message to\nthe contract.\n26.6.4 Cross-Chain Transactions\nUp to this point, we have assumed implicitly that a blockchain transaction is limited\nto one speci\ufb01c blockchain. If one wished to transfer currency from an account on one\nblockchain to another account that is on a di\ufb00erent blockchain, not only is there the\nissue that the currencies are not the same, but also there is the problem that the two\nblockchains have to agree on the state of this cross-chain transaction at each point in\ntime.\nWe have seen a related problem for distributed databases. If a single organization\ncontrols the entire distributed system, then two-phase commit can be used. However, if\nthe system is controlled by multiple organizations as in the federated systems discussed\nin Section 23.5.3, coordination is more di\ufb03cult. In the blockchain setting, the high level\nof autonomy of each system and the requirement of immutability set an even higher\nbarrier.\nThe simplest solution is to use a trusted intermediary organization that operates\nmuch like one that exchanges traditional \ufb01at currencies.\nIf both users have accounts on both blockchains, a trustless transaction can be\nde\ufb01ned by creating transactions on each chain for the required funds transfer that are\ndesigned such that if one transaction is added to its blockchain its smart-contract code\nreveals a secret that ensures that other transactions cannot be canceled. Techniques\nused include the following, among others:\n\u2022Time-lock transactions that reverse after a certain period of time unless speci\ufb01c\nevents occur.\n\u2022Cross-chain exchange of Merkle-tree headers for validation purposes.\nA risk in these techniques is the possibility that a successfully mined transaction winds\nup on an orphaned fork, though there are ways to mitigate these risks. The details are\nsystem speci\ufb01c. See the Further Reading section at the end of the chapter.\nA more general solution is to create a smart contract that implements a market sim-\nilar conceptually to a stock exchange in which willing buyers and sellers are matched.\nSuch a contract operates in the role of trusted intermediary rather than a human-run\nbank or brokerage as would be used for \ufb01at currencies. The technical issues in cross-\nchain transactions remain an area of active research.\n", "1302": "1274 Chapter 26 Blockchain Databases\n26.7 Performance Enhancement\nAt a high level, a blockchain system may be viewed as having three major components:\n1.Consensus management: Proof-of-work, proof-of-stake, Byzantine consensus, or\nsome hybrid approach. Transaction processing performance is dominated by the\nperformance of consensus management.\n2.State-access management: Access methods to retrieve current blockchain state,\nranging from a simple index to locate transactions from a speci\ufb01c account-id or\nuser ID,t ok e y - v a l u es t o r es y s t e m s ,t oaf u l l SQL interface.\n3.Smart contract execution: The environment that runs the (possibly compiled)\nsmart-contract code, typically in a virtualized environment for security and safety.\nThe rate of transaction processing, referred to as throughput, in blockchain systems\nis signi\ufb01cantly lower than in traditional database systems. Traditional database systems\nare able to process simple funds-transfer transactions at peak rates on the order of tens\nof thousands of transactions per second. Blo ckchain systems\u2019 rates are less; Bitcoin\nprocesses less than 10 per second, and Ethereum, at present, only slightly more than\n10 per second.8The reason is that techniques such as proof-of-work limit the number\nof blocks that can be added to the chain per unit time, with Bitcoin targeting one\nblock every 10 minutes. A block may contain multiple transactions, so the transaction\nprocessing rate is signi\ufb01cantly more than 1 in 10 minutes, but is nevertheless limited.\nIn most applications, transaction throughput is not the only performance metric. A\nsecond and often more important metric is transaction latency, or response time. Here,\nthe distributed consensus required by blockchain systems presents a serious problem.\nAs an example, we consider Bitcoin\u2019s design in which the mining rate is maintained\nclose to 1 block every 10 minutes. That alone creates signi\ufb01cant latency, but added\nto that is the need to wait for several subsequent blocks to be mined so as to reduce\nthe probability that a fork will cause the transaction\u2019s block to be orphaned. Using the\nusual recommendation of waiting for 6 blocks, we get a true latency of 1 hour. Such\nresponse times are unacceptable for interactive, real-time transaction processing. In\ncontrast, traditional database systems commit individual transactions and can easily\nachieve millisecond response time.\nThese transaction processing performanc e issues are primarily issues due to con-\nsensus overhead with public blockchains. Permissioned blockchains are able to use\nfaster message-based Byzantine consensus algorithms, but other performance issues\nstill remain, and are continuing to be addressed.\n8At the time of publication, Ethereum\u2019s architects are contem plating advocating a fork to allow faster, lower-overhead\nmining.\n", "1303": "26.7 Performance Enhancement 1275\n26.7.1 Enhancing Consensus Performance\nThere are two primary approaches to improve the performance of blockchain consen-\nsus:\n1.Sharding: distributing the task of mining new blocks to enable parallelism among\nnodes.\n2.O\ufb00-chain transaction processing: Trusted systems that process transactions inter-\nnally without putting them on the blockchain. These transactions are grouped\ninto a single transaction that is then placed on the blockchain. This grouping\nmay occur with some agreed-upon periodicity or occur only at the termination\nof the agreement.\nSharding is the partitioning of the accounts in a blockchain into shards that are\nmined separately in parallel. In the case where a transaction spans shards, a separate\ntransaction is run on each shard with a special system-internal cross-shard transaction\nrecorded to ensure that both parts of the given transaction are committed. The over-\nhead of the cross-shard transaction is low. There are some risks resulting from the fact\nthat splitting the mining nodes up by shard results in smaller sets of miners that are\nthen more vulnerable to attack since the cost to attack a smaller set of miners is less.\nHowever, there are ways to mitigate this risk.\nO\ufb00-chain transactions require deployment of a separate system to manage those\ntransactions. The best known of these is the Lightning network, which not only\nspeeds blockchain transactions via o\ufb00-chain processing but also can process certain\ncross-chain transactions. Lightning promises transaction throughput and latency at\ntraditional database-system rates, but provides this at the cost of some degree of\nanonymity and immutability (i.e., transactions that commit o\ufb00-chain, but are rejected\nat the blockchain). By increasing the frequency of transaction con\ufb01rmations to the\nblockchain, one can decrease the loss of immutability at the price of reduced perfor-\nmance improvement.\n26.7.2 Enhancing Query Performance\nSome blockchain systems o\ufb00er little more than an index on user or account identi\ufb01ers\nto facilitate looking up unspent transactions. This su\ufb03ces for a simple funds-transfer\ntransaction. Complex smart contracts, however, may need to execute general-purpose\nqueries against the stored current state of the blockchain. Such queries may perform\nthe equivalent of join queries, whose optimization we studied at length in Chapter\n16. However, the structure of blockchain systems, in which state-access management\nmay be separate from the execution engine may limit the use of database-style query\noptimization. Furthermore, the data structures used for state representation, such as\nthe Merkle-Patricia tree structure we saw in Section 26.5.2, may limit the choice of\nalgorithms to implement join-style queries.\n", "1304": "1276 Chapter 26 Blockchain Databases\nBlockchain systems built on a traditional or a NoSQL database keep state informa-\ntion within that database and allow smart contracts to run higher-level database-style\nqueries against that state. Those advantages come at the cost of using a database-storage\nformat that may lack the rigorous cryptographic protection of a true blockchain. A\ngood compromise is for the database to be hosted by a trusted provider with updates\ngoing not only to the database but also to the blockchain, thus enabling any user who\nso wishes to validate the database against the secure blockchain.\n26.7.3 Fault-Tolerance and Scalability\nPerformance in the presence of failures is a critical aspect of a blockchain system.\nIn traditional database systems, this is measured by the performance of the recovery\nmanager and, as we saw in Section 19.9, the ARIES recovery algorithm is designed to\noptimize recovery time. A blockchain system, in contrast, uses a consensus mechanism\nand a replication strategy designed for continuous operation during failures and mali-\ncious attacks, though perhaps with lower performance during such periods. Therefore,\nbesides measuring throughput and latency, one must also measure how these perfor-\nmance statistics change in times of failure or attack.\nScalability is a performance concern in any distributed system as we saw in Chap-\nter 20. The architectural di\ufb00erences between blockchain systems and parallel or dis-\ntributed database systems introduce challenges in both the measure of scaleup and its\noptimization. We illustrate the di\ufb00erences by considering the relative scalability of 2PC\nand Byzantine consensus. In 2PC, a transaction accessing a \ufb01xed number of nodes, say\n\ufb01ve, needs only the agreement of these \ufb01ve nodes, regardless of the number of nodes\nin the system. If we scale the system up to more nodes, that transaction still needs only\nthose \ufb01ve nodes to agree (unless the scaling added a replica site). Under Byzantine con-\nsensus, every transaction needs the agreement of a majority of the non-failed nodes,\nand so, the number of nodes that must agree not only starts much larger but also grows\nfaster as the network scales.\nThe Further Reading section at the end of the chapter provides references that deal\nwith the emerging issue of blockchain performance measurement and optimization.\n26.8 Emerging Applications\nHaving seen how blockchains work and the bene\ufb01ts they o\ufb00er, we can look at areas\nwhere blockchain technology is currently in use or may be used in the near future.\nApplications most likely to bene\ufb01t from the use of a blockchain are those that\nhave high-value data, including possibly historical data, that need to be kept safe from\nmalicious modi\ufb01cation. Updates would consist mostly of appends in such applications.\nAnother class of applications that are likely to bene\ufb01t area those that involve multiple\ncooperating parties, who trust each other to some extent, but not fully, and desire to\nhave a shared record of transactions that are digitally signed, and are kept safe from\ntampering. In this latter case, the cooperating parties could include the general public.\n", "1305": "26.8 Emerging Applications 1277\nBelow, we provide a list of several application domains along with a short explana-\ntion of the value provided by a blockchain implementation of the application. In some\ncases, the value added by a blockchain is a novel capability; in others, the value added\nis the ability to do something that could have been done previously only at prohibitive\ncost.\n\u2022Academic certi\ufb01cates and transcripts: Universities can put student certi\ufb01cates and\ntranscripts on a public blockchain secured by the student\u2019s public key and signed\ndigitally by the university. Only the student can read the records, but the student\ncan then authorize access to those records. As a result, students can obtain cer-\nti\ufb01cates and transcripts for future study or for prospective employers in a secure\nmanner from a public source. This approach was prototyped by MIT in 2017.\n\u2022Accounting and audit: Double-entry bookkeeping is a fundamental principle of\naccounting that helps ensure accurate and auditable records. A similar bene\ufb01t can\nbe gained from cryptographically signed blockchain entries in a digital distributed\nledger. In particular, the use of a blockchain ensures that the ledger is tamperproof,\neven against insider attacks and hackers who may gain control of the database.\nAlso, if the enterprise\u2019s auditor is a participant, then ledger entries can become\nvisible immediately to auditors, enabling a continuous rather than periodic audit.\n\u2022Asset management: Tracking ownership records on a blockchain enables veri\ufb01able\naccess to ownership records and secure, signed updates. As an example, real-estate\nownership records, a matter of public record, could be made accessible to the\npublic on a blockchain, while updates to those records could be made only by\ntransactions signed by the parties to the transaction. A similar approach can be\napplied to ownership of \ufb01nancial assets such as stocks and bonds. While stock\nexchanges manage trading of stocks and bonds, long term records are kept by\ndepositories that users must trust. Blockchain can help track such assets without\nhaving to trust a depository.\n\u2022E-government: A single government blockchain would eliminate agency dupli-\ncation of records and create a common, authoritative information source. A\nhighly notable user of this approach is the government of Estonia, which uses its\nblockchain for taxation, voting, health, and an innovative \u201ce-Residency\u201d program.\n\u2022Foreign-currency exchange: International \ufb01nancial transactions are often slow\nand costly. Use of an intermediary cryptocurrency can enable blockchain-based\nforeign-currency exchange at a relative r apid pace with full, irrefutable traceabil-\nity. Ripple is o\ufb00ering such capability using the XRP currency.\n\u2022Health care: Health records are notorious for their nonavailability across health-\ncare providers, their inconsistency, and their inaccuracy even with the increased\nuse of electronic health records. Data are added from a large number of sources\nand the provenance of materials used may not be well documented (see discussion\nof supply chains below). A uni\ufb01ed blockchain is suitable for distributed update,\n", "1306": "1278 Chapter 26 Blockchain Databases\nand cryptographic data protection, unlockable by the patient\u2019s private key, would\nenable access to a patient\u2019s full health record anytime, anywhere in an emergency.\nThe actual records may be kept o\ufb00chain, but the blockchain acts as the trusted\nmechanism for accessing the data.\n\u2022Insurance claims: The processing of insurance claims is a complex work\ufb02ow of data\nfrom the scene of the claim, various contractors involved in repairs, statements\nfrom witnesses, etc. A blockchain\u2019s ability to capture data from many sources\nand distribute it rapidly, accurately, and securely, promises e\ufb03ciency and accu-\nracy gains in the insurance industry.\n\u2022Internet of things: The Internet of Things ( IoT) is a term that refers to systems\nof many interacting devices (\u201cthings\u201d), i ncluding within smart buildings, smart\ncities, self-monitoring civil infrastructure, and so on. These devices could act as\nnodes that can pass blockchain transactions into the network without having to\nensure the transmission of data reaches a central server. In the late 2010s, research\nis underway to see if this data-collection approach can be e\ufb00ective in lowering\ncosts and increasing performance. Adding so many entries to a blockchain in a\nshort period of time may suggest a replacement of the chain data structure with\na directed, acyclic graph. The Iota blockchain is an example of one such system,\nwhere the graph structure is called a tangle .\n\u2022Loyalty programs and aggregation of transactions: There are a variety of situa-\ntions where a customer or user makes multiple purchases from the same vendor,\nsuch as within a theme park, inside a video game, or from a large online retailer.\nThese vendors could create internal cryptocurrencies in a proprietary, permis-\nsioned blockchain, with currency value pegged to a \ufb01at currency like the dollar.\nThe vendor gains by replacing credit-card transactions with vendor-internal trans-\nactions. This saves credit-card fees and allows the vendor to capture more of the\nvaluable customer data coming from these transactions. The same concept can\napply to retail loyalty points, exempli\ufb01ed by airline frequent-\ufb02yer miles. It is costly\nfor vendors to maintain these systems and coordinate with partner vendors in the\nprogram. A blockchain-based system allows the hosting vendor to distribute the\nworkload among the partners and allows transactions to be posted in a decentral-\nized manner, releaving the vendor from have to run its own online transaction\nprocessing system. In the late 2010s, busi ness strategies were being tested around\nthese concepts.\n\u2022Supply chain: Blockchain enables every participant in a supply chain to log every\naction. This facilitates tracking the movement of every item in the chain rather\nthan only aggregates like crates, shipments, etc. In the event of a recall, the set\nof a\ufb00ected products can be pinpointed to a smaller set of products and done so\nquickly. When a quality issue suggests a recall, some supply-chain members may\nbe tempted to cover up their role, but the immutability of the blockchain prevents\nrecord falsi\ufb01cation after the fact.\n", "1307": "26.9 Summary 1279\n\u2022Tickets for events: Suppose a person Ahas bought tickets for an event, but now\nwishes to sell them, and Bbuys the ticket from A. Given that tickets are all sold\nonline, Bwould need to trust that the ticket given by Ais genuine, and Ahas not\nalready sold the ticket, that is, the ticket has not been double-spent. If ticket trans-\nactions are carried out on a blockchain, double-spending can be detected easily.\nTickets can be veri\ufb01ed if they are signed digitally by the event organizer (whether\nor not they are on a blockchain).\n\u2022Trade \ufb01nance: Companies often depend on loans from banks, issued through let-\nters of credits, to \ufb01nance purchases. Such letters of credit are issued against goods\nbased on bills of lading indicating that the goods are ready for shipment. The\nownership of the goods (title) is then transferred to the buyer. These transactions\ninvolve multiple parties including the seller, buyer, the buyer\u2019s bank, the seller\u2019s\nbank, a shipping company and so forth, which trust each other to some extent, but\nnot fully. Traditionally, these processes were based on physical documents that\nhave to be signed and shipped between parties that may be anywhere on the globe,\nresulting in signi\ufb01cant delays in these processes. Blockchain technology can be\nused to keep these documents in a digital form, and automate these processes in a\nway that is highly secure yet very fast (at least compared to processing of physical\ndocuments).\nOther applications beyond those we have listed continue to emerge.\n26.9 Summary\n\u2022Blockchains provide a degree of privacy, anonymity, and decentralization that is\nhard to achieve with a traditional database.\n\u2022Public blockchains are accessibly openly on the internet. Permissioned\nblockchains are managed by an organization and usually serve a speci\ufb01c enter-\nprise or group of enterprises.\n\u2022The main consensus mechanisms for public blockchains are proof-of-work and\nproof-of-stake. Miners compete to add the next block to the blockchain in exchange\nfor a reward of blockchain currency.\n\u2022Many permissioned blockchains use a Byzantine consensus algorithm to choose\nthe node to add the next block to the chain.\n\u2022Nodes adding a block to the chain \ufb01rst validate the block. Then all full nodes\nmaintaining a replica of the chain validate the new block.\n\u2022Key blockchain properties include irrefutability and tamper resistance.\n\u2022Cryptographic hash functions must exhibit c ollision resistance, irreversibility, and\npuzzle friendliness.\n", "1308": "1280 Chapter 26 Blockchain Databases\n\u2022Public-key encryption is based on a user having both a public and private key to\nenable both the encryption of data and the digital signature of documents.\n\u2022Proof-of-work requires a large amount of computation to guess a successful nonce\nthat allows the hash target to be met. Proof-of-stake is based on ownership of\nblockchain currency. Hybrid schemes are possible.\n\u2022Smart contracts are executable pieces of code in a blockchain. In some chains,\nthey may operate as independent entities with their own data and account. Smart\ncontracts may encode complex business agreements and they may provide ongoing\nservices to nodes participating in the blockchain.\n\u2022Smart contracts get input from the outside world via trusted oracles that serve as\nar e a l - t i m ed a t as o u r c e .\n\u2022Blockchains that retain state can serve in a manner similar to a database system\nand may bene\ufb01t from the use of database indexing methods and access optimiza-\ntion, but the blockchain structure may place limits on this.\nReview Terms\n\u2022Public and permissioned blockchain\n\u2022Cryptographic hash\n\u2022Mining\n\u2022Light and full nodes\n\u2022Proof-of-work\n\u2022Proof-of-stake\n\u2022Byzantine consensus\n\u2022Tamper resistance\n\u2022Collision resistance\n\u2022Irreversibility\n\u2022Public-key encryption\n\u2022Digital signature\n\u2022Irrefutability\n\u2022Forks: hard and soft\u2022Double spend\n\u2022Orphaned block\n\u2022Nonce\n\u2022Block validation\n\u2022Merkle tree\n\u2022Patricia tree\n\u2022Bitcoin\n\u2022Ethereum\n\u2022Gas\n\u2022Smart contract\n\u2022Oracles\n\u2022Cross-chain transaction\n\u2022Sharding\n\u2022O\ufb00-chain processing\nPractice Exercises\n26.1 What is a blockchain fork? List the two types of fork and explain their di\ufb00er-\nences.\n", "1309": "Exercises 1281\n26.2 Consider a hash function h(x)=xmod 2256, that is, the hash function returns\nthe last 256 bits of x.\nDoes this function have\na. collision resistance\nb. irreversibility\nc. puzzle friendliness\nWhy or why not?\n26.3 If you were designing a new public blockchain, why might you choose proof-\nof-stake rather than proof-of-work?\n26.4 If you were designing a new public blockchain, why might you choose proof-\nof-work rather than proof-of-stake?\n26.5 Explain the distinction between a public and a permissioned blockchain and\nwhen each would be more desirable.\n26.6 Data stored in a blockchain are protected by the tamper-resistance property\nof a blockchain. In what way is this tamper resistance more secure in practice\nthan the security provided by a traditional enterprise database system?\n26.7 In a public blockchain, how might someone determine the real-world identity\nthat corresponds to a given user ID?\n26.8 What is the purpose of gasin Ethereum?\n26.9 Suppose we are in an environment where users can be assumed not to be ma-\nlicious. In that case, what advantages, if any, does Byzantine consensus have\nover 2PC?\n26.10 Explain the bene\ufb01ts and potential risks of sharding.\n26.11 Why do enterprise blockchains often incorporate database-style access?\nExercises\n26.12 In what order are blockchain transactions serialized?\n26.13 Since blockchains are immutable, how is a transaction abort implemented so\nas not to violate immutability?\n26.14 Since pointers in a blockchain include a cryptographic hash of the previous\nblock, why is there the additional need for replication of the blockchain to\nensure immutability?\n26.15 Suppose a user forgets or loses her or his private key? How is the user a\ufb00ected?\n", "1310": "1282 Chapter 26 Blockchain Databases\n26.16 How is the di\ufb03culty of proof-of-work mining adjusted as more nodes join the\nnetwork, thus increasing the total computational power of the network? De-\nscribe the process in detail.\n26.17 Why is Byzantine consensus a poor consensus mechanism in a public\nblockchain?\n26.18 Explain how o\ufb00-chain transaction processing can enhance throughput. What\nare the trade-o\ufb00s for this bene\ufb01t?\n26.19 Choose an enterprise of personal interest to you and explain how blockchain\ntechnology could be employed usefully in that business.\nTools\nOne can download blockchain software to create a full node for public blockchains\nsuch as Bitcoin ( bitcoin.org ) and Ethereum ( www.ethereum.org ) and begin mining,\nthough the economic return for the investment of power may be questionable. Tools\nexist also to join mining pools. Browsing tools exist to view the contents of public\nblockchains. For some blockchains, notably Ethereum, it is possible to install a private\ncopy of the blockchain software managing a private blockchain as an educational tool.\nEthereum also o\ufb00ers a public test network where smart contracts can be debugged\nwithout the expense of gas on the real network.\nHyperledger ( www.hyperledger.org ) which is supported by a large consortium\nof companies, provides a wide variety of open source blockchain platforms and\ntools. Corda ( www.corda.net ) and BigchainDB ( www.bigchaindb.com )a r et w o\nother blockchain platforms, with BigchainDB having a speci\ufb01c focus on blockchain\ndatabases.\nBlockchain based systems for supporting academic certi\ufb01cates and medical\nrecords, such as Blockcert and Medrec (both from MIT), and several other applica-\ntions are available online. The set of tools for blockchain are evolving rapidly. Due to\nthe rapid rate of change and development, as of late 2018 we are unable to identify a\nbest set of tools, beyond the few mentioned above, that we can recommend. We recom-\nmend you perform a web search for the latest tools.\nFurther Reading\nThe newness of blockchain technology and applications means that, unlike the more\nestablished technical topics elsewhere in this text, there are fewer references in the\nacademic literature and fewer textbooks. Many of the key papers are published only\non the website of a particular blockchain. The URLs for those references are likely\nto change often. Thus, web searches for key topics are a highly important source for\nfurther reading. Here, we cite some classic references as well as URLsc u r r e n ta so ft h e\npublication date.\n", "1311": "Further Reading 1283\nThe original Bitcoin paper [Nakamoto (2008)] is authored under a pseudonym,\nwith the identity of the author or authors still the subject of speculation. The original\nEthereum paper [Buterin (2013)] has been superseded by newer Ethereum white pa-\npers (see ethereum.org ), but the original work by Ethereum\u2019s creator, Vitalik Buterin,\nremains interesting reading. Solidity, the primary programming language for Ethereum\nsmart contracts, is discussed in solidity.readthedocs.io .T h e ERC-20 standard is de-\nscribed in [Vogelsteller and Buterin (2013)] and the proposed (as of the publication\ndate of this text) Casper upgrade to the performance of Ethereum\u2019s consensus mech-\nanism appears in [Buterin and Gri\ufb03th (2017)]. Another approach to using proof-of-\nstake is used by the Cardano blockchain ( www.cardano.org ).\nMany of the theoretical results that make blockchain possible were \ufb01rst developed\nin the 20th century. The concepts behind cryptographic hash functions and public-key\nencryption were introduced in [Di\ufb03e and Hellman (1976)] and [Rivest et al. (1978)].\nA good reference for cryptography is [Katz and Lindell (2014)]. [Narayanan et al.\n(2016)] is a good reference for the basics of cryptocurrency, though its focus is mainly\non Bitcoin. There is a large body of literat ure on Byzantine consensus. Early papers\nthat laid the foundation for this work include [Pease et al. (1980)] and [Lamport et al.\n(1982)]. Practical Byzantine fault toleran ce ([Castro and Liskov (1999)]) serves as the\nbasis for much of the current blockchain Byzantine consensus algorithms. [Mazi `eres\n(2016)] describes in detail a consensus proto col speci\ufb01cally designed to allow for open,\nrather than permissioned, membership in the consensus group. References pertaining\nto Merkle trees appears in Chapter 23. Patricia trees were introduced in [Morrison\n(1968)].\nA benchmarking framework for permissioned blockchains appears in [Dinh et al.\n(2017)]. A detailed comparison of blockchain systems appears in [Dinh et al. (2018)].\nForkBase, a storage system designed for improved blockchain performance, is dis-\ncussed in [Wang et al. (2018)].\nThe Lightning network( lightning.network ) aims to accelerate Bitcoin transactions\nand provide some degree of cross-chain transactions. Ripple ( ripple.com )p r o v i d e s\na network for international \ufb01at currency exchange using the XRP token. Loopring\n(loopring.org ) is a cryptocurrency exchange platform that allows users to retain con-\ntrol of their currency without having to surrender control to the exchange.\nMany of the blockchains discussed in the chapter have their best descriptions\non their respective web sites. These include Corda ( docs.corda.net ), Iota ( iota.org ),\nand Hyperledger ( www.hyperledger.org ). Many \ufb01nancial \ufb01rms are creating their own\nblockchains, and some of those are publicly available, including J.P. Morgan\u2019s Quorum\n(www.jpmorgan.com/global/Quorum ).\nBibliography\n[Buterin (2013)] V. Buterin, \u201cEthereum: The Ultimate Smart Contract and Decentralized\nApplication Platform\u201d, Technical report (2013).\n", "1312": "1284 Chapter 26 Blockchain Databases\n[Buterin and Gri\ufb03th (2017)] V. Buterin and V. Gri\ufb03th, \u201cCasper the Friendly Finality Gad-\nget\u201d, Technical report (2017).\n[Castro and Liskov (1999)] M. Castro and B. Liskov, \u201cPractical Byzantine Fault Tolerance\u201d,\nInSymp. on Operating Systems Design and Implementation (OSDI) , USENIX (1999).\n[Di\ufb03e and Hellman (1976)] W. Di\ufb03e and M. E. Hellman, \u201cNew Directions in Cryptogra-\nphy\u201d, IEEE Transactions on Information Theory , Volume 22, Number 6 (1976).\n[Dinh et al. (2017)] T. T. A. Dinh, J. Wang, G. Chen, R. Liu, B. C. Ooi, and K.-L. Tan,\n\u201cBLOCKBENCH: A Framework for Analyzing Private Blockchains\u201d, In Proc. of the ACM\nSIGMOD Conf. on Management of Data (2017), pages 1085\u20131100.\n[Dinh et al. (2018)] T. T. A. Dinh, R. Liu, M. H. Zhang, G. Chen, B. C. Ooi, and J. Wang,\n\u201cUntangling Blockchain: A Data Processing View of Blockchain Systems\u201d, volume 30 (2018),\npages 1366\u20131385.\n[Katz and Lindell (2014)] J. Katz and Y. Lindell, Introduction to Modern Cryptography ,3 r d\nedition, Chapman and Hall/CRC (2014).\n[Lamport et al. (1982)] L. Lamport, R. Shostak, and M. Pease, \u201cThe Byzantine Generals\nProblem\u201d, ACM Transactions on Programming Languages and Systems , Volume 4, Number 3\n(1982), pages 382\u2013401.\n[Mazi `eres (2016)] D. Mazi` eres, \u201cThe Stellar Consensus Protocol\u201d, Technical report (2016).\n[Morrison (1968)] D. Morrison, \u201cPractical Algorithm To Retrieve Information Coded in Al-\nphanumeric\u201d, Journal of the ACM , Volume 15, Number 4 (1968), pages 514\u2013534.\n[Nakamoto (2008)] S. Nakamoto, \u201cBitcoin: A Peer-to-Peer Electronic Cash System\u201d, Tech-\nnical report, Bitcoin.org (2008).\n[Narayanan et al. (2016)] A. Narayanan, J. Bonneau, E. Felten, A. Miller, and S. Goldfeder,\nBitcoin and Cryptocurrency Technologies , Princeton University Press (2016).\n[Pease et al. (1980)] M. Pease, R. Shostak, and L. Lamport, \u201cReaching Agreement in the\nPresence of Faults\u201d, Journal of the ACM , Volume 27, Number 2 (1980), pages 228\u2013234.\n[Rivest et al. (1978)] R. L. Rivest, A. Shamir, and L. Adleman, \u201cA Method for Obtaining\nDigital Signatures and Public-Key Cryptosystems\u201d, Communications of the ACM ,V o l u m e2 1 ,\nNumber 2 (1978), pages 120\u2013126.\n[Vogelsteller and Buterin (2013)] F. Vogelsteller and V. Buterin, \u201cERC-20 Token Standard\u201d,\nTechnical report (2013).\n[Wang et al. (2018)] S. Wang, T. T. A. Dihn, Q. Lin, Z. Xie, M. Zhang, Q. Cai, G. Chen,\nB. C. Ooi, and P. Ruan, \u201cForkBase: An E\ufb03cient Storage Engine for Blockchain and Forkable\nApplications\u201d, In Proc. of the International Conf. on Very Large Databases (2018), pages 1085\u2013\n1100.\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\n", "1313": "PART10\nAPPENDIX A\nAppendix A presents the full details of the university database that we have used as our\nrunning example, including an E-Rdiagram, SQL DDL , and sample data that we have\nused throughout the book. (The DDL and sample data are also available on the web\nsite of the book, db-book.com , for use in laboratory exercises.)\n1285\n", "1314": "", "1315": "APPENDIXA\nDetailed University Schema\nIn this appendix, we present the full details of our running-example university database.\nIn Section A.1 we present the full schema as used in the text and the E-Rdiagram that\ncorresponds to that schema. In Section A.2 we present a relatively complete SQL data\nde\ufb01nition for our running university example. Besides listing a datatype for each at-\ntribute, we include a substantial number of constraints. Finally, in Section A.3, we\npresent sample data that correspond to our schema. SQL s c r i p t st oc r e a t ea l lt h er e l a -\ntions in the schema, and to populate them with sample data, are available on the web\nsite of the book, db-book.com .\nA.1 Full Schema\nThe full schema of the university database that is used in the text follows. The corre-\nsponding schema diagram, and the one used throughout the text, is shown in Figure\nA.1.\nclassroom (building\n ,room\n number\n ,capacity )\ndepartment (dept\n name\n ,building ,budget )\ncourse (course\n id\n,title,dept\n name ,credits )\ninstructor (ID\n,name ,dept\n name ,salary )\nsection (course\n id\n,sec\nid\n,semester\n ,year\n,building ,room\n number ,time\n slot\nid)\nteaches (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n)\nstudent (ID\n,name ,dept\n name ,tot\ncred)\ntakes (ID\n,course\n id\n,sec\nid\n,semester\n ,year\n,grade )\nadvisor (s\nID\n,i\nID)\ntime\n slot(time\n slot\nid\n,day\n,start\n time\n,end\n time)\nprereq (course\n id\n,prereq\n id\n)\n1287\n", "1316": "1288 Appendix A Detailed University Schema\nID\ncourse_id\nsec_id\nsemester\nyear\ngradeID\nname\ndept_name\ntot_cred\nbuilding\nroom_number\ncapacitys_id\ni_id\nID\ncourse_id\nsec_id\nsemester\nyeartakes\nsection\nclassroom\nteachesprereq\ncourse_id\nprereq_idcourse_id\ntitle\ndept_name\ncreditscoursestudent\ndept_name\nbuilding\nbudgetdepartment\ninstructor\nID\nname\ndept_name\nsalaryadvisor\ntime_slot\ntime_slot_id\nday\nstart_time\nend_timecourse_id\nsec_id\nsemester\nyear\nbuilding\nroom_number\ntime_slot_id\nFigure A.1 Schema diagram for the university database.\nA.2 DDL\nIn this section, we present a relatively complete SQL data de\ufb01nition for our example.\nBesides listing a datatype for each attribut e, we include a substantial number of con-\nstraints.\ncreate table classroom\n(building varchar (15),\nroom\n number varchar (7),\ncapacity numeric (4,0),\nprimary key (building ,room\n number ));\ncreate table department\n(dept\n name varchar (20),\nbuilding varchar (15),\nbudget numeric (12,2) check (budget >0 ) ,\nprimary key (dept\n name ));\n", "1317": "A.2 DDL 1289\ncreate table course\n(course\n id varchar (7),\ntitle varchar (50),\ndept\n name varchar (20),\ncredits numeric (2,0) check (credits >0 ) ,\nprimary key (course\n id),\nforeign key (dept\n name )references department\non delete set null );\ncreate table instructor\n(ID varchar (5),\nname varchar (20) not null ,\ndept\n name varchar (20),\nsalary numeric (8,2) check (salary > 29000),\nprimary key (ID),\nforeign key (dept\n name )references department\non delete set null );\ncreate table section\n(course\n id varchar (8),\nsec\nid varchar (8),\nsemester varchar (6)check (semester in\n(\u2019Fall\u2019, \u2019Winter\u2019, \u2019Spring\u2019, \u2019Summer\u2019)),\nyear numeric (4,0) check (year >1 7 0 1a n d year < 2100),\nbuilding varchar (15),\nroom\n number varchar (7),\ntime\n slot\nid varchar (4),\nprimary key (course\n id,sec\nid,semester ,year),\nforeign key (course\n id)references course\non delete cascade ,\nforeign key (building ,room\n number )references classroom\non delete set null );\nIn the preceding DDL ,w ea d dt h e on delete cascade speci\ufb01cation to a foreign key\nconstraint if the existence of the tuple depends on the referenced tuple. For example,\nwe add the on delete cascade speci\ufb01cation to the foreign key constraint from section\n(which was generated from weak entity section ), to course (which was its identifying re-\nlationship). In other foreign key constraints we either specify on delete set null ,w h i c h\nallows deletion of a referenced tuple by setting the referencing value to null, or we do\nnot add any speci\ufb01cation, which prevents the deletion of any referenced tuple. For ex-\nample, if a department is deleted, we would not wish to delete associated instructors;\n", "1318": "1290 Appendix A Detailed University Schema\nthe foreign key constraint from instructor todepartment instead sets the dept\n name at-\ntribute to null. On the other hand, the foreign key constraint for the prereq relation,\nshown later, prevents the deletion of a course that is required as a prerequisite for an-\nother course. For the advisor relation, shown later, we allow i\nIDto be set to null if an\ninstructor is deleted but delete an advisor tuple if the referenced student is deleted.\ncreate table teaches\n(ID varchar (5),\ncourse\n id varchar (8),\nsec\nid varchar (8),\nsemester varchar (6),\nyear numeric (4,0),\nprimary key (ID,course\n id,sec\nid,semester ,year),\nforeign key (course\n id,sec\nid,semester ,year)references section\non delete cascade ,\nforeign key (ID)references instructor\non delete cascade );\ncreate table student\n(ID varchar (5),\nname varchar (20) not null ,\ndept\n name varchar (20),\ntot\ncred numeric (3,0) check (tot\ncred>= 0),\nprimary key (ID),\nforeign key (dept\n name )references department\non delete set null );\ncreate table takes\n(ID varchar (5),\ncourse\n id varchar (8),\nsec\nid varchar (8),\nsemester varchar (6),\nyear numeric (4,0),\ngrade varchar (2),\nprimary key (ID,course\n id,sec\nid,semester ,year),\nforeign key (course\n id,sec\nid,semester ,year)references section\non delete cascade ,\nforeign key (ID)references student\non delete cascade );\n", "1319": "A.2 DDL 1291\ncreate table advisor\n(s\nID varchar (5),\ni\nID varchar (5),\nprimary key (s\nID),\nforeign key (i\nID)references instructor (ID)\non delete set null ,\nforeign key (s\nID)references student (ID)\non delete cascade );\ncreate table prereq\n(course\n id varchar (8),\nprereq\n id varchar (8),\nprimary key (course\n id,prereq\n id),\nforeign key (course\n id)references course\non delete cascade ,\nforeign key (prereq\n id)references course );\nThe following create table statement for the table time\n slotcan be run on most\ndatabase systems, but it does not work on Oracle (at least as of Oracle version 11),\ns i n c eO r a c l ed o e sn o ts u p p o r tt h e SQL standard type time.\ncreate table timeslot\n(time\n slot\nid varchar (4),\nday varchar (1)check (dayin( \u2019 M \u2019 ,\u2019 T \u2019 ,\u2019 W \u2019 ,\u2019 R \u2019 ,\u2019 F \u2019 ,\u2019 S \u2019 ,\u2019 U \u2019 ) ) ,\nstart\n time time,\nend\n time time,\nprimary key (time\n slot\nid,day,start\n time));\nThe syntax for specifying time in SQL is illustrated by these examples: \u201908:30\u2019,\n\u201913:55\u2019, and \u20195:30 PM\u2019. Since Oracle does not support the time type, for Oracle we use\nthe following schema instead:\ncreate table timeslot\n(time\n slot\nid varchar (4),\nday varchar (1),\nstart\n hr numeric (2) check ( start\n hr> =0a n d end\n hr<2 4 ) ,\nstart\n min numeric (2) check ( start\n min> =0a n d start\n min< 60),\nend\n hr numeric (2) check ( end\n hr> =0a n d end\n hr<2 4 ) ,\nend\n min numeric (2) check ( end\n min>= 0 and end\n min< 60),\nprimary key (time\n slot\nid,day,start\n hr,start\n min));\nThe di\ufb00erence is that start\n time has been replaced by two attributes start\n hrand\nstart\n min, and similarly end\n time has been replaced by attributes end\n hrand end\n min.\n", "1320": "1292 Appendix A Detailed University Schema\nThese attributes also have constraints that ensure that only numbers representing valid\ntime values appear in those attributes. This version of the schema for time\n slotworks\non all databases, including Oracle. Note that although Oracle supports the datetime\ndatatype, datetime includes a speci\ufb01c day, month, and year as well as a time, and is\nnot appropriate here since we want only a time. There are two alternatives to splitting\nthe time attributes into an hour and a minute component, but neither is desirable. The\n\ufb01rst alternative is to use a varchar type, but that makes it hard to enforce validity con-\nstraints on the string as well as to perform comparison on time. The second alternative\nis to encode time as an integer representing a number of minutes (or seconds) from\nmidnight, but this alternative requires extra code with each query to convert values be-\ntween the standard time representation and the integer encoding. We therefore choose\nthe two-part solution.\nA.3 Sample Data\nIn this section we provide sample data for each of the relations de\ufb01ned in the previous\nsection.\nbuilding\n room\n number\n capacity\nPackard\n 101\n 500\nPainter\n 514\n 10\nTaylor\n 3128\n 70\nWatson\n 100\n 30\nWatson\n 120\n 50\nFigure A.2 The classroom relation.\ndept\n name\n building\n budget\nBiology\n Watson\n 90000\nComp. Sci.\n Taylor\n 100000\nElec. Eng.\n Taylor\n 85000\nFinance\n Painter\n 120000\nHistory\n Painter\n 50000\nMusic\n Packard\n 80000\nPhysics\n Watson\n 70000\nFigure A.3 The department relation.\n", "1321": "A.3 Sample Data 1293\nCredits\nThe photo of the sailboats in the beginning of the chapter is due to \u00a9Pavel Nes-\nvadba/Shutterstock.\ncourse\n id\n title\n dept\n name\n credits\nBIO-101\n Intro. to Biology\n Biology\n 4\nBIO-301\n Genetics\n Biology\n 4\nBIO-399\n Computational Biology\n Biology\n 3\nCS-101\n Intro. to Computer Science\n Comp. Sci.\n 4\nCS-190\n Game Design\n Comp. Sci.\n 4\nCS-315\n Robotics\n Comp. Sci.\n 3\nCS-319\n Image Processing\n Comp. Sci.\n 3\nCS-347\n Database System Concepts\n Comp. Sci.\n 3\nEE-181\n Intro. to Digital Systems\n Elec. Eng.\n 3\nFIN-201\n Investment Banking\n Finance\n 3\nHIS-351\n World History\n History\n 3\nMU-199\n Music Video Production\n Music\n 3\nPHY-101\n Physical Principles\n Physics\n 4\nFigure A.4 The course relation.\nID\n name\n dept\n name\n salary\n10101\n Srinivasan\n Comp. Sci.\n 65000\n12121\n Wu\n Finance\n 90000\n15151\n Mozart\n Music\n 40000\n22222\n Einstein\n Physics\n 95000\n32343\n El Said\n History\n 60000\n33456\n Gold\n Physics\n 87000\n45565\n Katz\n Comp. Sci.\n 75000\n58583\n Cali\ufb01eri\n History\n 62000\n76543\n Singh\n Finance\n 80000\n76766\n Crick\n Biology\n 72000\n83821\n Brandt\n Comp. Sci.\n 92000\n98345\n Kim\n Elec. Eng.\n 80000\nFigure A.5 The instructor relation.\n", "1322": "1294 Appendix A Detailed University Schema\ncourse\n id\n sec\nid\n semester\n year\n building\n room\n number\n time\n slot\nid\nBIO-101\n 1\n Summer\n 2017\n Painter\n 514\n B\nBIO-301\n 1\n Summer\n 2018\n Painter\n 514\n A\nCS-101\n 1\n Fall\n 2017\n Packard\n 101\n H\nCS-101\n 1\n Spring\n 2018\n Packard\n 101\n F\nCS-190\n 1\n Spring\n 2017\n Taylor\n 3128\n E\nCS-190\n 2\n Spring\n 2017\n Taylor\n 3128\n A\nCS-315\n 1\n Spring\n 2018\n Watson\n 120\n D\nCS-319\n 1\n Spring\n 2018\n Watson\n 100\n B\nCS-319\n 2\n Spring\n 2018\n Taylor\n 3128\n C\nCS-347\n 1\n Fall\n 2017\n Taylor\n 3128\n A\nEE-181\n 1\n Spring\n 2017\n Taylor\n 3128\n C\nFIN-201\n 1\n Spring\n 2018\n Packard\n 101\n B\nHIS-351\n 1\n Spring\n 2018\n Painter\n 514\n C\nMU-199\n 1\n Spring\n 2018\n Packard\n 101\n D\nPHY-101\n 1\n Fall\n 2017\n Watson\n 100\n A\nFigure A.6 The section relation.\nID\n course\n id\n sec\nid\n semester\n year\n10101\n CS-101\n 1\n Fall\n 2017\n10101\n CS-315\n 1\n Spring\n 2018\n10101\n CS-347\n 1\n Fall\n 2017\n12121\n FIN-201\n 1\n Spring\n 2018\n15151\n MU-199\n 1\n Spring\n 2018\n22222\n PHY-101\n 1\n Fall\n 2017\n32343\n HIS-351\n 1\n Spring\n 2018\n45565\n CS-101\n 1\n Spring\n 2018\n45565\n CS-319\n 1\n Spring\n 2018\n76766\n BIO-101\n 1\n Summer\n 2017\n76766\n BIO-301\n 1\n Summer\n 2018\n83821\n CS-190\n 1\n Spring\n 2017\n83821\n CS-190\n 2\n Spring\n 2017\n83821\n CS-319\n 2\n Spring\n 2018\n98345\n EE-181\n 1\n Spring\n 2017\nFigure A.7 The teaches relation.\n", "1323": "A.3 Sample Data 1295\nID\n name\n dept\n name\n tot\ncred\n00128\n Zhang\n Comp. Sci.\n 102\n12345\n Shankar\n Comp. Sci.\n 32\n19991\n Brandt\n History\n 80\n23121\n Chavez\n Finance\n 110\n44553\n Peltier\n Physics\n 56\n45678\n Levy\n Physics\n 46\n54321\n Williams\n Comp. Sci.\n 54\n55739\n Sanchez\n Music\n 38\n70557\n Snow\n Physics\n 0\n76543\n Brown\n Comp. Sci.\n 58\n76653\n Aoi\n Elec. Eng.\n 60\n98765\n Bourikas\n Elec. Eng.\n 98\n98988\n Tanaka\n Biology\n 120\nFigure A.8 The student relation.\n", "1324": "1296 Appendix A Detailed University Schema\nID\n course\n id\n sec\nid\n semester\n year\n grade\n00128\n CS-101\n 1\n Fall\n 2017\n A\n00128\n CS-347\n 1\n Fall\n 2017\n A-\n12345\n CS-101\n 1\n Fall\n 2017\n C\n12345\n CS-190\n 2\n Spring\n 2017\n A\n12345\n CS-315\n 1\n Spring\n 2018\n A\n12345\n CS-347\n 1\n Fall\n 2017\n A\n19991\n HIS-351\n 1\n Spring\n 2018\n B\n23121\n FIN-201\n 1\n Spring\n 2018\n C+\n44553\n PHY-101\n 1\n Fall\n 2017\n B-\n45678\n CS-101\n 1\n Fall\n 2017\n F\n45678\n CS-101\n 1\n Spring\n 2018\n B+\n45678\n CS-319\n 1\n Spring\n 2018\n B\n54321\n CS-101\n 1\n Fall\n 2017\n A-\n54321\n CS-190\n 2\n Spring\n 2017\n B+\n55739\n MU-199\n 1\n Spring\n 2018\n A-\n76543\n CS-101\n 1\n Fall\n 2017\n A\n76543\n CS-319\n 2\n Spring\n 2018\n A\n76653\n EE-181\n 1\n Spring\n 2017\n C\n98765\n CS-101\n 1\n Fall\n 2017\n C-\n98765\n CS-315\n 1\n Spring\n 2018\n B\n98988\n BIO-101\n 1\n Summer\n 2017\n A\n98988\n BIO-301\n 1\n Summer\n 2018\n null\nFigure A.9 The takes relation.\ns\nid\n i\nid\n00128\n 45565\n12345\n 10101\n23121\n 76543\n44553\n 22222\n45678\n 22222\n76543\n 45565\n76653\n 98345\n98765\n 98345\n98988\n 76766\nFigure A.10 The advisor relation.\n", "1325": "A.3 Sample Data 1297\ntime\n slot\nid\n day\n start\n time\n end\n time\nA\n M\n 8:00\n 8:50\nA\n W\n 8:00\n 8:50\nA\n F\n 8:00\n 8:50\nB\n M\n 9:00\n 9:50\nB\n W\n 9:00\n 9:50\nB\n F\n 9:00\n 9:50\nC\n M\n 11:00\n 11:50\nC\n W\n 11:00\n 11:50\nC\n F\n 11:00\n 11:50\nD\n M\n 13:00\n 13:50\nD\n W\n 13:00\n 13:50\nD\n F\n 13:00\n 13:50\nE\n T\n 10:30\n 11:45\nE\n R\n 10:30\n 11:45\nF\n T\n 14:30\n 15:45\nF\n R\n 14:30\n 15:45\nG\n M\n 16:00\n 16:50\nG\n W\n 16:00\n 16:50\nG\n F\n 16:00\n 16:50\nH\n W\n 10:00\n 12:30\nFigure A.11 The time\n slotrelation.\ncourse\n id\n prereq\n id\nBIO-301\n BIO-101\nBIO-399\n BIO-101\nCS-190\n CS-101\nCS-315\n CS-101\nCS-319\n CS-101\nCS-347\n CS-101\nEE-181\n PHY-101\nFigure A.12 The prereq relation.\n", "1326": "1298 Appendix A Detailed University Schema\ntime\n slot\nid\n day\n start\n hr\n start\n min\n end\n hr\n end\n min\nA\n M\n 8\n 0\n 8\n 50\nA\n W\n 8\n 0\n 8\n 50\nA\n F\n 8\n 0\n 8\n 50\nB\n M\n 9\n 0\n 9\n 50\nB\n W\n 9\n 0\n 9\n 50\nB\n F\n 9\n 0\n 9\n 50\nC\n M\n 11\n 0\n 11\n 50\nC\n W\n 11\n 0\n 11\n 50\nC\n F\n 11\n 0\n 11\n 50\nD\n M\n 13\n 0\n 13\n 50\nD\n W\n 13\n 0\n 13\n 50\nD\n F\n 13\n 0\n 13\n 50\nE\n T\n 10\n 30\n 11\n 45\nE\n R\n 10\n 30\n 11\n 45\nF\n T\n 14\n 30\n 15\n 45\nF\n R\n 14\n 30\n 15\n 45\nG\n M\n 16\n 0\n 16\n 50\nG\n W\n 16\n 0\n 16\n 50\nG\n F\n 16\n 0\n 16\n 50\nH\n W\n 10\n 0\n 12\n 30\nFigure A.13 The time\n slotrelation with start and end times separated into hour and\nminute.\n", "1327": "Index\naborted transactions, 805\u2013807,\n819\u2013820\nabstraction, 2, 9\u201312, 15\nacceptors, 1148, 1152\naccessing data. See also security\nfrom application programs,\n16\u201317\nconcurrent-access anomalies,\n7\ndi\ufb03culties in, 6\nindices for, 19\nrecovery systems and,\n910\u2013912\ntypes of access, 15\naccess paths, 695\naccess time\nindices and, 624, 627\u2013628\nquery processing and, 692\nstorage and, 561, 566, 567,\n578\naccess types, 624\naccount nonces, 1271\nACID properties. Seeatomicity;\nconsistency; durability;\nisolation\nActive Server Page (ASP), 405\nactive transactions, 806\nActiveX DataObjects (ADO),\n1239\nadaptive lock granularity,\n969\u2013970\nadd constraint, 146\nADO (ActiveX DataObjects),\n1239\nADO.NET, 184, 1239Advanced Encryption Standard\n(AES), 448, 449\nadvanced SQL, 183\u2013231\naccessing from programming\nlanguages, 183\u2013198\naggregate features, 219\u2013231\nembedded, 197\u2013198\nfunctions and procedures,\n198\u2013206\nJDBC and, 184\u2013193\nODBC and, 194\u2013197\nPython and, 193\u2013194\ntriggers and, 206\u2013213\nadvertisement data, 469\nAES (Advanced Encryption\nStandard), 448, 449\nafter triggers, 210\naggregate functions, 91\u201396\nbasic, 91\u201392\nwith Boolean values, 96\nde\ufb01ned, 91\nwith grouping, 92\u201395\nhaving clause, 95\u201396\nwith null values, 96\naggregation\nde\ufb01ned, 277\nentity-relationship (E-R)\nmodel and, 276\u2013277\nintraoperation parallelism\nand, 1049\non multidimensional data,\n527\u2013532\npartial, 1049\npivoting and, 226\u2013227, 530\nquery optimization and, 764query processing and, 723\nranking and, 219\u2013223\nrepresentation of, 279\nrollup and cube, 227\u2013231\nskew and, 1049\u20131050\nof transactions, 1278\nview maintenance and,\n781\u2013782\nwindowing and, 223\u2013226\naggregation operation, 57\naggregation switch, 977\nairlines, database applications\nfor, 3\nAjax, 423\u2013426, 1015\nalgebraic operations. See\nrelational algebra\naliases, 81, 336, 1242\nall construct, 100\nalter table, 71, 146\nalter trigger, 210\nalter type, 159\nAmdahl\u2019s law, 974\nAmerican National Standards\nInstitute (ANSI), 65, 1237\nanalysis pass, 944\nanalytics. Seedata analytics\nand connective, 74\nand operation, 89\u201390\nanonymity, 1252, 1253, 1258,\n1259\nANSI (American National\nStandards Institute), 65,\n1237\nanticipatory standards, 1237\nanti-join operation, 108, 776\n1299\n", "1328": "1300 Index\nanti-semijoin operation, 776\u2013777\nApache\nAsterixDB, 668\nCassandra, 477, 489, 668,\n1024, 1028\nFlink project, 504, 508\nGiraph system, 511\nHBase, 477, 480, 489, 668,\n971, 1024, 1028\u20131031\nHive, 494, 495, 500\nJakarta Project, 416\nKafka system, 506, 507,\n1072\u20131073, 1075, 1137\nSpark, 495\u2013500, 508, 511,\n1061\nStorm stream-processing\nsystem, 506\u2013508\nTez, 495\nAPIs. Seeapplication program\ninterfaces\napplication design, 403\u2013453\napplication architectures and,\n429\u2013434\nauthentication and, 441\u2013443\nbusiness logic and, 23, 404,\n411\u2013412, 430, 431, 445\nclient-server architecture and,\n404\nclient-side code and web\nservices, 421\u2013429\ncommon gateway interface\nstandard and, 409\ncookies and, 410\u2013415, 411n2\ndata access layer and,\n430\u2013434\ndisconnected operation and,\n427\u2013428\nencryption and, 447\u2013453\nHTML and, 404, 406\u2013408,\n426\nHTTP and, 405\u2013413\nJavaScript and, 404\u2013405,\n421\u2013426\nJava Server Pages and, 405,\n417\u2013418\nmobile application platforms,\n428\u2013429\nperformance and, 434\u2013437\nsecurity and, 437\u2013446\nservlets and, 411\u2013421standardization and,\n1237\u20131240\ntesting, 1234\u20131235\ntuning and ( seeperformance\ntuning)\nURLs and, 405\u2013406\nuser interfaces and, 403\u2013405\nweb and, 405\u2013411\napplication migration,\n1035\u20131036\napplication program interfaces\n(APIs)\nADO, 1239\nADO.NET, 184, 1239\napplication design and, 411,\n413, 416\nC++, 1239\ndatabase access from, 16\u201317\nJava ( seeJava)\nLDAP, 1243\nmap displays and, 393\nMongoDB, 477\u2013479, 482,\n489, 668, 1024, 1028\nOpen Database Connectivity\n(seeODBC)\nPython ( seePython)\nSpark, 495\u2013500, 508, 511,\n1061\nstandards for, 1238\u20131240\nsystem architectures and, 962\nTez, 495\nweb services and, 427\napplication programmers, 24\napplication servers, 23, 416\narchitectures, 961\u2013995\nbusiness logic and, 23\ncentralized databases,\n962\u2013963\nclient-server systems, 23, 971\ncloud-based, 990\u2013995, 1026,\n1027\ndatabase storage, 587\u2013588\ndata-server systems, 963\u2013964,\n968\u2013970\ndata warehousing, 522\u2013523\ndestination-driven, 522\ndistributed databases, 22,\n986\u2013989, 1098\u20131100\nhierarchical, 979, 980, 986\nhypercube, 976\u2013977lambda, 504, 1071\nmesh, 976\nmicroservices, 994\nmultiuser systems, 962\nNon-Uniform Memory\nAccess, 981, 1063\noverview, 961\u2013962\nparallel databases, 22,\n970\u2013986\nplatform-as-a-service model,\n992\u2013993\nrecovery systems, 932\nserver system, 962\u2013970,\n977\u2013978\nshared disk, 979, 980,\n984\u2013985\nshared memory, 21\u201322,\n979\u2013984, 1061\u20131064\nshared nothing, 979, 980,\n985\u2013986, 1040\u20131041,\n1061\u20131063\nsingle-user systems, 962\nsoftware-as-a-service model,\n993\nsource-driven, 522\nstorage area network, 562\nthree-tier, 23\ntransaction-server systems,\n963\u2013968\ntwo-phase commit protocol,\n989, 1276\ntwo-tier, 23\nwide-area networks, 989\narchival data, 561\narchival dump, 931\nARIES\nanalysis pass and, 944\ncompensation log records\nand, 942, 945\ndata structures and, 942\u2013944\ndirty page table and, 941\u2013947\n\ufb01ne-grained locking and, 947\nfuzzy checkpoints and, 941\nlog sequence number and,\n941\u2013946\nnested top actions and, 946\noptimization and, 947\nphysiological redo and, 941\nrecovery algorithm, 944\u2013946,\n1276\n", "1329": "Index 1301\nredo pass and, 944\u2013945\nrollback and, 945\u2013946\nsavepoints and, 947\nundo pass and, 944\u2013946\narity, 54\nArmstrong\u2019s axioms, 321\narray databases, 367\narray types, 366, 367, 378\nasc expression, 84\nas clause, 79, 81\nas of period for, 157\nASP (Active Server Page), 405\nASP.NET, 417\nassertions, 152\u2013153\nasset management, 1277\nassignment operation, 55\u201356,\n201\nassociations\ndata mining and, 541\nentity sets ( seeentity sets)\nrelation schema for, 42\nrelationship sets ( see\nrelationship sets)\nrules for, 546\u2013547\nassociative property, 749\u2013750\nAsterixDB, 668\nasymmetric\nfragment-and-replicate\njoins, 1046, 1062\nasymmetric-key encryption, 448\nasynchronous replication, 1122,\n1135\u20131138\nasynchronous view maintenance,\n1138\u20131140\nat-least-once semantics, 1074\nat-most-once semantics, 1074\natomic commit, 1029\natomic domains, 40, 342\u2013343\natomic instructions, 966\u2013967\natomicity\ncascadeless schedules and,\n820\u2013821\ncommit protocols and,\n1100\u20131110\nde\ufb01ned, 20, 800\nin \ufb01le-processing systems, 6\u20137\nisolation and, 819\u2013821\nlog records and, 913\u2013919\nrecoverable schedules and,\n819\u2013820recovery systems and, 803,\n912\u2013922\nstorage structure and,\n804\u2013805\nof transactions, 20\u201321, 144,\n481, 800\u2013807, 819\u2013821\nattribute inheritance, 274\u2013275\nattributes\natomic domains and, 342\u2013343\nbitmap indices and, 670\u2013672\nclassi\ufb01ers and, 541\u2013543, 545\nclosure of attribute sets,\n322\u2013324\ncomplex, 249\u2013252, 265\u2013267\ncomposite, 250, 252\ndecomposition and, 305\u2013313,\n330\u2013335, 339\u2013340\nderived, 251, 252\ndescriptive, 248\ndesign issues and, 281\u2013282\ndiscriminator, 259\ndomain of, 39\u201340, 249\nentity-relationship diagrams\nand, 265\u2013267\nentity-relationship (E-R)\nmodel and, 245, 248\u2013252,\n274\u2013275, 281\u2013282,\n342\u2013343\nentity sets and, 245, 265\u2013267,\n281\u2013282\nextraneous, 325\nhistograms and, 758\u2013760\nmultiple-key access and,\n661\u2013663\nmultivalued, 251, 252, 342\nnaming of, 345\u2013346\nnull values and, 251\u2013252\npartitioning, 479\nprimary keys and, 310n4\nprime, 356\nin relational model, 39\nrelationship sets and, 248\nsearch key and, 624\nsimple, 250, 265\nsingle-valued, 251\nUni\ufb01ed Modeling Language\nand, 289\nuniqui\ufb01ers, 649\u2013650\nvalue set of, 249\nattribute-value skew, 1008audit trails, 445\u2013446\naugmentation rule, 321\nauthentication\napplication-level, 441\u2013443\nchallenge-response system\nand, 451\ndigital certi\ufb01cates and,\n451\u2013453\ndigital signatures and, 451\nencryption and, 450\u2013453\nsingle sign-on system and,\n442\u2013443\nsmart cards and, 451, 451n9\ntwo-factor, 441\u2013442\nweb sessions and, 410\nauthorization\nadministrators and, 166\napplication-level, 443\u2013445\ndatabase design and, 291\nend-user information and, 443\ngranting privileges, 25,\n166\u2013167, 170\u2013171\nlack of \ufb01ne-grained, 443\u2013445\npermissioned blockchains\nand, 1253\nrevoking privileges, 166\u2013167,\n171\u2013173\nroles and, 167\u2013169\nrow-level, 173\non schema, 170\nSecurity Assertion Markup\nLanguage and, 442\u2013443\nSQL DDL and, 66\nsql security invoker, 170\nstorage manager and, 19\ntransfer of privileges, 170\u2013171\ntypes of, 14, 165\nupdates and, 14, 170, 171\non views, 169\u2013170\nauthorization graph, 171\nautomatic commit, 144, 144n6,\n822\nautonomous smart contracts,\n1272\u20131273\nautonomy, 988\navailability\nCAP theorem and, 1134\ndistributed databases and,\n987\n", "1330": "1302 Index\nhigh availability, 907,\n931\u2013933, 987, 1121\nnetwork partitions and, 481,\n989\nrobustness and, 1121\ntrading o\ufb00 consistency for,\n1134\u20131135\naverage latency time, 566\naverage response time, 809\naverage seek time, 566\navg expression, 91\u201396, 723,\n781\u2013782\nAvro, 490, 499\naxioms, 321\nAzure Stream Analytics, 505\nbackpropagation algorithm, 545\nbackup. See also recovery systems\napplication design and, 450\nremote systems for, 909,\n931\u2013935\nreplica systems, 212\u2013213\ntransactions and, 805\nbackup coordinators, 1146\u20131147\nbalanced range-partitioning\nvector, 1008\u20131009\nbalanced trees, 634\nbanking\nanalytics for, 520\u2013521\ndatabase applications for, 3,\n4, 7, 144\nBASE properties, 1135\nbase query, 217\nbatch scaleup, 973\nbatch update, 1221\nBayesian classi\ufb01ers, 543\u2013544\nBayes\u2019 theorem, 543\nBCNF. SeeBoyce\u2013Codd normal\nform\nBCNF decomposition algorithm,\n331\u2013333, 336\nbefore triggers, 210\nbegin atomic...end, 145, 201,\n208, 209, 211\nbegin transaction operation, 799\nbenchmarks. Seeperformance\nbenchmarks\nbestplan array, 768\u2013770\nbiased protocol, 1124\nBI (business intelligence), 521big-bang approach, 1236\nBigchainDB, 1269\nBig Data, 467\u2013511\nalgebraic operations and,\n494\u2013500\ncomparison with traditional\ndatabases, 468\nde\ufb01ned, 467\ndistributed \ufb01le systems for,\n472\u2013475, 489\ngraph databases and, 508\u2013511\nkey-value systems and, 471,\n473, 476\u2013480\nMapReduce paradigm and,\n481, 483\u2013494\nmotivations for, 467\u2013472\nparallel and distributed\ndatabases for, 473,\n480\u2013481\nquery processing and,\n470\u2013472\nreplication and consistency\nin, 481\u2013482\nsharding and, 473, 475\u2013476\nsources and uses of, 468\u2013470\nstorage systems, 472\u2013482, 668\nstreaming data, 468, 500\u2013508\nBigtable, 477, 479\u2013480, 668,\n1024\u20131025, 1028\u20131030\nbinary operations, 48\nbinary relationship sets, 249,\n283\u2013285\nBing Maps, 393\nBitcoin\nanonymity and, 1253, 1258\ndata mining and, 1265\nforking and, 1258\ngrowth and development of,\n1251\u20131253\nlanguage and, 1269\u20131270\nprocessing speed, 1274\nas public blockchain, 1253,\n1255, 1263\ntransactions and, 1261\u20131263,\n1268\u20131271\nbit-level striping, 571\u2013572\nbitmap indices\nattributes and, 670\u2013672\nB+-trees and, 1185\u20131186e\ufb03cient implementation of,\n1184\u20131185\nexistence, 1184\nintersection and, 671, 1183\nprocessing speed and, 662,\n663\nscans of, 698\u2013699\nsequential records and, 1182\nstructure of, 1182\u20131184\nusefulness of, 671\u2013672\nbit rot, 575\nblind writes, 868\nB-link trees, 886\nblobs, 156, 193, 594, 652\nblockchain databases,\n1251\u20131279\nanonymity and, 1252, 1253,\n1258, 1259\napplications for use,\n1251\u20131252, 1276\u20131279\nconcurrency control and,\n1262\u20131263\nconsensus mechanisms for,\n1254, 1256\u20131257,\n1263\u20131267\ncryptocurrencies and,\n1251\u20131253, 1257\ncryptographic hash functions\nand, 1253, 1259\u20131263,\n1265\ndata mining and, 1256, 1258,\n1264\u20131266\ndecentralization and, 1251,\n1252, 1259, 1270\ndigital ledgers and, 1251, 1252\ndigital signatures and, 1257,\n1261\nencryption and, 1260\u20131261\nexternal input and, 1271\u20131272\nfault-tolerance of, 1276\nforking and, 1257, 1258, 1263\ngenesis blocks and,\n1254\u20131255\nirrefutability and, 1257, 1259\nlanguages and, 1258,\n1269\u20131271\nlookup in, 1268\nmanagement of data in,\n1267\u20131269\n", "1331": "Index 1303\norphaned blocks and, 1257,\n1263\nperformance enhancement of,\n1274\u20131276\npermissioned, 1253\u20131254,\n1256\u20131257, 1263, 1266,\n1274\nproperties and components\nof, 1254\u20131259, 1274\npublic, 1253, 1255,\n1257\u20131259, 1263, 1264\nquery processing and, 1254,\n1275\u20131276\nscalability of, 1276\nsmart contracts and, 1258,\n1269\u20131273\nstate-based, 1269, 1271\ntamper resistant nature of,\n1253\u20131255, 1259, 1260\ntransactions and, 1261\u20131263,\n1268\u20131271, 1273\nblock identi\ufb01ers, 474\u2013475, 1020\nblocking edges, 728\nblocking factor, 725\nblocking operations, 728, 728n7\nblocking problem, 1104\u20131106\nblock-interleaved parity\norganization, 573\nblock-level striping, 572\nblock nested-loop join, 705\u2013707\nblock-oriented interface, 560\nblocks\nbu\ufb00er, 910\u2013912\ndirty, 928\u2013929\ndisk, 566\u2013567, 577\u2013580\nevicted, 605\n\ufb01le organization and, 588\ngenesis, 1254\u20131255\norphaned, 1257, 1263\nover\ufb02ow, 598\nphysical, 910\npinned, 605\nBloom \ufb01lters, 667, 1083,\n1175\u20131176, 1181\nBoolean operations, 89, 96, 103,\n188, 201, 1242. See\nalso speci\ufb01c operations\nbottlenecks\napplication design and, 437\nI/O parallelism and, 1007performance tuning and,\n1211\u20131213, 1215, 1227\nsingle lock-manager and, 1111\nsystem architectures and, 981,\n985\nbottom-up B+-tree construction,\n654\u2013655\nbottom-up design, 273\nbounding boxes, 674\u2013675, 1187\nBoyce\u2013Codd normal form\n(BCNF)\ncomparison with third normal\nform, 318\u2013319\ndecomposition algorithm and,\n331\u2013333, 336\nde\ufb01ned, 313\u2013315\ndependency preservation and,\n315\u2013316\nrelational database design\nand, 313\u2013316\ntesting for, 330\u2013331\nbroadcasting, 1055\u20131056\nbroadcast join, 1046\nBSP (bulk synchronous\nprocessing), 510\u2013511\nB-trees, comparison with\nB+-trees, 655\u2013656\nB+-trees, 634\u2013658\nbalanced, 634\nbitmap indices and,\n1185\u20131186\nbottom-up construction of,\n654\u2013655\nbulk loading of, 653\u2013655\ncomparison with B-trees,\n655\u2013656\ndeletion and, 641, 645\u2013649\nextensions and variations of,\n650\u2013658\nfanout and, 635\non \ufb02ash storage, 656\u2013657\nindexing strings and, 653\ninsertion and, 641\u2013645, 647,\n649\ninternal nodes and, 635\nleaf nodes of, 635\u2013656,\n665\u2013669, 673, 674\nin main memory, 657\u2013658\nnonleaf nodes of, 635\u2013636,\n642, 645\u2013656, 663nonunique search keys and,\n649\u2013650\norganization of, 595,\n650\u2013652, 697, 697n4\nparallel key-value stores and,\n1028\nperformance and, 634,\n665\u2013666\nqueries on, 637\u2013641, 690\nrecord relocation and,\n652\u2013653\nsecondary indices and,\n652\u2013653\nspatial data and, 672\u2013673\nstructure of, 634\u2013637\ntemporal data and, 676\ntuning of, 1215\nupdates on, 641\u2013649\nbuckets, 659\u2013661, 1194\u20131195\nbu\ufb00er blocks, 910\u2013912\nbu\ufb00er manager, 19, 604\u2013607\nbu\ufb00ers\ndatabase bu\ufb00ering, 927\u2013929\nde\ufb01ned, 604\ndisk blocks and, 578, 910\ndouble bu\ufb00ering, 725\nforce/no-force policy and, 927\nforce output and, 912\nlog-record, 926\u2013927\nmanagement of, 926\u2013930\noperating system role in,\n929\u2013930\noutput of blocks and,\n606\u2013607\nrecovery systems and,\n926\u2013930\nreordering of writes and\nrecovery, 609\u2013610\nreplacement strategies, 605,\n607\u2013609\nshared and exclusive locks on,\n605\u2013606\nsteal/no-steal policy and, 927\nstorage and, 604\u2013610\ntransaction servers and, 965\nwrite-ahead logging rule and,\n926\u2013929\nbu\ufb00er trees, 668\u2013670\nbugs\n", "1332": "1304 Index\napplication design and, 440,\n1234, 1236\ndebugging, 199n4\nfailure classi\ufb01cation and, 908\nbuild input, 713\nbulk export utility, 1222\nbulk insert utility, 1222\nbulk loads, 653\u2013655, 1221\u20131223\nbulk synchronous processing\n(BSP), 510\u2013511\nbully algorithm, 1148\nbusiness intelligence (BI), 521\nbusiness logic, 23, 198, 404,\n411\u2013412, 430\u2013431, 445\nbusiness-logic layer, 430, 431\nbusiness rules, 431\nbus system, 975\u2013976\nByzantine consensus, 1254,\n1256, 1266\u20131267, 1276\nByzantine failure, 1266\u20131267\nC\nadvanced SQL and, 183, 197,\n199, 205\napplication design and, 16\nODBC and, 195\u2013196\nstruct declarations used by,\n11n1\nUni\ufb01ed Modeling Language\nand, 289\nC++\nadvanced SQL and, 197, 199,\n205, 206\napplication design and, 16,\n417\nobject-oriented programming\nand, 377\nstandards for, 1239\nstruct declarations used by,\n11n1\nUni\ufb01ed Modeling Language\nand, 289\ncache-conscious algorithms,\n732\u2013733\ncache line, 732, 983\ncache memory, 559\ncache misses, 982\ncaching\napplication design and,\n435\u2013437coherency and, 969, 983\u2013984\ncolumn-oriented storage and,\n612\ndata servers and, 968\u2013970\nlocks and, 969\nquery plans and, 774, 965\nreplication and, 1014n4\nshared-memory architecture\nand, 982\u2013984\nCAD (computer-aided design),\n390\u2013391, 968\ncallable statements, 190\u2013191\ncall back, 969\nCall Level Interface (CLI)\nstandards, 197, 1238\u20131239\ncall statement, 201\ncandidate keys, 44\ncanonical cover, 324\u2013328\nCAP theorem, 1134\nCartesian-product operation,\n50\u201352\nCartesian products\nequivalence and, 748, 749,\n755\njoin expressions and, 135\nquery optimization and, 748,\n749, 755, 763\u2013764, 775\nSQL and, 76\u201379, 81, 127n1,\n230\ncascadeless schedules, 820\u2013821\ncascades, 150, 172, 210\ncascading rollback, 820\u2013821,\n841\u2013842\ncascading stylesheet (CSS)\nstandard, 408\ncase construct, 112\u2013113\nCassandra, 477, 489, 668, 1024,\n1028\ncast, 155, 159\ncatalogs\napplication design and, 1239\nindices and ( seeindices)\nquery optimization and,\n758\u2013760, 762, 764\nSQL and, 162\u2013163, 192,\n196\u2013197\nsystem, 602\u2013604, 1009\ncentralized databases, 962\u2013963\ncentralized deadlock detection,\n1114centroid, 548, 548n3\nCEP (complex event processing)\nsystems, 504\nCGI (common gateway interface)\nstandard, 409\nchain replication protocol,\n1127\u20131128\nchallenge-response system, 451\nchange isolation level, 822\nchange relation, 211\nchar, 67\ncheck clause\nassertions and, 152\u2013153\nintegrity constraints and,\n147\u2013149, 152\u2013153\nuser-de\ufb01ned types and, 159\ncheck constraints, 151, 170, 315,\n800\ncheckpoint log records, 943\ncheckpoint process, 965\ncheckpoints\nfuzzy, 922, 930, 941\nrecovery systems and,\n920\u2013922, 930\ntransaction servers and, 965\nchecksums, 565\nchicken-little approach, 1236\nChubby, 1150\ncircular arcs, 388\nclassi\ufb01ers\nattributes and, 541\u2013543, 545\nBayesian, 543\u2013544\ndata mining and, 541\u2013546\ndecision-tree, 542\nneural-net, 545\u2013546\nprediction and, 541\u2013543,\n545\u2013546\nSupport Vector Machine,\n544\u2013545\ntraining instances and, 541\nCLI (Call Level Interface)\nstandards, 197, 1238\u20131239\nclick-through, 469\nclient-server systems\napplication design and, 404,\n1221, 1239\nrecovery systems and, 931\nsystem architecture and, 23,\n971\n", "1333": "Index 1305\nclient-side scripting languages,\n421\u2013429\nclobs, 156, 193, 594, 652\nclosed addressing, 659, 1194\nclosed hashing, 659, 1194\nclosed polygons, 388n3\nclosed time intervals, 675\nclosure of a set, 312, 320\u2013324\ncloud-based data storage, 28,\n563, 992\u2013993\ncloud computing\narchitecture for, 990\u2013995,\n1026, 1027\nbene\ufb01ts and limitations of,\n995\nservice models, 991\u2013995\nstorage systems and, 28, 563\nCLR (Common Language\nRuntime), 206\nCLRs (compensation log\nrecords), 922, 942, 945\nclustering indices, 625,\n632\u2013633, 695, 697\u2013698\ncluster key, 600\u2013601\ncluster membership, 1158\nclusters\ndata mining and, 541,\n548\u2013549\nhierarchical, 548\nkey-value storage systems and,\n477\nmultitable, 595, 598\u2013601\nsystem architecture and, 978\ncoalesce function, 114, 155,\n230\u2013231\ncoalescing nodes, 641, 886\ncoarse-grained parallelism, 963,\n970\nCodd, Edgar, 26\ncode breaking. Seeencryption\ncollision resistant hash functions,\n1259\u20131260\ncolocation of data, 1068\u20131069\ncolumn family, 1025\ncolumn-oriented storage,\n525\u2013526, 588, 611\u2013617,\n734, 1182\ncolumn stores, 612, 615, 1025,\n1224\ncombinatorics, 811combine function, 490\ncomma-separated values, 1222\ncommit dependency, 847\ncommit protocols, 1100\u20131110\ncommitted transactions\nde\ufb01ned, 806\ndurability and, 933\u2013934\nlog records and, 917\nobservable external writes\nand, 807\npartially committed, 806\nscheduling and, 810, 819\u2013820\nupdates and, 874\ncommit time, 933\u2013934\ncommit wait, 1130\u20131131\ncommit work, 143\u2013145\ncommon gateway interface (CGI)\nstandard, 409\nCommon Language Runtime\n(CLR), 206\ncommon subexpression\nelimination, 785\ncommutative property, 747\u2013750\ncommute, 1143\ncompare-and-swap, 966\ncompatibility function, 836\ncompatible relations, 54\ncompensating operation, 892\ncompensating transactions, 805\ncompensation log records\n(CLRs), 922, 942, 945\ncomplete axioms, 321\ncompleteness constraint, 275\ncomplex attributes, 249\u2013252,\n265\u2013267\ncomplex data types, 365\u2013394\nobject orientation, 376\u2013382\nsemi-structured, 365\u2013376\nspatial, 387\u2013394\ntextual, 382\u2013387\nuser-de\ufb01ned, 158\ncomplex event processing (CEP)\nsystems, 504\ncomposite attributes, 250, 252\ncomposite indices, 700\ncomposite price/performance\nmetric, 1234\ncomposite query per hour metric,\n1234\ncompressioncolumn-oriented storage and,\n611, 612\ndata warehousing and, 526\nof disk block data, 615n8\npre\ufb01x, 653\nworkload compression, 1217\ncomputer-aided design (CAD),\n390\u2013391, 968\nconceptual-design phase, 17\u201318,\n242\nconcurrency control, 835\u2013894\naccess anomalies and, 7\nblind writes and, 868\nblockchain databases and,\n1262\u20131263\ncommit protocols and, 1105\nconsistency and, 880\u2013885\ndeadlock handling and,\n849\u2013853\ndeletion and, 857\u2013858\ndistributed databases and,\n990, 1105, 1111\u20131120\nextended protocols,\n1129\u20131133\nfalse cycles and, 1114\u20131115\nin federated databases,\n1132\u20131133\nindices and, 884\u2013887\ninsertion and, 857, 858\nisolation and, 803\u2013804,\n807\u2013812, 823\nleases and, 1115\u20131116\nlocking protocols and,\n835\u2013848 ( see also locks)\nlogical undo operations and,\n940\u2013941\nlong-duration transactions\nand, 890\u2013891\nin main-memory databases,\n887\u2013890\nmultiple granularity and,\n853\u2013857\nmultiversion schemes and,\n869\u2013872, 1129\u20131131\nwith operations, 891\u2013894\noptimistic, 869\nparallel databases and, 990\nparallel key-value stores and,\n1028\u20131029\n", "1334": "1306 Index\nphantom phenomenon and,\n827, 858\u2013861, 877\u2013879,\n877n5, 885, 887\npredicate reads and, 858\u2013861\nreal-time transaction systems\nand, 894\nrecovery systems and, 916\nreplication and, 1123\u20131125\nrollback and, 841\u2013844,\n849\u2013850, 853, 868\u2013871\nserializability and, 836,\n840\u2013843, 846\u2013848, 856,\n861\u2013871, 875\u2013887\nsnapshot isolation and,\n872\u2013879, 882, 916,\n1131\u20131132\ntimestamp-based protocols\nand, 861\u2013866, 882\ntrends in, 808\nuser interactions and,\n881\u2013883\nvalidation and, 866\u2013869, 882,\n916\nconcurrency-control manager, 21\nconcurrency-control schemes,\n809\nconcurrent transactions,\n1224\u20131227\ncon\ufb01dence, 540, 547\ncon\ufb02ict equivalence, 815, 815n2\ncon\ufb02ict serializability, 813\u2013816\nconformance levels, 196\u2013197\nconjunctive selection, 699\u2013700,\n747, 762\nconnection pooling, 436\nconsensus protocols\nblockchain databases and,\n1254, 1256\u20131257,\n1263\u20131267\nByzantine, 1254, 1256,\n1266\u20131267, 1276\ndistributed databases and,\n1106\u20131107, 1150\u20131161,\n1266, 1267\nmessage-based, 1266\nmultiple consensus protocol,\n1151\nPaxos, 1152\u20131155, 1160\u20131161,\n1267\nproof-of-stake, 1256, 1266proof-of-work, 1256,\n1264\u20131266\nRaft, 1148, 1155\u20131158, 1267\nreplication and, 1016\nZab, 1152\nconsistency\nBig Data and, 481\u2013482\nCAP theorem and, 1134\nconcurrency control and,\n880\u2013885\ncursor stability and, 881\ndeadlock and, 838\u2013839\nde\ufb01ned, 20\ndegree-two, 880\u2013881\neventual, 1016, 1139\nexternal, 1131\n\ufb01le system consistency check,\n610\nhashing and, 1013\nlogical operations and,\n936\u2013937\nreplication and, 1015\u20131016,\n1121\u20131123, 1133\u20131146\nrequirement of, 802\ntrading o\ufb00 for availability,\n1134\u20131135\nof transactions, 20, 800, 802,\n807\u2013808, 821\u2013823\nuser interactions and,\n881\u2013883\nweak levels of, 880\u2013883\nconstraints\ncheck, 151, 170, 315\ncompleteness, 275\nconsistency, 13\u201314\ndeadlines, 894\ndecomposition and, 336\ndependency preservation and,\n315\u2013316\nentity-relationship (E-R)\nmodel and, 253\u2013256,\n275\u2013276\nforeign key, 45\u201346\nintegrity ( seeintegrity\nconstraints)\nkeys and, 258\nmapping cardinalities and,\n253\u2013256\nnot null, 69\nprimary key, 44on specialization, 275\u2013276\ntransactions and, 800\nUni\ufb01ed Modeling Language\nand, 289\ncontainers, 992\u2013994\ncontains operation, 101\ncontinuous queries, 503, 731\ncontinuous-stream data, 731\nconversations, 883\nconversions, 155\u2013156, 469, 843\ncookies, 410\u2013415, 411n2,\n439\u2013440\ncoordinators, 1099, 1104,\n1106\u20131107, 1146\u20131150\nCorda, 1269\ncores, 962\u2013963, 970, 976,\n980\u2013983\ncore switch, 977\ncorrelated evaluation, 775\ncorrelated subqueries, 101\ncorrelation name, 81, 101\ncorrelation variables, 81, 775\ncost-based optimizers, 766\nCouchbase, 1024\ncount function, 91\u201392, 94, 96,\n723, 766, 781\ncount values, 220n11\ncovering indices, 663\ncrabbing protocol, 885\u2013886\ncrashes. See also recovery\nsystems\nactions following, 923\u2013925\nalgorithms for, 922\u2013925\nARIES and, 941\u2013947\ncheckpoints and, 920\u2013922\nfailure classi\ufb01cation and, 908\nmagnetic disks and, 565\nstorage and, 607, 609\u2013610\ntransactions and, 800\ncrawling the web, 383\ncreate assertion, 153\ncreate cluster, 601\ncreate distinct type, 160\ncreate domain, 159\u2013160\ncreate function, 200, 203, 204,\n215\ncreate index, 164\u2013165, 664\ncreate or replace, 199n4\ncreate procedure, 200, 205\ncreate recursive view, 218\n", "1335": "Index 1307\ncreate role, 168\ncreate schema, 163\ncreate sequence construct, 161\ncreate table\nwith data clause, 162\ndefault values and, 156\nextensions for, 162\nintegrity constraints and,\n146\u2013149\nmultitable clustering and, 601\nobject-based databases and,\n378\u2013380\nshipping SQL statements to\ndatabase and, 187\nSQL schema de\ufb01nition and,\n68\u201371\ncreate table...as, 162\ncreate table...like, 162\ncreate temporary table, 214\ncreate type, 158\u2013160, 378\u2013380\ncreate unique index, 165, 664\ncreate view, 138\u2013143, 162, 169\ncredit bureaus, 521, 521n1\ncross-chain transactions, 1273\ncross join, 127n1\ncross-site request forgery\n(XSRF), 439\u2013440\ncross-site scripting (XSS),\n439\u2013440\ncross-tabulation, 226\u2013227,\n528\u2013533\nCRUD web interfaces, 419\ncryptocurrencies, 1251\u20131253,\n1257. See also Bitcoin\ncryptographic hash functions,\n1253, 1259\u20131263, 1265\nCSS (cascading stylesheet)\nstandard, 408\nC-Store, 615\ncube construct, 227\u2013231,\n536\u2013538\ncurrent date, 154\ncursor stability, 881\ncurve \ufb01tting, 546\ncylinders, 565\nCypher query language, 509\nDAGs (directed acyclic graphs),\n499, 506\u2013507, 1071\u20131072DAOs (distributed autonomous\norganizations), 1272,\n1272n7\nDart language, 428\u2013429\ndata abstraction, 2, 9\u201312, 15\ndata access layer, 430\u2013434\ndata analytics, 519\u2013549. See also\ndata mining\ndecision-support systems and,\n519\u2013522\nde\ufb01ned, 4, 519\nOLAP systems, 520, 527\u2013540\noverview, 519\u2013521\npredictive models in, 4\u20135\nstatistical analysis, 520, 527\nwarehousing and, 519\u2013527\ndata-at-rest, 502\ndatabase administrators (DBAs),\n24\u201325\ndatabase-as-a-service platform,\n993\ndatabase design\nalternatives in, 243\u2013244,\n285\u2013291\napplications and ( see\napplication design)\narchitecture of ( see\narchitectures)\nauthorization requirements\nand, 291\nbottom-up, 273\nbu\ufb00ers and, 604\u2013610\nclient-server ( seeclient-server\nsystems)\ncomplexity of, 241\ncomputer-aided, 390\u2013391\nconceptual-design phase of,\n17\u201318, 242\ndirect design process, 241\nencryption and, 447\u2013453\nengines, 18\u201321\nE-R model and ( see\nentity-relationship model)\nfunctional requirements of,\n291\nincompleteness in, 243\u2013244\nlogical-design phase of, 18,\n242\nnormalization in, 17\noverview of process, 241\u2013244phases of, 17\u201318, 241\u2013243\nphysical-design phase of, 18,\n242\u2013243\nredundancy in, 243\nrelational ( seerelational\ndatabase design)\nschema evolution and, 292\nspeci\ufb01cation of functional\nrequirements in, 17\u201318\ntop-down, 273\nuser requirements in, 17\u201318,\n241\u2013242, 274\nwork\ufb02ow and, 291\u2013292\ndatabase graph, 846\u2013848\ndatabase instance, 41\ndatabase-management systems\n(DBMSs)\nde\ufb01ned, 1\nobjectives of, 1, 24\norganizational data\nprocessing prior to, 472\nproduct-speci\ufb01c calls needed\nby, 186\ndatabases\nabstraction and, 2, 9\u201312, 15\nadministrators of, 24\u201325\napplications for, 1\u20135\narchitecture ( see\narchitectures)\narray, 367\nblockchain ( seeblockchain\ndatabases)\nbu\ufb00ering and, 927\u2013929\ncentralized, 962\u2013963\nconcurrency control and ( see\nconcurrency control)\nde\ufb01ned, 1\ndesign of ( seedatabase\ndesign)\ndocument, 3\ndumping and, 930\u2013931\ne \ufb03 c i e n c yo f ,1 ,2 ,5 ,9\nembedded, 198, 962\nas \ufb01le-processing systems, 5\u20138\nforce output and, 912\ngraph, 508\u2013511\nhistory of, 25\u201328\nindexing and ( seeindices)\nlanguages for, 13\u201317\nlocks and ( seelocks)\n", "1336": "1308 Index\nmain memory ( see\nmain-memory databases)\nmaintenance for, 25\nmodi\ufb01cation of, 108\u2013114,\n915\u2013916\nobject-based ( seeobject-based\ndatabases)\nobject-oriented, 9, 26, 377,\n431, 1239\u20131240\nobject-relational, 377\u2013381\nparallel ( seeparallel\ndatabases)\npurpose of, 5\u20138\nquery processor components\nof, 18, 20\nrecovery of ( seerecovery\nsystems)\nstorage for ( seestorage)\ntransaction manager in, 18\u201321\nuniversity ( seeuniversity\ndatabases)\nuser interaction with, 4\u20135, 24\ndatabases administrator (DBA),\n171\ndatabase schema. Seeschemas\ndatabase writer process, 965\ndata center fabric, 978\ndata centers, 970, 1014\u20131015\ndata cleansing, 523\ndata cubes, 529\u2013530\ndata-de\ufb01nition language (DDL)\napplication programs and, 17\nauthorization and, 66\nbasic types supported by,\n67\u201368\nin consistency constraint\nspeci\ufb01cation, 13\u201314\nde\ufb01ned, 13, 65\ndumping and, 931\ngranting and revoking\nprivileges and, 166\nindices and, 67\nintegrity and, 66\ninterpreter, 20\noutput of, 14\nschema de\ufb01nition and, 24, 66,\n68\u201371\nsecurity and, 67\nset of relations in, 66\u201367\nSQL and, 14\u201315, 65\u201371storage and, 67\ndata dictionary, 14, 19, 602\u2013604\ndata distribution skew, 1008\nData Encryption Standard\n(DES), 448\ndata \ufb01les, 19\ndata inconsistency, 6\ndata isolation. Seeisolation\ndata-item identi\ufb01ers, 913\ndata items, 968\ndata lakes, 527, 1078\ndata-manipulation language\n(DML)\napplication programs and, 17\ncompiler, 20\ndeclarative, 15\nde\ufb01ned, 13, 15, 66\nprocedural, 15\nSQL and, 16\nstorage manager and, 19\ndata mining, 540\u2013549\nassociation rules and,\n546\u2013547\nblockchain databases and,\n1256, 1258, 1264\u20131266\nclassi\ufb01ers and, 541\u2013546\nclustering and, 541, 548\u2013549\nde\ufb01ned, 5, 540\ndescriptive patterns and, 541\ngrowth of, 27\nmodels for, 540\noverview, 521\nprediction and, 541\nregression and, 546\nrules for, 540\ntask types in, 541\ntext mining, 549\ndata models, 8\u20139. See also\nspeci\ufb01c models\ndatanodes, 475, 1020\ndata parallelism, 1042, 1057\ndata partitioning, 989n5\ndata-server systems, 963\u2013964,\n968\u2013970\nDataSet type, 499\ndata storage and de\ufb01nition\nlanguage, 13\ndata storage systems. Seestorage\ndata streams, 731\ndata striping, 571\u2013572data-transfer failures, 909\ndata-transfer rate, 566, 569\ndata types. Seetypes\ndata virtualization, 1077\ndata visualization, 538\u2013540\ndata warehousing, 519\u2013527\narchitecture for, 522\u2013523\ncolumn-oriented storage and,\n525\u2013526\ncomponents of, 522\u2013524\ndatabase support for,\n525\u2013526\ndata integration vs.,\n1077\u20131078\ndata lakes and, 527, 1078\ndeduplication and, 523\nde\ufb01ned, 519, 522\nETL tasks and, 520, 524\nfact tables and, 524\nhouseholding and, 523\nmerger-purge operation and,\n523\nmultidimensional data and,\n524\noverview, 519\u2013520\nschemas used for, 523\u2013525\ntransformation and cleansing,\n523\nupdates and, 523\ndatetime data type, 154, 531\nDBAs (database administrators),\n24\u201325\nDBMSs. See\ndatabase-management\nsystems\nDDL. Seedata-de\ufb01nition\nlanguage\nDDL interpreter, 20\ndeadlines, 894\ndeadlocks\nconsistency and, 838\u2013839\ndetection of, 849, 851\u2013852\ndistributed databases and,\n1111\u20131115\nhandling of, 849\u2013853\nprevention of, 849\u2013851\nrecovery and, 849, 851, 853\nrollback and, 853\nstarvation and, 853\nvictim selection and, 853\n", "1337": "Index 1309\nwait-for graphs and, 851\u2013852,\n1113\u20131114\ndebugging, 199n4\ndecentralization, 1251, 1252,\n1259, 1270\ndecision support, 521,\n1231\u20131233\ndecision-support queries, 521,\n971\ndecision-support systems,\n519\u2013522\ndecision-support tasks, 521\ndecision-tree classi\ufb01ers, 542\ndeclarative DMLs, 15\ndeclarative queries, 47,\n1030\u20131031\ndeclare statement, 201\u2013203\ndecode, 155\u2013156\ndecomposition\nalgorithms for, 330\u2013335\nattributes and, 305\u2013313,\n330\u2013335, 339\u2013340\nBoyce\u2013Codd normal form\nand, 313\u2013316, 330\u2013333\ndependency preservation and,\n315\u2013316, 329\nfourth normal form and,\n339\u2013341\nfunctional dependencies and,\n308\u2013313, 330\u2013341\nhigher normal forms and, 319\nkeys and, 309\u2013312\nlossless, 307\u2013308, 307n1,\n312\u2013313\nlossy, 307\nmultivalued dependencies\nand, 336\u2013341\nnormalization theory and,\n308\nnotational conventions and,\n309\nrelational database design\nand, 305\u2013313, 330\u2013341\nthird normal form and,\n317\u2013319, 333\u2013335\ndecomposition rule, 321\ndecompression, 613, 615n8\ndecorrelation, 777\u2013778\nDEC Rdb, 26\ndeduplication, 523deep learning, 546\ndeep neural networks, 546\nde facto standards, 1237\ndefault values\nclassi\ufb01ers and, 545\nprivileges and, 167\nsetting reference \ufb01eld to, 150\nuser-de\ufb01ned types and, 159\ndeferred integrity constraints,\n151\ndeferred-modi\ufb01cation technique,\n915\ndeferred view maintenance, 779,\n1215\u20131216\ndegree of relationship sets, 249\ndegree-two consistency, 880\u2013881\ndelete authorization, 14\ndeletion\nB+-trees and, 641, 645\u2013649\nconcurrency control and,\n857\u2013858\ndatabase modi\ufb01cation and,\n108\u2013110\nhashing and, 1190,\n1194\u20131195, 1198\nintegrity constraints and, 150\nLSM trees and, 1178\u20131179\nof messages, 1110\nordered indices and, 624,\n631\u2013632\nprivileges and, 166\u2013167\nR-trees and, 1189\nshipping SQL statements to\ndatabase and, 187\nSQL schema de\ufb01nition and,\n69, 71\ntransactions and, 801, 826\ntriggers and, 208\u2013209\ntuples and, 108\u2013110, 613\nviews and, 142\ndeletion entries, 668, 1178\u20131179\ndelta relation, 211\ndemand-driven pipeline, 726\u2013728\ndenial-of-service attacks, 502\ndenormalization, 346\ndense indices, 626\u2013628, 630\u2013631\ndependency of transactions, 819\ndependency preservation,\n315\u2013316, 328\u2013330\nderived attributes, 251, 252desc expression, 84\ndescriptive attributes, 248\ndescriptive patterns, 541\nDES (Data Encryption\nStandard), 448\ndesign. Seedatabase design\ndestination-driven architecture,\n522\ndicing, 530\ndictionary attacks, 449\ndi\ufb00erentials, 780\ndigital certi\ufb01cates, 451\u2013453\ndigital ledgers, 1251, 1252\ndigital signatures, 451, 1257,\n1261\ndigital video disks (DVDs),\n560\u2013561\ndimension attributes, 524\ndimension tables, 524\ndirect-access storage, 561\ndirected acyclic graphs (DAGs),\n499, 506\u2013507, 1071\u20131072\ndirectory access protocols, 1084,\n1240\u20131243\ndirectory information trees\n(DITs), 1242, 1243\ndirectory systems, 1020,\n1084\u20131086, 1240\u20131243\ndirty blocks, 928\u2013929\ndirty page table, 941\u2013947\ndirty writes, 822\ndisable trigger, 210\ndisambiguation, 549\ndisconnected operation, 427\u2013428\ndiscretized streams, 508\ndiscriminator attributes, 259\ndisjoint generalization, 279, 290\ndisjoint specialization, 272, 275\ndisjoint subtrees, 847\ndisjunctive selection, 699, 700,\n762\ndisk arms, 565\ndisk-arm\u2013scheduling, 578\u2013579\ndisk blocks, 566\u2013567, 577\u2013580\ndisk bu\ufb00er, 578, 910\ndisk controllers, 565\ndisk failure, 908\ndistinct types, 90, 92, 98\u2013100,\n158\u2013160\n", "1338": "1310 Index\ndistinguished name (DN),\n1241\u20131242\ndistributed autonomous\norganizations (DAOs),\n1272, 1272n7\ndistributed consensus problem,\n1106\u20131107, 1151\ndistributed databases\narchitecture of, 22, 986\u2013989,\n1098\u20131100\nautonomy and, 988\nBig Data and, 473, 480\u2013481\ncommit protocols and,\n1100\u20131110\nconcurrency control and,\n990, 1105, 1111\u20131120\nconsensus in, 1106\u20131107,\n1150\u20131161, 1266, 1267\ndirectory systems and, 1020,\n1084\u20131086, 1240\u20131243\nfailure and, 1104\nfederated, 988, 1076\u20131077,\n1132\u20131133\n\ufb01le systems in, 472\u2013475, 489,\n1003, 1019\u20131022\nglobal transactions and, 988,\n1098, 1132\nheterogeneous, 988, 1132\nhomogeneous, 988\nleases and, 1115\u20131116\nlocal transactions and, 988,\n1098, 1132\nlocks and, 1111\u20131116\nnodes and, 987\npartitions and, 1104\u20131105\npersistent messaging and,\n1108\u20131110, 1137\nquery optimization and, 1084\nquery processing and,\n1076\u20131086\nrecovery and, 1105\nreplication and, 987,\n1121\u20131128\nsharing data and, 988\nsites and, 986\nsnapshot isolation and,\n1131\u20131132\ntimestamps and, 1116\u20131118\ntransaction processing in,\n989\u2013990, 1098\u20131100validation and, 1119\u20131120\ndistributed \ufb01le systems, 472\u2013475,\n489, 1003, 1019\u20131022\ndistributed hash tables, 1013\ndistributed-lock manager, 1112\ndistributed query processing,\n1076\u20131086\ndata integration from multiple\nsources, 1076\u20131078\ndirectory systems and,\n1084\u20131086\njoin location and join\nordering in, 1081\u20131082\nacross multiple sources,\n1080\u20131084\noptimization and, 1084\nschema and data integration\nin, 1078\u20131080\nsemijoin strategy and,\n1082\u20131084\nDITs (directory information\ntrees), 1242, 1243\nDjango framework, 382,\n419\u2013421, 433\u2013435, 1240\nDKNF (domain-key normal\nform), 341\nDML. Seedata-manipulation\nlanguage\nDML compiler, 20\nDN (distinguished name),\n1241\u20131242\nDNS (Domain Name Service)\nsystem, 1084, 1085\nDocker, 995\ndocument databases, 3\nDocument Object Model\n(DOM), 423\ndocument stores, 477, 1023\ndomain constraints, 13\u201314, 146\ndomain-key normal form\n(DKNF), 341\nDomain Name Service (DNS)\nsystem, 1084, 1085\ndomain of attributes, 39\u201340, 249\ndouble bu\ufb00ering, 725\ndouble-pipelined hash-join, 731\ndouble-pipelined join technique,\n730\u2013731\ndouble-spend transactions,\n1261\u20131262, 1264downgrade, 843\ndrill down, 531, 540\nDriverManager class, 186\ndrop index, 165, 664\ndrop schema, 163\ndrop table, 69, 71, 190\ndrop trigger, 210\ndrop type, 159\ndumping, 930\u2013931\nduplicate elimination, 719\u2013720,\n1049\ndurability\nde\ufb01ned, 800\none-safe, 933\nremote backup systems and,\n933\u2013934\nstorage structure and,\n804\u2013805\nof transactions, 20\u201321,\n800\u2013807\ntwo-safe, 934\ntwo-very-safe, 933\nDVDs (digital video disks),\n560\u2013561\ndynamic handling of join skew,\n1048\ndynamic hashing, 661,\n1195\u20131203\ndynamic-programming algorithm,\n767\ndynamic repartitioning,\n1010\u20131013\ndynamic SQL, 66, 184, 201\nDynamo, 477, 489, 1024\nEclipse, 416\ne-commerce, streaming data and,\n501\nedge switches, 977\ne\ufb03ciency of databases, 1, 2, 5, 9\ne-government, 1277\nelasticity, 992, 1010, 1024\nelection algorithms, 1147\nelevator algorithm, 578\nembedded databases, 198, 962\nembedded multivalued\ndependencies, 341\nembedded SQL, 66, 184,\n197\u2013198, 965, 1269\nempty relations test, 101\u2013102\n", "1339": "Index 1311\nencryption\nAdvanced Encryption\nStandard, 448, 449\napplications of, 447\u2013453\nasymmetric-key, 448\nauthentication and, 450\u2013453\nblockchain databases and,\n1260\u20131261\nchallenge-response system\nand, 451\ndatabase support and,\n449\u2013450\ndictionary attacks and, 449\ndigital certi\ufb01cates and,\n451\u2013453\ndigital signatures and, 451\nnonrepudiation and, 451\nprime numbers and, 449\nprivate-key, 1260\u20131261\npublic-key, 448\u2013449,\n1260\u20131261\nRijndael algorithm and, 448\nsymmetric-key, 448\ntechniques of, 447\u2013449\nend transaction operation, 799\nend-user information, 443\nenterprise information, database\napplications for, 2\u20134\nentities, 243, 244, 247\u2013248\nentity group, 1031\nentity recognition, 549\nentity-relationship (E-R)\ndiagrams\naggregation and, 279\nalternative notations for\nmodeling data, 285\u2013291\ncommon mistakes in,\n280\u2013281\ncomplex attributes and,\n265\u2013267\nde\ufb01ned, 244\nentity sets and, 245\u2013246,\n265\u2013268\ngeneralization and, 278\u2013279\nparticipation illustrated by,\n255\nreduction to relational\nschema, 264\u2013271, 277\u2013279\nrelationship sets and,\n247\u2013250, 268\u2013271Uni\ufb01ed Modeling Language\nand, 289\u2013291\nfor university enterprise,\n263\u2013264\nwith weak entity set, 260\nentity-relationship (E-R) model,\n244\u2013291\naggregation and, 276\u2013277\nalternative notations for\nmodeling data, 285\u2013291\natomic domains and, 342\u2013343\nattributes and, 245, 248\u2013252,\n274\u2013275, 281\u2013282,\n342\u2013343\nconstraints and, 253\u2013256,\n275\u2013276\ndatabase design and ( see\ndatabase design)\ndesign issues and, 279\u2013285\ndevelopment of, 244\ndiagrams ( see\nentity-relationship\ndiagrams)\nentity sets and, 244\u2013246,\n261\u2013264, 281\u2013283\nextended features, 271\u2013279\ngeneralization and, 273\u2013274\nmapping cardinalities and,\n252\u2013256\nnormalization and, 344\u2013345\noverview, 8\nprimary keys and, 256\u2013260\nredundancy and, 261\u2013264\nrelationship sets and,\n246\u2013249, 282\u2013285\nschemas and, 244, 246,\n269\u2013270, 277\u2013279\nspecialization and, 271\u2013273\nUni\ufb01ed Modeling Language\nand, 288\u2013291\nentity sets\nalternative notations for,\n285\u2013291\nattributes and, 245, 265\u2013267,\n281\u2013282\nde\ufb01ned, 245\ndesign issues and, 281\u2013283\nentity-relationship diagrams\nand, 245\u2013246, 265\u2013268entity-relationship (E-R)\nmodel and, 244\u2013246,\n261\u2013264, 281\u2013283\nextension of, 245\nhierarchies of, 273, 275\nidentifying, 259\nprimary keys and, 257\nproperties of, 244\u2013246\nrelationship sets and,\n246\u2013249, 282\u2013283\nremoving redundancy in,\n261\u2013264\nrepresentation of, 265\u2013268\nstrong, 259, 265\u2013267\nsubclass, 274\nsuperclass, 274\nUni\ufb01ed Modeling Language\nand, 288\u2013291\nvalue and, 245\nweak, 259\u2013260, 267\u2013268\nentries, 1241\nequality-generating dependencies,\n337\nequi-depth histograms, 759\nequi-joins, 704, 707\u2013713, 718,\n722, 730, 1043\nequivalence\ncon\ufb02ict, 815, 815n2\ncost analysis and, 771\nenumeration of expressions,\n755\u2013757\njoin ordering and, 754\u2013755\nrelational algebra and, 58,\n747\u2013757\ntransformation examples for,\n752\u2013754\nview, 818, 818n4\nequivalence rules, 747\u2013752, 754,\n771\nequivalent queries, 58\nequi-width histograms, 759\nerase block, 568\nE-R diagrams. See\nentity-relationship diagrams\nE-R model. Seeentity-relationship\n(E-R) model\nescape, 83\nEthereum, 1258, 1262, 1265,\n1267\u20131272, 1274\nEthernet, 978\n", "1340": "1312 Index\nETL (extract, transform and\nload) tasks, 520, 524\nevaluation primitive, 691\neventual consistency, 1016, 1139\nevery function, 96\nevicted blocks, 605\nexactly-once semantics, 1074\nexcept all, 89, 97\nexcept clause, 216\nexcept construct, 102\nexception conditions, 202\nexceptions, 187\nexcept operation, 88\u201389\nexchange-operator model,\n1055\u20131057\nexclusive locks\nbiased protocol and, 1124\nconcurrency control and,\n835\u2013843, 888, 892, 893\ndegree-two consistency and,\n880\ngraph-based protocols and,\n846\u2013847\nmultiple granularity and,\n854\u2013855\nmultiversion, 871\nrecovery systems and, 916\ntransactions and, 825, 928\nexclusive-mode locks, 835, 842\nEXEC SQL, 197\nexecute privilege, 169\u2013170\nexecution skew, 1007, 1008,\n1043\nexistence bitmaps, 1184\nexistence dependence, 259\nexists construct, 101, 102, 108\nexpiration of leases, 1115\nexplain command, 746\nexplicit locks, 854\nextendable hashing, 661, 1195,\n1196\nExtensible Markup Language.\nSeeXML\nextension of entity sets, 245\nextent (blocks), 579\nexternal consistency, 1131\nexternal data, 1077\nexternal language routines,\n203\u2013206\nexternal sorting, 701external sort-merge algorithm,\n701\u2013704\nextract, transform and load\n(ETL) tasks, 520, 524\nextraneous attributes, 325\nextraneous functional\ndependencies, 324\nfactorials, 811\nfact tables, 524\nfailed transactions, 806, 907,\n909\nfail-stop assumption, 908, 1267\nfailure recovery, 21\nfalse cycles, 1114\u20131115\nfalse positives, 1083\nfalse values, 96\nfanout, 635\nfat-tree topology, 977\nfault tolerance\nblockchain databases and,\n1276\ngeographic distribution and,\n1027\ninterconnection networks\nand, 978\nMapReduce paradigm and,\n1060\u20131061\nin query-evaluation plans,\n1059\u20131061\nreplicated state machines and,\n1158\u20131161\nshared-disk architecture and,\n984\nwith streaming data,\n1074\u20131076\nupdates and, 1138\nfault-tolerant key-value store,\n1160\nfault-tolerant lock manager, 1160\nfederated distributed databases,\n988, 1076\u20131077,\n1132\u20131133\nfetching. See also information\nretrieval\nadvanced SQL and, 187\u2013188,\n193, 195\u2013197, 202, 205,\n222\napplication design and,\n421\u2013427, 431, 437, 1218,\n1229by bu\ufb00er manager, 19\ndata warehousing and, 526\nlarge-object types and, 156,\n158\nprefetching, 969\nstorage and, 567, 572, 587\n\ufb01at currencies, 1252, 1273\nFiber Channel FC interface, 563\nFiber Channel Protocol, 978\n\ufb01fth normal form, 341\n\ufb01le headers, 590\u2013591\n\ufb01le manager, 19\n\ufb01le organization, 588\u2013602\nblobs, 156, 193, 594, 652\nblocks and, 579, 588\nB+-tree, 595, 650\u2013652, 697,\n697n4\nclobs, 156, 193, 594, 652\ndistributed, 472\u2013475, 489,\n1003, 1019\u20131022\n\ufb01xed-length records and,\n589\u2013592\nhash, 595, 659\nheap \ufb01le organization,\n595\u2013597\nindexing and ( seeindices)\njournaling systems, 610\nlarge objects and, 594\u2013595\nmultitable clustering, 595,\n598\u2013601\nnull values and, 593\npartitioning and, 601\u2013602\npointers and, 588, 591,\n594\u2013598, 601\nreorganization, 598\nsequential, 595, 597\u2013598\nvariable-length records and,\n592\u2013594\n\ufb01le-processing systems, 5\u20138\n\ufb01le scans, 695\u2013697, 704\u2013707,\n727\n\ufb01le system consistency check, 610\n\ufb01nancial sector, database\napplications for, 3, 1279\n\ufb01ne-grained locking, 947\n\ufb01ne-grained parallelism, 963, 970\n\ufb01rm deadlines, 894\n\ufb01rst committer wins, 874\n\ufb01rst normal form (1NF),\n342\u2013343\n", "1341": "Index 1313\n\ufb01rst updater wins, 874\u2013875\n\ufb01ve minute rule, 1229\n\ufb01xed-length records, 589\u2013592\n\ufb01xed point of recursive view\nde\ufb01nition, 217\n\ufb02ash-as-bu\ufb00er approach, 1229\n\ufb02ash memory, 567\u2013570\ncost of, 560\nerase block and, 568\nhybrid, 569\u2013570\nindexing on, 656\u2013657\nLSM trees and, 1182\nNAND, 567\u2013568\nNOR, 567\nwear leveling and, 568\n\ufb02ash translation layer, 568\n\ufb02atMap function, 496\u2013498\n\ufb02exible schema, 366\nFlinkCEP, 504\n\ufb02oat, 67\nFlutter framework, 428\nfollowers, 1155\nforced output, 607, 912\nforce policy, 927\nfor clause, 534\nfor each row clause, 207\u2013210,\n212\nfor each statement clause, 76,\n209\u2013210\nforeign-currency exchange, 1277\nforeign keys, 45\u201346, 69\u201370,\n148\u2013150, 267\u2013268, 268n5\nforeign tables, 1077\nforking, 1257, 1258, 1263\nformal standards, 1237\nfourth normal form (4NF), 336,\n339\u2013341\nfragment-and-replicate joins,\n1046\u20131047, 1062\nfragmentation, 579\nfree lists, 591\nfree-space maps, 596\u2013597\nfrom clause\naggregate functions and,\n91\u201396\nbasic SQL queries and, 71\u201379\non multiple relations, 74\u201379\nin multiset relational algebra,\n97\nnull values and, 90query optimization and,\n775\u2013777\nrename operation and, 79,\n81\u201382\nset operations and, 85\u201389\non single relation, 71\u201374\nstring operations and, 82\u201383\nsubqueries and, 104\u2013105\nfull nodes, 1256, 1268\nfull outer joins, 132\u2013136, 722\nfunctional dependencies\nalgorithms for decomposition\nusing, 330\u2013335\nattribute set closure and,\n322\u2013324\naugmentation rule and, 321\naxioms and, 321\nBoyce\u2013Codd normal form\nand, 313\u2013316, 330\u2013333\ncanonical cover and, 324\u2013328\nclosure of a set, 320\u2013324\ndecomposition rule and, 321\ndependency preservation and,\n315\u2013316, 328\u2013330\nextraneous, 324\nhigher normal forms and, 319\nkeys and, 309\u2013312\nlossless decomposition and,\n312\u2013313\nmultivalued, 336\u2013341\nnotational conventions and,\n309\npseudotransitivity rule and,\n321\nre\ufb02exivity rule and, 321\nin schema design, 145\ntheory of, 320\u2013330\nthird normal form and,\n317\u2013319, 333\u2013335\ntransitivity rule and, 321\ntrivial, 311\nunion rule and, 321\nfunctionally determined\nattributes, 322\u2013324\nfunctional query language, 47\nfunctions. See also speci\ufb01c\nfunctions\ndeclaring, 199\u2013201\nexternal language routines\nand, 203\u2013206hash ( seehash functions)\nlanguage constructs for,\n201\u2013203\nsyntax and, 199, 201\u2013205\nwriting in SQL, 198\u2013206\nfuzzy checkpoints, 922, 930, 941\nfuzzy dump, 931\nfuzzy lookup, 523\ngas concept for transactions,\n1270\u20131271\nGAV (global-as-view) approach,\n1078\u20131079\ngeneralization\nattribute inheritance and,\n274\u2013275\nbottom-up design and, 273\ndisjoint, 279, 290\nentity-relationship (E-R)\nmodel and, 273\u2013274\noverlapping, 279, 290\npartial, 275\nrepresentation of, 278\u2013279\nsubclass set and, 274\nsuperclass set and, 274\ntop-down design and, 273\ntotal, 275\nGeneralized Search Tree (GiST),\n670\ngenesis blocks, 1254\u20131255\ngeographically distributed\nstorage, 1026\u20131027\ngeographic data\napplications of, 391\u2013392\nexamples of, 387\noverlays and, 393\nraster data, 392\nrepresentation of, 392\u2013393\nsubtypes of, 390\ntopographical, 393\nvector data, 392\u2013393\ngeographic information systems,\n387\ngeometric data, 388\u2013390\ngetColumnCount method,\n191\u2013192\ngetConnection method, 186,\n186n1\ngetFloat method, 188\nget function, 477\u2013479\n", "1342": "1314 Index\nGET method, 440\ngetString method, 188\nGFS. SeeGoogle File System\nGiST (Generalized Search Tree),\n670\nGlass\ufb01sh, 416\nglobal-as-view (GAV) approach,\n1078\u20131079\nglobal indices, 1017\u20131019\nGlobal Positioning System\n(GPS), 1130\nglobal schema, 1076, 1078\u20131079\nglobal transactions, 988, 1098,\n1132\nglobal wait-for graphs,\n1113\u20131114\nGoogle\napplication design and,\n406\u2013408, 410, 428\u2013429\nBigtable, 477, 479\u2013480, 668,\n1024\nPageRank from, 385\u2013386,\n493, 510\nPregel system developed by,\n511\nSpanner, 1160\u20131161\nGoogle File System (GFS), 473,\n474, 1020, 1022\nGoogle Maps, 393\nGPS (Global Positioning\nSystem), 1130\nGPUs (graphics processing\nunits), 1064\ngrant command, 170\ngranted by current role, 172\u2013173\ngrant privileges, 166\u2013167,\n170\u2013171\ngraph-based protocols, 846\u2013848\ngraph databases, 508\u2013511\ngraphics processing units\n(GPUs), 1064\nGraphX, 511\ngroup by clause, 92\u201396, 105,\n142, 221, 227\u2013230\ngroup by construct, 534,\n536\u2013537\ngroup by cube, 536\ngroup by rollup, 537\ngroup-commit technique, 925\ngrouping function, 536\u2013537grouping sets construct, 230, 538\ngrowing phase, 841, 843\nGustafson\u2019s law, 974\nhackers. Seesecurity\nHadoop File System (HDFS),\n473\u2013475, 489\u2013493, 971,\n1020\u20131022\nHalloween problem, 785\nhandlers, 202\nhard deadlines, 894\nhard disk drives (HDDs). See\nmagnetic disks\nhard disks, 26\nhard forks, 1257, 1258\nhardware RAID, 574\u2013576\nhardware threads, 982\nhardware tuning, 1227\u20131230\nharmonic mean, 1231\nhash \ufb01le organization, 595, 659\nhash functions\nBloom \ufb01lters and, 1175\u20131176\nclosed, 659, 1194\ncollision resistant, 1259\u20131260\nconsistent, 1013\ncryptographic, 1253,\n1259\u20131263, 1265\nde\ufb01ned, 624, 659\ndeletion, 1190, 1194\u20131195,\n1198\ndynamic, 661, 1195\u20131203\nextendable, 661, 1195, 1196\ninsertion, 1194\u20131195,\n1197\u20131202\nirreversibility of, 1260\njoins and, 1045\nlinear, 661, 1203\nlookup, 1197, 1198,\n1202\u20131203\nopen, 1194\npartitioning and, 1045\npasswords and, 1260n4\nqueries and, 624, 1197\u20131202\nstatic, 661, 1190\u20131195,\n1202\u20131203\nupdates and, 624, 1197\u20131202\nhash indices\nbucket over\ufb02ow and,\n659\u2013660, 1194\u20131195\ncomparison with ordered\nindices, 1203data structure and, 1195\u20131196\nde\ufb01ned, 624\ndynamic hashing and, 661,\n1195\u20131203\nextendable hashing and, 661\n\ufb01le organization and, 595,\n659\ninsu\ufb03cient buckets and, 1194\nlinear hashing and, 661, 1203\nin main memory, 658\u2013659\nover\ufb02ow chaining and,\n659\u2013660\nskew and, 660, 1194\nstatic hashing and, 661,\n1190\u20131195, 1202\u20131203\ntuning of, 1215\nhash join\nbasics of, 712\u2013714\nbuild input and, 713\ncost of, 715\u2013717\nhybrid, 717\u2013718\nover\ufb02ows and, 715\npipelining and, 728\u2013731\nprobe input and, 713\nquery optimization and, 769,\n771\nquery processing and,\n712\u2013718, 786, 1063\nrecursive partitioning and,\n714\u2013715\nskewed partitioning and, 715\nhash partitioning, 476,\n1005\u20131007\nhash-table over\ufb02ow, 715\nhash trees. SeeMerkle trees\nhaving clause, 95\u201396, 104\u2013105,\n142\nHBase, 477, 480, 489, 668, 971,\n1024, 1028\u20131031\nHDDs (hard disk drives). See\nmagnetic disks\nHDFS. SeeHadoop File System\nhead-disk assemblies, 565\nhealth care, blockchain\napplications for, 1277\u20131278\nheap \ufb01les, 595\u2013597, 1203\nheart-beat messages, 1147\nheterogeneous distributed\ndatabases, 988, 1132\n", "1343": "Index 1315\nheuristics, 766, 771\u2013774, 786,\n1189\nHibernate system, 382,\n431\u2013433, 1240\nhierarchical architecture, 979,\n980, 986\nhierarchical clustering, 548\nhierarchical data models, 26\nhierarchies\ncross-tabulation and, 532\non dimensions, 531\nof entity sets, 273, 275\nrelational representation of,\n533\ntransitive closures on, 214\nhigh availability, 907, 931\u2013933,\n987, 1121\nhigh availability proxy, 932\u2013933\nhigher-level locks, 935\u2013936\nhigher normal forms, 319\nhistograms\nattributes and, 758\u2013760\ndistribution approximated by,\n543\nequi-depth, 759\nequi-width, 759\nexamples of, 758\u2013759, 1009\njoin size estimation and,\n763\u2013764\npercentile-based, 223\nrandom samples and, 761\nrange-partitioning vectors\nand, 1009\nselection size estimation and,\n760\nHive, 494, 495, 500\nHOLAP (hybrid OLAP), 535\nHollerith, Herman, 25\nhomogeneous distributed\ndatabases, 988\nhopping window, 505\nhorizontal partitioning, 1004,\n1216\u20131217\nhost language, 16, 197\nhot-spare con\ufb01guration, 933\nhot swapping, 575\nhouseholding, 523\nHTML. SeeHyperText Markup\nLanguageHTTP. SeeHyperText Transfer\nProtocol\nhybrid disk drives, 569\u2013570\nhybrid hash join, 717\u2013718\nhybrid merge-join algorithm, 712\nhybrid OLAP (HOLAP), 535\nhybrid row/column stores, 615\nhypercubes, 976\u2013977\nHyperledger Fabric, 1269, 1271\nhyperlinks, 385\u2013386\nHyperText Markup Language\n(HTML)\napplication design and, 404,\n406\u2013408, 426\nclient-side scripting and, 421\nJava Server Pages and,\n417\u2013418\nsecurity and, 439, 440\nserver-side scripting and,\n416\u2013418\nstylesheets and, 408\nweb sessions and, 408\u2013411\nHyperText Transfer Protocol\n(HTTP)\napplication design and,\n405\u2013413\nconnectionless nature of,\n409\u2013410\nman-in-the-middle attacks\nand, 442\nRepresentation State Transfer\nand, 426\nsecurity and, 440, 452\nhyper-threading, 982\nhypervisor, 994\nIBM DB2\nadvanced SQL and, 206\nhistory of, 26\nlimit clause in, 222\nquery optimization and, 774,\n783\nSpatial Extender, 388\nstatistical analysis and, 761\ntrigger syntax and, 212\ntypes and domains supported\nby, 160\nICOs (initial coin o\ufb00erings),\n1272\nIDEF1X standard, 288IDE (integrated development\nenvironment), 416\nidempotent operations, 937\nidenti\ufb01ers\nblock, 474\u2013475, 1020\ndata-item, 913\nindices and, 700\nlog records and, 913\nquery processing and, 700\nselection and, 700\ntransaction, 913\nidentifying entity sets, 259\nidentifying relationship, 259\u2013260\nidentity declaration, 1226\nidentity speci\ufb01cation, 161\nidentity theft, 447\nIDF (inverse document\nfrequency), 384\nIEEE (Institute of Electrical and\nElectronics Engineers),\n1237\nif clauses, 212\nif-then-else statements, 202\nimmediate-modi\ufb01cation\ntechnique, 915\nimmediate view maintenance,\n779, 1215\u20131216\nimperative query language, 47\nimplicit locks, 854\u2013855\nincompleteness in database\ndesign, 243\u2013244\ninconsistent data, 6\ninconsistent state, 802, 803, 812\nin construct, 99\nincremental view maintenance,\n779\u2013782\nincrement lock, 892\u2013893\nincrement operation, 892\nindependent parallelism,\n1054\u20131055\nindexed nested-loop join,\n707\u2013708, 728\nindex entries, 626\nindexing strings, 653\nindex-locking protocol, 860\nindex-locking technique, 860\nindex records, 626\nindex scans, 696, 698\u2013699, 769,\n769n2\n", "1344": "1316 Index\nindex-sequential \ufb01les, 625,\n634\u2013635\nindices, 623\u2013676, 1175\u20131203\naccess time and, 624,\n627\u2013628\naccess types and, 624\nbasic concepts related to,\n623\u2013624\nbitmap ( seebitmap indices)\nBloom \ufb01lters and, 667,\n1175\u20131176, 1181\nB+-tree ( seeB+-trees)\nbu\ufb00er trees and, 668\u2013670\nbulk loading of, 653\u2013655\nclustering, 625, 632\u2013633,\n695, 697\u2013698\ncomparisons and, 698\u2013699\ncomposite, 700\nconcurrency control and,\n884\u2013887\ncovering, 663\ncreation of, 664\u2013665,\n884\u2013885\nde\ufb01ned, 19\nde\ufb01nition in SQL, 164\u2013165,\n664\u2013665\ndeletion time and, 624,\n631\u2013632, 641, 645\u2013649\ndense, 626\u2013628, 630\u2013631\non \ufb02ash storage, 656\u2013657\nGeneralized Search Tree, 670\nglobal, 1017\u20131019\nhash ( seehash indices)\nidenti\ufb01ers and, 700\ninsertion time and, 624,\n630\u2013631, 641\u2013645, 647,\n649\ninverted, 721\nkey-value stores and, 1028\nlinear search and, 695\nlocal, 1017\nLSM trees and, 666\u2013668,\n1176\u20131182, 1215\nin main memory, 657\u2013658\nmaterialized views and, 783\nmultilevel, 628\u2013630\nmultiple-key access and,\n633\u2013634, 661\u2013663\nnonclustering, 625, 695\nordered ( seeordered indices)parallel, 1017\u20131019\nperformance tuning and, 1215\npointers and, 700\nprimary, 625, 695, 1017\u20131018\nquery processing and,\n695\u2013697\nrecord relocation and,\n652\u2013653\nsearch keys and, 624\u2013634\nsecondary, 625, 632\u2013633,\n652\u2013653, 695\u2013698,\n1017\u20131019\nselection operation and,\n695\u2013697, 783\nsequential, 625, 634\u2013635\nsorting and, 701\u2013704\nspace overhead and, 624,\n627\u2013628, 634, 1202\nsparse, 626\u2013632\nspatial data and, 672\u2013675,\n1186\u20131190\nSQL DDL and, 67\nstepped-merge, 667,\n1179\u20131181\nof temporal data, 675\u2013676\nupdates and, 630\u2013632\nwrite-optimized structures,\n665\u2013670\nin-doubt transactions, 1105\ninfeasibility, 1259\u20131260\nIn\ufb01niband standard, 978\ninformation extraction, 549\ninformation retrieval. See also\nqueries\nde\ufb01ned, 382\nkeywords and, 383\nmeasuring e\ufb00ectiveness of,\n386\nPageRank and, 385\u2013386\nprecision and, 386\nrecall and, 386\nrelevance ranking and,\n383\u2013386\nstop words and, 385\nstructured data queries and,\n386\u2013387\nTF-IDF approach and,\n384\u2013385\ninfrastructure-as-a-service model,\n991Ingres system, 26\ninheritance\nattribute, 274\u2013275\nmultiple, 275\nsingle, 275\ntables and, 379\u2013380\ntypes and, 378\u2013379\ninitial coin o\ufb00erings (ICOs),\n1272\ninitialization vector, 449\ninitially deferred integrity\nconstraints, 151\ninner joins, 132\u2013136, 771\ninner relation, 704\ninsert authorization, 14\ninsertion\nalgorithm for, 1180\u20131181\nB+-trees and, 641\u2013645, 647,\n649\nconcurrency control and, 857,\n858\ndatabase modi\ufb01cation and,\n110\u2013111\ndefault values and, 156\nhashing and, 1194\u20131195,\n1197\u20131202\nLSM trees and, 1177\u20131178,\n1180\u20131181\nordered indices and, 624,\n630\u2013631\nprepared statements and,\n188\u2013190\nprivileges and, 166\u2013167\nR-trees and, 1188\u20131189\nshipping SQL statements to\ndatabase and, 187\nSQL schema de\ufb01nition and,\n69\ntransactions and, 801, 826\nviews and, 141\u2013143\ninstances, 12, 309, 547. See also\ntraining instances\ninstead of feature, 143\nInstitute of Electrical and\nElectronics Engineers\n(IEEE), 1237\ninsurance claims, 1278\nintegrated development\nenvironment (IDE), 416\nintegrity constraints\n", "1345": "Index 1317\nadd, 146\nalter table and, 146\nassertions and, 152\u2013153\nassigning names to, 151\nauthorization, 14\ncheck clause and, 147\u2013149,\n152\u2013153\ncreate table and, 146\u2013149\ndeferred, 151\ndomain, 13\u201314\nexamples of, 145\nin \ufb01le-processing systems, 6\nforeign keys and, 148\u2013150\nfunctional dependencies ( see\nfunctional dependencies)\nnot null, 146, 150\nprimary keys and, 147, 148\nreferential, 14, 46, 149\u2013153,\n207\u2013208, 800\nschema diagrams of, 46\u201347\non single relation, 146\nspatial, 391\nSQL and, 14\u201315, 66, 145\u2013153\nunique, 147\nuser-de\ufb01ned types and,\n159\u2013160\nviolation during transactions,\n151\u2013152\nintegrity manager, 19\nIntel, 569, 1064\nintention-exclusive (IX) mode,\n855\nintention lock modes, 855\nintention-shard (IS) mode, 855\ninterconnection networks,\n975\u2013979\ninteresting sort order, 770\ninterference, 974, 1232\nintermediate keys, 1050\nintermediate SQL, 125\u2013173\nauthorization and, 165\u2013173\ncreate table extensions and,\n162\ndata/time types in, 154\ndefault values and, 156\ngenerating unique key values\nand, 160\u2013161\nindex de\ufb01nition in, 164\u2013165\nintegrity constraints and,\n145\u2013153join expressions and, 125\u2013136\nlarge-object types and, 156,\n158\nroles and, 167\u2013169\nschemas, catalogs, and\nenvironments, 162\u2013163\ntransactions and, 143\u2013145\ntype conversion and\nformatting functions,\n155\u2013156\nuser-de\ufb01ned types and,\n158\u2013160\nviews and, 137\u2013143\ninternal nodes, 635\nInternational Organization for\nStandardization (ISO), 65,\n1237,\n1241\nInternet. SeeWorld Wide Web\nInternet of Things (IoT), 470,\n1278\ninteroperation parallelism, 1040,\n1052\u20131055\ninterquery parallelism, 1039\nintersect all, 88, 97\nintersection of bitmaps, 671,\n1183\nintersection operation, 750, 782\nintersect operation, 54\u201355,\n87\u201388\ninterval data type, 154\nintra-node partitioning, 1004\nintraoperation parallelism\naggregation and, 1049\nde\ufb01ned, 1040\nduplicate elimination and,\n1049\nmap and reduce operations\nand, 1050\u20131052\nparallel external sort-merge\nand, 1042\u20131043\nparallel join and, 1043\u20131048\nparallel sort and, 1041\u20131043\nprojection and, 1049\nrange-partitioning sort and,\n1041\u20131042\nselection and, 1049\nintraquery parallelism, 1039\ninvalidation reports, 1141\ninvalidation timestamps, 873n3inverse document frequency\n(IDF), 384\ninverted indices, 721\nI/O operations per second\n(IOPS), 567, 577, 578,\n693n3\nI/O parallelism\nhashing and, 1005\u20131007\npartitioning techniques and,\n1004\u20131007\nrange scheme and, 1005, 1007\nround-robin scheme and,\n1005, 1006\nIoT (Internet of Things), 470,\n1278\nirrefutability, 1257, 1259\nirreversibility, 1260\nIS (intention-shard) mode, 855\nis not null, 89, 90\nis not unknown, 90\nis null, 89\nISO (International Organization\nfor Standardization), 65,\n1237,\n1241\nisolation\natomicity and, 819\u2013821\ncascadeless schedules and,\n820\u2013821\nconcurrency control and,\n803\u2013804, 807\u2013812, 823\nof data, 6\nde\ufb01ned, 800\nfactorials and, 811\nimproved throughput and,\n808\ninconsistent state and, 803\nlevels of, 821\u2013826\nlocking and, 823\u2013825\nmultiple versions and,\n825\u2013826\nread committed, 1225\nrecoverable schedules and,\n819\u2013820\nresource allocation and, 808\nserializability and, 821\u2013826\nsnapshot ( seesnapshot\nisolation)\ntimestamps and, 825\n", "1346": "1318 Index\nof transactions, 800\u2013804,\n807\u2013812, 819\u2013826\nutilization and, 808\nwait time and, 808\u2013809\nis unknown, 90\niteration, 201\u2013202, 214\u2013216\niterators, 727\nIX (intention-exclusive) mode,\n855\nJakarta Project, 416\nJava. See also JDBC (Java\nDatabase Connectivity)\nadvanced SQL and, 183, 197,\n199, 205\napplication design and, 16,\n417\nmetadata and, 191\u2013193\nobject-oriented programming\nand, 377\nobject-relational mapping\nsystem for, 382\nResilient Distributed Dataset\nand, 496\u2013498\nUni\ufb01ed Modeling Language\nand, 289\nJava Database Connectivity. See\nJDBC\nJava 2 Enterprise Edition\n(J2EE), 416\nJavaScript\napplication design and,\n404\u2013405, 421\u2013426\ninput validation and, 422\u2013423\ninterfacing with web services,\n423\u2013426\nRepresentation State Transfer\nand, 426, 427\nresponsive user interfaces\nand, 423\nsecurity and, 439\nJavaScript Object Notation\n(JSON)\napplications for use, 368\u2013369\nde\ufb01ned, 368\nemergence of, 27\nencoding results with, 426\nexample of, 368, 369\n\ufb02exibility of, 367\u2013368\nkey-value stores and, 1024mapping to data models, 480\nas semi-structured data\nmodel, 8, 27\nSQL in support of, 369\u2013370\nfor transferring data, 423\nJava Server Pages (JSP)\napplication design and, 405,\n417\u2013418\nsecurity and, 440\nserver-side scripting and,\n417\u2013418\nservlets and, 417\u2013418\nJava Servlets, 411\u2013416, 419, 424\nJBoss, 416\nJDBC (Java Database\nConnectivity), 184\u2013193\nblob column and, 193\ncaching and, 435\u2013436\ncallable statements and,\n190\u2013191\nclob column and, 193\nconnecting to database,\n185\u2013186\ncorresponding interface\nde\ufb01ned by, 17\nexception and resource\nmanagement, 187\nmetadata features and,\n191\u2013193\nprepared statements and,\n188\u2013190\nprotocol information, 186\nretrieving query results,\n187\u2013188\nshipping SQL statements to,\n186\u2013187\nupdatable result sets and, 193\nweb sessions and, 409\njoin conditions, 130\u2013131\njoin dependencies, 341\njoin operation, 52\u201353\njoins\nant-ijoin operation, 108, 776\nanti-semijoin operation,\n776\u2013777\nbroadcast, 1046\ncomplex, 718\ncost analysis and, 710\u2013712,\n767\u2013770equi-joins, 704, 707\u2013713, 718,\n722, 730, 1043\nfragment-and-replicate,\n1046\u20131047, 1062\nfull outer, 132\u2013136, 722\nhash ( seehash join)\nhybrid merge, 712\ninner, 132\u2013136, 771\ninner relation of, 704\nleft outer, 131\u2013136, 722\nmerge-join, 708\u2013712, 1045\nminimization and, 784\nnatural ( seenatural joins)\nnested loop ( seenested-loop\njoin)\nordering, 754\u2013755\nouter, 57, 131\u2013136, 722\u2013723,\n765, 782\nouter relation of, 704\nparallel, 1043\u20131048\npartitioned, 714\u2013715,\n1043\u20131046\nquery processing and,\n704\u2013719, 722\u2013723,\n1081\u20131082\nright outer, 132\u2013136, 722\nsemijoin operation, 108, 776,\n1082\u20131084\nsize estimation and, 762\u2013764\nskew in, 1047\u20131048\nsort-merge-join, 708\u2013712\nspatial, 394\nspatial data and, 719\nstreaming data and, 506\ntheta, 748\u2013749\ntypes and conditions,\n130\u2013131, 136\nview maintenance and, 780\njoin skew avoidance, 1048\njoin using operation, 129\u2013130\njournaling \ufb01le systems, 610\nJSON. SeeJavaScript Object\nNotation\nJSP. SeeJava Server Pages\nJ2EE (Java 2 Enterprise\nEdition), 416\njukebox systems, 561\nKafka system, 506, 507,\n1072\u20131073, 1075, 1137\n", "1347": "Index 1319\nk-d B trees, 674\nKDD (knowledge discovery in\ndatabases), 540\nk-d trees, 673\u2013674\nkernel functions, 545\nkeys\ncandidate, 44\ncluster key, 600\u2013601\nconstraints and, 258\nencryption and, 448\u2013449,\n451\u2013453\nequality on, 697\nforeign, 45\u201346, 69\u201370,\n148\u2013150, 267\u2013268, 268n5\nfunctional dependencies and,\n309\u2013312\nintermediate, 1050\nmultiple access, 633\u2013634,\n661\u2013663\npartitioning, 475\u2013476\nprimary ( seeprimary keys)\nreduce, 485\nin relational model, 43\u201346\nsearch ( seesearch keys)\nsmart cards and, 451\nsuperkeys, 43\u201344, 257\u2013258,\n309\u2013310, 312\nunique values, 160\u2013161\nkey-value locking, 887\nkey-value maps, 366\u2013367\nkey-value storage systems\nB i gD a t aa n d ,4 7 1 ,4 7 3 ,\n476\u2013480\nclusters and, 477\ndocument, 477, 1023\nfault tolerant, 1160\nparallel, 1003, 1023\u20131031\nreplication and, 476\nsocial-networking sites and,\n471\nwide-column, 1023\nkeyword queries, 383, 385\u2013387,\n721\nkilling the mutant, 1234\nknowledge discovery in databases\n(KDD), 540\nknowledge graphs, 374\u2013375,\n386\u2013387, 549\nknowledge representation, 368\nKubernetes, 995lambda architecture, 504, 1071\nlanguage constructs, 201\u2013203\nLanguage Integrated Query\n(LINQ), 198\nLANs. Seelocal-area networks\nlarge object storage, 594\u2013595\nlarge-object types, 156, 158\nLastLSN, 943, 946\nlatches, 886, 928\nlatch-free data structure,\n888\u2013890\nlatency, 989\nlatent failure, 575\nlateral clause, 105\nLAV (local-as-view) approach,\n1079\nlazy generation of tuples, 727\nlazy propagation of updates,\n1122, 1136\nLDAP Data Interchange Format\n(LDIF), 1242\nLDAP (Lightweight Directory\nAccess Protocol), 442,\n1085,\n1240\u20131243\nleaders, 1155\nleaf nodes, 635\u2013656, 665\u2013669,\n673, 674\nlearners, 1148, 1153\nleases, 1115\u20131116, 1147\nleast recently used (LRU)\nstrategy, 605, 607\nledgers, digital, 1251, 1252\nleft-deep join orders, 773\nleft outer join, 131\u2013136, 722\nlegacy systems, 1035\u20131036\nlegal instance, 309\nLevelDB, 668\nLightning network, 1275\nlight nodes, 1256, 1268\nLightweight Directory Access\nProtocol (LDAP), 442,\n1085,\n1240\u20131243\nlike operator, 82\u201383\nlimit clause, 222\nlinear hashing, 661, 1203\nlinearizability, 1121\nlinear probing, 1194\nlinear regression, 546linear scaleup, 972\u2013973\nlinear search, 695\nlinear speedup, 972\nline segments, 388\u2013390\nlinestrings, 388, 390\nLinked Data project, 376, 1080\nLINQ (Language Integrated\nQuery), 198\nload balancers, 934\u2013935, 1214\nload barrier, 983\nlocal-area networks (LANs), 977,\n978, 985, 989\nlocal-as-view (LAV) approach,\n1079\nlocal autonomy, 988\nlocal indices, 1017\nlocal schema, 1076\nlocaltimestamp, 154\nlocal transactions, 988, 1098,\n1132\nlocal wait-for graphs, 1113\nlock conversions, 843\nlock-free data structure, 890\nlocking protocols\nB-link tree, 886\nconcurrency control and,\n835\u2013848\nde\ufb01ned, 839\ndistributed lock-manager,\n1112\ngraph-based, 846\u2013848\nimplementation of, 844\u2013846\nindex, 860\nkey-value, 887\nmultiple granularity, 856\u2013857\nnext-key, 887\nsingle lock-manager, 1111\ntransactions and, 823\u2013825\ntwo-phase, 841\u2013844, 871\u2013872,\n1129\u20131131\nlock managers, 844\u2013845, 965,\n1160\nlock point, 841\nlocks\nadaptive granularity and,\n969\u2013970\ncaching and, 969\ncall back and, 969\ncompatibility function and,\n836\n", "1348": "1320 Index\ndeadlocks ( seedeadlocks)\nde-escalation and, 970\ndistributed databases and,\n1111\u20131116\nescalation and, 857, 1227\nexclusive ( seeexclusive locks)\nexplicit, 854\nfalse cycles and, 1114\u20131115\n\ufb01ne-grained, 947\ngranting of, 836, 840\u2013841\ngrowing phase and, 841, 843\nhigher-level, 935\u2013936\nimplicit, 854\u2013855\nincrement, 892\u2013893\nintention modes and, 855\nleases, 1115\u20131116, 1147\nlogical undo operations and,\n935\u2013941\nlower-level, 935\u2013936\nmultiple granularity and,\n853\u2013857\nmultiversion schemes and,\n871\u2013872, 1129\u20131131\npredicate, 828, 861, 861n1\nrecovery systems and,\n935\u2013941\nrequest operation and,\n835\u2013841, 844\u2013846,\n849\u2013853, 886\nshared, 825, 835, 854, 880,\n1124\nshrinking phase and, 841, 843\nstarvation and, 853\ntimeouts and, 850\u2013851\ntimestamps and, 861\u2013866\ntransaction servers and,\n965\u2013968\ntrue matrix value and, 836\nwait-for graph and, 851\u2013852,\n1113\u20131114\nlock table, 844, 845, 967\nlog disks, 610\nlog force, 927\nlogical clock, 1118\nlogical counter, 862\nlogical-design phase, 18, 242\nlogical error, 907\nlogical level of abstraction, 9\u201312\nlogical logging, 936\nlogically implied schema, 320logical operations\nconcurrency control and,\n940\u2013941\nconsistency and, 936\u2013937\nde\ufb01ned, 935\nearly lock release and,\n935\u2013941\nidempotent, 937\nlog records and, 936\u2013940\nrollback and, 937\u2013939\nlogical routing of tuples,\n506\u2013507, 1071\u20131073\nlogical schema, 12\u201313,\n1223\u20131224\nlogical undo operations, 935\u2013941\nlog of transactions, 805\nlog processing, 486\u2013488\nlog records\nARIES and, 941\u2013946\nbu\ufb00ering and, 926\u2013927\ncheckpoint, 943\ncompensation, 922, 942, 945\ndatabase modi\ufb01cation and,\n915\u2013916\nforce/no-force policy and, 927\nidenti\ufb01ers and, 913\nlogical undo operations and,\n936\u2013940\nold/new values and, 913\nphysical, 936\nrecovery systems and,\n913\u2013919, 926\u2013927\nredo operation and, 915\u2013919\nsteal/no-steal policy and, 927\nundo operation and, 915\u2013919\nwrite-ahead logging rule and,\n926\u2013929\nlog sequence number (LSN),\n941\u2013946\nlog-structured merge (LSM)\ntrees, 1176\u20131182\nbasic, 1179\nBloom \ufb01lters and, 1181\ndeletion and, 1178\u20131179\nfor \ufb02ash storage, 1182\ninsertion into, 1177\u20131178,\n1180\u20131181\nlevels of, 1176\nlookup and, 1181parallel key-value stores and,\n1028\nperformance tuning and, 1215\nrolling merges and, 1178\nstepped-merge indices and,\n1179\u20131181\nupdates and, 1178\u20131179\nwrite-optimized structure of,\n666\u2013668\nlog writer process, 965\nlong-duration transactions,\n890\u2013891\nlookup\nin blockchain databases, 1268\nBloom \ufb01lters and, 667, 1181\nconcurrency control and,\n884\u2013887\ndata-storage systems and, 480\nfuzzy, 523\nhashing and, 1197, 1198,\n1202\u20131203\nindices and, 630, 637,\n640\u2013641, 645\u2013651,\n656\u2013661, 666\u2013669, 676\nLSM trees and, 1181\nquery optimization and, 769\nquery processing and, 698,\n707\nlossless decomposition, 307\u2013308,\n307n1, 312\u2013313\nlossy decompositions, 307\nlost update problem, 1016\nlost updates, 874\nlower-level locks, 935\u2013936\nloyalty programs, 1278\nLRU (least recently used)\nstrategy, 605, 607\nLSM trees. Seelog-structured\nmerge trees\nLSN (log sequence number),\n941\u2013946\nmachine-learning algorithms, 495\nmagnetic disks, 563\u2013567\naccess time and, 561, 566, 567\nblocks and, 566\u2013567\ncapacity of, 560\nchecksums and, 565\ncrashes and, 565\ndata-transfer rate and, 566\n", "1349": "Index 1321\ndisk controller and, 565\nfailure classi\ufb01cation and, 908\nhybrid, 569\u2013570\nmean time to failure and, 567\nperformance measures of,\n565\u2013567\nphysical characteristics of,\n563\u2013565\nread-write heads and,\n564\u2013565\nrecording density and, 565\nsectors and, 564\u2013566\nseek time and, 566, 566n2,\n567\nsizes of, 563\nmain memory, 559\u2013560,\n657\u2013658\nmain-memory databases\naccessing data in, 910\nconcurrency control in,\n887\u2013890\nrecovery in, 947\u2013948\nstorage in, 588, 615\u2013617\nmajority protocol, 1123\u20131126\nmanagement of data. See\ndatabase-management\nsystems\n(DBMSs)\nman-in-the-middle attacks, 442\nmanufacturing, database\napplications for, 3, 5\nmany-to-many mapping, 253\u2013255\nmany-to-one mapping, 252\u2013255\nmap function, 483\u2013494, 510. See\nalsoMapReduce paradigm\nmapping cardinalities, 252\u2013256\nMapReduce paradigm, 483\u2013494\ndevelopment of, 27, 481\nfault tolerance and,\n1060\u20131061\nin Hadoop, 489\u2013493\nintraoperation parallelism\nand, 1050\u20131052\nlog processing and, 486\u2013488\nparallel processing of tasks,\n488\u2013489\nSQL on, 493\u2013494\nword count program and,\n483\u2013486, 484n2, 490\u2013492markup languages. See speci\ufb01c\nlanguages\nmassively parallel systems, 970\nmaster nodes, 1012, 1136\nmaster replica, 1016\nmaster sites, 1026\nmaster-slave replication, 1137\nmaster table, 1222\nmatch clause, 509\nmaterialization, 724\u2013725\nmaterialized edges, 728\nmaterialized views\naggregation and, 781\u2013782\nde\ufb01ned, 140, 778\nindex selection and, 783\njoin operation and, 780\nparallel maintenance of,\n1069\u20131070\nperformance tuning and,\n1215\u20131216\nprojection and, 780\u2013781\nquery optimization and,\n778\u2013783\nselection and, 780\u2013781\nview maintenance and, 140,\n779\u2013782\nmax function, 91\u201392, 105, 723,\n766, 782\nmaximum margin line, 544\nmean time between failures\n(MTBF), 567n3\nmean time to data loss, 571\nmean time to failure (MTTF),\n567, 567n3\nmean time to repair, 571\nmeasure attributes, 524\nmediators, 1077\nmemcached system, 436\u2013437,\n482\nmemoization, 771\nmemory. See also storage\nbulk loading of indices and,\n653\u2013655\ncache, 559 ( see also caching)\ndata access and, 910\u2013912\n\ufb02ash, 560, 567\u2013570, 656\u2013657\nforce output and, 912\nmagnetic-disk, 560, 563\u2013567\nmain, 559\u2013560, 657\u2013658non-volatile random-access,\n579\u2013580\noptical, 560\u2013561\nover\ufb02ows and, 715\nquery costs and, 697\nquery processing in, 731\u2013734\nrecovery systems and,\n910\u2013912\nstorage class, 569, 588, 948\nmemory barrier, 983\u2013984\nmerge-join, 708\u2013712, 1045\nmerge-purge operation, 523\nmerging\nduplicate elimination and,\n719\u2013720\nexchange-operator model and,\n1055\u20131057\nordered, 1056\nparallel external sort-merge,\n1042\u20131043\nperformance tuning and,\n1222\u20131223\nquery processing and,\n701\u2013704, 708\u2013712\nrandom, 1056\nrolling merge, 1178\nMerkle-Patricia trees, 1269,\n1275\nMerkle trees, 1143\u20131146, 1268,\n1269\nmesh system, 976\nMESI protocol, 984\nmessage-based consensus, 1266\nmessage delivery process,\n1109\u20131110\nmetadata, 14, 191\u2013193, 470,\n602\u2013604, 1020\u20131022\nmicroservices architecture, 994\nMicrosoft\napplication design and, 417,\n442\nDatabase Tuning Assistant,\n1217\nquery languages developed by,\n538\nquery optimization and, 783\nStreamInsight, 504\nMicrosoft SQL Server\nadvanced SQL and, 206\nimplements, 160\n", "1350": "1322 Index\nlimit clause in, 222\nperformance monitoring\ntools, 1212\nperformance tuning tools,\n1218\nprocedural languages\nsupported by, 199\nsnapshot isolation and, 1225\nspatial data and, 388, 390\nstring operations and, 82\nmin function, 91\u201392, 723, 766,\n782\nminibatch transactions, 1227\nminimal equivalence rules, 754\nmining pools, 1266. See also data\nmining\nminus, 88n7\nmirroring, 571\u2013573, 576, 577\nmobile application platforms,\n428\u2013429\nmobile phone applications, 469\nmodels for data mining, 540\nmodel-view-controller (MVC)\narchitecture, 429\u2013430\nMOLAP (multidimensional\nOLAP), 535\nMonetDB, 367, 615\nMongoDB, 477\u2013479, 482, 489,\n668, 1024, 1028\nmonotonic queries, 218\nMoore\u2019s law, 980n4\nmost recently used (MRU)\nstrategy, 608\u2013609\nMRU (most recently used)\nstrategy, 608\u2013609\nMTBF (mean time between\nfailures), 567n3\nMTTF (mean time to failure),\n567, 567n3\nmultidimensional data, 524,\n527\u2013532\nmultidimensional OLAP\n(MOLAP), 535\nmultilevel indices, 628\u2013630\nmultimaster replication, 1137\nmultiple consensus protocol, 1151\nmultiple granularity\nconcurrency control and,\n853\u2013857\nhierarchy of, 854intention-exclusive mode and,\n855\nintention-shared mode and,\n855\nlocking protocol and,\n856\u2013857\nrequest operation and, 854,\n855\nshared and intention-exclusive\nmode and, 855\ntree architecture and,\n853\u2013857\nmultiple inheritance, 275\nmultiple-key access, 633\u2013634,\n661\u2013663\nmultiprogramming, 809\nmultiquery optimization,\n785\u2013786\nmultiset except, 88n7\nmultiset relational algebra, 80,\n97, 108, 136, 747\nmultiset types, 366\nmultisig instruction, 1269\u20131270\nmultitable clustering \ufb01le\norganization, 595,\n598\u2013601\nmultitasking, 961, 963\nmultiuser systems, 962\nmultivalued attributes, 251, 252,\n342\nmultivalued data types, 366\u2013367\nmultivalued dependencies,\n336\u2013341\nmultiversion concurrency control\n(MVCC), 869\u2013872\nmultiversion timestamp-ordering\nscheme, 870\u2013871, 1118\nmultiversion two-phase locking\n(MV2PL) protocol,\n871\u2013872,\n1129\u20131131\nmutations, 1234\nmutual exclusion, 965, 967\nMVCC (multiversion concurrency\ncontrol), 869\u2013872\nMVC (model-view-controller)\narchitecture, 429\u2013430\nMV2PL (multiversion two-phase\nlocking) protocol, 871\u2013872,\n1129\u20131131MySQL\nattributes and, 149n8\ngrowth of, 27\njoins and, 133n4\nLSM trees and, 668\nperformance monitoring\ntools, 1212\nstring operations and, 82\nunique key values in, 161\nna\u00c3\u00afve Bayesian classi\ufb01ers, 543\nna\u00c3\u00afve users, 24\nnamenode, 475, 1020\nNAND \ufb02ash memory, 567\u2013568\nNAS (network attached storage),\n563, 934\nnatural join operation, 57\nnatural joins, 126\u2013136\non condition and, 130\u2013131\nconditions and, 130\u2013131\nfull outer, 132\u2013136, 722\ninner, 132\u2013136, 771\nleft outer, 131\u2013136, 722\nouter, 57, 131\u2013136\nright outer, 132\u2013136, 722\nnavigation systems, database\napplications for, 3\nnearest-neighbor queries, 394,\n672, 674\nnearness queries, 394\nnegation, 762\nNeo4j graph database, 509, 510\nnested data types, 367\u2013368\nnested-loop join\nblock, 705\u2013707\nindexed, 707\u2013708, 728\nparallel, 1045\u20131046\nquery optimization and, 768,\n769, 771, 773\nquery processing and,\n704\u2013708, 713\u2013719, 722,\n786\nnested subqueries, 98\u2013107\nfrom clause and, 104\u2013105\nwith clause and, 105\u2013106\nduplicate tuples and, 103\nempty relations test and,\n101\u2013102\noptimization of, 774\u2013778,\n1220\n", "1351": "Index 1323\nscalar, 106\u2013107\nset operations and, 98\u2013101\nnesting, 854, 946\nNetBeans, 416\nnetwork attached storage (NAS),\n563, 934\nnetwork latency, 969\nnetwork partition, 481, 989,\n989n5, 1104\u20131105\nnetwork round-trip time, 969\nnetworks\ndeep neural, 546\ninterconnection, 975\u2013979\nlocal area, 977, 978, 985, 989\nspatial, 390\nwide-area, 989\nneural-net classi\ufb01ers, 545\u2013546\nnew value, 913\nnext-key locking protocols, 887\nnext method, 188\nnextval for, 1226\nNFNF (non \ufb01rst-normal-form),\n367\nnodes\nin blockchain databases,\n1255\u20131257, 1263\u20131268\ncoalescing, 641, 886\ndatanodes, 475, 1020\nde\ufb01ned, 468\ndistributed databases and,\n987\nfailure of, 1103\u20131104\nfull, 1256, 1268\nin Hadoop File System, 474,\n475\ninternal, 635\nleaf, 635\u2013656, 665\u2013669, 673,\n674\nleases and, 1115\u20131116\nlight, 1256, 1268\nmaster, 1012, 1136\nmesh architecture and, 976\nmultiple granularity and,\n853\u2013857\nnamenode, 475, 1020\nnonleaf, 635\u2013636, 642,\n645\u2013656, 663\noperation, 506\u2013507\nin parallel databases, 970\nprimary, 1123ring architecture and, 976\nsplitting of, 641\u2013645, 886\nstraggler, 1060\u20131061\nupdates and, 641\u2013647\nvirtual, 1009\u20131010\nno-force policy, 927\nnonbinary relationship sets,\n283\u2013285\nnon-blocking two-phase commit,\n1161\nnonces, 1265, 1271\nnonclustering indices, 625, 695\nnondeclarative actions, 183\nnon \ufb01rst-normal-form (NFNF),\n367\nnonleaf nodes, 635\u2013636, 642,\n645\u2013656, 663\nnonprocedural DMLs, 15\nnonprocedural languages, 15, 16,\n18, 26\nnonrepudiation, 451\nNon-Uniform Memory Access\n(NUMA), 981, 1063\nnonunique search keys, 632, 637,\n640, 649\u2013650\nNon-Volatile Memory Express\n(NVMe) interface, 562\nnon-volatile random-access\nmemory (NVRAM),\n579\u2013580, 948\nnon-volatile storage, 560, 562,\n587\u2013588, 804, 908\u2013910,\n930\u2013931\nnon-volatile write bu\ufb00ers,\n579\u2013580\nNOR \ufb02ash memory, 567\nnormal forms\natomic domains and, 342\u2013343\nBoyce\u2013Codd, 313\u2013316,\n330\u2013333, 336\ndomain-key, 341\n\ufb01fth, 341\n\ufb01rst, 342\u2013343\nfourth, 336, 339\u2013341\nhigher, 319\njoin dependencies and, 341\nproject-join, 341\nsecond, 316n8, 341\u2013342, 356\nthird, 317\u2013319, 333\u2013335\nnormalizationin conceptual-design process,\n17\ndenormalization, 346\nentity-relationship (E-R)\nmodel and, 344\u2013345\nperformance and, 346\nrelational database design\nand, 308\nNoSQL systems, 28, 473, 477,\n1269, 1276\nno-steal policy, 927\nnot connective, 74\nnot exists construct, 101\u2013102,\n108, 218\nnoti\ufb01cations, 436\nnot in construct, 99, 100\nnot null, 69, 89\u201390, 142, 146,\n150, 159\nnot operation, 89\u201390\nnot unique construct, 103\nnull bitmap, 593\nnull rejecting property, 751\nnull values\naggregation with, 96\nattributes and, 251\u2013252\nde\ufb01ned, 40, 67\n\ufb01le organization and, 593\nintegrity constraints and,\n145\u2013147, 150\nSQL and, 89\u201390\ntemporal data and, 347\ntriggers and, 209\nuser-de\ufb01ned types and, 159\nNUMA (Non-Uniform Memory\nAccess), 981, 1063\nnumeric, 67, 70\nnvarchar, 68\nNVMe (Non-Volatile Memory\nExpress) interface, 562\nNVRAM (non-volatile\nrandom-access memory),\n579\u2013580, 948\nN-way merge, 702\nOAuth protocol, 443\nobfuscation, 1235\nobject-based databases\narray types and, 378\ncomplex data types and,\n376\u2013382\n", "1352": "1324 Index\ninheritance and, 378\u2013380\nmapping and, 377, 381\u2013382\noverview, 9\nreference types and, 380\u2013381\nobject classes, 1242\nObject Database Management\nGroup (ODMG),\n1239\u20131240\nObject Management Group\n(OMG), 288\nobject of triples, 372\nobject-oriented databases\n(OODB), 9, 26, 377, 431,\n1239\u20131240\nobject-relational databases,\n377\u2013381\nde\ufb01ned, 377\nreference types and, 380\u2013381\ntable inheritance and,\n379\u2013380\ntype inheritance and,\n378\u2013379\nuser-de\ufb01ned types, 378\nobject-relational data models, 27,\n376\u2013382\nobject-relational mapping\n(ORM), 377, 381\u2013382,\n431\u2013434,\n1239\u20131240\nobservable external writes, 807\nODBC (Open Database\nConnectivity)\nadvanced SQL and, 194\u2013197\nAPI de\ufb01ned by, 194\u2013195\napplication interfaces de\ufb01ned\nby, 17\ncaching and, 436\nconformance levels and,\n196\u2013197\nstandards for, 1238\u20131239\ntype de\ufb01nition and, 196\nweb sessions and, 409\nODMG (Object Database\nManagement Group),\n1239\u20131240\no\ufb00-chain transactions, 1275\no\ufb04ine storage, 561\nOGC (Open Geospatial\nConsortium), 388OLAP. Seeonline analytical\nprocessing\nold value, 913\nOLE-DB, 1239\nOLTP (online transaction\nprocessing), 4, 521,\n1231\u20131232\nOMG (Object Management\nGroup), 288\non condition, 130\u2013131\non delete cascade, 150, 210,\n268n5\n1NF (\ufb01rst normal form),\n342\u2013343\none-to-many mapping, 252\u2013255\none-to-one mapping, 252\u2013254\nonline analytical processing\n(OLAP), 527\u2013540\naggregation on\nmultidimensional data,\n527\u2013532\ncross-tabulation and, 528\u2013533\ndata cubes and, 529\u2013530\nde\ufb01ned, 520, 530\ndicing and, 530\ndrill down and, 531, 540\nhybrid, 535\nimplementation of, 535\nmultidimensional, 535\nperformance benchmarks\nand, 1231\u20131232\nrelational, 535\nreporting and visualization\ntools, 538\u2013540\nrollup and, 530\u2013531, 536\u2013538\nslicing and, 530\nin SQL, 533\u2013534, 536\u2013538\nonline index creation, 884\u2013885\nonline storage, 561\nonline transaction processing\n(OLTP), 4, 521,\n1231\u20131232\non update cascade, 150\nOODB. Seeobject-oriented\ndatabases\nopen addressing, 1194\nOpen Database Connectivity. See\nODBC\nOpen Geospatial Consortium\n(OGC), 388open hashing, 1194\nOpenID protocol, 443\nopen polygons, 388n3\nopen time intervals, 675\noperation consistent state,\n936\u2013937\noperation nodes, 506\u2013507\noperation serializability, 885\noperator trees, 724, 1040\noptical storage, 560\u2013561\noptimistic concurrency control,\n869\noptimistic concurrency control\nwithout read validation,\n883,\n891\noptimization cost budget, 774\nOracle\nadvanced SQL and, 206\napplication server, 416\ndatabase design and, 443n6,\n444\u2013445\ndecode function in, 155\nEvent Processing, 504\nGeoRaster extension, 367\nhistory of, 26\nJDBC interface and, 185, 186\nkeywords in, 81n3, 88n7\nlimit clause in, 222\nnested subqueries and, 104\nperformance monitoring\ntools, 1212\nperformance tuning tools,\n1218\nprocedural languages\nsupported by, 199\nquery-evaluation plans and,\n746\nquery optimization and, 773,\n774, 783\nreference types and, 380\nset and array types supported\nby, 367\nsnapshot isolation and, 1225\nSpatial and Graph, 388\nstatistical analysis and, 761\nsyntax supported by, 204,\n212, 218, 218n9\ntransactions and, 822, 826,\n872\u2013873, 879\n", "1353": "Index 1325\ntypes and domains supported\nby, 160\nVirtual Private Database, 173,\n444\u2013445\noracles, 1271\u20131272\nORC, 490, 499, 613\u2013614\nor connective, 74\norder by clause, 83\u201384,\n219\u2013222, 534\nordered indices, 624\u2013634\ncomparison with hash\nindices, 1203\nde\ufb01ned, 624\ndense, 626\u2013628, 630\u2013631\nmultilevel, 628\u2013630\nsecondary, 625, 632\u2013633\nsequential, 625, 634\u2013635\nsparse, 626\u2013632\ntechniques for, 624\nupdates and, 630\u2013632\nordered merge, 1056\nORM. Seeobject-relational\nmapping\nor operation, 89\u201390\norphaned blocks, 1257, 1263\nouter-join operation, 57,\n131\u2013136, 722\u2013723, 765,\n782\nouter relation, 704\nouter union operations, 229n16\noutsourcing, 28\nover\ufb02ow avoidance, 715\nover\ufb02ow blocks, 598\nover\ufb02ow buckets, 659\u2013660,\n1194\u20131195\nover\ufb02ow chaining, 659\u2013660\nover\ufb02ow resolution, 715\noverlapping generalization, 279,\n290\noverlapping specialization, 272,\n275\noverlays, 393\npage (blocks), 567\nPageLSN, 942\u2013945\nPageRank, 385\u2013386\npage shipping, 968\nparallel databases\narchitecture of, 22, 970\u2013986\nBig Data and, 473, 480\u2013481coarse-grain, 963, 970\nconcurrency control and, 990\nde\ufb01ned, 480\nexchange-operator model and,\n1055\u20131057\n\ufb01ne-grain, 963, 970\nhierarchical, 979, 980, 986\nindices in, 1017\u20131019\ninterconnection networks\nand, 975\u2013979\ninterference and, 974\ninteroperation parallelism\nand, 1040, 1052\u20131055\ninterquery parallelism and,\n1039\nintraoperation parallelism\nand, 1040\u20131052\nintraquery parallelism and,\n1039\nI/O parallelism and,\n1004\u20131007\nkey-value stores and,\n1023\u20131031\nmassively parallel, 970\nmotivation for, 970\u2013971\noperator trees and, 1040\npartitioning techniques and,\n1004\u20131007\nperformance measures for,\n971\u2013974\npipelines and, 1053\u20131054\nquery optimization and,\n1064\u20131070\nreplication and, 1013\u20131016\nresponse time and, 971\u2013972\nscaleup and, 972\u2013974\nshared disk, 979, 980,\n984\u2013985\nshared memory, 979\u2013984,\n1061\u20131064\nshared nothing, 979, 980,\n985\u2013986, 1040\u20131041,\n1061\u20131063\nskew and, 974, 1007\u20131013,\n1043, 1062\nspeedup and, 972\u2013974\nstart-up costs and, 974, 1066\nthroughput and, 971\ntransaction processing in,\n989\u2013990parallel external sort-merge,\n1042\u20131043\nparallel indices, 1017\u20131019\nparallelism\ncoarse-grained, 963\ndata, 1042, 1057\n\ufb01ne-grained, 963\nimprovement of performance\nvia, 571\u2013572\nindependent, 1054\u20131055\ninteroperation, 1040,\n1052\u20131055\ninterquery, 1039\nintraoperation, 1040\u20131052\nintraquery, 1039\nI/O ( seeI/O parallelism)\nSingle Instruction Multiple\nData, 1064\nparallel joins, 1043\u20131048\nfragment-and-replicate,\n1046\u20131047, 1062\nhash, 1045\nnested-loop, 1045\u20131046\npartitioned, 1043\u20131046\nskew in, 1047\u20131048\nparallel key-value stores,\n1023\u20131031\natomic commit and, 1029\nconcurrency control and,\n1028\u20131029\ndata representation and,\n1024\u20131025\nde\ufb01ned, 1003\nelasticity and, 1024\nfailures and, 1029\u20131030\ngeographically distributed,\n1026\u20131027\nindex structure and, 1028\nmanaging without declarative\nqueries, 1030\u20131031\noverview, 1023\u20131024\nperformance optimizations\nand, 1031\nstoring and retrieving data,\n1025\u20131028\nsupport for transactions,\n1028\u20131030\nparallel processing, 437,\n488\u2013489\nparallel query plans\n", "1354": "1326 Index\nchoosing, 1066\u20131068\ncolocation of data and,\n1068\u20131069\ncost of, 1065\u20131066\nevaluation of, 1052\u20131061\nmaterialized views and,\n1069\u20131070\nspace for, 1064\u20131065\nparallel sort, 1041\u20131043\nparameterized views, 200\nparameter style general, 205\nparametric query optimization,\n786\np a r i t yb i t s ,5 7 2 ,5 7 4 ,5 7 7\nParquet, 490, 499\nparsing\napplication design and, 418\nbulk loads and, 1221\u20131223\nquery processing and,\n689\u2013690\npartial aggregation, 1049\npartial dependency, 356\npartial failure, 909\npartial generalization, 275\npartially committed transactions,\n806\npartial participation, 255\npartial rollback, 853\npartial schedules, 819\npartial specialization, 275\nparticipation in relationship sets,\n247\npartitioning attribute, 479\npartitioning keys, 475\u2013476\npartitioning vector, 1005\npartitions\nbalanced range, 1008\u20131009\ndata, 989n5\nde\ufb01ned, 1100\ndistributed databases and,\n1104\u20131105\ndistributed \ufb01le systems and,\n473\ndynamic repartitioning,\n1010\u20131013\nexchange-operator model and,\n1055\u20131057\n\ufb01le organization and,\n601\u2013602\nhash, 476, 1005\u20131007, 1045horizontal, 1004, 1216\u20131217\nintra-node, 1004\njoins and, 714\u2013715,\n1043\u20131046\nnetwork, 481, 989, 989n5,\n1104\u20131105\nparallel databases and,\n1004\u20131007\npoint queries and, 1006\nquery optimization and, 1065\nrange, 476, 1005, 1007, 1178\nrecursive, 714\u2013715\nof relation schema, 1216\u20131217\nround-robin, 1005, 1006\nscanning a relation and, 1006\nsharding and, 473, 475\u2013476,\n1275\nskew and, 715, 1007\u20131013\ntopic-partition, 1073\nvertical, 1004\nvirtual node, 1009\u20131010\npartition tables, 1011\u20131014\npasswords\napplication design and, 403,\n411, 414, 432, 450\u2013451\ndictionary attacks and, 449\ndistributed databases and,\n1240\nhash functions and, 1260n4\nleakage of, 440\u2013441\nman-in-the-middle attacks\nand, 442\none-time, 441\nsingle sign-on system and,\n442\u2013443\nSQL and, 163, 186, 196\nstorage and, 602, 603\nunencrypted, 414n4\npath expressions, 372, 381\npattern matching, 504\nPaxos protocol, 1152\u20131155,\n1160\u20131161, 1267\nPCIe interface, 562\npen drives, 560\nperformance\naccess time and, 561,\n566\u2013567, 578, 624,\n627\u2013628, 692\napplication design and,\n434\u2013437benchmarks ( seeperformance\nbenchmarks)\nof blockchain databases,\n1274\u20131276\nB+-trees and, 634, 665\u2013666\ncaching and, 435\u2013437\ndata-transfer rate and, 566\ndenormalization for, 346\nimprovement via parallelism,\n571\u2013572\nmagnetic disk storage and,\n565\u2013567\nmean time to failure and, 567,\n567n3\nmonitoring tools, 1212\nparallel databases and,\n971\u2013974\nparallel key-value stores and,\n1031\nparallel processing and, 437\nresponse time ( seeresponse\ntime)\nseek time and, 566, 566n2,\n567, 692, 710\nsequential indices and,\n634\u2013635\ntesting, 1235\nthroughput ( seethroughput)\ntuning ( seeperformance\ntuning)\nweb applications and,\n405\u2013411\nperformance benchmarks,\n1230\u20131234\ndatabase-application classes\nand, 1231\u20131232\nde\ufb01ned, 1230\nsuites of tasks and, 1231\nof Transaction Processing\nPerformance Council,\n1232\u20131234\nperformance tuning, 1210\u20131230\nautomated, 1217\u20131218\nbottleneck locations and,\n1211\u20131213, 1215, 1227\nbulk loads and, 1221\u20131223\nof concurrent transactions,\n1224\u20131227\nof hardware, 1227\u20131230\n", "1355": "Index 1327\nhorizontal partitioning of\nrelation schema,\n1216\u20131217\nindices and, 1215\nlevels of, 1213\u20131214\nmaterialized views and,\n1215\u20131216\nmotivation for, 1210\u20131211\nparameter adjustment and,\n1210, 1213\u20131215, 1220,\n1228, 1230\nphysical design and,\n1217\u20131218\nof queries, 1219\u20131223\nRAID and, 1214, 1229\u20131230\nof schema, 1214\u20131218,\n1223\u20131224\nset orientation and,\n1220\u20131221\nsimulation and, 1230\ntools for, 1218\nupdates and, 1221\u20131223,\n1225\u20131227\nperiod declaration, 157\nPerl, 206\npermissioned blockchains,\n1253\u20131254, 1256\u20131257,\n1263, 1266,\n1274\npersistent messaging, 990, 1016,\n1108\u20131110, 1137\nPersistent Storage Module\n(PSM), 201\nphantom phenomenon, 827,\n858\u2013861, 877\u2013879, 877n5,\n885, 887\nPHP, 405, 417, 418\nphysical blocks, 910\nphysical data independence,\n9\u201310, 13\nphysical-design phase, 18,\n242\u2013243\nphysical equivalence rules, 771\nphysical level of abstraction, 9,\n11, 12, 15\nphysical logging, 936\nphysical schema, 12, 13\nphysical storage systems,\n559\u2013580\ncache memory, 559cost per byte, 560, 561,\n566n2, 569, 576\ndisk-block access and,\n577\u2013580\n\ufb02ash memory, 560, 567\u2013570,\n656\u2013657\nhierarchy of, 561, 562\nindices and, 630n2\ninterfaces for, 562\u2013563\nmagnetic disks, 560, 563\u2013567\nmain memory, 559\u2013560\noptical storage, 560\u2013561\nRAID, 562, 570\u2013577\nsolid-state drives, 18, 560\ntape storage, 561\nvolatility of, 560, 562\nphysiological redo operations,\n941\nPig Latin, 494\npin count, 605\npinned blocks, 605\npin operations, 605\npipelined edges, 728\npipeline stage, 728\npipelining, 724\u2013731\nbene\ufb01ts of, 726\nfor continuous-stream data,\n731\ndemand-driven, 726\u2013728\nevaluation algorithms for,\n728\u2013731\nimplementation of, 726\u2013728\nparallelism and, 1053\u20131054\nproducer-driven, 726\u2013728\nuses for, 691\u2013692, 724, 725\npivot attribute, 227\npivot clause, 227, 534\npivoting, 226\u2013227, 530\npivot-table, 226\u2013227, 528\u2013529\nPJNF (project-join normal form),\n341\nplan caching, 774\nplatform-as-a-service model,\n992\u2013993\nplatters, 563, 565\nPL/SQL, 199, 204\npointers. See also indices\nblockchain databases and,\n1254\u20131255, 1261, 1269\nB+-tree ( seeB+-trees)concurrency control and,\n886, 888, 889\nquery processing and, 697,\n698, 700, 708\nrecovery systems and, 914,\n945\nredistribution of, 646\nSQL basics and, 193, 205\nstorage and, 588, 591,\n594\u2013598, 601\npoint queries, 1006, 1190\npolygons, 388\u2013390, 388n3, 393\npolylines, 388\u2013389, 388n3\npopulation, 547\nPostgreSQL\nadvanced SQL and, 206\narray types on, 378\nconcurrency control and, 873,\n879\nGeneralized Search Tree and,\n670\ngrowth of, 27\nheap \ufb01le organization and,\n596\nJDBC interface and, 185\nJSON and, 370\nperformance monitoring\ntools, 1212\nPostGIS extension, 367, 388,\n390\nprocedural languages\nsupported by, 199\nquery-evaluation plans and,\n746\nquery processing and, 692,\n694, 698\u2013699\nset and array types supported\nby, 367\nsnapshot isolation and, 1225\nstatistical analysis and, 761\ntransaction management in,\n822, 826\ntypes and domains supported\nby, 160\nunique key values in, 161\nP + Q redundancy schema, 573,\n574\nPractical Byzantine Fault\nTolerance, 1267\nprecedence graph, 816\u2013817\n", "1356": "1328 Index\nprecision, 386\nprecision locking, 861n1\npredicate locking, 828, 861,\n861n1\npredicate of triples, 372\npredicate reads, 858\u2013861\nprediction, 541\u2013543, 545\u2013546\npredictive models, 4\u20135\npreemption, 850\nprefetching, 969\npre\ufb01x compression, 653\nPregel system, 511\nprepared statements, 188\u2013190\npresentation layer, 429\nprice per TPS, 1232\nprimary copy, 1123\nprimary indices, 625, 695,\n1017\u20131018\nprimary keys\nattributes and, 310n4\nde\ufb01ned, 44\nentity-relationship (E-R)\nmodel and, 256\u2013260\nfunctional dependencies and,\n313\nintegrity constraints and, 147,\n148\nin relational model, 44\u201346\nSQL schema de\ufb01nition and,\n68\u201370\nprimary nodes, 1123\nprimary site, 931\nprimary storage, 561\nprime attributes, 356\nprivacy, 438, 446, 1252\nprivate-key encryption,\n1260\u20131261\nprivileges\nall, 166\nde\ufb01ned, 165\nexecute, 169\u2013170\ngranting, 166\u2013167, 170\u2013171\npublic, 167\nreferences, 170\nrevoking, 166\u2013167, 171\u2013173\nselect, 171, 172\ntransfer of, 170\u2013171\nupdate, 170\nprobe input, 713\nprocedural DMLs, 15procedural languages, 47n3, 184,\n199, 204\nprocedures\ndeclaring, 199\u2013201\nexternal language routines\nand, 203\u2013206\nlanguage constructs for,\n201\u2013203\nsyntax and, 199, 201\u2013205\nwriting in SQL, 198\u2013206\nprocess monitor process, 965\nproducer-driven pipeline,\n726\u2013728\nprogramming languages. See also\nspeci\ufb01c\nlanguages\naccessing SQL from, 183\u2013198\nmismatch and, 184\nobject-oriented, 377\nvariable operation of, 184\nProgressive Web Apps (PWA),\n429\nprojection\nintraoperation parallelism\nand, 1049\nquery optimization and, 764\nquery processing and, 720\nview maintenance and,\n780\u2013781\nproject-join normal form (PJNF),\n341\nproject operation, 49\u201350\nproof-of-stake consensus, 1256,\n1266\nproof-of-work consensus, 1256,\n1264\u20131266\nproposers, 1148, 1152\nproximity of terms, 385\nPR quadtrees, 1187\npseudotransitivity rule, 321\nPSM (Persistent Storage\nModule), 201\npublic blockchains, 1253, 1255,\n1257\u20131259, 1263, 1264\npublic-key encryption, 448\u2013449,\n1260\u20131261\npublish-subscribe (pub-sub)\nsystems, 507, 1072,\n1137\u20131139\npulling data, 727punctuations, 503\u2013504\npushing data, 727\nput function, 477, 478\npuzzle friendliness, 1265\nPWA (Progressive Web Apps),\n429\nPython\nadvanced SQL and, 183,\n193\u2013194, 206\napplication design and, 16,\n405, 416, 419\nobject-oriented programming\nand, 377\nobject-relational mapping\nsystem for, 382\nweb services and, 424\nquadratic split heuristic, 1189\nquads, 376\nquadtrees, 392, 674, 1186\u20131187\nqueries. See also information\nretrieval\nADO.NET and, 184\nbasic structure of SQL\nqueries, 71\u201379\non B+-trees, 637\u2013641, 690\ncaching and, 435\u2013437\nCartesian product and,\n76\u201379, 81, 230\ncompilation of, 733\ncontinuous, 503, 731\ncorrelated subqueries and,\n101\ncost of ( seequery cost)\ndecision-support, 521, 971\ndeclarative, 1030\u20131031\nde\ufb01ned, 15\ndeletion and, 108\u2013110\nequivalent, 58\nevaluation of ( see\nquery-evaluation plans)\nhash functions and, 624,\n1197\u20131202\nindices and, 623, 695\u2013697,\n707\u2013708\ninsertion and, 110\u2013111\nintermediate SQL and ( see\nintermediate SQL)\nJDBC and, 184\u2013193\njoin expressions and, 125\u2013136\n", "1357": "Index 1329\nkeyword, 383, 385\u2013387, 721\nlanguages ( seequery\nlanguages)\nmetadata and, 191\u2013193\nmonotonic, 218\nmultiple-key access and,\n661\u2013663\non multiple relations, 74\u201379\nnearest-neighbor, 394, 672,\n674\nnested subqueries, 98\u2013107\nnull values and, 89\u201390\nODBC and, 194\u2013197\noptimization of ( seequery\noptimization)\nPageRank and, 385\u2013386\nperformance tuning of,\n1219\u20131223\npoint, 1006, 1190\nprocessing ( seequery\nprocessing)\nprogramming language access\nand, 183\u2013198\nPython and, 193\u2013194\nrange, 638, 672, 674, 1006,\n1190\nread only, 1039\nrecursive, 213\u2013218\nregion, 393\u2013394\nrename operation and, 79,\n81\u201382\nResultSet object and, 185,\n187\u2013188, 191\u2013193,\n638\u2013639\nretrieving results, 187\u2013188\nscalar subqueries, 106\u2013107\nsecurity and, 437\u2013446\nservlets and, 411\u2013421\nset operations and, 85\u201389,\n98\u2013101\non single relation, 71\u201374\nspatial, 393\u2013394\nspatial graph, 394\nstreaming data and, 502\u2013506,\n1070\u20131071\nstring operations and, 82\u201383\ntransaction servers and, 965\nuniversal Turing machines\nand, 16\nviews and, 137\u2013143query cost\noptimization and, 745\u2013746,\n757\u2013766\nprocessing and, 692\u2013695,\n697, 702\u2013704, 710\u2013712,\n715\u2013717\nquery-evaluation engine, 20\nquery-evaluation plans\nchoice of, 766\u2013778\ncost of, 1065\u20131066\nde\ufb01ned, 691\nexpressions and, 724\u2013731\nfault tolerance in, 1059\u20131061\nmaterialization and, 724\u2013725\noptimization and ( seequery\noptimization)\nparallel ( seeparallel query\nplans)\nperformance tuning of,\n1219\u20131220\npipelining and, 691\u2013692,\n724\u2013731\nrelational algebra and,\n690\u2013691\nresource consumption and,\n694\u2013695, 1065\u20131066\nresponse time and, 694\u2013695\nrole in query processing, 689,\n690\nviewing, 746\nquery-execution engine, 691\nquery-execution plans, 691\nquery languages. See also speci\ufb01c\nlanguages\naccessing from programming\nlanguages, 183\u2013198\ncategorization of, 47\nCypher, 509\nde\ufb01ned, 15, 47\nin relational model, 47\u201348\nSPARQL, 375\u2013376\nstream, 503\u2013506\nquery optimization, 743\u2013787\nadaptive, 786\u2013787\naggregation and, 764\nCartesian product and, 748,\n749, 755, 763\u2013764, 775\ncost analysis and, 745\u2013746,\n757\u2013766\nde\ufb01ned, 20, 691, 743distributed, 1084\nequivalence and, 747\u2013757\nestimating statistics of\nexpression results, 757\u2013766\nheuristics in, 766, 771\u2013774,\n786\nhybrid hash join and, 717\u2013718\nindexed nested-loop join and,\n707\u2013708\njoin minimization and, 784\nmaterialized views and,\n778\u2013783\nmultiquery, 785\u2013786\nnested subqueries and,\n774\u2013778, 1220\nparallel databases and,\n1064\u20131070\nparametric, 786\nplan choice for, 766\u2013778\nprojection and, 764\nrelational algebra and,\n743\u2013749, 752, 755\nrole in query processing, 689,\n690\nset operations and, 764\u2013765\nshared scans and, 785\u2013786\ntop-K,784\ntransformations and, 747\u2013757\nupdates and, 784\u2013785\nquery processing, 689\u2013734\nadaptive, 786\u2013787\naggregation and, 723\nbasic steps of, 689, 690\nBig Data and, 470\u2013472\nblockchain databases and,\n1254, 1275\u20131276\ncomparisons and, 698\u2013699\ncost analysis of, 692\u2013695,\n697, 702\u2013704, 710\u2013712,\n715\u2013717\nCPU speeds and, 692\nDDL interpreter in, 20\nde\ufb01ned, 689\ndistributed databases and,\n1076\u20131086\nDML compiler in, 20\nduplicate elimination and,\n719\u2013720\nevaluation of expressions,\n724\u2013731\n", "1358": "1330 Index\n\ufb01le scans and, 695\u2013697,\n704\u2013707, 727\nhashing and, 712\u2013718, 1063\nhistory of, 26\u201327\nidenti\ufb01ers and, 700\nindices and, 695\u2013697\njoin operation and, 704\u2013719,\n722\u2013723, 1081\u20131082\nmaterialization and, 724\u2013725\nin memory, 731\u2013734\noperation evaluation and,\n690\u2013691\nparsing and translation in,\n689\u2013690\npipelining and, 691\u2013692,\n724\u2013731\nPostgreSQL and, 692, 694,\n698\u2013699\nprojection and, 720\nrecursive partitioning and,\n714\u2013715\nrelational algebra and,\n689\u2013691\nscalability of, 471\u2013472\nselection operation and,\n695\u2013700\nset operations and, 720\u2013722\non shared-memory\narchitecture, 1061\u20131064\nsorting and, 701\u2013704\nSQL and, 689\u2013690, 701, 720\nsyntax and, 689\nquery processor, 18, 20\nquery-server systems, 963\nqueueing systems, 1212\u20131213\nqueueing theory, 1213\nquorum consensus protocol,\n1124\u20131125\nRaft protocol, 1148, 1155\u20131158,\n1267\nRAID. Seeredundant arrays of\nindependent disks\nrandom access, 567, 578\nrandomized retry, 1148\nrandom merge, 1056\nrandom samples, 761\nrange partitioning, 476, 1005,\n1007, 1178\nrange-partitioning sort,\n1041\u20131042range-partitioning vector,\n1008\u20131009\nrange queries, 638, 672, 674,\n1006, 1190\nranking, 219\u2013223, 383\u2013386\nraster data, 392\nRDDs (Resilient Distributed\nDatasets), 496\u2013499, 1061\nRDF. SeeResource Description\nFramework\nRDMA (remote direct memory\naccess), 979\nRDNs (relative distinguished\nnames), 1241\u20131242\nreactionary standards, 1237\nReact Native framework, 428\nread-ahead, 578\nread authorization, 14\nread committed, 821, 880, 1225\nread cost, 1127\u20131128\nread one, write all copies\nprotocol, 1125\nread one, write all protocol, 1125\nread only queries, 1039\nread-only transactions, 871\nread phase, 866\nread quorum, 1124\nread uncommitted, 821\nread-write contention,\n1224\u20131225\nready state, 989, 1102\nreal, double precision, 67\nreal-time transaction systems,\n894\nrebuild performance, 576\nrecall, 386\nRecLSN, 942\u2013945\nreconciliation of updates,\n1142\u20131143\nrecon\ufb01guration, 1128\nrecord-based models, 8\nrecord relocation, 652\u2013653\nrecovery manager, 21\nrecovery systems, 907\u2013948\nactions following crashes,\n923\u2013925\nalgorithms for, 922\u2013925,\n944\u2013946, 1276\nARIES, 941\u2013947, 1276\natomicity and, 803, 912\u2013922bu\ufb00er management and,\n926\u2013930\ncheckpoints and, 920\u2013922,\n930\ncommit protocols and, 1105\nconcurrency control and, 916\ndata access and, 910\u2013912\ndatabase modi\ufb01cation and,\n915\u2013916\ndistributed databases and,\n1105\nearly lock release and,\n935\u2013941\nfail-stop assumption and, 908,\n1267\nfailure and, 907\u2013909,\n930\u2013932\nforce/no-force policy and, 927\nlogical undo operations and,\n935\u2013941\nlog records and, 913\u2013919,\n926\u2013927\nlog sequence number and,\n941\u2013946\nmain-memory databases and,\n947\u2013948\nredo operation and, 915\u2013919,\n922\u2013925\nremote backup, 909, 931\u2013935\nrollback and, 916\u2013919, 922\nshadow-copy scheme and, 914\nsnapshot isolation and, 916\nsteal/no-steal policy and, 927\nstorage and, 908\u2013912,\n920\u2013922, 930\u2013931\nsuccessful completion and,\n909\ntransactions and, 21, 803, 805\ntriggers and, 212\u2013213\nundo operation and, 915\u2013919,\n922\u2013925\nwrite-ahead logging rule and,\n926\u2013929\nrecovery time, 933\nrecursive partitioning, 714\u2013715\nrecursive queries, 213\u2013218\niteration and, 214\u2013216\nSQL and, 216\u2013218\ntransitive closure and,\n214\u2013216\n", "1359": "Index 1331\nrecursive relationship sets,\n247\u2013248\nRedis, 436\u2013437, 482, 1024\nredistribution of pointers, 646\nredo-only log records, 918\nredo operation, 915\u2013919,\n922\u2013925, 941\nredo pass, 944\u2013945\nredo phase, 923, 924\nreduceByKey function, 498\nreduce function, 483\u2013494, 510.\nSee also MapReduce\nparadigm\nreduce key, 485\nredundancy\nin database design, 243\nentity-relationship (E-R)\nmodel and, 261\u2013264\nin \ufb01le-processing systems, 6\nreliability improvement via,\n570\u2013571\nof schemas, 269\u2013270\nredundant arrays of independent\ndisks (RAID), 570\u2013577\nbit-level striping and, 571\u2013572\nblock-level striping and, 572\nhardware issues, 574\u2013576\nhot swapping and, 575\nlevels, 572\u2013574, 572n4, 573n6,\n576\u2013577\nmirroring and, 571\u2013573, 576,\n577\nparity bits and, 572, 574, 577\nperformance improvement via\nparallelism, 571\u2013572\nperformance tuning and,\n1214, 1229\u20131230\npurpose of, 562\nrebuild performance of, 576\nrecovery systems and, 909\nreliability improvement via\nredundancy, 570\u2013571\nscrubbing and, 575\nsoftware RAID, 574, 575\nstriping data and, 571\u2013572\nre-engineering, 1236\nreferenced relation, 45\u201346\nreferences, 149\u2013150\nreferences privilege, 170\nreference types, 380\u2013381referencing new row as clause,\n207, 208\nreferencing new table as clause,\n210\nreferencing old row as clause,\n208\nreferencing old table as clause,\n210\nreferencing relation, 45\u201346\nreferential integrity, 14, 46,\n149\u2013153, 207\u2013208, 800\nreferer, 440\nreferrals, 1243\nref from clause, 380\nre\ufb02exivity rule, 321\nregion quadtrees, 1187\nregion queries, 393\u2013394\nregression, 546, 1234\nrei\ufb01cation, 376\nreintegration, 1128\nrelation, de\ufb01ned, 39\nrelational algebra, 48\u201358\nantijoin operation, 108\nassignment operation, 55\u201356\nBig Data and, 494\u2013500\nCartesian-product operation,\n50\u201352\nequivalence and, 58, 747\u2013757\nexpression transformation\nand, 747\u2013757\njoin operation, 52\u201353\nmotivation for, 495\u2013496\nmultiset, 80, 97, 108, 136, 747\nprede\ufb01ned functions\nsupported by, 48n4\nproject operation, 49\u201350\nquery optimization and,\n743\u2013749, 752, 755\nquery processing and,\n689\u2013691\nrename operation, 56\u201357\nselect operation, 49\nsemijoin operation, 108,\n1082\u20131084\nset operations, 53\u201355\nin Spark, 495\u2013500, 508\nSQL operations and, 80\non streams, 504, 506\u2013508\nunary vs. binary operations\nin, 48relational-algebra expressions, 50\nrelational database design,\n303\u2013351\natomic domains and, 342\u2013343\nBoyce\u2013Codd normal form\nand, 313\u2013316\nclosure of a set and, 312,\n320\u2013324\ndecomposition and, 305\u2013313,\n330\u2013341\ndesign process, 343\u2013347\nfeatures of good designs,\n303\u2013308\n\ufb01rst normal form and,\n342\u2013343\nfourth normal form and, 336,\n339\u2013341\nfunctional dependencies and,\n308\u2013313, 320\u2013330\nlarger schemas and, 330, 346\nmultivalued dependencies\nand, 336\u2013341\nnaming of attributes and\nrelationships in, 345\u2013346\nnormalization and, 308\nsecond normal form and,\n316n8, 341\u2013342, 356\nsmaller schemas and, 305,\n308, 344\ntemporal data modeling and,\n347\u2013351\nthird normal form and,\n317\u2013319\nrelational model, 37\u201359\nconceptual-design process for,\n17\ndisadvantages of, 26\nhistory of, 26\nkeys for, 43\u201346\nobject-relational, 27, 376\u2013382\noperations in, 48\u201358\noverview, 8, 37\nquery languages in, 47\u201348\nschema diagrams for, 46\u201347\nschema in ( seerelational\nschema)\nSQL language in, 13\nstructure of, 37\u201340\ntables in, 9, 10, 37\u201340\ntuples in, 39, 41, 43\u201346\n", "1360": "1332 Index\nrelational OLAP (ROLAP), 535\nrelational schema\nBoyce\u2013Codd normal form\nand, 313\u2013316, 330\u2013333\ncanonical cover and, 324\u2013328\ndatabase design process and,\n343\u2013347\ndecomposition and, 305\u2013313\nde\ufb01ned, 41\nin \ufb01rst normal form, 342\u2013343\nfourth normal form and,\n339\u2013341\nfunctional dependencies and,\n308\u2013313, 320\u2013330\nhorizontal partitioning of,\n1216\u20131217\nlogically implied, 320\nmultivalued dependencies\nand, 336\u2013341\nreduction of\nentity-relationship\ndiagrams to, 264\u2013271,\n277\u2013279\nredundancy in, 243\ntemporal data and, 347\u2013351\nthird normal form and,\n317\u2013319, 333\u2013335\nfor university databases,\n41\u201343, 303\u2013305\nrelation instance, 39, 41\nrelation scans, 769\nrelationship instance, 246\u2013247\nrelationships, de\ufb01ned, 246\nrelationship sets\nalternative notations for,\n285\u2013291\natomic domains and, 342\u2013343\nbinary, 249, 283\u2013285\ncombination of schemas and,\n270\u2013271\ndegree of, 249\ndescriptive attributes and, 248\ndesign issues and, 282\u2013285\nentity-relationship diagrams\nand, 247\u2013250, 268\u2013271\nentity-relationship (E-R)\nmodel and, 246\u2013249,\n282\u2013285\nmapping cardinalities and,\n252\u2013256naming of, 345\u2013346\nnonbinary, 283\u2013285\nparticipation in, 247\nprimary keys and, 257\u2013259\nrecursive, 247\u2013248\nredundancy and, 269\u2013270\nrepresentation of, 268\u2013269\nternary, 249, 250, 284\nUni\ufb01ed Modeling Language\nand, 288\u2013291\nrelations (tables), 8\nrelative distinguished names\n(RDNs), 1241\u20131242\nrelevance\nhyperlinks and, 385\u2013386\nPageRank and, 385\u2013386\nTF-IDF approach and,\n384\u2013385\nrelevance ranking, 383\u2013386\nreliability, improvement via\nredundancy, 570\u2013571\nremapping bad sectors, 565\nremote backup systems, 909,\n931\u2013935\nremote direct memory access\n(RDMA), 979\nrename operation, 56\u201357, 79,\n81\u201382\nrenewing leases, 1115\nreorganization of \ufb01les, 598\nrepeatable read, 821\nrepeating history, 924\nrepeat loop, 214\u2013216, 323\nrepeat statements, 201\nreplication\nasynchronous, 1122,\n1135\u20131138\nbiased protocol and, 1124\nBig Data and, 481\u2013482\ncaching, 1014n4\nchain replication protocol,\n1127\u20131128\nconcurrency control and,\n1123\u20131125\nconsistency and, 1015\u20131016,\n1121\u20131123, 1133\u20131146\ndata centers and, 1014\u20131015\ndistributed databases and,\n987, 1121\u20131128\nfailure and, 1125\u20131128key-value storage systems and,\n476\nlocation of, 1014\u20131015\nmajority protocol and,\n1123\u20131126\nmaster replica, 1016\nmaster-slave, 1137\nmultimaster, 1137\nparallel databases and,\n1013\u20131016\nprimary copy, 1123\nquorum consensus protocol\nand, 1124\u20131125\nrecon\ufb01guration and\nreintegration, 1128\nsharding and, 476\nstate machines and,\n1158\u20131161\nsynchronous, 522\u2013523, 1136\ntwo-phase commit protocol\nand, 1016\nupdate-anywhere, 1137\nupdates and, 1015\u20131016\nview maintenance and,\n1138\u20131140\nreport generators, 538\u2013540\nRepresentation State Transfer\n(REST), 426\u2013427\nrequest forgery, 439\u2013440\nrequest operation\ndeadlock handling and,\n849\u2013853\nlocks and, 835\u2013841, 844\u2013846,\n849\u2013853, 886\nmultiple granularity and, 854,\n855\nmultiversion schemes and,\n871\nsnapshot isolation and, 874\ntimestamps and, 861\nResilient Distributed Datasets\n(RDDs), 496\u2013499, 1061\nresolution of con\ufb02icting updates,\n1142\u20131143\nresource consumption, 694\u2013695,\n1065\u20131066\nResource Description Framework\n(RDF)\nde\ufb01ned, 372\n", "1361": "Index 1333\ngraph representation of,\n374\u2013375\nn-ary relationships and, 376\noverview, 368\nrei\ufb01cation in, 376\nSPARQL and, 375\u2013376\ntriple representation and,\n372\u2013374\nresources, in RDF, 372\nresponse time\napplication design and,\n434\u2013435, 1229, 1232\nblockchain databases and,\n1274\nparallel databases and,\n971\u2013972\npartitioning and, 1007\nquery-evaluation plans and,\n694\u2013695\nquery processing and,\n694\u2013695\nskew and, 1066\nstorage and, 572\ntransactions and, 808\u2013809\nresponse time cost model, 1066\nresponsive user interfaces, 423\nREST (Representation State\nTransfer), 426\u2013427\nrestriction, 172, 328, 339\u2013340\nResultSet object, 185, 187\u2013188,\n191\u2013193, 638\u2013639\nresynchronization, 575\nreverse engineering, 1236\nrevoke privileges, 166\u2013167,\n171\u2013173\nright outer join, 132\u2013136, 722\nrigorous two-phase locking\nprotocol, 842, 843\nRijndael algorithm, 448\nring system, 976\nrobustness, 1121, 1125\u20131126\nROLAP (relational OLAP), 535\nroles\nauthorization and, 167\u2013169\nentity, 247\u2013248\nindicators associated with,\n268\nUni\ufb01ed Modeling Language\nand, 289\nrollbackARIES and, 945\u2013946\ncascading, 820\u2013821, 841\u2013842\nconcurrency control and,\n841\u2013844, 849\u2013850, 853,\n868\u2013871\nlogical operations and,\n937\u2013939\npartial, 853\nrecovery systems and,\n916\u2013919, 922\nremote backup systems and,\n933\ntimestamps and, 862\u2013865\ntotal, 853\ntransactions and, 143\u2013145,\n193, 196, 805\u2013806, 922,\n937\u2013939, 945\u2013946\nundo operation and, 916\u2013919,\n937\u2013939\nrollback work, 143\u2013145\nrolling merge, 1178\nrollup clause, 228\nrollup construct, 227\u2013231,\n530\u2013531, 536\u2013538\nrotational latency time, 566\nround-robin scheme, 1005, 1006\nrouters, 1012\u20131013\nrow-level authorization, 173\nrow-oriented storage, 611, 615\nrow stores, 612, 615\nR-timestamp, 862, 865, 870\nR-trees, 663, 670, 674\u2013676,\n1187\u20131190\nRuby on Rails, 417, 419\nrules for data mining, 540\nruns, 701\u2013702\nrunstats, 761\nSAML (Security Assertion\nMarkup Language),\n442\u2013443\nSAN. Seestorage area network\nsandbox, 205\nSAP HANA, 615, 1132\nSAS (Serial Attached SCSI)\ninterface, 562\nSATA (Serial ATA) interface,\n562, 568, 569\nsavepoints, 947\nscalability, 471\u2013472, 477, 482,\n1276scalar subqueries, 106\u2013107\nscaleup, 972\u2013974\nscanning a relation, 1006\nscheduling\ndisk-arm, 578\u2013579\nquery optimization and, 1065\ntransactions and, 810\u2013811,\n811n1\nschema diagrams, 46\u201347\nschemas\nalternative notations for\nmodeling data, 285\u2013291\nauthorization on, 170\nbasic SQL query structures\nand, 71\u201379\ncombination of, 270\u2013271\ncomposite attributes in, 250\nconcurrency control and ( see\nconcurrency control)\ncreation of, 24\ndata mining, 540\u2013549\ndata warehousing, 523\u2013525\nde\ufb01ned, 12\nentity-relationship diagrams\nand, 264\u2013271, 277\u2013279\nentity-relationship (E-R)\nmodel and, 244, 246,\n269\u2013270, 277\u2013279\nevolution of, 292\n\ufb02exibility of, 366\nglobal, 1076, 1078\u20131079\nintegration of, 1076,\n1078\u20131080\nintermediate SQL and,\n162\u2013163\nlarger, 330, 346\nlocal, 1076\nlocks and ( seelocks)\nlogical, 12\u201313, 1223\u20131224\nperformance tuning of,\n1214\u20131218, 1223\u20131224\nphysical, 12, 13\nphysical-organization\nmodi\ufb01cation of, 25\nP + Q redundancy, 573, 574\nrecovery systems and ( see\nrecovery systems)\nredundancy of, 269\u2013270\nrelational ( seerelational\nschema)\n", "1362": "1334 Index\nrelationship sets and,\n268\u2013269\nshadow-copy, 914\nsmaller, 305, 308, 344\nsnow\ufb02ake, 524\nSQL DDL and, 24, 66, 68\u201371\nstar, 524\u2013525\nstrong entity sets and,\n265\u2013267\nsubschemas, 12\ntimestamps and, 861\u2013866\nfor university databases,\n1287\u20131288\nversion-numbering, 1141\nversion-vector, 1141\u20131142\nweak entity sets and, 267\u2013268\nSciDB, 367\nSCM (storage class memory),\n569, 588, 948\nSCOPE, 494\nscope clause, 380\nscripting languages, 404\u2013405,\n416\u2013418, 421, 439\nscrubbing, 575\nSCSI (small-computer-system\ninterconnect), 562, 563\nsearch keys\nB+-trees and, 634\u2013650\nhashing and, 1190\u20131195,\n1197\u20131199\nindex creation and, 664\nnonunique, 632, 637, 640,\n649\u2013650\nordered indices and, 624\u2013634\nstorage and, 595, 597\u2013598\nuniqui\ufb01ers and, 649\u2013650\nsearch operation, 1188\nsecondary indices, 625,\n632\u2013633, 652\u2013653,\n695\u2013698,\n1017\u20131019\nsecondary site, 931\nsecondary storage, 561\nsecond normal form (2NF),\n316n8, 341\u2013342, 356\nsectors, 564\u2013566\nsecurity\nabstraction levels and, 12\napplication design and,\n437\u2013446audit trails and, 445\u2013446\nauthentication ( see\nauthentication)\nauthorization ( see\nauthorization)\nof blockchain databases,\n1253\u20131255, 1259\nconcurrency control and ( see\nconcurrency control)\ncross-site scripting and,\n439\u2013440\ndictionary attacks and, 449\nencryption and, 447\u2013453\nend-user information and, 443\nin \ufb01le-processing systems, 7\nGET method and, 440\nintegrity manager and, 19\nlocks and ( seelocks)\nman-in-the-middle attacks\nand, 442\npasswords ( seepasswords)\nprivacy and, 438, 446\nrequest forgery and, 439\u2013440\nsingle sign-on system and,\n442\u2013443\nSQL DDL and, 67\nSQL injection and, 438\u2013439\nunique identi\ufb01cation and,\n446, 446n8\nSecurity Assertion Markup\nLanguage (SAML),\n442\u2013443\nseek time, 566, 566n2, 567, 692,\n710\nselect all, 73\nselect authorization, privileges\nand, 166\u2013167\nselect clause\naggregate functions and,\n91\u201396\nattribute speci\ufb01cation in, 83\nbasic SQL queries and, 71\u201379\non multiple relations, 74\u201379\nin multiset relational algebra,\n97\nnull values and, 90\nOLAP and, 534, 536\u2013537\nranking and, 220\u2013222\nrename operation and, 79,\n81\u201382set membership and, 98\u201399\nset operations and, 85\u201389\non single relation, 71\u201374\nstring operations and, 82\u201383\nselect distinct, 72\u201373, 90,\n99\u2013100, 142\nselect-from-where\ndeletion and, 108\u2013110\nfunction/procedure writing\nand, 199\u2013206\ninsertion and, 110\u2013111\njoin expressions and, 125\u2013136\nnatural joins and, 126\u2013130\nnested subqueries and,\n98\u2013107\nupdates and, 111\u2013114\nviews and, 137\u2013143\nselection\ncomparisons and, 698\u2013699\ncomplex, 699\u2013700\nconjunctive, 699\u2013700, 747,\n762\ndisjunctive, 699, 700, 762\nequivalence and, 747\u2013757\n\ufb01le scans and, 695\u2013697,\n704\u2013707, 727\nidenti\ufb01ers and, 700\nindices and, 695\u2013697, 783\nintraoperation parallelism\nand, 1049\nlinear search and, 695\nsize estimation and, 760, 762\nview maintenance and,\n780\u2013781\nselectivity, 762\nselect operation, 49\nselect privilege, 171, 172\nsemijoin operation, 108, 776,\n1082\u20131084\nsemi-structured data models,\n365\u2013376\n\ufb02exible schema and, 366\nhistory of, 8, 27\nJSON, 8, 27, 367\u2013370\nknowledge representation\nand, 368\nmotivations for use of,\n365\u2013366\nmultivalued, 366\u2013367\nnested, 367\u2013368\n", "1363": "Index 1335\noverview, 366\u2013368\nRDF and knowledge graphs,\n368, 372\u2013376\nXML, 8, 27, 367\u2013368,\n370\u2013372\nsensor data, 470, 501\nsentiment analysis, 549\nSequel, 65\nsequence counters, 1226\nsequential-access storage, 561,\n567, 578\nsequential computation, 974\nsequential \ufb01le organization, 595,\n597\u2013598\nSerial ATA (SATA) interface,\n562, 568, 569\nSerial Attached SCSI (SAS)\ninterface, 562\nserializability\nblind writes and, 868\nconcurrency control and,\n836, 840\u2013843, 846\u2013848,\n856, 861\u2013871,\n875\u2013887\ncon\ufb02ict, 813\u2013816\nisolation and, 821\u2013826\noperation, 885\norder of, 817\nperformance tuning and, 1225\nprecedence graph and,\n816\u2013817\nin the real world, 824\nsnapshot isolation and,\n875\u2013879\ntopological sorting and,\n817\u2013818\ntransactions and, 812\u2013819,\n821\u2013826\nview, 818\u2013819, 867\u2013868\nserializable schedules, 811, 812\nserializable snapshot isolation\n(SSI) protocol, 878\nserver-side scripting, 416\u2013418\nserver systems, 962\u2013970\ncategorization of, 963\u2013964\ndata-server, 963\u2013964,\n968\u2013970\nde\ufb01ned, 962\ntransaction-server, 963\u2013968\ntree-like, 977\u2013978services, 994\nservice time, 1230\nservlets, 411\u2013421\nalternative server-side\nframeworks, 416\u2013421\nexample of, 411\u2013413\nlife cycle and, 415\u2013416\nserver-side scripting and,\n416\u2013418\nsessions and, 413\u2013415\nsupport for, 416\nweb application frameworks\nand, 418\u2013419\nsession window, 505\nset autocommit o\ufb00, 144\nset clause, 113\nset default, 150\nset-di\ufb00erence operation, 55, 750\nset null, 150\nset operations\nexcept, 88\u201389\nintersect, 54\u201355, 87\u201388, 750\nnested subqueries and,\n98\u2013101\nquery optimization and,\n764\u2013765\nquery processing and,\n720\u2013722\nset comparison and, 99\u2013101\nset di\ufb00erence, 55\nunion, 53\u201354, 86\u201387, 750\nset orientation, 1220\u20131221\nset role, 172\ns e ts t a t e m e n t ,2 0 1 ,2 0 9\nset transaction isolation level\nserializable, 822\nset types, 366, 367\nshadow-copy scheme, 914\nshadowing, 571\nshadow paging, 914\nSHA-256 hash function, 1260\nsharding, 473, 475\u2013476, 1275\nshard key, 479\nshared and intention-exclusive\n(SIX) mode, 855\nshared-disk architecture, 979,\n980, 984\u2013985\nshared locks, 825, 835, 854,\n880, 1124shared-memory architecture,\n21\u201322, 979\u2013984,\n1061\u20131064\nshared-mode locks, 835\nshared-nothing architecture, 979,\n980, 985\u2013986, 1040\u20131041,\n1061\u20131063\nshared scans, 785\u2013786\nSherpa/PNUTS, 477, 1024,\n1026, 1028\u20131030\nshow warnings, 746\nshrinking phase, 841, 843\nshu\ufb04e step, 486, 1061\nSIMD (Single Instruction\nMultiple Data), 1064\nsimple attributes, 250, 265\nsimulation model, 1230\nsingle inheritance, 275\nSingle Instruction Multiple Data\n(SIMD), 1064\nsingle lock-manager, 1111\nsingle sign-on system, 442\u2013443\nsingle-user systems, 962\nsingle-valued attributes, 251\nsites, 986\nSIX (shared and\nintention-exclusive) mode,\n855\nskew\naggregation and, 1049\u20131050\nattribute-value, 1008\ndata distribution, 1008\nin distribution of records, 660\nexecution, 1007, 1008, 1043\nhash indices and, 1194\nin joins, 1047\u20131048\nparallel databases and, 974,\n1007\u20131013, 1043, 1062\npartitioning and, 715,\n1007\u20131013\nresponse time and, 1066\nwrite skew, 876\u2013877\nslicing, 530\nsliding window, 505\nslotted-page structure, 593\nsmall-computer-system\ninterconnect (SCSI), 562,\n563\nsmart cards, 451, 451n9\n", "1364": "1336 Index\nsmart contracts, 1258,\n1269\u20131273\nsnapshot isolation\nconcurrency control and,\n872\u2013879, 882, 916,\n1131\u20131132\ndistributed, 1131\u20131132\nperformance tuning and, 1225\nrecovery systems and, 916\nserializability and, 875\u2013879\ntransactions and, 825\u2013826,\n1136\nvalidation and, 874\u2013875\nsnow\ufb02ake schema, 524\nsocial-networking sites\nB i gD a t aa n d ,4 6 7 ,4 7 0\ndatabase applications for, 2,\n3, 27\nkey-value store and, 471\nstreaming data and, 502\nsoft deadlines, 894\nsoft forks, 1257, 1258\nsoftware-as-a-service model, 993\nsoftware RAID, 574, 575\nsolid-state drives (SSDs), 18,\n560, 568\u2013570, 693n3,\n1229\nsome construct, 100, 100n10\nsome function, 96\nsophisticated users, 24\nsorting\ncost analysis of, 702\u2013704\nduplicate elimination and,\n719\u2013720\nexternal sort-merge algorithm,\n701\u2013704\nparallel external sort-merge,\n1042\u20131043\nquery processing and,\n701\u2013704\nrange-partitioning, 1041\u20131042\ntopological, 817\u2013818\nsort-merge-join algorithm,\n708\u2013712\nsound axioms, 321\nsource-driven architecture, 522\nspace overhead, 624, 627\u2013628,\n634, 1202\nSpark, 495\u2013500, 508, 511, 1061\nSPARQL, 375\u2013376sparse column data\nrepresentation, 366\nsparse indices, 626\u2013632\nspatial data\ndesign databases, 390\u2013391\ngeographic, 387, 390\u2013393\ngeometric, 388\u2013390\nindexing of, 672\u2013675,\n1186\u20131190\njoins over, 719\nquadtrees, 392, 674,\n1186\u20131187\nqueries and, 393\u2013394\nR-trees, 663, 670, 674\u2013676,\n1187\u20131190\ntriangulation and, 388, 393\nspatial data indices, 672\u2013675\nspatial graph queries, 394\nspatial graphs, 390\nspatial-integrity constraints, 391\nspatial join, 394\nspatial networks, 390\nspecialization\nattribute inheritance and,\n274\u2013275\nconstraints on, 275\u2013276\ndisjoint, 272, 275\nentity-relationship (E-R)\nmodel and, 271\u2013273\noverlapping, 272, 275\npartial, 275\nsingle entity set and, 274\nsuperclass-subclass\nrelationship and, 272\ntotal, 275\nspeci\ufb01cation of functional\nrequirements, 17\u201318, 242\nspeedup, 972\u2013974\nsplitting nodes, 641\u2013645, 886\nSQL Access Group, 1238\nSQLAlchemy, 382\nSQL/DS, 26\nSQL environment, 163\nSQL injection, 189, 438\u2013439\nSQLite, 668\nSQLLoader, 1222\nSQL MED, 1077\nsql security invoker, 170\nsqlstate, 205SQL (Structured Query\nLanguage), 65\u2013114\nadvanced ( seeadvanced SQL)\naggregate functions and,\n91\u201396\napplication-level\nauthorization and,\n443\u2013445\napplication programs and,\n16\u201317\nattribute speci\ufb01cation in\nselect clause, 83\nauthorization and, 66\nbasic types supported by,\n67\u201368\nblobs and, 156, 193, 594, 652\nbulk loads and, 1221\u20131223\nclobs and, 156, 193, 594, 652\ncreate table and, 68\u201371\ndatabase modi\ufb01cation and,\n108\u2013114\ndata de\ufb01nition for university\ndatabases, 69\u201371,\n1288\u20131292\nDDL and, 14\u201315, 65\u201371\ndecision-support systems and,\n521\ndeletion and, 108\u2013110\nDML and, 16, 66\ndumping and, 931\ndynamic, 66, 184, 201\nembedded, 66, 184, 197\u2013198,\n965, 1269\nindex creation and, 164\u2013165,\n664\u2013665\ninjection and, 189, 438\u2013439\ninputs and outputs in, 747\ninsertion and, 110\u2013111\nintegrity constraints and,\n14\u201315, 66, 145\u2013153\nintermediate ( see\nintermediate SQL)\nisolation levels and, 821\u2013826\nJSON and, 369\u2013370\nlimitations of, 468, 472\non MapReduce, 493\u2013494\nMySQL ( seeMySQL)\nnested subqueries and,\n98\u2013107\nnonstandard syntax and, 204\n", "1365": "Index 1337\nNoSQL systems, 28, 473, 477,\n1269, 1276\nnull values and, 89\u201390\nOLAP in, 533\u2013534, 536\u2013538\nordering display of tuples\nand, 83\u201384\noverview, 65\u201366\nPostgreSQL ( see\nPostgreSQL)\nprepared statements and,\n188\u2013190\nprevalence of use, 13\nquery processing and,\n689\u2013690, 701, 720\nquery structure, 71\u201379\nrelational algebra and, 80\nrename operation and, 79,\n81\u201382\nResultSet object and, 185,\n187\u2013188, 191\u2013193,\n638\u2013639\nschemas and, 24, 66, 68\u201371\nsecurity and, 438\u2013439\nset operations and, 85\u201389\nstandards for, 65, 1237\u20131238\nstream extensions to,\n504\u2013506\nstring operations and, 82\u201383\nSystem R and, 26\nSystem R project and, 65\ntheoretical basis of, 48\ntransactions and, 66,\n143\u2013145, 965\nupdates and, 111\u2013114\nviews and, 66, 137\u2013143,\n169\u2013170\nwhere-clause predicates and,\n84\u201385\nXML and, 372\nSSDs. Seesolid-state drives\nSSI (serializable snapshot\nisolation) protocol, 878\nstable storage, 804\u2013805,\n908\u2013910\nstale messages, 1149\nstalls in processing, 733\nstandards\nANSI, 65, 1237\nanticipatory, 1237\nCLI, 197, 1238\u20131239database connectivity,\n1238\u20131239\nde facto, 1237\nde\ufb01ned, 1237\nformal, 1237\nISO, 65, 1237, 1241\nobject-oriented, 1239\u20131240\nODBC, 1238\u20131239\nreactionary, 1237\nSQL, 65, 1237\u20131238\nX/Open XA, 1239\nstar schema, 524\u2013525\nstart-up costs, 974, 1066\nstart with/connect by prior\nsyntax, 218\nstarved transactions, 841, 853\nstate-based blockchains, 1269,\n1271\nstate machines, 1158\u20131161\nStatement object, 186\u2013187, 189\nstate of execution, 727\nstatic hashing, 661, 1190\u20131195,\n1202\u20131203\nstatistical analysis, 520, 527\nstatistics, 757\u2013766\ncatalog information and,\n758\u2013760\ncomputing, 761\njoin size estimation and,\n762\u2013764\nmaintaining, 761\nnumber of distinct values and,\n765\u2013766\nselection size estimation and,\n760, 762\nsteal policy, 927\nstepped-merge indices, 667,\n1179\u20131181\nstock market, streaming data\nand, 501\nstop words, 385\nstorage, 587\u2013617\naccess time and, 561, 566,\n567, 578\narchitecture for, 587\u2013588\narchival, 561\natomicity and, 804\u2013805\nauthorization and, 19\nbackup ( seebackup)\nBig Data and, 472\u2013482, 668bit-level striping and, 571\u2013572\nblockchain ( seeblockchain\ndatabases)\nblock-level striping and, 572\nbu\ufb00ers and, 19, 604\u2013610\nbyte amounts of, 18\ncheckpoints and, 920\u2013922,\n930\ncloud-based, 28, 563,\n992\u2013993\ncolumn-oriented, 525\u2013526,\n588, 611\u2013617, 734, 1182\ncrashes and, 607, 609\u2013610\ndata access and, 910\u2013912\ndata-dictionary, 602\u2013604\ndata mining and, 27, 540\u2013549\ndata-transfer rate and, 566,\n569\nin decision-support systems,\n519\u2013520\ndirect-access, 561\ndistributed ( seedistributed\ndatabases)\ndistributed \ufb01le systems for,\n472\u2013475, 489, 1003,\n1019\u20131022\ndumping and, 930\u2013931\ndurability and, 804\u2013805\nelasticity of, 1010\n\ufb01le manager for, 19\n\ufb01le organization and,\n588\u2013602\nforce output and, 912\ngeographically distributed,\n1026\u20131027\nhard disks for, 26\nintegrity manager and, 19\nkey-value, 471, 473, 476\u2013480,\n1003, 1023\u20131031\nof large objects, 594\u2013595\nlog disks, 610\nin main-memory databases,\n588, 615\u2013617\nmirroring and, 571\u2013573, 576,\n577\nnon-volatile, 560, 562,\n587\u2013588, 804, 908\u2013910,\n930\u2013931\no\ufb04ine, 561\nonline, 561\n", "1366": "1338 Index\noutsourcing, 28\nparallel ( seeparallel\ndatabases)\nphysical ( seephysical storage\nsystems)\npointers and, 588, 591,\n594\u2013598, 601\nprimary, 561\npunched cards for, 25\nrandom access, 567, 578\nrecovery systems and,\n908\u2013912, 920\u2013922,\n930\u2013931\nredundant arrays of\nindependent disks, 562\nresponse time and, 572\nrow-oriented, 611, 615\nR-trees and, 1189\u20131190\nscrubbing and, 575\nsecondary, 561\nseek time and, 566, 566n2,\n567, 692, 710\nsequential access, 561, 567,\n578\nsharding and, 473, 475\u2013476\nSQL DDL and, 67\nstable, 804\u2013805, 908\u2013910\nstriping data and, 571\u2013572\nstructure and access-method\nde\ufb01nition, 24\ntertiary, 561\ntransaction manager for, 19\nvolatile, 560, 562, 804, 908\nwallets and, 450\nwarehousing ( seedata\nwarehousing)\nstorage area network (SAN),\n562, 563, 570, 934, 985\nstorage class memory (SCM),\n569, 588, 948\nstorage manager, 18\u201320\nstore barrier, 983\nstored functions/procedures,\n1031\nstraggler nodes, 1060\u20131061\nstreaming data, 500\u2013508\nalgebraic operations and, 504,\n506\u2013508\napplications of, 500\u2013502\ncontinuous, 731de\ufb01ned, 500\nfault tolerance with,\n1074\u20131076\nprocessing, 468, 1070\u20131076\nquerying, 502\u2013506,\n1070\u20131071\nrouting of tuples and,\n1071\u20131073\nstream query languages,\n503\u2013506\nstrict two-phase locking protocol,\n842, 843\nstring operations\naggregate, 91\nescape, 83\nJDBC and, 184\u2013193\nlike, 82\u201383\nlower function, 82\nquery result retrieval and, 188\nsimilar to, 83\ntrim, 82\nupper function, 82\nstripe, 613\u2013614\nstriping data, 571\u2013572\nstrong entity sets, 259, 265\u2013267\nStructured Query Language. See\nSQL\nstructured types, 158\u2013160\nstylesheets, 408\nsubject of triples, 372\nsublinear scaleup, 973\nsublinear speedup, 972\nsubschemas, 12\nsu\ufb03x, 1243\nsum function, 91, 139, 228, 536,\n723, 766, 781\nsuperclass-subclass relationship,\n272, 274\nsuperkeys, 43\u201344, 257\u2013258,\n309\u2013310, 312\nsupersteps, 510\nsuperusers, 166\nsupply chains, 1266, 1278\nsupport, 547\nSupport Vector Machine (SVM),\n544\u2013545\nswap space, 929\nSybase IQ, 615\nSybil attacks, 1255, 1256, 1264,\n1266symmetric fragment-and-replicate\njoins, 1046\nsymmetric-key encryption, 448\nsynchronous replication,\n522\u2013523, 1136\nsyntax, 199, 201\u2013205, 689\nsys\ncontext function, 173\nsystem architecture. See\narchitecture\nsystem catalogs, 602\u2013604, 1009\nsystem clock, 862\nsystem error, 907\nSystem R, 26, 65, 772\u2013773,\n772n3\ntable alias, 81\ntable functions, 200\ntable inheritance, 379\u2013380\ntable partitioning, 601\u2013602\ntables\nde\ufb01ned, 1011\ndimension, 524\ndirty page, 941\u2013947\ndistributed hash, 1013\nfact, 524\nforeign, 1077\npartition, 1011\u20131014\npivot-table, 226\u2013227,\n528\u2013529\nin relational model, 9, 10,\n37\u201340\nin SQL DDL, 14\u201315\ntransition, 210\ntablets, 1011, 1025\ntablet server, 1025\ntab-separated values, 1222\ntag library, 418\ntags, 370\u2013372, 406\u2013407, 418,\n440\ntamper resistance, 1253\u20131255,\n1259, 1260\ntangles, 1278\ntape storage, 561\nTapestry, 419\ntasks, 1051\u20131052. See also\nwork\ufb02ow\nTcl, 206\ntelecommunications, database\napplications for, 3\ntemporal data, 347\u2013351, 347n10\n", "1367": "Index 1339\ntemporal data indices, 675\u2013676\ntemporal validity, 157\nterm frequency (TF), 384\ntermination of transactions, 806\nterms, 384, 1149\nternary relationship sets, 249,\n250, 284\ntertiary storage, 561\ntest-and-set, 966\ntest suites, 1234\u20131235\ntext mining, 549\ntextual data, 382\u2013387. See also\ninformation retrieval\nkeyword queries, 383,\n386\u2013387\noverview, 382\nrelevance ranking and,\n383\u2013386\nTez, 495\nTF-IDF approach, 384\u2013385\nTF (term frequency), 384\nthen clause, 212\ntheta-join operations, 748\u2013749\nthird normal form (3NF),\n317\u2013319, 333\u2013335\nThomas\u2019 write rule, 864\u2013866\nthreads, 965, 982, 1062\n3D-XPoint memory technology,\n569\n3NF decomposition algorithm,\n334\u2013335\n3NF synthesis algorithm, 335\n3NF (third normal form),\n317\u2013319, 333\u2013335\nthree-phase commit (3PC)\nprotocol, 1107\nthree-tier architecture, 23\nthroughput\napplication design and,\n1230\u20131232, 1234\nin blockchain databases, 1274\nharmonic mean of, 1231\nimproved, 808\nparallel databases and, 971\nrange partitioning and, 1007\nstorage and, 572\nsystem architectures and, 963,\n971\ntransactions and, 808\nthroughput test, 1234tickets and ticketing, 1133, 1279\ntiles, 392\ntime intervals, 675\u2013676\ntime-lock transactions, 1273\ntimestamps\nconcurrency control and,\n861\u2013866, 882\nfor data-storage systems, 480\nde\ufb01ned, 154\ndistributed databases and,\n1116\u20131118\ngeneration of, 1117\u20131118\ninvalidation, 873n3\nlogical clock, 1118\nlogical counter and, 862\nmultiversion schemes and,\n870\u2013871\nnondeterministic, 508n3\nordering scheme and,\n862\u2013864, 870\u2013871, 1118\nrollback and, 862\u2013865\nsnapshot isolation and, 873,\n873n2\nsystem clock and, 862\nThomas\u2019 write rule and,\n864\u2013866\ntransactions and, 825\ntuples and, 502, 503, 505\ntime to completion, 1231\ntimezone, 154\nTIN (triangulated irregular\nnetwork), 393\ntokens, 1272\nTomcat Server, 416\ntop-down design, 273\ntopic-partition system, 1073\ntop- Koptimization, 784\ntopographical information, 393\ntopological sorting, 817\u2013818\ntoss-immediate strategy, 608\ntotal failure, 909\ntotal generalization, 275\ntotal rollback, 853\ntotal specialization, 275\nTPC (Transaction Processing\nPerformance Council),\n1232\u20131234\nTPS (transactions per second),\n1232\ntracks, 564training instances, 541, 543, 546\ntransaction control, 66\ntransaction coordinators, 1099,\n1104, 1106\u20131107\ntransaction identi\ufb01ers, 913\ntransaction managers, 18\u201321,\n1098\u20131099\nTransaction Processing\nPerformance Council\n(TPC), 1232\u20131234\ntransactions, 799\u2013828\naborted, 805\u2013807, 819\u2013820\nactions following crashes,\n923\u2013925\nactive, 806\naggregation of, 1278\nalternative models of\nprocessing, 1108\u20131110\nassociation rules and,\n546\u2013547\natomicity of, 20\u201321, 144, 481,\n800\u2013807, 819\u2013821\nbegin/end operations and,\n799\nblockchain, 1261\u20131263,\n1268\u20131271, 1273\ncascadeless schedules and,\n820\u2013821\ncheck constraints and, 800\ncommit protocols and,\n1100\u20131110\ncommitted ( seecommitted\ntransactions)\ncommit work and, 143\u2013145\ncompensating, 805\nconcept of, 799\u2013801\nconcurrency control and ( see\nconcurrency control)\nconcurrent, 1224\u20131227\nconsistency of, 20, 800, 802,\n807\u2013808, 821\u2013823\ncrashes and, 800\ncross-chain, 1273\ndata mining, 540\u2013549\nde\ufb01ned, 20, 799\ndistributed, 989\u2013990,\n1098\u20131100\ndouble-spend, 1261\u20131262,\n1264\ndurability of, 20\u201321, 800\u2013807\n", "1368": "1340 Index\nfailure of, 806, 907, 909, 1100\nforce/no-force policy and, 927\ngas concept for, 1270\u20131271\nglobal, 988, 1098, 1132\nin-doubt, 1105\nintegrity constraint violation\nand, 151\u2013152\nisolation of, 800\u2013804,\n807\u2013812, 819\u2013826\nkilling, 807\nlocal, 988, 1098, 1132\nlocks and ( seelocks)\nlog of ( seelog records)\nlong-duration, 890\u2013891\nminibatch, 1227\nmultiversion schemes and,\n869\u2013872, 1129\u20131131\nobservable external writes\nand, 807\no\ufb00-chain, 1275\nonline, 4, 521\nperformance tuning of,\n1224\u20131227\npersistent messaging and,\n990, 1016, 1108\u20131110, 1137\nread-only, 871\nreal-time systems, 894\nrecoverable schedules and,\n819\u2013820\nrecovery systems and, 21,\n803, 805\nremote backup systems and,\n931\u2013935\nrestarting, 807\nrollback and, 143\u2013145, 193,\n196, 805\u2013806, 922,\n937\u2013939, 945\u2013946\nscalability and, 471\nserializability and, 812\u2013819,\n821\u2013826\nshadow-copy scheme and, 914\nsimple model for, 801\u2013804\nas SQL statements, 826\u2013828\nstarved, 841, 853\nstates of, 805\u2013807\nsteal/no-steal policy and, 927\nstorage structure and,\n804\u2013805\nsupport for, 1028\u20131030\nterminated, 806time-lock, 1273\ntimestamps and, 861\u2013866\ntwo-phase commit protocol\nand, 989, 1016, 1276\nas unit of program execution,\n799\nupdate, 871\nvalidation and, 866\u2013869\nwait-for graph and, 851\u2013852,\n1113\u20131114\nwrite-ahead logging rule and,\n926\u2013929\nwrite operations and, 826\ntransaction scaleup, 973\ntransactions-consistent snapshot,\n1136\ntransaction-server systems,\n963\u2013968\ntransactions per second (TPS),\n1232\ntransaction time, 347n10\nTransactSQL, 199\ntransfer of control, 932\u2013933\ntransformations\ndata warehousing and, 523\nequivalence rules and,\n747\u2013752\nexamples of, 752\u2013754\njoin ordering and, 754\u2013755\nquery optimization and,\n747\u2013757\nrelational algebra and,\n747\u2013757\ntransition tables, 210\ntransition variables, 207\ntransitive closure, 214\u2013216\ntransitive dependencies, 317n9,\n356\ntransitivity rule, 321\ntranslation, query processing\nand, 689\u2013690\ntranslation table, 568\ntree-like server systems, 977\u2013978\ntree-like topology, 977\ntree protocol, 846\u2013848\ntrees\nB(seeB-trees)\nB+(seeB+-trees)\nB-link, 886\ndecision-tree classi\ufb01ers, 542directory information, 1242,\n1243\ndisjoint subtrees, 847\nGeneralized Search Tree, 670\nk-d, 673\u2013674\nk-d B, 674\nleft-deep join, 773\nLSM, 666\u2013668, 1028,\n1176\u20131182, 1215\nMerkle, 1143\u20131146, 1268,\n1269\nMerkle-Patricia, 1269, 1275\nmultiple granularity and,\n853\u2013857\noperator, 724, 1040\nquadratic split heuristic and,\n1189\nquadtrees, 392, 674,\n1186\u20131187\nR, 663, 670, 674\u2013676,\n1187\u20131190\ntree topology, 977\ntriangulated irregular network\n(TIN), 393\ntriangulation, 388, 393\ntriggers\nalter, 210\nde\ufb01ned, 206\ndisable, 210\ndrop, 210\nneed for, 206\u2013207\nnonstandard syntax and, 212\nrecovery and, 212\u2013213\nin SQL, 207\u2013210\ntransition tables and, 210\nwhen not to use, 210\u2013213\ntrim, 82\ntriple representation, 372\u2013374\ntrivial functional dependencies,\n311\ntrue predicate, 76\ntrue values, 96, 101\ntry-with-resources construct, 187,\n187n3\ntumbling window, 505\ntuning. Seeperformance tuning\ntuning wizards, 1215\ntuple-generating dependencies,\n337\ntuples\n", "1369": "Index 1341\naggregate functions and,\n91\u201396\nin Cartesian-product\noperation, 51, 52\nde\ufb01ned, 39\ndeletion and, 108\u2013110, 613\nduplicate, 103\neager generation of, 726, 727\ninsertion and, 110\u2013111\njoin operation for, 52\u201353 ( see\nalsojoins)\nlazy generation of, 727\nlogical routing of, 506\u2013507,\n1071\u20131073\nordering display of, 83\u201384\nphysical routing of,\n1072\u20131073\npipelining and, 691\u2013692,\n724\u2013731\nquery optimization and ( see\nquery optimization)\nquery processing and ( see\nquery processing)\nranking and, 219\u2013223\nreconstruction costs, 612\u2013613\nrelational algebra and,\n747\u2013757\nin relational model, 39, 41,\n43\u201346\nselect operation for, 49\nset operations and, 54\u201355,\n85\u201389\nstreaming data and, 501\u2013503,\n505\u2013507, 1071\u20131073\ntimestamps and, 502, 503,\n505\nupdates and, 111\u2013114, 613\nviews and, 137\u2013143\nwindowing and, 223\u2013226\ntuple variables, 81\nTuring-complete languages,\n1258, 1269, 1270\ntwo-factor authentication,\n441\u2013442\n2NF (second normal form),\n316n8, 341\u2013342, 356\ntwo-phase commit (2PC)\nprotocol, 989, 1016,\n1101\u20131107, 1161,\n1276two-phase locking protocol,\n841\u2013844, 871\u2013872,\n1129\u20131131\ntwo-tier architecture, 23\ntype inheritance, 378\u2013379\ntypes\nblobs, 156, 193, 594, 652\nclobs, 156, 193, 594, 652\ncomplex ( seecomplex data\ntypes)\nlarge-object, 156, 158\nperformance tuning and, 1226\nreference, 380\u2013381\nuser-de\ufb01ned, 158\u2013160, 378\nUML (Uni\ufb01ed Modeling\nLanguage), 288\u2013291\nunary operations, 48\nundo operation\nconcurrency control and,\n940\u2013941\nlogical, 936\u2013941\nrecovery systems and,\n915\u2013919, 922\u2013925\nrollback and, 916\u2013919,\n937\u2013939\nundo pass, 944\u2013946\nundo phase, 923\u2013925\nUni\ufb01ed Modeling Language\n(UML), 288\u2013291\nuniform resource locators\n(URLs), 405\u2013406\nunion all, 86, 97, 217n8\nunion of sets, 54, 750\nunion operation, 53\u201354, 86\u201387,\n228\nunion rule, 321\nunique construct, 103, 147\nunique key values, 160\u2013161\nunique-role assumption, 345\nuniqui\ufb01ers, 649\u2013650\nUnited States\naddress format used in, 250n1\nidenti\ufb01cation numbers in, 447\nprimary keys in, 44\u201345\nprivacy laws in, 995\nuniversal front end, 404\nUniversal Serial Bus (USB) slots,\n560\nuniversal Turing machines, 16university databases\nabstraction levels for, 11\u201312\napplication design and, 2\u20133,\n5\u20137, 403\u2013404, 431,\n442\u2013444\natomic domains and, 343\nBig Data and, 499\u2013500\nblockchain, 1277\nbu\ufb00er-replacement strategies\nand, 607\u2013609\nCartesian product and, 76\u201379\ncombination of schemas and,\n270\u2013271\ncomplex attributes and,\n249\u2013252\nconcurrency control and, 879\nconsistency constraints for, 6,\n13\u201315\ndecomposition and, 305\u2013307,\n310\u2013312\ndeletion requests and,\n109\u2013110\ndesign issues for, 346\u2013347\ndistributed, 988\nentities in, 243\u2013246,\n265\u2013268, 281\u2013283\nentity-relationship diagram\nfor, 263\u2013264\nfull schema for, 1287\u20131288\nfunctions and procedures for,\n198\u2013199\ngeneralization and, 273\u2013274\nhash functions and,\n1190\u20131193\nincompleteness of, 243\u2013244\nindices and, 625\u2013628, 664,\n1017\u20131018\ninsertion requests and,\n110\u2013111\nintegrity constraints for,\n145\u2013153\nmapping cardinalities and,\n253\u2013256\nmultivalued dependencies\nand, 336\nquery optimization and,\n751\u2013755, 775\u2013778\nquery processing and,\n690\u2013691, 704, 723\u2013724\n", "1370": "1342 Index\nrecursive queries and,\n213\u2013214, 217\u2013218\nredundancy in, 243, 261\u2013264,\n269\u2013270\nrelational algebra for, 49\u201358\nrelational model for, 9, 10,\n37\u201347\nrelational schema for, 41\u201343,\n303\u2013305\nrelationship sets and,\n246\u2013249, 268\u2013269,\n282\u2013283\nroles and authorizations for,\n167\u2013169\nsample data for, 1292\u20131298\nspecialization and, 271\u2013273\nSQL data de\ufb01nition for,\n69\u201371, 1288\u20131292\nSQL queries for, 16\nstorage and, 589\u2013591,\n597\u2013601\ntransactions and, 144,\n826\u2013827\ntriggers and, 211\u2013213\ntriple representation of,\n373\u2013374\nunique key values for, 160\u2013161\nuser interfaces for, 24\nviews and, 141\u2013143\nUniversity of California,\nBerkeley, 26\nUnix, 83, 914\nunknown values, 89\u201390, 96\nunpartitioned site, 1056\nunpin operations, 605\nupdatable result sets, 193\nupdate-anywhere replication,\n1137\nupdate hot spots, 1225\nupdates\nauthorization and, 14, 170,\n171\nbatch, 1221\non B+-trees, 641\u2013649\ncomplexity of, 647\u2013649\ndatabase modi\ufb01cation and,\n111\u2013114\ndata warehousing and, 523\ndeletion time and, 641,\n645\u2013649EXEC SQL and, 197\nhashing and, 624, 1197\u20131202\ninconsistent, 1140\u20131142\nindices and, 630\u2013632\ninsertion time and, 641\u2013645,\n647, 649\nlazy propagation of, 1122,\n1136\nlog records and, 913\u2013914,\n917\u2013918\nlost, 874\nLSM trees and, 1178\u20131179\nperformance tuning and,\n1221\u20131223, 1225\u20131227\nprivileges and, 166\u2013167\nquery optimization and,\n784\u2013785\nreconciliation of, 1142\u20131143\nreplication and, 1015\u20131016\nshipping SQL statements to\ndatabase and, 187\nsnapshot isolation and,\n873\u2013879\ntriggers and, 208, 212\ntuples and, 111\u2013114, 613\nof views, 140\u2013143\nupdate transactions, 871\nupgrade, 843\nURLs (uniform resource\nlocators), 405\u2013406\nU.S. National Institute for\nStandards and Technology,\n288\nUSB (Universal Serial Bus) slots,\n560\nuser-de\ufb01ned types, 158\u2013160, 378\nuser-interface layer, 429\nuser interfaces\napplication architectures and,\n429\u2013434\napplication programs and,\n403\u2013405\nback-end component of, 404\nbusiness-logic layer and, 430,\n431\nclient-server architecture and,\n404\nclient-side scripting and,\n421\u2013429common gateway interface\nstandard and, 409\ncookies and, 410\u2013415, 411n2,\n439\u2013440\nCRUD, 419\ndata access layer and,\n430\u2013434\ndisconnected operation and,\n427\u2013428\nfront-end component of, 404\nHTTP ( seeHyperText\nTransfer Protocol)\nmobile application platforms\nand, 428\u2013429\nf o rn a \u00c3 \u00af v eu s e r s ,2 4\npresentation layer and, 429\nresponsive, 423\nsecurity and, 437\u2013446\nfor sophisticated users, 24\nstorage and, 562\u2013563\nWeb services ( seeWorld\nWide Web)\nweb services and, 426\u2013429\nuser requirements in database\ndesign, 17\u201318, 241\u2013242,\n274\nutilization of resources, 808\nvalidation\nconcurrency control and,\n866\u2013869, 882\ndistributed, 1119\u20131120\n\ufb01rst committer wins and, 874\n\ufb01rst updater wins and,\n874\u2013875\nphases of, 866\nrecovery systems and, 916\nsnapshot isolation and,\n874\u2013875\ntest for, 868\nview serializability and,\n867\u2013868\nvalidation phase, 866\nvalid interval, 870\nvalid time, 157, 347\u2013350,\n347n10\nvalue for entity set attributes, 245\nvalue set of attributes, 249\nvarchar, 67\u201368, 70\nvariable-length records, 592\u2013594\n", "1371": "Index 1343\nvariety of data, 468\nVBScript, 417\nvector data, 392\u2013393\nvector processing, 612\nVectorwise, 615\nvelocity of data, 468\nveri\ufb01cation of contents, 1145\nversion numbering, 1141\nversions period for, 157\nversion-vector scheme,\n1141\u20131142\nVertica, 615\nvertical partitioning, 1004\nview de\ufb01nition, 66\nview equivalence, 818, 818n4\nview level of abstraction, 10\u201312\nview maintenance, 140, 779\u2013782,\n1138\u20131140, 1215\u20131216\nviews\nauthorization on, 169\u2013170\nwith check option, 143\ncreate view, 138\u2013143, 162,\n169\ndeferred maintenance and,\n779, 1215\u20131216\nde\ufb01ned, 137\u2013138\ndeletion and, 142\nimmediate maintenance and,\n779, 1215\u20131216\ninsertion and, 141\u2013143\nmaterialized ( seematerialized\nviews)\nperformance tuning and,\n1215\u20131216\nSQL queries and, 138\u2013139\nupdate of, 140\u2013143\nview serializability, 818\u2013819,\n867\u2013868\nvirtual machines (VMs), 970,\n991\u2013994\nvirtual nodes, 1009\u20131010\nVirtual Private Database (VPD),\n173, 444\u2013445\nvirtual processor approach,\n1010n3\nVisual Basic, 184, 206\nvisualization tools, 538\u2013540\nVMs (virtual machines), 970,\n991\u2013994volatile storage, 560, 562, 804,\n908\nvolume of data, 468\nVPD (Virtual Private Database),\n173, 444\u2013445\nwait-die scheme, 850, 1112\nwait-for graphs, 851\u2013852,\n1113\u20131114\nWAL (write-ahead logging),\n926\u2013929, 934\nWANs (wide-area networks), 989\nweak entity sets, 259\u2013260,\n267\u2013268\nwear leveling, 568\nweb application frameworks,\n418\u2013419\nweb-based services, database\napplications for, 3\nweb crawlers, 383\nWeblogic Application Server, 416\nWebObjects, 419\nweb servers, 408\u2013411\nweb services, 423\u2013429\nde\ufb01ned, 426\ndisconnected operation and,\n427\u2013428\ninterfacing with, 423\u2013426\nmobile application platforms,\n428\u2013429\nweb sessions, 408\u2013411\nWebSphere Application Server,\n416\nwhen clause, 212\nwhen statement, 208\nwhere clause\naggregate functions and,\n91\u201396\nbasic SQL queries and, 71\u201379\nbetween comparison, 84\non multiple relations, 74\u201379\nin multiset relational algebra,\n97\nnot between comparison, 84\nnull values and, 89\u201390\npredicates, 84\u201385\nquery optimization and,\n774\u2013777\nranking and, 222\nrename operation and, 79,\n81\u201382security and, 445\nset operations and, 85\u201389\non single relation, 71\u201374\nstring operations and, 82\u201383\ntransactions and, 824,\n826\u2013827\nwhile loop, 196\nwhile statements, 201\nwide-area networks (WANs), 989\nwide column data representation,\n366\nwide-column stores, 1023\nwindows and windowing,\n223\u2013226, 502\u2013506\nWiredTiger, 1028\nwireframe models, 390\nwith check option, 143\nwith clause, 105\u2013106, 217\nwith data clause, 162\nwith grant option, 170\u2013171\nwith recursive clause, 217\nwith timezone speci\ufb01cation, 154\nwitness data, 1258\nword count program, 483\u2013486,\n484n2, 490\u2013492\nworkers, 1051\nwork\ufb02ow\nbusiness-logic layer and, 431\ndatabase design and, 291\u2013292\ndistributed transaction\nprocessing and, 1110\nmanagement systems for, 990\nworkload, 783, 1215, 1217\nworkload compression, 1217\nwork stealing, 1048, 1062\nWorld Wide Web\napplication design and,\n405\u2013411\ncookies and, 410\u2013415, 411n2,\n439\u2013440\nencryption and, 447\u2013453\ngrowth of, 467\nHTML and ( seeHyperText\nMarkup Language)\nHTTP and ( seeHyperText\nTransfer Protocol)\nimpact on database systems,\n27\nsecurity and, 437\u2013446\nservers and sessions, 408\u2013411\n", "1372": "1344 Index\nURLs and, 405\u2013406\nWORM (write once, read-many)\ndisks, 561, 1022\nwound-wait scheme, 850\nwrappers, 1077, 1236\nwrite-ahead logging (WAL),\n926\u2013929, 934\nwrite ampli\ufb01cation, 1180\nwrite once, read-many (WORM)\ndisks, 561, 1022\nwrite operations, 826\nwrite-optimized index structures,\n665\u2013670\nwrite phase, 866write quorum, 1124\nwrite skew, 876\u2013877\nwrite-write contention, 1225\nW-timestamp, 862, 865, 870\nX.500 directory access protocol,\n1241\nXML (Extensible Markup\nLanguage)\nemergence of, 27\n\ufb02exibility of, 367\u2013368\nas semi-structured data\nmodel, 8, 27\nSQL in support of, 372\ntags and, 370\u2013372for transferring data, 423\nX/Open XA standards, 1239\nXOR operation, 448\nXPath, 372\nXQuery, 372\nXSRF (cross-site request\nforgery), 439\u2013440\nXSS (cross-site scripting),\n439\u2013440\nYahoo Message Bus service,\n1137\u20131138\nZab protocol, 1152\nZooKeeper, 1150, 1152\n"}}