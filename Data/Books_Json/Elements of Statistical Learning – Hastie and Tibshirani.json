{"Name": "Elements of Statistical Learning \u2013 Hastie and Tibshirani.pdf", "Pages": {"0": "Springer Series in Statistics\nTrevor Hastie\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\nThe Elements of\nStatistical Learning\nData Mining, Inference, and Prediction\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book\u2019s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting\u2014the first comprehensive treatment of thistopic in any book.\nThis major new edition features many topics not covered in the original, including graphical\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for \u201cwide\u201d data (p bigger than n), including multiple testing and false discovery rates.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n\u203aspringer.comSTATISTICS\nisbn 978-0-387-84857-0Trevor Hastie \u2022 Robert Tibshirani \u2022 Jerome Friedman\nThe Elements of Statictical Learning\nHastie \u2022 Tibshirani \u2022 Friedman\nSecond Edition", "1": "This is page v\nPrinter: Opaque this\nTo our parents:\nValerie and Patrick Hastie\nVera and Sami Tibshirani\nFlorence and Harry Friedman\nand to our families:\nSamantha, Timothy, and Lynda\nCharlie, Ryan, Julie, and Cheryl\nMelanie, Dora, Monika, and Ildiko", "2": "vi", "3": "This is page vii\nPrinter: Opaque this\nPreface to the Second Edition\nIn God we trust, all others bring data.\n\u2013William Edwards Deming (1900-1993)1\nWe have been grati\ufb01ed by the popularity of the \ufb01rst edition of The\nElements of Statistical Learning. This, along with the fast pace of research\nin the statistical learning \ufb01eld, motivated us to update our book with a\nsecond edition.\nWe have added four new chapters and updated some of the existing\nchapters. Because many readers are familiar with the layout of the \ufb01rst\nedition, we have tried to change it as little as possible. Here is a summary\nof the main changes:\n1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\nHayden; however Professor Hayden told us that he can claim no credit for this quote,\nand ironically we could \ufb01nd no \u201cdata\u201d con\ufb01rming that Deming a ctually said this.", "4": "viii Preface to the Second Edition\nChapter What\u2019s new\n1.Introduction\n2.Overview of Supervised Learning\n3.Linear Methods for Regression LAR algorithm and generalizations\nof the lasso\n4.Linear Methods for Classi\ufb01cation Lasso path for logistic regression\n5.Basis Expansions and Regulariza-\ntionAdditional illustrations of RKHS\n6.Kernel Smoothing Methods\n7.Model Assessment and Selection Strengths and pitfalls of cross-\nvalidation\n8.Model Inference and Averaging\n9.Additive Models, Trees, and\nRelated Methods\n10.Boosting and Additive Trees New example from ecology; some\nmaterial split o\ufb00 to Chapter 16.\n11.Neural Networks Bayesian neural nets and the NIPS\n2003 challenge\n12.Support Vector Machines and\nFlexible DiscriminantsPath algorithm for SVM classi\ufb01er\n13. Prototype Methods and\nNearest-Neighbors\n14.Unsupervised Learning Spectral clustering, kernel PCA,\nsparse PCA, non-negative matrix\nfactorization archetypal analysis,\nnonlinear dimension reduction,\nGoogle page rank algorithm, a\ndirect approach to ICA\n15.Random Forests New\n16.Ensemble Learning New\n17.Undirected Graphical Models New\n18.High-Dimensional Problems New\nSome further notes:\n\u2022Our \ufb01rst edition was unfriendly to colorblind readers; in particular,\nwe tended to favor red/green contrasts which are particularly trou-\nblesome. We have changed the color palette in this edition to a large\nextent, replacing the above with an orange /bluecontrast.\n\u2022We have changed the name of Chapter 6 from \u201cKernel Methods\u201d to\n\u201cKernel Smoothing Methods\u201d, to avoid confusion with the machine-\nlearning kernel method that is discussed in the context of support vec-\ntor machines (Chapter 11) and more generally in Chapters 5 and 14.\n\u2022In the \ufb01rst edition, the discussion of error-rate estimation in Chap-\nter 7 was sloppy, as we did not clearly di\ufb00erentiate the notions of\nconditional error rates (conditional on the training set) and uncondi-\ntional rates. We have \ufb01xed this in the new edition.", "5": "Preface to the Second Edition ix\n\u2022Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\nters are probably best read in that order.\n\u2022In Chapter 17, we have not attempted a comprehensive treatment\nof graphical models, and discuss only undirected models and some\nnew methods for their estimation. Due to a lack of space, we have\nspeci\ufb01cally omitted coverage of directed graphical models.\n\u2022Chapter 18 explores the \u201c p\u226bN\u201d problem, which is learning in high-\ndimensional feature spaces. These problems arise in many areas, in-\ncluding genomic and proteomic studies, and document classi\ufb01cation.\nWe thank the many readers who have found the (too numerous) errors in\nthe \ufb01rst edition. We apologize for those and have done our best to avoid er-\nrors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\nWasserman for comments on some of the new chapters, and many Stanford\ngraduate and post-doctoral students who o\ufb00ered comments, in particular\nMohammed AlQuraishi, John Boik, Holger Hoe\ufb02ing, Arian Maleki, Donal\nMcMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\nHui Zou. We thank John Kimmel for his patience in guiding us through this\nnew edition. RT dedicates this edition to the memory of Anna McPhee.\nTrevor Hastie\nRobert Tibshirani\nJerome Friedman\nStanford, California\nAugust 2008", "6": "x Preface to the Second Edition", "7": "This is page xi\nPrinter: Opaque this\nPreface to the First Edition\nWe are drowning in information and starving for knowledge.\n\u2013Rutherford D. Roger\nThe \ufb01eld of Statistics is constantly challenged by the problems that science\nand industry brings to its door. In the early days, these problems often came\nfrom agricultural and industrial experiments and were relatively small in\nscope. With the advent of computers and the information age, statistical\nproblems have exploded both in size and complexity. Challenges in the\nareas of data storage, organization and searching have led to the new \ufb01eld\nof \u201cdata mining\u201d; statistical and computational problems in biology and\nmedicine have created \u201cbioinformatics.\u201d Vast amounts of data are being\ngenerated in many \ufb01elds, and the statistician\u2019s job is to make sense of it\nall: to extract important patterns and trends, and understand \u201cwhat the\ndata says.\u201d We call this learning from data .\nThe challenges in learning from data have led to a revolution in the sta-\ntistical sciences. Since computation plays such a key role, it is not surprising\nthat much of this new development has been done by researchers in other\n\ufb01elds such as computer science and engineering.\nThe learning problems that we consider can be roughly categorized as\neither supervised orunsupervised . In supervised learning, the goal is to pre-\ndict the value of an outcome measure based on a number of input measures;\nin unsupervised learning, there is no outcome measure, and the goal is to\ndescribe the associations and patterns among a set of input measures.", "8": "xii Preface to the First Edition\nThis book is our attempt to bring together many of the important new\nideas in learning, and explain them in a statistical framework. While some\nmathematical details are needed, we emphasize the methods and their con-\nceptual underpinnings rather than their theoretical properties. As a result,\nwe hope that this book will appeal not just to statisticians but also to\nresearchers and practitioners in a wide variety of \ufb01elds.\nJust as we have learned a great deal from researchers outside of the \ufb01eld\nof statistics, our statistical viewpoint may help others to better understa nd\ndi\ufb00erent aspects of learning:\nThere is no true interpretation of anything; interpretatio n is a\nvehicle in the service of human comprehension. The value of\ninterpretation is in enabling others to fruitfully think ab out an\nidea.\n\u2013Andreas Buja\nWe would like to acknowledge the contribution of many people to the\nconception and completion of this book. David Andrews, Leo Breiman,\nAndreas Buja, John Chambers, Bradley Efron, Geo\ufb00rey Hinton, Werner\nStuetzle, and John Tukey have greatly in\ufb02uenced our careers. Balasub-\nramanian Narasimhan gave us advice and help on many computational\nproblems, and maintained an excellent computing environment. Shin-Ho\nBang helped in the production of a number of the \ufb01gures. Lee Wilkinson\ngave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\nGupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\ndan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\nZhu, two reviewers and many students read parts of the manuscript and\no\ufb00ered helpful suggestions. John Kimmel was supportive, patient and help-\nful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\nproduction team at Springer. Trevor Hastie would like to thank the statis-\ntics department at the University of Cape Town for their hospitality during\nthe \ufb01nal stages of this book. We gratefully acknowledge NSF and NIH for\ntheir support of this work. Finally, we would like to thank our families and\nour parents for their love and support.\nTrevor Hastie\nRobert Tibshirani\nJerome Friedman\nStanford, California\nMay 2001\nThe quiet statisticians have changed our world; not by disco v-\nering new facts or technical developments, but by changing t he\nways that we reason, experiment and form our opinions ....\n\u2013Ian Hacking", "9": "This is page xiii\nPrinter: Opaque this\nContents\nPreface to the Second Edition vii\nPreface to the First Edition xi\n1 Introduction 1\n2 Overview of Supervised Learning 9\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n2.3 Two Simple Approaches to Prediction:\nLeast Squares and Nearest Neighbors . . . . . . . . . . . 11\n2.3.1 Linear Models and Least Squares . . . . . . . . 11\n2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n2.6 Statistical Models, Supervised Learning\nand Function Approximation . . . . . . . . . . . . . . . . 28\n2.6.1 A Statistical Model\nfor the Joint Distribution Pr( X,Y) . . . . . . . 28\n2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n2.6.3 Function Approximation . . . . . . . . . . . . . 29\n2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n2.7.1 Di\ufb03culty of the Problem . . . . . . . . . . . . . 32", "10": "xiv Contents\n2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n2.8.2 Kernel Methods and Local Regression . . . . . . 34\n2.8.3 Basis Functions and Dictionary Methods . . . . 35\n2.9 Model Selection and the Bias\u2013Variance Tradeo\ufb00 . . . . . 37\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3 Linear Methods for Regression 43\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.2 Linear Regression Models and Least Squares . . . . . . . 44\n3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n3.2.2 The Gauss\u2013Markov Theorem . . . . . . . . . . . 51\n3.2.3 Multiple Regression\nfrom Simple Univariate Regression . . . . . . . . 52\n3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n3.3.4 Prostate Cancer Data Example (Continued) . . 61\n3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n3.4.3 Discussion: Subset Selection, Ridge Regression\nand the Lasso . . . . . . . . . . . . . . . . . . . 69\n3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n3.5.1 Principal Components Regression . . . . . . . . 79\n3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n3.6 Discussion: A Comparison of the Selection\nand Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n3.8.1 Incremental Forward Stagewise Regression . . . 86\n3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n3.9 Computational Considerations . . . . . . . . . . . . . . . 93\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94", "11": "Contents xv\n4 Linear Methods for Classi\ufb01cation 101\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n4.4.2 Example: South African Heart Disease . . . . . 122\n4.4.3 Quadratic Approximations and Inference . . . . 124\n4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\n4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n4.5.1 Rosenblatt\u2019s Perceptron Learning Algorithm . . 130\n4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n5 Basis Expansions and Regularization 139\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n5.2.2 Example: South African Heart Disease (Continued)146\n5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n5.5 Automatic Selection of the Smoothing Parameters . . . . 156\n5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n5.5.2 The Bias\u2013Variance Tradeo\ufb00 . . . . . . . . . . . . 158\n5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n5.8.1 Spaces of Functions Generated by Kernels . . . 168\n5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\nAppendix: Computational Considerations for Splines . . . . . . 186\nAppendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\nAppendix: Computations for Smoothing Splines . . . . . 189", "12": "xvi Contents\n6 Kernel Smoothing Methods 191\n6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n6.4.2 Structured Regression Functions . . . . . . . . . 203\n6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n6.6 Kernel Density Estimation and Classi\ufb01cation . . . . . . . 208\n6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n6.6.2 Kernel Density Classi\ufb01cation . . . . . . . . . . . 210\n6.6.3 The Naive Bayes Classi\ufb01er . . . . . . . . . . . . 210\n6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n6.8 Mixture Models for Density Estimation and Classi\ufb01cation 214\n6.9 Computational Considerations . . . . . . . . . . . . . . . 216\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n7 Model Assessment and Selection 219\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n7.3 The Bias\u2013Variance Decomposition . . . . . . . . . . . . . 223\n7.3.1 Example: Bias\u2013Variance Tradeo\ufb00 . . . . . . . . 226\n7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n7.6 The E\ufb00ective Number of Parameters . . . . . . . . . . . . 232\n7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n7.9 Vapnik\u2013Chervonenkis Dimension . . . . . . . . . . . . . . 237\n7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\n7.10.2 The Wrong and Right Way\nto Do Cross-validation . . . . . . . . . . . . . . . 245\n7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n8 Model Inference and Averaging 261\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261", "13": "Contents xvii\n8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n8.4 Relationship Between the Bootstrap\nand Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n8.5.3 EM as a Maximization\u2013Maximization Procedure 277\n8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n8.7.1 Example: Trees with Simulated Data . . . . . . 283\n8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n9 Additive Models, Trees, and Related Methods 295\n9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n9.1.2 Example: Additive Logistic Regression . . . . . 299\n9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n9.2.3 Classi\ufb01cation Trees . . . . . . . . . . . . . . . . 308\n9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\n9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n9.7 Computational Considerations . . . . . . . . . . . . . . . 334\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n10 Boosting and Additive Trees 337\n10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340", "14": "xviii Contents\n10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n10.7 \u201cO\ufb00-the-Shelf\u201d Procedures for Data Mining . . . . . . . . 350\n10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n10.10.3 Implementations of Gradient Boosting . . . . . . 360\n10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n10.13.1 Relative Importance of Predictor Variables . . . 367\n10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n11 Neural Networks 389\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n11.5.2 Over\ufb01tting . . . . . . . . . . . . . . . . . . . . . 398\n11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n11.10 Computational Considerations . . . . . . . . . . . . . . . 414\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415", "15": "Contents xix\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n12 Support Vector Machines and\nFlexible Discriminants 417\n12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n12.2 The Support Vector Classi\ufb01er . . . . . . . . . . . . . . . . 417\n12.2.1 Computing the Support Vector Classi\ufb01er . . . . 420\n12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n12.3.1 Computing the SVM for Classi\ufb01cation . . . . . . 423\n12.3.2 The SVM as a Penalization Method . . . . . . . 426\n12.3.3 Function Estimation and Reproducing Kernels . 428\n12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n12.3.5 A Path Algorithm for the SVM Classi\ufb01er . . . . 432\n12.3.6 Support Vector Machines for Regression . . . . . 434\n12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\n12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n13 Prototype Methods and Nearest-Neighbors 459\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\n13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n13.3 k-Nearest-Neighbor Classi\ufb01ers . . . . . . . . . . . . . . . 463\n13.3.1 Example: A Comparative Study . . . . . . . . . 468\n13.3.2 Example: k-Nearest-Neighbors\nand Image Scene Classi\ufb01cation . . . . . . . . . . 470\n13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n13.4.2 Global Dimension Reduction\nfor Nearest-Neighbors . . . . . . . . . . . . . . . 479\n13.5 Computational Considerations . . . . . . . . . . . . . . . 480\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481", "16": "xx Contents\n14 Unsupervised Learning 485\n14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n14.2.5 Generalized Association Rules . . . . . . . . . . 497\n14.2.6 Choice of Supervised Learning Method . . . . . 499\n14.2.7 Example: Market Basket Analysis (Continued) . 499\n14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\n14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n14.3.8 Example: Human Tumor Microarray Data . . . 512\n14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\n14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n14.7 Independent Component Analysis\nand Exploratory Projection Pursuit . . . . . . . . . . . . 557\n14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n14.7.2 Independent Component Analysis . . . . . . . . 560\n14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n14.9 Nonlinear Dimension Reduction\nand Local Multidimensional Scaling . . . . . . . . . . . . 572\n14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579", "17": "Contents xxi\n15 Random Forests 587\n15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n15.2 De\ufb01nition of Random Forests . . . . . . . . . . . . . . . . 587\n15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n15.3.4 Random Forests and Over\ufb01tting . . . . . . . . . 596\n15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n15.4.1 Variance and the De-Correlation E\ufb00ect . . . . . 597\n15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n16 Ensemble Learning 605\n16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n16.2.2 The \u201cBet on Sparsity\u201d Principle . . . . . . . . . 610\n16.2.3 Regularization Paths, Over-\ufb01tting and Margins . 613\n16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n17 Undirected Graphical Models 625\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n17.3 Undirected Graphical Models for Continuous Variables . 630\n17.3.1 Estimation of the Parameters\nwhen the Graph Structure is Known . . . . . . . 631\n17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n17.4 Undirected Graphical Models for Discrete Variables . . . 638\n17.4.1 Estimation of the Parameters\nwhen the Graph Structure is Known . . . . . . . 639\n17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n18 High-Dimensional Problems: p\u226bN 649\n18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649", "18": "xxii Contents\n18.2 Diagonal Linear Discriminant Analysis\nand Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n18.3 Linear Classi\ufb01ers with Quadratic Regularization . . . . . 654\n18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n18.3.2 Logistic Regression\nwith Quadratic Regularization . . . . . . . . . . 657\n18.3.3 The Support Vector Classi\ufb01er . . . . . . . . . . 657\n18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n18.3.5 Computational Shortcuts When p\u226bN. . . . . 659\n18.4 Linear Classi\ufb01ers with L1Regularization . . . . . . . . . 661\n18.4.1 Application of Lasso\nto Protein Mass Spectroscopy . . . . . . . . . . 664\n18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n18.5 Classi\ufb01cation When Features are Unavailable . . . . . . . 668\n18.5.1 Example: String Kernels\nand Protein Classi\ufb01cation . . . . . . . . . . . . . 668\n18.5.2 Classi\ufb01cation and Other Models Using\nInner-Product Kernels and Pairwise Distances . 670\n18.5.3 Example: Abstracts Classi\ufb01cation . . . . . . . . 672\n18.6 High-Dimensional Regression:\nSupervised Principal Components . . . . . . . . . . . . . 674\n18.6.1 Connection to Latent-Variable Modeling . . . . 678\n18.6.2 Relationship with Partial Least Squares . . . . . 680\n18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\nReferences 699\nAuthor Index 729\nIndex 737", "19": "This is page 1\nPrinter: Opaque this\n1\nIntroduction\nStatistical learning plays a key role in many areas of science, \ufb01nance and\nindustry. Here are some examples of learning problems:\n\u2022Predict whether a patient, hospitalized due to a heart attack, will\nhave a second heart attack. The prediction is to be based on demo-\ngraphic, diet and clinical measurements for that patient.\n\u2022Predict the price of a stock in 6 months from now, on the basis of\ncompany performance measures and economic data.\n\u2022Identify the numbers in a handwritten ZIP code, from a digitized\nimage.\n\u2022Estimate the amount of glucose in the blood of a diabetic person,\nfrom the infrared absorption spectrum of that person\u2019s blood.\n\u2022Identify the risk factors for prostate cancer, based on clinical and\ndemographic variables.\nThe science of learning plays a key role in the \ufb01elds of statistics, data\nmining and arti\ufb01cial intelligence, intersecting with areas of engineering and\nother disciplines.\nThis book is about learning from data. In a typical scenario, we have\nan outcome measurement, usually quantitative (such as a stock price) or\ncategorical (such as heart attack/no heart attack), that we wish to predict\nbased on a set of features (such as diet and clinical measurements). We\nhave a training set of data, in which we observe the outcome and feature", "20": "2 1. Introduction\nTABLE 1.1. Average percentage of words or characters in an email message\nequal to the indicated word or character. We have chosen the wo rds and characters\nshowing the largest di\ufb00erence between spamandemail.\ngeorge you your hp free hpl ! our re edu remove\nspam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\nemail 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\nmeasurements for a set of objects (such as people). Using this data we build\na prediction model, or learner , which will enable us to predict the outcome\nfor new unseen objects. A good learner is one that accurately predicts such\nan outcome.\nThe examples above describe what is called the supervised learning prob-\nlem. It is called \u201csupervised\u201d because of the presence of the outcome vari-\nable to guide the learning process. In the unsupervised learning problem ,\nwe observe only the features and have no measurements of the outcome.\nOur task is rather to describe how the data are organized or clustered. We\ndevote most of this book to supervised learning; the unsupervised problem\nis less developed in the literature, and is the focus of Chapter 14.\nHere are some examples of real learning problems that are discussed in\nthis book.\nExample 1: Email Spam\nThe data for this example consists of information from 4601 email mes-\nsages, in a study to try to predict whether the email was junk email, or\n\u201cspam.\u201d The objective was to design an automatic spam detector that\ncould \ufb01lter out spam before clogging the users\u2019 mailboxes. For all 4601\nemail messages, the true outcome (email type) email orspamis available,\nalong with the relative frequencies of 57 of the most commonly occurring\nwords and punctuation marks in the email message. This is a supervised\nlearning problem, with the outcome the class variable email/spam. It is also\ncalled a classi\ufb01cation problem.\nTable 1.1 lists the words and characters showing the largest average\ndi\ufb00erence between spamandemail.\nOur learning method has to decide which features to use and how: for\nexample, we might use a rule such as\nif (%george <0.6) & (%you>1.5) then spam\nelseemail.\nAnother form of a rule might be:\nif (0.2\u2264%you\u22120.3\u2264%george )>0 then spam\nelseemail.", "21": "1. Introduction 3\nlpsa\u22121 1 2 3 4\noooooo ooo ooo o oooo o oooo oooooooooo oooooooooooooooooooooooooooooo oo oooo ooooooooooooooooooooooooooooo\nooo ooo ooooo oooooooooooooooooooooooooo o ooooooooo oo ooooooo oooooooooooo ooooooooooooooooooooooooooooo40 50 60 70 80\noo oooo ooo ooo oooooo o ooooooooooooo oooooooooooooooo o o oooooo oo oooo oooooooooooooooooooo ooooooooooooooo\noo o ooo o o o oo ooooo o oo o o o ooooo ooo ooo oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8\noo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo\noo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo ooo ooooooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0\noo oooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo ooooooooooo o ooooooooo o ooooooooooooo\n0 1 2 3 4 5oo oooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o ooo ooo o ooo o oooooooo oooo oooooo o\u22121 1 2 3 4ooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\noooo\noo\nooo\nooooo\noo\nooo\nooo\noo\nooo\nooo\noo\nooo\nooo\nooooo\noooo\noooo\noo\nooo\noo\nooo\nooo\nlcavol\nooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\noooo\noo\nooo\nooooo\noo\nooo\nooo\noo\nooo\nooo\noo\nooo\nooo\nooooo\noooo\noooo\noo\nooo\noo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\noooo\noo\nooo\nooooo\noo\nooo\nooo\noo\no oo\nooo\noo\nooo\nooo\nooooo\noooo\noo oo\noo\nooo\noo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\nooo o\noo\no oo\nooooo\noo\no oo\nooo\noo\nooo\nooo\no o\nooo\nooo\no oooo\noo oo\noooo\noo\nooo\noo\nooo\no oo\nooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\noooo\noo\nooo\nooooo\noo\nooo\nooo\noo\nooo\nooo\no o\nooo\nooo\no oooo\noo o o\noo o o\noo\nooo\noo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\nooo o\noo\nooo\nooooo\noo\nooo\nooo\noo\nooo\nooo\no o\nooo\nooo\nooooo\noo o o\noooo\noo\nooo\noo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\noooo\noo\no oo\nooooo\noo\nooo\nooo\noo\nooo\nooo\noo\nooo\nooo\nooooo\noooo\nooo o\noo\nooo\noo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noo\noooo\noo\noo\no\nooo\nooo\noooo\noo\no oo\nooooo\noo\no oo\nooo\noo\nooo\nooo\no o\noo o\nooo\nooooo\noo oo\noooo\noo\nooo\noo\nooo\no oo\noo\noooooo o\nooo\nooo\nooo\noo\nooooo\nooo\nooo\no\noooo\no\noo\nooooo\noooo\noooo\noo\noo\nooo\noo\nooo\noooooo\no\noo\noo\nooo\noo\nooooooo\noo\noo\nooo\nooo\noo\nooooooo\nooo\nooo\nooo\noo\nooooo\nooo\nooo\no\noooo\no\noo\nooooo\noooo\noooo\noo\noo\nooo\noo\nooo\noooooo\no\noo\noo\nooo\noo\nooooooo\noo\noo\nooo\nooolweight\noo\nooooooo\nooo\nooo\nooo\noo\nooooo\nooo\nooo\no\noooo\no\noo\nooooo\noooo\no ooo\noo\noo\nooo\noo\nooo\noooooo\no\noo\noo\nooo\noo\nooooooo\noo\noo\noo o\nooo\noo\noooooo o\noo o\nooo\nooo\noo\no oooo\nooo\nooo\no\noooo\no\noo\noooo o\noooo\no ooo\noo\noo\nooo\noo\nooo\noooooo\no\noo\noo\nooo\noo\nooooooo\noo\noo\nooo\nooo\noo\noooooo o\nooo\nooo\nooo\noo\nooooo\nooo\nooo\no\noooo\no\noo\nooooo\nooo o\noooo\noo\noo\nooo\noo\nooo\noooooo\no\noo\noo\nooo\noo\nooo oooo\noo\noo\nooo\nooo\noo\noooooo o\nooo\nooo\nooo\noo\no oo oo\nooo\nooo\no\noooo\no\noo\nooooo\nooo o\noooo\noo\noo\nooo\noo\nooo\noooooo\no\noo\noo\nooo\noo\nooooooo\noo\noo\nooo\nooo\noo\noooooo o\nooo\nooo\nooo\noo\nooooo\nooo\nooo\no\noooo\no\noo\nooooo\nooo o\noooo\noo\noo\nooo\noo\nooo\noooooo\no\noo\noo\nooo\noo\nooo oooo\noo\noo\nooo\nooo\n2.5 3.5 4.5oo\noooooo o\nooo\nooo\nooo\noo\nooo oo\nooo\nooo\no\noooo\no\noo\nooooo\nooo o\noooo\noo\noo\nooo\noo\nooo\noo oooo\no\noo\noo\noo o\noo\nooooooo\noo\noo\nooo\nooo40 50 60 70 80ooo\noo\noo\no\noooo oo\nooo\no\noo\noooooooo oo o oo\noooo\noo\nooo\nooo\noo\no\nooo\no oo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\nooo o\nooo\noo\noo\no\noooo oo\nooo\no\noo\noo oooooo ooo oo\noooo\noo\nooo\nooo\noo\no\nooo\nooo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\nooo o\nooo\noo\noo\no\noooooo\nooo\no\noo\noooooooooo ooo\noooo\noo\nooo\nooo\noo\no\nooo\nooo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\noooo\nage\nooo\noo\noo\no\noooo oo\nooo\no\noo\no o oooooooo o oo\noo oo\noo\nooo\no oo\noo\no\nooo\nooo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\noooo\nooo\noo\noo\no\noooo oo\nooo\no\noo\noooooooo oo o oo\noooo\noo\nooo\nooo\noo\no\nooo\no oo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\nooo o\nooo\noo\noo\no\noooo oo\nooo\no\noo\no o oooooo oo ooo\noooo\noo\nooo\nooo\noo\no\nooo\no oo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\nooo o\nooo\noo\noo\no\noooo oo\nooo\no\noo\noooooooo oo o oo\noooo\noo\nooo\nooo\noo\no\nooo\no oo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\nooo o\nooo\noo\noo\no\noooo oo\nooo\no\noo\noooooooo oo o oo\noooo\noo\nooo\nooo\noo\no\nooo\no oo\noo\nooo\noo\noo\nooooooo\noo\no\nooooo\no\noo\noo\no\noo\noooo\no\noo\nooo o\noo o ooooo\no ooo\no o o oo\no oo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\no ooo\nooo\noo\nooo\noo\noooo\no\noooo oooo\no o oo\no oooo\no oo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\no ooo\nooo\noo\no oo\noo\no ooo\no\no oo ooooo\noo oo\no o ooo\nooo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\no ooo\nooo\noo\no oo\noo\no ooo\no\no o oo oooo\no ooo\no oo oo\no oo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\noooo\nooo\noo\nooo\noo\no o oo\nolbph\no o o o o ooo\no o oo\no o o oo\no oo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\no ooo\nooo\noo\no oo\noo\no o oo\no\no o o o o ooo\no o oo\noo ooo\nooo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\no ooo\nooo\noo\no oo\noo\no ooo\no\no o oo o ooo\no o oo\no o ooo\no oo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\no ooo\nooo\noo\no oo\noo\no o oo\no\n\u22121 0 1 2o o oo o ooo\no o oo\noo o oo\no oo\noo\nooo\no\noo\no\nooo\no\no oo\no\noo\no\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\noo\noooo\noo\noo\noo\noooo\noooo\nooo\noo\no oo\noo\noooo\no0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo\no o o o o o oo\no o o o o o o o o o o o o oo\noo\no o o o o oo\noo oo o\no oo\no o oo\no oo\nooo o\noooooo o\noooo oo o oo o oo o oooo oo o o oo oo ooo o oo o oo o o ooo\no o oo oooo\no oo o oo o oooo o ooo\noo\noo ooo oo\noo o o o\no oo\noo oo\nooo\noo oo\nooo oo o o\no oo ooo o o oo o oo o oo ooo oooo ooo ooo o oooo o oo oo\noo oo oo oo\noo o o oo oo oo o oo oo\noo\noo oo o oo\nooo oo\no oo\noo oo\nooo\noo oo\nooo oo oo\no o oo oo ooo ooo o oo o oo o oo o o o o ooo oo o o oo o o ooo\no o oo o ooo\no o o oo o oo ooo oo oo\noo\noo o oooo\nooo o o\no oo\noooo\nooo\nooo o\noo oo o o o\no o o o o o o oo o o oo o o o oo o oo oo o ooo ooo o oo o o oooo\no o o oo o oo\no o oo o oo o oo oo o oo\noo\no o o oo oo\noo oo o\nooo\no o oo\no oo\noo o o\noo o o o oo\nsvi\no o o o o o o o o o o o oo oo o oo o o oo oo o oo o ooo o o oo ooo\noo o o oooo\noo oo o o oo o oo o o oo\noo\no o ooo oo\noo o o o\nooo\no o oo\nooo\noo oo\noo o oo o o\no o oo o o o o o o o o o o oo oo o o o oo oo o o o oo o o o o o o ooo\no ooo o o oo\noo o oo o o o o oo o ooo\noo\no o o oo oo\noo oo o\no oo\no o oo\no oo\noo o o\noo o o o o o\no o oo o o o o o o o o oo o o oo o o o oo oo o oo oo o o o o oo o oo\no oooo o oo\noo o oo ooo oo o o ooo\noo\no o ooooo\noo oo o\no oo\noo oo\nooo\noo o o\noo ooo oo\noo o ooo o o o oo oo\noo\nooo\no o oo\noo\no oo\nooo\no\no o oo\noo\noo\no\no ooo\no\noo\no\noo\no ooo\no oo\no o o oooo\nooo\no\nooo\nooooo\no\noo\nooo\noo\noo\nooo\no\no oooo\noo\noooo oo o oo o ooo\noo\nooo\no o oo\noo\no oo\nooo\no\no ooo\noo\noo\no\no ooo\no\noo\no\noo\no ooo\nooo\no o ooooo\nooo\no\nooo\nooooo\no\noo\nooo\noo\noo\nooo\no\nooooo\noo\no oo ooo o o oo o oo\noo\nooo\no ooo\noo\nooo\nooo\no\noooo\noo\noo\no\no ooo\no\noo\no\noo\no ooo\no oo\no oo oooo\nooo\no\nooo\nooooo\no\noo\nooo\noo\noo\nooo\no\nooooo\noo\no o oo oo ooo oooo\noo\nooo\no ooo\noo\no oo\nooo\no\no ooo\noo\noo\no\no ooo\no\noo\no\noo\noooo\no oo\no oo oooo\nooo\no\nooo\nooooo\no\noo\nooo\noo\noo\nooo\no\nooooo\noo\no o o o o o o oo o o oo\noo\nooo\no ooo\noo\nooo\nooo\no\noo oo\noo\noo\no\no ooo\no\noo\no\noo\no ooo\no oo\noo o oooo\nooo\no\nooo\nooooo\no\noo\no oo\noo\noo\nooo\no\no oooo\noo\no o o o o o o o o o o oo\noo\nooo\no o oo\noo\no oo\nooo\no\no o oo\noo\noo\no\no ooo\no\noo\no\noo\no ooo\no oo\no o o oooo\nooo\no\nooo\nooooo\no\noo\nooo\noo\noo\nooo\no\no oooo\noo\nlcp\no o oo o o o o o o o oo\noo\nooo\no o oo\noo\no oo\nooo\no\no o oo\noo\noo\no\no ooo\no\noo\no\noo\noooo\no oo\no o ooooo\nooo\no\nooo\nooooo\no\noo\nooo\noo\noo\nooo\no\no oooo\noo\n\u22121 0 1 2 3o o oo o o o o o o o oo\noo\nooo\no o oo\noo\no oo\nooo\no\no o oo\noo\noo\no\no ooo\no\noo\no\noo\no ooo\no oo\no o ooooo\nooo\no\nooo\nooooo\no\noo\nooo\noo\noo\nooo\no\no oooo\noo6.0 7.0 8.0 9.0ooo\nooo o o o oo oo o o\noo\no o o oo\noo\no oo o o\no o o o o ooo\no o oo\no\noo o oo\no\no oo\noo o o o o\noo o\nooo\no\noo o o\noo o o oo\no o o o o o o o oo\no o\nooo o\noooooo o\nooo\no oo o oo o ooo oo\noo\no o o oo\noo\no ooo o\noo o oo ooo\no ooo\no\nooooo\no\nooo\noo o ooo\noo o\nooo\no\noo oo\noo oo oo\no oo o o oo o oo\no o\noo oo\nooo oo o o\no oo\nooo o o oo o oo o o\noo\noo ooo\noo\nooooo\no oooo ooo\no ooo\no\nooo oo\no\no oo\noo oo oo\nooo\nooo\no\noo oo\noooo oo\noo o oo oo o oo\noo\noo oo\nooo oo oo\no oo\no oo ooo oooo oo\noo\no o ooo\noo\no ooo o\no o o oo ooo\no ooo\no\noo ooo\no\no oo\noo oo oo\nooo\nooo\no\noo o o\nooo ooo\no o o ooooo oo\no o\nooo o\noo oo o o o\no oo\no o o o oo o o oo o o\noo\no o ooo\noo\nooo oo\no o oo o ooo\no o oo\no\noo o oo\no\no oo\nooo o oo\noo o\nooo\no\noo o o\noo o ooo\no o ooo o o ooo\no o\noo o o\noo o o o oo\no oo\no o o o o o o o oo o o\noo\no o o oo\noo\no oo o o\no o o o o ooo\no ooo\no\noo o oo\no\no oo\noo o o o o\noo o\nooo\no\noo o o\noo oo oo\no o o o oo o o oo\no o\noo o o\noo o o o o o\no oo\no o o o o o o o ooo o\noo\noo o oo\noo\no ooo o\no oo o o ooo\no ooo\no\nooooo\no\no oo\noo oo o o\noo o\nooo\no\noo oo\noo oo oo\no ooo oo o ooo\no o\noo oo\noo o oo o ogleason\no oo\no o o o o o o o ooo o\noo\no o o oo\noo\no oo o o\no o o o o ooo\no ooo\no\noo o oo\no\no oo\noooo oo\noo o\nooo\no\noo oo\noo oo oo\no o oo ooo ooo\no o\noo o o\noo ooo oo\n0 1 2 3 4 5ooo\nooo o o o oo oo\no ooo\no o o oo\noo\no oo\noo\no o o o o ooo oo\noo\no\nooooo\no\no oo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\nooo\no oo o oo o ooo\noooo\no o o oo\noo\no oo\noo\noo o oo ooooo\noo\no\nooooo\no\nooo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\n2.5 3.5 4.5o oo\nooo o o oo o oo\no ooo\noo ooo\noo\nooo\noo\no oooo ooo oo\noo\no\nooooo\no\no oo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\no oo\no oo ooo oooo\noooo\no o ooo\noo\no oo\noo\no o o oo ooooo\noo\no\nooooo\no\no oo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\n\u22121 0 1 2o oo\no o o o oo o o oo\no ooo\no o ooo\noo\nooo\noo\no o oo o ooooo\noo\no\nooooo\no\no oo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\no oo\no o o o o o o o oo\no ooo\no o o oo\noo\no oo\noo\no o o o o ooo oo\noo\no\nooooo\no\no oo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\n\u22121 0 1 2 3o oo\no o o o o o o o oo\no ooo\noo o oo\noo\no oo\noo\no oo o o ooo oo\noo\no\nooooo\no\no oo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\no oo\no o o o o o o o oo\no ooo\no o o oo\noo\no oo\noo\no o o o o ooooo\noo\no\nooooo\no\no oo\noo\no\noo\noooo\nooo\no\nooo\no\nooo\nooo\nooo\noo\no\noo\noo\noo\noooo\nooo\no\noo\no\n0 20 60 100\n0 20 60 100pgg45\nFIGURE 1.1. Scatterplot matrix of the prostate cancer data. The \ufb01rst row sho ws\nthe response against each of the predictors in turn. Two of the pr edictors, sviand\ngleason , are categorical.\nFor this problem not all errors are equal; we want to avoid \ufb01ltering out\ngood email, while letting spam get through is not desirable but less serious\nin its consequences. We discuss a number of di\ufb00erent methods for tackling\nthis learning problem in the book.\nExample 2: Prostate Cancer\nThe data for this example, displayed in Figure 1.11, come from a study\nby Stamey et al. (1989) that examined the correlation between the level of\n1There was an error in these data in the \ufb01rst edition of this boo k. Subject 32 had\na value of 6.1 for lweight , which translates to a 449 gm prostate! The correct value is\n44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.", "22": "4 1. Introduction\nFIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.\nprostate speci\ufb01c antigen (PSA) and a number of clinical measures, in 97\nmen who were about to receive a radical prostatectomy.\nThe goal is to predict the log of PSA ( lpsa) from a number of measure-\nments including log cancer volume ( lcavol ), log prostate weight lweight ,\nage, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-\nvasionsvi, log of capsular penetration lcp, Gleason score gleason , and\npercent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix\nof the variables. Some correlations with lpsaare evident, but a good pre-\ndictive model is di\ufb03cult to construct by eye.\nThis is a supervised learning problem, known as a regression problem ,\nbecause the outcome measurement is quantitative.\nExample 3: Handwritten Digit Recognition\nThe data from this example come from the handwritten ZIP codes on\nenvelopes from U.S. postal mail. Each image is a segment from a \ufb01ve digi t\nZIP code, isolating a single digit. The images are 16 \u00d716 eight-bit grayscale\nmaps, with each pixel ranging in intensity from 0 to 255. Some sample\nimages are shown in Figure 1.2.\nThe images have been normalized to have approximately the same size\nand orientation. The task is to predict, from the 16 \u00d716 matrix of pixel\nintensities, the identity of each image (0 ,1,... ,9) quickly and accurately. If\nit is accurate enough, the resulting algorithm would be used as part of an\nautomatic sorting procedure for envelopes. This is a classi\ufb01cation problem\nfor which the error rate needs to be kept very low to avoid misdirection of", "23": "1. Introduction 5\nmail. In order to achieve this low error rate, some objects can be assigned\nto a \u201cdon\u2019t know\u201d category, and sorted instead by hand.\nExample 4: DNA Expression Microarrays\nDNA stands for deoxyribonucleic acid, and is the basic material that makes\nup human chromosomes. DNA microarrays measure the expression of a\ngene in a cell by measuring the amount of mRNA (messenger ribonucleic\nacid) present for that gene. Microarrays are considered a breakthrough\ntechnology in biology, facilitating the quantitative study of thousands of\ngenes simultaneously from a single sample of cells.\nHere is how a DNA microarray works. The nucleotide sequences for a few\nthousand genes are printed on a glass slide. A target sample and a reference\nsample are labeled with red and green dyes, and each are hybridized with\nthe DNA on the slide. Through \ufb02uoroscopy, the log (red/green) intensities\nof RNA hybridizing at each site is measured. The result is a few thousand\nnumbers, typically ranging from say \u22126 to 6, measuring the expression level\nof each gene in the target relative to the reference sample. Positive values\nindicate higher expression in the target versus the reference, and vice versa\nfor negative values.\nA gene expression dataset collects together the expression values from a\nseries of DNA microarray experiments, with each column representing an\nexperiment. There are therefore several thousand rows representing individ-\nual genes, and tens of columns representing samples: in the particular ex-\nample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\nalthough for clarity only a random sample of 100 rows are shown. The \ufb01g-\nure displays the data set as a heat map, ranging from green (negative) to\nred (positive). The samples are 64 cancer tumors from di\ufb00erent patients.\nThe challenge here is to understand how the genes and samples are or-\nganized. Typical questions include the following:\n(a) which samples are most similar to each other, in terms of their expres-\nsion pro\ufb01les across genes?\n(b) which genes are most similar to each other, in terms of their expression\npro\ufb01les across samples?\n(c) do certain genes show very high (or low) expression for certain cancer\nsamples?\nWe could view this task as a regression problem, with two categorical\npredictor variables\u2014genes and samples\u2014with the response variable being\nthe level of expression. However, it is probably more useful to view it as\nunsupervised learning problem. For example, for question (a) above, we\nthink of the samples as points in 6830\u2013dimensional space, which we want\ntocluster together in some way.", "24": "6 1. Introduction\nSID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW510534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST\nRENAL\nMELANOMAMELANOMA\nMCF7D-repro\nCOLONCOLON\nK562B-repro\nCOLON\nNSCLC\nLEUKEMIA\nRENAL\nMELANOMA\nBREAST\nCNSCNS\nRENAL\nMCF7A-repro\nNSCLC\nK562A-repro\nCOLON\nCNS\nNSCLCNSCLC\nLEUKEMIA\nCNS\nOVARIAN\nBREAST\nLEUKEMIA\nMELANOMAMELANOMA\nOVARIANOVARIAN\nNSCLC\nRENAL\nBREAST\nMELANOMA\nOVARIANOVARIAN\nNSCLC\nRENAL\nBREAST\nMELANOMA\nLEUKEMIA\nCOLON\nBREAST\nLEUKEMIA\nCOLON\nCNS\nMELANOMA\nNSCLC\nPROSTATE\nNSCLC\nRENALRENAL\nNSCLC\nRENAL\nLEUKEMIA\nOVARIAN\nPROSTATE\nCOLON\nBREAST\nRENAL\nUNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)\nand64samples (columns), for the human tumor data. Only a random sampl e\nof100rows are shown. The display is a heat map, ranging from bright gre en\n(negative, under expressed) to bright red (positive, over expre ssed). Missing values\nare gray. The rows and columns are displayed in a randomly chosen order.", "25": "1. Introduction 7\nWho Should Read this Book\nThis book is designed for researchers and students in a broad variety of\n\ufb01elds: statistics, arti\ufb01cial intelligence, engineering, \ufb01nance and others. We\nexpect that the reader will have had at least one elementary course in\nstatistics, covering basic topics including linear regression.\nWe have not attempted to write a comprehensive catalog of learning\nmethods, but rather to describe some of the most important techniques.\nEqually notable, we describe the underlying concepts and considerations\nby which a researcher can judge a learning method. We have tried to write\nthis book in an intuitive fashion, emphasizing concepts rather than math-\nematical details.\nAs statisticians, our exposition will naturally re\ufb02ect our backgrounds and\nareas of expertise. However in the past eight years we have been attending\nconferences in neural networks, data mining and machine learning, and our\nthinking has been heavily in\ufb02uenced by these exciting \ufb01elds. This in\ufb02uence\nis evident in our current research, and in this book.\nHow This Book is Organized\nOur view is that one must understand simple methods before trying to\ngrasp more complex ones. Hence, after giving an overview of the supervis-\ning learning problem in Chapter 2 , we discuss linear methods for regression\nand classi\ufb01cation in Chapters 3 and4. InChapter 5 we describe splines,\nwavelets and regularization/penalization methods for a single predictor,\nwhile Chapter 6 covers kernel methods and local regression. Both of these\nsets of methods are important building blocks for high-dimensional learn-\ning techniques. Model assessment and selection is the topic of Chapter 7 ,\ncovering the concepts of bias and variance, over\ufb01tting and methods such as\ncross-validation for choosing models. Chapter 8 discusses model inference\nand averaging, including an overview of maximum likelihood, Bayesian in-\nference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\nA related procedure called boosting is the focus of Chapter 10 .\nInChapters 9\u201313 we describe a series of structured methods for su-\npervised learning, with Chapters 9 and 11 covering regression and Chap-\nters 12 and 13 focusing on classi\ufb01cation. Chapter 14 describes methods for\nunsupervised learning. Two recently proposed techniques, random forests\nand ensemble learning, are discussed in Chapters 15 and 16 . We describe\nundirected graphical models in Chapter 17 and \ufb01nally we study high-\ndimensional problems in Chapter 18 .\nAt the end of each chapter we discuss computational considerations im-\nportant for data mining applications, including how the computations scale\nwith the number of observations and predictors. Each chapter ends with\nBibliographic Notes giving background references for the material.", "26": "8 1. Introduction\nWe recommend that Chapters 1\u20134 be \ufb01rst read in sequence. Chapter 7\nshould also be considered mandatory, as it covers central concepts that\npertain to all learning methods. With this in mind, the rest of the book\ncan be read sequentially, or sampled, depending on the reader\u2019s interest.\nThe symbol\n indicates a technically di\ufb03cult section, one that can\nbe skipped without interrupting the \ufb02ow of the discussion.\nBook Website\nThe website for this book is located at\nhttp://www-stat.stanford.edu/ElemStatLearn\nIt contains a number of resources, including many of the datasets used in\nthis book.\nNote for Instructors\nWe have successively used the \ufb01rst edition of this book as the basis for a\ntwo-quarter course, and with the additional materials in this second edition,\nit could even be used for a three-quarter sequence. Exercises are provided at\nthe end of each chapter. It is important for students to have access to good\nsoftware tools for these topics. We used the R and S-PLUS programming\nlanguages in our courses.", "27": "This is page 9\nPrinter: Opaque this\n2\nOverview of Supervised Learning\n2.1 Introduction\nThe \ufb01rst three examples described in Chapter 1 have several components\nin common. For each there is a set of variables that might be denoted as\ninputs , which are measured or preset. These have some in\ufb02uence on one or\nmoreoutputs . For each example the goal is to use the inputs to predict the\nvalues of the outputs. This exercise is called supervised learning .\nWe have used the more modern language of machine learning. In the\nstatistical literature the inputs are often called the predictors , a term we\nwill use interchangeably with inputs, and more classically the independent\nvariables . In the pattern recognition literature the term features is preferred,\nwhich we use as well. The outputs are called the responses , or classically\nthedependent variables .\n2.2 Variable Types and Terminology\nThe outputs vary in nature among the examples. In the glucose prediction\nexample, the output is a quantitative measurement, where some measure-\nments are bigger than others, and measurements close in value are close\nin nature. In the famous Iris discrimination example due to R. A. Fisher,\nthe output is qualitative (species of Iris) and assumes values in a \ufb01nite set\nG={Virginica ,Setosa andVersicolor }. In the handwritten digit example\nthe output is one of 10 di\ufb00erent digit classes :G={0,1,... ,9}. In both of", "28": "10 2. Overview of Supervised Learning\nthese there is no explicit ordering in the classes, and in fact often descrip-\ntive labels rather than numbers are used to denote the classes. Qualitative\nvariables are also referred to as categorical ordiscrete variables as well as\nfactors .\nFor both types of outputs it makes sense to think of using the inputs to\npredict the output. Given some speci\ufb01c atmospheric measurements today\nand yesterday, we want to predict the ozone level tomorrow. Given the\ngrayscale values for the pixels of the digitized image of the handwritten\ndigit, we want to predict its class label.\nThis distinction in output type has led to a naming convention for the\nprediction tasks: regression when we predict quantitative outputs, and clas-\nsi\ufb01cation when we predict qualitative outputs. We will see that these two\ntasks have a lot in common, and in particular both can be viewed as a task\nin function approximation.\nInputs also vary in measurement type; we can have some of each of qual-\nitative and quantitative input variables. These have also led to distinctio ns\nin the types of methods that are used for prediction: some methods are\nde\ufb01ned most naturally for quantitative inputs, some most naturally for\nqualitative and some for both.\nA third variable type is ordered categorical , such as small, medium and\nlarge, where there is an ordering between the values, but no metric notion\nis appropriate (the di\ufb00erence between medium and small need not be the\nsame as that between large and medium). These are discussed further in\nChapter 4.\nQualitative variables are typically represented numerically by codes. The\neasiest case is when there are only two classes or categories, such as \u201csuc-\ncess\u201d or \u201cfailure,\u201d \u201csurvived\u201d or \u201cdied.\u201d These are often represented by a\nsingle binary digit or bit as 0 or 1, or else by \u22121 and 1. For reasons that will\nbecome apparent, such numeric codes are sometimes referred to as targets.\nWhen there are more than two categories, several alternatives are available.\nThe most useful and commonly used coding is via dummy variables . Here a\nK-level qualitative variable is represented by a vector of Kbinary variables\nor bits, only one of which is \u201con\u201d at a time. Although more compact coding\nschemes are possible, dummy variables are symmetric in the levels of the\nfactor.\nWe will typically denote an input variable by the symbol X. IfXis\na vector, its components can be accessed by subscripts Xj. Quantitative\noutputs will be denoted by Y, and qualitative outputs by G(for group).\nWe use uppercase letters such as X,YorGwhen referring to the generic\naspects of a variable. Observed values are written in lowercase; hence the\nith observed value of Xis written as xi(where xiis again a scalar or\nvector). Matrices are represented by bold uppercase letters; for example, a\nset of Ninput p-vectors xi, i= 1,... ,N would be represented by the N\u00d7p\nmatrix X. In general, vectors will not be bold, except when they have N\ncomponents; this convention distinguishes a p-vector of inputs xifor the", "29": "2.3 Least Squares and Nearest Neighbors 11\nith observation from the N-vector xjconsisting of all the observations on\nvariable Xj. Since all vectors are assumed to be column vectors, the ith\nrow of XisxT\ni, the vector transpose of xi.\nFor the moment we can loosely state the learning task as follows: given\nthe value of an input vector X, make a good prediction of the output Y,\ndenoted by \u02c6Y(pronounced \u201cy-hat\u201d). If Ytakes values in IR then so should\n\u02c6Y; likewise for categorical outputs, \u02c6Gshould take values in the same set G\nassociated with G.\nFor a two-class G, one approach is to denote the binary coded target\nasY, and then treat it as a quantitative output. The predictions \u02c6Ywill\ntypically lie in [0 ,1], and we can assign to \u02c6Gthe class label according to\nwhether \u02c6 y >0.5. This approach generalizes to K-level qualitative outputs\nas well.\nWe need data to construct prediction rules, often a lot of it. We thus\nsuppose we have available a set of measurements ( xi,yi) or ( xi,gi), i=\n1,... ,N , known as the training data , with which to construct our prediction\nrule.\n2.3 Two Simple Approaches to Prediction: Least\nSquares and Nearest Neighbors\nIn this section we develop two simple but powerful prediction methods: the\nlinear model \ufb01t by least squares and the k-nearest-neighbor prediction rule.\nThe linear model makes huge assumptions about structure and yields stable\nbut possibly inaccurate predictions. The method of k-nearest neighbors\nmakes very mild structural assumptions: its predictions are often accurate\nbut can be unstable.\n2.3.1 Linear Models and Least Squares\nThe linear model has been a mainstay of statistics for the past 30 years\nand remains one of our most important tools. Given a vector of inputs\nXT= (X1,X2,... ,X p), we predict the output Yvia the model\n\u02c6Y=\u02c6\u03b20+p/summationdisplay\nj=1Xj\u02c6\u03b2j. (2.1)\nThe term \u02c6\u03b20is the intercept, also known as the biasin machine learning.\nOften it is convenient to include the constant variable 1 in X, include \u02c6\u03b20in\nthe vector of coe\ufb03cients \u02c6\u03b2, and then write the linear model in vector form\nas an inner product\n\u02c6Y=XT\u02c6\u03b2, (2.2)", "30": "12 2. Overview of Supervised Learning\nwhere XTdenotes vector or matrix transpose ( Xbeing a column vector).\nHere we are modeling a single output, so \u02c6Yis a scalar; in general \u02c6Ycan be\naK\u2013vector, in which case \u03b2would be a p\u00d7Kmatrix of coe\ufb03cients. In the\n(p+ 1)-dimensional input\u2013output space, ( X,\u02c6Y) represents a hyperplane.\nIf the constant is included in X, then the hyperplane includes the origin\nand is a subspace; if not, it is an a\ufb03ne set cutting the Y-axis at the point\n(0,\u02c6\u03b20). From now on we assume that the intercept is included in \u02c6\u03b2.\nViewed as a function over the p-dimensional input space, f(X) =XT\u03b2\nis linear, and the gradient f\u2032(X) =\u03b2is a vector in input space that points\nin the steepest uphill direction.\nHow do we \ufb01t the linear model to a set of training data? There are\nmany di\ufb00erent methods, but by far the most popular is the method of\nleast squares . In this approach, we pick the coe\ufb03cients \u03b2to minimize the\nresidual sum of squares\nRSS(\u03b2) =N/summationdisplay\ni=1(yi\u2212xT\ni\u03b2)2. (2.3)\nRSS(\u03b2) is a quadratic function of the parameters, and hence its minimum\nalways exists, but may not be unique. The solution is easiest to characterize\nin matrix notation. We can write\nRSS(\u03b2) = (y\u2212X\u03b2)T(y\u2212X\u03b2), (2.4)\nwhere Xis an N\u00d7pmatrix with each row an input vector, and yis an\nN-vector of the outputs in the training set. Di\ufb00erentiating w.r.t. \u03b2we get\nthenormal equations\nXT(y\u2212X\u03b2) = 0. (2.5)\nIfXTXis nonsingular, then the unique solution is given by\n\u02c6\u03b2= (XTX)\u22121XTy, (2.6)\nand the \ufb01tted value at the ith input xiis \u02c6yi= \u02c6y(xi) =xT\ni\u02c6\u03b2. At an arbi-\ntrary input x0the prediction is \u02c6 y(x0) =xT\n0\u02c6\u03b2. The entire \ufb01tted surface is\ncharacterized by the pparameters \u02c6\u03b2. Intuitively, it seems that we do not\nneed a very large data set to \ufb01t such a model.\nLet\u2019s look at an example of the linear model in a classi\ufb01cation context.\nFigure 2.1 shows a scatterplot of training data on a pair of inputs X1and\nX2. The data are simulated, and for the present the simulation model is\nnot important. The output class variable Ghas the values BLUEorORANGE ,\nand is represented as such in the scatterplot. There are 100 points in each\nof the two classes. The linear regression model was \ufb01t to these data, with\nthe response Ycoded as 0 for BLUEand 1 for ORANGE . The \ufb01tted values \u02c6Y\nare converted to a \ufb01tted class variable \u02c6Gaccording to the rule\n\u02c6G=/braceleft\uf8ecigg\nORANGE if\u02c6Y >0.5,\nBLUE if\u02c6Y\u22640.5.(2.7)", "31": "2.3 Least Squares and Nearest Neighbors 13\nLinear Regression of 0/1 Response\n.. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nFIGURE 2.1. A classi\ufb01cation example in two dimensions. The classes are coded\nas a binary variable ( BLUE= 0,ORANGE = 1), and then \ufb01t by linear regression.\nThe line is the decision boundary de\ufb01ned by xT\u02c6\u03b2= 0.5. The orange shaded region\ndenotes that part of input space classi\ufb01ed as ORANGE , while the blue region is\nclassi\ufb01ed as BLUE.\nThe set of points in IR2classi\ufb01ed as ORANGE corresponds to {x:xT\u02c6\u03b2 >0.5},\nindicated in Figure 2.1, and the two predicted classes are separated by the\ndecision boundary {x:xT\u02c6\u03b2= 0.5}, which is linear in this case. We see\nthat for these data there are several misclassi\ufb01cations on both sides of the\ndecision boundary. Perhaps our linear model is too rigid\u2014 or are such errors\nunavoidable? Remember that these are errors on the training data itself,\nand we have not said where the constructed data came from. Consider the\ntwo possible scenarios:\nScenario 1: The training data in each class were generated from bivariate\nGaussian distributions with uncorrelated components and di\ufb00erent\nmeans.\nScenario 2: The training data in each class came from a mixture of 10 low-\nvariance Gaussian distributions, with individual means themselves\ndistributed as Gaussian.\nA mixture of Gaussians is best described in terms of the generative\nmodel. One \ufb01rst generates a discrete variable that determines which of", "32": "14 2. Overview of Supervised Learning\nthe component Gaussians to use, and then generates an observation from\nthe chosen density. In the case of one Gaussian per class, we will see in\nChapter 4 that a linear decision boundary is the best one can do, and that\nour estimate is almost optimal. The region of overlap is inevitable, and\nfuture data to be predicted will be plagued by this overlap as well.\nIn the case of mixtures of tightly clustered Gaussians the story is dif-\nferent. A linear decision boundary is unlikely to be optimal, and in fact is\nnot. The optimal decision boundary is nonlinear and disjoint, and as such\nwill be much more di\ufb03cult to obtain.\nWe now look at another classi\ufb01cation and regression procedure that is\nin some sense at the opposite end of the spectrum to the linear model, and\nfar better suited to the second scenario.\n2.3.2 Nearest-Neighbor Methods\nNearest-neighbor methods use those observations in the training set Tclos-\nest in input space to xto form \u02c6Y. Speci\ufb01cally, the k-nearest neighbor \ufb01t\nfor\u02c6Yis de\ufb01ned as follows:\n\u02c6Y(x) =1\nk/summationdisplay\nxi\u2208Nk(x)yi, (2.8)\nwhere Nk(x) is the neighborhood of xde\ufb01ned by the kclosest points xiin\nthe training sample. Closeness implies a metric, which for the moment we\nassume is Euclidean distance. So, in words, we \ufb01nd the kobservations with\nxiclosest to xin input space, and average their responses.\nIn Figure 2.2 we use the same training data as in Figure 2.1, and use\n15-nearest-neighbor averaging of the binary coded response as the method\nof \ufb01tting. Thus \u02c6Yis the proportion of ORANGE \u2019s in the neighborhood, and\nso assigning class ORANGE to\u02c6Gif\u02c6Y >0.5 amounts to a majority vote in\nthe neighborhood. The colored regions indicate all those points in input\nspace classi\ufb01ed as BLUEorORANGE by such a rule, in this case found by\nevaluating the procedure on a \ufb01ne grid in input space. We see that the\ndecision boundaries that separate the BLUEfrom the ORANGE regions are far\nmore irregular, and respond to local clusters where one class dominates.\nFigure 2.3 shows the results for 1-nearest-neighbor classi\ufb01cation: \u02c6Yis\nassigned the value y\u2113of the closest point x\u2113toxin the training data. In\nthis case the regions of classi\ufb01cation can be computed relatively easily, and\ncorrespond to a Voronoi tessellation of the training data. Each point xi\nhas an associated tile bounding the region for which it is the closest input\npoint. For all points xin the tile, \u02c6G(x) =gi. The decision boundary is even\nmore irregular than before.\nThe method of k-nearest-neighbor averaging is de\ufb01ned in exactly the\nsame way for regression of a quantitative output Y, although k= 1 would\nbe an unlikely choice.", "33": "2.3 Least Squares and Nearest Neighbors 15\n15-Nearest Neighbor Classifier\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .\n... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nFIGURE 2.2. The same classi\ufb01cation example in two dimensions as in Fig-\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1)and\nthen \ufb01t by 15-nearest-neighbor averaging as in (2.8). The predicted class i s hence\nchosen by majority vote amongst the 15-nearest neighbors.\nIn Figure 2.2 we see that far fewer training observations are misclassi\ufb01ed\nthan in Figure 2.1. This should not give us too much comfort, though, since\nin Figure 2.3 noneof the training data are misclassi\ufb01ed. A little thought\nsuggests that for k-nearest-neighbor \ufb01ts, the error on the training data\nshould be approximately an increasing function of k, and will always be 0\nfork= 1. An independent test set would give us a more satisfactory means\nfor comparing the di\ufb00erent methods.\nIt appears that k-nearest-neighbor \ufb01ts have a single parameter, the num-\nber of neighbors k, compared to the pparameters in least-squares \ufb01ts. Al-\nthough this is the case, we will see that the e\ufb00ective number of parameters\nofk-nearest neighbors is N/kand is generally bigger than p, and decreases\nwith increasing k. To get an idea of why, note that if the neighborhoods\nwere nonoverlapping, there would be N/kneighborhoods and we would \ufb01t\none parameter (a mean) in each neighborhood.\nIt is also clear that we cannot use sum-of-squared errors on the training\nset as a criterion for picking k, since we would always pick k= 1! It would\nseem that k-nearest-neighbor methods would be more appropriate for the\nmixture Scenario 2 described above, while for Gaussian data the decision\nboundaries of k-nearest neighbors would be unnecessarily noisy.", "34": "16 2. Overview of Supervised Learning\n1\u2212Nearest Neighbor Classifier\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\nooooo\nooo\no\no\nooo\noooooo\noo\no\noo\nooo\no\nFIGURE 2.3. The same classi\ufb01cation example in two dimensions as in Fig-\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1), and\nthen predicted by 1-nearest-neighbor classi\ufb01cation.\n2.3.3 From Least Squares to Nearest Neighbors\nThe linear decision boundary from least squares is very smooth, and ap-\nparently stable to \ufb01t. It does appear to rely heavily on the assumption\nthat a linear decision boundary is appropriate. In language we will develop\nshortly, it has low variance and potentially high bias.\nOn the other hand, the k-nearest-neighbor procedures do not appear to\nrely on any stringent assumptions about the underlying data, and can adapt\nto any situation. However, any particular subregion of the decision bound-\nary depends on a handful of input points and their particular positions,\nand is thus wiggly and unstable\u2014high variance and low bias.\nEach method has its own situations for which it works best; in particular\nlinear regression is more appropriate for Scenario 1 above, while nearest\nneighbors are more suitable for Scenario 2. The time has come to expose\nthe oracle! The data in fact were simulated from a model somewhere be-\ntween the two, but closer to Scenario 2. First we generated 10 means mk\nfrom a bivariate Gaussian distribution N((1,0)T,I) and labeled this class\nBLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class\nORANGE . Then for each class we generated 100 observations as follows: for\neach observation, we picked an mkat random with probability 1 /10, and", "35": "2.3 Least Squares and Nearest Neighbors 17\nDegrees of Freedom \u2212 N/kTest Error\n0.10 0.15 0.20 0.25 0.30\n  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1\nTrain\nTest\nBayesk \u2212  Number of Nearest Neighbors\nLinear\nFIGURE 2.4. Misclassi\ufb01cation curves for the simulation example used in Fi g-\nures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test\nsample of size 10,000. The orange curves are test and the blue are training er-\nror for k-nearest-neighbor classi\ufb01cation. The results for linear regres sion are the\nbigger orange and blue squares at three degrees of freedom. The purple line is the\noptimal Bayes error rate.\nthen generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-\nters for each class. Figure 2.4 shows the results of classifying 10,000 new\nobservations generated from the model. We compare the results for least\nsquares and those for k-nearest neighbors for a range of values of k.\nA large subset of the most popular techniques in use today are variants of\nthese two simple procedures. In fact 1-nearest-neighbor, the simplest of all,\ncaptures a large percentage of the market for low-dimensional problems.\nThe following list describes some ways in which these simple procedures\nhave been enhanced:\n\u2022Kernel methods use weights that decrease smoothly to zero with dis-\ntance from the target point, rather than the e\ufb00ective 0 /1 weights used\nbyk-nearest neighbors.\n\u2022In high-dimensional spaces the distance kernels are modi\ufb01ed to em-\nphasize some variable more than others.", "36": "18 2. Overview of Supervised Learning\n\u2022Local regression \ufb01ts linear models by locally weighted least squares,\nrather than \ufb01tting constants locally.\n\u2022Linear models \ufb01t to a basis expansion of the original inputs allow\narbitrarily complex models.\n\u2022Projection pursuit and neural network models consist of sums of non-\nlinearly transformed linear models.\n2.4 Statistical Decision Theory\nIn this section we develop a small amount of theory that provides a frame-\nwork for developing models such as those discussed informally so far. We\n\ufb01rst consider the case of a quantitative output, and place ourselves in the\nworld of random variables and probability spaces. Let X\u2208IRpdenote a\nreal valued random input vector, and Y\u2208IR a real valued random out-\nput variable, with joint distribution Pr( X,Y). We seek a function f(X)\nfor predicting Ygiven values of the input X. This theory requires a loss\nfunction L(Y,f(X)) for penalizing errors in prediction, and by far the most\ncommon and convenient is squared error loss :L(Y,f(X)) = ( Y\u2212f(X))2.\nThis leads us to a criterion for choosing f,\nEPE(f) = E( Y\u2212f(X))2(2.9)\n=/integraldisplay\n[y\u2212f(x)]2Pr(dx,dy ), (2.10)\nthe expected (squared) prediction error . By conditioning1onX, we can\nwrite EPE as\nEPE(f) = E XEY|X/parenleftbig\n[Y\u2212f(X)]2|X/parenrightbig\n(2.11)\nand we see that it su\ufb03ces to minimize EPE pointwise:\nf(x) = argmincEY|X/parenleftbig\n[Y\u2212c]2|X=x/parenrightbig\n. (2.12)\nThe solution is\nf(x) = E( Y|X=x), (2.13)\nthe conditional expectation, also known as the regression function. Thus\nthe best prediction of Yat any point X=xis the conditional mean, when\nbest is measured by average squared error.\nThe nearest-neighbor methods attempt to directly implement this recipe\nusing the training data. At each point x, we might ask for the average of all\n1Conditioning here amounts to factoring the joint density Pr (X, Y) = Pr( Y|X)Pr(X)\nwhere Pr( Y|X) = Pr( Y, X)/Pr(X), and splitting up the bivariate integral accordingly.", "37": "2.4 Statistical Decision Theory 19\nthose yis with input xi=x. Since there is typically at most one observation\nat any point x, we settle for\n\u02c6f(x) = Ave( yi|xi\u2208Nk(x)), (2.14)\nwhere \u201cAve\u201d denotes average, and Nk(x) is the neighborhood containing\nthekpoints in T closest to x. Two approximations are happening here:\n\u2022expectation is approximated by averaging over sample data;\n\u2022conditioning at a point is relaxed to conditioning on some region\n\u201cclose\u201d to the target point.\nFor large training sample size N, the points in the neighborhood are likely\nto be close to x, and as kgets large the average will get more stable.\nIn fact, under mild regularity conditions on the joint probability distri-\nbution Pr( X,Y), one can show that as N,k\u2192 \u221e such that k/N\u21920,\n\u02c6f(x)\u2192E(Y|X=x). In light of this, why look further, since it seems\nwe have a universal approximator? We often do not have very large sam-\nples. If the linear or some more structured model is appropriate, then we\ncan usually get a more stable estimate than k-nearest neighbors, although\nsuch knowledge has to be learned from the data as well. There are other\nproblems though, sometimes disastrous. In Section 2.5 we see that as the\ndimension pgets large, so does the metric size of the k-nearest neighbor-\nhood. So settling for nearest neighborhood as a surrogate for conditioning\nwill fail us miserably. The convergence above still holds, but the rateof\nconvergence decreases as the dimension increases.\nHow does linear regression \ufb01t into this framework? The simplest explana-\ntion is that one assumes that the regression function f(x) is approximately\nlinear in its arguments:\nf(x)\u2248xT\u03b2. (2.15)\nThis is a model-based approach\u2014we specify a model for the regression func-\ntion. Plugging this linear model for f(x) into EPE (2.9) and di\ufb00erentiating\nwe can solve for \u03b2theoretically:\n\u03b2= [E(XXT)]\u22121E(XY). (2.16)\nNote we have notconditioned on X; rather we have used our knowledge\nof the functional relationship to poolover values of X. The least squares\nsolution (2.6) amounts to replacing the expectation in (2.16) by averages\nover the training data.\nSo both k-nearest neighbors and least squares end up approximating\nconditional expectations by averages. But they di\ufb00er dramatically in terms\nof model assumptions:\n\u2022Least squares assumes f(x) is well approximated by a globally linear\nfunction.", "38": "20 2. Overview of Supervised Learning\n\u2022k-nearest neighbors assumes f(x) is well approximated by a locally\nconstant function.\nAlthough the latter seems more palatable, we have already seen that we\nmay pay a price for this \ufb02exibility.\nMany of the more modern techniques described in this book are model\nbased, although far more \ufb02exible than the rigid linear model. For example,\nadditive models assume that\nf(X) =p/summationdisplay\nj=1fj(Xj). (2.17)\nThis retains the additivity of the linear model, but each coordinate function\nfjis arbitrary. It turns out that the optimal estimate for the additive model\nuses techniques such as k-nearest neighbors to approximate univariate con-\nditional expectations simultaneously for each of the coordinate functions.\nThus the problems of estimating a conditional expectation in high dimen-\nsions are swept away in this case by imposing some (often unrealistic) model\nassumptions, in this case additivity.\nAre we happy with the criterion (2.11)? What happens if we replace the\nL2loss function with the L1:E|Y\u2212f(X)|? The solution in this case is the\nconditional median,\n\u02c6f(x) = median( Y|X=x), (2.18)\nwhich is a di\ufb00erent measure of location, and its estimates are more robust\nthan those for the conditional mean. L1criteria have discontinuities in\ntheir derivatives, which have hindered their widespread use. Other more\nresistant loss functions will be mentioned in later chapters, but squared\nerror is analytically convenient and the most popular.\nWhat do we do when the output is a categorical variable G? The same\nparadigm works here, except we need a di\ufb00erent loss function for penalizing\nprediction errors. An estimate \u02c6Gwill assume values in G, the set of possible\nclasses. Our loss function can be represented by a K\u00d7Kmatrix L, where\nK= card( G).Lwill be zero on the diagonal and nonnegative elsewhere,\nwhere L(k,\u2113) is the price paid for classifying an observation belonging to\nclassGkasG\u2113. Most often we use the zero\u2013one loss function, where all\nmisclassi\ufb01cations are charged a single unit. The expected prediction error\nis\nEPE = E[ L(G,\u02c6G(X))], (2.19)\nwhere again the expectation is taken with respect to the joint distribution\nPr(G,X). Again we condition, and can write EPE as\nEPE = E XK/summationdisplay\nk=1L[Gk,\u02c6G(X)]Pr(Gk|X) (2.20)", "39": "2.4 Statistical Decision Theory 21\nBayes Optimal Classifier\n... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nFIGURE 2.5. The optimal Bayes decision boundary for the simulation exampl e\nof Figures 2.1, 2.2 and 2.3. Since the generating density is known for each class,\nthis boundary can be calculated exactly (Exercise 2.2).\nand again it su\ufb03ces to minimize EPE pointwise:\n\u02c6G(x) = argming\u2208GK/summationdisplay\nk=1L(Gk,g)Pr(Gk|X=x). (2.21)\nWith the 0\u20131 loss function this simpli\ufb01es to\n\u02c6G(x) = argming\u2208G[1\u2212Pr(g|X=x)] (2.22)\nor simply\n\u02c6G(X) =Gkif Pr(Gk|X=x) = max\ng\u2208GPr(g|X=x). (2.23)\nThis reasonable solution is known as the Bayes classi\ufb01er , and says that\nwe classify to the most probable class, using the conditional (discrete) dis-\ntribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary\nfor our simulation example. The error rate of the Bayes classi\ufb01er is called\ntheBayes rate .", "40": "22 2. Overview of Supervised Learning\nAgain we see that the k-nearest neighbor classi\ufb01er directly approximates\nthis solution\u2014a majority vote in a nearest neighborhood amounts to ex-\nactly this, except that conditional probability at a point is relaxed to con-\nditional probability within a neighborhood of a point, and probabilities ar e\nestimated by training-sample proportions.\nSuppose for a two-class problem we had taken the dummy-variable ap-\nproach and coded Gvia a binary Y, followed by squared error loss estima-\ntion. Then \u02c6f(X) = E( Y|X) = Pr( G=G1|X) ifG1corresponded to Y= 1.\nLikewise for a K-class problem, E( Yk|X) = Pr( G=Gk|X). This shows\nthat our dummy-variable regression procedure, followed by classi\ufb01cation to\nthe largest \ufb01tted value, is another way of representing the Bayes classi\ufb01er.\nAlthough this theory is exact, in practice problems can occur, depending\non the regression model used. For example, when linear regression is used,\n\u02c6f(X) need not be positive, and we might be suspicious about using it as\nan estimate of a probability. We will discuss a variety of approaches to\nmodeling Pr( G|X) in Chapter 4.\n2.5 Local Methods in High Dimensions\nWe have examined two learning techniques for prediction so far: the stable\nbut biased linear model and the less stable but apparently less biased class\nofk-nearest-neighbor estimates. It would seem that with a reasonably large\nset of training data, we could always approximate the theoretically optimal\nconditional expectation by k-nearest-neighbor averaging, since we should\nbe able to \ufb01nd a fairly large neighborhood of observations close to any x\nand average them. This approach and our intuition breaks down in high\ndimensions, and the phenomenon is commonly referred to as the curse\nof dimensionality (Bellman, 1961). There are many manifestations of this\nproblem, and we will examine a few here.\nConsider the nearest-neighbor procedure for inputs uniformly distributed\nin ap-dimensional unit hypercube, as in Figure 2.6. Suppose we send out a\nhypercubical neighborhood about a target point to capture a fraction rof\nthe observations. Since this corresponds to a fraction rof the unit volume,\nthe expected edge length will be ep(r) =r1/p. In ten dimensions e10(0.01) =\n0.63 and e10(0.1) = 0 .80, while the entire range for each input is only 1 .0.\nSo to capture 1% or 10% of the data to form a local average, we must cover\n63% or 80% of the range of each input variable. Such neighborhoods are no\nlonger \u201clocal.\u201d Reducing rdramatically does not help much either, since\nthe fewer observations we average, the higher is the variance of our \ufb01t.\nAnother consequence of the sparse sampling in high dimensions is that\nall sample points are close to an edge of the sample. Consider Ndata points\nuniformly distributed in a p-dimensional unit ball centered at the origin.\nSuppose we consider a nearest-neighbor estimate at the origin. The median", "41": "2.5 Local Methods in High Dimensions 23\n1\n10Unit Cube\nFraction of VolumeDistance\n0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10\nNeighborhood\nFIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l\nneighborhood for uniform data in a unit cube. The \ufb01gure on the righ t shows the\nside-length of the subcube needed to capture a fraction rof the volume of the data,\nfor di\ufb00erent dimensions p. In ten dimensions we need to cover 80%of the range\nof each coordinate to capture 10%of the data.\ndistance from the origin to the closest data point is given by the expression\nd(p,N) =/parenleft\uf8ecig\n1\u22121\n21/N/parenright\uf8ecig1/p\n(2.24)\n(Exercise 2.3). A more complicated expression exists for the mean distance\nto the closest point. For N= 500, p= 10 , d(p,N)\u22480.52, more than\nhalfway to the boundary. Hence most data points are closer to the boundary\nof the sample space than to any other data point. The reason that this\npresents a problem is that prediction is much more di\ufb03cult near the edges\nof the training sample. One must extrapolate from neighboring sample\npoints rather than interpolate between them.\nAnother manifestation of the curse is that the sampling density is pro-\nportional to N1/p, where pis the dimension of the input space and Nis the\nsample size. Thus, if N1= 100 represents a dense sample for a single input\nproblem, then N10= 10010is the sample size required for the same sam-\npling density with 10 inputs. Thus in high dimensions all feasible training\nsamples sparsely populate the input space.\nLet us construct another uniform example. Suppose we have 1000 train-\ning examples xigenerated uniformly on [ \u22121,1]p. Assume that the true\nrelationship between XandYis\nY=f(X) =e\u22128||X||2,\nwithout any measurement error. We use the 1-nearest-neighbor rule to\npredict y0at the test-point x0= 0. Denote the training set by T. We can", "42": "24 2. Overview of Supervised Learning\ncompute the expected prediction error at x0for our procedure, averaging\nover all such samples of size 1000. Since the problem is deterministic, this\nis the mean squared error (MSE) for estimating f(0):\nMSE( x0) = E T[f(x0)\u2212\u02c6y0]2\n= E T[\u02c6y0\u2212ET(\u02c6y0)]2+ [ET(\u02c6y0)\u2212f(x0)]2\n= Var T(\u02c6y0) + Bias2(\u02c6y0). (2.25)\nFigure 2.7 illustrates the setup. We have broken down the MSE into two\ncomponents that will become familiar as we proceed: variance and squared\nbias. Such a decomposition is always possible and often useful, and is known\nas the bias\u2013variance decomposition . Unless the nearest neighbor is at 0,\n\u02c6y0will be smaller than f(0) in this example, and so the average estimate\nwill be biased downward. The variance is due to the sampling variance of\nthe 1-nearest neighbor. In low dimensions and with N= 1000, the nearest\nneighbor is very close to 0, and so both the bias and variance are small. As\nthe dimension increases, the nearest neighbor tends to stray further from\nthe target point, and both bias and variance are incurred. By p= 10, for\nmore than 99% of the samples the nearest neighbor is a distance greater\nthan 0 .5 from the origin. Thus as pincreases, the estimate tends to be 0\nmore often than not, and hence the MSE levels o\ufb00 at 1 .0, as does the bias,\nand the variance starts dropping (an artifact of this example).\nAlthough this is a highly contrived example, similar phenomena occur\nmore generally. The complexity of functions of many variables can grow\nexponentially with the dimension, and if we wish to be able to estimate\nsuch functions with the same accuracy as function in low dimensions, then\nwe need the size of our training set to grow exponentially as well. In this\nexample, the function is a complex interaction of all pvariables involved.\nThe dependence of the bias term on distance depends on the truth, and\nit need not always dominate with 1-nearest neighbor. For example, if the\nfunction always involves only a few dimensions as in Figure 2.8, then the\nvariance can dominate instead.\nSuppose, on the other hand, that we know that the relationship between\nYandXis linear,\nY=XT\u03b2+\u03b5, (2.26)\nwhere \u03b5\u223cN(0,\u03c32) and we \ufb01t the model by least squares to the train-\ning data. For an arbitrary test point x0, we have \u02c6 y0=xT\n0\u02c6\u03b2, which can\nbe written as \u02c6 y0=xT\n0\u03b2+/summationtextN\ni=1\u2113i(x0)\u03b5i, where \u2113i(x0) is the ith element\nofX(XTX)\u22121x0. Since under this model the least squares estimates are", "43": "2.5 Local Methods in High Dimensions 25\nXf(X)\n-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0\u20221-NN in One Dimension\nX1X2\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u20221-NN in One vs. Two Dimensions\nDimensionAverage Distance to Nearest Neighbor\n2 4 6 8 100.0 0.2 0.4 0.6 0.8\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Distance to 1-NN vs. Dimension\nDimensionMse\n2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022MSE vs. Dimension\n\u2022MSE\n\u2022Variance\n\u2022Sq. Bias\nFIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\nity and its e\ufb00ect on MSE, bias and variance. The input features are u niformly\ndistributed in [\u22121,1]pforp= 1, . . . ,10The top left panel shows the target func-\ntion (no noise) in I R:f(X) =e\u22128||X||2, and demonstrates the error that 1-nearest\nneighbor makes in estimating f(0). The training point is indicated by the blue tick\nmark. The top right panel illustrates why the radius of the 1-nearest neighborhood\nincreases with dimension p. The lower left panel shows the average radius of the\n1-nearest neighborhoods. The lower-right panel shows the MSE, sq uared bias and\nvariance curves as a function of dimension p.", "44": "26 2. Overview of Supervised Learning\nXf(X)\n-1.0 -0.5 0.0 0.5 1.00 1 2 3 4\u20221-NN in One Dimension\nDimensionMSE\n2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022MSE  vs. Dimension\n\u2022MSE\n\u2022Variance\n\u2022Sq. Bias\nFIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here\nthe function is constant in all but one dimension: F(X) =1\n2(X1+ 1)3. The\nvariance dominates.\nunbiased, we \ufb01nd that\nEPE(x0) = E y0|x0ET(y0\u2212\u02c6y0)2\n= Var( y0|x0) + E T[\u02c6y0\u2212ET\u02c6y0]2+ [ET\u02c6y0\u2212xT\n0\u03b2]2\n= Var( y0|x0) + Var T(\u02c6y0) + Bias2(\u02c6y0)\n=\u03c32+ ETxT\n0(XTX)\u22121x0\u03c32+ 02. (2.27)\nHere we have incurred an additional variance \u03c32in the prediction error,\nsince our target is not deterministic. There is no bias, and the variance\ndepends on x0. IfNis large and Twere selected at random, and assuming\nE(X) = 0, then XTX\u2192NCov(X) and\nEx0EPE(x0)\u223cEx0xT\n0Cov(X)\u22121x0\u03c32/N+\u03c32\n= trace[Cov( X)\u22121Cov(x0)]\u03c32/N+\u03c32\n=\u03c32(p/N) +\u03c32. (2.28)\nHere we see that the expected EPE increases linearly as a function of p,\nwith slope \u03c32/N. IfNis large and/or \u03c32is small, this growth in vari-\nance is negligible (0 in the deterministic case). By imposing some heavy\nrestrictions on the class of models being \ufb01tted, we have avoided the curse\nof dimensionality. Some of the technical details in (2.27) and (2.28) are\nderived in Exercise 2.5.\nFigure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\ntions, both of which have the form Y=f(X) +\u03b5,Xuniform as before,\nand\u03b5\u223cN(0,1). The sample size is N= 500. For the orange curve, f(x)", "45": "2.5 Local Methods in High Dimensions 27\nDimensionEPE Ratio\n2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Expected Prediction Error of 1NN vs. OLS\n\u2022Linear\n\u2022Cubic\nFIGURE 2.9. The curves show the expected prediction error (at x0= 0) for\n1-nearest neighbor relative to least squares for the model Y=f(X) +\u03b5. For the\norange curve, f(x) =x1, while for the blue curve f(x) =1\n2(x1+ 1)3.\nis linear in the \ufb01rst coordinate, for the blue curve, cubic as in Figure 2.8.\nShown is the relative EPE of 1-nearest neighbor to least squares, which\nappears to start at around 2 for the linear case. Least squares is unbiased\nin this case, and as discussed above the EPE is slightly above \u03c32= 1.\nThe EPE for 1-nearest neighbor is always above 2, since the variance of\n\u02c6f(x0) in this case is at least \u03c32, and the ratio increases with dimension as\nthe nearest neighbor strays from the target point. For the cubic case, least\nsquares is biased, which moderates the ratio. Clearly we could manufacture\nexamples where the bias of least squares would dominate the variance, and\nthe 1-nearest neighbor would come out the winner.\nBy relying on rigid assumptions, the linear model has no bias at all and\nnegligible variance, while the error in 1-nearest neighbor is substantially\nlarger. However, if the assumptions are wrong, all bets are o\ufb00 and the\n1-nearest neighbor may dominate. We will see that there is a whole spec-\ntrum of models between the rigid linear models and the extremely \ufb02exible\n1-nearest-neighbor models, each with their own assumptions and biases,\nwhich have been proposed speci\ufb01cally to avoid the exponential growth in\ncomplexity of functions in high dimensions by drawing heavily on these\nassumptions.\nBefore we delve more deeply, let us elaborate a bit on the concept of\nstatistical models and see how they \ufb01t into the prediction framework.", "46": "28 2. Overview of Supervised Learning\n2.6 Statistical Models, Supervised Learning and\nFunction Approximation\nOur goal is to \ufb01nd a useful approximation \u02c6f(x) to the function f(x) that\nunderlies the predictive relationship between the inputs and outputs. In the\ntheoretical setting of Section 2.4, we saw that squared error loss lead us\nto the regression function f(x) = E( Y|X=x) for a quantitative response.\nThe class of nearest-neighbor methods can be viewed as direct estimates\nof this conditional expectation, but we have seen that they can fail in at\nleast two ways:\n\u2022if the dimension of the input space is high, the nearest neighbors need\nnot be close to the target point, and can result in large errors;\n\u2022if special structure is known to exist, this can be used to reduce both\nthe bias and the variance of the estimates.\nWe anticipate using other classes of models for f(x), in many cases specif-\nically designed to overcome the dimensionality problems, and here we dis-\ncuss a framework for incorporating them into the prediction problem.\n2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)\nSuppose in fact that our data arose from a statistical model\nY=f(X) +\u03b5, (2.29)\nwhere the random error \u03b5has E( \u03b5) = 0 and is independent of X. Note that\nfor this model, f(x) = E( Y|X=x), and in fact the conditional distribution\nPr(Y|X) depends on Xonlythrough the conditional mean f(x).\nThe additive error model is a useful approximation to the truth. For\nmost systems the input\u2013output pairs ( X,Y) will not have a deterministic\nrelationship Y=f(X). Generally there will be other unmeasured variables\nthat also contribute to Y, including measurement error. The additive model\nassumes that we can capture all these departures from a deterministic re-\nlationship via the error \u03b5.\nFor some problems a deterministic relationship does hold. Many of the\nclassi\ufb01cation problems studied in machine learning are of this form, where\nthe response surface can be thought of as a colored map de\ufb01ned in IRp.\nThe training data consist of colored examples from the map {xi,gi}, and\nthe goal is to be able to color any point. Here the function is deterministic,\nand the randomness enters through the xlocation of the training points.\nFor the moment we will not pursue such problems, but will see that they\ncan be handled by techniques appropriate for the error-based models.\nThe assumption in (2.29) that the errors are independent and identically\ndistributed is not strictly necessary, but seems to be at the back of our mind", "47": "2.6 Statistical Models, Supervised Learning and Function Approxi mation 29\nwhen we average squared errors uniformly in our EPE criterion. With such\na model it becomes natural to use least squares as a data criterion for\nmodel estimation as in (2.1). Simple modi\ufb01cations can be made to avoid\nthe independence assumption; for example, we can have Var( Y|X=x) =\n\u03c3(x), and now both the mean and variance depend on X. In general the\nconditional distribution Pr( Y|X) can depend on Xin complicated ways,\nbut the additive error model precludes these.\nSo far we have concentrated on the quantitative response. Additive error\nmodels are typically not used for qualitative outputs G; in this case the tar-\nget function p(X)isthe conditional density Pr( G|X), and this is modeled\ndirectly. For example, for two-class data, it is often reasonable to assume\nthat the data arise from independent binary trials, with the probability of\none particular outcome being p(X), and the other 1 \u2212p(X). Thus if Yis\nthe 0\u20131 coded version of G, then E( Y|X=x) =p(x), but the variance\ndepends on xas well: Var( Y|X=x) =p(x)[1\u2212p(x)].\n2.6.2 Supervised Learning\nBefore we launch into more statistically oriented jargon, we present the\nfunction-\ufb01tting paradigm from a machine learning point of view. Suppose\nfor simplicity that the errors are additive and that the model Y=f(X)+\u03b5\nis a reasonable assumption. Supervised learning attempts to learn fby\nexample through a teacher . One observes the system under study, both\nthe inputs and outputs, and assembles a training set of observations T=\n(xi,yi), i= 1,... ,N . The observed input values to the system xiare also\nfed into an arti\ufb01cial system, known as a learning algorithm (usually a com-\nputer program), which also produces outputs \u02c6f(xi) in response to the in-\nputs. The learning algorithm has the property that it can modify its in-\nput/output relationship \u02c6fin response to di\ufb00erences yi\u2212\u02c6f(xi) between the\noriginal and generated outputs. This process is known as learning by exam-\nple. Upon completion of the learning process the hope is that the arti\ufb01cial\nand real outputs will be close enough to be useful for all sets of inputs likely\nto be encountered in practice.\n2.6.3 Function Approximation\nThe learning paradigm of the previous section has been the motivation\nfor research into the supervised learning problem in the \ufb01elds of machine\nlearning (with analogies to human reasoning) and neural networks (with\nbiological analogies to the brain). The approach taken in applied mathe-\nmatics and statistics has been from the perspective of function approxima-\ntion and estimation. Here the data pairs {xi,yi}are viewed as points in a\n(p+ 1)-dimensional Euclidean space. The function f(x) has domain equal\nto the p-dimensional input subspace, and is related to the data via a model", "48": "30 2. Overview of Supervised Learning\nsuch as yi=f(xi) +\u03b5i. For convenience in this chapter we will assume the\ndomain is IRp, ap-dimensional Euclidean space, although in general the\ninputs can be of mixed type. The goal is to obtain a useful approximation\ntof(x) for all xin some region of IRp, given the representations in T.\nAlthough somewhat less glamorous than the learning paradigm, treating\nsupervised learning as a problem in function approximation encourages the\ngeometrical concepts of Euclidean spaces and mathematical concepts of\nprobabilistic inference to be applied to the problem. This is the approach\ntaken in this book.\nMany of the approximations we will encounter have associated a set of\nparameters \u03b8that can be modi\ufb01ed to suit the data at hand. For example,\nthe linear model f(x) =xT\u03b2has\u03b8=\u03b2. Another class of useful approxi-\nmators can be expressed as linear basis expansions\nf\u03b8(x) =K/summationdisplay\nk=1hk(x)\u03b8k, (2.30)\nwhere the hkare a suitable set of functions or transformations of the input\nvector x. Traditional examples are polynomial and trigonometric expan-\nsions, where for example hkmight be x2\n1,x1x2\n2, cos(x1) and so on. We\nalso encounter nonlinear expansions, such as the sigmoid transformation\ncommon to neural network models,\nhk(x) =1\n1 + exp( \u2212xT\u03b2k). (2.31)\nWe can use least squares to estimate the parameters \u03b8inf\u03b8as we did\nfor the linear model, by minimizing the residual sum-of-squares\nRSS(\u03b8) =N/summationdisplay\ni=1(yi\u2212f\u03b8(xi))2(2.32)\nas a function of \u03b8. This seems a reasonable criterion for an additive error\nmodel. In terms of function approximation, we imagine our parameterized\nfunction as a surface in p+ 1 space, and what we observe are noisy re-\nalizations from it. This is easy to visualize when p= 2 and the vertical\ncoordinate is the output y, as in Figure 2.10. The noise is in the output\ncoordinate, so we \ufb01nd the set of parameters such that the \ufb01tted surface\ngets as close to the observed points as possible, where close is measured by\nthe sum of squared vertical errors in RSS( \u03b8).\nFor the linear model we get a simple closed form solution to the mini-\nmization problem. This is also true for the basis function methods, if the\nbasis functions themselves do not have any hidden parameters. Otherwise\nthe solution requires either iterative methods or numerical optimization.\nWhile least squares is generally very convenient, it is not the only crite-\nrion used and in some cases would not make much sense. A more general", "49": "2.6 Statistical Models, Supervised Learning and Function Approxi mation 31\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\nFIGURE 2.10. Least squares \ufb01tting of a function of two inputs. The parameters\noff\u03b8(x)are chosen so as to minimize the sum-of-squared vertical erro rs.\nprinciple for estimation is maximum likelihood estimation . Suppose we have\na random sample yi, i= 1,... ,N from a density Pr \u03b8(y) indexed by some\nparameters \u03b8. The log-probability of the observed sample is\nL(\u03b8) =N/summationdisplay\ni=1log Pr \u03b8(yi). (2.33)\nThe principle of maximum likelihood assumes that the most reasonable\nvalues for \u03b8are those for which the probability of the observed sample is\nlargest. Least squares for the additive error model Y=f\u03b8(X) +\u03b5, with\n\u03b5\u223cN(0,\u03c32), is equivalent to maximum likelihood using the conditional\nlikelihood\nPr(Y|X,\u03b8) =N(f\u03b8(X),\u03c32). (2.34)\nSo although the additional assumption of normality seems more restrictive,\nthe results are the same. The log-likelihood of the data is\nL(\u03b8) =\u2212N\n2log(2\u03c0)\u2212Nlog\u03c3\u22121\n2\u03c32N/summationdisplay\ni=1(yi\u2212f\u03b8(xi))2, (2.35)\nand the only term involving \u03b8is the last, which is RSS( \u03b8) up to a scalar\nnegative multiplier.\nA more interesting example is the multinomial likelihood for the regres-\nsion function Pr( G|X) for a qualitative output G. Suppose we have a model\nPr(G=Gk|X=x) =pk,\u03b8(x), k= 1,... ,K for the conditional probabil-\nity of each class given X, indexed by the parameter vector \u03b8. Then the", "50": "32 2. Overview of Supervised Learning\nlog-likelihood (also referred to as the cross-entropy) is\nL(\u03b8) =N/summationdisplay\ni=1logpgi,\u03b8(xi), (2.36)\nand when maximized it delivers values of \u03b8that best conform with the data\nin this likelihood sense.\n2.7 Structured Regression Models\nWe have seen that although nearest-neighbor and other local methods focus\ndirectly on estimating the function at a point, they face problems in high\ndimensions. They may also be inappropriate even in low dimensions in\ncases where more structured approaches can make more e\ufb03cient use of the\ndata. This section introduces classes of such structured approaches. Before\nwe proceed, though, we discuss further the need for such classes.\n2.7.1 Di\ufb03culty of the Problem\nConsider the RSS criterion for an arbitrary function f,\nRSS(f) =N/summationdisplay\ni=1(yi\u2212f(xi))2. (2.37)\nMinimizing (2.37) leads to in\ufb01nitely many solutions: any function \u02c6fpassing\nthrough the training points ( xi,yi) is a solution. Any particular solution\nchosen might be a poor predictor at test points di\ufb00erent from the training\npoints. If there are multiple observation pairs xi,yi\u2113, \u2113= 1,... ,N iat each\nvalue of xi, the risk is limited. In this case, the solutions pass through\nthe average values of the yi\u2113at each xi; see Exercise 2.6. The situation is\nsimilar to the one we have already visited in Section 2.4; indeed, (2.37) is\nthe \ufb01nite sample version of (2.11) on page 18. If the sample size Nwere\nsu\ufb03ciently large such that repeats were guaranteed and densely arranged,\nit would seem that these solutions might all tend to the limiting conditional\nexpectation.\nIn order to obtain useful results for \ufb01nite N, we must restrict the eligible\nsolutions to (2.37) to a smaller set of functions. How to decide on the\nnature of the restrictions is based on considerations outside of the data.\nThese restrictions are sometimes encoded via the parametric representation\noff\u03b8, or may be built into the learning method itself, either implicitly or\nexplicitly. These restricted classes of solutions are the major topic of this\nbook. One thing should be clear, though. Any restrictions imposed on f\nthat lead to a unique solution to (2.37) do not really remove the ambiguity", "51": "2.8 Classes of Restricted Estimators 33\ncaused by the multiplicity of solutions. There are in\ufb01nitely many possible\nrestrictions, each leading to a unique solution, so the ambiguity has simply\nbeen transferred to the choice of constraint.\nIn general the constraints imposed by most learning methods can be\ndescribed as complexity restrictions of one kind or another. This usually\nmeans some kind of regular behavior in small neighborhoods of the input\nspace. That is, for all input points xsu\ufb03ciently close to each other in\nsome metric, \u02c6fexhibits some special structure such as nearly constant,\nlinear or low-order polynomial behavior. The estimator is then obtained by\naveraging or polynomial \ufb01tting in that neighborhood.\nThe strength of the constraint is dictated by the neighborhood size. The\nlarger the size of the neighborhood, the stronger the constraint, and the\nmore sensitive the solution is to the particular choice of constraint. For\nexample, local constant \ufb01ts in in\ufb01nitesimally small neighborhoods is no\nconstraint at all; local linear \ufb01ts in very large neighborhoods is almost a\nglobally linear model, and is very restrictive.\nThe nature of the constraint depends on the metric used. Some methods,\nsuch as kernel and local regression and tree-based methods, directly specify\nthe metric and size of the neighborhood. The nearest-neighbor methods\ndiscussed so far are based on the assumption that locally the function is\nconstant; close to a target input x0, the function does not change much, and\nso close outputs can be averaged to produce \u02c6f(x0). Other methods such\nas splines, neural networks and basis-function methods implicitly de\ufb01ne\nneighborhoods of local behavior. In Section 5.4.1 we discuss the concept\nof anequivalent kernel (see Figure 5.8 on page 157), which describes this\nlocal dependence for any method linear in the outputs. These equivalent\nkernels in many cases look just like the explicitly de\ufb01ned weighting kernels\ndiscussed above\u2014peaked at the target point and falling away smoothly\naway from it.\nOne fact should be clear by now. Any method that attempts to pro-\nduce locally varying functions in small isotropic neighborhoods will run\ninto problems in high dimensions\u2014again the curse of dimensionality. And\nconversely, all methods that overcome the dimensionality problems have an\nassociated\u2014and often implicit or adaptive\u2014metric for measuring neighbor-\nhoods, which basically does not allow the neighborhood to be simultane-\nously small in all directions.\n2.8 Classes of Restricted Estimators\nThe variety of nonparametric regression techniques or learning methods fall\ninto a number of di\ufb00erent classes depending on the nature of the restrictions\nimposed. These classes are not distinct, and indeed some methods fall in\nseveral classes. Here we give a brief summary, since detailed descriptions", "52": "34 2. Overview of Supervised Learning\nare given in later chapters. Each of the classes has associated with it one\nor more parameters, sometimes appropriately called smoothing parameters,\nthat control the e\ufb00ective size of the local neighborhood. Here we describe\nthree broad classes.\n2.8.1 Roughness Penalty and Bayesian Methods\nHere the class of functions is controlled by explicitly penalizing RSS( f)\nwith a roughness penalty\nPRSS( f;\u03bb) = RSS( f) +\u03bbJ(f). (2.38)\nThe user-selected functional J(f) will be large for functions fthat vary too\nrapidly over small regions of input space. For example, the popular cubic\nsmoothing spline for one-dimensional inputs is the solution to the penalized\nleast-squares criterion\nPRSS( f;\u03bb) =N/summationdisplay\ni=1(yi\u2212f(xi))2+\u03bb/integraldisplay\n[f\u2032\u2032(x)]2dx. (2.39)\nThe roughness penalty here controls large values of the second derivative\noff, and the amount of penalty is dictated by \u03bb\u22650. For \u03bb= 0 no penalty\nis imposed, and any interpolating function will do, while for \u03bb=\u221eonly\nfunctions linear in xare permitted.\nPenalty functionals Jcan be constructed for functions in any dimension,\nand special versions can be created to impose special structure. For ex-\nample, additive penalties J(f) =/summationtextp\nj=1J(fj) are used in conjunction with\nadditive functions f(X) =/summationtextp\nj=1fj(Xj) to create additive models with\nsmooth coordinate functions. Similarly, projection pursuit regression mod-\nels have f(X) =/summationtextM\nm=1gm(\u03b1T\nmX) for adaptively chosen directions \u03b1m, and\nthe functions gmcan each have an associated roughness penalty.\nPenalty function, or regularization methods, express our prior belief that\nthe type of functions we seek exhibit a certain type of smooth behavior, and\nindeed can usually be cast in a Bayesian framework. The penalty Jcorre-\nsponds to a log-prior, and PRSS( f;\u03bb) the log-posterior distribution, and\nminimizing PRSS( f;\u03bb) amounts to \ufb01nding the posterior mode. We discuss\nroughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\nChapter 8.\n2.8.2 Kernel Methods and Local Regression\nThese methods can be thought of as explicitly providing estimates of the re-\ngression function or conditional expectation by specifying the nature of the\nlocal neighborhood, and of the class of regular functions \ufb01tted locally. The\nlocal neighborhood is speci\ufb01ed by a kernel function K\u03bb(x0,x) which assigns", "53": "2.8 Classes of Restricted Estimators 35\nweights to points xin a region around x0(see Figure 6.1 on page 192). For\nexample, the Gaussian kernel has a weight function based on the Gaussian\ndensity function\nK\u03bb(x0,x) =1\n\u03bbexp/bracketleftbigg\n\u2212||x\u2212x0||2\n2\u03bb/bracketrightbigg\n(2.40)\nand assigns weights to points that die exponentially with their squared\nEuclidean distance from x0. The parameter \u03bbcorresponds to the variance\nof the Gaussian density, and controls the width of the neighborhood. The\nsimplest form of kernel estimate is the Nadaraya\u2013Watson weighted averag e\n\u02c6f(x0) =/summationtextN\ni=1K\u03bb(x0,xi)yi/summationtextN\ni=1K\u03bb(x0,xi). (2.41)\nIn general we can de\ufb01ne a local regression estimate of f(x0) asf\u02c6\u03b8(x0),\nwhere \u02c6\u03b8minimizes\nRSS(f\u03b8,x0) =N/summationdisplay\ni=1K\u03bb(x0,xi)(yi\u2212f\u03b8(xi))2, (2.42)\nandf\u03b8is some parameterized function, such as a low-order polynomial.\nSome examples are:\n\u2022f\u03b8(x) =\u03b80, the constant function; this results in the Nadaraya\u2013\nWatson estimate in (2.41) above.\n\u2022f\u03b8(x) =\u03b80+\u03b81xgives the popular local linear regression model.\nNearest-neighbor methods can be thought of as kernel methods having a\nmore data-dependent metric. Indeed, the metric for k-nearest neighbors is\nKk(x,x0) =I(||x\u2212x0|| \u2264 ||x(k)\u2212x0||),\nwhere x(k)is the training observation ranked kth in distance from x0, and\nI(S) is the indicator of the set S.\nThese methods of course need to be modi\ufb01ed in high dimensions, to avoid\nthe curse of dimensionality. Various adaptations are discussed in Chapter 6.\n2.8.3 Basis Functions and Dictionary Methods\nThis class of methods includes the familiar linear and polynomial expan-\nsions, but more importantly a wide variety of more \ufb02exible models. The\nmodel for fis a linear expansion of basis functions\nf\u03b8(x) =M/summationdisplay\nm=1\u03b8mhm(x), (2.43)", "54": "36 2. Overview of Supervised Learning\nwhere each of the hmis a function of the input x, and the term linear here\nrefers to the action of the parameters \u03b8. This class covers a wide variety of\nmethods. In some cases the sequence of basis functions is prescribed, such\nas a basis for polynomials in xof total degree M.\nFor one-dimensional x, polynomial splines of degree Kcan be represented\nby an appropriate sequence of Mspline basis functions, determined in turn\nbyM\u2212Kknots. These produce functions that are piecewise polynomials\nof degree Kbetween the knots, and joined up with continuity of degree\nK\u22121 at the knots. As an example consider linear splines, or piecewise\nlinear functions. One intuitively satisfying basis consists of the functions\nb1(x) = 1, b2(x) =x, and bm+2(x) = ( x\u2212tm)+,m= 1,... ,M \u22122,\nwhere tmis the mth knot, and z+denotes positive part. Tensor products\nof spline bases can be used for inputs with dimensions larger than one\n(see Section 5.2, and the CART and MARS models in Chapter 9.) The\nparameter \u03b8can be the total degree of the polynomial or the number of\nknots in the case of splines.\nRadial basis functions are symmetric p-dimensional kernels located at\nparticular centroids,\nf\u03b8(x) =M/summationdisplay\nm=1K\u03bbm(\u03b8m,x)\u03b8m; (2.44)\nfor example, the Gaussian kernel K\u03bb(\u03b8,x) =e\u2212||x\u2212\u03b8||2/2\u03bbis popular.\nRadial basis functions have centroids \u03b8mand scales \u03bbmthat have to\nbe determined. The spline basis functions have knots. In general we would\nlike the data to dictate them as well. Including these as parameters changes\nthe regression problem from a straightforward linear problem to a combi-\nnatorially hard nonlinear problem. In practice, shortcuts such as greedy\nalgorithms or two stage processes are used. Section 6.7 describes some such\napproaches.\nA single-layer feed-forward neural network model with linear output\nweights can be thought of as an adaptive basis function method. The model\nhas the form\nf\u03b8(x) =M/summationdisplay\nm=1\u03b2m\u03c3(\u03b1T\nmx+bm), (2.45)\nwhere \u03c3(x) = 1 /(1 +e\u2212x) is known as the activation function. Here, as\nin the projection pursuit model, the directions \u03b1mand the biasterms bm\nhave to be determined, and their estimation is the meat of the computation.\nDetails are give in Chapter 11.\nThese adaptively chosen basis function methods are also known as dictio-\nnarymethods, where one has available a possibly in\ufb01nite set or dictionary\nDof candidate basis functions from which to choose, and models are built\nup by employing some kind of search mechanism.", "55": "2.9 Model Selection and the Bias\u2013Variance Tradeo\ufb00 37\n2.9 Model Selection and the Bias\u2013Variance\nTradeo\ufb00\nAll the models described above and many others discussed in later chapters\nhave a smoothing orcomplexity parameter that has to be determined:\n\u2022the multiplier of the penalty term;\n\u2022the width of the kernel;\n\u2022or the number of basis functions.\nIn the case of the smoothing spline, the parameter \u03bbindexes models ranging\nfrom a straight line \ufb01t to the interpolating model. Similarly a local degr ee-\nmpolynomial model ranges between a degree- mglobal polynomial when\nthe window size is in\ufb01nitely large, to an interpolating \ufb01t when the window\nsize shrinks to zero. This means that we cannot use residual sum-of-squares\non the training data to determine these parameters as well, since we would\nalways pick those that gave interpolating \ufb01ts and hence zero residuals. Such\na model is unlikely to predict future data well at all.\nThek-nearest-neighbor regression \ufb01t \u02c6fk(x0) usefully illustrates the com-\npeting forces that e\ufb00ect the predictive ability of such approximations. Sup-\npose the data arise from a model Y=f(X) +\u03b5, with E( \u03b5) = 0 and\nVar(\u03b5) =\u03c32. For simplicity here we assume that the values of xiin the\nsample are \ufb01xed in advance (nonrandom). The expected prediction error\natx0, also known as testorgeneralization error, can be decomposed:\nEPE k(x0) = E[( Y\u2212\u02c6fk(x0))2|X=x0]\n=\u03c32+ [Bias2(\u02c6fk(x0)) + Var T(\u02c6fk(x0))] (2.46)\n=\u03c32+/bracketleft\uf8ecig\nf(x0)\u22121\nkk/summationdisplay\n\u2113=1f(x(\u2113))/bracketright\uf8ecig2\n+\u03c32\nk. (2.47)\nThe subscripts in parentheses ( \u2113) indicate the sequence of nearest neighbors\ntox0.\nThere are three terms in this expression. The \ufb01rst term \u03c32is the ir-\nreducible error\u2014the variance of the new test target\u2014and is beyond our\ncontrol, even if we know the true f(x0).\nThe second and third terms are under our control, and make up the\nmean squared error of\u02c6fk(x0) in estimating f(x0), which is broken down\ninto a bias component and a variance component. The bias term is the\nsquared di\ufb00erence between the true mean f(x0) and the expected value of\nthe estimate\u2014[E T(\u02c6fk(x0))\u2212f(x0)]2\u2014where the expectation averages the\nrandomness in the training data. This term will most likely increase with\nk, if the true function is reasonably smooth. For small kthe few closest\nneighbors will have values f(x(\u2113)) close to f(x0), so their average should", "56": "38 2. Overview of Supervised Learning\nHigh Bias\nLow VarianceLow Bias\nHigh VariancePrediction Error\nModel ComplexityTraining SampleTest Sample\nLow High\nFIGURE 2.11. Test and training error as a function of model complexity.\nbe close to f(x0). As kgrows, the neighbors are further away, and then\nanything can happen.\nThe variance term is simply the variance of an average here, and de-\ncreases as the inverse of k. So as kvaries, there is a bias\u2013variance tradeo\ufb00.\nMore generally, as the model complexity of our procedure is increased,\nthe variance tends to increase and the squared bias tends to decreases.\nThe opposite behavior occurs as the model complexity is decreased. For\nk-nearest neighbors, the model complexity is controlled by k.\nTypically we would like to choose our model complexity to trade bias\no\ufb00 with variance in such a way as to minimize the test error. An obvious\nestimate of test error is the training error1\nN/summationtext\ni(yi\u2212\u02c6yi)2. Unfortunately\ntraining error is not a good estimate of test error, as it does not properly\naccount for model complexity.\nFigure 2.11 shows the typical behavior of the test and training error, as\nmodel complexity is varied. The training error tends to decrease whenever\nwe increase the model complexity, that is, whenever we \ufb01t the data harder.\nHowever with too much \ufb01tting, the model adapts itself too closely to the\ntraining data, and will not generalize well (i.e., have large test error). In\nthat case the predictions \u02c6f(x0) will have large variance, as re\ufb02ected in the\nlast term of expression (2.46). In contrast, if the model is not complex\nenough, it will under\ufb01t and may have large bias, again resulting in poor\ngeneralization. In Chapter 7 we discuss methods for estimating the test\nerror of a prediction method, and hence estimating the optimal amount of\nmodel complexity for a given prediction method and training set.", "57": "Exercises 39\nBibliographic Notes\nSome good general books on the learning problem are Duda et al. (2000),\nBishop (1995),(Bishop, 2006), Ripley (1996), Cherkassky and Mulier (2 007)\nand Vapnik (1996). Parts of this chapter are based on Friedman (1994b).\nExercises\nEx. 2.1 Suppose each of K-classes has an associated target tk, which is a\nvector of all zeros, except a one in the kth position. Show that classifying to\nthe largest element of \u02c6 yamounts to choosing the closest target, min k||tk\u2212\n\u02c6y||, if the elements of \u02c6 ysum to one.\nEx. 2.2 Show how to compute the Bayes decision boundary for the simula-\ntion example in Figure 2.5.\nEx. 2.3 Derive equation (2.24).\nEx. 2.4 The edge e\ufb00ect problem discussed on page 23 is not peculiar to\nuniform sampling from bounded domains. Consider inputs drawn from a\nspherical multinormal distribution X\u223cN(0,Ip). The squared distance\nfrom any sample point to the origin has a \u03c72\npdistribution with mean p.\nConsider a prediction point x0drawn from this distribution, and let a=\nx0/||x0||be an associated unit vector. Let zi=aTxibe the projection of\neach of the training points on this direction.\nShow that the ziare distributed N(0,1) with expected squared distance\nfrom the origin 1, while the target point has expected squared distance p\nfrom the origin.\nHence for p= 10, a randomly drawn test point is about 3 .1 standard\ndeviations from the origin, while all the training points are on average\none standard deviation along direction a. So most prediction points see\nthemselves as lying on the edge of the training set.\nEx. 2.5\n(a) Derive equation (2.27). The last line makes use of (3.8) through a\nconditioning argument.\n(b) Derive equation (2.28), making use of the cyclic property of the trace\noperator [trace( AB) = trace( BA)], and its linearity (which allows us\nto interchange the order of trace and expectation).\nEx. 2.6 Consider a regression problem with inputs xiand outputs yi, and a\nparameterized model f\u03b8(x) to be \ufb01t by least squares. Show that if there are\nobservations with tiedoridentical values of x, then the \ufb01t can be obtained\nfrom a reduced weighted least squares problem.", "58": "40 2. Overview of Supervised Learning\nEx. 2.7 Suppose we have a sample of Npairs xi,yidrawn i.i.d. from the\ndistribution characterized as follows:\nxi\u223ch(x),the design density\nyi=f(xi) +\u03b5i, fis the regression function\n\u03b5i\u223c(0,\u03c32) (mean zero, variance \u03c32)\nWe construct an estimator for flinear in the yi,\n\u02c6f(x0) =N/summationdisplay\ni=1\u2113i(x0;X)yi,\nwhere the weights \u2113i(x0;X) do not depend on the yi, but do depend on the\nentire training sequence of xi, denoted here by X.\n(a) Show that linear regression and k-nearest-neighbor regression are mem-\nbers of this class of estimators. Describe explicitly the weights \u2113i(x0;X)\nin each of these cases.\n(b) Decompose the conditional mean-squared error\nEY|X(f(x0)\u2212\u02c6f(x0))2\ninto a conditional squared bias and a conditional variance component.\nLikeX,Yrepresents the entire training sequence of yi.\n(c) Decompose the (unconditional) mean-squared error\nEY,X(f(x0)\u2212\u02c6f(x0))2\ninto a squared bias and a variance component.\n(d) Establish a relationship between the squared biases and variances in\nthe above two cases.\nEx. 2.8 Compare the classi\ufb01cation performance of linear regression and k\u2013\nnearest neighbor classi\ufb01cation on the zipcode data. In particular, consider\nonly the 2\u2019s and3\u2019s, and k= 1,3,5,7 and 15. Show both the training and\ntest error for each choice. The zipcode data are available from the book\nwebsitewww-stat.stanford.edu/ElemStatLearn .\nEx. 2.9 Consider a linear regression model with pparameters, \ufb01t by least\nsquares to a set of training data ( x1,y1),... ,(xN,yN) drawn at random\nfrom a population. Let \u02c6\u03b2be the least squares estimate. Suppose we have\nsome test data (\u02dc x1,\u02dcy1),... ,(\u02dcxM,\u02dcyM) drawn at random from the same pop-\nulation as the training data. If Rtr(\u03b2) =1\nN/summationtextN\n1(yi\u2212\u03b2Txi)2andRte(\u03b2) =\n1\nM/summationtextM\n1(\u02dcyi\u2212\u03b2T\u02dcxi)2, prove that\nE[Rtr(\u02c6\u03b2)]\u2264E[Rte(\u02c6\u03b2)],", "59": "Exercises 41\nwhere the expectations are over all that is random in each expression. [This\nexercise was brought to our attention by Ryan Tibshirani, from a homework\nassignment given by Andrew Ng.]", "60": "42 2. Overview of Supervised Learning", "61": "This is page 43\nPrinter: Opaque this\n3\nLinear Methods for Regression\n3.1 Introduction\nA linear regression model assumes that the regression function E( Y|X) is\nlinear in the inputs X1,... ,X p. Linear models were largely developed in\nthe precomputer age of statistics, but even in today\u2019s computer era there\nare still good reasons to study and use them. They are simple and often\nprovide an adequate and interpretable description of how the inputs a\ufb00ect\nthe output. For prediction purposes they can sometimes outperform fancier\nnonlinear models, especially in situations with small numbers of training\ncases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\napplied to transformations of the inputs and this considerably expands their\nscope. These generalizations are sometimes called basis-function methods,\nand are discussed in Chapter 5.\nIn this chapter we describe linear methods for regression, while in the\nnext chapter we discuss linear methods for classi\ufb01cation. On some topics we\ngo into considerable detail, as it is our \ufb01rm belief that an understanding\nof linear methods is essential for understanding nonlinear ones. In fact,\nmany nonlinear techniques are direct generalizations of the linear methods\ndiscussed here.", "62": "44 3. Linear Methods for Regression\n3.2 Linear Regression Models and Least Squares\nAs introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\nand want to predict a real-valued output Y. The linear regression model\nhas the form\nf(X) =\u03b20+p/summationdisplay\nj=1Xj\u03b2j. (3.1)\nThe linear model either assumes that the regression function E( Y|X) is\nlinear, or that the linear model is a reasonable approximation. Here the\n\u03b2j\u2019s are unknown parameters or coe\ufb03cients, and the variables Xjcan come\nfrom di\ufb00erent sources:\n\u2022quantitative inputs;\n\u2022transformations of quantitative inputs, such as log, square-root or\nsquare;\n\u2022basis expansions, such as X2=X2\n1,X3=X3\n1, leading to a polynomial\nrepresentation;\n\u2022numeric or \u201cdummy\u201d coding of the levels of qualitative inputs. For\nexample, if Gis a \ufb01ve-level factor input, we might create Xj, j=\n1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\nsents the e\ufb00ect of Gby a set of level-dependent constants, since in/summationtext5\nj=1Xj\u03b2j, one of the Xjs is one, and the others are zero.\n\u2022interactions between variables, for example, X3=X1\u2264X2.\nNo matter the source of the Xj, the model is linear in the parameters.\nTypically we have a set of training data ( x1,y1)...(xN,yN) from which\nto estimate the parameters \u03b2. Each xi= (xi1,xi2,... ,x ip)Tis a vector\nof feature measurements for the ith case. The most popular estimation\nmethod is least squares , in which we pick the coe\ufb03cients \u03b2= (\u03b20,\u03b21,... ,\u03b2 p)T\nto minimize the residual sum of squares\nRSS(\u03b2) =N/summationdisplay\ni=1(yi\u2212f(xi))2\n=N/summationdisplay\ni=1/parenleft\uf8ecig\nyi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j/parenright\uf8ecig2\n. (3.2)\nFrom a statistical point of view, this criterion is reasonable if the tr aining\nobservations ( xi,yi) represent independent random draws from their popu-\nlation. Even if the xi\u2019s were not drawn randomly, the criterion is still valid\nif the yi\u2019s are conditionally independent given the inputs xi. Figure 3.1\nillustrates the geometry of least-squares \ufb01tting in the IRp+1-dimensional", "63": "3.2 Linear Regression Models and Least Squares 45\n\u2022\u2022 \u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\nX1X2Y\nFIGURE 3.1. Linear least squares \ufb01tting with X\u2208I R2. We seek the linear\nfunction of Xthat minimizes the sum of squared residuals from Y.\nspace occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions\nabout the validity of model (3.1); it simply \ufb01nds the best linear \ufb01t to the\ndata. Least squares \ufb01tting is intuitively satisfying no matter how the data\narise; the criterion measures the average lack of \ufb01t.\nHow do we minimize (3.2)? Denote by XtheN\u00d7(p+ 1) matrix with\neach row an input vector (with a 1 in the \ufb01rst position), and similarly let\nybe the N-vector of outputs in the training set. Then we can write the\nresidual sum-of-squares as\nRSS(\u03b2) = (y\u2212X\u03b2)T(y\u2212X\u03b2). (3.3)\nThis is a quadratic function in the p+ 1 parameters. Di\ufb00erentiating with\nrespect to \u03b2we obtain\n\u2202RSS\n\u2202\u03b2=\u22122XT(y\u2212X\u03b2)\n\u22022RSS\n\u2202\u03b2\u2202\u03b2T= 2XTX.(3.4)\nAssuming (for the moment) that Xhas full column rank, and hence XTX\nis positive de\ufb01nite, we set the \ufb01rst derivative to zero\nXT(y\u2212X\u03b2) = 0 (3.5)\nto obtain the unique solution\n\u02c6\u03b2= (XTX)\u22121XTy. (3.6)", "64": "46 3. Linear Methods for Regression\nx1x2y\n\u02c6 y\nFIGURE 3.2. TheN-dimensional geometry of least squares regression with two\npredictors. The outcome vector yis orthogonally projected onto the hyperplane\nspanned by the input vectors x1andx2. The projection \u02c6yrepresents the vector\nof the least squares predictions\nThe predicted values at an input vector x0are given by \u02c6f(x0) = (1 : x0)T\u02c6\u03b2;\nthe \ufb01tted values at the training inputs are\n\u02c6y=X\u02c6\u03b2=X(XTX)\u22121XTy, (3.7)\nwhere \u02c6 yi=\u02c6f(xi). The matrix H=X(XTX)\u22121XTappearing in equation\n(3.7) is sometimes called the \u201chat\u201d matrix because it puts the hat on y.\nFigure 3.2 shows a di\ufb00erent geometrical representation of the least squares\nestimate, this time in IRN. We denote the column vectors of Xbyx0,x1,... ,xp,\nwithx0\u22611. For much of what follows, this \ufb01rst column is treated like any\nother. These vectors span a subspace of IRN, also referred to as the column\nspace of X. We minimize RSS( \u03b2) =\u221d\u230aa\u2207\u2308\u230aly\u2212X\u03b2\u221d\u230aa\u2207\u2308\u230al2by choosing \u02c6\u03b2so that the\nresidual vector y\u2212\u02c6yis orthogonal to this subspace. This orthogonality is\nexpressed in (3.5), and the resulting estimate \u02c6yis hence the orthogonal pro-\njection ofyonto this subspace. The hat matrix Hcomputes the orthogonal\nprojection, and hence it is also known as a projection matrix.\nIt might happen that the columns of Xare not linearly independent, so\nthatXis not of full rank. This would occur, for example, if two of the\ninputs were perfectly correlated, (e.g., x2= 3x1). Then XTXis singular\nand the least squares coe\ufb03cients \u02c6\u03b2are not uniquely de\ufb01ned. However,\nthe \ufb01tted values \u02c6y=X\u02c6\u03b2are still the projection of yonto the column\nspace of X; there is just more than one way to express that projection\nin terms of the column vectors of X. The non-full-rank case occurs most\noften when one or more qualitative inputs are coded in a redundant fashion.\nThere is usually a natural way to resolve the non-unique representation,\nby recoding and/or dropping redundant columns in X. Most regression\nsoftware packages detect these redundancies and automatically implement", "65": "3.2 Linear Regression Models and Least Squares 47\nsome strategy for removing them. Rank de\ufb01ciencies can also occur in signal\nand image analysis, where the number of inputs pcan exceed the number\nof training cases N. In this case, the features are typically reduced by\n\ufb01ltering or else the \ufb01tting is controlled by regularization (Section 5.2.3 and\nChapter 18).\nUp to now we have made minimal assumptions about the true distribu-\ntion of the data. In order to pin down the sampling properties of \u02c6\u03b2, we now\nassume that the observations yiare uncorrelated and have constant vari-\nance\u03c32, and that the xiare \ufb01xed (non random). The variance\u2013covariance\nmatrix of the least squares parameter estimates is easily derived from (3.6 )\nand is given by\nVar(\u02c6\u03b2) = (XTX)\u22121\u03c32. (3.8)\nTypically one estimates the variance \u03c32by\n\u02c6\u03c32=1\nN\u2212p\u22121N/summationdisplay\ni=1(yi\u2212\u02c6yi)2.\nTheN\u2212p\u22121 rather than Nin the denominator makes \u02c6 \u03c32an unbiased\nestimate of \u03c32: E(\u02c6\u03c32) =\u03c32.\nTo draw inferences about the parameters and the model, additional as-\nsumptions are needed. We now assume that (3.1) is the correct model for\nthe mean; that is, the conditional expectation of Yis linear in X1,... ,X p.\nWe also assume that the deviations of Yaround its expectation are additive\nand Gaussian. Hence\nY= E( Y|X1,... ,X p) +\u03b5\n=\u03b20+p/summationdisplay\nj=1Xj\u03b2j+\u03b5, (3.9)\nwhere the error \u03b5is a Gaussian random variable with expectation zero and\nvariance \u03c32, written \u03b5\u223cN(0,\u03c32).\nUnder (3.9), it is easy to show that\n\u02c6\u03b2\u223cN(\u03b2,(XTX)\u22121\u03c32). (3.10)\nThis is a multivariate normal distribution with mean vector and variance\u2013\ncovariance matrix as shown. Also\n(N\u2212p\u22121)\u02c6\u03c32\u223c\u03c32\u03c72\nN\u2212p\u22121, (3.11)\na chi-squared distribution with N\u2212p\u22121 degrees of freedom. In addition \u02c6\u03b2\nand \u02c6\u03c32are statistically independent. We use these distributional properties\nto form tests of hypothesis and con\ufb01dence intervals for the parameters \u03b2j.", "66": "48 3. Linear Methods for Regression\nZTail Probabilities\n2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30\nt100\nnormal\nFIGURE 3.3. The tail probabilities Pr(|Z|> z)for three distributions, t30,t100\nand standard normal. Shown are the appropriate quantiles for test ing signi\ufb01cance\nat the p= 0.05and0.01levels. The di\ufb00erence between tand the standard normal\nbecomes negligible for Nbigger than about 100.\nTo test the hypothesis that a particular coe\ufb03cient \u03b2j= 0, we form the\nstandardized coe\ufb03cient or Z-score\nzj=\u02c6\u03b2j\n\u02c6\u03c3\u221avj, (3.12)\nwhere vjis the jth diagonal element of ( XTX)\u22121. Under the null hypothesis\nthat\u03b2j= 0,zjis distributed as tN\u2212p\u22121(atdistribution with N\u2212p\u22121\ndegrees of freedom), and hence a large (absolute) value of zjwill lead to\nrejection of this null hypothesis. If \u02c6 \u03c3is replaced by a known value \u03c3, then\nzjwould have a standard normal distribution. The di\ufb00erence between the\ntail quantiles of a t-distribution and a standard normal become negligible\nas the sample size increases, and so we typically use the normal quantiles\n(see Figure 3.3).\nOften we need to test for the signi\ufb01cance of groups of coe\ufb03cients simul-\ntaneously. For example, to test if a categorical variable with klevels can\nbe excluded from a model, we need to test whether the coe\ufb03cients of the\ndummy variables used to represent the levels can all be set to zero. Here\nwe use the Fstatistic,\nF=(RSS 0\u2212RSS1)/(p1\u2212p0)\nRSS1/(N\u2212p1\u22121), (3.13)\nwhere RSS 1is the residual sum-of-squares for the least squares \ufb01t of the big-\nger model with p1+1 parameters, and RSS 0the same for the nested smaller\nmodel with p0+1 parameters, having p1\u2212p0parameters constrained to be", "67": "3.2 Linear Regression Models and Least Squares 49\nzero. The Fstatistic measures the change in residual sum-of-squares per\nadditional parameter in the bigger model, and it is normalized by an esti-\nmate of \u03c32. Under the Gaussian assumptions, and the null hypothesis that\nthe smaller model is correct, the Fstatistic will have a Fp1\u2212p0,N\u2212p1\u22121dis-\ntribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent\nto the Fstatistic for dropping the single coe\ufb03cient \u03b2jfrom the model. For\nlargeN, the quantiles of the Fp1\u2212p0,N\u2212p1\u22121approach those of the \u03c72\np1\u2212p0.\nSimilarly, we can isolate \u03b2jin (3.10) to obtain a 1 \u22122\u03b1con\ufb01dence interval\nfor\u03b2j:\n(\u02c6\u03b2j\u2212z(1\u2212\u03b1)v1\n2\nj\u02c6\u03c3,\u02c6\u03b2j+z(1\u2212\u03b1)v1\n2\nj\u02c6\u03c3). (3.14)\nHerez(1\u2212\u03b1)is the 1 \u2212\u03b1percentile of the normal distribution:\nz(1\u22120.025)= 1.96,\nz(1\u2212.05)= 1.645,etc.\nHence the standard practice of reporting \u02c6\u03b2\u00b12\u2264se(\u02c6\u03b2) amounts to an ap-\nproximate 95% con\ufb01dence interval. Even if the Gaussian error assumption\ndoes not hold, this interval will be approximately correct, with its coverage\napproaching 1 \u22122\u03b1as the sample size N\u2192 \u221e.\nIn a similar fashion we can obtain an approximate con\ufb01dence set for the\nentire parameter vector \u03b2, namely\nC\u03b2={\u03b2|(\u02c6\u03b2\u2212\u03b2)TXTX(\u02c6\u03b2\u2212\u03b2)\u2264\u02c6\u03c32\u03c72\np+1(1\u2212\u03b1)}, (3.15)\nwhere \u03c72\n\u2113(1\u2212\u03b1)is the 1 \u2212\u03b1percentile of the chi-squared distribution on \u2113\ndegrees of freedom: for example, \u03c72\n5(1\u22120.05)= 11.1,\u03c72\n5(1\u22120.1)= 9.2. This\ncon\ufb01dence set for \u03b2generates a corresponding con\ufb01dence set for the true\nfunction f(x) =xT\u03b2, namely {xT\u03b2|\u03b2\u2208C\u03b2}(Exercise 3.2; see also Fig-\nure 5.4 in Section 5.2.2 for examples of con\ufb01dence bands for functions).\n3.2.1 Example: Prostate Cancer\nThe data for this example come from a study by Stamey et al. (1989). They\nexamined the correlation between the level of prostate-speci\ufb01c antigen and\na number of clinical measures in men who were about to receive a radical\nprostatectomy. The variables are log cancer volume ( lcavol ), log prostate\nweight ( lweight ),age, log of the amount of benign prostatic hyperplasia\n(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),\nGleason score ( gleason ), and percent of Gleason scores 4 or 5 ( pgg45).\nThe correlation matrix of the predictors given in Table 3.1 shows many\nstrong correlations. Figure 1.1 (page 3) of Chapter 1 is a scatterplot matr ix\nshowing every pairwise plot between the variables. We see that sviis a\nbinary variable, and gleason is an ordered categorical variable. We see, for", "68": "50 3. Linear Methods for Regression\nTABLE 3.1. Correlations of predictors in the prostate cancer data.\nlcavol lweight age lbph svi lcp gleason\nlweight 0.300\nage 0.286 0.317\nlbph 0.063 0.437 0.287\nsvi 0.593 0.181 0.129 \u22120.139\nlcp 0.692 0.157 0.173 \u22120.089 0.671\ngleason 0.426 0.024 0.366 0.033 0.307 0.476\npgg45 0.483 0.074 0.276 \u22120.030 0.481 0.663 0.757\nTABLE 3.2. Linear model \ufb01t to the prostate cancer data. The Z score is the\ncoe\ufb03cient divided by its standard error (3.12). Roughly a Zscore larger than two\nin absolute value is signi\ufb01cantly nonzero at the p= 0.05level.\nTerm Coe\ufb03cient Std. Error ZScore\nIntercept 2.46 0.09 27.60\nlcavol 0.68 0.13 5.37\nlweight 0.26 0.10 2.75\nage \u22120.14 0.10 \u22121.40\nlbph 0.21 0.10 2.06\nsvi 0.31 0.12 2.47\nlcp \u22120.29 0.15 \u22121.87\ngleason \u22120.02 0.15 \u22120.15\npgg45 0.27 0.15 1.74\nexample, that both lcavol andlcpshow a strong relationship with the\nresponse lpsa, and with each other. We need to \ufb01t the e\ufb00ects jointly to\nuntangle the relationships between the predictors and the response.\nWe \ufb01t a linear model to the log of prostate-speci\ufb01c antigen, lpsa, after\n\ufb01rst standardizing the predictors to have unit variance. We randomly split\nthe dataset into a training set of size 67 and a test set of size 30. We ap-\nplied least squares estimation to the training set, producing the estimates,\nstandard errors and Z-scores shown in Table 3.2. The Z-scores are de\ufb01ned\nin (3.12), and measure the e\ufb00ect of dropping that variable from the model.\nAZ-score greater than 2 in absolute value is approximately signi\ufb01cant at\nthe 5% level. (For our example, we have nine parameters, and the 0 .025 tail\nquantiles of the t67\u22129distribution are \u00b12.002!) The predictor lcavol shows\nthe strongest e\ufb00ect, with lweight andsvialso strong. Notice that lcpis\nnot signi\ufb01cant, once lcavol is in the model (when used in a model without\nlcavol ,lcpis strongly signi\ufb01cant). We can also test for the exclusion of\na number of terms at once, using the F-statistic (3.13). For example, we\nconsider dropping all the non-signi\ufb01cant terms in Table 3.2, namely age,", "69": "3.2 Linear Regression Models and Least Squares 51\nlcp,gleason , andpgg45. We get\nF=(32.81\u221229.43)/(9\u22125)\n29.43/(67\u22129)= 1.67, (3.16)\nwhich has a p-value of 0 .17 (Pr( F4,58>1.67) = 0 .17), and hence is not\nsigni\ufb01cant.\nThe mean prediction error on the test data is 0 .521. In contrast, predic-\ntion using the mean training value of lpsahas a test error of 1 .057, which\nis called the \u201cbase error rate.\u201d Hence the linear model reduces the base\nerror rate by about 50%. We will return to this example later to compare\nvarious selection and shrinkage methods.\n3.2.2 The Gauss\u2013Markov Theorem\nOne of the most famous results in statistics asserts that the least squares\nestimates of the parameters \u03b2have the smallest variance among all linear\nunbiased estimates. We will make this precise here, and also make clear\nthat the restriction to unbiased estimates is not necessarily a wise one. This\nobservation will lead us to consider biased estimates such as ridge regression\nlater in the chapter. We focus on estimation of any linear combination of\nthe parameters \u03b8=aT\u03b2; for example, predictions f(x0) =xT\n0\u03b2are of this\nform. The least squares estimate of aT\u03b2is\n\u02c6\u03b8=aT\u02c6\u03b2=aT(XTX)\u22121XTy. (3.17)\nConsidering Xto be \ufb01xed, this is a linear function cT\n0yof the response\nvector y. If we assume that the linear model is correct, aT\u02c6\u03b2is unbiased\nsince\nE(aT\u02c6\u03b2) = E( aT(XTX)\u22121XTy)\n=aT(XTX)\u22121XTX\u03b2\n=aT\u03b2. (3.18)\nThe Gauss\u2013Markov theorem states that if we have any other linear estima-\ntor\u02dc\u03b8=cTythat is unbiased for aT\u03b2, that is, E( cTy) =aT\u03b2, then\nVar(aT\u02c6\u03b2)\u2264Var(cTy). (3.19)\nThe proof (Exercise 3.3) uses the triangle inequality. For simplicity we hav e\nstated the result in terms of estimation of a single parameter aT\u03b2, but with\na few more de\ufb01nitions one can state it in terms of the entire parameter\nvector \u03b2(Exercise 3.3).\nConsider the mean squared error of an estimator \u02dc\u03b8in estimating \u03b8:\nMSE( \u02dc\u03b8) = E( \u02dc\u03b8\u2212\u03b8)2\n= Var( \u02dc\u03b8) + [E( \u02dc\u03b8)\u2212\u03b8]2. (3.20)", "70": "52 3. Linear Methods for Regression\nThe \ufb01rst term is the variance, while the second term is the squared bias.\nThe Gauss-Markov theorem implies that the least squares estimator has the\nsmallest mean squared error of all linear estimators with no bias. However ,\nthere may well exist a biased estimator with smaller mean squared error.\nSuch an estimator would trade a little bias for a larger reduction in varia nce.\nBiased estimates are commonly used. Any method that shrinks or sets to\nzero some of the least squares coe\ufb03cients may result in a biased estimate.\nWe discuss many examples, including variable subset selection and ridge\nregression, later in this chapter. From a more pragmatic point of view, m ost\nmodels are distortions of the truth, and hence are biased; picking the right\nmodel amounts to creating the right balance between bias and variance.\nWe go into these issues in more detail in Chapter 7.\nMean squared error is intimately related to prediction accuracy, as dis-\ncussed in Chapter 2. Consider the prediction of the new response at input\nx0,\nY0=f(x0) +\u03b50. (3.21)\nThen the expected prediction error of an estimate \u02dcf(x0) =xT\n0\u02dc\u03b2is\nE(Y0\u2212\u02dcf(x0))2=\u03c32+ E(xT\n0\u02dc\u03b2\u2212f(x0))2\n=\u03c32+ MSE( \u02dcf(x0)). (3.22)\nTherefore, expected prediction error and mean squared error di\ufb00er only by\nthe constant \u03c32, representing the variance of the new observation y0.\n3.2.3 Multiple Regression from Simple Univariate Regressi on\nThe linear model (3.1) with p >1 inputs is called the multiple linear\nregression model . The least squares estimates (3.6) for this model are best\nunderstood in terms of the estimates for the univariate (p= 1) linear\nmodel, as we indicate in this section.\nSuppose \ufb01rst that we have a univariate model with no intercept, that is,\nY=X\u03b2+\u03b5. (3.23)\nThe least squares estimate and residuals are\n\u02c6\u03b2=/summationtextN\n1xiyi/summationtextN\n1x2\ni,\nri=yi\u2212xi\u02c6\u03b2.(3.24)\nIn convenient vector notation, we let y= (y1,... ,y N)T,x= (x1,... ,x N)T\nand de\ufb01ne\n\u221dan}\u230a\u2207a\u230bketle{tx,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht=N/summationdisplay\ni=1xiyi,\n=xTy, (3.25)", "71": "3.2 Linear Regression Models and Least Squares 53\ntheinner product between xandy1. Then we can write\n\u02c6\u03b2=\u221dan}\u230a\u2207a\u230bketle{tx,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht\n\u221dan}\u230a\u2207a\u230bketle{tx,x\u221dan}\u230a\u2207a\u230bket\u2207i}ht,\nr=y\u2212x\u02c6\u03b2.(3.26)\nAs we will see, this simple univariate regression provides the building block\nfor multiple linear regression. Suppose next that the inputs x1,x2,... ,xp\n(the columns of the data matrix X) are orthogonal; that is \u221dan}\u230a\u2207a\u230bketle{txj,xk\u221dan}\u230a\u2207a\u230bket\u2207i}ht= 0\nfor all j\u221dne}ationslash=k. Then it is easy to check that the multiple least squares esti-\nmates \u02c6\u03b2jare equal to \u221dan}\u230a\u2207a\u230bketle{txj,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht/\u221dan}\u230a\u2207a\u230bketle{txj,xj\u221dan}\u230a\u2207a\u230bket\u2207i}ht\u2014the univariate estimates. In other\nwords, when the inputs are orthogonal, they have no e\ufb00ect on each other\u2019s\nparameter estimates in the model.\nOrthogonal inputs occur most often with balanced, designed experiments\n(where orthogonality is enforced), but almost never with observational\ndata. Hence we will have to orthogonalize them in order to carry this idea\nfurther. Suppose next that we have an intercept and a single input x. Then\nthe least squares coe\ufb03cient of xhas the form\n\u02c6\u03b21=\u221dan}\u230a\u2207a\u230bketle{tx\u2212\u00afx1,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht\n\u221dan}\u230a\u2207a\u230bketle{tx\u2212\u00afx1,x\u2212\u00afx1\u221dan}\u230a\u2207a\u230bket\u2207i}ht, (3.27)\nwhere \u00af x=/summationtext\nixi/N, and1=x0, the vector of Nones. We can view the\nestimate (3.27) as the result of two applications of the simple regression\n(3.26). The steps are:\n1. regress xon1to produce the residual z=x\u2212\u00afx1;\n2. regress yon the residual zto give the coe\ufb03cient \u02c6\u03b21.\nIn this procedure, \u201cregress bona\u201d means a simple univariate regression of b\nonawith no intercept, producing coe\ufb03cient \u02c6 \u03b3=\u221dan}\u230a\u2207a\u230bketle{ta,b\u221dan}\u230a\u2207a\u230bket\u2207i}ht/\u221dan}\u230a\u2207a\u230bketle{ta,a\u221dan}\u230a\u2207a\u230bket\u2207i}htand residual\nvector b\u2212\u02c6\u03b3a. We say that bis adjusted for a, or is \u201corthogonalized\u201d with\nrespect to a.\nStep 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\nunivariate regression, using the orthogonal predictors 1andz. Figure 3.4\nshows this process for two general inputs x1andx2. The orthogonalization\ndoes not change the subspace spanned by x1andx2, it simply produces an\northogonal basis for representing it.\nThis recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.\nNote that the inputs z0,... ,zj\u22121in step 2 are orthogonal, hence the simple\nregression coe\ufb03cients computed there are in fact also the multiple regres-\nsion coe\ufb03cients.\n1The inner-product notation is suggestive of generalizatio ns of linear regression to\ndi\ufb00erent metric spaces, as well as to probability spaces.", "72": "54 3. Linear Methods for Regression\nx1x2y\n\u02c6 yz z z z z\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs. The\nvector x2is regressed on the vector x1, leaving the residual vector z. The regres-\nsion of yonzgives the multiple regression coe\ufb03cient of x2. Adding together the\nprojections of yon each of x1andzgives the least squares \ufb01t \u02c6y.\nAlgorithm 3.1 Regression by Successive Orthogonalization.\n1. Initialize z0=x0=1.\n2. For j= 1,2,... ,p\nRegress xjonz0,z1,... ,,zj\u22121to produce coe\ufb03cients \u02c6 \u03b3\u2113j=\n\u221dan}\u230a\u2207a\u230bketle{tz\u2113,xj\u221dan}\u230a\u2207a\u230bket\u2207i}ht/\u221dan}\u230a\u2207a\u230bketle{tz\u2113,z\u2113\u221dan}\u230a\u2207a\u230bket\u2207i}ht,\u2113= 0,... ,j \u22121 and residual vector zj=\nxj\u2212/summationtextj\u22121\nk=0\u02c6\u03b3kjzk.\n3. Regress yon the residual zpto give the estimate \u02c6\u03b2p.\nThe result of this algorithm is\n\u02c6\u03b2p=\u221dan}\u230a\u2207a\u230bketle{tzp,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht\n\u221dan}\u230a\u2207a\u230bketle{tzp,zp\u221dan}\u230a\u2207a\u230bket\u2207i}ht. (3.28)\nRe-arranging the residual in step 2, we can see that each of the xjis a linear\ncombination of the zk, k\u2264j. Since the zjare all orthogonal, they form\na basis for the column space of X, and hence the least squares projection\nonto this subspace is \u02c6y. Since zpalone involves xp(with coe\ufb03cient 1), we\nsee that the coe\ufb03cient (3.28) is indeed the multiple regression coe\ufb03cient of\nyonxp. This key result exposes the e\ufb00ect of correlated inputs in multiple\nregression. Note also that by rearranging the xj, any one of them could\nbe in the last position, and a similar results holds. Hence stated more\ngenerally, we have shown that the jth multiple regression coe\ufb03cient is the\nunivariate regression coe\ufb03cient of yonxj\u2264012...(j\u22121)(j+1)...,p, the residual\nafter regressing xjonx0,x1,... ,xj\u22121,xj+1,... ,xp:", "73": "3.2 Linear Regression Models and Least Squares 55\nThe multiple regression coe\ufb03cient \u02c6\u03b2jrepresents the additional\ncontribution of xjony, afterxjhas been adjusted for x0,x1,... ,xj\u22121,\nxj+1,... ,xp.\nIfxpis highly correlated with some of the other xk\u2019s, the residual vector\nzpwill be close to zero, and from (3.28) the coe\ufb03cient \u02c6\u03b2pwill be very\nunstable. This will be true for all the variables in the correlated set. In\nsuch situations, we might have all the Z-scores (as in Table 3.2) be smal l\u2014\nany one of the set can be deleted\u2014yet we cannot delete them all. From\n(3.28) we also obtain an alternate formula for the variance estimates ( 3.8),\nVar(\u02c6\u03b2p) =\u03c32\n\u221dan}\u230a\u2207a\u230bketle{tzp,zp\u221dan}\u230a\u2207a\u230bket\u2207i}ht=\u03c32\n\u221d\u230aa\u2207\u2308\u230alzp\u221d\u230aa\u2207\u2308\u230al2. (3.29)\nIn other words, the precision with which we can estimate \u02c6\u03b2pdepends on\nthe length of the residual vector zp; this represents how much of xpis\nunexplained by the other xk\u2019s.\nAlgorithm 3.1 is known as the Gram\u2013Schmidt procedure for multiple\nregression, and is also a useful numerical strategy for computing the esti-\nmates. We can obtain from it not just \u02c6\u03b2p, but also the entire multiple least\nsquares \ufb01t, as shown in Exercise 3.4.\nWe can represent step 2 of Algorithm 3.1 in matrix form:\nX=Z\u0393, (3.30)\nwhereZhas as columns the zj(in order), and \u0393is the upper triangular ma-\ntrix with entries \u02c6 \u03b3kj. Introducing the diagonal matrix Dwithjth diagonal\nentry Djj=\u221d\u230aa\u2207\u2308\u230alzj\u221d\u230aa\u2207\u2308\u230al, we get\nX=ZD\u22121D\u0393\n=QR, (3.31)\nthe so-called QRdecomposition of X. Here Qis anN\u00d7(p+1) orthogonal\nmatrix, QTQ=I, andRis a (p+ 1)\u00d7(p+ 1) upper triangular matrix.\nTheQRdecomposition represents a convenient orthogonal basis for the\ncolumn space of X. It is easy to see, for example, that the least squares\nsolution is given by\n\u02c6\u03b2=R\u22121QTy, (3.32)\n\u02c6y=QQTy. (3.33)\nEquation (3.32) is easy to solve because Ris upper triangular\n(Exercise 3.4).", "74": "56 3. Linear Methods for Regression\n3.2.4 Multiple Outputs\nSuppose we have multiple outputs Y1,Y2,... ,Y Kthat we wish to predict\nfrom our inputs X0,X1,X2,... ,X p. We assume a linear model for each\noutput\nYk=\u03b20k+p/summationdisplay\nj=1Xj\u03b2jk+\u03b5k (3.34)\n=fk(X) +\u03b5k. (3.35)\nWith Ntraining cases we can write the model in matrix notation\nY=XB+E. (3.36)\nHereYis the N\u00d7Kresponse matrix, with ikentry yik,Xis the N\u00d7(p+1)\ninput matrix, Bis the ( p+ 1)\u00d7Kmatrix of parameters and Eis the\nN\u00d7Kmatrix of errors. A straightforward generalization of the univariat e\nloss function (3.2) is\nRSS(B) =K/summationdisplay\nk=1N/summationdisplay\ni=1(yik\u2212fk(xi))2(3.37)\n= tr[( Y\u2212XB)T(Y\u2212XB)]. (3.38)\nThe least squares estimates have exactly the same form as before\n\u02c6B= (XTX)\u22121XTY. (3.39)\nHence the coe\ufb03cients for the kth outcome are just the least squares es-\ntimates in the regression of ykonx0,x1,... ,xp. Multiple outputs do not\na\ufb00ect one another\u2019s least squares estimates.\nIf the errors \u03b5= (\u03b51,... ,\u03b5 K) in (3.34) are correlated, then it might seem\nappropriate to modify (3.37) in favor of a multivariate version. Speci\ufb01ca lly,\nsuppose Cov( \u03b5) =\u03a3, then the multivariate weighted criterion\nRSS(B;\u03a3) =N/summationdisplay\ni=1(yi\u2212f(xi))T\u03a3\u22121(yi\u2212f(xi)) (3.40)\narises naturally from multivariate Gaussian theory. Here f(x) is the vector\nfunction ( f1(x),... ,f K(x)), and yithe vector of Kresponses for observa-\ntioni. However, it can be shown that again the solution is given by (3.39);\nKseparate regressions that ignore the correlations (Exercise 3.11). If the \u03a3i\nvary among observations, then this is no longer the case, and the solution\nforBno longer decouples.\nIn Section 3.7 we pursue the multiple outcome problem, and consider\nsituations where it does pay to combine the regressions.", "75": "3.3 Subset Selection 57\n3.3 Subset Selection\nThere are two reasons why we are often not satis\ufb01ed with the least squares\nestimates (3.6).\n\u2022The \ufb01rst is prediction accuracy : the least squares estimates often have\nlow bias but large variance. Prediction accuracy can sometimes be\nimproved by shrinking or setting some coe\ufb03cients to zero. By doing\nso we sacri\ufb01ce a little bit of bias to reduce the variance of the predicted\nvalues, and hence may improve the overall prediction accuracy.\n\u2022The second reason is interpretation . With a large number of predic-\ntors, we often would like to determine a smaller subset that exhibit\nthe strongest e\ufb00ects. In order to get the \u201cbig picture,\u201d we are willing\nto sacri\ufb01ce some of the small details.\nIn this section we describe a number of approaches to variable subset selec-\ntion with linear regression. In later sections we discuss shrinkage and hybrid\napproaches for controlling variance, as well as other dimension-reduction\nstrategies. These all fall under the general heading model selection . Model\nselection is not restricted to linear models; Chapter 7 covers this topic in\nsome detail.\nWith subset selection we retain only a subset of the variables, and elim-\ninate the rest from the model. Least squares regression is used to estimate\nthe coe\ufb03cients of the inputs that are retained. There are a number of dif-\nferent strategies for choosing the subset.\n3.3.1 Best-Subset Selection\nBest subset regression \ufb01nds for each k\u2208 {0,1,2,... ,p }the subset of size k\nthat gives smallest residual sum of squares (3.2). An e\ufb03cient algorithm\u2014\ntheleaps and bounds procedure (Furnival and Wilson, 1974)\u2014makes this\nfeasible for pas large as 30 or 40. Figure 3.5 shows all the subset models\nfor the prostate cancer example. The lower boundary represents the models\nthat are eligible for selection by the best-subsets approach. Note that the\nbest subset of size 2, for example, need not include the variable that was\nin the best subset of size 1 (for this example all the subsets are nested).\nThe best-subset curve (red lower boundary in Figure 3.5) is necessarily\ndecreasing, so cannot be used to select the subset size k. The question of\nhow to choose kinvolves the tradeo\ufb00 between bias and variance, along with\nthe more subjective desire for parsimony. There are a number of criteria\nthat one may use; typically we choose the smallest model that minimizes\nan estimate of the expected prediction error.\nMany of the other approaches that we discuss in this chapter are similar,\nin that they use the training data to produce a sequence of models varying\nin complexity and indexed by a single parameter. In the next section we use", "76": "58 3. Linear Methods for Regression\nSubset Size kResidual Sum\u2212of\u2212Squares\n0 20 40 60 80 100\n0 1 2 3 4 5 6 7 8\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\nFIGURE 3.5. All possible subset models for the prostate cancer example. At\neach subset size is shown the residual sum-of-squares for ea ch model of that size.\ncross-validation to estimate prediction error and select k; the AIC criterion\nis a popular alternative. We defer more detailed discussion of these and\nother approaches to Chapter 7.\n3.3.2 Forward- and Backward-Stepwise Selection\nRather than search through all possible subsets (which becomes infeasible\nforpmuch larger than 40), we can seek a good path through them. Forward-\nstepwise selection starts with the intercept, and then sequentially adds into\nthe model the predictor that most improves the \ufb01t. With many candidate\npredictors, this might seem like a lot of computation; however, clever up-\ndating algorithms can exploit the QR decomposition for the current \ufb01t to\nrapidly establish the next candidate (Exercise 3.9). Like best-subset re-\ngression, forward stepwise produces a sequence of models indexed by k, the\nsubset size, which must be determined.\nForward-stepwise selection is a greedy algorithm , producing a nested se-\nquence of models. In this sense it might seem sub-optimal compared to\nbest-subset selection. However, there are several reasons why it might be\npreferred:", "77": "3.3 Subset Selection 59\n\u2022Computational; for large pwe cannot compute the best subset se-\nquence, but we can always compute the forward stepwise sequence\n(even when p\u226bN).\n\u2022Statistical; a price is paid in variance for selecting the best subset\nof each size; forward stepwise is a more constrained search, and will\nhave lower variance, but perhaps more bias.\n0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset\nForward Stepwise\nBackward Stepwise\nForward StagewiseE||\u02c6\u03b2(k)\u2212\u03b2||2\nSubset Size k\nFIGURE 3.6. Comparison of four subset-selection techniques on a simulat ed lin-\near regression problem Y=XT\u03b2+\u03b5. There are N= 300 observations on p= 31\nstandard Gaussian variables, with pairwise correlations all equal to 0.85. For10of\nthe variables, the coe\ufb03cients are drawn at random from a N(0,0.4)distribution;\nthe rest are zero. The noise \u03b5\u223cN(0,6.25), resulting in a signal-to-noise ratio of\n0.64. Results are averaged over 50simulations. Shown is the mean-squared error\nof the estimated coe\ufb03cient \u02c6\u03b2(k)at each step from the true \u03b2.\nBackward-stepwise selection starts with the full model, and sequentially\ndeletes the predictor that has the least impact on the \ufb01t. The candidate for\ndropping is the variable with the smallest Z-score (Exercise 3.10). Backw ard\nselection can only be used when N > p , while forward stepwise can always\nbe used.\nFigure 3.6 shows the results of a small simulation study to compare\nbest-subset regression with the simpler alternatives forward and backward\nselection. Their performance is very similar, as is often the case. Included in\nthe \ufb01gure is forward stagewise regression (next section), which takes longer\nto reach minimum error.", "78": "60 3. Linear Methods for Regression\nOn the prostate cancer example, best-subset, forward and backward se-\nlection all gave exactly the same sequence of terms.\nSome software packages implement hybrid stepwise-selection strategies\nthat consider both forward and backward moves at each step, and select\nthe \u201cbest\u201d of the two. For example in the Rpackage the stepfunction uses\nthe AIC criterion for weighing the choices, which takes proper account of\nthe number of parameters \ufb01t; at each step an add or drop will be performed\nthat minimizes the AIC score.\nOther more traditional packages base the selection on F-statistics, adding\n\u201csigni\ufb01cant\u201d terms, and dropping \u201cnon-signi\ufb01cant\u201d terms. These are out\nof fashion, since they do not take proper account of the multiple testing\nissues. It is also tempting after a model search to print out a summary of\nthe chosen model, such as in Table 3.2; however, the standard errors are\nnot valid, since they do not account for the search process. The bootstrap\n(Section 8.2) can be useful in such settings.\nFinally, we note that often variables come in groups (such as the dummy\nvariables that code a multi-level categorical predictor). Smart stepwise pro-\ncedures (such as stepinR) will add or drop whole groups at a time, taking\nproper account of their degrees-of-freedom.\n3.3.3 Forward-Stagewise Regression\nForward-stagewise regression (FS) is even more constrained than forward-\nstepwise regression. It starts like forward-stepwise regression, wit h an in-\ntercept equal to \u00af y, and centered predictors with coe\ufb03cients initially all 0.\nAt each step the algorithm identi\ufb01es the variable most correlated with the\ncurrent residual. It then computes the simple linear regression coe\ufb03cient\nof the residual on this chosen variable, and then adds it to the current co-\ne\ufb03cient for that variable. This is continued till none of the variables have\ncorrelation with the residuals\u2014i.e. the least-squares \ufb01t when N > p .\nUnlike forward-stepwise regression, none of the other variables are ad-\njusted when a term is added to the model. As a consequence, forward\nstagewise can take many more than psteps to reach the least squares \ufb01t,\nand historically has been dismissed as being ine\ufb03cient. It turns out that\nthis \u201cslow \ufb01tting\u201d can pay dividends in high-dimensional problems. We\nsee in Section 3.8.1 that both forward stagewise and a variant which is\nslowed down even further are quite competitive, especially in very high-\ndimensional problems.\nForward-stagewise regression is included in Figure 3.6. In this example it\ntakes over 1000 steps to get all the correlations below 10\u22124. For subset size\nk, we plotted the error for the last step for which there where knonzero\ncoe\ufb03cients. Although it catches up with the best \ufb01t, it takes longer to\ndo so.", "79": "3.4 Shrinkage Methods 61\n3.3.4 Prostate Cancer Data Example (Continued)\nTable 3.3 shows the coe\ufb03cients from a number of di\ufb00erent selection and\nshrinkage methods. They are best-subset selection using an all-subsets search,\nridge regression , thelasso,principal components regression andpartial least\nsquares . Each method has a complexity parameter, and this was chosen to\nminimize an estimate of prediction error based on tenfold cross-validation;\nfull details are given in Section 7.10. Brie\ufb02y, cross-validation works by divid-\ning the training data randomly into ten equal parts. The learning method\nis \ufb01t\u2014for a range of values of the complexity parameter\u2014to nine-tenths of\nthe data, and the prediction error is computed on the remaining one-tenth.\nThis is done in turn for each one-tenth of the data, and the ten prediction\nerror estimates are averaged. From this we obtain an estimated prediction\nerror curve as a function of the complexity parameter.\nNote that we have already divided these data into a training set of size\n67 and a test set of size 30. Cross-validation is applied to the training set,\nsince selecting the shrinkage parameter is part of the training process. The\ntest set is there to judge the performance of the selected model.\nThe estimated prediction error curves are shown in Figure 3.7. Many of\nthe curves are very \ufb02at over large ranges near their minimum. Included\nare estimated standard error bands for each estimated error rate, based on\nthe ten error estimates computed by cross-validation. We have used the\n\u201cone-standard-error\u201d rule\u2014we pick the most parsimonious model within\none standard error of the minimum (Section 7.10, page 244). Such a rule\nacknowledges the fact that the tradeo\ufb00 curve is estimated with error, and\nhence takes a conservative approach.\nBest-subset selection chose to use the two predictors lcvol andlweight .\nThe last two lines of the table give the average prediction error (and its\nestimated standard error) over the test set.\n3.4 Shrinkage Methods\nBy retaining a subset of the predictors and discarding the rest, subset selec-\ntion produces a model that is interpretable and has possibly lower predic-\ntion error than the full model. However, because it is a discrete process\u2014\nvariables are either retained or discarded\u2014it often exhibits high variance,\nand so doesn\u2019t reduce the prediction error of the full model. Shrinkage\nmethods are more continuous, and don\u2019t su\ufb00er as much from high\nvariability.\n3.4.1 Ridge Regression\nRidge regression shrinks the regression coe\ufb03cients by imposing a penalty\non their size. The ridge coe\ufb03cients minimize a penalized residual sum of", "80": "62 3. Linear Methods for Regression\nSubset SizeCV Error\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022All Subsets\nDegrees of FreedomCV Error\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022Ridge Regression\nShrinkage Factor sCV Error\n0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022Lasso\nNumber of DirectionsCV Error\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Principal Components Regression\nNumber of  DirectionsCV Error\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022Partial Least Squares\nFIGURE 3.7. Estimated prediction error curves and their standard errors f or\nthe various selection and shrinkage methods. Each curve is plo tted as a function\nof the corresponding complexity parameter for that method. The horizontal axis\nhas been chosen so that the model complexity increases as we mov e from left to\nright. The estimates of prediction error and their standard er rors were obtained by\ntenfold cross-validation; full details are given in Section 7. 10. The least complex\nmodel within one standard error of the best is chosen, indicated b y the purple\nvertical broken lines.", "81": "3.4 Shrinkage Methods 63\nTABLE 3.3. Estimated coe\ufb03cients and test error results, for di\ufb00erent subs et\nand shrinkage methods applied to the prostate data. The blank ent ries correspond\nto variables omitted.\nTerm LS Best Subset Ridge Lasso PCR PLS\nIntercept 2.465 2.477 2.452 2.468 2.497 2.452\nlcavol 0.680 0.740 0.420 0.533 0.543 0.419\nlweight 0.263 0.316 0.238 0.169 0.289 0.344\nage \u22120.141 \u22120.046 \u22120.152 \u22120.026\nlbph 0.210 0.162 0.002 0.214 0.220\nsvi 0.305 0.227 0.094 0.315 0.243\nlcp \u22120.288 0.000 \u22120.051 0.079\ngleason \u22120.021 0.040 0.232 0.011\npgg45 0.267 0.133 \u22120.056 0.084\nTest Error 0.521 0.492 0.492 0.479 0.449 0.528\nStd Error 0.179 0.143 0.165 0.164 0.105 0.152\nsquares,\n\u02c6\u03b2ridge= argmin\n\u03b2/braceleftbiggN/summationdisplay\ni=1/parenleftbig\nyi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j/parenrightbig2+\u03bbp/summationdisplay\nj=1\u03b22\nj/bracerightbigg\n. (3.41)\nHere\u03bb\u22650 is a complexity parameter that controls the amount of shrink-\nage: the larger the value of \u03bb, the greater the amount of shrinkage. The\ncoe\ufb03cients are shrunk toward zero (and each other). The idea of penaliz-\ning by the sum-of-squares of the parameters is also used in neural networks,\nwhere it is known as weight decay (Chapter 11).\nAn equivalent way to write the ridge problem is\n\u02c6\u03b2ridge= argmin\n\u03b2N/summationdisplay\ni=1/parenleft\uf8ecig\nyi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j/parenright\uf8ecig2\n,\nsubject top/summationdisplay\nj=1\u03b22\nj\u2264t,(3.42)\nwhich makes explicit the size constraint on the parameters. There is a one-\nto-one correspondence between the parameters \u03bbin (3.41) and tin (3.42).\nWhen there are many correlated variables in a linear regression model,\ntheir coe\ufb03cients can become poorly determined and exhibit high variance.\nA wildly large positive coe\ufb03cient on one variable can be canceled by a\nsimilarly large negative coe\ufb03cient on its correlated cousin. By imposing a\nsize constraint on the coe\ufb03cients, as in (3.42), this problem is alleviated.\nThe ridge solutions are not equivariant under scaling of the inputs, and\nso one normally standardizes the inputs before solving (3.41). In addition,", "82": "64 3. Linear Methods for Regression\nnotice that the intercept \u03b20has been left out of the penalty term. Penal-\nization of the intercept would make the procedure depend on the origin\nchosen for Y; that is, adding a constant cto each of the targets yiwould\nnot simply result in a shift of the predictions by the same amount c. It\ncan be shown (Exercise 3.5) that the solution to (3.41) can be separated\ninto two parts, after reparametrization using centered inputs: each xijgets\nreplaced by xij\u2212\u00afxj. We estimate \u03b20by \u00afy=1\nN/summationtextN\n1yi. The remaining co-\ne\ufb03cients get estimated by a ridge regression without intercept, using the\ncentered xij. Henceforth we assume that this centering has been done, so\nthat the input matrix Xhasp(rather than p+ 1) columns.\nWriting the criterion in (3.41) in matrix form,\nRSS(\u03bb) = (y\u2212X\u03b2)T(y\u2212X\u03b2) +\u03bb\u03b2T\u03b2, (3.43)\nthe ridge regression solutions are easily seen to be\n\u02c6\u03b2ridge= (XTX+\u03bbI)\u22121XTy, (3.44)\nwhereIis the p\u00d7pidentity matrix. Notice that with the choice of quadratic\npenalty \u03b2T\u03b2, the ridge regression solution is again a linear function of\ny. The solution adds a positive constant to the diagonal of XTXbefore\ninversion. This makes the problem nonsingular, even if XTXis not of full\nrank, and was the main motivation for ridge regression when it was \ufb01rst\nintroduced in statistics (Hoerl and Kennard, 1970). Traditional descriptions\nof ridge regression start with de\ufb01nition (3.44). We choose to motivat e it via\n(3.41) and (3.42), as these provide insight into how it works.\nFigure 3.8 shows the ridge coe\ufb03cient estimates for the prostate can-\ncer example, plotted as functions of df( \u03bb), the e\ufb00ective degrees of freedom\nimplied by the penalty \u03bb(de\ufb01ned in (3.50) on page 68). In the case of or-\nthonormal inputs, the ridge estimates are just a scaled version of the least\nsquares estimates, that is, \u02c6\u03b2ridge=\u02c6\u03b2/(1 +\u03bb).\nRidge regression can also be derived as the mean or mode of a poste-\nrior distribution, with a suitably chosen prior distribution. In detail, sup-\nposeyi\u223cN(\u03b20+xT\ni\u03b2,\u03c32), and the parameters \u03b2jare each distributed as\nN(0,\u03c42), independently of one another. Then the (negative) log-posterior\ndensity of \u03b2, with \u03c42and\u03c32assumed known, is equal to the expression\nin curly braces in (3.41), with \u03bb=\u03c32/\u03c42(Exercise 3.6). Thus the ridge\nestimate is the mode of the posterior distribution; since the distribution is\nGaussian, it is also the posterior mean.\nThesingular value decomposition (SVD) of the centered input matrix X\ngives us some additional insight into the nature of ridge regression. This de-\ncomposition is extremely useful in the analysis of many statistical metho ds.\nThe SVD of the N\u00d7pmatrix Xhas the form\nX=UDVT. (3.45)", "83": "3.4 Shrinkage Methods 65Coefficients\n0 2 4 6 8\u22120.2 0.0 0.2 0.4 0.6\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022lcavol\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022lweight\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\nage\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022lbph\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022svi\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\nlcp\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022gleason\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022pgg45\ndf(\u03bb)\nFIGURE 3.8. Pro\ufb01les of ridge coe\ufb03cients for the prostate cancer example, as\nthe tuning parameter \u03bbis varied. Coe\ufb03cients are plotted versus df(\u03bb), the e\ufb00ective\ndegrees of freedom. A vertical line is drawn at df = 5 .0, the value chosen by\ncross-validation.", "84": "66 3. Linear Methods for Regression\nHereUandVareN\u00d7pandp\u00d7porthogonal matrices, with the columns\nofUspanning the column space of X, and the columns of Vspanning the\nrow space. Dis ap\u00d7pdiagonal matrix, with diagonal entries d1\u2265d2\u2265\n\u2264\u2264\u2264 \u2265 dp\u22650 called the singular values of X. If one or more values dj= 0,\nXis singular.\nUsing the singular value decomposition we can write the least squares\n\ufb01tted vector as\nX\u02c6\u03b2ls=X(XTX)\u22121XTy\n=UUTy, (3.46)\nafter some simpli\ufb01cation. Note that UTyare the coordinates of ywith\nrespect to the orthonormal basis U. Note also the similarity with (3.33);\nQandUare generally di\ufb00erent orthogonal bases for the column space of\nX(Exercise 3.8).\nNow the ridge solutions are\nX\u02c6\u03b2ridge=X(XTX+\u03bbI)\u22121XTy\n=U D(D2+\u03bbI)\u22121D UTy\n=p/summationdisplay\nj=1ujd2\nj\nd2\nj+\u03bbuT\njy, (3.47)\nwhere the ujare the columns of U. Note that since \u03bb\u22650, we have d2\nj/(d2\nj+\n\u03bb)\u22641. Like linear regression, ridge regression computes the coordinates of\nywith respect to the orthonormal basis U. It then shrinks these coordinates\nby the factors d2\nj/(d2\nj+\u03bb). This means that a greater amount of shrinkage\nis applied to the coordinates of basis vectors with smaller d2\nj.\nWhat does a small value of d2\njmean? The SVD of the centered matrix\nXis another way of expressing the principal components of the variables\ninX. The sample covariance matrix is given by S=XTX/N, and from\n(3.45) we have\nXTX=VD2VT, (3.48)\nwhich is the eigen decomposition ofXTX(and of S, up to a factor N).\nThe eigenvectors vj(columns of V) are also called the principal compo-\nnents (or Karhunen\u2013Loeve) directions of X. The \ufb01rst principal component\ndirection v1has the property that z1=Xv1has the largest sample vari-\nance amongst all normalized linear combinations of the columns of X. This\nsample variance is easily seen to be\nVar(z1) = Var( Xv1) =d2\n1\nN, (3.49)\nand in fact z1=Xv1=u1d1. The derived variable z1is called the \ufb01rst\nprincipal component of X, and hence u1is the normalized \ufb01rst principal", "85": "3.4 Shrinkage Methods 67\n-4 -2 0 2 4-4 -2 0 2 4ooo\nooooooo\no\no\nooo\noo\noo\noo\nooo\no\no\noooo\noo\no\nooo\nooo\nooooo\noo\nooo\nooo\noo\noooo\noo\no\noo\noo\nooo\nooo\nooo\nooo\no\noo\nooooo\no\noooo\nooo\no\noo\no\noo\noo\no\noo\noooooo\nooo\noo\nooo\noo\no\nooo\noooo\no\nooo\nooo\no\nooo\nooo\noo\no\noooo\no\noooo\noo\noo\noo\nooo\no\nooo\noo\noo\no\noo\noo\noooo\no\nooo\nooo\noo\noo\noo\noo\no\noooLargest Principal\nComponent\nSmallest Principal\nComponent\nX1X2\nFIGURE 3.9. Principal components of some input data points. The largest prin-\ncipal component is the direction that maximizes the variance of t he projected data,\nand the smallest principal component minimizes that variance. Rid ge regression\nprojects yonto these components, and then shrinks the coe\ufb03cients of the low\u2013\nvariance components more than the high-variance components.\ncomponent. Subsequent principal components zjhave maximum variance\nd2\nj/N, subject to being orthogonal to the earlier ones. Conversely the last\nprincipal component has minimum variance. Hence the small singular val-\nuesdjcorrespond to directions in the column space of Xhaving small\nvariance, and ridge regression shrinks these directions the most.\nFigure 3.9 illustrates the principal components of some data points in\ntwo dimensions. If we consider \ufb01tting a linear surface over this domain\n(theY-axis is sticking out of the page), the con\ufb01guration of the data allow\nus to determine its gradient more accurately in the long direction than\nthe short. Ridge regression protects against the potentially high variance\nof gradients estimated in the short directions. The implicit assumption is\nthat the response will tend to vary most in the directions of high variance\nof the inputs. This is often a reasonable assumption, since predictors are\noften chosen for study because they vary with the response variable, but\nneed not hold in general.", "86": "68 3. Linear Methods for Regression\nIn Figure 3.7 we have plotted the estimated prediction error versus the\nquantity\ndf(\u03bb) = tr[ X(XTX+\u03bbI)\u22121XT],\n= tr( H\u03bb)\n=p/summationdisplay\nj=1d2\nj\nd2\nj+\u03bb. (3.50)\nThis monotone decreasing function of \u03bbis the e\ufb00ective degrees of freedom\nof the ridge regression \ufb01t. Usually in a linear-regression \ufb01t with pvariables,\nthe degrees-of-freedom of the \ufb01t is p, the number of free parameters. The\nidea is that although all pcoe\ufb03cients in a ridge \ufb01t will be non-zero, they\nare \ufb01t in a restricted fashion controlled by \u03bb. Note that df( \u03bb) =pwhen\n\u03bb= 0 (no regularization) and df( \u03bb)\u21920 as\u03bb\u2192 \u221e . Of course there\nis always an additional one degree of freedom for the intercept, which was\nremoved apriori . This de\ufb01nition is motivated in more detail in Section 3.4.4\nand Sections 7.4\u20137.6. In Figure 3.7 the minimum occurs at df( \u03bb) = 5 .0.\nTable 3.3 shows that ridge regression reduces the test error of the full least\nsquares estimates by a small amount.\n3.4.2 The Lasso\nThe lasso is a shrinkage method like ridge, with subtle but important dif-\nferences. The lasso estimate is de\ufb01ned by\n\u02c6\u03b2lasso= argmin\n\u03b2N/summationdisplay\ni=1/parenleft\uf8ecig\nyi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j/parenright\uf8ecig2\nsubject top/summationdisplay\nj=1|\u03b2j| \u2264t. (3.51)\nJust as in ridge regression, we can re-parametrize the constant \u03b20by stan-\ndardizing the predictors; the solution for \u02c6\u03b20is \u00afy, and thereafter we \ufb01t a\nmodel without an intercept (Exercise 3.5). In the signal processing litera-\nture, the lasso is also known as basis pursuit (Chen et al., 1998).\nWe can also write the lasso problem in the equivalent Lagrangian form\n\u02c6\u03b2lasso= argmin\n\u03b2/braceleftbigg1\n2N/summationdisplay\ni=1/parenleftbig\nyi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j/parenrightbig2+\u03bbp/summationdisplay\nj=1|\u03b2j|/bracerightbigg\n.(3.52)\nNotice the similarity to the ridge regression problem (3.42) or (3.41) : the\nL2ridge penalty/summationtextp\n1\u03b22\njis replaced by the L1lasso penalty/summationtextp\n1|\u03b2j|. This\nlatter constraint makes the solutions nonlinear in the yi, and there is no\nclosed form expression as in ridge regression. Computing the lasso solution", "87": "3.4 Shrinkage Methods 69\nis a quadratic programming problem, although we see in Section 3.4.4 that\ne\ufb03cient algorithms are available for computing the entire path of solutions\nas\u03bbis varied, with the same computational cost as for ridge regression.\nBecause of the nature of the constraint, making tsu\ufb03ciently small will\ncause some of the coe\ufb03cients to be exactly zero. Thus the lasso does a kind\nof continuous subset selection. If tis chosen larger than t0=/summationtextp\n1|\u02c6\u03b2j|(where\n\u02c6\u03b2j=\u02c6\u03b2ls\nj, the least squares estimates), then the lasso estimates are the \u02c6\u03b2j\u2019s.\nOn the other hand, for t=t0/2 say, then the least squares coe\ufb03cients are\nshrunk by about 50% on average. However, the nature of the shrinkage\nis not obvious, and we investigate it further in Section 3.4.4 below. Like\nthe subset size in variable subset selection, or the penalty parameter in\nridge regression, tshould be adaptively chosen to minimize an estimate of\nexpected prediction error.\nIn Figure 3.7, for ease of interpretation, we have plotted the lasso pre-\ndiction error estimates versus the standardized parameter s=t//summationtextp\n1|\u02c6\u03b2j|.\nA value \u02c6 s\u22480.36 was chosen by 10-fold cross-validation; this caused four\ncoe\ufb03cients to be set to zero (\ufb01fth column of Table 3.3). The resulting\nmodel has the second lowest test error, slightly lower than the full least\nsquares model, but the standard errors of the test error estimates (last line\nof Table 3.3) are fairly large.\nFigure 3.10 shows the lasso coe\ufb03cients as the standardized tuning pa-\nrameter s=t//summationtextp\n1|\u02c6\u03b2j|is varied. At s= 1.0 these are the least squares\nestimates; they decrease to 0 as s\u21920. This decrease is not always strictly\nmonotonic, although it is in this example. A vertical line is drawn at\ns= 0.36, the value chosen by cross-validation.\n3.4.3 Discussion: Subset Selection, Ridge Regression and t he\nLasso\nIn this section we discuss and compare the three approaches discussed so far\nfor restricting the linear regression model: subset selection, ridge regression\nand the lasso.\nIn the case of an orthonormal input matrix Xthe three procedures have\nexplicit solutions. Each method applies a simple transformation to the leas t\nsquares estimate \u02c6\u03b2j, as detailed in Table 3.4.\nRidge regression does a proportional shrinkage. Lasso translates each\ncoe\ufb03cient by a constant factor \u03bb, truncating at zero. This is called \u201csoft\nthresholding,\u201d and is used in the context of wavelet-based smoothing in Sec-\ntion 5.9. Best-subset selection drops all variables with coe\ufb03cients smaller\nthan the Mth largest; this is a form of \u201chard-thresholding.\u201d\nBack to the nonorthogonal case; some pictures help understand their re-\nlationship. Figure 3.11 depicts the lasso (left) and ridge regression (righ t)\nwhen there are only two parameters. The residual sum of squares has ellip-\ntical contours, centered at the full least squares estimate. The constraint", "88": "70 3. Linear Methods for Regression\n0.0 0.2 0.4 0.6 0.8 1.0\u22120.2 0.0 0.2 0.4 0.6\nShrinkage Factor sCoefficientslcavol\nlweight\nagelbphsvi\nlcpgleasonpgg45\nFIGURE 3.10. Pro\ufb01les of lasso coe\ufb03cients, as the tuning parameter tis varied.\nCoe\ufb03cients are plotted versus s=t/Pp\n1|\u02c6\u03b2j|. A vertical line is drawn at s= 0.36,\nthe value chosen by cross-validation. Compare Figure 3.8 on p age 65; the lasso\npro\ufb01les hit zero, while those for ridge do not. The pro\ufb01les are pi ece-wise linear,\nand so are computed only at the points displayed; see Section 3.4. 4 for details.", "89": "3.4 Shrinkage Methods 71\nTABLE 3.4. Estimators of \u03b2jin the case of orthonormal columns of X.Mand\u03bb\nare constants chosen by the corresponding techniques; signdenotes the sign of its\nargument ( \u00b11), and x+denotes \u201cpositive part\u201d of x. Below the table, estimators\nare shown by broken red lines. The 45\u25e6line in gray shows the unrestricted estimate\nfor reference.\nEstimator Formula\nBest subset (size M)\u02c6\u03b2j\u2264I(|\u02c6\u03b2j| \u2265 |\u02c6\u03b2(M)|)\nRidge \u02c6\u03b2j/(1 +\u03bb)\nLasso sign( \u02c6\u03b2j)(|\u02c6\u03b2j| \u2212\u03bb)+\n(0,0) (0,0) (0,0)|\u02c6\u03b2(M)|\u03bbBest Subset Ridge Lasso\n\u03b2^\u03b2^ 2. . \u03b2\n1\u03b22\n\u03b21\u03b2\nFIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\n(right). Shown are contours of the error and constraint functions. T he solid blue\nareas are the constraint regions |\u03b21|+|\u03b22| \u2264tand\u03b22\n1+\u03b22\n2\u2264t2, respectively,\nwhile the red ellipses are the contours of the least squares er ror function.", "90": "72 3. Linear Methods for Regression\nregion for ridge regression is the disk \u03b22\n1+\u03b22\n2\u2264t, while that for lasso is\nthe diamond |\u03b21|+|\u03b22| \u2264t. Both methods \ufb01nd the \ufb01rst point where the\nelliptical contours hit the constraint region. Unlike the disk, the diamond\nhas corners; if the solution occurs at a corner, then it has one parameter\n\u03b2jequal to zero. When p >2, the diamond becomes a rhomboid, and has\nmany corners, \ufb02at edges and faces; there are many more opportunities for\nthe estimated parameters to be zero.\nWe can generalize ridge regression and the lasso, and view them as Bayes\nestimates. Consider the criterion\n\u02dc\u03b2= argmin\n\u03b2/braceleft\uf8eciggN/summationdisplay\ni=1/parenleftbig\nyi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j/parenrightbig2+\u03bbp/summationdisplay\nj=1|\u03b2j|q/braceright\uf8ecigg\n(3.53)\nforq\u22650. The contours of constant value of/summationtext\nj|\u03b2j|qare shown in Fig-\nure 3.12, for the case of two inputs.\nThinking of |\u03b2j|qas the log-prior density for \u03b2j, these are also the equi-\ncontours of the prior distribution of the parameters. The value q= 0 corre-\nsponds to variable subset selection, as the penalty simply counts the number\nof nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge\nregression. Notice that for q\u22641, the prior is not uniform in direction, but\nconcentrates more mass in the coordinate directions. The prior correspond-\ning to the q= 1 case is an independent double exponential (or Laplace)\ndistribution for each input, with density (1 /2\u03c4)exp(\u2212|\u03b2|/\u03c4) and \u03c4= 1/\u03bb.\nThe case q= 1 (lasso) is the smallest qsuch that the constraint region\nis convex; non-convex constraint regions make the optimization problem\nmore di\ufb03cult.\nIn this view, the lasso, ridge regression and best subset selection are\nBayes estimates with di\ufb00erent priors. Note, however, that they are derived\nas posterior modes, that is, maximizers of the posterior. It is more com mon\nto use the mean of the posterior as the Bayes estimate. Ridge regression is\nalso the posterior mean, but the lasso and best subset selection are not.\nLooking again at the criterion (3.53), we might try using other values\nofqbesides 0, 1, or 2. Although one might consider estimating qfrom\nthe data, our experience is that it is not worth the e\ufb00ort for the extra\nvariance incurred. Values of q\u2208(1,2) suggest a compromise between the\nlasso and ridge regression. Although this is the case, with q >1,|\u03b2j|qis\ndi\ufb00erentiable at 0, and so does not share the ability of lasso ( q= 1) for\nq= 4 q= 2 q= 1 q= 0.5 q= 0.1\nFIGURE 3.12. Contours of constant value ofP\nj|\u03b2j|qfor given values of q.", "91": "3.4 Shrinkage Methods 73\nq= 1.2 \u03b1= 0.2\nLq Elastic Net\nFIGURE 3.13. Contours of constant value ofP\nj|\u03b2j|qforq= 1.2(left plot),\nand the elastic-net penaltyP\nj(\u03b1\u03b22\nj+(1\u2212\u03b1)|\u03b2j|)for\u03b1= 0.2(right plot). Although\nvisually very similar, the elastic-net has sharp (non-di\ufb00erent iable) corners, while\ntheq= 1.2penalty does not.\nsetting coe\ufb03cients exactly to zero. Partly for this reason as well as for\ncomputational tractability, Zou and Hastie (2005) introduced the elastic-\nnetpenalty\n\u03bbp/summationdisplay\nj=1/parenleftbig\n\u03b1\u03b22\nj+ (1\u2212\u03b1)|\u03b2j|/parenrightbig\n, (3.54)\na di\ufb00erent compromise between ridge and lasso. Figure 3.13 compares the\nLqpenalty with q= 1.2 and the elastic-net penalty with \u03b1= 0.2; it is\nhard to detect the di\ufb00erence by eye. The elastic-net selects variables like\nthe lasso, and shrinks together the coe\ufb03cients of correlated predictors like\nridge. It also has considerable computational advantages over the Lqpenal-\nties. We discuss the elastic-net further in Section 18.4.\n3.4.4 Least Angle Regression\nLeast angle regression (LAR) is a relative newcomer (Efron et al., 2004) ,\nand can be viewed as a kind of \u201cdemocratic\u201d version of forward stepwise\nregression (Section 3.3.2). As we will see, LAR is intimately connected\nwith the lasso, and in fact provides an extremely e\ufb03cient algorithm for\ncomputing the entire lasso path as in Figure 3.10.\nForward stepwise regression builds a model sequentially, adding one vari-\nable at a time. At each step, it identi\ufb01es the best variable to include in the\nactive set , and then updates the least squares \ufb01t to include all the active\nvariables.\nLeast angle regression uses a similar strategy, but only enters \u201cas much\u201d\nof a predictor as it deserves. At the \ufb01rst step it identi\ufb01es the variable\nmost correlated with the response. Rather than \ufb01t this variable completely,\nLAR moves the coe\ufb03cient of this variable continuously toward its least-\nsquares value (causing its correlation with the evolving residual to decrease\nin absolute value). As soon as another variable \u201ccatches up\u201d in terms of\ncorrelation with the residual, the process is paused. The second variable\nthen joins the active set, and their coe\ufb03cients are moved together in a way\nthat keeps their correlations tied and decreasing. This process is continued", "92": "74 3. Linear Methods for Regression\nuntil all the variables are in the model, and ends at the full least-squares\n\ufb01t. Algorithm 3.2 provides the details. The termination condition in step 5\nrequires some explanation. If p > N \u22121, the LAR algorithm reaches a zero\nresidual solution after N\u22121 steps (the \u22121 is because we have centered the\ndata).\nAlgorithm 3.2 Least Angle Regression.\n1. Standardize the predictors to have mean zero and unit norm. Start\nwith the residual r=y\u2212\u00afy,\u03b21,\u03b22,... ,\u03b2 p= 0.\n2. Find the predictor xjmost correlated with r.\n3. Move \u03b2jfrom 0 towards its least-squares coe\ufb03cient \u221dan}\u230a\u2207a\u230bketle{txj,r\u221dan}\u230a\u2207a\u230bket\u2207i}ht, until some\nother competitor xkhas as much correlation with the current residual\nas does xj.\n4. Move \u03b2jand\u03b2kin the direction de\ufb01ned by their joint least squares\ncoe\ufb03cient of the current residual on ( xj,xk), until some other com-\npetitor xlhas as much correlation with the current residual.\n5. Continue in this way until all ppredictors have been entered. After\nmin(N\u22121,p) steps, we arrive at the full least-squares solution.\nSuppose Akis the active set of variables at the beginning of the kth\nstep, and let \u03b2Akbe the coe\ufb03cient vector for these variables at this step;\nthere will be k\u22121 nonzero values, and the one just entered will be zero. If\nrk=y\u2212XAk\u03b2Akis the current residual, then the direction for this step is\n\u03b4k= (XT\nAkXAk)\u22121XT\nAkrk. (3.55)\nThe coe\ufb03cient pro\ufb01le then evolves as \u03b2Ak(\u03b1) =\u03b2Ak+\u03b1\u2264\u03b4k. Exercise 3.23\nveri\ufb01es that the directions chosen in this fashion do what is claimed: keep\nthe correlations tied and decreasing. If the \ufb01t vector at the beginning of\nthis step is \u02c6fk, then it evolves as \u02c6fk(\u03b1) =\u02c6fk+\u03b1\u2264uk, where uk=XAk\u03b4k\nis the new \ufb01t direction. The name \u201cleast angle\u201d arises from a geometrical\ninterpretation of this process; ukmakes the smallest (and equal) angle\nwith each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the\nabsolute correlations decreasing and joining ranks with each step of the\nLAR algorithm, using simulated data.\nBy construction the coe\ufb03cients in LAR change in a piecewise linear fash-\nion. Figure 3.15 [left panel] shows the LAR coe\ufb03cient pro\ufb01le evolving as a\nfunction of their L1arc length2. Note that we do not need to take small\n2TheL1arc-length of a di\ufb00erentiable curve \u03b2(s) fors\u2208[0, S] is given by TV( \u03b2, S) =RS\n0||\u02d9\u03b2(s)||1ds,where \u02d9\u03b2(s) =\u2202\u03b2(s)/\u2202s. For the piecewise-linear LAR coe\ufb03cient pro\ufb01le,\nthis amounts to summing the L1norms of the changes in coe\ufb03cients from step to step.", "93": "3.4 Shrinkage Methods 75\n0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1\nL1Arc LengthAbsolute Correlations\nFIGURE 3.14. Progression of the absolute correlations during each step of t he\nLAR procedure, using a simulated data set with six predictors . The labels at the\ntop of the plot indicate which variables enter the active set at each step. The step\nlength are measured in units of L1arc length.\n0 5 10 15\u22121.5 \u22121.0 \u22120.5 0.0 0.5Least Angle Regression\n0 5 10 15\u22121.5 \u22121.0 \u22120.5 0.0 0.5Lasso\nL1Arc Length L1Arc Length\nCoe\ufb03cientsCoe\ufb03cients\nFIGURE 3.15. Left panel shows the LAR coe\ufb03cient pro\ufb01les on the simulated\ndata, as a function of the L1arc length. The right panel shows the Lasso pro\ufb01le.\nThey are identical until the dark-blue coe\ufb03cient crosses zero a t an arc length of\nabout 18.", "94": "76 3. Linear Methods for Regression\nsteps and recheck the correlations in step 3; using knowledge of the covari-\nance of the predictors and the piecewise linearity of the algorithm, we can\nwork out the exact step length at the beginning of each step (Exercise 3.25).\nThe right panel of Figure 3.15 shows the lasso coe\ufb03cient pro\ufb01les on the\nsame data. They are almost identical to those in the left panel, and di\ufb00er\nfor the \ufb01rst time when the blue coe\ufb03cient passes back through zero. For the\nprostate data, the LAR coe\ufb03cient pro\ufb01le turns out to be identical to the\nlasso pro\ufb01le in Figure 3.10, which never crosses zero. These observations\nlead to a simple modi\ufb01cation of the LAR algorithm that gives the entire\nlasso path, which is also piecewise-linear.\nAlgorithm 3.2a Least Angle Regression: Lasso Modi\ufb01cation .\n4a. If a non-zero coe\ufb03cient hits zero, drop its variable from the active set\nof variables and recompute the current joint least squares direction.\nThe LAR(lasso) algorithm is extremely e\ufb03cient, requiring the same order\nof computation as that of a single least squares \ufb01t using the ppredictors.\nLeast angle regression always takes psteps to get to the full least squares\nestimates. The lasso path can have more than psteps, although the two\nare often quite similar. Algorithm 3.2 with the lasso modi\ufb01cation 3. 2a is\nan e\ufb03cient way of computing the solution to any lasso problem, especially\nwhen p\u226bN. Osborne et al. (2000a) also discovered a piecewise-linear path\nfor computing the lasso, which they called a homotopy algorithm.\nWe now give a heuristic argument for why these procedures are so similar.\nAlthough the LAR algorithm is stated in terms of correlations, if the input\nfeatures are standardized, it is equivalent and easier to work with inner-\nproducts. Suppose Ais the active set of variables at some stage in the\nalgorithm, tied in their absolute inner-product with the current residuals\ny\u2212X\u03b2. We can express this as\nxT\nj(y\u2212X\u03b2) =\u03b3\u2264sj,\u2200j\u2208 A (3.56)\nwhere sj\u2208 {\u22121,1}indicates the sign of the inner-product, and \u03b3is the\ncommon value. Also |xT\nk(y\u2212X\u03b2)| \u2264\u03b3\u2200k\u221dne}ationslash\u2208 A. Now consider the lasso\ncriterion (3.52), which we write in vector form\nR(\u03b2) =1\n2||y\u2212X\u03b2||2\n2+\u03bb||\u03b2||1. (3.57)\nLetBbe the active set of variables in the solution for a given value of \u03bb.\nFor these variables R(\u03b2) is di\ufb00erentiable, and the stationarity conditions\ngive\nxT\nj(y\u2212X\u03b2) =\u03bb\u2264sign(\u03b2j),\u2200j\u2208 B (3.58)\nComparing (3.58) with (3.56), we see that they are identical only if the\nsign of \u03b2jmatches the sign of the inner product. That is why the LAR", "95": "3.4 Shrinkage Methods 77\nalgorithm and lasso start to di\ufb00er when an active coe\ufb03cient passes through\nzero; condition (3.58) is violated for that variable, and it is kicked out o f the\nactive set B. Exercise 3.23 shows that these equations imply a piecewise-\nlinear coe\ufb03cient pro\ufb01le as \u03bbdecreases. The stationarity conditions for the\nnon-active variables require that\n|xT\nk(y\u2212X\u03b2)| \u2264\u03bb,\u2200k\u221dne}ationslash\u2208 B, (3.59)\nwhich again agrees with the LAR algorithm.\nFigure 3.16 compares LAR and lasso to forward stepwise and stagewise\nregression. The setup is the same as in Figure 3.6 on page 59, except here\nN= 100 here rather than 300, so the problem is more di\ufb03cult. We see\nthat the more aggressive forward stepwise starts to over\ufb01t quite earl y (well\nbefore the 10 true variables can enter the model), and ultimately performs\nworse than the slower forward stagewise regression. The behavior of LAR\nand lasso is similar to that of forward stagewise regression. Increment al\nforward stagewise is similar to LAR and lasso, and is described in Sec-\ntion 3.8.1.\nDegrees-of-Freedom Formula for LAR and Lasso\nSuppose that we \ufb01t a linear model via the least angle regression procedure,\nstopping at some number of steps k < p, or equivalently using a lasso bound\ntthat produces a constrained version of the full least squares \ufb01t. How many\nparameters, or \u201cdegrees of freedom\u201d have we used?\nConsider \ufb01rst a linear regression using a subset of kfeatures. If this subset\nis prespeci\ufb01ed in advance without reference to the training data, then the\ndegrees of freedom used in the \ufb01tted model is de\ufb01ned to be k. Indeed, in\nclassical statistics, the number of linearly independent parameters is what\nis meant by \u201cdegrees of freedom.\u201d Alternatively, suppose that we carry out\na best subset selection to determine the \u201coptimal\u201d set of kpredictors. Then\nthe resulting model has kparameters, but in some sense we have used up\nmore than kdegrees of freedom.\nWe need a more general de\ufb01nition for the e\ufb00ective degrees of freedom of\nan adaptively \ufb01tted model. We de\ufb01ne the degrees of freedom of the \ufb01tted\nvector \u02c6y= (\u02c6y1,\u02c6y2,... ,\u02c6yN) as\ndf(\u02c6y) =1\n\u03c32N/summationdisplay\ni=1Cov(\u02c6yi,yi). (3.60)\nHere Cov(\u02c6 yi,yi) refers to the sampling covariance between the predicted\nvalue \u02c6 yiand its corresponding outcome value yi. This makes intuitive sense:\nthe harder that we \ufb01t to the data, the larger this covariance and hence\ndf(\u02c6y). Expression (3.60) is a useful notion of degrees of freedom, one that\ncan be applied to any model prediction \u02c6y. This includes models that are", "96": "78 3. Linear Methods for Regression\n0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise\nLAR\nLasso\nForward Stagewise\nIncremental Forward StagewiseE||\u02c6\u03b2(k)\u2212\u03b2||2\nFraction of L1arc-length\nFIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\nstagewise (FS) and incremental forward stagewise (FS 0) regression. The setup\nis the same as in Figure 3.6, except N= 100 here rather than 300. Here the\nslower FS regression ultimately outperforms forward stepw ise. LAR and lasso\nshow similar behavior to FS and FS 0. Since the procedures take di\ufb00erent numbers\nof steps (across simulation replicates and methods), we plot the MSE as a function\nof the fraction of total L1arc-length toward the least-squares \ufb01t.\nadaptively \ufb01tted to the training data. This de\ufb01nition is motivated and\ndiscussed further in Sections 7.4\u20137.6.\nNow for a linear regression with k\ufb01xed predictors, it is easy to show\nthat df( \u02c6y) =k. Likewise for ridge regression, this de\ufb01nition leads to the\nclosed-form expression (3.50) on page 68: df( \u02c6y) = tr( S\u03bb). In both these\ncases, (3.60) is simple to evaluate because the \ufb01t \u02c6y=H\u03bbyis linear in y.\nIf we think about de\ufb01nition (3.60) in the context of a best subset selection\nof size k, it seems clear that df( \u02c6y) will be larger than k, and this can be\nveri\ufb01ed by estimating Cov(\u02c6 yi,yi)/\u03c32directly by simulation. However there\nis no closed form method for estimating df( \u02c6y) for best subset selection.\nFor LAR and lasso, something magical happens. These techniques are\nadaptive in a smoother way than best subset selection, and hence estimation\nof degrees of freedom is more tractable. Speci\ufb01cally it can be shown that\nafter the kth step of the LAR procedure, the e\ufb00ective degrees of freedom of\nthe \ufb01t vector is exactly k. Now for the lasso, the (modi\ufb01ed) LAR procedure", "97": "3.5 Methods Using Derived Input Directions 79\noften takes more than psteps, since predictors can drop out. Hence the\nde\ufb01nition is a little di\ufb00erent; for the lasso, at any stage df( \u02c6y) approximately\nequals the number of predictors in the model. While this approximation\nworks reasonably well anywhere in the lasso path, for each kit works best\nat the lastmodel in the sequence that contains kpredictors. A detailed\nstudy of the degrees of freedom for the lasso may be found in Zou et al.\n(2007).\n3.5 Methods Using Derived Input Directions\nIn many situations we have a large number of inputs, often very correlated.\nThe methods in this section produce a small number of linear combinations\nZm, m= 1,... ,M of the original inputs Xj, and the Zmare then used in\nplace of the Xjas inputs in the regression. The methods di\ufb00er in how the\nlinear combinations are constructed.\n3.5.1 Principal Components Regression\nIn this approach the linear combinations Zmused are the principal com-\nponents as de\ufb01ned in Section 3.4.1 above.\nPrincipal component regression forms the derived input columns zm=\nXvm, and then regresses yonz1,z2,... ,zMfor some M\u2264p. Since the zm\nare orthogonal, this regression is just a sum of univariate regressions:\n\u02c6ypcr\n(M)= \u00afy1+M/summationdisplay\nm=1\u02c6\u03b8mzm, (3.61)\nwhere \u02c6\u03b8m=\u221dan}\u230a\u2207a\u230bketle{tzm,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht/\u221dan}\u230a\u2207a\u230bketle{tzm,zm\u221dan}\u230a\u2207a\u230bket\u2207i}ht. Since the zmare each linear combinations\nof the original xj, we can express the solution (3.61) in terms of coe\ufb03cients\nof thexj(Exercise 3.13):\n\u02c6\u03b2pcr(M) =M/summationdisplay\nm=1\u02c6\u03b8mvm. (3.62)\nAs with ridge regression, principal components depend on the scaling of\nthe inputs, so typically we \ufb01rst standardize them. Note that if M=p, we\nwould just get back the usual least squares estimates, since the columns of\nZ=UDspan the column space of X. ForM < p we get a reduced regres-\nsion. We see that principal components regression is very similar to ridge\nregression: both operate via the principal components of the input ma-\ntrix. Ridge regression shrinks the coe\ufb03cients of the principal components\n(Figure 3.17), shrinking more depending on the size of the corresponding\neigenvalue; principal components regression discards the p\u2212Msmallest\neigenvalue components. Figure 3.17 illustrates this.", "98": "80 3. Linear Methods for Regression\nIndexShrinkage Factor\n2 4 6 80.0 0.2 0.4 0.6 0.8 1.0\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n\u2022 \u2022ridge\npcr\nFIGURE 3.17. Ridge regression shrinks the regression coe\ufb03cients of the pri n-\ncipal components, using shrinkage factors d2\nj/(d2\nj+\u03bb)as in (3.47). Principal\ncomponent regression truncates them. Shown are the shrinkage and t runcation\npatterns corresponding to Figure 3.7, as a function of the princip al component\nindex.\nIn Figure 3.7 we see that cross-validation suggests seven terms; the re-\nsulting model has the lowest test error in Table 3.3.\n3.5.2 Partial Least Squares\nThis technique also constructs a set of linear combinations of the inputs\nfor regression, but unlike principal components regression it uses y(in ad-\ndition to X) for this construction. Like principal component regression,\npartial least squares (PLS) is not scale invariant, so we assume that eac h\nxjis standardized to have mean 0 and variance 1. PLS begins by com-\nputing \u02c6 \u03d51j=\u221dan}\u230a\u2207a\u230bketle{txj,y\u221dan}\u230a\u2207a\u230bket\u2207i}htfor each j. From this we construct the derived input\nz1=/summationtext\nj\u02c6\u03d51jxj, which is the \ufb01rst partial least squares direction. Hence\nin the construction of each zm, the inputs are weighted by the strength\nof their univariate e\ufb00ect on y3. The outcome yis regressed on z1giving\ncoe\ufb03cient \u02c6\u03b81, and then we orthogonalize x1,... ,xpwith respect to z1. We\ncontinue this process, until M\u2264pdirections have been obtained. In this\nmanner, partial least squares produces a sequence of derived, orthogonal\ninputs or directions z1,z2,... ,zM. As with principal-component regres-\nsion, if we were to construct all M=pdirections, we would get back a\nsolution equivalent to the usual least squares estimates; using M < p di-\nrections produces a reduced regression. The procedure is described fully in\nAlgorithm 3.3.\n3Since the xjare standardized, the \ufb01rst directions \u02c6 \u03d51jare the univariate regression\ncoe\ufb03cients (up to an irrelevant constant); this is not the ca se for subsequent directions.", "99": "3.5 Methods Using Derived Input Directions 81\nAlgorithm 3.3 Partial Least Squares.\n1. Standardize each xjto have mean zero and variance one. Set \u02c6y(0)=\n\u00afy1, andx(0)\nj=xj, j= 1,... ,p .\n2. For m= 1,2,... ,p\n(a)zm=/summationtextp\nj=1\u02c6\u03d5mjx(m\u22121)\nj, where \u02c6 \u03d5mj=\u221dan}\u230a\u2207a\u230bketle{tx(m\u22121)\nj,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht.\n(b)\u02c6\u03b8m=\u221dan}\u230a\u2207a\u230bketle{tzm,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht/\u221dan}\u230a\u2207a\u230bketle{tzm,zm\u221dan}\u230a\u2207a\u230bket\u2207i}ht.\n(c)\u02c6y(m)=\u02c6y(m\u22121)+\u02c6\u03b8mzm.\n(d) Orthogonalize each x(m\u22121)\nj with respect to zm:x(m)\nj=x(m\u22121)\nj\u2212\n[\u221dan}\u230a\u2207a\u230bketle{tzm,x(m\u22121)\nj\u221dan}\u230a\u2207a\u230bket\u2207i}ht/\u221dan}\u230a\u2207a\u230bketle{tzm,zm\u221dan}\u230a\u2207a\u230bket\u2207i}ht]zm,j= 1,2,... ,p .\n3. Output the sequence of \ufb01tted vectors {\u02c6y(m)}p\n1. Since the {z\u2113}m\n1are\nlinear in the original xj, so is \u02c6y(m)=X\u02c6\u03b2pls(m). These linear coe\ufb03-\ncients can be recovered from the sequence of PLS transformations.\nIn the prostate cancer example, cross-validation chose M= 2 PLS direc-\ntions in Figure 3.7. This produced the model given in the rightmost column\nof Table 3.3.\nWhat optimization problem is partial least squares solving? Since it uses\nthe response yto construct its directions, its solution path is a nonlinear\nfunction of y. It can be shown (Exercise 3.15) that partial least squares\nseeks directions that have high variance andhave high correlation with the\nresponse, in contrast to principal components regression which keys only\non high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In\nparticular, the mth principal component direction vmsolves:\nmax \u03b1Var(X\u03b1) (3.63)\nsubject to ||\u03b1||= 1, \u03b1TSv\u2113= 0, \u2113= 1,... ,m \u22121,\nwhereSis the sample covariance matrix of the xj. The conditions \u03b1TSv\u2113=\n0 ensures that zm=X\u03b1is uncorrelated with all the previous linear com-\nbinations z\u2113=Xv\u2113. The mth PLS direction \u02c6 \u03d5msolves:\nmax \u03b1Corr2(y,X\u03b1)Var(X\u03b1) (3.64)\nsubject to ||\u03b1||= 1, \u03b1TS\u02c6\u03d5\u2113= 0, \u2113= 1,... ,m \u22121.\nFurther analysis reveals that the variance aspect tends to dominate, and\nso partial least squares behaves much like ridge regression and principal\ncomponents regression. We discuss this further in the next section.\nIf the input matrix Xis orthogonal, then partial least squares \ufb01nds the\nleast squares estimates after m= 1 steps. Subsequent steps have no e\ufb00ect", "100": "82 3. Linear Methods for Regression\nsince the \u02c6 \u03d5mjare zero for m >1 (Exercise 3.14). It can also be shown that\nthe sequence of PLS coe\ufb03cients for m= 1,2,... ,p represents the conjugate\ngradient sequence for computing the least squares solutions (Exercise 3.18).\n3.6 Discussion: A Comparison of the Selection and\nShrinkage Methods\nThere are some simple settings where we can understand better the rela-\ntionship between the di\ufb00erent methods described above. Consider an exam-\nple with two correlated inputs X1andX2, with correlation \u03c1. We assume\nthat the true regression coe\ufb03cients are \u03b21= 4 and \u03b22= 2. Figure 3.18\nshows the coe\ufb03cient pro\ufb01les for the di\ufb00erent methods, as their tuning pa-\nrameters are varied. The top panel has \u03c1= 0.5, the bottom panel \u03c1=\u22120.5.\nThe tuning parameters for ridge and lasso vary over a continuous range,\nwhile best subset, PLS and PCR take just two discrete steps to the least\nsquares solution. In the top panel, starting at the origin, ridge regression\nshrinks the coe\ufb03cients together until it \ufb01nally converges to least squares.\nPLS and PCR show similar behavior to ridge, although are discrete and\nmore extreme. Best subset overshoots the solution and then backtracks.\nThe behavior of the lasso is intermediate to the other methods. When the\ncorrelation is negative (lower panel), again PLS and PCR roughly track\nthe ridge path, while all of the methods are more similar to one another.\nIt is interesting to compare the shrinkage behavior of these di\ufb00erent\nmethods. Recall that ridge regression shrinks all directions, but shrinks\nlow-variance directions more. Principal components regression leaves M\nhigh-variance directions alone, and discards the rest. Interestingly, it can\nbe shown that partial least squares also tends to shrink the low-variance\ndirections, but can actually in\ufb02ate some of the higher variance directions.\nThis can make PLS a little unstable, and cause it to have slightly higher\nprediction error compared to ridge regression. A full study is given in Frank\nand Friedman (1993). These authors conclude that for minimizing predic-\ntion error, ridge regression is generally preferable to variable subset selec-\ntion, principal components regression and partial least squares. However\nthe improvement over the latter two methods was only slight.\nTo summarize, PLS, PCR and ridge regression tend to behave similarly.\nRidge regression may be preferred because it shrinks smoothly, rather than\nin discrete steps. Lasso falls somewhere between ridge regression and best\nsubset regression, and enjoys some of the properties of each.", "101": "3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\n0Ridge\nLasso\nBest SubsetPLS PCR\n\u2022\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\nRidge\nBest Subset\nPLS\nPCRLasso\u2022\n0\u03c1= 0.5\n\u03c1=\u22120.5\n\u03b21\u03b21\u03b22 \u03b22\nFIGURE 3.18. Coe\ufb03cient pro\ufb01les from di\ufb00erent methods for a simple problem:\ntwo inputs with correlation \u00b10.5, and the true regression coe\ufb03cients \u03b2= (4,2).", "102": "84 3. Linear Methods for Regression\n3.7 Multiple Outcome Shrinkage and Selection\nAs noted in Section 3.2.4, the least squares estimates in a multiple-output\nlinear model are simply the individual least squares estimates for each of\nthe outputs.\nTo apply selection and shrinkage methods in the multiple output case,\none could apply a univariate technique individually to each outcome or si-\nmultaneously to all outcomes. With ridge regression, for example, we could\napply formula (3.44) to each of the Kcolumns of the outcome matrix Y,\nusing possibly di\ufb00erent parameters \u03bb, or apply it to all columns using the\nsame value of \u03bb. The former strategy would allow di\ufb00erent amounts of\nregularization to be applied to di\ufb00erent outcomes but require estimation\nofkseparate regularization parameters \u03bb1,... ,\u03bb k, while the latter would\npermit all koutputs to be used in estimating the sole regularization pa-\nrameter \u03bb.\nOther more sophisticated shrinkage and selection strategies that exploit\ncorrelations in the di\ufb00erent responses can be helpful in the multiple output\ncase. Suppose for example that among the outputs we have\nYk=f(X) +\u03b5k (3.65)\nY\u2113=f(X) +\u03b5\u2113; (3.66)\ni.e., (3.65) and (3.66) share the same structural part f(X) in their models.\nIt is clear in this case that we should pool our observations on YkandYl\nto estimate the common f.\nCombining responses is at the heart of canonical correlation analysis\n(CCA), a data reduction technique developed for the multiple output case.\nSimilar to PCA, CCA \ufb01nds a sequence of uncorrelated linear combina-\ntionsXvm, m= 1,... ,M of the xj, and a corresponding sequence of\nuncorrelated linear combinations Yumof the responses yk, such that the\ncorrelations\nCorr2(Yum,Xvm) (3.67)\nare successively maximized. Note that at most M= min( K,p) directions\ncan be found. The leading canonical response variates are those linear com-\nbinations (derived responses) best predicted by the xj; in contrast, the\ntrailing canonical variates can be poorly predicted by the xj, and are can-\ndidates for being dropped. The CCA solution is computed using a general-\nized SVD of the sample cross-covariance matrix YTX/N(assuming Yand\nXare centered; Exercise 3.20).\nReduced-rank regression (Izenman, 1975; van der Merwe and Zidek, 1980)\nformalizes this approach in terms of a regression model that explicitly pool s\ninformation. Given an error covariance Cov( \u03b5) =\u03a3, we solve the following", "103": "3.7 Multiple Outcome Shrinkage and Selection 85\nrestricted multivariate regression problem:\n\u02c6Brr(m) = argmin\nrank(B)=mN/summationdisplay\ni=1(yi\u2212BTxi)T\u03a3\u22121(yi\u2212BTxi). (3.68)\nWith\u03a3replaced by the estimate YTY/N, one can show (Exercise 3.21)\nthat the solution is given by a CCA of YandX:\n\u02c6Brr(m) =\u02c6BUmU\u2212\nm, (3.69)\nwhereUmis the K\u00d7msub-matrix of Uconsisting of the \ufb01rst mcolumns,\nandUis the K\u00d7Mmatrix of leftcanonical vectors u1,u2,... ,u M.U\u2212\nm\nis its generalized inverse. Writing the solution as\n\u02c6Brr(M) = (XTX)\u22121XT(YU m)U\u2212\nm, (3.70)\nwe see that reduced-rank regression performs a linear regression on the\npooled response matrix YU m, and then maps the coe\ufb03cients (and hence\nthe \ufb01ts as well) back to the original response space. The reduced-rank \ufb01ts\nare given by\n\u02c6Yrr(m) =X(XTX)\u22121XTYU mU\u2212\nm\n=HYP m,(3.71)\nwhere His the usual linear regression projection operator, and Pmis the\nrank-mCCA response projection operator. Although a better estimate of\n\u03a3would be ( Y\u2212X\u02c6B)T(Y\u2212X\u02c6B)/(N\u2212pK), one can show that the solution\nremains the same (Exercise 3.22).\nReduced-rank regression borrows strength among responses by truncat-\ning the CCA. Breiman and Friedman (1997) explored with some success\nshrinkage of the canonical variates between XandY, a smooth version of\nreduced rank regression. Their proposal has the form (compare (3.69))\n\u02c6Bc+w=\u02c6BU\u039bU\u22121, (3.72)\nwhere \u039bis a diagonal shrinkage matrix (the \u201cc+w\u201d stands for \u201cCurds\nand Whey,\u201d the name they gave to their procedure). Based on optimal\nprediction in the population setting, they show that \u039bhas diagonal entries\n\u03bbm=c2\nm\nc2m+p\nN(1\u2212c2m), m= 1,... ,M, (3.73)\nwhere cmis the mth canonical correlation coe\ufb03cient. Note that as the ratio\nof the number of input variables to sample size p/Ngets small, the shrink-\nage factors approach 1. Breiman and Friedman (1997) proposed modi\ufb01ed\nversions of \u039bbased on training data and cross-validation, but the general\nform is the same. Here the \ufb01tted response has the form\n\u02c6Yc+w=HYSc+w, (3.74)", "104": "86 3. Linear Methods for Regression\nwhere Sc+w=U\u039bU\u22121is the response shrinkage operator.\nBreiman and Friedman (1997) also suggested shrinking in both the Y\nspace and Xspace. This leads to hybrid shrinkage models of the form\n\u02c6Yridge,c+w=A\u03bbYSc+w, (3.75)\nwhereA\u03bb=X(XTX+\u03bbI)\u22121XTis the ridge regression shrinkage operator,\nas in (3.46) on page 66. Their paper and the discussions thereof contain\nmany more details.\n3.8 More on the Lasso and Related Path\nAlgorithms\nSince the publication of the LAR algorithm (Efron et al., 2004) there has\nbeen a lot of activity in developing algorithms for \ufb01tting regularization\npaths for a variety of di\ufb00erent problems. In addition, L1regularization has\ntaken on a life of its own, leading to the development of the \ufb01eld compressed\nsensing in the signal-processing literature. (Donoho, 2006a; Candes, 2006).\nIn this section we discuss some related proposals and other path algorithms,\nstarting o\ufb00 with a precursor to the LAR algorithm.\n3.8.1 Incremental Forward Stagewise Regression\nHere we present another LAR-like algorithm, this time focused on forward\nstagewise regression. Interestingly, e\ufb00orts to understand a \ufb02exible nonlinear\nregression procedure (boosting) led to a new algorithm for linear models\n(LAR). In reading the \ufb01rst edition of this book and the forward stagewise\nAlgorithm 3.4 Incremental Forward Stagewise Regression\u2014FS \u01eb.\n1. Start with the residual requal to yand\u03b21,\u03b22,... ,\u03b2 p= 0. All the\npredictors are standardized to have mean zero and unit norm.\n2. Find the predictor xjmost correlated with r\n3. Update \u03b2j\u2190\u03b2j+\u03b4j, where \u03b4j=\u01eb\u2264sign[\u221dan}\u230a\u2207a\u230bketle{txj,r\u221dan}\u230a\u2207a\u230bket\u2207i}ht] and \u01eb >0 is a small\nstep size, and set r\u2190r\u2212\u03b4jxj.\n4. Repeat steps 2 and 3 many times, until the residuals are uncorrelated\nwith all the predictors.\nAlgorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\n4In the \ufb01rst edition, this was Algorithm 10.4 in Chapter 10.", "105": "3.8 More on the Lasso and Related Path Algorithms 87\u22120.2 0.0 0.2 0.4 0.6lcavol\nlweight\nagelbphsvi\nlcpgleasonpgg45\n0 50 100 150 200\n\u22120.2 0.0 0.2 0.4 0.6lcavol\nlweight\nagelbphsvi\nlcpgleasonpgg45\n0.0 0.5 1.0 1.5 2.0FS\u01eb FS0\nIteration\nCoe\ufb03cientsCoe\ufb03cients\nL1Arc-length of Coe\ufb03cients\nFIGURE 3.19. Coe\ufb03cient pro\ufb01les for the prostate data. The left panel shows\nincremental forward stagewise regression with step size \u01eb= 0.01. The right panel\nshows the in\ufb01nitesimal version FS 0obtained letting \u01eb\u21920. This pro\ufb01le was \ufb01t by\nthe modi\ufb01cation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0pro\ufb01les\nare monotone, and hence identical to those of lasso and LAR.\nlinear models, one could explicitly construct the piecewise-linear lasso paths\nof Figure 3.10. This led him to propose the LAR procedure of Section 3.4.4,\nas well as the incremental version of forward-stagewise regression presented\nhere.\nConsider the linear-regression version of the forward-stagewise boosting\nalgorithm 16.1 proposed in Section 16.1 (page 608). It generates a coe\ufb03cient\npro\ufb01le by repeatedly updating (by a small amount \u01eb) the coe\ufb03cient of the\nvariable most correlated with the current residuals. Algorithm 3.4 gives\nthe details. Figure 3.19 (left panel) shows the progress of the algorithm on\nthe prostate data with step size \u01eb= 0.01. If \u03b4j=\u221dan}\u230a\u2207a\u230bketle{txj,r\u221dan}\u230a\u2207a\u230bket\u2207i}ht(the least-squares\ncoe\ufb03cient of the residual on jth predictor), then this is exactly the usual\nforward stagewise procedure (FS) outlined in Section 3.3.3.\nHere we are mainly interested in small values of \u01eb. Letting \u01eb\u21920 gives\nthe right panel of Figure 3.19, which in this case is identical to the lasso\npath in Figure 3.10. We call this limiting procedure in\ufb01nitesimal forward\nstagewise regression or FS 0. This procedure plays an important role in\nnon-linear, adaptive methods like boosting (Chapters 10 and 16) and is the\nversion of incremental forward stagewise regression that is most amenabl e\nto theoretical analysis. B\u00a8 uhlmann and Hothorn (2007) refer to the same\nprocedure as \u201cL2boost\u201d, because of its connections to boosting.", "106": "88 3. Linear Methods for Regression\nEfron originally thought that the LAR Algorithm 3.2 was an implemen-\ntation of FS 0, allowing each tied predictor a chance to update their coe\ufb03-\ncients in a balanced way, while remaining tied in correlation. However, he\nthen realized that the LAR least-squares \ufb01t amongst the tied predictors\ncan result in coe\ufb03cients moving in the opposite direction to their correla-\ntion, which cannot happen in Algorithm 3.4. The following modi\ufb01cation of\nthe LAR algorithm implements FS 0:\nAlgorithm 3.2b Least Angle Regression: FS 0Modi\ufb01cation .\n4. Find the new direction by solving the constrained least squares prob-\nlem\nmin\nb||r\u2212XAb||2\n2subject to bjsj\u22650, j\u2208 A,\nwhere sjis the sign of \u221dan}\u230a\u2207a\u230bketle{txj,r\u221dan}\u230a\u2207a\u230bket\u2207i}ht.\nThe modi\ufb01cation amounts to a non-negative least squares \ufb01t, keeping the\nsigns of the coe\ufb03cients the same as those of the correlations. One can show\nthat this achieves the optimal balancing of in\ufb01nitesimal \u201cupdate turns\u201d\nfor the variables tied for maximal correlation (Hastie et al., 2007) . Like\nlasso, the entire FS 0path can be computed very e\ufb03ciently via the LAR\nalgorithm.\nAs a consequence of these results, if the LAR pro\ufb01les are monotone non-\nincreasing or non-decreasing, as they are in Figure 3.19, then all three\nmethods\u2014LAR, lasso, and FS 0\u2014give identical pro\ufb01les. If the pro\ufb01les are\nnot monotone but do not cross the zero axis, then LAR and lasso are\nidentical.\nSince FS 0is di\ufb00erent from the lasso, it is natural to ask if it optimizes\na criterion. The answer is more complex than for lasso; the FS 0coe\ufb03cient\npro\ufb01le is the solution to a di\ufb00erential equation. While the lasso makes op-\ntimal progress in terms of reducing the residual sum-of-squares per unit\nincrease in L1-norm of the coe\ufb03cient vector \u03b2, FS0is optimal per unit\nincrease in L1arc-length traveled along the coe\ufb03cient path. Hence its co-\ne\ufb03cient path is discouraged from changing directions too often.\nFS0is more constrained than lasso, and in fact can be viewed as a mono-\ntone version of the lasso; see Figure 16.3 on page 614 for a dramatic exa m-\nple. FS 0may be useful in p\u226bNsituations, where its coe\ufb03cient pro\ufb01les\nare much smoother and hence have less variance than those of lasso. More\ndetails on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-\nure 3.16 includes FS 0where its performance is very similar to that of the\nlasso.", "107": "3.8 More on the Lasso and Related Path Algorithms 89\n3.8.2 Piecewise-Linear Path Algorithms\nThe least angle regression procedure exploits the piecewise linear nature of\nthe lasso solution paths. It has led to similar \u201cpath algorithms\u201d for o ther\nregularized problems. Suppose we solve\n\u02c6\u03b2(\u03bb) = argmin\u03b2[R(\u03b2) +\u03bbJ(\u03b2)], (3.76)\nwith\nR(\u03b2) =N/summationdisplay\ni=1L(yi,\u03b20+p/summationdisplay\nj=1xij\u03b2j), (3.77)\nwhere both the loss function Land the penalty function Jare convex.\nThen the following are su\ufb03cient conditions for the solution path \u02c6\u03b2(\u03bb) to\nbe piecewise linear (Rosset and Zhu, 2007):\n1.Ris quadratic or piecewise-quadratic as a function of \u03b2, and\n2.Jis piecewise linear in \u03b2.\nThis also implies (in principle) that the solution path can be e\ufb03ciently\ncomputed. Examples include squared- and absolute-error loss, \u201cHuberized\u201d\nlosses, and the L1,L\u221epenalties on \u03b2. Another example is the \u201chinge loss\u201d\nfunction used in the support vector machine. There the loss is piecewise\nlinear, and the penalty is quadratic. Interestingly, this leads to a piecewise-\nlinear path algorithm in the dual space ; more details are given in Sec-\ntion 12.3.5.\n3.8.3 The Dantzig Selector\nCandes and Tao (2007) proposed the following criterion:\nmin\u03b2||\u03b2||1subject to ||XT(y\u2212X\u03b2)||\u221e\u2264s. (3.78)\nThey call the solution the Dantzig selector (DS). It can be written equiva-\nlently as\nmin\u03b2||XT(y\u2212X\u03b2)||\u221esubject to ||\u03b2||1\u2264t. (3.79)\nHere|| \u2264 ||\u221edenotes the L\u221enorm, the maximum absolute value of the\ncomponents of the vector. In this form it resembles the lasso, replacing\nsquared error loss by the maximum absolute value of its gradient. Note\nthat as tgets large, both procedures yield the least squares solution if\nN < p . Ifp\u2265N, they both yield the least squares solution with minimum\nL1norm. However for smaller values of t, the DS procedure produces a\ndi\ufb00erent path of solutions than the lasso.\nCandes and Tao (2007) show that the solution to DS is a linear pro-\ngramming problem; hence the name Dantzig selector, in honor of the late", "108": "90 3. Linear Methods for Regression\nGeorge Dantzig, the inventor of the simplex method for linear program-\nming. They also prove a number of interesting mathematical properties for\nthe method, related to its ability to recover an underlying sparse coe\ufb03-\ncient vector. These same properties also hold for the lasso, as shown later\nby Bickel et al. (2008).\nUnfortunately the operating properties of the DS method are somewhat\nunsatisfactory. The method seems similar in spirit to the lasso, especiall y\nwhen we look at the lasso\u2019s stationary conditions (3.58). Like the LAR a l-\ngorithm, the lasso maintains the same inner product (and correlation) with\nthe current residual for all variables in the active set, and moves their co-\ne\ufb03cients to optimally decrease the residual sum of squares. In the process,\nthis common correlation is decreased monotonically (Exercise 3.23), and at\nall times this correlation is larger than that for non-active variables. The\nDantzig selector instead tries to minimize the maximum inner product of\nthe current residual with all the predictors. Hence it can achieve a smaller\nmaximum than the lasso, but in the process a curious phenomenon can\noccur. If the size of the active set is m, there will be mvariables tied with\nmaximum correlation. However, these need not coincide with the active set!\nHence it can include a variable in the model that has smaller correlation\nwith the current residual than some of the excluded variables (Efron et\nal., 2007). This seems unreasonable and may be responsible for its some-\ntimes inferior prediction accuracy. Efron et al. (2007) also show that DS\ncan yield extremely erratic coe\ufb03cient paths as the regularization parameter\nsis varied.\n3.8.4 The Grouped Lasso\nIn some problems, the predictors belong to pre-de\ufb01ned groups; for example\ngenes that belong to the same biological pathway, or collections of indicator\n(dummy) variables for representing the levels of a categorical predictor. In\nthis situation it may be desirable to shrink and select the members of a\ngroup together. The grouped lasso is one way to achieve this. Suppose that\ntheppredictors are divided into Lgroups, with p\u2113the number in group\n\u2113. For ease of notation, we use a matrix X\u2113to represent the predictors\ncorresponding to the \u2113th group, with corresponding coe\ufb03cient vector \u03b2\u2113.\nThe grouped-lasso minimizes the convex criterion\nmin\n\u03b2\u2208I Rp/parenleft\uf8ecigg\n||y\u2212\u03b201\u2212L/summationdisplay\n\u2113=1X\u2113\u03b2\u2113||2\n2+\u03bbL/summationdisplay\n\u2113=1\u221ap\u2113||\u03b2\u2113||2/parenright\uf8ecigg\n, (3.80)\nwhere the\u221ap\u2113terms accounts for the varying group sizes, and || \u2264 ||2is\nthe Euclidean norm (not squared). Since the Euclidean norm of a vector\n\u03b2\u2113is zero only if all of its components are zero, this procedure encourages\nsparsity at both the group and individual levels. That is, for some values of\n\u03bb, an entire group of predictors may drop out of the model. This procedure", "109": "3.8 More on the Lasso and Related Path Algorithms 91\nwas proposed by Bakin (1999) and Lin and Zhang (2006), and studied and\ngeneralized by Yuan and Lin (2007). Generalizations include more general\nL2norms ||\u03b7||K= (\u03b7TK\u03b7)1/2, as well as allowing overlapping groups of\npredictors (Zhao et al., 2008). There are also connections to methods for\n\ufb01tting sparse additive models (Lin and Zhang, 2006; Ravikumar et al.,\n2008).\n3.8.5 Further Properties of the Lasso\nA number of authors have studied the ability of the lasso and related pro-\ncedures to recover the correct model, as Nandpgrow. Examples of this\nwork include Knight and Fu (2000), Greenshtein and Ritov (2004), Tropp\n(2004), Donoho (2006b), Meinshausen (2007), Meinshausen and B\u00a8 uhlmann\n(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006), and Bunea\net al. (2007). For example Donoho (2006b) focuses on the p > N case and\nconsiders the lasso solution as the bound tgets large. In the limit this gives\nthe solution with minimum L1norm among all models with zero training\nerror. He shows that under certain assumptions on the model matrix X, if\nthe true model is sparse, this solution identi\ufb01es the correct predictors with\nhigh probability.\nMany of the results in this area assume a condition on the model matrix\nof the form\n||(XSTXS)\u22121XSTXSc||\u221e\u2264(1\u2212\u01eb) for some \u01eb\u2208(0,1]. (3.81)\nHereSindexes the subset of features with non-zero coe\ufb03cients in the true\nunderlying model, and XSare the columns of Xcorresponding to those\nfeatures. Similarly Scare the features with true coe\ufb03cients equal to zero,\nandXScthe corresponding columns. This says that the least squares coef-\n\ufb01cients for the columns of XSconXSare not too large, that is, the \u201cgood\u201d\nvariables Sare not too highly correlated with the nuisance variables Sc.\nRegarding the coe\ufb03cients themselves, the lasso shrinkage causes the esti-\nmates of the non-zero coe\ufb03cients to be biased towards zero, and in general\nthey are not consistent5. One approach for reducing this bias is to run\nthe lasso to identify the set of non-zero coe\ufb03cients, and then \ufb01t an un-\nrestricted linear model to the selected set of features. This is not always\nfeasible, if the selected set is large. Alternatively, one can use the lasso to\nselect the set of non-zero predictors, and then apply the lasso again, but\nusing only the selected predictors from the \ufb01rst step. This is known as the\nrelaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\nestimate the initial penalty parameter for the lasso, and then again for a\nsecond penalty parameter applied to the selected set of predictors. Since\n5Statistical consistency means as the sample size grows, the estimates converge to\nthe true values.", "110": "92 3. Linear Methods for Regression\nthe variables in the second step have less \u201ccompetition\u201d from noise vari-\nables, cross-validation will tend to pick a smaller value for \u03bb, and hence\ntheir coe\ufb03cients will be shrunken less than those in the initial estimate.\nAlternatively, one can modify the lasso penalty function so that larger co-\ne\ufb03cients are shrunken less severely; the smoothly clipped absolute deviation\n(SCAD) penalty of Fan and Li (2005) replaces \u03bb|\u03b2|byJa(\u03b2,\u03bb), where\ndJa(\u03b2,\u03bb)\nd\u03b2=\u03bb\u2264sign(\u03b2)/bracketleft\uf8ecig\nI(|\u03b2| \u2264\u03bb) +(a\u03bb\u2212 |\u03b2|)+\n(a\u22121)\u03bbI(|\u03b2|> \u03bb)/bracketright\uf8ecig\n(3.82)\nfor some a\u22652. The second term in square-braces reduces the amount of\nshrinkage in the lasso for larger values of \u03b2, with ultimately no shrinkage\nasa\u2192 \u221e. Figure 3.20 shows the SCAD penalty, along with the lasso and\n\u22124 \u22122 0 2 40 1 2 3 4 5\n\u22124 \u22122 0 2 40.0 0.5 1.0 1.5 2.0 2.5\n\u22124 \u22122 0 2 40.5 1.0 1.5 2.0|\u03b2| SCAD |\u03b2|1\u2212\u03bd\n\u03b2 \u03b2 \u03b2\nFIGURE 3.20. The lasso and two alternative non-convex penalties designed to\npenalize large coe\ufb03cients less. For SCAD we use \u03bb= 1anda= 4, and \u03bd=1\n2in\nthe last panel.\n|\u03b2|1\u2212\u03bd. However this criterion is non-convex, which is a drawback since it\nmakes the computation much more di\ufb03cult. The adaptive lasso (Zou, 2006)\nuses a weighted penalty of the form/summationtextp\nj=1wj|\u03b2j|where wj= 1/|\u02c6\u03b2j|\u03bd,\u02c6\u03b2jis\nthe ordinary least squares estimate and \u03bd >0. This is a practical approxi-\nmation to the |\u03b2|qpenalties ( q= 1\u2212\u03bdhere) discussed in Section 3.4.3. The\nadaptive lasso yields consistent estimates of the parameters while retaining\nthe attractive convexity property of the lasso.\n3.8.6 Pathwise Coordinate Optimization\nAn alternate approach to the LARS algorithm for computing the lasso\nsolution is simple coordinate descent. This idea was proposed by Fu (1998)\nand Daubechies et al. (2004), and later studied and generalized by Friedman\net al. (2007), Wu and Lange (2008) and others. The idea is to \ufb01x the penalty\nparameter \u03bbin the Lagrangian form (3.52) and optimize successively over\neach parameter, holding the other parameters \ufb01xed at their current values.\nSuppose the predictors are all standardized to have mean zero and unit\nnorm. Denote by \u02dc\u03b2k(\u03bb) the current estimate for \u03b2kat penalty parameter", "111": "3.9 Computational Considerations 93\n\u03bb. We can rearrange (3.52) to isolate \u03b2j,\nR(\u02dc\u03b2(\u03bb),\u03b2j) =1\n2N/summationdisplay\ni=1/parenleft\uf8ecigg\nyi\u2212/summationdisplay\nk/ne}ationslash=jxik\u02dc\u03b2k(\u03bb)\u2212xij\u03b2j/parenright\uf8ecigg2\n+\u03bb/summationdisplay\nk/ne}ationslash=j|\u02dc\u03b2k(\u03bb)|+\u03bb|\u03b2j|,\n(3.83)\nwhere we have suppressed the intercept and introduced a factor1\n2for con-\nvenience. This can be viewed as a univariate lasso problem with response\nvariable the partial residual yi\u2212\u02dcy(j)\ni=yi\u2212/summationtext\nk/ne}ationslash=jxik\u02dc\u03b2k(\u03bb). This has an\nexplicit solution, resulting in the update\n\u02dc\u03b2j(\u03bb)\u2190S/parenleft\uf8eciggN/summationdisplay\ni=1xij(yi\u2212\u02dcy(j)\ni),\u03bb/parenright\uf8ecigg\n. (3.84)\nHereS(t,\u03bb) = sign( t)(|t|\u2212\u03bb)+is the soft-thresholding operator in Table 3.4\non page 71. The \ufb01rst argument to S(\u2264) is the simple least-squares coe\ufb03cient\nof the partial residual on the standardized variable xij. Repeated iteration\nof (3.84)\u2014cycling through each variable in turn until convergence\u2014yields\nthe lasso estimate \u02c6\u03b2(\u03bb).\nWe can also use this simple algorithm to e\ufb03ciently compute the lasso\nsolutions at a grid of values of \u03bb. We start with the smallest value \u03bbmax\nfor which \u02c6\u03b2(\u03bbmax) = 0, decrease it a little and cycle through the variables\nuntil convergence. Then \u03bbis decreased again and the process is repeated,\nusing the previous solution as a \u201cwarm start\u201d for the new value of \u03bb. This\ncan be faster than the LARS algorithm, especially in large problems. A\nkey to its speed is the fact that the quantities in (3.84) can be updated\nquickly as jvaries, and often the update is to leave \u02dc\u03b2j= 0. On the other\nhand, it delivers solutions over a grid of \u03bbvalues, rather than the entire\nsolution path. The same kind of algorithm can be applied to the elastic\nnet, the grouped lasso and many other models in which the penalty is a\nsum of functions of the individual parameters (Friedman et al., 2010). It\ncan also be applied, with some substantial modi\ufb01cations, to the fused lasso\n(Section 18.4.2); details are in Friedman et al. (2007).\n3.9 Computational Considerations\nLeast squares \ufb01tting is usually done via the Cholesky decomposition of\nthe matrix XTXor a QR decomposition of X. With Nobservations and p\nfeatures, the Cholesky decomposition requires p3+Np2/2 operations, while\nthe QR decomposition requires Np2operations. Depending on the relative\nsize of Nandp, the Cholesky can sometimes be faster; on the other hand,\nit can be less numerically stable (Lawson and Hansen, 1974). Computation\nof the lasso via the LAR algorithm has the same order of computation as\na least squares \ufb01t.", "112": "94 3. Linear Methods for Regression\nBibliographic Notes\nLinear regression is discussed in many statistics books, for example, Seber\n(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\nintroduced by Hoerl and Kennard (1970), while the lasso was proposed by\nTibshirani (1996). Around the same time, lasso-type penalties were pro-\nposed in the basis pursuit method for signal processing (Chen et al., 1998).\nThe least angle regression procedure was proposed in Efron et al. (2004);\nrelated to this is the earlier homotopy procedure of Osborne et al. (2000a)\nand Osborne et al. (2000b). Their algorithm also exploits the piecewise\nlinearity used in the LAR/lasso algorithm, but lacks its transparency. The\ncriterion for the forward stagewise criterion is discussed in Hastie et a l.\n(2007). Park and Hastie (2007) develop a path algorithm similar to l east\nangle regression for generalized regression models. Partial least squares\nwas introduced by Wold (1975). Comparisons of shrinkage methods may\nbe found in Copas (1983) and Frank and Friedman (1993).\nExercises\nEx. 3.1 Show that the Fstatistic (3.13) for dropping a single coe\ufb03cient\nfrom a model is equal to the square of the corresponding z-score (3.12).\nEx. 3.2 Given data on two variables XandY, consider \ufb01tting a cubic\npolynomial regression model f(X) =/summationtext3\nj=0\u03b2jXj. In addition to plotting\nthe \ufb01tted curve, you would like a 95% con\ufb01dence band about the curve.\nConsider the following two approaches:\n1. At each point x0, form a 95% con\ufb01dence interval for the linear func-\ntionaT\u03b2=/summationtext3\nj=0\u03b2jxj\n0.\n2. Form a 95% con\ufb01dence set for \u03b2as in (3.15), which in turn generates\ncon\ufb01dence intervals for f(x0).\nHow do these approaches di\ufb00er? Which band is likely to be wider? Conduct\na small simulation experiment to compare the two methods.\nEx. 3.3 Gauss\u2013Markov theorem:\n(a) Prove the Gauss\u2013Markov theorem: the least squares estimate of a\nparameter aT\u03b2has variance no bigger than that of any other linear\nunbiased estimate of aT\u03b2(Section 3.2.2).\n(b) The matrix inequality B\u221d\u221a\u2207e\u230be\u2308esequalAholds if A\u2212Bis positive semide\ufb01nite.\nShow that if \u02c6Vis the variance-covariance matrix of the least squares\nestimate of \u03b2and\u02dcVis the variance-covariance matrix of any other\nlinear unbiased estimate, then \u02c6V\u221d\u221a\u2207e\u230be\u2308esequal\u02dcV.", "113": "Exercises 95\nEx. 3.4 Show how the vector of least squares coe\ufb03cients can be obtained\nfrom a single pass of the Gram\u2013Schmidt procedure (Algorithm 3.1). Rep-\nresent your solution in terms of the QR decomposition of X.\nEx. 3.5 Consider the ridge regression problem (3.41). Show that this prob-\nlem is equivalent to the problem\n\u02c6\u03b2c= argmin\n\u03b2c/braceleft\uf8eciggN/summationdisplay\ni=1/bracketleftbig\nyi\u2212\u03b2c\n0\u2212p/summationdisplay\nj=1(xij\u2212\u00afxj)\u03b2c\nj/bracketrightbig2+\u03bbp/summationdisplay\nj=1\u03b2c\nj2/braceright\uf8ecigg\n.(3.85)\nGive the correspondence between \u03b2cand the original \u03b2in (3.41). Char-\nacterize the solution to this modi\ufb01ed criterion. Show that a similar result\nholds for the lasso.\nEx. 3.6 Show that the ridge regression estimate is the mean (and mode)\nof the posterior distribution, under a Gaussian prior \u03b2\u223cN(0,\u03c4I), and\nGaussian sampling model y\u223cN(X\u03b2,\u03c32I). Find the relationship between\nthe regularization parameter \u03bbin the ridge formula, and the variances \u03c4\nand\u03c32.\nEx. 3.7 Assume yi\u223cN(\u03b20+xT\ni\u03b2,\u03c32),i= 1,2,... ,N , and the parameters\n\u03b2jare each distributed as N(0,\u03c42), independently of one another. Assuming\n\u03c32and\u03c42are known, show that the (minus) log-posterior density of \u03b2is\nproportional to/summationtextN\ni=1(yi\u2212\u03b20\u2212/summationtext\njxij\u03b2j)2+\u03bb/summationtextp\nj=1\u03b22\njwhere \u03bb=\u03c32/\u03c42.\nEx. 3.8 Consider the QR decomposition of the uncentered N\u00d7(p+ 1)\nmatrix X(whose \ufb01rst column is all ones), and the SVD of the N\u00d7p\ncentered matrix \u02dcX. Show that Q2andUspan the same subspace, where\nQ2is the sub-matrix of Qwith the \ufb01rst column removed. Under what\ncircumstances will they be the same, up to sign \ufb02ips?\nEx. 3.9 Forward stepwise regression. Suppose we have the QR decomposi-\ntion for the N\u00d7qmatrix X1in a multiple regression problem with response\ny, and we have an additional p\u2212qpredictors in the matrix X2. Denote the\ncurrent residual by r. We wish to establish which one of these additional\nvariables will reduce the residual-sum-of squares the most when included\nwith those in X1. Describe an e\ufb03cient procedure for doing this.\nEx. 3.10 Backward stepwise regression. Suppose we have the multiple re-\ngression \ufb01t of yonX, along with the standard errors and Z-scores as in\nTable 3.2. We wish to establish which variable, when dropped, will increase\nthe residual sum-of-squares the least. How would you do this?\nEx. 3.11 Show that the solution to the multivariate linear regression prob-\nlem (3.40) is given by (3.39). What happens if the covariance matrices \u03a3i\nare di\ufb00erent for each observation?", "114": "96 3. Linear Methods for Regression\nEx. 3.12 Show that the ridge regression estimates can be obtained by\nordinary least squares regression on an augmented data set. We augment\nthe centered matrix Xwithpadditional rows\u221a\n\u03bbI, and augment ywithp\nzeros. By introducing arti\ufb01cial data having response value zero, the \ufb01tting\nprocedure is forced to shrink the coe\ufb03cients toward zero. This is related to\nthe idea of hintsdue to Abu-Mostafa (1995), where model constraints are\nimplemented by adding arti\ufb01cial data examples that satisfy them.\nEx. 3.13 Derive the expression (3.62), and show that \u02c6\u03b2pcr(p) =\u02c6\u03b2ls.\nEx. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,\nbecause subsequent \u02c6 \u03d5mjin step 2 in Algorithm 3.3 are zero.\nEx. 3.15 Verify expression (3.64), and hence show that the partial least\nsquares directions are a compromise between the ordinary regression coef-\n\ufb01cient and the principal component directions.\nEx. 3.16 Derive the entries in Table 3.4, the explicit forms for estimators\nin the orthogonal case.\nEx. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\nChapter 1.\nEx. 3.18 Read about conjugate gradient algorithms (Murray et al., 1981, for\nexample), and establish a connection between these algorithms and partial\nleast squares.\nEx. 3.19 Show that \u221d\u230aa\u2207\u2308\u230al\u02c6\u03b2ridge\u221d\u230aa\u2207\u2308\u230alincreases as its tuning parameter \u03bb\u21920. Does\nthe same property hold for the lasso and partial least squares estimates?\nFor the latter, consider the \u201ctuning parameter\u201d to be the successive steps\nin the algorithm.\nEx. 3.20 Consider the canonical-correlation problem (3.67). Show that the\nleading pair of canonical variates u1andv1solve the problem\nmax\nuT(YTY)u=1\nvT(XTX)v=1uT(YTX)v, (3.86)\na generalized SVD problem. Show that the solution is given by u1=\n(YTY)\u22121\n2u\u2217\n1, and v1= (XTX)\u22121\n2v\u2217\n1, where u\u2217\n1andv\u2217\n1are the leading left\nand right singular vectors in\n(YTY)\u22121\n2(YTX)(XTX)\u22121\n2=U\u2217D\u2217V\u2217T. (3.87)\nShow that the entire sequence um, vm, m= 1,... ,min(K,p) is also given\nby (3.87).\nEx. 3.21 Show that the solution to the reduced-rank regression problem\n(3.68), with \u03a3estimated by YTY/N, is given by (3.69). Hint: Transform", "115": "Exercises 97\nYtoY\u2217=Y\u03a3\u22121\n2, and solved in terms of the canonical vectors u\u2217\nm. Show\nthatUm=\u03a3\u22121\n2U\u2217\nm, and a generalized inverse is U\u2212\nm=U\u2217\nmT\u03a31\n2.\nEx. 3.22 Show that the solution in Exercise 3.21 does not change if \u03a3is\nestimated by the more natural quantity ( Y\u2212X\u02c6B)T(Y\u2212X\u02c6B)/(N\u2212pK).\nEx. 3.23 Consider a regression problem with all variables and response hav-\ning mean zero and standard deviation one. Suppose also that each variable\nhas identical absolute correlation with the response:\n1\nN|\u221dan}\u230a\u2207a\u230bketle{txj,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht|=\u03bb, j= 1,... ,p.\nLet\u02c6\u03b2be the least-squares coe\ufb03cient of yonX, and let u(\u03b1) =\u03b1X\u02c6\u03b2for\n\u03b1\u2208[0,1] be the vector that moves a fraction \u03b1toward the least squares \ufb01t\nu. LetRSSbe the residual sum-of-squares from the full least squares \ufb01t.\n(a) Show that\n1\nN|\u221dan}\u230a\u2207a\u230bketle{txj,y\u2212u(\u03b1)\u221dan}\u230a\u2207a\u230bket\u2207i}ht|= (1\u2212\u03b1)\u03bb, j= 1,... ,p,\nand hence the correlations of each xjwith the residuals remain equal\nin magnitude as we progress toward u.\n(b) Show that these correlations are all equal to\n\u03bb(\u03b1) =(1\u2212\u03b1)/radical\uf8ecig\n(1\u2212\u03b1)2+\u03b1(2\u2212\u03b1)\nN\u2264RSS\u2264\u03bb,\nand hence they decrease monotonically to zero.\n(c) Use these results to show that the LAR algorithm in Section 3.4.4\nkeeps the correlations tied and monotonically decreasing, as claimed\nin (3.55).\nEx. 3.24 LAR directions. Using the notation around equation (3.55) on\npage 74, show that the LAR direction makes an equal angle with each of\nthe predictors in Ak.\nEx. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-\nginning of the kth step of the LAR algorithm, derive expressions to identify\nthe next variable to enter the active set at step k+1, and the value of \u03b1at\nwhich this occurs (using the notation around equation (3.55) on page 74).\nEx. 3.26 Forward stepwise regression enters the variable at each step that\nmost reduces the residual sum-of-squares. LAR adjusts variables that have\nthe most (absolute) correlation with the current residuals. Show that these\ntwo entry criteria are not necessarily the same. [Hint: let xj.Abe the jth", "116": "98 3. Linear Methods for Regression\nvariable, linearly adjusted for all the variables currently in the model. Show\nthat the \ufb01rst criterion amounts to identifying the jfor which Cor( xj.A,r)\nis largest in magnitude.\nEx. 3.27 Lasso and LAR : Consider the lasso problem in Lagrange multiplier\nform: with L(\u03b2) =1\n2/summationtext\ni(yi\u2212/summationtext\njxij\u03b2j)2, we minimize\nL(\u03b2) +\u03bb/summationdisplay\nj|\u03b2j| (3.88)\nfor \ufb01xed \u03bb >0.\n(a) Setting \u03b2j=\u03b2+\nj\u2212\u03b2\u2212\njwith\u03b2+\nj,\u03b2\u2212\nj\u22650, expression (3.88) becomes\nL(\u03b2) +\u03bb/summationtext\nj(\u03b2+\nj+\u03b2\u2212\nj). Show that the Lagrange dual function is\nL(\u03b2) +\u03bb/summationdisplay\nj(\u03b2+\nj+\u03b2\u2212\nj)\u2212/summationdisplay\nj\u03bb+\nj\u03b2+\nj\u2212/summationdisplay\nj\u03bb\u2212\nj\u03b2\u2212\nj (3.89)\nand the Karush\u2013Kuhn\u2013Tucker optimality conditions are\n\u2207L(\u03b2)j+\u03bb\u2212\u03bb+\nj= 0\n\u2212\u2207L(\u03b2)j+\u03bb\u2212\u03bb\u2212\nj= 0\n\u03bb+\nj\u03b2+\nj= 0\n\u03bb\u2212\nj\u03b2\u2212\nj= 0,\nalong with the non-negativity constraints on the parameters and all\nthe Lagrange multipliers.\n(b) Show that |\u2207L(\u03b2)j| \u2264\u03bb\u2200j,and that the KKT conditions imply one\nof the following three scenarios:\n\u03bb= 0\u21d2 \u2207 L(\u03b2)j= 0\u2200j\n\u03b2+\nj>0, \u03bb > 0\u21d2\u03bb+\nj= 0,\u2207L(\u03b2)j=\u2212\u03bb <0, \u03b2\u2212\nj= 0\n\u03b2\u2212\nj>0, \u03bb > 0\u21d2\u03bb\u2212\nj= 0,\u2207L(\u03b2)j=\u03bb >0, \u03b2+\nj= 0.\nHence show that for any \u201cactive\u201d predictor having \u03b2j\u221dne}ationslash= 0, we must\nhave\u2207L(\u03b2)j=\u2212\u03bbif\u03b2j>0, and \u2207L(\u03b2)j=\u03bbif\u03b2j<0. Assuming\nthe predictors are standardized, relate \u03bbto the correlation between\nthejth predictor and the current residuals.\n(c) Suppose that the set of active predictors is unchanged for \u03bb0\u2265\u03bb\u2265\u03bb1.\nShow that there is a vector \u03b30such that\n\u02c6\u03b2(\u03bb) =\u02c6\u03b2(\u03bb0)\u2212(\u03bb\u2212\u03bb0)\u03b30 (3.90)\nThus the lasso solution path is linear as \u03bbranges from \u03bb0to\u03bb1(Efron\net al., 2004; Rosset and Zhu, 2007).", "117": "Exercises 99\nEx. 3.28 Suppose for a given tin (3.51), the \ufb01tted lasso coe\ufb03cient for\nvariable Xjis\u02c6\u03b2j=a. Suppose we augment our set of variables with an\nidentical copy X\u2217\nj=Xj. Characterize the e\ufb00ect of this exact collinearity\nby describing the set of solutions for \u02c6\u03b2jand\u02c6\u03b2\u2217\nj, using the same value of t.\nEx. 3.29 Suppose we run a ridge regression with parameter \u03bbon a single\nvariable X, and get coe\ufb03cient a. We now include an exact copy X\u2217=X,\nand re\ufb01t our ridge regression. Show that both coe\ufb03cients are identical, and\nderive their value. Show in general that if mcopies of a variable Xjare\nincluded in a ridge regression, their coe\ufb03cients are all the same.\nEx. 3.30 Consider the elastic-net optimization problem:\nmin\n\u03b2||y\u2212X\u03b2||2+\u03bb/bracketleftbig\n\u03b1||\u03b2||2\n2+ (1\u2212\u03b1)||\u03b2||1/bracketrightbig\n. (3.91)\nShow how one can turn this into a lasso problem, using an augmented\nversion of Xandy.", "118": "100 3. Linear Methods for Regression", "119": "This is page 101\nPrinter: Opaque this\n4\nLinear Methods for Classi\ufb01cation\n4.1 Introduction\nIn this chapter we revisit the classi\ufb01cation problem and focus on linear\nmethods for classi\ufb01cation. Since our predictor G(x) takes values in a dis-\ncrete set G, we can always divide the input space into a collection of regions\nlabeled according to the classi\ufb01cation. We saw in Chapter 2 that the bound-\naries of these regions can be rough or smooth, depending on the prediction\nfunction. For an important class of procedures, these decision boundaries\nare linear; this is what we will mean by linear methods for classi\ufb01cation.\nThere are several di\ufb00erent ways in which linear decision boundaries can\nbe found. In Chapter 2 we \ufb01t linear regression models to the class indicator\nvariables, and classify to the largest \ufb01t. Suppose there are Kclasses, for\nconvenience labeled 1 ,2,... ,K , and the \ufb01tted linear model for the kth\nindicator response variable is \u02c6fk(x) =\u02c6\u03b2k0+\u02c6\u03b2T\nkx. The decision boundary\nbetween class kand\u2113is that set of points for which \u02c6fk(x) =\u02c6f\u2113(x), that is,\nthe set {x: (\u02c6\u03b2k0\u2212\u02c6\u03b2\u21130) + (\u02c6\u03b2k\u2212\u02c6\u03b2\u2113)Tx= 0}, an a\ufb03ne set or hyperplane1\nSince the same is true for any pair of classes, the input space is divided\ninto regions of constant classi\ufb01cation, with piecewise hyperplanar decision\nboundaries. This regression approach is a member of a class of methods\nthat model discriminant functions \u03b4k(x) for each class, and then classify x\nto the class with the largest value for its discriminant function. Methods\n1Strictly speaking, a hyperplane passes through the origin, while an a\ufb03ne set need\nnot. We sometimes ignore the distinction and refer in genera l to hyperplanes.", "120": "102 4. Linear Methods for Classi\ufb01cation\nthat model the posterior probabilities Pr( G=k|X=x) are also in this\nclass. Clearly, if either the \u03b4k(x) or Pr( G=k|X=x) are linear in x, then\nthe decision boundaries will be linear.\nActually, all we require is that some monotone transformation of \u03b4kor\nPr(G=k|X=x) be linear for the decision boundaries to be linear. For\nexample, if there are two classes, a popular model for the posterior proba-\nbilities is\nPr(G= 1|X=x) =exp(\u03b20+\u03b2Tx)\n1 + exp( \u03b20+\u03b2Tx),\nPr(G= 2|X=x) =1\n1 + exp( \u03b20+\u03b2Tx).(4.1)\nHere the monotone transformation is the logittransformation: log[ p/(1\u2212p)],\nand in fact we see that\nlogPr(G= 1|X=x)\nPr(G= 2|X=x)=\u03b20+\u03b2Tx. (4.2)\nThe decision boundary is the set of points for which the log-odds are zero,\nand this is a hyperplane de\ufb01ned by/braceleftbig\nx|\u03b20+\u03b2Tx= 0/bracerightbig\n. We discuss two very\npopular but di\ufb00erent methods that result in linear log-odds or logits: linear\ndiscriminant analysis and linear logistic regression. Although they di\ufb00er in\ntheir derivation, the essential di\ufb00erence between them is in the way the\nlinear function is \ufb01t to the training data.\nA more direct approach is to explicitly model the boundaries between\nthe classes as linear. For a two-class problem in a p-dimensional input\nspace, this amounts to modeling the decision boundary as a hyperplane\u2014in\nother words, a normal vector and a cut-point. We will look at two methods\nthat explicitly look for \u201cseparating hyperplanes.\u201d The \ufb01rst is the well-\nknown perceptron model of Rosenblatt (1958), with an algorithm that \ufb01nds\na separating hyperplane in the training data, if one exists. The second\nmethod, due to Vapnik (1996), \ufb01nds an optimally separating hyperplane if\none exists, else \ufb01nds a hyperplane that minimizes some measure of overlap\nin the training data. We treat the separable case here, and defer treatment\nof the nonseparable case to Chapter 12.\nWhile this entire chapter is devoted to linear decision boundaries, there is\nconsiderable scope for generalization. For example, we can expand our vari-\nable set X1,... ,X pby including their squares and cross-products X2\n1,X2\n2,... ,\nX1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions\nin the augmented space map down to quadratic functions in the original\nspace\u2014hence linear decision boundaries to quadratic decision boundaries.\nFigure 4.1 illustrates the idea. The data are the same: the left plot uses\nlinear decision boundaries in the two-dimensional space shown, while the\nright plot uses linear decision boundaries in the augmented \ufb01ve-dimensional\nspace described above. This approach can be used with any basis transfor-", "121": "4.2 Linear Regression of an Indicator Matrix 103\n1\n11\n11\n111\n1\n111\n1\n11\n11\n1111 1\n111\n1\n11\n111\n1111\n11\n1\n11\n111\n11\n1111\n1 1\n111\n11\n1\n1\n1 1\n11\n1111\n11111\n1\n11\n111\n111\n1111\n1\n111\n11\n11\n11\n1\n11\n11\n111\n11\n1\n1\n11\n111\n111\n11\n11\n11\n11 1\n11\n11 111\n1 11\n1\n11\n1\n11\n1\n11\n1\n11 1\n1\n11\n11\n1111\n1111\n1\n111\n11\n111\n1\n111\n111\n1111\n11\n1 11\n111\n11\n1\n11\n11\n11\n111\n12\n22\n22\n222\n2\n2\n22\n2 2\n2\n22\n22\n22\n2222\n2\n22\n222\n2\n22\n2 2\n22\n222\n2\n222\n22222\n22\n2\n222\n22\n2\n22\n22\n22\n22\n22\n2222\n22\n22\n222\n222\n22222\n22\n222\n22\n22\n22\n2\n22\n2222\n22\n2\n22\n2\n222\n2\n222\n22\n222\n222\n22\n2\n22\n22\n2222 2\n222\n22\n2\n22\n22\n222\n222\n2\n22\n22\n22222\n22\n222\n22\n2\n222\n22\n22\n222\n222\n22\n2\n22\n22\n22222\n22\n3\n33\n33\n33\n3\n3333\n33\n3\n33\n3\n333\n33\n33\n333\n333\n3\n3\n33\n333\n333\n333\n333\n3333\n333\n3333\n3\n33333\n33\n3\n33\n3\n33\n3333\n333\n3\n333\n333\n33\n333\n3\n3333\n33\n333\n33\n3\n3333\n333\n33333\n3\n3\n3\n333\n33\n3\n33\n3\n33\n33\n333\n33\n3\n3333\n33\n333\n3\n333\n33333\n33\n333\n33\n33\n3333\n333\n33\n33\n3\n33\n33\n3\n33\n33\n3\n333\n3\n333\n33\n33\n33\n1\n11\n11\n111\n1\n111\n1\n11\n11\n1111 1\n111\n1\n11\n111\n1111\n11\n1\n11\n111\n11\n1111\n1 1\n111\n11\n1\n1\n1 1\n11\n1111\n11111\n1\n11\n111\n111\n1111\n1\n111\n11\n11\n11\n1\n11\n11\n111\n11\n1\n1\n11\n111\n111\n11\n11\n11\n11 1\n11\n11 111\n1 11\n1\n11\n1\n11\n1\n11\n1\n11 1\n1\n11\n11\n1111\n1111\n1\n111\n11\n111\n1\n111\n111\n1111\n11\n1 11\n111\n11\n1\n11\n11\n11\n111\n12\n22\n22\n222\n2\n2\n22\n2 2\n2\n22\n22\n22\n2222\n2\n22\n222\n2\n22\n2 2\n22\n222\n2\n222\n22222\n22\n2\n222\n22\n2\n22\n22\n22\n22\n22\n2222\n22\n22\n222\n222\n22222\n22\n222\n22\n22\n22\n2\n22\n2222\n22\n2\n22\n2\n222\n2\n222\n22\n222\n222\n22\n2\n22\n22\n2222 2\n222\n22\n2\n22\n22\n222\n222\n2\n22\n22\n22222\n22\n222\n22\n2\n222\n22\n22\n222\n222\n22\n2\n22\n22\n22222\n22\n3\n33\n33\n33\n3\n3333\n33\n3\n33\n3\n333\n33\n33\n333\n333\n3\n3\n33\n333\n333\n333\n333\n3333\n333\n3333\n3\n33333\n33\n3\n33\n3\n33\n3333\n333\n3\n333\n333\n33\n333\n3\n3333\n33\n333\n33\n3\n3333\n333\n33333\n3\n3\n3\n333\n33\n3\n33\n3\n33\n33\n333\n33\n3\n3333\n33\n333\n3\n333\n33333\n33\n333\n33\n33\n3333\n333\n33\n33\n3\n33\n33\n3\n33\n33\n3\n333\n3\n333\n33\n33\n33\nFIGURE 4.1. The left plot shows some data from three classes, with linear\ndecision boundaries found by linear discriminant analysis. The ri ght plot shows\nquadratic decision boundaries. These were obtained by \ufb01nding line ar boundaries\nin the \ufb01ve-dimensional space X1, X2, X1X2, X2\n1, X2\n2. Linear inequalities in this\nspace are quadratic inequalities in the original space.\nmation h(X) where h: IRp\u221dma\u221asto\u2192IRqwithq > p, and will be explored in later\nchapters.\n4.2 Linear Regression of an Indicator Matrix\nHere each of the response categories are coded via an indicator variable.\nThus if GhasKclasses, there will be Ksuch indicators Yk, k= 1,... ,K ,\nwithYk= 1 if G=kelse 0. These are collected together in a vector\nY= (Y1,... ,Y K), and the Ntraining instances of these form an N\u00d7K\nindicator response matrix Y.Yis a matrix of 0\u2019s and 1\u2019s, with each row\nhaving a single 1. We \ufb01t a linear regression model to each of the columns\nofYsimultaneously, and the \ufb01t is given by\n\u02c6Y=X(XTX)\u22121XTY. (4.3)\nChapter 3 has more details on linear regression. Note that we have a coe\ufb03-\ncient vector for each response column yk, and hence a ( p+1)\u00d7Kcoe\ufb03cient\nmatrix \u02c6B= (XTX)\u22121XTY. HereXis the model matrix with p+1 columns\ncorresponding to the pinputs, and a leading column of 1\u2019s for the intercept.\nA new observation with input xis classi\ufb01ed as follows:\n\u2022compute the \ufb01tted output \u02c6f(x)T= (1,xT)\u02c6B, aKvector;\n\u2022identify the largest component and classify accordingly:\n\u02c6G(x) = argmaxk\u2208G\u02c6fk(x). (4.4)", "122": "104 4. Linear Methods for Classi\ufb01cation\nWhat is the rationale for this approach? One rather formal justi\ufb01cation\nis to view the regression as an estimate of conditional expectation. For the\nrandom variable Yk,E(Yk|X=x) = Pr( G=k|X=x), so conditional\nexpectation of each of the Ykseems a sensible goal. The real issue is: how\ngood an approximation to conditional expectation is the rather rigid linear\nregression model? Alternatively, are the \u02c6fk(x) reasonable estimates of the\nposterior probabilities Pr( G=k|X=x), and more importantly, does this\nmatter?\nIt is quite straightforward to verify that/summationtext\nk\u2208G\u02c6fk(x) = 1 for any x, as\nlong as there is an intercept in the model (column of 1\u2019s in X). However,\nthe\u02c6fk(x) can be negative or greater than 1, and typically some are. This\nis a consequence of the rigid nature of linear regression, especially if we\nmake predictions outside the hull of the training data. These violations in\nthemselves do not guarantee that this approach will not work, and in fact\non many problems it gives similar results to more standard linear meth-\nods for classi\ufb01cation. If we allow linear regression onto basis expansions\nh(X) of the inputs, this approach can lead to consistent estimates of the\nprobabilities. As the size of the training set Ngrows bigger, we adaptively\ninclude more basis elements so that linear regression onto these basis func-\ntions approaches conditional expectation. We discuss such approaches in\nChapter 5.\nA more simplistic viewpoint is to construct targets tkfor each class,\nwhere tkis the kth column of the K\u00d7Kidentity matrix. Our prediction\nproblem is to try and reproduce the appropriate target for an observation.\nWith the same coding as before, the response vector yi(ith row of Y) for\nobservation ihas the value yi=tkifgi=k. We might then \ufb01t the linear\nmodel by least squares:\nmin\nBN/summationdisplay\ni=1||yi\u2212[(1,xT\ni)B]T||2. (4.5)\nThe criterion is a sum-of-squared Euclidean distances of the \ufb01tted vectors\nfrom their targets. A new observation is classi\ufb01ed by computing its \ufb01tted\nvector \u02c6f(x) and classifying to the closest target:\n\u02c6G(x) = argmin\nk||\u02c6f(x)\u2212tk||2. (4.6)\nThis is exactly the same as the previous approach:\n\u2022The sum-of-squared-norm criterion is exactly the criterion for multi-\nple response linear regression, just viewed slightly di\ufb00erently. Since\na squared norm is itself a sum of squares, the components decouple\nand can be rearranged as a separate linear model for each element.\nNote that this is only possible because there is nothing in the model\nthat binds the di\ufb00erent responses together.", "123": "4.2 Linear Regression of an Indicator Matrix 105\nLinear Regression\n1\n11\n1\n1\n11111\n11\n1\n11111\n111\n11 111\n111\n1111\n111\n1\n11111\n111\n11\n1111\n11\n11\n11111\n11\n11\n11\n1\n11\n11\n1\n11\n11\n1\n11\n1\n1\n11\n111\n111111\n11\n111\n11 1\n111\n1\n1\n11\n1\n11\n11\n11\n111\n111\n11\n11\n11\n1\n11\n1111\n11\n1\n111\n11111\n1\n1111\n11\n111\n111\n111\n1\n11\n1111\n11\n1\n111 11\n11\n11\n11\n11\n1\n1\n11\n11\n1\n111\n1\n11\n11\n1\n111\n111 111\n11\n11\n11\n111111\n1\n11\n11\n11\n1111\n1111\n111\n1\n11 1\n111\n1111\n11\n111\n111\n11\n1\n11\n111\n111\n111\n11\n11\n11\n111\n11 1\n111\n11\n11\n11\n1\n11\n11\n1\n11\n11\n1\n111\n1\n11111\n11111\n1\n11\n111\n1\n11\n111 1\n11\n1\n11 11\n1\n111\n111\n11\n1\n111\n1\n1\n1111\n1111\n11\n11\n1111\n11\n1\n11\n11\n11\n1 1\n111\n1 11\n1\n11\n11\n11 1\n11\n1\n1111111\n1\n11\n11\n1\n11\n111\n11\n111\n1111\n11\n11\n11\n1 1\n11\n11111\n1\n11\n11\n111\n11\n11\n11\n1\n1\n11\n11\n111\n111\n11\n111\n11\n111\n1\n11\n1\n11\n11\n1\n111\n1111\n11\n1\n11\n1\n11\n1 12\n22\n2\n222\n22\n222\n2\n22\n222\n222\n222\n2222\n2\n22222\n2\n2\n222\n2\n222\n2\n222\n2222\n2\n22\n22\n22\n22222\n2222\n2\n222\n222\n22\n2\n22\n22\n222\n2\n2222222\n22\n22\n222\n222\n222\n22\n2\n222\n22\n2\n22\n22\n2 2\n22\n2222\n2\n2\n222\n2222\n22\n222\n222\n222\n222\n22\n222\n2\n2\n222\n222\n2\n22\n2222\n222\n2222\n22\n222\n2\n2222\n222\n22\n2\n222\n2222\n222\n22\n2\n222\n22\n2\n22222\n2222\n22\n222\n22\n22\n22\n2222\n2222\n2\n22\n22\n22\n222\n22222\n22\n22\n2\n22\n2\n2\n22\n2\n22\n22\n2\n22\n22\n22\n22\n2\n222\n22\n2222\n2222\n222\n222\n222\n2222\n2\n222\n2\n22\n222\n222\n22\n222 22\n222\n222\n22222\n22 222\n222\n2\n22\n22 2\n2\n2222\n2\n22\n22\n2\n22\n22\n22\n222 22\n2\n222\n2222\n222\n2\n2 2222 2\n22\n22\n2222\n22\n222\n2\n22\n22\n2\n22\n22\n222\n22\n22\n2222\n22\n2222\n2222\n2\n222\n22 2\n22\n2\n22\n22222\n22\n22\n22\n222\n2\n222\n22\n2\n2 22 222\n22\n22\n222\n22 22\n222\n22\n22\n22 22\n22\n223\n33\n33\n33\n3\n3\n333\n3\n33\n3\n333\n33\n3\n33\n333\n33\n333\n3\n333\n3333\n3\n33\n33\n333\n33\n33\n33\n3\n33\n33\n33333\n3\n33\n33\n3\n33\n33\n3\n33\n3\n33\n3\n333\n33\n3333\n33\n33\n3\n33\n33\n3333\n33\n3333\n3\n33\n3\n33\n33\n3\n33\n33\n3\n333\n333\n33\n33\n3333\n3\n33\n33\n333\n3\n3\n33\n33\n3\n33333\n33\n333\n333\n3333\n33\n33\n333\n3\n33\n33\n33\n333\n3\n3\n3333\n333\n333\n3\n333\n3\n3\n33333333333\n33\n333\n3\n33\n333\n33\n33\n3\n3\n333\n33\n3\n33\n3\n333\n3\n3333\n333\n33\n33\n33\n333\n33\n3\n33\n33\n33\n33\n33333\n33\n33\n33\n33\n3 33\n333\n3\n33 33\n3\n33\n333\n333333\n3\n3333\n33\n33\n3333\n33\n33\n33\n333\n333\n3\n3333\n3\n33\n3\n3\n33333\n333\n3333\n3\n33\n333\n3\n333\n3\n333\n33\n3\n33\n33\n3333\n333\n33\n33\n33 3\n3\n3333\n333\n3\n3\n3\n333\n33\n3\n333\n33\n3\n3333\n33\n3\n33\n3333\n333\n33\n333\n33\n3\n3\n33\n333\n3\n3\n3333333\n3333\n33\n3\n3\n33\n3\n3333\n3\n33\n33 3\n33\n3\n3333\n333\n33\n3\n3\n333\n33\n3\n3333\n3\n33Linear Discriminant Analysis\n1\n11\n1\n1\n11111\n11\n1\n11111\n111\n11 111\n111\n1111\n111\n1\n11111\n111\n11\n1111\n11\n11\n11111\n11\n11\n11\n1\n11\n11\n1\n11\n11\n1\n11\n1\n1\n11\n111\n111111\n11\n111\n11 1\n111\n1\n1\n11\n1\n11\n11\n11\n111\n111\n11\n11\n11\n1\n11\n1111\n11\n1\n111\n11111\n1\n1111\n11\n111\n111\n111\n1\n11\n1111\n11\n1\n111 11\n11\n11\n11\n11\n1\n1\n11\n11\n1\n111\n1\n11\n11\n1\n111\n111 111\n11\n11\n11\n111111\n1\n11\n11\n11\n1111\n1111\n111\n1\n11 1\n111\n1111\n11\n111\n111\n11\n1\n11\n111\n111\n111\n11\n11\n11\n111\n11 1\n111\n11\n11\n11\n1\n11\n11\n1\n11\n11\n1\n111\n1\n11111\n11111\n1\n11\n111\n1\n11\n111 1\n11\n1\n11 11\n1\n111\n111\n11\n1\n111\n1\n1\n1111\n1111\n11\n11\n1111\n11\n1\n11\n11\n11\n1 1\n111\n1 11\n1\n11\n11\n11 1\n11\n1\n1111111\n1\n11\n11\n1\n11\n111\n11\n111\n1111\n11\n11\n11\n1 1\n11\n11111\n1\n11\n11\n111\n11\n11\n11\n1\n1\n11\n11\n111\n111\n11\n111\n11\n111\n1\n11\n1\n11\n11\n1\n111\n1111\n11\n1\n11\n1\n11\n1 12\n22\n2\n222\n22\n222\n2\n22\n222\n222\n222\n2222\n2\n22222\n2\n2\n222\n2\n222\n2\n222\n2222\n2\n22\n22\n22\n22222\n2222\n2\n222\n222\n22\n2\n22\n22\n222\n2\n2222222\n22\n22\n222\n222\n222\n22\n2\n222\n22\n2\n22\n22\n2 2\n22\n2222\n2\n2\n222\n2222\n22\n222\n222\n222\n222\n22\n222\n2\n2\n222\n222\n2\n22\n2222\n222\n2222\n22\n222\n2\n2222\n222\n22\n2\n222\n2222\n222\n22\n2\n222\n22\n2\n22222\n2222\n22\n222\n22\n22\n22\n2222\n2222\n2\n22\n22\n22\n222\n22222\n22\n22\n2\n22\n2\n2\n22\n2\n22\n22\n2\n22\n22\n22\n22\n2\n222\n22\n2222\n2222\n222\n222\n222\n2222\n2\n222\n2\n22\n222\n222\n22\n222 22\n222\n222\n22222\n22 222\n222\n2\n22\n22 2\n2\n2222\n2\n22\n22\n2\n22\n22\n22\n222 22\n2\n222\n2222\n222\n2\n2 2222 2\n22\n22\n2222\n22\n222\n2\n22\n22\n2\n22\n22\n222\n22\n22\n2222\n22\n2222\n2222\n2\n222\n22 2\n22\n2\n22\n22222\n22\n22\n22\n222\n2\n222\n22\n2\n2 22 222\n22\n22\n222\n22 22\n222\n22\n22\n22 22\n22\n223\n33\n33\n33\n3\n3\n333\n3\n33\n3\n333\n33\n3\n33\n333\n33\n333\n3\n333\n3333\n3\n33\n33\n333\n33\n33\n33\n3\n33\n33\n33333\n3\n33\n33\n3\n33\n33\n3\n33\n3\n33\n3\n333\n33\n3333\n33\n33\n3\n33\n33\n3333\n33\n3333\n3\n33\n3\n33\n33\n3\n33\n33\n3\n333\n333\n33\n33\n3333\n3\n33\n33\n333\n3\n3\n33\n33\n3\n33333\n33\n333\n333\n3333\n33\n33\n333\n3\n33\n33\n33\n333\n3\n3\n3333\n333\n333\n3\n333\n3\n3\n33333333333\n33\n333\n3\n33\n333\n33\n33\n3\n3\n333\n33\n3\n33\n3\n333\n3\n3333\n333\n33\n33\n33\n333\n33\n3\n33\n33\n33\n33\n33333\n33\n33\n33\n33\n3 33\n333\n3\n33 33\n3\n33\n333\n333333\n3\n3333\n33\n33\n3333\n33\n33\n33\n333\n333\n3\n3333\n3\n33\n3\n3\n33333\n333\n3333\n3\n33\n333\n3\n333\n3\n333\n33\n3\n33\n33\n3333\n333\n33\n33\n33 3\n3\n3333\n333\n3\n3\n3\n333\n33\n3\n333\n33\n3\n3333\n33\n3\n33\n3333\n333\n33\n333\n33\n3\n3\n33\n333\n3\n3\n3333333\n3333\n33\n3\n3\n33\n3\n3333\n3\n33\n33 3\n33\n3\n3333\n333\n33\n3\n3\n333\n33\n3\n3333\n3\n33\nX1 X1\nX2X2\nFIGURE 4.2. The data come from three classes in I R2and are easily separated\nby linear decision boundaries. The right plot shows the boundar ies found by linear\ndiscriminant analysis. The left plot shows the boundaries found b y linear regres-\nsion of the indicator response variables. The middle class is c ompletely masked\n(never dominates).\n\u2022The closest target classi\ufb01cation rule (4.6) is easily seen to be exactly\nthe same as the maximum \ufb01tted component criterion (4.4), but does\nrequire that the \ufb01tted values sum to 1.\nThere is a serious problem with the regression approach when the number\nof classes K\u22653, especially prevalent when Kis large. Because of the rigid\nnature of the regression model, classes can be masked by others. Figure 4.2\nillustrates an extreme situation when K= 3. The three classes are perfectly\nseparated by linear decision boundaries, yet linear regression misses the\nmiddle class completely.\nIn Figure 4.3 we have projected the data onto the line joining the three\ncentroids (there is no information in the orthogonal direction in this case),\nand we have included and coded the three response variables Y1,Y2and\nY3. The three regression lines (left panel) are included, and we see that\nthe line corresponding to the middle class is horizontal and its \ufb01tted values\nare never dominant! Thus, observations from class 2 are classi\ufb01ed either\nas class 1 or class 3. The right panel uses quadratic regression rather than\nlinear regression. For this simple example a quadratic rather than linear\n\ufb01t (for the middle class at least) would solve the problem. However, it\ncan be seen that if there were four rather than three classes lined up like\nthis, a quadratic would not come down fast enough, and a cubic would\nbe needed as well. A loose but general rule is that if K\u22653 classes are\nlined up, polynomial terms up to degree K\u22121 might be needed to resolve\nthem. Note also that these are polynomials along the derived direction", "124": "106 4. Linear Methods for Classi\ufb01cation\n111\n111\n1\n1\n1111\n11\n11\n1\n11\n11111\n1\n11111\n11\n1\n11\n11\n11\n11111\n111\n111\n1\n1111\n111\n111\n111\n111111\n11\n1\n11\n111\n1111\n11\n111\n111\n111\n111\n111\n1\n11\n11\n111\n111\n11\n11\n1111\n11\n1\n111\n1\n11\n1\n11\n1\n11\n11\n111\n1\n1111\n111\n111\n12222 2222222 2 2 222 2 2 222222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222222 22 2 222222222 22222222 222 222 2 222222222222222 2\n3\n3\n33\n3\n3333\n33\n33\n33\n333\n333\n3\n3\n33333\n3\n33\n333\n33\n333333\n333333\n3\n3333\n3\n33\n3\n33\n3\n33\n333\n33\n33\n33333\n33\n3\n3333\n33\n333\n33\n3333\n33333\n3333\n33333\n33\n3\n33\n33\n33\n33\n33\n333\n3\n333\n333\n333\n3333\n3\n3333\n333\n3\n33333\n0.00.51.0\n0.0 0.2 0.4 0.6 0.8 1.0111\n111\n1\n1\n1111\n11\n11\n1\n11\n11111\n1\n11\n111\n11\n1\n11\n11\n11\n11111\n11\n1\n111\n1\n1111\n111\n111\n111\n111111\n111\n11111\n111111\n111\n111\n111\n111\n111\n1\n1111111 111\n111111111111\n1111 1111111 11111111111111112\n2\n22\n2\n2222\n22\n22\n22\n222\n222\n2\n2\n22222\n2\n22\n222\n22\n222222\n222222\n2\n22222\n2\n222\n222222222222222\n22\n22222222222222222 22 22\n22 22\n22\n22\n222\n222\n22\n22\n2222\n22\n2\n222\n2\n22\n2\n22\n2\n22\n22\n222\n2\n222\n222\n222\n2333333\n333333\n3333\n33333333\n333333\n33\n33333\n3 333333333333 3333\n33\n3\n33\n3\n33\n3333 333\n33333\n33\n3\n333333333\n33\n3333\n33333\n3333\n33333\n33\n3\n33\n33\n33\n33\n33\n333\n3\n333\n333\n333\n3333\n3\n333\n3\n33\n3\n33333\n0.00.51.0\n0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04\nFIGURE 4.3. The e\ufb00ects of masking on linear regression in I Rfor a three-class\nproblem. The rug plot at the base indicates the positions and class membership of\neach observation. The three curves in each panel are the \ufb01tted re gressions to the\nthree-class indicator variables; for example, for the blue cl ass,yblueis1for the\nblue observations, and 0for the green and orange. The \ufb01ts are linear and quadratic\npolynomials. Above each plot is the training error rate. The Bay es error rate is\n0.025for this problem, as is the LDA error rate.\npassing through the centroids, which can have arbitrary orientation. So in\np-dimensional input space, one would need general polynomial terms and\ncross-products of total degree K\u22121,O(pK\u22121) terms in all, to resolve such\nworst-case scenarios.\nThe example is extreme, but for large Kand small psuch maskings\nnaturally occur. As a more realistic illustration, Figure 4.4 is a project ion\nof the training data for a vowel recognition problem onto an informative\ntwo-dimensional subspace. There are K= 11 classes in p= 10 dimensions.\nThis is a di\ufb03cult classi\ufb01cation problem, and the best methods achieve\naround 40% errors on the test data. The main point here is summarized in\nTable 4.1; linear regression has an error rate of 67%, while a close relat ive,\nlinear discriminant analysis, has an error rate of 56%. It seems that mask ing\nhas hurt in this case. While all the other methods in this chapter are based\non linear functions of xas well, they use them in such a way that avoids\nthis masking problem.\n4.3 Linear Discriminant Analysis\nDecision theory for classi\ufb01cation (Section 2.4) tells us that we need to know\nthe class posteriors Pr( G|X) for optimal classi\ufb01cation. Suppose fk(x) is\nthe class-conditional density of Xin class G=k, and let \u03c0kbe the prior\nprobability of class k, with/summationtextK\nk=1\u03c0k= 1. A simple application of Bayes", "125": "4.3 Linear Discriminant Analysis 107\nCoordinate 1 for Training DataCoordinate 2 for Training Data\n-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo\nooooooo\no\noooo\noo\no\no\no\no\noooooooooooo\no\no\no\no\no ooooooo\nooooooo\no\noo\no\nooooo\no\nooooooo\noo\noooo\no\no\nooooooooooooo\no\noooooo\no\nooo\noooo\noooo\no\nooooo\noooooo o\no\no\noooooooooo\no\noo\no\noo\nooooooooooooo\noooooooooooooooooo\noooooooooooo\no\no\nooooooo\no\no\nooooooo\nooooo\nooooooo\no\no\noooooooooooooooo\no\no\no\no\noooooooo\noooo\noo\noooooooooo\no\nooo\no\no\no\noooo\no\no\nooooo\no\no o\no\no\no\no\nooooooooo oooo\no\no\noooooo\no\noooooooo\no\noo\no\noooooooo\no\no\no\no\nooo\nooooooooooooooo\nooo\noooooooo\no\nooo\nooooooooooooo\no\noooo\nooooo\nooooo\no\noo\no\no\no\no\nooo\nooo\no\nooooo\no\noooo\no\nooooooooooooo\no ooooooooooooooooo\no\no\nooooo\nooooooooooo\no\no\nooo\nooo\noo oo\noooooo\noooooo\nooooooooo\no\no\nooooooo\noooooooooooo\noo\no\no\noo\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022Linear Discriminant Analysis\nFIGURE 4.4. A two-dimensional plot of the vowel training data. There are\neleven classes with X\u2208I R10, and this is the best view in terms of a LDA model\n(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.\nThe class overlap is considerable.\nTABLE 4.1. Training and test error rates using a variety of linear techniques\non the vowel data. There are eleven classes in ten dimensions, o f which three\naccount for 90%of the variance (via a principal components analysis). We see\nthat linear regression is hurt by masking, increasing the test and training error\nby over 10%.\nTechnique Error Rates\nTraining Test\nLinear regression 0.48 0.67\nLinear discriminant analysis 0.32 0.56\nQuadratic discriminant analysis 0.01 0.53\nLogistic regression 0.22 0.51", "126": "108 4. Linear Methods for Classi\ufb01cation\ntheorem gives us\nPr(G=k|X=x) =fk(x)\u03c0k/summationtextK\n\u2113=1f\u2113(x)\u03c0\u2113. (4.7)\nWe see that in terms of ability to classify, having the fk(x) is almost equiv-\nalent to having the quantity Pr( G=k|X=x).\nMany techniques are based on models for the class densities:\n\u2022linear and quadratic discriminant analysis use Gaussian densities;\n\u2022more \ufb02exible mixtures of Gaussians allow for nonlinear decision bound-\naries (Section 6.8);\n\u2022general nonparametric density estimates for each class density allow\nthe most \ufb02exibility (Section 6.6.2);\n\u2022Naive Bayes models are a variant of the previous case, and assume\nthat each of the class densities are products of marginal densities;\nthat is, they assume that the inputs are conditionally independent in\neach class (Section 6.6.3).\nSuppose that we model each class density as multivariate Gaussian\nfk(x) =1\n(2\u03c0)p/2|\u03a3k|1/2e\u22121\n2(x\u2212\u03b8k)T\u03a3\u22121\nk(x\u2212\u03b8k). (4.8)\nLinear discriminant analysis (LDA) arises in the special case when we\nassume that the classes have a common covariance matrix \u03a3k=\u03a3\u2200k. In\ncomparing two classes kand\u2113, it is su\ufb03cient to look at the log-ratio, and\nwe see that\nlogPr(G=k|X=x)\nPr(G=\u2113|X=x)= logfk(x)\nf\u2113(x)+ log\u03c0k\n\u03c0\u2113\n= log\u03c0k\n\u03c0\u2113\u22121\n2(\u03b8k+\u03b8\u2113)T\u03a3\u22121(\u03b8k\u2212\u03b8\u2113)\n+xT\u03a3\u22121(\u03b8k\u2212\u03b8\u2113),(4.9)\nan equation linear in x. The equal covariance matrices cause the normal-\nization factors to cancel, as well as the quadratic part in the exponents.\nThis linear log-odds function implies that the decision boundary between\nclasses kand\u2113\u2014the set where Pr( G=k|X=x) = Pr( G=\u2113|X=x)\u2014is\nlinear in x; inpdimensions a hyperplane. This is of course true for any pair\nof classes, so all the decision boundaries are linear. If we divide IRpinto\nregions that are classi\ufb01ed as class 1, class 2, etc., these regions will be sep-\narated by hyperplanes. Figure 4.5 (left panel) shows an idealized example\nwith three classes and p= 2. Here the data do arise from three Gaus-\nsian distributions with a common covariance matrix. We have included in", "127": "4.3 Linear Discriminant Analysis 109\n+++\n3\n21\n11\n233\n3\n123\n32\n11211\n33\n12 1\n23\n23\n3\n12\n211\n1\n13\n222\n21 3\n2 23\n13\n13\n32\n13\n3\n23\n133\n2133\n22\n3\n22\n21\n11\n11\n2\n133\n1\n13\n32\n222 3\n12\nFIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me\ncovariance and di\ufb00erent means. Included are the contours of constant density\nenclosing 95% of the probability in each case. The Bayes decision boundari es\nbetween each pair of classes are shown (broken straight lines ), and the Bayes\ndecision boundaries separating all three classes are the thic ker solid lines (a subset\nof the former). On the right we see a sample of 30drawn from each Gaussian\ndistribution, and the \ufb01tted LDA decision boundaries.\nthe \ufb01gure the contours corresponding to 95% highest probability density,\nas well as the class centroids. Notice that the decision boundaries are not\nthe perpendicular bisectors of the line segments joining the centroids. This\nwould be the case if the covariance \u03a3were spherical \u03c32I, and the class\npriors were equal. From (4.9) we see that the linear discriminant functions\n\u03b4k(x) =xT\u03a3\u22121\u03b8k\u22121\n2\u03b8T\nk\u03a3\u22121\u03b8k+ log\u03c0k (4.10)\nare an equivalent description of the decision rule, with G(x) = argmaxk\u03b4k(x).\nIn practice we do not know the parameters of the Gaussian distributions,\nand will need to estimate them using our training data:\n\u2022\u02c6\u03c0k=Nk/N, where Nkis the number of class- kobservations;\n\u2022\u02c6\u03b8k=/summationtext\ngi=kxi/Nk;\n\u2022\u02c6\u03a3=/summationtextK\nk=1/summationtext\ngi=k(xi\u2212\u02c6\u03b8k)(xi\u2212\u02c6\u03b8k)T/(N\u2212K).\nFigure 4.5 (right panel) shows the estimated decision boundaries based on\na sample of size 30 each from three Gaussian distributions. Figure 4.1 on\npage 103 is another example, but here the classes are not Gaussian.\nWith two classes there is a simple correspondence between linear dis-\ncriminant analysis and classi\ufb01cation by linear least squares, as in (4.5) .\nThe LDA rule classi\ufb01es to class 2 if\nxT\u02c6\u03a3\u22121(\u02c6\u03b82\u2212\u02c6\u03b81)>1\n2\u02c6\u03b8T\n2\u02c6\u03a3\u22121\u02c6\u03b82\u22121\n2\u02c6\u03b8T\n1\u02c6\u03a3\u22121\u02c6\u03b81+ log( N1/N)\u2212log(N2/N)\n(4.11)", "128": "110 4. Linear Methods for Classi\ufb01cation\nand class 1 otherwise. Suppose we code the targets in the two classes as +1\nand\u22121, respectively. It is easy to show that the coe\ufb03cient vector from least\nsquares is proportional to the LDA direction given in (4.11) (Exercise 4. 2).\n[In fact, this correspondence occurs for any (distinct) coding of the targets;\nsee Exercise 4.2]. However unless N1=N2the intercepts are di\ufb00erent and\nhence the resulting decision rules are di\ufb00erent.\nSince this derivation of the LDA direction via least squares does not use a\nGaussian assumption for the features, its applicability extends beyond the\nrealm of Gaussian data. However the derivation of the particular intercept\nor cut-point given in (4.11) doesrequire Gaussian data. Thus it makes\nsense to instead choose the cut-point that empirically minimizes training\nerror for a given dataset. This is something we have found to work well in\npractice, but have not seen it mentioned in the literature.\nWith more than two classes, LDA is not the same as linear regression of\nthe class indicator matrix, and it avoids the masking problems associated\nwith that approach (Hastie et al., 1994). A correspondence between regres-\nsion and LDA can be established through the notion of optimal scoring ,\ndiscussed in Section 12.5.\nGetting back to the general discriminant problem (4.8), if the \u03a3kare\nnot assumed to be equal, then the convenient cancellations in (4.9) do not\noccur; in particular the pieces quadratic in xremain. We then get quadratic\ndiscriminant functions (QDA),\n\u03b4k(x) =\u22121\n2log|\u03a3k| \u22121\n2(x\u2212\u03b8k)T\u03a3\u22121\nk(x\u2212\u03b8k) + log \u03c0k. (4.12)\nThe decision boundary between each pair of classes kand\u2113is described by\na quadratic equation {x:\u03b4k(x) =\u03b4\u2113(x)}.\nFigure 4.6 shows an example (from Figure 4.1 on page 103) where the\nthree classes are Gaussian mixtures (Section 6.8) and the decision bound-\naries are approximated by quadratic equations in x. Here we illustrate\ntwo popular ways of \ufb01tting these quadratic boundaries. The right plot\nuses QDA as described here, while the left plot uses LDA in the enlarged\n\ufb01ve-dimensional quadratic polynomial space. The di\ufb00erences are generally\nsmall; QDA is the preferred approach, with the LDA method a convenient\nsubstitute2.\nThe estimates for QDA are similar to those for LDA, except that separate\ncovariance matrices must be estimated for each class. When pis large this\ncan mean a dramatic increase in parameters. Since the decision boundaries\nare functions of the parameters of the densities, counting the number of\nparameters must be done with care. For LDA, it seems there are ( K\u2212\n1)\u00d7(p+ 1) parameters, since we only need the di\ufb00erences \u03b4k(x)\u2212\u03b4K(x)\n2For this \ufb01gure and many similar \ufb01gures in the book we compute t he decision bound-\naries by an exhaustive contouring method. We compute the dec ision rule on a \ufb01ne lattice\nof points, and then use contouring algorithms to compute the boundaries.", "129": "4.3 Linear Discriminant Analysis 111\n1\n11\n11\n111\n1\n111\n1\n11\n11\n1111 1\n111\n1\n11\n111\n1111\n11\n1\n11\n111\n11\n1111\n1 1\n111\n11\n1\n1\n1 1\n11\n1111\n11111\n1\n11\n111\n111\n1111\n1\n111\n11\n11\n11\n1\n11\n11\n111\n11\n1\n1\n11\n111\n111\n11\n11\n11\n11 1\n11\n11 111\n1 11\n1\n11\n1\n11\n1\n11\n1\n11 1\n1\n11\n11\n1111\n1111\n1\n111\n11\n111\n1\n111\n111\n1111\n11\n1 11\n111\n11\n1\n11\n11\n11\n111\n12\n22\n22\n222\n2\n2\n22\n2 2\n2\n22\n22\n22\n2222\n2\n22\n222\n2\n22\n2 2\n22\n222\n2\n222\n22222\n22\n2\n222\n22\n2\n22\n22\n22\n22\n22\n2222\n22\n22\n222\n222\n22222\n22\n222\n22\n22\n22\n2\n22\n2222\n22\n2\n22\n2\n222\n2\n222\n22\n222\n222\n22\n2\n22\n22\n2222 2\n222\n22\n2\n22\n22\n222\n222\n2\n22\n22\n22222\n22\n222\n22\n2\n222\n22\n22\n222\n222\n22\n2\n22\n22\n22222\n22\n3\n33\n33\n33\n3\n3333\n33\n3\n33\n3\n333\n33\n33\n333\n333\n3\n3\n33\n333\n333\n333\n333\n3333\n333\n3333\n3\n33333\n33\n3\n33\n3\n33\n3333\n333\n3\n333\n333\n33\n333\n3\n3333\n33\n333\n33\n3\n3333\n333\n33333\n3\n3\n3\n333\n33\n3\n33\n3\n33\n33\n333\n33\n3\n3333\n33\n333\n3\n333\n33333\n33\n333\n33\n33\n3333\n333\n33\n33\n3\n33\n33\n3\n33\n33\n3\n333\n3\n333\n33\n33\n33\n1\n11\n11\n111\n1\n111\n1\n11\n11\n1111 1\n111\n1\n11\n111\n1111\n11\n1\n11\n111\n11\n1111\n1 1\n111\n11\n1\n1\n1 1\n11\n1111\n11111\n1\n11\n111\n111\n1111\n1\n111\n11\n11\n11\n1\n11\n11\n111\n11\n1\n1\n11\n111\n111\n11\n11\n11\n11 1\n11\n11 111\n1 11\n1\n11\n1\n11\n1\n11\n1\n11 1\n1\n11\n11\n1111\n1111\n1\n111\n11\n111\n1\n111\n111\n1111\n11\n1 11\n111\n11\n1\n11\n11\n11\n111\n12\n22\n22\n222\n2\n2\n22\n2 2\n2\n22\n22\n22\n2222\n2\n22\n222\n2\n22\n2 2\n22\n222\n2\n222\n22222\n22\n2\n222\n22\n2\n22\n22\n22\n22\n22\n2222\n22\n22\n222\n222\n22222\n22\n222\n22\n22\n22\n2\n22\n2222\n22\n2\n22\n2\n222\n2\n222\n22\n222\n222\n22\n2\n22\n22\n2222 2\n222\n22\n2\n22\n22\n222\n222\n2\n22\n22\n22222\n22\n222\n22\n2\n222\n22\n22\n222\n222\n22\n2\n22\n22\n22222\n22\n3\n33\n33\n33\n3\n3333\n33\n3\n33\n3\n333\n33\n33\n333\n333\n3\n3\n33\n333\n333\n333\n333\n3333\n333\n3333\n3\n33333\n33\n3\n33\n3\n33\n3333\n333\n3\n333\n333\n33\n333\n3\n3333\n33\n333\n33\n3\n3333\n333\n33333\n3\n3\n3\n333\n33\n3\n33\n3\n33\n33\n333\n33\n3\n3333\n33\n333\n3\n333\n33333\n33\n333\n33\n33\n3333\n333\n33\n33\n3\n33\n33\n3\n33\n33\n3\n333\n3\n333\n33\n33\n33\nFIGURE 4.6. Two methods for \ufb01tting quadratic boundaries. The left plot show s\nthe quadratic decision boundaries for the data in Figure 4.1 ( obtained using LDA\nin the \ufb01ve-dimensional space X1, X2, X1X2, X2\n1, X2\n2). The right plot shows the\nquadratic decision boundaries found by QDA. The di\ufb00erences are small, as is\nusually the case.\nbetween the discriminant functions where Kis some pre-chosen class (here\nwe have chosen the last), and each di\ufb00erence requires p+ 1 parameters3.\nLikewise for QDA there will be ( K\u22121)\u00d7 {p(p+ 3)/2 + 1}parameters.\nBoth LDA and QDA perform well on an amazingly large and diverse set\nof classi\ufb01cation tasks. For example, in the STATLOG project (Michie et\nal., 1994) LDA was among the top three classi\ufb01ers for 7 of the 22 datasets,\nQDA among the top three for four datasets, and one of the pair were in the\ntop three for 10 datasets. Both techniques are widely used, and entire books\nare devoted to LDA. It seems that whatever exotic tools are the rage of the\nday, we should always have available these two simple tools. The question\narises why LDA and QDA have such a good track record. The reason is not\nlikely to be that the data are approximately Gaussian, and in addition for\nLDA that the covariances are approximately equal. More likely a reason is\nthat the data can only support simple decision boundaries such as linear or\nquadratic, and the estimates provided via the Gaussian models are stable.\nThis is a bias variance tradeo\ufb00\u2014we can put up with the bias of a linear\ndecision boundary because it can be estimated with much lower variance\nthan more exotic alternatives. This argument is less believable for QDA,\nsince it can have many parameters itself, although perhaps fewer than the\nnon-parametric alternatives.\n3Although we \ufb01t the covariance matrix \u02c6\u03a3to compute the LDA discriminant functions,\na much reduced function of it is all that is required to estima te the O(p) parameters\nneeded to compute the decision boundaries.", "130": "112 4. Linear Methods for Classi\ufb01cation\nMisclassification Rate\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Regularized Discriminant Analysis on the Vowel Data\nTest Data\nTrain Data\n\u03b1\nFIGURE 4.7. Test and training errors for the vowel data, using regularized\ndiscriminant analysis with a series of values of \u03b1\u2208[0,1]. The optimum for the\ntest data occurs around \u03b1= 0.9, close to quadratic discriminant analysis.\n4.3.1 Regularized Discriminant Analysis\nFriedman (1989) proposed a compromise between LDA and QDA, which\nallows one to shrink the separate covariances of QDA toward a common\ncovariance as in LDA. These methods are very similar in \ufb02avor to ridge\nregression. The regularized covariance matrices have the form\n\u02c6\u03a3k(\u03b1) =\u03b1\u02c6\u03a3k+ (1\u2212\u03b1)\u02c6\u03a3, (4.13)\nwhere \u02c6\u03a3is the pooled covariance matrix as used in LDA. Here \u03b1\u2208[0,1]\nallows a continuum of models between LDA and QDA, and needs to be\nspeci\ufb01ed. In practice \u03b1can be chosen based on the performance of the\nmodel on validation data, or by cross-validation.\nFigure 4.7 shows the results of RDA applied to the vowel data. Both\nthe training and test error improve with increasing \u03b1, although the test\nerror increases sharply after \u03b1= 0.9. The large discrepancy between the\ntraining and test error is partly due to the fact that there are many repeat\nmeasurements on a small number of individuals, di\ufb00erent in the training\nand test set.\nSimilar modi\ufb01cations allow \u02c6\u03a3itself to be shrunk toward the scalar\ncovariance,\n\u02c6\u03a3(\u03b3) =\u03b3\u02c6\u03a3+ (1\u2212\u03b3)\u02c6\u03c32I (4.14)\nfor\u03b3\u2208[0,1]. Replacing \u02c6\u03a3in (4.13) by \u02c6\u03a3(\u03b3) leads to a more general family\nof covariances \u02c6\u03a3(\u03b1,\u03b3) indexed by a pair of parameters.\nIn Chapter 12, we discuss other regularized versions of LDA, which are\nmore suitable when the data arise from digitized analog signals and images.", "131": "4.3 Linear Discriminant Analysis 113\nIn these situations the features are high-dimensional and correlated, and the\nLDA coe\ufb03cients can be regularized to be smooth or sparse in the original\ndomain of the signal. This leads to better generalization and allows for\neasier interpretation of the coe\ufb03cients. In Chapter 18 we also deal with\nvery high-dimensional problems, where for example the features are gene-\nexpression measurements in microarray studies. There the methods focus\non the case \u03b3= 0 in (4.14), and other severely regularized versions of LDA.\n4.3.2 Computations for LDA\nAs a lead-in to the next topic, we brie\ufb02y digress on the computations\nrequired for LDA and especially QDA. Their computations are simpli\ufb01ed\nby diagonalizing \u02c6\u03a3or\u02c6\u03a3k. For the latter, suppose we compute the eigen-\ndecomposition for each \u02c6\u03a3k=UkDkUT\nk, where Ukisp\u00d7porthonormal,\nandDka diagonal matrix of positive eigenvalues dk\u2113. Then the ingredients\nfor\u03b4k(x) (4.12) are\n\u2022(x\u2212\u02c6\u03b8k)T\u02c6\u03a3\u22121\nk(x\u2212\u02c6\u03b8k) = [UT\nk(x\u2212\u02c6\u03b8k)]TD\u22121\nk[UT\nk(x\u2212\u02c6\u03b8k)];\n\u2022log|\u02c6\u03a3k|=/summationtext\n\u2113logdk\u2113.\nIn light of the computational steps outlined above, the LDA classi\ufb01er\ncan be implemented by the following pair of steps:\n\u2022Sphere the data with respect to the common covariance estimate \u02c6\u03a3:\nX\u2217\u2190D\u22121\n2UTX, where \u02c6\u03a3=UDUT. The common covariance esti-\nmate of X\u2217will now be the identity.\n\u2022Classify to the closest class centroid in the transformed space, modulo\nthe e\ufb00ect of the class prior probabilities \u03c0k.\n4.3.3 Reduced-Rank Linear Discriminant Analysis\nSo far we have discussed LDA as a restricted Gaussian classi\ufb01er. Part of\nits popularity is due to an additional restriction that allows us to view\ninformative low-dimensional projections of the data.\nTheKcentroids in p-dimensional input space lie in an a\ufb03ne subspace\nof dimension \u2264K\u22121, and if pis much larger than K, this will be a con-\nsiderable drop in dimension. Moreover, in locating the closest centroid, we\ncan ignore distances orthogonal to this subspace, since they will contribute\nequally to each class. Thus we might just as well project the X\u2217onto this\ncentroid-spanning subspace HK\u22121, and make distance comparisons there.\nThus there is a fundamental dimension reduction in LDA, namely, that we\nneed only consider the data in a subspace of dimension at most K\u22121.", "132": "114 4. Linear Methods for Classi\ufb01cation\nIfK= 3, for instance, this could allow us to view the data in a two-\ndimensional plot, color-coding the classes. In doing so we would not have\nrelinquished any of the information needed for LDA classi\ufb01cation.\nWhat if K >3? We might then ask for a L < K \u22121 dimensional subspace\nHL\u2286HK\u22121optimal for LDA in some sense. Fisher de\ufb01ned optimal to\nmean that the projected centroids were spread out as much as possible in\nterms of variance. This amounts to \ufb01nding principal component subspaces\nof the centroids themselves (principal components are described brie\ufb02y in\nSection 3.5.1, and in more detail in Section 14.5.1). Figure 4.4 shows such an\noptimal two-dimensional subspace for the vowel data. Here there are eleven\nclasses, each a di\ufb00erent vowel sound, in a ten-dimensional input space. The\ncentroids require the full space in this case, since K\u22121 =p, but we have\nshown an optimal two-dimensional subspace. The dimensions are ordered,\nso we can compute additional dimensions in sequence. Figure 4.8 shows four\nadditional pairs of coordinates, also known as canonical ordiscriminant\nvariables. In summary then, \ufb01nding the sequences of optimal subspaces\nfor LDA involves the following steps:\n\u2022compute the K\u00d7pmatrix of class centroids Mand the common\ncovariance matrix W(forwithin-class covariance);\n\u2022compute M\u2217=MW\u22121\n2using the eigen-decomposition of W;\n\u2022compute B\u2217, the covariance matrix of M\u2217(Bforbetween-class covari-\nance), and its eigen-decomposition B\u2217=V\u2217DBV\u2217T. The columns\nv\u2217\n\u2113ofV\u2217in sequence from \ufb01rst to last de\ufb01ne the coordinates of the\noptimal subspaces.\nCombining all these operations the \u2113thdiscriminant variable is given by\nZ\u2113=vT\n\u2113Xwithv\u2113=W\u22121\n2v\u2217\n\u2113.\nFisher arrived at this decomposition via a di\ufb00erent route, without refer-\nring to Gaussian distributions at all. He posed the problem:\nFind the linear combination Z=aTXsuch that the between-\nclass variance is maximized relative to the within-class var iance.\nAgain, the between class variance is the variance of the class means of\nZ, and the within class variance is the pooled variance about the means.\nFigure 4.9 shows why this criterion makes sense. Although the direction\njoining the centroids separates the means as much as possible (i.e., max-\nimizes the between-class variance), there is considerable overlap between\nthe projected classes due to the nature of the covariances. By taking the\ncovariance into account as well, a direction with minimum overlap can be\nfound.\nThe between-class variance of ZisaTBaand the within-class variance\naTWa, where Wis de\ufb01ned earlier, and Bis the covariance matrix of the\nclass centroid matrix M. Note that B+W=T, where Tis the total\ncovariance matrix of X, ignoring class information.", "133": "4.3 Linear Discriminant Analysis 115\nCoordinate 1 Coordinate 3 \n-4 -2 0 2 4-2 0 2o\nooooo\no\no\nooo\nooo\nooooooo\no\noo\nooo\nooo\noo\nooooooooo\nooo\no\nooo\nooooo\noooooo ooooooo\no\no\nooo oo\no\no\nooo\no\no\noooooooooooo\noooo\noooooo\noo\nooo\noooooooo\nooo\noooo\noo ooo\noooo\noooo\noo\noooo\no\noo\noooooooo\noooo\noooooooooo\noo\nooo\no\nooo\noooooo\nooo\noooooo\no\no\noooooo\noooooooooooo\no\nooo\noo\nooo\nooooo\noo\nooo\nooooo\no\nooooo\no\nooooooo\noooo\no\noooo\noo\nooooo\noooo\no\noo\nooooo\no\no\no\nooo\no\nooooo\nooooooo\no\noooo\noooooo\noooo\no\no\noo ooo\noo\nooo\no\noooo\no\nooooooo\no\noo\noo\no\nooo\nooooo\no\nooooo\no\noo\no\noo\no\noooooo\no\noo\nooooooooooo\noo\noooooo\noo\no\no\noooo\no\nooooo\no\no\no\nooo\noooooooooo\noooooo\noooooo\noooooo\no\no\nooo\nooooooooooo\noooo\nooo\noooooo\no\no\noooooooo\nooo\noooooo\no\nooooo\nooo\no\nooo\no\noooooooooo oooo ooooooo\nooooooo\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\nCoordinate 2 Coordinate 3 \n-6 -4 -2 0 2 4-2 0 2o\nooooo\no\no\nooo\nooo\nooooooo\no\noo\nooo\nooo\noo\nooooooooo\nooo\no\nooo\nooooo\nooooooooooooo\no\no\nooo oo\no\no\nooo\no\no\noooooooooooo\noooo\noooooo\noo\nooo\noooooooo\nooo\noooo\nooooo\noooo\noooo\noo\noooo\no\noo\noooooooo\noooo\noooooooooo\noo\nooo\no\nooo\noooooo\nooo\noooooo\no\no\noooooo\noooooooooooo\no\nooo\noo\nooo\nooooo\noo\nooo\nooooo\no\nooooo\no\nooooooo\noooo\no\noooo\noo\nooooo\noooo\no\noo\nooooo\no\no\no\nooo\no\nooooo\nooooooo\no\noooo\noooooo\noooo\no\no\noo ooo\noo\nooo\no\noooo\no\nooooooo\no\noo\noo\no\nooo\nooooo\no\nooooo\no\noo\no\noo\no\noooooo\no\noo\nooooooooooo\noo\noooooo\noo\no\no\noooo\no\nooooo\no\no\no\nooo\noooooooooo\noooooo\noooooo\noooooo\no\no\nooo\nooooooooooo\noo oo\nooo\noooooo\no\no\noooooooo\nooo\noooooo\no\nooooo\nooo\no\nooo\no\noooooooooo ooooooooooo\nooooooo\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\nCoordinate 1 Coordinate 7 \n-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo\no\nooooooo\noooo\nooooooooooooooo\nooo\nooo\no\no\nooooooooooooo\noooo\no\nooooo\no\nooooooo\no\no\nooooo\nooooooooooo\nooo\no\noooo\noooo\noooo\noo\noo\no\nooo\noooooooooo\nooo\nooooo\noooooooooooo\noo oooo\noooooo\noooooo\noooooooo\no\nooooooooo\no\nooooooooooo\no\nooooo\noooooo\noo\noooo\nooooooooooo\nooo o\nooo\nooooooooo\nooo\noooooo\noooo\noooooooo\no\noooooo\nooo\nooo\no\noooo\nooooooooooo\no\noooooo\nooooo\nooooooooooooo\no\nooooooo\noooo\noooooooo\noooo\noooooo\no\nooooo\nooooo\no\noooooo\noooooooooooo\nooooo\nooooooooooooo\nooo\no\nooo\noo\no\noooooooo\nooooooo\nooo\noo\noooooooooooo\nooooooooo\nooo\noo\nooooo\no\nooo\no\no\nooo\no\noo\no\noooooo\nooooo\noo\noo\no\noooooo\nooo\noo\nooooooooooo\no\noooooo\no\nooooooooooo\noooo\no\noo\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\nCoordinate 9 Coordinate 10 \n-2 -1 0 1 2 3-2 -1 0 1 2oo\no\no\noooooooo\nooooo\no\nooooooo\nooooo\nooooo\no\nooooooo\no\no\no\noo\noo\no\nooo\noooooooooo\no\no\noooooooooooo o\noo\nooo\nooooooo\nooooo\noo\noooo\noooooo o\noo\noooo\noooo\nooooooo\noooooo\noo\no\noooooo\no\noo\no\noooooooooooooooooo\nooo\nooo\nooo\noo\nooooooo\no\noooo\nooooo\no\nooooooooooo\nooooo\nooooooooooooo\no\noooo\nooooo\no\noo\no\no\noooo\nooooo\no\nooo\no\noooo\noo\no\noo\noo\noooo\noooo\no\nooooooooooo\no\nooo\noo\no\nooo\no\no oo\noooooooo\nooo\nooooo\nooooo\no\no\nooo\noooooooo\nooo\noo\noo\noooooooooooo\no\no\no\noo\nooooooooo\noooo\no\noooooooooo\nooo\no\no\no\no\no\noo\nooooooo\no\no\noo\no\no\noooooooooo\noooo\nooo\noooooooo\nooo\noooo\nooo\no\nooo\nooooo\nooooooooooooo\nooooo\no\no\nooo\nooooooo\nooo\no\noo\noooo\nooooo\noo\no\no\no\noooooooooooooooo\nooo\no\no ooooo\noo\noo\no\no\no\no\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Linear Discriminant Analysis\nFIGURE 4.8. Four projections onto pairs of canonical variates. Notice that a s\nthe rank of the canonical variates increases, the centroids becom e less spread out.\nIn the lower right panel they appear to be superimposed, and the classes most\nconfused.", "134": "116 4. Linear Methods for Classi\ufb01cation\n++\n++\nFIGURE 4.9. Although the line joining the centroids de\ufb01nes the direction of\ngreatest centroid spread, the projected data overlap becaus e of the covariance\n(left panel). The discriminant direction minimizes this overla p for Gaussian data\n(right panel).\nFisher\u2019s problem therefore amounts to maximizing the Rayleigh quotient ,\nmax\naaTBa\naTWa, (4.15)\nor equivalently\nmax\naaTBasubject to aTWa= 1. (4.16)\nThis is a generalized eigenvalue problem, with agiven by the largest\neigenvalue of W\u22121B. It is not hard to show (Exercise 4.1) that the optimal\na1is identical to v1de\ufb01ned above. Similarly one can \ufb01nd the next direction\na2, orthogonal in Wtoa1, such that aT\n2Ba2/aT\n2Wa2is maximized; the\nsolution is a2=v2, and so on. The a\u2113are referred to as discriminant\ncoordinates , not to be confused with discriminant functions. They are also\nreferred to as canonical variates , since an alternative derivation of these\nresults is through a canonical correlation analysis of the indicator response\nmatrix Yon the predictor matrix X. This line is pursued in Section 12.5.\nTo summarize the developments so far:\n\u2022Gaussian classi\ufb01cation with common covariances leads to linear deci-\nsion boundaries. Classi\ufb01cation can be achieved by sphering the data\nwith respect to W, and classifying to the closest centroid (modulo\nlog\u03c0k) in the sphered space.\n\u2022Since only the relative distances to the centroids count, one can con-\n\ufb01ne the data to the subspace spanned by the centroids in the sphered\nspace.\n\u2022This subspace can be further decomposed into successively optimal\nsubspaces in term of centroid separation. This decomposition is iden-\ntical to the decomposition due to Fisher.", "135": "4.3 Linear Discriminant Analysis 117\nDimensionMisclassification Rate\n2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data\n\u2022\n\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022Test Data\nTrain Data\nFIGURE 4.10. Training and test error rates for the vowel data, as a function\nof the dimension of the discriminant subspace. In this case the b est error rate is\nfor dimension 2. Figure 4.11 shows the decision boundaries in this space.\nThe reduced subspaces have been motivated as a data reduction (for\nviewing) tool. Can they also be used for classi\ufb01cation, and what is the\nrationale? Clearly they can, as in our original derivation; we simply limit\nthe distance-to-centroid calculations to the chosen subspace. One can show\nthat this is a Gaussian classi\ufb01cation rule with the additional restriction\nthat the centroids of the Gaussians lie in a L-dimensional subspace of IRp.\nFitting such a model by maximum likelihood, and then constructing the\nposterior probabilities using Bayes\u2019 theorem amounts to the classi\ufb01cation\nrule described above (Exercise 4.8).\nGaussian classi\ufb01cation dictates the log \u03c0kcorrection factor in the dis-\ntance calculation. The reason for this correction can be seen in Figure 4.9.\nThe misclassi\ufb01cation rate is based on the area of overlap between the two\ndensities. If the \u03c0kare equal (implicit in that \ufb01gure), then the optimal\ncut-point is midway between the projected means. If the \u03c0kare not equal,\nmoving the cut-point toward the smaller class will improve the error rate.\nAs mentioned earlier for two classes, one can derive the linear rule using\nLDA (or any other method), and then choose the cut-point to minimize\nmisclassi\ufb01cation error over the training data.\nAs an example of the bene\ufb01t of the reduced-rank restriction, we return\nto the vowel data. There are 11 classes and 10 variables, and hence 10\npossible dimensions for the classi\ufb01er. We can compute the training and\ntest error in each of these hierarchical subspaces; Figure 4.10 shows the\nresults. Figure 4.11 shows the decision boundaries for the classi\ufb01er based\non the two-dimensional LDA solution.\nThere is a close connection between Fisher\u2019s reduced rank discriminant\nanalysis and regression of an indicator response matrix. It turns out that", "136": "118 4. Linear Methods for Classi\ufb01cation\noooo\noo o\noo\noo\no\noo o\noo\noooo\no\noo\nooooo\noo\nooo\no\nooo\noooo\noo\noooo\no\nooo\no\noo\nooo\no\no\no\nooo\no\noo\noo oo\noo\nooo\no\noo\noo\noo\no\noo\no\noooo\no\noo\noo\noo\noooo\nooo\no\no\no\no\noo\no\noo\noo\no\no\nooo\no\nooo\nooo\noo\no\nooo\noo\noooo\noo\noo\no\nooo\no\no\nooo\no\nooo\noo\noo\noooo\noo\noo\no\noo\no\nooo\no\noooo\noooo\noo\no o\nooo\no\no\no\noo\noooo\noo\noo\no\noo\nooo\no\noo\noo\noo\no oo\nooo\no\noo\noo\noooo\noo\noo\noo\no\noo\no\noo\noooo\no\nooo\noooooo\nooo\noo\noo\nooooo\nooo\no\nooo\no oo\noo\nooo\no\noooo\noo\no\nooo\no\nooo\nooooo\no\noo\noo\no oo\noo\noo\noo\noo\no\nooo\noo\no\no\noo\no\noo\noooo\no\no\noooo\no\noo\noo\no\noo\noo\no\noo\nooo\noooo\noo\noo\nooo\noo\noo\no\noo\noo\nooo\no\noo\noo\no\noooo\noooo\no\noo\noo\no\nooo\nooooo\nooo\noo\noo\noo\noo\no\no\no\no\no\noo\noo\no\noo\nooo\no\no o\noo\no o\noo\no\noo\no\nooo\noo\nooo\no\noo\no\noo o\nooo\noo\noo\no\noo\noo\noo\noo\nooo\nooo\no\noo\no\noo\noo\noo\noo\no\noo\noo\noo\noo\nooo\no\noo\nooo\noo\noo\no\noo\no\no\no\nCanonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\nFIGURE 4.11. Decision boundaries for the vowel training data, in the two-di-\nmensional subspace spanned by the \ufb01rst two canonical variates. Note that in\nany higher-dimensional subspace, the decision boundaries are h igher-dimensional\na\ufb03ne planes, and could not be represented as lines.", "137": "4.4 Logistic Regression 119\nLDA amounts to the regression followed by an eigen-decomposition of\n\u02c6YTY. In the case of two classes, there is a single discriminant variable\nthat is identical up to a scalar multiplication to either of the columns of \u02c6Y.\nThese connections are developed in Chapter 12. A related fact is that if one\ntransforms the original predictors Xto\u02c6Y, then LDA using \u02c6Yis identical\nto LDA in the original space (Exercise 4.3).\n4.4 Logistic Regression\nThe logistic regression model arises from the desire to model the posterior\nprobabilities of the Kclasses via linear functions in x, while at the same\ntime ensuring that they sum to one and remain in [0 ,1]. The model has\nthe form\nlogPr(G= 1|X=x)\nPr(G=K|X=x)=\u03b210+\u03b2T\n1x\nlogPr(G= 2|X=x)\nPr(G=K|X=x)=\u03b220+\u03b2T\n2x\n...\nlogPr(G=K\u22121|X=x)\nPr(G=K|X=x)=\u03b2(K\u22121)0+\u03b2T\nK\u22121x.(4.17)\nThe model is speci\ufb01ed in terms of K\u22121 log-odds or logit transformations\n(re\ufb02ecting the constraint that the probabilities sum to one). Although the\nmodel uses the last class as the denominator in the odds-ratios, the choice\nof denominator is arbitrary in that the estimates are equivariant under this\nchoice. A simple calculation shows that\nPr(G=k|X=x) =exp(\u03b2k0+\u03b2T\nkx)\n1 +/summationtextK\u22121\n\u2113=1exp(\u03b2\u21130+\u03b2T\n\u2113x), k= 1,... ,K \u22121,\nPr(G=K|X=x) =1\n1 +/summationtextK\u22121\n\u2113=1exp(\u03b2\u21130+\u03b2T\n\u2113x), (4.18)\nand they clearly sum to one. To emphasize the dependence on the entire pa-\nrameter set \u03b8={\u03b210,\u03b2T\n1,... ,\u03b2 (K\u22121)0,\u03b2T\nK\u22121}, we denote the probabilities\nPr(G=k|X=x) =pk(x;\u03b8).\nWhen K= 2, this model is especially simple, since there is only a single\nlinear function. It is widely used in biostatistical applications where binary\nresponses (two classes) occur quite frequently. For example, patients survive\nor die, have heart disease or not, or a condition is present or absent.", "138": "120 4. Linear Methods for Classi\ufb01cation\n4.4.1 Fitting Logistic Regression Models\nLogistic regression models are usually \ufb01t by maximum likelihood, using the\nconditional likelihood of Ggiven X. Since Pr( G|X) completely speci\ufb01es the\nconditional distribution, the multinomial distribution is appropriate. The\nlog-likelihood for Nobservations is\n\u2113(\u03b8) =N/summationdisplay\ni=1logpgi(xi;\u03b8), (4.19)\nwhere pk(xi;\u03b8) = Pr( G=k|X=xi;\u03b8).\nWe discuss in detail the two-class case, since the algorithms simplify\nconsiderably. It is convenient to code the two-class givia a 0 /1 response yi,\nwhere yi= 1 when gi= 1, and yi= 0 when gi= 2. Let p1(x;\u03b8) =p(x;\u03b8),\nandp2(x;\u03b8) = 1\u2212p(x;\u03b8). The log-likelihood can be written\n\u2113(\u03b2) =N/summationdisplay\ni=1/braceleft\uf8ecig\nyilogp(xi;\u03b2) + (1 \u2212yi)log(1 \u2212p(xi;\u03b2))/braceright\uf8ecig\n=N/summationdisplay\ni=1/braceleft\uf8ecig\nyi\u03b2Txi\u2212log(1 + e\u03b2Txi)/braceright\uf8ecig\n. (4.20)\nHere\u03b2={\u03b210,\u03b21}, and we assume that the vector of inputs xiincludes\nthe constant term 1 to accommodate the intercept.\nTo maximize the log-likelihood, we set its derivatives to zero. These score\nequations are\n\u2202\u2113(\u03b2)\n\u2202\u03b2=N/summationdisplay\ni=1xi(yi\u2212p(xi;\u03b2)) = 0 , (4.21)\nwhich are p+1 equations nonlinear in\u03b2. Notice that since the \ufb01rst compo-\nnent of xiis 1, the \ufb01rst score equation speci\ufb01es that/summationtextN\ni=1yi=/summationtextN\ni=1p(xi;\u03b2);\ntheexpected number of class ones matches the observed number (and hence\nalso class twos.)\nTo solve the score equations (4.21), we use the Newton\u2013Raphson algo-\nrithm, which requires the second-derivative or Hessian matrix\n\u22022\u2113(\u03b2)\n\u2202\u03b2\u2202\u03b2T=\u2212N/summationdisplay\ni=1xixiTp(xi;\u03b2)(1\u2212p(xi;\u03b2)). (4.22)\nStarting with \u03b2old, a single Newton update is\n\u03b2new=\u03b2old\u2212/parenleftbigg\u22022\u2113(\u03b2)\n\u2202\u03b2\u2202\u03b2T/parenrightbigg\u22121\u2202\u2113(\u03b2)\n\u2202\u03b2, (4.23)\nwhere the derivatives are evaluated at \u03b2old.", "139": "4.4 Logistic Regression 121\nIt is convenient to write the score and Hessian in matrix notation. Let\nydenote the vector of yivalues, XtheN\u00d7(p+ 1) matrix of xivalues,\npthe vector of \ufb01tted probabilities with ith element p(xi;\u03b2old) andWa\nN\u00d7Ndiagonal matrix of weights with ith diagonal element p(xi;\u03b2old)(1\u2212\np(xi;\u03b2old)). Then we have\n\u2202\u2113(\u03b2)\n\u2202\u03b2=XT(y\u2212p) (4.24)\n\u22022\u2113(\u03b2)\n\u2202\u03b2\u2202\u03b2T=\u2212XTWX (4.25)\nThe Newton step is thus\n\u03b2new=\u03b2old+ (XTWX)\u22121XT(y\u2212p)\n= (XTWX)\u22121XTW/parenleftbig\nX\u03b2old+W\u22121(y\u2212p)/parenrightbig\n= (XTWX)\u22121XTWz. (4.26)\nIn the second and third line we have re-expressed the Newton step as a\nweighted least squares step, with the response\nz=X\u03b2old+W\u22121(y\u2212p), (4.27)\nsometimes known as the adjusted response . These equations get solved re-\npeatedly, since at each iteration pchanges, and hence so does Wandz.\nThis algorithm is referred to as iteratively reweighted least squares or IRLS,\nsince each iteration solves the weighted least squares problem:\n\u03b2new\u2190arg min\n\u03b2(z\u2212X\u03b2)TW(z\u2212X\u03b2). (4.28)\nIt seems that \u03b2= 0 is a good starting value for the iterative procedure,\nalthough convergence is never guaranteed. Typically the algorithm does\nconverge, since the log-likelihood is concave, but overshooting can occur.\nIn the rare cases that the log-likelihood decreases, step size halving will\nguarantee convergence.\nFor the multiclass case ( K\u22653) the Newton algorithm can also be ex-\npressed as an iteratively reweighted least squares algorithm, but with a\nvector ofK\u22121 responses and a nondiagonal weight matrix per observation.\nThe latter precludes any simpli\ufb01ed algorithms, and in this case it is numer-\nically more convenient to work with the expanded vector \u03b8directly (Ex-\nercise 4.4). Alternatively coordinate-descent methods (Section 3.8.6) can\nbe used to maximize the log-likelihood e\ufb03ciently. The Rpackageglmnet\n(Friedman et al., 2010) can \ufb01t very large logistic regression problems ef-\n\ufb01ciently, both in Nandp. Although designed to \ufb01t regularized models,\noptions allow for unregularized \ufb01ts.\nLogistic regression models are used mostly as a data analysis and infer-\nence tool, where the goal is to understand the role of the input variables", "140": "122 4. Linear Methods for Classi\ufb01cation\nTABLE 4.2. Results from a logistic regression \ufb01t to the South African hear t\ndisease data.\nCoe\ufb03cient Std. Error ZScore\n(Intercept) \u22124.130 0 .964 \u22124.285\nsbp 0.006 0 .006 1 .023\ntobacco 0.080 0 .026 3 .034\nldl 0.185 0 .057 3 .219\nfamhist 0.939 0 .225 4 .178\nobesity -0.035 0 .029 \u22121.187\nalcohol 0.001 0 .004 0 .136\nage 0.043 0 .010 4 .184\ninexplaining the outcome. Typically many models are \ufb01t in a search for a\nparsimonious model involving a subset of the variables, possibly with some\ninteractions terms. The following example illustrates some of the issues\ninvolved.\n4.4.2 Example: South African Heart Disease\nHere we present an analysis of binary data to illustrate the traditional\nstatistical use of the logistic regression model. The data in Figure 4.1 2 are a\nsubset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried\nout in three rural areas of the Western Cape, South Africa (Rousseauw et\nal., 1983). The aim of the study was to establish the intensity of ischemic\nheart disease risk factors in that high-incidence region. The data represent\nwhite males between 15 and 64, and the response variable is the presence or\nabsence of myocardial infarction (MI) at the time of the survey (the overall\nprevalence of MI was 5.1% in this region). There are 160 cases in our data\nset, and a sample of 302 controls. These data are described in more detail\nin Hastie and Tibshirani (1987).\nWe \ufb01t a logistic-regression model by maximum likelihood, giving the\nresults shown in Table 4.2. This summary includes Zscores for each of the\ncoe\ufb03cients in the model (coe\ufb03cients divided by their standard errors); a\nnonsigni\ufb01cant Zscore suggests a coe\ufb03cient can be dropped from the model.\nEach of these correspond formally to a test of the null hypothesis that the\ncoe\ufb03cient in question is zero, while all the others are not (also known as\nthe Wald test). A Zscore greater than approximately 2 in absolute value\nis signi\ufb01cant at the 5% level.\nThere are some surprises in this table of coe\ufb03cients, which must be in-\nterpreted with caution. Systolic blood pressure ( sbp) is not signi\ufb01cant! Nor\nisobesity , and its sign is negative. This confusion is a result of the corre-\nlation between the set of predictors. On their own, both sbpandobesity\nare signi\ufb01cant, and with positive sign. However, in the presence of many", "141": "4.4 Logistic Regression 123\nsbp0 10 20 30\no\no\noo\nooo\noooo\no\noo\nooooo\no\nooo\noo\nooooooo\nooo\noo\nooo\noo\noo\nooo\noooooo\nooo\noo\nooooooooo\noooooooooooo\nooooo\nooooooooo\noo\nooooo\noo\noooo\noo\noooo\noo\nooooo\noooooo\noooooo\noooo\noooooo\nooo\noo\nooo\noo\noo\nooooo\noo\noo\noo\noo\noo\nooo\nooo\nooooo\noo\noo\no\nooo\noooo\no\noooooooo\nooooooo\noooooooooo\nooo\noo\noooo\noooo\noooooooo\nooooo\no\nooooo\noo\noo\nooo\noooo\nooooo\nooo\noooooo\nooo\nooo\nooo\noo\nooo\noooooo\noooooo\nooo\noooo\noooo\noooo\no\nooo\noooo\nooo\noo\nooo\nooooo\nooo\noooo\no oo\no\nooo\nooo\nooooooo\nooooo\noo\noooo\nooooooooo\noooooo\noooo\noooo\noooo\noooooo\no\nooo\noooooo\no\nooo\noooo\noo\noooo\noooooooooooo\nooo\noooo\noooo\noo\noo\nooo\no\noooo\no\noo\nooo\noooo\no\noo\noo ooo\no\nooooo\nooooooo\nooo\noo\nooo\noo\noo\nooo\noooooo\nooo\noo\nooooo\nooo\nooooooooooooo\nooooo\nooooooooo\noo\nooooo\noo\noooo\noo\noooo\noo\nooooo\noooooo\noooooo\noooooooooo\nooo\noo\nooo\noo\noo\nooooo\noo\noo\noo\noo\noo\nooo\nooo\nooooo\noo\noo\no\nooo\nooooo\noooooooo\nooooooo\noooooooooo\nooo\noo\noooo\noooo\noooooooo\nooooo\no\nooooo\noo\noo\nooo\noooo\noooooo oo\noooooo\nooo\noooooo\noo\noooo\nooooo\noooooo\nooo\noooo\noooo\noooo\no\nooo\noooo\nooo\noo\nooo\nooooo\nooo\noooo\nooo\no\nooo\nooo\noooo ooo\nooooo\noo\noooo\noooo\nooooo\noooooo\noooo\noooo\noooo\noooooo\no\nooo\noooo oo\noooo\noooo\noo\noooo\nooooo\nooooooo\nooo\noooooooo\noo\noo\nooo\no\nooo0.0 0.4 0.8\no\no\noo\nooo\noooo\no\noo\noo ooo\no\nooo\noo\nooo o ooo\nooo\noo\nooo\noo\noo\nooo\nooo ooo\nooo\noo\nooo oo\noooo\noooooo o ooo oo\noooo o\no ooo\no oooo\noo\no oo\noo\noo\noooo\noo\noooo\noo\nooooo\noooooo\noo\no ooo\noooooooo oo\no oo\noo\noo ooo\noo\nooooo\noo\noo\noo\noo\noo\nooo\nooo\nooooo\noo\nooo\nooo\noooo\no\nooo o\noo oo\noo ooooo\noo o ooooooo\nooo\noo\noo oo\noo oo\noooooooo\nooooo\no\no ooo o\noo\noo\nooo\noo oo\noooooooo\noooo oo\nooo\nooo\nooo\noo\nooooooooo\noooooo\no oo\noooo\noo\noo\noooo\no\nooo\noooo\no oo\noo\nooo\nooooo\nooo\noo oo\nooo\no\no oo\nooo\nooo oo oo\no oooo\noo\nooo o\no o oo\no\no o oo\nooo\nooo\no ooo\noooo\noooo\no oo ooo\no\nooo\noooooo\noooo\no ooo\noo\noooo\nooooo\nooo o ooo\nooo\noo o ooooo\noo\noo\nooo\no\noooo\no\noo\nooo\noooo\no\noo\noo ooo\no\nooooo\nooooooo\nooo\noo\nooo\noo\noo\nooo\noooooo\nooo\noo\nooooooooooooooooooooo\nooooo\nooooooooo\noo\nooo\noo\noo\noooo\noo\noooo\noo\nooooo\noooooo\noo\noooo\noooo\noooooo\nooo\noo\nooooo\nooooooo\noo\noo\noo\noo\noo\nooo\nooo\nooooo\noo\noo\no\nooo\noooo\no\noooooooo\nooooooo\noooo\noooooo\nooo\noo\noooo\noooo\noooooooo\nooooo\no\nooooo\noo\noo\nooo\noooo\noooooooo\noooooo\nooo\noooooo\noo\nooo\noooooo\noooooo\nooo\noooo\noooo\noooo\no\nooo\noooo\nooo\noo\nooo\nooooo\nooo\noooo\nooo\no\nooo\nooo\nooooooo\nooooo\noo\noooo\noooo\nooooo\noooooo\noooo\noooo\noooo\noooooo\no\nooo\noooooo\noooo\noooo\noo\no\nooo\nooooo\nooooooo\nooo\noooooooo\noo\noo\nooo\no\nooo0 50 100\no\no\noo\nooo\noooo\no\noo\nooooo\no\nooooo\nooooooo\nooo\noo\nooo\noo\noo\nooo\noooooo\nooo\noo\noooooooo\nooooooooooooo\nooooo\noooo\nooooo\noo\nooooo\noo\noooo\noo\noo\noo\noo\nooooo\nooo ooo\noo\noooo\noooooooooo\nooo\noo\nooo\noo\nooooooo\noo\noo\noo\noo\noo\nooo\nooo\nooooo\noooo\no\nooo\nooooo\noooooooo\nooooooo\noooooooooo\nooo\noo\noooo\noooo\noooooooo\nooo\noo\no\nooooo\noo\noo\nooo\noooo\noooooooo\noooooo\nooo\noooooo\noo\noooo\nooooo\noooooo\nooo\noooo\noo\noo\noooo\no\nooo\noooo\nooo\noo\nooo\nooooo\nooo\noooo\nooo\no\nooo\nooo\nooooooo\nooooo\noo\noooooooo\nooooo\noooooo\noooo\noooo\noooo\noooooo\no\nooo\noooooo\noooo\noooo\noo\noooo\nooooo\nooooooo\nooo\noooooooo\noo\noo\nooo\no\nooo\n100 160 220o\no\noo\nooo\no ooo\no\noo\nooooo\no\nooooo\nooooooo\nooo\noo\nooo\noo\noo\nooo\noooooo\nooo\noo\nooooooo o\noooooooooo ooo\nooooo\noooo\nooooo\noo\nooo\noo\noo\noooo\noo\noooo\noo\nooooo\no ooooo\noooooo\noooo\noooo oo\nooo\noo\nooooo\nooooooo\noo\noo\noo\noo\noo\nooo\nooo\nooooo\noo\noo\no\nooo\nooooo\nooooooo\no\nooo oooo\noo ooooo ooo\nooo\noo\noooo\noooo\noooooooo\nooo\noo\no\nooooo\noo\noo\nooo\noooo\noo ooo\nooo\noooooo\nooo\noooooo\noo\noooooo\nooo\noooo oo\nooo\noooo\noo\noo\noooo\no\nooo\noooo\nooo\no o\nooo\nooooo\nooo\noooo\nooo\no\nooo\nooo\nooo oooo\nooooo\noooooo\noo oo\nooooo\nooo\nooo\noooo\noooo\noooo\noooooo\no\nooo\nooo\nooo\noooo\noooo\noo\noooo\nooooo\nooooooo\nooo\noooooooo\noo\noo\nooo\no\nooo0 10 20 30o\noooo\nooooooo\nooo\nooo\noo\noo\nooo\nooo\no\nooooo\noooo\noo\nooooooo\nooo\noooo\noo\noo\nooooo\noooo\noo\noooo\noooooo\nooo\noo\nooo\nooooooooooooo\no\nooooooo\noooo\nooo\noo\nooooooooo\nooooo\noooooooo oo\nooooooooo\noo\noooo\noo\noo\noo\nooooo\nooooooooooooo\nooooooo\noo\no oooooooooooooooo\noooo\noooooo\noooo\noooooooo\noooo\noooo\noo\nooooo\noo\noooooo\nooooo\nooo\noooooo\nooo\noo\noooo\noo\noo\nooooooo\noooooooooooooo\no\nooooooo\no\noooo\nooo\noooo\no\noooo\nooo oo\nooooo\noooo\nooooo\nooooo\nooo\nooo\nooooooo\noo\noooooo\nooo\noo\noooooooo\nooo\no\noooo\nooooo\noooo\noo\noo o\nooo\no\no\noo\noo\noo\noo\noooo\noo\no\nooooo\nooo oooooooo\nooo\noooooooooooooooo oo\notobaccoo\noooo\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noooooo\noooo\noo\nooooooo\nooo\noooo\noo\noo\nooooo\noooo\noo\noooo\noooooo\nooooo\nooo\nooooooooooooo\no\nooooooo\noooo\nooo\noo\noo\nooooooo\nooooo\noooooooo oo\nooo\noooooo\noo\noooo\noo\noo\noo\nooooo\nooooooooooooo\nooooooo\noo\nooooooooooooooooo\noooo\noooooo\noooo\noooooooo\noooo\noooo\noo\nooooo\noo\nooo\nooo\nooooo\nooo\noooooo\nooo\noo\noooooo\noo\noooo\nooo\noooooooooooooo\no\nooooooo\no\noooo\nooo\noooo\no\noooo\nooooo\nooooo\noooo\nooooo\noo ooo\nooo\nooo\nooooooo\noo\nooooooooo\noo\noooo oo\noo\nooo\no\noo\noo\nooooo\noooo\noo\nooo\nooo\no\no\noo\noo\noo\noo\noooo\noo\no\nooooo\nooooooooooo\nooo\noooooooooo\noooooooo\noo\no ooo\noo o\noooo\nooo\nooo\noo\noo\no oo\nooo\no\nooooooooo\noo\no o ooooo\nooo\noooo\noo\noo\no o o oo\noooo\noo\noooo\no ooo oo\nooooo\no oo\noo ooooooo oooo\no\no o ooooo\noooo\nooo\noooo\noooooo o\no oooo\nooooooo o oo\no oooo oooo\noo\noooo\noo\noo\noo\nooooo\noooooo o oooooo\no oooo\noo\noo\no o oo oo o oo oooo ooo ooooo\noooooo\noooo\no ooo oo oo\noooo\noooo\no o\nooo oooo\no ooooo\nooooo\nooo\noo oooo\nooo\noo\noo oo\noo\noo\nooooooo\noooooooooooooo\no\nooo o o oo\no\noooo\nooo\noooo\no\noo oo\noo o oo\nooooo\nooo o\nooooo\noo oo o\nooo\nooo\nooooooo\noooo oo oo\nooo\noo\no ooo oooo\no oo\no\noo\noo\nooooo\noooo\noo\nooo\nooo\no\nooo\noo\noo\noo\noooo\noo\noooo oo\nooooo o o o ooo\no oo\noo oo oooooo\no ooo oooo\noo\noooo\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\no\nooooooooo\noo\nooooooo\nooo\noooo\noo\noo\nooooo\noooo\noo\noooo\noooooo\nooo\noo\nooo\nooooo\noooooooo\no\nooooooo\noooo\nooo\noo\nooooooooo\nooooo\noooooooooo\nooooooooo\noo\noooo\noo\noo\noo\nooooo\nooooooooooooo\nooooo\noo\noo\nooooooooooooooooo\noooo\noooooo\noooo\noooooooo\noooo\noooo\noo\nooooooo\noooooo\nooooo\nooo\noooooo\nooo\noo\noooooo\noo\nooooooo\noooooooooooooo\no\nooooooo\no\noooo\nooooooo\no\noooo\nooooo\nooooo\noooo\nooooo\nooooo\nooo\nooo\nooooooo\noooooooo\nooo\noo\noooooooo\nooo\no\noo\noo\nooooo\noooo\noo\nooo\nooo\no\no\noo\noo\noo\noo\noooo\noo\no\nooooo\nooooooo oooo\nooo\noooooooooo\noooooooo\noo\noooo\nooo\noooo\no oo\nooo\noo\noo\nooo\nooo\no\nooooooooo\noo\nooooooo\nooooooo\noo\noo\nooooo\noooo\noo\noooo\noooooo\nooooo\nooo\nooooooooooooo\no\nooooooo\noooo\nooo\noo\nooooooooo\nooooo\noooooooooo\nooo\noooooo\noo\noooo\noo\noo\noo\no oooo\nooooooooooooo\nooooo\noo\noo\nooooooooooooooooo\noooo\noooooo\noooo\noooooooo\noooo\noo oo\noo\nooooooo\nooooo o\nooooo\nooo\noooooo\nooo\noo\noooo\noo\noo\noooo\nooo\noooooooooooooo\noooooooo\no\noooo\nooo\noooo\no\noooo\nooooo\nooooo\noooo\nooooo\nooooo\nooo\nooo\nooooooo\nooooooooooo\noo\no ooo oo\noo\nooo\no\noo\noo\nooooo\noooo\noo\nooo\nooo\no\no\noo\noo\noo\noo\noooo\noo\no\nooooo\no ooooooo ooo\nooo\noooooooooooooooooo\noo\noooo\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\no\nooooo\noooo\noo\nooooooo\nooo\noooo\noo\noo\no o ooo\noooo\noo\noooo\noo oooo\nooooo\no oo\nooo oooooo oooo\no\nooooooo\noooo\nooo\noo\noooo ooooo\nooooo\no ooooooo oo\nooo\noooooo\noo\noooo\noo\noo\noo\nooooo\nooooooooooooo\nooooo\noo\noo\nooooooooooooooooooooo\noooo oo\noooo\no ooooooo\noooo\noooo\noo\nooooo\noo\noooooo\nooooo\nooo\nooo ooo\nooo\noo\noooo\noo\noo\noooo\nooo\nooooooooo\nooooo\nooooo ooo\no\noooo\nooo\noooo\nooooo\nooo oo\no oooo\noooo\nooooo\nooooo\nooo\nooo\nooooooo\nooooooooooo\noo\noooooooo\nooo\nooo\noo\nooooo\noooo\noo\nooo\nooo\no\no\noo\noo\noo\noo\noooo\noo\no\nooooo\noooooooooooooo\noooooooooo\nooo o oooo\no\noooo\noo\noooo\noo\nooooo\nooo\nooo\nooo\no\nooo\nooooo\noo\no\nooo\no\noooooo\nooo\noo\nooo\noooo\noooo\nooooo\noooooo\noooooo\noo\no\nooooo\nooo\nooo\noooo\noo\noooo\noooo\nooo\noo\noo\nooo\noooo\nooo\nooo\noo\nooo\noo\nooooooooo\noooooo\noooo\nooo\nooo\nooooooo\nooo\noo\nooo\nooo\noooo\noo\nooooo\noooo\nooooo\no\nooooooooo\noo\no\no\nooo\noooo\noooo\nooo\nooo\noo\nooooooo\nooooo\no\nooooooo\noo\nooo\nooo\nooooo\nooooo\noooo\noooo\noooooooooooo\noo\noo\noooooooo\noo\nooooooo\noooo\noooooooooooo\nooo\nooo\noo\nooo\no\noo\noo\nooo\no\noooooo\noo\noooooo\noooooooooo\nooo\noooooo\nooo\nooo\noo\nooooooooo\noo\noooooo\no\nooooo\noo\noo\noo\noooooooooo\noooo\nooooo ooooo\noo\noooo\nooo\noooo\no\noooo\noo\noo\nooo\noo\noooo\noo\nooooo\nooo\nooo\no oo\no\nooo\nooooo\noo\no\nooo\no\noooooo\nooo\noo\nooo\noooo\noooo\nooooo\noooooo\noooooo\noo\noooooo\nooo\nooo\noooo\noo\noooo\noooo\nooo\noo\noo\nooo\noooo\nooo\nooo\nooooo\noo\nooooooooo\noooooo\noooo\nooo\nooo\no oooooo\nooo\noo\nooo\nooo\nooo\no\noo\nooooo\noo\noo\nooooo\no\nooo\noooooo\noo\no\no\nooo\noooo\noooo\nooo\nooo\nooo\noooooo\nooooo\no\noooo\noo\no\noo\nooo\nooo\nooooo\nooooo\noooo\noooo\noooooooo oooo\noo\noo\noooooooo\noo\nooooooo\noooo\noooooooooooo\nooo\nooo\noo\nooo\no\noo\noo\nooo\no\nooo\nooo\noo\noooooo\noooooo\noooo\nooo\noooooo\nooo\nooo\noo\no\noooooooo\noo\noooooo\no\nooooo\noo\noo\noo\noooooooooo\noooo\noooo\noooooo\noo\no\nooo\nooo\noooo\no\nooo\no\noo\noldl\noo oo\noo\noooo\noo\noo o oo\nooo\nooo\nooo\no\no oo\noo ooo\noo\noooo\no\noo oooo\nooo\noo\no oo\noooo\nooo o\no oo\noo\noooooo\noo oo oo\noo\noooo oo\nooo\nooo\noooo\noo\noo oo\nooo o\noo o\noo\noo\nooo\noooo\nooo\nooo\noo\nooo\noo\no o ooooooo\nooo ooo\nooooo ooooo\nooooooo\nooo\noo\no oo\no oo\nooo\no\noo\nooooo\noooo\nooooo\no\nooo\no oo ooo\noo\no\no\nooo\noooo\noooo\nooo\nooo\noo\no\noooooo\noo ooo\no\noooooo\no\noo\nooo\nooo\no oooo\nooooo\no ooo\noooo\noooooooooooo\noo\noo\nooooo o oo\noo\no oo\noo oo\no ooo\nooooo ooooooo\nooo\no oo\nooooo\no\noo\noo\nooo\no\nooo\nooo\noo\noooo\noo\noooooo\noooo\no o o\nooo ooo\nooo\nooo\noo\no\noooooooo\noo\nooo ooo\no\nooooo\noo\noo\noo\noo oooooooo\noo oo\noooo\no ooooo\noo\noooo\nooo\noo oo\no\nooo\no\noo\nooooo\noo\noooo\noo\nooooo\nooo\nooo\nooo\no\nooo\nooooo\noo\no\nooo\no\noo o ooo\nooo\noo\nooo\noooo\noooo\nooooo\noooooo\noooooo\noo\nooooooooo\nooo\noooo\noo\noooo\noooo\nooo\noo\noo\nooo\noooo\nooo\nooo\noo\nooo\noo\noo ooooooo\noooooo\noo\noo\nooo\nooo\nooooooo\nooo\noo\nooo\nooo\noooo\noo\nooooo\noooo\nooo oo\no\nooooooooooo\no\no\nooo\noooo\noooo\nooo\nooo\nooooooooo\nooooo\no\noooooo\no\noo\nooo\nooo\nooooo\nooooo\noooo\noooo\noooooooooooo\noo\noo\noooooooo\noo\nooo\noooo\noooo\noooooooooooo\nooo\nooo\noo\nooo\no\noo\noo\nooo\no\noooooo\noo\noooooo\noooooooooo\nooo\noooooo\nooo\nooo\noo\no\noooooooo\noo\noooooo\no\nooooo\noo\noo\noo\noooooooooo\noooo\noooooooooo\noo\noooo\nooo\noooo\no\nooo\no\noo\nooooo\noo\noooo\noo\nooo oo\nooo\nooo\nooo\no\nooo\nooooo\noo\noooo\no\noooooo\nooo\noo\nooo\noooo\noooo\nooooo\noooooo\noooooo\noo\nooooooo\noo\nooo\noooo\noo\noooo\noooo\nooo\noo\noo\nooo\noooo\nooo\nooo\noo\nooo\noo\nooooooooo\noooooo\noooo\no oo\nooo\nooooooo\nooo\noo\nooo\nooo\nooo\no\noo\nooooo\noooo\nooooo\no\nooo\noooooo\noo\no\no\nooo\noo\noo\noooo\nooo\nooo\noo\noo ooooo\nooooo\no\noooooo\no\noo\nooo\nooo\nooooo\nooooo\noooo\noooo\noooooooooooo\noo\noo\noooooooo\noo\nooooooo\noooo\noooooooooooo\no oo\nooo\noo\nooo\no\nooo\no\nooo\no\nooo\nooo\noo\noooo\noo\noooooo\nooooo oo\noooooo\nooo\nooo\noo\no\noooooooo\noo\noooooo\no\nooooo\noo\noo\noo\noooo oooooo\noooo\noooo\noooooo\noo\noooo\nooooooo\no\noooo\noo\no\n2 6 10 14oooo\noo\noooo\noo\nooooo\nooo\nooo\nooo\no\nooo\nooooo\noo\no\nooo\no\noo o ooo\nooo\noo\nooo\noooo\noooo\no oooo\noooooo\noooooo\noo\noooooo\noooooo\no ooo\noo\noooo\noooo\nooo\noo\noo\nooo\noo oo\nooo\nooo\noo\no oo\noo\nooo oooooo\noooooo\no oooo oo\nooo\no ooo ooo\nooo\noo\no oo\nooo\noooo\noo\nooooo\noo\noo\nooooo\no\nooooooooo\noo\no\no\nooo\noooo\noooo\nooo\nooo\noo\nooooooo\no oooo\no\noooooo\no\noo\nooo\nooo\noo ooo\nooooo\noooo\noooo\noooooooooooo\noo\noo\noooooooo\noo\nooo\noooo\noooo\nooo ooooooooo\nooo\nooo\nooooo\no\noo\noo\nooo\no\nooo\nooo\noo\noooo\noo\noooooooooo\noo o\noooooo\nooo\nooo\noo\no\noooooooo\noo\noooooo\no\nooooo\noo\noo\noo\noooooooooo\noooo\noooo\noooooo\noo\noooo\nooooooo\no\noooo\noo\no0.0 0.4 0.8o\nooooo\noooo\noo\noooo\noooo\nooo\no oo\nooo\noooo\noooooooo\nooo\noooo\nooo\nooo\noooooo\nooo\noooo\noo\nooooo\noooo\nooo\noo\noo\noooo\nooooo ooooo\noo\noo\nooooo\noooo\noo\noo\nooo\nooooooo\noo\noo\noooooo\noo\noo\noo\noooooo\noooooooo\nooooo o ooooooooo\noooooo\nooooooooo\noooo\nooo\nooo\nooo\nooo\nooo\nooo\noo\noo\nooooooo o oo\noo\nooo\nooo o\nooooooo\noooo\nooo\nooo\noo\noooooooo\noo\nooooo\nooooo o\noo\noooo\noooooo oo\nooooooooo\nooooooo\nooo\nooo\noooo\noo\noooo\noo\nooo\noo\nooo\noooooo\noooo\noo\noooo\noooo\noo\nooooooo\nooo\noo\noooo\noooo\nooo\nooo\nooo\nooooo\noooo\nooooo\nooooo\noo\nooooo\noooo\noooooo oo\nooo\no oooo\nooo\noo\noooooo\noo\nooooo\noooo\noooo\noooo\nooo\noo ooo o\nooooo\noooo\noo\noooo\nooo o\nooo\no oo\nooo\noooo\noooooooo\nooo\noooo\nooo\nooo\noooooo\nooo\noooo\noo\nooooo\noooo\nooo\noo\noo\noooo\noooooooooo\noo\noo\no oooo\noooo\noo\noo\nooo\noo ooooo\noo\noo\noooooo\noo\noo\noo\noooooo\noooooooo\nooooo o ooooooooo\noo oooo\nooooooooo\no ooo\nooo\nooo\nooo\nooo\nooo\nooo\noo\noo\noooooooooo\noo\nooo\noooo\noooo ooo\noooo\nooo\nooo\noo\noooooooo\noo\nooooo\noooooo\noo\noooo\noooooooo\nooooooooo\nooooooo\nooo\nooo\noooo\noo\noooo\noo\nooo\noo\nooo\noooooo\no ooo\noo\noooo\noooo\noo\nooooooo\nooo\noo\noooo\noooo\nooo\nooo\nooo\nooooo\noooo\nooooo\nooooo\noo\nooooo\noo oo\noo o ooooo\nooo\nooooo\nooo\noo\noooooo\noo\nooooo\noooo\noooo\noooo\nooo\nooooo o\nooooo\noooo\noo\noooo\noooo\nooo\nooo\nooo\noooo\noooooooo\nooo\noooo\nooo\nooo\noooooo\nooo\noooo\noo\nooooo\noooo\nooo\noo\noo\noooo\noooooooooo\noo\noo\nooooo\noooo\noo\noo\nooo\nooooooo\noo\noo\noooooo\noo\noo\noo\noooooo\noooooooo\nooooooooooooooo\noooooo\nooooooooo\noooo\nooo\nooo\nooo\nooo\nooo\nooo\noo\noo\noooooooooo\noo\nooo\noooo\nooo oooo\noooo\nooo\nooo\noo\noooooooo\noo\nooooo\noooooo\noo\noooo\noooooooo\nooooooooo\nooooooo\nooo\nooo\noooo\noo\noooo\noo\nooo\noo\nooo\noooooo\noooo\noo\noooo\noooo\noo\noo ooooo\nooo\noo\noooo\noooo\nooo\nooo\nooo\nooooo\noooo\nooooo\nooooo\noo\nooooo\noooo\noooo oooo\nooo\nooooo\nooo\noo\noooooo\noo\nooooo\noooo\noooo\noooo\nooo\nooo oo\nfamhisto\nooooo\noooo\noo\noooo\noooo\nooo\nooo\nooo\noooo\noooooooo\nooo\no ooo\nooo\nooo\noooooo\noo o\noooo\noo\noo ooo\noooo\nooo\noo\noo\noooo\noooooooooo\noo\noo\nooooo\noooo\noo\noo\nooo\nooo oooo\noo\noo\noooooo\noo\noo\noo\noooooo\noooooooo\nooooooooooooooo\noooooo\nooooooooo\noooo\nooo\nooo\nooo\nooo\nooo\nooo\noo\noo\noooooooooo\noo\nooo\noooo\nooooooo\noooo\nooo\nooo\noo\noooooooo\noo\nooooo\noooooo\noo\noooo\noooooooo\nooooooooo\nooooooo\nooo\nooo\noooo\noo\noooo\noo\noo o\noo\nooo\noooooo\noooo\noo\noooo\noooo\noo\nooooooo\nooo\noo\noooo\noooo\nooo\nooo\nooo\nooooo\noooo\nooooo\nooooo\noo\nooooo\noooo\noooooooo\nooo\nooooo\nooo\noo\noooooo\noo\nooooo\noooo\noooo\noooo\nooo\nooooo o\nooooo\noooo\noo\noooo\noooo\nooo\nooo\nooo\noooo\noooooooo\nooo\noooo\nooo\nooo\noo oooo\nooo\noooo\noo\nooooo\noooo\nooo\noo\noo\noooo\noooooooooo\noo\noo\noo o oo\noooo\noo\noo\nooo\nooooooo\noo\noo\noooooo\noo\noo\noo\noooooo\noooooooo\no ooooooo ooooooo\noooooo\nooooooooo\noooo\nooo\nooo\nooo\nooo\nooo\nooo\noo\noo\noooooooooo\noo\nooo\nooo o\nooo o ooo\noooo\nooo\nooo\noo\no ooooooo\noo\nooooo\no ooooo\noo\noooo\noooooooo\nooooooooo\nooooooo\nooo\nooo\noooo\noo\noooo\noo\nooo\noo\nooo\noooooo\noooo\noo\nooo o\noooo\noo\nooooooo\nooo\noo\noooo\noooo\noo o\noo o\nooo\nooooo\noooo\nooooo\nooooo\noo\nooooo\noooo\no ooooooo\nooo\noo ooo\nooo\noo\noooooo\noo\nooooo\noooo\nooo o\noooo\nooo\nooooo o\nooooo\noo oo\noo\no ooo\noooo\nooo\nooo\nooo\noooo\noooooo oo\nooo\no ooo\no oo\nooo\noooooo\nooo\noooo\noo\noo ooo\noooo\nooo\noo\noo\noooo\no o o ooooooo\noo\noo\noo ooo\noooo\noo\noo\nooo\no o ooooo\noo\noo\noooooo\noo\noo\noo\noooooo\noooooooo\no oooo o ooo oooooo\noo oooo\noo o oooooo\noooo\nooo\nooo\nooo\nooo\no oo\noo o\noo\noo\no oo oooo o oo\noo\nooo\noooo\noo ooooo\noo oo\nooo\nooo\noo\no o oooooo\noo\noo ooo\noo oooo\noo\noooo\noooooooo\nooooooo oo\noooo ooo\nooo\nooo\noooo\noo\no ooo\noo\nooo\noo\nooo\nooo ooo\noooo\noo\noooo\no oo o\noo\nooooooo\nooo\noo\noooo\noooo\nooo\noo o\nooo\nooooo\noooo\nooooo\nooooo\noo\nooooo\noooo\noo oooooo\nooo\noo ooo\nooo\noo\noooooo\noo\nooooo\noooo\noooo\noooo\nooo\nooooo\noooo\noo\nooooo\nooooooo\noo\nooooo\noo\nooo\nooooo\nooo\nooo\noooo\nooo\nooooo\nooooooooooo\nooo\noo\noooo\noo\nooooooo\noo\noo\noo\noo\nooo\noo\nooooooooooo\noo\noooo\noo\noo\noooo\noooooo\noo\noo\noo\nooooooooo\nooo\nooooo\nooooooooo\noooo\noooooooo\no\nooo\nooo\nooooo\no\noo\noo\nooooo\noo\nooooo\noooo\nooooo\noo\noo\noo\noo\noooo\noo\noo\nooooooooo\nooooo\noooooo\no\noooooo\noooooo\nooo\noooo\noooo\noooo\noooooo\nooooooo\nooooo\nooooo\nooooo\nooo\no\nooo\nooo\noooo\no\nooooooooo\nooo\noooooo\nooooooo\noooooo\no\noooooo\nooooo\nooooooooo\no\nooooo\nooooooo\nooo\nooo\noooooo\nooo\noo\nooooo\nooo\nooo\noooo\no\noo\nooo\nooo oo\nooooo\noo\noooo\nooooooo\noooo\noo\nooo\nooooo\noo\noo\noo\nooooo\noo\noooo\no\nooooooooo\nooooooo\nooo\nooooo\nooo\nooo\noooo\nooo\nooooo\nooooooooooo\nooo\noo\noooo\noo\nooooooo\noo\noo\noo\noo\nooo\noo\noooo\nooooooo\noo\noooo\noo\noo\noooo\noooooo\noo\noo\noo\nooooooooo\nooo\nooooo\nooooooooo\noooo\noooooooo\no\nooo\nooo\nooooo\no\noo\nooooooo\noo\nooooooooo\nooooooooo\noo\noo\noooo\noo\noo\noooooo\nooo\nooooo\noooooo\no\noooooo\noooooo\nooo\noooo\noo oo\noooo\noooooo\nooooooo\nooooo\nooooo\nooooo\nooo\no\nooo\nooo\noooo\no\nooooooooo\nooo\noooooo\nooooooo\nooooooo\noooooo\nooooo\nooooooooo\no\noooooooo\noooo\nooo\nooo\noooooo\nooo\noo\nooooo\nooo\nooo\noooo\no\noo\nooo\nooooo\nooooo\noo\nooooooooooo\noooo\noo\nooo\nooooo\noo\noo\noo\nooooo\noo\noooooooooo oo\noo\nooooooo\nooo\nooooo\nooo\nooo\noooo\nooo\nooooo\nooooooooooo\nooo\noo\noooo\noo\nooooo\noo\noo\noo\noo\noo\nooo\noo\nooooo oooooo\noo\noooo\noo\noo\noooo\noooooo\noo\noo\noo\noo\noo\nooooo\nooo\nooooo\nooooooooo\noooo\noooooooo\no\nooo\nooo\nooooo\no\noo\noo\nooooo\noo\noooo\nooooo\nooooooo\noo\noooo\noooo\noo\noo\nooooooooo\nooooo\noooooo\no\noooooo\noooooo\nooo\noooo\noooo\noooo\noooooo\nooooooo\nooooo\nooooo\nooooo\nooo\no\nooo\nooo\noooo\no\nooooooooo\nooo\nooo\nooo\nooooooo\nooooooo\noooooo\nooooo\noooo\nooooo\no\noooooooo\noooo\nooo\nooo\noooooo\nooo\noo\nooooo\nooo\nooo\noooo\no\noo\nooo\nooooo\nooooo\noo\nooooooooooo\noooo\noo\nooo\nooooo\noo\noo\noo\nooo oo\noo\nooooo\nooo oo oooo\nooooooo\noo o\nooooo\nooo\nooo\noooo\nooo\noo ooo\noooooo o ooo o\nooo\noo\noooo\noo\nooooo\noo\no o\noo\noo\noo\nooo\noo\noooooooo ooo\noo\nooo o\noo\noo\noooo\noooooo\noo\noo\noo\nooooo o ooo\nooo\noo oo o\nooooooo oo\noooo\noooooooo\no\nooo\nooo\nooooo\no\noo\nooo oooo\noo\no ooo\nooo oo\no oooooo\noo\noo\noo\noooo\noo\noo\nooo o ooooo\nooooo\noooo oo\no\no oo oo o\noooooo\no oo\noooo\no\nooo\noooo\no o\noooo\nooooooo\nooooo\nooooo\no\noooo\nooo\no\nooo\nooo\noooo\no\no oooo oo oo\nooo\nooo\nooo\noo ooooo\nooooooo\noooooo\nooooo\no oooooo oo\no\noo\noooooooooo\nooo\nooo\noooooo\nooo\noo\noo ooo\nooo\nooo\noooo\no\noo\no oo\nooooo\nooooo\noo\nooo oooooooo\noooo\noo\nooo\nooooo\noo\noo\noo\noobesity\noooo\noo\noooo\noooooooooo\nooooo\noo\nooo\nooooo\nooo\nooo\noooo\nooo\nooooo\nooooooooooo\nooo\noo\noooo\noo\nooooooo\noo\noo\noo\noo\nooooo\noooo\nooooooooo\noooo\noo\noo\noooo\noooooo\noo\noo\noo\noo\nooooooo\nooo\nooooo\nooooooooo\noooo\noooooooo\no\nooo\nooo\nooooo\no\noo\nooooooo\noo\noooo\nooooo\nooooooo\noo\noo\noo\noooo\noo\noo\nooooooooo\nooooo\noooooo\no\noooooo\noooooo\nooo\noooo\no\nooo\noooo\nooooo\no\nooooooo\nooooo\nooooo\nooooo\nooo\no\nooo\nooo\noooo\no\nooooooooo\nooo\nooo\nooo\nooooooo\nooooooo\noooooo\nooooo\nooooooooo\no\noo\nooooo\nooooo\nooo\nooo\noooooo\nooo\noo\nooooo\noooooo\noooo\no\noo\nooo\nooo oo\nooooo\noo\nooooooooo oo\noooo\noo\nooo\noooo\no\noo\noo\noo\no\n15 25 35 45oooo\noo\nooooo\nooo oooooo\nooooooo\nooo\nooooo\nooo\nooo\noooo\nooo\nooooo\noooooo ooooo\nooo\noo\noooo\noo\nooooooo\noo\noo\noo\noo\nooo\noo\nooooooooooo\noo\noooo\noo\noo\noooo\noooooo\noo\noo\noo\noooo\nooooo\nooo\nooooo\noooo\nooo oo\noooo\nooo ooooo\no\nooo\nooo\nooooo\no\noo\noo\nooooo\noo\nooooo\noooo\nooooooo\noo\noo\noo\noooo\noo\noo\nooooooooo\nooooo\noooooo\no\noooooo\noooooo\nooo\noooo\no\nooo\noooo\noo\nooo\no\nooooooo\nooooo\nooooo\no\noooo\nooo\no\nooo\nooo\noooo\no\nooooooooo\nooo\nooo\nooo\nooooooo\nooooooo\noooooo\nooooo\nooooooooo\no\noo\noooooo\noooo\nooo\nooo\noooooo\nooo\noo\nooooo\nooo\nooo\noooo\no\noo\nooo\nooooo\nooooo\noo\noooo\nooooooo\noooo\noo\nooo\nooooo\noo\noo\noo\no0 50 100o\noooo\noooooo\noooooo\noo\nooo\noooooo\nooooooooo\nooo\nooooooooo\nooo\noooo\noooooooooo\nooooooo\nooooooo\noooooo\nooooooooooo\nooooooooo\no\noooooooo\noo\noo\noooooo\noooo\nooo\nooo\nooo\noooo\noo\nooooooooooo\no\noo\noo\noo\nooooooooo oooooooooooooo\nooooooooo\nooooooo\no\nooo\nooooo\noooooooooooooooo\noo\nooo\noooo\noooooo\noo\noooooooo\noooooo\nooo\noooo\noooooo\noooooooooooo\nooo\nooooo\noooo\noo\nooo\nooooooo\nooo\no\noo\nooo\no\noo\nooo\noo\noooo oooooooooo\nooo\no\noo\nooo\noooooo\noo\nooo\nooooooo\noo\noooooo\no\noo\noo\noo\nooo\nooo\nooooo\noooo\noooo\no\nooooooo\noo\nooooo\no ooo\noooo\noo\noooo\no\noooooo ooooooooo\noo\nooooo\no\noo\nooo\nooooo\noo\noooo\noooooo\noo oooo\noo\nooo\no ooooo\nooooooooo\nooo\noooo\nooooo\nooo\noooo\noooooooooo\nooooooo\nooo\noooo\noooooo\nooooooooooo\nooo\noooooo\nooooooooo\noo\noo\noooooo\noooo\nooo\nooo\nooo\noooo\noo\nooooooooooo\no\noo\noo\noo\nooooooooooooooooooooooo\nooooooooo\nooooooo\no\nooo\nooo\noo\noo\noooooooooooooooo\nooo\noooo\noooooo\noo\noooooooo\noooooo\nooo\noooo\noooooo\noooooooooooo\nooo\nooooo\noooo\noo\nooo\noooooooooo\no\noo\nooo\no\noo\nooo\noo\nooooooooo\nooooo\nooo\no\noo\nooo\noooooooo\nooo\noo\nooooo\noo\noooooo\no\noo\noo\noo\nooo\nooo\nooooo\noooo\noooo\no\nooooooooo\noo ooo\noooo\noooo\noo\noooo\no\noo ooooo oooooooo\noo\nooooo\no\noo\nooo\nooooo\noo\noooo\noooooo\noooooo\noo\nooo\noooooo\nooooooooo\nooo\nooooo oooo\nooo\noo\noo\noooooooooo\nooooooo\nooooooo\noooooo\nooooooooooo\nooo\noooooo\nooooooooo\noo\noo\noooooo\noooo\nooo\nooo\nooo\noooooo\nooooo\noooooo\no\noo\noo\noo\nooooooooooooooooooooooo\noooooooooooooooo\no\nooo\nooo\noo\noo\noooooooooooooo\noo\nooo\noooo\noooooo\noo\noooooooo\noooooo\nooo\noooo\noooooo\noooooooooooo\nooo\nooooo\noooo\noo\nooo\noooooooooo\no\noo\nooo\no\noo\nooo\noo\noooooooooooooo\nooo\no\noo\nooo\noooooo\noo\nooo\nooooooo\noo\noooooo\no\noo\noo\noo\nooo\nooo\nooooo\noooo\noooo\no\nooooooooo\nooooooooo\noooo\noo\noooo\no\noooooooooo ooooo\noo\nooooo\no\noo\nooo\noooo o\noo\no ooo\noo oooo\no oo oo o\noo\nooo\no o o o oo\no oooooooo\nooo\no ooooo ooo\no oo\noooo\nooo o o ooooo\nooooooo\no ooo ooo\no o oooo\no oo oooooooo\noooo o oooo\noo ooo o o oo\noo\noo\noooooo\no ooo\nooo\nooo\no oo\noooo\noo\nooooo\noooooo\no\noo\noo\noo\nooooooooo ooo ooooooooooo\no oooo o ooooooo ooo\no\nooo\no oo\noo\nooooooooooo ooo oo\noo\nooo\noooo\noooooo\noo\no oo oo ooo\noo oooo\nooo\noooo\no ooooo\no o ooo ooooooo\nooo\noo ooo\noooo\noo\nooo\noo oo o oo\noooo\noo\nooo\no\noo\nooo\noo\no oo o oooooooooo\nooo\no\noo\no oo\noo o ooooo\nooo\noo\noo ooo\noo\nooo o oo\no\noo\noo\noo\nooo\no oo\noo ooo\noooo\noo oo\no\nooooo o ooo\no oooooooo\noooo\noo\no ooo\no\no ooooo o oooooo oo\noo\no oooo\no\noo\nooo\nooooo\noo\noooo\noooooo\noooooo\noo\nooo\nooooooooooooooo\nooo\nooooo oooo\nooo\noooo\nooooooo\nooo\nooooooo\nooooooo\noooooo\nooooooooooo\nooooooooo\no\noooooooo\noo\noo\noooooo\noooo\nooo\nooo\nooo\noooooo\nooooo\noo oooo\no\noo\noo\noo\nooooooooooooooooooooooo\noooooo oooooooooo\no\nooo\nooo\noo\nooooooooooo ooooo\noo\nooo\noooo\noooooo\noo\noooooooo\noooooo\nooo\noooo\noooooo\noooooooooooo\nooo\nooooo\noooo\noo\nooo\nooooooo\nooo\no\noo\nooo\no\noo\nooo\noo\noooooooooooooo\nooo\no\noo\nooo\noooooooo\nooo\noo\nooooo\noo\noooooo\no\noo\noo\noo\nooo\nooo\nooooo\noooo\noooo\no\nooooooooo\nooooooooo\noooo\noo\noooo\no\nooooooooooooooo\noo\nooooo\no\noo\nooo\nooooo\noalcoholo\noooo\noooooo\nooo ooo\noo\nooo\noooooo\nooooooooo\nooo\nooooo oooo\nooo\noooo\nooo o ooo\nooo\nooooooo\nooooooo\noooooo\noooo o oooooo\nooooooooo\nooooo oooo\noo\noo\noo oooo\no ooo\nooo\nooo\nooo\noooo\noo\nooooo\noooooo\no\noo\noo\noo\nooooooooooooooooo o ooooo\nooooooooo\nooooooo\no\nooo\nooooo\noooo o oooooo ooooo\noo\nooo\noo oo\nooo o\noo\noo\noooooooo\no oooo\no\nooo\noooo\no oooo o\noooooooooooo\nooo\nooooo\noooo\noo\nooo\nooooo oooooo\noo\nooo\no\noo\nooo\noo\noooo ooooo\nooooo\nooo\nooo\nooo\noooooooo\nooo\noooo ooo\noo\noooooo\no\noo\noo\noo\nooo\nooo\nooooo\noooo\noooo\no\nooooooooo\noo ooo\noooo\noooo\noo\noooo\no\no ooooo ooooooooo\noo\nooooo\no\noo\nooo\no oooo\no\n100 160 220oo\noo\noo\noo\nooo\no\nooo\nooooo\noooo\no\nooo\noo\nooo\noo\nooo\noo\no\no\noo\nooo\no\noooo\no\nooo\noo\noo\nooo\no\noo\noooo\nooo\noooooo\nooooo\nooo\nooo\noo\noooo\nooo\noo\noo\noooo\no\no\nooo\nooo\no\nooo\noo\nooo\noo\noooooo\noo\noo\noo\nooo\nooo\no o\noo\noo\noo\noooo\noo\noo\noo\nooooo\no\noo\no\nooooo\no\noooooo\noo\noo\nooo\noo\no\noooooo\no\noooo\no\no\nooooo\nooo\noo\no\nooo\nooo\noo\no\nooo\noooo\nooooo\no\noooooooooo\no\no\noo\nooo\nooo\no\nooo\noo\nooo\nooo\no\nooooo\no\nooooo\noo\nooooo\noooo\noo\noooo\no\nooo\noooo\nooo\noo\nooo\no\noo\noo\nooo\no\noo\nooo\nooooo\nooo\nooooo\noo\nooo\noo\no\nooo\noo\nooo\noo\nooo\no\noooooo\no\noo\noo\noo\noo\no\no\no\noooo\no\noo\noooo\no\noo\noo\noooooo\nooo\no\nooo\noo\no\noo\noo\nooo\noo\no\nooo\noo\noooooooooo\no\noo\no\nooo\nooo\noooo\no\nooooo\noooo\noo\noo\noo\nooo\no\nooo\nooooo\noooo\no\nooo\noo\nooo\noo\nooo\noo\no\no\noo\nooo\no\nooooo\nooo\noo\noo\nooo\no\noo\noooo\nooo\noooooo\nooooo\nooo\nooo\noo\noooo\nooo\noo\noo\noooo\no\no\nooo\noo o\no\nooo\noo\nooo\noo\nooooo\no\noo\noo\noo\nooo\nooo\noo\noo\noo\noo\no\nooo\noo\noo\noo\noo\nooo\no\noo\no\nooooo\no\noooo\noo\noo\noo\nooo\noo\no\no\nooooo\nooooo\no\no\nooooo\nooo\noo\no\nooo\nooo\noo\no\nooo\noooo\nooooo\no\noooooooooo\no\no\noo\nooo\nooo\no\nooo\noo\nooo\nooo\no\nooooo\no\nooooo\noo\nooooo\noooo\noo\noooo\no\nooo\noooo\nooo\noo\nooo\no\noo\noo\nooo\no\noo\nooo\noooo o\nooo\nooooo\noo\nooo\noo\no\nooo\noo\nooo\noo\nooo\no\noooooo\no\noo\noo\noo\noo\no\no\no\noooo\no\noo\noooo\no\noo\noo\noooooo\nooo\no\nooooo\no\noo\noo\nooo\noo\no\nooo\noo\noooooooooo\no\noo\no\nooo\nooo\noooo\no\nooo\noo\noo\n2 6 10 14oo\noo\noo\noo\nooo\no\nooo\nooooo\noooo\no\nooooo\nooo\noo\nooo\noo\no\no\noo\nooo\no\noooo\no\noo\no\noo\noo\nooo\no\noo\noooo\nooo\noooooo\nooooo\nooo\nooo\noo\noooo\nooo\noo\noo\noooo\no\no\nooo\nooo\no\nooo\noo\nooo\noo\noooooo\noo\noo\noo\nooo\nooo\noo\noo\noo\noo\noooo\noo\noo\noo\noo\nooo\no\noo\no\nooooo\no\noooooo\noo\noo\nooo\noo\no\noooooo\nooooo\no\no\noooo o\nooo\noo\no\nooo\nooo\noo\no\nooo\nooo\no\nooooo\no\nooo\noooooooo\no\noo\nooo\nooo\no\nooo\noo\nooo\nooo\no\nooooo\no\nooooo\noo\nooooo\noooo\noo\noooo\no\nooo\noo\noo\nooo\noo\nooo\no\noo\noo\nooo\no\noo\nooo\nooooo\nooo\nooooo\noo\nooo\noo\no\nooo\noo\nooo\noo\nooo\no\noooooo\no\noo\noo\noo\noo\no\no\no\noooo\no\noo\noooo\no\noo\noo\noooooo\nooo\no\nooooo\no\noo\noo\nooo\noo\no\nooo\noo\noooooooooo\no\noo\no\nooo\nooo\no\nooo\no\nooo\noo\noooo\noo\noo\noo\nooo\no\nooo\nooooo\noooo\no\nooooo\nooo\noo\nooo\noo\no\no\noo\nooo\no\noooo\no\nooo\noo\noo\nooo\no\noo\no oo o\nooo\noooo o o\noooo o\nooo\nooo\noo\noooo\nooo\noo\noo\noooo\no\no\nooo\noo o\no\nooo\noo\nooo\noo\noooo oo\noo\noo\noo\noo o\nooo\noo\noo\noo\noo\noooo\noo\noo\noo\noo\nooo\no\noo\no\nooooo\no\noooooo\noo\noo\nooo\noo\no\noooooo\nooo oo\no\no\noo o o o\nooo\noo\no\nooo\nooo\noo\no\nooo\noooo\nooooo\no\nooo\noooo oooo\no\noo\nooo\nooo\no\nooo\noo\nooo\nooo\no\nooooo\no\nooooo\noo\nooooo\noooo\noo\noooo\no\nooo\noooo\nooo\noo\nooo\no\noo\noo\nooo\no\noo\nooo\nooooo\noo o\noooo o\noo\nooo\noo\no\nooo\noo\noo o\noo\nooo\no\noooooo\no\noo\noo\noo\noo\no\noo\noooo\no\noo\noooo\no\noo\noo\nooooo o\noo o\no\nooo\noo\no\noo\noo\nooo\noo\no\no oo\no o\nooooo ooooo\no\noo\no\nooo\nooo\no\nooo\no\noo o\noo\noo\n15 25 35 45oo\noo\no\no\noo\nooo\no\nooo\nooooo\noooo\no\nooo\noo\nooo\noo\nooo\noo\no\no\noo\nooo\no\noooo\no\nooo\noo\noo\nooo\no\noo\noooo\nooo\noooooo\nooo oo\nooo\nooo\noo\noooo\nooo\noo\noo\noooo\no\no\nooo\nooo\no\nooo\noo\nooo\noo\noooooo\noo\noo\noo\nooo\nooo\noo\noo\noooo\noooo\noo\noo\noo\nooooo\no\noo\no\nooooo\no\noooooo\noo\noo\nooo\noo\no\noooooo\noooooo\no\nooooo\nooo\noo\no\nooo\nooo\noo\no\nooo\noooo\nooooo\no\nooo\nooooooo\no\no\noo\nooo\nooo\no\nooo\noo\nooo\nooo\no\nooooo\no\nooooo\noo\nooooo\noooo\noo\noooo\no\nooo\noooo\nooo\noo\nooo\no\noo\noo\nooo\no\noo\nooo\nooooo\nooo\nooooo\noo\nooo\noo\no\nooo\noo\nooo\noo\nooo\no\noooooo\no\noo\noo\noo\noo\no\noo\noooo\no\noo\noooo\no\noo\noo\nooooooooo\no\nooooo\no\noo\noo\nooo\noo\no\nooo\noo\noooooooooo\no\noo\no\nooo\nooo\noooo\no\nooooo\noooo\noo\noo\noo\nooo\no\nooo\nooooo\noooo\no\nooooo\nooo\noo\nooo\noo\no\no\noo\nooo\no\noooo\no\noo\no\noo\noo\nooo\no\noo\noooo\nooo\noooooo\nooooo\nooo\nooo\noo\noooo\nooo\noo\noo\noooo\no\no\nooo\nooo\no\nooo\noo\nooo\noo\noooooo\noo\noo\noo\nooo\nooo\noo\noo\noo\noo\noooo\noo\noo\noo\noo\nooo\no\noo\no\nooooo\no\noooooo\noo\noo\nooo\noo\no\noooooo\nooooo\no\no\nooooo\nooo\noo\no\nooo\nooo\noo\no\nooo\noooo\no oooo\no\nooo\nooooooo\no\no\noo\nooo\nooo\no\nooo\noo\nooo\nooo\no\nooooo\no\nooooo\noo\nooooo\noooo\noo\noooo\no\nooo\noooo\nooo\noo\nooo\no\noo\noo\nooo\no\noo\nooo\nooooo\nooo\nooooo\noo\nooo\noo\no\nooo\noo\nooo\noo\nooo\no\noooooo\no\noo\noo\noo\noo\no\noo\noooo\no\noo\noooo\no\noo\noo\noooooo\nooo\no\nooooo\no\noo\noo\nooo\noo\no\nooo\noo\noooooooooo\no\noo\no\nooo\nooo\no\nooo\no\nooo\noo\noo\n20 40 60\n20 40 60age\nFIGURE 4.12. A scatterplot matrix of the South African heart disease data.\nEach plot shows a pair of risk factors, and the cases and controls are color coded\n(red is a case). The variable family history of heart disease ( famhist )is binary\n(yes or no).", "142": "124 4. Linear Methods for Classi\ufb01cation\nTABLE 4.3. Results from stepwise logistic regression \ufb01t to South African heart\ndisease data.\nCoe\ufb03cient Std. Error Zscore\n(Intercept) \u22124.204 0 .498 \u22128.45\ntobacco 0.081 0 .026 3 .16\nldl 0.168 0 .054 3 .09\nfamhist 0.924 0 .223 4 .14\nage 0.044 0 .010 4 .52\nother correlated variables, they are no longer needed (and can even get a\nnegative sign).\nAt this stage the analyst might do some model selection; \ufb01nd a subset\nof the variables that are su\ufb03cient for explaining their joint e\ufb00ect on the\nprevalence of chd. One way to proceed by is to drop the least signi\ufb01cant co-\ne\ufb03cient, and re\ufb01t the model. This is done repeatedly until no further terms\ncan be dropped from the model. This gave the model shown in Table 4.3.\nA better but more time-consuming strategy is to re\ufb01t each of the models\nwith one variable removed, and then perform an analysis of deviance to\ndecide which variable to exclude. The residual deviance of a \ufb01tted model\nis minus twice its log-likelihood, and the deviance between two models is\nthe di\ufb00erence of their individual residual deviances (in analogy to sums-of-\nsquares). This strategy gave the same \ufb01nal model as above.\nHow does one interpret a coe\ufb03cient of 0 .081 (Std. Error = 0 .026) for\ntobacco , for example? Tobacco is measured in total lifetime usage in kilo-\ngrams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus\nan increase of 1kg in lifetime tobacco usage accounts for an increase in the\nodds of coronary heart disease of exp(0 .081) = 1 .084 or 8 .4%. Incorporat-\ning the standard error we get an approximate 95% con\ufb01dence interval of\nexp(0.081\u00b12\u00d70.026) = (1 .03,1.14).\nWe return to these data in Chapter 5, where we see that some of the\nvariables have nonlinear e\ufb00ects, and when modeled appropriately, are not\nexcluded from the model.\n4.4.3 Quadratic Approximations and Inference\nThe maximum-likelihood parameter estimates \u02c6\u03b2satisfy a self-consistency\nrelationship: they are the coe\ufb03cients of a weighted least squares \ufb01t, where\nthe responses are\nzi=xT\ni\u02c6\u03b2+(yi\u2212\u02c6pi)\n\u02c6pi(1\u2212\u02c6pi), (4.29)", "143": "4.4 Logistic Regression 125\nand the weights are wi= \u02c6pi(1\u2212\u02c6pi), both depending on \u02c6\u03b2itself. Apart from\nproviding a convenient algorithm, this connection with least squares has\nmore to o\ufb00er:\n\u2022The weighted residual sum-of-squares is the familiar Pearson chi-\nsquare statistic\nN/summationdisplay\ni=1(yi\u2212\u02c6pi)2\n\u02c6pi(1\u2212\u02c6pi), (4.30)\na quadratic approximation to the deviance.\n\u2022Asymptotic likelihood theory says that if the model is correct, then\n\u02c6\u03b2is consistent (i.e., converges to the true\u03b2).\n\u2022A central limit theorem then shows that the distribution of \u02c6\u03b2con-\nverges to N(\u03b2,(XTWX)\u22121). This and other asymptotics can be de-\nrived directly from the weighted least squares \ufb01t by mimicking normal\ntheory inference.\n\u2022Model building can be costly for logistic regression models, because\neach model \ufb01tted requires iteration. Popular shortcuts are the Rao\nscore test which tests for inclusion of a term, and the Wald test which\ncan be used to test for exclusion of a term. Neither of these require\niterative \ufb01tting, and are based on the maximum-likelihood \ufb01t of the\ncurrent model. It turns out that both of these amount to adding\nor dropping a term from the weighted least squares \ufb01t, using the\nsame weights. Such computations can be done e\ufb03ciently, without\nrecomputing the entire weighted least squares \ufb01t.\nSoftware implementations can take advantage of these connections. For\nexample, the generalized linear modeling software in R (which includes lo-\ngistic regression as part of the binomial family of models) exploits them\nfully. GLM (generalized linear model) objects can be treated as linear model\nobjects, and all the tools available for linear models can be applied auto-\nmatically.\n4.4.4 L1Regularized Logistic Regression\nTheL1penalty used in the lasso (Section 3.4.2) can be used for variable\nselection and shrinkage with any linear regression model. For logistic re-\ngression, we would maximize a penalized version of (4.20):\nmax\n\u03b20,\u03b2\uf8f1\n\uf8f2\n\uf8f3N/summationdisplay\ni=1/bracketleft\uf8ecig\nyi(\u03b20+\u03b2Txi)\u2212log(1 + e\u03b20+\u03b2Txi)/bracketright\uf8ecig\n\u2212\u03bbp/summationdisplay\nj=1|\u03b2j|\uf8fc\n\uf8fd\n\uf8fe.(4.31)\nAs with the lasso, we typically do not penalize the intercept term, and stan-\ndardize the predictors for the penalty to be meaningful. Criterion (4.31) is", "144": "126 4. Linear Methods for Classi\ufb01cation\nconcave, and a solution can be found using nonlinear programming meth-\nods (Koh et al., 2007, for example). Alternatively, using the same quadratic\napproximations that were used in the Newton algorithm in Section 4.4.1,\nwe can solve (4.31) by repeated application of a weighted lasso algorit hm.\nInterestingly, the score equations [see (4.24)] for the variables with non-zer o\ncoe\ufb03cients have the form\nxT\nj(y\u2212p) =\u03bb\u2264sign(\u03b2j), (4.32)\nwhich generalizes (3.58) in Section 3.4.4; the active variables are tied in\ntheirgeneralized correlation with the residuals.\nPath algorithms such as LAR for lasso are more di\ufb03cult, because the\ncoe\ufb03cient pro\ufb01les are piecewise smooth rather than linear. Nevertheless,\nprogress can be made using quadratic approximations.\n******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************\n0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************\n******************************* ************** * ****************************************************************************************************************************************************************************** *****************\n****************************** ********************************************************************************************************************************************************************************************** *****************\n******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** * ********************************************************************************************************************************************************************************************************************************************* *****************\nobesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coe\ufb03cients \u03b2j(\u03bb)\n||\u03b2(\u03bb)||1\nFIGURE 4.13. L1regularized logistic regression coe\ufb03cients for the South\nAfrican heart disease data, plotted as a function of the L1norm. The variables\nwere all standardized to have unit variance. The pro\ufb01les are comp uted exactly at\neach of the plotted points.\nFigure 4.13 shows the L1regularization path for the South African\nheart disease data of Section 4.4.2. This was produced using the Rpackage\nglmpath (Park and Hastie, 2007), which uses predictor\u2013corrector methods\nof convex optimization to identify the exact values of \u03bbat which the active\nset of non-zero coe\ufb03cients changes (vertical lines in the \ufb01gure). Here the\npro\ufb01les look almost linear; in other examples the curvature will be more\nvisible.\nCoordinate descent methods (Section 3.8.6) are very e\ufb03cient for comput-\ning the coe\ufb03cient pro\ufb01les on a grid of values for \u03bb. TheRpackageglmnet", "145": "4.4 Logistic Regression 127\n(Friedman et al., 2010) can \ufb01t coe\ufb03cient paths for very large logistic re-\ngression problems e\ufb03ciently (large in Norp). Their algorithms can exploit\nsparsity in the predictor matrix X, which allows for even larger problems.\nSee Section 18.4 for more details, and a discussion of L1-regularized multi-\nnomial models.\n4.4.5 Logistic Regression or LDA?\nIn Section 4.3 we \ufb01nd that the log-posterior odds between class kandK\nare linear functions of x(4.9):\nlogPr(G=k|X=x)\nPr(G=K|X=x)= log\u03c0k\n\u03c0K\u22121\n2(\u03b8k+\u03b8K)T\u03a3\u22121(\u03b8k\u2212\u03b8K)\n+xT\u03a3\u22121(\u03b8k\u2212\u03b8K)\n=\u03b1k0+\u03b1T\nkx. (4.33)\nThis linearity is a consequence of the Gaussian assumption for the class\ndensities, as well as the assumption of a common covariance matrix. The\nlinear logistic model (4.17) by construction has linear logits:\nlogPr(G=k|X=x)\nPr(G=K|X=x)=\u03b2k0+\u03b2T\nkx. (4.34)\nIt seems that the models are the same. Although they have exactly the same\nform, the di\ufb00erence lies in the way the linear coe\ufb03cients are estimated. The\nlogistic regression model is more general, in that it makes less assumptio ns.\nWe can write the joint density ofXandGas\nPr(X,G=k) = Pr( X)Pr(G=k|X), (4.35)\nwhere Pr( X) denotes the marginal density of the inputs X. For both LDA\nand logistic regression, the second term on the right has the logit-linear\nform\nPr(G=k|X=x) =e\u03b2k0+\u03b2T\nkx\n1 +/summationtextK\u22121\n\u2113=1e\u03b2\u21130+\u03b2T\n\u2113x, (4.36)\nwhere we have again arbitrarily chosen the last class as the reference.\nThe logistic regression model leaves the marginal density of Xas an arbi-\ntrary density function Pr( X), and \ufb01ts the parameters of Pr( G|X) by max-\nimizing the conditional likelihood \u2014the multinomial likelihood with proba-\nbilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think\nof this marginal density as being estimated in a fully nonparametric and\nunrestricted fashion, using the empirical distribution function which places\nmass 1 /Nat each observation.\nWith LDA we \ufb01t the parameters by maximizing the full log-likelihood,\nbased on the joint density\nPr(X,G=k) =\u03c6(X;\u03b8k,\u03a3)\u03c0k, (4.37)", "146": "128 4. Linear Methods for Classi\ufb01cation\nwhere \u03c6is the Gaussian density function. Standard normal theory leads\neasily to the estimates \u02c6 \u03b8k,\u02c6\u03a3, and \u02c6 \u03c0kgiven in Section 4.3. Since the linear\nparameters of the logistic form (4.33) are functions of the Gaussian para m-\neters, we get their maximum-likelihood estimates by plugging in the corre-\nsponding estimates. However, unlike in the conditional case, the marginal\ndensity Pr( X) does play a role here. It is a mixture density\nPr(X) =K/summationdisplay\nk=1\u03c0k\u03c6(X;\u03b8k,\u03a3), (4.38)\nwhich also involves the parameters.\nWhat role can this additional component/restriction play? By relying\non the additional model assumptions, we have more information about the\nparameters, and hence can estimate them more e\ufb03ciently (lower variance).\nIf in fact the true fk(x) are Gaussian, then in the worst case ignoring this\nmarginal part of the likelihood constitutes a loss of e\ufb03ciency of about 30%\nasymptotically in the error rate (Efron, 1975). Paraphrasing: with 3 0%\nmore data, the conditional likelihood will do as well.\nFor example, observations far from the decision boundary (which are\ndown-weighted by logistic regression) play a role in estimating the common\ncovariance matrix. This is not all good news, because it also means that\nLDA is not robust to gross outliers.\nFrom the mixture formulation, it is clear that even observations without\nclass labels have information about the parameters. Often it is expensive\nto generate class labels, but unclassi\ufb01ed observations come cheaply. By\nrelying on strong model assumptions, such as here, we can use both types\nof information.\nThe marginal likelihood can be thought of as a regularizer, requiring\nin some sense that class densities be visible from this marginal view. For\nexample, if the data in a two-class logistic regression model can be per-\nfectly separated by a hyperplane, the maximum likelihood estimates of the\nparameters are unde\ufb01ned (i.e., in\ufb01nite; see Exercise 4.5). The LDA coe\ufb03-\ncients for the same data will be well de\ufb01ned, since the marginal likelihood\nwill not permit these degeneracies.\nIn practice these assumptions are never correct, and often some of the\ncomponents of Xare qualitative variables. It is generally felt that logistic\nregression is a safer, more robust bet than the LDA model, relying on fewer\nassumptions. It is our experience that the models give very similar results,\neven when LDA is used inappropriately, such as with qualitative predictors.", "147": "4.5 Separating Hyperplanes 129\nFIGURE 4.14. A toy example with two classes separable by a hyperplane. The\norange line is the least squares solution, which misclassi\ufb01es one of the training\npoints. Also shown are two blue separating hyperplanes found by t heperceptron\nlearning algorithm with di\ufb00erent random starts.\n4.5 Separating Hyperplanes\nWe have seen that linear discriminant analysis and logistic regression bot h\nestimate linear decision boundaries in similar but slightly di\ufb00erent ways.\nFor the rest of this chapter we describe separating hyperplane classi\ufb01ers.\nThese procedures construct linear decision boundaries that explicitly try\nto separate the data into di\ufb00erent classes as well as possible. They provide\nthe basis for support vector classi\ufb01ers, discussed in Chapter 12. The math-\nematical level of this section is somewhat higher than that of the previous\nsections.\nFigure 4.14 shows 20 data points in two classes in IR2. These data can be\nseparated by a linear boundary. Included in the \ufb01gure (blue lines) are two\nof the in\ufb01nitely many possible separating hyperplanes . The orange line is\nthe least squares solution to the problem, obtained by regressing the \u22121/1\nresponse YonX(with intercept); the line is given by\n{x:\u02c6\u03b20+\u02c6\u03b21x1+\u02c6\u03b22x2= 0}. (4.39)\nThis least squares solution does not do a perfect job in separating the\npoints, and makes one error. This is the same boundary found by LDA,\nin light of its equivalence with linear regression in the two-class case (Sec-\ntion 4.3 and Exercise 4.2).\nClassi\ufb01ers such as (4.39), that compute a linear combination of the input\nfeatures and return the sign, were called perceptrons in the engineering liter-", "148": "130 4. Linear Methods for Classi\ufb01cation\nx0x\n\u03b2\u2217\u03b20+\u03b2Tx= 0\nFIGURE 4.15. The linear algebra of a hyperplane (a\ufb03ne set).\nature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\nfor the neural network models of the 1980s and 1990s.\nBefore we continue, let us digress slightly and review some vector algebra.\nFigure 4.15 depicts a hyperplane or a\ufb03ne set Lde\ufb01ned by the equation\nf(x) =\u03b20+\u03b2Tx= 0; since we are in IR2this is a line.\nHere we list some properties:\n1. For any two points x1andx2lying in L,\u03b2T(x1\u2212x2) = 0, and hence\n\u03b2\u2217=\u03b2/||\u03b2||is the vector normal to the surface of L.\n2. For any point x0inL,\u03b2Tx0=\u2212\u03b20.\n3. The signed distance of any point xtoLis given by\n\u03b2\u2217T(x\u2212x0) =1\n\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al(\u03b2Tx+\u03b20)\n=1\n||f\u2032(x)||f(x). (4.40)\nHence f(x) is proportional to the signed distance from xto the hyperplane\nde\ufb01ned by f(x) = 0.\n4.5.1 Rosenblatt\u2019s Perceptron Learning Algorithm\nTheperceptron learning algorithm tries to \ufb01nd a separating hyperplane by\nminimizing the distance of misclassi\ufb01ed points to the decision boundary. If", "149": "4.5 Separating Hyperplanes 131\na response yi= 1 is misclassi\ufb01ed, then xT\ni\u03b2+\u03b20<0, and the opposite for\na misclassi\ufb01ed response with yi=\u22121. The goal is to minimize\nD(\u03b2,\u03b20) =\u2212/summationdisplay\ni\u2208Myi(xT\ni\u03b2+\u03b20), (4.41)\nwhere Mindexes the set of misclassi\ufb01ed points. The quantity is non-\nnegative and proportional to the distance of the misclassi\ufb01ed points to\nthe decision boundary de\ufb01ned by \u03b2Tx+\u03b20= 0. The gradient (assuming\nMis \ufb01xed) is given by\n\u2202D(\u03b2,\u03b20)\n\u2202\u03b2=\u2212/summationdisplay\ni\u2208Myixi, (4.42)\n\u2202D(\u03b2,\u03b20)\n\u2202\u03b20=\u2212/summationdisplay\ni\u2208Myi. (4.43)\nThe algorithm in fact uses stochastic gradient descent to minimize this\npiecewise linear criterion. This means that rather than computing the sum\nof the gradient contributions of each observation followed by a step in the\nnegative gradient direction, a step is taken after each observation is visit ed.\nHence the misclassi\ufb01ed observations are visited in some sequence, and the\nparameters \u03b2are updated via\n/parenleftbigg\n\u03b2\n\u03b20/parenrightbigg\n\u2190/parenleftbigg\n\u03b2\n\u03b20/parenrightbigg\n+\u03c1/parenleftbigg\nyixi\nyi/parenrightbigg\n. (4.44)\nHere\u03c1is the learning rate, which in this case can be taken to be 1 without\nloss in generality. If the classes are linearly separable, it can be shown that\nthe algorithm converges to a separating hyperplane in a \ufb01nite number of\nsteps (Exercise 4.6). Figure 4.14 shows two solutions to a toy problem, eac h\nstarted at a di\ufb00erent random guess.\nThere are a number of problems with this algorithm, summarized in\nRipley (1996):\n\u2022When the data are separable, there are many solutions, and which\none is found depends on the starting values.\n\u2022The \u201c\ufb01nite\u201d number of steps can be very large. The smaller the gap,\nthe longer the time to \ufb01nd it.\n\u2022When the data are not separable, the algorithm will not converge,\nand cycles develop. The cycles can be long and therefore hard to\ndetect.\nThe second problem can often be eliminated by seeking a hyperplane not\nin the original space, but in a much enlarged space obtained by creating", "150": "132 4. Linear Methods for Classi\ufb01cation\nmany basis-function transformations of the original variables. This is a nal-\nogous to driving the residuals in a polynomial regression problem down\nto zero by making the degree su\ufb03ciently large. Perfect separation cannot\nalways be achieved: for example, if observations from two di\ufb00erent classes\nshare the same input. It may not be desirable either, since the resulting\nmodel is likely to be over\ufb01t and will not generalize well. We return to this\npoint at the end of the next section.\nA rather elegant solution to the \ufb01rst problem is to add additional con-\nstraints to the separating hyperplane.\n4.5.2 Optimal Separating Hyperplanes\nTheoptimal separating hyperplane separates the two classes and maximizes\nthe distance to the closest point from either class (Vapnik, 1996). Not only\ndoes this provide a unique solution to the separating hyperplane problem,\nbut by maximizing the margin between the two classes on the training data,\nthis leads to better classi\ufb01cation performance on test data.\nWe need to generalize criterion (4.41). Consider the optimization problem\nmax\n\u03b2,\u03b20,||\u03b2||=1M\nsubject to yi(xT\ni\u03b2+\u03b20)\u2265M, i= 1,... ,N.(4.45)\nThe set of conditions ensure that all the points are at least a signed\ndistance Mfrom the decision boundary de\ufb01ned by \u03b2and\u03b20, and we seek\nthe largest such Mand associated parameters. We can get rid of the ||\u03b2||=\n1 constraint by replacing the conditions with\n1\n||\u03b2||yi(xT\ni\u03b2+\u03b20)\u2265M, (4.46)\n(which rede\ufb01nes \u03b20) or equivalently\nyi(xT\ni\u03b2+\u03b20)\u2265M||\u03b2||. (4.47)\nSince for any \u03b2and\u03b20satisfying these inequalities, any positively scaled\nmultiple satis\ufb01es them too, we can arbitrarily set ||\u03b2||= 1/M. Thus (4.45)\nis equivalent to\nmin\n\u03b2,\u03b201\n2||\u03b2||2\nsubject to yi(xT\ni\u03b2+\u03b20)\u22651, i= 1,... ,N.(4.48)\nIn light of (4.40), the constraints de\ufb01ne an empty slab or margin around the\nlinear decision boundary of thickness 1 /||\u03b2||. Hence we choose \u03b2and\u03b20to\nmaximize its thickness. This is a convex optimization problem (quadratic", "151": "4.5 Separating Hyperplanes 133\ncriterion with linear inequality constraints). The Lagrange (primal) func-\ntion, to be minimized w.r.t. \u03b2and\u03b20, is\nLP=1\n2||\u03b2||2\u2212N/summationdisplay\ni=1\u03b1i[yi(xT\ni\u03b2+\u03b20)\u22121]. (4.49)\nSetting the derivatives to zero, we obtain:\n\u03b2=N/summationdisplay\ni=1\u03b1iyixi, (4.50)\n0 =N/summationdisplay\ni=1\u03b1iyi, (4.51)\nand substituting these in (4.49) we obtain the so-called Wolfe dual\nLD=N/summationdisplay\ni=1\u03b1i\u22121\n2N/summationdisplay\ni=1N/summationdisplay\nk=1\u03b1i\u03b1kyiykxT\nixk\nsubject to \u03b1i\u22650. (4.52)\nThe solution is obtained by maximizing LDin the positive orthant, a sim-\npler convex optimization problem, for which standard software can be used.\nIn addition the solution must satisfy the Karush\u2013Kuhn\u2013Tucker conditions,\nwhich include (4.50), (4.51), (4.52) and\n\u03b1i[yi(xT\ni\u03b2+\u03b20)\u22121] = 0 \u2200i. (4.53)\nFrom these we can see that\n\u2022if\u03b1i>0, then yi(xT\ni\u03b2+\u03b20) = 1, or in other words, xiis on the\nboundary of the slab;\n\u2022ifyi(xT\ni\u03b2+\u03b20)>1,xiis not on the boundary of the slab, and \u03b1i= 0.\nFrom (4.50) we see that the solution vector \u03b2is de\ufb01ned in terms of a linear\ncombination of the support points xi\u2014those points de\ufb01ned to be on the\nboundary of the slab via \u03b1i>0. Figure 4.16 shows the optimal separating\nhyperplane for our toy example; there are three support points. Likewise,\n\u03b20is obtained by solving (4.53) for any of the support points.\nThe optimal separating hyperplane produces a function \u02c6f(x) =xT\u02c6\u03b2+\u02c6\u03b20\nfor classifying new observations:\n\u02c6G(x) = sign \u02c6f(x). (4.54)\nAlthough none of the training observations fall in the margin (by con-\nstruction), this will not necessarily be the case for test observations. The", "152": "134 4. Linear Methods for Classi\ufb01cation\nFIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates\nthe maximum margin separating the two classes. There are three support points\nindicated, which lie on the boundary of the margin, and the optima l separating\nhyperplane (blue line) bisects the slab. Included in the \ufb01gure is the boundary found\nusing logistic regression (red line), which is very close to th e optimal separating\nhyperplane (see Section 12.3.3).\nintuition is that a large margin on the training data will lead to good\nseparation on the test data.\nThe description of the solution in terms of support points seems to sug-\ngest that the optimal hyperplane focuses more on the points that count,\nand is more robust to model misspeci\ufb01cation. The LDA solution, on the\nother hand, depends on all of the data, even points far away from the de-\ncision boundary. Note, however, that the identi\ufb01cation of these support\npoints required the use of all the data. Of course, if the classes are really\nGaussian, then LDA is optimal, and separating hyperplanes will pay a price\nfor focusing on the (noisier) data at the boundaries of the classes.\nIncluded in Figure 4.16 is the logistic regression solution to this prob-\nlem, \ufb01t by maximum likelihood. Both solutions are similar in this case.\nWhen a separating hyperplane exists, logistic regression will always \ufb01nd\nit, since the log-likelihood can be driven to 0 in this case (Exercise 4.5).\nThe logistic regression solution shares some other qualitative features with\nthe separating hyperplane solution. The coe\ufb03cient vector is de\ufb01ned by a\nweighted least squares \ufb01t of a zero-mean linearized response on the input\nfeatures, and the weights are larger for points near the decision boundary\nthan for those further away.\nWhen the data are not separable, there will be no feasible solution to\nthis problem, and an alternative formulation is needed. Again one can en-\nlarge the space using basis transformations, but this can lead to arti\ufb01cial", "153": "Exercises 135\nseparation through over-\ufb01tting. In Chapter 12 we discuss a more attractive\nalternative known as the support vector machine , which allows for overlap,\nbut minimizes a measure of the extent of this overlap.\nBibliographic Notes\nGood general texts on classi\ufb01cation include Duda et al. (2000), Hand\n(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1979) have\na concise discussion of linear discriminant analysis. Michie et al. (1994)\ncompare a large number of popular classi\ufb01ers on benchmark datasets. Lin-\near separating hyperplanes are discussed in Vapnik (1996). Our account of\nthe perceptron learning algorithm follows Ripley (1996).\nExercises\nEx. 4.1 Show how to solve the generalized eigenvalue problem max aTBa\nsubject to aTWa= 1 by transforming to a standard eigenvalue problem.\nEx. 4.2 Suppose we have features x\u2208IRp, a two-class response, with class\nsizesN1,N2, and the target coded as \u2212N/N 1,N/N 2.\n(a) Show that the LDA rule classi\ufb01es to class 2 if\nxT\u02c6\u03a3\u22121(\u02c6\u03b82\u2212\u02c6\u03b81)>1\n2\u02c6\u03b8T\n2\u02c6\u03a3\u22121\u02c6\u03b82\u22121\n2\u02c6\u03b8T\n1\u02c6\u03a3\u22121\u02c6\u03b81+ log/parenleft\uf8ecigN1\nN/parenright\uf8ecig\n\u2212log/parenleft\uf8ecigN2\nN/parenright\uf8ecig\n,\nand class 1 otherwise.\n(b) Consider minimization of the least squares criterion\nN/summationdisplay\ni=1(yi\u2212\u03b20\u2212\u03b2Txi)2. (4.55)\nShow that the solution \u02c6\u03b2satis\ufb01es\n/bracketleftbigg\n(N\u22122)\u02c6\u03a3+N1N2\nN\u02c6\u03a3B/bracketrightbigg\n\u03b2=N(\u02c6\u03b82\u2212\u02c6\u03b81) (4.56)\n(after simpli\ufb01cation),where \u02c6\u03a3B= (\u02c6\u03b82\u2212\u02c6\u03b81)(\u02c6\u03b82\u2212\u02c6\u03b81)T.\n(c) Hence show that \u02c6\u03a3B\u03b2is in the direction (\u02c6 \u03b82\u2212\u02c6\u03b81) and thus\n\u02c6\u03b2\u221d\u02c6\u03a3\u22121(\u02c6\u03b82\u2212\u02c6\u03b81). (4.57)\nTherefore the least squares regression coe\ufb03cient is identical to the\nLDA coe\ufb03cient, up to a scalar multiple.", "154": "136 4. Linear Methods for Classi\ufb01cation\n(d) Show that this result holds for any (distinct) coding of the two classes.\n(e) Find the solution \u02c6\u03b20, and hence the predicted values \u02c6f=\u02c6\u03b20+\u02c6\u03b2Tx.\nConsider the following rule: classify to class 2 if \u02c6 yi>0 and class\n1 otherwise. Show this is not the same as the LDA rule unless the\nclasses have equal numbers of observations.\n(Fisher, 1936; Ripley, 1996)\nEx. 4.3 Suppose we transform the original predictors Xto\u02c6Yvia linear\nregression. In detail, let \u02c6Y=X(XTX)\u22121XTY=X\u02c6B, where Yis the\nindicator response matrix. Similarly for any input x\u2208IRp, we get a trans-\nformed vector \u02c6 y=\u02c6BTx\u2208IRK. Show that LDA using \u02c6Yis identical to\nLDA in the original space.\nEx. 4.4 Consider the multilogit model with Kclasses (4.17). Let \u03b2be the\n(p+ 1)(K\u22121)-vector consisting of all the coe\ufb03cients. De\ufb01ne a suitably\nenlarged version of the input vector xto accommodate this vectorized co-\ne\ufb03cient matrix. Derive the Newton-Raphson algorithm for maximizing the\nmultinomial log-likelihood, and describe how you would implement this\nalgorithm.\nEx. 4.5 Consider a two-class logistic regression problem with x\u2208IR. Char-\nacterize the maximum-likelihood estimates of the slope and intercept pa-\nrameter if the sample xifor the two classes are separated by a point x0\u2208IR.\nGeneralize this result to (a) x\u2208IRp(see Figure 4.16), and (b) more than\ntwo classes.\nEx. 4.6 Suppose we have Npoints xiin IRpin general position, with class\nlabels yi\u2208 {\u22121,1}. Prove that the perceptron learning algorithm converges\nto a separating hyperplane in a \ufb01nite number of steps:\n(a) Denote a hyperplane by f(x) =\u03b2T\n1x+\u03b20= 0, or in more compact\nnotation \u03b2Tx\u2217= 0, where x\u2217= (x,1) and \u03b2= (\u03b21,\u03b20). Let zi=\nx\u2217\ni/||x\u2217\ni||. Show that separability implies the existence of a \u03b2sepsuch\nthatyi\u03b2T\nsepzi\u22651\u2200i\n(b) Given a current \u03b2old, the perceptron algorithm identi\ufb01es a point zithat\nis misclassi\ufb01ed, and produces the update \u03b2new\u2190\u03b2old+yizi. Show\nthat||\u03b2new\u2212\u03b2sep||2\u2264 ||\u03b2old\u2212\u03b2sep||2\u22121, and hence that the algorithm\nconverges to a separating hyperplane in no more than ||\u03b2start\u2212\u03b2sep||2\nsteps (Ripley, 1996).\nEx. 4.7 Consider the criterion\nD\u2217(\u03b2,\u03b20) =\u2212N/summationdisplay\ni=1yi(xT\ni\u03b2+\u03b20), (4.58)", "155": "Exercises 137\na generalization of (4.41) where we sum over all the observations. Consider\nminimizing D\u2217subject to ||\u03b2||= 1. Describe this criterion in words. Does\nit solve the optimal separating hyperplane problem?\nEx. 4.8 Consider the multivariate Gaussian model X|G=k\u223cN(\u03b8k,\u03a3),\nwith the additional restriction that rank {\u03b8k}K\n1=L < max(K\u22121,p).\nDerive the constrained MLEs for the \u03b8kand\u03a3. Show that the Bayes clas-\nsi\ufb01cation rule is equivalent to classifying in the reduced subspace computed\nby LDA (Hastie and Tibshirani, 1996b).\nEx. 4.9 Write a computer program to perform a quadratic discriminant\nanalysis by \ufb01tting a separate Gaussian model per class. Try it out on the\nvowel data, and compute the misclassi\ufb01cation error for the test data. The\ndata can be found in the book website www-stat.stanford.edu/ElemStatLearn .", "156": "138 4. Linear Methods for Classi\ufb01cation", "157": "This is page 139\nPrinter: Opaque this\n5\nBasis Expansions and Regularization\n5.1 Introduction\nWe have already made use of models linear in the input features, both for\nregression and classi\ufb01cation. Linear regression, linear discriminant analysi s,\nlogistic regression and separating hyperplanes all rely on a linear model.\nIt is extremely unlikely that the true function f(X) is actually linear in\nX. In regression problems, f(X) = E( Y|X) will typically be nonlinear and\nnonadditive in X, and representing f(X) by a linear model is usually a con-\nvenient, and sometimes a necessary, approximation. Convenient because a\nlinear model is easy to interpret, and is the \ufb01rst-order Taylor approxima-\ntion to f(X). Sometimes necessary, because with Nsmall and/or plarge,\na linear model might be all we are able to \ufb01t to the data without over\ufb01t-\nting. Likewise in classi\ufb01cation, a linear, Bayes-optimal decision boundary\nimplies that some monotone transformation of Pr( Y= 1|X) is linear in X.\nThis is inevitably an approximation.\nIn this chapter and the next we discuss popular methods for moving\nbeyond linearity. The core idea in this chapter is to augment/replace the\nvector of inputs Xwith additional variables, which are transformations of\nX, and then use linear models in this new space of derived input features.\nDenote by hm(X) : IRp\u221dma\u221asto\u2192IR the mth transformation of X,m=\n1,... ,M . We then model\nf(X) =M/summationdisplay\nm=1\u03b2mhm(X), (5.1)", "158": "140 5. Basis Expansions and Regularization\nalinear basis expansion inX. The beauty of this approach is that once the\nbasis functions hmhave been determined, the models are linear in these\nnew variables, and the \ufb01tting proceeds as before.\nSome simple and widely used examples of the hmare the following:\n\u2022hm(X) =Xm, m= 1,... ,p recovers the original linear model.\n\u2022hm(X) =X2\njorhm(X) =XjXkallows us to augment the inputs with\npolynomial terms to achieve higher-order Taylor expansions. Note,\nhowever, that the number of variables grows exponentially in the de-\ngree of the polynomial. A full quadratic model in pvariables requires\nO(p2) square and cross-product terms, or more generally O(pd) for a\ndegree- dpolynomial.\n\u2022hm(X) = log( Xj),/radicalbig\nXj,...permits other nonlinear transformations\nof single inputs. More generally one can use similar functions involv-\ning several inputs, such as hm(X) =||X||.\n\u2022hm(X) =I(Lm\u2264Xk< Um), an indicator for a region of Xk. By\nbreaking the range of Xkup into Mksuch nonoverlapping regions\nresults in a model with a piecewise constant contribution for Xk.\nSometimes the problem at hand will call for particular basis functions hm,\nsuch as logarithms or power functions. More often, however, we use the basis\nexpansions as a device to achieve more \ufb02exible representations for f(X).\nPolynomials are an example of the latter, although they are limited by\ntheir global nature\u2014tweaking the coe\ufb03cients to achieve a functional form\nin one region can cause the function to \ufb02ap about madly in remote regions.\nIn this chapter we consider more useful families of piecewise-polynomials\nandsplines that allow for local polynomial representations. We also discuss\nthewavelet bases, especially useful for modeling signals and images. These\nmethods produce a dictionary Dconsisting of typically a very large number\n|D|of basis functions, far more than we can a\ufb00ord to \ufb01t to our data. Along\nwith the dictionary we require a method for controlling the complexity\nof our model, using basis functions from the dictionary. There are three\ncommon approaches:\n\u2022Restriction methods, where we decide before-hand to limit the class\nof functions. Additivity is an example, where we assume that our\nmodel has the form\nf(X) =p/summationdisplay\nj=1fj(Xj)\n=p/summationdisplay\nj=1Mj/summationdisplay\nm=1\u03b2jmhjm(Xj). (5.2)", "159": "5.2 Piecewise Polynomials and Splines 141\nThe size of the model is limited by the number of basis functions Mj\nused for each component function fj.\n\u2022Selection methods, which adaptively scan the dictionary and include\nonly those basis functions hmthat contribute signi\ufb01cantly to the \ufb01t of\nthe model. Here the variable selection techniques discussed in Chap-\nter 3 are useful. The stagewise greedy approaches such as CART,\nMARS and boosting fall into this category as well.\n\u2022Regularization methods where we use the entire dictionary but re-\nstrict the coe\ufb03cients. Ridge regression is a simple example of a regu-\nlarization approach, while the lasso is both a regularization and selec-\ntion method. Here we discuss these and more sophisticated methods\nfor regularization.\n5.2 Piecewise Polynomials and Splines\nWe assume until Section 5.7 that Xis one-dimensional. A piecewise poly-\nnomial function f(X) is obtained by dividing the domain of Xinto contigu-\nous intervals, and representing fby a separate polynomial in each interval.\nFigure 5.1 shows two simple piecewise polynomials. The \ufb01rst is piecewise\nconstant, with three basis functions:\nh1(X) =I(X < \u03be 1), h 2(X) =I(\u03be1\u2264X < \u03be 2), h 3(X) =I(\u03be2\u2264X).\nSince these are positive over disjoint regions, the least squares estimate o f\nthe model f(X) =/summationtext3\nm=1\u03b2mhm(X) amounts to \u02c6\u03b2m=\u00afYm, the mean of Y\nin the mth region.\nThe top right panel shows a piecewise linear \ufb01t. Three additional basis\nfunctions are needed: hm+3=hm(X)X, m = 1,... ,3. Except in special\ncases, we would typically prefer the third panel, which is also piecewise\nlinear, but restricted to be continuous at the two knots. These continu-\nity restrictions lead to linear constraints on the parameters; for example,\nf(\u03be\u2212\n1) =f(\u03be+\n1) implies that \u03b21+\u03be1\u03b24=\u03b22+\u03be1\u03b25. In this case, since there\nare two restrictions, we expect to get back two parameters, leaving four free\nparameters.\nA more direct way to proceed in this case is to use a basis that incorpo-\nrates the constraints:\nh1(X) = 1, h 2(X) =X, h 3(X) = (X\u2212\u03be1)+, h 4(X) = (X\u2212\u03be2)+,\nwhere t+denotes the positive part. The function h3is shown in the lower\nright panel of Figure 5.1. We often prefer smoother functions, and these\ncan be achieved by increasing the order of the local polynomial. Figure 5.2\nshows a series of piecewise-cubic polynomials \ufb01t to the same data, with", "160": "142 5. Basis Expansions and Regularization\nOO\nOOO\nO\nOO\nO\nOO\nO\nOO\nOO\nOO\nO\nOOO\nO\nO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOOO\nO\nO\nOO\nO\nOOPiecewise Constant\nOO\nOOO\nO\nOO\nO\nOO\nO\nOO\nOO\nOO\nO\nOOO\nO\nO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOOO\nO\nO\nOO\nO\nOOPiecewise Linear\nOO\nOOO\nO\nOO\nO\nOO\nO\nOO\nOO\nOO\nO\nOOO\nO\nO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOOO\nO\nO\nOO\nO\nOOContinuous Piecewise Linear Piecewise-linear Basis Function\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022 \u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u03be1 \u03be1\u03be1 \u03be1\n\u03be2 \u03be2\u03be2 \u03be2\n(X\u2212\u03be1)+\nFIGURE 5.1. The top left panel shows a piecewise constant function \ufb01t to some\narti\ufb01cial data. The broken vertical lines indicate the positio ns of the two knots\n\u03be1and\u03be2. The blue curve represents the true function, from which the dat a were\ngenerated with Gaussian noise. The remaining two panels show piec ewise lin-\near functions \ufb01t to the same data\u2014the top right unrestricted, and t he lower left\nrestricted to be continuous at the knots. The lower right panel sh ows a piecewise\u2013\nlinear basis function, h3(X) = ( X\u2212\u03be1)+, continuous at \u03be1. The black points\nindicate the sample evaluations h3(xi), i= 1, . . . , N .", "161": "5.2 Piecewise Polynomials and Splines 143\nOO\nOOO\nOOO\nO\nOOO\nOO\nOO\nOO\nO\nOOO\nO\nO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOOOO\nO\nOO\nOOODiscontinuous\nOO\nOOO\nOOO\nO\nOOO\nOO\nOO\nOO\nO\nOOO\nO\nO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOOOO\nO\nOO\nOOOContinuous\nOO\nOOO\nOOO\nO\nOOO\nOO\nOO\nOO\nO\nOOO\nO\nO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOOOO\nO\nOO\nOOOContinuous First Derivative\nOO\nOOO\nOOO\nO\nOOO\nOO\nOO\nOO\nO\nOOO\nO\nO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOOOO\nO\nOO\nOOOContinuous Second DerivativePiecewise Cubic Polynomials\n\u03be1 \u03be1\u03be1 \u03be1\n\u03be2 \u03be2\u03be2 \u03be2\nFIGURE 5.2. A series of piecewise-cubic polynomials, with increasing orde rs of\ncontinuity.\nincreasing orders of continuity at the knots. The function in the lower\nright panel is continuous, and has continuous \ufb01rst and second derivatives\nat the knots. It is known as a cubic spline . Enforcing one more order of\ncontinuity would lead to a global cubic polynomial. It is not hard to show\n(Exercise 5.1) that the following basis represents a cubic spline with knots\nat\u03be1and\u03be2:\nh1(X) = 1, h 3(X) =X2, h5(X) = (X\u2212\u03be1)3\n+,\nh2(X) =X, h 4(X) =X3, h6(X) = (X\u2212\u03be2)3\n+.(5.3)\nThere are six basis functions corresponding to a six-dimensional linear space\nof functions. A quick check con\ufb01rms the parameter count: (3 regions) \u00d7(4\nparameters per region) \u2212(2 knots) \u00d7(3 constraints per knot)= 6.", "162": "144 5. Basis Expansions and Regularization\nMore generally, an order- Mspline with knots \u03bej, j= 1,... ,K is a\npiecewise-polynomial of order M, and has continuous derivatives up to\norder M\u22122. A cubic spline has M= 4. In fact the piecewise-constant\nfunction in Figure 5.1 is an order-1 spline, while the continuous piece-\nwise linear function is an order-2 spline. Likewise the general form for the\ntruncated-power basis set would be\nhj(X) = Xj\u22121, j= 1,... ,M,\nhM+\u2113(X) = ( X\u2212\u03be\u2113)M\u22121\n+, \u2113= 1,... ,K.\nIt is claimed that cubic splines are the lowest-order spline for which the\nknot-discontinuity is not visible to the human eye. There is seldom any\ngood reason to go beyond cubic-splines, unless one is interested in smooth\nderivatives. In practice the most widely used orders are M= 1,2 and 4.\nThese \ufb01xed-knot splines are also known as regression splines . One needs\nto select the order of the spline, the number of knots and their placement.\nOne simple approach is to parameterize a family of splines by the number\nof basis functions or degrees of freedom, and have the observations xide-\ntermine the positions of the knots. For example, the expression bs(x,df=7)\ninRgenerates a basis matrix of cubic-spline functions evaluated at the N\nobservations in x, with the 7 \u22123 = 41interior knots at the appropriate per-\ncentiles of x(20,40,60 and 80th.) One can be more explicit, however; bs(x,\ndegree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\nwith three interior knots, and returns an N\u00d74 matrix.\nSince the space of spline functions of a particular order and knot sequence\nis a vector space, there are many equivalent bases for representing them\n(just as there are for ordinary polynomials.) While the truncated power\nbasis is conceptually simple, it is not too attractive numerically: powers of\nlarge numbers can lead to severe rounding problems. The B-spline basis,\ndescribed in the Appendix to this chapter, allows for e\ufb03cient computations\neven when the number of knots Kis large.\n5.2.1 Natural Cubic Splines\nWe know that the behavior of polynomials \ufb01t to data tends to be erratic\nnear the boundaries, and extrapolation can be dangerous. These problems\nare exacerbated with splines. The polynomials \ufb01t beyond the boundary\nknots behave even more wildly than the corresponding global polynomials\nin that region. This can be conveniently summarized in terms of the point-\nwise variance of spline functions \ufb01t by least squares (see the example in the\nnext section for details on these variance calculations). Figure 5.3 compares\n1A cubic spline with four knots is eight-dimensional. The bs() function omits by\ndefault the constant term in the basis, since terms like this are typically included with\nother terms in the model.", "163": "5.2 Piecewise Polynomials and Splines 145\nXPointwise Variances\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022Global Linear\nGlobal Cubic Polynomial\nCubic Spline - 2 knots\nNatural Cubic Spline - 6 knots\nFIGURE 5.3. Pointwise variance curves for four di\ufb00erent models, with Xcon-\nsisting of 50points drawn at random from U[0,1], and an assumed error model\nwith constant variance. The linear and cubic polynomial \ufb01ts have two and four\ndegrees of freedom, respectively, while the cubic spline and na tural cubic spline\neach have six degrees of freedom. The cubic spline has two knots at0.33and0.66,\nwhile the natural spline has boundary knots at 0.1and0.9, and four interior knots\nuniformly spaced between them.\nthe pointwise variances for a variety of di\ufb00erent models. The explosion of\nthe variance near the boundaries is clear, and inevitably is worst for cubic\nsplines.\nAnatural cubic spline adds additional constraints, namely that the func-\ntion is linear beyond the boundary knots. This frees up four degrees of\nfreedom (two constraints each in both boundary regions), which can be\nspent more pro\ufb01tably by sprinkling more knots in the interior region. This\ntradeo\ufb00 is illustrated in terms of variance in Figure 5.3. There will be a\nprice paid in bias near the boundaries, but assuming the function is lin-\near near the boundaries (where we have less information anyway) is often\nconsidered reasonable.\nA natural cubic spline with Kknots is represented by Kbasis functions.\nOne can start from a basis for cubic splines, and derive the reduced ba-\nsis by imposing the boundary constraints. For example, starting from the\ntruncated power series basis described in Section 5.2, we arrive at (Exer-\ncise 5.4):\nN1(X) = 1, N 2(X) =X, N k+2(X) =dk(X)\u2212dK\u22121(X),(5.4)", "164": "146 5. Basis Expansions and Regularization\nwhere\ndk(X) =(X\u2212\u03bek)3\n+\u2212(X\u2212\u03beK)3\n+\n\u03beK\u2212\u03bek. (5.5)\nEach of these basis functions can be seen to have zero second and third\nderivative for X\u2265\u03beK.\n5.2.2 Example: South African Heart Disease (Continued)\nIn Section 4.4.2 we \ufb01t linear logistic regression models to the South African\nheart disease data. Here we explore nonlinearities in the functions using\nnatural splines. The functional form of the model is\nlogit[Pr(chd|X)] =\u03b80+h1(X1)T\u03b81+h2(X2)T\u03b82+\u2264\u2264\u2264+hp(Xp)T\u03b8p,(5.6)\nwhere each of the \u03b8jare vectors of coe\ufb03cients multiplying their associated\nvector of natural spline basis functions hj.\nWe use four natural spline bases for each term in the model. For example,\nwithX1representing sbp,h1(X1) is a basis consisting of four basis func-\ntions. This actually implies three rather than two interior knots (chosen at\nuniform quantiles of sbp), plus two boundary knots at the extremes of the\ndata, since we exclude the constant term from each of the hj.\nSincefamhist is a two-level factor, it is coded by a simple binary or\ndummy variable, and is associated with a single coe\ufb03cient in the \ufb01t of the\nmodel.\nMore compactly we can combine all pvectors of basis functions (and\nthe constant term) into one big vector h(X), and then the model is simply\nh(X)T\u03b8, with total number of parameters df = 1 +/summationtextp\nj=1dfj, the sum of\nthe parameters in each component term. Each basis function is evaluated\nat each of the Nsamples, resulting in a N\u00d7df basis matrix H. At this\npoint the model is like any other linear logistic model, and the algorithms\ndescribed in Section 4.4.1 apply.\nWe carried out a backward stepwise deletion process, dropping terms\nfrom this model while preserving the group structure of each term, rather\nthan dropping one coe\ufb03cient at a time. The AIC statistic (Section 7.5) was\nused to drop terms, and all the terms remaining in the \ufb01nal model would\ncause AIC to increase if deleted from the model (see Table 5.1). Figure 5.4\nshows a plot of the \ufb01nal model selected by the stepwise regression. The\nfunctions displayed are \u02c6fj(Xj) =hj(Xj)T\u02c6\u03b8jfor each variable Xj. The\ncovariance matrix Cov( \u02c6\u03b8) =\u03a3is estimated by \u02c6\u03a3= (HTWH)\u22121, where W\nis the diagonal weight matrix from the logistic regression. Hence vj(Xj) =\nVar[\u02c6fj(Xj)] =hj(Xj)T\u02c6\u03a3jjhj(Xj) is the pointwise variance function of \u02c6fj,\nwhere Cov( \u02c6\u03b8j) =\u02c6\u03a3jjis the appropriate sub-matrix of \u02c6\u03a3. The shaded region\nin each panel is de\ufb01ned by \u02c6fj(Xj)\u00b12/radicalbig\nvj(Xj).\nThe AIC statistic is slightly more generous than the likelihood-ratio tes t\n(deviance test). Both sbpandobesity are included in this model, while", "165": "5.2 Piecewise Polynomials and Splines 147\n100 120 140 160 180 200 220-2 0 2 4\n0 5 10 15 20 25 300 2 4 6 8\n2 4 6 8 10 12 14-4 -2 0 2 4\n-4 -2 0 2 4\nAbsent Present\n15 20 25 30 35 40 45-2 0 2 4 6\n20 30 40 50 60-6 -4 -2 0 2\u02c6f(sbp)\nsbp\n\u02c6f(tobacco )\ntobacco\u02c6f(ldl)\nldl\u02c6f(obesity )\nobesity\n\u02c6f(age)\nage\u02c6f(famhist )\nfamhist\nFIGURE 5.4. Fitted natural-spline functions for each of the terms in the \ufb01nal\nmodel selected by the stepwise procedure. Included are pointw ise standard-error\nbands. The rug plot at the base of each \ufb01gure indicates the location of each of the\nsample values for that variable (jittered to break ties).", "166": "148 5. Basis Expansions and Regularization\nTABLE 5.1. Final logistic regression model, after stepwise deletion of natural\nsplines terms. The column labeled \u201cLRT\u201d is the likelihood-ra tio test statistic when\nthat term is deleted from the model, and is the change in deviance from the full\nmodel (labeled \u201cnone\u201d).\nTerms Df Deviance AIC LRT P-value\nnone 458.09 502.09\nsbp 4 467.16 503.16 9.076 0.059\ntobacco 4 470.48 506.48 12.387 0.015\nldl 4 472.39 508.39 14.307 0.006\nfamhist 1 479.44 521.44 21.356 0.000\nobesity 4 466.24 502.24 8.147 0.086\nage 4 481.86 517.86 23.768 0.000\nthey were not in the linear model. The \ufb01gure explains why, since their\ncontributions are inherently nonlinear. These e\ufb00ects at \ufb01rst may come as\na surprise, but an explanation lies in the nature of the retrospective data.\nThese measurements were made sometime after the patients su\ufb00ered a\nheart attack, and in many cases they had already bene\ufb01ted from a healthier\ndiet and lifestyle, hence the apparent increase in risk at low values for\nobesity andsbp. Table 5.1 shows a summary of the selected model.\n5.2.3 Example: Phoneme Recognition\nIn this example we use splines to reduce \ufb02exibility rather than increase it;\nthe application comes under the general heading of functional modeling. In\nthe top panel of Figure 5.5 are displayed a sample of 15 log-periodograms\nfor each of the two phonemes \u201caa\u201d and \u201cao\u201d measured at 256 frequencies.\nThe goal is to use such data to classify a spoken phoneme. These two\nphonemes were chosen because they are di\ufb03cult to separate.\nThe input feature is a vector xof length 256, which we can think of as\na vector of evaluations of a function X(f) over a grid of frequencies f. In\nreality there is a continuous analog signal which is a function of frequency,\nand we have a sampled version of it.\nThe gray lines in the lower panel of Figure 5.5 show the coe\ufb03cients of\na linear logistic regression model \ufb01t by maximum likelihood to a training\nsample of 1000 drawn from the total of 695 \u201caa\u201ds and 1022 \u201cao\u201ds. The\ncoe\ufb03cients are also plotted as a function of frequency, and in fact we can\nthink of the model in terms of its continuous counterpart\nlogPr(aa|X)\nPr(ao|X)=/integraldisplay\nX(f)\u03b2(f)df, (5.7)", "167": "5.2 Piecewise Polynomials and Splines 149\nFrequencyLog-periodogram\n0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples\naa\nao\nFrequencyLogistic Regression Coefficients\n0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression\nFIGURE 5.5. The top panel displays the log-periodogram as a function of fre -\nquency for 15examples each of the phonemes \u201caa\u201d and \u201cao\u201d sampled from a total\nof695\u201caa\u201ds and 1022\u201cao\u201ds. Each log-periodogram is measured at 256uniformly\nspaced frequencies. The lower panel shows the coe\ufb03cients (as a f unction of fre-\nquency) of a logistic regression \ufb01t to the data by maximum likeli hood, using the\n256log-periodogram values as inputs. The coe\ufb03cients are restric ted to be smooth\nin the red curve, and are unrestricted in the jagged gray curve.", "168": "150 5. Basis Expansions and Regularization\nwhich we approximate by\n256/summationdisplay\nj=1X(fj)\u03b2(fj) =256/summationdisplay\nj=1xj\u03b2j. (5.8)\nThe coe\ufb03cients compute a contrast functional, and will have appreciable\nvalues in regions of frequency where the log-periodograms di\ufb00er between\nthe two classes.\nThe gray curves are very rough. Since the input signals have fairly strong\npositive autocorrelation, this results in negative autocorrelation in t he co-\ne\ufb03cients. In addition the sample size e\ufb00ectively provides only four obser-\nvations per coe\ufb03cient.\nApplications such as this permit a natural regularization. We force the\ncoe\ufb03cients to vary smoothly as a function of frequency. The red curve in the\nlower panel of Figure 5.5 shows such a smooth coe\ufb03cient curve \ufb01t to these\ndata. We see that the lower frequencies o\ufb00er the most discriminatory power.\nNot only does the smoothing allow easier interpretation of the contrast, it\nalso produces a more accurate classi\ufb01er:\nRaw Regularized\nTraining error 0.080 0.185\nTest error 0.255 0.158\nThe smooth red curve was obtained through a very simple use of natural\ncubic splines. We can represent the coe\ufb03cient function as an expansion of\nsplines \u03b2(f) =/summationtextM\nm=1hm(f)\u03b8m. In practice this means that \u03b2=H\u03b8where,\nHis ap\u00d7Mbasis matrix of natural cubic splines, de\ufb01ned on the set of\nfrequencies. Here we used M= 12 basis functions, with knots uniformly\nplaced over the integers 1 ,2,... ,256 representing the frequencies. Since\nxT\u03b2=xTH\u03b8, we can simply replace the input features xby their \ufb01ltered\nversions x\u2217=HTx, and \ufb01t \u03b8by linear logistic regression on the x\u2217. The\nred curve is thus \u02c6\u03b2(f) =h(f)T\u02c6\u03b8.\n5.3 Filtering and Feature Extraction\nIn the previous example, we constructed a p\u00d7Mbasis matrix H, and then\ntransformed our features xinto new features x\u2217=HTx. These \ufb01ltered\nversions of the features were then used as inputs into a learning procedure:\nin the previous example, this was linear logistic regression.\nPreprocessing of high-dimensional features is a very general and pow-\nerful method for improving the performance of a learning algorithm. The\npreprocessing need not be linear as it was above, but can be a general", "169": "5.4 Smoothing Splines 151\n(nonlinear) function of the form x\u2217=g(x). The derived features x\u2217can\nthen be used as inputs into any (linear or nonlinear) learning procedure.\nFor example, for signal or image recognition a popular approach is to \ufb01rst\ntransform the raw features via a wavelet transform x\u2217=HTx(Section 5.9)\nand then use the features x\u2217as inputs into a neural network (Chapter 11).\nWavelets are e\ufb00ective in capturing discrete jumps or edges, and the neural\nnetwork is a powerful tool for constructing nonlinear functions of these\nfeatures for predicting the target variable. By using domain knowledge\nto construct appropriate features, one can often improve upon a learning\nmethod that has only the raw features xat its disposal.\n5.4 Smoothing Splines\nHere we discuss a spline basis method that avoids the knot selection prob-\nlem completely by using a maximal set of knots. The complexity of the \ufb01t\nis controlled by regularization. Consider the following problem: among all\nfunctions f(x) with two continuous derivatives, \ufb01nd one that minimizes the\npenalized residual sum of squares\nRSS(f,\u03bb) =N/summationdisplay\ni=1{yi\u2212f(xi)}2+\u03bb/integraldisplay\n{f\u2032\u2032(t)}2dt, (5.9)\nwhere \u03bbis a \ufb01xed smoothing parameter . The \ufb01rst term measures closeness\nto the data, while the second term penalizes curvature in the function, and\n\u03bbestablishes a tradeo\ufb00 between the two. Two special cases are:\n\u03bb= 0 : fcan be any function that interpolates the data.\n\u03bb=\u221e:the simple least squares line \ufb01t, since no second derivative can\nbe tolerated.\nThese vary from very rough to very smooth, and the hope is that \u03bb\u2208(0,\u221e)\nindexes an interesting class of functions in between.\nThe criterion (5.9) is de\ufb01ned on an in\ufb01nite-dimensional function space\u2014\nin fact, a Sobolev space of functions for which the second term is de\ufb01ned.\nRemarkably, it can be shown that (5.9) has an explicit, \ufb01nite-dimensional,\nunique minimizer which is a natural cubic spline with knots at the unique\nvalues of the xi, i= 1,... ,N (Exercise 5.7). At face value it seems that\nthe family is still over-parametrized, since there are as many as Nknots,\nwhich implies Ndegrees of freedom. However, the penalty term translates\nto a penalty on the spline coe\ufb03cients, which are shrunk some of the way\ntoward the linear \ufb01t.\nSince the solution is a natural spline, we can write it as\nf(x) =N/summationdisplay\nj=1Nj(x)\u03b8j, (5.10)", "170": "152 5. Basis Expansions and Regularization\nAgeRelative Change in Spinal BMD\n10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022 \u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022 \u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022Male\nFemale\nFIGURE 5.6. The response is the relative change in bone mineral density mea-\nsured at the spine in adolescents, as a function of age. A separat e smoothing spline\nwas \ufb01t to the males and females, with \u03bb\u22480.00022. This choice corresponds to\nabout 12degrees of freedom.\nwhere the Nj(x) are an N-dimensional set of basis functions for repre-\nsenting this family of natural splines (Section 5.2.1 and Exercise 5.4). The\ncriterion thus reduces to\nRSS(\u03b8,\u03bb) = (y\u2212N\u03b8)T(y\u2212N\u03b8) +\u03bb\u03b8T\u2126N\u03b8, (5.11)\nwhere {N}ij=Nj(xi) and {\u2126N}jk=/integraltext\nN\u2032\u2032\nj(t)N\u2032\u2032\nk(t)dt. The solution is\neasily seen to be\n\u02c6\u03b8= (NTN+\u03bb\u2126N)\u22121NTy, (5.12)\na generalized ridge regression. The \ufb01tted smoothing spline is given by\n\u02c6f(x) =N/summationdisplay\nj=1Nj(x)\u02c6\u03b8j. (5.13)\nE\ufb03cient computational techniques for smoothing splines are discussed in\nthe Appendix to this chapter.\nFigure 5.6 shows a smoothing spline \ufb01t to some data on bone mineral\ndensity (BMD) in adolescents. The response is relative change in spinal\nBMD over two consecutive visits, typically about one year apart. The data\nare color coded by gender, and two separate curves were \ufb01t. This simple", "171": "5.4 Smoothing Splines 153\nsummary reinforces the evidence in the data that the growth spurt for\nfemales precedes that for males by about two years. In both cases the\nsmoothing parameter \u03bbwas approximately 0 .00022; this choice is discussed\nin the next section.\n5.4.1 Degrees of Freedom and Smoother Matrices\nWe have not yet indicated how \u03bbis chosen for the smoothing spline. Later\nin this chapter we describe automatic methods using techniques such as\ncross-validation. In this section we discuss intuitive ways of prespecifying\nthe amount of smoothing.\nA smoothing spline with prechosen \u03bbis an example of a linear smoother\n(as in linear operator). This is because the estimated parameters in (5.12)\nare a linear combination of the yi. Denote by \u02c6ftheN-vector of \ufb01tted values\n\u02c6f(xi) at the training predictors xi. Then\n\u02c6f=N(NTN+\u03bb\u2126N)\u22121NTy\n=S\u03bby. (5.14)\nAgain the \ufb01t is linear in y, and the \ufb01nite linear operator S\u03bbis known as\nthesmoother matrix . One consequence of this linearity is that the recipe\nfor producing \u02c6ffromydoes not depend on yitself;S\u03bbdepends only on\nthexiand\u03bb.\nLinear operators are familiar in more traditional least squares \ufb01tting as\nwell. Suppose B\u03beis aN\u00d7Mmatrix of Mcubic-spline basis functions\nevaluated at the Ntraining points xi, with knot sequence \u03be, and M\u226aN.\nThen the vector of \ufb01tted spline values is given by\n\u02c6f=B\u03be(BT\n\u03beB\u03be)\u22121BT\n\u03bey\n=H\u03bey. (5.15)\nHere the linear operator H\u03beis a projection operator, also known as the hat\nmatrix in statistics. There are some important similarities and di \ufb00erences\nbetween H\u03beandS\u03bb:\n\u2022Both are symmetric, positive semide\ufb01nite matrices.\n\u2022H\u03beH\u03be=H\u03be(idempotent), while S\u03bbS\u03bb\u221d\u221a\u2207e\u230be\u2308esequalS\u03bb, meaning that the right-\nhand side exceeds the left-hand side by a positive semide\ufb01nite matrix.\nThis is a consequence of the shrinking nature of S\u03bb, which we discuss\nfurther below.\n\u2022H\u03behas rank M, while S\u03bbhas rank N.\nThe expression M= trace( H\u03be) gives the dimension of the projection space,\nwhich is also the number of basis functions, and hence the number of pa-\nrameters involved in the \ufb01t. By analogy we de\ufb01ne the e\ufb00ective degrees of", "172": "154 5. Basis Expansions and Regularization\nfreedom of a smoothing spline to be\ndf\u03bb= trace( S\u03bb), (5.16)\nthe sum of the diagonal elements of S\u03bb. This very useful de\ufb01nition allows\nus a more intuitive way to parameterize the smoothing spline, and indeed\nmany other smoothers as well, in a consistent fashion. For example, in Fig-\nure 5.6 we speci\ufb01ed df \u03bb= 12 for each of the curves, and the corresponding\n\u03bb\u22480.00022 was derived numerically by solving trace( S\u03bb) = 12. There are\nmany arguments supporting this de\ufb01nition of degrees of freedom, and we\ncover some of them here.\nSinceS\u03bbis symmetric (and positive semide\ufb01nite), it has a real eigen-\ndecomposition. Before we proceed, it is convenient to rewrite S\u03bbin the\nReinsch form\nS\u03bb= (I+\u03bbK)\u22121, (5.17)\nwhere Kdoes not depend on \u03bb(Exercise 5.9). Since \u02c6f=S\u03bbysolves\nmin\nf(y\u2212f)T(y\u2212f) +\u03bbfTKf, (5.18)\nKis known as the penalty matrix , and indeed a quadratic form in Khas\na representation in terms of a weighted sum of squared (divided) second\ndi\ufb00erences. The eigen-decomposition of S\u03bbis\nS\u03bb=N/summationdisplay\nk=1\u03c1k(\u03bb)ukuT\nk (5.19)\nwith\n\u03c1k(\u03bb) =1\n1 +\u03bbdk, (5.20)\nanddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-\nsults of applying a cubic smoothing spline to some air pollution data (128\nobservations). Two \ufb01ts are given: a smoother \ufb01t corresponding to a larger\npenalty \u03bband a rougher \ufb01t for a smaller penalty. The lower panels repre-\nsent the eigenvalues (lower left) and some eigenvectors (lower right) of the\ncorresponding smoother matrices. Some of the highlights of the eigenrep-\nresentation are the following:\n\u2022The eigenvectors are not a\ufb00ected by changes in \u03bb, and hence the whole\nfamily of smoothing splines (for a particular sequence x) indexed by\n\u03bbhave the same eigenvectors.\n\u2022S\u03bby=/summationtextN\nk=1uk\u03c1k(\u03bb)\u221dan}\u230a\u2207a\u230bketle{tuk,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht, and hence the smoothing spline oper-\nates by decomposing yw.r.t. the (complete) basis {uk}, and di\ufb00er-\nentially shrinking the contributions using \u03c1k(\u03bb). This is to be con-\ntrasted with a basis-regression method, where the components are", "173": "5.4 Smoothing Splines 155\nDaggot Pressure GradientOzone Concentration\n-50 0 50 1000 10 20 30\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nOrderEigenvalues\n5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022df=5\ndf=11\n-50 0 50 100 -50 0 50 100\nFIGURE 5.7. (Top:) Smoothing spline \ufb01t of ozone concentration versus Daggot\npressure gradient. The two \ufb01ts correspond to di\ufb00erent values of t he smoothing\nparameter, chosen to achieve \ufb01ve and eleven e\ufb00ective degrees o f freedom, de\ufb01ned\nby df\u03bb=trace(S\u03bb). (Lower left:) First 25eigenvalues for the two smoothing-spline\nmatrices. The \ufb01rst two are exactly 1, and all are \u22650. (Lower right:) Third to\nsixth eigenvectors of the spline smoother matrices. In each cas e,ukis plotted\nagainst x, and as such is viewed as a function of x. The rugat the base of the\nplots indicate the occurrence of data points. The damped functio ns represent the\nsmoothed versions of these functions (using the 5df smoother).", "174": "156 5. Basis Expansions and Regularization\neither left alone, or shrunk to zero\u2014that is, a projection matrix such\nasH\u03beabove has Meigenvalues equal to 1, and the rest are 0. For\nthis reason smoothing splines are referred to as shrinking smoothers,\nwhile regression splines are projection smoothers (see Figure 3.17 on\npage 80).\n\u2022The sequence of uk, ordered by decreasing \u03c1k(\u03bb), appear to increase\nin complexity. Indeed, they have the zero-crossing behavior of polyno-\nmials of increasing degree. Since S\u03bbuk=\u03c1k(\u03bb)uk, we see how each of\nthe eigenvectors themselves are shrunk by the smoothing spline: the\nhigher the complexity, the more they are shrunk. If the domain of X\nis periodic, then the ukare sines and cosines at di\ufb00erent frequencies.\n\u2022The \ufb01rst two eigenvalues are always one, and they correspond to the\ntwo-dimensional eigenspace of functions linear in x(Exercise 5.11),\nwhich are never shrunk.\n\u2022The eigenvalues \u03c1k(\u03bb) = 1/(1 +\u03bbdk) are an inverse function of the\neigenvalues dkof the penalty matrix K, moderated by \u03bb;\u03bbcontrols\nthe rate at which the \u03c1k(\u03bb) decrease to zero. d1=d2= 0 and again\nlinear functions are not penalized.\n\u2022One can reparametrize the smoothing spline using the basis vectors\nuk(theDemmler\u2013Reinsch basis). In this case the smoothing spline\nsolves\nmin\n\u03b8\u221d\u230aa\u2207\u2308\u230aly\u2212U\u03b8\u221d\u230aa\u2207\u2308\u230al2+\u03bb\u03b8TD\u03b8, (5.21)\nwhere Uhas columns ukandDis a diagonal matrix with elements\ndk.\n\u2022df\u03bb= trace( S\u03bb) =/summationtextN\nk=1\u03c1k(\u03bb). For projection smoothers, all the\neigenvalues are 1, each one corresponding to a dimension of the pro-\njection subspace.\nFigure 5.8 depicts a smoothing spline matrix, with the rows ordered with\nx. The banded nature of this representation suggests that a smoothing\nspline is a local \ufb01tting method, much like the locally weighted regression\nprocedures in Chapter 6. The right panel shows in detail selected rows of\nS, which we call the equivalent kernels . As\u03bb\u21920, df \u03bb\u2192N, andS\u03bb\u2192I,\ntheN-dimensional identity matrix. As \u03bb\u2192 \u221e, df\u03bb\u21922, and S\u03bb\u2192H, the\nhat matrix for linear regression on x.\n5.5 Automatic Selection of the Smoothing\nParameters\nThe smoothing parameters for regression splines encompass the degree of\nthe splines, and the number and placement of the knots. For smoothing", "175": "5.5 Automatic Selection of the Smoothing Parameters 157\n11510075502512Smoother Matrix\n\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022Row 115\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022Row 100\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Row 75\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022Row 50\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022Row 25\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022Row 12Equivalent Kernels\nFIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,\nindicating an equivalent kernel with local support. The left pane l represents the\nelements of Sas an image. The right panel shows the equivalent kernel or weigh t-\ning function in detail for the indicated rows.", "176": "158 5. Basis Expansions and Regularization\nsplines, we have only the penalty parameter \u03bbto select, since the knots are\nat all the unique training X\u2019s, and cubic degree is almost always used in\npractice.\nSelecting the placement and number of knots for regression splines can be\na combinatorially complex task, unless some simpli\ufb01cations are enforced.\nThe MARS procedure in Chapter 9 uses a greedy algorithm with some\nadditional approximations to achieve a practical compromise. We will not\ndiscuss this further here.\n5.5.1 Fixing the Degrees of Freedom\nSince df \u03bb= trace( S\u03bb) is monotone in \u03bbfor smoothing splines, we can in-\nvert the relationship and specify \u03bbby \ufb01xing df. In practice this can be\nachieved by simple numerical methods. So, for example, in Rone can use\nsmooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-\nages a more traditional mode of model selection, where we might try a cou-\nple of di\ufb00erent values of df, and select one based on approximate F-tests,\nresidual plots and other more subjective criteria. Using df in this way pro-\nvides a uniform approach to compare many di\ufb00erent smoothing methods.\nIt is particularly useful in generalized additive models (Chapter 9), where\nseveral smoothing methods can be simultaneously used in one model.\n5.5.2 The Bias\u2013Variance Tradeo\ufb00\nFigure 5.9 shows the e\ufb00ect of the choice of df \u03bbwhen using a smoothing\nspline on a simple example:\nY=f(X) +\u03b5,\nf(X) =sin(12( X+ 0.2))\nX+ 0.2,(5.22)\nwithX\u223cU[0,1] and \u03b5\u223cN(0,1). Our training sample consists of N= 100\npairsxi,yidrawn independently from this model.\nThe \ufb01tted splines for three di\ufb00erent values of df \u03bbare shown. The yellow\nshaded region in the \ufb01gure represents the pointwise standard error of \u02c6f\u03bb,\nthat is, we have shaded the region between \u02c6f\u03bb(x)\u00b12\u2264se(\u02c6f\u03bb(x)). Since\n\u02c6f=S\u03bby,\nCov(\u02c6f) = S\u03bbCov(y)ST\n\u03bb\n=S\u03bbST\n\u03bb. (5.23)\nThe diagonal contains the pointwise variances at the training xi. The bias\nis given by\nBias(\u02c6f) = f\u2212E(\u02c6f)\n=f\u2212S\u03bbf, (5.24)", "177": "5.5 Automatic Selection of the Smoothing Parameters 159\n6 8 10 12 140.9 1.0 1.1 1.2\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\ny\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\nO\nOO\nOOO\nO\nO\nO\nOOO\nOO\nO\nOOO\nOOO\nO\nOOO\nO\nOO\nOOOOOOOO\nOOO\nOOOO\nO\nO\nO\nOOOO\nOO\nO\nOO\nOO\nOO\nO\nOO\nOO\nOO\nOO\nOOO\nOO\nOO\nOOO\nOO\nOOOO\nO\nOOO\nOOO\nOO\nOO\nOO\nO\nOy\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\nO\nOO\nOOO\nO\nO\nO\nOOO\nOO\nO\nOOO\nOOO\nO\nOOO\nO\nOO\nOOOOOOOO\nOOO\nOOOO\nO\nO\nO\nOOOO\nOO\nO\nOO\nOO\nOO\nO\nOO\nOO\nOO\nOO\nOOO\nOO\nOO\nOOO\nOO\nOOOO\nO\nOOO\nOOO\nOO\nOO\nOO\nO\nO\ny\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\nO\nOO\nOOO\nO\nO\nO\nOOO\nOO\nO\nOOO\nOOO\nO\nOOO\nO\nOO\nOOOOOOOO\nOOO\nOOOO\nO\nO\nO\nOOOO\nOO\nO\nOO\nOO\nOO\nO\nOO\nOO\nOO\nOO\nOOO\nOO\nOO\nOOO\nOO\nOOOO\nO\nOOO\nOOO\nOO\nOO\nOO\nO\nOEPECV\nX XXdf\u03bb= 5\ndf\u03bb= 9 df\u03bb= 15df\u03bbCross-ValidationEPE( \u03bb) and CV( \u03bb)\nFIGURE 5.9. The top left panel shows the EPE( \u03bb)andCV(\u03bb)curves for a\nrealization from a nonlinear additive error model (5.22). The r emaining panels\nshow the data, the true functions (in purple), and the \ufb01tted curve s (in green) with\nyellow shaded \u00b12\u00d7standard error bands, for three di\ufb00erent values of df\u03bb.", "178": "160 5. Basis Expansions and Regularization\nwherefis the (unknown) vector of evaluations of the true fat the training\nX\u2019s. The expectations and variances are with respect to repeated draws\nof samples of size N= 100 from the model (5.22). In a similar fashion\nVar(\u02c6f\u03bb(x0)) and Bias( \u02c6f\u03bb(x0)) can be computed at any point x0(Exer-\ncise 5.10). The three \ufb01ts displayed in the \ufb01gure give a visual demonstration\nof the bias-variance tradeo\ufb00 associated with selecting the smoothing\nparameter.\ndf\u03bb= 5:The spline under \ufb01ts, and clearly trims down the hills and \ufb01lls in\nthe valleys . This leads to a bias that is most dramatic in regions of\nhigh curvature. The standard error band is very narrow, so we esti-\nmate a badly biased version of the true function with great reliability!\ndf\u03bb= 9:Here the \ufb01tted function is close to the true function, although a\nslight amount of bias seems evident. The variance has not increased\nappreciably.\ndf\u03bb= 15: The \ufb01tted function is somewhat wiggly, but close to the true\nfunction. The wiggliness also accounts for the increased width of the\nstandard error bands\u2014the curve is starting to follow some individual\npoints too closely.\nNote that in these \ufb01gures we are seeing a single realization of data and\nhence \ufb01tted spline \u02c6fin each case, while the bias involves an expectation\nE(\u02c6f). We leave it as an exercise (5.10) to compute similar \ufb01gures where the\nbias is shown as well. The middle curve seems \u201cjust right,\u201d in that it has\nachieved a good compromise between bias and variance.\nThe integrated squared prediction error (EPE) combines both bias and\nvariance in a single summary:\nEPE( \u02c6f\u03bb) = E( Y\u2212\u02c6f\u03bb(X))2\n= Var( Y) + E/bracketleft\uf8ecig\nBias2(\u02c6f\u03bb(X)) + Var( \u02c6f\u03bb(X))/bracketright\uf8ecig\n=\u03c32+ MSE( \u02c6f\u03bb). (5.25)\nNote that this is averaged both over the training sample (giving rise to \u02c6f\u03bb),\nand the values of the (independently chosen) prediction points ( X,Y). EPE\nis a natural quantity of interest, and does create a tradeo\ufb00 between bias\nand variance. The blue points in the top left panel of Figure 5.9 suggest\nthat df \u03bb= 9 is spot on!\nSince we don\u2019t know the true function, we do not have access to EPE, and\nneed an estimate. This topic is discussed in some detail in Chapter 7, and\ntechniques such as K-fold cross-validation, GCV and Cpare all in common\nuse. In Figure 5.9 we include the N-fold (leave-one-out) cross-validation\ncurve:", "179": "5.6 Nonparametric Logistic Regression 161\nCV(\u02c6f\u03bb) =1\nNN/summationdisplay\ni=1(yi\u2212\u02c6f(\u2212i)\n\u03bb(xi))2(5.26)\n=1\nNN/summationdisplay\ni=1/parenleft\uf8ecigg\nyi\u2212\u02c6f\u03bb(xi)\n1\u2212S\u03bb(i,i)/parenright\uf8ecigg2\n, (5.27)\nwhich can (remarkably) be computed for each value of \u03bbfrom the original\n\ufb01tted values and the diagonal elements S\u03bb(i,i) ofS\u03bb(Exercise 5.13).\nThe EPE and CV curves have a similar shape, but the entire CV curve\nis above the EPE curve. For some realizations this is reversed, and overall\nthe CV curve is approximately unbiased as an estimate of the EPE curve.\n5.6 Nonparametric Logistic Regression\nThe smoothing spline problem (5.9) in Section 5.4 is posed in a regression\nsetting. It is typically straightforward to transfer this technology t o other\ndomains. Here we consider logistic regression with a single quantitativ e\ninput X. The model is\nlogPr(Y= 1|X=x)\nPr(Y= 0|X=x)=f(x), (5.28)\nwhich implies\nPr(Y= 1|X=x) =ef(x)\n1 +ef(x). (5.29)\nFitting f(x) in a smooth fashion leads to a smooth estimate of the condi-\ntional probability Pr( Y= 1|x), which can be used for classi\ufb01cation or risk\nscoring.\nWe construct the penalized log-likelihood criterion\n\u2113(f;\u03bb) =N/summationdisplay\ni=1[yilogp(xi) + (1 \u2212yi)log(1 \u2212p(xi))]\u22121\n2\u03bb/integraldisplay\n{f\u2032\u2032(t)}2dt\n=N/summationdisplay\ni=1/bracketleft\uf8ecig\nyif(xi)\u2212log(1 + ef(xi))/bracketright\uf8ecig\n\u22121\n2\u03bb/integraldisplay\n{f\u2032\u2032(t)}2dt, (5.30)\nwhere we have abbreviated p(x) = Pr( Y= 1|x). The \ufb01rst term in this ex-\npression is the log-likelihood based on the binomial distribution (c.f. Chap-\nter 4, page 120). Arguments similar to those used in Section 5.4 show that\nthe optimal fis a \ufb01nite-dimensional natural spline with knots at the unique", "180": "162 5. Basis Expansions and Regularization\nvalues of x. This means that we can represent f(x) =/summationtextN\nj=1Nj(x)\u03b8j. We\ncompute the \ufb01rst and second derivatives\n\u2202\u2113(\u03b8)\n\u2202\u03b8=NT(y\u2212p)\u2212\u03bb\u2126\u03b8, (5.31)\n\u22022\u2113(\u03b8)\n\u2202\u03b8\u2202\u03b8T=\u2212NTWN\u2212\u03bb\u2126, (5.32)\nwhere pis the N-vector with elements p(xi), and Wis a diagonal matrix\nof weights p(xi)(1\u2212p(xi)). The \ufb01rst derivative (5.31) is nonlinear in \u03b8, so\nwe need to use an iterative algorithm as in Section 4.4.1. Using Newton\u2013\nRaphson as in (4.23) and (4.26) for linear logistic regression, the updat e\nequation can be written\n\u03b8new= (NTWN+\u03bb\u2126)\u22121NTW/parenleftbig\nN\u03b8old+W\u22121(y\u2212p)/parenrightbig\n= (NTWN+\u03bb\u2126)\u22121NTWz. (5.33)\nWe can also express this update in terms of the \ufb01tted values\nfnew=N(NTWN+\u03bb\u2126)\u22121NTW/parenleftbig\nfold+W\u22121(y\u2212p)/parenrightbig\n=S\u03bb,wz. (5.34)\nReferring back to (5.12) and (5.14), we see that the update \ufb01ts a weighted\nsmoothing spline to the working response z(Exercise 5.12).\nThe form of (5.34) is suggestive. It is tempting to replace S\u03bb,wby any\nnonparametric (weighted) regression operator, and obtain general fami-\nlies of nonparametric logistic regression models. Although here xis one-\ndimensional, this procedure generalizes naturally to higher-dimensional x.\nThese extensions are at the heart of generalized additive models , which we\npursue in Chapter 9.\n5.7 Multidimensional Splines\nSo far we have focused on one-dimensional spline models. Each of the ap-\nproaches have multidimensional analogs. Suppose X\u2208IR2, and we have\na basis of functions h1k(X1), k= 1,... ,M 1for representing functions of\ncoordinate X1, and likewise a set of M2functions h2k(X2) for coordinate\nX2. Then the M1\u00d7M2dimensional tensor product basis de\ufb01ned by\ngjk(X) =h1j(X1)h2k(X2), j= 1,... ,M 1, k= 1,... ,M 2 (5.35)\ncan be used for representing a two-dimensional function:\ng(X) =M1/summationdisplay\nj=1M2/summationdisplay\nk=1\u03b8jkgjk(X). (5.36)", "181": "5.7 Multidimensional Splines 163\nFIGURE 5.10. A tensor product basis of B-splines, showing some selected pair s.\nEach two-dimensional function is the tensor product of the corre sponding one\ndimensional marginals.\nFigure 5.10 illustrates a tensor product basis using B-splines. The coe\ufb03-\ncients can be \ufb01t by least squares, as before. This can be generalized to d\ndimensions, but note that the dimension of the basis grows exponentially\nfast\u2014yet another manifestation of the curse of dimensionality. The MARS\nprocedure discussed in Chapter 9 is a greedy forward algorithm for includ-\ning only those tensor products that are deemed necessary by least squares.\nFigure 5.11 illustrates the di\ufb00erence between additive and tensor product\n(natural) splines on the simulated classi\ufb01cation example from Chapter 2.\nA logistic regression model logit[Pr( T|x)] =h(x)T\u03b8is \ufb01t to the binary re-\nsponse, and the estimated decision boundary is the contour h(x)T\u02c6\u03b8= 0.\nThe tensor product basis can achieve more \ufb02exibility at the decision bound-\nary, but introduces some spurious structure along the way.", "182": "164 5. Basis Expansions and Regularization\nAdditive Natural Cubic Splines - 4 df each\n.. .. . .. . . .. . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . . .. . . .. . . .. . . .. . .. . .. . .. .. .. ...\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.23\nTest Error:       0.28\nBayes Error:    0.21\nNatural Cubic Splines - Tensor Product - 4 df each\n. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . ... . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.230\nTest Error:       0.282\nBayes Error:    0.210\nFIGURE 5.11. The simulation example of Figure 2.1. The upper panel shows the\ndecision boundary of an additive logistic regression model, using natural splines\nin each of the two coordinates (total df = 1 + (4 \u22121) + (4 \u22121) = 7 ). The lower\npanel shows the results of using a tensor product of natural spline bases in each\ncoordinate (total df = 4\u00d74 = 16) . The broken purple boundary is the Bayes\ndecision boundary for this problem.", "183": "5.7 Multidimensional Splines 165\nOne-dimensional smoothing splines (via regularization) generalize to high-\ner dimensions as well. Suppose we have pairs yi,xiwithxi\u2208IRd, and we\nseek a d-dimensional regression function f(x). The idea is to set up the\nproblem\nmin\nfN/summationdisplay\ni=1{yi\u2212f(xi)}2+\u03bbJ[f], (5.37)\nwhere Jis an appropriate penalty functional for stabilizing a function fin\nIRd. For example, a natural generalization of the one-dimensional roughness\npenalty (5.9) for functions on IR2is\nJ[f] =/integraldisplay /integraldisplay\nI R2/bracketleft\uf8ecig/parenleftbigg\u22022f(x)\n\u2202x2\n1/parenrightbigg2\n+2/parenleftbigg\u22022f(x)\n\u2202x1\u2202x2/parenrightbigg2\n+/parenleftbigg\u22022f(x)\n\u2202x2\n2/parenrightbigg2/bracketright\uf8ecig\ndx1dx2.(5.38)\nOptimizing (5.37) with this penalty leads to a smooth two-dimensional\nsurface, known as a thin-plate spline. It shares many properties with the\none-dimensional cubic smoothing spline:\n\u2022as\u03bb\u21920, the solution approaches an interpolating function [the one\nwith smallest penalty (5.38)];\n\u2022as\u03bb\u2192 \u221e, the solution approaches the least squares plane;\n\u2022for intermediate values of \u03bb, the solution can be represented as a\nlinear expansion of basis functions, whose coe\ufb03cients are obtained\nby a form of generalized ridge regression.\nThe solution has the form\nf(x) =\u03b20+\u03b2Tx+N/summationdisplay\nj=1\u03b1jhj(x), (5.39)\nwhere hj(x) =||x\u2212xj||2log||x\u2212xj||. These hjare examples of radial\nbasis functions , which are discussed in more detail in the next section. The\ncoe\ufb03cients are found by plugging (5.39) into (5.37), which reduces to a\n\ufb01nite-dimensional penalized least squares problem. For the penalty to be\n\ufb01nite, the coe\ufb03cients \u03b1jhave to satisfy a set of linear constraints; see\nExercise 5.14.\nThin-plate splines are de\ufb01ned more generally for arbitrary dimension d,\nfor which an appropriately more general Jis used.\nThere are a number of hybrid approaches that are popular in practice,\nboth for computational and conceptual simplicity. Unlike one-dimensional\nsmoothing splines, the computational complexity for thin-plate splines is\nO(N3), since there is not in general any sparse structure that can be ex-\nploited. However, as with univariate smoothing splines, we can get away\nwith substantially less than the Nknots prescribed by the solution (5.39).", "184": "166 5. Basis Expansions and Regularization\n125\n130135140145150155\n15202530354045\n20 30 40 50 60\nAgeObesitySystolic Blood Pressure\n120125130135140145150155160\n\u2022\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022 \u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\nFIGURE 5.12. A thin-plate spline \ufb01t to the heart disease data, displayed as a\ncontour plot. The response is systolic blood pressure , modeled as a function\nofageandobesity . The data points are indicated, as well as the lattice of points\nused as knots. Care should be taken to use knots from the lattice inside the convex\nhull of the data (red), and ignore those outside (green).\nIn practice, it is usually su\ufb03cient to work with a lattice of knots covering\nthe domain. The penalty is computed for the reduced expansion just as\nbefore. Using Kknots reduces the computations to O(NK2+K3). Fig-\nure 5.12 shows the result of \ufb01tting a thin-plate spline to some heart disease\nrisk factors, representing the surface as a contour plot. Indicated are the\nlocation of the input features, as well as the knots used in the \ufb01t. Note that\n\u03bbwas speci\ufb01ed via df \u03bb= trace( S\u03bb) = 15.\nMore generally one can represent f\u2208IRdas an expansion in any arbi-\ntrarily large collection of basis functions, and control the complexity by a p-\nplying a regularizer such as (5.38). For example, we could construct a basis\nby forming the tensor products of all pairs of univariate smoothing-spline\nbasis functions as in (5.35), using, for example, the univariate B-splines\nrecommended in Section 5.9.2 as ingredients. This leads to an exponential", "185": "5.8 Regularization and Reproducing Kernel Hilbert Spaces 167\ngrowth in basis functions as the dimension increases, and typically we have\nto reduce the number of functions per coordinate accordingly.\nThe additive spline models discussed in Chapter 9 are a restricted class\nof multidimensional splines. They can be represented in this general formu-\nlation as well; that is, there exists a penalty J[f] that guarantees that the\nsolution has the form f(X) =\u03b1+f1(X1) +\u2264\u2264\u2264+fd(Xd) and that each of\nthe functions fjare univariate splines. In this case the penalty is somewhat\ndegenerate, and it is more natural to assume thatfis additive, and then\nsimply impose an additional penalty on each of the component functions:\nJ[f] = J(f1+f2+\u2264\u2264\u2264+fd)\n=d/summationdisplay\nj=1/integraldisplay\nf\u2032\u2032\nj(tj)2dtj. (5.40)\nThese are naturally extended to ANOVA spline decompositions,\nf(X) =\u03b1+/summationdisplay\njfj(Xj) +/summationdisplay\nj<kfjk(Xj,Xk) +\u2264\u2264\u2264, (5.41)\nwhere each of the components are splines of the required dimension. There\nare many choices to be made:\n\u2022The maximum order of interaction\u2014we have shown up to order 2\nabove.\n\u2022Which terms to include\u2014not all main e\ufb00ects and interactions are\nnecessarily needed.\n\u2022What representation to use\u2014some choices are:\n\u2013regression splines with a relatively small number of basis func-\ntions per coordinate, and their tensor products for interactions;\n\u2013a complete basis as in smoothing splines, and include appropri-\nate regularizers for each term in the expansion.\nIn many cases when the number of potential dimensions (features) is large,\nautomatic methods are more desirable. The MARS and MART procedures\n(Chapters 9 and 10, respectively), both fall into this category.\n5.8 Regularization and Reproducing Kernel\nHilbert Spaces\nIn this section we cast splines into the larger context of regularization meth-\nods and reproducing kernel Hilbert spaces. This section is quite technical\nand can be skipped by the disinterested or intimidated reader.", "186": "168 5. Basis Expansions and Regularization\nA general class of regularization problems has the form\nmin\nf\u2208H/bracketleft\uf8eciggN/summationdisplay\ni=1L(yi,f(xi)) +\u03bbJ(f)/bracketright\uf8ecigg\n(5.42)\nwhere L(y,f(x)) is a loss function, J(f) is a penalty functional, and His\na space of functions on which J(f) is de\ufb01ned. Girosi et al. (1995) describe\nquite general penalty functionals of the form\nJ(f) =/integraldisplay\nI Rd|\u02dcf(s)|2\n\u02dcG(s)ds, (5.43)\nwhere \u02dcfdenotes the Fourier transform of f, and \u02dcGis some positive function\nthat falls o\ufb00 to zero as ||s|| \u2192 \u221e . The idea is that 1 /\u02dcGincreases the penalty\nfor high-frequency components of f. Under some additional assumptions\nthey show that the solutions have the form\nf(X) =K/summationdisplay\nk=1\u03b1k\u03c6k(X) +N/summationdisplay\ni=1\u03b8iG(X\u2212xi), (5.44)\nwhere the \u03c6kspan the null space of the penalty functional J, and Gis the\ninverse Fourier transform of \u02dcG. Smoothing splines and thin-plate splines\nfall into this framework. The remarkable feature of this solution is tha t\nwhile the criterion (5.42) is de\ufb01ned over an in\ufb01nite-dimensional space, the\nsolution is \ufb01nite-dimensional. In the next sections we look at some speci\ufb01c\nexamples.\n5.8.1 Spaces of Functions Generated by Kernels\nAn important subclass of problems of the form (5.42) are generated by\na positive de\ufb01nite kernel K(x,y), and the corresponding space of func-\ntionsHKis called a reproducing kernel Hilbert space (RKHS). The penalty\nfunctional Jis de\ufb01ned in terms of the kernel as well. We give a brief and\nsimpli\ufb01ed introduction to this class of models, adapted from Wahba (1990)\nand Girosi et al. (1995), and nicely summarized in Evgeniou et al. (2000).\nLetx,y\u2208IRp. We consider the space of functions generated by the linear\nspan of {K(\u2264,y), y\u2208IRp)}; i.e arbitrary linear combinations of the form\nf(x) =/summationtext\nm\u03b1mK(x,ym), where each kernel term is viewed as a function\nof the \ufb01rst argument, and indexed by the second. Suppose that Khas an\neigen-expansion\nK(x,y) =\u221e/summationdisplay\ni=1\u03b3i\u03c6i(x)\u03c6i(y) (5.45)\nwith\u03b3i\u22650,/summationtext\u221e\ni=1\u03b32\ni<\u221e. Elements of HKhave an expansion in terms of\nthese eigen-functions,\nf(x) =\u221e/summationdisplay\ni=1ci\u03c6i(x), (5.46)", "187": "5.8 Regularization and Reproducing Kernel Hilbert Spaces 169\nwith the constraint that\n||f||2\nHKdef=\u221e/summationdisplay\ni=1c2\ni/\u03b3i<\u221e, (5.47)\nwhere ||f||HKis the norm induced by K. The penalty functional in (5.42)\nfor the space HKis de\ufb01ned to be the squared norm J(f) =||f||2\nHK. The\nquantity J(f) can be interpreted as a generalized ridge penalty, where\nfunctions with large eigenvalues in the expansion (5.45) get penalized less,\nand vice versa.\nRewriting (5.42) we have\nmin\nf\u2208HK/bracketleft\uf8eciggN/summationdisplay\ni=1L(yi,f(xi)) +\u03bb||f||2\nHK/bracketright\uf8ecigg\n(5.48)\nor equivalently\nmin\n{cj}\u221e\n1\uf8ee\n\uf8f0N/summationdisplay\ni=1L(yi,\u221e/summationdisplay\nj=1cj\u03c6j(xi)) +\u03bb\u221e/summationdisplay\nj=1c2\nj/\u03b3j\uf8f9\n\uf8fb. (5.49)\nIt can be shown (Wahba, 1990, see also Exercise 5.15) that the solution\nto (5.48) is \ufb01nite-dimensional, and has the form\nf(x) =N/summationdisplay\ni=1\u03b1iK(x,xi). (5.50)\nThe basis function hi(x) =K(x,xi) (as a function of the \ufb01rst argument) is\nknown as the representer of evaluation atxiinHK, since for f\u2208 H K, it is\neasily seen that \u221dan}\u230a\u2207a\u230bketle{tK(\u2264,xi),f\u221dan}\u230a\u2207a\u230bket\u2207i}htHK=f(xi). Similarly \u221dan}\u230a\u2207a\u230bketle{tK(\u2264,xi),K(\u2264,xj)\u221dan}\u230a\u2207a\u230bket\u2207i}htHK=\nK(xi,xj) (the reproducing property of HK), and hence\nJ(f) =N/summationdisplay\ni=1N/summationdisplay\nj=1K(xi,xj)\u03b1i\u03b1j (5.51)\nforf(x) =/summationtextN\ni=1\u03b1iK(x,xi).\nIn light of (5.50) and (5.51), (5.48) reduces to a \ufb01nite-dimensional crite-\nrion\nmin\n\u03b1L(y,K\u03b1) +\u03bb\u03b1TK\u03b1. (5.52)\nWe are using a vector notation, in which Kis the N\u00d7Nmatrix with ijth\nentry K(xi,xj) and so on. Simple numerical algorithms can be used to\noptimize (5.52). This phenomenon, whereby the in\ufb01nite-dimensional prob-\nlem (5.48) or (5.49) reduces to a \ufb01nite dimensional optimization problem,\nhas been dubbed the kernel property in the literature on support-vector\nmachines (see Chapter 12).", "188": "170 5. Basis Expansions and Regularization\nThere is a Bayesian interpretation of this class of models, in which f\nis interpreted as a realization of a zero-mean stationary Gaussian process,\nwith prior covariance function K. The eigen-decomposition produces a se-\nries of orthogonal eigen-functions \u03c6j(x) with associated variances \u03b3j. The\ntypical scenario is that \u201csmooth\u201d functions \u03c6jhave large prior variance,\nwhile \u201crough\u201d \u03c6jhave small prior variances. The penalty in (5.48) is the\ncontribution of the prior to the joint likelihood, and penalizes more those\ncomponents with smaller prior variance (compare with (5.43)).\nFor simplicity we have dealt with the case here where all members of H\nare penalized, as in (5.48). More generally, there may be some components\ninHthat we wish to leave alone, such as the linear functions for cubic\nsmoothing splines in Section 5.4. The multidimensional thin-plate splines\nof Section 5.7 and tensor product splines fall into this category as well.\nIn these cases there is a more convenient representation H=H0\u2295 H1,\nwith the null space H0consisting of, for example, low degree polynomi-\nals in xthat do not get penalized. The penalty becomes J(f) =\u221d\u230aa\u2207\u2308\u230alP1f\u221d\u230aa\u2207\u2308\u230al,\nwhere P1is the orthogonal projection of fontoH1. The solution has the\nformf(x) =/summationtextM\nj=1\u03b2jhj(x) +/summationtextN\ni=1\u03b1iK(x,xi), where the \ufb01rst term repre-\nsents an expansion in H0. From a Bayesian perspective, the coe\ufb03cients of\ncomponents in H0have improper priors, with in\ufb01nite variance.\n5.8.2 Examples of RKHS\nThe machinery above is driven by the choice of the kernel Kand the loss\nfunction L. We consider \ufb01rst regression using squared-error loss. In this\ncase (5.48) specializes to penalized least squares, and the solution can be\ncharacterized in two equivalent ways corresponding to (5.49) or (5.52):\nmin\n{cj}\u221e\n1N/summationdisplay\ni=1\uf8eb\n\uf8edyi\u2212\u221e/summationdisplay\nj=1cj\u03c6j(xi)\uf8f6\n\uf8f82\n+\u03bb\u221e/summationdisplay\nj=1c2\nj\n\u03b3j(5.53)\nan in\ufb01nite-dimensional, generalized ridge regression problem, or\nmin\n\u03b1(y\u2212K\u03b1)T(y\u2212K\u03b1) +\u03bb\u03b1TK\u03b1. (5.54)\nThe solution for \u03b1is obtained simply as\n\u02c6\u03b1= (K+\u03bbI)\u22121y, (5.55)\nand\n\u02c6f(x) =N/summationdisplay\nj=1\u02c6\u03b1jK(x,xj). (5.56)", "189": "5.8 Regularization and Reproducing Kernel Hilbert Spaces 171\nThe vector of N\ufb01tted values is given by\n\u02c6f=K\u02c6\u03b1\n=K(K+\u03bbI)\u22121y (5.57)\n= (I+\u03bbK\u22121)\u22121y. (5.58)\nThe estimate (5.57) also arises as the kriging estimate of a Gaussian ran-\ndom \ufb01eld in spatial statistics (Cressie, 1993). Compare also (5.58) w ith the\nsmoothing spline \ufb01t (5.17) on page 154.\nPenalized Polynomial Regression\nThe kernel K(x,y) = (\u221dan}\u230a\u2207a\u230bketle{tx,y\u221dan}\u230a\u2207a\u230bket\u2207i}ht+ 1)d(Vapnik, 1996), for x,y\u2208IRp, has\nM=/parenleftbigp+d\nd/parenrightbig\neigen-functions that span the space of polynomials in IRpof\ntotal degree d. For example, with p= 2 and d= 2,M= 6 and\nK(x,y) = 1 + 2 x1y1+ 2x2y2+x2\n1y2\n1+x2\n2y2\n2+ 2x1x2y1y2(5.59)\n=M/summationdisplay\nm=1hm(x)hm(y) (5.60)\nwith\nh(x)T= (1,\u221a\n2x1,\u221a\n2x2,x2\n1,x2\n2,\u221a\n2x1x2). (5.61)\nOne can represent hin terms of the Morthogonal eigen-functions and\neigenvalues of K,\nh(x) =VD1\n2\u03b3\u03c6(x), (5.62)\nwhere D\u03b3= diag( \u03b31,\u03b32,... ,\u03b3 M), and VisM\u00d7Mand orthogonal.\nSuppose we wish to solve the penalized polynomial regression problem\nmin\n{\u03b2m}M\n1N/summationdisplay\ni=1/parenleft\uf8ecigg\nyi\u2212M/summationdisplay\nm=1\u03b2mhm(xi)/parenright\uf8ecigg2\n+\u03bbM/summationdisplay\nm=1\u03b22\nm. (5.63)\nSubstituting (5.62) into (5.63), we get an expression of the form (5.53) to\noptimize (Exercise 5.16).\nThe number of basis functions M=/parenleftbigp+d\nd/parenrightbig\ncan be very large, often much\nlarger than N. Equation (5.55) tells us that if we use the kernel represen-\ntation for the solution function, we have only to evaluate the kernel N2\ntimes, and can compute the solution in O(N3) operations.\nThis simplicity is not without implications. Each of the polynomials hm\nin (5.61) inherits a scaling factor from the particular form of K, which has\na bearing on the impact of the penalty in (5.63). We elaborate on this in\nthe next section.", "190": "172 5. Basis Expansions and Regularization\n\u22122 \u22121 0 1 2 3 40.0 0.4 0.8\nXRadial Kernel in I R1K(\u2264, xm)\nFIGURE 5.13. Radial kernels kk(x)for the mixture data, with scale parameter\n\u03bd= 1. The kernels are centered at \ufb01ve points xmchosen at random from the 200.\nGaussian Radial Basis Functions\nIn the preceding example, the kernel is chosen because it represents an\nexpansion of polynomials and can conveniently compute high-dimensional\ninner products. In this example the kernel is chosen because of its functional\nform in the representation (5.50).\nThe Gaussian kernel K(x,y) =e\u2212\u03bd||x\u2212y||2along with squared-error loss,\nfor example, leads to a regression model that is an expansion in Gaussian\nradial basis functions,\nkm(x) =e\u2212\u03bd||x\u2212xm||2, m= 1,... ,N, (5.64)\neach one centered at one of the training feature vectors xm. The coe\ufb03cients\nare estimated using (5.54).\nFigure 5.13 illustrates radial kernels in IR1using the \ufb01rst coordinate of\nthe mixture example from Chapter 2. We show \ufb01ve of the 200 kernel basis\nfunctions km(x) =K(x,xm).\nFigure 5.14 illustrates the implicit feature space for the radial kernel\nwithx\u2208IR1. We computed the 200 \u00d7200 kernel matrix K, and its eigen-\ndecomposition \u03a6D\u03b3\u03a6T. We can think of the columns of \u03a6and the corre-\nsponding eigenvalues in D\u03b3as empirical estimates of the eigen expansion\n(5.45)2. Although the eigenvectors are discrete, we can represent them as\nfunctions on IR1(Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-\nues ofK. The leading eigenfunctions are smooth, and they are successively\nmore wiggly as the order increases. This brings to life the penalty in (5.49) ,\nwhere we see the coe\ufb03cients of higher-order functions get penalized more\nthan lower-order ones. The right panel in Figure 5.14 shows the correspond-\n2The\u2113th column of \u03a6is an estimate of \u03c6\u2113, evaluated at each of the Nobservations.\nAlternatively, the ith row of \u03a6is the estimated vector of basis functions \u03c6(xi), evaluated\nat the point xi. Although in principle, there can be in\ufb01nitely many element s in\u03c6, our\nestimate has at most Nelements.", "191": "5.8 Regularization and Reproducing Kernel Hilbert Spaces 173\n*\n******* ** ** ******\n******\n*******\n******\n**********\n****\n** *******\n*****\n**\n***\n**********\n*****\n*****\n***\n**\n***\n*****\n*************\n******\n***\n**\n***\n**\n***********\n**\n****\n**** *\n** ******* * **\n**************\n********* ***\n**\n*******\n***\n**\n****\n***\n***\n**\n***\n*****\n*******\n**\n**\n**\n**\n***\n**\n*****\n**\n******\n*\n***\n**\n**\n*********\n**\n**\n****\n**\n**\n**\n****\n****\n***\n**\n**\n*******\n**\n***\n**\n*\n****\n**\n****\n**\n**\n****\n****\n**\n**\n*****\n****\n**\n****\n*\n**\n***\n**\n**\n****\n****\n**\n**\n*\n***\n**\n*********\n******\n*\n***\n** **\n****\n*\n***\n***\n***\n***\n*******\n**\n**\n**\n*\n*******\n**\n***\n***\n******\n**\n***\n**\n******\n**\n**\n*\n**\n**\n**\n*\n***\n**\n**\n****\n*** **\n*********\n****\n**\n****\n** **\n** ****\n****\n***\n***\n**\n** ***\n**\n**\n*\n***\n***\n* * * **\n****\n**\n**\n****\n*****\n****\n* **** ***\n**\n*** **** * *\n**\n* *\n**\n**\n******\n***\n***\n****\n***\n***\n*\n**\n**\n**\n**\n***\n***\n***\n****\n***\n********\n*******\n** ** **\n**\n***\n***\n**\n*\n*********\n**********\n*\n****\n***\n***\n**\n**\n***\n*****\n****\n**\n**\n****\n***\n***\n*\n**\n**\n***\n**\n**\n******\n*******\n***\n***\n*** *\n****\n***\n****\n* ***\n* * ** *\n* ***\n**\n*\n**\n***\n***\n**\n***\n**\n*******\n**\n**\n*******\n**\n*\n* *********\n***\n******\n******\n** ***\n*\n**\n*\n**\n**\n***\n* **\n***\n*\n***\n**\n***********\n**\n**\n**\n***\n**\n***\n**\n**\n*********\n***\n***\n**\n**\n*\n**\n*\n**\n***\n* * **\n*****\n**\n**\n*****\n**\n**\n** ******\n**\n**\n***** ****\n***\n****\n* *****\n****\n****\n***\n**\n**\n**\n* *******\n****\n****\n***\n**\n****\n*****\n******\n*****\n*\n*****\n***\n*\n***\n******\n*****\n***\n***********\n***\n***** **\n* *\n***\n*****\n*******\n*****\n**\n****\n**\n*\n**\n*\n*****\n***\n*******\n******* *\n*****\n*\n**\n***\n*\n*****\n**\n*****\n**\n*** *\n***\n***\n******\n**\n***\n********\n***\n**\n**\n***\n***** ****\n***\n**\n****\n********\n*******\n***\n**\n***\n**\n***\n**\n*********\n*\n*******\n*****\n*\n**\n**\n***\n***\n*\n**\n*\n**\n**\n******\n**\n*\n*****\n*\n***\n** **\n*\n***\n** ***\n****\n**\n*******\n****\n**\n****\n*****\n******\n**\n****\n**\n**\n** ***\n*****\n*\n**\n**\n***\n**\n****\n**\n*****\n***\n**\n****\n***\n**\n**\n*\n**\n***\n*\n*****\n***\n******* ***\n**\n****\n* ***\n******\n*****\n*****\n***\n**\n* ** ***\n***\n***\n*\n***\n***\n****\n****\n***\n*****\n*****\n***\n****\n**\n**\n**\n*\n*****\n**\n******\n********* *\n**\n*\n****\n**\n*\n* *\n***\n**\n****\n***\n****\n*** ****\n****\n**\n*****\n***\n**\n**\n***\n*****\n*\n***\n*** *\n******\n*\n***********\n**\n******\n**\n*******\n**\n****\n*\n**********\n*\n**** *\n**\n******\n*\n* **\n***\n*\n**\n*****\n*\n**\n*\n****\n**\n*\n***\n** *\n****\n******\n***\n**\n*\n**\n**** ****\n**\n***\n**\n***\n***\n*****\n**** * *\n**\n**\n******\n*****\n*****\n***\n** *\n****\n*\n****\n** *\n*****\n****\n**** *\n*****\n**\n****\n*****\n** **\n***\n****\n****\n**\n** *****\n**\n*****\n***\n**\n****\n*\n****\n****\n**\n*****\n***\n********\n*\n****\n***\n**\n***\n**\n*****\n**\n***\n*****\n********\n****\n* *******\n******* * *\n***\n****\n**** *\n**\n**\n***\n****\n**\n**\n**\n******\n**\n**\n**\n****\n*****\n***\n****\n*****\n*\n***********\n**\n**\n**\n**\n**\n*******\n******\n***********\n***\n***\n*****\n****\n***\n* ***\n*****\n**\n*********\n*\n**\n***\n** ****\n** * *\n**\n***\n***\n**\n*****\n***\n*****\n****\n**\n**\n***\n*****\n* * **\n**\n**\n* **\n***\n**\n*****\n***\n***\n***\n*******\n**\n*** *\n****\n*\n* ***\n****\n***\n***\n***\n***\n*****\n* ***\n****\n****\n***\n****\n*****\n*******\n***\n**\n***\n****\n*\n***\n***\n*\n**\n*\n*****\n**\n***\n**\n**\n***\n**\n*** *\n******\n**\n* **\n****\n**\n***\n**\n***\n*******\n**\n***\n**\n****\n***\n****\n* * *\n****\n* ***\n***\n**\n**\n******\n***\n**\n*****\n**\n**\n*\n***\n***\n**\n** *\n******\n*****\n***\n*\n***\n****\n**\n* **\n*******\n****\n*******\n**\n***\n***\n*\n*****\n*\n********\n***\n*\n***\n***\n**\n*******\n**\n*\n*****\n***\n**\n***\n***\n****\n* ***\n******\n***\n***\n*****\n*** **\n**\n**\n***\n***\n********\n**\n****\n**** *****\n****\n****\n**\n***\n**\n****\n** *****\n**\n**\n*****\n**\n**\n**\n***\n****\n*****\n*** *\n*******\n****\n********\n**\n** **\n*****\n**\n***\n*\n******\n**\n***\n***\n*\n****\n**\n***\n**\n******\n***\n***\n*\n**\n*******\n**\n****\n****\n*\n*******\n**** ******\n*\n****\n****\n***\n*\n**\n****\n** **\n** *\n*** *\n**\n****\n***************\n***\n***\n*****\n*****\n* *****\n***\n****\n*****\n**\n***\n**\n****\n**\n***\n*\n*****\n***\n**\n****\n***\n**\n**\n*******\n*****\n***\n*\n***\n*******\n*\n**\n**** ***\n****\n*\n****\n****\n*****\n*****\n***\n***\n*****\n**\n***\n**\n*\n**\n**\n*****\n**\n**\n*\n**\n**\n**\n****\n**** *\n*\n**\n******\n****\n**\n****\n***\n*\n*\n****\n****\n****\n***\n***\n******\n***\n****\n*\n***\n**\n*****\n*\n* ****\n****\n**\n*\n**\n*\n***\n****\n**\n**\n***\n****\n***\n**\n*\n**\n**\n********* *\n**\n******\n**\n***\n**\n**\n****\n***\n***\n**\n**\n*******\n***\n**\n**\n***\n**\n*\n**\n*\n**\n**Orthonormal Basis \u03a6\n*\n*****\n** *\n* ***\n*\n****\n***\n*\n**\n*\n***\n***\n******\n**\n**\n***\n***\n****\n**\n*\n**\n****\n*****\n**\n***\n**\n******\n**\n**\n*\n**\n**\n***\n***\n**\n***\n*****\n*\n***\n*\n***\n*****\n***\n***\n***\n**\n***\n**\n*\n**\n********\n**\n****\n*\n***\n*\n** *\n*\n***\n** ***\n**\n**\n**\n**\n***\n***\n***\n***\n***\n***\n**\n***\n****\n***\n**\n****\n***\n***\n**\n***\n****\n*\n**\n**\n***\n**\n**\n**\n**\n**\n*\n**\n****\n*\n**\n******\n*\n***\n**\n**\n*\n**\n**\n*\n*\n**\n**\n**\n****\n*\n*\n**\n**\n**\n**\n****\n***\n**\n**\n***\n***\n*\n**\n***\n**\n*\n****\n**\n*\n***\n**\n**\n****\n****\n**\n**\n****\n*\n****\n**\n****\n*\n**\n***\n**\n**\n***\n*\n****\n**\n**\n*\n***\n**\n***\n**\n*\n**\n*\n******\n*\n**\n*\n***\n*\n***\n*\n*\n***\n***\n***\n*\n**\n****\n**\n*\n**\n**\n**\n*\n***\n***\n*\n**\n***\n**\n*\n****\n**\n**\n***\n**\n*****\n*\n**\n**\n*\n**\n**\n**\n*\n***\n**\n**\n***\n*\n*****\n*****\n****\n****\n**\n*\n***\n** **\n***\n***\n***\n*\n**\n*\n***\n**\n** ***\n**\n**\n*\n***\n***\n** ***\n****\n**\n**\n***\n*\n**\n***\n*\n***\n*******\n*\n**\n*\n***\n*** **\n**\n* *\n**\n**\n***\n***\n***\n***\n****\n***\n***\n*\n**\n**\n**\n*\n*\n***\n**\n*\n*\n**\n**\n**\n***\n***\n*****\n*\n******\n** ****\n**\n***\n**\n*\n**\n*\n****\n*\n****\n**\n********\n*\n****\n**\n*\n***\n**\n**\n***\n*****\n****\n**\n**\n****\n***\n*\n**\n*\n**\n**\n***\n**\n**\n******\n*****\n**\n***\n**\n*\n****\n****\n***\n*\n**\n*\n* ***\n* *** *\n* ***\n**\n*\n**\n***\n***\n**\n*\n**\n**\n*******\n**\n**\n**\n*****\n**\n*\n* ****\n***\n**\n***\n*\n*****\n******\n** ***\n*\n**\n*\n**\n**\n*\n**\n* **\n***\n*\n***\n**\n*****\n***\n***\n**\n**\n**\n***\n**\n***\n**\n**\n**\n*******\n*\n**\n***\n**\n**\n*\n**\n*\n**\n***\n* ***\n*****\n**\n**\n*****\n**\n**\n** ***\n*\n**\n**\n**\n***** ****\n***\n****\n******\n****\n****\n***\n**\n**\n**\n* *******\n**\n**\n****\n*\n**\n**\n****\n*****\n******\n*****\n*\n*****\n***\n*\n***\n******\n*****\n***\n****\n*****\n**\n***\n***** **\n* *\n***\n*****\n*******\n****\n*\n**\n****\n**\n*\n**\n*\n***\n**\n***\n***\n****\n******* *\n*****\n*\n**\n***\n*\n*****\n**\n*****\n**\n*** *\n***\n***\n******\n**\n***\n********\n***\n**\n**\n***\n***** ****\n***\n**\n****\n********\n*******\n***\n**\n***\n**\n***\n**\n*********\n*\n*******\n*****\n*\n**\n**\n***\n***\n*\n**\n*\n**\n**\n******\n**\n*\n*****\n*\n***\n** **\n*\n***\n** ***\n****\n**\n*******\n****\n**\n****\n*****\n******\n**\n****\n****\n** ********\n***\n*****\n**\n****\n**\n**********\n****\n***\n**\n***\n**\n****\n*****\n***\n******* ***\n**\n****\n* ***\n******\n*****\n*****\n*****\n* ** ***\n***\n***\n*\n***\n***\n****\n*******\n*****\n*****\n***\n* *****\n**\n**\n*\n*****\n**\n******\n********* ****\n****\n***\n* ****\n**\n****\n***\n******* ** ********\n********\n**\n**\n********\n****\n*** *\n***** ***************\n******\n**\n*** ****\n******************** ** ****** ***\n** **\n***\n*\n**********\n*****\n******** *\n****\n** * ******\n********* * ***\n**\n********\n******* ***** * *\n****** ***** *** *********** ************ ** ***** ******* **\n**** ************** *** ********* **** * ******************* ********** ***************** *******\n*****\n***\n******** *************** ******* ** * ****\n******* * ******* ** ** * ***** *** ** ************* ******** ** ***** ** ***** **\n**** * ************************* ******************************** ** * ** *** *********** * ************** * ****** * *************** ***************\n****** * **** * ** ** *** ***** ***** ************* ******** ** ***** ** ************ ************* * ********** * **** ********* ********************* ** * *** *********** ** ********** ** * * ****** * **************** ****** *** ***********\n* **** * ** ** ** * **** * *** ** ************* ******** ********* ***** ****** * ************* * ** ******** * **** ********* *********** ** ****** *** ** *** *********** * * ***** * **** ** * * ****** * * ******* ******* * ****** ** * *********** * **** * ** ** ** * **** * *** ** ************* ******** ** ***** ** ***** ** **** * **** ********* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** ** ****** * * ************** * ****** ** * **** * ****** * ** ** * ** ** ** * **** * *** ** ************* ******** ** **** *** ***** ** **** * ************* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** * * ****** * * ************** * ****** ** * **** * ****** * **** * ** ** ** * **** * *** ** **** ***** **** ******** ** ***** ** ***** ** **** * ************* * ** ******** * **** ********* *** * ******* ** ****** ** * * * *** *********** ** ***** * **** ** * * * * **** * * ************** * ****** ** * **** * ******Feature Space H\nFIGURE 5.14. (Left panel) The \ufb01rst 16normalized eigenvectors of K, the\n200\u00d7200kernel matrix for the \ufb01rst coordinate of the mixture data. These a re\nviewed as estimates \u02c6\u03c6\u2113of the eigenfunctions in (5.45), and are represented as\nfunctions in I R1with the observed values superimposed in color. They are arr anged\nin rows, starting at the top left. (Right panel) Rescaled versi onsh\u2113=\u221a\u02c6\u03b3\u2113\u02c6\u03c6\u2113of\nthe functions in the left panel, for which the kernel computes the \u201cinner product.\u201d\n0 10 20 30 40 501e\u221215 1e\u221211 1e\u221207 1e\u221203 1e+01Eigenvalue\nFIGURE 5.15. The largest 50eigenvalues of K; all those beyond the 30th are\ne\ufb00ectively zero.", "192": "174 5. Basis Expansions and Regularization\ningfeature space representation of the eigenfunctions\nh\u2113(x) =/radicalbig\n\u02c6\u03b3\u2113\u02c6\u03c6\u2113(x), \u2113= 1,... ,N. (5.65)\nNote that \u221dan}\u230a\u2207a\u230bketle{th(xi),h(xi\u2032)\u221dan}\u230a\u2207a\u230bket\u2207i}ht=K(xi,xi\u2032). The scaling by the eigenvalues quickly\nshrinks most of the functions down to zero, leaving an e\ufb00ective dimension\nof about 12 in this case. The corresponding optimization problem is a stan-\ndard ridge regression, as in (5.63). So although in principle the implicit\nfeature space is in\ufb01nite dimensional, the e\ufb00ective dimension is dramat-\nically lower because of the relative amounts of shrinkage applied to each\nbasis function. The kernel scale parameter \u03bdplays a role here as well; larger\n\u03bdimplies more local kmfunctions, and increases the e\ufb00ective dimension of\nthe feature space. See Hastie and Zhu (2006) for more details.\nIt is also known (Girosi et al., 1995) that a thin-plate spline (Section 5.7 )\nis an expansion in radial basis functions, generated by the kernel\nK(x,y) =\u221d\u230aa\u2207\u2308\u230alx\u2212y\u221d\u230aa\u2207\u2308\u230al2log(\u221d\u230aa\u2207\u2308\u230alx\u2212y\u221d\u230aa\u2207\u2308\u230al). (5.66)\nRadial basis functions are discussed in more detail in Section 6.7.\nSupport Vector Classi\ufb01ers\nThe support vector machines of Chapter 12 for a two-class classi\ufb01cation\nproblem have the form f(x) =\u03b10+/summationtextN\ni=1\u03b1iK(x,xi), where the parameters\nare chosen to minimize\nmin\n\u03b10,\u03b1/braceleft\uf8eciggN/summationdisplay\ni=1[1\u2212yif(xi)]++\u03bb\n2\u03b1TK\u03b1/braceright\uf8ecigg\n, (5.67)\nwhere yi\u2208 {\u22121,1}, and [ z]+denotes the positive part of z. This can be\nviewed as a quadratic optimization problem with linear constraints, and\nrequires a quadratic programming algorithm for its solution. The name\nsupport vector arises from the fact that typically many of the \u02c6 \u03b1i= 0 [due\nto the piecewise-zero nature of the loss function in (5.67)], and so \u02c6fis an\nexpansion in a subset of the K(\u2264,xi). See Section 12.3.3 for more details.\n5.9 Wavelet Smoothing\nWe have seen two di\ufb00erent modes of operation with dictionaries of basis\nfunctions. With regression splines, we select a subset of the bases, using\neither subject-matter knowledge, or else automatically. The more adaptive\nprocedures such as MARS (Chapter 9) can capture both smooth and non-\nsmooth behavior. With smoothing splines, we use a complete basis, but\nthen shrink the coe\ufb03cients toward smoothness.", "193": "5.9 Wavelet Smoothing 175\nTime0.0 0.2 0.4 0.6 0.8 1.0Haar Wavelets\nTime0.0 0.2 0.4 0.6 0.8 1.0Symmlet-8 Wavelets\n\u03c81,0\u03c82,1\u03c82,3\u03c83,2\u03c83,5\u03c84,4\u03c84,9\u03c85,1\u03c85,15\u03c86,15\u03c86,35\nFIGURE 5.16. Some selected wavelets at di\ufb00erent translations and dilations\nfor the Haar and symmlet families. The functions have been scale d to suit the\ndisplay.\nWavelets typically use a complete orthonormal basis to represent func-\ntions, but then shrink and select the coe\ufb03cients toward a sparse represen-\ntation. Just as a smooth function can be represented by a few spline basis\nfunctions, a mostly \ufb02at function with a few isolated bumps can be repre-\nsented with a few (bumpy) basis functions. Wavelets bases are very popular\nin signal processing and compression, since they are able to represent both\nsmooth and/or locally bumpy functions in an e\ufb03cient way\u2014a phenomenon\ndubbed time and frequency localization . In contrast, the traditional Fourier\nbasis allows only frequency localization.\nBefore we give details, let\u2019s look at the Haar wavelets in the left panel\nof Figure 5.16 to get an intuitive idea of how wavelet smoothing works.\nThe vertical axis indicates the scale (frequency) of the wavelets, from low\nscale at the bottom to high scale at the top. At each scale the wavelets are\n\u201cpacked in\u201d side-by-side to completely \ufb01ll the time axis: we have only shown", "194": "176 5. Basis Expansions and Regularization\na selected subset. Wavelet smoothing \ufb01ts the coe\ufb03cients for this basis by\nleast squares, and then thresholds (discards, \ufb01lters) the smaller coe\ufb03cients.\nSince there are many basis functions at each scale, it can use bases where\nit needs them and discard the ones it does not need, to achieve time and\nfrequency localization. The Haar wavelets are simple to understand, but not\nsmooth enough for most purposes. The symmlet wavelets in the right panel\nof Figure 5.16 have the same orthonormal properties, but are smoother.\nFigure 5.17 displays an NMR (nuclear magnetic resonance) signal, which\nappears to be composed of smooth components and isolated spikes, plus\nsome noise. The wavelet transform, using a symmlet basis, is shown in the\nlower left panel. The wavelet coe\ufb03cients are arranged in rows, from lowest\nscale at the bottom, to highest scale at the top. The length of each line\nsegment indicates the size of the coe\ufb03cient. The bottom right panel shows\nthe wavelet coe\ufb03cients after they have been thresholded. The threshold\nprocedure, given below in equation (5.69), is the same soft-thresholding\nrule that arises in the lasso procedure for linear regression (Section 3.4.2).\nNotice that many of the smaller coe\ufb03cients have been set to zero. The\ngreen curve in the top panel shows the back-transform of the thresholded\ncoe\ufb03cients: this is the smoothed version of the original signal. In the next\nsection we give the details of this process, including the construction of\nwavelets and the thresholding rule.\n5.9.1 Wavelet Bases and the Wavelet Transform\nIn this section we give details on the construction and \ufb01ltering of wavelets.\nWavelet bases are generated by translations and dilations of a single scal-\ning function \u03c6(x) (also known as the father ). The red curves in Figure 5.18\nare the Haar andsymmlet-8 scaling functions. The Haar basis is particu-\nlarly easy to understand, especially for anyone with experience in analysis\nof variance or trees, since it produces a piecewise-constant representation.\nThus if \u03c6(x) =I(x\u2208[0,1]), then \u03c60,k(x) =\u03c6(x\u2212k),kan integer, generates\nan orthonormal basis for functions with jumps at the integers. Call this ref-\nerence space V0. The dilations \u03c61,k(x) =\u221a\n2\u03c6(2x\u2212k) form an orthonormal\nbasis for a space V1\u2283V0of functions piecewise constant on intervals of\nlength1\n2. In fact, more generally we have \u2264\u2264\u2264 \u2283 V1\u2283V0\u2283V\u22121\u2283 \u2264\u2264\u2264 where\neachVjis spanned by \u03c6j,k= 2j/2\u03c6(2jx\u2212k).\nNow to the de\ufb01nition of wavelets. In analysis of variance, we often rep-\nresent a pair of means \u03b81and\u03b82by their grand mean \u03b8=1\n2(\u03b81+\u03b82), and\nthen a contrast \u03b1=1\n2(\u03b81\u2212\u03b82). A simpli\ufb01cation occurs if the contrast \u03b1is\nvery small, because then we can set it to zero. In a similar manner we might\nrepresent a function in Vj+1by a component in Vjplus the component in\nthe orthogonal complement WjofVjtoVj+1, written as Vj+1=Vj\u2295Wj.\nThe component in Wjrepresents detail, and we might wish to set some ele-\nments of this component to zero. It is easy to see that the functions \u03c8(x\u2212k)", "195": "5.9 Wavelet Smoothing 177\nNMR Signal\n0 200 400 600 800 10000 20 40 60\n0 200 400 600 800 1000Wavelet Transform - Original Signal\n0 200 400 600 800 1000Wavelet Transform - WaveShrunk Signal\nSignal Signal\nW9 W9\nW8 W8\nW7 W7\nW6 W6\nW5 W5\nW4 W4\nV4 V4\nFIGURE 5.17. The top panel shows an NMR signal, with the wavelet-shrunk\nversion superimposed in green. The lower left panel represents the wavelet trans-\nform of the original signal, down to V4, using the symmlet-8 basis. Each coe\ufb03-\ncient is represented by the height (positive or negative) of the vertical bar. The\nlower right panel represents the wavelet coe\ufb03cients after being shrunken using\nthewaveshrink function in S-PLUS, which implements the SureShrink method\nof wavelet adaptation of Donoho and Johnstone.", "196": "178 5. Basis Expansions and Regularization\nHaar Basis Symmlet Basis\n\u03c6(x) \u03c6(x)\n\u03c8(x) \u03c8(x)\nFIGURE 5.18. TheHaarandsymmlet father (scaling) wavelet \u03c6(x)and mother\nwavelet \u03c8(x).\ngenerated by the mother wavelet \u03c8(x) =\u03c6(2x)\u2212\u03c6(2x\u22121) form an orthonor-\nmal basis for W0for the Haar family. Likewise \u03c8j,k= 2j/2\u03c8(2jx\u2212k) form\na basis for Wj.\nNowVj+1=Vj\u2295Wj=Vj\u22121\u2295Wj\u22121\u2295Wj, so besides representing a\nfunction by its level- jdetail and level- jrough components, the latter can\nbe broken down to level-( j\u22121) detail and rough, and so on. Finally we get\na representation of the form VJ=V0\u2295W0\u2295W1\u2264\u2264\u2264 \u2295WJ\u22121. Figure 5.16\non page 175 shows particular wavelets \u03c8j,k(x).\nNotice that since these spaces are orthogonal, all the basis functions are\northonormal. In fact, if the domain is discrete with N= 2J(time) points,\nthis is as far as we can go. There are 2jbasis elements at level j, and\nadding up, we have a total of 2J\u22121 elements in the Wj, and one in V0.\nThis structured orthonormal basis allows for a multiresolution analysis ,\nwhich we illustrate in the next section.\nWhile helpful for understanding the construction above, the Haar basis\nis often too coarse for practical purposes. Fortunately, many clever wavelet\nbases have been invented. Figures 5.16 and 5.18 include the Daubechies\nsymmlet-8 basis. This basis has smoother elements than the corresponding\nHaar basis, but there is a tradeo\ufb00:\n\u2022Each wavelet has a support covering 15 consecutive time intervals,\nrather than one for the Haar basis. More generally, the symmlet- p\nfamily has a support of 2 p\u22121 consecutive intervals. The wider the\nsupport, the more time the wavelet has to die to zero, and so it can", "197": "5.9 Wavelet Smoothing 179\nachieve this more smoothly. Note that the e\ufb00ective support seems to\nbe much narrower.\n\u2022The symmlet- pwavelet \u03c8(x) has pvanishing moments; that is,\n/integraldisplay\n\u03c8(x)xjdx= 0, j= 0,... ,p \u22121.\nOne implication is that any order- ppolynomial over the N= 2Jtimes\npoints is reproduced exactly in V0(Exercise 5.18). In this sense V0\nis equivalent to the null space of the smoothing-spline penalty. The\nHaar wavelets have one vanishing moment, and V0can reproduce any\nconstant function.\nThe symmlet- pscaling functions are one of many families of wavelet\ngenerators. The operations are similar to those for the Haar basis:\n\u2022IfV0is spanned by \u03c6(x\u2212k), then V1\u2283V0is spanned by \u03c61,k(x) =\u221a\n2\u03c6(2x\u2212k) and \u03c6(x) =/summationtext\nk\u2208Zh(k)\u03c61,k(x), for some \ufb01lter coe\ufb03cients\nh(k).\n\u2022W0is spanned by \u03c8(x) =/summationtext\nk\u2208Zg(k)\u03c61,k(x), with \ufb01lter coe\ufb03cients\ng(k) = (\u22121)1\u2212kh(1\u2212k).\n5.9.2 Adaptive Wavelet Filtering\nWavelets are particularly useful when the data are measured on a uniform\nlattice, such as a discretized signal, image, or a time series. We will focus o n\nthe one-dimensional case, and having N= 2Jlattice-points is convenient.\nSuppose yis the response vector, and Wis the N\u00d7Northonormal wavelet\nbasis matrix evaluated at the Nuniformly spaced observations. Then y\u2217=\nWTyis called the wavelet transform ofy(and is the full least squares\nregression coe\ufb03cient). A popular method for adaptive wavelet \ufb01tting is\nknown as SURE shrinkage (Stein Unbiased Risk Estimation, Donoho and\nJohnstone (1994)). We start with the criterion\nmin\n\u03b8||y\u2212W\u03b8||2\n2+ 2\u03bb||\u03b8||1, (5.68)\nwhich is the same as the lasso criterion in Chapter 3. Because Wis or-\nthonormal, this leads to the simple solution:\n\u02c6\u03b8j= sign( y\u2217\nj)(|y\u2217\nj| \u2212\u03bb)+. (5.69)\nThe least squares coe\ufb03cients are translated toward zero, and truncated\nat zero. The \ufb01tted function (vector) is then given by the inverse wavelet\ntransform \u02c6f=W\u02c6\u03b8.", "198": "180 5. Basis Expansions and Regularization\nA simple choice for \u03bbis\u03bb=\u03c3\u221a2logN, where \u03c3is an estimate of the\nstandard deviation of the noise. We can give some motivation for this cho ice.\nSinceWis an orthonormal transformation, if the elements of yare white\nnoise (independent Gaussian variates with mean 0 and variance \u03c32), then\nso arey\u2217. Furthermore if random variables Z1,Z2,... ,Z Nare white noise,\nthe expected maximum of |Zj|,j= 1,... ,N is approximately \u03c3\u221a2logN.\nHence all coe\ufb03cients below \u03c3\u221a2logNare likely to be noise and are set to\nzero.\nThe space Wcould be any basis of orthonormal functions: polynomials,\nnatural splines or cosinusoids. What makes wavelets special is the particular\nform of basis functions used, which allows for a representation localized in\ntime and in frequency .\nLet\u2019s look again at the NMR signal of Figure 5.17. The wavelet transfor m\nwas computed using a symmlet \u22128 basis. Notice that the coe\ufb03cients do not\ndescend all the way to V0, but stop at V4which has 16 basis functions.\nAs we ascend to each level of detail, the coe\ufb03cients get smaller, except in\nlocations where spiky behavior is present. The wavelet coe\ufb03cients represent\ncharacteristics of the signal localized in time (the basis functions at each\nlevel are translations of each other) and localized in frequency. Each dilation\nincreases the detail by a factor of two, and in this sense corresponds to\ndoubling the frequency in a traditional Fourier representation. In fact, a\nmore mathematical understanding of wavelets reveals that the wavelets at\na particular scale have a Fourier transform that is restricted to a limited\nrange or octave of frequencies.\nThe shrinking/truncation in the right panel was achieved using the SURE\napproach described in the introduction to this section. The orthonormal\nN\u00d7Nbasis matrix Whas columns which are the wavelet basis functions\nevaluated at the Ntime points. In particular, in this case there will be 16\ncolumns corresponding to the \u03c64,k(x), and the remainder devoted to the\n\u03c8j,k(x), j= 4,... ,11. In practice \u03bbdepends on the noise variance, and has\nto be estimated from the data (such as the variance of the coe\ufb03cients at\nthe highest level).\nNotice the similarity between the SURE criterion (5.68) on page 179,\nand the smoothing spline criterion (5.21) on page 156:\n\u2022Both are hierarchically structured from coarse to \ufb01ne detail, although\nwavelets are also localized in time within each resolution level.\n\u2022The splines build in a bias toward smooth functions by imposing\ndi\ufb00erential shrinking constants dk. Early versions of SURE shrinkage\ntreated all scales equally. The S+wavelets function waveshrink() has\nmany options, some of which allow for di\ufb00erential shrinkage.\n\u2022The spline L2penalty cause pure shrinkage, while the SURE L1\npenalty does shrinkage and selection.", "199": "Exercises 181\nMore generally smoothing splines achieve compression of the original signal\nby imposing smoothness, while wavelets impose sparsity. Figure 5.19 co m-\npares a wavelet \ufb01t (using SURE shrinkage) to a smoothing spline \ufb01t (using\ncross-validation) on two examples di\ufb00erent in nature. For the NMR data in\nthe upper panel, the smoothing spline introduces detail everywhere in order\nto capture the detail in the isolated spikes; the wavelet \ufb01t nicely localizes\nthe spikes. In the lower panel, the true function is smooth, and the noise is\nrelatively high. The wavelet \ufb01t has let in some additional and unnecessary\nwiggles\u2014a price it pays in variance for the additional adaptivity.\nThe wavelet transform is not performed by matrix multiplication as in\ny\u2217=WTy. In fact, using clever pyramidal schemes y\u2217can be obtained\ninO(N) computations, which is even faster than the Nlog(N) of the fast\nFourier transform (FFT). While the general construction is beyond the\nscope of this book, it is easy to see for the Haar basis (Exercise 5.19).\nLikewise, the inverse wavelet transform W\u02c6\u03b8is also O(N).\nThis has been a very brief glimpse of this vast and growing \ufb01eld. There is\na very large mathematical and computational base built on wavelets. Mod-\nern image compression is often performed using two-dimensional wavelet\nrepresentations.\nBibliographic Notes\nSplines and B-splines are discussed in detail in de Boor (1978). Green\nand Silverman (1994) and Wahba (1990) give a thorough treatment of\nsmoothing splines and thin-plate splines; the latter also covers reproducing\nkernel Hilbert spaces. See also Girosi et al. (1995) and Evgeniou et al.\n(2000) for connections between many nonparametric regression techniques\nusing RKHS approaches. Modeling functional data, as in Section 5.2.3, is\ncovered in detail in Ramsay and Silverman (1997).\nDaubechies (1992) is a classic and mathematical treatment of wavelets.\nOther useful sources are Chui (1992) and Wickerhauser (1994). Donoho and\nJohnstone (1994) developed the SURE shrinkage and selection technology\nfrom a statistical estimation framework; see also Vidakovic (199 9). Bruce\nand Gao (1996) is a useful applied introduction, which also describes the\nwavelet software in S-PLUS.\nExercises\nEx. 5.1 Show that the truncated power basis functions in (5.3) represent a\nbasis for a cubic spline with the two knots as indicated.", "200": "182 5. Basis Expansions and Regularization\nNMR Signal0 200 400 600 800 10000 20 40 60spline\nwavelet\nSmooth Function (Simulated)n\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2 4spline\nwavelet\ntrue\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\nFIGURE 5.19. Wavelet smoothing compared with smoothing splines on two\nexamples. Each panel compares the SURE-shrunk wavelet \ufb01t to the cro ss-validated\nsmoothing spline \ufb01t.", "201": "Exercises 183\nEx. 5.2 Suppose that Bi,M(x) is an order- M B-spline de\ufb01ned in the Ap-\npendix on page 186 through the sequence (5.77)\u2013(5.78).\n(a) Show by induction that Bi,M(x) = 0 for x\u221dne}ationslash\u2208[\u03c4i,\u03c4i+M]. This shows, for\nexample, that the support of cubic B-splines is at most 5 knots.\n(b) Show by induction that Bi,M(x)>0 forx\u2208(\u03c4i,\u03c4i+M). The B-splines\nare positive in the interior of their support.\n(c) Show by induction that/summationtextK+M\ni=1Bi,M(x) = 1\u2200x\u2208[\u03be0,\u03beK+1].\n(d) Show that Bi,Mis a piecewise polynomial of order M(degree M\u22121)\non [\u03be0,\u03beK+1], with breaks only at the knots \u03be1,... ,\u03be K.\n(e) Show that an order- M B-spline basis function is the density function\nof a convolution of Muniform random variables.\nEx. 5.3 Write a program to reproduce Figure 5.3 on page 145.\nEx. 5.4 Consider the truncated power series representation for cubic splines\nwithKinterior knots. Let\nf(X) =3/summationdisplay\nj=0\u03b2jXj+K/summationdisplay\nk=1\u03b8k(X\u2212\u03bek)3\n+. (5.70)\nProve that the natural boundary conditions for natural cubic splines (Sec-\ntion 5.2.1) imply the following linear constraints on the coe\ufb03cients:\n\u03b22= 0,/summationtextK\nk=1\u03b8k= 0,\n\u03b23= 0,/summationtextK\nk=1\u03bek\u03b8k= 0.(5.71)\nHence derive the basis (5.4) and (5.5).\nEx. 5.5 Write a program to classify the phoneme data using a quadratic dis-\ncriminant analysis (Section 4.3). Since there are many correlated features,\nyou should \ufb01lter them using a smooth basis of natural cubic splines (Sec-\ntion 5.2.3). Decide beforehand on a series of \ufb01ve di\ufb00erent choices for the\nnumber and position of the knots, and use tenfold cross-validation to make\nthe \ufb01nal selection. The phoneme data are available from the book website\nwww-stat.stanford.edu/ElemStatLearn .\nEx. 5.6 Suppose you wish to \ufb01t a periodic function, with a known period T.\nDescribe how you could modify the truncated power series basis to achieve\nthis goal.\nEx. 5.7 Derivation of smoothing splines (Green and Silverman, 1994). Sup-\npose that N\u22652, and that gis the natural cubic spline interpolant to the\npairs{xi,zi}N\n1, with a < x 1<\u2264\u2264\u2264< x N< b. This is a natural spline", "202": "184 5. Basis Expansions and Regularization\nwith a knot at every xi; being an N-dimensional space of functions, we can\ndetermine the coe\ufb03cients such that it interpolates the sequence ziexactly.\nLet \u02dcgbe any other di\ufb00erentiable function on [ a,b] that interpolates the N\npairs.\n(a) Let h(x) = \u02dcg(x)\u2212g(x). Use integration by parts and the fact that gis\na natural cubic spline to show that\n/integraldisplayb\nag\u2032\u2032(x)h\u2032\u2032(x)dx=\u2212N\u22121/summationdisplay\nj=1g\u2032\u2032\u2032(x+\nj){h(xj+1)\u2212h(xj)}(5.72)\n= 0.\n(b) Hence show that/integraldisplayb\na\u02dcg\u2032\u2032(t)2dt\u2265/integraldisplayb\nag\u2032\u2032(t)2dt,\nand that equality can only hold if his identically zero in [ a,b].\n(c) Consider the penalized least squares problem\nmin\nf/bracketleft\uf8eciggN/summationdisplay\ni=1(yi\u2212f(xi))2+\u03bb/integraldisplayb\naf\u2032\u2032(t)2dt/bracketright\uf8ecigg\n.\nUse (b) to argue that the minimizer must be a cubic spline with knots\nat each of the xi.\nEx. 5.8 In the appendix to this chapter we show how the smoothing spline\ncomputations could be more e\ufb03ciently carried out using a ( N+ 4) dimen-\nsional basis of B-splines. Describe a slightly simpler scheme using a ( N+2)\ndimensional B-spline basis de\ufb01ned on the N\u22122 interior knots.\nEx. 5.9 Derive the Reinsch form S\u03bb= (I+\u03bbK)\u22121for the smoothing spline.\nEx. 5.10 Derive an expression for Var( \u02c6f\u03bb(x0)) and bias( \u02c6f\u03bb(x0)). Using the\nexample (5.22), create a version of Figure 5.9 where the mean and several\n(pointwise) quantiles of \u02c6f\u03bb(x) are shown.\nEx. 5.11 Prove that for a smoothing spline the null space of Kis spanned\nby functions linear in X.\nEx. 5.12 Characterize the solution to the following problem,\nmin\nfRSS(f,\u03bb) =N/summationdisplay\ni=1wi{yi\u2212f(xi)}2+\u03bb/integraldisplay\n{f\u2032\u2032(t)}2dt, (5.73)\nwhere the wi\u22650 are observation weights.\nCharacterize the solution to the smoothing spline problem (5.9) when\nthe training data have ties in X.", "203": "Exercises 185\nEx. 5.13 You have \ufb01tted a smoothing spline \u02c6f\u03bbto a sample of Npairs\n(xi,yi). Suppose you augment your original sample with the pair x0,\u02c6f\u03bb(x0),\nand re\ufb01t; describe the result. Use this to derive the N-fold cross-validation\nformula (5.26).\nEx. 5.14 Derive the constraints on the \u03b1jin the thin-plate spline expan-\nsion (5.39) to guarantee that the penalty J(f) is \ufb01nite. How else could one\nensure that the penalty was \ufb01nite?\nEx. 5.15 This exercise derives some of the results quoted in Section 5.8.1.\nSuppose K(x,y) satisfying the conditions (5.45) and let f(x)\u2208 H K. Show\nthat\n(a)\u221dan}\u230a\u2207a\u230bketle{tK(\u2264,xi),f\u221dan}\u230a\u2207a\u230bket\u2207i}htHK=f(xi).\n(b)\u221dan}\u230a\u2207a\u230bketle{tK(\u2264,xi),K(\u2264,xj)\u221dan}\u230a\u2207a\u230bket\u2207i}htHK=K(xi,xj).\n(c) If g(x) =/summationtextN\ni=1\u03b1iK(x,xi), then\nJ(g) =N/summationdisplay\ni=1N/summationdisplay\nj=1K(xi,xj)\u03b1i\u03b1j.\nSuppose that \u02dc g(x) =g(x) +\u03c1(x), with \u03c1(x)\u2208 H K, and orthogonal in HK\nto each of K(x,xi), i= 1,... ,N . Show that\n(d)\nN/summationdisplay\ni=1L(yi,\u02dcg(xi)) +\u03bbJ(\u02dcg)\u2265N/summationdisplay\ni=1L(yi,g(xi)) +\u03bbJ(g) (5.74)\nwith equality i\ufb00 \u03c1(x) = 0.\nEx. 5.16 Consider the ridge regression problem (5.53), and assume M\u2265N.\nAssume you have a kernel Kthat computes the inner product K(x,y) =/summationtextM\nm=1hm(x)hm(y).\n(a) Derive (5.62) on page 171 in the text. How would you compute the\nmatrices VandD\u03b3, given K? Hence show that (5.63) is equivalent\nto (5.53).\n(b) Show that\n\u02c6f=H\u02c6\u03b2\n=K(K+\u03bbI)\u22121y, (5.75)\nwhereHis the N\u00d7Mmatrix of evaluations hm(xi), and K=HHT\ntheN\u00d7Nmatrix of inner-products h(xi)Th(xj).", "204": "186 5. Basis Expansions and Regularization\n(c) Show that\n\u02c6f(x) = h(x)T\u02c6\u03b2\n=N/summationdisplay\ni=1K(x,xi)\u02c6\u03b1i (5.76)\nand\u02c6\u03b1= (K+\u03bbI)\u22121y.\n(d) How would you modify your solution if M < N ?\nEx. 5.17 Show how to convert the discrete eigen-decomposition of Kin\nSection 5.8.2 to estimates of the eigenfunctions of K.\nEx. 5.18 The wavelet function \u03c8(x) of the symmlet- pwavelet basis has\nvanishing moments up to order p. Show that this implies that polynomials\nof order pare represented exactly in V0, de\ufb01ned on page 176.\nEx. 5.19 Show that the Haar wavelet transform of a signal of length N= 2J\ncan be computed in O(N) computations.\nAppendix: Computations for Splines\nIn this Appendix, we describe the B-spline basis for representing polyno-\nmial splines. We also discuss their use in the computations of smoothing\nsplines.\nB-splines\nBefore we can get started, we need to augment the knot sequence de\ufb01ned\nin Section 5.2. Let \u03be0< \u03be1and\u03beK< \u03beK+1be two boundary knots, which\ntypically de\ufb01ne the domain over which we wish to evaluate our spline. We\nnow de\ufb01ne the augmented knot sequence \u03c4such that\n\u2022\u03c41\u2264\u03c42\u2264 \u2264\u2264\u2264 \u2264 \u03c4M\u2264\u03be0;\n\u2022\u03c4j+M=\u03bej, j= 1,\u2264\u2264\u2264,K;\n\u2022\u03beK+1\u2264\u03c4K+M+1\u2264\u03c4K+M+2\u2264 \u2264\u2264\u2264 \u2264 \u03c4K+2M.\nThe actual values of these additional knots beyond the boundary are arbi-\ntrary, and it is customary to make them all the same and equal to \u03be0and\n\u03beK+1, respectively.\nDenote by Bi,m(x) the ithB-spline basis function of order mfor the\nknot-sequence \u03c4,m\u2264M. They are de\ufb01ned recursively in terms of divided", "205": "Appendix: Computations for Splines 187\ndi\ufb00erences as follows:\nBi,1(x) =/braceleftbigg\n1 if\u03c4i\u2264x < \u03c4 i+1\n0 otherwise(5.77)\nfori= 1,... ,K + 2M\u22121. These are also known as Haar basis functions.\nBi,m(x) =x\u2212\u03c4i\n\u03c4i+m\u22121\u2212\u03c4iBi,m\u22121(x) +\u03c4i+m\u2212x\n\u03c4i+m\u2212\u03c4i+1Bi+1,m\u22121(x)\nfori= 1,... ,K + 2M\u2212m.\n(5.78)\nThus with M= 4,Bi,4, i= 1,\u2264\u2264\u2264,K+ 4 are the K+ 4 cubic B-spline\nbasis functions for the knot sequence \u03be. This recursion can be contin-\nued and will generate the B-spline basis for any order spline. Figure 5.20\nshows the sequence of B-splines up to order four with knots at the points\n0.0,0.1,... ,1.0. Since we have created some duplicate knots, some care\nhas to be taken to avoid division by zero. If we adopt the convention\nthatBi,1= 0 if \u03c4i=\u03c4i+1, then by induction Bi,m= 0 if \u03c4i=\u03c4i+1=\n...=\u03c4i+m. Note also that in the construction above, only the subset\nBi,m, i=M\u2212m+ 1,... ,M +Kare required for the B-spline basis\nof order m < M with knots \u03be.\nTo fully understand the properties of these functions, and to show that\nthey do indeed span the space of cubic splines for the knot sequence, re-\nquires additional mathematical machinery, including the properties of di-\nvided di\ufb00erences. Exercise 5.2 explores these issues.\nThe scope of B-splines is in fact bigger than advertised here, and has to\ndo with knot duplication. If we duplicate an interior knot in the construc-\ntion of the \u03c4sequence above, and then generate the B-spline sequence as\nbefore, the resulting basis spans the space of piecewise polynomials with\none less continuous derivative at the duplicated knot. In general, if in ad-\ndition to the repeated boundary knots, we include the interior knot \u03bej\n1\u2264rj\u2264Mtimes, then the lowest-order derivative to be discontinuous\natx=\u03bejwill be order M\u2212rj. Thus for cubic splines with no repeats,\nrj= 1, j= 1,... ,K , and at each interior knot the third derivatives (4 \u22121)\nare discontinuous. Repeating the jth knot three times leads to a discontin-\nuous 1st derivative; repeating it four times leads to a discontinuous zeroth\nderivative, i.e., the function is discontinuous at x=\u03bej. This is exactly what\nhappens at the boundary knots; we repeat the knots Mtimes, so the spline\nbecomes discontinuous at the boundary knots (i.e., unde\ufb01ned beyond the\nboundary).\nThe local support of B-splines has important computational implica-\ntions, especially when the number of knots Kis large. Least squares com-\nputations with Nobservations and K+Mvariables (basis functions) take\nO(N(K+M)2+ (K+M)3) \ufb02ops (\ufb02oating point operations.) If Kis some\nappreciable fraction of N, this leads to O(N3) algorithms which becomes", "206": "188 5. Basis Expansions and Regularization\nB-splines of Order 1\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\nB-splines of Order 2\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\nB-splines of Order 3\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\nB-splines of Order 4\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\nFIGURE 5.20. The sequence of B-splines up to order four with ten knots evenly\nspaced from 0to1. The B-splines have local support ; they are nonzero on an\ninterval spanned by M+ 1knots.", "207": "Appendix: Computations for Splines 189\nunacceptable for large N. If the Nobservations are sorted, the N\u00d7(K+M)\nregression matrix consisting of the K+M B-spline basis functions evalu-\nated at the Npoints has many zeros, which can be exploited to reduce the\ncomputational complexity back to O(N). We take this up further in the\nnext section.\nComputations for Smoothing Splines\nAlthough natural splines (Section 5.2.1) provide a basis for smoothing\nsplines, it is computationally more convenient to operate in the larger space\nof unconstrained B-splines. We write f(x) =/summationtextN+4\n1\u03b3jBj(x), where \u03b3jare\ncoe\ufb03cients and the Bjare the cubic B-spline basis functions. The solution\nlooks the same as before,\n\u02c6\u03b3= (BTB+\u03bb\u2126B)\u22121BTy, (5.79)\nexcept now the N\u00d7Nmatrix Nis replaced by the N\u00d7(N+ 4) matrix\nB, and similarly the ( N+ 4)\u00d7(N+ 4) penalty matrix \u2126Breplaces the\nN\u00d7Ndimensional \u2126N. Although at face value it seems that there are\nno boundary derivative constraints, it turns out that the penalty term\nautomatically imposes them by giving e\ufb00ectively in\ufb01nite weight to any non\nzero derivative beyond the boundary. In practice, \u02c6 \u03b3is restricted to a linear\nsubspace for which the penalty is always \ufb01nite.\nSince the columns of Bare the evaluated B-splines, in order from left\nto right and evaluated at the sorted values of X, and the cubic B-splines\nhave local support, Bis lower 4-banded. Consequently the matrix M=\n(BTB+\u03bb\u2126) is 4-banded and hence its Cholesky decomposition M=LLT\ncan be computed easily. One then solves LLT\u03b3=BTyby back-substitution\nto give \u03b3and hence the solution \u02c6finO(N) operations.\nIn practice, when Nis large, it is unnecessary to use all Ninterior knots,\nand any reasonable thinning strategy will save in computations and have\nnegligible e\ufb00ect on the \ufb01t. For example, the smooth.spline function in S-\nPLUS uses an approximately logarithmic strategy: if N <50 all knots are\nincluded, but even at N= 5,000 only 204 knots are used.", "208": "190 5. Basis Expansions and Regularization", "209": "This is page 191\nPrinter: Opaque this\n6\nKernel Smoothing Methods\nIn this chapter we describe a class of regression techniques that achieve\n\ufb02exibility in estimating the regression function f(X) over the domain IRp\nby \ufb01tting a di\ufb00erent but simple model separately at each query point x0.\nThis is done by using only those observations close to the target point x0to\n\ufb01t the simple model, and in such a way that the resulting estimated function\n\u02c6f(X) issmooth in IRp. This localization is achieved via a weighting function\norkernel K\u03bb(x0,xi), which assigns a weight to xibased on its distance from\nx0. The kernels K\u03bbare typically indexed by a parameter \u03bbthat dictates\nthe width of the neighborhood. These memory-based methods require in\nprinciple little or no training; all the work gets done at evaluation time.\nThe only parameter that needs to be determined from the training data is\n\u03bb. The model, however, is the entire training data set.\nWe also discuss more general classes of kernel-based techniques , which\ntie in with structured methods in other chapters, and are useful for density\nestimation and classi\ufb01cation.\nThe techniques in this chapter should not be confused with those asso-\nciated with the more recent usage of the phrase \u201ckernel methods\u201d. In this\nchapter kernels are mostly used as a device for localization. We discuss ker-\nnel methods in Sections 5.8, 14.5.4, 18.5 and Chapter 12; in those contexts\nthe kernel computes an inner product in a high-dimensional (implicit) fea-\nture space, and is used for regularized nonlinear modeling. We make some\nconnections to the methodology in this chapter at the end of Section 6.7.", "210": "192 6. Kernel Smoothing Methods\nNearest-Neighbor Kernel\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\nOOOO\nOO\nOO\nOOOO\nOO\nOO\nOOOOO\nOO\nOO\nO\nOO\nO\nOO\nOO\nO\nO\nOO\nO\nOO\nOO\nO\nOO\nO\nOOO\nOO\nOOOO\nOO\nOO\nOO\nOO\nOO\nOO\nOOO\nO\nOO\nOOO\nOOO\nO\nOO\nO\nOO\nO\nOOO\nO\nO\nOO\nOO\nO\nOOOO\nO\nOO\nOO\nO\nOO\nO\nOOO\nOO\nOOOO\nOO\nOO\nOO\nOO\nOO\nOO\nOO\u2022\nx0\u02c6f(x0)Epanechnikov Kernel\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\nOOOO\nOO\nOO\nOOOO\nOO\nOO\nOOOOO\nOO\nOO\nO\nOO\nO\nOO\nOO\nO\nO\nOO\nO\nOO\nOO\nO\nOO\nO\nOOO\nOO\nOOOO\nOO\nOO\nOO\nOO\nOO\nOO\nOOO\nO\nOO\nOOO\nOOO\nO\nOO\nO\nOO\nO\nOOO\nO\nO\nOO\nOO\nO\nOOO\nOO\nOO\nO\nO\nOO\nO\nOO\nOO\nO\nOO\nO\nOOO\nOO\nOOOO\nOO\nOO\nOO\nOO\nOO\nOO\nOOO\nO\nOO\u2022\nx0\u02c6f(x0)\nFIGURE 6.1. In each panel 100pairs xi, yiare generated at random from the\nblue curve with Gaussian errors: Y= sin(4 X)+\u03b5,X\u223cU[0,1],\u03b5\u223cN(0,1/3). In\nthe left panel the green curve is the result of a 30-nearest-neighbor running-mean\nsmoother. The red point is the \ufb01tted constant \u02c6f(x0), and the red circles indicate\nthose observations contributing to the \ufb01t at x0. The solid yellow region indicates\nthe weights assigned to observations. In the right panel, the gr een curve is the\nkernel-weighted average, using an Epanechnikov kernel with (hal f) window width\n\u03bb= 0.2.\n6.1 One-Dimensional Kernel Smoothers\nIn Chapter 2, we motivated the k\u2013nearest-neighbor average\n\u02c6f(x) = Ave( yi|xi\u2208Nk(x)) (6.1)\nas an estimate of the regression function E( Y|X=x). Here Nk(x) is the set\nofkpoints nearest to xin squared distance, and Ave denotes the average\n(mean). The idea is to relax the de\ufb01nition of conditional expectation, as\nillustrated in the left panel of Figure 6.1, and compute an average in a\nneighborhood of the target point. In this case we have used the 30-nearest\nneighborhood\u2014the \ufb01t at x0is the average of the 30 pairs whose xivalues\nare closest to x0. The green curve is traced out as we apply this de\ufb01nition\nat di\ufb00erent values x0. The green curve is bumpy, since \u02c6f(x) is discontinuous\ninx. As we move x0from left to right, the k-nearest neighborhood remains\nconstant, until a point xito the right of x0becomes closer than the furthest\npoint xi\u2032in the neighborhood to the left of x0, at which time xireplaces xi\u2032.\nThe average in (6.1) changes in a discrete way, leading to a discontinuous\n\u02c6f(x).\nThis discontinuity is ugly and unnecessary. Rather than give all the\npoints in the neighborhood equal weight, we can assign weights that die\no\ufb00 smoothly with distance from the target point. The right panel shows\nan example of this, using the so-called Nadaraya\u2013Watson kernel-weighted", "211": "6.1 One-Dimensional Kernel Smoothers 193\naverage\n\u02c6f(x0) =/summationtextN\ni=1K\u03bb(x0,xi)yi/summationtextN\ni=1K\u03bb(x0,xi), (6.2)\nwith the Epanechnikov quadratic kernel\nK\u03bb(x0,x) =D/parenleftbigg|x\u2212x0|\n\u03bb/parenrightbigg\n, (6.3)\nwith\nD(t) =/braceleftbigg3\n4(1\u2212t2) if|t| \u22641;\n0 otherwise .(6.4)\nThe \ufb01tted function is now continuous, and quite smooth in the right panel\nof Figure 6.1. As we move the target from left to right, points enter t he\nneighborhood initially with weight zero, and then their contribution slowly\nincreases (see Exercise 6.1).\nIn the right panel we used a metric window size \u03bb= 0.2 for the kernel\n\ufb01t, which does not change as we move the target point x0, while the size\nof the 30-nearest-neighbor smoothing window adapts to the local density\nof the xi. One can, however, also use such adaptive neighborhoods with\nkernels, but we need to use a more general notation. Let h\u03bb(x0) be a width\nfunction (indexed by \u03bb) that determines the width of the neighborhood at\nx0. Then more generally we have\nK\u03bb(x0,x) =D/parenleftbigg|x\u2212x0|\nh\u03bb(x0)/parenrightbigg\n. (6.5)\nIn (6.3), h\u03bb(x0) =\u03bbis constant. For k-nearest neighborhoods, the neigh-\nborhood size kreplaces \u03bb, and we have hk(x0) =|x0\u2212x[k]|where x[k]is\nthekth closest xitox0.\nThere are a number of details that one has to attend to in practice:\n\u2022The smoothing parameter \u03bb, which determines the width of the local\nneighborhood, has to be determined. Large \u03bbimplies lower variance\n(averages over more observations) but higher bias (we essentially as-\nsume the true function is constant within the window).\n\u2022Metric window widths (constant h\u03bb(x)) tend to keep the bias of the\nestimate constant, but the variance is inversely proportional to the\nlocal density. Nearest-neighbor window widths exhibit the opposite\nbehavior; the variance stays constant and the absolute bias varies\ninversely with local density.\n\u2022Issues arise with nearest-neighbors when there are ties in the xi. With\nmost smoothing techniques one can simply reduce the data set by\naveraging the yiat tied values of X, and supplementing these new\nobservations at the unique values of xiwith an additional weight wi\n(which multiples the kernel weight).", "212": "194 6. Kernel Smoothing Methods\n-3 -2 -1 0 1 2 30.0 0.4 0.8Epanechnikov\nTri-cube\nGaussianK\u03bb(x0, x)\nFIGURE 6.2. A comparison of three popular kernels for local smoothing. Eac h\nhas been calibrated to integrate to 1. The tri-cube kernel is compact and has two\ncontinuous derivatives at the boundary of its support, while th e Epanechnikov ker-\nnel has none. The Gaussian kernel is continuously di\ufb00erentiable, bu t has in\ufb01nite\nsupport.\n\u2022This leaves a more general problem to deal with: observation weights\nwi. Operationally we simply multiply them by the kernel weights be-\nfore computing the weighted average. With nearest neighborhoods, it\nis now natural to insist on neighborhoods with a total weight content\nk(relative to/summationtextwi). In the event of over\ufb02ow (the last observation\nneeded in a neighborhood has a weight wjwhich causes the sum of\nweights to exceed the budget k), then fractional parts can be used.\n\u2022Boundary issues arise. The metric neighborhoods tend to contain less\npoints on the boundaries, while the nearest-neighborhoods get wider.\n\u2022The Epanechnikov kernel has compact support (needed when used\nwith nearest-neighbor window size). Another popular compact kernel\nis based on the tri-cube function\nD(t) =/braceleftbigg\n(1\u2212 |t|3)3if|t| \u22641;\n0 otherwise(6.6)\nThis is \ufb02atter on the top (like the nearest-neighbor box) and is di\ufb00er-\nentiable at the boundary of its support. The Gaussian density func-\ntionD(t) =\u03c6(t) is a popular noncompact kernel, with the standard-\ndeviation playing the role of the window size. Figure 6.2 compares\nthe three.\n6.1.1 Local Linear Regression\nWe have progressed from the raw moving average to a smoothly varying\nlocally weighted average by using kernel weighting. The smooth kernel \ufb01t\nstill has problems, however, as exhibited in Figure 6.3 (left panel). Locally-\nweighted averages can be badly biased on the boundaries of the domain,", "213": "6.1 One-Dimensional Kernel Smoothers 195\nN-W Kernel at Boundary\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\nOO\nOO\nOO\nOO\nOO\nOOOOO\nOOO\nOOO\nOOO\nO\nOO\nOO\nO\nOO\nOOO\nOOOO\nO\nOO\nOO\nO\nOO\nOO\nOOOO\nOO\nOOOO\nOOO\nOO\nOO\nO\nOOO\nOOOO\nO\nO\nOO\nOOO\nOO\nO\nOOO\nOO\nO\nOO\nO\nOOO\nO\nOOO\nOO\nOO\nOO\nOO\nOO\nOOOOO\nOOO\nOOO\nOOO\nO\nOO\nOO\n\u2022\nx0\u02c6f(x0)Local Linear Regression at Boundary\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\nOO\nOO\nOO\nOO\nOO\nOOOOO\nOOO\nOOO\nOOO\nO\nOO\nOO\nO\nOO\nOOO\nOOOO\nO\nOO\nOO\nO\nOO\nOO\nOOOO\nOO\nOOOO\nOOO\nOO\nOO\nO\nOOO\nOOOO\nO\nO\nOO\nOOO\nOO\nO\nOOO\nOO\nO\nOO\nO\nOOO\nO\nOOO\nOO\nOO\nOO\nOO\nOO\nOOOOO\nOOO\nOOO\nOOO\nO\nOO\nOO\n\u2022\nx0\u02c6f(x0)\nFIGURE 6.3. The locally weighted average has bias problems at or near the\nboundaries of the domain. The true function is approximately line ar here, but\nmost of the observations in the neighborhood have a higher mean than the target\npoint, so despite weighting, their mean will be biased upwards . By \ufb01tting a locally\nweighted linear regression (right panel), this bias is remove d to \ufb01rst order\nbecause of the asymmetry of the kernel in that region. By \ufb01tting straight\nlines rather than constants locally, we can remove this bias exactly to \ufb01rst\norder; see Figure 6.3 (right panel). Actually, this bias can be present in the\ninterior of the domain as well, if the Xvalues are not equally spaced (for\nthe same reasons, but usually less severe). Again locally weighted linear\nregression will make a \ufb01rst-order correction.\nLocally weighted regression solves a separate weighted least squares prob-\nlem at each target point x0:\nmin\n\u03b1(x0),\u03b2(x0)N/summationdisplay\ni=1K\u03bb(x0,xi)[yi\u2212\u03b1(x0)\u2212\u03b2(x0)xi]2. (6.7)\nThe estimate is then \u02c6f(x0) = \u02c6\u03b1(x0) +\u02c6\u03b2(x0)x0. Notice that although we \ufb01t\nan entire linear model to the data in the region, we only use it to evaluate\nthe \ufb01t at the single point x0.\nDe\ufb01ne the vector-valued function b(x)T= (1,x). Let Bbe the N\u00d72\nregression matrix with ith row b(xi)T, andW(x0) the N\u00d7Ndiagonal\nmatrix with ith diagonal element K\u03bb(x0,xi). Then\n\u02c6f(x0) = b(x0)T(BTW(x0)B)\u22121BTW(x0)y (6.8)\n=N/summationdisplay\ni=1li(x0)yi. (6.9)\nEquation (6.8) gives an explicit expression for the local linear regression\nestimate, and (6.9) highlights the fact that the estimate is linear in the", "214": "196 6. Kernel Smoothing Methods\nLocal Linear Equivalent Kernel at Boundary\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\nOO\nOO\nOOO\nOO\nOOO\nOOO\nO\nOOOOOOO\nOO\nOO\nOO\nOO\nO\nOO\nOOOOO\nOO\nOO\nOOOO\nOO\nO\nOO\nO\nOO\nOO\nOO\nO\nOO\nOOO\nOOO\nOOOO\nOO\nO\nOOO\nOOOOO\nO\nOO\nOO\nOOOO\nOO\nOOO\nOOO\nOO\nOO\nOOO\nOO\nOOO\nOOO\nO\nOOOOOO\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\nx0\u02c6f(x0)Local Linear Equivalent Kernel in Interior\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\nOO\nOO\nOOO\nOO\nOOO\nOOO\nO\nOOOOOOO\nOO\nOO\nOO\nOO\nO\nOO\nOOOOO\nOO\nOO\nOOOO\nOO\nO\nOO\nO\nOO\nOO\nOO\nO\nOO\nOOO\nOOO\nOOOO\nOO\nO\nOOO\nOOOOO\nO\nOO\nOO\nOOOO\nOO\nOOO\nOO\nOO\nOO\nOO\nO\nOO\nOOOOO\nOO\nOO\nOOOO\nOO\nO\nOO\nO\nOO\nOO\nOO\nO\nOO\nOOO\nOOO\u2022\n\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\nx0\u02c6f(x0)\nFIGURE 6.4. The green points show the equivalent kernel li(x0)for local re-\ngression. These are the weights in \u02c6f(x0) =PN\ni=1li(x0)yi, plotted against their\ncorresponding xi. For display purposes, these have been rescaled, since in fac t\nthey sum to 1. Since the yellow shaded region is the (rescaled) equivalent ke rnel\nfor the Nadaraya\u2013Watson local average, we see how local regr ession automati-\ncally modi\ufb01es the weighting kernel to correct for biases due to a symmetry in the\nsmoothing window.\nyi(theli(x0) do not involve y). These weights li(x0) combine the weight-\ning kernel K\u03bb(x0,\u2264) and the least squares operations, and are sometimes\nreferred to as the equivalent kernel . Figure 6.4 illustrates the e\ufb00ect of lo-\ncal linear regression on the equivalent kernel. Historically, the bias in the\nNadaraya\u2013Watson and other local average kernel methods were corrected\nby modifying the kernel. These modi\ufb01cations were based on theoretical\nasymptotic mean-square-error considerations, and besides being tedious to\nimplement, are only approximate for \ufb01nite sample sizes. Local linear re-\ngression automatically modi\ufb01es the kernel to correct the bias exactly to\n\ufb01rst order, a phenomenon dubbed as automatic kernel carpentry . Consider\nthe following expansion for E \u02c6f(x0), using the linearity of local regression\nand a series expansion of the true function faround x0,\nE\u02c6f(x0) =N/summationdisplay\ni=1li(x0)f(xi)\n=f(x0)N/summationdisplay\ni=1li(x0) +f\u2032(x0)N/summationdisplay\ni=1(xi\u2212x0)li(x0)\n+f\u2032\u2032(x0)\n2N/summationdisplay\ni=1(xi\u2212x0)2li(x0) +R, (6.10)\nwhere the remainder term Rinvolves third- and higher-order derivatives of\nf, and is typically small under suitable smoothness assumptions. It can be", "215": "6.1 One-Dimensional Kernel Smoothers 197\nLocal Linear in Interior\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\nO\nOO\nOOO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOO\nOO\nO\nOO\nOOO\nOO\nOO\nO\nOOO\nOO\nOOO\nOO\nOO\nO\nOO\nO\nOOOOO\nOOO\nO\nO\nO\nOOO\nOOOO\nOOOO\nOOOOO\nOO\nOO\nOO\nO\nOO\nO\nO\nOO\nOO\nOOO\nOOO\nO\nOOO\nO\nOOO\nOO\nO\nOO\nOOO\nOO\nOO\nO\nOOO\nOO\nOOO\nOO\nOO\nO\nOO\nO\nOOOOO\nO\u2022\u02c6f(x0)Local Quadratic in Interior\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\nO\nOO\nOOO\nOOO\nO\nOOO\nOOO\nO\nOOO\nO\nOOO\nOO\nO\nOO\nOOO\nOO\nOO\nO\nOOO\nOO\nOOO\nOO\nOO\nO\nOO\nO\nOOOOO\nOOO\nO\nO\nO\nOOO\nOOOO\nOOOO\nOOOOO\nOO\nOO\nOO\nO\nOO\nO\nO\nOO\nOO\nOOO\nOOO\nO\nOOO\nO\nOOO\nOO\nO\nOO\nOOO\nOO\nOO\nO\nOOO\nOO\nOOO\nOO\nOO\nO\nOO\nO\nOOOOO\nO\u2022\u02c6f(x0)\nFIGURE 6.5. Local linear \ufb01ts exhibit bias in regions of curvature of the true\nfunction. Local quadratic \ufb01ts tend to eliminate this bias.\nshown (Exercise 6.2) that for local linear regression,/summationtextN\ni=1li(x0) = 1 and/summationtextN\ni=1(xi\u2212x0)li(x0) = 0. Hence the middle term equals f(x0), and since\nthe bias is E \u02c6f(x0)\u2212f(x0), we see that it depends only on quadratic and\nhigher\u2013order terms in the expansion of f.\n6.1.2 Local Polynomial Regression\nWhy stop at local linear \ufb01ts? We can \ufb01t local polynomial \ufb01ts of any de-\ngreed,\nmin\n\u03b1(x0),\u03b2j(x0), j=1,...,dN/summationdisplay\ni=1K\u03bb(x0,xi)\uf8ee\n\uf8f0yi\u2212\u03b1(x0)\u2212d/summationdisplay\nj=1\u03b2j(x0)xj\ni\uf8f9\n\uf8fb2\n(6.11)\nwith solution \u02c6f(x0) = \u02c6\u03b1(x0)+/summationtextd\nj=1\u02c6\u03b2j(x0)xj\n0. In fact, an expansion such as\n(6.10) will tell us that the bias will only have components of degree d+1 and\nhigher (Exercise 6.2). Figure 6.5 illustrates local quadratic regression. Local\nlinear \ufb01ts tend to be biased in regions of curvature of the true function, a\nphenomenon referred to as trimming the hills and\ufb01lling the valleys . Local\nquadratic regression is generally able to correct this bias.\nThere is of course a price to be paid for this bias reduction, and that is\nincreased variance. The \ufb01t in the right panel of Figure 6.5 is slightly more\nwiggly, especially in the tails. Assuming the model yi=f(xi) +\u03b5i, with\n\u03b5iindependent and identically distributed with mean zero and variance\n\u03c32, Var( \u02c6f(x0)) =\u03c32||l(x0)||2, where l(x0) is the vector of equivalent kernel\nweights at x0. It can be shown (Exercise 6.3) that ||l(x0)||increases with d,\nand so there is a bias\u2013variance tradeo\ufb00 in selecting the polynomial degree.\nFigure 6.6 illustrates these variance curves for degree zero, one and two", "216": "198 6. Kernel Smoothing Methods\nVariance\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5Constant\nLinear\nQuadratic\nFIGURE 6.6. The variances functions ||l(x)||2for local constant, linear and\nquadratic regression, for a metric bandwidth ( \u03bb= 0.2) tri-cube kernel.\nlocal polynomials. To summarize some collected wisdom on this issue:\n\u2022Local linear \ufb01ts can help bias dramatically at the boundaries at a\nmodest cost in variance. Local quadratic \ufb01ts do little at the bound-\naries for bias, but increase the variance a lot.\n\u2022Local quadratic \ufb01ts tend to be most helpful in reducing bias due to\ncurvature in the interior of the domain.\n\u2022Asymptotic analysis suggest that local polynomials of odd degree\ndominate those of even degree. This is largely due to the fact that\nasymptotically the MSE is dominated by boundary e\ufb00ects.\nWhile it may be helpful to tinker, and move from local linear \ufb01ts at the\nboundary to local quadratic \ufb01ts in the interior, we do not recommend such\nstrategies. Usually the application will dictate the degree of the \ufb01t. For\nexample, if we are interested in extrapolation, then the boundary is of\nmore interest, and local linear \ufb01ts are probably more reliable.\n6.2 Selecting the Width of the Kernel\nIn each of the kernels K\u03bb,\u03bbis a parameter that controls its width:\n\u2022For the Epanechnikov or tri-cube kernel with metric width, \u03bbis the\nradius of the support region.\n\u2022For the Gaussian kernel, \u03bbis the standard deviation.\n\u2022\u03bbis the number kof nearest neighbors in k-nearest neighborhoods,\noften expressed as a fraction or spank/Nof the total training sample.", "217": "6.2 Selecting the Width of the Kernel 199\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\nFIGURE 6.7. Equivalent kernels for a local linear regression smoother (tri -cube\nkernel; orange) and a smoothing spline (blue), with matching degre es of freedom.\nThe vertical spikes indicates the target points.\nThere is a natural bias\u2013variance tradeo\ufb00 as we change the width of the\naveraging window, which is most explicit for local averages:\n\u2022If the window is narrow, \u02c6f(x0) is an average of a small number of yi\nclose to x0, and its variance will be relatively large\u2014close to that of\nan individual yi. The bias will tend to be small, again because each\nof the E(yi) =f(xi) should be close to f(x0).\n\u2022If the window is wide, the variance of \u02c6f(x0) will be small relative to\nthe variance of any yi, because of the e\ufb00ects of averaging. The bias\nwill be higher, because we are now using observations xifurther from\nx0, and there is no guarantee that f(xi) will be close to f(x0).\nSimilar arguments apply to local regression estimates, say local linear: a s\nthe width goes to zero, the estimates approach a piecewise-linear function\nthat interpolates the training data1; as the width gets in\ufb01nitely large, the\n\ufb01t approaches the global linear least-squares \ufb01t to the data.\nThe discussion in Chapter 5 on selecting the regularization parameter for\nsmoothing splines applies here, and will not be repeated. Local regression\nsmoothers are linear estimators; the smoother matrix in \u02c6f=S\u03bbyis built up\nfrom the equivalent kernels (6.8), and has ijth entry {S\u03bb}ij=li(xj). Leave-\none-out cross-validation is particularly simple (Exercise 6.7), as is genera l-\nized cross-validation, Cp(Exercise 6.10), and k-fold cross-validation. The\ne\ufb00ective degrees of freedom is again de\ufb01ned as trace( S\u03bb), and can be used\nto calibrate the amount of smoothing. Figure 6.7 compares the equivalent\nkernels for a smoothing spline and local linear regression. The local regres-\nsion smoother has a span of 40%, which results in df = trace( S\u03bb) = 5.86.\nThe smoothing spline was calibrated to have the same df, and their equiv-\nalent kernels are qualitatively quite similar.\n1With uniformly spaced xi; with irregularly spaced xi, the behavior can deteriorate.", "218": "200 6. Kernel Smoothing Methods\n6.3 Local Regression in I Rp\nKernel smoothing and local regression generalize very naturally to two or\nmore dimensions. The Nadaraya\u2013Watson kernel smoother \ufb01ts a constant\nlocally with weights supplied by a p-dimensional kernel. Local linear re-\ngression will \ufb01t a hyperplane locally in X, by weighted least squares, with\nweights supplied by a p-dimensional kernel. It is simple to implement and\nis generally preferred to the local constant \ufb01t for its superior performance\non the boundaries.\nLetb(X) be a vector of polynomial terms in Xof maximum degree d.\nFor example, with d= 1 and p= 2 we get b(X) = (1 ,X1,X2); with d= 2\nwe get b(X) = (1 ,X1,X2,X2\n1,X2\n2,X1X2); and trivially with d= 0 we get\nb(X) = 1. At each x0\u2208IRpsolve\nmin\n\u03b2(x0)N/summationdisplay\ni=1K\u03bb(x0,xi)(yi\u2212b(xi)T\u03b2(x0))2(6.12)\nto produce the \ufb01t \u02c6f(x0) =b(x0)T\u02c6\u03b2(x0). Typically the kernel will be a radial\nfunction, such as the radial Epanechnikov or tri-cube kernel\nK\u03bb(x0,x) =D/parenleftbigg||x\u2212x0||\n\u03bb/parenrightbigg\n, (6.13)\nwhere ||\u2264||is the Euclidean norm. Since the Euclidean norm depends on the\nunits in each coordinate, it makes most sense to standardize each predictor,\nfor example, to unit standard deviation, prior to smoothing.\nWhile boundary e\ufb00ects are a problem in one-dimensional smoothing,\nthey are a much bigger problem in two or higher dimensions, since the\nfraction of points on the boundary is larger. In fact, one of the manifesta -\ntions of the curse of dimensionality is that the fraction of points close to the\nboundary increases to one as the dimension grows. Directly modifying the\nkernel to accommodate two-dimensional boundaries becomes very messy,\nespecially for irregular boundaries. Local polynomial regression seamless ly\nperforms boundary correction to the desired order in any dimensions. Fig-\nure 6.8 illustrates local linear regression on some measurements from an\nastronomical study with an unusual predictor design (star-shaped). Here\nthe boundary is extremely irregular, and the \ufb01tted surface must also inter-\npolate over regions of increasing data sparsity as we approach the boundary.\nLocal regression becomes less useful in dimensions much higher than two\nor three. We have discussed in some detail the problems of dimensional-\nity, for example, in Chapter 2. It is impossible to simultaneously main-\ntain localness ( \u21d2low bias) and a sizable sample in the neighborhood ( \u21d2\nlow variance) as the dimension increases, without the total sample size in-\ncreasing exponentially in p. Visualization of \u02c6f(X) also becomes di\ufb03cult in\nhigher dimensions, and this is often one of the primary goals of smoothing.", "219": "6.4 Structured Local Regression Models in I Rp201\nEast-WestSouth-NorthVelocity\nEast-WestSouth-NorthVelocity\nFIGURE 6.8. The left panel shows three-dimensional data, where the response\nis the velocity measurements on a galaxy, and the two predictors record positions\non the celestial sphere. The unusual \u201cstar\u201d-shaped design ind icates the way the\nmeasurements were made, and results in an extremely irregular b oundary. The\nright panel shows the results of local linear regression smoot hing in I R2, using a\nnearest-neighbor window with 15%of the data.\nAlthough the scatter-cloud and wire-frame pictures in Figure 6.8 look at-\ntractive, it is quite di\ufb03cult to interpret the results except at a gross level.\nFrom a data analysis perspective, conditional plots are far more useful.\nFigure 6.9 shows an analysis of some environmental data with three pre-\ndictors. The trellis display here shows ozone as a function of radiation,\nconditioned on the other two variables, temperature and wind speed. How-\never, conditioning on the value of a variable really implies local to that\nvalue (as in local regression). Above each of the panels in Figure 6.9 is an\nindication of the range of values present in that panel for each of the condi-\ntioning values. In the panel itself the data subsets are displayed (response\nversus remaining variable), and a one-dimensional local linear regression is\n\ufb01t to the data. Although this is not quite the same as looking at slices of\na \ufb01tted three-dimensional surface, it is probably more useful in terms of\nunderstanding the joint behavior of the data.\n6.4 Structured Local Regression Models in I Rp\nWhen the dimension to sample-size ratio is unfavorable, local regression\ndoes not help us much, unless we are willing to make some structural as-\nsumptions about the model. Much of this book is about structured regres-\nsion and classi\ufb01cation models. Here we focus on some approaches directly\nrelated to kernel methods.", "220": "202 6. Kernel Smoothing Methods\n12345TempWind\n0 50 150 250TempWind\nTempWind\n0 50 150 250TempWindTempWind\nTempWind\nTempWind\n12345TempWind12345TempWind\nTempWind\nTempWind\nTempWindTempWind\nTempWind0 50 150 250\nTempWind\n12345TempWind0 50 150 250\nSolar Radiation (langleys)Cube Root Ozone (cube root ppb)\nFIGURE 6.9. Three-dimensional smoothing example. The response is (cube-roo t\nof) ozone concentration, and the three predictors are temperatur e, wind speed and\nradiation. The trellis display shows ozone as a function of radiation, conditioned\non intervals of temperature and wind speed (indicated by darker g reen or orange\nshaded bars). Each panel contains about 40%of the range of each of the condi-\ntioned variables. The curve in each panel is a univariate local l inear regression,\n\ufb01t to the data in the panel.", "221": "6.4 Structured Local Regression Models in I Rp203\n6.4.1 Structured Kernels\nOne line of approach is to modify the kernel. The default spherical ker-\nnel (6.13) gives equal weight to each coordinate, and so a natural default\nstrategy is to standardize each variable to unit standard deviation. A more\ngeneral approach is to use a positive semide\ufb01nite matrix Ato weigh the\ndi\ufb00erent coordinates:\nK\u03bb,A(x0,x) =D/parenleftbigg(x\u2212x0)TA(x\u2212x0)\n\u03bb/parenrightbigg\n. (6.14)\nEntire coordinates or directions can be downgraded or omitted by imposing\nappropriate restrictions on A. For example, if Ais diagonal, then we can\nincrease or decrease the in\ufb02uence of individual predictors Xjby increasing\nor decreasing Ajj. Often the predictors are many and highly correlated,\nsuch as those arising from digitized analog signals or images. The covari ance\nfunction of the predictors can be used to tailor a metric Athat focuses less,\nsay, on high-frequency contrasts (Exercise 6.4). Proposals have been made\nfor learning the parameters for multidimensional kernels. For example, the\nprojection-pursuit regression model discussed in Chapter 11 is of this \ufb02avor,\nwhere low-rank versions of Aimply ridge functions for \u02c6f(X). More general\nmodels for Aare cumbersome, and we favor instead the structured forms\nfor the regression function discussed next.\n6.4.2 Structured Regression Functions\nWe are trying to \ufb01t a regression function E(Y|X) =f(X1,X2,... ,X p) in\nIRp, in which every level of interaction is potentially present. It is natural\nto consider analysis-of-variance (ANOVA) decompositions of the form\nf(X1,X2,... ,X p) =\u03b1+/summationdisplay\njgj(Xj) +/summationdisplay\nk<\u2113gk\u2113(Xk,X\u2113) +\u2264\u2264\u2264 (6.15)\nand then introduce structure by eliminating some of the higher-order terms.\nAdditive models assume only main e\ufb00ect terms: f(X) =\u03b1+/summationtextp\nj=1gj(Xj);\nsecond-order models will have terms with interactions of order at most\ntwo, and so on. In Chapter 9, we describe iterative back\ufb01tting algorithms\nfor \ufb01tting such low-order interaction models. In the additive model, for\nexample, if all but the kth term is assumed known, then we can estimate gk\nby local regression of Y\u2212/summationtext\nj/ne}ationslash=kgj(Xj) onXk. This is done for each function\nin turn, repeatedly, until convergence. The important detail is that at any\nstage, one-dimensional local regression is all that is needed. The same ideas\ncan be used to \ufb01t low-dimensional ANOVA decompositions.\nAn important special case of these structured models are the class of\nvarying coe\ufb03cient models . Suppose, for example, that we divide the ppre-\ndictors in Xinto a set ( X1,X2,... ,X q) with q < p, and the remainder of", "222": "204 6. Kernel Smoothing Methods\n1012141618202224DepthFemale\n20 30 40 50 60DepthFemale\nDepthFemale\n20 30 40 50 60DepthFemale\nDepthFemale\n20 30 40 50 60DepthFemaleDepthMale\nDepthMale20 30 40 50 60\nDepthMale\nDepthMale20 30 40 50 60\nDepthMale\n1012141618202224DepthMale20 30 40 50 60\nAgeDiameterAortic Diameter vs Age\nFIGURE 6.10. In each panel the aorta diameter is modeled as a linear func-\ntion ofage. The coe\ufb03cients of this model vary with gender anddepth down\ntheaorta (left is near the top, right is low down). There is a clear trend in the\ncoe\ufb03cients of the linear model.\nthe variables we collect in the vector Z. We then assume the conditionally\nlinear model\nf(X) =\u03b1(Z) +\u03b21(Z)X1+\u2264\u2264\u2264+\u03b2q(Z)Xq. (6.16)\nFor given Z, this is a linear model, but each of the coe\ufb03cients can vary\nwithZ. It is natural to \ufb01t such a model by locally weighted least squares:\nmin\n\u03b1(z0),\u03b2(z0)N/summationdisplay\ni=1K\u03bb(z0,zi)(yi\u2212\u03b1(z0)\u2212x1i\u03b21(z0)\u2212 \u2264\u2264\u2264 \u2212 xqi\u03b2q(z0))2.\n(6.17)\nFigure 6.10 illustrates the idea on measurements of the human aorta.\nA longstanding claim has been that the aorta thickens with age. Here we\nmodel the diameter of the aorta as a linear function of age, but allow the\ncoe\ufb03cients to vary with gender anddepth down the aorta. We used a local\nregression model separately for males and females. While the aorta clearly\ndoes thicken with age at the higher regions of the aorta, the relationship\nfades with distance down the aorta. Figure 6.11 shows the intercept and\nslope as a function of depth.", "223": "6.5 Local Likelihood and Other Models 205\nMaleAge Intercept\nDistance Down AortaAge Slope\n0.0 0.2 0.4 0.6 0.8 1.0Female\n14 16 18 20\nDistance Down Aorta\n0.0 0.4 0.8 1.2\n0.0 0.2 0.4 0.6 0.8 1.0\nFIGURE 6.11. The intercept and slope of ageas a function of distance down\nthe aorta, separately for males and females. The yellow bands i ndicate one stan-\ndard error.\n6.5 Local Likelihood and Other Models\nThe concept of local regression and varying coe\ufb03cient models is extremely\nbroad: any parametric model can be made local if the \ufb01tting method ac-\ncommodates observation weights. Here are some examples:\n\u2022Associated with each observation yiis a parameter \u03b8i=\u03b8(xi) =xT\ni\u03b2\nlinear in the covariate(s) xi, and inference for \u03b2is based on the log-\nlikelihood l(\u03b2) =/summationtextN\ni=1l(yi,xT\ni\u03b2). We can model \u03b8(X) more \ufb02exibly\nby using the likelihood local to x0for inference of \u03b8(x0) =xT\n0\u03b2(x0):\nl(\u03b2(x0)) =N/summationdisplay\ni=1K\u03bb(x0,xi)l(yi,xT\ni\u03b2(x0)).\nMany likelihood models, in particular the family of generalized linear\nmodels including logistic and log-linear models, involve the covariates\nin a linear fashion. Local likelihood allows a relaxation from a global ly\nlinear model to one that is locally linear.", "224": "206 6. Kernel Smoothing Methods\n\u2022As above, except di\ufb00erent variables are associated with \u03b8from those\nused for de\ufb01ning the local likelihood:\nl(\u03b8(z0)) =N/summationdisplay\ni=1K\u03bb(z0,zi)l(yi,\u03b7(xi,\u03b8(z0))).\nFor example, \u03b7(x,\u03b8) =xT\u03b8could be a linear model in x. This will \ufb01t\na varying coe\ufb03cient model \u03b8(z) by maximizing the local likelihood.\n\u2022Autoregressive time series models of order khave the form yt=\n\u03b20+\u03b21yt\u22121+\u03b22yt\u22122+\u2264\u2264\u2264+\u03b2kyt\u2212k+\u03b5t. Denoting the lag set by\nzt= (yt\u22121,yt\u22122,... ,y t\u2212k), the model looks like a standard linear\nmodel yt=zT\nt\u03b2+\u03b5t, and is typically \ufb01t by least squares. Fitting\nby local least squares with a kernel K(z0,zt) allows the model to\nvary according to the short-term history of the series. This is to be\ndistinguished from the more traditional dynamic linear models that\nvary by windowing time.\nAs an illustration of local likelihood, we consider the local version of the\nmulticlass linear logistic regression model (4.36) of Chapter 4. The data\nconsist of features xiand an associated categorical response gi\u2208 {1,2,... ,J },\nand the linear model has the form\nPr(G=j|X=x) =e\u03b2j0+\u03b2T\njx\n1 +/summationtextJ\u22121\nk=1e\u03b2k0+\u03b2T\nkx. (6.18)\nThe local log-likelihood for this Jclass model can be written\nN/summationdisplay\ni=1K\u03bb(x0,xi)/braceleft\uf8ecigg\n\u03b2gi0(x0) +\u03b2gi(x0)T(xi\u2212x0)\n\u2212log/bracketleft\uf8ecigg\n1 +J\u22121/summationdisplay\nk=1exp/parenleftbig\n\u03b2k0(x0) +\u03b2k(x0)T(xi\u2212x0)/parenrightbig/bracketright\uf8ecigg/braceright\uf8ecigg\n.\n(6.19)\nNotice that\n\u2022we have used gias a subscript in the \ufb01rst line to pick out the appro-\npriate numerator;\n\u2022\u03b2J0= 0 and \u03b2J= 0 by the de\ufb01nition of the model;\n\u2022we have centered the local regressions at x0, so that the \ufb01tted poste-\nrior probabilities at x0are simply\n\u02c6Pr(G=j|X=x0) =e\u02c6\u03b2j0(x0)\n1 +/summationtextJ\u22121\nk=1e\u02c6\u03b2k0(x0). (6.20)", "225": "6.5 Local Likelihood and Other Models 207\nSystolic Blood PressurePrevalence CHD\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\nObesityPrevalence CHD\n15 25 35 450.0 0.2 0.4 0.6 0.8 1.0\nFIGURE 6.12. Each plot shows the binary response CHD (coronary heart dis-\nease) as a function of a risk factor for the South African heart d isease data.\nFor each plot we have computed the \ufb01tted prevalence of CHD using a local linear\nlogistic regression model. The unexpected increase in the prev alence of CHD at\nthe lower ends of the ranges is because these are retrospective data, and some of\nthe subjects had already undergone treatment to reduce their bl ood pressure and\nweight. The shaded region in the plot indicates an estimated p ointwise standard\nerror band.\nThis model can be used for \ufb02exible multiclass classi\ufb01cation in moderately\nlow dimensions, although successes have been reported with the high-\ndimensional ZIP-code classi\ufb01cation problem. Generalized additive models\n(Chapter 9) using kernel smoothing methods are closely related, and avoid\ndimensionality problems by assuming an additive structure for the regres-\nsion function.\nAs a simple illustration we \ufb01t a two-class local linear logistic model to\nthe heart disease data of Chapter 4. Figure 6.12 shows the univariate local\nlogistic models \ufb01t to two of the risk factors (separately). This is a useful\nscreening device for detecting nonlinearities, when the data themselves have\nlittle visual information to o\ufb00er. In this case an unexpected anomaly is\nuncovered in the data, which may have gone unnoticed with traditional\nmethods.\nSinceCHDis a binary indicator, we could estimate the conditional preva-\nlence Pr( G=j|x0) by simply smoothing this binary response directly with-\nout resorting to a likelihood formulation. This amounts to \ufb01tting a locall y\nconstant logistic regression model (Exercise 6.5). In order to enjoy the bia s-\ncorrection of local-linear smoothing, it is more natural to operate on the\nunrestricted logit scale.\nTypically with logistic regression, we compute parameter estimates as\nwell as their standard errors. This can be done locally as well, and so", "226": "208 6. Kernel Smoothing Methods\nSystolic Blood Pressure (for CHD group)Density Estimate\n100 120 140 160 180 200 2200.0 0.005 0.010 0.015 0.020\nFIGURE 6.13. A kernel density estimate for systolic blood pressure (for the\nCHD group). The density estimate at each point is the average co ntribution from\neach of the kernels at that point. We have scaled the kernels down by a factor of\n10 to make the graph readable.\nwe can produce, as shown in the plot, estimated pointwise standard-error\nbands about our \ufb01tted prevalence.\n6.6 Kernel Density Estimation and Classi\ufb01cation\nKernel density estimation is an unsupervised learning procedure, which\nhistorically precedes kernel regression. It also leads naturally to a simple\nfamily of procedures for nonparametric classi\ufb01cation.\n6.6.1 Kernel Density Estimation\nSuppose we have a random sample x1,... ,x Ndrawn from a probability\ndensity fX(x), and we wish to estimate fXat a point x0. For simplicity we\nassume for now that X\u2208IR. Arguing as before, a natural local estimate\nhas the form\n\u02c6fX(x0) =#xi\u2208 N(x0)\nN\u03bb, (6.21)\nwhere N(x0) is a small metric neighborhood around x0of width \u03bb. This\nestimate is bumpy, and the smooth Parzen estimate is preferred\n\u02c6fX(x0) =1\nN\u03bbN/summationdisplay\ni=1K\u03bb(x0,xi), (6.22)", "227": "6.6 Kernel Density Estimation and Classi\ufb01cation 209\nSystolic Blood PressureDensity Estimates\n100 140 180 2200.0 0.010 0.020CHD\nno CHD\nSystolic Blood PressurePosterior Estimate\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\nFIGURE 6.14. The left panel shows the two separate density estimates for\nsystolic blood pressure in the CHD versus no-CHD groups, using a Gaussian\nkernel density estimate in each. The right panel shows the estim ated posterior\nprobabilities for CHD, using (6.25).\nbecause it counts observations close to x0with weights that decrease with\ndistance from x0. In this case a popular choice for K\u03bbis the Gaussian kernel\nK\u03bb(x0,x) =\u03c6(|x\u2212x0|/\u03bb). Figure 6.13 shows a Gaussian kernel density \ufb01t\nto the sample values for systolic blood pressure for theCHDgroup. Letting\n\u03c6\u03bbdenote the Gaussian density with mean zero and standard-deviation \u03bb,\nthen (6.22) has the form\n\u02c6fX(x) =1\nNN/summationdisplay\ni=1\u03c6\u03bb(x\u2212xi)\n= (\u02c6F \u22c6 \u03c6 \u03bb)(x), (6.23)\nthe convolution of the sample empirical distribution \u02c6Fwith\u03c6\u03bb. The dis-\ntribution \u02c6F(x) puts mass 1 /Nat each of the observed xi, and is jumpy; in\n\u02c6fX(x) we have smoothed \u02c6Fby adding independent Gaussian noise to each\nobservation xi.\nThe Parzen density estimate is the equivalent of the local average, and\nimprovements have been proposed along the lines of local regression [on the\nlog scale for densities; see Loader (1999)]. We will not pursue these here.\nIn IRpthe natural generalization of the Gaussian density estimate amounts\nto using the Gaussian product kernel in (6.23),\n\u02c6fX(x0) =1\nN(2\u03bb2\u03c0)p\n2N/summationdisplay\ni=1e\u22121\n2(||xi\u2212x0||/\u03bb)2. (6.24)", "228": "210 6. Kernel Smoothing Methods\n0.00.51.0\nFIGURE 6.15. The population class densities may have interesting structure\n(left) that disappears when the posterior probabilities ar e formed (right).\n6.6.2 Kernel Density Classi\ufb01cation\nOne can use nonparametric density estimates for classi\ufb01cation in a straight-\nforward fashion using Bayes\u2019 theorem. Suppose for a Jclass problem we \ufb01t\nnonparametric density estimates \u02c6fj(X), j= 1,... ,J separately in each of\nthe classes, and we also have estimates of the class priors \u02c6 \u03c0j(usually the\nsample proportions). Then\n\u02c6Pr(G=j|X=x0) =\u02c6\u03c0j\u02c6fj(x0)/summationtextJ\nk=1\u02c6\u03c0k\u02c6fk(x0). (6.25)\nFigure 6.14 uses this method to estimate the prevalence of CHD for the\nheart risk factor study, and should be compared with the left panel of Fig-\nure 6.12. The main di\ufb00erence occurs in the region of high SBP in the right\npanel of Figure 6.14. In this region the data are sparse for both classes, a nd\nsince the Gaussian kernel density estimates use metric kernels, the density\nestimates are low and of poor quality (high variance) in these regions. The\nlocal logistic regression method (6.20) uses the tri-cube kernel with k-NN\nbandwidth; this e\ufb00ectively widens the kernel in this region, and makes use\nof the local linear assumption to smooth out the estimate (on the logit\nscale).\nIf classi\ufb01cation is the ultimate goal, then learning the separate class den-\nsities well may be unnecessary, and can in fact be misleading. Figure 6.15\nshows an example where the densities are both multimodal, but the pos-\nterior ratio is quite smooth. In learning the separate densities from data,\none might decide to settle for a rougher, high-variance \ufb01t to capture these\nfeatures, which are irrelevant for the purposes of estimating the posterior\nprobabilities. In fact, if classi\ufb01cation is the ultimate goal, then we need onl y\nto estimate the posterior well near the decision boundary (for two classes,\nthis is the set {x|Pr(G= 1|X=x) =1\n2}).\n6.6.3 The Naive Bayes Classi\ufb01er\nThis is a technique that has remained popular over the years, despite its\nname (also known as \u201cIdiot\u2019s Bayes\u201d!) It is especially appropriate when", "229": "6.6 Kernel Density Estimation and Classi\ufb01cation 211\nthe dimension pof the feature space is high, making density estimation\nunattractive. The naive Bayes model assumes that given a class G=j, the\nfeatures Xkare independent:\nfj(X) =p/productdisplay\nk=1fjk(Xk). (6.26)\nWhile this assumption is generally not true, it does simplify the estimation\ndramatically:\n\u2022The individual class-conditional marginal densities fjkcan each be\nestimated separately using one-dimensional kernel density estimates.\nThis is in fact a generalization of the original naive Bayes procedures,\nwhich used univariate Gaussians to represent these marginals.\n\u2022If a component XjofXis discrete, then an appropriate histogram\nestimate can be used. This provides a seamless way of mixing variable\ntypes in a feature vector.\nDespite these rather optimistic assumptions, naive Bayes classi\ufb01ers often\noutperform far more sophisticated alternatives. The reasons are related to\nFigure 6.15: although the individual class density estimates may be biased,\nthis bias might not hurt the posterior probabilities as much, especially\nnear the decision regions. In fact, the problem may be able to withstand\nconsiderable bias for the savings in variance such a \u201cnaive\u201d assumption\nearns.\nStarting from (6.26) we can derive the logit-transform (using class Jas\nthe base):\nlogPr(G=\u2113|X)\nPr(G=J|X)= log\u03c0\u2113f\u2113(X)\n\u03c0JfJ(X)\n= log\u03c0\u2113/producttextp\nk=1f\u2113k(Xk)\n\u03c0J/producttextp\nk=1fJk(Xk)\n= log\u03c0\u2113\n\u03c0J+p/summationdisplay\nk=1logf\u2113k(Xk)\nfJk(Xk)\n=\u03b1\u2113+p/summationdisplay\nk=1g\u2113k(Xk).(6.27)\nThis has the form of a generalized additive model , which is described in more\ndetail in Chapter 9. The models are \ufb01t in quite di\ufb00erent ways though; their\ndi\ufb00erences are explored in Exercise 6.9. The relationship between naive\nBayes and generalized additive models is analogous to that between linear\ndiscriminant analysis and logistic regression (Section 4.4.5).", "230": "212 6. Kernel Smoothing Methods\n6.7 Radial Basis Functions and Kernels\nIn Chapter 5, functions are represented as expansions in basis functions:\nf(x) =/summationtextM\nj=1\u03b2jhj(x). The art of \ufb02exible modeling using basis expansions\nconsists of picking an appropriate family of basis functions, and then con-\ntrolling the complexity of the representation by selection, regularization, or\nboth. Some of the families of basis functions have elements that are de\ufb01ned\nlocally; for example, B-splines are de\ufb01ned locally in IR. If more \ufb02exibility\nis desired in a particular region, then that region needs to be represented\nby more basis functions (which in the case of B-splines translates to more\nknots). Tensor products of IR-local basis functions deliver basis functions\nlocal in IRp. Not all basis functions are local\u2014for example, the truncated\npower bases for splines, or the sigmoidal basis functions \u03c3(\u03b10+\u03b1x) used\nin neural-networks (see Chapter 11). The composed function f(x) can nev-\nertheless show local behavior, because of the particular signs and values\nof the coe\ufb03cients causing cancellations of global e\ufb00ects. For example, the\ntruncated power basis has an equivalent B-spline basis for the same space\nof functions; the cancellation is exact in this case.\nKernel methods achieve \ufb02exibility by \ufb01tting simple models in a region\nlocal to the target point x0. Localization is achieved via a weighting kernel\nK\u03bb, and individual observations receive weights K\u03bb(x0,xi).\nRadial basis functions combine these ideas, by treating the kernel func-\ntionsK\u03bb(\u03be,x) as basis functions. This leads to the model\nf(x) =M/summationdisplay\nj=1K\u03bbj(\u03bej,x)\u03b2j\n=M/summationdisplay\nj=1D/parenleftbigg||x\u2212\u03bej||\n\u03bbj/parenrightbigg\n\u03b2j, (6.28)\nwhere each basis element is indexed by a location or prototype parameter \u03bej\nand a scale parameter \u03bbj. A popular choice for Dis the standard Gaussian\ndensity function. There are several approaches to learning the parameters\n{\u03bbj,\u03bej,\u03b2j}, j= 1,... ,M . For simplicity we will focus on least squares\nmethods for regression, and use the Gaussian kernel.\n\u2022Optimize the sum-of-squares with respect to all the parameters:\nmin\n{\u03bbj,\u03bej,\u03b2j}M\n1N/summationdisplay\ni=1\uf8eb\n\uf8edyi\u2212\u03b20\u2212M/summationdisplay\nj=1\u03b2jexp/braceleft\uf8ecigg\n\u2212(xi\u2212\u03bej)T(xi\u2212\u03bej)\n\u03bb2\nj/braceright\uf8ecigg\uf8f6\n\uf8f82\n.\n(6.29)\nThis model is commonly referred to as an RBF network, an alterna-\ntive to the sigmoidal neural network discussed in Chapter 11; the \u03bej\nand\u03bbjplaying the role of the weights. This criterion is nonconvex", "231": "6.7 Radial Basis Functions and Kernels 2130 2 4 6 8 0.0 0.4 0.8 1.2\nFIGURE 6.16. Gaussian radial basis functions in I Rwith \ufb01xed width can leave\nholes (top panel). Renormalized Gaussian radial basis functio ns avoid this prob-\nlem, and produce basis functions similar in some respects to B-splines.\nwith multiple local minima, and the algorithms for optimization are\nsimilar to those used for neural networks.\n\u2022Estimate the {\u03bbj,\u03bej}separately from the \u03b2j. Given the former, the\nestimation of the latter is a simple least squares problem. Often the\nkernel parameters \u03bbjand\u03bejare chosen in an unsupervised way using\ntheXdistribution alone. One of the methods is to \ufb01t a Gaussian\nmixture density model to the training xi, which provides both the\ncenters \u03bejand the scales \u03bbj. Other even more adhoc approaches use\nclustering methods to locate the prototypes \u03bej, and treat \u03bbj=\u03bb\nas a hyper-parameter. The obvious drawback of these approaches is\nthat the conditional distribution Pr( Y|X) and in particular E(Y|X)\nis having no say in where the action is concentrated. On the positive\nside, they are much simpler to implement.\nWhile it would seem attractive to reduce the parameter set and assume\na constant value for \u03bbj=\u03bb, this can have an undesirable side e\ufb00ect of\ncreating holes\u2014regions of IRpwhere none of the kernels has appreciable\nsupport, as illustrated in Figure 6.16 (upper panel). Renormalized radial\nbasis functions,\nhj(x) =D(||x\u2212\u03bej||/\u03bb)/summationtextM\nk=1D(||x\u2212\u03bek||/\u03bb), (6.30)\navoid this problem (lower panel).\nThe Nadaraya\u2013Watson kernel regression estimator (6.2) in IRpcan be\nviewed as an expansion in renormalized radial basis functions,\n\u02c6f(x0) =/summationtextN\ni=1yiK\u03bb(x0,xi)PN\ni=1K\u03bb(x0,xi)\n=/summationtextN\ni=1yihi(x0) (6.31)", "232": "214 6. Kernel Smoothing Methods\nwith a basis function hilocated at every observation and coe\ufb03cients yi;\nthat is, \u03bei=xi,\u02c6\u03b2i=yi, i= 1,... ,N .\nNote the similarity between the expansion (6.31) and the solution (5.50)\non page 169 to the regularization problem induced by the kernel K. Radial\nbasis functions form the bridge between the modern \u201ckernel methods\u201d and\nlocal \ufb01tting technology.\n6.8 Mixture Models for Density Estimation and\nClassi\ufb01cation\nThe mixture model is a useful tool for density estimation, and can be viewed\nas a kind of kernel method. The Gaussian mixture model has the form\nf(x) =M/summationdisplay\nm=1\u03b1m\u03c6(x;\u03b8m,\u03a3m) (6.32)\nwith mixing proportions \u03b1m,/summationtext\nm\u03b1m= 1, and each Gaussian density has\na mean \u03b8mand covariance matrix \u03a3m. In general, mixture models can use\nany component densities in place of the Gaussian in (6.32): the Gaussian\nmixture model is by far the most popular.\nThe parameters are usually \ufb01t by maximum likelihood, using the EM\nalgorithm as described in Chapter 8. Some special cases arise:\n\u2022If the covariance matrices are constrained to be scalar: \u03a3m=\u03c3mI,\nthen (6.32) has the form of a radial basis expansion.\n\u2022If in addition \u03c3m=\u03c3 >0 is \ufb01xed, and M\u2191N, then the max-\nimum likelihood estimate for (6.32) approaches the kernel density\nestimate (6.22) where \u02c6 \u03b1m= 1/Nand \u02c6\u03b8m=xm.\nUsing Bayes\u2019 theorem, separate mixture densities in each class lead to \ufb02ex-\nible models for Pr( G|X); this is taken up in some detail in Chapter 12.\nFigure 6.17 shows an application of mixtures to the heart disease risk-\nfactor study. In the top row are histograms of Agefor theno CHD andCHD\ngroups separately, and then combined on the right. Using the combined\ndata, we \ufb01t a two-component mixture of the form (6.32) with the (scalars)\n\u03a31and\u03a32not constrained to be equal. Fitting was done via the EM\nalgorithm (Chapter 8): note that the procedure does not use knowledge of\ntheCHDlabels. The resulting estimates were\n\u02c6\u03b81= 36.4, \u02c6\u03a31= 157 .7, \u02c6\u03b11= 0.7,\n\u02c6\u03b82= 58.0, \u02c6\u03a32= 15.6, \u02c6\u03b12= 0.3.\nThe component densities \u03c6(\u02c6\u03b81,\u02c6\u03a31) and \u03c6(\u02c6\u03b82,\u02c6\u03a32) are shown in the lower-\nleft and middle panels. The lower-right panel shows these component den-\nsities (orange and blue) along with the estimated mixture density (green).", "233": "6.8 Mixture Models for Density Estimation and Classi\ufb01cation 215\nNo CHD\nAgeCount\n20 30 40 50 600 5 10 15 20CHD\nAgeCount\n20 30 40 50 600 5 10 15Combined\nAgeCount\n20 30 40 50 600 5 10 15 20 25 30\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\nAgeMixture Estimate\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\nAgeMixture Estimate\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\nAgeMixture Estimate\nFIGURE 6.17. Application of mixtures to the heart disease risk-factor stu dy.\n(Top row:) Histograms of Agefor theno CHD andCHDgroups separately, and\ncombined. (Bottom row:) estimated component densities from a Ga ussian mix-\nture model, (bottom left, bottom middle); (bottom right:) E stimated component\ndensities (blue and orange) along with the estimated mixture densi ty (green). The\norange density has a very large standard deviation, and approximat es a uniform\ndensity.\nThe mixture model also provides an estimate of the probability that\nobservation ibelongs to component m,\n\u02c6rim=\u02c6\u03b1m\u03c6(xi; \u02c6\u03b8m,\u02c6\u03a3m)/summationtextM\nk=1\u02c6\u03b1k\u03c6(xi; \u02c6\u03b8k,\u02c6\u03a3k), (6.33)\nwhere xiisAgein our example. Suppose we threshold each value \u02c6 ri2and\nhence de\ufb01ne \u02c6\u03b4i=I(\u02c6ri2>0.5). Then we can compare the classi\ufb01cation of\neach observation by CHDand the mixture model:\nMixture model\n\u02c6\u03b4= 0 \u02c6\u03b4= 1\nCHD No 232 70\nYes 76 84\nAlthough the mixture model did not use the CHDlabels, it has done a fair\njob in discovering the two CHDsubpopulations. Linear logistic regression,\nusing the CHDas a response, achieves the same error rate (32%) when \ufb01t to\nthese data using maximum-likelihood (Section 4.4).", "234": "216 6. Kernel Smoothing Methods\n6.9 Computational Considerations\nKernel and local regression and density estimation are memory-based meth-\nods: the model is the entire training data set, and the \ufb01tting is done at\nevaluation or prediction time. For many real-time applications, this can\nmake this class of methods infeasible.\nThe computational cost to \ufb01t at a single observation x0isO(N) \ufb02ops,\nexcept in oversimpli\ufb01ed cases (such as square kernels). By comparison,\nan expansion in Mbasis functions costs O(M) for one evaluation, and\ntypically M\u223cO(logN). Basis function methods have an initial cost of at\nleastO(NM2+M3).\nThe smoothing parameter(s) \u03bbfor kernel methods are typically deter-\nmined o\ufb00-line, for example using cross-validation, at a cost of O(N2) \ufb02ops.\nPopular implementations of local regression, such as the loess function in\nS-PLUS and Rand the locfit procedure (Loader, 1999), use triangulation\nschemes to reduce the computations. They compute the \ufb01t exactly at M\ncarefully chosen locations ( O(NM)), and then use blending techniques to\ninterpolate the \ufb01t elsewhere ( O(M) per evaluation).\nBibliographic Notes\nThere is a vast literature on kernel methods which we will not attempt to\nsummarize. Rather we will point to a few good references that themselves\nhave extensive bibliographies. Loader (1999) gives excellent coverage of lo-\ncal regression and likelihood, and also describes state-of-the-art software\nfor \ufb01tting these models. Fan and Gijbels (1996) cover these models from\na more theoretical aspect. Hastie and Tibshirani (1990) discuss local re-\ngression in the context of additive modeling. Silverman (1986) gives a goo d\noverview of density estimation, as does Scott (1992).\nExercises\nEx. 6.1 Show that the Nadaraya\u2013Watson kernel smooth with \ufb01xed metric\nbandwidth \u03bband a Gaussian kernel is di\ufb00erentiable. What can be said for\nthe Epanechnikov kernel? What can be said for the Epanechnikov kernel\nwith adaptive nearest-neighbor bandwidth \u03bb(x0)?\nEx. 6.2 Show that/summationtextN\ni=1(xi\u2212x0)li(x0) = 0 for local linear regression. De\ufb01ne\nbj(x0) =/summationtextN\ni=1(xi\u2212x0)jli(x0). Show that b0(x0) = 1 for local polynomial\nregression of any degree (including local constants). Show that bj(x0) = 0\nfor all j\u2208 {1,2,... ,k }for local polynomial regression of degree k. What\nare the implications of this on the bias?", "235": "Exercises 217\nEx. 6.3 Show that ||l(x)||(Section 6.1.2) increases with the degree of the\nlocal polynomial.\nEx. 6.4 Suppose that the ppredictors Xarise from sampling relatively\nsmooth analog curves at puniformly spaced abscissa values. Denote by\nCov(X|Y) =\u03a3the conditional covariance matrix of the predictors, and\nassume this does not change much with Y. Discuss the nature of Maha-\nlanobis choice A=\u03a3\u22121for the metric in (6.14). How does this compare\nwithA=I? How might you construct a kernel Athat (a) downweights\nhigh-frequency components in the distance metric; (b) ignores them\ncompletely?\nEx. 6.5 Show that \ufb01tting a locally constant multinomial logit model of\nthe form (6.19) amounts to smoothing the binary response indicators for\neach class separately using a Nadaraya\u2013Watson kernel smoother with kernel\nweights K\u03bb(x0,xi).\nEx. 6.6 Suppose that all you have is software for \ufb01tting local regression,\nbut you can specify exactly which monomials are included in the \ufb01t. How\ncould you use this software to \ufb01t a varying-coe\ufb03cient model in some of the\nvariables?\nEx. 6.7 Derive an expression for the leave-one-out cross-validated residual\nsum-of-squares for local polynomial regression.\nEx. 6.8 Suppose that for continuous response Yand predictor X, we model\nthe joint density of X,Yusing a multivariate Gaussian kernel estimator.\nNote that the kernel in this case would be the product kernel \u03c6\u03bb(X)\u03c6\u03bb(Y).\nShow that the conditional mean E(Y|X) derived from this estimate is a\nNadaraya\u2013Watson estimator. Extend this result to classi\ufb01cation by pro-\nviding a suitable kernel for the estimation of the joint distribution of a\ncontinuous Xand discrete Y.\nEx. 6.9 Explore the di\ufb00erences between the naive Bayes model (6.27) and\na generalized additive logistic regression model, in terms of (a) model as-\nsumptions and (b) estimation. If all the variables Xkare discrete, what can\nyou say about the corresponding GAM?\nEx. 6.10 Suppose we have Nsamples generated from the model yi=f(xi)+\n\u03b5i, with \u03b5iindependent and identically distributed with mean zero and\nvariance \u03c32, thexiassumed \ufb01xed (non random). We estimate fusing a\nlinear smoother (local regression, smoothing spline, etc.) with smoothing\nparameter \u03bb. Thus the vector of \ufb01tted values is given by \u02c6f=S\u03bby. Consider\nthein-sample prediction error\nPE(\u03bb) = E1\nNN/summationdisplay\ni=1(y\u2217\ni\u2212\u02c6f\u03bb(xi))2(6.34)", "236": "218 6. Kernel Smoothing Methods\nfor predicting new responses at the Ninput values. Show that the aver-\nage squared residual on the training data, ASR( \u03bb), is a biased estimate\n(optimistic) for PE( \u03bb), while\nC\u03bb= ASR( \u03bb) +2\u03c32\nNtrace(S\u03bb) (6.35)\nis unbiased.\nEx. 6.11 Show that for the Gaussian mixture model (6.32) the likelihood\nis maximized at + \u221e, and describe how.\nEx. 6.12 Write a computer program to perform a local linear discrimi-\nnant analysis. At each query point x0, the training data receive weights\nK\u03bb(x0,xi) from a weighting kernel, and the ingredients for the linear deci-\nsion boundaries (see Section 4.3) are computed by weighted averages. Try\nout your program on the zipcode data, and show the training and test er-\nrors for a series of \ufb01ve pre-chosen values of \u03bb. Thezipcode data are available\nfrom the book website www-stat.stanford.edu/ElemStatLearn .", "237": "This is page 219\nPrinter: Opaque this\n7\nModel Assessment and Selection\n7.1 Introduction\nThegeneralization performance of a learning method relates to its predic-\ntion capability on independent test data. Assessment of this performance\nis extremely important in practice, since it guides the choice of learning\nmethod or model, and gives us a measure of the quality of the ultimately\nchosen model.\nIn this chapter we describe and illustrate the key methods for perfor-\nmance assessment, and show how they are used to select models. We begin\nthe chapter with a discussion of the interplay between bias, variance and\nmodel complexity.\n7.2 Bias, Variance and Model Complexity\nFigure 7.1 illustrates the important issue in assessing the ability of a learn-\ning method to generalize. Consider \ufb01rst the case of a quantitative or interval\nscale response. We have a target variable Y, a vector of inputs X, and a\nprediction model \u02c6f(X) that has been estimated from a training set T.\nThe loss function for measuring errors between Yand\u02c6f(X) is denoted by\nL(Y,\u02c6f(X)). Typical choices are\nL(Y,\u02c6f(X)) =/braceleft\uf8ecigg\n(Y\u2212\u02c6f(X))2squared error\n|Y\u2212\u02c6f(X)| absolute error .(7.1)", "238": "220 7. Model Assessment and Selection\n0 5 10 15 20 25 30 350.0 0.2 0.4 0.6 0.8 1.0 1.2\nModel Complexity (df)Prediction ErrorHigh Bias Low Bias\nHigh Variance Low Variance\nFIGURE 7.1. Behavior of test sample and training sample error as the model\ncomplexity is varied. The light blue curves show the training er rorerr, while the\nlight red curves show the conditional test error ErrTfor100training sets of size\n50each, as the model complexity is increased. The solid curves sh ow the expected\ntest error Errand the expected training error E[err].\nTest error , also referred to as generalization error , is the prediction error\nover an independent test sample\nErrT= E[L(Y,\u02c6f(X))|T] (7.2)\nwhere both XandYare drawn randomly from their joint distribution\n(population). Here the training set Tis \ufb01xed, and test error refers to the\nerror for this speci\ufb01c training set. A related quantity is the expected pre-\ndiction error (or expected test error)\nErr = E[ L(Y,\u02c6f(X))] = E[Err T]. (7.3)\nNote that this expectation averages over everything that is random, includ-\ning the randomness in the training set that produced \u02c6f.\nFigure 7.1 shows the prediction error (light red curves) Err Tfor 100\nsimulated training sets each of size 50. The lasso (Section 3.4.2) was used\nto produce the sequence of \ufb01ts. The solid red curve is the average, and\nhence an estimate of Err.\nEstimation of Err Twill be our goal, although we will see that Err is\nmore amenable to statistical analysis, and most methods e\ufb00ectively esti-\nmate the expected error. It does not seem possible to estimate conditional", "239": "7.2 Bias, Variance and Model Complexity 221\nerror e\ufb00ectively, given only the information in the same training set. Some\ndiscussion of this point is given in Section 7.12.\nTraining error is the average loss over the training sample\nerr =1\nNN/summationdisplay\ni=1L(yi,\u02c6f(xi)). (7.4)\nWe would like to know the expected test error of our estimated model\n\u02c6f. As the model becomes more and more complex, it uses the training\ndata more and is able to adapt to more complicated underlying structures.\nHence there is a decrease in bias but an increase in variance. There is some\nintermediate model complexity that gives minimum expected test error.\nUnfortunately training error is not a good estimate of the test error,\nas seen in Figure 7.1. Training error consistently decreases with model\ncomplexity, typically dropping to zero if we increase the model complexity\nenough. However, a model with zero training error is over\ufb01t to the training\ndata and will typically generalize poorly.\nThe story is similar for a qualitative or categorical response Gtaking\none of Kvalues in a set G, labeled for convenience as 1 ,2,... ,K . Typically\nwe model the probabilities pk(X) = Pr( G=k|X) (or some monotone\ntransformations fk(X)), and then \u02c6G(X) = arg max k\u02c6pk(X). In some cases,\nsuch as 1-nearest neighbor classi\ufb01cation (Chapters 2 and 13) we produce\n\u02c6G(X) directly. Typical loss functions are\nL(G,\u02c6G(X)) = I(G\u221dne}ationslash=\u02c6G(X)) (0\u20131 loss) , (7.5)\nL(G,\u02c6p(X)) = \u22122K/summationdisplay\nk=1I(G=k)log \u02c6pk(X)\n=\u22122log \u02c6pG(X) (\u22122\u00d7log-likelihood) .(7.6)\nThe quantity \u22122\u00d7the log-likelihood is sometimes referred to as the\ndeviance .\nAgain, test error here is Err T= E[L(G,\u02c6G(X))|T], the population mis-\nclassi\ufb01cation error of the classi\ufb01er trained on T, and Err is the expected\nmisclassi\ufb01cation error.\nTraining error is the sample analogue, for example,\nerr =\u22122\nNN/summationdisplay\ni=1log \u02c6pgi(xi), (7.7)\nthe sample log-likelihood for the model.\nThe log-likelihood can be used as a loss-function for general response\ndensities, such as the Poisson, gamma, exponential, log-normal and others.\nIf Pr \u03b8(X)(Y) is the density of Y, indexed by a parameter \u03b8(X) that depends\non the predictor X, then\nL(Y,\u03b8(X)) =\u22122\u2264log Pr \u03b8(X)(Y). (7.8)", "240": "222 7. Model Assessment and Selection\nThe \u201c\u22122\u201d in the de\ufb01nition makes the log-likelihood loss for the Gaussian\ndistribution match squared-error loss.\nFor ease of exposition, for the remainder of this chapter we will use Yand\nf(X) to represent all of the above situations, since we focus mainly on the\nquantitative response (squared-error loss) setting. For the other situati ons,\nthe appropriate translations are obvious.\nIn this chapter we describe a number of methods for estimating the\nexpected test error for a model. Typically our model will have a tuning\nparameter or parameters \u03b1and so we can write our predictions as \u02c6f\u03b1(x).\nThe tuning parameter varies the complexity of our model, and we wish to\n\ufb01nd the value of \u03b1that minimizes error, that is, produces the minimum of\nthe average test error curve in Figure 7.1. Having said this, for brevity w e\nwill often suppress the dependence of \u02c6f(x) on\u03b1.\nIt is important to note that there are in fact two separate goals that we\nmight have in mind:\nModel selection: estimating the performance of di\ufb00erent models in order\nto choose the best one.\nModel assessment: having chosen a \ufb01nal model, estimating its predic-\ntion error (generalization error) on new data.\nIf we are in a data-rich situation, the best approach for both problems is\nto randomly divide the dataset into three parts: a training set, a validation\nset, and a test set. The training set is used to \ufb01t the models; the validation\nset is used to estimate prediction error for model selection; the test set is\nused for assessment of the generalization error of the \ufb01nal chosen model.\nIdeally, the test set should be kept in a \u201cvault,\u201d and be brought out only\nat the end of the data analysis. Suppose instead that we use the test-set\nrepeatedly, choosing the model with smallest test-set error. Then the test\nset error of the \ufb01nal chosen model will underestimate the true test error,\nsometimes substantially.\nIt is di\ufb03cult to give a general rule on how to choose the number of\nobservations in each of the three parts, as this depends on the signal-to-\nnoise ratio in the data and the training sample size. A typical split might\nbe 50% for training, and 25% each for validation and testing:\nTest Train Validation TestTrain Validation Test Validation Train Validation TestTrain\nThe methods in this chapter are designed for situations where there is\ninsu\ufb03cient data to split it into three parts. Again it is too di\ufb03cult to give\na general rule on how much training data is enough; among other things,\nthis depends on the signal-to-noise ratio of the underlying function, and\nthe complexity of the models being \ufb01t to the data.", "241": "7.3 The Bias\u2013Variance Decomposition 223\nThe methods of this chapter approximate the validation step either an-\nalytically (AIC, BIC, MDL, SRM) or by e\ufb03cient sample re-use (cross-\nvalidation and the bootstrap). Besides their use in model selection, we also\nexamine to what extent each method provides a reliable estimate of test\nerror of the \ufb01nal chosen model.\nBefore jumping into these topics, we \ufb01rst explore in more detail the\nnature of test error and the bias\u2013variance tradeo\ufb00.\n7.3 The Bias\u2013Variance Decomposition\nAs in Chapter 2, if we assume that Y=f(X) +\u03b5where E( \u03b5) = 0 and\nVar(\u03b5) =\u03c32\n\u03b5, we can derive an expression for the expected prediction error\nof a regression \ufb01t \u02c6f(X) at an input point X=x0, using squared-error loss:\nErr(x0) = E[(Y\u2212\u02c6f(x0))2|X=x0]\n=\u03c32\n\u03b5+ [E\u02c6f(x0)\u2212f(x0)]2+E[\u02c6f(x0)\u2212E\u02c6f(x0)]2\n=\u03c32\n\u03b5+ Bias2(\u02c6f(x0)) + Var( \u02c6f(x0))\n= Irreducible Error + Bias2+ Variance . (7.9)\nThe \ufb01rst term is the variance of the target around its true mean f(x0), and\ncannot be avoided no matter how well we estimate f(x0), unless \u03c32\n\u03b5= 0.\nThe second term is the squared bias, the amount by which the average of\nour estimate di\ufb00ers from the true mean; the last term is the variance; the\nexpected squared deviation of \u02c6f(x0) around its mean. Typically the more\ncomplex we make the model \u02c6f, the lower the (squared) bias but the higher\nthe variance.\nFor the k-nearest-neighbor regression \ufb01t, these expressions have the sim-\nple form\nErr(x0) = E[(Y\u2212\u02c6fk(x0))2|X=x0]\n=\u03c32\n\u03b5+/bracketleft\uf8ecigg\nf(x0)\u22121\nkk/summationdisplay\n\u2113=1f(x(\u2113))/bracketright\uf8ecigg2\n+\u03c32\n\u03b5\nk. (7.10)\nHere we assume for simplicity that training inputs xiare \ufb01xed, and the ran-\ndomness arises from the yi. The number of neighbors kis inversely related\nto the model complexity. For small k, the estimate \u02c6fk(x) can potentially\nadapt itself better to the underlying f(x). As we increase k, the bias\u2014the\nsquared di\ufb00erence between f(x0) and the average of f(x) at the k-nearest\nneighbors\u2014will typically increase, while the variance decreases.\nFor a linear model \ufb01t \u02c6fp(x) =xT\u02c6\u03b2, where the parameter vector \u03b2with\npcomponents is \ufb01t by least squares, we have\nErr(x0) = E[(Y\u2212\u02c6fp(x0))2|X=x0]", "242": "224 7. Model Assessment and Selection\n=\u03c32\n\u03b5+ [f(x0)\u2212E\u02c6fp(x0)]2+||h(x0)||2\u03c32\n\u03b5.(7.11)\nHereh(x0) =X(XTX)\u22121x0, theN-vector of linear weights that produce\nthe \ufb01t \u02c6fp(x0) =x0T(XTX)\u22121XTy, and hence Var[ \u02c6fp(x0)] =||h(x0)||2\u03c32\n\u03b5.\nWhile this variance changes with x0, its average (with x0taken to be each\nof the sample values xi) is (p/N)\u03c32\n\u03b5, and hence\n1\nNN/summationdisplay\ni=1Err(xi) =\u03c32\n\u03b5+1\nNN/summationdisplay\ni=1[f(xi)\u2212E\u02c6f(xi)]2+p\nN\u03c32\n\u03b5, (7.12)\nthein-sample error. Here model complexity is directly related to the num-\nber of parameters p.\nThe test error Err( x0) for a ridge regression \ufb01t \u02c6f\u03b1(x0) is identical in\nform to (7.11), except the linear weights in the variance term are di\ufb00erent:\nh(x0) =X(XTX+\u03b1I)Tx0. The bias term will also be di\ufb00erent.\nFor a linear model family such as ridge regression, we can break down\nthe bias more \ufb01nely. Let \u03b2\u2217denote the parameters of the best-\ufb01tting linear\napproximation to f:\n\u03b2\u2217= arg min\n\u03b2E/parenleftbig\nf(X)\u2212XT\u03b2/parenrightbig2. (7.13)\nHere the expectation is taken with respect to the distribution of the input\nvariables X. Then we can write the average squared bias as\nEx0/bracketleft\uf8ecig\nf(x0)\u2212E\u02c6f\u03b1(x0)/bracketright\uf8ecig2\n= E x0/bracketleftbig\nf(x0)\u2212xT\n0\u03b2\u2217/bracketrightbig2+ Ex0/bracketleft\uf8ecig\nxT\n0\u03b2\u2217\u2212ExT\n0\u02c6\u03b2\u03b1/bracketright\uf8ecig2\n= Ave[Model Bias]2+ Ave[Estimation Bias]2\n(7.14)\nThe \ufb01rst term on the right-hand side is the average squared model bias , the\nerror between the best-\ufb01tting linear approximation and the true function.\nThe second term is the average squared estimation bias , the error between\nthe average estimate E( xT\n0\u02c6\u03b2) and the best-\ufb01tting linear approximation.\nFor linear models \ufb01t by ordinary least squares, the estimation bias is zero.\nFor restricted \ufb01ts, such as ridge regression, it is positive, and we trade i t o\ufb00\nwith the bene\ufb01ts of a reduced variance. The model bias can only be reduced\nby enlarging the class of linear models to a richer collection of models, by\nincluding interactions and transformations of the variables in the model.\nFigure 7.2 shows the bias\u2013variance tradeo\ufb00 schematically. In the case\nof linear models, the model space is the set of all linear predictions from\npinputs and the black dot labeled \u201cclosest \ufb01t\u201d is xT\u03b2\u2217. The blue-shaded\nregion indicates the error \u03c3\u03b5with which we see the truth in the training\nsample.\nAlso shown is the variance of the least squares \ufb01t, indicated by the large\nyellow circle centered at the black dot labeled \u201cclosest \ufb01t in population,\u2019", "243": "7.3 The Bias\u2013Variance Decomposition 225\nRealizationClosest fit in population\nEstimation BiasSPACE\nVarianceEstimationClosest fit\nTruth\nModel bias\nRESTRICTEDShrunken fit\nMODELSPACEMODEL\nFIGURE 7.2. Schematic of the behavior of bias and variance. The model space\nis the set of all possible predictions from the model, with the \u201cclosest \ufb01t\u201d labeled\nwith a black dot. The model bias from the truth is shown, along wi th the variance,\nindicated by the large yellow circle centered at the black dot l abeled \u201cclosest \ufb01t\nin population.\u201d A shrunken or regularized \ufb01t is also shown, having additional\nestimation bias, but smaller prediction error due to its dec reased variance.", "244": "226 7. Model Assessment and Selection\nNow if we were to \ufb01t a model with fewer predictors, or regularize the coef-\n\ufb01cients by shrinking them toward zero (say), we would get the \u201cshrunken\n\ufb01t\u201d shown in the \ufb01gure. This \ufb01t has an additional estimation bias, due to\nthe fact that it is not the closest \ufb01t in the model space. On the other hand,\nit has smaller variance. If the decrease in variance exceeds the increase in\n(squared) bias, then this is worthwhile.\n7.3.1 Example: Bias\u2013Variance Tradeo\ufb00\nFigure 7.3 shows the bias\u2013variance tradeo\ufb00 for two simulated examples.\nThere are 80 observations and 20 predictors, uniformly distributed in the\nhypercube [0 ,1]20. The situations are as follows:\nLeft panels: Yis 0 if X1\u22641/2 and 1 if X1>1/2, and we apply k-nearest\nneighbors.\nRight panels: Yis 1 if/summationtext10\nj=1Xjis greater than 5 and 0 otherwise, and we\nuse best subset linear regression of size p.\nThe top row is regression with squared error loss; the bottom row is cla ssi-\n\ufb01cation with 0\u20131 loss. The \ufb01gures show the prediction error (red), squared\nbias (green) and variance (blue), all computed for a large test sample.\nIn the regression problems, bias and variance add to produce the predic-\ntion error curves, with minima at about k= 5 for k-nearest neighbors, and\np\u226510 for the linear model. For classi\ufb01cation loss (bottom \ufb01gures), some\ninteresting phenomena can be seen. The bias and variance curves are the\nsame as in the top \ufb01gures, and prediction error now refers to misclassi\ufb01-\ncation rate. We see that prediction error is no longer the sum of squared\nbias and variance. For the k-nearest neighbor classi\ufb01er, prediction error\ndecreases or stays the same as the number of neighbors is increased to 20,\ndespite the fact that the squared bias is rising. For the linear model classi-\n\ufb01er the minimum occurs for p\u226510 as in regression, but the improvement\nover the p= 1 model is more dramatic. We see that bias and variance seem\nto interact in determining prediction error.\nWhy does this happen? There is a simple explanation for the \ufb01rst phe-\nnomenon. Suppose at a given input point, the true probability of class 1 is\n0.9 while the expected value of our estimate is 0 .6. Then the squared bias\u2014\n(0.6\u22120.9)2\u2014is considerable, but the prediction error is zero since we make\nthe correct decision. In other words, estimation errors that leave us on the\nright side of the decision boundary don\u2019t hurt. Exercise 7.2 demonstrates\nthis phenomenon analytically, and also shows the interaction e\ufb00ect between\nbias and variance.\nThe overall point is that the bias\u2013variance tradeo\ufb00 behaves di\ufb00erently\nfor 0\u20131 loss than it does for squared error loss. This in turn means that\nthe best choices of tuning parameters may di\ufb00er substantially in the two", "245": "7.3 The Bias\u2013Variance Decomposition 2270.0 0.1 0.2 0.3 0.4\nNumber of Neighbors k50 40 30 20 10 0k\u2212NN \u2212 Regression\n5 10 15 200.0 0.1 0.2 0.3 0.4\nSubset Size pLinear Model \u2212 Regression0.0 0.1 0.2 0.3 0.4\nNumber of Neighbors k50 40 30 20 10 0k\u2212NN \u2212 Classification\n5 10 15 200.0 0.1 0.2 0.3 0.4\nSubset Size pLinear Model \u2212 Classification\nFIGURE 7.3. Expected prediction error (orange), squared bias (green) and va ri-\nance (blue) for a simulated example. The top row is regression w ith squared error\nloss; the bottom row is classi\ufb01cation with 0\u20131loss. The models are k-nearest\nneighbors (left) and best subset regression of size p(right). The variance and bias\ncurves are the same in regression and classi\ufb01cation, but the pre diction error curve\nis di\ufb00erent.", "246": "228 7. Model Assessment and Selection\nsettings. One should base the choice of tuning parameter on an estimate of\nprediction error, as described in the following sections.\n7.4 Optimism of the Training Error Rate\nDiscussions of error rate estimation can be confusing, because we have\nto make clear which quantities are \ufb01xed and which are random1. Before\nwe continue, we need a few de\ufb01nitions, elaborating on the material of Sec-\ntion 7.2. Given a training set T={(x1,y1),(x2,y2),...(xN,yN)}the gen-\neralization error of a model \u02c6fis\nErrT= EX0,Y0[L(Y0,\u02c6f(X0))|T]; (7.15)\nNote that the training set Tis \ufb01xed in expression (7.15). The point ( X0,Y0)\nis a new test data point, drawn from F, the joint distribution of the data.\nAveraging over training sets Tyields the expected error\nErr = E TEX0,Y0[L(Y0,\u02c6f(X0))|T], (7.16)\nwhich is more amenable to statistical analysis. As mentioned earlier, it\nturns out that most methods e\ufb00ectively estimate the expected error rather\nthan E T; see Section 7.12 for more on this point.\nNow typically, the training error\nerr =1\nNN/summationdisplay\ni=1L(yi,\u02c6f(xi)) (7.17)\nwill be less than the true error Err T, because the same data is being used\nto \ufb01t the method and assess its error (see Exercise 2.9). A \ufb01tting method\ntypically adapts to the training data, and hence the apparent or training\nerror err will be an overly optimistic estimate of the generalization error\nErrT.\nPart of the discrepancy is due to where the evaluation points occur. The\nquantity Err Tcan be thought of as extra-sample error, since the test input\nvectors don\u2019t need to coincide with the training input vectors. The nature\nof the optimism in err is easiest to understand when we focus instead on\nthein-sample error\nErrin=1\nNN/summationdisplay\ni=1EY0[L(Y0\ni,\u02c6f(xi))|T] (7.18)\nTheY0notation indicates that we observe Nnew response values at\neach of the training points xi, i= 1,2,... ,N . We de\ufb01ne the optimism as\n1Indeed, in the \ufb01rst edition of our book, this section wasn\u2019t s u\ufb03ciently clear.", "247": "7.4 Optimism of the Training Error Rate 229\nthe di\ufb00erence between Err inand the training error err:\nop\u2261Errin\u2212err. (7.19)\nThis is typically positive since err is usually biased downward as an estimate\nof prediction error. Finally, the average optimism is the expectation of the\noptimism over training sets\n\u03c9\u2261Ey(op). (7.20)\nHere the predictors in the training set are \ufb01xed, and the expectation is\nover the training set outcome values; hence we have used the notation E y\ninstead of E T. We can usually estimate only the expected error \u03c9rather\nthan op, in the same way that we can estimate the expected error Err\nrather than the conditional error Err T.\nFor squared error, 0\u20131, and other loss functions, one can show quite\ngenerally that\n\u03c9=2\nNN/summationdisplay\ni=1Cov(\u02c6yi,yi), (7.21)\nwhere Cov indicates covariance. Thus the amount by which err underesti-\nmates the true error depends on how strongly yia\ufb00ects its own prediction.\nThe harder we \ufb01t the data, the greater Cov(\u02c6 yi,yi) will be, thereby increas-\ning the optimism. Exercise 7.4 proves this result for squared error loss where\n\u02c6yiis the \ufb01tted value from the regression. For 0\u20131 loss, \u02c6 yi\u2208 {0,1}is the\nclassi\ufb01cation at xi, and for entropy loss, \u02c6 yi\u2208[0,1] is the \ufb01tted probability\nof class 1 at xi.\nIn summary, we have the important relation\nEy(Errin) = E y(err) +2\nNN/summationdisplay\ni=1Cov(\u02c6yi,yi). (7.22)\nThis expression simpli\ufb01es if \u02c6 yiis obtained by a linear \ufb01t with dinputs\nor basis functions. For example,\nN/summationdisplay\ni=1Cov(\u02c6yi,yi) =d\u03c32\n\u03b5 (7.23)\nfor the additive error model Y=f(X) +\u03b5, and so\nEy(Errin) = E y(err) + 2 \u2264d\nN\u03c32\n\u03b5. (7.24)\nExpression (7.23) is the basis for the de\ufb01nition of the e\ufb00ective number of\nparameters discussed in Section 7.6 The optimism increases linearly with", "248": "230 7. Model Assessment and Selection\nthe number dof inputs or basis functions we use, but decreases as the\ntraining sample size increases. Versions of (7.24) hold approximately for\nother error models, such as binary data and entropy loss.\nAn obvious way to estimate prediction error is to estimate the optimism\nand then add it to the training error err. The methods described in the\nnext section\u2014 Cp, AIC, BIC and others\u2014work in this way, for a special\nclass of estimates that are linear in their parameters.\nIn contrast, cross-validation and bootstrap methods, described later in\nthe chapter, are direct estimates of the extra-sample error Err. These gen-\neral tools can be used with any loss function, and with nonlinear, adaptive\n\ufb01tting techniques.\nIn-sample error is not usually of direct interest since future values of the\nfeatures are not likely to coincide with their training set values. But for\ncomparison between models, in-sample error is convenient and often leads\nto e\ufb00ective model selection. The reason is that the relative (rather than\nabsolute) size of the error is what matters.\n7.5 Estimates of In-Sample Prediction Error\nThe general form of the in-sample estimates is\n/hatwidestErrin=err + \u02c6\u03c9, (7.25)\nwhere \u02c6 \u03c9is an estimate of the average optimism.\nUsing expression (7.24), applicable when dparameters are \ufb01t under\nsquared error loss, leads to a version of the so-called Cpstatistic,\nCp=err + 2 \u2264d\nN\u02c6\u03c3\u03b52. (7.26)\nHere \u02c6\u03c3\u03b52is an estimate of the noise variance, obtained from the mean-\nsquared error of a low-bias model. Using this criterion we adjust the training\nerror by a factor proportional to the number of basis functions used.\nTheAkaike information criterion is a similar but more generally appli-\ncable estimate of Err inwhen a log-likelihood loss function is used. It relies\non a relationship similar to (7.24) that holds asymptotically as N\u2192 \u221e:\n\u22122\u2264E[log Pr \u02c6\u03b8(Y)]\u2248 \u22122\nN\u2264E[loglik] + 2 \u2264d\nN. (7.27)\nHere Pr \u03b8(Y) is a family of densities for Y(containing the \u201ctrue\u201d density),\n\u02c6\u03b8is the maximum-likelihood estimate of \u03b8, and \u201cloglik\u201d is the maximized\nlog-likelihood:\nloglik =N/summationdisplay\ni=1log Pr \u02c6\u03b8(yi). (7.28)", "249": "7.5 Estimates of In-Sample Prediction Error 231\nFor example, for the logistic regression model, using the binomial log-\nlikelihood, we have\nAIC = \u22122\nN\u2264loglik + 2 \u2264d\nN. (7.29)\nFor the Gaussian model (with variance \u03c32\n\u03b5= \u02c6\u03c3\u03b52assumed known), the AIC\nstatistic is equivalent to Cp, and so we refer to them collectively as AIC.\nTo use AIC for model selection, we simply choose the model giving small-\nest AIC over the set of models considered. For nonlinear and other complex\nmodels, we need to replace dby some measure of model complexity. We\ndiscuss this in Section 7.6.\nGiven a set of models f\u03b1(x) indexed by a tuning parameter \u03b1, denote\nbyerr(\u03b1) and d(\u03b1) the training error and number of parameters for each\nmodel. Then for this set of models we de\ufb01ne\nAIC(\u03b1) =err(\u03b1) + 2\u2264d(\u03b1)\nN\u02c6\u03c3\u03b52. (7.30)\nThe function AIC( \u03b1) provides an estimate of the test error curve, and we\n\ufb01nd the tuning parameter \u02c6 \u03b1that minimizes it. Our \ufb01nal chosen model\nisf\u02c6\u03b1(x). Note that if the basis functions are chosen adaptively, (7.23) no\nlonger holds. For example, if we have a total of pinputs, and we choose\nthe best-\ufb01tting linear model with d < p inputs, the optimism will exceed\n(2d/N)\u03c32\n\u03b5. Put another way, by choosing the best-\ufb01tting model with d\ninputs, the e\ufb00ective number of parameters \ufb01t is more than d.\nFigure 7.4 shows AIC in action for the phoneme recognition example\nof Section 5.2.3 on page 148. The input vector is the log-periodogram of\nthe spoken vowel, quantized to 256 uniformly spaced frequencies. A lin-\near logistic regression model is used to predict the phoneme class, with\ncoe\ufb03cient function \u03b2(f) =/summationtextM\nm=1hm(f)\u03b8m, an expansion in Mspline ba-\nsis functions. For any given M, a basis of natural cubic splines is used\nfor the hm, with knots chosen uniformly over the range of frequencies (so\nd(\u03b1) =d(M) =M). Using AIC to select the number of basis functions will\napproximately minimize Err( M) for both entropy and 0\u20131 loss.\nThe simple formula\n(2/N)N/summationdisplay\ni=1Cov(\u02c6yi,yi) = (2 d/N)\u03c32\n\u03b5\nholds exactly for linear models with additive errors and squared error loss,\nand approximately for linear models and log-likelihoods. In particular, the\nformula does not hold in general for 0\u20131 loss (Efron, 1986), although many\nauthors nevertheless use it in that context (right panel of Figure 7.4).", "250": "232 7. Model Assessment and Selection\nNumber of Basis FunctionsLog-likelihood\n0.5 1.0 1.5 2.0 2.5Log-likelihood Loss\n2 4 8 16 32 64 128O\nO\nO\nOOO\nO\nOO\nO\nO\nOOOOO\nO\nO\nO\nOOOOOTrain\nTest\nAIC\nNumber of Basis FunctionsMisclassification Error\n0.10 0.15 0.20 0.25 0.30 0.350-1 Loss\n2 4 8 16 32 64 128O\nO\nO\nOO\nO\nO\nOO\nO\nO\nOOOOOO\nO\nO\nOOOOO\nFIGURE 7.4. AIC used for model selection for the phoneme recogni-\ntion example of Section 5.2.3. The logistic regression coe\ufb03ci ent function\n\u03b2(f) =PM\nm=1hm(f)\u03b8mis modeled as an expansion in Mspline basis functions.\nIn the left panel we see the AIC statistic used to estimate Errinusing log-likeli-\nhood loss. Included is an estimate of Errbased on an independent test sample. It\ndoes well except for the extremely over-parametrized case ( M= 256 parameters\nforN= 1000 observations). In the right panel the same is done for 0\u20131 loss.\nAlthough the AIC formula does not strictly apply here, it does a reasonable job in\nthis case.\n7.6 The E\ufb00ective Number of Parameters\nThe concept of \u201cnumber of parameters\u201d can be generalized, especially to\nmodels where regularization is used in the \ufb01tting. Suppose we stack the\noutcomes y1,y2,... ,y Ninto a vector y, and similarly for the predictions\n\u02c6y. Then a linear \ufb01tting method is one for which we can write\n\u02c6y=Sy, (7.31)\nwhereSis anN\u00d7Nmatrix depending on the input vectors xibut not on\ntheyi. Linear \ufb01tting methods include linear regression on the original fea-\ntures or on a derived basis set, and smoothing methods that use quadratic\nshrinkage, such as ridge regression and cubic smoothing splines. Then the\ne\ufb00ective number of parameters is de\ufb01ned as\ndf(S) = trace( S), (7.32)\nthe sum of the diagonal elements of S(also known as the e\ufb00ective degrees-\nof-freedom ). Note that if Sis an orthogonal-projection matrix onto a basis", "251": "7.7 The Bayesian Approach and BIC 233\nset spanned by Mfeatures, then trace( S) =M. It turns out that trace( S) is\nexactly the correct quantity to replace das the number of parameters in the\nCpstatistic (7.26). If yarises from an additive-error model Y=f(X) +\u03b5\nwith Var( \u03b5) =\u03c32\n\u03b5, then one can show that/summationtextN\ni=1Cov(\u02c6yi,yi) = trace( S)\u03c32\n\u03b5,\nwhich motivates the more general de\ufb01nition\ndf(\u02c6y) =/summationtextN\ni=1Cov(\u02c6yi,yi)\n\u03c32\u03b5(7.33)\n(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuit ion\nfor the de\ufb01nition df = trace( S) in the context of smoothing splines.\nFor models like neural networks, in which we minimize an error function\nR(w) with weight decay penalty (regularization) \u03b1/summationtext\nmw2\nm, the e\ufb00ective\nnumber of parameters has the form\ndf(\u03b1) =M/summationdisplay\nm=1\u03b8m\n\u03b8m+\u03b1, (7.34)\nwhere the \u03b8mare the eigenvalues of the Hessian matrix \u22022R(w)/\u2202w\u2202wT.\nExpression (7.34) follows from (7.32) if we make a quadratic approx imation\nto the error function at the solution (Bishop, 1995).\n7.7 The Bayesian Approach and BIC\nThe Bayesian information criterion (BIC), like AIC, is applicable in s ettings\nwhere the \ufb01tting is carried out by maximization of a log-likelihood. The\ngeneric form of BIC is\nBIC = \u22122\u2264loglik + (log N)\u2264d. (7.35)\nThe BIC statistic (times 1/2) is also known as the Schwarz criterion ( Schwarz,\n1978).\nUnder the Gaussian model, assuming the variance \u03c32\n\u03b5is known, \u22122\u2264loglik\nequals (up to a constant)/summationtext\ni(yi\u2212\u02c6f(xi))2/\u03c32\n\u03b5, which is N\u2264err/\u03c32\n\u03b5for squared\nerror loss. Hence we can write\nBIC =N\n\u03c32\u03b5/bracketleft\uf8ecig\nerr + (log N)\u2264d\nN\u03c32\n\u03b5/bracketright\uf8ecig\n. (7.36)\nTherefore BIC is proportional to AIC ( Cp), with the factor 2 replaced\nby log N. Assuming N > e2\u22487.4, BIC tends to penalize complex models\nmore heavily, giving preference to simpler models in selection. As with AIC,\n\u03c32\n\u03b5is typically estimated by the mean squared error of a low-bias model.\nFor classi\ufb01cation problems, use of the multinomial log-likelihood leads to a\nsimilar relationship with the AIC, using cross-entropy as the error measur e.", "252": "234 7. Model Assessment and Selection\nNote however that the misclassi\ufb01cation error measure does not arise in the\nBIC context, since it does not correspond to the log-likelihood of the data\nunder any probability model.\nDespite its similarity with AIC, BIC is motivated in quite a di\ufb00erent\nway. It arises in the Bayesian approach to model selection, which we now\ndescribe.\nSuppose we have a set of candidate models Mm,m= 1,... ,M and\ncorresponding model parameters \u03b8m, and we wish to choose a best model\nfrom among them. Assuming we have a prior distribution Pr( \u03b8m|Mm) for\nthe parameters of each model Mm, the posterior probability of a given\nmodel is\nPr(Mm|Z)\u221dPr(Mm)\u2264Pr(Z|Mm) (7.37)\n\u221dPr(Mm)\u2264/integraldisplay\nPr(Z|\u03b8m,Mm)Pr(\u03b8m|Mm)d\u03b8m,\nwhere Zrepresents the training data {xi,yi}N\n1. To compare two models\nMmandM\u2113, we form the posterior odds\nPr(Mm|Z)\nPr(M\u2113|Z)=Pr(Mm)\nPr(M\u2113)\u2264Pr(Z|Mm)\nPr(Z|M\u2113). (7.38)\nIf the odds are greater than one we choose model m, otherwise we choose\nmodel \u2113. The rightmost quantity\nBF(Z) =Pr(Z|Mm)\nPr(Z|M\u2113)(7.39)\nis called the Bayes factor , the contribution of the data toward the posterior\nodds.\nTypically we assume that the prior over models is uniform, so that\nPr(Mm) is constant. We need some way of approximating Pr( Z|Mm).\nA so-called Laplace approximation to the integral followed by some other\nsimpli\ufb01cations (Ripley, 1996, page 64) to (7.37) gives\nlog Pr( Z|Mm) = log Pr( Z|\u02c6\u03b8m,Mm)\u2212dm\n2\u2264logN+O(1).(7.40)\nHere \u02c6\u03b8mis a maximum likelihood estimate and dmis the number of free\nparameters in model Mm. If we de\ufb01ne our loss function to be\n\u22122log Pr( Z|\u02c6\u03b8m,Mm),\nthis is equivalent to the BIC criterion of equation (7.35).\nTherefore, choosing the model with minimum BIC is equivalent to choos-\ning the model with largest (approximate) posterior probability. But this\nframework gives us more. If we compute the BIC criterion for a set of M,", "253": "7.8 Minimum Description Length 235\nmodels, giving BIC m,m= 1,2,... ,M , then we can estimate the posterior\nprobability of each model Mmas\ne\u22121\n2\u2264BIC m\n/summationtextM\n\u2113=1e\u22121\n2\u2264BIC \u2113. (7.41)\nThus we can estimate not only the best model, but also assess the relative\nmerits of the models considered.\nFor model selection purposes, there is no clear choice between AIC and\nBIC. BIC is asymptotically consistent as a selection criterion. What this\nmeans is that given a family of models, including the true model, the prob-\nability that BIC will select the correct model approaches one as the sample\nsizeN\u2192 \u221e. This is not the case for AIC, which tends to choose models\nwhich are too complex as N\u2192 \u221e. On the other hand, for \ufb01nite samples,\nBIC often chooses models that are too simple, because of its heavy penalty\non complexity.\n7.8 Minimum Description Length\nThe minimum description length (MDL) approach gives a selection cri-\nterion formally identical to the BIC approach, but is motivated from an\noptimal coding viewpoint. We \ufb01rst review the theory of coding for data\ncompression, and then apply it to model selection.\nWe think of our datum zas a message that we want to encode and\nsend to someone else (the \u201creceiver\u201d). We think of our model as a way of\nencoding the datum, and will choose the most parsimonious model, that is\nthe shortest code, for the transmission.\nSuppose \ufb01rst that the possible messages we might want to transmit are\nz1,z2,... ,z m. Our code uses a \ufb01nite alphabet of length A: for example, we\nmight use a binary code {0,1}of length A= 2. Here is an example with\nfour possible messages and a binary coding:\nMessage z1z2z3z4\nCode 010110 111(7.42)\nThis code is known as an instantaneous pre\ufb01x code: no code is the pre-\n\ufb01x of any other, and the receiver (who knows all of the possible codes),\nknows exactly when the message has been completely sent. We restrict our\ndiscussion to such instantaneous pre\ufb01x codes.\nOne could use the coding in (7.42) or we could permute the codes, for\nexample use codes 110 ,10,111,0 forz1,z2,z3,z4. How do we decide which\nto use? It depends on how often we will be sending each of the messages.\nIf, for example, we will be sending z1most often, it makes sense to use the\nshortest code 0 for z1. Using this kind of strategy\u2014shorter codes for more\nfrequent messages\u2014the average message length will be shorter.", "254": "236 7. Model Assessment and Selection\nIn general, if messages are sent with probabilities Pr( zi),i= 1,2,... ,4,\na famous theorem due to Shannon says we should use code lengths li=\n\u2212log2Pr(zi) and the average message length satis\ufb01es\nE(length) \u2265 \u2212/summationdisplay\nPr(zi)log2(Pr(zi)). (7.43)\nThe right-hand side above is also called the entropy of the distribution\nPr(zi). The inequality is an equality when the probabilities satisfy pi=\nA\u2212li. In our example, if Pr( zi) = 1/2,1/4,1/8,1/8,respectively, then the\ncoding shown in (7.42) is optimal and achieves the entropy lower bound.\nIn general the lower bound cannot be achieved, but procedures like the\nHu\ufb00mann coding scheme can get close to the bound. Note that with an\nin\ufb01nite set of messages, the entropy is replaced by \u2212/integraltext\nPr(z)log2Pr(z)dz.\nFrom this result we glean the following:\nTo transmit a random variable zhaving probability density func-\ntionPr(z), we require about \u2212log2Pr(z)bits of information.\nWe henceforth change notation from log2Pr(z) to log Pr( z) = logePr(z);\nthis is for convenience, and just introduces an unimportant multiplicative\nconstant.\nNow we apply this result to the problem of model selection. We have\na model Mwith parameters \u03b8, and data Z= (X,y) consisting of both\ninputs and outputs. Let the (conditional) probability of the outputs under\nthe model be Pr( y|\u03b8,M,X), assume the receiver knows all of the inputs,\nand we wish to transmit the outputs. Then the message length required to\ntransmit the outputs is\nlength = \u2212log Pr( y|\u03b8,M,X)\u2212log Pr( \u03b8|M), (7.44)\nthe log-probability of the target values given the inputs. The second term\nis the average code length for transmitting the model parameters \u03b8, while\nthe \ufb01rst term is the average code length for transmitting the discrepancy\nbetween the model and actual target values. For example suppose we have\na single target ywithy\u223cN(\u03b8,\u03c32), parameter \u03b8\u223cN(0,1) and no input\n(for simplicity). Then the message length is\nlength = constant + log \u03c3+(y\u2212\u03b8)2\n\u03c32+\u03b82\n2. (7.45)\nNote that the smaller \u03c3is, the shorter on average is the message length,\nsinceyis more concentrated around \u03b8.\nThe MDL principle says that we should choose the model that mini-\nmizes (7.44). We recognize (7.44) as the (negative) log-posterior distribu-\ntion, and hence minimizing description length is equivalent to maximizing\nposterior probability. Hence the BIC criterion, derived as approximation to\nlog-posterior probability, can also be viewed as a device for (approximate)\nmodel choice by minimum description length.", "255": "7.9 Vapnik\u2013Chervonenkis Dimension 237\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 0.0 1.0\nxsin(50 \u2264x)\nFIGURE 7.5. The solid curve is the function sin(50 x)forx\u2208[0,1]. The green\n(solid) and blue (hollow) points illustrate how the associate d indicator function\nI(sin(\u03b1x)>0)can shatter (separate) an arbitrarily large number of points b y\nchoosing an appropriately high frequency \u03b1.\nNote that we have ignored the precision with which a random variable\nzis coded. With a \ufb01nite code length we cannot code a continuous variable\nexactly. However, if we code zwithin a tolerance \u03b4z, the message length\nneeded is the log of the probability in the interval [ z,z+\u03b4z] which is well ap-\nproximated by \u03b4zPr(z) if\u03b4zis small. Since log \u03b4zPr(z) = log \u03b4z+log Pr( z),\nthis means we can just ignore the constant log \u03b4zand use log Pr( z) as our\nmeasure of message length, as we did above.\nThe preceding view of MDL for model selection says that we should\nchoose the model with highest posterior probability. However, many Bayes-\nians would instead do inference by sampling from the posterior distribution.\n7.9 Vapnik\u2013Chervonenkis Dimension\nA di\ufb03culty in using estimates of in-sample error is the need to specify the\nnumber of parameters (or the complexity) dused in the \ufb01t. Although the\ne\ufb00ective number of parameters introduced in Section 7.6 is useful for some\nnonlinear models, it is not fully general. The Vapnik\u2013Chervonenkis (VC)\ntheory provides such a general measure of complexity, and gives associated\nbounds on the optimism. Here we give a brief review of this theory.\nSuppose we have a class of functions {f(x,\u03b1)}indexed by a parameter\nvector \u03b1, with x\u2208IRp. Assume for now that fis an indicator function,\nthat is, takes the values 0 or 1. If \u03b1= (\u03b10,\u03b11) and fis the linear indi-\ncator function I(\u03b10+\u03b1T\n1x >0), then it seems reasonable to say that the\ncomplexity of the class fis the number of parameters p+ 1. But what\nabout f(x,\u03b1) =I(sin\u03b1\u2264x) where \u03b1is any real number and x\u2208IR? The\nfunction sin(50 \u2264x) is shown in Figure 7.5. This is a very wiggly function\nthat gets even rougher as the frequency \u03b1increases, but it has only one\nparameter: despite this, it doesn\u2019t seem reasonable to conclude that it has\nless complexity than the linear indicator function I(\u03b10+\u03b11x) inp= 1\ndimension.", "256": "238 7. Model Assessment and Selection\nFIGURE 7.6. The \ufb01rst three panels show that the class of lines in the plane\ncan shatter three points. The last panel shows that this class c annot shatter four\npoints, as no line will put the hollow points on one side and the solid points on\nthe other. Hence the VC dimension of the class of straight lines i n the plane is\nthree. Note that a class of nonlinear curves could shatter four p oints, and hence\nhas VC dimension greater than three.\nThe Vapnik\u2013Chervonenkis dimension is a way of measuring the com-\nplexity of a class of functions by assessing how wiggly its members can\nbe.\nTheVC dimension of the class {f(x,\u03b1)}is de\ufb01ned to be the\nlargest number of points (in some con\ufb01guration) that can be\nshattered by members of {f(x,\u03b1)}.\nA set of points is said to be shattered by a class of functions if, no matter\nhow we assign a binary label to each point, a member of the class can\nperfectly separate them.\nFigure 7.6 shows that the VC dimension of linear indicator functions\nin the plane is 3 but not 4, since no four points can be shattered by a\nset of lines. In general, a linear indicator function in pdimensions has VC\ndimension p+1, which is also the number of free parameters. On the other\nhand, it can be shown that the family sin( \u03b1x) has in\ufb01nite VC dimension,\nas Figure 7.5 suggests. By appropriate choice of \u03b1, any set of points can be\nshattered by this class (Exercise 7.8).\nSo far we have discussed the VC dimension only of indicator functions,\nbut this can be extended to real-valued functions. The VC dimension of a\nclass of real-valued functions {g(x,\u03b1)}is de\ufb01ned to be the VC dimension\nof the indicator class {I(g(x,\u03b1)\u2212\u03b2 >0)}, where \u03b2takes values over the\nrange of g.\nOne can use the VC dimension in constructing an estimate of (extra-\nsample) prediction error; di\ufb00erent types of results are available. Using the\nconcept of VC dimension, one can prove results about the optimism of the\ntraining error when using a class of functions. An example of such a result is\nthe following. If we \ufb01t Ntraining points using a class of functions {f(x,\u03b1)}\nhaving VC dimension h, then with probability at least 1 \u2212\u03b7over training", "257": "7.9 Vapnik\u2013Chervonenkis Dimension 239\nsets:\nErrT\u2264err +\u01eb\n2/parenleft\uf8ecig\n1 +/radicalbigg\n1 +4\u2264err\n\u01eb/parenright\uf8ecig\n(binary classi\ufb01cation)\nErrT\u2264err\n(1\u2212c\u221a\u01eb)+(regression) (7.46)\nwhere \u01eb=a1h[log (a2N/h) + 1]\u2212log (\u03b7/4)\nN,\nand 0 < a1\u22644,0< a2\u22642\nThese bounds hold simultaneously for all members f(x,\u03b1), and are taken\nfrom Cherkassky and Mulier (2007, pages 116\u2013118). They recommend the\nvalue c= 1. For regression they suggest a1=a2= 1, and for classi\ufb01cation\nthey make no recommendation, with a1= 4 and a2= 2 corresponding\nto worst-case scenarios. They also give an alternative practical bound for\nregression\nErrT\u2264err/parenleft\uf8ecigg\n1\u2212/radicalbigg\n\u03c1\u2212\u03c1log\u03c1+logN\n2N/parenright\uf8ecigg\u22121\n+(7.47)\nwith\u03c1=h\nN, which is free of tuning constants. The bounds suggest that the\noptimism increases with hand decreases with Nin qualitative agreement\nwith the AIC correction d/Ngiven is (7.24). However, the results in (7.46)\nare stronger: rather than giving the expected optimism for each \ufb01xed func-\ntionf(x,\u03b1), they give probabilistic upper bounds for all functions f(x,\u03b1),\nand hence allow for searching over the class.\nVapnik\u2019s structural risk minimization (SRM) approach \ufb01ts a nested se-\nquence of models of increasing VC dimensions h1< h2<\u2264\u2264\u2264, and then\nchooses the model with the smallest value of the upper bound.\nWe note that upper bounds like the ones in (7.46) are often very loose,\nbut that doesn\u2019t rule them out as good criteria for model selection, where\nthe relative (not absolute) size of the test error is important. The main\ndrawback of this approach is the di\ufb03culty in calculating the VC dimension\nof a class of functions. Often only a crude upper bound for VC dimension\nis obtainable, and this may not be adequate. An example in which the\nstructural risk minimization program can be successfully carried out is the\nsupport vector classi\ufb01er, discussed in Section 12.2.\n7.9.1 Example (Continued)\nFigure 7.7 shows the results when AIC, BIC and SRM are used to select\nthe model size for the examples of Figure 7.3. For the examples labeled KNN,\nthe model index \u03b1refers to neighborhood size, while for those labeled REG\u03b1\nrefers to subset size. Using each selection method (e.g., AIC) we estimated\nthe best model \u02c6 \u03b1and found its true prediction error Err T(\u02c6\u03b1) on a test\nset. For the same training set we computed the prediction error of the best", "258": "240 7. Model Assessment and Selection\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestAIC\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBIC\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestSRM\nFIGURE 7.7. Boxplots show the distribution of the relative error\n100\u00d7[Err T(\u02c6\u03b1)\u2212min \u03b1ErrT(\u03b1)]/[max \u03b1ErrT(\u03b1)\u2212min \u03b1ErrT(\u03b1)]over the four\nscenarios of Figure 7.3. This is the error in using the chosen mo del relative to\nthe best model. There are 100training sets each of size 80represented in each\nboxplot, with the errors computed on test sets of size 10,000.", "259": "7.10 Cross-Validation 241\nand worst possible model choices: min \u03b1ErrT(\u03b1) and max \u03b1ErrT(\u03b1). The\nboxplots show the distribution of the quantity\n100\u00d7ErrT(\u02c6\u03b1)\u2212min\u03b1ErrT(\u03b1)\nmax \u03b1ErrT(\u03b1)\u2212min\u03b1ErrT(\u03b1),\nwhich represents the error in using the chosen model relative to the best\nmodel. For linear regression the model complexity was measured by the\nnumber of features; as mentioned in Section 7.5, this underestimates the\ndf, since it does not charge for the search for the best model of that size.\nThis was also used for the VC dimension of the linear classi\ufb01er. For k-\nnearest neighbors, we used the quantity N/k. Under an additive-error re-\ngression model, this can be justi\ufb01ed as the exact e\ufb00ective degrees of free-\ndom (Exercise 7.6); we do not know if it corresponds to the VC dimen-\nsion. We used a1=a2= 1 for the constants in (7.46); the results for SRM\nchanged with di\ufb00erent constants, and this choice gave the most favorable re-\nsults. We repeated the SRM selection using the alternative practical bound\n(7.47), and got almost identical results. For misclassi\ufb01cation error w e used\n\u02c6\u03c3\u03b52= [N/(N\u2212d)]\u2264err(\u03b1) for the least restrictive model ( k= 5 for KNN,\nsincek= 1 results in zero training error). The AIC criterion seems to work\nwell in all four scenarios, despite the lack of theoretical support with 0\u20131\nloss. BIC does nearly as well, while the performance of SRM is mixed.\n7.10 Cross-Validation\nProbably the simplest and most widely used method for estimating predic-\ntion error is cross-validation. This method directly estimates the expected\nextra-sample error Err = E[ L(Y,\u02c6f(X))], the average generalization error\nwhen the method \u02c6f(X) is applied to an independent test sample from the\njoint distribution of XandY. As mentioned earlier, we might hope that\ncross-validation estimates the conditional error, with the training set T\nheld \ufb01xed. But as we will see in Section 7.12, cross-validation typically\nestimates well only the expected prediction error.\n7.10.1 K-Fold Cross-Validation\nIdeally, if we had enough data, we would set aside a validation set and use\nit to assess the performance of our prediction model. Since data are often\nscarce, this is usually not possible. To \ufb01nesse the problem, K-fold cross-\nvalidation uses part of the available data to \ufb01t the model, and a di\ufb00erent\npart to test it. We split the data into Kroughly equal-sized parts; for\nexample, when K= 5, the scenario looks like this:", "260": "242 7. Model Assessment and Selection\nValidation Train1 2 3 4 5\nTrain Train Train\nFor the kth part (third above), we \ufb01t the model to the other K\u22121 parts\nof the data, and calculate the prediction error of the \ufb01tted model when\npredicting the kth part of the data. We do this for k= 1,2,... ,K and\ncombine the Kestimates of prediction error.\nHere are more details. Let \u03ba:{1,... ,N } \u221dma\u221asto\u2192 { 1,... ,K }be an indexing\nfunction that indicates the partition to which observation iis allocated by\nthe randomization. Denote by \u02c6f\u2212k(x) the \ufb01tted function, computed with\nthekth part of the data removed. Then the cross-validation estimate of\nprediction error is\nCV(\u02c6f) =1\nNN/summationdisplay\ni=1L(yi,\u02c6f\u2212\u03ba(i)(xi)). (7.48)\nTypical choices of Kare 5 or 10 (see below). The case K=Nis known\nasleave-one-out cross-validation. In this case \u03ba(i) =i, and for the ith\nobservation the \ufb01t is computed using all the data except the ith.\nGiven a set of models f(x,\u03b1) indexed by a tuning parameter \u03b1, denote\nby\u02c6f\u2212k(x,\u03b1) the\u03b1th model \ufb01t with the kth part of the data removed. Then\nfor this set of models we de\ufb01ne\nCV(\u02c6f,\u03b1) =1\nNN/summationdisplay\ni=1L(yi,\u02c6f\u2212\u03ba(i)(xi,\u03b1)). (7.49)\nThe function CV( \u02c6f,\u03b1) provides an estimate of the test error curve, and we\n\ufb01nd the tuning parameter \u02c6 \u03b1that minimizes it. Our \ufb01nal chosen model is\nf(x,\u02c6\u03b1), which we then \ufb01t to all the data.\nIt is interesting to wonder about what quantity K-fold cross-validation\nestimates. With K= 5 or 10, we might guess that it estimates the ex-\npected error Err, since the training sets in each fold are quite di\ufb00erent\nfrom the original training set. On the other hand, if K=Nwe might\nguess that cross-validation estimates the conditional error Err T. It turns\nout that cross-validation only estimates e\ufb00ectively the average error Err,\nas discussed in Section 7.12.\nWhat value should we choose for K? With K=N, the cross-validation\nestimator is approximately unbiased for the true (expected) prediction er-\nror, but can have high variance because the N\u201ctraining sets\u201d are so similar\nto one another. The computational burden is also considerable, requiring\nNapplications of the learning method. In certain special problems, this\ncomputation can be done quickly\u2014see Exercises 7.3 and 5.13.", "261": "7.10 Cross-Validation 243\nSize of Training Set1-Err\n0 50 100 150 2000.0 0.2 0.4 0.6 0.8\nFIGURE 7.8. Hypothetical learning curve for a classi\ufb01er on a given task: a\nplot of 1\u2212Errversus the size of the training set N. With a dataset of 200\nobservations, 5-fold cross-validation would use training sets of size 160, which\nwould behave much like the full set. However, with a dataset o f50observations\n\ufb01vefold cross-validation would use training sets of size 40, and this would result\nin a considerable overestimate of prediction error.\nOn the other hand, with K= 5 say, cross-validation has lower variance.\nBut bias could be a problem, depending on how the performance of the\nlearning method varies with the size of the training set. Figure 7.8 shows\na hypothetical \u201clearning curve\u201d for a classi\ufb01er on a given task, a plot of\n1\u2212Err versus the size of the training set N. The performance of the\nclassi\ufb01er improves as the training set size increases to 100 observations;\nincreasing the number further to 200 brings only a small bene\ufb01t. If our\ntraining set had 200 observations, \ufb01vefold cross-validation would estimat e\nthe performance of our classi\ufb01er over training sets of size 160, which from\nFigure 7.8 is virtually the same as the performance for training set size\n200. Thus cross-validation would not su\ufb00er from much bias. However if the\ntraining set had 50 observations, \ufb01vefold cross-validation would estimate\nthe performance of our classi\ufb01er over training sets of size 40, and from the\n\ufb01gure that would be an underestimate of 1 \u2212Err. Hence as an estimate of\nErr, cross-validation would be biased upward.\nTo summarize, if the learning curve has a considerable slope at the given\ntraining set size, \ufb01ve- or tenfold cross-validation will overestimate the tr ue\nprediction error. Whether this bias is a drawback in practice depends on\nthe objective. On the other hand, leave-one-out cross-validation has low\nbias but can have high variance. Overall, \ufb01ve- or tenfold cross-validation\nare recommended as a good compromise: see Breiman and Spector (1992)\nand Kohavi (1995).\nFigure 7.9 shows the prediction error and tenfold cross-validation curve\nestimated from a single training set, from the scenario in the bottom righ t\npanel of Figure 7.3. This is a two-class classi\ufb01cation problem, using a lin-", "262": "244 7. Model Assessment and Selection\nSubset Size pMisclassification Error\n5 10 15 200.0 0.1 0.2 0.3 0.4 0.5 0.6\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022 \u2022\nFIGURE 7.9. Prediction error (orange) and tenfold cross-validation curve\n(blue) estimated from a single training set, from the scenario in the bottom right\npanel of Figure 7.3.\near model with best subsets regression of subset size p. Standard error bars\nare shown, which are the standard errors of the individual misclassi\ufb01cation\nerror rates for each of the ten parts. Both curves have minima at p= 10,\nalthough the CV curve is rather \ufb02at beyond 10. Often a \u201cone-standard\nerror\u201d rule is used with cross-validation, in which we choose the most par-\nsimonious model whose error is no more than one standard error above\nthe error of the best model. Here it looks like a model with about p= 9\npredictors would be chosen, while the true model uses p= 10.\nGeneralized cross-validation provides a convenient approximation to leave-\none out cross-validation, for linear \ufb01tting under squared-error loss. As de-\n\ufb01ned in Section 7.6, a linear \ufb01tting method is one for which we can write\n\u02c6y=Sy. (7.50)\nNow for many linear \ufb01tting methods,\n1\nNN/summationdisplay\ni=1[yi\u2212\u02c6f\u2212i(xi)]2=1\nNN/summationdisplay\ni=1/bracketleft\uf8ecigyi\u2212\u02c6f(xi)\n1\u2212Sii/bracketright\uf8ecig2\n, (7.51)\nwhere Siiis the ith diagonal element of S(see Exercise 7.3). The GCV\napproximation is\nGCV( \u02c6f) =1\nNN/summationdisplay\ni=1/bracketleft\uf8ecigg\nyi\u2212\u02c6f(xi)\n1\u2212trace(S)/N/bracketright\uf8ecigg2\n. (7.52)", "263": "7.10 Cross-Validation 245\nThe quantity trace( S) is the e\ufb00ective number of parameters, as de\ufb01ned in\nSection 7.6.\nGCV can have a computational advantage in some settings, where the\ntrace of Scan be computed more easily than the individual elements Sii.\nIn smoothing problems, GCV can also alleviate the tendency of cross-\nvalidation to undersmooth. The similarity between GCV and AIC can be\nseen from the approximation 1 /(1\u2212x)2\u22481 + 2x(Exercise 7.7).\n7.10.2 The Wrong and Right Way to Do Cross-validation\nConsider a classi\ufb01cation problem with a large number of predictors, as may\narise, for example, in genomic or proteomic applications. A typical strategy\nfor analysis might be as follows:\n1. Screen the predictors: \ufb01nd a subset of \u201cgood\u201d predictors that show\nfairly strong (univariate) correlation with the class labels\n2. Using just this subset of predictors, build a multivariate classi\ufb01er.\n3. Use cross-validation to estimate the unknown tuning parameters and\nto estimate the prediction error of the \ufb01nal model.\nIs this a correct application of cross-validation? Consider a scenario with\nN= 50 samples in two equal-sized classes, and p= 5000 quantitative\npredictors (standard Gaussian) that are independent of the class labels.\nThe true (test) error rate of any classi\ufb01er is 50%. We carried out the above\nrecipe, choosing in step (1) the 100 predictors having highest correlation\nwith the class labels, and then using a 1-nearest neighbor classi\ufb01er, based\non just these 100 predictors, in step (2). Over 50 simulations from this\nsetting, the average CV error rate was 3%. This is far lower than the true\nerror rate of 50%.\nWhat has happened? The problem is that the predictors have an unfair\nadvantage, as they were chosen in step (1) on the basis of all of the samples .\nLeaving samples out afterthe variables have been selected does not cor-\nrectly mimic the application of the classi\ufb01er to a completely independent\ntest set, since these predictors \u201chave already seen\u201d the left out samples.\nFigure 7.10 (top panel) illustrates the problem. We selected the 100 pre-\ndictors having largest correlation with the class labels over all 50 sampl es.\nThen we chose a random set of 10 samples, as we would do in \ufb01ve-fold cross-\nvalidation, and computed the correlations of the pre-selected 100 predictors\nwith the class labels over just these 10 samples (top panel). We see that\nthe correlations average about 0.28, rather than 0, as one might expect.\nHere is the correct way to carry out cross-validation in this example:\n1. Divide the samples into Kcross-validation folds (groups) at random.\n2. For each fold k= 1,2,... ,K", "264": "246 7. Model Assessment and Selection\nCorrelations of Selected Predictors with OutcomeFrequency\n\u22121.0 \u22120.5 0.0 0.5 1.00 10 20 30Wrong way\nCorrelations of Selected Predictors with OutcomeFrequency\n\u22121.0 \u22120.5 0.0 0.5 1.00 10 20 30Right way\nFIGURE 7.10. Cross-validation the wrong and right way: histograms shows th e\ncorrelation of class labels, in 10randomly chosen samples, with the 100predic-\ntors chosen using the incorrect (upper red) and correct (lower g reen) versions of\ncross-validation.\n(a) Find a subset of \u201cgood\u201d predictors that show fairly strong (uni-\nvariate) correlation with the class labels, using all of the samples\nexcept those in fold k.\n(b) Using just this subset of predictors, build a multivariate classi-\n\ufb01er, using all of the samples except those in fold k.\n(c) Use the classi\ufb01er to predict the class labels for the samples in\nfoldk.\nThe error estimates from step 2(c) are then accumulated over all Kfolds, to\nproduce the cross-validation estimate of prediction error. The lower panel\nof Figure 7.10 shows the correlations of class labels with the 100 predictor s\nchosen in step 2(a) of the correct procedure, over the samples in a typical\nfoldk. We see that they average about zero, as they should.\nIn general, with a multistep modeling procedure, cross-validation must\nbe applied to the entire sequence of modeling steps. In particular, samples\nmust be \u201cleft out\u201d before any selection or \ufb01ltering steps are applied. There\nis one quali\ufb01cation: initial unsupervised screening steps can be done be-\nfore samples are left out. For example, we could select the 1000 predictors", "265": "7.10 Cross-Validation 247\nwith highest variance across all 50 samples, before starting cross-valida tion.\nSince this \ufb01ltering does not involve the class labels, it does not give the\npredictors an unfair advantage.\nWhile this point may seem obvious to the reader, we have seen this\nblunder committed many times in published papers in top rank journals.\nWith the large numbers of predictors that are so common in genomic and\nother areas, the potential consequences of this error have also increased\ndramatically; see Ambroise and McLachlan (2002) for a detailed discussion\nof this issue.\n7.10.3 Does Cross-Validation Really Work?\nWe once again examine the behavior of cross-validation in a high-dimensional\nclassi\ufb01cation problem. Consider a scenario with N= 20 samples in two\nequal-sized classes, and p= 500 quantitative predictors that are indepen-\ndent of the class labels. Once again, the true error rate of any classi\ufb01er is\n50%. Consider a simple univariate classi\ufb01er: a single split that minimizes\nthe misclassi\ufb01cation error (a \u201cstump\u201d). Stumps are trees with a single split ,\nand are used in boosting methods (Chapter 10). A simple argument sug-\ngests that cross-validation will not work properly in this setting2:\nFitting to the entire training set, we will \ufb01nd a predictor th at\nsplits the data very well If we do 5-fold cross-validation, thi s\nsame predictor should split any 4/5ths and 1/5th of the data\nwell too, and hence its cross-validation error will be small ( much\nless than 50%) Thus CV does not give an accurate estimate of\nerror.\nTo investigate whether this argument is correct, Figure 7.11 shows the\nresult of a simulation from this setting. There are 500 predictors and 20\nsamples, in each of two equal-sized classes, with all predictors having a\nstandard Gaussian distribution. The panel in the top left shows the number\nof training errors for each of the 500 stumps \ufb01t to the training data. We\nhave marked in color the six predictors yielding the fewest errors. In the top\nright panel, the training errors are shown for stumps \ufb01t to a random 4 /5ths\npartition of the data (16 samples), and tested on the remaining 1 /5th (four\nsamples). The colored points indicate the same predictors marked in the\ntop left panel. We see that the stump for the blue predictor (whose stump\nwas the best in the top left panel), makes two out of four test errors (50%),\nand is no better than random.\nWhat has happened? The preceding argument has ignored the fact that\nin cross-validation, the model must be completely retrained for each fold\n2This argument was made to us by a scientist at a proteomics lab meeting, and led\nto material in this section.", "266": "248 7. Model Assessment and Selection\n0 100 200 300 400 5002 3 4 5 6 7 8 9\nPredictorError on Full Training Set\n1 2 3 4 5 6 7 80 1 2 3 4\nError on 4/5Error on 1/5\n\u22121 0 1 2\nPredictor 436 (blue)Class Label\n0 1\nfull\n4/5\n0.0 0.2 0.4 0.6 0.8 1.0\nCV Errors\nFIGURE 7.11. Simulation study to investigate the performance of cross vali-\ndation in a high-dimensional problem where the predictors are independent of the\nclass labels. The top-left panel shows the number of errors mad e by individual\nstump classi\ufb01ers on the full training set ( 20observations). The top right panel\nshows the errors made by individual stumps trained on a random sp lit of the\ndataset into 4/5ths (16observations) and tested on the remaining 1/5th (4ob-\nservations). The best performers are depicted by colored dot s in each panel. The\nbottom left panel shows the e\ufb00ect of re-estimating the split po int in each fold: the\ncolored points correspond to the four samples in the 4/5ths validation set. The\nsplit point derived from the full dataset classi\ufb01es all four sa mples correctly, but\nwhen the split point is re-estimated on the 4/5ths data (as it should be), it com-\nmits two errors on the four validation samples. In the bottom right we see the\noverall result of \ufb01ve-fold cross-validation applied to 50simulated datasets. The\naverage error rate is about 50%, as it should be.", "267": "7.11 Bootstrap Methods 249\nof the process. In the present example, this means that the best predictor\nand corresponding split point are found from 4 /5ths of the data. The e\ufb00ect\nof predictor choice is seen in the top right panel. Since the class labels are\nindependent of the predictors, the performance of a stump on the 4 /5ths\ntraining data contains no information about its performance in the remain-\ning 1/5th. The e\ufb00ect of the choice of split point is shown in the bottom left\npanel. Here we see the data for predictor 436, corresponding to the blue\ndot in the top left plot. The colored points indicate the 1 /5th data, while\nthe remaining points belong to the 4 /5ths. The optimal split points for this\npredictor based on both the full training set and 4 /5ths data are indicated.\nThe split based on the full data makes no errors on the 1 /5ths data. But\ncross-validation must base its split on the 4 /5ths data, and this incurs two\nerrors out of four samples.\nThe results of applying \ufb01ve-fold cross-validation to each of 50 simulated\ndatasets is shown in the bottom right panel. As we would hope, the average\ncross-validation error is around 50%, which is the true expected prediction\nerror for this classi\ufb01er. Hence cross-validation has behaved as it should.\nOn the other hand, there is considerable variability in the error, underscor-\ning the importance of reporting the estimated standard error of the CV\nestimate. See Exercise 7.10 for another variation of this problem.\n7.11 Bootstrap Methods\nThe bootstrap is a general tool for assessing statistical accuracy. Firs t we\ndescribe the bootstrap in general, and then show how it can be used to\nestimate extra-sample prediction error. As with cross-validation, the boo t-\nstrap seeks to estimate the conditional error Err T, but typically estimates\nwell only the expected prediction error Err.\nSuppose we have a model \ufb01t to a set of training data. We denote the\ntraining set by Z= (z1,z2,... ,z N) where zi= (xi,yi). The basic idea is\nto randomly draw datasets with replacement from the training data, each\nsample the same size as the original training set. This is done Btimes\n(B= 100 say), producing Bbootstrap datasets, as shown in Figure 7.12.\nThen we re\ufb01t the model to each of the bootstrap datasets, and examine\nthe behavior of the \ufb01ts over the Breplications.\nIn the \ufb01gure, S(Z) is any quantity computed from the data Z, for ex-\nample, the prediction at some input point. From the bootstrap sampling\nwe can estimate any aspect of the distribution of S(Z), for example, its\nvariance,\n/hatwidestVar[S(Z)] =1\nB\u22121B/summationdisplay\nb=1(S(Z\u2217b)\u2212\u00afS\u2217)2, (7.53)", "268": "250 7. Model Assessment and Selection\n  Bootstrap\nBootstrapreplications\nsamples\nsampleTrainingZ= (z1,z2,... ,z N)Z\u22171Z\u22172Z\u2217BS(Z\u22171) S(Z\u22172) S(Z\u2217B)\nFIGURE 7.12. Schematic of the bootstrap process. We wish to assess the sta-\ntistical accuracy of a quantity S(Z)computed from our dataset. Btraining sets\nZ\u2217b, b= 1, . . . , B each of size Nare drawn with replacement from the original\ndataset. The quantity of interest S(Z)is computed from each bootstrap training\nset, and the values S(Z\u22171), . . . , S (Z\u2217B)are used to assess the statistical accuracy\nofS(Z).\nwhere \u00afS\u2217=/summationtext\nbS(Z\u2217b)/B. Note that /hatwidestVar[S(Z)] can be thought of as a\nMonte-Carlo estimate of the variance of S(Z) under sampling from the\nempirical distribution function \u02c6Ffor the data ( z1,z2,... ,z N).\nHow can we apply the bootstrap to estimate prediction error? One ap-\nproach would be to \ufb01t the model in question on a set of bootstrap samples,\nand then keep track of how well it predicts the original training set. If\n\u02c6f\u2217b(xi) is the predicted value at xi, from the model \ufb01tted to the bth boot-\nstrap dataset, our estimate is\n/hatwidestErrboot=1\nB1\nNB/summationdisplay\nb=1N/summationdisplay\ni=1L(yi,\u02c6f\u2217b(xi)). (7.54)\nHowever, it is easy to see that /hatwidestErrbootdoes not provide a good estimate in\ngeneral. The reason is that the bootstrap datasets are acting as the training\nsamples, while the original training set is acting as the test sample, and\nthese two samples have observations in common. This overlap can make\nover\ufb01t predictions look unrealistically good, and is the reason that cross-\nvalidation explicitly uses non-overlapping data for the training and test\nsamples. Consider for example a 1-nearest neighbor classi\ufb01er applied to a\ntwo-class classi\ufb01cation problem with the same number of observations in", "269": "7.11 Bootstrap Methods 251\neach class, in which the predictors and class labels are in fact independent.\nThen the true error rate is 0 .5. But the contributions to the bootstrap\nestimate /hatwidestErrbootwill be zero unless the observation idoes not appear in the\nbootstrap sample b. In this latter case it will have the correct expectation\n0.5. Now\nPr{observation i\u2208bootstrap sample b}= 1\u2212/parenleft\uf8ecig\n1\u22121\nN/parenright\uf8ecigN\n\u22481\u2212e\u22121\n= 0.632. (7.55)\nHence the expectation of /hatwidestErrbootis about 0 .5\u00d70.368 = 0 .184, far below\nthe correct error rate 0 .5.\nBy mimicking cross-validation, a better bootstrap estimate can be ob-\ntained. For each observation, we only keep track of predictions from boot-\nstrap samples not containing that observation. The leave-one-out bootstrap\nestimate of prediction error is de\ufb01ned by\n/hatwidestErr(1)=1\nNN/summationdisplay\ni=11\n|C\u2212i|/summationdisplay\nb\u2208C\u2212iL(yi,\u02c6f\u2217b(xi)). (7.56)\nHereC\u2212iis the set of indices of the bootstrap samples bthat do notcontain\nobservation i, and|C\u2212i|is the number of such samples. In computing /hatwidestErr(1),\nwe either have to choose Blarge enough to ensure that all of the |C\u2212i|are\ngreater than zero, or we can just leave out the terms in (7.56) corresponding\nto|C\u2212i|\u2019s that are zero.\nThe leave-one out bootstrap solves the over\ufb01tting problem su\ufb00ered by\n/hatwidestErrboot, but has the training-set-size bias mentioned in the discussion of\ncross-validation. The average number of distinct observations in each boot-\nstrap sample is about 0 .632\u2264N, so its bias will roughly behave like that of\ntwofold cross-validation. Thus if the learning curve has considerable slope\nat sample size N/2, the leave-one out bootstrap will be biased upward as\nan estimate of the true error.\nThe \u201c .632 estimator\u201d is designed to alleviate this bias. It is de\ufb01ned by\n/hatwidestErr(.632)=.368\u2264err +.632\u2264/hatwidestErr(1). (7.57)\nThe derivation of the .632 estimator is complex; intuitively it pulls the\nleave-one out bootstrap estimate down toward the training error rate, and\nhence reduces its upward bias. The use of the constant .632 relates to (7.55).\nThe.632 estimator works well in \u201clight \ufb01tting\u201d situations, but can break\ndown in over\ufb01t ones. Here is an example due to Breiman et al. (1984).\nSuppose we have two equal-size classes, with the targets independent of\nthe class labels, and we apply a one-nearest neighbor rule. Then err = 0,", "270": "252 7. Model Assessment and Selection\n/hatwidestErr(1)= 0.5 and so /hatwidestErr(.632)=.632\u00d70.5 =.316. However, the true error\nrate is 0.5.\nOne can improve the .632 estimator by taking into account the amount\nof over\ufb01tting. First we de\ufb01ne \u03b3to be the no-information error rate : this\nis the error rate of our prediction rule if the inputs and class labels were\nindependent. An estimate of \u03b3is obtained by evaluating the prediction rule\non all possible combinations of targets yiand predictors xi\u2032\n\u02c6\u03b3=1\nN2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1L(yi,\u02c6f(xi\u2032)). (7.58)\nFor example, consider the dichotomous classi\ufb01cation problem: let \u02c6 p1be\nthe observed proportion of responses yiequaling 1, and let \u02c6 q1be the ob-\nserved proportion of predictions \u02c6f(xi\u2032) equaling 1. Then\n\u02c6\u03b3= \u02c6p1(1\u2212\u02c6q1) + (1 \u2212\u02c6p1)\u02c6q1. (7.59)\nWith a rule like 1-nearest neighbors for which \u02c6 q1= \u02c6p1the value of \u02c6 \u03b3is\n2\u02c6p1(1\u2212\u02c6p1). The multi-category generalization of (7.59) is \u02c6 \u03b3=/summationtext\n\u2113\u02c6p\u2113(1\u2212\u02c6q\u2113).\nUsing this, the relative over\ufb01tting rate is de\ufb01ned to be\n\u02c6R=/hatwidestErr(1)\u2212err\n\u02c6\u03b3\u2212err, (7.60)\na quantity that ranges from 0 if there is no over\ufb01tting ( /hatwidestErr(1)=err) to 1\nif the over\ufb01tting equals the no-information value \u02c6 \u03b3\u2212err. Finally, we de\ufb01ne\nthe \u201c.632+\u201d estimator by\n/hatwidestErr(.632+)= (1 \u2212\u02c6w)\u2264err + \u02c6w\u2264/hatwidestErr(1)(7.61)\nwith \u02c6w=.632\n1\u2212.368\u02c6R.\nThe weight wranges from .632 if \u02c6R= 0 to 1 if \u02c6R= 1, so /hatwidestErr(.632+)\nranges from /hatwidestErr(.632)to/hatwidestErr(1). Again, the derivation of (7.61) is compli-\ncated: roughly speaking, it produces a compromise between the leave-one-\nout bootstrap and the training error rate that depends on the amount of\nover\ufb01tting. For the 1-nearest-neighbor problem with class labels indepen-\ndent of the inputs, \u02c6 w=\u02c6R= 1, so /hatwidestErr(.632+)=/hatwidestErr(1), which has the correct\nexpectation of 0.5. In other problems with less over\ufb01tting, /hatwidestErr(.632+)will\nlie somewhere between err and /hatwidestErr(1).\n7.11.1 Example (Continued)\nFigure 7.13 shows the results of tenfold cross-validation and the .632+ bo ot-\nstrap estimate in the same four problems of Figures 7.7. As in that \ufb01gure,", "271": "7.11 Bootstrap Methods 253\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestCross\u2212validation\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBootstrap\nFIGURE 7.13. Boxplots show the distribution of the relative error\n100\u2264[Err\u02c6\u03b1\u2212min \u03b1Err(\u03b1)]/[max \u03b1Err(\u03b1)\u2212min \u03b1Err(\u03b1)]over the four scenar-\nios of Figure 7.3. This is the error in using the chosen model re lative to the best\nmodel. There are 100training sets represented in each boxplot.\nFigure 7.13 shows boxplots of 100 \u2264[Err\u02c6\u03b1\u2212min\u03b1Err(\u03b1)]/[max \u03b1Err(\u03b1)\u2212\nmin\u03b1Err(\u03b1)], the error in using the chosen model relative to the best model.\nThere are 100 di\ufb00erent training sets represented in each boxplot. Both mea-\nsures perform well overall, perhaps the same or slightly worse that the AI C\nin Figure 7.7.\nOur conclusion is that for these particular problems and \ufb01tting methods,\nminimization of either AIC, cross-validation or bootstrap yields a model\nfairly close to the best available. Note that for the purpose of model selec-\ntion, any of the measures could be biased and it wouldn\u2019t a\ufb00ect things, as\nlong as the bias did not change the relative performance of the methods.\nFor example, the addition of a constant to any of the measures would not\nchange the resulting chosen model. However, for many adaptive, nonlinear\ntechniques (like trees), estimation of the e\ufb00ective number of parameters is\nvery di\ufb03cult. This makes methods like AIC impractical and leaves us with\ncross-validation or bootstrap as the methods of choice.\nA di\ufb00erent question is: how well does each method estimate test error?\nOn the average the AIC criterion overestimated prediction error of its cho-", "272": "254 7. Model Assessment and Selection\nsen model by 38%, 37%, 51%, and 30%, respectively, over the four scenarios,\nwith BIC performing similarly. In contrast, cross-validation over estimated\nthe error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the\nsame. Hence the extra work involved in computing a cross-validation or\nbootstrap measure is worthwhile, if an accurate estimate of test error i s\nrequired. With other \ufb01tting methods like trees, cross-validation and boot-\nstrap can underestimate the true error by 10%, because the search for best\ntree is strongly a\ufb00ected by the validation set. In these situations only a\nseparate test set will provide an unbiased estimate of test error.\n7.12 Conditional or Expected Test Error?\nFigures 7.14 and 7.15 examine the question of whether cross-validation does\na good job in estimating Err T, the error conditional on a given training set\nT(expression (7.15) on page 228), as opposed to the expected test error.\nFor each of 100 training sets generated from the \u201creg/linear\u201d setting in\nthe top-right panel of Figure 7.3, Figure 7.14 shows the conditional error\ncurves Err Tas a function of subset size (top left). The next two panels show\n10-fold and N-fold cross-validation, the latter also known as leave-one-out\n(LOO). The thick red curve in each plot is the expected error Err, while\nthe thick black curves are the expected cross-validation curves. The lower\nright panel shows how well cross-validation approximates the conditional\nand expected error.\nOne might have expected N-fold CV to approximate Err Twell, since it\nalmost uses the full training sample to \ufb01t a new test point. 10-fold CV, on\nthe other hand, might be expected to estimate Err well, since it averages\nover somewhat di\ufb00erent training sets. From the \ufb01gure it appears 10-fold\ndoes a better job than N-fold in estimating Err T, and estimates Err even\nbetter. Indeed, the similarity of the two black curves with the red curve\nsuggests both CV curves are approximately unbiased for Err, with 10-fold\nhaving less variance. Similar trends were reported by Efron (1983).\nFigure 7.15 shows scatterplots of both 10-fold and N-fold cross-validation\nerror estimates versus the true conditional error for the 100 simulations.\nAlthough the scatterplots do not indicate much correlation, the lower right\npanel shows that for the most part the correlations are negative, a curi-\nous phenomenon that has been observed before. This negative correlation\nexplains why neither form of CV estimates Err Twell. The broken lines in\neach plot are drawn at Err( p), the expected error for the best subset of\nsizep. We see again that both forms of CV are approximately unbiased for\nexpected error, but the variation in test error for di\ufb00erent training sets is\nquite substantial.\nAmong the four experimental conditions in 7.3, this \u201creg/linear\u201d scenario\nshowed the highest correlation between actual and predicted test error. This", "273": "7.12 Conditional or Expected Test Error? 255\n5 10 15 200.1 0.2 0.3 0.4Prediction Error\nSubset Size pError\n5 10 15 200.1 0.2 0.3 0.410\u2212Fold CV Error\nSubset Size pError\n5 10 15 200.1 0.2 0.3 0.4Leave\u2212One\u2212Out CV Error\nSubset Size pError\n5 10 15 200.015 0.025 0.035 0.045Approximation Error\nSubset Size pMean Absolute DeviationET|CV10\u2212Err|\nET|CV10\u2212ErrT|\nET|CVN\u2212ErrT|\nFIGURE 7.14. Conditional prediction-error ErrT,10-fold cross-validation, and\nleave-one-out cross-validation curves for a 100simulations from the top-right\npanel in Figure 7.3. The thick red curve is the expected predict ion error Err,\nwhile the thick black curves are the expected CV curves ETCV10andETCVN.\nThe lower-right panel shows the mean absolute deviation of th e CV curves from\nthe conditional error, ET|CVK\u2212ErrT|forK= 10(blue) and K=N(green),\nas well as from the expected error ET|CV10\u2212Err|(orange).", "274": "256 7. Model Assessment and Selection\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 1\nPrediction ErrorCV Error\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 5\nPrediction ErrorCV Error\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 10\nPrediction ErrorCV Error\n5 10 15 20\u22120.6 \u22120.4 \u22120.2 0.0 0.2\nSubset SizeCorrelation\nLeave\u2212one\u2212out\n10\u2212Fold\nFIGURE 7.15. Plots of the CV estimates of error versus the true conditional\nerror for each of the 100training sets, for the simulation setup in the top right\npanel Figure 7.3. Both 10-fold and leave-one-out CV are depicted in di\ufb00erent\ncolors. The \ufb01rst three panels correspond to di\ufb00erent subset size sp, and vertical\nand horizontal lines are drawn at Err(p). Although there appears to be little cor-\nrelation in these plots, we see in the lower right panel that fo r the most part the\ncorrelation is negative .", "275": "Exercises 257\nphenomenon also occurs for bootstrap estimates of error, and we would\nguess, for any other estimate of conditional prediction error.\nWe conclude that estimation of test error for a particular training set is\nnot easy in general, given just the data from that same training set. Instead,\ncross-validation and related methods may provide reasonable estimates of\ntheexpected error Err.\nBibliographic Notes\nKey references for cross-validation are Stone (1974), Stone (1977) and\nAllen (1974). The AIC was proposed by Akaike (1973), while the BIC\nwas introduced by Schwarz (1978). Madigan and Raftery (1994) give an\noverview of Bayesian model selection. The MDL criterion is due to Rissa-\nnen (1983). Cover and Thomas (1991) contains a good description of coding\ntheory and complexity. VC dimension is described in Vapnik (1996). Stone\n(1977) showed that the AIC and leave-one out cross-validation are asymp-\ntotically equivalent. Generalized cross-validation is described by Golub et\nal. (1979) and Wahba (1980); a further discussion of the topic may be found\nin the monograph by Wahba (1990). See also Hastie and Tibshirani (1990),\nChapter 3. The bootstrap is due to Efron (1979); see Efron and Tibshi-\nrani (1993) for an overview. Efron (1983) proposes a number of bootst rap\nestimates of prediction error, including the optimism and .632 estimates.\nEfron (1986) compares CV, GCV and bootstrap estimates of error rates.\nThe use of cross-validation and the bootstrap for model selection is stud-\nied by Breiman and Spector (1992), Breiman (1992), Shao (1996), Zhang\n(1993) and Kohavi (1995). The .632+ estimator was proposed by Efron\nand Tibshirani (1997).\nCherkassky and Ma (2003) published a study on the performance of\nSRM for model selection in regression, in response to our study of section\n7.9.1. They complained that we had been unfair to SRM because had not\napplied it properly. Our response can be found in the same issue of the\njournal (Hastie et al. (2003)).\nExercises\nEx. 7.1 Derive the estimate of in-sample error (7.24).\nEx. 7.2 For 0\u20131 loss with Y\u2208 {0,1}and Pr( Y= 1|x0) =f(x0), show that\nErr(x0) = Pr( Y\u221dne}ationslash=\u02c6G(x0)|X=x0)\n= Err B(x0) +|2f(x0)\u22121|Pr(\u02c6G(x0)\u221dne}ationslash=G(x0)|X=x0),\n(7.62)", "276": "258 7. Model Assessment and Selection\nwhere \u02c6G(x) =I(\u02c6f(x)>1\n2),G(x) =I(f(x)>1\n2) is the Bayes classi\ufb01er,\nand Err B(x0) = Pr( Y\u221dne}ationslash=G(x0)|X=x0), the irreducible Bayes error atx0.\nUsing the approximation \u02c6f(x0)\u223cN(E\u02c6f(x0),Var(\u02c6f(x0)), show that\nPr(\u02c6G(x0)\u221dne}ationslash=G(x0)|X=x0)\u2248\u03a6/parenleft\uf8ecigg\nsign(1\n2\u2212f(x0))(E\u02c6f(x0)\u22121\n2)/radical\uf8ecig\nVar(\u02c6f(x0))/parenright\uf8ecigg\n.(7.63)\nIn the above,\n\u03a6(t) =1\u221a\n2\u03c0/integraldisplayt\n\u2212\u221eexp(\u2212t2/2)dt,\nthe cumulative Gaussian distribution function. This is an increasing func-\ntion, with value 0 at t=\u2212\u221eand value 1 at t= +\u221e.\nWe can think of sign(1\n2\u2212f(x0))(E\u02c6f(x0)\u22121\n2) as a kind of boundary-\nbiasterm, as it depends on the true f(x0) only through which side of the\nboundary (1\n2) that it lies. Notice also that the bias and variance combine\nin a multiplicative rather than additive fashion. If E \u02c6f(x0) is on the same\nside of1\n2asf(x0), then the bias is negative, and decreasing the variance\nwill decrease the misclassi\ufb01cation error. On the other hand, if E \u02c6f(x0) is\non the opposite side of1\n2tof(x0), then the bias is positive and it pays to\nincrease the variance! Such an increase will improve the chance that \u02c6f(x0)\nfalls on the correct side of1\n2(Friedman, 1997).\nEx. 7.3 Let\u02c6f=Sybe a linear smoothing of y.\n(a) IfSiiis the ith diagonal element of S, show that for Sarising from least\nsquares projections and cubic smoothing splines, the cross-validated\nresidual can be written as\nyi\u2212\u02c6f\u2212i(xi) =yi\u2212\u02c6f(xi)\n1\u2212Sii. (7.64)\n(b) Use this result to show that |yi\u2212\u02c6f\u2212i(xi)| \u2265 |yi\u2212\u02c6f(xi)|.\n(c) Find general conditions on any smoother Sto make result (7.64) hold.\nEx. 7.4 Consider the in-sample prediction error (7.18) and the training\nerrorerr in the case of squared-error loss:\nErrin=1\nNN/summationdisplay\ni=1EY0(Y0\ni\u2212\u02c6f(xi))2\nerr =1\nNN/summationdisplay\ni=1(yi\u2212\u02c6f(xi))2.", "277": "Exercises 259\nAdd and subtract f(xi) and E \u02c6f(xi) in each expression and expand. Hence\nestablish that the average optimism in the training error is\n2\nNN/summationdisplay\ni=1Cov(\u02c6yi,yi),\nas given in (7.21).\nEx. 7.5 For a linear smoother \u02c6y=Sy, show that\nN/summationdisplay\ni=1Cov(\u02c6yi,yi) = trace( S)\u03c32\n\u03b5, (7.65)\nwhich justi\ufb01es its use as the e\ufb00ective number of parameters.\nEx. 7.6 Show that for an additive-error model, the e\ufb00ective degrees-of-\nfreedom for the k-nearest-neighbors regression \ufb01t is N/k.\nEx. 7.7 Use the approximation 1 /(1\u2212x)2\u22481+2xto expose the relationship\nbetween Cp/AIC (7.26) and GCV (7.52), the main di\ufb00erence being the\nmodel used to estimate the noise variance \u03c32\n\u03b5.\nEx. 7.8 Show that the set of functions {I(sin(\u03b1x)>0)}can shatter the\nfollowing points on the line:\nz1= 10\u22121,... ,z\u2113= 10\u2212\u2113, (7.66)\nfor any \u2113. Hence the VC dimension of the class {I(sin(\u03b1x)>0)}is in\ufb01nite.\nEx. 7.9 For the prostate data of Chapter 3, carry out a best-subset linear\nregression analysis, as in Table 3.3 (third column from left). Compute t he\nAIC, BIC, \ufb01ve- and tenfold cross-validation, and bootstrap .632 estimat es\nof prediction error. Discuss the results.\nEx. 7.10 Referring to the example in Section 7.10.3, suppose instead that\nall of the ppredictors are binary, and hence there is no need to estimate\nsplit points. The predictors are independent of the class labels as before.\nThen if pis very large, we can probably \ufb01nd a predictor that splits the\nentire training data perfectly, and hence would split the validation data\n(one-\ufb01fth of data) perfectly as well. This predictor would therefore have\nzero cross-validation error. Does this mean that cross-validation does not\nprovide a good estimate of test error in this situation? [This question wa s\nsuggested by Li Ma.]", "278": "260 7. Model Assessment and Selection", "279": "This is page 261\nPrinter: Opaque this\n8\nModel Inference and Averaging\n8.1 Introduction\nFor most of this book, the \ufb01tting (learning) of models has been achieved by\nminimizing a sum of squares for regression, or by minimizing cross-entropy\nfor classi\ufb01cation. In fact, both of these minimizations are instances of the\nmaximum likelihood approach to \ufb01tting.\nIn this chapter we provide a general exposition of the maximum likeli-\nhood approach, as well as the Bayesian method for inference. The boot-\nstrap, introduced in Chapter 7, is discussed in this context, and its relation\nto maximum likelihood and Bayes is described. Finally, we present some\nrelated techniques for model averaging and improvement, including com-\nmittee methods, bagging, stacking and bumping.\n8.2 The Bootstrap and Maximum Likelihood\nMethods\n8.2.1 A Smoothing Example\nThe bootstrap method provides a direct computational way of assessing\nuncertainty, by sampling from the training data. Here we illustrate the\nbootstrap in a simple one-dimensional smoothing problem, and show its\nconnection to maximum likelihood.", "280": "262 8. Model Inference and Averaging\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.2 0.4 0.6 0.8 1.0\nxB-spline Basis\nFIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of\nseven B-spline basis functions. The broken vertical lines indicate the p lacement\nof the three knots.\nDenote the training data by Z={z1,z2,... ,z N}, with zi= (xi,yi),\ni= 1,2,... ,N . Here xiis a one-dimensional input, and yithe outcome,\neither continuous or categorical. As an example, consider the N= 50 data\npoints shown in the left panel of Figure 8.1.\nSuppose we decide to \ufb01t a cubic spline to the data, with three knots\nplaced at the quartiles of the Xvalues. This is a seven-dimensional lin-\near space of functions, and can be represented, for example, by a linear\nexpansion of B-spline basis functions (see Section 5.9.2):\n\u03b8(x) =7/summationdisplay\nj=1\u03b2jhj(x). (8.1)\nHere the hj(x),j= 1,2,... ,7 are the seven functions shown in the right\npanel of Figure 8.1. We can think of \u03b8(x) as representing the conditional\nmean E( Y|X=x).\nLetHbe the N\u00d77 matrix with ijth element hj(xi). The usual estimate\nof\u03b2, obtained by minimizing the squared error over the training set, is\ngiven by\n\u02c6\u03b2= (HTH)\u22121HTy. (8.2)\nThe corresponding \ufb01t \u02c6 \u03b8(x) =/summationtext7\nj=1\u02c6\u03b2jhj(x) is shown in the top left panel\nof Figure 8.2.\nThe estimated covariance matrix of \u02c6\u03b2is\n/hatwidestVar(\u02c6\u03b2) = (HTH)\u22121\u02c6\u03c32, (8.3)\nwhere we have estimated the noise variance by \u02c6 \u03c32=/summationtextN\ni=1(yi\u2212\u02c6\u03b8(xi))2/N.\nLetting h(x)T= (h1(x),h2(x),... ,h 7(x)), the standard error of a predic-", "281": "8.2 The Bootstrap and Maximum Likelihood Methods 263\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\nxy\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\nxy\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\nFIGURE 8.2. (Top left:) B-spline smooth of data. (Top right:) B-spline smooth\nplus and minus 1.96\u00d7standard error bands. (Bottom left:) Ten bootstrap repli-\ncates of the B-spline smooth. (Bottom right:) B-spline smooth with 95% standard\nerror bands computed from the bootstrap distribution.", "282": "264 8. Model Inference and Averaging\ntion \u02c6\u03b8(x) =h(x)T\u02c6\u03b2is\n/hatwidese[\u02c6\u03b8(x)] = [h(x)T(HTH)\u22121h(x)]1\n2\u02c6\u03c3. (8.4)\nIn the top right panel of Figure 8.2 we have plotted \u02c6 \u03b8(x)\u00b11.96\u2264/hatwidese[\u02c6\u03b8(x)].\nSince 1.96 is the 97.5% point of the standard normal distribution, these\nrepresent approximate 100 \u22122\u00d72.5% = 95% pointwise con\ufb01dence bands\nfor\u03b8(x).\nHere is how we could apply the bootstrap in this example. We draw B\ndatasets each of size N= 50 with replacement from our training data, the\nsampling unit being the pair zi= (xi,yi). To each bootstrap dataset Z\u2217\nwe \ufb01t a cubic spline \u02c6 \u03b8\u2217(x); the \ufb01ts from ten such samples are shown in the\nbottom left panel of Figure 8.2. Using B= 200 bootstrap samples, we can\nform a 95% pointwise con\ufb01dence band from the percentiles at each x: we\n\ufb01nd the 2 .5%\u00d7200 = \ufb01fth largest and smallest values at each x. These are\nplotted in the bottom right panel of Figure 8.2. The bands look similar to\nthose in the top right, being a little wider at the endpoints.\nThere is actually a close connection between the least squares estimates\n(8.2) and (8.3), the bootstrap, and maximum likelihood. Suppose we further\nassume that the model errors are Gaussian,\nY=\u03b8(X) +\u03b5;\u03b5\u223cN(0,\u03c32),\n\u03b8(x) =7/summationdisplay\nj=1\u03b2jhj(x). (8.5)\nThe bootstrap method described above, in which we sample with re-\nplacement from the training data, is called the nonparametric bootstrap .\nThis really means that the method is \u201cmodel-free,\u201d since it uses the raw\ndata, not a speci\ufb01c parametric model, to generate new datasets. Consider\na variation of the bootstrap, called the parametric bootstrap , in which we\nsimulate new responses by adding Gaussian noise to the predicted values:\ny\u2217\ni= \u02c6\u03b8(xi) +\u03b5\u2217\ni;\u03b5\u2217\ni\u223cN(0,\u02c6\u03c32);i= 1,2,... ,N. (8.6)\nThis process is repeated Btimes, where B= 200 say. The resulting boot-\nstrap datasets have the form ( x1,y\u2217\n1),... ,(xN,y\u2217\nN) and we recompute the\nB-spline smooth on each. The con\ufb01dence bands from this method will ex-\nactly equal the least squares bands in the top right panel, as the number of\nbootstrap samples goes to in\ufb01nity. A function estimated from a bootstrap\nsample y\u2217is given by \u02c6 \u03b8\u2217(x) =h(x)T(HTH)\u22121HTy\u2217, and has distribution\n\u02c6\u03b8\u2217(x)\u223cN(\u02c6\u03b8(x),h(x)T(HTH)\u22121h(x)\u02c6\u03c32). (8.7)\nNotice that the mean of this distribution is the least squares estimate, and\nthe standard deviation is the same as the approximate formula (8.4).", "283": "8.2 The Bootstrap and Maximum Likelihood Methods 265\n8.2.2 Maximum Likelihood Inference\nIt turns out that the parametric bootstrap agrees with least squares in the\nprevious example because the model (8.5) has additive Gaussian errors. In\ngeneral, the parametric bootstrap agrees not with least squares but with\nmaximum likelihood, which we now review.\nWe begin by specifying a probability density or probability mass function\nfor our observations\nzi\u223cg\u03b8(z). (8.8)\nIn this expression \u03b8represents one or more unknown parameters that gov-\nern the distribution of Z. This is called a parametric model forZ. As an\nexample, if Zhas a normal distribution with mean \u03b8and variance \u03c32, then\n\u03b8= (\u03b8,\u03c32), (8.9)\nand\ng\u03b8(z) =1\u221a\n2\u03c0\u03c3e\u22121\n2(z\u2212\u03b8)2/\u03c32. (8.10)\nMaximum likelihood is based on the likelihood function , given by\nL(\u03b8;Z) =N/productdisplay\ni=1g\u03b8(zi), (8.11)\nthe probability of the observed data under the model g\u03b8. The likelihood is\nde\ufb01ned only up to a positive multiplier, which we have taken to be one.\nWe think of L(\u03b8;Z) as a function of \u03b8, with our data Z\ufb01xed.\nDenote the logarithm of L(\u03b8;Z) by\n\u2113(\u03b8;Z) =N/summationdisplay\ni=1\u2113(\u03b8;zi)\n=N/summationdisplay\ni=1logg\u03b8(zi), (8.12)\nwhich we will sometimes abbreviate as \u2113(\u03b8). This expression is called the\nlog-likelihood, and each value \u2113(\u03b8;zi) = log g\u03b8(zi) is called a log-likelihood\ncomponent. The method of maximum likelihood chooses the value \u03b8=\u02c6\u03b8\nto maximize \u2113(\u03b8;Z).\nThe likelihood function can be used to assess the precision of \u02c6\u03b8. We need\na few more de\ufb01nitions. The score function is de\ufb01ned by\n\u02d9\u2113(\u03b8;Z) =N/summationdisplay\ni=1\u02d9\u2113(\u03b8;zi), (8.13)", "284": "266 8. Model Inference and Averaging\nwhere \u02d9\u2113(\u03b8;zi) =\u2202\u2113(\u03b8;zi)/\u2202\u03b8. Assuming that the likelihood takes its maxi-\nmum in the interior of the parameter space, \u02d9\u2113(\u02c6\u03b8;Z) = 0. The information\nmatrix is\nI(\u03b8) =\u2212N/summationdisplay\ni=1\u22022\u2113(\u03b8;zi)\n\u2202\u03b8\u2202\u03b8T. (8.14)\nWhenI(\u03b8) is evaluated at \u03b8=\u02c6\u03b8, it is often called the observed information .\nTheFisher information (or expected information) is\ni(\u03b8) = E \u03b8[I(\u03b8)]. (8.15)\nFinally, let \u03b80denote the true value of \u03b8.\nA standard result says that the sampling distribution of the maximum\nlikelihood estimator has a limiting normal distribution\n\u02c6\u03b8\u2192N(\u03b80,i(\u03b80)\u22121), (8.16)\nasN\u2192 \u221e. Here we are independently sampling from g\u03b80(z). This suggests\nthat the sampling distribution of \u02c6\u03b8may be approximated by\nN(\u02c6\u03b8,i(\u02c6\u03b8)\u22121) orN(\u02c6\u03b8,I(\u02c6\u03b8)\u22121), (8.17)\nwhere \u02c6\u03b8represents the maximum likelihood estimate from the observed\ndata.\nThe corresponding estimates for the standard errors of \u02c6\u03b8jare obtained\nfrom\n/radical\uf8ecig\ni(\u02c6\u03b8)\u22121\njj and/radical\uf8ecig\nI(\u02c6\u03b8)\u22121\njj. (8.18)\nCon\ufb01dence points for \u03b8jcan be constructed from either approximation\nin (8.17). Such a con\ufb01dence point has the form\n\u02c6\u03b8j\u2212z(1\u2212\u03b1)\u2264/radical\uf8ecig\ni(\u02c6\u03b8)\u22121\njj or \u02c6\u03b8j\u2212z(1\u2212\u03b1)\u2264/radical\uf8ecig\nI(\u02c6\u03b8)\u22121\njj,\nrespectively, where z(1\u2212\u03b1)is the 1 \u2212\u03b1percentile of the standard normal\ndistribution. More accurate con\ufb01dence intervals can be derived from the\nlikelihood function, by using the chi-squared approximation\n2[\u2113(\u02c6\u03b8)\u2212\u2113(\u03b80)]\u223c\u03c72\np, (8.19)\nwhere pis the number of components in \u03b8. The resulting 1 \u22122\u03b1con\ufb01-\ndence interval is the set of all \u03b80such that 2[ \u2113(\u02c6\u03b8)\u2212\u2113(\u03b80)]\u2264\u03c72\np(1\u22122\u03b1),\nwhere \u03c72\np(1\u22122\u03b1)is the 1 \u22122\u03b1percentile of the chi-squared distribution with\npdegrees of freedom.", "285": "8.3 Bayesian Methods 267\nLet\u2019s return to our smoothing example to see what maximum likelihood\nyields. The parameters are \u03b8= (\u03b2,\u03c32). The log-likelihood is\n\u2113(\u03b8) =\u2212N\n2log\u03c322\u03c0\u22121\n2\u03c32N/summationdisplay\ni=1(yi\u2212h(xi)T\u03b2)2. (8.20)\nThe maximum likelihood estimate is obtained by setting \u2202\u2113/\u2202\u03b2 = 0 and\n\u2202\u2113/\u2202\u03c32= 0, giving\n\u02c6\u03b2= (HTH)\u22121HTy,\n\u02c6\u03c32=1\nN/summationdisplay\n(yi\u2212\u02c6\u03b8(xi))2,(8.21)\nwhich are the same as the usual estimates given in (8.2) and below (8.3).\nThe information matrix for \u03b8= (\u03b2,\u03c32) is block-diagonal, and the block\ncorresponding to \u03b2is\nI(\u03b2) = (HTH)/\u03c32, (8.22)\nso that the estimated variance ( HTH)\u22121\u02c6\u03c32agrees with the least squares\nestimate (8.3).\n8.2.3 Bootstrap versus Maximum Likelihood\nIn essence the bootstrap is a computer implementation of nonparametric or\nparametric maximum likelihood. The advantage of the bootstrap over the\nmaximum likelihood formula is that it allows us to compute maximum like-\nlihood estimates of standard errors and other quantities in settings where\nno formulas are available.\nIn our example, suppose that we adaptively choose by cross-validation\nthe number and position of the knots that de\ufb01ne the B-splines, rather\nthan \ufb01x them in advance. Denote by \u03bbthe collection of knots and their\npositions. Then the standard errors and con\ufb01dence bands should account\nfor the adaptive choice of \u03bb, but there is no way to do this analytically.\nWith the bootstrap, we compute the B-spline smooth with an adaptive\nchoice of knots for each bootstrap sample. The percentiles of the resulting\ncurves capture the variability from both the noise in the targets as well as\nthat from \u02c6\u03bb. In this particular example the con\ufb01dence bands (not shown)\ndon\u2019t look much di\ufb00erent than the \ufb01xed \u03bbbands. But in other problems,\nwhere more adaptation is used, this can be an important e\ufb00ect to capture.\n8.3 Bayesian Methods\nIn the Bayesian approach to inference, we specify a sampling model Pr( Z|\u03b8)\n(density or probability mass function) for our data given the parameters,", "286": "268 8. Model Inference and Averaging\nand a prior distribution for the parameters Pr( \u03b8) re\ufb02ecting our knowledge\nabout \u03b8before we see the data. We then compute the posterior distribution\nPr(\u03b8|Z) =Pr(Z|\u03b8)\u2264Pr(\u03b8)/integraltext\nPr(Z|\u03b8)\u2264Pr(\u03b8)d\u03b8, (8.23)\nwhich represents our updated knowledge about \u03b8after we see the data. To\nunderstand this posterior distribution, one might draw samples from it or\nsummarize by computing its mean or mode. The Bayesian approach di\ufb00ers\nfrom the standard (\u201cfrequentist\u201d) method for inference in its use of a prior\ndistribution to express the uncertainty present before seeing the data, and\nto allow the uncertainty remaining after seeing the data to be expressed in\nthe form of a posterior distribution.\nThe posterior distribution also provides the basis for predicting the values\nof a future observation znew, via the predictive distribution :\nPr(znew|Z) =/integraldisplay\nPr(znew|\u03b8)\u2264Pr(\u03b8|Z)d\u03b8. (8.24)\nIn contrast, the maximum likelihood approach would use Pr( znew|\u02c6\u03b8),\nthe data density evaluated at the maximum likelihood estimate, to predict\nfuture data. Unlike the predictive distribution (8.24), this does not account\nfor the uncertainty in estimating \u03b8.\nLet\u2019s walk through the Bayesian approach in our smoothing example.\nWe start with the parametric model given by equation (8.5), and assume\nfor the moment that \u03c32is known. We assume that the observed feature\nvalues x1,x2,... ,x Nare \ufb01xed, so that the randomness in the data comes\nsolely from yvarying around its mean \u03b8(x).\nThe second ingredient we need is a prior distribution. Distributions on\nfunctions are fairly complex entities: one approach is to use a Gaussian\nprocess prior in which we specify the prior covariance between any two\nfunction values \u03b8(x) and \u03b8(x\u2032) (Wahba, 1990; Neal, 1996).\nHere we take a simpler route: by considering a \ufb01nite B-spline basis for\n\u03b8(x), we can instead provide a prior for the coe\ufb03cients \u03b2, and this implicitly\nde\ufb01nes a prior for \u03b8(x). We choose a Gaussian prior centered at zero\n\u03b2\u223cN(0,\u03c4\u03a3) (8.25)\nwith the choices of the prior correlation matrix \u03a3and variance \u03c4to be\ndiscussed below. The implicit process prior for \u03b8(x) is hence Gaussian,\nwith covariance kernel\nK(x,x\u2032) = cov[ \u03b8(x),\u03b8(x\u2032)]\n=\u03c4\u2264h(x)T\u03a3h(x\u2032). (8.26)", "287": "8.3 Bayesian Methods 269\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-3 -2 -1 0 1 2 3\u03b8(x)\nx\nFIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-\nbution for the function \u03b8(x).\nThe posterior distribution for \u03b2is also Gaussian, with mean and covariance\nE(\u03b2|Z) =/parenleftbigg\nHTH+\u03c32\n\u03c4\u03a3\u22121/parenrightbigg\u22121\nHTy,\ncov(\u03b2|Z) =/parenleftbigg\nHTH+\u03c32\n\u03c4\u03a3\u22121/parenrightbigg\u22121\n\u03c32,(8.27)\nwith the corresponding posterior values for \u03b8(x),\nE(\u03b8(x)|Z) =h(x)T/parenleftbigg\nHTH+\u03c32\n\u03c4\u03a3\u22121/parenrightbigg\u22121\nHTy,\ncov[\u03b8(x),\u03b8(x\u2032)|Z] =h(x)T/parenleftbigg\nHTH+\u03c32\n\u03c4\u03a3\u22121/parenrightbigg\u22121\nh(x\u2032)\u03c32.(8.28)\nHow do we choose the prior correlation matrix \u03a3? In some settings the\nprior can be chosen from subject matter knowledge about the parameters.\nHere we are willing to say the function \u03b8(x) should be smooth, and have\nguaranteed this by expressing \u03b8in a smooth low-dimensional basis of B-\nsplines. Hence we can take the prior correlation matrix to be the identity\n\u03a3=I. When the number of basis functions is large, this might not be suf-\n\ufb01cient, and additional smoothness can be enforced by imposing restrictions\non\u03a3; this is exactly the case with smoothing splines (Section 5.8.1).\nFigure 8.3 shows ten draws from the corresponding prior for \u03b8(x). To\ngenerate posterior values of the function \u03b8(x), we generate values \u03b2\u2032from its\nposterior (8.27), giving corresponding posterior value \u03b8\u2032(x) =/summationtext7\n1\u03b2\u2032\njhj(x).\nTen such posterior curves are shown in Figure 8.4. Two di\ufb00erent values\nwere used for the prior variance \u03c4, 1 and 1000. Notice how similar the\nright panel looks to the bootstrap distribution in the bottom left panel", "288": "270 8. Model Inference and Averaging\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u03b8(x)\u03b8(x)\nx x\u03c4= 1 \u03c4= 1000\nFIGURE 8.4. Smoothing example: Ten draws from the posterior distribution\nfor the function \u03b8(x), for two di\ufb00erent values of the prior variance \u03c4. The purple\ncurves are the posterior means.\nof Figure 8.2 on page 263. This similarity is no accident. As \u03c4\u2192 \u221e, the\nposterior distribution (8.27) and the bootstrap distribution (8.7) co incide.\nOn the other hand, for \u03c4= 1, the posterior curves \u03b8(x) in the left panel\nof Figure 8.4 are smoother than the bootstrap curves, because we have\nimposed more prior weight on smoothness.\nThe distribution (8.25) with \u03c4\u2192 \u221e is called a noninformative prior for\n\u03b8. In Gaussian models, maximum likelihood and parametric bootstrap anal -\nyses tend to agree with Bayesian analyses that use a noninformative prior\nfor the free parameters. These tend to agree, because with a constant prior,\nthe posterior distribution is proportional to the likelihood. This corresp on-\ndence also extends to the nonparametric case, where the nonparametric\nbootstrap approximates a noninformative Bayes analysis; Section 8.4 has\nthe details.\nWe have, however, done some things that are not proper from a Bayesian\npoint of view. We have used a noninformative (constant) prior for \u03c32and\nreplaced it with the maximum likelihood estimate \u02c6 \u03c32in the posterior. A\nmore standard Bayesian analysis would also put a prior on \u03c3(typically\ng(\u03c3)\u221d1/\u03c3), calculate a joint posterior for \u03b8(x) and \u03c3, and then integrate\nout\u03c3, rather than just extract the maximum of the posterior distribution\n(\u201cMAP\u201d estimate).", "289": "8.4 Relationship Between the Bootstrap and Bayesian Inference 271\n8.4 Relationship Between the Bootstrap and\nBayesian Inference\nConsider \ufb01rst a very simple example, in which we observe a single obser-\nvation zfrom a normal distribution\nz\u223cN(\u03b8,1). (8.29)\nTo carry out a Bayesian analysis for \u03b8, we need to specify a prior. The\nmost convenient and common choice would be \u03b8\u223cN(0,\u03c4) giving posterior\ndistribution\n\u03b8|z\u223cN/parenleftbiggz\n1 + 1/\u03c4,1\n1 + 1/\u03c4/parenrightbigg\n. (8.30)\nNow the larger we take \u03c4, the more concentrated the posterior becomes\naround the maximum likelihood estimate \u02c6\u03b8=z. In the limit as \u03c4\u2192 \u221e we\nobtain a noninformative (constant) prior, and the posterior distribution is\n\u03b8|z\u223cN(z,1). (8.31)\nThis is the same as a parametric bootstrap distribution in which we gen-\nerate bootstrap values z\u2217from the maximum likelihood estimate of the\nsampling density N(z,1).\nThere are three ingredients that make this correspondence work:\n1. The choice of noninformative prior for \u03b8.\n2. The dependence of the log-likelihood \u2113(\u03b8;Z) on the data Zonly\nthrough the maximum likelihood estimate \u02c6\u03b8. Hence we can write the\nlog-likelihood as \u2113(\u03b8;\u02c6\u03b8).\n3. The symmetry of the log-likelihood in \u03b8and\u02c6\u03b8, that is, \u2113(\u03b8;\u02c6\u03b8) =\n\u2113(\u02c6\u03b8;\u03b8) + constant.\nProperties (2) and (3) essentially only hold for the Gaussian distribu-\ntion. However, they also hold approximately for the multinomial distribu-\ntion, leading to a correspondence between the nonparametric bootstrap\nand Bayes inference, which we outline next.\nAssume that we have a discrete sample space with Lcategories. Let wjbe\nthe probability that a sample point falls in category j, and \u02c6wjthe observed\nproportion in category j. Letw= (w1,w2,... ,w L),\u02c6w= ( \u02c6w1,\u02c6w2,... ,\u02c6wL).\nDenote our estimator by S( \u02c6w); take as a prior distribution for wa sym-\nmetric Dirichlet distribution with parameter a:\nw\u223cDiL(a1), (8.32)", "290": "272 8. Model Inference and Averaging\nthat is, the prior probability mass function is proportional to/producttextL\n\u2113=1wa\u22121\n\u2113.\nThen the posterior density of wis\nw\u223cDiL(a1 +N\u02c6w), (8.33)\nwhere Nis the sample size. Letting a\u21920 to obtain a noninformative prior\ngives\nw\u223cDiL(N\u02c6w). (8.34)\nNow the bootstrap distribution, obtained by sampling with replacement\nfrom the data, can be expressed as sampling the category proportions from\na multinomial distribution. Speci\ufb01cally,\nN\u02c6w\u2217\u223cMult( N,\u02c6w), (8.35)\nwhere Mult( N,\u02c6w) denotes a multinomial distribution, having probability\nmass function/parenleftbigN\nN\u02c6w\u2217\n1,...,N \u02c6w\u2217\nL/parenrightbig/producttext\u02c6wN\u02c6w\u2217\n\u2113\n\u2113. This distribution is similar to the pos-\nterior distribution above, having the same support, same mean, and nearly\nthe same covariance matrix. Hence the bootstrap distribution of S( \u02c6w\u2217) will\nclosely approximate the posterior distribution of S(w).\nIn this sense, the bootstrap distribution represents an (approximate)\nnonparametric, noninformative posterior distribution for our parameter.\nBut this bootstrap distribution is obtained painlessly\u2014without having to\nformally specify a prior and without having to sample from the posterior\ndistribution. Hence we might think of the bootstrap distribution as a \u201cpoor\nman\u2019s\u201d Bayes posterior. By perturbing the data, the bootstrap approxi-\nmates the Bayesian e\ufb00ect of perturbing the parameters, and is typically\nmuch simpler to carry out.\n8.5 The EM Algorithm\nThe EM algorithm is a popular tool for simplifying di\ufb03cult maximum\nlikelihood problems. We \ufb01rst describe it in the context of a simple mixture\nmodel.\n8.5.1 Two-Component Mixture Model\nIn this section we describe a simple mixture model for density estimation,\nand the associated EM algorithm for carrying out maximum likelihood\nestimation. This has a natural connection to Gibbs sampling methods for\nBayesian inference. Mixture models are discussed and demonstrated in sev-\neral other parts of the book, in particular Sections 6.8, 12.7 and 13.2.3.\nThe left panel of Figure 8.5 shows a histogram of the 20 \ufb01ctitious data\npoints in Table 8.1.", "291": "8.5 The EM Algorithm 273\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0\ny ydensity\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\nFIGURE 8.5. Mixture example. (Left panel:) Histogram of data. (Right panel: )\nMaximum likelihood \ufb01t of Gaussian densities (solid red) and resp onsibility (dotted\ngreen) of the left component density for observation y, as a function of y.\nTABLE 8.1. Twenty \ufb01ctitious data points used in the two-component mixture\nexample in Figure 8.5.\n-0.39 0.12 0.94 1.67 1.76 2.44 3.72 4.28 4.92 5.53\n0.06 0.48 1.01 1.68 1.80 3.25 4.12 4.60 5.28 6.22\nWe would like to model the density of the data points, and due to the\napparent bi-modality, a Gaussian distribution would not be appropriate.\nThere seems to be two separate underlying regimes, so instead we model\nYas a mixture of two normal distributions:\nY1\u223cN(\u03b81,\u03c32\n1),\nY2\u223cN(\u03b82,\u03c32\n2), (8.36)\nY= (1 \u2212\u2206)\u2264Y1+ \u2206\u2264Y2,\nwhere \u2206 \u2208 {0,1}with Pr(\u2206 = 1) = \u03c0. This generative representation is\nexplicit: generate a \u2206 \u2208 {0,1}with probability \u03c0, and then depending on\nthe outcome, deliver either Y1orY2. Let\u03c6\u03b8(x) denote the normal density\nwith parameters \u03b8= (\u03b8,\u03c32). Then the density of Yis\ngY(y) = (1 \u2212\u03c0)\u03c6\u03b81(y) +\u03c0\u03c6\u03b82(y). (8.37)\nNow suppose we wish to \ufb01t this model to the data in Figure 8.5 by maxi-\nmum likelihood. The parameters are\n\u03b8= (\u03c0,\u03b81,\u03b82) = (\u03c0,\u03b81,\u03c32\n1,\u03b82,\u03c32\n2). (8.38)\nThe log-likelihood based on the Ntraining cases is\n\u2113(\u03b8;Z) =N/summationdisplay\ni=1log[(1 \u2212\u03c0)\u03c6\u03b81(yi) +\u03c0\u03c6\u03b82(yi)]. (8.39)", "292": "274 8. Model Inference and Averaging\nDirect maximization of \u2113(\u03b8;Z) is quite di\ufb03cult numerically, because of\nthe sum of terms inside the logarithm. There is, however, a simpler ap-\nproach. We consider unobserved latent variables \u2206 itaking values 0 or 1 as\nin (8.36): if \u2206 i= 1 then Yicomes from model 2, otherwise it comes from\nmodel 1. Suppose we knew the values of the \u2206 i\u2019s. Then the log-likelihood\nwould be\n\u21130(\u03b8;Z,\u2206) =N/summationdisplay\ni=1[(1\u2212\u2206i)log\u03c6\u03b81(yi) + \u2206 ilog\u03c6\u03b82(yi)]\n+N/summationdisplay\ni=1[(1\u2212\u2206i)log(1 \u2212\u03c0) + \u2206 ilog\u03c0],(8.40)\nand the maximum likelihood estimates of \u03b81and\u03c32\n1would be the sample\nmean and variance for those data with \u2206 i= 0, and similarly those for \u03b82\nand\u03c32\n2would be the sample mean and variance of the data with \u2206 i= 1.\nThe estimate of \u03c0would be the proportion of \u2206 i= 1.\nSince the values of the \u2206 i\u2019s are actually unknown, we proceed in an\niterative fashion, substituting for each \u2206 iin (8.40) its expected value\n\u03b3i(\u03b8) = E(\u2206 i|\u03b8,Z) = Pr(\u2206 i= 1|\u03b8,Z), (8.41)\nalso called the responsibility of model 2 for observation i. We use a proce-\ndure called the EM algorithm, given in Algorithm 8.1 for the special case of\nGaussian mixtures. In the expectation step, we do a soft assignment of each\nobservation to each model: the current estimates of the parameters are used\nto assign responsibilities according to the relative density of the training\npoints under each model. In the maximization step, these responsibilities\nare used in weighted maximum-likelihood \ufb01ts to update the estimates of\nthe parameters.\nA good way to construct initial guesses for \u02c6 \u03b81and \u02c6\u03b82is simply to choose\ntwo of the yiat random. Both \u02c6 \u03c32\n1and \u02c6\u03c32\n2can be set equal to the overall\nsample variance/summationtextN\ni=1(yi\u2212\u00afy)2/N. The mixing proportion \u02c6 \u03c0can be started\nat the value 0 .5.\nNote that the actual maximizer of the likelihood occurs when we put a\nspike of in\ufb01nite height at any one data point, that is, \u02c6 \u03b81=yifor some\niand \u02c6\u03c32\n1= 0. This gives in\ufb01nite likelihood, but is not a useful solution.\nHence we are actually looking for a good local maximum of the likelihood,\none for which \u02c6 \u03c32\n1,\u02c6\u03c32\n2>0. To further complicate matters, there can be\nmore than one local maximum having \u02c6 \u03c32\n1,\u02c6\u03c32\n2>0. In our example, we\nran the EM algorithm with a number of di\ufb00erent initial guesses for the\nparameters, all having \u02c6 \u03c32\nk>0.5, and chose the run that gave us the highest\nmaximized likelihood. Figure 8.6 shows the progress of the EM algorithm in\nmaximizing the log-likelihood. Table 8.2 shows \u02c6 \u03c0=/summationtext\ni\u02c6\u03b3i/N, the maximum\nlikelihood estimate of the proportion of observations in class 2, at sel ected\niterations of the EM procedure.", "293": "8.5 The EM Algorithm 275\nAlgorithm 8.1 EM Algorithm for Two-component Gaussian Mixture.\n1. Take initial guesses for the parameters \u02c6 \u03b81,\u02c6\u03c32\n1,\u02c6\u03b82,\u02c6\u03c32\n2,\u02c6\u03c0(see text).\n2.Expectation Step : compute the responsibilities\n\u02c6\u03b3i=\u02c6\u03c0\u03c6\u02c6\u03b82(yi)\n(1\u2212\u02c6\u03c0)\u03c6\u02c6\u03b81(yi) + \u02c6\u03c0\u03c6\u02c6\u03b82(yi), i= 1,2,... ,N. (8.42)\n3.Maximization Step : compute the weighted means and variances:\n\u02c6\u03b81=/summationtextN\ni=1(1\u2212\u02c6\u03b3i)yi/summationtextN\ni=1(1\u2212\u02c6\u03b3i), \u02c6\u03c32\n1=/summationtextN\ni=1(1\u2212\u02c6\u03b3i)(yi\u2212\u02c6\u03b81)2\n/summationtextN\ni=1(1\u2212\u02c6\u03b3i),\n\u02c6\u03b82=/summationtextN\ni=1\u02c6\u03b3iyi/summationtextN\ni=1\u02c6\u03b3i, \u02c6\u03c32\n2=/summationtextN\ni=1\u02c6\u03b3i(yi\u2212\u02c6\u03b82)2\n/summationtextN\ni=1\u02c6\u03b3i,\nand the mixing probability \u02c6 \u03c0=/summationtextN\ni=1\u02c6\u03b3i/N.\n4. Iterate steps 2 and 3 until convergence.\nTABLE 8.2. Selected iterations of the EM algorithm for mixture example.\nIteration \u02c6 \u03c0\n1 0.485\n5 0.493\n10 0.523\n15 0.544\n20 0.546\nThe \ufb01nal maximum likelihood estimates are\n\u02c6\u03b81= 4.62, \u02c6\u03c32\n1= 0.87,\n\u02c6\u03b82= 1.06, \u02c6\u03c32\n2= 0.77,\n\u02c6\u03c0= 0.546.\nThe right panel of Figure 8.5 shows the estimated Gaussian mixture density\nfrom this procedure (solid red curve), along with the responsibilities (dotted\ngreen curve). Note that mixtures are also useful for supervised learning; in\nSection 6.7 we show how the Gaussian mixture model leads to a version of\nradial basis functions.", "294": "276 8. Model Inference and Averaging\nIterationObserved Data Log-likelihood\n5 10 15 20-44 -43 -42 -41 -40 -39\nooooooooooooooo o o o o o\nFIGURE 8.6. EM algorithm: observed data log-likelihood as a function of t he\niteration number.\n8.5.2 The EM Algorithm in General\nThe above procedure is an example of the EM (or Baum\u2013Welch) algorithm\nfor maximizing likelihoods in certain classes of problems. These problems\nare ones for which maximization of the likelihood is di\ufb03cult, but made\neasier by enlarging the sample with latent (unobserved) data. This is called\ndata augmentation . Here the latent data are the model memberships \u2206 i.\nIn other problems, the latent data are actual data that should have been\nobserved but are missing.\nAlgorithm 8.2 gives the general formulation of the EM algorithm. Our\nobserved data is Z, having log-likelihood \u2113(\u03b8;Z) depending on parameters\n\u03b8. The latent or missing data is Zm, so that the complete data is T=\n(Z,Zm) with log-likelihood \u21130(\u03b8;T),\u21130based on the complete density. In\nthe mixture problem ( Z,Zm) = (y,\u2206), and \u21130(\u03b8;T) is given in (8.40).\nIn our mixture example, E( \u21130(\u03b8\u2032;T)|Z,\u02c6\u03b8(j)) is simply (8.40) with the \u2206 i\nreplaced by the responsibilities \u02c6 \u03b3i(\u02c6\u03b8), and the maximizers in step 3 are just\nweighted means and variances.\nWe now give an explanation of why the EM algorithm works in general.\nSince\nPr(Zm|Z,\u03b8\u2032) =Pr(Zm,Z|\u03b8\u2032)\nPr(Z|\u03b8\u2032), (8.44)\nwe can write\nPr(Z|\u03b8\u2032) =Pr(T|\u03b8\u2032)\nPr(Zm|Z,\u03b8\u2032). (8.45)\nIn terms of log-likelihoods, we have \u2113(\u03b8\u2032;Z) =\u21130(\u03b8\u2032;T)\u2212\u21131(\u03b8\u2032;Zm|Z), where\n\u21131is based on the conditional density Pr( Zm|Z,\u03b8\u2032). Taking conditional\nexpectations with respect to the distribution of T|Zgoverned by parameter\n\u03b8gives\n\u2113(\u03b8\u2032;Z) = E[ \u21130(\u03b8\u2032;T)|Z,\u03b8]\u2212E[\u21131(\u03b8\u2032;Zm|Z)|Z,\u03b8]", "295": "8.5 The EM Algorithm 277\nAlgorithm 8.2 The EM Algorithm.\n1. Start with initial guesses for the parameters \u02c6\u03b8(0).\n2.Expectation Step : at the jth step, compute\nQ(\u03b8\u2032,\u02c6\u03b8(j)) = E( \u21130(\u03b8\u2032;T)|Z,\u02c6\u03b8(j)) (8.43)\nas a function of the dummy argument \u03b8\u2032.\n3.Maximization Step : determine the new estimate \u02c6\u03b8(j+1)as the maxi-\nmizer of Q(\u03b8\u2032,\u02c6\u03b8(j)) over \u03b8\u2032.\n4. Iterate steps 2 and 3 until convergence.\n\u2261Q(\u03b8\u2032,\u03b8)\u2212R(\u03b8\u2032,\u03b8). (8.46)\nIn the Mstep, the EM algorithm maximizes Q(\u03b8\u2032,\u03b8) over \u03b8\u2032, rather than\nthe actual objective function \u2113(\u03b8\u2032;Z). Why does it succeed in maximizing\n\u2113(\u03b8\u2032;Z)? Note that R(\u03b8\u2217,\u03b8) is the expectation of a log-likelihood of a density\n(indexed by \u03b8\u2217), with respect to the same density indexed by \u03b8, and hence\n(by Jensen\u2019s inequality) is maximized as a function of \u03b8\u2217, when \u03b8\u2217=\u03b8(see\nExercise 8.1). So if \u03b8\u2032maximizes Q(\u03b8\u2032,\u03b8), we see that\n\u2113(\u03b8\u2032;Z)\u2212\u2113(\u03b8;Z) = [ Q(\u03b8\u2032,\u03b8)\u2212Q(\u03b8,\u03b8)]\u2212[R(\u03b8\u2032,\u03b8)\u2212R(\u03b8,\u03b8)]\n\u22650. (8.47)\nHence the EM iteration never decreases the log-likelihood.\nThis argument also makes it clear that a full maximization in the M\nstep is not necessary: we need only to \ufb01nd a value \u02c6\u03b8(j+1)so that Q(\u03b8\u2032,\u02c6\u03b8(j))\nincreases as a function of the \ufb01rst argument, that is, Q(\u02c6\u03b8(j+1),\u02c6\u03b8(j))>\nQ(\u02c6\u03b8(j),\u02c6\u03b8(j)). Such procedures are called GEM (generalized EM) algorithms.\nThe EM algorithm can also be viewed as a minorization procedure: see\nExercise 8.7.\n8.5.3 EM as a Maximization\u2013Maximization Procedure\nHere is a di\ufb00erent view of the EM procedure, as a joint maximization\nalgorithm. Consider the function\nF(\u03b8\u2032,\u02dcP) = E \u02dcP[\u21130(\u03b8\u2032;T)]\u2212E\u02dcP[log\u02dcP(Zm)]. (8.48)\nHere \u02dcP(Zm) is any distribution over the latent data Zm. In the mixture\nexample, \u02dcP(Zm) comprises the set of probabilities \u03b3i= Pr(\u2206 i= 1|\u03b8,Z).\nNote that Fevaluated at \u02dcP(Zm) = Pr( Zm|Z,\u03b8\u2032), is the log-likelihood of", "296": "278 8. Model Inference and Averaging\n1 2 3 4 50 1 2 3 40.10.3\n0.50.7\n0.9Model Parameters\nLatent Data ParametersEMEM\nFIGURE 8.7. Maximization\u2013maximization view of the EM algorithm. Shown\nare the contours of the (augmented) observed data log-likelih oodF(\u03b8\u2032,\u02dcP). The\nEstep is equivalent to maximizing the log-likelihood over the pa rameters of the\nlatent data distribution. The Mstep maximizes it over the parameters of the\nlog-likelihood. The red curve corresponds to the observed da ta log-likelihood, a\npro\ufb01le obtained by maximizing F(\u03b8\u2032,\u02dcP)for each value of \u03b8\u2032.\nthe observed data, from (8.46)1. The function Fexpands the domain of\nthe log-likelihood, to facilitate its maximization.\nThe EM algorithm can be viewed as a joint maximization method for F\nover\u03b8\u2032and\u02dcP(Zm), by \ufb01xing one argument and maximizing over the other.\nThe maximizer over \u02dcP(Zm) for \ufb01xed \u03b8\u2032can be shown to be\n\u02dcP(Zm) = Pr( Zm|Z,\u03b8\u2032) (8.49)\n(Exercise 8.2). This is the distribution computed by the Estep, for example,\n(8.42) in the mixture example. In the Mstep, we maximize F(\u03b8\u2032,\u02dcP) over \u03b8\u2032\nwith\u02dcP\ufb01xed: this is the same as maximizing the \ufb01rst term E \u02dcP[\u21130(\u03b8\u2032;T)|Z,\u03b8]\nsince the second term does not involve \u03b8\u2032.\nFinally, since F(\u03b8\u2032,\u02dcP) and the observed data log-likelihood agree when\n\u02dcP(Zm) = Pr( Zm|Z,\u03b8\u2032), maximization of the former accomplishes maxi-\nmization of the latter. Figure 8.7 shows a schematic view of this process.\nThis view of the EM algorithm leads to alternative maximization proce-\n1(8.46) holds for all \u03b8, including \u03b8=\u03b8\u2032.", "297": "8.6 MCMC for Sampling from the Posterior 279\nAlgorithm 8.3 Gibbs Sampler.\n1. Take some initial values U(0)\nk,k= 1,2,... ,K .\n2. Repeat for t= 1,2,... ,. :\nFork= 1,2,... ,K generate U(t)\nkfrom\nPr(U(t)\nk|U(t)\n1,... ,U(t)\nk\u22121,U(t\u22121)\nk+1,... ,U(t\u22121)\nK).\n3. Continue step 2 until the joint distribution of ( U(t)\n1,U(t)\n2,... ,U(t)\nK)\ndoes not change.\ndures. For example, one does not need to maximize with respect to all of\nthe latent data parameters at once, but could instead maximize over one\nof them at a time, alternating with the Mstep.\n8.6 MCMC for Sampling from the Posterior\nHaving de\ufb01ned a Bayesian model, one would like to draw samples from\nthe resulting posterior distribution, in order to make inferences about the\nparameters. Except for simple models, this is often a di\ufb03cult computa-\ntional problem. In this section we discuss the Markov chain Monte Carlo\n(MCMC) approach to posterior sampling. We will see that Gibbs sampling,\nan MCMC procedure, is closely related to the EM algorithm: the main dif-\nference is that it samples from the conditional distributions rather than\nmaximizing over them.\nConsider \ufb01rst the following abstract problem. We have random variables\nU1,U2,... ,U Kand we wish to draw a sample from their joint distribution.\nSuppose this is di\ufb03cult to do, but it is easy to simulate from the conditional\ndistributions Pr( Uj|U1,U2,... ,U j\u22121,Uj+1,... ,U K), j= 1,2,... ,K . The\nGibbs sampling procedure alternatively simulates from each of these distri-\nbutions and when the process stabilizes, provides a sample from the desired\njoint distribution. The procedure is de\ufb01ned in Algorithm 8.3.\nUnder regularity conditions it can be shown that this procedure even-\ntually stabilizes, and the resulting random variables are indeed a sample\nfrom the joint distribution of U1,U2,... ,U K. This occurs despite the fact\nthat the samples ( U(t)\n1,U(t)\n2,... ,U(t)\nK) are clearly not independent for dif-\nferent t. More formally, Gibbs sampling produces a Markov chain whose\nstationary distribution is the true joint distribution, and hence the term\n\u201cMarkov chain Monte Carlo.\u201d It is not surprising that the true joint dis -\ntribution is stationary under this process, as the successive steps leave the\nmarginal distributions of the Uk\u2019s unchanged.", "298": "280 8. Model Inference and Averaging\nNote that we don\u2019t need to know the explicit form of the conditional\ndensities, but just need to be able to sample from them. After the procedure\nreaches stationarity, the marginal density of any subset of the variables\ncan be approximated by a density estimate applied to the sample values.\nHowever if the explicit form of the conditional density Pr( Uk,|U\u2113,\u2113\u221dne}ationslash=k)\nis available, a better estimate of say the marginal density of Ukcan be\nobtained from (Exercise 8.3):\n/hatwiderPrUk(u) =1\n(M\u2212m+ 1)M/summationdisplay\nt=mPr(u|U(t)\n\u2113,\u2113\u221dne}ationslash=k). (8.50)\nHere we have averaged over the last M\u2212m+ 1 members of the sequence,\nto allow for an initial \u201cburn-in\u201d period before stationarity is reached.\nNow getting back to Bayesian inference, our goal is to draw a sample from\nthe joint posterior of the parameters given the data Z. Gibbs sampling will\nbe helpful if it is easy to sample from the conditional distribution of each\nparameter given the other parameters and Z. An example\u2014the Gaussian\nmixture problem\u2014is detailed next.\nThere is a close connection between Gibbs sampling from a posterior and\nthe EM algorithm in exponential family models. The key is to consider the\nlatent data Zmfrom the EM procedure to be another parameter for the\nGibbs sampler. To make this explicit for the Gaussian mixture problem,\nwe take our parameters to be ( \u03b8,Zm). For simplicity we \ufb01x the variances\n\u03c32\n1,\u03c32\n2and mixing proportion \u03c0at their maximum likelihood values so that\nthe only unknown parameters in \u03b8are the means \u03b81and\u03b82. The Gibbs\nsampler for the mixture problem is given in Algorithm 8.4. We see that\nsteps 2(a) and 2(b) are the same as the EandMsteps of the EM pro-\ncedure, except that we sample rather than maximize. In step 2(a), rather\nthan compute the maximum likelihood responsibilities \u03b3i= E(\u2206 i|\u03b8,Z),\nthe Gibbs sampling procedure simulates the latent data \u2206 ifrom the distri-\nbutions Pr(\u2206 i|\u03b8,Z). In step 2(b), rather than compute the maximizers of\nthe posterior Pr( \u03b81,\u03b82,\u2206|Z) we simulate from the conditional distribution\nPr(\u03b81,\u03b82|\u2206,Z).\nFigure 8.8 shows 200 iterations of Gibbs sampling, with the mean param-\neters\u03b81(lower) and \u03b82(upper) shown in the left panel, and the proportion\nof class 2 observations/summationtext\ni\u2206i/Non the right. Horizontal broken lines have\nbeen drawn at the maximum likelihood estimate values \u02c6 \u03b81,\u02c6\u03b82and/summationtext\ni\u02c6\u03b3i/N\nin each case. The values seem to stabilize quite quickly, and are distributed\nevenly around the maximum likelihood values.\nThe above mixture model was simpli\ufb01ed, in order to make the clear\nconnection between Gibbs sampling and the EM algorithm. More realisti-\ncally, one would put a prior distribution on the variances \u03c32\n1,\u03c32\n2and mixing\nproportion \u03c0, and include separate Gibbs sampling steps in which we sam-\nple from their posterior distributions, conditional on the other parameters.\nOne can also incorporate proper (informative) priors for the mean param-", "299": "8.6 MCMC for Sampling from the Posterior 281\nAlgorithm 8.4 Gibbs sampling for mixtures.\n1. Take some initial values \u03b8(0)= (\u03b8(0)\n1,\u03b8(0)\n2).\n2. Repeat for t= 1,2,... ,.\n(a) For i= 1,2,... ,N generate \u2206(t)\ni\u2208 {0,1}with Pr(\u2206(t)\ni= 1) =\n\u02c6\u03b3i(\u03b8(t)), from equation (8.42).\n(b) Set\n\u02c6\u03b81=/summationtextN\ni=1(1\u2212\u2206(t)\ni)\u2264yi/summationtextN\ni=1(1\u2212\u2206(t)\ni),\n\u02c6\u03b82=/summationtextN\ni=1\u2206(t)\ni\u2264yi/summationtextN\ni=1\u2206(t)\ni,\nand generate \u03b8(t)\n1\u223cN(\u02c6\u03b81,\u02c6\u03c32\n1) and \u03b8(t)\n2\u223cN(\u02c6\u03b82,\u02c6\u03c32\n2).\n3. Continue step 2 until the joint distribution of ( \u2206(t),\u03b8(t)\n1,\u03b8(t)\n2) doesn\u2019t\nchange\nGibbs IterationMean Parameters\n0 50 100 150 2000 2 4 6 8\nGibbs IterationMixing Proportion\n0 50 100 150 2000.3 0.4 0.5 0.6 0.7\nFIGURE 8.8. Mixture example. (Left panel:) 200values of the two mean param-\neters from Gibbs sampling; horizontal lines are drawn at the maxi mum likelihood\nestimates \u02c6\u03b81,\u02c6\u03b82. (Right panel:) Proportion of values with \u2206i= 1, for each of the\n200Gibbs sampling iterations; a horizontal line is drawn atP\ni\u02c6\u03b3i/N.", "300": "282 8. Model Inference and Averaging\neters. These priors must not be improper as this will lead to a degenerate\nposterior, with all the mixing weight on one component.\nGibbs sampling is just one of a number of recently developed procedures\nfor sampling from posterior distributions. It uses conditional sampling of\neach parameter given the rest, and is useful when the structure of the prob-\nlem makes this sampling easy to carry out. Other methods do not require\nsuch structure, for example the Metropolis\u2013Hastings algorithm. These and\nother computational Bayesian methods have been applied to sophisticated\nlearning algorithms such as Gaussian process models and neural networks.\nDetails may be found in the references given in the Bibliographic Notes at\nthe end of this chapter.\n8.7 Bagging\nEarlier we introduced the bootstrap as a way of assessing the accuracy of a\nparameter estimate or a prediction. Here we show how to use the bootstrap\nto improve the estimate or prediction itself. In Section 8.4 we investigat ed\nthe relationship between the bootstrap and Bayes approaches, and found\nthat the bootstrap mean is approximately a posterior average. Bagging\nfurther exploits this connection.\nConsider \ufb01rst the regression problem. Suppose we \ufb01t a model to our\ntraining data Z={(x1,y1),(x2,y2),... ,(xN,yN)}, obtaining the predic-\ntion\u02c6f(x) at input x. Bootstrap aggregation or bagging averages this predic-\ntion over a collection of bootstrap samples, thereby reducing its variance.\nFor each bootstrap sample Z\u2217b,b= 1,2,... ,B , we \ufb01t our model, giving\nprediction \u02c6f\u2217b(x). The bagging estimate is de\ufb01ned by\n\u02c6fbag(x) =1\nBB/summationdisplay\nb=1\u02c6f\u2217b(x). (8.51)\nDenote by \u02c6Pthe empirical distribution putting equal probability 1 /Non\neach of the data points ( xi,yi). In fact the \u201ctrue\u201d bagging estimate is\nde\ufb01ned by E \u02c6P\u02c6f\u2217(x), where Z\u2217= (x\u2217\n1,y\u2217\n1),(x\u2217\n2,y\u2217\n2),... ,(x\u2217\nN,y\u2217\nN) and each\n(x\u2217\ni,y\u2217\ni)\u223c\u02c6P. Expression (8.51) is a Monte Carlo estimate of the true\nbagging estimate, approaching it as B\u2192 \u221e.\nThe bagged estimate (8.51) will di\ufb00er from the original estimate \u02c6f(x)\nonly when the latter is a nonlinear or adaptive function of the data. For\nexample, to bag the B-spline smooth of Section 8.2.1, we average the curves\nin the bottom left panel of Figure 8.2 at each value of x. The B-spline\nsmoother is linear in the data if we \ufb01x the inputs; hence if we sample using\nthe parametric bootstrap in equation (8.6), then \u02c6fbag(x)\u2192\u02c6f(x) asB\u2192 \u221e\n(Exercise 8.4). Hence bagging just reproduces the original smooth in the", "301": "8.7 Bagging 283\ntop left panel of Figure 8.2. The same is approximately true if we were to\nbag using the nonparametric bootstrap.\nA more interesting example is a regression tree, where \u02c6f(x) denotes the\ntree\u2019s prediction at input vector x(regression trees are described in Chap-\nter 9). Each bootstrap tree will typically involve di\ufb00erent features tha n the\noriginal, and might have a di\ufb00erent number of terminal nodes. The bagged\nestimate is the average prediction at xfrom these Btrees.\nNow suppose our tree produces a classi\ufb01er \u02c6G(x) for a K-class response.\nHere it is useful to consider an underlying indicator-vector function \u02c6f(x),\nwith value a single one and K\u22121 zeroes, such that \u02c6G(x) = arg max k\u02c6f(x).\nThen the bagged estimate \u02c6fbag(x) (8.51) is a K-vector [ p1(x),p2(x),... ,\npK(x)], with pk(x) equal to the proportion of trees predicting class katx.\nThe bagged classi\ufb01er selects the class with the most \u201cvotes\u201d from the B\ntrees, \u02c6Gbag(x) = arg max k\u02c6fbag(x).\nOften we require the class-probability estimates at x, rather than the\nclassi\ufb01cations themselves. It is tempting to treat the voting proportions\npk(x) as estimates of these probabilities. A simple two-class example shows\nthat they fail in this regard. Suppose the true probability of class 1 at xis\n0.75, and each of the bagged classi\ufb01ers accurately predict a 1. Then p1(x) =\n1, which is incorrect. For many classi\ufb01ers \u02c6G(x), however, there is already\nan underlying function \u02c6f(x) that estimates the class probabilities at x(for\ntrees, the class proportions in the terminal node). An alternative bagging\nstrategy is to average these instead, rather than the vote indicator vectors.\nNot only does this produce improved estimates of the class probabilities,\nbut it also tends to produce bagged classi\ufb01ers with lower variance, especially\nfor small B(see Figure 8.10 in the next example).\n8.7.1 Example: Trees with Simulated Data\nWe generated a sample of size N= 30, with two classes and p= 5 features,\neach having a standard Gaussian distribution with pairwise correlation\n0.95. The response Ywas generated according to Pr( Y= 1|x1\u22640.5) = 0 .2,\nPr(Y= 1|x1>0.5) = 0 .8. The Bayes error is 0 .2. A test sample of size 2000\nwas also generated from the same population. We \ufb01t classi\ufb01cation trees to\nthe training sample and to each of 200 bootstrap samples (classi\ufb01cation\ntrees are described in Chapter 9). No pruning was used. Figure 8.9 shows\nthe original tree and eleven bootstrap trees. Notice how the trees are all\ndi\ufb00erent, with di\ufb00erent splitting features and cutpoints. The test error for\nthe original tree and the bagged tree is shown in Figure 8.10. In this ex-\nample the trees have high variance due to the correlation in the predictors.\nBagging succeeds in smoothing out this variance and hence reducing the\ntest error.\nBagging can dramatically reduce the variance of unstable procedures\nlike trees, leading to improved prediction. A simple argument shows why", "302": "284 8. Model Inference and Averaging\n|x.1 < 0.395\n010\n101\n10Original Tree\n|x.1 < 0.555\n0\n1001b = 1\n|x.2 < 0.205\n0101\n01b = 2\n|x.2 < 0.285\n1 1010b = 3\n|x.3 < 0.985\n0\n1\n011 1b = 4\n|x.4 < \u22121.36\n0\n1\n1010\n10b = 5\n|x.1 < 0.395\n1 10 01b = 6\n|x.1 < 0.395\n01011b = 7\n|x.3 < 0.985\n010 010b = 8\n|x.1 < 0.395\n0\n1\n0110b = 9\n|x.1 < 0.555\n101\n01b = 10\n|x.1 < 0.555\n0 101b = 11\nFIGURE 8.9. Bagging trees on simulated dataset. The top left panel shows th e\noriginal tree. Eleven trees grown on bootstrap samples are sh own. For each tree,\nthe top split is annotated.", "303": "8.7 Bagging 285\n0 50 100 150 2000.20 0.25 0.30 0.35 0.40 0.45 0.50\nNumber of Bootstrap SamplesTest ErrorBagged TreesOriginal Tree\nBayesConsensus\nProbability\nFIGURE 8.10. Error curves for the bagging example of Figure 8.9. Shown is\nthe test error of the original tree and bagged trees as a function of the number of\nbootstrap samples. The orange points correspond to the consensus vote, while the\ngreen points average the probabilities.\nbagging helps under squared-error loss, in short because averaging reduces\nvariance and leaves bias unchanged.\nAssume our training observations ( xi,yi), i= 1,... ,N are indepen-\ndently drawn from a distribution P, and consider the ideal aggregate es-\ntimator fag(x) = E P\u02c6f\u2217(x). Here xis \ufb01xed and the bootstrap dataset Z\u2217\nconsists of observations x\u2217\ni,y\u2217\ni,i= 1,2,... ,N sampled from P. Note that\nfag(x) is a bagging estimate, drawing bootstrap samples from the actual\npopulation Prather than the data. It is not an estimate that we can use\nin practice, but is convenient for analysis. We can write\nEP[Y\u2212\u02c6f\u2217(x)]2= E P[Y\u2212fag(x) +fag(x)\u2212\u02c6f\u2217(x)]2\n= E P[Y\u2212fag(x)]2+ EP[\u02c6f\u2217(x)\u2212fag(x)]2\n\u2265EP[Y\u2212fag(x)]2. (8.52)\nThe extra error on the right-hand side comes from the variance of \u02c6f\u2217(x)\naround its mean fag(x). Therefore true population aggregation never in-\ncreases mean squared error. This suggests that bagging\u2014drawing samples\nfrom the training data\u2014 will often decrease mean-squared error.\nThe above argument does not hold for classi\ufb01cation under 0-1 loss, be-\ncause of the nonadditivity of bias and variance. In that setting, bagging a", "304": "286 8. Model Inference and Averaging\ngood classi\ufb01er can make it better, but bagging a bad classi\ufb01er can make it\nworse. Here is a simple example, using a randomized rule. Suppose Y= 1\nfor all x, and the classi\ufb01er \u02c6G(x) predicts Y= 1 (for all x) with proba-\nbility 0.4 and predicts Y= 0 (for all x) with probability 0.6. Then the\nmisclassi\ufb01cation error of \u02c6G(x) is 0.6 but that of the bagged classi\ufb01er is 1.0.\nFor classi\ufb01cation we can understand the bagging e\ufb00ect in terms of a\nconsensus of independent weak learners (Dietterich, 2000a). Let the Bayes\noptimal decision at xbeG(x) = 1 in a two-class example. Suppose each\nof the weak learners G\u2217\nbhave an error-rate eb=e <0.5, and let S1(x) =/summationtextB\nb=1I(G\u2217\nb(x) = 1) be the consensus vote for class 1. Since the weak learn-\ners are assumed to be independent, S1(x)\u223cBin(B,1\u2212e), and Pr( S1>\nB/2)\u21921 asBgets large. This concept has been popularized outside of\nstatistics as the \u201cWisdom of Crowds\u201d (Surowiecki, 2004) \u2014 the collective\nknowledge of a diverse and independent body of people typically exceeds\nthe knowledge of any single individual, and can be harnessed by voting.\nOf course, the main caveat here is \u201cindependent,\u201d and bagged trees are\nnot. Figure 8.11 illustrates the power of a consensus vote in a simulated\nexample, where only 30% of the voters have some knowledge.\nIn Chapter 15 we see how random forests improve on bagging by reducing\nthe correlation between the sampled trees.\nNote that when we bag a model, any simple structure in the model is\nlost. As an example, a bagged tree is no longer a tree. For interpretation\nof the model this is clearly a drawback. More stable procedures like near-\nest neighbors are typically not a\ufb00ected much by bagging. Unfortunately,\nthe unstable models most helped by bagging are unstable because of the\nemphasis on interpretability, and this is lost in the bagging process.\nFigure 8.12 shows an example where bagging doesn\u2019t help. The 100 data\npoints shown have two features and two classes, separated by the gray\nlinear boundary x1+x2= 1. We choose as our classi\ufb01er \u02c6G(x) a single\naxis-oriented split, choosing the split along either x1orx2that produces\nthe largest decrease in training misclassi\ufb01cation error.\nThe decision boundary obtained from bagging the 0-1 decision rule over\nB= 50 bootstrap samples is shown by the blue curve in the left panel.\nIt does a poor job of capturing the true boundary. The single split rule,\nderived from the training data, splits near 0 (the middle of the range of x1\norx2), and hence has little contribution away from the center. Averaging\nthe probabilities rather than the classi\ufb01cations does not help here. Bagging\nestimates the expected class probabilities from the single split rule, that is,\naveraged over many replications. Note that the expected class probabilities\ncomputed by bagging cannot be realized on any single replication, in the\nsame way that a woman cannot have 2.4 children. In this sense, bagging\nincreases somewhat the space of models of the individual base classi\ufb01er.\nHowever, it doesn\u2019t help in this and many other examples where a greater\nenlargement of the model class is needed. \u201cBoosting\u201d is a way of doing this", "305": "8.7 Bagging 2870 2 4 6 8 10\nP \u2212  Probability of Informed Person Being CorrectExpected Correct out of 10Wisdom of Crowds\nConsensus\nIndividual\n0.25 0.50 0.75 1.00\nFIGURE 8.11. Simulated academy awards voting. 50members vote in 10 cat-\negories, each with 4nominations. For any category, only 15voters have some\nknowledge, represented by their probability of selecting the \u201c correct\u201d candidate in\nthat category (so P= 0.25means they have no knowledge). For each category, the\n15experts are chosen at random from the 50. Results show the expected correct\n(based on 50simulations) for the consensus, as well as for the individuals. T he\nerror bars indicate one standard deviation. We see, for example, t hat if the 15\ninformed for a category have a 50% chance of selecting the correct candidate, the\nconsensus doubles the expected performance of an individual.", "306": "288 8. Model Inference and Averaging\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022Bagged Decision Rule\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022Boosted Decision Rule\nFIGURE 8.12. Data with two features and two classes, separated by a linear\nboundary. (Left panel:) Decision boundary estimated from bagg ing the decision\nrule from a single split, axis-oriented classi\ufb01er. (Right panel: ) Decision boundary\nfrom boosting the decision rule of the same classi\ufb01er. The test error rates are\n0.166, and 0.065, respectively. Boosting is described in Chapter 10.\nand is described in Chapter 10. The decision boundary in the right panel is\nthe result of the boosting procedure, and it roughly captures the diagonal\nboundary.\n8.8 Model Averaging and Stacking\nIn Section 8.4 we viewed bootstrap values of an estimator as approximate\nposterior values of a corresponding parameter, from a kind of nonparamet-\nric Bayesian analysis. Viewed in this way, the bagged estimate (8.51) i s\nan approximate posterior Bayesian mean. In contrast, the training sampl e\nestimate \u02c6f(x) corresponds to the mode of the posterior. Since the posterior\nmean (not mode) minimizes squared-error loss, it is not surprising that\nbagging can often reduce mean squared-error.\nHere we discuss Bayesian model averaging more generally. We have a\nset of candidate models Mm, m= 1,... ,M for our training set Z. These\nmodels may be of the same type with di\ufb00erent parameter values (e.g.,\nsubsets in linear regression), or di\ufb00erent models for the same task (e.g.,\nneural networks and regression trees).\nSuppose \u03b6is some quantity of interest, for example, a prediction f(x) at\nsome \ufb01xed feature value x. The posterior distribution of \u03b6is\nPr(\u03b6|Z) =M/summationdisplay\nm=1Pr(\u03b6|Mm,Z)Pr(Mm|Z), (8.53)", "307": "8.8 Model Averaging and Stacking 289\nwith posterior mean\nE(\u03b6|Z) =M/summationdisplay\nm=1E(\u03b6|Mm,Z)Pr(Mm|Z). (8.54)\nThis Bayesian prediction is a weighted average of the individual predictions,\nwith weights proportional to the posterior probability of each model.\nThis formulation leads to a number of di\ufb00erent model-averaging strate-\ngies.Committee methods take a simple unweighted average of the predic-\ntions from each model, essentially giving equal probability to each model.\nMore ambitiously, the development in Section 7.7 shows the BIC criterion\ncan be used to estimate posterior model probabilities. This is applicable\nin cases where the di\ufb00erent models arise from the same parametric model,\nwith di\ufb00erent parameter values. The BIC gives weight to each model de-\npending on how well it \ufb01ts and how many parameters it uses. One can also\ncarry out the Bayesian recipe in full. If each model Mmhas parameters\n\u03b8m, we write\nPr(Mm|Z)\u221dPr(Mm)\u2264Pr(Z|Mm)\n\u221dPr(Mm)\u2264/integraldisplay\nPr(Z|\u03b8m,Mm)Pr(\u03b8m|Mm)d\u03b8m.\n(8.55)\nIn principle one can specify priors Pr( \u03b8m|Mm) and numerically com-\npute the posterior probabilities from (8.55), to be used as model-averaging\nweights. However, we have seen no real evidence that this is worth all of\nthe e\ufb00ort, relative to the much simpler BIC approximation.\nHow can we approach model averaging from a frequentist viewpoint?\nGiven predictions \u02c6f1(x),\u02c6f2(x),... ,\u02c6fM(x), under squared-error loss, we can\nseek the weights w= (w1,w2,... ,w M) such that\n\u02c6w= argmin\nwEP/bracketleft\uf8ecig\nY\u2212M/summationdisplay\nm=1wm\u02c6fm(x)/bracketright\uf8ecig2\n. (8.56)\nHere the input value xis \ufb01xed and the Nobservations in the dataset Z(and\nthe target Y) are distributed according to P. The solution is the population\nlinear regression of Yon\u02c6F(x)T\u2261[\u02c6f1(x),\u02c6f2(x),... ,\u02c6fM(x)]:\n\u02c6w= EP[\u02c6F(x)\u02c6F(x)T]\u22121EP[\u02c6F(x)Y]. (8.57)\nNow the full regression has smaller error than any single model\nEP/bracketleft\uf8ecigg\nY\u2212M/summationdisplay\nm=1\u02c6wm\u02c6fm(x)/bracketright\uf8ecigg2\n\u2264EP/bracketleft\uf8ecig\nY\u2212\u02c6fm(x)/bracketright\uf8ecig2\n\u2200m (8.58)\nso combining models never makes things worse, at the population level.", "308": "290 8. Model Inference and Averaging\nOf course the population linear regression (8.57) is not available, and it\nis natural to replace it with the linear regression over the training set. But\nthere are simple examples where this does not work well. For example, if\n\u02c6fm(x), m= 1,2,... ,M represent the prediction from the best subset of\ninputs of size mamong Mtotal inputs, then linear regression would put all\nof the weight on the largest model, that is, \u02c6 wM= 1,\u02c6wm= 0, m < M . The\nproblem is that we have not put each of the models on the same footing\nby taking into account their complexity (the number of inputs min this\nexample).\nStacked generalization , orstacking , is a way of doing this. Let \u02c6f\u2212i\nm(x)\nbe the prediction at x, using model m, applied to the dataset with the\nith training observation removed. The stacking estimate of the weights is\nobtained from the least squares linear regression of yion\u02c6f\u2212i\nm(xi), m=\n1,2,... ,M . In detail the stacking weights are given by\n\u02c6wst= argmin\nwN/summationdisplay\ni=1/bracketleft\uf8ecigg\nyi\u2212M/summationdisplay\nm=1wm\u02c6f\u2212i\nm(xi)/bracketright\uf8ecigg2\n. (8.59)\nThe \ufb01nal prediction is/summationtext\nm\u02c6wst\nm\u02c6fm(x). By using the cross-validated pre-\ndictions \u02c6f\u2212i\nm(x), stacking avoids giving unfairly high weight to models with\nhigher complexity. Better results can be obtained by restricting the weights\nto be nonnegative, and to sum to 1. This seems like a reasonable restriction\nif we interpret the weights as posterior model probabilities as in equation\n(8.54), and it leads to a tractable quadratic programming problem.\nThere is a close connection between stacking and model selection via\nleave-one-out cross-validation (Section 7.10). If we restrict the minimizatio n\nin (8.59) to weight vectors wthat have one unit weight and the rest zero,\nthis leads to a model choice \u02c6 mwith smallest leave-one-out cross-validation\nerror. Rather than choose a single model, stacking combines them with\nestimated optimal weights. This will often lead to better prediction, but\nless interpretability than the choice of only one of the Mmodels.\nThe stacking idea is actually more general than described above. One\ncan use any learning method, not just linear regression, to combine the\nmodels as in (8.59); the weights could also depend on the input location\nx. In this way, learning methods are \u201cstacked\u201d on top of one another, to\nimprove prediction performance.\n8.9 Stochastic Search: Bumping\nThe \ufb01nal method described in this chapter does not involve averaging or\ncombining models, but rather is a technique for \ufb01nding a better single\nmodel. Bumping uses bootstrap sampling to move randomly through model\nspace. For problems where \ufb01tting method \ufb01nds many local minima, bump-\ning can help the method to avoid getting stuck in poor solutions.", "309": "8.9 Stochastic Search: Bumping 291\nRegular 4-Node Tree\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022Bumped 4-Node Tree\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\nFIGURE 8.13. Data with two features and two classes (blue and orange), dis-\nplaying a pure interaction. The left panel shows the partition fo und by three splits\nof a standard, greedy, tree-growing algorithm. The vertical g rey line near the left\nedge is the \ufb01rst split, and the broken lines are the two subsequent splits. The al-\ngorithm has no idea where to make a good initial split, and makes a poor choice.\nThe right panel shows the near-optimal splits found by bumping th e tree-growing\nalgorithm 20times.\nAs in bagging, we draw bootstrap samples and \ufb01t a model to each. But\nrather than average the predictions, we choose the model estimated from a\nbootstrap sample that best \ufb01ts the training data. In detail, we draw boot-\nstrap samples Z\u22171,... ,Z\u2217Band \ufb01t our model to each, giving predictions\n\u02c6f\u2217b(x), b= 1,2,... ,B at input point x. We then choose the model that\nproduces the smallest prediction error, averaged over the original training\nset. For squared error, for example, we choose the model obtained from\nbootstrap sample \u02c6b, where\n\u02c6b= arg min\nbN/summationdisplay\ni=1[yi\u2212\u02c6f\u2217b(xi)]2. (8.60)\nThe corresponding model predictions are \u02c6f\u2217\u02c6b(x). By convention we also\ninclude the original training sample in the set of bootstrap samples, so that\nthe method is free to pick the original model if it has the lowest training\nerror.\nBy perturbing the data, bumping tries to move the \ufb01tting procedure\naround to good areas of model space. For example, if a few data points are\ncausing the procedure to \ufb01nd a poor solution, any bootstrap sample that\nomits those data points should procedure a better solution.\nFor another example, consider the classi\ufb01cation data in Figure 8.13, the\nnotorious exclusive or (XOR) problem. There are two classes (blue and\norange) and two input features, with the features exhibiting a pure inter-", "310": "292 8. Model Inference and Averaging\naction. By splitting the data at x1= 0 and then splitting each resulting\nstrata at x2= 0, (or vice versa) a tree-based classi\ufb01er could achieve per-\nfect discrimination. However, the greedy, short-sighted CART algorithm\n(Section 9.2) tries to \ufb01nd the best split on either feature, and then splits\nthe resulting strata. Because of the balanced nature of the data, all initial\nsplits on x1orx2appear to be useless, and the procedure essentially gener-\nates a random split at the top level. The actual split found for these data is\nshown in the left panel of Figure 8.13. By bootstrap sampling from the data ,\nbumping breaks the balance in the classes, and with a reasonable number\nof bootstrap samples (here 20), it will by chance produce at least one tree\nwith initial split near either x1= 0 or x2= 0. Using just 20 bootstrap\nsamples, bumping found the near optimal splits shown in the right panel\nof Figure 8.13. This shortcoming of the greedy tree-growing algorithm is\nexacerbated if we add a number of noise features that are independent of\nthe class label. Then the tree-growing algorithm cannot distinguish x1or\nx2from the others, and gets seriously lost.\nSince bumping compares di\ufb00erent models on the training data, one must\nensure that the models have roughly the same complexity. In the case of\ntrees, this would mean growing trees with the same number of terminal\nnodes on each bootstrap sample. Bumping can also help in problems where\nit is di\ufb03cult to optimize the \ufb01tting criterion, perhaps because of a lack of\nsmoothness. The trick is to optimize a di\ufb00erent, more convenient criterion\nover the bootstrap samples, and then choose the model producing the best\nresults for the desired criterion on the training sample.\nBibliographic Notes\nThere are many books on classical statistical inference: Cox and Hink-\nley (1974) and Silvey (1975) give nontechnical accounts. The bootstrap\nis due to Efron (1979) and is described more fully in Efron and Tibshi-\nrani (1993) and Hall (1992). A good modern book on Bayesian inference\nis Gelman et al. (1995). A lucid account of the application of Bayesian\nmethods to neural networks is given in Neal (1996). The statistical appli-\ncation of Gibbs sampling is due to Geman and Geman (1984), and Gelfand\nand Smith (1990), with related work by Tanner and Wong (1987). Markov\nchain Monte Carlo methods, including Gibbs sampling and the Metropolis\u2013\nHastings algorithm, are discussed in Spiegelhalter et al. (1996). The EM\nalgorithm is due to Dempster et al. (1977); as the discussants in that pa-\nper make clear, there was much related, earlier work. The view of EM as\na joint maximization scheme for a penalized complete-data log-likelihood\nwas elucidated by Neal and Hinton (1998); they credit Csiszar and Tusn\u00b4 ady\n(1984) and Hathaway (1986) as having noticed this connection earlier. Bag-\nging was proposed by Breiman (1996a). Stacking is due to Wolpert (1992) ;", "311": "Exercises 293\nBreiman (1996b) contains an accessible discussion for statisticians. Lebla nc\nand Tibshirani (1996) describe variations on stacking based on the boot-\nstrap. Model averaging in the Bayesian framework has been recently advo-\ncated by Madigan and Raftery (1994). Bumping was proposed by Tibshi-\nrani and Knight (1999).\nExercises\nEx. 8.1 Letr(y) and q(y) be probability density functions. Jensen\u2019s in-\nequality states that for a random variable Xand a convex function \u03c6(x),\nE[\u03c6(X)]\u2265\u03c6[E(X)]. Use Jensen\u2019s inequality to show that\nEqlog[r(Y)/q(Y)] (8.61)\nis maximized as a function of r(y) when r(y) =q(y). Hence show that\nR(\u03b8,\u03b8)\u2265R(\u03b8\u2032,\u03b8) as stated below equation (8.46).\nEx. 8.2 Consider the maximization of the log-likelihood (8.48), over dis-\ntributions \u02dcP(Zm) such that \u02dcP(Zm)\u22650 and/summationtext\nZm\u02dcP(Zm) = 1. Use La-\ngrange multipliers to show that the solution is the conditional distribution\n\u02dcP(Zm) = Pr( Zm|Z,\u03b8\u2032), as in (8.49).\nEx. 8.3 Justify the estimate (8.50), using the relationship\nPr(A) =/integraldisplay\nPr(A|B)d(Pr(B)).\nEx. 8.4 Consider the bagging method of Section 8.7. Let our estimate \u02c6f(x)\nbe the B-spline smoother \u02c6 \u03b8(x) of Section 8.2.1. Consider the parametric\nbootstrap of equation (8.6), applied to this estimator. Show that if we ba g\n\u02c6f(x), using the parametric bootstrap to generate the bootstrap samples,\nthe bagging estimate \u02c6fbag(x) converges to the original estimate \u02c6f(x) as\nB\u2192 \u221e.\nEx. 8.5 Suggest generalizations of each of the loss functions in Figure 10.4\nto more than two classes, and design an appropriate plot to compare them.\nEx. 8.6 Consider the bone mineral density data of Figure 5.6.\n(a) Fit a cubic smooth spline to the relative change in spinal BMD, as a\nfunction of age. Use cross-validation to estimate the optimal amount\nof smoothing. Construct pointwise 90% con\ufb01dence bands for the un-\nderlying function.\n(b) Compute the posterior mean and covariance for the true function via\n(8.28), and compare the posterior bands to those obtained in (a).", "312": "294 8. Model Inference and Averaging\n(c) Compute 100 bootstrap replicates of the \ufb01tted curves, as in the bottom\nleft panel of Figure 8.2. Compare the results to those obtained in (a)\nand (b).\nEx. 8.7 EM as a minorization algorithm (Hunter and Lange, 2004; Wu and\nLange, 2007). A function g(x,y) to said to minorize a function f(x) if\ng(x,y)\u2264f(x), g(x,x) =f(x) (8.62)\nfor all x,yin the domain. This is useful for maximizing f(x) since is easy\nto show that f(x) is non-decreasing under the update\nxs+1= argmaxxg(x,xs) (8.63)\nThere are analogous de\ufb01nitions for majorization , for minimizing a function\nf(x). The resulting algorithms are known as MMalgorithms, for \u201cMinorize-\nMaximize\u201d or \u201cMajorize-Minimize.\u201d\nShow that the EM algorithm (Section 8.5.2) is an example of an MM al-\ngorithm, using Q(\u03b8\u2032,\u03b8)+log Pr( Z|\u03b8)\u2212Q(\u03b8,\u03b8) to minorize the observed data\nlog-likelihood \u2113(\u03b8\u2032;Z). (Note that only the \ufb01rst term involves the relevant\nparameter \u03b8\u2032).", "313": "This is page 295\nPrinter: Opaque this\n9\nAdditive Models, Trees, and Related\nMethods\nIn this chapter we begin our discussion of some speci\ufb01c methods for super-\nvised learning. These techniques each assume a (di\ufb00erent) structured form\nfor the unknown regression function, and by doing so they \ufb01nesse the curse\nof dimensionality. Of course, they pay the possible price of misspecifying\nthe model, and so in each case there is a tradeo\ufb00 that has to be made. They\ntake o\ufb00 where Chapters 3\u20136 left o\ufb00. We describe \ufb01ve related techniques:\ngeneralized additive models, trees, multivariate adaptive regression splines,\nthe patient rule induction method, and hierarchical mixtures of experts.\n9.1 Generalized Additive Models\nRegression models play an important role in many data analyses, providi ng\nprediction and classi\ufb01cation rules, and data analytic tools for understand-\ning the importance of di\ufb00erent inputs.\nAlthough attractively simple, the traditional linear model often fails in\nthese situations: in real life, e\ufb00ects are often not linear. In earlier chapters\nwe described techniques that used prede\ufb01ned basis functions to achieve\nnonlinearities. This section describes more automatic \ufb02exible statistical\nmethods that may be used to identify and characterize nonlinear regression\ne\ufb00ects. These methods are called \u201cgeneralized additive models.\u201d\nIn the regression setting, a generalized additive model has the form\nE(Y|X1,X2,... ,X p) =\u03b1+f1(X1) +f2(X2) +\u2264\u2264\u2264+fp(Xp).(9.1)", "314": "296 9. Additive Models, Trees, and Related Methods\nAs usual X1,X2,... ,X prepresent predictors and Yis the outcome; the fj\u2019s\nare unspeci\ufb01ed smooth (\u201cnonparametric\u201d) functions. If we were to model\neach function using an expansion of basis functions (as in Chapter 5), the\nresulting model could then be \ufb01t by simple least squares. Our approach\nhere is di\ufb00erent: we \ufb01t each function using a scatterplot smoother (e.g., a\ncubic smoothing spline or kernel smoother), and provide an algorithm for\nsimultaneously estimating all pfunctions (Section 9.1.1).\nFor two-class classi\ufb01cation, recall the logistic regression model for binar y\ndata discussed in Section 4.4. We relate the mean of the binary response\n\u03b8(X) = Pr( Y= 1|X) to the predictors via a linear regression model and\nthelogitlink function:\nlog/parenleftbigg\u03b8(X)\n1\u2212\u03b8(X)/parenrightbigg\n=\u03b1+\u03b21X1+\u2264\u2264\u2264+\u03b2pXp. (9.2)\nTheadditive logistic regression model replaces each linear term by a more\ngeneral functional form\nlog/parenleftbigg\u03b8(X)\n1\u2212\u03b8(X)/parenrightbigg\n=\u03b1+f1(X1) +\u2264\u2264\u2264+fp(Xp), (9.3)\nwhere again each fjis an unspeci\ufb01ed smooth function. While the non-\nparametric form for the functions fjmakes the model more \ufb02exible, the\nadditivity is retained and allows us to interpret the model in much the\nsame way as before. The additive logistic regression model is an example\nof a generalized additive model. In general, the conditional mean \u03b8(X) of\na response Yis related to an additive function of the predictors via a link\nfunction g:\ng[\u03b8(X)] =\u03b1+f1(X1) +\u2264\u2264\u2264+fp(Xp). (9.4)\nExamples of classical link functions are the following:\n\u2022g(\u03b8) =\u03b8is the identity link, used for linear and additive models for\nGaussian response data.\n\u2022g(\u03b8) = logit( \u03b8) as above, or g(\u03b8) = probit( \u03b8), theprobit link function,\nfor modeling binomial probabilities. The probit function is the inverse\nGaussian cumulative distribution function: probit( \u03b8) = \u03a6\u22121(\u03b8).\n\u2022g(\u03b8) = log( \u03b8) for log-linear or log-additive models for Poisson count\ndata.\nAll three of these arise from exponential family sampling models, which\nin addition include the gamma and negative-binomial distributions. These\nfamilies generate the well-known class of generalized linear models, which\nare all extended in the same way to generalized additive models.\nThe functions fjare estimated in a \ufb02exible manner, using an algorithm\nwhose basic building block is a scatterplot smoother. The estimated func-\ntion\u02c6fjcan then reveal possible nonlinearities in the e\ufb00ect of Xj. Not all", "315": "9.1 Generalized Additive Models 297\nof the functions fjneed to be nonlinear. We can easily mix in linear and\nother parametric forms with the nonlinear terms, a necessity when some of\nthe inputs are qualitative variables (factors). The nonlinear terms are not\nrestricted to main e\ufb00ects either; we can have nonlinear components in two\nor more variables, or separate curves in Xjfor each level of the factor Xk.\nThus each of the following would qualify:\n\u2022g(\u03b8) =XT\u03b2+\u03b1k+f(Z)\u2014asemiparametric model, where Xis a\nvector of predictors to be modeled linearly, \u03b1kthe e\ufb00ect for the kth\nlevel of a qualitative input V, and the e\ufb00ect of predictor Zis modeled\nnonparametrically.\n\u2022g(\u03b8) =f(X) +gk(Z)\u2014again kindexes the levels of a qualitative\ninput V, and thus creates an interaction term g(V,Z) =gk(Z) for\nthe e\ufb00ect of VandZ.\n\u2022g(\u03b8) =f(X) +g(Z,W) where gis a nonparametric function in two\nfeatures.\nAdditive models can replace linear models in a wide variety of settings,\nfor example an additive decomposition of time series,\nYt=St+Tt+\u03b5t, (9.5)\nwhere Stis a seasonal component, Ttis a trend and \u03b5is an error term.\n9.1.1 Fitting Additive Models\nIn this section we describe a modular algorithm for \ufb01tting additive models\nand their generalizations. The building block is the scatterplot smoother\nfor \ufb01tting nonlinear e\ufb00ects in a \ufb02exible way. For concreteness we use as our\nscatterplot smoother the cubic smoothing spline described in Chapter 5.\nThe additive model has the form\nY=\u03b1+p/summationdisplay\nj=1fj(Xj) +\u03b5, (9.6)\nwhere the error term \u03b5has mean zero. Given observations xi,yi, a criterion\nlike the penalized sum of squares (5.9) of Section 5.4 can be speci\ufb01ed for\nthis problem,\nPRSS( \u03b1,f1,f2,... ,f p) =N/summationdisplay\ni=1/parenleft\uf8ecigg\nyi\u2212\u03b1\u2212p/summationdisplay\nj=1fj(xij)/parenright\uf8ecigg2\n+p/summationdisplay\nj=1\u03bbj/integraldisplay\nf\u2032\u2032\nj(tj)2dtj,\n(9.7)\nwhere the \u03bbj\u22650 are tuning parameters. It can be shown that the minimizer\nof (9.7) is an additive cubic spline model; each of the functions fjis a", "316": "298 9. Additive Models, Trees, and Related Methods\nAlgorithm 9.1 The Back\ufb01tting Algorithm for Additive Models.\n1. Initialize: \u02c6 \u03b1=1\nN/summationtextN\n1yi,\u02c6fj\u22610,\u2200i,j.\n2. Cycle: j= 1,2,... ,p,... , 1,2,... ,p,... ,\n\u02c6fj\u2190 S j/bracketleft\uf8ecigg\n{yi\u2212\u02c6\u03b1\u2212/summationdisplay\nk/ne}ationslash=j\u02c6fk(xik)}N\n1/bracketright\uf8ecigg\n,\n\u02c6fj\u2190\u02c6fj\u22121\nNN/summationdisplay\ni=1\u02c6fj(xij).\nuntil the functions \u02c6fjchange less than a prespeci\ufb01ed threshold.\ncubic spline in the component Xj, with knots at each of the unique values\nofxij, i= 1,... ,N . However, without further restrictions on the model,\nthe solution is not unique. The constant \u03b1is not identi\ufb01able, since we\ncan add or subtract any constants to each of the functions fj, and adjust\n\u03b1accordingly. The standard convention is to assume that/summationtextN\n1fj(xij) =\n0\u2200j\u2014the functions average zero over the data. It is easily seen that \u02c6 \u03b1=\nave(yi) in this case. If in addition to this restriction, the matrix of input\nvalues (having ijth entry xij) has full column rank, then (9.7) is a strictly\nconvex criterion and the minimizer is unique. If the matrix is singular, then\nthelinear part of the components fjcannot be uniquely determined (while\nthe nonlinear parts can!)(Buja et al., 1989).\nFurthermore, a simple iterative procedure exists for \ufb01nding the solution.\nWe set \u02c6 \u03b1= ave( yi), and it never changes. We apply a cubic smoothing\nspline Sjto the targets {yi\u2212\u02c6\u03b1\u2212/summationtext\nk/ne}ationslash=j\u02c6fk(xik)}N\n1, as a function of xij,\nto obtain a new estimate \u02c6fj. This is done for each predictor in turn, using\nthe current estimates of the other functions \u02c6fkwhen computing yi\u2212\u02c6\u03b1\u2212/summationtext\nk/ne}ationslash=j\u02c6fk(xik). The process is continued until the estimates \u02c6fjstabilize. This\nprocedure, given in detail in Algorithm 9.1, is known as \u201cback\ufb01tting\u201d and\nthe resulting \ufb01t is analogous to a multiple regression for linear models.\nIn principle, the second step in (2) of Algorithm 9.1 is not needed, since\nthe smoothing spline \ufb01t to a mean-zero response has mean zero (Exer-\ncise 9.1). In practice, machine rounding can cause slippage, and the ad-\njustment is advised.\nThis same algorithm can accommodate other \ufb01tting methods in exactly\nthe same way, by specifying appropriate smoothing operators Sj:\n\u2022other univariate regression smoothers such as local polynomial re-\ngression and kernel methods;", "317": "9.1 Generalized Additive Models 299\n\u2022linear regression operators yielding polynomial \ufb01ts, piecewise con-\nstant \ufb01ts, parametric spline \ufb01ts, series and Fourier \ufb01ts;\n\u2022more complicated operators such as surface smoothers for second or\nhigher-order interactions or periodic smoothers for seasonal e\ufb00ects.\nIf we consider the operation of smoother Sjonly at the training points, it\ncan be represented by an N\u00d7Noperator matrix Sj(see Section 5.4.1).\nThen the degrees of freedom for the jth term are (approximately) computed\nas df j= trace[ Sj]\u22121, by analogy with degrees of freedom for smoothers\ndiscussed in Chapters 5 and 6.\nFor a large class of linear smoothers Sj, back\ufb01tting is equivalent to a\nGauss\u2013Seidel algorithm for solving a certain linear system of equations.\nDetails are given in Exercise 9.2.\nFor the logistic regression model and other generalized additive models,\nthe appropriate criterion is a penalized log-likelihood. To maximize it, the\nback\ufb01tting procedure is used in conjunction with a likelihood maximizer.\nThe usual Newton\u2013Raphson routine for maximizing log-likelihoods in gen-\neralized linear models can be recast as an IRLS (iteratively reweighted\nleast squares) algorithm. This involves repeatedly \ufb01tting a weighted linear\nregression of a working response variable on the covariates; each regress ion\nyields a new value of the parameter estimates, which in turn give new work-\ning responses and weights, and the process is iterated (see Section 4.4.1).\nIn the generalized additive model, the weighted linear regression is simply\nreplaced by a weighted back\ufb01tting algorithm. We describe the algorithm in\nmore detail for logistic regression below, and more generally in Chapter 6\nof Hastie and Tibshirani (1990).\n9.1.2 Example: Additive Logistic Regression\nProbably the most widely used model in medical research is the logistic\nmodel for binary data. In this model the outcome Ycan be coded as 0\nor 1, with 1 indicating an event (like death or relapse of a disease) and\n0 indicating no event. We wish to model Pr( Y= 1|X), the probability of\nan event given values of the prognostic factors XT= (X1,... ,X p). The\ngoal is usually to understand the roles of the prognostic factors, rather\nthan to classify new individuals. Logistic models are also used in applica-\ntions where one is interested in estimating the class probabilities, for use\nin risk screening. Apart from medical applications, credit risk screening is\na popular application.\nThe generalized additive logistic model has the form\nlogPr(Y= 1|X)\nPr(Y= 0|X)=\u03b1+f1(X1) +\u2264\u2264\u2264+fp(Xp). (9.8)\nThe functions f1,f2,... ,f pare estimated by a back\ufb01tting algorithm\nwithin a Newton\u2013Raphson procedure, shown in Algorithm 9.2.", "318": "300 9. Additive Models, Trees, and Related Methods\nAlgorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regres-\nsion Model.\n1. Compute starting values: \u02c6 \u03b1= log[\u00af y/(1\u2212\u00afy)], where \u00af y= ave( yi), the\nsample proportion of ones, and set \u02c6fj\u22610\u2200j.\n2. De\ufb01ne \u02c6 \u03b7i= \u02c6\u03b1+/summationtext\nj\u02c6fj(xij) and \u02c6 pi= 1/[1 + exp( \u2212\u02c6\u03b7i)].\nIterate:\n(a) Construct the working target variable\nzi= \u02c6\u03b7i+(yi\u2212\u02c6pi)\n\u02c6pi(1\u2212\u02c6pi).\n(b) Construct weights wi= \u02c6pi(1\u2212\u02c6pi)\n(c) Fit an additive model to the targets ziwith weights wi, us-\ning a weighted back\ufb01tting algorithm. This gives new estimates\n\u02c6\u03b1,\u02c6fj,\u2200j\n3. Continue step 2. until the change in the functions falls below a pre-\nspeci\ufb01ed threshold.\nThe additive model \ufb01tting in step (2) of Algorithm 9.2 requires a weighted\nscatterplot smoother. Most smoothing procedures can accept observation\nweights (Exercise 5.12); see Chapter 3 of Hastie and Tibshirani (1990) fo r\nfurther details.\nThe additive logistic regression model can be generalized further to han-\ndle more than two classes, using the multilogit formulation as outlined in\nSection 4.4. While the formulation is a straightforward extension of ( 9.8),\nthe algorithms for \ufb01tting such models are more complex. See Yee and Wild\n(1996) for details, and the VGAMsoftware currently available from:\nhttp://www.stat.auckland.ac.nz/ \u223cyee.\nExample: Predicting Email Spam\nWe apply a generalized additive model to the spam data introduced in\nChapter 1. The data consists of information from 4601 email messages, in\na study to screen email for \u201cspam\u201d (i.e., junk email). The data is publicly\navailable at ftp.ics.uci.edu , and was donated by George Forman from\nHewlett-Packard laboratories, Palo Alto, California.\nThe response variable is binary, with values email orspam, and there are\n57 predictors as described below:\n\u202248 quantitative predictors\u2014the percentage of words in the email that\nmatch a given word. Examples include business ,address ,internet ,", "319": "9.1 Generalized Additive Models 301\nTABLE 9.1. Test data confusion matrix for the additive logistic regress ion model\n\ufb01t to the spam training data. The overall test error rate is 5.5%.\nPredicted Class\nTrue Class email (0)spam(1)\nemail (0) 58.3% 2.5%\nspam(1) 3.0% 36.3%\nfree, andgeorge . The idea was that these could be customized for\nindividual users.\n\u20226 quantitative predictors\u2014the percentage of characters in the email\nthat match a given character. The characters are ch;,ch(,ch[,ch!,\nch$, andch#.\n\u2022The average length of uninterrupted sequences of capital letters:\nCAPAVE .\n\u2022The length of the longest uninterrupted sequence of capital letters:\nCAPMAX .\n\u2022The sum of the length of uninterrupted sequences of capital letters:\nCAPTOT .\nWe coded spamas 1 and email as zero. A test set of size 1536 was randomly\nchosen, leaving 3065 observations in the training set. A generalized additive\nmodel was \ufb01t, using a cubic smoothing spline with a nominal four degrees of\nfreedom for each predictor. What this means is that for each predictor Xj,\nthe smoothing-spline parameter \u03bbjwas chosen so that trace[ Sj(\u03bbj)]\u22121 = 4,\nwhereSj(\u03bb) is the smoothing spline operator matrix constructed using the\nobserved values xij, i= 1,... ,N . This is a convenient way of specifying\nthe amount of smoothing in such a complex model.\nMost of the spampredictors have a very long-tailed distribution. Before\n\ufb01tting the GAM model, we log-transformed each variable (actually log( x+\n0.1)), but the plots in Figure 9.1 are shown as a function of the original\nvariables.\nThe test error rates are shown in Table 9.1; the overall error rate is 5.3 %.\nBy comparison, a linear logistic regression has a test error rate of 7.6% .\nTable 9.2 shows the predictors that are highly signi\ufb01cant in the additive\nmodel.\nFor ease of interpretation, in Table 9.2 the contribution for each variabl e\nis decomposed into a linear component and the remaining nonlinear com-\nponent. The top block of predictors are positively correlated with spam,\nwhile the bottom block is negatively correlated. The linear component is a\nweighted least squares linear \ufb01t of the \ufb01tted curve on the predictor, while\nthe nonlinear part is the residual. The linear component of an estimated", "320": "302 9. Additive Models, Trees, and Related Methods\nTABLE 9.2. Signi\ufb01cant predictors from the additive model \ufb01t to the spam train-\ning data. The coe\ufb03cients represent the linear part of \u02c6fj, along with their standard\nerrors and Z-score. The nonlinear P-value is for a test of nonlineari ty of \u02c6fj.\nName Num. df Coe\ufb03cient Std. Error ZScore Nonlinear\nP-value\nPositive e\ufb00ects\nour 5 3.9 0.566 0.114 4.970 0.052\nover 6 3.9 0.244 0.195 1.249 0.004\nremove 7 4.0 0.949 0.183 5.201 0.093\ninternet 8 4.0 0.524 0.176 2.974 0.028\nfree 16 3.9 0.507 0.127 4.010 0.065\nbusiness 17 3.8 0.779 0.186 4.179 0.194\nhpl 26 3.8 0.045 0.250 0.181 0.002\nch! 52 4.0 0.674 0.128 5.283 0.164\nch$ 53 3.9 1.419 0.280 5.062 0.354\nCAPMAX 56 3.8 0.247 0.228 1.080 0.000\nCAPTOT 57 4.0 0.755 0.165 4.566 0.063\nNegative e\ufb00ects\nhp 25 3.9 \u22121.404 0.224 \u22126.262 0.140\ngeorge 27 3.7 \u22125.003 0.744 \u22126.722 0.045\n1999 37 3.8 \u22120.672 0.191 \u22123.512 0.011\nre 45 3.9 \u22120.620 0.133 \u22124.649 0.597\nedu 46 4.0 \u22121.183 0.209 \u22125.647 0.000\nfunction is summarized by the coe\ufb03cient, standard error and Z-score; the\nlatter is the coe\ufb03cient divided by its standard error, and is considered\nsigni\ufb01cant if it exceeds the appropriate quantile of a standard normal dis-\ntribution. The column labeled nonlinear P-value is a test of nonlinearity\nof the estimated function. Note, however, that the e\ufb00ect of each predictor\nis fully adjusted for the entire e\ufb00ects of the other predictors, not just for\ntheir linear parts. The predictors shown in the table were judged signi\ufb01-\ncant by at least one of the tests (linear or nonlinear) at the p= 0.01 level\n(two-sided).\nFigure 9.1 shows the estimated functions for the signi\ufb01cant predictors\nappearing in Table 9.2. Many of the nonlinear e\ufb00ects appear to account for\na strong discontinuity at zero. For example, the probability of spamdrops\nsigni\ufb01cantly as the frequency of george increases from zero, but then does\nnot change much after that. This suggests that one might replace each of\nthe frequency predictors by an indicator variable for a zero count, and resort\nto a linear logistic model. This gave a test error rate of 7 .4%; including the\nlinear e\ufb00ects of the frequencies as well dropped the test error to 6 .6%. It\nappears that the nonlinearities in the additive model have an additional\npredictive power.", "321": "9.1 Generalized Additive Models 303\n0 2 4 6 8-5 0 5\n0 1 2 3-5 0 5\n0 2 4 6-5 0 5 10\n0 2 4 6 8 10-5 0 5 10\n0 2 4 6 8 10-5 0 5 10\n0 2 4 6-5 0 5 10\n0 5 10 15 20-10 -5 0\n0 5 10-10 -5 0\n0 10 20 30-10 -5 0 5\n0 2 4 6-5 0 5\n0 5 10 15 20-10 -5 0 5\n0 5 10 15-10 -5 0\n0 10 20 30-5 0 5 10\n0 1 2 3 4 5 6-5 0 5 10\n0 2000 6000 10000-5 0 5\n0 5000 10000 15000-5 0 5our over remove internet\nfree business hp hpl\ngeorge 1999 re edu\nch! ch$ CAPMAX CAPTOT\u02c6f(our)\n\u02c6f(over)\n\u02c6f(remove )\n\u02c6f(internet )\u02c6f(free)\n\u02c6f(business )\n\u02c6f(hp)\n\u02c6f(hpl)\u02c6f(george )\n\u02c6f(1999)\n\u02c6f(re)\n\u02c6f(edu)\u02c6f(ch!)\n\u02c6f(ch$)\n\u02c6f(CAPMAX )\n\u02c6f(CAPTOT )\nFIGURE 9.1. Spam analysis: estimated functions for signi\ufb01cant predictors. The\nrug plot along the bottom of each frame indicates the observed values of the cor-\nresponding predictor. For many of the predictors the nonlinearity picks up the\ndiscontinuity at zero.", "322": "304 9. Additive Models, Trees, and Related Methods\nIt is more serious to classify a genuine email message as spam, since then\na good email would be \ufb01ltered out and would not reach the user. We can\nalter the balance between the class error rates by changing the losses (see\nSection 2.4). If we assign a loss L01for predicting a true class 0 as class 1,\nandL10for predicting a true class 1 as class 0, then the estimated Bayes\nrule predicts class 1 if its probability is greater than L01/(L01+L10). For\nexample, if we take L01= 10,L10= 1 then the (true) class 0 and class 1\nerror rates change to 0.8% and 8.7%.\nMore ambitiously, we can encourage the model to \ufb01t better data in the\nclass 0 by using weights L01for the class 0 observations and L10for the\nclass 1 observations. As above, we then use the estimated Bayes rule to\npredict. This gave error rates of 1.2% and 8.0% in (true) class 0 and class 1,\nrespectively. We discuss below the issue of unequal losses further, in the\ncontext of tree-based models.\nAfter \ufb01tting an additive model, one should check whether the inclusion\nof some interactions can signi\ufb01cantly improve the \ufb01t. This can be done\n\u201cmanually,\u201d by inserting products of some or all of the signi\ufb01cant inputs,\nor automatically via the MARS procedure (Section 9.4).\nThis example uses the additive model in an automatic fashion. As a data\nanalysis tool, additive models are often used in a more interactive fashi on,\nadding and dropping terms to determine their e\ufb00ect. By calibrating the\namount of smoothing in terms of df j, one can move seamlessly between\nlinear models (df j= 1) and partially linear models, where some terms are\nmodeled more \ufb02exibly. See Hastie and Tibshirani (1990) for more details.\n9.1.3 Summary\nAdditive models provide a useful extension of linear models, making them\nmore \ufb02exible while still retaining much of their interpretability. The famil iar\ntools for modeling and inference in linear models are also available for\nadditive models, seen for example in Table 9.2. The back\ufb01tting procedure\nfor \ufb01tting these models is simple and modular, allowing one to choose a\n\ufb01tting method appropriate for each input variable. As a result they have\nbecome widely used in the statistical community.\nHowever additive models can have limitations for large data-mining ap-\nplications. The back\ufb01tting algorithm \ufb01ts all predictors, which is not feasi-\nble or desirable when a large number are available. The BRUTO procedure\n(Hastie and Tibshirani, 1990, Chapter 9) combines back\ufb01tting with selec-\ntion of inputs, but is not designed for large data-mining problems. There\nhas also been recent work using lasso-type penalties to estimate sparse ad-\nditive models, for example the COSSO procedure of Lin and Zhang (2006)\nand the SpAM proposal of Ravikumar et al. (2008). For large problems a\nforward stagewise approach such as boosting (Chapter 10) is more e\ufb00ectiv e,\nand also allows for interactions to be included in the model.", "323": "9.2 Tree-Based Methods 305\n9.2 Tree-Based Methods\n9.2.1 Background\nTree-based methods partition the feature space into a set of rectangles, and\nthen \ufb01t a simple model (like a constant) in each one. They are conceptually\nsimple yet powerful. We \ufb01rst describe a popular method for tree-based\nregression and classi\ufb01cation called CART, and later contrast it with C4. 5,\na major competitor.\nLet\u2019s consider a regression problem with continuous response Yand in-\nputsX1andX2, each taking values in the unit interval. The top left panel\nof Figure 9.2 shows a partition of the feature space by lines that are parall el\nto the coordinate axes. In each partition element we can model Ywith a\ndi\ufb00erent constant. However, there is a problem: although each partitioning\nline has a simple description like X1=c, some of the resulting regions are\ncomplicated to describe.\nTo simplify matters, we restrict attention to recursive binary partitio ns\nlike that in the top right panel of Figure 9.2. We \ufb01rst split the space into\ntwo regions, and model the response by the mean of Yin each region.\nWe choose the variable and split-point to achieve the best \ufb01t. Then one\nor both of these regions are split into two more regions, and this process\nis continued, until some stopping rule is applied. For example, in the top\nright panel of Figure 9.2, we \ufb01rst split at X1=t1. Then the region X1\u2264t1\nis split at X2=t2and the region X1> t1is split at X1=t3. Finally, the\nregion X1> t3is split at X2=t4. The result of this process is a partition\ninto the \ufb01ve regions R1,R2,... ,R 5shown in the \ufb01gure. The corresponding\nregression model predicts Ywith a constant cmin region Rm, that is,\n\u02c6f(X) =5/summationdisplay\nm=1cmI{(X1,X2)\u2208Rm}. (9.9)\nThis same model can be represented by the binary tree in the bottom left\npanel of Figure 9.2. The full dataset sits at the top of the tree. Observations\nsatisfying the condition at each junction are assigned to the left branch,\nand the others to the right branch. The terminal nodes or leaves of the\ntree correspond to the regions R1,R2,... ,R 5. The bottom right panel of\nFigure 9.2 is a perspective plot of the regression surface from this model.\nFor illustration, we chose the node means c1=\u22125,c2=\u22127,c3= 0,c4=\n2,c5= 4 to make this plot.\nA key advantage of the recursive binary tree is its interpretability. The\nfeature space partition is fully described by a single tree. With more than\ntwo inputs, partitions like that in the top right panel of Figure 9.2 are\ndi\ufb03cult to draw, but the binary tree representation works in the same\nway. This representation is also popular among medical scientists, perhaps\nbecause it mimics the way that a doctor thinks. The tree strati\ufb01es the", "324": "306 9. Additive Models, Trees, and Related Methods\n|t1t2\nt3t4\nR1R1\nR2R2\nR3R3\nR4R4\nR5R5\nX1X1 X1\nX2X2X2\nX1\u2264t1\nX2\u2264t2 X1\u2264t3\nX2\u2264t4\nFIGURE 9.2. Partitions and CART. Top right panel shows a partition of a\ntwo-dimensional feature space by recursive binary splitting, a s used in CART,\napplied to some fake data. Top left panel shows a general partit ion that cannot\nbe obtained from recursive binary splitting. Bottom left panel s hows the tree cor-\nresponding to the partition in the top right panel, and a perspect ive plot of the\nprediction surface appears in the bottom right panel.", "325": "9.2 Tree-Based Methods 307\npopulation into strata of high and low outcome, on the basis of patient\ncharacteristics.\n9.2.2 Regression Trees\nWe now turn to the question of how to grow a regression tree. Our data\nconsists of pinputs and a response, for each of Nobservations: that is,\n(xi,yi) for i= 1,2,... ,N , with xi= (xi1,xi2,... ,x ip). The algorithm\nneeds to automatically decide on the splitting variables and split points,\nand also what topology (shape) the tree should have. Suppose \ufb01rst that we\nhave a partition into Mregions R1,R2,... ,R M, and we model the response\nas a constant cmin each region:\nf(x) =M/summationdisplay\nm=1cmI(x\u2208Rm). (9.10)\nIf we adopt as our criterion minimization of the sum of squares/summationtext(yi\u2212\nf(xi))2, it is easy to see that the best \u02c6 cmis just the average of yiin region\nRm:\n\u02c6cm= ave( yi|xi\u2208Rm). (9.11)\nNow \ufb01nding the best binary partition in terms of minimum sum of squares\nis generally computationally infeasible. Hence we proceed with a greedy\nalgorithm. Starting with all of the data, consider a splitting variable jand\nsplit point s, and de\ufb01ne the pair of half-planes\nR1(j,s) ={X|Xj\u2264s}andR2(j,s) ={X|Xj> s}. (9.12)\nThen we seek the splitting variable jand split point sthat solve\nmin\nj, s/bracketleft\uf8ecig\nmin\nc1/summationdisplay\nxi\u2208R1(j,s)(yi\u2212c1)2+ min\nc2/summationdisplay\nxi\u2208R2(j,s)(yi\u2212c2)2/bracketright\uf8ecig\n. (9.13)\nFor any choice jands, the inner minimization is solved by\n\u02c6c1= ave( yi|xi\u2208R1(j,s)) and \u02c6 c2= ave( yi|xi\u2208R2(j,s)). (9.14)\nFor each splitting variable, the determination of the split point scan\nbe done very quickly and hence by scanning through all of the inputs,\ndetermination of the best pair ( j,s) is feasible.\nHaving found the best split, we partition the data into the two resulting\nregions and repeat the splitting process on each of the two regions. Then\nthis process is repeated on all of the resulting regions.\nHow large should we grow the tree? Clearly a very large tree might over\ufb01t\nthe data, while a small tree might not capture the important structure.", "326": "308 9. Additive Models, Trees, and Related Methods\nTree size is a tuning parameter governing the model\u2019s complexity, and the\noptimal tree size should be adaptively chosen from the data. One approach\nwould be to split tree nodes only if the decrease in sum-of-squares due to the\nsplit exceeds some threshold. This strategy is too short-sighted, however,\nsince a seemingly worthless split might lead to a very good split below it.\nThe preferred strategy is to grow a large tree T0, stopping the splitting\nprocess only when some minimum node size (say 5) is reached. Then this\nlarge tree is pruned using cost-complexity pruning , which we now describe.\nWe de\ufb01ne a subtree T\u2282T0to be any tree that can be obtained by\npruning T0, that is, collapsing any number of its internal (non-terminal)\nnodes. We index terminal nodes by m, with node mrepresenting region\nRm. Let|T|denote the number of terminal nodes in T. Letting\nNm= #{xi\u2208Rm},\n\u02c6cm=1\nNm/summationdisplay\nxi\u2208Rmyi,\nQm(T) =1\nNm/summationdisplay\nxi\u2208Rm(yi\u2212\u02c6cm)2,(9.15)\nwe de\ufb01ne the cost complexity criterion\nC\u03b1(T) =|T|/summationdisplay\nm=1NmQm(T) +\u03b1|T|. (9.16)\nThe idea is to \ufb01nd, for each \u03b1, the subtree T\u03b1\u2286T0to minimize C\u03b1(T).\nThe tuning parameter \u03b1\u22650 governs the tradeo\ufb00 between tree size and its\ngoodness of \ufb01t to the data. Large values of \u03b1result in smaller trees T\u03b1, and\nconversely for smaller values of \u03b1. As the notation suggests, with \u03b1= 0 the\nsolution is the full tree T0. We discuss how to adaptively choose \u03b1below.\nFor each \u03b1one can show that there is a unique smallest subtree T\u03b1that\nminimizes C\u03b1(T). To \ufb01nd T\u03b1we use weakest link pruning : we successively\ncollapse the internal node that produces the smallest per-node increase in/summationtext\nmNmQm(T), and continue until we produce the single-node (root) tree.\nThis gives a (\ufb01nite) sequence of subtrees, and one can show this sequence\nmust contain T\u03b1. See Breiman et al. (1984) or Ripley (1996) for details.\nEstimation of \u03b1is achieved by \ufb01ve- or tenfold cross-validation: we choose\nthe value \u02c6 \u03b1to minimize the cross-validated sum of squares. Our \ufb01nal tree\nisT\u02c6\u03b1.\n9.2.3 Classi\ufb01cation Trees\nIf the target is a classi\ufb01cation outcome taking values 1 ,2,... ,K , the only\nchanges needed in the tree algorithm pertain to the criteria for splitting\nnodes and pruning the tree. For regression we used the squared-error node", "327": "9.2 Tree-Based Methods 309\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5\npEntropy\nGini index\nMisclassification error\nFIGURE 9.3. Node impurity measures for two-class classi\ufb01cation, as a funct ion\nof the proportion pin class 2. Cross-entropy has been scaled to pass through\n(0.5,0.5).\nimpurity measure Qm(T) de\ufb01ned in (9.15), but this is not suitable for\nclassi\ufb01cation. In a node m, representing a region RmwithNmobservations,\nlet\n\u02c6pmk=1\nNm/summationdisplay\nxi\u2208RmI(yi=k),\nthe proportion of class kobservations in node m. We classify the obser-\nvations in node mto class k(m) = arg max k\u02c6pmk, the majority class in\nnodem. Di\ufb00erent measures Qm(T) of node impurity include the following:\nMisclassi\ufb01cation error:1\nNm/summationtext\ni\u2208RmI(yi\u221dne}ationslash=k(m)) = 1 \u2212\u02c6pmk(m).\nGini index:/summationtext\nk/ne}ationslash=k\u2032\u02c6pmk\u02c6pmk\u2032=/summationtextK\nk=1\u02c6pmk(1\u2212\u02c6pmk).\nCross-entropy or deviance: \u2212/summationtextK\nk=1\u02c6pmklog \u02c6pmk.\n(9.17)\nFor two classes, if pis the proportion in the second class, these three mea-\nsures are 1 \u2212max(p,1\u2212p), 2p(1\u2212p) and \u2212plogp\u2212(1\u2212p)log (1 \u2212p),\nrespectively. They are shown in Figure 9.3. All three are similar, but cross -\nentropy and the Gini index are di\ufb00erentiable, and hence more amenable to\nnumerical optimization. Comparing (9.13) and (9.15), we see that we need\nto weight the node impurity measures by the number NmLandNmRof\nobservations in the two child nodes created by splitting node m.\nIn addition, cross-entropy and the Gini index are more sensitive to changes\nin the node probabilities than the misclassi\ufb01cation rate. For example, in\na two-class problem with 400 observations in each class (denote this by\n(400,400)), suppose one split created nodes (300 ,100) and (100 ,300), while", "328": "310 9. Additive Models, Trees, and Related Methods\nthe other created nodes (200 ,400) and (200 ,0). Both splits produce a mis-\nclassi\ufb01cation rate of 0.25, but the second split produces a pure node and is\nprobably preferable. Both the Gini index and cross-entropy are lower for the\nsecond split. For this reason, either the Gini index or cross-entropy should\nbe used when growing the tree. To guide cost-complexity pruning, any of\nthe three measures can be used, but typically it is the misclassi\ufb01cation rate.\nThe Gini index can be interpreted in two interesting ways. Rather than\nclassify observations to the majority class in the node, we could classify\nthem to class kwith probability \u02c6 pmk. Then the training error rate of this\nrule in the node is/summationtext\nk/ne}ationslash=k\u2032\u02c6pmk\u02c6pmk\u2032\u2014the Gini index. Similarly, if we code\neach observation as 1 for class kand zero otherwise, the variance over the\nnode of this 0-1 response is \u02c6 pmk(1\u2212\u02c6pmk). Summing over classes kagain\ngives the Gini index.\n9.2.4 Other Issues\nCategorical Predictors\nWhen splitting a predictor having qpossible unordered values, there are\n2q\u22121\u22121 possible partitions of the qvalues into two groups, and the com-\nputations become prohibitive for large q. However, with a 0 \u22121 outcome,\nthis computation simpli\ufb01es. We order the predictor classes according to the\nproportion falling in outcome class 1. Then we split this predictor as if it\nwere an ordered predictor. One can show this gives the optimal split, in\nterms of cross-entropy or Gini index, among all possible 2q\u22121\u22121 splits. This\nresult also holds for a quantitative outcome and square error loss\u2014the cat-\negories are ordered by increasing mean of the outcome. Although intuitive,\nthe proofs of these assertions are not trivial. The proof for binary outcomes\nis given in Breiman et al. (1984) and Ripley (1996); the proof for quanti ta-\ntive outcomes can be found in Fisher (1958). For multicategory outcomes,\nno such simpli\ufb01cations are possible, although various approximations have\nbeen proposed (Loh and Vanichsetakul, 1988).\nThe partitioning algorithm tends to favor categorical predictors with\nmany levels q; the number of partitions grows exponentially in q, and the\nmore choices we have, the more likely we can \ufb01nd a good one for the data\nat hand. This can lead to severe over\ufb01tting if qis large, and such variables\nshould be avoided.\nThe Loss Matrix\nIn classi\ufb01cation problems, the consequences of misclassifying observations\nare more serious in some classes than others. For example, it is probably\nworse to predict that a person will not have a heart attack when he/she\nactually will, than vice versa. To account for this, we de\ufb01ne a K\u00d7Kloss\nmatrix L, with Lkk\u2032being the loss incurred for classifying a class kobser-\nvation as class k\u2032. Typically no loss is incurred for correct classi\ufb01cations,", "329": "9.2 Tree-Based Methods 311\nthat is, Lkk= 0\u2200k. To incorporate the losses into the modeling process,\nwe could modify the Gini index to/summationtext\nk/ne}ationslash=k\u2032Lkk\u2032\u02c6pmk\u02c6pmk\u2032; this would be the\nexpected loss incurred by the randomized rule. This works for the multi-\nclass case, but in the two-class case has no e\ufb00ect, since the coe\ufb03cient of\n\u02c6pmk\u02c6pmk\u2032isLkk\u2032+Lk\u2032k. For two classes a better approach is to weight the\nobservations in class kbyLkk\u2032. This can be used in the multiclass case only\nif, as a function of k,Lkk\u2032doesn\u2019t depend on k\u2032. Observation weighting can\nbe used with the deviance as well. The e\ufb00ect of observation weighting is to\nalter the prior probability on the classes. In a terminal node, the empirical\nBayes rule implies that we classify to class k(m) = arg min k/summationtext\n\u2113L\u2113k\u02c6pm\u2113.\nMissing Predictor Values\nSuppose our data has some missing predictor values in some or all of the\nvariables. We might discard any observation with some missing values, but\nthis could lead to serious depletion of the training set. Alternatively we\nmight try to \ufb01ll in (impute) the missing values, with say the mean of that\npredictor over the nonmissing observations. For tree-based models, there\nare two better approaches. The \ufb01rst is applicable to categorical predictors:\nwe simply make a new category for \u201cmissing.\u201d From this we might dis-\ncover that observations with missing values for some measurement behave\ndi\ufb00erently than those with nonmissing values. The second more general\napproach is the construction of surrogate variables. When considering a\npredictor for a split, we use only the observations for which that predictor\nis not missing. Having chosen the best (primary) predictor and split point,\nwe form a list of surrogate predictors and split points. The \ufb01rst surroga te\nis the predictor and corresponding split point that best mimics the split of\nthe training data achieved by the primary split. The second surrogate is\nthe predictor and corresponding split point that does second best, and so\non. When sending observations down the tree either in the training phase\nor during prediction, we use the surrogate splits in order, if the primary\nsplitting predictor is missing. Surrogate splits exploit correlations between\npredictors to try and alleviate the e\ufb00ect of missing data. The higher the cor-\nrelation between the missing predictor and the other predictors, the smaller\nthe loss of information due to the missing value. The general problem of\nmissing data is discussed in Section 9.6.\nWhy Binary Splits?\nRather than splitting each node into just two groups at each stage (as\nabove), we might consider multiway splits into more than two groups. Whil e\nthis can sometimes be useful, it is not a good general strategy. The problem\nis that multiway splits fragment the data too quickly, leaving insu\ufb03cient\ndata at the next level down. Hence we would want to use such splits only\nwhen needed. Since multiway splits can be achieved by a series of binary\nsplits, the latter are preferred.", "330": "312 9. Additive Models, Trees, and Related Methods\nOther Tree-Building Procedures\nThe discussion above focuses on the CART (classi\ufb01cation and regression\ntree) implementation of trees. The other popular methodology is ID3 and\nits later versions, C4.5 and C5.0 (Quinlan, 1993). Early versions of the\nprogram were limited to categorical predictors, and used a top-down rule\nwith no pruning. With more recent developments, C5.0 has become quite\nsimilar to CART. The most signi\ufb01cant feature unique to C5.0 is a scheme\nfor deriving rule sets. After a tree is grown, the splitting rules that de\ufb01ne the\nterminal nodes can sometimes be simpli\ufb01ed: that is, one or more condition\ncan be dropped without changing the subset of observations that fall in\nthe node. We end up with a simpli\ufb01ed set of rules de\ufb01ning each terminal\nnode; these no longer follow a tree structure, but their simplicity might\nmake them more attractive to the user.\nLinear Combination Splits\nRather than restricting splits to be of the form Xj\u2264s, one can allow splits\nalong linear combinations of the form/summationtextajXj\u2264s. The weights ajand\nsplit point sare optimized to minimize the relevant criterion (such as the\nGini index). While this can improve the predictive power of the tree, it can\nhurt interpretability. Computationally, the discreteness of the split point\nsearch precludes the use of a smooth optimization for the weights. A better\nway to incorporate linear combination splits is in the hierarchical mixtures\nof experts (HME) model, the topic of Section 9.5.\nInstability of Trees\nOne major problem with trees is their high variance. Often a small change\nin the data can result in a very di\ufb00erent series of splits, making interpre-\ntation somewhat precarious. The major reason for this instability is the\nhierarchical nature of the process: the e\ufb00ect of an error in the top split\nis propagated down to all of the splits below it. One can alleviate this to\nsome degree by trying to use a more stable split criterion, but the inherent\ninstability is not removed. It is the price to be paid for estimating a simple,\ntree-based structure from the data. Bagging (Section 8.7) averages many\ntrees to reduce this variance.\nLack of Smoothness\nAnother limitation of trees is the lack of smoothness of the prediction sur-\nface, as can be seen in the bottom right panel of Figure 9.2. In classi\ufb01cation\nwith 0/1 loss, this doesn\u2019t hurt much, since bias in estimation of the class\nprobabilities has a limited e\ufb00ect. However, this can degrade performance\nin the regression setting, where we would normally expect the underlying\nfunction to be smooth. The MARS procedure, described in Section 9.4,", "331": "9.2 Tree-Based Methods 313\nTABLE 9.3. Spam data: confusion rates for the 17-node tree (chosen by cross\u2013\nvalidation) on the test data. Overall error rate is 9.3%.\nPredicted\nTrueemail spam\nemail 57.3% 4.0%\nspam 5.3% 33.4%\ncan be viewed as a modi\ufb01cation of CART designed to alleviate this lack of\nsmoothness.\nDi\ufb03culty in Capturing Additive Structure\nAnother problem with trees is their di\ufb03culty in modeling additive struc-\nture. In regression, suppose, for example, that Y=c1I(X1< t1)+c2I(X2<\nt2) +\u03b5where \u03b5is zero-mean noise. Then a binary tree might make its \ufb01rst\nsplit on X1neart1. At the next level down it would have to split both nodes\nonX2att2in order to capture the additive structure. This might happen\nwith su\ufb03cient data, but the model is given no special encouragement to \ufb01nd\nsuch structure. If there were ten rather than two additive e\ufb00ects, it would\ntake many fortuitous splits to recreate the structure, and the data analyst\nwould be hard pressed to recognize it in the estimated tree. The \u201cblame\u201d\nhere can again be attributed to the binary tree structure, which has both\nadvantages and drawbacks. Again the MARS method (Section 9.4) gives\nup this tree structure in order to capture additive structure.\n9.2.5 Spam Example (Continued)\nWe applied the classi\ufb01cation tree methodology to the spamexample intro-\nduced earlier. We used the deviance measure to grow the tree and mis-\nclassi\ufb01cation rate to prune it. Figure 9.4 shows the 10-fold cross-validat ion\nerror rate as a function of the size of the pruned tree, along with \u00b12 stan-\ndard errors of the mean, from the ten replications. The test error curve is\nshown in orange. Note that the cross-validation error rates are indexed by\na sequence of values of \u03b1andnottree size; for trees grown in di\ufb00erent folds,\na value of \u03b1might imply di\ufb00erent sizes. The sizes shown at the base of the\nplot refer to |T\u03b1|, the sizes of the pruned original tree.\nThe error \ufb02attens out at around 17 terminal nodes, giving the pruned tree\nin Figure 9.5. Of the 13 distinct features chosen by the tree, 11 overlap with\nthe 16 signi\ufb01cant features in the additive model (Table 9.2). The overall\nerror rate shown in Table 9.3 is about 50% higher than for the additive\nmodel in Table 9.1.\nConsider the rightmost branches of the tree. We branch to the right\nwith aspamwarning if more than 5.5% of the characters are the $ sign.", "332": "314 9. Additive Models, Trees, and Related Methods\n0 10 20 30 400.0 0.1 0.2 0.3 0.4\nTree SizeMisclassification Rate176 21 7 5 3 2 0\u03b1\nFIGURE 9.4. Results for spamexample. The blue curve is the 10-fold cross-val-\nidation estimate of misclassi\ufb01cation rate as a function of tre e size, with standard\nerror bars. The minimum occurs at a tree size with about 17terminal nodes (using\nthe \u201cone-standard-error\u201d rule). The orange curve is the test er ror, which tracks\nthe CV error quite closely. The cross-validation is indexed by values of \u03b1, shown\nabove. The tree sizes shown below refer to |T\u03b1|, the size of the original tree indexed\nby\u03b1.\nHowever, if in addition the phrase hpoccurs frequently, then this is likely\nto be company business and we classify as email. All of the 22 cases in\nthe test set satisfying these criteria were correctly classi\ufb01ed. If the second\ncondition is not met, and in addition the average length of repeated capital\nlettersCAPAVE is larger than 2.9, then we classify as spam. Of the 227 test\ncases, only seven were misclassi\ufb01ed.\nIn medical classi\ufb01cation problems, the terms sensitivity andspeci\ufb01city\nare used to characterize a rule. They are de\ufb01ned as follows:\nSensitivity: probability of predicting disease given true state is disease.\nSpeci\ufb01city: probability of predicting non-disease given true state is non-\ndisease.", "333": "9.2 Tree-Based Methods 315\n600/1536\n280/1177\n180/1065\n 80/861\n 80/652\n 77/423\n 20/238\n 19/236   1/2 57/185\n 48/113\n 37/101   1/12  9/72  3/229  0/209100/204\n 36/123\n 16/94\n 14/89   3/5  9/29 16/81  9/112\n  6/109   0/3 48/359\n 26/337\n 19/110\n 18/109   0/1  7/227  0/22\nspam\nspamspamspamspam\nspamspam\nspam\nspam\nspam\nspamspamemail\nemailemail\nemailemailemailemail\nemail\nemail\nemail\nemailemailemail\nemailemailemailemailemailemailemailemail\nch$<0.0555\nremove<0.06\nch!<0.191\ngeorge<0.005\nhp<0.03\nCAPMAX<10.5\nreceive<0.125 edu<0.045\nour<1.2CAPAVE<2.7505\nfree<0.065\nbusiness<0.145george<0.15hp<0.405\nCAPAVE<2.907\n1999<0.58ch$>0.0555\nremove>0.06\nch!>0.191\ngeorge>0.005\nhp>0.03\nCAPMAX>10.5\nreceive>0.125 edu>0.045\nour>1.2CAPAVE>2.7505\nfree>0.065\nbusiness>0.145george>0.15hp>0.405\nCAPAVE>2.907\n1999>0.58\nFIGURE 9.5. The pruned tree for the spamexample. The split variables are\nshown in blue on the branches, and the classi\ufb01cation is shown in e very node.The\nnumbers under the terminal nodes indicate misclassi\ufb01cation rates on the test data.", "334": "316 9. Additive Models, Trees, and Related Methods\nSpecificitySensitivity\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\u2022 \u2022\n\u2022\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022Tree (0.95)\nGAM (0.98)\nWeighted Tree (0.90)\nFIGURE 9.6. ROC curves for the classi\ufb01cation rules \ufb01t to the spamdata. Curves\nthat are closer to the northeast corner represent better classi \ufb01ers. In this case the\nGAM classi\ufb01er dominates the trees. The weighted tree achieves better sensitivity\nfor higher speci\ufb01city than the unweighted tree. The numbers in t he legend repre-\nsent the area under the curve.\nIf we think of spamandemail as the presence and absence of disease, re-\nspectively, then from Table 9.3 we have\nSensitivity = 100 \u00d733.4\n33.4 + 5.3= 86.3%,\nSpeci\ufb01city = 100 \u00d757.3\n57.3 + 4.0= 93.4%.\nIn this analysis we have used equal losses. As before let Lkk\u2032be the\nloss associated with predicting a class kobject as class k\u2032. By varying the\nrelative sizes of the losses L01andL10, we increase the sensitivity and\ndecrease the speci\ufb01city of the rule, or vice versa. In this example, we want\nto avoid marking good email asspam, and thus we want the speci\ufb01city to\nbe very high. We can achieve this by setting L01>1 say, with L10= 1.\nThe Bayes\u2019 rule in each terminal node classi\ufb01es to class 1 ( spam) if the\nproportion of spamis\u2265L01/(L10+L01), and class zero otherwise. The", "335": "9.3 PRIM: Bump Hunting 317\nreceiver operating characteristic curve (ROC) is a commonly used summary\nfor assessing the tradeo\ufb00 between sensitivity and speci\ufb01city. It is a plot of\nthe sensitivity versus speci\ufb01city as we vary the parameters of a classi\ufb01cation\nrule. Varying the loss L01between 0.1 and 10, and applying Bayes\u2019 rule to\nthe 17-node tree selected in Figure 9.4, produced the ROC curve shown\nin Figure 9.6. The standard error of each curve near 0.9 is approximately/radicalbig\n0.9(1\u22120.9)/1536 = 0 .008, and hence the standard error of the di\ufb00erence\nis about 0 .01. We see that in order to achieve a speci\ufb01city of close to 100%,\nthe sensitivity has to drop to about 50%. The area under the curve is a\ncommonly used quantitative summary; extending the curve linearly in each\ndirection so that it is de\ufb01ned over [0 ,100], the area is approximately 0 .95.\nFor comparison, we have included the ROC curve for the GAM model \ufb01t\nto these data in Section 9.2; it gives a better classi\ufb01cation rule for any los s,\nwith an area of 0 .98.\nRather than just modifying the Bayes rule in the nodes, it is better to\ntake full account of the unequal losses in growing the tree, as was done\nin Section 9.2. With just two classes 0 and 1, losses may be incorporated\ninto the tree-growing process by using weight Lk,1\u2212kfor an observation in\nclassk. Here we chose L01= 5,L10= 1 and \ufb01t the same size tree as before\n(|T\u03b1|= 17). This tree has higher sensitivity at high values of the speci\ufb01city\nthan the original tree, but does more poorly at the other extreme. Its top\nfew splits are the same as the original tree, and then it departs from it.\nFor this application the tree grown using L01= 5 is clearly better than the\noriginal tree.\nThe area under the ROC curve, used above, is sometimes called the c-\nstatistic . Interestingly, it can be shown that the area under the ROC curve\nis equivalent to the Mann-Whitney U statistic (or Wilcoxon rank-sum test),\nfor the median di\ufb00erence between the prediction scores in the two groups\n(Hanley and McNeil, 1982). For evaluating the contribution of an additional\npredictor when added to a standard model, the c-statistic may not be an\ninformative measure. The new predictor can be very signi\ufb01cant in terms\nof the change in model deviance, but show only a small increase in the c-\nstatistic. For example, removal of the highly signi\ufb01cant term george from\nthe model of Table 9.2 results in a decrease in the c-statistic of less than\n0.01. Instead, it is useful to examine how the additional predictor changes\nthe classi\ufb01cation on an individual sample basis. A good discussion of this\npoint appears in Cook (2007).\n9.3 PRIM: Bump Hunting\nTree-based methods (for regression) partition the feature space into box-\nshaped regions, to try to make the response averages in each box as di\ufb00er-", "336": "318 9. Additive Models, Trees, and Related Methods\nent as possible. The splitting rules de\ufb01ning the boxes are related to each\nthrough a binary tree, facilitating their interpretation.\nThe patient rule induction method (PRIM) also \ufb01nds boxes in the feature\nspace, but seeks boxes in which the response average is high. Hence it looks\nfor maxima in the target function, an exercise known as bump hunting . (If\nminima rather than maxima are desired, one simply works with the negative\nresponse values.)\nPRIM also di\ufb00ers from tree-based partitioning methods in that the box\nde\ufb01nitions are not described by a binary tree. This makes interpretation of\nthe collection of rules more di\ufb03cult; however, by removing the binary tree\nconstraint, the individual rules are often simpler.\nThe main box construction method in PRIM works from the top down,\nstarting with a box containing all of the data. The box is compressed along\none face by a small amount, and the observations then falling outside the\nbox are peeled o\ufb00. The face chosen for compression is the one resulting in\nthe largest box mean, after the compression is performed. Then the process\nis repeated, stopping when the current box contains some minimum number\nof data points.\nThis process is illustrated in Figure 9.7. There are 200 data points uni-\nformly distributed over the unit square. The color-coded plot indicates the\nresponse Ytaking the value 1 (red) when 0 .5< X1<0.8 and 0 .4< X2<\n0.6. and zero (blue) otherwise. The panels shows the successive boxes found\nby the top-down peeling procedure, peeling o\ufb00 a proportion \u03b1= 0.1 of the\nremaining data points at each stage.\nFigure 9.8 shows the mean of the response values in the box, as the box\nis compressed.\nAfter the top-down sequence is computed, PRIM reverses the process,\nexpanding along any edge, if such an expansion increases the box mean.\nThis is called pasting . Since the top-down procedure is greedy at each step,\nsuch an expansion is often possible.\nThe result of these steps is a sequence of boxes, with di\ufb00erent numbers\nof observation in each box. Cross-validation, combined with the judgment\nof the data analyst, is used to choose the optimal box size.\nDenote by B1the indices of the observations in the box found in step 1.\nThe PRIM procedure then removes the observations in B1from the training\nset, and the two-step process\u2014top down peeling, followed by bottom-up\npasting\u2014is repeated on the remaining dataset. This entire process is re-\npeated several times, producing a sequence of boxes B1,B2,... ,B k. Each\nbox is de\ufb01ned by a set of rules involving a subset of predictors like\n(a1\u2264X1\u2264b1) and ( b1\u2264X3\u2264b2).\nA summary of the PRIM procedure is given Algorithm 9.3.\nPRIM can handle a categorical predictor by considering all partitions of\nthe predictor, as in CART. Missing values are also handled in a manner\nsimilar to CART. PRIM is designed for regression (quantitative respo nse", "337": "9.3 PRIM: Bump Hunting 319\n1\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no2\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no3\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no4\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no\n5\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no6\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no7\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no8\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no\n12\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no17\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no22\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no27\nooo\noo\noooo\nooo\noo\noo\noo\noo\no\noooo\noo\noooo\no oo\noo\noo\no\noo\no\no\noooo\nooo\nooo\noo\noo\nooo\noo\nooo\no\nooooo\no\nooo o\noo\noo\nooo\nooo\nooo\no\noo\noo\nooo\no\noo\no\nooo\noooo\noo\nooo\no\noo\noo oo\noo\no\nooo\nooo\nooo\noo\noo\no\nooo\noooo\nooo\nooo\noo\noo\nooo\nooo\noo\noo\noo oo\no\nooo\noo\no\noooo\noo\no\nFIGURE 9.7. Illustration of PRIM algorithm. There are two classes, indica ted\nby the blue (class 0) and red (class 1) points. The procedure starts with a rectangle\n(broken black lines) surrounding all of the data, and then peels a way points along\none edge by a prespeci\ufb01ed amount in order to maximize the mean of th e points\nremaining in the box. Starting at the top left panel, the sequence of p eelings is\nshown, until a pure red region is isolated in the bottom right pa nel. The iteration\nnumber is indicated at the top of each panel.\nNumber of Observations in BoxBox Mean\n50 100 1500.2 0.4 0.6 0.8 1.0\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\nFIGURE 9.8. Box mean as a function of number of observations in the box.", "338": "320 9. Additive Models, Trees, and Related Methods\nAlgorithm 9.3 Patient Rule Induction Method.\n1. Start with all of the training data, and a maximal box containing all\nof the data.\n2. Consider shrinking the box by compressing one face, so as to peel o\ufb00\nthe proportion \u03b1of observations having either the highest values of\na predictor Xj, or the lowest. Choose the peeling that produces the\nhighest response mean in the remaining box. (Typically \u03b1= 0.05 or\n0.10.)\n3. Repeat step 2 until some minimal number of observations (say 10)\nremain in the box.\n4. Expand the box along any face, as long as the resulting box mean\nincreases.\n5. Steps 1\u20134 give a sequence of boxes, with di\ufb00erent numbers of obser-\nvations in each box. Use cross-validation to choose a member of the\nsequence. Call the box B1.\n6. Remove the data in box B1from the dataset and repeat steps 2\u20135 to\nobtain a second box, and continue to get as many boxes as desired.\nvariable); a two-class outcome can be handled simply by coding it as 0 and\n1. There is no simple way to deal with k >2 classes simultaneously: one\napproach is to run PRIM separately for each class versus a baseline class.\nAn advantage of PRIM over CART is its patience. Because of its bi-\nnary splits, CART fragments the data quite quickly. Assuming splits of\nequal size, with Nobservations it can only make log2(N)\u22121 splits before\nrunning out of data. If PRIM peels o\ufb00 a proportion \u03b1of training points\nat each stage, it can perform approximately \u2212log(N)/log(1\u2212\u03b1) peeling\nsteps before running out of data. For example, if N= 128 and \u03b1= 0.10,\nthen log2(N)\u22121 = 6 while \u2212log(N)/log(1\u2212\u03b1)\u224846. Taking into account\nthat there must be an integer number of observations at each stage, PRIM\nin fact can peel only 29 times. In any case, the ability of PRIM to be more\npatient should help the top-down greedy algorithm \ufb01nd a better solution.\n9.3.1 Spam Example (Continued)\nWe applied PRIM to the spamdata, with the response coded as 1 for spam\nand 0 for email.\nThe \ufb01rst two boxes found by PRIM are summarized below:", "339": "9.4 MARS: Multivariate Adaptive Regression Splines 321\nRule 1 Global Mean Box Mean Box Support\nTraining 0.3931 0.9607 0.1413\nTest 0.3958 1.0000 0.1536\nRule 1\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3ch!>0.029\nCAPAVE >2.331\nyour >0.705\n1999 <0.040\nCAPTOT >79.50\nedu <0.070\nre<0.535\nch;<0.030\nRule 2 Remain Mean Box Mean Box Support\nTraining 0.2998 0.9560 0.1043\nTest 0.2862 0.9264 0.1061\nRule 2/braceleftbigg\nremove >0.010\ngeorge <0.110\nThe box support is the proportion of observations falling in the box.\nThe \ufb01rst box is purely spam, and contains about 15% of the test data.\nThe second box contains 10.6% of the test observations, 92.6% of which\narespam. Together the two boxes contain 26% of the data and are about\n97%spam. The next few boxes (not shown) are quite small, containing only\nabout 3% of the data.\nThe predictors are listed in order of importance. Interestingly the top\nsplitting variables in the CART tree (Figure 9.5) do not appear in PRIM\u2019s\n\ufb01rst box.\n9.4 MARS: Multivariate Adaptive Regression\nSplines\nMARS is an adaptive procedure for regression, and is well suited for high-\ndimensional problems (i.e., a large number of inputs). It can be viewed as a\ngeneralization of stepwise linear regression or a modi\ufb01cation of the CART\nmethod to improve the latter\u2019s performance in the regression setting. We\nintroduce MARS from the \ufb01rst point of view, and later make the connection\nto CART.\nMARS uses expansions in piecewise linear basis functions of the form\n(x\u2212t)+and (t\u2212x)+. The \u201c+\u201d means positive part, so\n(x\u2212t)+=/braceleftbigg\nx\u2212t,ifx > t,\n0,otherwise,and ( t\u2212x)+=/braceleftbigg\nt\u2212x,ifx < t,\n0,otherwise .", "340": "322 9. Additive Models, Trees, and Related Methods\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5(x\u2212t)+ (t\u2212x)+\nxtBasis Function\nFIGURE 9.9. The basis functions (x\u2212t)+(solid orange) and (t\u2212x)+(broken\nblue) used by MARS.\nAs an example, the functions ( x\u22120.5)+and (0 .5\u2212x)+are shown in Fig-\nure 9.9.\nEach function is piecewise linear, with a knotat the value t. In the\nterminology of Chapter 5, these are linear splines. We call the two functions\nare\ufb02ected pair in the discussion below. The idea is to form re\ufb02ected pairs\nfor each input Xjwith knots at each observed value xijof that input.\nTherefore, the collection of basis functions is\nC={(Xj\u2212t)+,(t\u2212Xj)+}t\u2208 {x1j, x2j, . . . , x Nj}\nj= 1,2, . . . , p.(9.18)\nIf all of the input values are distinct, there are 2 Npbasis functions alto-\ngether. Note that although each basis function depends only on a single\nXj, for example, h(X) = (Xj\u2212t)+, it is considered as a function over the\nentire input space IRp.\nThe model-building strategy is like a forward stepwise linear regression,\nbut instead of using the original inputs, we are allowed to use functions\nfrom the set Cand their products. Thus the model has the form\nf(X) =\u03b20+M/summationdisplay\nm=1\u03b2mhm(X), (9.19)\nwhere each hm(X) is a function in C, or a product of two or more such\nfunctions.\nGiven a choice for the hm, the coe\ufb03cients \u03b2mare estimated by minimiz-\ning the residual sum-of-squares, that is, by standard linear regression. The\nreal art, however, is in the construction of the functions hm(x). We start\nwith only the constant function h0(X) = 1 in our model, and all functions\nin the set Care candidate functions. This is depicted in Figure 9.10.\nAt each stage we consider as a new basis function pair all products of a\nfunction hmin the model set Mwith one of the re\ufb02ected pairs in C. We\nadd to the model Mthe term of the form\n\u02c6\u03b2M+1h\u2113(X)\u2264(Xj\u2212t)++\u02c6\u03b2M+2h\u2113(X)\u2264(t\u2212Xj)+, h\u2113\u2208 M,", "341": "9.4 MARS: Multivariate Adaptive Regression Splines 323\nX1\nX1X1\nX1\nX2X2\nX2X2\nX2\nXpXpXpConstant\nFIGURE 9.10. Schematic of the MARS forward model-building procedure. On\nthe left are the basis functions currently in the model: initiall y, this is the constant\nfunction h(X) = 1. On the right are all candidate basis functions to be considered\nin building the model. These are pairs of piecewise linear basi s functions as in\nFigure 9.9, with knots tat all unique observed values xijof each predictor Xj.\nAt each stage we consider all products of a candidate pair with a basis function\nin the model. The product that decreases the residual error t he most is added into\nthe current model. Above we illustrate the \ufb01rst three steps of t he procedure, with\nthe selected functions shown in red.", "342": "324 9. Additive Models, Trees, and Related Methods\nX1X2h(X1, X2)\nFIGURE 9.11. The function h(X1, X2) = (X1\u2212x51)+\u2264(x72\u2212X2)+, resulting\nfrom multiplication of two piecewise linear MARS basis functio ns.\nthat produces the largest decrease in training error. Here \u02c6\u03b2M+1and\u02c6\u03b2M+2\nare coe\ufb03cients estimated by least squares, along with all the other M+ 1\ncoe\ufb03cients in the model. Then the winning products are added to the\nmodel and the process is continued until the model set Mcontains some\npreset maximum number of terms.\nFor example, at the \ufb01rst stage we consider adding to the model a function\nof the form \u03b21(Xj\u2212t)++\u03b22(t\u2212Xj)+;t\u2208 {xij}, since multiplication by\nthe constant function just produces the function itself. Suppose the best\nchoice is \u02c6\u03b21(X2\u2212x72)++\u02c6\u03b22(x72\u2212X2)+. Then this pair of basis functions\nis added to the set M, and at the next stage we consider including a pair\nof products the form\nhm(X)\u2264(Xj\u2212t)+and hm(X)\u2264(t\u2212Xj)+, t\u2208 {xij},\nwhere for hmwe have the choices\nh0(X) = 1 ,\nh1(X) = ( X2\u2212x72)+,or\nh2(X) = ( x72\u2212X2)+.\nThe third choice produces functions such as ( X1\u2212x51)+\u2264(x72\u2212X2)+,\ndepicted in Figure 9.11.\nAt the end of this process we have a large model of the form (9.19). This\nmodel typically over\ufb01ts the data, and so a backward deletion procedure\nis applied. The term whose removal causes the smallest increase in resid-\nual squared error is deleted from the model at each stage, producing an\nestimated best model \u02c6f\u03bbof each size (number of terms) \u03bb. One could use\ncross-validation to estimate the optimal value of \u03bb, but for computational", "343": "9.4 MARS: Multivariate Adaptive Regression Splines 325\nsavings the MARS procedure instead uses generalized cross-validation. This\ncriterion is de\ufb01ned as\nGCV( \u03bb) =/summationtextN\ni=1(yi\u2212\u02c6f\u03bb(xi))2\n(1\u2212M(\u03bb)/N)2. (9.20)\nThe value M(\u03bb) is the e\ufb00ective number of parameters in the model: this\naccounts both for the number of terms in the models, plus the number\nof parameters used in selecting the optimal positions of the knots. Some\nmathematical and simulation results suggest that one should pay a price\nof three parameters for selecting a knot in a piecewise linear regression.\nThus if there are rlinearly independent basis functions in the model, and\nKknots were selected in the forward process, the formula is M(\u03bb) =r+cK,\nwhere c= 3. (When the model is restricted to be additive\u2014details below\u2014\na penalty of c= 2 is used). Using this, we choose the model along the\nbackward sequence that minimizes GCV( \u03bb).\nWhy these piecewise linear basis functions, and why this particular model\nstrategy? A key property of the functions of Figure 9.9 is their ability to\noperate locally; they are zero over part of their range. When they are mul-\ntiplied together, as in Figure 9.11, the result is nonzero only over the small\npart of the feature space where both component functions are nonzero. As\na result, the regression surface is built up parsimoniously, using nonzero\ncomponents locally\u2014only where they are needed. This is important, since\none should \u201cspend\u201d parameters carefully in high dimensions, as they can\nrun out quickly. The use of other basis functions such as polynomials, would\nproduce a nonzero product everywhere, and would not work as well.\nThe second important advantage of the piecewise linear basis function\nconcerns computation. Consider the product of a function in Mwith each\nof the Nre\ufb02ected pairs for an input Xj. This appears to require the \ufb01tting\nofNsingle-input linear regression models, each of which uses O(N) oper-\nations, making a total of O(N2) operations. However, we can exploit the\nsimple form of the piecewise linear function. We \ufb01rst \ufb01t the re\ufb02ected pair\nwith rightmost knot. As the knot is moved successively one position at a\ntime to the left, the basis functions di\ufb00er by zero over the left part of the\ndomain, and by a constant over the right part. Hence after each such move\nwe can update the \ufb01t in O(1) operations. This allows us to try every knot\nin only O(N) operations.\nThe forward modeling strategy in MARS is hierarchical, in the sense that\nmultiway products are built up from products involving terms already in\nthe model. For example, a four-way product can only be added to the model\nif one of its three-way components is already in the model. The philosophy\nhere is that a high-order interaction will likely only exist if some of its lo wer-\norder \u201cfootprints\u201d exist as well. This need not be true, but is a reasonable\nworking assumption and avoids the search over an exponentially growing\nspace of alternatives.", "344": "326 9. Additive Models, Trees, and Related Methods\nRank of ModelTest Misclassification Error\n0 20 40 60 80 1000.1 0.2 0.3 0.4\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n0.055GCV choice\nFIGURE 9.12. Spam data: test error misclassi\ufb01cation rate for the MARS pro-\ncedure, as a function of the rank (number of independent basis functi ons) in the\nmodel.\nThere is one restriction put on the formation of model terms: each input\ncan appear at most once in a product. This prevents the formation of\nhigher-order powers of an input, which increase or decrease too sharply\nnear the boundaries of the feature space. Such powers can be approximated\nin a more stable way with piecewise linear functions.\nA useful option in the MARS procedure is to set an upper limit on\nthe order of interaction. For example, one can set a limit of two, allowing\npairwise products of piecewise linear functions, but not three- or higher-\nway products. This can aid in the interpretation of the \ufb01nal model. An\nupper limit of one results in an additive model.\n9.4.1 Spam Example (Continued)\nWe applied MARS to the \u201cspam\u201d data analyzed earlier in this chapter. To\nenhance interpretability, we restricted MARS to second-degree interactions.\nAlthough the target is a two-class variable, we used the squared-error loss\nfunction nonetheless (see Section 9.4.3). Figure 9.12 shows the test error\nmisclassi\ufb01cation rate as a function of the rank (number of independent ba-\nsis functions) in the model. The error rate levels o\ufb00 at about 5 .5%, which is\nslightly higher than that of the generalized additive model (5 .3%) discussed\nearlier. GCV chose a model size of 60, which is roughly the smallest model\ngiving optimal performance. The leading interactions found by MARS in-\nvolved inputs ( ch$, remove ), (ch$, free ) and (hp, CAPTOT ). However, these\ninteractions give no improvement in performance over the generalized ad-\nditive model.", "345": "9.4 MARS: Multivariate Adaptive Regression Splines 327\n9.4.2 Example (Simulated Data)\nHere we examine the performance of MARS in three contrasting scenarios.\nThere are N= 100 observations, and the predictors X1,X2,... ,X pand\nerrors \u03b5have independent standard normal distributions.\nScenario 1: The data generation model is\nY= (X1\u22121)++ (X1\u22121)+\u2264(X2\u2212.8)++ 0.12\u2264\u03b5. (9.21)\nThe noise standard deviation 0.12 was chosen so that the signal-to-\nnoise ratio was about 5. We call this the tensor-product scenario; the\nproduct term gives a surface that looks like that of Figure 9.11.\nScenario 2: This is the same as scenario 1, but with p= 20 total predictors;\nthat is, there are 18 inputs that are independent of the response.\nScenario 3: This has the structure of a neural network:\n\u21131=X1+X2+X3+X4+X5,\n\u21132=X6\u2212X7+X8\u2212X9+X10,\n\u03c3(t) = 1 /(1 +e\u2212t),\nY=\u03c3(\u21131) +\u03c3(\u21132) + 0.12\u2264\u03b5.(9.22)\nScenarios 1 and 2 are ideally suited for MARS, while scenario 3 contains\nhigh-order interactions and may be di\ufb03cult for MARS to approximate. We\nran \ufb01ve simulations from each model, and recorded the results.\nIn scenario 1, MARS typically uncovered the correct model almost per-\nfectly. In scenario 2, it found the correct structure but also found a few\nextraneous terms involving other predictors.\nLet\u03b8(x) be the true mean of Y, and let\nMSE 0= ave x\u2208Test(\u00afy\u2212\u03b8(x))2,\nMSE = ave x\u2208Test(\u02c6f(x)\u2212\u03b8(x))2.(9.23)\nThese represent the mean-square error of the constant model and the \ufb01tted\nMARS model, estimated by averaging at the 1000 test values of x. Table 9.4\nshows the proportional decrease in model error or R2for each scenario:\nR2=MSE 0\u2212MSE\nMSE 0. (9.24)\nThe values shown are means and standard error over the \ufb01ve simulations.\nThe performance of MARS is degraded only slightly by the inclusion of the\nuseless inputs in scenario 2; it performs substantially worse in scenario 3.", "346": "328 9. Additive Models, Trees, and Related Methods\nTABLE 9.4. Proportional decrease in model error ( R2) when MARS is applied\nto three di\ufb00erent scenarios.\nScenario Mean (S.E.)\n1: Tensor product p= 2 0.97 (0.01)\n2: Tensor product p= 20 0.96 (0.01)\n3: Neural network 0.79 (0.01)\n9.4.3 Other Issues\nMARS for Classi\ufb01cation\nThe MARS method and algorithm can be extended to handle classi\ufb01cation\nproblems. Several strategies have been suggested.\nFor two classes, one can code the output as 0/1 and treat the problem as\na regression; we did this for the spamexample. For more than two classes,\none can use the indicator response approach described in Section 4.2. One\ncodes the Kresponse classes via 0/1 indicator variables, and then per-\nforms a multi-response MARS regression. For the latter we use a common\nset of basis functions for all response variables. Classi\ufb01cation is made to\nthe class with the largest predicted response value. There are, however, po-\ntential masking problems with this approach, as described in Section 4.2.\nA generally superior approach is the \u201coptimal scoring\u201d method discussed\nin Section 12.5.\nStone et al. (1997) developed a hybrid of MARS called PolyMARS specif-\nically designed to handle classi\ufb01cation problems. It uses the multiple logistic\nframework described in Section 4.4. It grows the model in a forward stage-\nwise fashion like MARS, but at each stage uses a quadratic approximation\nto the multinomial log-likelihood to search for the next basis-function pair.\nOnce found, the enlarged model is \ufb01t by maximum likelihood, and the\nprocess is repeated.\nRelationship of MARS to CART\nAlthough they might seem quite di\ufb00erent, the MARS and CART strategies\nactually have strong similarities. Suppose we take the MARS procedure and\nmake the following changes:\n\u2022Replace the piecewise linear basis functions by step functions I(x\u2212t >\n0) and I(x\u2212t\u22640).\n\u2022When a model term is involved in a multiplication by a candidate\nterm, it gets replaced by the interaction, and hence is not available\nfor further interactions.\nWith these changes, the MARS forward procedure is the same as the CART\ntree-growing algorithm. Multiplying a step function by a pair of re\ufb02ected", "347": "9.5 Hierarchical Mixtures of Experts 329\nstep functions is equivalent to splitting a node at the step. The second\nrestriction implies that a node may not be split more than once, and leads\nto the attractive binary-tree representation of the CART model. On the\nother hand, it is this restriction that makes it di\ufb03cult for CART to model\nadditive structures. MARS forgoes the tree structure and gains the ability\nto capture additive e\ufb00ects.\nMixed Inputs\nMars can handle \u201cmixed\u201d predictors\u2014quantitative and qualitative\u2014in a\nnatural way, much like CART does. MARS considers all possible binary\npartitions of the categories for a qualitative predictor into two groups.\nEach such partition generates a pair of piecewise constant basis functions\u2014\nindicator functions for the two sets of categories. This basis pair is now\ntreated as any other, and is used in forming tensor products with other\nbasis functions already in the model.\n9.5 Hierarchical Mixtures of Experts\nThe hierarchical mixtures of experts (HME) procedure can be viewed as a\nvariant of tree-based methods. The main di\ufb00erence is that the tree splits\nare not hard decisions but rather soft probabilistic ones. At each node an\nobservation goes left or right with probabilities depending on its input val-\nues. This has some computational advantages since the resulting parameter\noptimization problem is smooth, unlike the discrete split point search in the\ntree-based approach. The soft splits might also help in prediction accuracy\nand provide a useful alternative description of the data.\nThere are other di\ufb00erences between HMEs and the CART implementa-\ntion of trees. In an HME, a linear (or logistic regression) model is \ufb01t in\neach terminal node, instead of a constant as in CART. The splits can be\nmultiway, not just binary, and the splits are probabilistic functions of a\nlinear combination of inputs, rather than a single input as in the standard\nuse of CART. However, the relative merits of these choices are not clear,\nand most were discussed at the end of Section 9.2.\nA simple two-level HME model in shown in Figure 9.13. It can be thought\nof as a tree with soft splits at each non-terminal node. However, the inven-\ntors of this methodology use a di\ufb00erent terminology. The terminal nodes\nare called experts , and the non-terminal nodes are called gating networks .\nThe idea is that each expert provides an opinion (prediction) about the\nresponse, and these are combined together by the gating networks. As we\nwill see, the model is formally a mixture model, and the two-level model\nin the \ufb01gure can be extend to multiple levels, hence the name hierarchical\nmixtures of experts .", "348": "330 9. Additive Models, Trees, and Related Methods\ng1 g2\ng1|1 g2|1g1|2 g2|2Gating GatingGatingGating\nGating GatingGating GatingGating\nNetwork Network NetworkNetwork\nNetwork\nNetworkNetwork\nNetwork Network NetworkNetwork\nNetworkNetwork NetworkNetwork Network\nNetwork Network NetworkNetwork\nExpert Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert\nPr(y|x, \u03b811) Pr( y|x, \u03b821) Pr( y|x, \u03b812) Pr( y|x, \u03b822)\nFIGURE 9.13. A two-level hierarchical mixture of experts (HME) model.\nConsider the regression or classi\ufb01cation problem, as described earlier in\nthe chapter. The data is ( xi,yi),i= 1,2,... ,N , with yieither a continuous\nor binary-valued response, and xia vector-valued input. For ease of nota-\ntion we assume that the \ufb01rst element of xiis one, to account for intercepts.\nHere is how an HME is de\ufb01ned. The top gating network has the output\ngj(x,\u03b3j) =e\u03b3T\njx\n/summationtextK\nk=1e\u03b3T\nkx, j= 1,2,... ,K, (9.25)\nwhere each \u03b3jis a vector of unknown parameters. This represents a soft\nK-way split ( K= 2 in Figure 9.13.) Each gj(x,\u03b3j) is the probability of\nassigning an observation with feature vector xto the jth branch. Notice\nthat with K= 2 groups, if we take the coe\ufb03cient of one of the elements of\nxto be + \u221e, then we get a logistic curve with in\ufb01nite slope. In this case,\nthe gating probabilities are either 0 or 1, corresponding to a hard split on\nthat input.\nAt the second level, the gating networks have a similar form:\ng\u2113|j(x,\u03b3j\u2113) =e\u03b3T\nj\u2113x\n/summationtextK\nk=1e\u03b3T\njkx, \u2113= 1,2,... ,K. (9.26)", "349": "9.5 Hierarchical Mixtures of Experts 331\nThis is the probability of assignment to the \u2113th branch, given assignment\nto the jth branch at the level above.\nAt each expert (terminal node), we have a model for the response variable\nof the form\nY\u223cPr(y|x,\u03b8j\u2113). (9.27)\nThis di\ufb00ers according to the problem.\nRegression: The Gaussian linear regression model is used, with \u03b8j\u2113=\n(\u03b2j\u2113,\u03c32\nj\u2113):\nY=\u03b2T\nj\u2113x+\u03b5and\u03b5\u223cN(0,\u03c32\nj\u2113). (9.28)\nClassi\ufb01cation: The linear logistic regression model is used:\nPr(Y= 1|x,\u03b8j\u2113) =1\n1 +e\u2212\u03b8T\nj\u2113x. (9.29)\nDenoting the collection of all parameters by \u03a8 = {\u03b3j,\u03b3j\u2113,\u03b8j\u2113}, the total\nprobability that Y=yis\nPr(y|x,\u03a8) =K/summationdisplay\nj=1gj(x,\u03b3j)K/summationdisplay\n\u2113=1g\u2113|j(x,\u03b3j\u2113)Pr(y|x,\u03b8j\u2113). (9.30)\nThis is a mixture model, with the mixture probabilities determined by the\ngating network models.\nTo estimate the parameters, we maximize the log-likelihood of the data,/summationtext\nilog Pr( yi|xi,\u03a8), over the parameters in \u03a8. The most convenient method\nfor doing this is the EM algorithm, which we describe for mixtures in\nSection 8.5. We de\ufb01ne latent variables \u2206 j, all of which are zero except for\na single one. We interpret these as the branching decisions made by the top\nlevel gating network. Similarly we de\ufb01ne latent variables \u2206 \u2113|jto describe\nthe gating decisions at the second level.\nIn the E-step, the EM algorithm computes the expectations of the \u2206 j\nand \u2206 \u2113|jgiven the current values of the parameters. These expectations\nare then used as observation weights in the M-step of the procedure, to\nestimate the parameters in the expert networks. The parameters in the\ninternal nodes are estimated by a version of multiple logistic regression.\nThe expectations of the \u2206 jor \u2206 \u2113|jare probability pro\ufb01les, and these are\nused as the response vectors for these logistic regressions.\nThe hierarchical mixtures of experts approach is a promising competitor\nto CART trees. By using soft splits rather than hard decision rules it can\ncapture situations where the transition from low to high response is gradual .\nThe log-likelihood is a smooth function of the unknown weights and hence\nis amenable to numerical optimization. The model is similar to CART with\nlinear combination splits, but the latter is more di\ufb03cult to optimize. On", "350": "332 9. Additive Models, Trees, and Related Methods\nthe other hand, to our knowledge there are no methods for \ufb01nding a good\ntree topology for the HME model, as there are in CART. Typically one uses\na \ufb01xed tree of some depth, possibly the output of the CART procedure.\nThe emphasis in the research on HMEs has been on prediction rather than\ninterpretation of the \ufb01nal model. A close cousin of the HME is the latent\nclass model (Lin et al., 2000), which typically has only one layer; here\nthe nodes or latent classes are interpreted as groups of subjects that show\nsimilar response behavior.\n9.6 Missing Data\nIt is quite common to have observations with missing values for one or mor e\ninput features. The usual approach is to impute (\ufb01ll-in) the missing values\nin some way.\nHowever, the \ufb01rst issue in dealing with the problem is determining wheth-\ner the missing data mechanism has distorted the observed data. Roughly\nspeaking, data are missing at random if the mechanism resulting in its\nomission is independent of its (unobserved) value. A more precise de\ufb01nition\nis given in Little and Rubin (2002). Suppose yis the response vector and X\nis the N\u00d7pmatrix of inputs (some of which are missing). Denote by Xobs\nthe observed entries in Xand let Z= (y,X),Zobs= (y,Xobs). Finally, if R\nis an indicator matrix with ijth entry 1 if xijis missing and zero otherwise,\nthen the data is said to be missing at random (MAR) if the distribution of\nRdepends on the data Zonly through Zobs:\nPr(R|Z,\u03b8) = Pr( R|Zobs,\u03b8). (9.31)\nHere\u03b8are any parameters in the distribution of R. Data are said to be\nmissing completely at random (MCAR) if the distribution of Rdoesn\u2019t\ndepend on the observed or missing data:\nPr(R|Z,\u03b8) = Pr( R|\u03b8). (9.32)\nMCAR is a stronger assumption than MAR: most imputation methods rely\non MCAR for their validity.\nFor example, if a patient\u2019s measurement was not taken because the doctor\nfelt he was too sick, that observation would not be MAR or MCAR. In this\ncase the missing data mechanism causes our observed training data to give a\ndistorted picture of the true population, and data imputation is dangerous\nin this instance. Often the determination of whether features are MCAR\nmust be made from information about the data collection process. For\ncategorical features, one way to diagnose this problem is to code \u201cmissing\u201d\nas an additional class. Then we \ufb01t our model to the training data and see\nif class \u201cmissing\u201d is predictive of the response.", "351": "9.6 Missing Data 333\nAssuming the features are missing completely at random, there are a\nnumber of ways of proceeding:\n1. Discard observations with any missing values.\n2. Rely on the learning algorithm to deal with missing values in its\ntraining phase.\n3. Impute all missing values before training.\nApproach (1) can be used if the relative amount of missing data is small,\nbut otherwise should be avoided. Regarding (2), CART is one learning\nalgorithm that deals e\ufb00ectively with missing values, through surrogate splits\n(Section 9.2.4). MARS and PRIM use similar approaches. In generalized\nadditive modeling, all observations missing for a given input feature are\nomitted when the partial residuals are smoothed against that feature in\nthe back\ufb01tting algorithm, and their \ufb01tted values are set to zero. Since the\n\ufb01tted curves have mean zero (when the model includes an intercept), this\namounts to assigning the average \ufb01tted value to the missing observations.\nFor most learning methods, the imputation approach (3) is necessary.\nThe simplest tactic is to impute the missing value with the mean or median\nof the nonmissing values for that feature. (Note that the above procedure\nfor generalized additive models is analogous to this.)\nIf the features have at least some moderate degree of dependence, one\ncan do better by estimating a predictive model for each feature given the\nother features and then imputing each missing value by its prediction from\nthe model. In choosing the learning method for imputation of the features,\none must remember that this choice is distinct from the method used for\npredicting yfromX. Thus a \ufb02exible, adaptive method will often be pre-\nferred, even for the eventual purpose of carrying out a linear regression of y\nonX. In addition, if there are many missing feature values in the training\nset, the learning method must itself be able to deal with missing feature\nvalues. CART therefore is an ideal choice for this imputation \u201cengine.\u201d\nAfter imputation, missing values are typically treated as if they were ac-\ntually observed. This ignores the uncertainty due to the imputation, which\nwill itself introduce additional uncertainty into estimates and predictions\nfrom the response model. One can measure this additional uncertainty by\ndoing multiple imputations and hence creating many di\ufb00erent training sets.\nThe predictive model for ycan be \ufb01t to each training set, and the variation\nacross training sets can be assessed. If CART was used for the imputation\nengine, the multiple imputations could be done by sampling from the values\nin the corresponding terminal nodes.", "352": "334 9. Additive Models, Trees, and Related Methods\n9.7 Computational Considerations\nWith Nobservations and ppredictors, additive model \ufb01tting requires some\nnumber mpof applications of a one-dimensional smoother or regression\nmethod. The required number of cycles mof the back\ufb01tting algorithm is\nusually less than 20 and often less than 10, and depends on the amount\nof correlation in the inputs. With cubic smoothing splines, for example,\nNlogNoperations are needed for an initial sort and Noperations for the\nspline \ufb01t. Hence the total operations for an additive model \ufb01t is pNlogN+\nmpN.\nTrees require pNlogNoperations for an initial sort for each predictor,\nand typically another pNlogNoperations for the split computations. If the\nsplits occurred near the edges of the predictor ranges, this number could\nincrease to N2p.\nMARS requires Nm2+pmN operations to add a basis function to a\nmodel with mterms already present, from a pool of ppredictors. Hence to\nbuild an M-term model requires NM3+pM2Ncomputations, which can\nbe quite prohibitive if Mis a reasonable fraction of N.\nEach of the components of an HME are typically inexpensive to \ufb01t at\neach M-step: Np2for the regressions, and Np2K2for a K-class logistic\nregression. The EM algorithm, however, can take a long time to converge,\nand so sizable HME models are considered costly to \ufb01t.\nBibliographic Notes\nThe most comprehensive source for generalized additive models is the text\nof that name by Hastie and Tibshirani (1990). Di\ufb00erent applications of\nthis work in medical problems are discussed in Hastie et al. (1989) and\nHastie and Herman (1990), and the software implementation in Splus is\ndescribed in Chambers and Hastie (1991). Green and Silverman (1994)\ndiscuss penalization and spline models in a variety of settings. Efron and\nTibshirani (1991) give an exposition of modern developments in statisti cs\n(including generalized additive models), for a nonmathematical audience.\nClassi\ufb01cation and regression trees date back at least as far as Morgan and\nSonquist (1963). We have followed the modern approaches of Breiman et\nal. (1984) and Quinlan (1993). The PRIM method is due to Friedman\nand Fisher (1999), while MARS is introduced in Friedman (1991), with an\nadditive precursor in Friedman and Silverman (1989). Hierarchical mixtures\nof experts were proposed in Jordan and Jacobs (1994); see also Jacobs et\nal. (1991).", "353": "Exercises 335\nExercises\nEx. 9.1 Show that a smoothing spline \ufb01t of yitoxipreserves the linear\npartof the \ufb01t. In other words, if yi= \u02c6yi+ri, where \u02c6 yirepresents the\nlinear regression \ufb01ts, and Sis the smoothing matrix, then Sy=\u02c6y+Sr.\nShow that the same is true for local linear regression (Section 6.1.1). Hence\nargue that the adjustment step in the second line of (2) in Algorithm 9.1\nis unnecessary.\nEx. 9.2 LetAbe a known k\u00d7kmatrix, bbe a known k-vector, and z\nbe an unknown k-vector. A Gauss\u2013Seidel algorithm for solving the linear\nsystem of equations Az=bworks by successively solving for element zjin\nthejth equation, \ufb01xing all other zj\u2019s at their current guesses. This process\nis repeated for j= 1,2,... ,k, 1,2,... ,k,... , until convergence (Golub and\nVan Loan, 1983).\n(a) Consider an additive model with Nobservations and pterms, with\nthejth term to be \ufb01t by a linear smoother Sj. Consider the following\nsystem of equations:\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edI S 1S1\u2264\u2264\u2264S1\nS2I S 2\u2264\u2264\u2264S2\n...............\nSpSpSp\u2264\u2264\u2264I\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edf1\nf2\n...\nfp\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8=\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edS1y\nS2y\n...\nSpy\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8. (9.33)\nHere each fjis an N-vector of evaluations of the jth function at\nthe data points, and yis anN-vector of the response values. Show\nthat back\ufb01tting is a blockwise Gauss\u2013Seidel algorithm for solving this\nsystem of equations.\n(b) Let S1andS2be symmetric smoothing operators (matrices) with\neigenvalues in [0 ,1). Consider a back\ufb01tting algorithm with response\nvector yand smoothers S1,S2. Show that with any starting values,\nthe algorithm converges and give a formula for the \ufb01nal iterates.\nEx. 9.3 Back\ufb01tting equations. Consider a back\ufb01tting procedure with orthog-\nonal projections, and let Dbe the overall regression matrix whose columns\nspanV=Lcol(S1)\u2295 Lcol(S2)\u2295 \u2264\u2264\u2264 \u2295 L col(Sp), where Lcol(S) denotes the\ncolumn space of a matrix S. Show that the estimating equations\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edI S 1S1\u2264\u2264\u2264S1\nS2I S 2\u2264\u2264\u2264S2\n...............\nSpSpSp\u2264\u2264\u2264I\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edf1\nf2\n...\nfp\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8=\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8edS1y\nS2y\n...\nSpy\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8\nare equivalent to the least squares normal equations DTD\u03b2=DTywhere\n\u03b2is the vector of coe\ufb03cients.", "354": "336 9. Additive Models, Trees, and Related Methods\nEx. 9.4 Suppose the same smoother Sis used to estimate both terms in a\ntwo-term additive model (i.e., both variables are identical). Assume that S\nis symmetric with eigenvalues in [0 ,1). Show that the back\ufb01tting residual\nconverges to ( I+S)\u22121(I\u2212S)y, and that the residual sum of squares con-\nverges upward. Can the residual sum of squares converge upward in less\nstructured situations? How does this \ufb01t compare to the \ufb01t with a single\nterm \ufb01t by S? [Hint: Use the eigen-decomposition of Sto help with this\ncomparison.]\nEx. 9.5 Degrees of freedom of a tree . Given data yiwith mean f(xi) and\nvariance \u03c32, and a \ufb01tting operation y\u2192\u02c6y, let\u2019s de\ufb01ne the degrees of\nfreedom of a \ufb01t by/summationtext\nicov(yi,\u02c6yi)/\u03c32.\nConsider a \ufb01t \u02c6yestimated by a regression tree, \ufb01t to a set of predictors\nX1,X2,... ,X p.\n(a) In terms of the number of terminal nodes m, give a rough formula for\nthe degrees of freedom of the \ufb01t.\n(b) Generate 100 observations with predictors X1,X2,... ,X 10as inde-\npendent standard Gaussian variates and \ufb01x these values.\n(c) Generate response values also as standard Gaussian ( \u03c32= 1), indepen-\ndent of the predictors. Fit regression trees to the data of \ufb01xed size 1,5\nand 10 terminal nodes and hence estimate the degrees of freedom of\neach \ufb01t. [Do ten simulations of the response and average the results,\nto get a good estimate of degrees of freedom.]\n(d) Compare your estimates of degrees of freedom in (a) and (c) and\ndiscuss.\n(e) If the regression tree \ufb01t were a linear operation, we could write \u02c6y=Sy\nfor some matrix S. Then the degrees of freedom would be tr( S).\nSuggest a way to compute an approximate Smatrix for a regression\ntree, compute it and compare the resulting degrees of freedom to\nthose in (a) and (c).\nEx. 9.6 Consider the ozone data of Figure 6.9.\n(a) Fit an additive model to the cube root of ozone concentration. as a\nfunction of temperature, wind speed, and radiation. Compare your\nresults to those obtained via the trellis display in Figure 6.9.\n(b) Fit trees, MARS, and PRIM to the same data, and compare the results\nto those found in (a) and in Figure 6.9.", "355": "This is page 337\nPrinter: Opaque this\n10\nBoosting and Additive Trees\n10.1 Boosting Methods\nBoosting is one of the most powerful learning ideas introduced in the last\ntwenty years. It was originally designed for classi\ufb01cation problems, but as\nwill be seen in this chapter, it can pro\ufb01tably be extended to regression\nas well. The motivation for boosting was a procedure that combines the\noutputs of many \u201cweak\u201d classi\ufb01ers to produce a powerful \u201ccommittee.\u201d\nFrom this perspective boosting bears a resemblance to bagging and other\ncommittee-based approaches (Section 8.8). However we shall see that the\nconnection is at best super\ufb01cial and that boosting is fundamentally di\ufb00er-\nent.\nWe begin by describing the most popular boosting algorithm due to\nFreund and Schapire (1997) called \u201cAdaBoost.M1.\u201d Consider a two-class\nproblem, with the output variable coded as Y\u2208 {\u22121,1}. Given a vector of\npredictor variables X, a classi\ufb01er G(X) produces a prediction taking one\nof the two values {\u22121,1}. The error rate on the training sample is\nerr =1\nNN/summationdisplay\ni=1I(yi\u221dne}ationslash=G(xi)),\nand the expected error rate on future predictions is E XYI(Y\u221dne}ationslash=G(X)).\nA weak classi\ufb01er is one whose error rate is only slightly better than\nrandom guessing. The purpose of boosting is to sequentially apply the\nweak classi\ufb01cation algorithm to repeatedly modi\ufb01ed versions of the data,\nthereby producing a sequence of weak classi\ufb01ers Gm(x),m= 1,2,... ,M .", "356": "338 10. Boosting and Additive Trees\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  SampleWeighted  Sample\nTraining  SampleWeighted  Sample\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\nWeighted  SampleWeighted  SampleWeighted  Sample\nTraining  SampleWeighted  SampleG(x) = sign/bracketleft\uf8ecig/summationtextM\nm=1\u03b1mGm(x)/bracketright\uf8ecig\nGM(x)\nG3(x)\nG2(x)\nG1(x)Final Classifier\nFIGURE 10.1. Schematic of AdaBoost. Classi\ufb01ers are trained on weighted ver-\nsions of the dataset, and then combined to produce a \ufb01nal predictio n.\nThe predictions from all of them are then combined through a weighted\nmajority vote to produce the \ufb01nal prediction:\nG(x) = sign/parenleft\uf8eciggM/summationdisplay\nm=1\u03b1mGm(x)/parenright\uf8ecigg\n. (10.1)\nHere\u03b11,\u03b12,... ,\u03b1 Mare computed by the boosting algorithm, and weight\nthe contribution of each respective Gm(x). Their e\ufb00ect is to give higher\nin\ufb02uence to the more accurate classi\ufb01ers in the sequence. Figure 10.1 shows\na schematic of the AdaBoost procedure.\nThe data modi\ufb01cations at each boosting step consist of applying weights\nw1,w2,... ,w Nto each of the training observations ( xi,yi), i= 1,2,... ,N .\nInitially all of the weights are set to wi= 1/N, so that the \ufb01rst step simply\ntrains the classi\ufb01er on the data in the usual manner. For each successive\niteration m= 2,3,... ,M the observation weights are individually modi-\n\ufb01ed and the classi\ufb01cation algorithm is reapplied to the weighted observa-\ntions. At step m, those observations that were misclassi\ufb01ed by the classi\ufb01er\nGm\u22121(x) induced at the previous step have their weights increased, whereas\nthe weights are decreased for those that were classi\ufb01ed correctly. Thus as\niterations proceed, observations that are di\ufb03cult to classify correctly re-\nceive ever-increasing in\ufb02uence. Each successive classi\ufb01er is thereby forced", "357": "10.1 Boosting Methods 339\nAlgorithm 10.1 AdaBoost.M1.\n1. Initialize the observation weights wi= 1/N, i = 1,2,... ,N .\n2. For m= 1 to M:\n(a) Fit a classi\ufb01er Gm(x) to the training data using weights wi.\n(b) Compute\nerrm=/summationtextN\ni=1wiI(yi\u221dne}ationslash=Gm(xi))\n/summationtextN\ni=1wi.\n(c) Compute \u03b1m= log((1 \u2212errm)/errm).\n(d) Set wi\u2190wi\u2264exp[\u03b1m\u2264I(yi\u221dne}ationslash=Gm(xi))], i= 1,2,... ,N .\n3. Output G(x) = sign/bracketleft\uf8ecig/summationtextM\nm=1\u03b1mGm(x)/bracketright\uf8ecig\n.\nto concentrate on those training observations that are missed by previous\nones in the sequence.\nAlgorithm 10.1 shows the details of the AdaBoost.M1 algorithm. The\ncurrent classi\ufb01er Gm(x) is induced on the weighted observations at line 2a.\nThe resulting weighted error rate is computed at line 2b. Line 2c calculates\nthe weight \u03b1mgiven to Gm(x) in producing the \ufb01nal classi\ufb01er G(x) (line\n3). The individual weights of each of the observations are updated for the\nnext iteration at line 2d. Observations misclassi\ufb01ed by Gm(x) have their\nweights scaled by a factor exp( \u03b1m), increasing their relative in\ufb02uence for\ninducing the next classi\ufb01er Gm+1(x) in the sequence.\nThe AdaBoost.M1 algorithm is known as \u201cDiscrete AdaBoost\u201d in Fried-\nman et al. (2000), because the base classi\ufb01er Gm(x) returns a discrete class\nlabel. If the base classi\ufb01er instead returns a real-valued prediction (e.g.,\na probability mapped to the interval [ \u22121,1]), AdaBoost can be modi\ufb01ed\nappropriately (see \u201cReal AdaBoost\u201d in Friedman et al. (2000)).\nThe power of AdaBoost to dramatically increase the performance of even\na very weak classi\ufb01er is illustrated in Figure 10.2. The features X1,... ,X 10\nare standard independent Gaussian, and the deterministic target Yis de-\n\ufb01ned by\nY=/braceleftbigg\n1 if/summationtext10\nj=1X2\nj> \u03c72\n10(0.5),\n\u22121 otherwise .(10.2)\nHere\u03c72\n10(0.5) = 9 .34 is the median of a chi-squared random variable with\n10 degrees of freedom (sum of squares of 10 standard Gaussians). There are\n2000 training cases, with approximately 1000 cases in each class, and 10,0 00\ntest observations. Here the weak classi\ufb01er is just a \u201cstump\u201d: a two terminal-\nnode classi\ufb01cation tree. Applying this classi\ufb01er alone to the training data\nset yields a very poor test set error rate of 45.8%, compared to 50% for", "358": "340 10. Boosting and Additive Trees\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5\nBoosting IterationsTest ErrorSingle Stump\n244 Node Tree\nFIGURE 10.2. Simulated data (10.2): test error rate for boosting with stump s,\nas a function of the number of iterations. Also shown are the test error rate for\na single stump, and a 244-node classi\ufb01cation tree.\nrandom guessing. However, as boosting iterations proceed the error rate\nsteadily decreases, reaching 5.8% after 400 iterations. Thus, boosting this\nsimple very weak classi\ufb01er reduces its prediction error rate by almost a\nfactor of four. It also outperforms a single large classi\ufb01cation tree ( error\nrate 24 .7%). Since its introduction, much has been written to explain the\nsuccess of AdaBoost in producing accurate classi\ufb01ers. Most of this work\nhas centered on using classi\ufb01cation trees as the \u201cbase learner\u201d G(x), where\nimprovements are often most dramatic. In fact, Breiman (NIPS Workshop,\n1996) referred to AdaBoost with trees as the \u201cbest o\ufb00-the-shelf classi\ufb01er in\nthe world\u201d (see also Breiman (1998)). This is especially the case for data -\nmining applications, as discussed more fully in Section 10.7 later in this\nchapter.\n10.1.1 Outline of This Chapter\nHere is an outline of the developments in this chapter:\n\u2022We show that AdaBoost \ufb01ts an additive model in a base learner,\noptimizing a novel exponential loss function. This loss function is", "359": "10.2 Boosting Fits an Additive Model 341\nvery similar to the (negative) binomial log-likelihood (Sections 10.2\u2013\n10.4).\n\u2022The population minimizer of the exponential loss function is shown\nto be the log-odds of the class probabilities (Section 10.5).\n\u2022We describe loss functions for regression and classi\ufb01cation that are\nmore robust than squared error or exponential loss (Section 10.6).\n\u2022It is argued that decision trees are an ideal base learner for data\nmining applications of boosting (Sections 10.7 and 10.9).\n\u2022We develop a class of gradient boosted models (GBMs), for boosting\ntrees with any loss function (Section 10.10).\n\u2022The importance of \u201cslow learning\u201d is emphasized, and implemented\nby shrinkage of each new term that enters the model (Section 10.12),\nas well as randomization (Section 10.12.2).\n\u2022Tools for interpretation of the \ufb01tted model are described (Section 10.13).\n10.2 Boosting Fits an Additive Model\nThe success of boosting is really not very mysterious. The key lies in ex-\npression (10.1). Boosting is a way of \ufb01tting an additive expansion in a set\nof elementary \u201cbasis\u201d functions. Here the basis functions are the individual\nclassi\ufb01ers Gm(x)\u2208 {\u22121,1}. More generally, basis function expansions take\nthe form\nf(x) =M/summationdisplay\nm=1\u03b2mb(x;\u03b3m), (10.3)\nwhere \u03b2m,m= 1,2,... ,M are the expansion coe\ufb03cients, and b(x;\u03b3)\u2208IR\nare usually simple functions of the multivariate argument x, characterized\nby a set of parameters \u03b3. We discuss basis expansions in some detail in\nChapter 5.\nAdditive expansions like this are at the heart of many of the learning\ntechniques covered in this book:\n\u2022In single-hidden-layer neural networks (Chapter 11), b(x;\u03b3) =\u03c3(\u03b30+\n\u03b3T\n1x), where \u03c3(t) = 1/(1+e\u2212t) is the sigmoid function, and \u03b3param-\neterizes a linear combination of the input variables.\n\u2022In signal processing, wavelets (Section 5.9.1) are a popular choice with\n\u03b3parameterizing the location and scale shifts of a \u201cmother\u201d wavelet.\n\u2022Multivariate adaptive regression splines (Section 9.4) uses truncated-\npower spline basis functions where \u03b3parameterizes the variables and\nvalues for the knots.", "360": "342 10. Boosting and Additive Trees\nAlgorithm 10.2 Forward Stagewise Additive Modeling.\n1. Initialize f0(x) = 0.\n2. For m= 1 to M:\n(a) Compute\n(\u03b2m,\u03b3m) = arg min\n\u03b2,\u03b3N/summationdisplay\ni=1L(yi,fm\u22121(xi) +\u03b2b(xi;\u03b3)).\n(b) Set fm(x) =fm\u22121(x) +\u03b2mb(x;\u03b3m).\n\u2022For trees, \u03b3parameterizes the split variables and split points at the\ninternal nodes, and the predictions at the terminal nodes.\nTypically these models are \ufb01t by minimizing a loss function averaged\nover the training data, such as the squared-error or a likelihood-based loss\nfunction,\nmin\n{\u03b2m,\u03b3m}M\n1N/summationdisplay\ni=1L/parenleft\uf8ecigg\nyi,M/summationdisplay\nm=1\u03b2mb(xi;\u03b3m)/parenright\uf8ecigg\n. (10.4)\nFor many loss functions L(y,f(x)) and/or basis functions b(x;\u03b3), this re-\nquires computationally intensive numerical optimization techniques. How-\never, a simple alternative often can be found when it is feasible to rapidly\nsolve the subproblem of \ufb01tting just a single basis function,\nmin\n\u03b2,\u03b3N/summationdisplay\ni=1L(yi,\u03b2b(xi;\u03b3)). (10.5)\n10.3 Forward Stagewise Additive Modeling\nForward stagewise modeling approximates the solution to (10.4) by sequen-\ntially adding new basis functions to the expansion without adjusting the\nparameters and coe\ufb03cients of those that have already been added. This is\noutlined in Algorithm 10.2. At each iteration m, one solves for the optimal\nbasis function b(x;\u03b3m) and corresponding coe\ufb03cient \u03b2mto add to the cur-\nrent expansion fm\u22121(x). This produces fm(x), and the process is repeated.\nPreviously added terms are not modi\ufb01ed.\nFor squared-error loss\nL(y,f(x)) = ( y\u2212f(x))2, (10.6)", "361": "10.4 Exponential Loss and AdaBoost 343\none has\nL(yi,fm\u22121(xi) +\u03b2b(xi;\u03b3)) = ( yi\u2212fm\u22121(xi)\u2212\u03b2b(xi;\u03b3))2\n= (rim\u2212\u03b2b(xi;\u03b3))2, (10.7)\nwhere rim=yi\u2212fm\u22121(xi) is simply the residual of the current model\non the ith observation. Thus, for squared-error loss, the term \u03b2mb(x;\u03b3m)\nthat best \ufb01ts the current residuals is added to the expansion at each step.\nThis idea is the basis for \u201cleast squares\u201d regression boosting discussed in\nSection 10.10.2. However, as we show near the end of the next section,\nsquared-error loss is generally not a good choice for classi\ufb01cation; hence\nthe need to consider other loss criteria.\n10.4 Exponential Loss and AdaBoost\nWe now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forwar d\nstagewise additive modeling (Algorithm 10.2) using the loss function\nL(y,f(x)) = exp( \u2212y f(x)). (10.8)\nThe appropriateness of this criterion is addressed in the next section.\nFor AdaBoost the basis functions are the individual classi\ufb01ers Gm(x)\u2208\n{\u22121,1}. Using the exponential loss function, one must solve\n(\u03b2m,Gm) = arg min\n\u03b2,GN/summationdisplay\ni=1exp[\u2212yi(fm\u22121(xi) +\u03b2 G(xi))]\nfor the classi\ufb01er Gmand corresponding coe\ufb03cient \u03b2mto be added at each\nstep. This can be expressed as\n(\u03b2m,Gm) = arg min\n\u03b2,GN/summationdisplay\ni=1w(m)\niexp(\u2212\u03b2 yiG(xi)) (10.9)\nwithw(m)\ni= exp( \u2212yifm\u22121(xi)). Since each w(m)\nidepends neither on \u03b2\nnorG(x), it can be regarded as a weight that is applied to each observa-\ntion. This weight depends on fm\u22121(xi), and so the individual weight values\nchange with each iteration m.\nThe solution to (10.9) can be obtained in two steps. First, for any value\nof\u03b2 >0, the solution to (10.9) for Gm(x) is\nGm= arg min\nGN/summationdisplay\ni=1w(m)\niI(yi\u221dne}ationslash=G(xi)), (10.10)", "362": "344 10. Boosting and Additive Trees\nwhich is the classi\ufb01er that minimizes the weighted error rate in predicting\ny. This can be easily seen by expressing the criterion in (10.9) as\ne\u2212\u03b2\u2264/summationdisplay\nyi=G(xi)w(m)\ni+e\u03b2\u2264/summationdisplay\nyi/ne}ationslash=G(xi)w(m)\ni,\nwhich in turn can be written as\n/parenleftbig\ne\u03b2\u2212e\u2212\u03b2/parenrightbig\n\u2264N/summationdisplay\ni=1w(m)\niI(yi\u221dne}ationslash=G(xi)) +e\u2212\u03b2\u2264N/summationdisplay\ni=1w(m)\ni. (10.11)\nPlugging this Gminto (10.9) and solving for \u03b2one obtains\n\u03b2m=1\n2log1\u2212errm\nerrm, (10.12)\nwhere err mis the minimized weighted error rate\nerrm=/summationtextN\ni=1w(m)\niI(yi\u221dne}ationslash=Gm(xi))\n/summationtextN\ni=1w(m)\ni. (10.13)\nThe approximation is then updated\nfm(x) =fm\u22121(x) +\u03b2mGm(x),\nwhich causes the weights for the next iteration to be\nw(m+1)\ni =w(m)\ni\u2264e\u2212\u03b2myiGm(xi). (10.14)\nUsing the fact that \u2212yiGm(xi) = 2\u2264I(yi\u221dne}ationslash=Gm(xi))\u22121, (10.14) becomes\nw(m+1)\ni =w(m)\ni\u2264e\u03b1mI(yi/ne}ationslash=Gm(xi))\u2264e\u2212\u03b2m, (10.15)\nwhere \u03b1m= 2\u03b2mis the quantity de\ufb01ned at line 2c of AdaBoost.M1 (Al-\ngorithm 10.1). The factor e\u2212\u03b2min (10.15) multiplies all weights by the\nsame value, so it has no e\ufb00ect. Thus (10.15) is equivalent to line 2(d) of\nAlgorithm 10.1.\nOne can view line 2(a) of the Adaboost.M1 algorithm as a method for\napproximately solving the minimization in (10.11) and hence (10.10). Hence\nwe conclude that AdaBoost.M1 minimizes the exponential loss criterion\n(10.8) via a forward-stagewise additive modeling approach.\nFigure 10.3 shows the training-set misclassi\ufb01cation error rate and aver-\nage exponential loss for the simulated data problem (10.2) of Figure 10.2 .\nThe training-set misclassi\ufb01cation error decreases to zero at around 250 it-\nerations (and remains there), but the exponential loss keeps decreasing.\nNotice also in Figure 10.2 that the test-set misclassi\ufb01cation error conti nues\nto improve after iteration 250. Clearly Adaboost is not optimizing tra ining-\nset misclassi\ufb01cation error; the exponential loss is more sensitive to cha nges\nin the estimated class probabilities.", "363": "10.5 Why Exponential Loss? 345\n0 100 200 300 4000.0 0.2 0.4 0.6 0.8 1.0\nBoosting IterationsTraining Error\nMisclassification RateExponential Loss\nFIGURE 10.3. Simulated data, boosting with stumps: misclassi\ufb01cation error\nrate on the training set, and average exponential loss: (1/N)PN\ni=1exp(\u2212yif(xi)).\nAfter about 250iterations, the misclassi\ufb01cation error is zero, while the expo nential\nloss continues to decrease.\n10.5 Why Exponential Loss?\nThe AdaBoost.M1 algorithm was originally motivated from a very di\ufb00er -\nent perspective than presented in the previous section. Its equivalence to\nforward stagewise additive modeling based on exponential loss was only\ndiscovered \ufb01ve years after its inception. By studying the properties of the\nexponential loss criterion, one can gain insight into the procedure and dis-\ncover ways it might be improved.\nThe principal attraction of exponential loss in the context of additive\nmodeling is computational; it leads to the simple modular reweighting Ad-\naBoost algorithm. However, it is of interest to inquire about its stat istical\nproperties. What does it estimate and how well is it being estimated? The\n\ufb01rst question is answered by seeking its population minimizer.\nIt is easy to show (Friedman et al., 2000) that\nf\u2217(x) = arg min\nf(x)EY|x(e\u2212Y f(x)) =1\n2logPr(Y= 1|x)\nPr(Y=\u22121|x), (10.16)", "364": "346 10. Boosting and Additive Trees\nor equivalently\nPr(Y= 1|x) =1\n1 +e\u22122f\u2217(x).\nThus, the additive expansion produced by AdaBoost is estimating one-\nhalf the log-odds of P(Y= 1|x). This justi\ufb01es using its sign as the classi\ufb01-\ncation rule in (10.1).\nAnother loss criterion with the same population minimizer is the bi-\nnomial negative log-likelihood or deviance (also known as cross-entropy),\ninterpreting fas the logit transform. Let\np(x) = Pr( Y= 1|x) =ef(x)\ne\u2212f(x)+ef(x)=1\n1 +e\u22122f(x)(10.17)\nand de\ufb01ne Y\u2032= (Y+ 1)/2\u2208 {0,1}. Then the binomial log-likelihood loss\nfunction is\nl(Y,p(x)) =Y\u2032logp(x) + (1 \u2212Y\u2032)log(1 \u2212p(x)),\nor equivalently the deviance is\n\u2212l(Y,f(x)) = log/parenleft\uf8ecig\n1 +e\u22122Y f(x)/parenright\uf8ecig\n. (10.18)\nSince the population maximizer of log-likelihood is at the true probabilities\np(x) = Pr( Y= 1|x), we see from (10.17) that the population minimizers of\nthe deviance E Y|x[\u2212l(Y,f(x))] and E Y|x[e\u2212Y f(x)] are the same. Thus, using\neither criterion leads to the same solution at the population level. Note that\ne\u2212Y fitself is not a proper log-likelihood, since it is not the logarithm of\nany probability mass function for a binary random variable Y\u2208 {\u22121,1}.\n10.6 Loss Functions and Robustness\nIn this section we examine the di\ufb00erent loss functions for classi\ufb01cation and\nregression more closely, and characterize them in terms of their robustness\nto extreme data.\nRobust Loss Functions for Classi\ufb01cation\nAlthough both the exponential (10.8) and binomial deviance (10.18) yield\nthe same solution when applied to the population joint distribution, the\nsame is not true for \ufb01nite data sets. Both criteria are monotone decreasing\nfunctions of the \u201cmargin\u201d yf(x). In classi\ufb01cation (with a \u22121/1 response)\nthe margin plays a role analogous to the residuals y\u2212f(x) in regression. The\nclassi\ufb01cation rule G(x) = sign[ f(x)] implies that observations with positive\nmargin yif(xi)>0 are classi\ufb01ed correctly whereas those with negative\nmargin yif(xi)<0 are misclassi\ufb01ed. The decision boundary is de\ufb01ned by", "365": "10.6 Loss Functions and Robustness 347\n\u22122 \u22121 0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0Misclassification\nExponential\nBinomial Deviance\nSquared Error\nSupport VectorLoss\ny\u2264f\nFIGURE 10.4. Loss functions for two-class classi\ufb01cation. The response is\ny=\u00b11; the prediction is f, with class prediction sign(f). The losses are\nmisclassi\ufb01cation: I(sign( f)/ne}ationslash=y); exponential: exp(\u2212yf); binomial deviance:\nlog(1 + exp( \u22122yf)); squared error: (y\u2212f)2; and support vector: (1\u2212yf)+(see\nSection 12.3). Each function has been scaled so that it passes t hrough the point\n(0,1).\nf(x) = 0. The goal of the classi\ufb01cation algorithm is to produce positive\nmargins as frequently as possible. Any loss criterion used for classi\ufb01cati on\nshould penalize negative margins more heavily than positive ones since\npositive margin observations are already correctly classi\ufb01ed.\nFigure 10.4 shows both the exponential (10.8) and binomial deviance\ncriteria as a function of the margin y\u2264f(x). Also shown is misclassi\ufb01cation\nlossL(y,f(x)) =I(y\u2264f(x)<0), which gives unit penalty for negative mar-\ngin values, and no penalty at all for positive ones. Both the exponential\nand deviance loss can be viewed as monotone continuous approximations\nto misclassi\ufb01cation loss. They continuously penalize increasingly negative\nmargin values more heavily than they reward increasingly positive ones.\nThe di\ufb00erence between them is in degree. The penalty associated with bi-\nnomial deviance increases linearly for large increasingly negative margin,\nwhereas the exponential criterion increases the in\ufb02uence of such observa-\ntions exponentially.\nAt any point in the training process the exponential criterion concen-\ntrates much more in\ufb02uence on observations with large negative margins.\nBinomial deviance concentrates relatively less in\ufb02uence on such observa-", "366": "348 10. Boosting and Additive Trees\ntions, more evenly spreading the in\ufb02uence among all of the data. It is\ntherefore far more robust in noisy settings where the Bayes error rate is\nnot close to zero, and especially in situations where there is misspeci\ufb01cation\nof the class labels in the training data. The performance of AdaBoost has\nbeen empirically observed to dramatically degrade in such situations.\nAlso shown in the \ufb01gure is squared-error loss. The minimizer of the cor-\nresponding risk on the population is\nf\u2217(x) = arg min\nf(x)EY|x(Y\u2212f(x))2= E(Y|x) = 2\u2264Pr(Y= 1|x)\u22121.(10.19)\nAs before the classi\ufb01cation rule is G(x) = sign[ f(x)]. Squared-error loss\nis not a good surrogate for misclassi\ufb01cation error. As seen in Figure 10 .4, it\nis not a monotone decreasing function of increasing margin yf(x). For mar-\ngin values yif(xi)>1 it increases quadratically, thereby placing increasing\nin\ufb02uence (error) on observations that are correctly classi\ufb01ed with increas-\ning certainty, thereby reducing the relative in\ufb02uence of those incorrectly\nclassi\ufb01ed yif(xi)<0. Thus, if class assignment is the goal, a monotone de-\ncreasing criterion serves as a better surrogate loss function. Figure 12.4 on\npage 426 in Chapter 12 includes a modi\ufb01cation of quadratic loss, the \u201cHu-\nberized\u201d square hinge loss (Rosset et al., 2004b), which enjoys the favorable\nproperties of the binomial deviance, quadratic loss and the SVM hinge loss.\nIt has the same population minimizer as the quadratic (10.19), is zero for\ny\u2264f(x)>1, and becomes linear for y\u2264f(x)<\u22121. Since quadratic functions\nare easier to compute with than exponentials, our experience suggests this\nto be a useful alternative to the binomial deviance.\nWith K-class classi\ufb01cation, the response Ytakes values in the unordered\nsetG={G1,... ,Gk}(see Sections 2.4 and 4.4). We now seek a classi\ufb01er\nG(x) taking values in G. It is su\ufb03cient to know the class conditional proba-\nbilities pk(x) = Pr( Y=Gk|x),k= 1,2,... ,K , for then the Bayes classi\ufb01er\nis\nG(x) =Gkwhere k= arg max\n\u2113p\u2113(x). (10.20)\nIn principal, though, we need not learn the pk(x), but simply which one is\nlargest. However, in data mining applications the interest is often more in\nthe class probabilities p\u2113(x), \u2113= 1,... ,K themselves, rather than in per-\nforming a class assignment. As in Section 4.4, the logistic model generalizes\nnaturally to Kclasses,\npk(x) =efk(x)\n/summationtextK\nl=1efl(x), (10.21)\nwhich ensures that 0 \u2264pk(x)\u22641 and that they sum to one. Note that\nhere we have Kdi\ufb00erent functions, one per class. There is a redundancy\nin the functions fk(x), since adding an arbitrary h(x) to each leaves the\nmodel unchanged. Traditionally one of them is set to zero: for example,", "367": "10.6 Loss Functions and Robustness 349\nfK(x) = 0, as in (4.17). Here we prefer to retain the symmetry, and impose\nthe constraint/summationtextK\nk=1fk(x) = 0. The binomial deviance extends naturally\nto the K-class multinomial deviance loss function:\nL(y,p(x)) = \u2212K/summationdisplay\nk=1I(y=Gk)logpk(x)\n=\u2212K/summationdisplay\nk=1I(y=Gk)fk(x) + log/parenleft\uf8eciggK/summationdisplay\n\u2113=1ef\u2113(x)/parenright\uf8ecigg\n.(10.22)\nAs in the two-class case, the criterion (10.22) penalizes incorrect predictions\nonly linearly in their degree of incorrectness.\nZhu et al. (2005) generalize the exponential loss for K-class problems.\nSee Exercise 10.5 for details.\nRobust Loss Functions for Regression\nIn the regression setting, analogous to the relationship between exponential\nloss and binomial log-likelihood is the relationship between squared-error\nlossL(y,f(x)) = ( y\u2212f(x))2and absolute loss L(y,f(x)) =|y\u2212f(x)|. The\npopulation solutions are f(x) = E( Y|x) for squared-error loss, and f(x) =\nmedian( Y|x) for absolute loss; for symmetric error distributions these are\nthe same. However, on \ufb01nite samples squared-error loss places much more\nemphasis on observations with large absolute residuals |yi\u2212f(xi)|during\nthe \ufb01tting process. It is thus far less robust, and its performance severely\ndegrades for long-tailed error distributions and especially for grossly mis-\nmeasured y-values (\u201coutliers\u201d). Other more robust criteria, such as abso-\nlute loss, perform much better in these situations. In the statistical ro -\nbustness literature, a variety of regression loss criteria have been proposed\nthat provide strong resistance (if not absolute immunity) to gross outliers\nwhile being nearly as e\ufb03cient as least squares for Gaussian errors. They\nare often better than either for error distributions with moderately heavy\ntails. One such criterion is the Huber loss criterion used for M-regression\n(Huber, 1964)\nL(y,f(x)) =/braceleftbigg\n[y\u2212f(x)]2for|y\u2212f(x)| \u2264\u03b4,\n2\u03b4|y\u2212f(x)| \u2212\u03b42otherwise.(10.23)\nFigure 10.5 compares these three loss functions.\nThese considerations suggest than when robustness is a concern, as is\nespecially the case in data mining applications (see Section 10.7), squared-\nerror loss for regression and exponential loss for classi\ufb01cation are not the\nbest criteria from a statistical perspective. However, they both lead to the\nelegant modular boosting algorithms in the context of forward stagewis e\nadditive modeling. For squared-error loss one simply \ufb01ts the base learner\nto the residuals from the current model yi\u2212fm\u22121(xi) at each step. For", "368": "350 10. Boosting and Additive Trees\n\u22123 \u22122 \u22121 0 1 2 30 2 4 6 8Squared Error\nAbsolute Error\nHuberLoss\ny\u2212f\nFIGURE 10.5. A comparison of three loss functions for regression, plotted as a\nfunction of the margin y\u2212f. The Huber loss function combines the good properties\nof squared-error loss near zero and absolute error loss when |y\u2212f|is large.\nexponential loss one performs a weighted \ufb01t of the base learner to the\noutput values yi, with weights wi= exp( \u2212yifm\u22121(xi)). Using other more\nrobust criteria directly in their place does not give rise to such simple\nfeasible boosting algorithms. However, in Section 10.10.2 we show how one\ncan derive simple elegant boosting algorithms based on any di\ufb00erentiable\nloss criterion, thereby producing highly robust boosting procedures for data\nmining.\n10.7 \u201cO\ufb00-the-Shelf\u201d Procedures for Data Mining\nPredictive learning is an important aspect of data mining. As can be seen\nfrom this book, a wide variety of methods have been developed for predic-\ntive learning from data. For each particular method there are situations\nfor which it is particularly well suited, and others where it performs badly\ncompared to the best that can be done with that data. We have attempted\nto characterize appropriate situations in our discussions of each of the re-\nspective methods. However, it is seldom known in advance which procedure\nwill perform best or even well for any given problem. Table 10.1 summarizes\nsome of the characteristics of a number of learning methods.\nIndustrial and commercial data mining applications tend to be especially\nchallenging in terms of the requirements placed on learning procedures.\nData sets are often very large in terms of number of observations and\nnumber of variables measured on each of them. Thus, computational con-", "369": "10.7 \u201cO\ufb00-the-Shelf\u201d Procedures for Data Mining 351\nTABLE 10.1. Some characteristics of di\ufb00erent learning methods. Key: \u25b2= good,\n\u25c6=fair, and \u25bc=poor.\nCharacteristic Neural SVM Trees MARS k-NN,\nNets Kernels\nNatural handling of data\nof \u201cmixed\u201d type\u25bc \u25bc \u25b2 \u25b2 \u25bc\nHandling of missing values \u25bc \u25bc \u25b2 \u25b2 \u25b2\nRobustness to outliers in\ninput space\u25bc \u25bc \u25b2\u25bc \u25b2\nInsensitive to monotone\ntransformations of inputs\u25bc \u25bc \u25b2\u25bc \u25bc\nComputational scalability\n(large N)\u25bc \u25bc \u25b2 \u25b2 \u25bc\nAbility to deal with irrel-\nevant inputs\u25bc \u25bc \u25b2 \u25b2 \u25bc\nAbility to extract linear\ncombinations of features\u25b2 \u25b2 \u25bc \u25bc \u25c6\nInterpretability \u25bc \u25bc \u25c6\u25b2 \u25bc\nPredictive power \u25b2 \u25b2 \u25bc\u25c6 \u25b2\nsiderations play an important role. Also, the data are usually messy : the\ninputs tend to be mixtures of quantitative, binary, and categorical vari-\nables, the latter often with many levels. There are generally many missing\nvalues, complete observations being rare. Distributions of numeric predic-\ntor and response variables are often long-tailed and highly skewed. This\nis the case for the spam data (Section 9.1.2); when \ufb01tting a generalized\nadditive model, we \ufb01rst log-transformed each of the predictors in order to\nget a reasonable \ufb01t. In addition they usually contain a substantial fraction\nof gross mis-measurements (outliers). The predictor variables are generally\nmeasured on very di\ufb00erent scales.\nIn data mining applications, usually only a small fraction of the large\nnumber of predictor variables that have been included in the analysis are\nactually relevant to prediction. Also, unlike many applications such as pat-\ntern recognition, there is seldom reliable domain knowledge to help create\nespecially relevant features and/or \ufb01lter out the irrelevant ones, the inclu-\nsion of which dramatically degrades the performance of many methods.\nIn addition, data mining applications generally require interpretable mod-\nels. It is not enough to simply produce predictions. It is also desirable to\nhave information providing qualitative understanding of the relationship", "370": "352 10. Boosting and Additive Trees\nbetween joint values of the input variables and the resulting predicted re-\nsponse value. Thus, black box methods such as neural networks, which can\nbe quite useful in purely predictive settings such as pattern recognition,\nare far less useful for data mining.\nThese requirements of speed, interpretability and the messy nature of\nthe data sharply limit the usefulness of most learning procedures as o\ufb00-\nthe-shelf methods for data mining. An \u201co\ufb00-the-shelf\u201d method is one that\ncan be directly applied to the data without requiring a great deal of time-\nconsuming data preprocessing or careful tuning of the learning procedure.\nOf all the well-known learning methods, decision trees come closest to\nmeeting the requirements for serving as an o\ufb00-the-shelf procedure for data\nmining. They are relatively fast to construct and they produce interpretable\nmodels (if the trees are small). As discussed in Section 9.2, they naturally\nincorporate mixtures of numeric and categorical predictor variables and\nmissing values. They are invariant under (strictly monotone) transforma-\ntions of the individual predictors. As a result, scaling and/or more general\ntransformations are not an issue, and they are immune to the e\ufb00ects of pre-\ndictor outliers. They perform internal feature selection as an integral part\nof the procedure. They are thereby resistant, if not completely immune,\nto the inclusion of many irrelevant predictor variables. These properties of\ndecision trees are largely the reason that they have emerged as the most\npopular learning method for data mining.\nTrees have one aspect that prevents them from being the ideal tool for\npredictive learning, namely inaccuracy. They seldom provide predictive ac-\ncuracy comparable to the best that can be achieved with the data at hand.\nAs seen in Section 10.1, boosting decision trees improves their accuracy,\noften dramatically. At the same time it maintains most of their desirabl e\nproperties for data mining. Some advantages of trees that are sacri\ufb01ced by\nboosting are speed, interpretability, and, for AdaBoost, robustness against\noverlapping class distributions and especially mislabeling of the training\ndata. A gradient boosted model (GBM) is a generalization of tree boosting\nthat attempts to mitigate these problems, so as to produce an accurate and\ne\ufb00ective o\ufb00-the-shelf procedure for data mining.\n10.8 Example: Spam Data\nBefore we go into the details of gradient boosting, we demonstrate its abi li-\nties on a two-class classi\ufb01cation problem. The spam data are introduced in\nChapter 1, and used as an example for many of the procedures in Chapter 9\n(Sections 9.1.2, 9.2.5, 9.3.1 and 9.4.1).\nApplying gradient boosting to these data resulted in a test error rate of\n4.5%, using the same test set as was used in Section 9.1.2. By comparison,\nan additive logistic regression achieved 5.5%, a CART tree fully grown and", "371": "10.9 Boosting Trees 353\npruned by cross-validation 8.7%, and MARS 5.5%. The standard error of\nthese estimates is around 0.6%, although gradient boosting is signi\ufb01cantly\nbetter than all of them using the McNemar test (Exercise 10.6).\nIn Section 10.13 below we develop a relative importance measure for\neach predictor, as well as a partial dependence plot describing a predictor\u2019s\ncontribution to the \ufb01tted model. We now illustrate these for the spam data.\nFigure 10.6 displays the relative importance spectrum for all 57 predictor\nvariables. Clearly some predictors are more important than others in sep-\naratingspamfromemail. The frequencies of the character strings !,$,hp,\nandremove are estimated to be the four most relevant predictor variables.\nAt the other end of the spectrum, the character strings 857,415,table, and\n3dhave virtually no relevance.\nThe quantity being modeled here is the log-odds of spamversusemail\nf(x) = logPr(spam|x)\nPr(email|x)(10.24)\n(see Section 10.13 below). Figure 10.7 shows the partial dependence of the\nlog-odds on selected important predictors, two positively associated with\nspam(!andremove ), and two negatively associated ( eduandhp). These\nparticular dependencies are seen to be essentially monotonic. There is a\ngeneral agreement with the corresponding functions found by the additive\nlogistic regression model; see Figure 9.1 on page 303.\nRunning a gradient boosted model on these data with J= 2 terminal-\nnode trees produces a purely additive (main e\ufb00ects) model for the log-\nodds, with a corresponding error rate of 4.7%, as compared to 4.5% for the\nfull gradient boosted model (with J= 5 terminal-node trees). Although\nnot signi\ufb01cant, this slightly higher error rate suggests that there may be\ninteractions among some of the important predictor variables. This can\nbe diagnosed through two-variable partial dependence plots. Figure 10.8\nshows one of the several such plots displaying strong interaction e\ufb00ects.\nOne sees that for very low frequencies of hp, the log-odds of spamare\ngreatly increased. For high frequencies of hp, the log-odds of spamtend to\nbe much lower and roughly constant as a function of !. As the frequency\nofhpdecreases, the functional relationship with !strengthens.\n10.9 Boosting Trees\nRegression and classi\ufb01cation trees are discussed in detail in Section 9.2.\nThey partition the space of all joint predictor variable values into disjoin t\nregions Rj,j= 1,2,... ,J , as represented by the terminal nodes of the tree.\nA constant \u03b3jis assigned to each such region and the predictive rule is\nx\u2208Rj\u21d2f(x) =\u03b3j.", "372": "354 10. Boosting and Additive Trees\n!$hpremovefreeCAPAVEyourCAPMAXgeorgeCAPTOTeduyouourmoneywill1999businessre(receiveinternet000emailmeeting;650overmailpmpeopletechnologyhplallorderaddressmakefontprojectdataoriginalreportconferencelab[creditparts#85tablecsdirect415857telnetlabsaddresses3d\n0 20 40 60 80 100\nRelative Importance\nFIGURE 10.6. Predictor variable importance spectrum for the spamdata. The\nvariable names are written on the vertical axis.", "373": "10.9 Boosting Trees 355\n!Partial Dependence\n0.0 0.2 0.4 0.6 0.8 1.0-0.2 0.0 0.2 0.4 0.6 0.8 1.0\nremovePartial Dependence\n0.0 0.2 0.4 0.6-0.2 0.0 0.2 0.4 0.6 0.8 1.0\neduPartial Dependence\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.6 -0.2 0.0 0.2\nhpPartial Dependence\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1.0 -0.6 -0.2 0.0 0.2\nFIGURE 10.7. Partial dependence of log-odds of spamon four important pre-\ndictors. The red ticks at the base of the plots are deciles of t he input variable.\n0.51.01.52.02.53.00.20.40.60.81.0-1.0-0.5 0.0 0.5 1.0\nhp!\nFIGURE 10.8. Partial dependence of the log-odds of spamvs.email as a func-\ntion of joint frequencies of hpand the character !.", "374": "356 10. Boosting and Additive Trees\nThus a tree can be formally expressed as\nT(x;\u0398) =J/summationdisplay\nj=1\u03b3jI(x\u2208Rj), (10.25)\nwith parameters \u0398 = {Rj,\u03b3j}J\n1.Jis usually treated as a meta-parameter.\nThe parameters are found by minimizing the empirical risk\n\u02c6\u0398 = arg min\n\u0398J/summationdisplay\nj=1/summationdisplay\nxi\u2208RjL(yi,\u03b3j). (10.26)\nThis is a formidable combinatorial optimization problem, and we usually\nsettle for approximate suboptimal solutions. It is useful to divide the opti -\nmization problem into two parts:\nFinding \u03b3jgiven Rj:Given the Rj, estimating the \u03b3jis typically trivial,\nand often \u02c6 \u03b3j= \u00afyj, the mean of the yifalling in region Rj. For mis-\nclassi\ufb01cation loss, \u02c6 \u03b3jis the modal class of the observations falling in\nregion Rj.\nFinding Rj:This is the di\ufb03cult part, for which approximate solutions are\nfound. Note also that \ufb01nding the Rjentails estimating the \u03b3jas well.\nA typical strategy is to use a greedy, top-down recursive partitioning\nalgorithm to \ufb01nd the Rj. In addition, it is sometimes necessary to\napproximate (10.26) by a smoother and more convenient criterion for\noptimizing the Rj:\n\u02dc\u0398 = arg min\n\u0398N/summationdisplay\ni=1\u02dcL(yi,T(xi,\u0398)). (10.27)\nThen given the \u02c6Rj=\u02dcRj, the \u03b3jcan be estimated more precisely\nusing the original criterion.\nIn Section 9.2 we described such a strategy for classi\ufb01cation trees. The Gini\nindex replaced misclassi\ufb01cation loss in the growing of the tree (identifying\ntheRj).\nThe boosted tree model is a sum of such trees,\nfM(x) =M/summationdisplay\nm=1T(x;\u0398m), (10.28)\ninduced in a forward stagewise manner (Algorithm 10.2). At each step in\nthe forward stagewise procedure one must solve\n\u02c6\u0398m= arg min\n\u0398mN/summationdisplay\ni=1L(yi,fm\u22121(xi) +T(xi;\u0398m)) (10.29)", "375": "10.9 Boosting Trees 357\nfor the region set and constants \u0398 m={Rjm,\u03b3jm}Jm\n1of the next tree, given\nthe current model fm\u22121(x).\nGiven the regions Rjm, \ufb01nding the optimal constants \u03b3jmin each region\nis typically straightforward:\n\u02c6\u03b3jm= arg min\n\u03b3jm/summationdisplay\nxi\u2208RjmL(yi,fm\u22121(xi) +\u03b3jm). (10.30)\nFinding the regions is di\ufb03cult, and even more di\ufb03cult than for a single\ntree. For a few special cases, the problem simpli\ufb01es.\nFor squared-error loss, the solution to (10.29) is no harder than for a\nsingle tree. It is simply the regression tree that best predicts the current\nresiduals yi\u2212fm\u22121(xi), and \u02c6 \u03b3jmis the mean of these residuals in each\ncorresponding region.\nFor two-class classi\ufb01cation and exponential loss, this stagewise approac h\ngives rise to the AdaBoost method for boosting classi\ufb01cation trees (Algo -\nrithm 10.1). In particular, if the trees T(x;\u0398m) are restricted to be scaled\nclassi\ufb01cation trees, then we showed in Section 10.4 that the solution to\n(10.29) is the tree that minimizes the weighted error rate/summationtextN\ni=1w(m)\niI(yi\u221dne}ationslash=\nT(xi;\u0398m)) with weights w(m)\ni=e\u2212yifm\u22121(xi). By a scaled classi\ufb01cation\ntree, we mean \u03b2mT(x;\u0398m), with the restriction that \u03b3jm\u2208 {\u22121,1}).\nWithout this restriction, (10.29) still simpli\ufb01es for exponential loss t o a\nweighted exponential criterion for the new tree:\n\u02c6\u0398m= arg min\n\u0398mN/summationdisplay\ni=1w(m)\niexp[\u2212yiT(xi;\u0398m)]. (10.31)\nIt is straightforward to implement a greedy recursive-partitioning algori thm\nusing this weighted exponential loss as a splitting criterion. Given the Rjm,\none can show (Exercise 10.7) that the solution to (10.30) is the weighted\nlog-odds in each corresponding region\n\u02c6\u03b3jm= log/summationtext\nxi\u2208Rjmw(m)\niI(yi= 1)\n/summationtext\nxi\u2208Rjmw(m)\niI(yi=\u22121). (10.32)\nThis requires a specialized tree-growing algorithm; in practice, we prefer\nthe approximation presented below that uses a weighted least squares re-\ngression tree.\nUsing loss criteria such as the absolute error or the Huber loss (10.23) in\nplace of squared-error loss for regression, and the deviance (10.22) in place\nof exponential loss for classi\ufb01cation, will serve to robustify boosting trees.\nUnfortunately, unlike their nonrobust counterparts, these robust criteria\ndo not give rise to simple fast boosting algorithms.\nFor more general loss criteria the solution to (10.30), given the Rjm,\nis typically straightforward since it is a simple \u201clocation\u201d estimat e. For", "376": "358 10. Boosting and Additive Trees\nabsolute loss it is just the median of the residuals in each respective region.\nFor the other criteria fast iterative algorithms exist for solving (10 .30),\nand usually their faster \u201csingle-step\u201d approximations are adequate. The\nproblem is tree induction. Simple fast algorithms do not exist for solving\n(10.29) for these more general loss criteria, and approximations like (1 0.27)\nbecome essential.\n10.10 Numerical Optimization via Gradient\nBoosting\nFast approximate algorithms for solving (10.29) with any di\ufb00erenti able loss\ncriterion can be derived by analogy to numerical optimization. The loss in\nusing f(x) to predict yon the training data is\nL(f) =N/summationdisplay\ni=1L(yi,f(xi)). (10.33)\nThe goal is to minimize L(f) with respect to f, where here f(x) is con-\nstrained to be a sum of trees (10.28). Ignoring this constraint, minimizing\n(10.33) can be viewed as a numerical optimization\n\u02c6f= arg min\nfL(f), (10.34)\nwhere the \u201cparameters\u201d f\u2208IRNare the values of the approximating func-\ntionf(xi) at each of the Ndata points xi:\nf={f(x1),f(x2)),... ,f (xN)}.\nNumerical optimization procedures solve (10.34) as a sum of component\nvectors\nfM=M/summationdisplay\nm=0hm,hm\u2208IRN,\nwhere f0=h0is an initial guess, and each successive fmis induced based\non the current parameter vector fm\u22121, which is the sum of the previously\ninduced updates. Numerical optimization methods di\ufb00er in their prescrip-\ntions for computing each increment vector hm(\u201cstep\u201d).\n10.10.1 Steepest Descent\nSteepest descent chooses hm=\u2212\u03c1mgmwhere \u03c1mis a scalar and gm\u2208IRN\nis the gradient of L(f) evaluated at f=fm\u22121. The components of the\ngradient gmare\ngim=/bracketleftbigg\u2202L(yi,f(xi))\n\u2202f(xi)/bracketrightbigg\nf(xi)=fm\u22121(xi)(10.35)", "377": "10.10 Numerical Optimization via Gradient Boosting 359\nThestep length \u03c1mis the solution to\n\u03c1m= arg min\n\u03c1L(fm\u22121\u2212\u03c1gm). (10.36)\nThe current solution is then updated\nfm=fm\u22121\u2212\u03c1mgm\nand the process repeated at the next iteration. Steepest descent can be\nviewed as a very greedy strategy, since \u2212gmis the local direction in IRN\nfor which L(f) is most rapidly decreasing at f=fm\u22121.\n10.10.2 Gradient Boosting\nForward stagewise boosting (Algorithm 10.2) is also a very greedy st rategy.\nAt each step the solution tree is the one that maximally reduces (10.29),\ngiven the current model fm\u22121and its \ufb01ts fm\u22121(xi). Thus, the tree predic-\ntionsT(xi;\u0398m) are analogous to the components of the negative gradient\n(10.35). The principal di\ufb00erence between them is that the tree compo-\nnentstm= (T(x1;\u0398m),... ,T (xN;\u0398m) are not independent. They are con-\nstrained to be the predictions of a Jm-terminal node decision tree, whereas\nthe negative gradient is the unconstrained maximal descent direction.\nThe solution to (10.30) in the stagewise approach is analogous to the li ne\nsearch (10.36) in steepest descent. The di\ufb00erence is that (10.30) performs\na separate line search for those components of tmthat correspond to each\nseparate terminal region {T(xi;\u0398m)}xi\u2208Rjm.\nIf minimizing loss on the training data (10.33) were the only goal, steep-\nest descent would be the preferred strategy. The gradient (10.35) is trivial\nto calculate for any di\ufb00erentiable loss function L(y,f(x)), whereas solving\n(10.29) is di\ufb03cult for the robust criteria discussed in Section 10.6. Unfor-\ntunately the gradient (10.35) is de\ufb01ned only at the training data points xi,\nwhereas the ultimate goal is to generalize fM(x) to new data not repre-\nsented in the training set.\nA possible resolution to this dilemma is to induce a tree T(x;\u0398m) at the\nmth iteration whose predictions tmare as close as possible to the negative\ngradient. Using squared error to measure closeness, this leads us to\n\u02dc\u0398m= arg min\n\u0398N/summationdisplay\ni=1(\u2212gim\u2212T(xi;\u0398))2. (10.37)\nThat is, one \ufb01ts the tree Tto the negative gradient values (10.35) by least\nsquares. As noted in Section 10.9 fast algorithms exist for least squares\ndecision tree induction. Although the solution regions \u02dcRjmto (10.37) will\nnot be identical to the regions Rjmthat solve (10.29), it is generally sim-\nilar enough to serve the same purpose. In any case, the forward stagewise", "378": "360 10. Boosting and Additive Trees\nTABLE 10.2. Gradients for commonly used loss functions.\nSetting Loss Function \u2212\u2202L(yi,f(xi))/\u2202f(xi)\nRegression1\n2[yi\u2212f(xi)]2yi\u2212f(xi)\nRegression |yi\u2212f(xi)| sign[yi\u2212f(xi)]\nRegression Huber yi\u2212f(xi) for|yi\u2212f(xi)| \u2264\u03b4m\n\u03b4msign[yi\u2212f(xi)] for|yi\u2212f(xi)|> \u03b4m\nwhere \u03b4m=\u03b1th-quantile {|yi\u2212f(xi)|}\nClassi\ufb01cation Deviance kth component: I(yi=Gk)\u2212pk(xi)\nboosting procedure, and top-down decision tree induction, are themselves\napproximation procedures. After constructing the tree (10.37), the corre-\nsponding constants in each region are given by (10.30).\nTable 10.2 summarizes the gradients for commonly used loss functions.\nFor squared error loss, the negative gradient is just the ordinary residual\n\u2212gim=yi\u2212fm\u22121(xi), so that (10.37) on its own is equivalent standard\nleast squares boosting. With absolute error loss, the negative gradient i s\nthesignof the residual, so at each iteration (10.37) \ufb01ts the tree to the\nsign of the current residuals by least squares. For Huber M-regression, the\nnegative gradient is a compromise between these two (see the table).\nFor classi\ufb01cation the loss function is the multinomial deviance (10.22),\nandKleast squares trees are constructed at each iteration. Each tree Tkm\nis \ufb01t to its respective negative gradient vector gkm,\n\u2212gikm=\u2202L(yi,f1m(xi),... ,f 1m(xi))\n\u2202fkm(xi)\n=I(yi=Gk)\u2212pk(xi), (10.38)\nwithpk(x) given by (10.21). Although Kseparate trees are built at each\niteration, they are related through (10.21). For binary classi\ufb01cation ( K=\n2), only one tree is needed (exercise 10.10).\n10.10.3 Implementations of Gradient Boosting\nAlgorithm 10.3 presents the generic gradient tree-boosting algorithm for\nregression. Speci\ufb01c algorithms are obtained by inserting di\ufb00erent loss cri-\nteriaL(y,f(x)). The \ufb01rst line of the algorithm initializes to the optimal\nconstant model, which is just a single terminal node tree. The components\nof the negative gradient computed at line 2(a) are referred to as general-\nized or pseudo residuals, r. Gradients for commonly used loss functions are\nsummarized in Table 10.2.", "379": "10.11 Right-Sized Trees for Boosting 361\nAlgorithm 10.3 Gradient Tree Boosting Algorithm.\n1. Initialize f0(x) = arg min \u03b3/summationtextN\ni=1L(yi,\u03b3).\n2. For m= 1 to M:\n(a) For i= 1,2,... ,N compute\nrim=\u2212/bracketleftbigg\u2202L(yi,f(xi))\n\u2202f(xi)/bracketrightbigg\nf=fm\u22121.\n(b) Fit a regression tree to the targets rimgiving terminal regions\nRjm, j= 1,2,... ,J m.\n(c) For j= 1,2,... ,J mcompute\n\u03b3jm= arg min\n\u03b3/summationdisplay\nxi\u2208RjmL(yi,fm\u22121(xi) +\u03b3).\n(d) Update fm(x) =fm\u22121(x) +/summationtextJm\nj=1\u03b3jmI(x\u2208Rjm).\n3. Output \u02c6f(x) =fM(x).\nThe algorithm for classi\ufb01cation is similar. Lines 2(a)\u2013(d) are repeated\nKtimes at each iteration m, once for each class using (10.38). The result\nat line 3 is Kdi\ufb00erent (coupled) tree expansions fkM(x),k= 1,2,... ,K .\nThese produce probabilities via (10.21) or do classi\ufb01cation as in (10.20).\nDetails are given in Exercise 10.9. Two basic tuning parameters are the\nnumber of iterations Mand the sizes of each of the constituent trees\nJm, m= 1,2,... ,M .\nThe original implementation of this algorithm was called MART for\n\u201cmultiple additive regression trees,\u201d and was referred to in the \ufb01rst edi-\ntion of this book. Many of the \ufb01gures in this chapter were produced by\nMART. Gradient boosting as described here is implemented in the R gbm\npackage (Ridgeway, 1999, \u201cGradient Boosted Models\u201d), and is freely avai l-\nable. The gbmpackage is used in Section 10.14.2, and extensively in Chap-\nters 16 and 15. Another R implementation of boosting is mboost (Hothorn\nand B\u00a8 uhlmann, 2006). A commercial implementation of gradient boost-\ning/MART called TreeNet\uf8e8is available from Salford Systems, Inc.\n10.11 Right-Sized Trees for Boosting\nHistorically, boosting was considered to be a technique for combining mod-\nels, here trees. As such, the tree building algorithm was regarded as a", "380": "362 10. Boosting and Additive Trees\nprimitive that produced models to be combined by the boosting proce-\ndure. In this scenario, the optimal size of each tree is estimated separately\nin the usual manner when it is built (Section 9.2). A very large (oversized)\ntree is \ufb01rst induced, and then a bottom-up procedure is employed to prune\nit to the estimated optimal number of terminal nodes. This approach as-\nsumes implicitly that each tree is the last one in the expansion (10.28).\nExcept perhaps for the very last tree, this is clearly a very poor assump-\ntion. The result is that trees tend to be much too large, especially during\nthe early iterations. This substantially degrades performance and increases\ncomputation.\nThe simplest strategy for avoiding this problem is to restrict all trees\nto be the same size, Jm=J\u2200m. At each iteration a J-terminal node\nregression tree is induced. Thus Jbecomes a meta-parameter of the entire\nboosting procedure, to be adjusted to maximize estimated performance for\nthe data at hand.\nOne can get an idea of useful values for Jby considering the properties\nof the \u201ctarget\u201d function\n\u03b7= arg min\nfEXYL(Y,f(X)). (10.39)\nHere the expected value is over the population joint distribution of ( X,Y).\nThe target function \u03b7(x) is the one with minimum prediction risk on future\ndata. This is the function we are trying to approximate.\nOne relevant property of \u03b7(X) is the degree to which the coordinate vari-\nables XT= (X1,X2,... ,X p) interact with one another. This is captured\nby its ANOVA (analysis of variance) expansion\n\u03b7(X) =/summationdisplay\nj\u03b7j(Xj)+/summationdisplay\njk\u03b7jk(Xj,Xk)+/summationdisplay\njkl\u03b7jkl(Xj,Xk,Xl)+\u2264\u2264\u2264.(10.40)\nThe \ufb01rst sum in (10.40) is over functions of only a single predictor variable\nXj. The particular functions \u03b7j(Xj) are those that jointly best approximate\n\u03b7(X) under the loss criterion being used. Each such \u03b7j(Xj) is called the\n\u201cmain e\ufb00ect\u201d of Xj. The second sum is over those two-variable functions\nthat when added to the main e\ufb00ects best \ufb01t \u03b7(X). These are called the\nsecond-order interactions of each respective variable pair ( Xj,Xk). The\nthird sum represents third-order interactions, and so on. For many problems\nencountered in practice, low-order interaction e\ufb00ects tend to dominate.\nWhen this is the case, models that produce strong higher-order interaction\ne\ufb00ects, such as large decision trees, su\ufb00er in accuracy.\nThe interaction level of tree-based approximations is limited by the tree\nsizeJ. Namely, no interaction e\ufb00ects of level greater that J\u22121 are pos-\nsible. Since boosted models are additive in the trees (10.28), this limit\nextends to them as well. Setting J= 2 (single split \u201cdecision stump\u201d)\nproduces boosted models with only main e\ufb00ects; no interactions are per-\nmitted. With J= 3, two-variable interaction e\ufb00ects are also allowed, and", "381": "10.11 Right-Sized Trees for Boosting 363\nNumber of TermsTest Error\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4Stumps\n10 Node\n100 Node\nAdaboost\nFIGURE 10.9. Boosting with di\ufb00erent sized trees, applied to the example (10. 2)\nused in Figure 10.2. Since the generative model is additive, stu mps perform the\nbest. The boosting algorithm used the binomial deviance loss in Algorithm 10.3;\nshown for comparison is the AdaBoost Algorithm 10.1.\nso on. This suggests that the value chosen for Jshould re\ufb02ect the level\nof dominant interactions of \u03b7(x). This is of course generally unknown, but\nin most situations it will tend to be low. Figure 10.9 illustrates the e\ufb00 ect\nof interaction order (choice of J) on the simulation example (10.2). The\ngenerative function is additive (sum of quadratic monomials), so boosting\nmodels with J >2 incurs unnecessary variance and hence the higher test\nerror. Figure 10.10 compares the coordinate functions found by boosted\nstumps with the true functions.\nAlthough in many applications J= 2 will be insu\ufb03cient, it is unlikely\nthatJ >10 will be required. Experience so far indicates that 4 \u2264J\u22648\nworks well in the context of boosting, with results being fairly insensiti ve\nto particular choices in this range. One can \ufb01ne-tune the value for Jby\ntrying several di\ufb00erent values and choosing the one that produces the low-\nest risk on a validation sample. However, this seldom provides signi\ufb01cant\nimprovement over using J\u22436.", "382": "364 10. Boosting and Additive Trees\nCoordinate Functions for Additive Logistic Trees\nf1(x1) f2(x2) f3(x3) f4(x4) f5(x5)\nf6(x6) f7(x7) f8(x8) f9(x9) f10(x10)\nFIGURE 10.10. Coordinate functions estimated by boosting stumps for the sim-\nulated example used in Figure 10.9. The true quadratic functio ns are shown for\ncomparison.\n10.12 Regularization\nBesides the size of the constituent trees, J, the other meta-parameter of\ngradient boosting is the number of boosting iterations M. Each iteration\nusually reduces the training risk L(fM), so that for Mlarge enough this risk\ncan be made arbitrarily small. However, \ufb01tting the training data too well\ncan lead to over\ufb01tting, which degrades the risk on future predictions. Thus,\nthere is an optimal number M\u2217minimizing future risk that is application\ndependent. A convenient way to estimate M\u2217is to monitor prediction risk\nas a function of Mon a validation sample. The value of Mthat minimizes\nthis risk is taken to be an estimate of M\u2217. This is analogous to the early\nstopping strategy often used with neural networks (Section 11.4).\n10.12.1 Shrinkage\nControlling the value of Mis not the only possible regularization strategy.\nAs with ridge regression and neural networks, shrinkage techniques can be\nemployed as well (see Sections 3.4.1 and 11.5). The simplest implementation\nof shrinkage in the context of boosting is to scale the contribution of each\ntree by a factor 0 < \u03bd < 1 when it is added to the current approximation.\nThat is, line 2(d) of Algorithm 10.3 is replaced by\nfm(x) =fm\u22121(x) +\u03bd\u2264J/summationdisplay\nj=1\u03b3jmI(x\u2208Rjm). (10.41)\nThe parameter \u03bdcan be regarded as controlling the learning rate of the\nboosting procedure. Smaller values of \u03bd(more shrinkage) result in larger\ntraining risk for the same number of iterations M. Thus, both \u03bdandM\ncontrol prediction risk on the training data. However, these parameters do", "383": "10.12 Regularization 365\nnot operate independently. Smaller values of \u03bdlead to larger values of M\nfor the same training risk, so that there is a tradeo\ufb00 between them.\nEmpirically it has been found (Friedman, 2001) that smaller values of \u03bd\nfavor better test error, and require correspondingly larger values of M. In\nfact, the best strategy appears to be to set \u03bdto be very small ( \u03bd <0.1)\nand then choose Mby early stopping. This yields dramatic improvements\n(over no shrinkage \u03bd= 1) for regression and for probability estimation. The\ncorresponding improvements in misclassi\ufb01cation risk via (10.20) are les s,\nbut still substantial. The price paid for these improvements is computa-\ntional: smaller values of \u03bdgive rise to larger values of M, and computation\nis proportional to the latter. However, as seen below, many iterations ar e\ngenerally computationally feasible even on very large data sets. This is\npartly due to the fact that small trees are induced at each step with no\npruning.\nFigure 10.11 shows test error curves for the simulated example (10.2) of\nFigure 10.2. A gradient boosted model (MART) was trained using binomial\ndeviance, using either stumps or six terminal-node trees, and with or with-\nout shrinkage. The bene\ufb01ts of shrinkage are evident, especially when the\nbinomial deviance is tracked. With shrinkage, each test error curve reaches\na lower value, and stays there for many iterations.\nSection 16.2.1 draws a connection between forward stagewise shrinkage\nin boosting and the use of an L1penalty for regularizing model parame-\nters (the \u201classo\u201d). We argue that L1penalties may be superior to the L2\npenalties used by methods such as the support vector machine.\n10.12.2 Subsampling\nWe saw in Section 8.7 that bootstrap averaging (bagging) improves the\nperformance of a noisy classi\ufb01er through averaging. Chapter 15 discusses\nin some detail the variance-reduction mechanism of this sampling followed\nby averaging. We can exploit the same device in gradient boosting, both\nto improve performance and computational e\ufb03ciency.\nWithstochastic gradient boosting (Friedman, 1999), at each iteration we\nsample a fraction \u03b7of the training observations (without replacement),\nand grow the next tree using that subsample. The rest of the algorithm is\nidentical. A typical value for \u03b7can be1\n2, although for large N,\u03b7can be\nsubstantially smaller than1\n2.\nNot only does the sampling reduce the computing time by the same\nfraction \u03b7, but in many cases it actually produces a more accurate model.\nFigure 10.12 illustrates the e\ufb00ect of subsampling using the simulated\nexample (10.2), both as a classi\ufb01cation and as a regression example. We\nsee in both cases that sampling along with shrinkage slightly outperform ed\nthe rest. It appears here that subsampling without shrinkage does poorly.", "384": "366 10. Boosting and Additive Trees\nBoosting IterationsTest Set Deviance\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\nShrinkage=0.2Stumps\nDeviance\nBoosting IterationsTest Set Misclassification Error\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\nShrinkage=0.2Stumps\nMisclassification Error\nBoosting IterationsTest Set Deviance\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\nShrinkage=0.66-Node Trees\nDeviance\nBoosting IterationsTest Set Misclassification Error\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\nShrinkage=0.66-Node Trees\nMisclassification Error\nFIGURE 10.11. Test error curves for simulated example (10.2) of Figure 10.9 ,\nusing gradient boosting (MART). The models were trained using bino mial de-\nviance, either stumps or six terminal-node trees, and with or wit hout shrinkage.\nThe left panels report test deviance, while the right panels sho w misclassi\ufb01cation\nerror. The bene\ufb01cial e\ufb00ect of shrinkage can be seen in all cases, especially for\ndeviance in the left panels.", "385": "10.13 Interpretation 367\n0 200 400 600 800 10000.4 0.6 0.8 1.0 1.2 1.4\nBoosting IterationsTest Set DevianceDeviance4\u2212Node Trees\n0 200 400 600 800 10000.30 0.35 0.40 0.45 0.50\nBoosting IterationsTest Set Absolute ErrorNo shrinkage\nShrink=0.1\nSample=0.5\nShrink=0.1 Sample=0.5Absolute Error\nFIGURE 10.12. Test-error curves for the simulated example (10.2), showing\nthe e\ufb00ect of stochasticity. For the curves labeled \u201cSample = 0.5\u201d, a di\ufb00erent 50%\nsubsample of the training data was used each time a tree was grow n. In the left\npanel the models were \ufb01t by gbmusing a binomial deviance loss function; in the\nright-hand panel using square-error loss.\nThe downside is that we now have four parameters to set: J,M,\u03bdand\n\u03b7. Typically some early explorations determine suitable values for J,\u03bdand\n\u03b7, leaving Mas the primary parameter.\n10.13 Interpretation\nSingle decision trees are highly interpretable. The entire model can be com-\npletely represented by a simple two-dimensional graphic (binary tree) that\nis easily visualized. Linear combinations of trees (10.28) lose this import ant\nfeature, and must therefore be interpreted in a di\ufb00erent way.\n10.13.1 Relative Importance of Predictor Variables\nIn data mining applications the input predictor variables are seldom equally\nrelevant. Often only a few of them have substantial in\ufb02uence on the re-\nsponse; the vast majority are irrelevant and could just as well have not\nbeen included. It is often useful to learn the relative importance or contri-\nbution of each input variable in predicting the response.", "386": "368 10. Boosting and Additive Trees\nFor a single decision tree T, Breiman et al. (1984) proposed\nI2\n\u2113(T) =J\u22121/summationdisplay\nt=1\u02c6\u01312\ntI(v(t) =\u2113) (10.42)\nas a measure of relevance for each predictor variable X\u2113. The sum is over\ntheJ\u22121 internal nodes of the tree. At each such node t, one of the input\nvariables Xv(t)is used to partition the region associated with that node into\ntwo subregions; within each a separate constant is \ufb01t to the response values.\nThe particular variable chosen is the one that gives maximal estimated\nimprovement \u02c6 \u01312\ntin squared error risk over that for a constant \ufb01t over the\nentire region. The squared relative importance of variable X\u2113is the sum of\nsuch squared improvements over all internal nodes for which it was chosen\nas the splitting variable.\nThis importance measure is easily generalized to additive tree expansions\n(10.28); it is simply averaged over the trees\nI2\n\u2113=1\nMM/summationdisplay\nm=1I2\n\u2113(Tm). (10.43)\nDue to the stabilizing e\ufb00ect of averaging, this measure turns out to be more\nreliable than is its counterpart (10.42) for a single tree. Also, because of\nshrinkage (Section 10.12.1) the masking of important variables by other s\nwith which they are highly correlated is much less of a problem. Note\nthat (10.42) and (10.43) refer to squared relevance; the actual relevances\nare their respective square roots. Since these measures are relative, it is\ncustomary to assign the largest a value of 100 and then scale the others\naccordingly. Figure 10.6 shows the relevant importance of the 57 inputs in\npredicting spamversusemail.\nForK-class classi\ufb01cation, Kseparate models fk(x),k= 1,2,... ,K are\ninduced, each consisting of a sum of trees\nfk(x) =M/summationdisplay\nm=1Tkm(x). (10.44)\nIn this case (10.43) generalizes to\nI2\n\u2113k=1\nMM/summationdisplay\nm=1I2\n\u2113(Tkm). (10.45)\nHereI\u2113kis the relevance of X\u2113in separating the class kobservations from\nthe other classes. The overall relevance of X\u2113is obtained by averaging over\nall of the classes\nI2\n\u2113=1\nKK/summationdisplay\nk=1I2\n\u2113k. (10.46)", "387": "10.13 Interpretation 369\nFigures 10.23 and 10.24 illustrate the use of these averaged and separate\nrelative importances.\n10.13.2 Partial Dependence Plots\nAfter the most relevant variables have been identi\ufb01ed, the next step is to\nattempt to understand the nature of the dependence of the approximation\nf(X) on their joint values. Graphical renderings of the f(X) as a function\nof its arguments provides a comprehensive summary of its dependence on\nthe joint values of the input variables.\nUnfortunately, such visualization is limited to low-dimensional views.\nWe can easily display functions of one or two arguments, either continuous\nor discrete (or mixed), in a variety of di\ufb00erent ways; this book is \ufb01lled\nwith such displays. Functions of slightly higher dimensions can be plotted\nby conditioning on particular sets of values of all but one or two of the\narguments, producing a trellis of plots (Becker et al., 1996).1\nFor more than two or three variables, viewing functions of the corre-\nsponding higher-dimensional arguments is more di\ufb03cult. A useful alterna-\ntive can sometimes be to view a collection of plots, each one of which shows\nthe partial dependence of the approximation f(X) on a selected small sub-\nset of the input variables. Although such a collection can seldom provide a\ncomprehensive depiction of the approximation, it can often produce helpful\nclues, especially when f(x) is dominated by low-order interactions (10.40).\nConsider the subvector XSof\u2113 < pof the input predictor variables XT=\n(X1,X2,... ,X p), indexed by S \u2282 { 1,2,... ,p }. LetCbe the complement\nset, with S \u222a C ={1,2,... ,p }. A general function f(X) will in principle\ndepend on all of the input variables: f(X) =f(XS,XC). One way to de\ufb01ne\nthe average or partial dependence of f(X) onXSis\nfS(XS) = E XCf(XS,XC). (10.47)\nThis is a marginal average of f, and can serve as a useful description of the\ne\ufb00ect of the chosen subset on f(X) when, for example, the variables in XS\ndo not have strong interactions with those in XC.\nPartial dependence functions can be used to interpret the results of any\n\u201cblack box\u201d learning method. They can be estimated by\n\u00affS(XS) =1\nNN/summationdisplay\ni=1f(XS,xiC), (10.48)\nwhere {x1C,x2C,... ,x NC}are the values of XCoccurring in the training\ndata. This requires a pass over the data for each set of joint values of XSfor\nwhich \u00affS(XS) is to be evaluated. This can be computationally intensive,\n1lattice in R.", "388": "370 10. Boosting and Additive Trees\neven for moderately sized data sets. Fortunately with decision trees, \u00affS(XS)\n(10.48) can be rapidly computed from the tree itself without reference to\nthe data (Exercise 10.11).\nIt is important to note that partial dependence functions de\ufb01ned in\n(10.47) represent the e\ufb00ect of XSonf(X) after accounting for the (av-\nerage) e\ufb00ects of the other variables XConf(X). They are notthe e\ufb00ect\nofXSonf(X)ignoring the e\ufb00ects of XC. The latter is given by the con-\nditional expectation\n\u02dcfS(XS) = E( f(XS,XC)|XS), (10.49)\nand is the best least squares approximation to f(X) by a function of XS\nalone. The quantities \u02dcfS(XS) and \u00affS(XS) will be the same only in the\nunlikely event that XSandXCare independent. For example, if the e\ufb00ect\nof the chosen variable subset happens to be purely additive,\nf(X) =h1(XS) +h2(XC). (10.50)\nThen (10.47) produces the h1(XS) up to an additive constant. If the e\ufb00ect\nis purely multiplicative,\nf(X) =h1(XS)\u2264h2(XC), (10.51)\nthen (10.47) produces h1(XS) up to a multiplicative constant factor. On\nthe other hand, (10.49) will not produce h1(XS) in either case. In fact,\n(10.49) can produce strong e\ufb00ects on variable subsets for which f(X) has\nno dependence at all.\nViewing plots of the partial dependence of the boosted-tree approxima-\ntion (10.28) on selected variables subsets can help to provide a qualitative\ndescription of its properties. Illustrations are shown in Sections 10.8 and\n10.14. Owing to the limitations of computer graphics, and human percep-\ntion, the size of the subsets XSmust be small ( l\u22481,2,3). There are of\ncourse a large number of such subsets, but only those chosen from among\nthe usually much smaller set of highly relevant predictors are likely to be\ninformative. Also, those subsets whose e\ufb00ect on f(X) is approximately\nadditive (10.50) or multiplicative (10.51) will be most revealing.\nForK-class classi\ufb01cation, there are Kseparate models (10.44), one for\neach class. Each one is related to the respective probabilities (10.21) thro ugh\nfk(X) = log pk(X)\u22121\nKK/summationdisplay\nl=1logpl(X). (10.52)\nThus each fk(X) is a monotone increasing function of its respective prob-\nability on a logarithmic scale. Partial dependence plots of each respective\nfk(X) (10.44) on its most relevant predictors (10.45) can help reveal how\nthe log-odds of realizing that class depend on the respective input variables.", "389": "10.14 Illustrations 371\n10.14 Illustrations\nIn this section we illustrate gradient boosting on a number of larger data sets,\nusing di\ufb00erent loss functions as appropriate.\n10.14.1 California Housing\nThis data set (Pace and Barry, 1997) is available from the Carnegie-Mellon\nStatLib repository2. It consists of aggregated data from each of 20,460\nneighborhoods (1990 census block groups) in California. The response vari-\nableYis the median house value in each neighborhood measured in units of\n$100,000. The predictor variables are demographics such as median income\nMedInc , housing density as re\ufb02ected by the number of houses House, and the\naverage occupancy in each house AveOccup . Also included as predictors are\nthe location of each neighborhood ( longitude andlatitude ), and several\nquantities re\ufb02ecting the properties of the houses in the neighborhood: av-\nerage number of rooms AveRooms and bedrooms AveBedrms . There are thus\na total of eight predictors, all numeric.\nWe \ufb01t a gradient boosting model using the MART procedure, with J= 6\nterminal nodes, a learning rate (10.41) of \u03bd= 0.1, and the Huber loss\ncriterion for predicting the numeric response. We randomly divided the\ndataset into a training set (80%) and a test set (20%).\nFigure 10.13 shows the average absolute error\nAAE = E|y\u2212\u02c6fM(x)| (10.53)\nas a function for number of iterations Mon both the training data and test\ndata. The test error is seen to decrease monotonically with increasing M,\nmore rapidly during the early stages and then leveling o\ufb00 to being nearly\nconstant as iterations increase. Thus, the choice of a particular value of M\nis not critical, as long as it is not too small. This tends to be the case in\nmany applications. The shrinkage strategy (10.41) tends to eliminate the\nproblem of over\ufb01tting, especially for larger data sets.\nThe value of AAE after 800 iterations is 0.31. This can be compared to\nthat of the optimal constant predictor median {yi}which is 0.89. In terms of\nmore familiar quantities, the squared multiple correlation coe\ufb03cient of t his\nmodel is R2= 0.84. Pace and Barry (1997) use a sophisticated spatial auto-\nregression procedure, where prediction for each neighborhood is based on\nmedian house values in nearby neighborhoods, using the other predictors as\ncovariates. Experimenting with transformations they achieved R2= 0.85,\npredicting log Y. Using log Yas the response the corresponding value for\ngradient boosting was R2= 0.86.\n2http://lib.stat.cmu.edu.", "390": "372 10. Boosting and Additive Trees\n0 200 400 600 8000.0 0.2 0.4 0.6 0.8\nIterations MAbsolute ErrorTraining and Test Absolute Error\nTrain Error\nTest Error\nFIGURE 10.13. Average-absolute error as a function of number of iterations\nfor the California housing data.\nFigure 10.14 displays the relative variable importances for each of the\neight predictor variables. Not surprisingly, median income in the neigh-\nborhood is the most relevant predictor. Longitude, latitude, and average\noccupancy all have roughly half the relevance of income, whereas the others\nare somewhat less in\ufb02uential.\nFigure 10.15 shows single-variable partial dependence plots on the most\nrelevant nonlocation predictors. Note that the plots are not strictly smoot h.\nThis is a consequence of using tree-based models. Decision trees produce\ndiscontinuous piecewise constant models (10.25). This carries over to sums\nof trees (10.28), with of course many more pieces. Unlike most of the meth-\nods discussed in this book, there is no smoothness constraint imposed on\nthe result. Arbitrarily sharp discontinuities can be modeled. The fact that\nthese curves generally exhibit a smooth trend is because that is what is\nestimated to best predict the response for this problem. This is often the\ncase.\nThe hash marks at the base of each plot delineate the deciles of the\ndata distribution of the corresponding variables. Note that here the data\ndensity is lower near the edges, especially for larger values. This causes the\ncurves to be somewhat less well determined in those regions. The vertical\nscales of the plots are the same, and give a visual comparison of the relativ e\nimportance of the di\ufb00erent variables.\nThe partial dependence of median house value on median income is\nmonotonic increasing, being nearly linear over the main body of data. House\nvalue is generally monotonic decreasing with increasing average occupancy,\nexcept perhaps for average occupancy rates less than one. Median house", "391": "10.14 Illustrations 373\nMedIncLongitudeAveOccupLatitudeHouseAgeAveRoomsAveBedrmsPopulation\n0 20 40 60 80 100\nRelative importance\nFIGURE 10.14. Relative importance of the predictors for the California hous ing\ndata.\nvalue has a nonmonotonic partial dependence on average number of rooms.\nIt has a minimum at approximately three rooms and is increasing both for\nsmaller and larger values.\nMedian house value is seen to have a very weak partial dependence on\nhouse age that is inconsistent with its importance ranking (Figure 10.14) .\nThis suggests that this weak main e\ufb00ect may be masking stronger interac-\ntion e\ufb00ects with other variables. Figure 10.16 shows the two-variable part ial\ndependence of housing value on joint values of median age and average oc-\ncupancy. An interaction between these two variables is apparent. For values\nof average occupancy greater than two, house value is nearly independent\nof median age, whereas for values less than two there is a strong dependence\non age.\nFigure 10.17 shows the two-variable partial dependence of the \ufb01tted\nmodel on joint values of longitude and latitude, displayed as a shaded\ncontour plot. There is clearly a very strong dependence of median house\nvalue on the neighborhood location in California. Note that Figure 10. 17 is\nnota plot of house value versus location ignoring the e\ufb00ects of the other\npredictors (10.49). Like all partial dependence plots, it represents the e\ufb00ect\nof location after accounting for the e\ufb00ects of the other neighborhood and\nhouse attributes (10.47). It can be viewed as representing an extra premium\none pays for location. This premium is seen to be relatively large near the\nPaci\ufb01c coast especially in the Bay Area and Los Angeles\u2013San Diego re-", "392": "374 10. Boosting and Additive Trees\nMedIncPartial Dependence\n2 4 6 8 10-0.5 0.0 0.5 1.0 1.5 2.0\nAveOccupPartial Dependence\n2 3 4 5-1.0 -0.5 0.0 0.5 1.0 1.5\nHouseAgePartial Dependence\n10 20 30 40 50-1.0 -0.5 0.0 0.5 1.0\nAveRoomsPartial Dependence\n4 6 8 10-1.0 -0.5 0.0 0.5 1.0 1.5\nFIGURE 10.15. Partial dependence of housing value on the nonlocation vari-\nables for the California housing data. The red ticks at the base of the plot are\ndeciles of the input variables.\n2\n3\n4\n510203040500.00.51.0\nAveOccupHouseAge\nFIGURE 10.16. Partial dependence of house value on median age and aver-\nage occupancy. There appears to be a strong interaction e\ufb00ect be tween these two\nvariables.", "393": "10.14 Illustrations 375\n\u2212124 \u2212122 \u2212120 \u2212118 \u2212116 \u221211434 36 38 40 42\nLongitudeLatitude\n\u22121.0\u22120.5 0.0 0.5 1.0\nFIGURE 10.17. Partial dependence of median house value on location in Cal-\nifornia. One unit is $100,000, at1990prices, and the values plotted are relative\nto the overall median of $180,000.\ngions. In the northern, central valley, and southeastern desert regions of\nCalifornia, location costs considerably less.\n10.14.2 New Zealand Fish\nPlant and animal ecologists use regression models to predict species pres-\nence, abundance and richness as a function of environmental variables.\nAlthough for many years simple linear and parametric models were popu-\nlar, recent literature shows increasing interest in more sophisticated mod-\nels such as generalized additive models (Section 9.1, GAM), multivariate\nadaptive regression splines (Section 9.4, MARS) and boosted regression\ntrees (Leathwick et al., 2005; Leathwick et al., 2006). Here we model the", "394": "376 10. Boosting and Additive Trees\npresence and abundance of the Black Oreo Dory , a marine \ufb01sh found in the\noceanic waters around New Zealand.3\nFigure 10.18 shows the locations of 17,000 trawls (deep-water net \ufb01shing,\nwith a maximum depth of 2km), and the red points indicate those 2353\ntrawls for which the Black Oreo was present, one of over a hundred species\nregularly recorded. The catch size in kg for each species was recorded for\neach trawl. Along with the species catch, a number of environmental mea-\nsurements are available for each trawl. These include the average depth of\nthe trawl ( AvgDepth ), and the temperature and salinity of the water. Since\nthe latter two are strongly correlated with depth, Leathwick et al. (2006)\nderived instead TempResid andSalResid , the residuals obtained when these\ntwo measures are adjusted for depth (via separate non-parametric regres-\nsions).SSTGrad is a measure of the gradient of the sea surface temperature,\nandChlais a broad indicator of ecosytem productivity via satellite-image\nmeasurements. SusPartMatter provides a measure of suspended particulate\nmatter, particularly in coastal waters, and is also satellite derived.\nThe goal of this analysis is to estimate the probability of \ufb01nding Black\nOreo in a trawl, as well as the expected catch size, standardized to take\ninto account the e\ufb00ects of variation in trawl speed and distance, as well\nas the mesh size of the trawl net. The authors used logistic regression\nfor estimating the probability. For the catch size, it might seem natural\nto assume a Poisson distribution and model the log of the mean count,\nbut this is often not appropriate because of the excessive number of zeros.\nAlthough specialized approaches have been developed, such as the zero-\nin\ufb02ated Poisson (Lambert, 1992), they chose a simpler approach. If Yis\nthe (non-negative) catch size,\nE(Y|X) = E( Y|Y >0,X)\u2264Pr(Y >0|X). (10.54)\nThe second term is estimated by the logistic regression, and the \ufb01rst term\ncan be estimated using only the 2353 trawls with a positive catch.\nFor the logistic regression the authors used a gradient boosted model\n(GBM)4with binomial deviance loss function, depth-10 trees, and a shrink-\nage factor \u03bd= 0.025. For the positive-catch regression, they modeled\nlog(Y) using a GBM with squared-error loss (also depth-10 trees, but\n\u03bd= 0.01), and un-logged the predictions. In both cases they used 10-fold\ncross-validation for selecting the number of terms, as well as the shrinkage\nfactor.\n3The models, data, and maps shown here were kindly provided by Dr John Leathwick\nof the National Institute of Water and Atmospheric Research in New Zealand, and Dr\nJane Elith, School of Botany, University of Melbourne. The co llection of the research\ntrawl data took place from 1979\u20132005, and was funded by the Ne w Zealand Ministry of\nFisheries.\n4Version 1.5-7 of package gbmin R, ver. 2.2.0.", "395": "10.14 Illustrations 377\nFIGURE 10.18. Map of New Zealand and its surrounding exclusive economic\nzone, showing the locations of 17,000 trawls (small blue dots) t aken between 1979\nand 2005. The red points indicate trawls for which the species Black Oreo Dory\nwere present.", "396": "378 10. Boosting and Additive Trees\n0 500 1000 15000.24 0.26 0.28 0.30 0.32 0.34\nNumber of TreesMean DevianceGBM Test\nGBM CV\nGAM Test\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nSpecificitySensitivity\nAUC\nGAM 0.97\nGBM 0.98\nFIGURE 10.19. The left panel shows the mean deviance as a function of the\nnumber of trees for the GBM logistic regression model \ufb01t to the p resence/absence\ndata. Shown are 10-fold cross-validation on the training data ( and1\u00d7s.e. bars),\nand test deviance on the test data. Also shown for comparison is the test deviance\nusing a GAM model with 8d ffor each term. The right panel shows ROC curves\non the test data for the chosen GBM model (vertical line in left plot) and the\nGAM model.\nFigure 10.19 (left panel) shows the mean binomial deviance for the se-\nquence of GBM models, both for 10-fold CV and test data. There is a mod-\nest improvement over the performance of a GAM model, \ufb01t using smoothing\nsplines with 8 degrees-of-freedom (df) per term. The right panel shows the\nROC curves (see Section 9.2.5) for both models, which measures predictive\nperformance. From this point of view, the performance looks very simi-\nlar, with GBM perhaps having a slight edge as summarized by the AUC\n(area under the curve). At the point of equal sensitivity/speci\ufb01city, GBM\nachieves 91%, and GAM 90%.\nFigure 10.20 summarizes the contributions of the variables in the logistic\nGBM \ufb01t. We see that there is a well-de\ufb01ned depth range over which Black\nOreo are caught, with much more frequent capture in colder waters. We do\nnot give details of the quantitative catch model; the important variabl es\nwere much the same.\nAll the predictors used in these models are available on a \ufb01ne geographi-\ncal grid; in fact they were derived from environmental atlases, satellite im -\nages and the like\u2014see Leathwick et al. (2006) for details. This also means\nthat predictions can be made on this grid, and imported into GIS mapping\nsystems. Figure 10.21 shows prediction maps for both presence and catch\nsize, with both standardized to a common set of trawl conditions; since the\npredictors vary in a continuous fashion with geographical location, so do\nthe predictions.", "397": "10.14 Illustrations 379\nOrbVelSpeedDistanceDisOrgMatterCodendSizePentadeTidalCurrSlopeChlaCase2SSTGradSalResidSusPartMatterAvgDepthTempResid\nRelative influence0 10 25 \u22124 0 2 4 6\u22127 \u22125 \u22123 \u22121\nTempResidf(TempResid)\n0 500 1000 2000\u22126 \u22124 \u22122\nAvgDepthf(AvgDepth)\n0 5 10 15\u22127 \u22125 \u22123\nSusPartMatterf(SusPartMatter)\n\u22120.8 \u22120.4 0.0 0.4\u22127 \u22125 \u22123 \u22121\nSalResidf(SalResid)\n0.00 0.05 0.10 0.15\u22127 \u22125 \u22123 \u22121\nSSTGradf(SSTGrad)\nFIGURE 10.20. The top-left panel shows the relative in\ufb02uence computed from\nthe GBM logistic regression model. The remaining panels show th e partial de-\npendence plots for the leading \ufb01ve variables, all plotted on the s ame scale for\ncomparison.\nBecause of their ability to model interactions and automatically select\nvariables, as well as robustness to outliers and missing data, GBM models\nare rapidly gaining popularity in this data-rich and enthusiastic community .\n10.14.3 Demographics Data\nIn this section we illustrate gradient boosting on a multiclass classi\ufb01ca -\ntion problem, using MART. The data come from 9243 questionnaires \ufb01lled\nout by shopping mall customers in the San Francisco Bay Area (Impact\nResources, Inc., Columbus, OH). Among the questions are 14 concerning\ndemographics. For this illustration the goal is to predict occupation us-\ning the other 13 variables as predictors, and hence identify demographic\nvariables that discriminate between di\ufb00erent occupational categories. We\nrandomly divided the data into a training set (80%) and test set (20%),\nand used J= 6 node trees with a learning rate \u03bd= 0.1.\nFigure 10.22 shows the K= 9 occupation class values along with their\ncorresponding error rates. The overall error rate is 42.5%, which can be\ncompared to the null rate of 69% obtained by predicting the most numerous", "398": "380 10. Boosting and Additive Trees\nFIGURE 10.21. Geological prediction maps of the presence probability (lef t\nmap) and catch size (right map) obtained from the gradient boost ed models.\nclassProf/Man (Professional/Managerial). The four best predicted classes\nare seen to be Retired ,Student ,Prof/Man , andHomemaker .\nFigure 10.23 shows the relative predictor variable importances as aver-\naged over all classes (10.46). Figure 10.24 displays the individual relati ve\nimportance distributions (10.45) for each of the four best predicted classes.\nOne sees that the most relevant predictors are generally di\ufb00erent for each\nrespective class. An exception is agewhich is among the three most relevant\nfor predicting Retired ,Student , andProf/Man .\nFigure 10.25 shows the partial dependence of the log-odds (10.52) on age\nfor these three classes. The abscissa values are ordered codes for respective\nequally spaced age intervals. One sees that after accounting for the contri-\nbutions of the other variables, the odds of being retired are higher for older\npeople, whereas the opposite is the case for being a student. The odds of\nbeing professional/managerial are highest for middle-aged people. These\nresults are of course not surprising. They illustrate that inspecting partial\ndependences separately for each class can lead to sensible results.\nBibliographic Notes\nSchapire (1990) developed the \ufb01rst simple boosting procedure in the PAC\nlearning framework (Valiant, 1984; Kearns and Vazirani, 1994). Schapire", "399": "10.14 Illustrations 381\nSalesUnemployedMilitaryClericalLaborHomemakerProf/ManRetiredStudent\n0.0 0.2 0.4 0.6 0.8 1.0\nError RateOverall Error Rate = 0.425\nFIGURE 10.22. Error rate for each occupation in the demographics data.\nageincomeeduhsld-statmar-dlincsexethnicmar-stattyp-homelangnum-hsldchildrenyrs-BA\n0 20 40 60 80 100\nRelative Importance\nFIGURE 10.23. Relative importance of the predictors as averaged over all\nclasses for the demographics data.", "400": "382 10. Boosting and Additive Trees\nagemar-dlincsexethnicincomehsld-statmar-statlangtyp-homechildrenedunum-hsldyrs-BA\n0 20 40 60 80 100\nRelative ImportanceClass =  Retired\nhsld-statageincomemar-stateduethnicnum-hsldtyp-homesexmar-dlinclangyrs-BAchildren\n0 20 40 60 80 100\nRelative ImportanceClass =  Student\neduincomeagemar-dlincethnichsld-stattyp-homesexnum-hsldlangmar-statyrs-BAchildren\n0 20 40 60 80 100\nRelative ImportanceClass =  Prof/Man\nsexmar-dlincchildrenethnicnum-hsldedumar-statlangtyp-homeincomeagehsld-statyrs-BA\n0 20 40 60 80 100\nRelative ImportanceClass =  Homemaker\nFIGURE 10.24. Predictor variable importances separately for each of the fo ur\nclasses with lowest error rate for the demographics data.", "401": "10.14 Illustrations 383\nagePartial Dependence\n1 2 3 4 5 6 70 1 2 3 4Retired\nagePartial Dependence\n1 2 3 4 5 6 7-2 -1 0 1 2Student\nagePartial Dependence\n1 2 3 4 5 6 7-2 -1 0 1 2Prof/Man\nFIGURE 10.25. Partial dependence of the odds of three di\ufb00erent occupations\non age, for the demographics data.\nshowed that a weak learner could always improve its performance by train-\ning two additional classi\ufb01ers on \ufb01ltered versions of the input data stream.\nA weak learner is an algorithm for producing a two-class classi\ufb01er with\nperformance guaranteed (with high probability) to be signi\ufb01cantly better\nthan a coin-\ufb02ip. After learning an initial classi\ufb01er G1on the \ufb01rst Ntraining\npoints,\n\u2022G2is learned on a new sample of Npoints, half of which are misclas-\nsi\ufb01ed by G1;\n\u2022G3is learned on Npoints for which G1andG2disagree;\n\u2022the boosted classi\ufb01er is GB=majority vote (G1,G2,G3).\nSchapire\u2019s \u201cStrength of Weak Learnability\u201d theorem proves that GBhas\nimproved performance over G1.\nFreund (1995) proposed a \u201cboost by majority\u201d variation which combined\nmany weak learners simultaneously and improved the performance of the\nsimple boosting algorithm of Schapire. The theory supporting both of these", "402": "384 10. Boosting and Additive Trees\nalgorithms requires the weak learner to produce a classi\ufb01er with a \ufb01xed\nerror rate. This led to the more adaptive and realistic AdaBoost (Freund\nand Schapire, 1996a) and its o\ufb00spring, where this assumption was dropped.\nFreund and Schapire (1996a) and Schapire and Singer (1999) provide\nsome theory to support their algorithms, in the form of upper bounds on\ngeneralization error. This theory has evolved in the computational learning\ncommunity, initially based on the concepts of PAC learning. Other theo-\nries attempting to explain boosting come from game theory (Freund and\nSchapire, 1996b; Breiman, 1999; Breiman, 1998), and VC theory (Schapire\net al., 1998). The bounds and the theory associated with the AdaBoost\nalgorithms are interesting, but tend to be too loose to be of practical im-\nportance. In practice, boosting achieves results far more impressive than\nthe bounds would imply. Schapire (2002) and Meir and R\u00a8 atsch (2003) give\nuseful overviews more recent than the \ufb01rst edition of this book.\nFriedman et al. (2000) and Friedman (2001) form the basis for our expo-\nsition in this chapter. Friedman et al. (2000) analyze AdaBoost statist ically,\nderive the exponential criterion, and show that it estimates the log-odds\nof the class probability. They propose additive tree models, the right-sized\ntrees and ANOVA representation of Section 10.11, and the multiclass logit\nformulation. Friedman (2001) developed gradient boosting and shrinkage\nfor classi\ufb01cation and regression, while Friedman (1999) explored stochast ic\nvariants of boosting. Mason et al. (2000) also embraced a gradient appro ach\nto boosting. As the published discussions of Friedman et al. (2000) shows,\nthere is some controversy about how and why boosting works.\nSince the publication of the \ufb01rst edition of this book, these debates have\ncontinued, and spread into the statistical community with a series of papers\non consistency of boosting (Jiang, 2004; Lugosi and Vayatis, 2004; Zhang\nand Yu, 2005; Bartlett and Traskin, 2007). Mease and Wyner (2008),\nthrough a series of simulation examples, challenge some of our interpre-\ntations of boosting; our response (Friedman et al., 2008a) puts most of\nthese objections to rest. A recent survey by B\u00a8 uhlmann and Hothorn (2007)\nsupports our approach to boosting.\nExercises\nEx. 10.1 Derive expression (10.12) for the update parameter in AdaBoost.\nEx. 10.2 Prove result (10.16), that is, the minimizer of the population\nversion of the AdaBoost criterion, is one-half of the log odds.\nEx. 10.3 Show that the marginal average (10.47) recovers additive and\nmultiplicative functions (10.50) and (10.51), while the conditional expec-\ntation (10.49) does not.", "403": "Exercises 385\nEx. 10.4\n(a) Write a program implementing AdaBoost with trees.\n(b) Redo the computations for the example of Figure 10.2. Plot the train-\ning error as well as test error, and discuss its behavior.\n(c) Investigate the number of iterations needed to make the test error\n\ufb01nally start to rise.\n(d) Change the setup of this example as follows: de\ufb01ne two classes, with\nthe features in Class 1 being X1,X2,... ,X 10, standard indepen-\ndent Gaussian variates. In Class 2, the features X1,X2,... ,X 10are\nalso standard independent Gaussian, but conditioned on the event/summationtext\njX2\nj>12. Now the classes have signi\ufb01cant overlap in feature space.\nRepeat the AdaBoost experiments as in Figure 10.2 and discuss the\nresults.\nEx. 10.5 Multiclass exponential loss (Zhu et al., 2005). For a K-class clas-\nsi\ufb01cation problem, consider the coding Y= (Y1,... ,Y K)Twith\nYk=/braceleftbigg1, ifG=Gk\n\u22121\nK\u22121,otherwise .(10.55)\nLetf= (f1,... ,f K)Twith/summationtextK\nk=1fk= 0, and de\ufb01ne\nL(Y,f) = exp/parenleftbigg\n\u22121\nKYTf/parenrightbigg\n. (10.56)\n(a) Using Lagrange multipliers, derive the population minimizer f\u2217of\nE(Y,f), subject to the zero-sum constraint, and relate these to the\nclass probabilities.\n(b) Show that a multiclass boosting using this loss function leads to a\nreweighting algorithm similar to Adaboost, as in Section 10.4.\nEx. 10.6 McNemar test (Agresti, 1996). We report the test error rates on\nthe spam data to be 5.5% for a generalized additive model (GAM), and\n4.5% for gradient boosting (GBM), with a test sample of size 1536.\n(a) Show that the standard error of these estimates is about 0.6%.\nSince the same test data are used for both methods, the error rates are\ncorrelated, and we cannot perform a two-sample t-test. We can compare\nthe methods directly on each test observation, leading to the summary\nGBM\nGAM Correct Error\nCorrect 1434 18\nError 33 51", "404": "386 10. Boosting and Additive Trees\nThe McNemar test focuses on the discordant errors, 33 vs. 18.\n(b) Conduct a test to show that GAM makes signi\ufb01cantly more errors\nthan gradient boosting, with a two-sided p-value of 0 .036.\nEx. 10.7 Derive expression (10.32).\nEx. 10.8 Consider a K-class problem where the targets yikare coded as\n1 if observation iis in class kand zero otherwise. Suppose we have a\ncurrent model fk(x), k= 1,... ,K , with/summationtextK\nk=1fk(x) = 0 (see (10.21) in\nSection 10.6). We wish to update the model for observations in a region R\nin predictor space, by adding constants fk(x) +\u03b3k, with \u03b3K= 0.\n(a) Write down the multinomial log-likelihood for this problem, and its\n\ufb01rst and second derivatives.\n(b) Using only the diagonal of the Hessian matrix in (1), and starting\nfrom\u03b3k= 0\u2200k, show that a one-step approximate Newton update\nfor\u03b3kis\n\u03b31\nk=/summationtext\nxi\u2208R(yik\u2212pik)/summationtext\nxi\u2208Rpik(1\u2212pik), k= 1,... ,K \u22121, (10.57)\nwhere pik= exp( fk(xi))/(/summationtextK\n\u2113=1f\u2113(xi)).\n(c) We prefer our update to sum to zero, as the current model does. Using\nsymmetry arguments, show that\n\u02c6\u03b3k=K\u22121\nK(\u03b31\nk\u22121\nKK/summationdisplay\n\u2113=1\u03b31\n\u2113), k= 1,... ,K (10.58)\nis an appropriate update, where \u03b31\nkis de\ufb01ned as in (10.57) for all\nk= 1,... ,K .\nEx. 10.9 Consider a K-class problem where the targets yikare coded as\n1 if observation iis in class kand zero otherwise. Using the multinomial\ndeviance loss function (10.22) and the symmetric logistic transform, use\nthe arguments leading to the gradient boosting Algorithm 10.3 to derive\nAlgorithm 10.4. Hint: See exercise 10.8 for step 2(b)iii.\nEx. 10.10 Show that for K= 2 class classi\ufb01cation, only one tree needs to\nbe grown at each gradient-boosting iteration.\nEx. 10.11 Show how to compute the partial dependence function fS(XS)\nin (10.47) e\ufb03ciently.\nEx. 10.12 Referring to (10.49), let S={1}andC={2}, with f(X1,X2) =\nX1. Assume X1andX2are bivariate Gaussian, each with mean zero, vari-\nance one, and E( X1,X2) =\u03c1. Show that E(f(X1,X2|X2) =\u03c1X2, even\nthough fis not a function of X2.", "405": "Exercises 387\nAlgorithm 10.4 Gradient Boosting for K-class Classi\ufb01cation.\n1. Initialize fk0(x) = 0, k= 1,2,... ,K .\n2. For m=1 to M:\n(a) Set\npk(x) =efk(x)\n/summationtextK\n\u2113=1ef\u2113(x), k= 1,2,... ,K.\n(b) For k= 1 to K:\ni. Compute rikm=yik\u2212pk(xi), i= 1,2,... ,N .\nii. Fit a regression tree to the targets rikm, i= 1,2,... ,N ,\ngiving terminal regions Rjkm, j= 1,2,... ,J m.\niii. Compute\n\u03b3jkm=K\u22121\nK/summationtext\nxi\u2208Rjkmrikm/summationtext\nxi\u2208Rjkm|rikm|(1\u2212 |rikm|), j= 1,2,... ,J m.\niv. Update fkm(x) =fk,m\u22121(x) +/summationtextJm\nj=1\u03b3jkmI(x\u2208Rjkm).\n3. Output \u02c6fk(x) =fkM(x), k= 1,2,... ,K .", "406": "388 10. Boosting and Additive Trees", "407": "This is page 389\nPrinter: Opaque this\n11\nNeural Networks\n11.1 Introduction\nIn this chapter we describe a class of learning methods that was developed\nseparately in di\ufb00erent \ufb01elds\u2014statistics and arti\ufb01cial intelligence\u2014based\non essentially identical models. The central idea is to extract linear com-\nbinations of the inputs as derived features, and then model the target as\na nonlinear function of these features. The result is a powerful learning\nmethod, with widespread applications in many \ufb01elds. We \ufb01rst discuss the\nprojection pursuit model, which evolved in the domain of semiparamet-\nric statistics and smoothing. The rest of the chapter is devoted to neural\nnetwork models.\n11.2 Projection Pursuit Regression\nAs in our generic supervised learning problem, assume we have an input\nvector Xwithpcomponents, and a target Y. Let\u03c9m, m= 1,2,... ,M, be\nunitp-vectors of unknown parameters. The projection pursuit regression\n(PPR) model has the form\nf(X) =M/summationdisplay\nm=1gm(\u03c9T\nmX). (11.1)\nThis is an additive model, but in the derived features Vm=\u03c9T\nmXrather\nthan the inputs themselves. The functions gmare unspeci\ufb01ed and are esti-", "408": "390 Neural Networks\ng(V)\nX1X2g(V)\nX1X2\nFIGURE 11.1. Perspective plots of two ridge functions.\n(Left:) g(V) = 1/[1 + exp( \u22125(V\u22120.5))], where V= (X1+X2)/\u221a\n2.\n(Right:) g(V) = (V+ 0.1) sin(1 /(V/3 + 0.1)), where V=X1.\nmated along with the directions \u03c9musing some \ufb02exible smoothing method\n(see below).\nThe function gm(\u03c9T\nmX) is called a ridge function in IRp. It varies only\nin the direction de\ufb01ned by the vector \u03c9m. The scalar variable Vm=\u03c9T\nmX\nis the projection of Xonto the unit vector \u03c9m, and we seek \u03c9mso that\nthe model \ufb01ts well, hence the name \u201cprojection pursuit.\u201d Figure 11.1 shows\nsome examples of ridge functions. In the example on the left \u03c9= (1/\u221a\n2)(1,1)T,\nso that the function only varies in the direction X1+X2. In the example\non the right, \u03c9= (1,0).\nThe PPR model (11.1) is very general, since the operation of forming\nnonlinear functions of linear combinations generates a surprisingly large\nclass of models. For example, the product X1\u2264X2can be written as [( X1+\nX2)2\u2212(X1\u2212X2)2]/4, and higher-order products can be represented simi-\nlarly.\nIn fact, if Mis taken arbitrarily large, for appropriate choice of gmthe\nPPR model can approximate any continuous function in IRparbitrarily\nwell. Such a class of models is called a universal approximator . However\nthis generality comes at a price. Interpretation of the \ufb01tted model is usually\ndi\ufb03cult, because each input enters into the model in a complex and multi-\nfaceted way. As a result, the PPR model is most useful for prediction, and\nnot very useful for producing an understandable model for the data. The\nM= 1 model, known as the single index model in econometrics, is an\nexception. It is slightly more general than the linear regression model, and\no\ufb00ers a similar interpretation.\nHow do we \ufb01t a PPR model, given training data ( xi,yi),i= 1,2,... ,N ?\nWe seek the approximate minimizers of the error function\nN/summationdisplay\ni=1/bracketleft\uf8ecigg\nyi\u2212M/summationdisplay\nm=1gm(\u03c9T\nmxi)/bracketright\uf8ecigg2\n(11.2)", "409": "11.2 Projection Pursuit Regression 391\nover functions gmand direction vectors \u03c9m,m= 1,2,... ,M . As in other\nsmoothing problems, we need either explicitly or implicitly to impose com-\nplexity constraints on the gm, to avoid over\ufb01t solutions.\nConsider just one term ( M= 1, and drop the subscript). Given the\ndirection vector \u03c9, we form the derived variables vi=\u03c9Txi. Then we have\na one-dimensional smoothing problem, and we can apply any scatterplot\nsmoother, such as a smoothing spline, to obtain an estimate of g.\nOn the other hand, given g, we want to minimize (11.2) over \u03c9. A Gauss\u2013\nNewton search is convenient for this task. This is a quasi-Newton method,\nin which the part of the Hessian involving the second derivative of gis\ndiscarded. It can be simply derived as follows. Let \u03c9oldbe the current\nestimate for \u03c9. We write\ng(\u03c9Txi)\u2248g(\u03c9T\noldxi) +g\u2032(\u03c9T\noldxi)(\u03c9\u2212\u03c9old)Txi (11.3)\nto give\nN/summationdisplay\ni=1/bracketleftbig\nyi\u2212g(\u03c9Txi)/bracketrightbig2\u2248N/summationdisplay\ni=1g\u2032(\u03c9T\noldxi)2/bracketleftbigg/parenleftbigg\n\u03c9T\noldxi+yi\u2212g(\u03c9T\noldxi)\ng\u2032(\u03c9T\noldxi)/parenrightbigg\n\u2212\u03c9Txi/bracketrightbigg2\n.\n(11.4)\nTo minimize the right-hand side, we carry out a least squares regression\nwith target \u03c9T\noldxi+(yi\u2212g(\u03c9T\noldxi))/g\u2032(\u03c9T\noldxi) on the input xi, with weights\ng\u2032(\u03c9T\noldxi)2and no intercept (bias) term. This produces the updated coef-\n\ufb01cient vector \u03c9new.\nThese two steps, estimation of gand\u03c9, are iterated until convergence.\nWith more than one term in the PPR model, the model is built in a forward\nstage-wise manner, adding a pair ( \u03c9m,gm) at each stage.\nThere are a number of implementation details.\n\u2022Although any smoothing method can in principle be used, it is conve-\nnient if the method provides derivatives. Local regression and smooth-\ning splines are convenient.\n\u2022After each step the gm\u2019s from previous steps can be readjusted using\nthe back\ufb01tting procedure described in Chapter 9. While this may\nlead ultimately to fewer terms, it is not clear whether it improves\nprediction performance.\n\u2022Usually the \u03c9mare not readjusted (partly to avoid excessive compu-\ntation), although in principle they could be as well.\n\u2022The number of terms Mis usually estimated as part of the forward\nstage-wise strategy. The model building stops when the next term\ndoes not appreciably improve the \ufb01t of the model. Cross-validation\ncan also be used to determine M.", "410": "392 Neural Networks\nThere are many other applications, such as density estimation (Friedman\net al., 1984; Friedman, 1987), where the projection pursuit idea can be used.\nIn particular, see the discussion of ICA in Section 14.7 and its relationship\nwith exploratory projection pursuit. However the projection pursuit re-\ngression model has not been widely used in the \ufb01eld of statistics, perhaps\nbecause at the time of its introduction (1981), its computational demands\nexceeded the capabilities of most readily available computers. But it does\nrepresent an important intellectual advance, one that has blossomed in its\nreincarnation in the \ufb01eld of neural networks, the topic of the rest of this\nchapter.\n11.3 Neural Networks\nThe term neural network has evolved to encompass a large class of models\nand learning methods. Here we describe the most widely used \u201cvanilla\u201d neu-\nral net, sometimes called the single hidden layer back-propagation network,\nor single layer perceptron. There has been a great deal of hypesurrounding\nneural networks, making them seem magical and mysterious. As we make\nclear in this section, they are just nonlinear statistical models, much like\nthe projection pursuit regression model discussed above.\nA neural network is a two-stage regression or classi\ufb01cation model, typ-\nically represented by a network diagram as in Figure 11.2. This network\napplies both to regression or classi\ufb01cation. For regression, typically K= 1\nand there is only one output unit Y1at the top. However, these networks\ncan handle multiple quantitative responses in a seamless fashion, so we will\ndeal with the general case.\nForK-class classi\ufb01cation, there are Kunits at the top, with the kth\nunit modeling the probability of class k. There are Ktarget measurements\nYk, k= 1,... ,K , each being coded as a 0 \u22121 variable for the kth class.\nDerived features Zmare created from linear combinations of the inputs,\nand then the target Ykis modeled as a function of linear combinations of\ntheZm,\nZm=\u03c3(\u03b10m+\u03b1T\nmX), m= 1,... ,M,\nTk=\u03b20k+\u03b2T\nkZ, k= 1,... ,K,\nfk(X) =gk(T), k= 1,... ,K,(11.5)\nwhere Z= (Z1,Z2,... ,Z M), and T= (T1,T2,... ,T K).\nThe activation function \u03c3(v) is usually chosen to be the sigmoid \u03c3(v) =\n1/(1 +e\u2212v); see Figure 11.3 for a plot of 1 /(1 +e\u2212v). Sometimes Gaussian\nradial basis functions (Chapter 6) are used for the \u03c3(v), producing what is\nknown as a radial basis function network .\nNeural network diagrams like Figure 11.2 are sometimes drawn with an\nadditional biasunit feeding into every unit in the hidden and output layers.", "411": "11.3 Neural Networks 393\n Y  Y Y 2 1 K\n Z  Z  Z1 Z2 3 m\n X  X Z  Z1 Z2 3\n1  Xp  X p-1  X2  X3M\n X p-1 3 X 2 X 1p Z Y  Y Y\n XK 1 2\n                                                                                                                                                /0/0/0\n/1/1/1\n/0/0/0\n/1/1/1\n/0/0/0\n/1/1/1\n/0/0/0\n/1/1/1\n/0/0/0\n/1/1/1\n/0/0/0\n/1/1/1\nFIGURE 11.2. Schematic of a single hidden layer, feed-forward neural network .\nThinking of the constant \u201c1\u201d as an additional input feature, this bias unit\ncaptures the intercepts \u03b10mand\u03b20kin model (11.5).\nThe output function gk(T) allows a \ufb01nal transformation of the vector of\noutputs T. For regression we typically choose the identity function gk(T) =\nTk. Early work in K-class classi\ufb01cation also used the identity function, but\nthis was later abandoned in favor of the softmax function\ngk(T) =eTk\n/summationtextK\n\u2113=1eT\u2113. (11.6)\nThis is of course exactly the transformation used in the multilogit model\n(Section 4.4), and produces positive estimates that sum to one. In Sec-\ntion 4.2 we discuss other problems with linear activation functions, in par-\nticular potentially severe masking e\ufb00ects.\nThe units in the middle of the network, computing the derived features\nZm, are called hidden units because the values Zmare not directly ob-\nserved. In general there can be more than one hidden layer, as illustrated\nin the example at the end of this chapter. We can think of the Zmas a\nbasis expansion of the original inputs X; the neural network is then a stan-\ndard linear model, or linear multilogit model, using these transformations\nas inputs. There is, however, an important enhancement over the basis-\nexpansion techniques discussed in Chapter 5; here the parameters of the\nbasis functions are learned from the data.", "412": "394 Neural Networks\n-10 -5 0 5 100.0 0.5 1.01/(1 +e\u2212v)\nv\nFIGURE 11.3. Plot of the sigmoid function \u03c3(v) = 1/(1+exp( \u2212v))(red curve),\ncommonly used in the hidden layer of a neural network. Included ar e\u03c3(sv)for\ns=1\n2(blue curve) and s= 10(purple curve). The scale parameter scontrols\nthe activation rate, and we can see that large samounts to a hard activation at\nv= 0. Note that \u03c3(s(v\u2212v0))shifts the activation threshold from 0tov0.\nNotice that if \u03c3is the identity function, then the entire model collapses\nto a linear model in the inputs. Hence a neural network can be thought of\nas a nonlinear generalization of the linear model, both for regression and\nclassi\ufb01cation. By introducing the nonlinear transformation \u03c3, it greatly\nenlarges the class of linear models. In Figure 11.3 we see that the rate of\nactivation of the sigmoid depends on the norm of \u03b1m, and if \u221d\u230aa\u2207\u2308\u230al\u03b1m\u221d\u230aa\u2207\u2308\u230alis very\nsmall, the unit will indeed be operating in the linear part of its activation\nfunction.\nNotice also that the neural network model with one hidden layer has\nexactly the same form as the projection pursuit model described above.\nThe di\ufb00erence is that the PPR model uses nonparametric functions gm(v),\nwhile the neural network uses a far simpler function based on \u03c3(v), with\nthree free parameters in its argument. In detail, viewing the neural network\nmodel as a PPR model, we identify\ngm(\u03c9T\nmX) = \u03b2m\u03c3(\u03b10m+\u03b1T\nmX)\n=\u03b2m\u03c3(\u03b10m+\u221d\u230aa\u2207\u2308\u230al\u03b1m\u221d\u230aa\u2207\u2308\u230al(\u03c9T\nmX)), (11.7)\nwhere \u03c9m=\u03b1m/\u221d\u230aa\u2207\u2308\u230al\u03b1m\u221d\u230aa\u2207\u2308\u230alis the mth unit-vector. Since \u03c3\u03b2,\u03b10,s(v) =\u03b2\u03c3(\u03b10+\nsv) has lower complexity than a more general nonparametric g(v), it is not\nsurprising that a neural network might use 20 or 100 such functions, while\nthe PPR model typically uses fewer terms ( M= 5 or 10, for example).\nFinally, we note that the name \u201cneural networks\u201d derives from the fact\nthat they were \ufb01rst developed as models for the human brain. Each unit\nrepresents a neuron, and the connections (links in Figure 11.2) represent\nsynapses. In early models, the neurons \ufb01red when the total signal passed to\nthat unit exceeded a certain threshold. In the model above, this corresponds", "413": "11.4 Fitting Neural Networks 395\nto use of a step function for \u03c3(Z) and gm(T). Later the neural network was\nrecognized as a useful tool for nonlinear statistical modeling, and for this\npurpose the step function is not smooth enough for optimization. Hence the\nstep function was replaced by a smoother threshold function, the sigmoid\nin Figure 11.3.\n11.4 Fitting Neural Networks\nThe neural network model has unknown parameters, often called weights ,\nand we seek values for them that make the model \ufb01t the training data well.\nWe denote the complete set of weights by \u03b8, which consists of\n{\u03b10m,\u03b1m;m= 1,2,... ,M }M(p+ 1) weights ,\n{\u03b20k,\u03b2k;k= 1,2,... ,K }K(M+ 1) weights .(11.8)\nFor regression, we use sum-of-squared errors as our measure of \ufb01t (error\nfunction)\nR(\u03b8) =K/summationdisplay\nk=1N/summationdisplay\ni=1(yik\u2212fk(xi))2. (11.9)\nFor classi\ufb01cation we use either squared error or cross-entropy (deviance):\nR(\u03b8) =\u2212N/summationdisplay\ni=1K/summationdisplay\nk=1yiklogfk(xi), (11.10)\nand the corresponding classi\ufb01er is G(x) = argmaxkfk(x). With the softmax\nactivation function and the cross-entropy error function, the neural network\nmodel is exactly a linear logistic regression model in the hidden units, and\nall the parameters are estimated by maximum likelihood.\nTypically we don\u2019t want the global minimizer of R(\u03b8), as this is likely\nto be an over\ufb01t solution. Instead some regularization is needed: this is\nachieved directly through a penalty term, or indirectly by early stopping.\nDetails are given in the next section.\nThe generic approach to minimizing R(\u03b8) is by gradient descent, called\nback-propagation in this setting. Because of the compositional form of the\nmodel, the gradient can be easily derived using the chain rule for di\ufb00eren-\ntiation. This can be computed by a forward and backward sweep over the\nnetwork, keeping track only of quantities local to each unit.", "414": "396 Neural Networks\nHere is back-propagation in detail for squared error loss. Let zmi=\n\u03c3(\u03b10m+\u03b1T\nmxi), from (11.5) and let zi= (z1i,z2i,... ,z Mi). Then we have\nR(\u03b8)\u2261N/summationdisplay\ni=1Ri\n=N/summationdisplay\ni=1K/summationdisplay\nk=1(yik\u2212fk(xi))2, (11.11)\nwith derivatives\n\u2202Ri\n\u2202\u03b2km=\u22122(yik\u2212fk(xi))g\u2032\nk(\u03b2T\nkzi)zmi,\n\u2202Ri\n\u2202\u03b1m\u2113=\u2212K/summationdisplay\nk=12(yik\u2212fk(xi))g\u2032\nk(\u03b2T\nkzi)\u03b2km\u03c3\u2032(\u03b1T\nmxi)xi\u2113.(11.12)\nGiven these derivatives, a gradient descent update at the ( r+ 1)st iter-\nation has the form\n\u03b2(r+1)\nkm=\u03b2(r)\nkm\u2212\u03b3rN/summationdisplay\ni=1\u2202Ri\n\u2202\u03b2(r)\nkm,\n\u03b1(r+1)\nm\u2113=\u03b1(r)\nm\u2113\u2212\u03b3rN/summationdisplay\ni=1\u2202Ri\n\u2202\u03b1(r)\nm\u2113,(11.13)\nwhere \u03b3ris the learning rate , discussed below.\nNow write (11.12) as\n\u2202Ri\n\u2202\u03b2km=\u03b4kizmi,\n\u2202Ri\n\u2202\u03b1m\u2113=smixi\u2113.(11.14)\nThe quantities \u03b4kiandsmiare \u201cerrors\u201d from the current model at the\noutput and hidden layer units, respectively. From their de\ufb01nitions, these\nerrors satisfy\nsmi=\u03c3\u2032(\u03b1T\nmxi)K/summationdisplay\nk=1\u03b2km\u03b4ki, (11.15)\nknown as the back-propagation equations . Using this, the updates in (11.13)\ncan be implemented with a two-pass algorithm. In the forward pass , the\ncurrent weights are \ufb01xed and the predicted values \u02c6fk(xi) are computed\nfrom formula (11.5). In the backward pass , the errors \u03b4kiare computed,\nand then back-propagated via (11.15) to give the errors smi. Both sets of\nerrors are then used to compute the gradients for the updates in (11.13),\nvia (11.14).", "415": "11.5 Some Issues in Training Neural Networks 397\nThis two-pass procedure is what is known as back-propagation. It has\nalso been called the delta rule (Widrow and Ho\ufb00, 1960). The computational\ncomponents for cross-entropy have the same form as those for the sum of\nsquares error function, and are derived in Exercise 11.3.\nThe advantages of back-propagation are its simple, local nature. In the\nback propagation algorithm, each hidden unit passes and receives infor-\nmation only to and from units that share a connection. Hence it can be\nimplemented e\ufb03ciently on a parallel architecture computer.\nThe updates in (11.13) are a kind of batch learning , with the parame-\nter updates being a sum over all of the training cases. Learning can also\nbe carried out online\u2014processing each observation one at a time, updat-\ning the gradient after each training case, and cycling through the training\ncases many times. In this case, the sums in equations (11.13) are replaced\nby a single summand. A training epoch refers to one sweep through the\nentire training set. Online training allows the network to handle very large\ntraining sets, and also to update the weights as new observations come in.\nThe learning rate \u03b3rfor batch learning is usually taken to be a con-\nstant, and can also be optimized by a line search that minimizes the error\nfunction at each update. With online learning \u03b3rshould decrease to zero\nas the iteration r\u2192 \u221e. This learning is a form of stochastic approxima-\ntion(Robbins and Munro, 1951); results in this \ufb01eld ensure convergence if\n\u03b3r\u21920,/summationtext\nr\u03b3r=\u221e, and/summationtext\nr\u03b32\nr<\u221e(satis\ufb01ed, for example, by \u03b3r= 1/r).\nBack-propagation can be very slow, and for that reason is usually not\nthe method of choice. Second-order techniques such as Newton\u2019s method\nare not attractive here, because the second derivative matrix of R(the\nHessian) can be very large. Better approaches to \ufb01tting include conjugate\ngradients and variable metric methods. These avoid explicit computation\nof the second derivative matrix while still providing faster convergence.\n11.5 Some Issues in Training Neural Networks\nThere is quite an art in training neural networks. The model is generally\noverparametrized, and the optimization problem is nonconvex and unstable\nunless certain guidelines are followed. In this section we summarize some\nof the important issues.\n11.5.1 Starting Values\nNote that if the weights are near zero, then the operative part of the sigmoid\n(Figure 11.3) is roughly linear, and hence the neural network collapses into\nan approximately linear model (Exercise 11.2). Usually starting values fo r\nweights are chosen to be random values near zero. Hence the model starts\nout nearly linear, and becomes nonlinear as the weights increase. Individual", "416": "398 Neural Networks\nunits localize to directions and introduce nonlinearities where needed. Use\nof exact zero weights leads to zero derivatives and perfect symmetry, and\nthe algorithm never moves. Starting instead with large weights often leads\nto poor solutions.\n11.5.2 Over\ufb01tting\nOften neural networks have too many weights and will over\ufb01t the data at\nthe global minimum of R. In early developments of neural networks, either\nby design or by accident, an early stopping rule was used to avoid over-\n\ufb01tting. Here we train the model only for a while, and stop well before we\napproach the global minimum. Since the weights start at a highly regular-\nized (linear) solution, this has the e\ufb00ect of shrinking the \ufb01nal model toward\na linear model. A validation dataset is useful for determining when to stop,\nsince we expect the validation error to start increasing.\nA more explicit method for regularization is weight decay , which is anal-\nogous to ridge regression used for linear models (Section 3.4.1). We add a\npenalty to the error function R(\u03b8) +\u03bbJ(\u03b8), where\nJ(\u03b8) =/summationdisplay\nkm\u03b22\nkm+/summationdisplay\nm\u2113\u03b12\nm\u2113 (11.16)\nand\u03bb\u22650 is a tuning parameter. Larger values of \u03bbwill tend to shrink\nthe weights toward zero: typically cross-validation is used to estimate \u03bb.\nThe e\ufb00ect of the penalty is to simply add terms 2 \u03b2kmand 2 \u03b1m\u2113to the\nrespective gradient expressions (11.13). Other forms for the penalty have\nbeen proposed, for example,\nJ(\u03b8) =/summationdisplay\nkm\u03b22\nkm\n1 +\u03b22\nkm+/summationdisplay\nm\u2113\u03b12\nm\u2113\n1 +\u03b12\nm\u2113, (11.17)\nknown as the weight elimination penalty. This has the e\ufb00ect of shrinking\nsmaller weights more than (11.16) does.\nFigure 11.4 shows the result of training a neural network with ten hidden\nunits, without weight decay (upper panel) and with weight decay (lower\npanel), to the mixture example of Chapter 2. Weight decay has clearly\nimproved the prediction. Figure 11.5 shows heat maps of the estimated\nweights from the training (grayscale versions of these are called Hinton\ndiagrams. ) We see that weight decay has dampened the weights in both\nlayers: the resulting weights are spread fairly evenly over the ten hidden\nunits.\n11.5.3 Scaling of the Inputs\nSince the scaling of the inputs determines the e\ufb00ective scaling of the weights\nin the bottom layer, it can have a large e\ufb00ect on the quality of the \ufb01nal", "417": "11.5 Some Issues in Training Neural Networks 399\nNeural Network - 10 Units, No Weight Decay\n. . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . .. . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.100\nTest Error:       0.259\nBayes Error:    0.210\nNeural Network - 10 Units, Weight Decay=0.02 \n. .. .. .. .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.160\nTest Error:       0.223\nBayes Error:    0.210\nFIGURE 11.4. A neural network on the mixture example of Chapter 2. The\nupper panel uses no weight decay, and over\ufb01ts the training data. The lower panel\nuses weight decay, and achieves close to the Bayes error rate ( broken purple\nboundary). Both use the softmax activation function and cross- entropy error.", "418": "400 Neural Networks\n1 1\n11\nx1 x1x2 x2y1 y1y2 y2\nz1z1\nz1z1\nz2z2\nz2z2\nz3z3\nz3z3\nz1z1\nz1z1\nz5z5\nz5z5\nz6z6\nz6z6\nz7z7\nz7z7\nz8z8\nz8z8\nz9z9\nz9z9\nz10z10\nz10z10No weight decay Weight decay\nFIGURE 11.5. Heat maps of the estimated weights from the training of neural\nnetworks from Figure 11.4. The display ranges from bright gree n (negative) to\nbright red (positive).\nsolution. At the outset it is best to standardize all inputs to have mean zero\nand standard deviation one. This ensures all inputs are treated equally in\nthe regularization process, and allows one to choose a meaningful range for\nthe random starting weights. With standardized inputs, it is typical to take\nrandom uniform weights over the range [ \u22120.7,+0.7].\n11.5.4 Number of Hidden Units and Layers\nGenerally speaking it is better to have too many hidden units than too few.\nWith too few hidden units, the model might not have enough \ufb02exibility to\ncapture the nonlinearities in the data; with too many hidden units, the\nextra weights can be shrunk toward zero if appropriate regularization is\nused. Typically the number of hidden units is somewhere in the range of\n5 to 100, with the number increasing with the number of inputs and num-\nber of training cases. It is most common to put down a reasonably large\nnumber of units and train them with regularization. Some researchers use\ncross-validation to estimate the optimal number, but this seems unneces-\nsary if cross-validation is used to estimate the regularization parameter .\nChoice of the number of hidden layers is guided by background knowledge\nand experimentation. Each layer extracts features of the input for regres-\nsion or classi\ufb01cation. Use of multiple hidden layers allows construction of\nhierarchical features at di\ufb00erent levels of resolution. An example of the\ne\ufb00ective use of multiple layers is given in Section 11.6.\n11.5.5 Multiple Minima\nThe error function R(\u03b8) is nonconvex, possessing many local minima. As a\nresult, the \ufb01nal solution obtained is quite dependent on the choice of start-", "419": "11.6 Example: Simulated Data 401\ning weights. One must at least try a number of random starting con\ufb01gura-\ntions, and choose the solution giving lowest (penalized) error. Probably a\nbetter approach is to use the average predictions over the collection of net-\nworks as the \ufb01nal prediction (Ripley, 1996). This is preferable to averaging\nthe weights, since the nonlinearity of the model implies that this averaged\nsolution could be quite poor. Another approach is via bagging , which aver-\nages the predictions of networks training from randomly perturbed versions\nof the training data. This is described in Section 8.7.\n11.6 Example: Simulated Data\nWe generated data from two additive error models Y=f(X) +\u03b5:\nSum of sigmoids: Y=\u03c3(aT\n1X) +\u03c3(aT\n2X) +\u03b51;\nRadial: Y=10/productdisplay\nm=1\u03c6(Xm) +\u03b52.\nHereXT= (X1,X2,... ,X p), each Xjbeing a standard Gaussian variate,\nwithp= 2 in the \ufb01rst model, and p= 10 in the second.\nFor the sigmoid model, a1= (3,3), a2= (3,\u22123); for the radial model,\n\u03c6(t) = (1 /2\u03c0)1/2exp(\u2212t2/2). Both \u03b51and\u03b52are Gaussian errors, with\nvariance chosen so that the signal-to-noise ratio\nVar(E( Y|X))\nVar(Y\u2212E(Y|X))=Var(f(X))\nVar(\u03b5)(11.18)\nis 4 in both models. We took a training sample of size 100 and a test sample\nof size 10 ,000. We \ufb01t neural networks with weight decay and various num-\nbers of hidden units, and recorded the average test error E Test(Y\u2212\u02c6f(X))2\nfor each of 10 random starting weights. Only one training set was gen-\nerated, but the results are typical for an \u201caverage\u201d training set. The test\nerrors are shown in Figure 11.6. Note that the zero hidden unit model refers\nto linear least squares regression. The neural network is perfectly suited to\nthe sum of sigmoids model, and the two-unit model does perform the best,\nachieving an error close to the Bayes rate. (Recall that the Bayes rate for\nregression with squared error is the error variance; in the \ufb01gures, we report\ntest error relative to the Bayes error). Notice, however, that with more hid-\nden units, over\ufb01tting quickly creeps in, and with some starting weights the\nmodel does worse than the linear model (zero hidden unit) model. Even\nwith two hidden units, two of the ten starting weight con\ufb01gurations pro-\nduced results no better than the linear model, con\ufb01rming the importance\nof multiple starting values.\nA radial function is in a sense the most di\ufb03cult for the neural net, as it is\nspherically symmetric and with no preferred directions. We see in the right", "420": "402 Neural Networks\n1.0 1.5 2.0 2.5 3.0\n0 1 2 3 4 5 6 7 8 9 10\nNumber of Hidden UnitsTest ErrorSum of Sigmoids\n0 5 10 15 20 25 30\n0 1 2 3 4 5 6 7 8 9 10\nNumber of Hidden UnitsTest ErrorRadial\nFIGURE 11.6. Boxplots of test error, for simulated data example, relative t o\nthe Bayes error (broken horizontal line). True function is a sum of two sigmoids\non the left, and a radial function is on the right. The test error is displayed for\n10di\ufb00erent starting weights, for a single hidden layer neural netwo rk with the\nnumber of units as indicated.\npanel of Figure 11.6 that it does poorly in this case, with the test error\nstaying well above the Bayes error (note the di\ufb00erent vertical scale from\nthe left panel). In fact, since a constant \ufb01t (such as the sample average)\nachieves a relative error of 5 (when the SNR is 4), we see that the neural\nnetworks perform increasingly worse than the mean.\nIn this example we used a \ufb01xed weight decay parameter of 0 .0005, rep-\nresenting a mild amount of regularization. The results in the left panel of\nFigure 11.6 suggest that more regularization is needed with greater num-\nbers of hidden units.\nIn Figure 11.7 we repeated the experiment for the sum of sigmoids model,\nwith no weight decay in the left panel, and stronger weight decay ( \u03bb= 0.1)\nin the right panel. With no weight decay, over\ufb01tting becomes even more\nsevere for larger numbers of hidden units. The weight decay value \u03bb= 0.1\nproduces good results for all numbers of hidden units, and there does not\nappear to be over\ufb01tting as the number of units increase. Finally, Figure 11.8\nshows the test error for a ten hidden unit network, varying the weight decay\nparameter over a wide range. The value 0 .1 is approximately optimal.\nIn summary, there are two free parameters to select: the weight decay \u03bb\nand number of hidden units M. As a learning strategy, one could \ufb01x either\nparameter at the value corresponding to the least constrained model, to\nensure that the model is rich enough, and use cross-validation to choose\nthe other parameter. Here the least constrained values are zero weight decay\nand ten hidden units. Comparing the left panel of Figure 11.7 to Figure\n11.8, we see that the test error is less sensitive to the value of the weight", "421": "11.6 Example: Simulated Data 4031.0 1.5 2.0 2.5 3.0\n0 1 2 3 4 5 6 7 8 9 10\nNumber of Hidden UnitsTest ErrorNo Weight Decay\n1.0 1.5 2.0 2.5 3.0\n0 1 2 3 4 5 6 7 8 9 10\nNumber of Hidden UnitsTest ErrorWeight Decay=0.1\nFIGURE 11.7. Boxplots of test error, for simulated data example, relative t o the\nBayes error. True function is a sum of two sigmoids. The test er ror is displayed\nfor ten di\ufb00erent starting weights, for a single hidden layer neur al network with\nthe number units as indicated. The two panels represent no weight de cay (left)\nand strong weight decay \u03bb= 0.1(right).1.0 1.2 1.4 1.6 1.8 2.0 2.2\n0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14\nWeight Decay ParameterTest ErrorSum of Sigmoids, 10 Hidden Unit Model\nFIGURE 11.8. Boxplots of test error, for simulated data example. True functi on\nis a sum of two sigmoids. The test error is displayed for ten di \ufb00erent starting\nweights, for a single hidden layer neural network with ten hidde n units and weight\ndecay parameter value as indicated.", "422": "404 Neural Networks\nFIGURE 11.9. Examples of training cases from ZIP code data. Each image is\na16\u00d716 8-bit grayscale representation of a handwritten digit.\ndecay parameter, and hence cross-validation of this parameter would be\npreferred.\n11.7 Example: ZIP Code Data\nThis example is a character recognition task: classi\ufb01cation of handwritten\nnumerals. This problem captured the attention of the machine learning and\nneural network community for many years, and has remained a benchmark\nproblem in the \ufb01eld. Figure 11.9 shows some examples of normalized hand-\nwritten digits, automatically scanned from envelopes by the U.S. Postal\nService. The original scanned digits are binary and of di\ufb00erent sizes and\norientations; the images shown here have been deslanted and size normal-\nized, resulting in 16 \u00d716 grayscale images (Le Cun et al., 1990). These 256\npixel values are used as inputs to the neural network classi\ufb01er.\nAblack box neural network is not ideally suited to this pattern recogni-\ntion task, partly because the pixel representation of the images lack certain\ninvariances (such as small rotations of the image). Consequently early at -\ntempts with neural networks yielded misclassi\ufb01cation rates around 4 .5%\non various examples of the problem. In this section we show some of the\npioneering e\ufb00orts to handcraft the neural network to overcome some these\nde\ufb01ciencies (Le Cun, 1989), which ultimately led to the state of the art in\nneural network performance(Le Cun et al., 1998)1.\nAlthough current digit datasets have tens of thousands of training and\ntest examples, the sample size here is deliberately modest in order to em-\n1The \ufb01gures and tables in this example were recreated from Le C un (1989).", "423": "11.7 Example: ZIP Code Data 405\n16x168x8x2\n16x1610\n4x44x4\n8x8x210\nShared WeightsNet-5Net-4Net-1\n4x4x4Local Connectivity10\n1010\nNet-3Net-28x812\n16x1616x1616x16\nFIGURE 11.10. Architecture of the \ufb01ve networks used in the ZIP code example.\nphasize the e\ufb00ects. The examples were obtained by scanning some actual\nhand-drawn digits, and then generating additional images by random hor-\nizontal shifts. Details may be found in Le Cun (1989). There are 320 digi ts\nin the training set, and 160 in the test set.\nFive di\ufb00erent networks were \ufb01t to the data:\nNet-1: No hidden layer, equivalent to multinomial logistic regression.\nNet-2: One hidden layer, 12 hidden units fully connected.\nNet-3: Two hidden layers locally connected.\nNet-4: Two hidden layers, locally connected with weight sharing.\nNet-5: Two hidden layers, locally connected, two levels of weight sharing.\nThese are depicted in Figure 11.10. Net-1 for example has 256 inputs, one\neach for the 16 \u00d716 input pixels, and ten output units for each of the digits\n0\u20139. The predicted value \u02c6fk(x) represents the estimated probability that\nan image xhas digit class k, fork= 0,1,2,... ,9.", "424": "406 Neural Networks\nTraining Epochs% Correct on Test Data\n0 5 10 15 20 25 3060708090100\nNet-1Net-2Net-3Net-4Net-5\nFIGURE 11.11. Test performance curves, as a function of the number of train-\ning epochs, for the \ufb01ve networks of Table 11.1 applied to the ZIP c ode data.\n(Le Cun, 1989)\nThe networks all have sigmoidal output units, and were all \ufb01t with the\nsum-of-squares error function. The \ufb01rst network has no hidden layer, and\nhence is nearly equivalent to a linear multinomial regression model (Exer-\ncise 11.4). Net-2 is a single hidden layer network with 12 hidden units, of\nthe kind described above.\nThe training set error for all of the networks was 0%, since in all cases\nthere are more parameters than training observations. The evolution of the\ntest error during the training epochs is shown in Figure 11.11. The linear\nnetwork (Net-1) starts to over\ufb01t fairly quickly, while test performance o f\nthe others level o\ufb00 at successively superior values.\nThe other three networks have additional features which demonstrate\nthe power and \ufb02exibility of the neural network paradigm. They introduce\nconstraints on the network, natural for the problem at hand, which allow\nfor more complex connectivity but fewer parameters.\nNet-3 uses local connectivity: this means that each hidden unit is con-\nnected to only a small patch of units in the layer below. In the \ufb01rst hidden\nlayer (an 8 \u00d78 array), each unit takes inputs from a 3 \u00d73 patch of the input\nlayer; for units in the \ufb01rst hidden layer that are one unit apart, their recep-\ntive \ufb01elds overlap by one row or column, and hence are two pixels apart.\nIn the second hidden layer, inputs are from a 5 \u00d75 patch, and again units\nthat are one unit apart have receptive \ufb01elds that are two units apart. The\nweights for all other connections are set to zero. Local connectivity makes\neach unit responsible for extracting local features from the layer below, and", "425": "11.7 Example: ZIP Code Data 407\nTABLE 11.1. Test set performance of \ufb01ve di\ufb00erent neural networks on a hand-\nwritten digit classi\ufb01cation example (Le Cun, 1989).\nNetwork Architecture Links Weights % Correct\nNet-1: Single layer network 2570 2570 80.0%\nNet-2: Two layer network 3214 3214 87.0%\nNet-3: Locally connected 1226 1226 88.5%\nNet-4: Constrained network 1 2266 1132 94.0%\nNet-5: Constrained network 2 5194 1060 98.4%\nreduces considerably the total number of weights. With many more hidden\nunits than Net-2, Net-3 has fewer links and hence weights (1226 vs. 3214),\nand achieves similar performance.\nNet-4 and Net-5 have local connectivity with shared weights. All units\nin a local feature map perform the sameoperation on di\ufb00erent parts of the\nimage, achieved by sharing the same weights. The \ufb01rst hidden layer of Net-\n4 has two 8 \u00d78 arrays, and each unit takes input from a 3 \u00d73 patch just like\nin Net-3. However, each of the units in a single 8 \u00d78 feature map share the\nsame set of nine weights (but have their own bias parameter). This forces\nthe extracted features in di\ufb00erent parts of the image to be computed by\nthe same linear functional, and consequently these networks are sometimes\nknown as convolutional networks . The second hidden layer of Net-4 has\nno weight sharing, and is the same as in Net-3. The gradient of the error\nfunction Rwith respect to a shared weight is the sum of the gradients of\nRwith respect to each connection controlled by the weights in question.\nTable 11.1 gives the number of links, the number of weights and the\noptimal test performance for each of the networks. We see that Net-4 has\nmore links but fewer weights than Net-3, and superior test performance.\nNet-5 has four 4 \u00d74 feature maps in the second hidden layer, each unit\nconnected to a 5 \u00d75 local patch in the layer below. Weights are shared\nin each of these feature maps. We see that Net-5 does the best, having\nerrors of only 1.6%, compared to 13% for the \u201cvanilla\u201d network Net-2.\nThe clever design of network Net-5, motivated by the fact that features of\nhandwriting style should appear in more than one part of a digit, was the\nresult of many person years of experimentation. This and similar networks\ngave better performance on ZIP code problems than any other learning\nmethod at that time (early 1990s). This example also shows that neural\nnetworks are not a fully automatic tool, as they are sometimes advertised.\nAs with all statistical models, subject matter knowledge can and should be\nused to improve their performance.\nThis network was later outperformed by the tangent distance approach\n(Simard et al., 1993) described in Section 13.3.3, which explicitly incorpo-\nrates natural a\ufb03ne invariances. At this point the digit recognition datasets\nbecome test beds for every new learning procedure, and researchers worked", "426": "408 Neural Networks\nhard to drive down the error rates. As of this writing, the best error rates o n\na large database (60 ,000 training, 10 ,000 test observations), derived from\nstandard NIST2databases, were reported to be the following: (Le Cun et\nal., 1998):\n\u20221.1% for tangent distance with a 1-nearest neighbor classi\ufb01er (Sec-\ntion 13.3.3);\n\u20220.8% for a degree-9 polynomial SVM (Section 12.3);\n\u20220.8% for LeNet-5 , a more complex version of the convolutional net-\nwork described here;\n\u20220.7% for boosted LeNet-4 . Boosting is described in Chapter 8. LeNet-\n4is a predecessor of LeNet-5.\nLe Cun et al. (1998) report a much larger table of performance results, and\nit is evident that many groups have been working very hard to bring these\ntest error rates down. They report a standard error of 0 .1% on the error\nestimates, which is based on a binomial average with N= 10,000 and\np\u22480.01. This implies that error rates within 0 .1\u20140.2% of one another\nare statistically equivalent. Realistically the standard error is even hi gher,\nsince the test data has been implicitly used in the tuning of the various\nprocedures.\n11.8 Discussion\nBoth projection pursuit regression and neural networks take nonlinear func-\ntions of linear combinations (\u201cderived features\u201d) of the inputs. This is a\npowerful and very general approach for regression and classi\ufb01cation, and\nhas been shown to compete well with the best learning methods on many\nproblems.\nThese tools are especially e\ufb00ective in problems with a high signal-to-noise\nratio and settings where prediction without interpretation is the goal. They\nare less e\ufb00ective for problems where the goal is to describe the physical pro-\ncess that generated the data and the roles of individual inputs. Each input\nenters into the model in many places, in a nonlinear fashion. Some authors\n(Hinton, 1989) plot a diagram of the estimated weights into each hidden\nunit, to try to understand the feature that each unit is extracting. This\nis limited however by the lack of identi\ufb01ability of the parameter vectors\n\u03b1m, m= 1,... ,M . Often there are solutions with \u03b1mspanning the same\nlinear space as the ones found during training, giving predicted values that\n2The National Institute of Standards and Technology maintai n large databases, in-\ncluding handwritten character databases; http://www.nist.gov/srd/ .", "427": "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 409\nare roughly the same. Some authors suggest carrying out a principal com-\nponent analysis of these weights, to try to \ufb01nd an interpretable solution. In\ngeneral, the di\ufb03culty of interpreting these models has limited their use in\n\ufb01elds like medicine, where interpretation of the model is very important.\nThere has been a great deal of research on the training of neural net-\nworks. Unlike methods like CART and MARS, neural networks are smooth\nfunctions of real-valued parameters. This facilitates the development of\nBayesian inference for these models. The next sections discusses a success-\nful Bayesian implementation of neural networks.\n11.9 Bayesian Neural Nets and the NIPS 2003\nChallenge\nA classi\ufb01cation competition was held in 2003, in which \ufb01ve labeled train-\ning datasets were provided to participants. It was organized for a Neural\nInformation Processing Systems (NIPS) workshop. Each of the data sets\nconstituted a two-class classi\ufb01cation problems, with di\ufb00erent sizes and from\na variety of domains (see Table 11.2). Feature measurements for a valida-\ntion dataset were also available.\nParticipants developed and applied statistical learning procedures to\nmake predictions on the datasets, and could submit predictions to a web-\nsite on the validation set for a period of 12 weeks. With this feedback,\nparticipants were then asked to submit predictions for a separate test set\nand they received their results. Finally, the class labels for the validation\nset were released and participants had one week to train their algorithms\non the combined training and validation sets, and submit their \ufb01nal pre-\ndictions to the competition website. A total of 75 groups participated, with\n20 and 16 eventually making submissions on the validation and test sets,\nrespectively.\nThere was an emphasis on feature extraction in the competition. Arti-\n\ufb01cial \u201cprobes\u201d were added to the data: these are noise features with dis-\ntributions resembling the real features but independent of the class labels.\nThe percentage of probes that were added to each dataset, relative to the\ntotal set of features, is shown on Table 11.2. Thus each learning algorithm\nhad to \ufb01gure out a way of identifying the probes and downweighting or\neliminating them.\nA number of metrics were used to evaluate the entries, including the\npercentage correct on the test set, the area under the ROC curve, and a\ncombined score that compared each pair of classi\ufb01ers head-to-head. The\nresults of the competition are very interesting and are detailed in Guyon et\nal. (2006). The most notable result: the entries of Neal and Zhang (2006)\nwere the clear overall winners. In the \ufb01nal competition they \ufb01nished \ufb01rst", "428": "410 Neural Networks\nTABLE 11.2. NIPS 2003 challenge data sets. The column labeled pis the number\nof features. For the Dorothea dataset the features are binary. Ntr,NvalandNte\nare the number of training, validation and test cases, respectiv ely\nDataset Domain Feature p Percent Ntr Nval Nte\nType Probes\nArcene Mass spectrometry Dense 10,000 30 100 100 700\nDexter Text classi\ufb01cation Sparse 20,000 50 300 300 2000\nDorothea Drug discovery Sparse 100,000 50 800 350 800\nGisette Digit recognition Dense 5000 30 6000 1000 6500\nMadelon Arti\ufb01cial Dense 500 96 2000 600 1800\nin three of the \ufb01ve datasets, and were 5th and 7th on the remaining two\ndatasets.\nIn their winning entries, Neal and Zhang (2006) used a series of pre-\nprocessing feature-selection steps, followed by Bayesian neural networks,\nDirichlet di\ufb00usion trees, and combinations of these methods. Here we focus\nonly on the Bayesian neural network approach, and try to discern which\naspects of their approach were important for its success. We rerun their\nprograms and compare the results to boosted neural networks and boosted\ntrees, and other related methods.\n11.9.1 Bayes, Boosting and Bagging\nLet us \ufb01rst review brie\ufb02y the Bayesian approach to inference and its appli-\ncation to neural networks. Given training data Xtr,ytr, we assume a sam-\npling model with parameters \u03b8; Neal and Zhang (2006) use a two-hidden-\nlayer neural network, with output nodes the class probabilities Pr( Y|X,\u03b8)\nfor the binary outcomes. Given a prior distribution Pr( \u03b8), the posterior\ndistribution for the parameters is\nPr(\u03b8|Xtr,ytr) =Pr(\u03b8)Pr(ytr|Xtr,\u03b8)/integraltext\nPr(\u03b8)Pr(ytr|Xtr,\u03b8)d\u03b8(11.19)\nFor a test case with features Xnew, the predictive distribution for the\nlabelYnewis\nPr(Ynew|Xnew,Xtr,ytr) =/integraldisplay\nPr(Ynew|Xnew,\u03b8)Pr(\u03b8|Xtr,ytr)d\u03b8(11.20)\n(c.f. equation 8.24). Since the integral in (11.20) is intractable, sophis ticated\nMarkov Chain Monte Carlo (MCMC) methods are used to sample from the\nposterior distribution Pr( Ynew|Xnew,Xtr,ytr). A few hundred values \u03b8are\ngenerated and then a simple average of these values estimates the integral.\nNeal and Zhang (2006) use di\ufb00use Gaussian priors for all of the parame-\nters. The particular MCMC approach that was used is called hybrid Monte\nCarlo, and may be important for the success of the method. It includes\nan auxiliary momentum vector and implements Hamiltonian dynamics in\nwhich the potential function is the target density. This is done to avoid", "429": "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 411\nrandom walk behavior; the successive candidates move across the sample\nspace in larger steps. They tend to be less correlated and hence converge\nto the target distribution more rapidly.\nNeal and Zhang (2006) also tried di\ufb00erent forms of pre-processing of the\nfeatures:\n1. univariate screening using t-tests, and\n2. automatic relevance determination.\nIn the latter method (ARD), the weights (coe\ufb03cients) for the jth feature\nto each of the \ufb01rst hidden layer units all share a common prior variance\n\u03c32\nj, and prior mean zero. The posterior distributions for each variance \u03c32\nj\nare computed, and the features whose posterior variance concentrates on\nsmall values are discarded.\nThere are thus three main features of this approach that could be im-\nportant for its success:\n(a) the feature selection and pre-processing,\n(b) the neural network model, and\n(c) the Bayesian inference for the model using MCMC.\nAccording to Neal and Zhang (2006), feature screening in (a) is carried\nout purely for computational e\ufb03ciency; the MCMC procedure is slow with\na large number of features. There is no need to use feature selection to avoid\nover\ufb01tting. The posterior average (11.20) takes care of this automatica lly.\nWe would like to understand the reasons for the success of the Bayesian\nmethod. In our view, power of modern Bayesian methods does not lie in\ntheir use as a formal inference procedure; most people would not believe\nthat the priors in a high-dimensional, complex neural network model are\nactually correct. Rather the Bayesian/MCMC approach gives an e\ufb03cient\nway of sampling the relevant parts of model space, and then averaging the\npredictions for the high-probability models.\nBagging and boosting are non-Bayesian procedures that have some simi-\nlarity to MCMC in a Bayesian model. The Bayesian approach \ufb01xes the data\nand perturbs the parameters, according to current estimate of the poste-\nrior distribution. Bagging perturbs the data in an i.i.d fashion and then\nre-estimates the model to give a new set of model parameters. At the end,\na simple average of the model predictions from di\ufb00erent bagged samples is\ncomputed. Boosting is similar to bagging, but \ufb01ts a model that is additive\nin the models of each individual base learner, which are learned using non\ni.i.d. samples. We can write all of these models in the form\n\u02c6f(xnew) =L/summationdisplay\n\u2113=1w\u2113E(Ynew|xnew,\u02c6\u03b8\u2113) (11.21)", "430": "412 Neural Networks\nIn all cases the \u02c6\u03b8\u2113are a large collection of model parameters. For the\nBayesian model the w\u2113= 1/L, and the average estimates the posterior\nmean (11.21) by sampling \u03b8\u2113from the posterior distribution. For bagging,\nw\u2113= 1/Las well, and the \u02c6\u03b8\u2113are the parameters re\ufb01t to bootstrap re-\nsamples of the training data. For boosting, the weights are all equal to\n1, but the \u02c6\u03b8\u2113are typically chosen in a nonrandom sequential fashion to\nconstantly improve the \ufb01t.\n11.9.2 Performance Comparisons\nBased on the similarities above, we decided to compare Bayesian neural\nnetworks to boosted trees, boosted neural networks, random forests and\nbagged neural networks on the \ufb01ve datasets in Table 11.2. Bagging and\nboosting of neural networks are not methods that we have previously used\nin our work. We decided to try them here, because of the success of Bayesian\nneural networks in this competition, and the good performance of bagging\nand boosting with trees. We also felt that by bagging and boosting neural\nnets, we could assess both the choice of model as well as the model search\nstrategy.\nHere are the details of the learning methods that were compared:\nBayesian neural nets. The results here are taken from Neal and Zhang\n(2006), using their Bayesian approach to \ufb01tting neural networks. The\nmodels had two hidden layers of 20 and 8 units. We re-ran some\nnetworks for timing purposes only.\nBoosted trees. We used the gbmpackage (version 1.5-7) in the R language.\nTree depth and shrinkage factors varied from dataset to dataset. We\nconsistently bagged 80% of the data at each boosting iteration (the\ndefault is 50%). Shrinkage was between 0.001 and 0.1. Tree depth was\nbetween 2 and 9.\nBoosted neural networks. Since boosting is typically most e\ufb00ective with\n\u201cweak\u201d learners, we boosted a single hidden layer neural network with\ntwo or four units, \ufb01t with the nnetpackage (version 7.2-36) in R.\nRandom forests. We used the R package randomForest (version 4.5-16)\nwith default settings for the parameters.\nBagged neural networks. We used the same architecture as in the Bayesian\nneural network above (two hidden layers of 20 and 8 units), \ufb01t using\nboth Neal\u2019s C language package \u201cFlexible Bayesian Modeling\u201d (2004-\n11-10 release), and Matlab neural-net toolbox (version 5.1).", "431": "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 413Test Error (%)\nArcene Dexter Dorothea Gisette Madelon5 15 25Univariate Screened Features\nBayesian neural nets\nboosted trees \nboosted neural nets\nrandom forests\nbagged neural networks \nTest Error (%)\nArcene Dexter Dorothea Gisette Madelon5 15 25ARD Reduced Features\nFIGURE 11.12. Performance of di\ufb00erent learning methods on \ufb01ve problems,\nusing both univariate screening of features (top panel) and a reduc ed feature set\nfrom automatic relevance determination. The error bars at the t op of each plot\nhave width equal to one standard error of the di\ufb00erence between t wo error rates.\nOn most of the problems several competitors are within this e rror bound.\nThis analysis was carried out by Nicholas Johnson, and full details may\nbe found in Johnson (2008)3. The results are shown in Figure 11.12 and\nTable 11.3.\nThe \ufb01gure and table show Bayesian, boosted and bagged neural networks,\nboosted trees, and random forests, using both the screened and reduced\nfeatures sets. The error bars at the top of each plot indicate one standard\nerror of the di\ufb00erence between two error rates. Bayesian neural networks\nagain emerge as the winner, although for some datasets the di\ufb00erences\nbetween the test error rates is not statistically signi\ufb01cant. Random forests\nperforms the best among the competitors using the selected feature set,\nwhile the boosted neural networks perform best with the reduced feature\nset, and nearly match the Bayesian neural net.\nThe superiority of boosted neural networks over boosted trees suggest\nthat the neural network model is better suited to these particular prob-\nlems. Speci\ufb01cally, individual features might not be good predictors here\n3We also thank Isabelle Guyon for help in preparing the result s of this section.", "432": "414 Neural Networks\nTABLE 11.3. Performance of di\ufb00erent methods. Values are average rank of tes t\nerror across the \ufb01ve problems (low is good), and mean computati on time and\nstandard error of the mean, in minutes.\nScreened Features ARD Reduced Features\nMethod Average Average Average Average\nRank Time Rank Time\nBayesian neural networks 1.5 384(138) 1.6 600(186)\nBoosted trees 3.4 3.03(2.5) 4.0 34.1(32.4)\nBoosted neural networks 3.8 9.4(8.6) 2.2 35.6(33.5)\nRandom forests 2.7 1.9(1.7) 3.2 11.2(9.3)\nBagged neural networks 3.6 3.5(1.1) 4.0 6.4(4.4)\nand linear combinations of features work better. However the impressive\nperformance of random forests is at odds with this explanation, and came\nas a surprise to us.\nSince the reduced feature sets come from the Bayesian neural network\napproach, only the methods that use the screened features are legitimate,\nself-contained procedures. However, this does suggest that better methods\nfor internal feature selection might help the overall performance of boosted\nneural networks.\nThe table also shows the approximate training time required for each\nmethod. Here the non-Bayesian methods show a clear advantage.\nOverall, the superior performance of Bayesian neural networks here may\nbe due to the fact that\n(a) the neural network model is well suited to these \ufb01ve problems, and\n(b) the MCMC approach provides an e\ufb03cient way of exploring the im-\nportant part of the parameter space, and then averaging the resulting\nmodels according to their quality.\nThe Bayesian approach works well for smoothly parametrized models like\nneural nets; it is not yet clear that it works as well for non-smooth models\nlike trees.\n11.10 Computational Considerations\nWithNobservations, ppredictors, Mhidden units and Ltraining epochs, a\nneural network \ufb01t typically requires O(NpML ) operations. There are many\npackages available for \ufb01tting neural networks, probably many more than\nexist for mainstream statistical methods. Because the available softwar e\nvaries widely in quality, and the learning problem for neural networks is\nsensitive to issues such as input scaling, such software should be carefully\nchosen and tested.", "433": "Exercises 415\nBibliographic Notes\nProjection pursuit was proposed by Friedman and Tukey (1974), and spe-\ncialized to regression by Friedman and Stuetzle (1981). Huber (1985) gives\na scholarly overview, and Roosen and Hastie (1994) present a formulatio n\nusing smoothing splines. The motivation for neural networks dates back\nto McCulloch and Pitts (1943), Widrow and Ho\ufb00 (1960) (reprinted in An-\nderson and Rosenfeld (1988)) and Rosenblatt (1962). Hebb (1949) heavily\nin\ufb02uenced the development of learning algorithms. The resurgence of neural\nnetworks in the mid 1980s was due to Werbos (1974), Parker (1985) and\nRumelhart et al. (1986), who proposed the back-propagation algorithm.\nToday there are many books written on the topic, for a broad range of\naudiences. For readers of this book, Hertz et al. (1991), Bishop (1995) and\nRipley (1996) may be the most informative. Bayesian learning for neural\nnetworks is described in Neal (1996). The ZIP code example was taken from\nLe Cun (1989); see also Le Cun et al. (1990) and Le Cun et al. (1998).\nWe do not discuss theoretical topics such as approximation properties of\nneural networks, such as the work of Barron (1993), Girosi et al. (1995 )\nand Jones (1992). Some of these results are summarized by Ripley (1996).\nExercises\nEx. 11.1 Establish the exact correspondence between the projection pur-\nsuit regression model (11.1) and the neural network (11.5). In particular,\nshow that the single-layer regression network is equivalent to a PPR model\nwithgm(\u03c9T\nmx) =\u03b2m\u03c3(\u03b10m+sm(\u03c9T\nmx)), where \u03c9mis the mth unit vector.\nEstablish a similar equivalence for a classi\ufb01cation network.\nEx. 11.2 Consider a neural network for a quantitative outcome as in (11.5),\nusing squared-error loss and identity output function gk(t) =t. Suppose\nthat the weights \u03b1mfrom the input to hidden layer are nearly zero. Show\nthat the resulting model is nearly linear in the inputs.\nEx. 11.3 Derive the forward and backward propagation equations for the\ncross-entropy loss function.\nEx. 11.4 Consider a neural network for a Kclass outcome that uses cross-\nentropy loss. If the network has no hidden layer, show that the model is\nequivalent to the multinomial logistic model described in Chapter 4.\nEx. 11.5\n(a) Write a program to \ufb01t a single hidden layer neural network (ten hidden\nunits) via back-propagation and weight decay.", "434": "416 Neural Networks\n(b) Apply it to 100 observations from the model\nY=\u03c3(aT\n1X) + (aT\n2X)2+ 0.30\u2264Z,\nwhere \u03c3is the sigmoid function, Zis standard normal, XT= (X1,X2),\neachXjbeing independent standard normal, and a1= (3,3),a2=\n(3,\u22123). Generate a test sample of size 1000, and plot the training and\ntest error curves as a function of the number of training epochs, for\ndi\ufb00erent values of the weight decay parameter. Discuss the over\ufb01tting\nbehavior in each case.\n(c) Vary the number of hidden units in the network, from 1 up to 10, and\ndetermine the minimum number needed to perform well for this task.\nEx. 11.6 Write a program to carry out projection pursuit regression, using\ncubic smoothing splines with \ufb01xed degrees of freedom. Fit it to the data\nfrom the previous exercise, for various values of the smoothing parameter\nand number of model terms. Find the minimum number of model terms\nnecessary for the model to perform well and compare this to the number\nof hidden units from the previous exercise.\nEx. 11.7 Fit a neural network to the spamdata of Section 9.1.2, and compare\nthe results to those for the additive model given in that chapter. Compare\nboth the classi\ufb01cation performance and interpretability of the \ufb01nal model.", "435": "This is page 417\nPrinter: Opaque this\n12\nSupport Vector Machines and\nFlexible Discriminants\n12.1 Introduction\nIn this chapter we describe generalizations of linear decision boundaries\nfor classi\ufb01cation. Optimal separating hyperplanes are introduced in Chap-\nter 4 for the case when two classes are linearly separable. Here we cover\nextensions to the nonseparable case, where the classes overlap. These tech-\nniques are then generalized to what is known as the support vector machine ,\nwhich produces nonlinear boundaries by constructing a linear boundary in\na large, transformed version of the feature space. The second set of methods\ngeneralize Fisher\u2019s linear discriminant analysis (LDA). The generalizations\ninclude \ufb02exible discriminant analysis which facilitates construction of non-\nlinear boundaries in a manner very similar to the support vector machines,\npenalized discriminant analysis for problems such as signal and image clas-\nsi\ufb01cation where the large number of features are highly correlated, and\nmixture discriminant analysis for irregularly shaped classes.\n12.2 The Support Vector Classi\ufb01er\nIn Chapter 4 we discussed a technique for constructing an optimal separat-\ning hyperplane between two perfectly separated classes. We review this and\ngeneralize to the nonseparable case, where the classes may not be separable\nby a linear boundary.", "436": "418 12. Flexible Discriminants\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\nmarginM=1\n/bardbl\u03b2/bardbl\nM=1\n/bardbl\u03b2/bardblxT\u03b2+\u03b20= 0\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\nmargin\u03be\u2217\n1\u03be\u2217\n1\u03be\u2217\n1\n\u03be\u2217\n2\u03be\u2217\n2\u03be\u2217\n2\u03be\u2217\n3\u03be\u2217\n3\u03be\u2217\n4\u03be\u2217\n4\u03be\u2217\n4\u03be\u2217\n5\nM=1\n/bardbl\u03b2/bardbl\nM=1\n/bardbl\u03b2/bardblxT\u03b2+\u03b20= 0\nFIGURE 12.1. Support vector classi\ufb01ers. The left panel shows the separable\ncase. The decision boundary is the solid line, while broken line s bound the shaded\nmaximal margin of width 2M= 2//bardbl\u03b2/bardbl. The right panel shows the nonseparable\n(overlap) case. The points labeled \u03be\u2217\njare on the wrong side of their margin by\nan amount \u03be\u2217\nj=M\u03bej; points on the correct side have \u03be\u2217\nj= 0. The margin is\nmaximized subject to a total budgetP\u03bei\u2264constant. HenceP\u03be\u2217\njis the total\ndistance of points on the wrong side of their margin.\nOur training data consists of Npairs ( x1,y1),(x2,y2),... ,(xN,yN), with\nxi\u2208IRpandyi\u2208 {\u2212 1,1}. De\ufb01ne a hyperplane by\n{x:f(x) =xT\u03b2+\u03b20= 0}, (12.1)\nwhere \u03b2is a unit vector: \u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al= 1. A classi\ufb01cation rule induced by f(x) is\nG(x) = sign[ xT\u03b2+\u03b20]. (12.2)\nThe geometry of hyperplanes is reviewed in Section 4.5, where we show that\nf(x) in (12.1) gives the signed distance from a point xto the hyperplane\nf(x) =xT\u03b2+\u03b20= 0. Since the classes are separable, we can \ufb01nd a function\nf(x) =xT\u03b2+\u03b20withyif(xi)>0\u2200i. Hence we are able to \ufb01nd the\nhyperplane that creates the biggest margin between the training points for\nclass 1 and \u22121 (see Figure 12.1). The optimization problem\nmax\n\u03b2,\u03b20,/bardbl\u03b2/bardbl=1M\nsubject to yi(xT\ni\u03b2+\u03b20)\u2265M, i= 1,... ,N,(12.3)\ncaptures this concept. The band in the \ufb01gure is Munits away from the\nhyperplane on either side, and hence 2 Munits wide. It is called the margin .\nWe showed that this problem can be more conveniently rephrased as\nmin\n\u03b2,\u03b20\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al\nsubject to yi(xT\ni\u03b2+\u03b20)\u22651, i= 1,... ,N,(12.4)", "437": "12.2 The Support Vector Classi\ufb01er 419\nwhere we have dropped the norm constraint on \u03b2. Note that M= 1/\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al.\nExpression (12.4) is the usual way of writing the support vector criterion\nfor separated data. This is a convex optimization problem (quadratic cri-\nterion, linear inequality constraints), and the solution is characterized in\nSection 4.5.2.\nSuppose now that the classes overlap in feature space. One way to deal\nwith the overlap is to still maximize M, but allow for some points to be on\nthe wrong side of the margin. De\ufb01ne the slack variables \u03be= (\u03be1,\u03be2,... ,\u03be N).\nThere are two natural ways to modify the constraint in (12.3):\nyi(xT\ni\u03b2+\u03b20)\u2265M\u2212\u03bei, (12.5)\nor\nyi(xT\ni\u03b2+\u03b20)\u2265M(1\u2212\u03bei), (12.6)\n\u2200i, \u03bei\u22650,/summationtextN\ni=1\u03bei\u2264constant. The two choices lead to di\ufb00erent solutions.\nThe \ufb01rst choice seems more natural, since it measures overlap in actual\ndistance from the margin; the second choice measures the overlap in relative\ndistance, which changes with the width of the margin M. However, the \ufb01rst\nchoice results in a nonconvex optimization problem, while the second is\nconvex; thus (12.6) leads to the \u201cstandard\u201d support vector classi\ufb01er, which\nwe use from here on.\nHere is the idea of the formulation. The value \u03beiin the constraint yi(xT\ni\u03b2+\n\u03b20)\u2265M(1\u2212\u03bei) is the proportional amount by which the prediction\nf(xi) =xT\ni\u03b2+\u03b20is on the wrong side of its margin. Hence by bounding the\nsum/summationtext\u03bei, we bound the total proportional amount by which predictions\nfall on the wrong side of their margin. Misclassi\ufb01cations occur when \u03bei>1,\nso bounding/summationtext\u03beiat a value Ksay, bounds the total number of training\nmisclassi\ufb01cations at K.\nAs in (4.48) in Section 4.5.2, we can drop the norm constraint on \u03b2,\nde\ufb01ne M= 1/\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al, and write (12.4) in the equivalent form\nmin\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230alsubject to/braceleft\uf8ecigg\nyi(xT\ni\u03b2+\u03b20)\u22651\u2212\u03bei\u2200i,\n\u03bei\u22650,/summationtext\u03bei\u2264constant .(12.7)\nThis is the usual way the support vector classi\ufb01er is de\ufb01ned for the non-\nseparable case. However we \ufb01nd confusing the presence of the \ufb01xed scale\n\u201c1\u201d in the constraint yi(xT\ni\u03b2+\u03b20)\u22651\u2212\u03bei, and prefer to start with (12.6).\nThe right panel of Figure 12.1 illustrates this overlapping case.\nBy the nature of the criterion (12.7), we see that points well inside their\nclass boundary do not play a big role in shaping the boundary. This seems\nlike an attractive property, and one that di\ufb00erentiates it from linear dis-\ncriminant analysis (Section 4.3). In LDA, the decision boundary is deter-\nmined by the covariance of the class distributions and the positions of the\nclass centroids. We will see in Section 12.3.3 that logistic regression i s more\nsimilar to the support vector classi\ufb01er in this regard.", "438": "420 12. Flexible Discriminants\n12.2.1 Computing the Support Vector Classi\ufb01er\nThe problem (12.7) is quadratic with linear inequality constraints, hence it\nis a convex optimization problem. We describe a quadratic programming\nsolution using Lagrange multipliers. Computationally it is convenient to\nre-express (12.7) in the equivalent form\nmin\n\u03b2,\u03b201\n2\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al2+CN/summationdisplay\ni=1\u03bei\nsubject to \u03bei\u22650, yi(xT\ni\u03b2+\u03b20)\u22651\u2212\u03bei\u2200i,(12.8)\nwhere the \u201ccost\u201d parameter Creplaces the constant in (12.7); the separable\ncase corresponds to C=\u221e.\nThe Lagrange (primal) function is\nLP=1\n2\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al2+CN/summationdisplay\ni=1\u03bei\u2212N/summationdisplay\ni=1\u03b1i[yi(xT\ni\u03b2+\u03b20)\u2212(1\u2212\u03bei)]\u2212N/summationdisplay\ni=1\u03b8i\u03bei,(12.9)\nwhich we minimize w.r.t \u03b2,\u03b20and\u03bei. Setting the respective derivatives to\nzero, we get\n\u03b2=N/summationdisplay\ni=1\u03b1iyixi, (12.10)\n0 =N/summationdisplay\ni=1\u03b1iyi, (12.11)\n\u03b1i=C\u2212\u03b8i,\u2200i, (12.12)\nas well as the positivity constraints \u03b1i, \u03b8i, \u03bei\u22650\u2200i. By substituting\n(12.10)\u2013(12.12) into (12.9), we obtain the Lagrangian (Wolfe) dua l objec-\ntive function\nLD=N/summationdisplay\ni=1\u03b1i\u22121\n2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1\u03b1i\u03b1i\u2032yiyi\u2032xT\nixi\u2032, (12.13)\nwhich gives a lower bound on the objective function (12.8) for any feasible\npoint. We maximize LDsubject to 0 \u2264\u03b1i\u2264Cand/summationtextN\ni=1\u03b1iyi= 0. In\naddition to (12.10)\u2013(12.12), the Karush\u2013Kuhn\u2013Tucker conditions include\nthe constraints\n\u03b1i[yi(xT\ni\u03b2+\u03b20)\u2212(1\u2212\u03bei)] = 0 , (12.14)\n\u03b8i\u03bei= 0, (12.15)\nyi(xT\ni\u03b2+\u03b20)\u2212(1\u2212\u03bei)\u22650, (12.16)\nfori= 1,... ,N . Together these equations (12.10)\u2013(12.16) uniquely char-\nacterize the solution to the primal and dual problem.", "439": "12.2 The Support Vector Classi\ufb01er 421\nFrom (12.10) we see that the solution for \u03b2has the form\n\u02c6\u03b2=N/summationdisplay\ni=1\u02c6\u03b1iyixi, (12.17)\nwith nonzero coe\ufb03cients \u02c6 \u03b1ionly for those observations ifor which the\nconstraints in (12.16) are exactly met (due to (12.14)). These observati ons\nare called the support vectors , since \u02c6\u03b2is represented in terms of them\nalone. Among these support points, some will lie on the edge of the margin\n(\u02c6\u03bei= 0), and hence from (12.15) and (12.12) will be characterized by\n0<\u02c6\u03b1i< C; the remainder ( \u02c6\u03bei>0) have \u02c6 \u03b1i=C. From (12.14) we can\nsee that any of these margin points (0 <\u02c6\u03b1i,\u02c6\u03bei= 0) can be used to solve\nfor\u03b20, and we typically use an average of all the solutions for numerical\nstability.\nMaximizing the dual (12.13) is a simpler convex quadratic programming\nproblem than the primal (12.9), and can be solved with standard techniques\n(Murray et al., 1981, for example).\nGiven the solutions \u02c6\u03b20and\u02c6\u03b2, the decision function can be written as\n\u02c6G(x) = sign[ \u02c6f(x)]\n= sign[ xT\u02c6\u03b2+\u02c6\u03b20]. (12.18)\nThe tuning parameter of this procedure is the cost parameter C.\n12.2.2 Mixture Example (Continued)\nFigure 12.2 shows the support vector boundary for the mixture example\nof Figure 2.5 on page 21, with two overlapping classes, for two di\ufb00erent\nvalues of the cost parameter C. The classi\ufb01ers are rather similar in their\nperformance. Points on the wrong side of the boundary are support vectors.\nIn addition, points on the correct side of the boundary but close to it (in\nthe margin), are also support vectors. The margin is larger for C= 0.01\nthan it is for C= 10,000. Hence larger values of Cfocus attention more\non (correctly classi\ufb01ed) points near the decision boundary, while smaller\nvalues involve data further away. Either way, misclassi\ufb01ed points are gi ven\nweight, no matter how far away. In this example the procedure is not very\nsensitive to choices of C, because of the rigidity of a linear boundary.\nThe optimal value for Ccan be estimated by cross-validation, as dis-\ncussed in Chapter 7. Interestingly, the leave-one-out cross-validation error\ncan be bounded above by the proportion of support points in the data. The\nreason is that leaving out an observation that is not a support vector will\nnot change the solution. Hence these observations, being classi\ufb01ed correctly\nby the original boundary, will be classi\ufb01ed correctly in the cross-validatio n\nprocess. However this bound tends to be too high, and not generally useful\nfor choosing C(62% and 85%, respectively, in our examples).", "440": "422 12. Flexible Discriminants\n.. . . .. . . . . . .. . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . .. . . . ..\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\n\u2022\u2022\n\u2022\nTraining Error: 0.270\nTest Error:       0.288\nBayes Error:    0.210\nC= 10000\n.. .. . . .. . . . .. . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .. . . . .. . . .. ..\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\u2022\nTraining Error: 0.26\nTest Error:       0.30\nBayes Error:    0.21\nC= 0.01\nFIGURE 12.2. The linear support vector boundary for the mixture data exam-\nple with two overlapping classes, for two di\ufb00erent values of C. The broken lines\nindicate the margins, where f(x) =\u00b11. The support points ( \u03b1i>0) are all the\npoints on the wrong side of their margin. The black solid dots are those support\npoints falling exactly on the margin ( \u03bei= 0, \u03b1i>0). In the upper panel 62%of\nthe observations are support points, while in the lower panel 85%are. The broken\npurple curve in the background is the Bayes decision boundary.", "441": "12.3 Support Vector Machines and Kernels 423\n12.3 Support Vector Machines and Kernels\nThe support vector classi\ufb01er described so far \ufb01nds linear boundaries in the\ninput feature space. As with other linear methods, we can make the pro-\ncedure more \ufb02exible by enlarging the feature space using basis expansions\nsuch as polynomials or splines (Chapter 5). Generally linear boundaries\nin the enlarged space achieve better training-class separation, and trans-\nlate to nonlinear boundaries in the original space. Once the basis functions\nhm(x), m= 1,... ,M are selected, the procedure is the same as before. We\n\ufb01t the SV classi\ufb01er using input features h(xi) = (h1(xi),h2(xi),... ,h M(xi)),\ni= 1,... ,N , and produce the (nonlinear) function \u02c6f(x) =h(x)T\u02c6\u03b2+\u02c6\u03b20.\nThe classi\ufb01er is \u02c6G(x) = sign( \u02c6f(x)) as before.\nThesupport vector machine classi\ufb01er is an extension of this idea, where\nthe dimension of the enlarged space is allowed to get very large, in\ufb01nite\nin some cases. It might seem that the computations would become pro-\nhibitive. It would also seem that with su\ufb03cient basis functions, the data\nwould be separable, and over\ufb01tting would occur. We \ufb01rst show how the\nSVM technology deals with these issues. We then see that in fact the SVM\nclassi\ufb01er is solving a function-\ufb01tting problem using a particular criterion\nand form of regularization, and is part of a much bigger class of problems\nthat includes the smoothing splines of Chapter 5. The reader may wish\nto consult Section 5.8, which provides background material and overlaps\nsomewhat with the next two sections.\n12.3.1 Computing the SVM for Classi\ufb01cation\nWe can represent the optimization problem (12.9) and its solution in a\nspecial way that only involves the input features via inner products. We do\nthis directly for the transformed feature vectors h(xi). We then see that for\nparticular choices of h, these inner products can be computed very cheaply.\nThe Lagrange dual function (12.13) has the form\nLD=N/summationdisplay\ni=1\u03b1i\u22121\n2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1\u03b1i\u03b1i\u2032yiyi\u2032\u221dan}\u230a\u2207a\u230bketle{th(xi),h(xi\u2032)\u221dan}\u230a\u2207a\u230bket\u2207i}ht. (12.19)\nFrom (12.10) we see that the solution function f(x) can be written\nf(x) = h(x)T\u03b2+\u03b20\n=N/summationdisplay\ni=1\u03b1iyi\u221dan}\u230a\u2207a\u230bketle{th(x),h(xi)\u221dan}\u230a\u2207a\u230bket\u2207i}ht+\u03b20. (12.20)\nAs before, given \u03b1i,\u03b20can be determined by solving yif(xi) = 1 in (12.20)\nfor any (or all) xifor which 0 < \u03b1i< C.", "442": "424 12. Flexible Discriminants\nSo both (12.19) and (12.20) involve h(x) only through inner products. In\nfact, we need not specify the transformation h(x) at all, but require only\nknowledge of the kernel function\nK(x,x\u2032) =\u221dan}\u230a\u2207a\u230bketle{th(x),h(x\u2032)\u221dan}\u230a\u2207a\u230bket\u2207i}ht (12.21)\nthat computes inner products in the transformed space. Kshould be a\nsymmetric positive (semi-) de\ufb01nite function; see Section 5.8.1.\nThree popular choices for Kin the SVM literature are\ndth-Degree polynomial: K(x,x\u2032) = (1 + \u221dan}\u230a\u2207a\u230bketle{tx,x\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht)d,\nRadial basis: K(x,x\u2032) = exp( \u2212\u03b3\u221d\u230aa\u2207\u2308\u230alx\u2212x\u2032\u221d\u230aa\u2207\u2308\u230al2),\nNeural network: K(x,x\u2032) = tanh( \u03ba1\u221dan}\u230a\u2207a\u230bketle{tx,x\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht+\u03ba2).(12.22)\nConsider for example a feature space with two inputs X1andX2, and a\npolynomial kernel of degree 2. Then\nK(X,X\u2032) = (1 + \u221dan}\u230a\u2207a\u230bketle{tX,X\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht)2\n= (1 + X1X\u2032\n1+X2X\u2032\n2)2\n= 1 + 2 X1X\u2032\n1+ 2X2X\u2032\n2+ (X1X\u2032\n1)2+ (X2X\u2032\n2)2+ 2X1X\u2032\n1X2X\u2032\n2.\n(12.23)\nThen M= 6, and if we choose h1(X) = 1, h2(X) =\u221a\n2X1,h3(X) =\u221a\n2X2,h4(X) =X2\n1,h5(X) =X2\n2, andh6(X) =\u221a\n2X1X2, then K(X,X\u2032) =\n\u221dan}\u230a\u2207a\u230bketle{th(X),h(X\u2032)\u221dan}\u230a\u2207a\u230bket\u2207i}ht. From (12.20) we see that the solution can be written\n\u02c6f(x) =N/summationdisplay\ni=1\u02c6\u03b1iyiK(x,xi) +\u02c6\u03b20. (12.24)\nThe role of the parameter Cis clearer in an enlarged feature space,\nsince perfect separation is often achievable there. A large value of Cwill\ndiscourage any positive \u03bei, and lead to an over\ufb01t wiggly boundary in the\noriginal feature space; a small value of Cwill encourage a small value of\n\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al, which in turn causes f(x) and hence the boundary to be smoother.\nFigure 12.3 show two nonlinear support vector machines applied to the\nmixture example of Chapter 2. The regularization parameter was chosen\nin both cases to achieve good test error. The radial basis kernel produces\na boundary quite similar to the Bayes optimal boundary for this example;\ncompare Figure 2.5.\nIn the early literature on support vectors, there were claims that the\nkernel property of the support vector machine is unique to it and allows\none to \ufb01nesse the curse of dimensionality. Neither of these claims is true,\nand we go into both of these issues in the next three subsections.", "443": "12.3 Support Vector Machines and Kernels 425\nSVM - Degree-4 Polynomial in Feature Space\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . .. . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\nTraining Error: 0.180\nTest Error:       0.245\nBayes Error:    0.210\nSVM - Radial Kernel in Feature Space\n. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . .. . . .. . . .. . . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\nTraining Error: 0.160\nTest Error:       0.218\nBayes Error:    0.210\nFIGURE 12.3. Two nonlinear SVMs for the mixture data. The upper plot uses\na4th degree polynomial kernel, the lower a radial basis kernel (wi th\u03b3= 1). In\neach case Cwas tuned to approximately achieve the best test error perform ance,\nandC= 1worked well in both cases. The radial basis kernel performs th e best\n(close to Bayes optimal), as might be expected given the data a rise from mixtures\nof Gaussians. The broken purple curve in the background is the B ayes decision\nboundary.", "444": "426 12. Flexible Discriminants\n\u22123 \u22122 \u22121 0 1 2 30.0 0.5 1.0 1.5 2.0 2.5 3.0Hinge Loss\nBinomial Deviance\nSquared Error\nClass HuberLoss\nyf\nFIGURE 12.4. The support vector loss function (hinge loss), compared to the\nnegative log-likelihood loss (binomial deviance) for logisti c regression, squared-er-\nror loss, and a \u201cHuberized\u201d version of the squared hinge loss. A ll are shown as a\nfunction of yfrather than f, because of the symmetry between the y= +1 and\ny=\u22121case. The deviance and Huber have the same asymptotes as the SVM\nloss, but are rounded in the interior. All are scaled to have the limiting left-tail\nslope of \u22121.\n12.3.2 The SVM as a Penalization Method\nWith f(x) =h(x)T\u03b2+\u03b20, consider the optimization problem\nmin\n\u03b20, \u03b2N/summationdisplay\ni=1[1\u2212yif(xi)]++\u03bb\n2\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al2(12.25)\nwhere the subscript \u201c+\u201d indicates positive part. This has the form loss+\npenalty , which is a familiar paradigm in function estimation. It is easy to\nshow (Exercise 12.1) that the solution to (12.25), with \u03bb= 1/C, is the\nsame as that for (12.8).\nExamination of the \u201chinge\u201d loss function L(y,f) = [1 \u2212yf]+shows that\nit is reasonable for two-class classi\ufb01cation, when compared to other more\ntraditional loss functions. Figure 12.4 compares it to the log-likelihood l oss\nfor logistic regression, as well as squared-error loss and a variant thereo f.\nThe (negative) log-likelihood or binomial deviance has similar tails as the\nSVM loss, giving zero penalty to points well inside their margin, and a", "445": "12.3 Support Vector Machines and Kernels 427\nTABLE 12.1. The population minimizers for the di\ufb00erent loss functions in Fig -\nure 12.4. Logistic regression uses the binomial log-likelih ood or deviance. Linear\ndiscriminant analysis (Exercise 4.2) uses squared-error loss. The SVM hinge loss\nestimates the mode of the posterior class probabilities, wh ereas the others estimate\na linear transformation of these probabilities.\nLoss Function L[y, f(x)] Minimizing Function\nBinomial\nDeviance log[1 + e\u2212yf(x)]f(x) = logPr(Y= +1|x)\nPr(Y= -1|x)\nSVM Hinge\nLoss[1\u2212yf(x)]+ f(x) = sign[Pr( Y= +1|x)\u22121\n2]\nSquared\nError[y\u2212f(x)]2= [1\u2212yf(x)]2f(x) = 2Pr( Y= +1|x)\u22121\n\u201cHuberised\u201d\nSquare\nHinge Loss\u22124yf(x), yf (x)<-1\n[1\u2212yf(x)]2\n+otherwisef(x) = 2Pr( Y= +1|x)\u22121\nlinear penalty to points on the wrong side and far away. Squared-error, on\nthe other hand gives a quadratic penalty, and points well inside their own\nmargin have a strong in\ufb02uence on the model as well. The squared hinge\nlossL(y,f) = [1 \u2212yf]2\n+is like the quadratic, except it is zero for points\ninside their margin. It still rises quadratically in the left tail, and wil l be\nless robust than hinge or deviance to misclassi\ufb01ed observations. Recently\nRosset and Zhu (2007) proposed a \u201cHuberized\u201d version of the squared hinge\nloss, which converts smoothly to a linear loss at yf=\u22121.\nWe can characterize these loss functions in terms of what they are es-\ntimating at the population level. We consider minimizing E L(Y,f(X)).\nTable 12.1 summarizes the results. Whereas the hinge loss estimates the\nclassi\ufb01er G(x) itself, all the others estimate a transformation of the class\nposterior probabilities. The \u201cHuberized\u201d square hinge loss shares attractive\nproperties of logistic regression (smooth loss function, estimates proba bili-\nties), as well as the SVM hinge loss (support points).\nFormulation (12.25) casts the SVM as a regularized function estimation\nproblem, where the coe\ufb03cients of the linear expansion f(x) =\u03b20+h(x)T\u03b2\nare shrunk toward zero (excluding the constant). If h(x) represents a hierar-\nchical basis having some ordered structure (such as ordered in roughness),", "446": "428 12. Flexible Discriminants\nthen the uniform shrinkage makes more sense if the rougher elements hjin\nthe vector hhave smaller norm.\nAll the loss-function in Table 12.1 except squared-error are so called\n\u201cmargin maximizing loss-functions\u201d (Rosset et al., 2004b). This means that\nif the data are separable, then the limit of \u02c6\u03b2\u03bbin (12.25) as \u03bb\u21920 de\ufb01nes\nthe optimal separating hyperplane1.\n12.3.3 Function Estimation and Reproducing Kernels\nHere we describe SVMs in terms of function estimation in reproducing\nkernel Hilbert spaces, where the kernel property abounds. This material is\ndiscussed in some detail in Section 5.8. This provides another view of the\nsupport vector classi\ufb01er, and helps to clarify how it works.\nSuppose the basis harises from the (possibly \ufb01nite) eigen-expansion of\na positive de\ufb01nite kernel K,\nK(x,x\u2032) =\u221e/summationdisplay\nm=1\u03c6m(x)\u03c6m(x\u2032)\u03b4m (12.26)\nandhm(x) =\u221a\u03b4m\u03c6m(x). Then with \u03b8m=\u221a\u03b4m\u03b2m, we can write (12.25)\nas\nmin\n\u03b20, \u03b8N/summationdisplay\ni=1/bracketleft\uf8ecigg\n1\u2212yi(\u03b20+\u221e/summationdisplay\nm=1\u03b8m\u03c6m(xi))/bracketright\uf8ecigg\n++\u03bb\n2\u221e/summationdisplay\nm=1\u03b82\nm\n\u03b4m. (12.27)\nNow (12.27) is identical in form to (5.49) on page 169 in Section 5.8, a nd\nthe theory of reproducing kernel Hilbert spaces described there guarantees\na \ufb01nite-dimensional solution of the form\nf(x) =\u03b20+N/summationdisplay\ni=1\u03b1iK(x,xi). (12.28)\nIn particular we see there an equivalent version of the optimization crite-\nrion (12.19) [Equation (5.67) in Section 5.8.2; see also Wahba et al. (2000)],\nmin\n\u03b20,\u03b1N/summationdisplay\ni=1(1\u2212yif(xi))++\u03bb\n2\u03b1TK\u03b1, (12.29)\nwhereKis the N\u00d7Nmatrix of kernel evaluations for all pairs of training\nfeatures (Exercise 12.2).\nThese models are quite general, and include, for example, the entire fam-\nily of smoothing splines, additive and interaction spline models discussed\n1For logistic regression with separable data, \u02c6\u03b2\u03bbdiverges, but \u02c6\u03b2\u03bb/||\u02c6\u03b2\u03bbconverges to\nthe optimal separating direction.", "447": "12.3 Support Vector Machines and Kernels 429\nin Chapters 5 and 9, and in more detail in Wahba (1990) and Hastie and\nTibshirani (1990). They can be expressed more generally as\nmin\nf\u2208HN/summationdisplay\ni=1[1\u2212yif(xi)]++\u03bbJ(f), (12.30)\nwhere His the structured space of functions, and J(f) an appropriate reg-\nularizer on that space. For example, suppose His the space of additive\nfunctions f(x) =/summationtextp\nj=1fj(xj), and J(f) =/summationtext\nj/integraltext\n{f\u2032\u2032\nj(xj)}2dxj. Then the\nsolution to (12.30) is an additive cubic spline, and has a kernel representa-\ntion (12.28) with K(x,x\u2032) =/summationtextp\nj=1Kj(xj,x\u2032\nj). Each of the Kjis the kernel\nappropriate for the univariate smoothing spline in xj(Wahba, 1990).\nConversely this discussion also shows that, for example, anyof the kernels\ndescribed in (12.22) above can be used with anyconvex loss function, and\nwill also lead to a \ufb01nite-dimensional representation of the form (12.28).\nFigure 12.5 uses the same kernel functions as in Figure 12.3, except using\nthe binomial log-likelihood as a loss function2. The \ufb01tted function is hence\nan estimate of the log-odds,\n\u02c6f(x) = log\u02c6Pr(Y= +1|x)\n\u02c6Pr(Y=\u22121|x)\n=\u02c6\u03b20+N/summationdisplay\ni=1\u02c6\u03b1iK(x,xi), (12.31)\nor conversely we get an estimate of the class probabilities\n\u02c6Pr(Y= +1|x) =1\n1 +e\u2212\u02c6\u03b20\u2212PN\ni=1\u02c6\u03b1iK(x,xi). (12.32)\nThe \ufb01tted models are quite similar in shape and performance. Examples\nand more details are given in Section 5.8.\nIt does happen that for SVMs, a sizable fraction of the Nvalues of \u03b1i\ncan be zero (the nonsupport points). In the two examples in Figure 12.3,\nthese fractions are 42% and 45%, respectively. This is a consequence of the\npiecewise linear nature of the \ufb01rst part of the criterion (12.25). The lower\nthe class overlap (on the training data), the greater this fraction will be.\nReducing \u03bbwill generally reduce the overlap (allowing a more \ufb02exible f).\nA small number of support points means that \u02c6f(x) can be evaluated more\nquickly, which is important at lookup time. Of course, reducing the overlap\ntoo much can lead to poor generalization.\n2Ji Zhu assisted in the preparation of these examples.", "448": "430 12. Flexible Discriminants\nLR - Degree-4 Polynomial in Feature Space\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . .. . . . . . .. . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.190\nTest Error:       0.263\nBayes Error:    0.210\nLR - Radial Kernel in Feature Space\n.. . . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.150\nTest Error:       0.221\nBayes Error:    0.210\nFIGURE 12.5. The logistic regression versions of the SVM models in Fig-\nure 12.3, using the identical kernels and hence penalties, but the l og-likelihood\nloss instead of the SVM loss function. The two broken contours corr espond to\nposterior probabilities of 0.75and0.25for the +1 class (or vice versa). The bro-\nken purple curve in the background is the Bayes decision bounda ry.", "449": "12.3 Support Vector Machines and Kernels 431\nTABLE 12.2. Skin of the orange: Shown are mean (standard error of the mean)\nof the test error over 50simulations. BRUTO \ufb01ts an additive spline model adap-\ntively, while MARS \ufb01ts a low-order interaction model adaptivel y.\nTest Error (SE)\nMethod No Noise Features Six Noise Features\n1 SV Classi\ufb01er 0.450 (0.003) 0.472 (0.003)\n2 SVM/poly 2 0.078 (0.003) 0.152 (0.004)\n3 SVM/poly 5 0.180 (0.004) 0.370 (0.004)\n4 SVM/poly 10 0.230 (0.003) 0.434 (0.002)\n5 BRUTO 0.084 (0.003) 0.090 (0.003)\n6 MARS 0.156 (0.004) 0.173 (0.005)\nBayes 0.029 0.029\n12.3.4 SVMs and the Curse of Dimensionality\nIn this section, we address the question of whether SVMs have some edge\non the curse of dimensionality. Notice that in expression (12.23) we are not\nallowed a fully general inner product in the space of powers and products.\nFor example, all terms of the form 2 XjX\u2032\njare given equal weight, and the\nkernel cannot adapt itself to concentrate on subspaces. If the number of\nfeatures pwere large, but the class separation occurred only in the linear\nsubspace spanned by say X1andX2, this kernel would not easily \ufb01nd the\nstructure and would su\ufb00er from having many dimensions to search over.\nOne would have to build knowledge about the subspace into the kernel;\nthat is, tell it to ignore all but the \ufb01rst two inputs. If such knowledge were\navailable a priori, much of statistical learning would be made much easier .\nA major goal of adaptive methods is to discover such structure.\nWe support these statements with an illustrative example. We generated\n100 observations in each of two classes. The \ufb01rst class has four standard\nnormal independent features X1,X2,X3,X4. The second class also has four\nstandard normal independent features, but conditioned on 9 \u2264/summationtextX2\nj\u226416.\nThis is a relatively easy problem. As a second harder problem, we aug-\nmented the features with an additional six standard Gaussian noise fea-\ntures. Hence the second class almost completely surrounds the \ufb01rst, like the\nskin surrounding the orange, in a four-dimensional subspace. The Bayes er-\nror rate for this problem is 0 .029 (irrespective of dimension). We generated\n1000 test observations to compare di\ufb00erent procedures. The average test\nerrors over 50 simulations, with and without noise features, are shown in\nTable 12.2.\nLine 1 uses the support vector classi\ufb01er in the original feature space.\nLines 2\u20134 refer to the support vector machine with a 2-, 5- and 10-dimension-\nal polynomial kernel. For all support vector procedures, we chose the cost\nparameter Cto minimize the test error, to be as fair as possible to the", "450": "432 12. Flexible Discriminants\n1e\u221201 1e+01 1e+030.20 0.25 0.30 0.35\n1e\u221201 1e+01 1e+03 1e\u221201 1e+01 1e+03 1e\u221201 1e+01 1e+03Test Error\nCTest Error Curves \u2212 SVM with Radial Kernel\n\u03b3= 5 \u03b3= 1 \u03b3= 0.5 \u03b3= 0.1\nFIGURE 12.6. Test-error curves as a function of the cost parameter Cfor the\nradial-kernel SVM classi\ufb01er on the mixture data. At the top of eac h plot is the\nscale parameter \u03b3for the radial kernel: K\u03b3(x, y) = exp \u2212\u03b3||x\u2212y||2. The optimal\nvalue for Cdepends quite strongly on the scale of the kernel. The Bayes erro r\nrate is indicated by the broken horizontal lines.\nmethod. Line 5 \ufb01ts an additive spline model to the ( \u22121,+1) response by\nleast squares, using the BRUTO algorithm for additive models, described\nin Hastie and Tibshirani (1990). Line 6 uses MARS (multivariate adaptiv e\nregression splines) allowing interaction of all orders, as described in Chap-\nter 9; as such it is comparable with the SVM/poly 10. Both BRUTO and\nMARS have the ability to ignore redundant variables. Test error was not\nused to choose the smoothing parameters in either of lines 5 or 6.\nIn the original feature space, a hyperplane cannot separate the classes,\nand the support vector classi\ufb01er (line 1) does poorly. The polynomial sup-\nport vector machine makes a substantial improvement in test error rate,\nbut is adversely a\ufb00ected by the six noise features. It is also very sensitive to\nthe choice of kernel: the second degree polynomial kernel (line 2) does best,\nsince the true decision boundary is a second-degree polynomial. However,\nhigher-degree polynomial kernels (lines 3 and 4) do much worse. BRUTO\nperforms well, since the boundary is additive. BRUTO and MARS adapt\nwell: their performance does not deteriorate much in the presence of noise.\n12.3.5 A Path Algorithm for the SVM Classi\ufb01er\nThe regularization parameter for the SVM classi\ufb01er is the cost parameter\nC, or its inverse \u03bbin (12.25). Common usage is to set Chigh, leading often\nto somewhat over\ufb01t classi\ufb01ers.\nFigure 12.6 shows the test error on the mixture data as a function of\nC, using di\ufb00erent radial-kernel parameters \u03b3. When \u03b3= 5 (narrow peaked\nkernels), the heaviest regularization (small C) is called for. With \u03b3= 1", "451": "12.3 Support Vector Machines and Kernels 433\n\u22120.5 0.0 0.5 1.0 1.5 2.0\u22121.0 \u22120.5 0.0 0.5 1.0 1.5789\n1011\n12\n123\n45\n61/||\u03b2|| f(x) = 0f(x) = +1\nf(x) =\u22121\n0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 101\n2\n34\n56\n789\n1011\n12\n\u03b1i(\u03bb)\u03bb\nFIGURE 12.7. A simple example illustrates the SVM path algorithm. (left\npanel:) This plot illustrates the state of the model at \u03bb= 0.05. The \u2018 \u2018 + 1\u201d\npoints are orange, the \u201c \u22121\u201d blue. \u03bb= 1/2, and the width of the soft margin\nis2/||\u03b2||= 2\u00d70.587. Two blue points {3,5}are misclassi\ufb01ed, while the two or-\nange points {10,12}are correctly classi\ufb01ed, but on the wrong side of their margin\nf(x) = +1 ; each of these has yif(xi)<1. The three square shaped points {2,6,7}\nare exactly on their margins. (right panel:) This plot shows the piecewise linear\npro\ufb01les \u03b1i(\u03bb). The horizontal broken line at \u03bb= 1/2indicates the state of the \u03b1i\nfor the model in the left plot.\n(the value used in Figure 12.3), an intermediate value of Cis required.\nClearly in situations such as these, we need to determine a good choice\nforC, perhaps by cross-validation. Here we describe a path algorithm (in\nthe spirit of Section 3.8) for e\ufb03ciently \ufb01tting the entire sequence of SVM\nmodels obtained by varying C.\nIt is convenient to use the loss+penalty formulation (12.25), along with\nFigure 12.4. This leads to a solution for \u03b2at a given value of \u03bb:\n\u03b2\u03bb=1\n\u03bbN/summationdisplay\ni=1\u03b1iyixi. (12.33)\nThe\u03b1iare again Lagrange multipliers, but in this case they all lie in [0 ,1].\nFigure 12.7 illustrates the setup. It can be shown that the KKT optimal-\nity conditions imply that the labeled points ( xi,yi) fall into three distinct\ngroups:", "452": "434 12. Flexible Discriminants\n\u2022Observations correctly classi\ufb01ed and outside their margins. They have\nyif(xi)>1, and Lagrange multipliers \u03b1i= 0. Examples are the\norange points 8, 9 and 11, and the blue points 1 and 4.\n\u2022Observations sitting on their margins with yif(xi) = 1, with Lagrange\nmultipliers \u03b1i\u2208[0,1]. Examples are the orange 7 and the blue 2 and\n8.\n\u2022Observations inside their margins have yif(xi)<1, with \u03b1i= 1.\nExamples are the blue 3 and 5, and the orange 10 and 12.\nThe idea for the path algorithm is as follows. Initially \u03bbis large, the\nmargin 1 /||\u03b2\u03bb||is wide, and all points are inside their margin and have\n\u03b1i= 1. As \u03bbdecreases, 1 /||\u03b2\u03bb||decreases, and the margin gets narrower.\nSome points will move from inside their margins to outside their margins,\nand their \u03b1iwill change from 1 to 0. By continuity of the \u03b1i(\u03bb), these points\nwilllinger on the margin during this transition. From (12.33) we see that\nthe points with \u03b1i= 1 make \ufb01xed contributions to \u03b2(\u03bb), and those with\n\u03b1i= 0 make no contribution. So all that changes as \u03bbdecreases are the\n\u03b1i\u2208[0,1] of those (small number) of points on the margin. Since all these\npoints have yif(xi) = 1, this results in a small set of linear equations that\nprescribe how \u03b1i(\u03bb) and hence \u03b2\u03bbchanges during these transitions. This\nresults in piecewise linear paths for each of the \u03b1i(\u03bb). The breaks occur\nwhen points cross the margin. Figure 12.7 (right panel) shows the \u03b1i(\u03bb)\npro\ufb01les for the small example in the left panel.\nAlthough we have described this for linear SVMs, exactly the same idea\nworks for nonlinear models, in which (12.33) is replaced by\nf\u03bb(x) =1\n\u03bbN/summationdisplay\ni=1\u03b1iyiK(x,xi). (12.34)\nDetails can be found in Hastie et al. (2004). An Rpackagesvmpath is\navailable on CRAN for \ufb01tting these models.\n12.3.6 Support Vector Machines for Regression\nIn this section we show how SVMs can be adapted for regression with a\nquantitative response, in ways that inherit some of the properties of the\nSVM classi\ufb01er. We \ufb01rst discuss the linear regression model\nf(x) =xT\u03b2+\u03b20, (12.35)\nand then handle nonlinear generalizations. To estimate \u03b2, we consider min-\nimization of\nH(\u03b2,\u03b20) =N/summationdisplay\ni=1V(yi\u2212f(xi)) +\u03bb\n2\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al2, (12.36)", "453": "12.3 Support Vector Machines and Kernels 435\n-4 -2 0 2 4-1 0 1 2 3 4\n-4 -2 0 2 40 2 4 6 8 10 12\n\u01eb \u2212\u01eb c \u2212cVH(r)V\u01eb(r)\nr r\nFIGURE 12.8. The left panel shows the \u01eb-insensitive error function used by the\nsupport vector regression machine. The right panel shows the e rror function used\nin Huber\u2019s robust regression (blue curve). Beyond |c|, the function changes from\nquadratic to linear.\nwhere\nV\u01eb(r) =/braceleft\uf8ecigg\n0 if |r|< \u01eb,\n|r| \u2212\u01eb,otherwise.(12.37)\nThis is an \u201c \u01eb-insensitive\u201d error measure, ignoring errors of size less than\n\u01eb(left panel of Figure 12.8). There is a rough analogy with the support\nvector classi\ufb01cation setup, where points on the correct side of the deci-\nsion boundary and far away from it, are ignored in the optimization. In\nregression, these \u201clow error\u201d points are the ones with small residuals.\nIt is interesting to contrast this with error measures used in robust re-\ngression in statistics. The most popular, due to Huber (1964), has the for m\nVH(r) =/braceleft\uf8ecigg\nr2/2 if |r| \u2264c,\nc|r| \u2212c2/2,|r|> c,(12.38)\nshown in the right panel of Figure 12.8. This function reduces from quadratic\nto linear the contributions of observations with absolute residual greater\nthan a prechosen constant c. This makes the \ufb01tting less sensitive to out-\nliers. The support vector error measure (12.37) also has linear tails (beyo nd\n\u01eb), but in addition it \ufb02attens the contributions of those cases with small\nresiduals.\nIf\u02c6\u03b2,\u02c6\u03b20are the minimizers of H, the solution function can be shown to\nhave the form\n\u02c6\u03b2=N/summationdisplay\ni=1(\u02c6\u03b1\u2217\ni\u2212\u02c6\u03b1i)xi, (12.39)\n\u02c6f(x) =N/summationdisplay\ni=1(\u02c6\u03b1\u2217\ni\u2212\u02c6\u03b1i)\u221dan}\u230a\u2207a\u230bketle{tx,xi\u221dan}\u230a\u2207a\u230bket\u2207i}ht+\u03b20, (12.40)", "454": "436 12. Flexible Discriminants\nwhere \u02c6 \u03b1i,\u02c6\u03b1\u2217\niare positive and solve the quadratic programming problem\nmin\n\u03b1i,\u03b1\u2217\ni\u01ebN/summationdisplay\ni=1(\u03b1\u2217\ni+\u03b1i)\u2212N/summationdisplay\ni=1yi(\u03b1\u2217\ni\u2212\u03b1i) +1\n2N/summationdisplay\ni,i\u2032=1(\u03b1\u2217\ni\u2212\u03b1i)(\u03b1\u2217\ni\u2032\u2212\u03b1i\u2032)\u221dan}\u230a\u2207a\u230bketle{txi,xi\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht\nsubject to the constraints\n0\u2264\u03b1i, \u03b1\u2217\ni\u22641/\u03bb,\nN/summationdisplay\ni=1(\u03b1\u2217\ni\u2212\u03b1i) = 0, (12.41)\n\u03b1i\u03b1\u2217\ni= 0.\nDue to the nature of these constraints, typically only a subset of the solution\nvalues (\u02c6 \u03b1\u2217\ni\u2212\u02c6\u03b1i) are nonzero, and the associated data values are called the\nsupport vectors. As was the case in the classi\ufb01cation setting, the solution\ndepends on the input values only through the inner products \u221dan}\u230a\u2207a\u230bketle{txi,xi\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht. Thus\nwe can generalize the methods to richer spaces by de\ufb01ning an appropriate\ninner product, for example, one of those de\ufb01ned in (12.22).\nNote that there are parameters, \u01eband\u03bb, associated with the criterion\n(12.36). These seem to play di\ufb00erent roles. \u01ebis a parameter of the loss\nfunction V\u01eb, just like cis for VH. Note that both V\u01ebandVHdepend on the\nscale of yand hence r. If we scale our response (and hence use VH(r/\u03c3) and\nV\u01eb(r/\u03c3) instead), then we might consider using preset values for cand\u01eb(the\nvalue c= 1.345 achieves 95% e\ufb03ciency for the Gaussian). The quantity \u03bb\nis a more traditional regularization parameter, and can be estimated for\nexample by cross-validation.\n12.3.7 Regression and Kernels\nAs discussed in Section 12.3.3, this kernel property is not unique to sup-\nport vector machines. Suppose we consider approximation of the regression\nfunction in terms of a set of basis functions {hm(x)},m= 1,2,... ,M :\nf(x) =M/summationdisplay\nm=1\u03b2mhm(x) +\u03b20. (12.42)\nTo estimate \u03b2and\u03b20we minimize\nH(\u03b2,\u03b20) =N/summationdisplay\ni=1V(yi\u2212f(xi)) +\u03bb\n2/summationdisplay\n\u03b22\nm (12.43)\nfor some general error measure V(r). For any choice of V(r), the solution\n\u02c6f(x) =/summationtext\u02c6\u03b2mhm(x) +\u02c6\u03b20has the form\n\u02c6f(x) =N/summationdisplay\ni=1\u02c6aiK(x,xi) (12.44)", "455": "12.3 Support Vector Machines and Kernels 437\nwithK(x,y) =/summationtextM\nm=1hm(x)hm(y). Notice that this has the same form\nas both the radial basis function expansion and a regularization estimate,\ndiscussed in Chapters 5 and 6.\nFor concreteness, let\u2019s work out the case V(r) =r2. LetHbe the N\u00d7M\nbasis matrix with imth element hm(xi), and suppose that M > N is large.\nFor simplicity we assume that \u03b20= 0, or that the constant is absorbed in\nh; see Exercise 12.3 for an alternative.\nWe estimate \u03b2by minimizing the penalized least squares criterion\nH(\u03b2) = (y\u2212H\u03b2)T(y\u2212H\u03b2) +\u03bb\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al2. (12.45)\nThe solution is\n\u02c6y=H\u02c6\u03b2 (12.46)\nwith\u02c6\u03b2determined by\n\u2212HT(y\u2212H\u02c6\u03b2) +\u03bb\u02c6\u03b2= 0. (12.47)\nFrom this it appears that we need to evaluate the M\u00d7Mmatrix of inner\nproducts in the transformed space. However, we can premultiply by Hto\ngive\nH\u02c6\u03b2= (HHT+\u03bbI)\u22121HHTy. (12.48)\nTheN\u00d7Nmatrix HHTconsists of inner products between pairs of obser-\nvations i,i\u2032; that is, the evaluation of an inner product kernel {HHT}i,i\u2032=\nK(xi,xi\u2032). It is easy to show (12.44) directly in this case, that the predicted\nvalues at an arbitrary xsatisfy\n\u02c6f(x) = h(x)T\u02c6\u03b2\n=N/summationdisplay\ni=1\u02c6\u03b1iK(x,xi), (12.49)\nwhere \u02c6 \u03b1= (HHT+\u03bbI)\u22121y. As in the support vector machine, we need not\nspecify or evaluate the large set of functions h1(x),h2(x),... ,h M(x). Only\nthe inner product kernel K(xi,xi\u2032) need be evaluated, at the Ntraining\npoints for each i,i\u2032and at points xfor predictions there. Careful choice\nofhm(such as the eigenfunctions of particular, easy-to-evaluate kernels\nK) means, for example, that HHTcan be computed at a cost of N2/2\nevaluations of K, rather than the direct cost N2M.\nNote, however, that this property depends on the choice of squared norm\n\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al2in the penalty. It does not hold, for example, for the L1norm |\u03b2|,\nwhich may lead to a superior model.", "456": "438 12. Flexible Discriminants\n12.3.8 Discussion\nThe support vector machine can be extended to multiclass problems, es-\nsentially by solving many two-class problems. A classi\ufb01er is built for each\npair of classes, and the \ufb01nal classi\ufb01er is the one that dominates the most\n(Kressel, 1999; Friedman, 1996; Hastie and Tibshirani, 1998). Alternati vely,\none could use the multinomial loss function along with a suitable kernel,\nas in Section 12.3.3. SVMs have applications in many other supervised\nand unsupervised learning problems. At the time of this writing, empirical\nevidence suggests that it performs well in many real learning problems.\nFinally, we mention the connection of the support vector machine and\nstructural risk minimization (7.9). Suppose the training points (or their\nbasis expansion) are contained in a sphere of radius R, and let G(x) =\nsign[f(x)] = sign[ \u03b2Tx+\u03b20] as in (12.2). Then one can show that the class\nof functions {G(x),\u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al \u2264A}has VC-dimension hsatisfying\nh\u2264R2A2. (12.50)\nIff(x) separates the training data, optimally for \u221d\u230aa\u2207\u2308\u230al\u03b2\u221d\u230aa\u2207\u2308\u230al \u2264A, then with\nprobability at least 1 \u2212\u03b7over training sets (Vapnik, 1996, page 139):\nError Test\u22644h[log (2 N/h) + 1]\u2212log (\u03b7/4)\nN. (12.51)\nThe support vector classi\ufb01er was one of the \ufb01rst practical learning pro-\ncedures for which useful bounds on the VC dimension could be obtained,\nand hence the SRM program could be carried out. However in the deriva-\ntion, balls are put around the data points\u2014a process that depends on the\nobserved values of the features. Hence in a strict sense, the VC complexity\nof the class is not \ufb01xed a priori, before seeing the features.\nThe regularization parameter Ccontrols an upper bound on the VC\ndimension of the classi\ufb01er. Following the SRM paradigm, we could choose C\nby minimizing the upper bound on the test error, given in (12.51). However,\nit is not clear that this has any advantage over the use of cross-validation\nfor choice of C.\n12.4 Generalizing Linear Discriminant Analysis\nIn Section 4.3 we discussed linear discriminant analysis (LDA), a funda-\nmental tool for classi\ufb01cation. For the remainder of this chapter we discuss\na class of techniques that produce better classi\ufb01ers than LDA by directly\ngeneralizing LDA.\nSome of the virtues of LDA are as follows:\n\u2022It is a simple prototype classi\ufb01er. A new observation is classi\ufb01ed to the\nclass with closest centroid. A slight twist is that distance is measured\nin the Mahalanobis metric, using a pooled covariance estimate.", "457": "12.4 Generalizing Linear Discriminant Analysis 439\n\u2022LDA is the estimated Bayes classi\ufb01er if the observations are multi-\nvariate Gaussian in each class, with a common covariance matrix.\nSince this assumption is unlikely to be true, this might not seem to\nbe much of a virtue.\n\u2022The decision boundaries created by LDA are linear, leading to deci-\nsion rules that are simple to describe and implement.\n\u2022LDA provides natural low-dimensional views of the data. For exam-\nple, Figure 12.12 is an informative two-dimensional view of data in\n256 dimensions with ten classes.\n\u2022Often LDA produces the best classi\ufb01cation results, because of its\nsimplicity and low variance. LDA was among the top three classi\ufb01ers\nfor 11 of the 22 datasets studied in the STATLOG project (Michie et\nal., 1994)3.\nUnfortunately the simplicity of LDA causes it to fail in a number of situa-\ntions as well:\n\u2022Often linear decision boundaries do not adequately separate the classes.\nWhen Nis large, it is possible to estimate more complex decision\nboundaries. Quadratic discriminant analysis (QDA) is often useful\nhere, and allows for quadratic decision boundaries. More generally\nwe would like to be able to model irregular decision boundaries.\n\u2022The aforementioned shortcoming of LDA can often be paraphrased\nby saying that a single prototype per class is insu\ufb03cient. LDA uses\na single prototype (class centroid) plus a common covariance matrix\nto describe the spread of the data in each class. In many situations,\nseveral prototypes are more appropriate.\n\u2022At the other end of the spectrum, we may have way too many (corre-\nlated) predictors, for example, in the case of digitized analogue signals\nand images. In this case LDA uses too many parameters, which are\nestimated with high variance, and its performance su\ufb00ers. In cases\nsuch as this we need to restrict or regularize LDA even further.\nIn the remainder of this chapter we describe a class of techniques that\nattend to all these issues by generalizing the LDA model. This is achieved\nlargely by three di\ufb00erent ideas.\nThe \ufb01rst idea is to recast the LDA problem as a linear regression problem.\nMany techniques exist for generalizing linear regression to more \ufb02exible,\nnonparametric forms of regression. This in turn leads to more \ufb02exible forms\nof discriminant analysis, which we call FDA. In most cases of interest, t he\n3This study predated the emergence of SVMs.", "458": "440 12. Flexible Discriminants\nregression procedures can be seen to identify an enlarged set of predictors\nvia basis expansions. FDA amounts to LDA in this enlarged space, the\nsame paradigm used in SVMs.\nIn the case of too many predictors, such as the pixels of a digitized image,\nwe do not want to expand the set: it is already too large. The second idea is\nto \ufb01t an LDA model, but penalize its coe\ufb03cients to be smooth or otherwise\ncoherent in the spatial domain, that is, as an image. We call this procedure\npenalized discriminant analysis or PDA. With FDA itself, the expanded\nbasis set is often so large that regularization is also required (again a s in\nSVMs). Both of these can be achieved via a suitably regularized regression\nin the context of the FDA model.\nThe third idea is to model each class by a mixture of two or more Gaus-\nsians with di\ufb00erent centroids, but with every component Gaussian, both\nwithin and between classes, sharing the same covariance matrix. This allows\nfor more complex decision boundaries, and allows for subspace reduction\nas in LDA. We call this extension mixture discriminant analysis or MDA.\nAll three of these generalizations use a common framework by exploiting\ntheir connection with LDA.\n12.5 Flexible Discriminant Analysis\nIn this section we describe a method for performing LDA using linear re-\ngression on derived responses. This in turn leads to nonparametric and \ufb02ex-\nible alternatives to LDA. As in Chapter 4, we assume we have observations\nwith a quantitative response Gfalling into one of Kclasses G={1,... ,K },\neach having measured features X. Suppose \u03b8:G \u221dma\u221asto\u2192IR1is a function that\nassigns scores to the classes, such that the transformed class labels are op-\ntimally predicted by linear regression on X: If our training sample has the\nform ( gi,xi), i= 1,2,... ,N , then we solve\nmin\n\u03b2,\u03b8N/summationdisplay\ni=1/parenleftbig\n\u03b8(gi)\u2212xT\ni\u03b2/parenrightbig2, (12.52)\nwith restrictions on \u03b8to avoid a trivial solution (mean zero and unit vari-\nance over the training data). This produces a one-dimensional separation\nbetween the classes.\nMore generally, we can \ufb01nd up to L\u2264K\u22121 sets of independent scorings\nfor the class labels, \u03b81,\u03b82,... ,\u03b8 L, andLcorresponding linear maps \u03b7\u2113(X) =\nXT\u03b2\u2113, \u2113= 1,... ,L , chosen to be optimal for multiple regression in IRp. The\nscores \u03b8\u2113(g) and the maps \u03b2\u2113are chosen to minimize the average squared\nresidual,\nASR=1\nNL/summationdisplay\n\u2113=1/bracketleft\uf8eciggN/summationdisplay\ni=1/parenleftbig\n\u03b8\u2113(gi)\u2212xT\ni\u03b2\u2113/parenrightbig2/bracketright\uf8ecigg\n. (12.53)", "459": "12.5 Flexible Discriminant Analysis 441\nThe set of scores are assumed to be mutually orthogonal and normalized\nwith respect to an appropriate inner product to prevent trivial zero\nsolutions.\nWhy are we going down this road? It can be shown that the sequence\nof discriminant (canonical) vectors \u03bd\u2113derived in Section 4.3.3 are identical\nto the sequence \u03b2\u2113up to a constant (Mardia et al., 1979; Hastie et al.,\n1995). Moreover, the Mahalanobis distance of a test point xto the kth\nclass centroid \u02c6 \u03b8kis given by\n\u03b4J(x,\u02c6\u03b8k) =K\u22121/summationdisplay\n\u2113=1w\u2113(\u02c6\u03b7\u2113(x)\u2212\u00af\u03b7k\n\u2113)2+D(x), (12.54)\nwhere \u00af \u03b7k\n\u2113is the mean of the \u02c6 \u03b7\u2113(xi) in the kth class, and D(x) does not\ndepend on k. Here w\u2113are coordinate weights that are de\ufb01ned in terms of\nthe mean squared residual r2\n\u2113of the \u2113th optimally scored \ufb01t\nw\u2113=1\nr2\n\u2113(1\u2212r2\n\u2113). (12.55)\nIn Section 4.3.2 we saw that these canonical distances are all that is needed\nfor classi\ufb01cation in the Gaussian setup, with equal covariances in each class.\nTo summarize:\nLDA can be performed by a sequence of linear regressions, fol-\nlowed by classi\ufb01cation to the closest class centroid in the space\nof \ufb01ts. The analogy applies both to the reduced rank version,\nor the full rank case when L=K\u22121.\nThe real power of this result is in the generalizations that it invites. We\ncan replace the linear regression \ufb01ts \u03b7\u2113(x) =xT\u03b2\u2113by far more \ufb02exible,\nnonparametric \ufb01ts, and by analogy achieve a more \ufb02exible classi\ufb01er than\nLDA. We have in mind generalized additive \ufb01ts, spline functions, MARS\nmodels and the like. In this more general form the regression problems are\nde\ufb01ned via the criterion\nASR({\u03b8\u2113,\u03b7\u2113}L\n\u2113=1) =1\nNL/summationdisplay\n\u2113=1/bracketleft\uf8eciggN/summationdisplay\ni=1(\u03b8\u2113(gi)\u2212\u03b7\u2113(xi))2+\u03bbJ(\u03b7\u2113)/bracketright\uf8ecigg\n,(12.56)\nwhere Jis a regularizer appropriate for some forms of nonparametric regres-\nsion, such as smoothing splines, additive splines and lower-order ANOVA\nspline models. Also included are the classes of functions and associated\npenalties generated by kernels, as in Section 12.3.3.\nBefore we describe the computations involved in this generalization, let\nus consider a very simple example. Suppose we use degree-2 polynomial\nregression for each \u03b7\u2113. The decision boundaries implied by the (12.54) will\nbe quadratic surfaces, since each of the \ufb01tted functions is quadratic, and as", "460": "442 12. Flexible Discriminants\n-2 0 2-2 0 2o\nooo\noo\nooo\noo\no o\no\noooo\noo\noo oo\no\no\nooo\noooo\noo\nooo\noo\noo\noo\noo\nooo\no\nooo\noo\no\noo\noo\noo\no\noo\no\nooo\noo\no\noo\no\noooo\nooo\noo\noo\nooo\noo o\noo\nooo\noo\no\nFIGURE 12.9. The data consist of 50points generated from each of N(0, I)and\nN(0,9\n4I). The solid black ellipse is the decision boundary found by FDA u sing\ndegree-two polynomial regression. The dashed purple circle i s the Bayes decision\nboundary.\nin LDA their squares cancel out when comparing distances. We could have\nachieved identical quadratic boundaries in a more conventional way, by\naugmenting our original predictors with their squares and cross-products.\nIn the enlarged space one performs an LDA, and the linear boundaries in\nthe enlarged space map down to quadratic boundaries in the original space.\nA classic example is a pair of multivariate Gaussians centered at the origi n,\none having covariance matrix I, and the other cIforc >1; Figure 12.9\nillustrates. The Bayes decision boundary is the sphere \u221d\u230aa\u2207\u2308\u230alx\u221d\u230aa\u2207\u2308\u230al=pclogc\n2(c\u22121), which\nis a linear boundary in the enlarged space.\nMany nonparametric regression procedures operate by generating a basis\nexpansion of derived variables, and then performing a linear regression in\nthe enlarged space. The MARS procedure (Chapter 9) is exactly of this\nform. Smoothing splines and additive spline models generate an extremely\nlarge basis set ( N\u00d7pbasis functions for additive splines), but then perform\na penalized regression \ufb01t in the enlarged space. SVMs do as well; see also\nthe kernel-based regression example in Section 12.3.7. FDA in this case can\nbe shown to perform a penalized linear discriminant analysis in the enlarged\nspace. We elaborate in Section 12.6. Linear boundaries in the enlarged space\nmap down to nonlinear boundaries in the reduced space. This is exactly the\nsame paradigm that is used with support vector machines (Section 12.3).\nWe illustrate FDA on the speech recognition example used in Chapter\n4.), with K= 11 classes and p= 10 predictors. The classes correspond to", "461": "12.5 Flexible Discriminant Analysis 443\noooo oo\noooooooo\noooo\noo\no\noo\no\noooooooooooo\nooo\no\noooooooo\nooooooo\no\noo\no\nooooo\no\nooooooo\noooooo\no\no\nooooooooooooo\nooooooooooo\noooo\noooo\no\nooooo\noooooo o\no\noooooooooooo\noo\no\noo\nooooooooooooo\noooooooooooooooooooooooooooooo\no\no\nooooooo\no\no\nooooooo\noooooooooooo\no\nooooooooooooooooo\noo\no\no\noooooooo\nooooooooooooooooo\noooo\no\no\nooooo\no\nooooo\no\noo\no\no\no\no\nooooooooooooo\noo\noooooo\nooooooooo\nooo\nooooooooo\no\noo\no\nooo\nooooooooooooooo\nooo\noooooooo\noooo\nooooooooooooo\no\noooo\nooooo\noooooo\noo\no\no\no\no\noooooo\no\noooooo\noooo\no\nooooooooooooo\no ooooooooooooooooo\no\no\nooooo\nooooooooooo\no\no\nooo\nooooooo\noooooo\noooooo\noooooooooo\no\nooooooo\noooooooooooo\noo\no\nooo\nCoordinate 1 for Training DataCoordinate 2 for Training DataLinear Discriminant Analysis\noo oooooo\noooo\noooooooooooo\noooooo\no\nooo\no\noooooo\noooo\nooo\nooooooo\nooooo\noooooo\nooooooooooo\nooooooo\nooooooo\no\noooo\noooooo\nooo ooooooooo\noooooooo\noooo\noooooo\no\noooo\nooooooo\noooooo\noooooooooooooooooo\noooooooooooooooooooooooooooo\nooooooooooooo\nooo\no\no oooooooooooooooooooo\noooo\no\nooooooo\noo\noooo\nooo\noo\nooooo\nooo\no\noooo ooooooooo\no\no\noooo\noooooooooo\nooo\noo\noo\nooo\nooo\no\no\no\noooooo\no\noooo\no\noooooo\nooo\nooooooo\nooo\nooooooooo\noooo\no\nooo\nooooo\no\nooooooo\no\noooooo\no\noooo\noooo\no\nooo\noooo\no\noooooo\no\noooooooooooo\noo\no\nooo\no\no\no\noooooo\no\noooooooooooooooooo\noooooooo\noooo\noooooo\nooooooooo\no\noo\nooooooo\noooooo\no\noooo\nooooooooooo\nooooo oo\noooo\noooo\noooo\nCoordinate 1 for Training DataCoordinate 2 for Training DataFlexible Discriminant Analysis -- Bruto\nFIGURE 12.10. The left plot shows the \ufb01rst two LDA canonical variates for\nthe vowel training data. The right plot shows the corresponding p rojection when\nFDA/BRUTO is used to \ufb01t the model; plotted are the \ufb01tted regress ion functions\n\u02c6\u03b71(xi)and\u02c6\u03b72(xi). Notice the improved separation. The colors represent the ele ven\ndi\ufb00erent vowel sounds.\n11 vowel sounds, each contained in 11 di\ufb00erent words. Here are the words,\npreceded by the symbols that represent them:\nVowel Word Vowel Word Vowel Word Vowel Word\ni: heed O hod I hid C: hoard\nE head U hood A had u: who\u2019d\na: hard 3: heard Y hud\nEach of eight speakers spoke each word six times in the training set, and\nlikewise seven speakers in the test set. The ten predictors are derived from\nthe digitized speech in a rather complicated way, but standard in the speech\nrecognition world. There are thus 528 training observations, and 462 test\nobservations. Figure 12.10 shows two-dimensional projections produced by\nLDA and FDA. The FDA model used adaptive additive-spline regression\nfunctions to model the \u03b7\u2113(x), and the points plotted in the right plot have\ncoordinates \u02c6 \u03b71(xi) and \u02c6 \u03b72(xi). The routine used in S-PLUS is called bruto ,\nhence the heading on the plot and in Table 12.3. We see that \ufb02exible model-\ning has helped to separate the classes in this case. Table 12.3 shows training\nand test error rates for a number of classi\ufb01cation techniques. FDA/MARS\nrefers to Friedman\u2019s multivariate adaptive regression splines; degree = 2\nmeans pairwise products are permitted. Notice that for FDA/MARS, the\nbest classi\ufb01cation results are obtained in a reduced-rank subspace.", "462": "444 12. Flexible Discriminants\nTABLE 12.3. Vowel recognition data performance results. The results for ne ural\nnetworks are the best among a much larger set, taken from a neural network\narchive. The notation FDA/BRUTO refers to the regression met hod used with\nFDA.\nTechnique Error Rates\nTraining Test\n(1) LDA 0.32 0.56\nSoftmax 0.48 0.67\n(2) QDA 0.01 0.53\n(3) CART 0.05 0.56\n(4) CART (linear combination splits) 0.05 0.54\n(5) Single-layer perceptron 0.67\n(6) Multi-layer perceptron (88 hidden units) 0.49\n(7) Gaussian node network (528 hidden units) 0.45\n(8) Nearest neighbor 0.44\n(9) FDA/BRUTO 0.06 0.44\nSoftmax 0.11 0.50\n(10) FDA/MARS (degree = 1) 0.09 0.45\nBest reduced dimension (=2) 0.18 0.42\nSoftmax 0.14 0.48\n(11) FDA/MARS (degree = 2) 0.02 0.42\nBest reduced dimension (=6) 0.13 0.39\nSoftmax 0.10 0.50\n12.5.1 Computing the FDA Estimates\nThe computations for the FDA coordinates can be simpli\ufb01ed in many im-\nportant cases, in particular when the nonparametric regression procedure\ncan be represented as a linear operator. We will denote this operator by\nS\u03bb; that is, \u02c6y=S\u03bby, where yis the vector of responses and \u02c6ythe vector\nof \ufb01ts. Additive splines have this property, if the smoothing parameters are\n\ufb01xed, as does MARS once the basis functions are selected. The subscript \u03bb\ndenotes the entire set of smoothing parameters. In this case optimal scoring\nis equivalent to a canonical correlation problem, and the solution can be\ncomputed by a single eigen-decomposition. This is pursued in Exercise 12.6,\nand the resulting algorithm is presented here.\nWe create an N\u00d7Kindicator response matrix Yfrom the responses gi,\nsuch that yik= 1 if gi=k, otherwise yik= 0. For a \ufb01ve-class problem Y\nmight look like the following:", "463": "12.5 Flexible Discriminant Analysis 445\n0\nBBBBBBBBB@C1C2C3C4C5\ng1= 2 0 1 0 0 0\ng2= 1 1 0 0 0 0\ng3= 1 1 0 0 0 0\ng4= 5 0 0 0 0 1\ng5= 4 0 0 0 1 0\n......\ngN= 3 0 0 1 0 01\nCCCCCCCCCA\nHere are the computational steps:\n1.Multivariate nonparametric regression. Fit a multiresponse, adaptive\nnonparametric regression of YonX, giving \ufb01tted values \u02c6Y. LetS\u03bb\nbe the linear operator that \ufb01ts the \ufb01nal chosen model, and \u03b7\u2217(x) be\nthe vector of \ufb01tted regression functions.\n2.Optimal scores. Compute the eigen-decomposition of YT\u02c6Y=YTS\u03bbY,\nwhere the eigenvectors \u0398are normalized: \u0398TD\u03c0\u0398=I. Here D\u03c0=\nYTY/Nis a diagonal matrix of the estimated class prior\nprobabilities.\n3.Update the model from step 1 using the optimal scores: \u03b7(x) =\u0398T\u03b7\u2217(x).\nThe \ufb01rst of the Kfunctions in \u03b7(x) is the constant function\u2014 a trivial\nsolution; the remaining K\u22121 functions are the discriminant functions. The\nconstant function, along with the normalization, causes all the remaining\nfunctions to be centered.\nAgainS\u03bbcan correspond to any regression method. When S\u03bb=HX, the\nlinear regression projection operator, then FDA is linear discriminant anal-\nysis. The software that we reference in the Computational Considerations\nsection on page 455 makes good use of this modularity; the fdafunction\nhas amethod= argument that allows one to supply anyregression function,\nas long as it follows some natural conventions. The regression functions\nwe provide allow for polynomial regression, adaptive additive models and\nMARS. They all e\ufb03ciently handle multiple responses, so step (1) is a single\ncall to a regression routine. The eigen-decomposition in step (2) simulta-\nneously computes all the optimal scoring functions.\nIn Section 4.2 we discussed the pitfalls of using linear regression on an\nindicator response matrix as a method for classi\ufb01cation. In particular, se-\nvere masking can occur with three or more classes. FDA uses the \ufb01ts from\nsuch a regression in step (1), but then transforms them further to produce\nuseful discriminant functions that are devoid of these pitfalls. Exercise 12.9\ntakes another view of this phenomenon.", "464": "446 12. Flexible Discriminants\n12.6 Penalized Discriminant Analysis\nAlthough FDA is motivated by generalizing optimal scoring, it can also be\nviewed directly as a form of regularized discriminant analysis. Suppose the\nregression procedure used in FDA amounts to a linear regression onto a\nbasis expansion h(X), with a quadratic penalty on the coe\ufb03cients:\nASR({\u03b8\u2113,\u03b2\u2113}L\n\u2113=1) =1\nNL/summationdisplay\n\u2113=1/bracketleft\uf8eciggN/summationdisplay\ni=1(\u03b8\u2113(gi)\u2212hT(xi)\u03b2\u2113)2+\u03bb\u03b2T\n\u2113\u2126\u03b2\u2113/bracketright\uf8ecigg\n.(12.57)\nThe choice of \u2126depends on the problem. If \u03b7\u2113(x) =h(x)\u03b2\u2113is an expansion\non spline basis functions, \u2126might constrain \u03b7\u2113to be smooth over IRp. In\nthe case of additive splines, there are Nspline basis functions for each\ncoordinate, resulting in a total of Npbasis functions in h(x);\u2126in this case\nisNp\u00d7Npand block diagonal.\nThe steps in FDA can then be viewed as a generalized form of LDA,\nwhich we call penalized discriminant analysis , or PDA:\n\u2022Enlarge the set of predictors Xvia a basis expansion h(X).\n\u2022Use (penalized) LDA in the enlarged space, where the penalized\nMahalanobis distance is given by\nD(x,\u03b8) = (h(x)\u2212h(\u03b8))T(\u03a3W+\u03bb\u2126)\u22121(h(x)\u2212h(\u03b8)),(12.58)\nwhere \u03a3Wis the within-class covariance matrix of the derived vari-\nables h(xi).\n\u2022Decompose the classi\ufb01cation subspace using a penalized metric:\nmaxuT\u03a3Betusubject to uT(\u03a3W+\u03bb\u2126)u= 1.\nLoosely speaking, the penalized Mahalanobis distance tends to give less\nweight to \u201crough\u201d coordinates, and more weight to \u201csmooth\u201d ones; since\nthe penalty is not diagonal, the same applies to linear combinations that\nare rough or smooth.\nFor some classes of problems, the \ufb01rst step, involving the basis expansion,\nis not needed; we already have far too many (correlated) predictors. A\nleading example is when the objects to be classi\ufb01ed are digitized analog\nsignals:\n\u2022the log-periodogram of a fragment of spoken speech, sampled at a set\nof 256 frequencies; see Figure 5.5 on page 149.\n\u2022the grayscale pixel values in a digitized image of a handwritten digit.", "465": "12.6 Penalized Discriminant Analysis 447\nLDA: Coefficient 1 PDA: Coefficient 1 LDA: Coefficient 2 PDA: Coefficient 2 LDA: Coefficient 3 PDA: Coefficient 3\nLDA: Coefficient 4 PDA: Coefficient 4 LDA: Coefficient 5 PDA: Coefficient 5 LDA: Coefficient 6 PDA: Coefficient 6\nLDA: Coefficient 7 PDA: Coefficient 7 LDA: Coefficient 8 PDA: Coefficient 8 LDA: Coefficient 9 PDA: Coefficient 9\nFIGURE 12.11. The images appear in pairs, and represent the nine discrim-\ninant coe\ufb03cient functions for the digit recognition problem. The l eft member of\neach pair is the LDA coe\ufb03cient, while the right member is the PD A coe\ufb03cient,\nregularized to enforce spatial smoothness.\nIt is also intuitively clear in these cases why regularization is needed.\nTake the digitized image as an example. Neighboring pixel values will tend\nto be correlated, being often almost the same. This implies that the pair\nof corresponding LDA coe\ufb03cients for these pixels can be wildly di\ufb00erent\nand opposite in sign, and thus cancel when applied to similar pixel values.\nPositively correlated predictors lead to noisy, negatively correlated coe\ufb03-\ncient estimates, and this noise results in unwanted sampling variance. A\nreasonable strategy is to regularize the coe\ufb03cients to be smooth over the\nspatial domain, as with images. This is what PDA does. The computations\nproceed just as for FDA, except that an appropriate penalized regression\nmethod is used. Here hT(X)\u03b2\u2113=X\u03b2\u2113, and \u2126 is chosen so that \u03b2T\n\u2113\u2126\u03b2\u2113\npenalizes roughness in \u03b2\u2113when viewed as an image. Figure 1.2 on page 4\nshows some examples of handwritten digits. Figure 12.11 shows the dis-\ncriminant variates using LDA and PDA. Those produced by LDA appear\nassalt-and-pepper images, while those produced by PDA are smooth im-\nages. The \ufb01rst smooth image can be seen as the coe\ufb03cients of a linear\ncontrast functional for separating images with a dark central vertical stri p\n(ones, possibly sevens) from images that are hollow in the middle (zeros,\nsome fours). Figure 12.12 supports this interpretation, and with more di f-\n\ufb01culty allows an interpretation of the second coordinate. This and other", "466": "448 12. Flexible Discriminants\n-5 0 5-6 -4 -2 0 2 4 60\n0\n00\n0000\n0\n0000\n00\n00\n00000\n00\n00000\n0\n0000\n000\n00\n0\n00\n00000\n0\n0000\n0\n0\n0000\n0\n00000\n0\n000\n00\n00\n000\n000\n0000\n00\n00\n000\n00\n000\n00\n000\n0\n0000\n0000\n000\n0\n00\n00\n00\n00\n00\n00\n000\n00000 000\n0000\n00\n0\n0000\n0000\n000\n000\n00 0\n0\n0000\n0\n00\n0 000\n0\n000\n00\n0\n0\n000\n00 00\n000\n00\n0\n000\n00\n0\n00\n00 0 0\n000\n0\n00\n0\n000000\n0\n0 0\n000\n00\n0 00\n000 0\n00\n000\n00\n000\n000\n0\n000\n000\n000\n000\n000\n00\n000\n00\n00\n00\n000\n0\n000\n0000\n0\n00 00\n0000\n00\n0\n000\n00\n00\n0\n00\n0\n0000\n0\n000000\n00\n000\n0\n000\n000\n00\n000\n000\n00\n000\n00000\n00\n0\n0000\n00\n0\n0\n011\n11\n11\n11\n1\n1111\n111\n111\n11\n1\n11\n11\n111\n11\n111\n11\n111\n11\n1\n111\n1\n11\n111\n1\n111\n111\n111\n11\n11\n1111\n111\n11\n1\n1\n111\n1\n11\n1111\n1\n111\n111\n111\n11\n1\n11111\n11\n1\n11\n111\n11111\n111\n1\n11111\n1\n111\n1111\n11\n11\n11\n11111\n1111\n11\n1\n11\n11\n1\n1\n1 1\n11\n1\n111\n111\n11\n1\n1\n111\n1\n11\n111\n111\n11111\n111\n1\n11111\n111111\n111\n1\n11\n11\n1111\n111\n11\n11\n11\n111\n11\n1111111\n1\n11\n111\n111\n1\n11111111\n1\n111\n111\n1\n22\n22\n2222\n222\n22\n2\n222\n22\n2\n22\n2\n22\n2222\n222\n2\n22\n2\n2\n2\n2222\n2222\n22222\n22\n2\n22\n22\n2\n22\n222\n222\n2\n2\n22\n22\n2\n222\n2\n2\n22\n22\n2\n2\n22\n22\n222222\n222\n2\n2\n2222222\n22222\n222\n222\n2\n22222\n2\n2\n2222\n222\n22\n222\n222\n2\n22\n2\n2222\n2\n222\n2222\n22\n222\n22\n222\n2\n2\n222 22\n2\n22\n22\n22\n2222\n22\n222\n22\n22\n2222\n333\n3\n33\n3\n3 3\n333\n333\n333\n33\n33\n3\n33\n3\n3\n333\n33333\n33\n33\n3\n33333\n33\n3\n3\n333\n3333\n33\n3333\n33\n33\n3\n33\n33\n33\n3\n333\n3\n3\n333\n33\n333\n333\n333\n3\n3\n333\n33333\n3\n3333\n33\n3\n3\n33\n3\n33\n3333\n33\n3333\n33\n33\n333\n333\n333\n33\n3\n333333\n3\n3\n33 3\n333\n333\n33\n33\n33\n4\n4444\n4\n4 44444\n44\n4\n44\n444\n4\n444\n44\n44\n4\n44\n444\n44\n4\n4\n44\n44\n44\n44\n444\n44444\n44\n444\n4\n44\n44\n4\n44\n4\n44444\n4\n444\n44\n44\n4 44\n4\n4\n444\n4\n444\n44\n4444\n4\n4444\n4444\n4\n444\n44\n44\n444\n44\n444\n44\n44\n4\n4\n44444\n44\n4\n4\n444\n44 44\n44\n444\n4\n44\n4\n44\n4\n444\n444444\n444\n4444\n44\n44\n4\n44\n4\n444\n444\n44\n444\n44\n444\n4455\n55\n5\n55\n555\n5555\n5\n5 5\n555\n5\n555\n55\n55\n55\n5\n5555\n555\n55\n5555\n5555\n5\n55\n55\n55\n5\n555\n55\n55\n55555\n555\n5 55\n5\n5555\n555\n5\n555\n5\n555\n555\n55\n555\n55\n5\n555\n55\n55\n555\n555\n55\n55\n555\n55\n5 555\n55\n5\n5\n5\n55\n5\n555555\n5\n55\n55\n55\n55\n55\n55\n555\n5\n55\n6\n6666\n66\n66\n6\n66\n66\n666\n6\n66\n66\n6\n666\n666\n6\n6\n6\n666\n6\n66\n66666\n66\n6\n66\n6\n666\n66\n666\n66\n66\n6\n66\n6\n66\n666\n6\n6\n66\n666\n6\n6666\n6\n666\n66\n66\n666\n66 6\n6 66\n66\n6\n666\n666\n6\n6\n66\n666\n666\n6666\n6\n6 66\n6\n66\n6\n66\n66\n6666\n666\n66\n6666\n666\n6666\n66\n666\n666\n66\n666\n6\n6\n6\n7\n7777\n77\n777\n77\n777\n777\n77777\n777\n7\n777\n77\n77\n7\n777\n777777\n7\n777\n77\n7 77\n7\n77\n77\n77777\n7\n77\n77 7\n777\n77\n7\n77\n777\n77777\n77\n77\n7\n77777\n777\n77\n7\n77\n77\n777\n7\n7\n7\n77\n77\n77\n77\n77\n7777\n7\n77\n77\n777\n77\n7777\n77777\n7\n7\n788\n8\n8\n88\n8888\n88\n8\n8\n888\n88\n8\n888\n88\n88\n8\n888\n8\n888\n88\n88\n888\n88\n888\n88 888\n8888\n88\n8888\n88 88\n88\n88\n8888\n88 8\n8\n8 8\n88\n888\n888\n88\n8\n888\n88\n888\n8\n8\n88\n8888\n888\n888\n88\n8\n888\n88\n88\n8\n8\n88888\n88\n8\n8888888 8\n8\n8\n8\n88\n888 888\n888\n88\n8\n888\n8\n8 888\n9999\n999\n999\n99999\n999\n99\n999\n999\n9\n9\n99\n999\n9\n9\n999\n999\n999\n9999\n9\n99\n999\n99\n999\n99\n9\n9\n9999\n99\n999\n99\n9\n99\n9\n9\n99\n99\n9\n9\n9\n9\n999\n999\n9\n9\n99\n9\n99\n9\n99\n999\n99\n9999\n99999\n9\n999\n99\n999\n99\n9\n999\n9\n99\n999\n9\n99\n99\n99\n9\n99\n99\n99\n9\n99\n9\n999\n99\n999\n9\n9999\n9\n99\n999\n9901\n2\n3\n456\n78\n9\nPDA: Discriminant Coordinate 1PDA: Discriminant Coordinate 2\nFIGURE 12.12. The \ufb01rst two penalized canonical variates, evaluated for the\ntest data. The circles indicate the class centroids. The \ufb01rst co ordinate contrasts\nmainly 0\u2019s and 1\u2019s, while the second contrasts 6\u2019s and 7/9\u2019s.", "467": "12.7 Mixture Discriminant Analysis 449\nexamples are discussed in more detail in Hastie et al. (1995), who also show\nthat the regularization improves the classi\ufb01cation performance of LDA on\nindependent test data by a factor of around 25% in the cases they tried.\n12.7 Mixture Discriminant Analysis\nLinear discriminant analysis can be viewed as a prototype classi\ufb01er. Each\nclass is represented by its centroid, and we classify to the closest using an\nappropriate metric. In many situations a single prototype is not su\ufb03cient\nto represent inhomogeneous classes, and mixture models are more appro-\npriate. In this section we review Gaussian mixture models and show how\nthey can be generalized via the FDA and PDA methods discussed earlier.\nA Gaussian mixture model for the kth class has density\nP(X|G=k) =Rk/summationdisplay\nr=1\u03c0kr\u03c6(X;\u03b8kr,\u03a3), (12.59)\nwhere the mixing proportions \u03c0krsum to one. This has Rkprototypes for\nthekth class, and in our speci\ufb01cation, the same covariance matrix \u03a3is\nused as the metric throughout. Given such a model for each class, the class\nposterior probabilities are given by\nP(G=k|X=x) =/summationtextRk\nr=1\u03c0kr\u03c6(X;\u03b8kr,\u03a3)\u03a0k/summationtextK\n\u2113=1/summationtextR\u2113\nr=1\u03c0\u2113r\u03c6(X;\u03b8\u2113r,\u03a3)\u03a0\u2113, (12.60)\nwhere \u03a0 krepresent the class prior probabilities.\nWe saw these calculations for the special case of two components in\nChapter 8. As in LDA, we estimate the parameters by maximum likelihood,\nusing the joint log-likelihood based on P(G,X):\nK/summationdisplay\nk=1/summationdisplay\ngi=klog/bracketleft\uf8eciggRk/summationdisplay\nr=1\u03c0kr\u03c6(xi;\u03b8kr,\u03a3)\u03a0k/bracketright\uf8ecigg\n. (12.61)\nThe sum within the log makes this a rather messy optimization problem\nif tackled directly. The classical and natural method for computing the\nmaximum-likelihood estimates (MLEs) for mixture distributions is the EM\nalgorithm (Dempster et al., 1977), which is known to possess good conver -\ngence properties. EM alternates between the two steps:", "468": "450 12. Flexible Discriminants\nE-step: Given the current parameters, compute the responsibility of sub-\nclassckrwithin class kfor each of the class- kobservations ( gi=k):\nW(ckr|xi,gi) =\u03c0kr\u03c6(xi;\u03b8kr,\u03a3)/summationtextRk\n\u2113=1\u03c0k\u2113\u03c6(xi;\u03b8k\u2113,\u03a3). (12.62)\nM-step: Compute the weighted MLEs for the parameters of each of the\ncomponent Gaussians within each of the classes, using the weights\nfrom the E-step.\nIn the E-step, the algorithm apportions the unit weight of an observation\nin class kto the various subclasses assigned to that class. If it is close to the\ncentroid of a particular subclass, and far from the others, it will receive a\nmass close to one for that subclass. On the other hand, observations halfway\nbetween two subclasses will get approximately equal weight for both.\nIn the M-step, an observation in class kis used Rktimes, to estimate the\nparameters in each of the Rkcomponent densities, with a di\ufb00erent weight\nfor each. The EM algorithm is studied in detail in Chapter 8. The algorithm\nrequires initialization, which can have an impact, since mixture likelihoods\nare generally multimodal. Our software (referenced in the Computational\nConsiderations on page 455) allows several strategies; here we describe the\ndefault. The user supplies the number Rkof subclasses per class. Within\nclassk, ak-means clustering model, with multiple random starts, is \ufb01tted\nto the data. This partitions the observations into Rkdisjoint groups, from\nwhich an initial weight matrix, consisting of zeros and ones, is created.\nOur assumption of an equal component covariance matrix \u03a3throughout\nbuys an additional simplicity; we can incorporate rank restrictions in the\nmixture formulation just like in LDA. To understand this, we review a litt le-\nknown fact about LDA. The rank- LLDA \ufb01t (Section 4.3.3) is equivalent to\nthe maximum-likelihood \ufb01t of a Gaussian model,where the di\ufb00erent mean\nvectors in each class are con\ufb01ned to a rank- Lsubspace of IRp(Exercise 4.8).\nWe can inherit this property for the mixture model, and maximize the log-\nlikelihood (12.61) subject to rank constraints on allthe/summationtext\nkRkcentroids:\nrank{\u03b8k\u2113}=L.\nAgain the EM algorithm is available, and the M-step turns out to be\na weighted version of LDA, with R=/summationtextK\nk=1Rk\u201cclasses.\u201d Furthermore,\nwe can use optimal scoring as before to solve the weighted LDA problem,\nwhich allows us to use a weighted version of FDA or PDA at this stage.\nOne would expect, in addition to an increase in the number of \u201cclasses,\u201d a\nsimilar increase in the number of \u201cobservations\u201d in the kth class by a factor\nofRk. It turns out that this is not the case if linear operators are used for\nthe optimal scoring regression. The enlarged indicator Ymatrix collapses\nin this case to a blurred response matrix Z, which is intuitively pleasing.\nFor example, suppose there are K= 3 classes, and Rk= 3 subclasses per\nclass. Then Zmight be", "469": "12.7 Mixture Discriminant Analysis 451\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edc11c12c13c21c22c23c31c32c33\ng1= 2 0 0 0 0 .3 0.5 0.2 0 0 0\ng2= 1 0 .9 0.1 0.0 0 0 0 0 0 0\ng3= 1 0 .1 0.8 0.1 0 0 0 0 0 0\ng4= 3 0 0 0 0 0 0 0 .5 0.4 0.1\ng5= 2 0 0 0 0 .7 0.1 0.2 0 0 0\n......\ngN= 3 0 0 0 0 0 0 0 .1 0.1 0.8\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,(12.63)\nwhere the entries in a class- krow correspond to W(ckr|x,gi).\nThe remaining steps are the same:\n\u02c6Z=SZ\nZT\u02c6Z=\u0398D\u0398T\nUpdate \u03c0s and \u03a0s\uf8fc\n\uf8fd\n\uf8feM-step of MDA .\nThese simple modi\ufb01cations add considerable \ufb02exibility to the mixture\nmodel:\n\u2022The dimension reduction step in LDA, FDA or PDA is limited by\nthe number of classes; in particular, for K= 2 classes no reduction is\npossible. MDA substitutes subclasses for classes, and then allows us\nto look at low-dimensional views of the subspace spanned by these\nsubclass centroids. This subspace will often be an important one for\ndiscrimination.\n\u2022By using FDA or PDA in the M-step, we can adapt even more to par-\nticular situations. For example, we can \ufb01t MDA models to digitized\nanalog signals and images, with smoothness constraints built in.\nFigure 12.13 compares FDA and MDA on the mixture example.\n12.7.1 Example: Waveform Data\nWe now illustrate some of these ideas on a popular simulated example,\ntaken from Breiman et al. (1984, pages 49\u201355), and used in Hastie and\nTibshirani (1996b) and elsewhere. It is a three-class problem with 21 vari-\nables, and is considered to be a di\ufb03cult pattern recognition problem. The\npredictors are de\ufb01ned by\nXj=Uh1(j) + (1 \u2212U)h2(j) +\u01ebjClass 1 ,\nXj=Uh1(j) + (1 \u2212U)h3(j) +\u01ebjClass 2 , (12.64)\nXj=Uh2(j) + (1 \u2212U)h3(j) +\u01ebjClass 3 ,\nwhere j= 1,2,... ,21,Uis uniform on (0 ,1),\u01ebjare standard normal vari-\nates, and the h\u2113are the shifted triangular waveforms: h1(j) = max(6 \u2212", "470": "452 12. Flexible Discriminants\nFDA / MARS - Degree 2\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.185\nTest Error:       0.235\nBayes Error:    0.210\nMDA - 5 Subclasses per Class\n.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\nTraining Error: 0.17\nTest Error:       0.22\nBayes Error:    0.21\nFIGURE 12.13. FDA and MDA on the mixture data. The upper plot uses\nFDA with MARS as the regression procedure. The lower plot uses MDA with\n\ufb01ve mixture centers per class (indicated). The MDA solution is cl ose to Bayes\noptimal, as might be expected given the data arise from mixture s of Gaussians.\nThe broken purple curve in the background is the Bayes decisio n boundary.", "471": "12.7 Mixture Discriminant Analysis 453\n1 1 1 1 1 1111111111\n1\n1\n1\n1\n1\n1 2 2 2 2 22222222222\n2\n2\n2\n2\n2\n2 3 3 3 3 3333333\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3 4 4 4 4 4444444\n4\n4\n4\n4\n4\n44444 5 5 5 5 55555555555\n5\n5\n5\n5\n5\n5Class 1\n11111111111\n1\n1\n1\n1\n1\n1 1 1 1 1 22222222222\n2\n2\n2\n2\n2\n2 2 2 2 2 33333333333\n3\n3\n3\n3\n3\n3 3 3 3 3 4444444\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4 4 4 4 4 5555555\n5\n5\n5\n5\n5\n5\n5\n5\n5\n5 5 5 5 5Class 2\n1111111\n1\n1\n1\n1\n1\n111111111 2222222\n2\n2\n2\n2\n2\n222222222 3333333\n3\n3333333\n3\n3\n3\n3\n3\n3 4444444\n4\n4\n4\n4\n4\n444444444 5555555\n5\n5\n5\n5\n5\n555555555Class 3\nFIGURE 12.14. Some examples of the waveforms generated from model (12.64)\nbefore the Gaussian noise is added.\n|j\u221211|,0),h2(j) =h1(j\u22124) and h3(j) =h1(j+ 4). Figure 12.14 shows\nsome example waveforms from each class.\nTable 12.4 shows the results of MDA applied to the waveform data, as\nwell as several other methods from this and other chapters. Each train-\ning sample has 300 observations, and equal priors were used, so there are\nroughly 100 observations in each class. We used test samples of size 500.\nThe two MDA models are described in the caption.\nFigure 12.15 shows the leading canonical variates for the penalized MDA\nmodel, evaluated at the test data. As we might have guessed, the classes\nappear to lie on the edges of a triangle. This is because the hj(i) are repre-\nsented by three points in 21-space, thereby forming vertices of a triangle,\nand each class is represented as a convex combination of a pair of vertices,\nand hence lie on an edge. Also it is clear visually that all the information\nlies in the \ufb01rst two dimensions; the percentage of variance explained by the\n\ufb01rst two coordinates is 99 .8%, and we would lose nothing by truncating the\nsolution there. The Bayes risk for this problem has been estimated to be\nabout 0 .14 (Breiman et al., 1984). MDA comes close to the optimal rate,\nwhich is not surprising since the structure of the MDA model is similar to\nthe generating model.", "472": "454 12. Flexible Discriminants\nTABLE 12.4. Results for waveform data. The values are averages over ten s im-\nulations, with the standard error of the average in parentheses . The \ufb01ve entries\nabove the line are taken from Hastie et al. (1994). The \ufb01rst mode l below the line\nis MDA with three subclasses per class. The next line is the same, except that the\ndiscriminant coe\ufb03cients are penalized via a roughness penalty to e \ufb00ectively 4df.\nThe third is the corresponding penalized LDA or PDA model.\nTechnique Error Rates\nTraining Test\nLDA 0.121(0.006) 0.191(0.006)\nQDA 0.039(0.004) 0.205(0.006)\nCART 0.072(0.003) 0.289(0.004)\nFDA/MARS (degree = 1) 0.100(0.006) 0.191(0.006)\nFDA/MARS (degree = 2) 0.068(0.004) 0.215(0.002)\nMDA (3 subclasses) 0.087(0.005) 0.169(0.006)\nMDA (3 subclasses, penalized 4 df) 0.137(0.006) 0.157(0.005)\nPDA (penalized 4 df) 0.150(0.005) 0.171(0.005)\nBayes 0.140\nDiscriminant Var 1Discriminant Var 2\n-6 -4 -2 0 2 4 6-6 -4 -2 0 2 411\n111\n11\n111\n11\n11\n111\n111\n1\n1\n11\n1\n111\n1111\n111\n11\n11\n11\n11\n11\n111\n11\n111\n11\n1\n11\n1\n1\n1\n11\n1\n111\n111111\n111\n11\n11\n11\n11\n11\n11\n11\n11\n1\n1111\n1\n11\n111\n111\n11\n11\n111\n1\n11\n1\n11111\n11\n1\n11111\n1\n11\n11\n111\n11\n1111\n111\n111\n1\n1\n1\n11\n11\n1\n11\n111\n11\n1111\n11\n11111\n11\n22\n22\n2\n22222\n2 22\n2\n222\n22\n2\n222\n22\n2\n222\n22\n222\n22\n22222\n2\n2222\n22\n22\n22\n222\n2\n222\n22\n222\n2\n2\n2\n22\n222\n22\n22\n2\n222\n2\n22\n2\n2\n222\n222\n2\n2\n22\n2\n222\n22\n22\n222\n22\n22\n22\n2\n222222\n22\n2\n22\n2\n2\n22\n222\n2\n22\n22\n22\n2\n2\n2\n22\n2\n22\n22\n2222\n2\n22\n33\n3 333\n3333 33\n33\n3\n3 3333\n33\n333\n333\n333\n3\n33\n33\n333\n33\n33\n3333333\n3\n3\n333\n3333\n333\n33\n333333\n333\n33\n3\n33\n333\n3\n333 3\n33\n3\n33\n3\n3\n33\n3\n3\n333\n3\n333 3\n3 3\n333\n33\n33\n33\n3\n333\n3\n3\n333\n33\n333\n333\n3\n33\n333\n333\n33\n333\n3\n33\n333\n33 333\n33\n3\n3 33\n3\n3\n33\n3\n33333\n333 Subclasses, Penalized 4 df\nDiscriminant Var 3Discriminant Var 4\n-2 -1 0 1 2-1.0 0.0 0.5 1.01\n1\n11\n111\n11\n11\n1\n11\n11\n11\n1\n111\n11\n11\n11\n11\n11\n11 111\n111\n11\n1\n11\n11\n11\n11\n1\n111\n11\n11111\n11111\n1\n1111\n1\n11\n1 1\n11\n1 111\n11\n1\n11\n1111\n11\n11\n11\n1\n11\n1\n11\n11\n11\n11\n111\n111\n1\n11\n11\n1\n11\n11\n11\n1\n11111\n1\n111\n11\n111\n1\n1\n1\n111\n11\n11\n11\n111\n1\n1\n111\n1111\n11\n111\n11\n1111\n22\n22\n22\n222\n22\n222\n22 2\n22\n22\n22\n22\n22\n22\n22\n222\n22\n2\n22222\n2\n222\n22\n2\n22222\n22\n22\n22\n22\n2\n2 22\n222\n2\n222\n22\n2\n22222\n222\n22\n222\n222\n2222\n222\n22\n2222\n2\n22\n2\n2\n22\n2\n222\n2\n222\n2\n22\n22\n22\n2\n22\n22 2\n22\n22\n2\n2\n2\n22\n2222\n2\n22\n222\n222\n3\n333\n3\n33\n3 33\n3\n33\n33\n33\n333\n3\n33\n33\n3333\n3\n333\n3\n33\n333\n33\n3\n3\n33\n33\n33\n3\n333\n3\n33333\n3\n3\n33\n33\n33\n3\n3\n3333\n33\n3333\n3 3\n33\n3333\n3\n3333\n33\n33\n3333\n33\n33\n333\n3\n33 3\n33\n3333\n333\n33\n3\n33\n3\n3\n333\n33\n33\n333\n3333333\n33\n3333\n3\n33\n333\n3\n33\n333\n3\n33\n3\n33\n3\n333\n33333 Subclasses, Penalized 4 df\nFIGURE 12.15. Some two-dimensional views of the MDA model \ufb01tted to a\nsample of the waveform model. The points are independent test dat a, projected\non to the leading two canonical coordinates (left panel), and the th ird and fourth\n(right panel). The subclass centers are indicated.", "473": "Exercises 455\nComputational Considerations\nWith Ntraining cases, ppredictors, and msupport vectors, the support\nvector machine requires m3+mN+mpN operations, assuming m\u2248N.\nThey do not scale well with N, although computational shortcuts are avail-\nable (Platt, 1999). Since these are evolving rapidly, the reader is urged to\nsearch the web for the latest technology.\nLDA requires Np2+p3operations, as does PDA. The complexity of\nFDA depends on the regression method used. Many techniques are linear\ninN, such as additive models and MARS. General splines and kernel-based\nregression methods will typically require N3operations.\nSoftware is available for \ufb01tting FDA, PDA and MDA models in the R\npackagemda, which is also available in S-PLUS.\nBibliographic Notes\nThe theory behind support vector machines is due to Vapnik and is de-\nscribed in Vapnik (1996). There is a burgeoning literature on SVMs; an\nonline bibliography, created and maintained by Alex Smola and Bernhard\nSch\u00a8 olkopf, can be found at:\nhttp://www.kernel-machines.org .\nOur treatment is based on Wahba et al. (2000) and Evgeniou et al. (2000),\nand the tutorial by Burges (Burges, 1998).\nLinear discriminant analysis is due to Fisher (1936) and Rao (1973). The\nconnection with optimal scoring dates back at least to Breiman and Ihaka\n(1984), and in a simple form to Fisher (1936). There are strong connections\nwith correspondence analysis (Greenacre, 1984). The description of \ufb02exible,\npenalized and mixture discriminant analysis is taken from Hastie et al.\n(1994), Hastie et al. (1995) and Hastie and Tibshirani (1996b), and al l\nthree are summarized in Hastie et al. (1998); see also Ripley (1996).\nExercises\nEx. 12.1 Show that the criteria (12.25) and (12.8) are equivalent.\nEx. 12.2 Show that the solution to (12.29) is the same as the solution to\n(12.25) for a particular kernel.\nEx. 12.3 Consider a modi\ufb01cation to (12.43) where you do not penalize the\nconstant. Formulate the problem, and characterize its solution.\nEx. 12.4 Suppose you perform a reduced-subspace linear discriminant anal-\nysis for a K-group problem. You compute the canonical variables of di-", "474": "456 12. Flexible Discriminants\nmension L\u2264K\u22121 given by z=UTx, where Uis the p\u00d7Lmatrix of\ndiscriminant coe\ufb03cients, and p > K is the dimension of x.\n(a) If L=K\u22121 show that\n\u221d\u230aa\u2207\u2308\u230alz\u2212\u00afzk\u221d\u230aa\u2207\u2308\u230al2\u2212 \u221d\u230aa\u2207\u2308\u230alz\u2212\u00afzk\u2032\u221d\u230aa\u2207\u2308\u230al2=\u221d\u230aa\u2207\u2308\u230alx\u2212\u00afxk\u221d\u230aa\u2207\u2308\u230al2\nW\u2212 \u221d\u230aa\u2207\u2308\u230alx\u2212\u00afxk\u2032\u221d\u230aa\u2207\u2308\u230al2\nW,\nwhere \u221d\u230aa\u2207\u2308\u230al\u2264\u221d\u230aa\u2207\u2308\u230alWdenotes Mahalanobis distance with respect to the covari-\nanceW.\n(b) If L < K \u22121, show that the same expression on the left measures\nthe di\ufb00erence in Mahalanobis squared distances for the distributions\nprojected onto the subspace spanned by U.\nEx. 12.5 The data in phoneme.subset , available from this book\u2019s website\nhttp://www-stat.stanford.edu/ElemStatLearn\nconsists of digitized log-periodograms for phonemes uttered by 60 speakers,\neach speaker having produced phonemes from each of \ufb01ve classes. It is\nappropriate to plot each vector of 256 \u201cfeatures\u201d against the frequencies\n0\u2013255.\n(a) Produce a separate plot of all the phoneme curves against frequency\nfor each class.\n(b) You plan to use a nearest prototype classi\ufb01cation scheme to classify\nthe curves into phoneme classes. In particular, you will use a K-means\nclustering algorithm in each class ( kmeans() inR), and then classify\nobservations to the class of the closest cluster center. The curves are\nhigh-dimensional and you have a rather small sample-size-to-variables\nratio. You decide to restrict all the prototypes to be smooth functions\nof frequency. In particular, you decide to represent each prototype m\nasm=B\u03b8where Bis a 256 \u00d7Jmatrix of natural spline basis\nfunctions with Jknots uniformly chosen in (0 ,255) and boundary\nknots at 0 and 255. Describe how to proceed analytically, and in\nparticular, how to avoid costly high-dimensional \ufb01tting procedures.\n(Hint: It may help to restrict Bto be orthogonal.)\n(c) Implement your procedure on the phoneme data, and try it out. Divide\nthe data into a training set and a test set (50-50), making sure that\nspeakers are not split across sets (why?). Use K= 1,3,5,7 centers\nper class, and for each use J= 5,10,15 knots (taking care to start\ntheK-means procedure at the same starting values for each value of\nJ), and compare the results.\nEx. 12.6 Suppose that the regression procedure used in FDA (Section 12.5.1)\nis a linear expansion of basis functions hm(x), m= 1,... ,M . LetD\u03c0=\nYTY/Nbe the diagonal matrix of class proportions.", "475": "Exercises 457\n(a) Show that the optimal scoring problem (12.52) can be written in vector\nnotation as\nmin\n\u03b8,\u03b2\u221d\u230aa\u2207\u2308\u230alY\u03b8\u2212H\u03b2\u221d\u230aa\u2207\u2308\u230al2, (12.65)\nwhere \u03b8is a vector of Kreal numbers, and His the N\u00d7Mmatrix\nof evaluations hj(xi).\n(b) Suppose that the normalization on \u03b8is\u03b8TD\u03c01 = 0 and \u03b8TD\u03c0\u03b8= 1.\nInterpret these normalizations in terms of the original scored \u03b8(gi).\n(c) Show that, with this normalization, (12.65) can be partially optimized\nw.r.t. \u03b2, and leads to\nmax\n\u03b8\u03b8TS\u03b8, (12.66)\nsubject to the normalization constraints, where Sis the projection\noperator corresponding to the basis matrix H.\n(d) Suppose that the hjinclude the constant function. Show that the\nlargest eigenvalue of Sis 1.\n(e) Let \u0398be aK\u00d7Kmatrix of scores (in columns), and suppose the\nnormalization is \u0398TD\u03c0\u0398=I. Show that the solution to (12.53) is\ngiven by the complete set of eigenvectors of S; the \ufb01rst eigenvector is\ntrivial, and takes care of the centering of the scores. The remainder\ncharacterize the optimal scoring solution.\nEx. 12.7 Derive the solution to the penalized optimal scoring problem\n(12.57).\nEx. 12.8 Show that coe\ufb03cients \u03b2\u2113found by optimal scoring are proportional\nto the discriminant directions \u03bd\u2113found by linear discriminant analysis.\nEx. 12.9 Let\u02c6Y=X\u02c6Bbe the \ufb01tted N\u00d7Kindicator response matrix after\nlinear regression on the N\u00d7pmatrix X, where p > K . Consider the reduced\nfeatures x\u2217\ni=\u02c6BTxi. Show that LDA using x\u2217\niis equivalent to LDA in the\noriginal space.\nEx. 12.10 Kernels and linear discriminant analysis . Suppose you wish to\ncarry out a linear discriminant analysis (two classes) using a vector of\ntransformations of the input variables h(x). Since h(x) is high-dimensional,\nyou will use a regularized within-class covariance matrix Wh+\u03b3I. Show\nthat the model can be estimated using only the inner products K(xi,xi\u2032) =\n\u221dan}\u230a\u2207a\u230bketle{th(xi),h(xi\u2032)\u221dan}\u230a\u2207a\u230bket\u2207i}ht. Hence the kernel property of support vector machines is also\nshared by regularized linear discriminant analysis.\nEx. 12.11 The MDA procedure models each class as a mixture of Gaussians.\nHence each mixture center belongs to one and only one class. A more\ngeneral model allows each mixture center to be shared by all classes. We\ntake the joint density of labels and features to be", "476": "458 12. Flexible Discriminants\nP(G,X) =R/summationdisplay\nr=1\u03c0rPr(G,X), (12.67)\na mixture of joint densities. Furthermore we assume\nPr(G,X) =Pr(G)\u03c6(X;\u03b8r,\u03a3). (12.68)\nThis model consists of regions centered at \u03b8r, and for each there is a class\npro\ufb01le Pr(G). The posterior class distribution is given by\nP(G=k|X=x) =/summationtextR\nr=1\u03c0rPr(G=k)\u03c6(x;\u03b8r,\u03a3)\n/summationtextR\nr=1\u03c0r\u03c6(x;\u03b8r,\u03a3), (12.69)\nwhere the denominator is the marginal distribution P(X).\n(a) Show that this model (called MDA2) can be viewed as a generalization\nof MDA since\nP(X|G=k) =/summationtextR\nr=1\u03c0rPr(G=k)\u03c6(x;\u03b8r,\u03a3)\n/summationtextR\nr=1\u03c0rPr(G=k), (12.70)\nwhere \u03c0rk=\u03c0rPr(G=k)//summationtextR\nr=1\u03c0rPr(G=k) corresponds to the\nmixing proportions for the kth class.\n(b) Derive the EM algorithm for MDA2.\n(c) Show that if the initial weight matrix is constructed as in MDA, in-\nvolving separate k-means clustering in each class, then the algorithm\nfor MDA2 is identical to the original MDA procedure.", "477": "This is page 459\nPrinter: Opaque this\n13\nPrototype Methods and\nNearest-Neighbors\n13.1 Introduction\nIn this chapter we discuss some simple and essentially model-free methods\nfor classi\ufb01cation and pattern recognition. Because they are highly unstruc-\ntured, they typically are not useful for understanding the nature of the\nrelationship between the features and class outcome. However, as black box\nprediction engines, they can be very e\ufb00ective, and are often among the best\nperformers in real data problems. The nearest-neighbor technique can also\nbe used in regression; this was touched on in Chapter 2 and works reason-\nably well for low-dimensional problems. However, with high-dimensional\nfeatures, the bias\u2013variance tradeo\ufb00 does not work as favorably for nearest-\nneighbor regression as it does for classi\ufb01cation.\n13.2 Prototype Methods\nThroughout this chapter, our training data consists of the Npairs ( x1,g1),\n... ,(xn,gN) where giis a class label taking values in {1,2,... ,K }. Pro-\ntotype methods represent the training data by a set of points in feature\nspace. These prototypes are typically not examples from the training sam-\nple, except in the case of 1-nearest-neighbor classi\ufb01cation discussed later.\nEach prototype has an associated class label, and classi\ufb01cation of a query\npoint xis made to the class of the closest prototype. \u201cClosest\u201d is usually\nde\ufb01ned by Euclidean distance in the feature space, after each feature has", "478": "460 13. Prototypes and Nearest-Neighbors\nbeen standardized to have overall mean 0 and variance 1 in the training\nsample. Euclidean distance is appropriate for quantitative features. We\ndiscuss distance measures between qualitative and other kinds of feature\nvalues in Chapter 14.\nThese methods can be very e\ufb00ective if the prototypes are well positioned\nto capture the distribution of each class. Irregular class boundaries can be\nrepresented, with enough prototypes in the right places in feature space.\nThe main challenge is to \ufb01gure out how many prototypes to use and where\nto put them. Methods di\ufb00er according to the number and way in which\nprototypes are selected.\n13.2.1 K-means Clustering\nK-means clustering is a method for \ufb01nding clusters and cluster centers in a\nset of unlabeled data. One chooses the desired number of cluster centers, say\nR, and the K-means procedure iteratively moves the centers to minimize\nthe total within cluster variance.1Given an initial set of centers, the K-\nmeans algorithm alternates the two steps:\n\u2022for each center we identify the subset of training points (its cluster)\nthat is closer to it than any other center;\n\u2022the means of each feature for the data points in each cluster are\ncomputed, and this mean vector becomes the new center for that\ncluster.\nThese two steps are iterated until convergence. Typically the initial centers\nareRrandomly chosen observations from the training data. Details of the\nK-means procedure, as well as generalizations allowing for di\ufb00erent variable\ntypes and more general distance measures, are given in Chapter 14.\nTo use K-means clustering for classi\ufb01cation of labeled data, the steps\nare:\n\u2022apply K-means clustering to the training data in each class sepa-\nrately, using Rprototypes per class;\n\u2022assign a class label to each of the K\u00d7Rprototypes;\n\u2022classify a new feature xto the class of the closest prototype.\nFigure 13.1 (upper panel) shows a simulated example with three classes\nand two features. We used R= 5 prototypes per class, and show the clas-\nsi\ufb01cation regions and the decision boundary. Notice that a number of the\n1The \u201c K\u201d inK-means refers to the number of cluster centers. Since we have already\nreserved Kto denote the number of classes, we denote the number of clust ers by R.", "479": "13.2 Prototype Methods 461\nK-means - 5 Prototypes  per Class\n................................................................................................................................................................................................................................................................................................................................................................................................................................. ........................ ................................ ..................................... ............................................................................................................................................................................. .................................. ....................................... ......................................... ............................................ .............................................. .................................................. ......................... .............................. ........................ ................................... ......................... ..................................... .......................... ...................................... ............................. .................................. .................................. ....................... ....................................... ............. ............................................... ...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ........... ........................................ ............................................................. ........ ........................................... ........................................................ ..... ................................................................................................. .. .............................................................................................. . .......................................................................................... .... .................................................................................... ....... .............................................................................. .......... ............................................................................ ......... ............................................................................. ......... ............................................................................. ......... ............................................................................. ......... ............................................................................... ....... .................................................................................... ..... ........................................................................................ ... ............................................................................................ . ............................................................................................... . ................................................................................................ ... ................................................................................................ ..... ................................................................................................ ....... ............................................ ................................ .................... .... ....................................................................... ..................... .... ........................................................... ................. ............. .......................................... ............. ...................... ........................ .............. ....\n................................................................................ ......................... ......................................... .................... .................................................... ................ ........................................................... ............... ............................................................. ............... ............................................................... ............... ............................................................... ............... ........................................................... ................ ....................................................... ................ .................................................... ................ ................................................... ............... ................................................. .............. .............................................. .............. ......................................... .................................................... ............................................... ............................................... ........... .......................................... .......... ................................................ .......... ...................................................... ......... .................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ .............................................................................................. ............................................................................................... ................................................................................................ ................................................................................................. ................................................\noo\noo\no\nooo\noo\no\noo\noo\noo\no\noo\no\nooo\nooo\nooo\noo\noooo\nooooo\noo\nooo\no\nooo\noo\nooo\no\noo\noo\noo\nooo\noo\no\noo\nooo\nooo\noo\noo\no\noo\no\no\noo\noo\nooo\nooo\noo\noo\no\nooo\nooo\no\noo\noo\no\noo\noooo\nooooo\noo\noooo\nooo\no\nooo\nooo\noo oo\noo\noo\nooo\noo\noooo\noo\noo\nooo\noo\no\noo\noo\no\noo\noo\noooo\no\no\noo\noo oo\no\nooo\nooo\noo\noooooo\no\nooo\noooo\noo\noooo\nooo\noo\no\noo\noo\no\noo\no\noooo\noo\no\noo\noo\no\noo\noooo\no\noo\nooo\noooo\no o o\nooo\noo\no\nooo\nooooo\noo\noo\no\no\noooo\no\noo\noo\noooo\nooooo\no\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\nLVQ - 5 Prototypes per Class\n......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ................................... ........................................ .............................................. .................................................. .................... .................................. ..................... ...................................... ..................... ......................................... ....................... ..................................... .............................. ............................ ..................................... .............. ............................................ .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... .... ............................................... ............................................................ .... ............................................... ........................................................... .... ............................................... .......................................................... ... ................................................ ........................................................ ... ................................................ ...................................................... ... ................................................ ..................................................... .. .................................................................................................... .. .................................................................................................. . ................................................................................................. . ................................................................................................ . ............................................................................................... ............................................................................................. ........................................................................................... . ......................................................................................... . ............................................................ .................... ......... ........................................... ........................ ..............\n.......................... ................................. ............................... ....................... ................................................. ................ .......................................................... ............... ............................................................ ............... ............................................................. ............... .............................................................. ............... .............................................................. ............... ................................................................ .............. .................................................................. .............. ................................................................... .............. ................................................................ .............. ........................................................... ............... ..................................................... ................ ................................................ ............... ............................................. .............. ......................................... ................................................... ................................................ .................................................. ........... .............................................. ........... ...................................................... .......... ....................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ ..........................................\noo\noo\no\nooo\noo\no\noo\noo\noo\no\noo\no\nooo\nooo\nooo\noo\noooo\nooooo\noo\nooo\no\nooo\noo\nooo\no\noo\noo\noo\nooo\noo\no\noo\nooo\nooo\noo\noo\no\noo\no\no\noo\noo\nooo\nooo\noo\noo\no\nooo\nooo\no\noo\noo\no\noo\noooo\nooooo\noo\noooo\nooo\no\nooo\nooo\noo oo\noo\noo\nooo\noo\noooo\noo\noo\nooo\noo\no\noo\noo\no\noo\noo\noooo\no\no\noo\noo oo\no\nooo\nooo\noo\noooooo\no\nooo\noooo\noo\noooo\nooo\noo\no\noo\noo\no\noo\no\noooo\noo\no\noo\noo\no\noo\noooo\no\noo\nooo\noooo\no o o\nooo\noo\no\nooo\nooooo\noo\noo\no\no\noooo\no\noo\noo\noooo\nooooo\no\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\nFIGURE 13.1. Simulated example with three classes and \ufb01ve prototypes per\nclass. The data in each class are generated from a mixture of Gau ssians. In the\nupper panel, the prototypes were found by applying the K-means clustering algo-\nrithm separately in each class. In the lower panel, the LVQ alg orithm (starting\nfrom the K-means solution) moves the prototypes away from the decision b ound-\nary. The broken purple curve in the background is the Bayes dec ision boundary.", "480": "462 13. Prototypes and Nearest-Neighbors\nAlgorithm 13.1 Learning Vector Quantization\u2014LVQ.\n1. Choose Rinitial prototypes for each class: m1(k),m2(k),... ,m R(k),\nk= 1,2,... ,K , for example, by sampling Rtraining points at random\nfrom each class.\n2. Sample a training point xirandomly (with replacement), and let ( j,k)\nindex the closest prototype mj(k) toxi.\n(a) If gi=k(i.e., they are in the same class), move the prototype\ntowards the training point:\nmj(k)\u2190mj(k) +\u01eb(xi\u2212mj(k)),\nwhere \u01ebis the learning rate .\n(b) If gi\u221dne}ationslash=k(i.e., they are in di\ufb00erent classes), move the prototype\naway from the training point:\nmj(k)\u2190mj(k)\u2212\u01eb(xi\u2212mj(k)).\n3. Repeat step 2, decreasing the learning rate \u01ebwith each iteration to-\nwards zero.\nprototypes are near the class boundaries, leading to potential misclassi\ufb01ca-\ntion errors for points near these boundaries. This results from an obvious\nshortcoming with this method: for each class, the other classes do not have\na say in the positioning of the prototypes for that class. A better approach,\ndiscussed next, uses all of the data to position all prototypes.\n13.2.2 Learning Vector Quantization\nIn this technique due to Kohonen (1989), prototypes are placed strategically\nwith respect to the decision boundaries in an ad-hoc way. LVQ is an online\nalgorithm\u2014observations are processed one at a time.\nThe idea is that the training points attract prototypes of the correct class,\nand repel other prototypes. When the iterations settle down, prototypes\nshould be close to the training points in their class. The learning rate \u01ebis\ndecreased to zero with each iteration, following the guidelines for stochastic\napproximation learning rates (Section 11.4.)\nFigure 13.1 (lower panel) shows the result of LVQ, using the K-means\nsolution as starting values. The prototypes have tended to move away from\nthe decision boundaries, and away from prototypes of competing classes.\nThe procedure just described is actually called LVQ1. Modi\ufb01cations\n(LVQ2, LVQ3, etc.) have been proposed, that can sometimes improve per-\nformance. A drawback of learning vector quantization methods is the fact", "481": "13.3k-Nearest-Neighbor Classi\ufb01ers 463\nthat they are de\ufb01ned by algorithms, rather than optimization of some \ufb01xed\ncriteria; this makes it di\ufb03cult to understand their properties.\n13.2.3 Gaussian Mixtures\nThe Gaussian mixture model can also be thought of as a prototype method,\nsimilar in spirit to K-means and LVQ. We discuss Gaussian mixtures in\nsome detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms\nof a Gaussian density, which has a centroid (as in K-means), and a covari-\nance matrix. The comparison becomes crisper if we restrict the component\nGaussians to have a scalar covariance matrix (Exercise 13.1). The two st eps\nof the alternating EM algorithm are very similar to the two steps in K-\nmeans:\n\u2022In the E-step, each observation is assigned a responsibility or weight\nfor each cluster, based on the likelihood of each of the correspond-\ning Gaussians. Observations close to the center of a cluster will most\nlikely get weight 1 for that cluster, and weight 0 for every other clus-\nter. Observations half-way between two clusters divide their weight\naccordingly.\n\u2022In the M-step, each observation contributes to the weighted means\n(and covariances) for every cluster.\nAs a consequence, the Gaussian mixture model is often referred to as a soft\nclustering method, while K-means is hard.\nSimilarly, when Gaussian mixture models are used to represent the fea-\nture density in each class, it produces smooth posterior probabilities \u02c6 p(x) =\n{\u02c6p1(x),... ,\u02c6pK(x)}for classifying x(see (12.60) on page 449.) Often this\nis interpreted as a soft classi\ufb01cation, while in fact the classi\ufb01cation rule i s\n\u02c6G(x) = arg max k\u02c6pk(x). Figure 13.2 compares the results of K-means and\nGaussian mixtures on the simulated mixture problem of Chapter 2. We\nsee that although the decision boundaries are roughly similar, those for the\nmixture model are smoother (although the prototypes are in approximately\nthe same positions.) We also see that while both procedures devote a blue\nprototype (incorrectly) to a region in the northwest, the Gaussian mixtur e\nclassi\ufb01er can ultimately ignore this region, while K-means cannot. LVQ\ngave very similar results to K-means on this example, and is not shown.\n13.3 k-Nearest-Neighbor Classi\ufb01ers\nThese classi\ufb01ers are memory-based , and require no model to be \ufb01t. Given\na query point x0, we \ufb01nd the ktraining points x(r),r= 1,... ,k closest in\ndistance to x0, and then classify using majority vote among the kneighbors.", "482": "464 13. Prototypes and Nearest-Neighbors\nK-means - 5 Prototypes per Class\n... .. . . .. . . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\nTraining Error: 0.170\nTest Error:       0.243\nBayes Error:    0.210\nGaussian Mixtures - 5 Subclasses per Class\n.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\nTraining Error: 0.17\nTest Error:       0.22\nBayes Error:    0.21\nFIGURE 13.2. The upper panel shows the K-means classi\ufb01er applied to the\nmixture data example. The decision boundary is piecewise linear . The lower panel\nshows a Gaussian mixture model with a common covariance for all component\nGaussians. The EM algorithm for the mixture model was started a t theK-means\nsolution. The broken purple curve in the background is the Baye s decision\nboundary.", "483": "13.3k-Nearest-Neighbor Classi\ufb01ers 465\nTies are broken at random. For simplicity we will assume that the features\nare real-valued, and we use Euclidean distance in feature space:\nd(i)=||x(i)\u2212x0||. (13.1)\nTypically we \ufb01rst standardize each of the features to have mean zero and\nvariance 1, since it is possible that they are measured in di\ufb00erent units. In\nChapter 14 we discuss distance measures appropriate for qualitative and\nordinal features, and how to combine them for mixed data. Adaptively\nchosen distance metrics are discussed later in this chapter.\nDespite its simplicity, k-nearest-neighbors has been successful in a large\nnumber of classi\ufb01cation problems, including handwritten digits, satellite\nimage scenes and EKG patterns. It is often successful where each class\nhas many possible prototypes, and the decision boundary is very irregular.\nFigure 13.3 (upper panel) shows the decision boundary of a 15-nearest-\nneighbor classi\ufb01er applied to the three-class simulated data. The decision\nboundary is fairly smooth compared to the lower panel, where a 1-nearest-\nneighbor classi\ufb01er was used. There is a close relationship between nearest-\nneighbor and prototype methods: in 1-nearest-neighbor classi\ufb01cation, each\ntraining point is a prototype.\nFigure 13.4 shows the training, test and tenfold cross-validation errors\nas a function of the neighborhood size, for the two-class mixture problem.\nSince the tenfold CV errors are averages of ten numbers, we can estimate\na standard error.\nBecause it uses only the training point closest to the query point, the bias\nof the 1-nearest-neighbor estimate is often low, but the variance is high.\nA famous result of Cover and Hart (1967) shows that asymptotically the\nerror rate of the 1-nearest-neighbor classi\ufb01er is never more than twice the\nBayes rate. The rough idea of the proof is as follows (using squared-error\nloss). We assume that the query point coincides with one of the training\npoints, so that the bias is zero. This is true asymptotically if the dimensio n\nof the feature space is \ufb01xed and the training data \ufb01lls up the space in a\ndense fashion. Then the error of the Bayes rule is just the variance of a\nBernoulli random variate (the target at the query point), while the error of\n1-nearest-neighbor rule is twicethe variance of a Bernoulli random variate,\none contribution each for the training and query targets.\nWe now give more detail for misclassi\ufb01cation loss. At xletk\u2217be the\ndominant class, and pk(x) the true conditional probability for class k. Then\nBayes error = 1 \u2212pk\u2217(x), (13.2)\n1-nearest-neighbor error =K/summationdisplay\nk=1pk(x)(1\u2212pk(x)), (13.3)\n\u22651\u2212pk\u2217(x). (13.4)\nThe asymptotic 1-nearest-neighbor error rate is that of a random rule; we\npick both the classi\ufb01cation and the test point at random with probabili-", "484": "466 13. Prototypes and Nearest-Neighbors\n15-Nearest Neighbors\n. ........ ......... ..... ...... ......... ......................................................................................... ............................................................................................................................................................................................................................................................................ .............................................................................................................. .. .. . ...................................... .. . .... .... ......................................... ........... ....................................................... ......... ............................................................ ...... .......................................................................... ............................................................................... ...................... .................................................. ................................................................... ........................... ...................................... ........ .. .................................................................................... . .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................. . . ................................................... ............................................................. . .................................................. ............................................................ ................................................... ............................................................... .. ................................................. .................................................... ...... ... ................................................... ............................................... ..... ... .. ................................................. .............................................. .... .... ................................................... ............................................... ...... ................................................... ..................................................... .... ............................................... ........................................................ .... ............................................... ........................ ............................... ... .............................................................. .... . . ........................ ....... ...................................................... ........................ ................. ................................... ................. .............................. ............... ....... ... ........................................ ................................................................ .............................. . ...................... ......\n.... .. . ..................... .............................. .................................................................................................. .......................................... .. ......... ............................................... ........... ..................................................... .............. ............................................................. .............. .................................................................. ........... .................................................................. . ............. .............................................................. ................ .......................................................... .. . . ............ .................... .................................... . . ............... .. ......... .. ................................... .................................................... ............................................... ................................................................................................................................ .................................................................................................... . ....................................................... . ........ ............................................................. ......................................................................... ................ ... ..................................................... ........................................................................... ............................................................................ .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................. ...........................................\noo\noo\no\nooo\noo\no\noo\noo\noo\no\noo\no\nooo\nooo\nooo\noo\noooo\nooooo\noo\nooo\no\nooo\noo\nooo\no\noo\noo\noo\nooo\noo\no\noo\nooo\nooo\noo\noo\no\noo\no\no\noo\noo\nooo\nooo\noo\noo\no\nooo\nooo\no\noo\noo\no\noo\noooo\nooooo\noo\noooo\nooo\no\nooo\nooo\noo oo\noo\noo\nooo\noo\noooo\noo\noo\nooo\noo\no\noo\noo\no\noo\noo\noooo\no\no\noo\noo oo\no\nooo\nooo\noo\noooooo\no\nooo\noooo\noo\noooo\nooo\noo\no\noo\noo\no\noo\no\noooo\noo\no\noo\noo\no\noo\noooo\no\noo\nooo\noooo\no o o\nooo\noo\no\nooo\nooooo\noo\noo\no\no\noooo\no\noo\noo\noooo\nooooo\no\n1-Nearest Neighbor\n................. ................. ............ .............. .................. ....................... ... ................ ..... ............... ..... ............. .... ................ ......... ..... ..... ...... ................... ..... ...... ...... .... ....... ... .......... ... .... .. .... . ........ ............ ............ ...... ............ .......... ........ .............. ........ ........ ............. .......... ...... ............... ....... ............. ............. ......... .......................... ............. ......................... . ..... .. .. ............................. ....... .. . ...... .......................... ....... ............ .............. ................ ............... .............................. ............. ........... ............ ....................... ............. ........... ....................... .......................................... ...... ..................... ..................... ..... ......................... .......... ............... ........ ....... ... .... ....................... .... ... ................ ...................... . ........................ . ......... ..... .... .............. ............... .............. ...... ... .................................. ......... .......... ... .............................. ........... ...... .......... ............................. ......... .......... ...................................... ... ................................................... ...................................... ..................................... .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................ ................................... .............................................................. ....... ............................................ .............................................. ...... .... ......................................................................................... ........ ... ................................................ ............................................ ........ ............... .............................................................................. ........ .......... ............................................................................... ....... ........ ......................................................... .............. ......... ... .. .......................................................... ............ ....... ... ........ ...................................................... .......... ...... .... .......... .................................................. ...... .. ....... ................ .................................................... . .. .............. .............. ..................................................... ... ............ .... ... ........... ............................................. .. ............. ... .................. ....................................... .............. ..... ... ..... ..... ....... .......................................... ............. ...... . ....... ... ......... ....................................... ............... .. .......... ...... ..... ... .............................................. .................. .......... ........ . ............................................ ..... .......... ......... ............. .................................. ... ..... ......... ..................... ........................... . ... ... ........ ........................ ....................... ..... ... ....... ............ ............. ................ .. .... ..... ............. ... .... .................... .... ..... . ........... ..... ...... ...........\n................................................. ............ ................. ................................................................. .................. ..................... ...................... ............................ ......................................... ................................................ .. ... .......... .......................... ...... . ...... ...................................... ...... . ..... ............ .................................... .. .............. .................................... .. . ................ ................................................ ... ....... .................................................. ...... .......... ........................................... ..... ........ ......... ........................................... ... . .. .. ... ................................... .. .. .. .. .................................. ........... .... . ...... . .... ............................................... ..... ..... ................................................... ...... . .................................................. .... .. . ......................................................... ..... ..... .................................................... ...... ............................................................ .................. ........................................................ ................... ......................................................... ................... .......................................................... ............................................................................... .............................................................................. .............................................................................. ................................................................................ .................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ........................................................................................... .........................................\noo\noo\no\nooo\noo\no\noo\noo\noo\no\noo\no\nooo\nooo\nooo\noo\noooo\nooooo\noo\nooo\no\nooo\noo\nooo\no\noo\noo\noo\nooo\noo\no\noo\nooo\nooo\noo\noo\no\noo\no\no\noo\noo\nooo\nooo\noo\noo\no\nooo\nooo\no\noo\noo\no\noo\noooo\nooooo\noo\noooo\nooo\no\nooo\nooo\noo oo\noo\noo\nooo\noo\noooo\noo\noo\nooo\noo\no\noo\noo\no\noo\noo\noooo\no\no\noo\noo oo\no\nooo\nooo\noo\noooooo\no\nooo\noooo\noo\noooo\nooo\noo\no\noo\noo\no\noo\no\noooo\noo\no\noo\noo\no\noo\noooo\no\noo\nooo\noooo\no o o\nooo\noo\no\nooo\nooooo\noo\noo\no\no\noooo\no\noo\noo\noooo\nooooo\no\nFIGURE 13.3. k-nearest-neighbor classi\ufb01ers applied to the simulation data o f\nFigure 13.1. The broken purple curve in the background is the B ayes decision\nboundary.", "485": "13.3k-Nearest-Neighbor Classi\ufb01ers 467\nNumber of NeighborsMisclassification Errors\n0 5 10 15 20 25 300.0 0.05 0.10 0.15 0.20 0.25 0.30\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022 \u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\nTest Error\n10-fold CV\nTraining Error\nBayes Error\n7-Nearest Neighbors\n.. .. . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\noo\no\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\no\noooo\noo\noo\noo\nooo\nooooooo\noo o\nooo\noo\noo\noooo\no\noo\noo\no\noooo\nooo\no\no\nooo\no\nooooo\noo\no\noo\nooo\no\nTraining Error: 0.145\nTest Error:       0.225\nBayes Error:    0.210\nFIGURE 13.4. k-nearest-neighbors on the two-class mixture data. The upper\npanel shows the misclassi\ufb01cation errors as a function of neighbo rhood size. Stan-\ndard error bars are included for 10-fold cross validation. The lower panel shows\nthe decision boundary for 7-nearest-neighbors, which appears to be optimal for\nminimizing test error. The broken purple curve in the backgrou nd is the Bayes\ndecision boundary.", "486": "468 13. Prototypes and Nearest-Neighbors\ntiespk(x), k= 1,... ,K . For K= 2 the 1-nearest-neighbor error rate is\n2pk\u2217(x)(1\u2212pk\u2217(x))\u22642(1\u2212pk\u2217(x)) (twice the Bayes error rate). More\ngenerally, one can show (Exercise 13.3)\nK/summationdisplay\nk=1pk(x)(1\u2212pk(x))\u22642(1\u2212pk\u2217(x))\u2212K\nK\u22121(1\u2212pk\u2217(x))2.(13.5)\nMany additional results of this kind have been derived; Ripley (1996) sum-\nmarizes a number of them.\nThis result can provide a rough idea about the best performance that\nis possible in a given problem. For example, if the 1-nearest-neighbor rule\nhas a 10% error rate, then asymptotically the Bayes error rate is at least\n5%. The kicker here is the asymptotic part, which assumes the bias of the\nnearest-neighbor rule is zero. In real problems the bias can be substantial.\nThe adaptive nearest-neighbor rules, described later in this chapter, are an\nattempt to alleviate this bias. For simple nearest-neighbors, the bias and\nvariance characteristics can dictate the optimal number of near neighbors\nfor a given problem. This is illustrated in the next example.\n13.3.1 Example: A Comparative Study\nWe tested the nearest-neighbors, K-means and LVQ classi\ufb01ers on two sim-\nulated problems. There are 10 independent features Xj, each uniformly\ndistributed on [0 ,1]. The two-class 0-1 target variable is de\ufb01ned as follows:\nY=I/parenleftbigg\nX1>1\n2/parenrightbigg\n; problem 1: \u201ceasy\u201d,\nY=I\uf8eb\n\uf8edsign\uf8f1\n\uf8f2\n\uf8f33/productdisplay\nj=1/parenleftbigg\nXj\u22121\n2/parenrightbigg\uf8fc\n\uf8fd\n\uf8fe>0\uf8f6\n\uf8f8; problem 2: \u201cdi\ufb03cult.\u201d(13.6)\nHence in the \ufb01rst problem the two classes are separated by the hyperplane\nX1= 1/2; in the second problem, the two classes form a checkerboard\npattern in the hypercube de\ufb01ned by the \ufb01rst three features. The Bayes\nerror rate is zero in both problems. There were 100 training and 1000 test\nobservations.\nFigure 13.5 shows the mean and standard error of the misclassi\ufb01cation\nerror for nearest-neighbors, K-means and LVQ over ten realizations, as\nthe tuning parameters are varied. We see that K-means and LVQ give\nnearly identical results. For the best choices of their tuning parameters,\nK-means and LVQ outperform nearest-neighbors for the \ufb01rst problem, and\nthey perform similarly for the second problem. Notice that the best value\nof each tuning parameter is clearly situation dependent. For example 25-\nnearest-neighbors outperforms 1-nearest-neighbor by a factor of 70% in the", "487": "13.3k-Nearest-Neighbor Classi\ufb01ers 469\nNumber of NeighborsMisclassification Error\n0 20 40 600.1 0.2 0.3 0.4 0.5Nearest Neighbors / Easy\nNumber of Prototypes per ClassMisclassification Error\n0 5 10 15 20 25 300.1 0.2 0.3 0.4 0.5K-means & LVQ / Easy\nNumber of NeighborsMisclassification Error\n0 20 40 600.40 0.45 0.50 0.55 0.60Nearest Neighbors / Difficult\nNumber of Prototypes per ClassMisclassification Error\n0 5 10 15 20 25 300.40 0.45 0.50 0.55 0.60K-means & LVQ / Difficult\nFIGURE 13.5. Mean \u00b1one standard error of misclassi\ufb01cation error for near-\nest-neighbors, K-means (blue) and LVQ (red) over ten realizations for two sim-\nulated problems: \u201ceasy\u201d and \u201cdi\ufb03cult,\u201d described in the text.", "488": "470 13. Prototypes and Nearest-Neighbors\nSpectral Band 1 Spectral Band 2 Spectral Band 3\nSpectral Band 4 Land Usage Predicted Land Usage\nFIGURE 13.6. The \ufb01rst four panels are LANDSAT images for an agricultural\narea in four spectral bands, depicted by heatmap shading. The r emaining two\npanels give the actual land usage (color coded) and the predicte d land usage using\na \ufb01ve-nearest-neighbor rule described in the text.\n\ufb01rst problem, while 1-nearest-neighbor is best in the second problem by a\nfactor of 18%. These results underline the importance of using an objective,\ndata-based method like cross-validation to estimate the best value of a\ntuning parameter (see Figure 13.4 and Chapter 7).\n13.3.2 Example: k-Nearest-Neighbors and Image Scene\nClassi\ufb01cation\nThe STATLOG project (Michie et al., 1994) used part of a LANDSAT\nimage as a benchmark for classi\ufb01cation (82 \u00d7100 pixels). Figure 13.6 shows\nfour heat-map images, two in the visible spectrum and two in the infrared,\nfor an area of agricultural land in Australia. Each pixel has a class label\nfrom the 7-element set G={red soil, cotton, vegetation stubble, mixture,\ngray soil, damp gray soil, very damp gray soil }, determined manually by\nresearch assistants surveying the area. The lower middle panel shows the\nactual land usage, shaded by di\ufb00erent colors to indicate the classes. The\nobjective is to classify the land usage at a pixel, based on the information\nin the four spectral bands.\nFive-nearest-neighbors produced the predicted map shown in the bot-\ntom right panel, and was computed as follows. For each pixel we extracted\nan 8-neighbor feature map\u2014the pixel itself and its 8 immediate neighbors", "489": "13.3k-Nearest-Neighbor Classi\ufb01ers 471\nN\nN\nN\n N\nX\nN\nN\nN\nN\nFIGURE 13.7. A pixel and its 8-neighbor feature map.\n(see Figure 13.7). This is done separately in the four spectral bands, giving\n(1+8) \u00d74 = 36 input features per pixel. Then \ufb01ve-nearest-neighbors classi-\n\ufb01cation was carried out in this 36-dimensional feature space. The resulting\ntest error rate was about 9 .5% (see Figure 13.8). Of all the methods used\nin the STATLOG project, including LVQ, CART, neural networks, linear\ndiscriminant analysis and many others, k-nearest-neighbors performed best\non this task. Hence it is likely that the decision boundaries in IR36are quite\nirregular.\n13.3.3 Invariant Metrics and Tangent Distance\nIn some problems, the training features are invariant under certain natural\ntransformations. The nearest-neighbor classi\ufb01er can exploit such invari-\nances by incorporating them into the metric used to measure the distances\nbetween objects. Here we give an example where this idea was used with\ngreat success, and the resulting classi\ufb01er outperformed all others at the\ntime of its development (Simard et al., 1993).\nThe problem is handwritten digit recognition, as discussed is Chapter 1\nand Section 11.7. The inputs are grayscale images with 16 \u00d716 = 256\npixels; some examples are shown in Figure 13.9. At the top of Figure 13.1 0,\na \u201c3\u201d is shown, in its actual orientation (middle) and rotated 7 .5\u25e6and 15\u25e6\nin either direction. Such rotations can often occur in real handwriting, and\nit is obvious to our eye that this \u201c3\u201d is still a \u201c3\u201d after small rotati ons.\nHence we want our nearest-neighbor classi\ufb01er to consider these two \u201c3\u201ds\nto be close together (similar). However the 256 grayscale pixel values for a\nrotated \u201c3\u201d will look quite di\ufb00erent from those in the original image, a nd\nhence the two objects can be far apart in Euclidean distance in IR256.\nWe wish to remove the e\ufb00ect of rotation in measuring distances between\ntwo digits of the same class. Consider the set of pixel values consisting of\nthe original \u201c3\u201d and its rotated versions. This is a one-dimensional curve in\nIR256, depicted by the green curve passing through the \u201c3\u201d in Figure 13.10.\nFigure 13.11 shows a stylized version of IR256, with two images indicated by\nxiandxi\u2032. These might be two di\ufb00erent \u201c3\u201ds, for example. Through each\nimage we have drawn the curve of rotated versions of that image, called", "490": "472 13. Prototypes and Nearest-Neighbors\nSTATLOG results\nMethodTest Misclassification Error\n2 4 6 8 10 12 140.0 0.05 0.10 0.15LVQRBFALLOC80CART NeuralNewID C4.5QDASMARTLogisticLDA\nDANNK-NN\nFIGURE 13.8. Test-error performance for a number of classi\ufb01ers, as reported\nby the STATLOG project. The entry DANN is a variant of k-nearest neighbors,\nusing an adaptive metric (Section 13.4.2).\nFIGURE 13.9. Examples of grayscale images of handwritten digits.", "491": "13.3k-Nearest-Neighbor Classi\ufb01ers 473\nTangent\n+ \u03b1.Transformations of 30 7.5 \u221215 \u22127.5\n3\n\u03b1=0 \u03b1=0.1 \u03b1=\u2212 0.2 \u03b1=\u2212 0.1 \u03b1=0.2\nLinear equation for \nimages above15\nPixel space\nFIGURE 13.10. The top row shows a \u201c 3\u201d in its original orientation (middle)\nand rotated versions of it. The green curve in the middle of the \ufb01g ure depicts\nthis set of rotated \u201c 3\u201d in256-dimensional space. The red line is the tangent line\nto the curve at the original image, with some \u201c 3\u201ds on this tangent line, and its\nequation shown at the bottom of the \ufb01gure.\ninvariance manifolds in this context. Now, rather than using the usual\nEuclidean distance between the two images, we use the shortest distance\nbetween the two curves. In other words, the distance between the two\nimages is taken to be the shortest Euclidean distance between any rotated\nversion of \ufb01rst image, and any rotated version of the second image. This\ndistance is called an invariant metric .\nIn principle one could carry out 1-nearest-neighbor classi\ufb01cation using\nthis invariant metric. However there are two problems with it. First, it is\nvery di\ufb03cult to calculate for real images. Second, it allows large trans-\nformations that can lead to poor performance. For example a \u201c6\u201d would\nbe considered close to a \u201c9\u201d after a rotation of 180\u25e6. We need to restrict\nattention to small rotations.\nThe use of tangent distance solves both of these problems. As shown in\nFigure 13.10, we can approximate the invariance manifold of the image\n\u201c3\u201d by its tangent at the original image. This tangent can be computed\nby estimating the direction vector from small rotations of the image, or b y\nmore sophisticated spatial smoothing methods (Exercise 13.4.) For large\nrotations, the tangent image no longer looks like a \u201c3,\u201d so the problem\nwith large transformations is alleviated.", "492": "474 13. Prototypes and Nearest-Neighbors\nTransformationsTransformations\nxixi\u2032ofxi\nofxi\u2032Tangent distance\nEuclidean distance\nbetween xiandxi\u2032Distance between\ntransformed\nxiandxi\u2032\nFIGURE 13.11. Tangent distance computation for two images xiandxi\u2032.\nRather than using the Euclidean distance between xiandxi\u2032, or the shortest\ndistance between the two curves, we use the shortest distance b etween the two\ntangent lines.\nThe idea then is to compute the invariant tangent line for each training\nimage. For a query image to be classi\ufb01ed, we compute its invariant tangent\nline, and \ufb01nd the closest line to it among the lines in the training set. The\nclass (digit) corresponding to this closest line is our predicted class for the\nquery image. In Figure 13.11 the two tangent lines intersect, but this is only\nbecause we have been forced to draw a two-dimensional representation of\nthe actual 256-dimensional situation. In IR256the probability of two such\nlines intersecting is e\ufb00ectively zero.\nNow a simpler way to achieve this invariance would be to add into the\ntraining set a number of rotated versions of each training image, and then\njust use a standard nearest-neighbor classi\ufb01er. This idea is called \u201chints\u201d in\nAbu-Mostafa (1995), and works well when the space of invariances is small.\nSo far we have presented a simpli\ufb01ed version of the problem. In addition to\nrotation, there are six other types of transformations under which we would\nlike our classi\ufb01er to be invariant. There are translation (two directio ns),\nscaling (two directions), sheer, and character thickness. Hence the curves\nand tangent lines in Figures 13.10 and 13.11 are actually 7-dimensional\nmanifolds and hyperplanes. It is infeasible to add transformed versions\nof each training image to capture all of these possibilities. The tangent\nmanifolds provide an elegant way of capturing the invariances.\nTable 13.1 shows the test misclassi\ufb01cation error for a problem with 7291\ntraining images and 2007 test digits (the U.S. Postal Services database), for\na carefully constructed neural network, and simple 1-nearest-neighbor and", "493": "13.4 Adaptive Nearest-Neighbor Methods 475\nTABLE 13.1. Test error rates for the handwritten ZIP code problem.\nMethod Error rate\nNeural-net 0.049\n1-nearest-neighbor/Euclidean distance 0.055\n1-nearest-neighbor/tangent distance 0.026\ntangent distance 1-nearest-neighbor rules. The tangent distance nearest-\nneighbor classi\ufb01er works remarkably well, with test error rates near those\nfor the human eye (this is a notoriously di\ufb03cult test set). In practice,\nit turned out that nearest-neighbors are too slow for online classi\ufb01cation\nin this application (see Section 13.5), and neural network classi\ufb01ers were\nsubsequently developed to mimic it.\n13.4 Adaptive Nearest-Neighbor Methods\nWhen nearest-neighbor classi\ufb01cation is carried out in a high-dimensional\nfeature space, the nearest neighbors of a point can be very far away, causing\nbias and degrading the performance of the rule.\nTo quantify this, consider Ndata points uniformly distributed in the unit\ncube [\u22121\n2,1\n2]p. LetRbe the radius of a 1-nearest-neighborhood centered at\nthe origin. Then\nmedian( R) =v\u22121/p\np/parenleft\uf8ecig\n1\u22121\n21/N/parenright\uf8ecig1/p\n, (13.7)\nwhere vprpis the volume of the sphere of radius rinpdimensions. Fig-\nure 13.12 shows the median radius for various training sample sizes and\ndimensions. We see that median radius quickly approaches 0 .5, the dis-\ntance to the edge of the cube.\nWhat can be done about this problem? Consider the two-class situation\nin Figure 13.13. There are two features, and a nearest-neighborhood at\na query point is depicted by the circular region. Implicit in near-neighbor\nclassi\ufb01cation is the assumption that the class probabilities are roughly con-\nstant in the neighborhood, and hence simple averages give good estimates.\nHowever, in this example the class probabilities vary only in the horizontal\ndirection. If we knew this, we would stretch the neighborhood in the verti-\ncal direction, as shown by the tall rectangular region. This will reduce the\nbias of our estimate and leave the variance the same.\nIn general, this calls for adapting the metric used in nearest-neighbor\nclassi\ufb01cation, so that the resulting neighborhoods stretch out in directions\nfor which the class probabilities don\u2019t change much. In high-dimensional\nfeature space, the class probabilities might change only a low-dimensional\nsubspace and hence there can be considerable advantage to adapting the\nmetric.", "494": "476 13. Prototypes and Nearest-Neighbors\nDimensionMedian Radius\n0 5 10 150.0 0.1 0.2 0.3 0.4 0.5 0.6N=100N=1,000\nN=10,000\nFIGURE 13.12. Median radius of a 1-nearest-neighborhood, for uniform data\nwithNobservations in pdimensions.\no\noo\noo\noo\nooo\no\noo\no\nooo\noo\no\noo\noo\no\n\u20225-Nearest Neighborhoods\nFIGURE 13.13. The points are uniform in the cube, with the vertical line sepa-\nrating class red and green. The vertical strip denotes the 5-nearest-neighbor region\nusing only the horizontal coordinate to \ufb01nd the nearest-neighbors fo r the target\npoint (solid dot). The sphere shows the 5-nearest-neighbor region using both co-\nordinates, and we see in this case it has extended into the class-re d region (and\nis dominated by the wrong class in this instance).", "495": "13.4 Adaptive Nearest-Neighbor Methods 477\nFriedman (1994a) proposed a method in which rectangular neighbor-\nhoods are found adaptively by successively carving away edges of a box\ncontaining the training data. Here we describe the discriminant adaptive\nnearest-neighbor (DANN) rule of Hastie and Tibshirani (1996a). Earlier,\nrelated proposals appear in Short and Fukunaga (1981) and Myles and\nHand (1990).\nAt each query point a neighborhood of say 50 points is formed, and the\nclass distribution among the points is used to decide how to deform the\nneighborhood\u2014that is, to adapt the metric. The adapted metric is then\nused in a nearest-neighbor rule at the query point. Thus at each query\npoint a potentially di\ufb00erent metric is used.\nIn Figure 13.13 it is clear that the neighborhood should be stretched in\nthe direction orthogonal to line joining the class centroids. This direction\nalso coincides with the linear discriminant boundary, and is the direction\nin which the class probabilities change the least. In general this direction\nof maximum change will not be orthogonal to the line joining the class cen-\ntroids (see Figure 4.9 on page 116.) Assuming a local discriminant model,\nthe information contained in the local within- and between-class covari-\nance matrices is all that is needed to determine the optimal shape of the\nneighborhood.\nThediscriminant adaptive nearest-neighbor (DANN) metric at a query\npoint x0is de\ufb01ned by\nD(x,x0) = (x\u2212x0)T\u03a3(x\u2212x0), (13.8)\nwhere\n\u03a3=W\u22121/2[W\u22121/2BW\u22121/2+\u01ebI]W\u22121/2\n=W\u22121/2[B\u2217+\u01ebI]W\u22121/2. (13.9)\nHereWis the pooled within-class covariance matrix/summationtextK\nk=1\u03c0kWkandB\nis the between class covariance matrix/summationtextK\nk=1\u03c0k(\u00afxk\u2212\u00afx)(\u00afxk\u2212\u00afx)T, with\nWandBcomputed using only the 50 nearest neighbors around x0. After\ncomputation of the metric, it is used in a nearest-neighbor rule at x0.\nThis complicated formula is actually quite simple in its operation. It \ufb01rst\nspheres the data with respect to W, and then stretches the neighborhood\nin the zero-eigenvalue directions of B\u2217(the between-matrix for the sphered\ndata ). This makes sense, since locally the observed class means do not dif-\nfer in these directions. The \u01ebparameter rounds the neighborhood, from an\nin\ufb01nite strip to an ellipsoid, to avoid using points far away from the quer y\npoint. The value of \u01eb= 1 seems to work well in general. Figure 13.14 shows\nthe resulting neighborhoods for a problem where the classes form two con-\ncentric circles. Notice how the neighborhoods stretch out orthogonally to\nthe decision boundaries when both classes are present in the neighborhood.\nIn the pure regions with only one class, the neighborhoods remain circular;", "496": "478 13. Prototypes and Nearest-Neighbors\no\noo ooo\noo\noo\noo\noo\no\noooo\no\no\nooo\no\noo\nooo\nooooo\noo\no\nooo\noo ooo\noo\no\nooo\nooo\noo\no oo\nooo\no\nooo\nooo\noooooo\no\no\noo\noo\nooooo\noo\no\noooo\noo\no\no\no\noo\noo\no\nooo\noooo\nooo\noo\noo\noo\no\nooo\noo\nooo\nooo\no\noo\noooo\noo o\noo\noooo\noo\noooo\noo\no\no\noo\no\nooo\nooo\noo\no\no\noo\noo\nooo\noo\noo\nooo\noo\noooo\noo\no\noo\no\nooo\no\noooo\noo\nooo\noooo\nooo\no\nooo\nooo\noo\noo\noo\noo\noooo\noooo\no\noo\nooooo\noo\no\noo\nooo\noo\no o\nooo\nooo\no\noooo\nooo\noo\no\nooo\noo\noo\noo\noo\noo\no\noo\no o\noooo\noo\noo\noo\noooo\no\nooo\nooo\noo\noo oo\no\noo\noooo\noo\nooo\no\noooo\no\noo\nooo o\no ooo\nooooo\noo\noo\no\no\nooo\noo\nooo\no\noo\nooo\noo\nooo\noo\noooo ooo\noo\noo\no\no\noo\nooo\nFIGURE 13.14. Neighborhoods found by the DANN procedure, at various query\npoints (centers of the crosses). There are two classes in the da ta, with one class\nsurrounding the other. 50nearest-neighbors were used to estimate the local met-\nrics. Shown are the resulting metrics used to form 15-nearest-neighborhoods.\nin these cases the between matrix B= 0, and the \u03a3in (13.8) is the identity\nmatrix.\n13.4.1 Example\nHere we generate two-class data in ten dimensions, analogous to the two-\ndimensional example of Figure 13.14. All ten predictors in class 1 are in-\ndependent standard normal, conditioned on the radius being greater than\n22.4 and less than 40, while the predictors in class 2 are independent stan-\ndard normal without the restriction. There are 250 observations in each\nclass. Hence the \ufb01rst class almost completely surrounds the second class in\nthe full ten-dimensional space.\nIn this example there are no pure noise variables, the kind that a nearest-\nneighbor subset selection rule might be able to weed out. At any given\npoint in the feature space, the class discrimination occurs along only one\ndirection. However, this direction changes as we move across the feature\nspace and all variables are important somewhere in the space.\nFigure 13.15 shows boxplots of the test error rates over ten realiza-\ntions, for standard 5-nearest-neighbors, LVQ, and discriminant adaptive\n5-nearest-neighbors. We used 50 prototypes per class for LVQ, to make\nit comparable to 5 nearest-neighbors (since 250 /5 = 50). The adaptive\nmetric signi\ufb01cantly reduces the error rate, compared to LVQ or standard\nnearest-neighbors.", "497": "13.4 Adaptive Nearest-Neighbor Methods 4790.0 0.1 0.2 0.3 0.4\n5NN LVQ DANNTest Error\nFIGURE 13.15. Ten-dimensional simulated example: boxplots of the test error\nrates over ten realizations, for standard 5-nearest-neighbors, LVQ with 50centers,\nand discriminant-adaptive 5-nearest-neighbors\n13.4.2 Global Dimension Reduction for Nearest-Neighbors\nThe discriminant-adaptive nearest-neighbor method carries out local di-\nmension reduction\u2014that is, dimension reduction separately at each query\npoint. In many problems we can also bene\ufb01t from global dimension re-\nduction, that is, apply a nearest-neighbor rule in some optimally chosen\nsubspace of the original feature space. For example, suppose that the two\nclasses form two nested spheres in four dimensions of feature space, and\nthere are an additional six noise features whose distribution is independent\nof class. Then we would like to discover the important four-dimensional\nsubspace, and carry out nearest-neighbor classi\ufb01cation in that reduced sub-\nspace. Hastie and Tibshirani (1996a) discuss a variation of the discriminan t-\nadaptive nearest-neighbor method for this purpose. At each training point\nxi, the between-centroids sum of squares matrix Biis computed, and then\nthese matrices are averaged over all training points:\n\u00afB=1\nNN/summationdisplay\ni=1Bi. (13.10)\nLete1,e2,... ,e pbe the eigenvectors of the matrix \u00afB, ordered from largest\nto smallest eigenvalue \u03b8k. Then these eigenvectors span the optimal sub-\nspaces for global subspace reduction. The derivation is based on the fact\nthat the best rank- Lapproximation to \u00afB,\u00afB[L]=/summationtextL\n\u2113=1\u03b8\u2113e\u2113eT\n\u2113, solves the\nleast squares problem\nmin\nrank(M)=LN/summationdisplay\ni=1trace[(Bi\u2212M)2]. (13.11)\nSince each Bicontains information on (a) the local discriminant subspace,\nand (b) the strength of discrimination in that subspace, (13.11) can be seen", "498": "480 13. Prototypes and Nearest-Neighbors\nas a way of \ufb01nding the best approximating subspace of dimension Lto a\nseries of Nsubspaces by weighted least squares (Exercise 13.5.)\nIn the four-dimensional sphere example mentioned above and examined\nin Hastie and Tibshirani (1996a), four of the eigenvalues \u03b8\u2113turn out to be\nlarge (having eigenvectors nearly spanning the interesting subspace), and\nthe remaining six are near zero. Operationally, we project the data into\nthe leading four-dimensional subspace, and then carry out nearest neighbor\nclassi\ufb01cation. In the satellite image classi\ufb01cation example in Section 13. 3.2,\nthe technique labeled DANNin Figure 13.8 used 5-nearest-neighbors in a\nglobally reduced subspace. There are also connections of this technique\nwith the sliced inverse regression proposal of Duan and Li (1991). These\nauthors use similar ideas in the regression setting, but do global rather\nthan local computations. They assume and exploit spherical symmetry of\nthe feature distribution to estimate interesting subspaces.\n13.5 Computational Considerations\nOne drawback of nearest-neighbor rules in general is the computational\nload, both in \ufb01nding the neighbors and storing the entire training set. With\nNobservations and ppredictors, nearest-neighbor classi\ufb01cation requires Np\noperations to \ufb01nd the neighbors per query point. There are fast algorithms\nfor \ufb01nding nearest-neighbors (Friedman et al., 1975; Friedman et al., 1977)\nwhich can reduce this load somewhat. Hastie and Simard (1998) reduce\nthe computations for tangent distance by developing analogs of K-means\nclustering in the context of this invariant metric.\nReducing the storage requirements is more di\ufb03cult, and various editing\nandcondensing procedures have been proposed. The idea is to isolate a\nsubset of the training set that su\ufb03ces for nearest-neighbor predictions, and\nthrow away the remaining training data. Intuitively, it seems important t o\nkeep the training points that are near the decision boundaries and on the\ncorrect side of those boundaries, while some points far from the boundaries\ncould be discarded.\nThemulti-edit algorithm of Devijver and Kittler (1982) divides the data\ncyclically into training and test sets, computing a nearest neighbor rule on\nthe training set and deleting test points that are misclassi\ufb01ed. The idea is\nto keep homogeneous clusters of training observations.\nThecondensing procedure of Hart (1968) goes further, trying to keep\nonly important exterior points of these clusters. Starting with a single ran-\ndomly chosen observation as the training set, each additional data item is\nprocessed one at a time, adding it to the training set only if it is misclas-\nsi\ufb01ed by a nearest-neighbor rule computed on the current training set.\nThese procedures are surveyed in Dasarathy (1991) and Ripley (1996).\nThey can also be applied to other learning procedures besides nearest-", "499": "Exercises 481\nneighbors. While such methods are sometimes useful, we have not had\nmuch practical experience with them, nor have we found any systematic\ncomparison of their performance in the literature.\nBibliographic Notes\nThe nearest-neighbor method goes back at least to Fix and Hodges (1951).\nThe extensive literature on the topic is reviewed by Dasarathy (1991);\nChapter 6 of Ripley (1996) contains a good summary. K-means cluster-\ning is due to Lloyd (1957) and MacQueen (1967). Kohonen (1989) intro-\nduced learning vector quantization. The tangent distance method is due to\nSimard et al. (1993). Hastie and Tibshirani (1996a) proposed the discrim -\ninant adaptive nearest-neighbor technique.\nExercises\nEx. 13.1 Consider a Gaussian mixture model where the covariance matrices\nare assumed to be scalar: \u03a3r=\u03c3I\u2200r= 1,... ,R , and \u03c3is a \ufb01xed param-\neter. Discuss the analogy between the K-means clustering algorithm and\nthe EM algorithm for \ufb01tting this mixture model in detail. Show that in the\nlimit\u03c3\u21920 the two methods coincide.\nEx. 13.2 Derive formula (13.7) for the median radius of the 1-nearest-\nneighborhood.\nEx. 13.3 LetE\u2217be the error rate of the Bayes rule in a K-class problem,\nwhere the true class probabilities are given by pk(x), k= 1,... ,K . As-\nsuming the test point and training point have identical features x, prove\n(13.5)\nK/summationdisplay\nk=1pk(x)(1\u2212pk(x))\u22642(1\u2212pk\u2217(x))\u2212K\nK\u22121(1\u2212pk\u2217(x))2.\nwhere k\u2217= arg max kpk(x). Hence argue that the error rate of the 1-\nnearest-neighbor rule converges in L1, as the size of the training set in-\ncreases, to a value E1, bounded above by\nE\u2217/parenleft\uf8ecig\n2\u2212E\u2217K\nK\u22121/parenright\uf8ecig\n. (13.12)\n[This statement of the theorem of Cover and Hart (1967) is taken from\nChapter 6 of Ripley (1996), where a short proof is also given].", "500": "482 13. Prototypes and Nearest-Neighbors\nEx. 13.4 Consider an image to be a function F(x) : IR2\u221dma\u221asto\u2192IR1over the two-\ndimensional spatial domain (paper coordinates). Then F(c+x0+A(x\u2212x0))\nrepresents an a\ufb03ne transformation of the image F, where Ais a 2 \u00d72\nmatrix.\n1. Decompose A(via Q-R) in such a way that parameters identifying\nthe four a\ufb03ne transformations (two scale, shear and rotation) are\nclearly identi\ufb01ed.\n2. Using the chain rule, show that the derivative of F(c+x0+A(x\u2212x0))\nw.r.t. each of these parameters can be represented in terms of the two\nspatial derivatives of F.\n3. Using a two-dimensional kernel smoother (Chapter 6), describe how\nto implement this procedure when the images are quantized to 16 \u00d716\npixels.\nEx. 13.5 LetBi,i= 1,2,... ,N be square p\u00d7ppositive semi-de\ufb01nite ma-\ntrices and let \u00afB= (1/N)/summationtextBi. Write the eigen-decomposition of \u00afBas/summationtextp\n\u2113=1\u03b8\u2113e\u2113eT\n\u2113with\u03b8\u2113\u2265\u03b8\u2113\u22121\u2265 \u2264\u2264\u2264 \u2265 \u03b81. Show that the best rank- Lapprox-\nimation for the Bi,\nmin\nrank(M)=LN/summationdisplay\ni=1trace[(Bi\u2212M)2],\nis given by \u00afB[L]=/summationtextL\n\u2113=1\u03b8\u2113e\u2113eT\n\u2113. (Hint: Write/summationtextN\ni=1trace[(Bi\u2212M)2] as\nN/summationdisplay\ni=1trace[(Bi\u2212\u00afB)2] +N/summationdisplay\ni=1trace[(M\u2212\u00afB)2]).\nEx. 13.6 Here we consider the problem of shape averaging . In particular,\nLi, i= 1,... ,M are each N\u00d72 matrices of points in IR2, each sampled\nfrom corresponding positions of handwritten (cursive) letters. We seek an\na\ufb03ne invariant average V, also N\u00d72,VTV=I, of the Mletters Liwith\nthe following property: Vminimizes\nM/summationdisplay\nj=1min\nAj\u221d\u230aa\u2207\u2308\u230alLj\u2212VAj\u221d\u230aa\u2207\u2308\u230al2.\nCharacterize the solution.\nThis solution can su\ufb00er if some of the letters are bigand dominate the\naverage. An alternative approach is to minimize instead:\nM/summationdisplay\nj=1min\nAj/vextenddouble/vextenddoubleLjA\u2217\nj\u2212V/vextenddouble/vextenddouble2.\nDerive the solution to this problem. How do the criteria di\ufb00er? Use the\nSVD of the Ljto simplify the comparison of the two approaches.", "501": "Exercises 483\nEx. 13.7 Consider the application of nearest-neighbors to the \u201ceasy\u201d and\n\u201chard\u201d problems in the left panel of Figure 13.5.\n1. Replicate the results in the left panel of Figure 13.5.\n2. Estimate the misclassi\ufb01cation errors using \ufb01vefold cross-validation,\nand compare the error rate curves to those in 1.\n3. Consider an \u201cAIC-like\u201d penalization of the training set misclassi\ufb01ca-\ntion error. Speci\ufb01cally, add 2 t/Nto the training set misclassi\ufb01cation\nerror, where tis the approximate number of parameters N/r,rbe-\ning the number of nearest-neighbors. Compare plots of the resulting\npenalized misclassi\ufb01cation error to those in 1 and 2. Which method\ngives a better estimate of the optimal number of nearest-neighbors:\ncross-validation or AIC?\nEx. 13.8 Generate data in two classes, with two features. These features\nare all independent Gaussian variates with standard deviation 1. Their\nmean vectors are ( \u22121,\u22121) in class 1 and (1 ,1) in class 2. To each feature\nvector apply a random rotation of angle \u03b8,\u03b8chosen uniformly from 0 to\n2\u03c0. Generate 50 observations from each class to form the training set, and\n500 in each class as the test set. Apply four di\ufb00erent classi\ufb01ers:\n1. Nearest-neighbors.\n2. Nearest-neighbors with hints: ten randomly rotated versions of each\ndata point are added to the training set before applying nearest-\nneighbors.\n3. Invariant metric nearest-neighbors, using Euclidean distance invari-\nant to rotations about the origin.\n4. Tangent distance nearest-neighbors.\nIn each case choose the number of neighbors by tenfold cross-validation.\nCompare the results.", "502": "484 13. Prototypes and Nearest-Neighbors", "503": "This is page 485\nPrinter: Opaque this\n14\nUnsupervised Learning\n14.1 Introduction\nThe previous chapters have been concerned with predicting the values\nof one or more outputs or response variables Y= (Y1,... ,Y m) for a\ngiven set of input or predictor variables XT= (X1,... ,X p). Denote by\nxT\ni= (xi1,... ,x ip) the inputs for the ith training case, and let yibe a\nresponse measurement. The predictions are based on the training sample\n(x1,y1),... ,(xN,yN) of previously solved cases, where the joint values of\nall of the variables are known. This is called supervised learning or \u201clearn-\ning with a teacher.\u201d Under this metaphor the \u201cstudent\u201d presents an an-\nswer \u02c6yifor each xiin the training sample, and the supervisor or \u201cteacher\u201d\nprovides either the correct answer and/or an error associated with the stu-\ndent\u2019s answer. This is usually characterized by some loss function L(y,\u02c6y),\nfor example, L(y,\u02c6y) = (y\u2212\u02c6y)2.\nIf one supposes that ( X,Y) are random variables represented by some\njoint probability density Pr( X,Y), then supervised learning can be formally\ncharacterized as a density estimation problem where one is concerned with\ndetermining properties of the conditional density Pr( Y|X). Usually the\nproperties of interest are the \u201clocation\u201d parameters \u03b8that minimize the\nexpected error at each x,\n\u03b8(x) = argmin\n\u03b8EY|XL(Y,\u03b8). (14.1)", "504": "486 14. Unsupervised Learning\nConditioning one has\nPr(X,Y) = Pr( Y|X)\u2264Pr(X),\nwhere Pr( X) is the joint marginal density of the Xvalues alone. In su-\npervised learning Pr( X) is typically of no direct concern. One is interested\nmainly in the properties of the conditional density Pr( Y|X). Since Yis of-\nten of low dimension (usually one), and only its location \u03b8(x) is of interest,\nthe problem is greatly simpli\ufb01ed. As discussed in the previous chapters,\nthere are many approaches for successfully addressing supervised learning\nin a variety of contexts.\nIn this chapter we address unsupervised learning or \u201clearning without a\nteacher.\u201d In this case one has a set of Nobservations ( x1,x2,... ,x N) of a\nrandom p-vector Xhaving joint density Pr( X). The goal is to directly infer\nthe properties of this probability density without the help of a supervisor or\nteacher providing correct answers or degree-of-error for each observation.\nThe dimension of Xis sometimes much higher than in supervised learn-\ning, and the properties of interest are often more complicated than simple\nlocation estimates. These factors are somewhat mitigated by the fact that\nXrepresents all of the variables under consideration; one is not required\nto infer how the properties of Pr( X) change, conditioned on the changing\nvalues of another set of variables.\nIn low-dimensional problems (say p\u22643), there are a variety of e\ufb00ective\nnonparametric methods for directly estimating the density Pr( X) itself at\nallX-values, and representing it graphically (Silverman, 1986, e.g.). Owing\nto the curse of dimensionality, these methods fail in high dimensions. One\nmust settle for estimating rather crude global models, such as Gaussian\nmixtures or various simple descriptive statistics that characterize Pr( X).\nGenerally, these descriptive statistics attempt to characterize X-values,\nor collections of such values, where Pr( X) is relatively large. Principal\ncomponents, multidimensional scaling, self-organizing maps, and principal\ncurves, for example, attempt to identify low-dimensional manifolds within\ntheX-space that represent high data density. This provides information\nabout the associations among the variables and whether or not they can be\nconsidered as functions of a smaller set of \u201clatent\u201d variables. Cluster anal-\nysis attempts to \ufb01nd multiple convex regions of the X-space that contain\nmodes of Pr( X). This can tell whether or not Pr( X) can be represented by\na mixture of simpler densities representing distinct types or classes of ob-\nservations. Mixture modeling has a similar goal. Association rules att empt\nto construct simple descriptions (conjunctive rules) that describe regions\nof high density in the special case of very high dimensional binary-valued\ndata.\nWith supervised learning there is a clear measure of success, or lack\nthereof, that can be used to judge adequacy in particular situations and\nto compare the e\ufb00ectiveness of di\ufb00erent methods over various situations.", "505": "14.2 Association Rules 487\nLack of success is directly measured by expected loss over the joint dis-\ntribution Pr( X,Y). This can be estimated in a variety of ways including\ncross-validation. In the context of unsupervised learning, there is no such\ndirect measure of success. It is di\ufb03cult to ascertain the validity of inferences\ndrawn from the output of most unsupervised learning algorithms. One must\nresort to heuristic arguments not only for motivating the algorithms, as is\noften the case in supervised learning as well, but also for judgments as to\nthe quality of the results. This uncomfortable situation has led to heavy\nproliferation of proposed methods, since e\ufb00ectiveness is a matter of opinion\nand cannot be veri\ufb01ed directly.\nIn this chapter we present those unsupervised learning techniques that\nare among the most commonly used in practice, and additionally, a few\nothers that are favored by the authors.\n14.2 Association Rules\nAssociation rule analysis has emerged as a popular tool for mining com-\nmercial data bases. The goal is to \ufb01nd joint values of the variables X=\n(X1,X2,... ,X p) that appear most frequently in the data base. It is most\noften applied to binary-valued data Xj\u2208 {0,1}, where it is referred to as\n\u201cmarket basket\u201d analysis. In this context the observations are sales trans -\nactions, such as those occurring at the checkout counter of a store. The\nvariables represent all of the items sold in the store. For observation i, each\nvariable Xjis assigned one of two values; xij= 1 if the jth item is pur-\nchased as part of the transaction, whereas xij= 0 if it was not purchased.\nThose variables that frequently have joint values of one represent items that\nare frequently purchased together. This information can be quite useful for\nstocking shelves, cross-marketing in sales promotions, catalog design, and\nconsumer segmentation based on buying patterns.\nMore generally, the basic goal of association rule analysis is to \ufb01nd a\ncollection of prototype X-values v1,... ,v Lfor the feature vector X, such\nthat the probability density Pr( vl) evaluated at each of those values is rela-\ntively large. In this general framework, the problem can be viewed as \u201cmode\n\ufb01nding\u201d or \u201cbump hunting.\u201d As formulated, this problem is impossibly dif-\n\ufb01cult. A natural estimator for each Pr( vl) is the fraction of observations\nfor which X=vl. For problems that involve more than a small number\nof variables, each of which can assume more than a small number of val-\nues, the number of observations for which X=vlwill nearly always be too\nsmall for reliable estimation. In order to have a tractable problem, both t he\ngoals of the analysis and the generality of the data to which it is applied\nmust be greatly simpli\ufb01ed.\nThe \ufb01rst simpli\ufb01cation modi\ufb01es the goal. Instead of seeking values x\nwhere Pr( x) is large, one seeks regions of the X-space with high probability", "506": "488 14. Unsupervised Learning\ncontent relative to their size or support. Let Sjrepresent the set of all\npossible values of the jth variable (its support ), and let sj\u2286 Sjbe a subset\nof these values. The modi\ufb01ed goal can be stated as attempting to \ufb01nd\nsubsets of variable values s1,... ,s psuch that the probability of each of the\nvariables simultaneously assuming a value within its respective subset,\nPr\uf8ee\n\uf8f0p/intersectiondisplay\nj=1(Xj\u2208sj)\uf8f9\n\uf8fb, (14.2)\nis relatively large. The intersection of subsets \u2229p\nj=1(Xj\u2208sj) is called a\nconjunctive rule . For quantitative variables the subsets sjare contiguous\nintervals; for categorical variables the subsets are delineated explicitly. No te\nthat if the subset sjis in fact the entire set of values sj=Sj, as is often\nthe case, the variable Xjis said notto appear in the rule (14.2).\n14.2.1 Market Basket Analysis\nGeneral approaches to solving (14.2) are discussed in Section 14.2.5. These\ncan be quite useful in many applications. However, they are not feasible\nfor the very large ( p\u2248104,N\u2248108) commercial data bases to which\nmarket basket analysis is often applied. Several further simpli\ufb01cations of\n(14.2) are required. First, only two types of subsets are considered; either\nsjconsists of a single value of Xj,sj=v0j, or it consists of the entire set\nof values that Xjcan assume, sj=Sj. This simpli\ufb01es the problem (14.2)\nto \ufb01nding subsets of the integers J \u2282 { 1,... ,p }, and corresponding values\nv0j, j\u2208 J, such that\nPr\uf8ee\n\uf8f0/intersectiondisplay\nj\u2208J(Xj=v0j)\uf8f9\n\uf8fb (14.3)\nis large. Figure 14.1 illustrates this assumption.\nOne can apply the technique of dummy variables to turn (14.3) into\na problem involving only binary-valued variables. Here we assume that\nthe support Sjis \ufb01nite for each variable Xj. Speci\ufb01cally, a new set of\nvariables Z1,... ,Z Kis created, one such variable for each of the values\nvljattainable by each of the original variables X1,... ,X p. The number of\ndummy variables Kis\nK=p/summationdisplay\nj=1|Sj|,\nwhere |Sj|is the number of distinct values attainable by Xj. Each dummy\nvariable is assigned the value Zk= 1 if the variable with which it is as-\nsociated takes on the corresponding value to which Zkis assigned, and\nZk= 0 otherwise. This transforms (14.3) to \ufb01nding a subset of the integers\nK \u2282 { 1,... ,K }such that", "507": "14.2 Association Rules 489\nX1 X1 X1\nX2X2X2\nFIGURE 14.1. Simpli\ufb01cations for association rules. Here there are two inputs\nX1andX2, taking four and six distinct values, respectively. The red squ ares\nindicate areas of high density. To simplify the computations, w e assume that the\nderived subset corresponds to either a single value of an input o r all values. With\nthis assumption we could \ufb01nd either the middle or right pattern, but not the left\none.\nPr/bracketleft\uf8ecigg/intersectiondisplay\nk\u2208K(Zk= 1)/bracketright\uf8ecigg\n= Pr/bracketleft\uf8ecigg/productdisplay\nk\u2208KZk= 1/bracketright\uf8ecigg\n(14.4)\nis large. This is the standard formulation of the market basket problem.\nThe set Kis called an \u201citem set.\u201d The number of variables Zkin the item\nset is called its \u201csize\u201d (note that the size is no bigger than p). The estimated\nvalue of (14.4) is taken to be the fraction of observations in the data bas e\nfor which the conjunction in (14.4) is true:\n/hatwiderPr/bracketleft\uf8ecigg/productdisplay\nk\u2208K(Zk= 1)/bracketright\uf8ecigg\n=1\nNN/summationdisplay\ni=1/productdisplay\nk\u2208Kzik. (14.5)\nHerezikis the value of Zkfor this ith case. This is called the \u201csupport\u201d or\n\u201cprevalence\u201d T(K) of the item set K. An observation ifor which/producttext\nk\u2208Kzik=\n1 is said to \u201ccontain\u201d the item set K.\nIn association rule mining a lower support bound tis speci\ufb01ed, and one\nseeksallitem sets Klthat can be formed from the variables Z1,... ,Z K\nwith support in the data base greater than this lower bound t\n{Kl|T(Kl)> t}. (14.6)\n14.2.2 The Apriori Algorithm\nThe solution to this problem (14.6) can be obtained with feasible compu-\ntation for very large data bases provided the threshold tis adjusted so that\n(14.6) consists of only a small fraction of all 2Kpossible item sets. The\n\u201cApriori\u201d algorithm (Agrawal et al., 1995) exploits several aspects o f the", "508": "490 14. Unsupervised Learning\ncurse of dimensionality to solve (14.6) with a small number of passes over\nthe data. Speci\ufb01cally, for a given support threshold t:\n\u2022The cardinality |{K|T(K)> t}|is relatively small.\n\u2022Any item set Lconsisting of a subset of the items in Kmust have\nsupport greater than or equal to that of K,L \u2286 K \u21d2 T(L)\u2265T(K).\nThe \ufb01rst pass over the data computes the support of all single-item sets.\nThose whose support is less than the threshold are discarded. The second\npass computes the support of all item sets of size two that can be formed\nfrom pairs of the single items surviving the \ufb01rst pass. In other words, to\ngenerate all frequent itemsets with |K|=m, we need to consider only\ncandidates such that allof their mancestral item sets of size m\u22121 are\nfrequent. Those size-two item sets with support less than the threshold are\ndiscarded. Each successive pass over the data considers only those item\nsets that can be formed by combining those that survived the previous\npass with those retained from the \ufb01rst pass. Passes over the data continue\nuntil all candidate rules from the previous pass have support less than the\nspeci\ufb01ed threshold. The Apriori algorithm requires only one pass over the\ndata for each value of |K|, which is crucial since we assume the data cannot\nbe \ufb01tted into a computer\u2019s main memory. If the data are su\ufb03ciently sparse\n(or if the threshold tis high enough), then the process will terminate in\nreasonable time even for huge data sets.\nThere are many additional tricks that can be used as part of this strat-\negy to increase speed and convergence (Agrawal et al., 1995). The Apriori\nalgorithm represents one of the major advances in data mining technology.\nEach high support item set K(14.6) returned by the Apriori algorithm is\ncast into a set of \u201cassociation rules.\u201d The items Zk,k\u2208 K, are partitioned\ninto two disjoint subsets, A\u222aB=K, and written\nA\u21d2B. (14.7)\nThe \ufb01rst item subset Ais called the \u201cantecedent\u201d and the second Bthe\n\u201cconsequent.\u201d Association rules are de\ufb01ned to have several properties based\non the prevalence of the antecedent and consequent item sets in the data\nbase. The \u201csupport\u201d of the rule T(A\u21d2B) is the fraction of observations\nin the union of the antecedent and consequent, which is just the support\nof the item set Kfrom which they were derived. It can be viewed as an\nestimate (14.5) of the probability of simultaneously observing both item\nsets Pr( AandB) in a randomly selected market basket. The \u201ccon\ufb01dence\u201d\nor \u201cpredictability\u201d C(A\u21d2B) of the rule is its support divided by the\nsupport of the antecedent\nC(A\u21d2B) =T(A\u21d2B)\nT(A), (14.8)\nwhich can be viewed as an estimate of Pr( B|A). The notation Pr( A), the\nprobability of an item set Aoccurring in a basket, is an abbreviation for", "509": "14.2 Association Rules 491\nPr(/producttext\nk\u2208AZk= 1). The \u201cexpected con\ufb01dence\u201d is de\ufb01ned as the support of\nthe consequent T(B), which is an estimate of the unconditional probability\nPr(B). Finally, the \u201clift\u201d of the rule is de\ufb01ned as the con\ufb01dence divided by\nthe expected con\ufb01dence\nL(A\u21d2B) =C(A\u21d2B)\nT(B).\nThis is an estimate of the association measure Pr( AandB)/Pr(A)Pr(B).\nAs an example, suppose the item set K={peanut butter, jelly, bread }\nand consider the rule {peanut butter, jelly } \u21d2 {bread}. A support value\nof 0.03 for this rule means that peanut butter ,jelly, andbread appeared\ntogether in 3% of the market baskets. A con\ufb01dence of 0.82 for this rule im-\nplies that when peanut butter andjelly were purchased, 82% of the time\nbread was also purchased. If bread appeared in 43% of all market baskets\nthen the rule {peanut butter, jelly } \u21d2 {bread}would have a lift of 1 .95.\nThe goal of this analysis is to produce association rules (14.7) with bot h\nhigh values of support and con\ufb01dence (14.8). The Apriori algorithm returns\nall item sets with high support as de\ufb01ned by the support threshold t(14.6).\nA con\ufb01dence threshold cis set, and all rules that can be formed from those\nitem sets (14.6) with con\ufb01dence greater than this value\n{A\u21d2B|C(A\u21d2B)> c} (14.9)\nare reported. For each item set Kof size |K|there are 2|K|\u22121\u22121 rules of\nthe form A\u21d2(K \u2212A),A\u2282 K. Agrawal et al. (1995) present a variant of\nthe Apriori algorithm that can rapidly determine which rules survive the\ncon\ufb01dence threshold (14.9) from all possible rules that can be formed from\nthe solution item sets (14.6).\nThe output of the entire analysis is a collection of association rules (14.7 )\nthat satisfy the constraints\nT(A\u21d2B)> t and C(A\u21d2B)> c.\nThese are generally stored in a data base that can be queried by the user.\nTypical requests might be to display the rules in sorted order of con\ufb01dence,\nlift or support. More speci\ufb01cally, one might request such a list conditioned\non particular items in the antecedent or especially the consequent. For\nexample, a request might be the following:\nDisplay all transactions in which ice skates are the consequ ent\nthat have con\ufb01dence over 80%and support of more than 2%.\nThis could provide information on those items (antecedent) that predicate\nsales of ice skates. Focusing on a particular consequent casts the problem\ninto the framework of supervised learning.\nAssociation rules have become a popular tool for analyzing very large\ncommercial data bases in settings where market basket is relevant. That is", "510": "492 14. Unsupervised Learning\nwhen the data can be cast in the form of a multidimensional contingency\ntable. The output is in the form of conjunctive rules (14.4) that are easily\nunderstood and interpreted. The Apriori algorithm allows this analysis to\nbe applied to huge data bases, much larger that are amenable to other types\nof analyses. Association rules are among data mining\u2019s biggest successes.\nBesides the restrictive form of the data to which they can be applied, as-\nsociation rules have other limitations. Critical to computational feasibi lity\nis the support threshold (14.6). The number of solution item sets, their size,\nand the number of passes required over the data can grow exponentially\nwith decreasing size of this lower bound. Thus, rules with high con\ufb01dence\nor lift, but low support, will not be discovered. For example, a high con\ufb01-\ndence rule such as vodka \u21d2caviar will not be uncovered owing to the low\nsales volume of the consequent caviar .\n14.2.3 Example: Market Basket Analysis\nWe illustrate the use of Apriori on a moderately sized demographics data\nbase. This data set consists of N= 9409 questionnaires \ufb01lled out by shop-\nping mall customers in the San Francisco Bay Area (Impact Resources, Inc.,\nColumbus OH, 1987). Here we use answers to the \ufb01rst 14 questions, relat-\ning to demographics, for illustration. These questions are listed in Table\n14.1. The data are seen to consist of a mixture of ordinal and (unordered)\ncategorical variables, many of the latter having more than a few values.\nThere are many missing values.\nWe used a freeware implementation of the Apriori algorithm due to Chris-\ntian Borgelt1. After removing observations with missing values, each ordinal\npredictor was cut at its median and coded by two dummy variables; each\ncategorical predictor with kcategories was coded by kdummy variables.\nThis resulted in a 6876 \u00d750 matrix of 6876 observations on 50 dummy\nvariables.\nThe algorithm found a total of 6288 association rules, involving \u22645\npredictors, with support of at least 10%. Understanding this large set of\nrules is itself a challenging data analysis task. We will not attempt this here,\nbut only illustrate in Figure 14.2 the relative frequency of each dummy\nvariable in the data (top) and the association rules (bottom). Prevalent\ncategories tend to appear more often in the rules, for example, the \ufb01rst\ncategory in language (English). However, others such as occupation are\nunder-represented, with the exception of the \ufb01rst and \ufb01fth level.\nHere are three examples of association rules found by the Apriori algo-\nrithm:\nAssociation rule 1: Support 25%, con\ufb01dence 99.7% and lift 1.03.\n1Seehttp://fuzzy.cs.uni-magdeburg.de/ \u223cborgelt.", "511": "14.2 Association Rules 493\n0 10 20 30 40 500.0 0.02 0.04 0.06\nAttributeRelative Frequency in Data\nincomesexmarstatageeduc occup yrs\u2212baydualincperhousperyounghousetypehome ethnic language\n0 10 20 30 40 500.0 0.04 0.08 0.12\nAttributeRelative Frequency in Association Rules\nincomesexmarstatageeduc occup yrs\u2212baydualincperhousperyounghousetypehome ethnic languageFIGURE 14.2. Market basket analysis: relative frequency of each dummy vari -\nable (coding an input category) in the data (top), and the associ ation rules found\nby the Apriori algorithm (bottom).", "512": "494 14. Unsupervised Learning\nTABLE 14.1. Inputs for the demographic data.\nFeature Demographic # Values Type\n1 Sex 2 Categorical\n2 Marital status 5 Categorical\n3 Age 7 Ordinal\n4 Education 6 Ordinal\n5 Occupation 9 Categorical\n6 Income 9 Ordinal\n7 Years in Bay Area 5 Ordinal\n8 Dual incomes 3 Categorical\n9 Number in household 9 Ordinal\n10 Number of children 9 Ordinal\n11 Householder status 3 Categorical\n12 Type of home 5 Categorical\n13 Ethnic classi\ufb01cation 8 Categorical\n14 Language in home 3 Categorical\n/bracketleftbiggnumber in household = 1\nnumber of children = 0/bracketrightbigg\n\u21d3\nlanguage in home = English\nAssociation rule 2: Support 13.4%, con\ufb01dence 80.8%, and lift 2.13.\n\uf8ee\n\uf8f0language in home = English\nhouseholder status = own\noccupation = {professional/managerial }\uf8f9\n\uf8fb\n\u21d3\nincome \u2265$40,000\nAssociation rule 3: Support 26.5%, con\ufb01dence 82.8% and lift 2.15.\n\uf8ee\n\uf8ef\uf8ef\uf8f0language in home = English\nincome <$40,000\nmarital status = not married\nnumber of children = 0\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u21d3\neducation /\u2208 {college graduate, graduate study }", "513": "14.2 Association Rules 495\nWe chose the \ufb01rst and third rules based on their high support. The second\nrule is an association rule with a high-income consequent, and could be\nused to try to target high-income individuals.\nAs stated above, we created dummy variables for each category of the\ninput predictors, for example, Z1=I(income <$40,000) and Z2=\nI(income \u2265$40,000) for below and above the median income. If we were\ninterested only in \ufb01nding associations with the high-income category, we\nwould include Z2but not Z1. This is often the case in actual market basket\nproblems, where we are interested in \ufb01nding associations with the presence\nof a relatively rare item, but not associations with its absence.\n14.2.4 Unsupervised as Supervised Learning\nHere we discuss a technique for transforming the density estimation prob-\nlem into one of supervised function approximation. This forms the basis\nfor the generalized association rules described in the next section.\nLetg(x) be the unknown data probability density to be estimated, and\ng0(x) be a speci\ufb01ed probability density function used for reference. For ex-\nample, g0(x) might be the uniform density over the range of the variables.\nOther possibilities are discussed below. The data set x1,x2,... ,x Nis pre-\nsumed to be an i.i.d.random sample drawn from g(x). A sample of size N0\ncan be drawn from g0(x) using Monte Carlo methods. Pooling these two\ndata sets, and assigning mass w=N0/(N+N0) to those drawn from g(x),\nandw0=N/(N+N0) to those drawn from g0(x), results in a random\nsample drawn from the mixture density ( g(x) +g0(x))/2. If one assigns\nthe value Y= 1 to each sample point drawn from g(x) and Y= 0 those\ndrawn from g0(x), then\n\u03b8(x) =E(Y|x) =g(x)\ng(x) +g0(x)\n=g(x)/g0(x)\n1 +g(x)/g0(x)(14.10)\ncan be estimated by supervised learning using the combined sample\n(y1,x1),(y2,x2),... ,(yN+N0,xN+N0) (14.11)\nas training data. The resulting estimate \u02c6 \u03b8(x) can be inverted to provide an\nestimate for g(x)\n\u02c6g(x) =g0(x)\u02c6\u03b8(x)\n1\u2212\u02c6\u03b8(x). (14.12)\nGeneralized versions of logistic regression (Section 4.4) are especially wel l\nsuited for this application since the log-odds,\nf(x) = logg(x)\ng0(x), (14.13)\nare estimated directly. In this case one has", "514": "496 14. Unsupervised Learning\n-1 0 1 2-2 0 2 4 6\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n-1 0 1 2-2 0 2 4 6\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022 \u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\nX1 X1\nX2X2\nFIGURE 14.3. Density estimation via classi\ufb01cation. (Left panel:) Training set\nof200data points. (Right panel:) Training set plus 200reference data points,\ngenerated uniformly over the rectangle containing the training data . The training\nsample was labeled as class 1, and the reference sample class 0, and a semipara-\nmetric logistic regression model was \ufb01t to the data. Some contou rs for \u02c6g(x)are\nshown.\n\u02c6g(x) =g0(x)e\u02c6f(x). (14.14)\nAn example is shown in Figure 14.3. We generated a training set of size\n200 shown in the left panel. The right panel shows the reference data (blue)\ngenerated uniformly over the rectangle containing the training data. The\ntraining sample was labeled as class 1, and the reference sample class 0,\nand a logistic regression model, using a tensor product of natural splines\n(Section 5.2.1), was \ufb01t to the data. Some probability contours of \u02c6 \u03b8(x) are\nshown in the right panel; these are also the contours of the density estimate\n\u02c6g(x), since \u02c6 g(x) = \u02c6\u03b8(x)/(1\u2212\u02c6\u03b8(x)), is a monotone function. The contours\nroughly capture the data density.\nIn principle any reference density can be used for g0(x) in (14.14). In\npractice the accuracy of the estimate \u02c6 g(x) can depend greatly on partic-\nular choices. Good choices will depend on the data density g(x) and the\nprocedure used to estimate (14.10) or (14.13). If accuracy is the goal, g0(x)\nshould be chosen so that the resulting functions \u03b8(x) orf(x) are approx-\nimated easily by the method being used. However, accuracy is not always\nthe primary goal. Both \u03b8(x) and f(x) are monotonic functions of the den-\nsity ratio g(x)/g0(x). They can thus be viewed as \u201ccontrast\u201d statistics that\nprovide information concerning departures of the data density g(x) from\nthe chosen reference density g0(x). Therefore, in data analytic settings, a\nchoice for g0(x) is dictated by types of departures that are deemed most\ninteresting in the context of the speci\ufb01c problem at hand. For example, if\ndepartures from uniformity are of interest, g0(x) might be the a uniform\ndensity over the range of the variables. If departures from joint normality", "515": "14.2 Association Rules 497\nare of interest, a good choice for g0(x) would be a Gaussian distribution\nwith the same mean vector and covariance matrix as the data. Departures\nfrom independence could be investigated by using\ng0(x) =p/productdisplay\nj=1gj(xj), (14.15)\nwhere gj(xj) is the marginal data density of Xj, thejth coordinate of X.\nA sample from this independent density (14.15) is easily generated from the\ndata itself by applying a di\ufb00erent random permutation to the data values\nof each of the variables.\nAs discussed above, unsupervised learning is concerned with revealing\nproperties of the data density g(x). Each technique focuses on a particu-\nlar property or set of properties. Although this approach of transforming\nthe problem to one of supervised learning (14.10)\u2013(14.14) seems to have\nbeen part of the statistics folklore for some time, it does not appear to\nhave had much impact despite its potential to bring well-developed su-\npervised learning methodology to bear on unsupervised learning problems.\nOne reason may be that the problem must be enlarged with a simulated\ndata set generated by Monte Carlo techniques. Since the size of this data\nset should be at least as large as the data sample N0\u2265N, the compu-\ntation and memory requirements of the estimation procedure are at least\ndoubled. Also, substantial computation may be required to generate the\nMonte Carlo sample itself. Although perhaps a deterrent in the past, these\nincreased computational requirements are becoming much less of a burden\nas increased resources become routinely available. We illustrate the use of\nsupervising learning methods for unsupervised learning in the next section.\n14.2.5 Generalized Association Rules\nThe more general problem (14.2) of \ufb01nding high-density regions in the data\nspace can be addressed using the supervised learning approach described\nabove. Although not applicable to the huge data bases for which market\nbasket analysis is feasible, useful information can be obtained from mod-\nerately sized data sets. The problem (14.2) can be formulated as \ufb01nding\nsubsets of the integers J \u2282 { 1,2,... ,p }and corresponding value subsets\nsj, j\u2208 Jfor the corresponding variables Xj, such that\n/hatwiderPr\uf8eb\n\uf8ed/intersectiondisplay\nj\u2208J(Xj\u2208sj)\uf8f6\n\uf8f8=1\nNN/summationdisplay\ni=1I\uf8eb\n\uf8ed/intersectiondisplay\nj\u2208J(xij\u2208sj)\uf8f6\n\uf8f8 (14.16)\nis large. Following the nomenclature of association rule analysis, {(Xj\u2208\nsj)}j\u2208Jwill be called a \u201cgeneralized\u201d item set. The subsets sjcorrespond-\ning to quantitative variables are taken to be contiguous intervals wit hin", "516": "498 14. Unsupervised Learning\ntheir range of values, and subsets for categorical variables can involve more\nthan a single value. The ambitious nature of this formulation precludes a\nthorough search for all generalized item sets with support (14.16) greater\nthan a speci\ufb01ed minimum threshold, as was possible in the more restric-\ntive setting of market basket analysis. Heuristic search methods must be\nemployed, and the most one can hope for is to \ufb01nd a useful collection of\nsuch generalized item sets.\nBoth market basket analysis (14.5) and the generalized formulation (14.1 6)\nimplicitly reference the uniform probability distribution. One seeks item\nsets that are more frequent than would be expected if all joint data values\n(x1,x2,... ,x N) were uniformly distributed. This favors the discovery of\nitem sets whose marginal constituents ( Xj\u2208sj) areindividually frequent,\nthat is, the quantity\n1\nNN/summationdisplay\ni=1I(xij\u2208sj) (14.17)\nis large. Conjunctions of frequent subsets (14.17) will tend to appear more\noften among item sets of high support (14.16) than conjunctions of margin-\nally less frequent subsets. This is why the rule vodka \u21d2caviar is not likely\nto be discovered in spite of a high association (lift); neither item has high\nmarginal support, so that their joint support is especially small. Reference\nto the uniform distribution can cause highly frequent item sets with low\nassociations among their constituents to dominate the collection of highest\nsupport item sets.\nHighly frequent subsets sjare formed as disjunctions of the most fre-\nquent Xj-values. Using the product of the variable marginal data densities\n(14.15) as a reference distribution removes the preference for highly fre-\nquent values of the individual variables in the discovered item sets. This is\nbecause the density ratio g(x)/g0(x) is uniform if there are no associations\namong the variables (complete independence), regardless of the frequency\ndistribution of the individual variable values. Rules like vodka \u21d2caviar\nwould have a chance to emerge. It is not clear however, how to incorporate\nreference distributions other than the uniform into the Apriori algorithm.\nAs explained in Section 14.2.4, it is straightforward to generate a sampl e\nfrom the product density (14.15), given the original data set.\nAfter choosing a reference distribution, and drawing a sample from it\nas in (14.11), one has a supervised learning problem with a binary-valued\noutput variable Y\u2208 {0,1}. The goal is to use this training data to \ufb01nd\nregions\nR=/intersectiondisplay\nj\u2208J(Xj\u2208sj) (14.18)\nfor which the target function \u03b8(x) =E(Y|x) is relatively large. In addition,\none might wish to require that the datasupport of these regions", "517": "14.2 Association Rules 499\nT(R) =/integraldisplay\nx\u2208Rg(x)dx (14.19)\nnot be too small.\n14.2.6 Choice of Supervised Learning Method\nThe regions (14.18) are de\ufb01ned by conjunctive rules. Hence supervised\nmethods that learn such rules would be most appropriate in this context.\nThe terminal nodes of a CART decision tree are de\ufb01ned by rules precisely\nof the form (14.18). Applying CART to the pooled data (14.11) will pro-\nduce a decision tree that attempts to model the target (14.10) over the\nentire data space by a disjoint set of regions (terminal nodes). Each region\nis de\ufb01ned by a rule of the form (14.18). Those terminal nodes twith high\naverage y-values\n\u00afyt= ave( yi|xi\u2208t)\nare candidates for high-support generalized item sets (14.16). The actual\n(data) support is given by\nT(R) = \u00afyt\u2264Nt\nN+N0,\nwhere Ntis the number of (pooled) observations within the region repre-\nsented by the terminal node. By examining the resulting decision tree, one\nmight discover interesting generalized item sets of relatively high-support.\nThese can then be partitioned into antecedents and consequents in a search\nfor generalized association rules of high con\ufb01dence and/or lift.\nAnother natural learning method for this purpose is the patient rule\ninduction method PRIM described in Section 9.3. PRIM also produces\nrules precisely of the form (14.18), but it is especially designed for \ufb01nding\nhigh-support regions that maximize the average target (14.10) value within\nthem, rather than trying to model the target function over the entire data\nspace. It also provides more control over the support/average-target-value\ntradeo\ufb00.\nExercise 14.3 addresses an issue that arises with either of these methods\nwhen we generate random data from the product of the marginal distribu-\ntions.\n14.2.7 Example: Market Basket Analysis (Continued)\nWe illustrate the use of PRIM on the demographics data of Table 14.1.\nThree of the high-support generalized item sets emerging from the PRIM\nanalysis were the following:\nItem set 1: Support= 24%.", "518": "500 14. Unsupervised Learning\n\uf8ee\n\uf8f0marital status = married\nhouseholder status = own\ntype of home \u221dne}ationslash=apartment\uf8f9\n\uf8fb\nItem set 2: Support= 24%.\n\uf8ee\n\uf8ef\uf8ef\uf8f0age\u226424\nmarital status \u2208 {living together-not married, single }\noccupation /\u2208 {professional, homemaker, retired }\nhouseholder status \u2208 {rent, live with family }\uf8f9\n\uf8fa\uf8fa\uf8fb\nItem set 3: Support= 15%.\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0householder status = rent\ntype of home \u221dne}ationslash=house\nnumber in household \u22642\nnumber of children = 0\noccupation /\u2208 {homemaker, student, unemployed }\nincome \u2208[$20,000 ,$150,000]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nGeneralized association rules derived from these item sets with con\ufb01dence\n(14.8) greater than 95% are the following:\nAssociation rule 1: Support 25%, con\ufb01dence 99.7% and lift 1.35.\n/bracketleftbigg\nmarital status = married\nhouseholder status = own/bracketrightbigg\n\u21d3\ntype of home \u221dne}ationslash=apartment\nAssociation rule 2: Support 25%, con\ufb01dence 98.7% and lift 1.97.\n\uf8ee\n\uf8f0age\u226424\noccupation /\u2208 {professional, homemaker, retired }\nhouseholder status \u2208 {rent, live with family }\uf8f9\n\uf8fb\n\u21d3\nmarital status \u2208 {single, living together-not married }\nAssociation rule 3: Support 25%, con\ufb01dence 95.9% and lift 2.61.\n/bracketleftbigghouseholder status = own\ntype of home \u221dne}ationslash=apartment/bracketrightbigg\n\u21d3\nmarital status = married", "519": "14.3 Cluster Analysis 501\nAssociation rule 4: Support 15%, con\ufb01dence 95.4% and lift 1.50.\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0householder status = rent\ntype of home \u221dne}ationslash=house\nnumber in household \u22642\noccupation /\u2208 {homemaker, student, unemployed }\nincome \u2208[$20,000 ,$150,000]\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u21d3\nnumber of children = 0\nThere are no great surprises among these particular rules. For the most\npart they verify intuition. In other contexts where there is less prior in-\nformation available, unexpected results have a greater chance to emerge.\nThese results do illustrate the type of information generalized associatio n\nrules can provide, and that the supervised learning approach, coupled with\na ruled induction method such as CART or PRIM, can uncover item sets\nexhibiting high associations among their constituents.\nHow do these generalized association rules compare to those found earlier\nby the Apriori algorithm? Since the Apriori procedure gives thousands of\nrules, it is di\ufb03cult to compare them. However some general points can be\nmade. The Apriori algorithm is exhaustive\u2014it \ufb01nds allrules with support\ngreater than a speci\ufb01ed amount. In contrast, PRIM is a greedy algorithm\nand is not guaranteed to give an \u201coptimal\u201d set of rules. On the other hand,\nthe Apriori algorithm can deal only with dummy variables and hence could\nnot \ufb01nd some of the above rules. For example, since type of home is a\ncategorical input, with a dummy variable for each level, Apriori could not\n\ufb01nd a rule involving the set\ntype of home \u221dne}ationslash=apartment .\nTo \ufb01nd this set, we would have to code a dummy variable for apartment\nversus the other categories of type of home. It will not generally be feasible\nto precode all such potentially interesting comparisons.\n14.3 Cluster Analysis\nCluster analysis, also called data segmentation, has a variety of goals. All\nrelate to grouping or segmenting a collection of objects into subsets or\n\u201cclusters,\u201d such that those within each cluster are more closely related to\none another than objects assigned to di\ufb00erent clusters. An object can be\ndescribed by a set of measurements, or by its relation to other objects.\nIn addition, the goal is sometimes to arrange the clusters into a natural\nhierarchy. This involves successively grouping the clusters themselves so", "520": "502 14. Unsupervised Learning\n\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\nX1X2\nFIGURE 14.4. Simulated data in the plane, clustered into three classes (repr e-\nsented by orange, blue and green) by the K-means clustering algorithm\nthat at each level of the hierarchy, clusters within the same group are more\nsimilar to each other than those in di\ufb00erent groups.\nCluster analysis is also used to form descriptive statistics to ascertain\nwhether or not the data consists of a set distinct subgroups, each group\nrepresenting objects with substantially di\ufb00erent properties. This latter goa l\nrequires an assessment of the degree of di\ufb00erence between the objects as-\nsigned to the respective clusters.\nCentral to all of the goals of cluster analysis is the notion of the degree of\nsimilarity (or dissimilarity) between the individual objects being clustered.\nA clustering method attempts to group the objects based on the de\ufb01nition\nof similarity supplied to it. This can only come from subject matter consid-\nerations. The situation is somewhat similar to the speci\ufb01cation of a loss or\ncost function in prediction problems (supervised learning). There the cost\nassociated with an inaccurate prediction depends on considerations outside\nthe data.\nFigure 14.4 shows some simulated data clustered into three groups via\nthe popular K-means algorithm. In this case two of the clusters are not\nwell separated, so that \u201csegmentation\u201d more accurately describes the part\nof this process than \u201cclustering.\u201d K-means clustering starts with guesses\nfor the three cluster centers. Then it alternates the following steps until\nconvergence:\n\u2022for each data point, the closest cluster center (in Euclidean distance)\nis identi\ufb01ed;", "521": "14.3 Cluster Analysis 503\n\u2022each cluster center is replaced by the coordinate-wise average of all\ndata points that are closest to it.\nWe describe K-means clustering in more detail later, including the prob-\nlem of how to choose the number of clusters (three in this example). K-\nmeans clustering is a top-down procedure, while other cluster approaches\nthat we discuss are bottom-up . Fundamental to all clustering techniques is\nthe choice of distance or dissimilarity measure between two objects. We\n\ufb01rst discuss distance measures before describing a variety of algorithms for\nclustering.\n14.3.1 Proximity Matrices\nSometimes the data is represented directly in terms of the proximity (alike-\nness or a\ufb03nity) between pairs of objects. These can be either similarities or\ndissimilarities (di\ufb00erence or lack of a\ufb03nity). For example, in social science\nexperiments, participants are asked to judge by how much certain objects\ndi\ufb00er from one another. Dissimilarities can then be computed by averaging\nover the collection of such judgments. This type of data can be represented\nby an N\u00d7Nmatrix D, where Nis the number of objects, and each element\ndii\u2032records the proximity between the ith and i\u2032th objects. This matrix is\nthen provided as input to the clustering algorithm.\nMost algorithms presume a matrix of dissimilarities with nonnegative\nentries and zero diagonal elements: dii= 0, i= 1,2,... ,N. If the original\ndata were collected as similarities, a suitable monotone-decreasing function\ncan be used to convert them to dissimilarities. Also, most algorithms as -\nsume symmetric dissimilarity matrices, so if the original matrix Dis not\nsymmetric it must be replaced by ( D+DT)/2. Subjectively judged dissimi-\nlarities are seldom distances in the strict sense, since the triangle inequality\ndii\u2032\u2264dik+di\u2032k, for all k\u2208 {1,... ,N }does not hold. Thus, some algorithms\nthat assume distances cannot be used with such data.\n14.3.2 Dissimilarities Based on Attributes\nMost often we have measurements xijfori= 1,2,... ,N , on variables\nj= 1,2,... ,p (also called attributes ). Since most of the popular clustering\nalgorithms take a dissimilarity matrix as their input, we must \ufb01rst const ruct\npairwise dissimilarities between the observations. In the most common cas e,\nwe de\ufb01ne a dissimilarity dj(xij,xi\u2032j) between values of the jth attribute,\nand then de\ufb01ne\nD(xi,xi\u2032) =p/summationdisplay\nj=1dj(xij,xi\u2032j) (14.20)\nas the dissimilarity between objects iandi\u2032. By far the most common\nchoice is squared distance", "522": "504 14. Unsupervised Learning\ndj(xij,xi\u2032j) = (xij\u2212xi\u2032j)2. (14.21)\nHowever, other choices are possible, and can lead to potentially di\ufb00erent\nresults. For nonquantitative attributes (e.g., categorical data), squared dis-\ntance may not be appropriate. In addition, it is sometimes desirable to\nweigh attributes di\ufb00erently rather than giving them equal weight as in\n(14.20).\nWe \ufb01rst discuss alternatives in terms of the attribute type:\nQuantitative variables. Measurements of this type of variable or attribute\nare represented by continuous real-valued numbers. It is natural to\nde\ufb01ne the \u201cerror\u201d between them as a monotone-increasing function\nof their absolute di\ufb00erence\nd(xi,xi\u2032) =l(|xi\u2212xi\u2032|).\nBesides squared-error loss ( xi\u2212xi\u2032)2, a common choice is the identity\n(absolute error). The former places more emphasis on larger di\ufb00er-\nences than smaller ones. Alternatively, clustering can be based on the\ncorrelation\n\u03c1(xi,xi\u2032) =/summationtext\nj(xij\u2212\u00afxi)(xi\u2032j\u2212\u00afxi\u2032)/radical\uf8ecig/summationtext\nj(xij\u2212\u00afxi)2/summationtext\nj(xi\u2032j\u2212\u00afxi\u2032)2, (14.22)\nwith \u00afxi=/summationtext\njxij/p. Note that this is averaged over variables , not ob-\nservations. If the observations are \ufb01rst standardized, then/summationtext\nj(xij\u2212\nxi\u2032j)2\u221d2(1\u2212\u03c1(xi,xi\u2032)). Hence clustering based on correlation (simi-\nlarity) is equivalent to that based on squared distance (dissimilarity).\nOrdinal variables. The values of this type of variable are often represented\nas contiguous integers, and the realizable values are considered to be\nan ordered set. Examples are academic grades (A, B, C, D, F), degree\nof preference (can\u2019t stand, dislike, OK, like, terri\ufb01c). Rank data are a\nspecial kind of ordinal data. Error measures for ordinal variables are\ngenerally de\ufb01ned by replacing their Moriginal values with\ni\u22121/2\nM, i= 1,... ,M (14.23)\nin the prescribed order of their original values. They are then treated\nas quantitative variables on this scale.\nCategorical variables. With unordered categorical (also called nominal)\nvariables, the degree-of-di\ufb00erence between pairs of values must be\ndelineated explicitly. If the variable assumes Mdistinct values, these\ncan be arranged in a symmetric M\u00d7Mmatrix with elements Lrr\u2032=\nLr\u2032r,Lrr= 0,Lrr\u2032\u22650. The most common choice is Lrr\u2032= 1 for all\nr\u221dne}ationslash=r\u2032, while unequal losses can be used to emphasize some errors\nmore than others.", "523": "14.3 Cluster Analysis 505\n14.3.3 Object Dissimilarity\nNext we de\ufb01ne a procedure for combining the p-individual attribute dissim-\nilarities dj(xij,xi\u2032j), j= 1,2,... ,p into a single overall measure of dissim-\nilarity D(xi,xi\u2032) between two objects or observations ( xi,xi\u2032) possessing\nthe respective attribute values. This is nearly always done by means of a\nweighted average (convex combination)\nD(xi,xi\u2032) =p/summationdisplay\nj=1wj\u2264dj(xij,xi\u2032j);p/summationdisplay\nj=1wj= 1. (14.24)\nHerewjis a weight assigned to the jth attribute regulating the relative\nin\ufb02uence of that variable in determining the overall dissimilarity between\nobjects. This choice should be based on subject matter considerations.\nIt is important to realize that setting the weight wjto the same value\nfor each variable (say, wj= 1\u2200j) does notnecessarily give all attributes\nequal in\ufb02uence. The in\ufb02uence of the jth attribute Xjon object dissimilarity\nD(xi,xi\u2032) (14.24) depends upon its relative contribution to the average\nobject dissimilarity measure over all pairs of observations in the data set\n\u00afD=1\nN2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1D(xi,xi\u2032) =p/summationdisplay\nj=1wj\u2264\u00afdj,\nwith\n\u00afdj=1\nN2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1dj(xij,xi\u2032j) (14.25)\nbeing the average dissimilarity on the jth attribute. Thus, the relative in-\n\ufb02uence of the jth variable is wj\u2264\u00afdj, and setting wj\u223c1/\u00afdjwould give all\nattributes equal in\ufb02uence in characterizing overall dissimilarity between ob-\njects. For example, with pquantitative variables and squared-error distance\nused for each coordinate, then (14.24) becomes the (weighted) squared Eu-\nclidean distance\nDI(xi,xi\u2032) =p/summationdisplay\nj=1wj\u2264(xij\u2212xi\u2032j)2(14.26)\nbetween pairs of points in an IRp, with the quantitative variables as axes.\nIn this case (14.25) becomes\n\u00afdj=1\nN2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1(xij\u2212xi\u2032j)2= 2\u2264varj, (14.27)\nwhere var jis the sample estimate of Var( Xj). Thus, the relative impor-\ntance of each such variable is proportional to its variance over the data", "524": "506 14. Unsupervised Learning\n-6 -4 -2 0 2 4-6 -4 -2 0 2 4\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n-2 -1 0 1 2-2 -1 0 1 2\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\nX1 X1\nX2X2\nFIGURE 14.5. Simulated data: on the left, K-means clustering (with K=2) has\nbeen applied to the raw data. The two colors indicate the clust er memberships. On\nthe right, the features were \ufb01rst standardized before cluster ing. This is equivalent\nto using feature weights 1/[2\u2264var(Xj)]. The standardization has obscured the two\nwell-separated groups. Note that each plot uses the same unit s in the horizontal\nand vertical axes.\nset. In general, setting wj= 1/\u00afdjfor all attributes, irrespective of type,\nwill cause each one of them to equally in\ufb02uence the overall dissimilarity\nbetween pairs of objects ( xi,xi\u2032). Although this may seem reasonable, and\nis often recommended, it can be highly counterproductive. If the goal is to\nsegment the data into groups of similar objects, all attributes may not con-\ntribute equally to the (problem-dependent) notion of dissimilarity between\nobjects. Some attribute value di\ufb00erences may re\ufb02ect greater actual object\ndissimilarity in the context of the problem domain.\nIf the goal is to discover natural groupings in the data, some attributes\nmay exhibit more of a grouping tendency than others. Variables that are\nmore relevant in separating the groups should be assigned a higher in\ufb02u-\nence in de\ufb01ning object dissimilarity. Giving all attributes equal in\ufb02uence\nin this case will tend to obscure the groups to the point where a clustering\nalgorithm cannot uncover them. Figure 14.5 shows an example.\nAlthough simple generic prescriptions for choosing the individual at-\ntribute dissimilarities dj(xij,xi\u2032j) and their weights wjcan be comforting,\nthere is no substitute for careful thought in the context of each individ-\nual problem. Specifying an appropriate dissimilarity measure is far more\nimportant in obtaining success with clustering than choice of clustering\nalgorithm. This aspect of the problem is emphasized less in the cluster-\ning literature than the algorithms themselves, since it depends on domain\nknowledge speci\ufb01cs and is less amenable to general research.", "525": "14.3 Cluster Analysis 507\nFinally, often observations have missing values in one or more of the\nattributes. The most common method of incorporating missing values in\ndissimilarity calculations (14.24) is to omit each observation pair xij,xi\u2032j\nhaving at least one value missing, when computing the dissimilarity be-\ntween observations xiandx\u2032\ni. This method can fail in the circumstance\nwhen both observations have no measured values in common. In this case\nboth observations could be deleted from the analysis. Alternatively, the\nmissing values could be imputed using the mean or median of each attribute\nover the nonmissing data. For categorical variables, one could consider the\nvalue \u201cmissing\u201d as just another categorical value, if it were reasonable to\nconsider two objects as being similar if they both have missing values on\nthe same variables.\n14.3.4 Clustering Algorithms\nThe goal of cluster analysis is to partition the observations into groups\n(\u201cclusters\u201d) so that the pairwise dissimilarities between those assigned t o\nthe same cluster tend to be smaller than those in di\ufb00erent clusters. Clus-\ntering algorithms fall into three distinct types: combinatorial algorit hms,\nmixture modeling, and mode seeking.\nCombinatorial algorithms work directly on the observed data with no\ndirect reference to an underlying probability model. Mixture modeling sup-\nposes that the data is an i.i.dsample from some population described by a\nprobability density function. This density function is characterized by a pa-\nrameterized model taken to be a mixture of component density functions;\neach component density describes one of the clusters. This model is then \ufb01t\nto the data by maximum likelihood or corresponding Bayesian approaches.\nMode seekers (\u201cbump hunters\u201d) take a nonparametric perspective, attempt-\ning to directly estimate distinct modes of the probability density function.\nObservations \u201cclosest\u201d to each respective mode then de\ufb01ne the individual\nclusters.\nMixture modeling is described in Section 6.8. The PRIM algorithm, dis-\ncussed in Sections 9.3 and 14.2.5, is an example of mode seeking or \u201cbump\nhunting.\u201d We discuss combinatorial algorithms next.\n14.3.5 Combinatorial Algorithms\nThe most popular clustering algorithms directly assign each observation\nto a group or cluster without regard to a probability model describing the\ndata. Each observation is uniquely labeled by an integer i\u2208 {1,\u2264 \u2264 \u2264,N}.\nA prespeci\ufb01ed number of clusters K < N is postulated, and each one is\nlabeled by an integer k\u2208 {1,... ,K }. Each observation is assigned to one\nand only one cluster. These assignments can be characterized by a many-\nto-one mapping, or encoder k=C(i), that assigns the ith observation to\nthekth cluster. One seeks the particular encoder C\u2217(i) that achieves the", "526": "508 14. Unsupervised Learning\nrequired goal (details below), based on the dissimilarities d(xi,xi\u2032) between\nevery pair of observations. These are speci\ufb01ed by the user as described\nabove. Generally, the encoder C(i) is explicitly delineated by giving its\nvalue (cluster assignment) for each observation i. Thus, the \u201cparameters\u201d\nof the procedure are the individual cluster assignments for each of the N\nobservations. These are adjusted so as to minimize a \u201closs\u201d function that\ncharacterizes the degree to which the clustering goal is notmet.\nOne approach is to directly specify a mathematical loss function and\nattempt to minimize it through some combinatorial optimization algorit hm.\nSince the goal is to assign close points to the same cluster, a natural loss\n(or \u201cenergy\u201d) function would be\nW(C) =1\n2K/summationdisplay\nk=1/summationdisplay\nC(i)=k/summationdisplay\nC(i\u2032)=kd(xi,xi\u2032). (14.28)\nThis criterion characterizes the extent to which observations assigned to\nthe same cluster tend to be close to one another. It is sometimes referred\nto as the \u201cwithin cluster\u201d point scatter since\nT=1\n2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1dii\u2032=1\n2K/summationdisplay\nk=1/summationdisplay\nC(i)=k\uf8eb\n\uf8ed/summationdisplay\nC(i\u2032)=kdii\u2032+/summationdisplay\nC(i\u2032)/ne}ationslash=kdii\u2032\uf8f6\n\uf8f8,\nor\nT=W(C) +B(C),\nwhere dii\u2032=d(xi,xi\u2032). Here Tis thetotalpoint scatter, which is a constant\ngiven the data, independent of cluster assignment. The quantity\nB(C) =1\n2K/summationdisplay\nk=1/summationdisplay\nC(i)=k/summationdisplay\nC(i\u2032)/ne}ationslash=kdii\u2032 (14.29)\nis the between-cluster point scatter. This will tend to be large when obser-\nvations assigned to di\ufb00erent clusters are far apart. Thus one has\nW(C) =T\u2212B(C)\nand minimizing W(C) is equivalent to maximizing B(C).\nCluster analysis by combinatorial optimization is straightforward in prin-\nciple. One simply minimizes Wor equivalently maximizes Bover all pos-\nsible assignments of the Ndata points to Kclusters. Unfortunately, such\noptimization by complete enumeration is feasible only for very small data\nsets. The number of distinct assignments is (Jain and Dubes, 1988)\nS(N,K) =1\nK!K/summationdisplay\nk=1(\u22121)K\u2212k/parenleftbiggK\nk/parenrightbigg\nkN. (14.30)\nFor example, S(10,4) = 34 ,105 which is quite feasible. But, S(N,K) grows\nvery rapidly with increasing values of its arguments. Already S(19,4)\u2243", "527": "14.3 Cluster Analysis 509\n1010, and most clustering problems involve much larger data sets than\nN= 19. For this reason, practical clustering algorithms are able to examine\nonly a very small fraction of all possible encoders k=C(i). The goal is to\nidentify a small subset that is likely to contain the optimal one, or at least\na good suboptimal partition.\nSuch feasible strategies are based on iterative greedy descent. An initial\npartition is speci\ufb01ed. At each iterative step, the cluster assignments are\nchanged in such a way that the value of the criterion is improved from\nits previous value. Clustering algorithms of this type di\ufb00er in their pre-\nscriptions for modifying the cluster assignments at each iteration. When\nthe prescription is unable to provide an improvement, the algorithm ter-\nminates with the current assignments as its solution. Since the assignment\nof observations to clusters at any iteration is a perturbation of that for the\nprevious iteration, only a very small fraction of all possible assignmen ts\n(14.30) are examined. However, these algorithms converge to localoptima\nwhich may be highly suboptimal when compared to the global optimum.\n14.3.6 K-means\nTheK-means algorithm is one of the most popular iterative descent clus-\ntering methods. It is intended for situations in which all variables are of\nthe quantitative type, and squared Euclidean distance\nd(xi,xi\u2032) =p/summationdisplay\nj=1(xij\u2212xi\u2032j)2=||xi\u2212xi\u2032||2\nis chosen as the dissimilarity measure. Note that weighted Euclidean dis-\ntance can be used by rede\ufb01ning the xijvalues (Exercise 14.1).\nThe within-point scatter (14.28) can be written as\nW(C) =1\n2K/summationdisplay\nk=1/summationdisplay\nC(i)=k/summationdisplay\nC(i\u2032)=k||xi\u2212xi\u2032||2\n=K/summationdisplay\nk=1Nk/summationdisplay\nC(i)=k||xi\u2212\u00afxk||2, (14.31)\nwhere \u00af xk= (\u00afx1k,... ,\u00afxpk) is the mean vector associated with the kth clus-\nter, and Nk=/summationtextN\ni=1I(C(i) =k). Thus, the criterion is minimized by\nassigning the Nobservations to the Kclusters in such a way that within\neach cluster the average dissimilarity of the observations from the cluster\nmean, as de\ufb01ned by the points in that cluster, is minimized.\nAn iterative descent algorithm for solving", "528": "510 14. Unsupervised Learning\nAlgorithm 14.1 K-means Clustering.\n1. For a given cluster assignment C, the total cluster variance (14.33) is\nminimized with respect to {m1,... ,m K}yielding the means of the\ncurrently assigned clusters (14.32).\n2. Given a current set of means {m1,... ,m K}, (14.33) is minimized by\nassigning each observation to the closest (current) cluster mean. That\nis,\nC(i) = argmin\n1\u2264k\u2264K||xi\u2212mk||2. (14.34)\n3. Steps 1 and 2 are iterated until the assignments do not change.\nC\u2217= min\nCK/summationdisplay\nk=1Nk/summationdisplay\nC(i)=k||xi\u2212\u00afxk||2\ncan be obtained by noting that for any set of observations S\n\u00afxS= argmin\nm/summationdisplay\ni\u2208S||xi\u2212m||2. (14.32)\nHence we can obtain C\u2217by solving the enlarged optimization problem\nmin\nC,{mk}K\n1K/summationdisplay\nk=1Nk/summationdisplay\nC(i)=k||xi\u2212mk||2. (14.33)\nThis can be minimized by an alternating optimization procedure given in\nAlgorithm 14.1.\nEach of steps 1 and 2 reduces the value of the criterion (14.33), so that\nconvergence is assured. However, the result may represent a suboptimal\nlocal minimum. The algorithm of Hartigan and Wong (1979) goes further,\nand ensures that there is no single switch of an observation from one group\nto another group that will decrease the objective. In addition, one should\nstart the algorithm with many di\ufb00erent random choices for the starting\nmeans, and choose the solution having smallest value of the objective func-\ntion.\nFigure 14.6 shows some of the K-means iterations for the simulated data\nof Figure 14.4. The centroids are depicted by \u201cO\u201ds. The straight lines show\nthe partitioning of points, each sector being the set of points closest to\neach centroid. This partitioning is called the Voronoi tessellation . After 20\niterations the procedure has converged.\n14.3.7 Gaussian Mixtures as Soft K-means Clustering\nTheK-means clustering procedure is closely related to the EM algorithm\nfor estimating a certain Gaussian mixture model. (Sections 6.8 and 8.5.1).", "529": "14.3 Cluster Analysis 511\n-4 -2 0 2 4 6-2 0 2 4 6Initial Centroids\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022Initial Partition\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022Iteration Number  2\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022Iteration Number  20\n\u2022\u2022\u2022\n\u2022\u2022\u2022\nFIGURE 14.6. Successive iterations of the K-means clustering algorithm for\nthe simulated data of Figure 14.4.", "530": "512 14. Unsupervised Learning\n\u2022 \u2022Responsibilities\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2022 \u2022Responsibilities\n0.0 0.2 0.4 0.6 0.8 1.0\u03c3= 1.0 \u03c3= 1.0\n\u03c3= 0.2 \u03c3= 0.2\nFIGURE 14.7. (Left panels:) two Gaussian densities g0(x) and g1(x)(blue and\norange) on the real line, and a single data point (green dot) at x= 0.5. The colored\nsquares are plotted at x=\u22121.0andx= 1.0, the means of each density. (Right\npanels:) the relative densities g0(x)/(g0(x) +g1(x))andg1(x)/(g0(x) +g1(x)),\ncalled the \u201cresponsibilities\u201d of each cluster, for this data point. In the top panels,\nthe Gaussian standard deviation \u03c3= 1.0; in the bottom panels \u03c3= 0.2. The\nEM algorithm uses these responsibilities to make a \u201csoft\u201d ass ignment of each\ndata point to each of the two clusters. When \u03c3is fairly large, the responsibilities\ncan be near 0.5(they are 0.36and0.64 in the top right panel). As \u03c3\u21920, the\nresponsibilities \u21921, for the cluster center closest to the target point, and 0for\nall other clusters. This \u201chard\u201d assignment is seen in the botto m right panel.\nThe E-step of the EM algorithm assigns \u201cresponsibilities\u201d for each data\npoint based in its relative density under each mixture component, while\nthe M-step recomputes the component density parameters based on the\ncurrent responsibilities. Suppose we specify Kmixture components, each\nwith a Gaussian density having scalar covariance matrix \u03c32I. Then the\nrelative density under each mixture component is a monotone function of\nthe Euclidean distance between the data point and the mixture center.\nHence in this setup EM is a \u201csoft\u201d version of K-means clustering, making\nprobabilistic (rather than deterministic) assignments of points to cluster\ncenters. As the variance \u03c32\u21920, these probabilities become 0 and 1, and\nthe two methods coincide. Details are given in Exercise 14.2. Figure 14.7\nillustrates this result for two clusters on the real line.\n14.3.8 Example: Human Tumor Microarray Data\nWe apply K-means clustering to the human tumor microarray data de-\nscribed in Chapter 1. This is an example of high-dimensional clustering.", "531": "14.3 Cluster Analysis 513\nNumber of Clusters KSum of Squares\n2 4 6 8 10160000 200000 240000\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\nFIGURE 14.8. Total within-cluster sum of squares for K-means clustering ap-\nplied to the human tumor microarray data.\nTABLE 14.2. Human tumor data: number of cancer cases of each type, in each\nof the three clusters from K-means clustering.\nCluster Breast CNS Colon K562 Leukemia MCF7\n1 3 5 0 0 0 0\n2 2 0 0 2 6 2\n3 2 0 7 0 0 0\nCluster Melanoma NSCLC Ovarian Prostate Renal Unknown\n1 1 7 6 2 9 1\n2 7 2 0 0 0 0\n3 0 0 0 0 0 0\nThe data are a 6830 \u00d764 matrix of real numbers, each representing an\nexpression measurement for a gene (row) and sample (column). Here we\ncluster the samples, each of which is a vector of length 6830, correspond-\ning to expression values for the 6830 genes. Each sample has a label such\nasbreast (for breast cancer), melanoma , and so on; we don\u2019t use these la-\nbels in the clustering, but will examine posthoc which labels fall into which\nclusters.\nWe applied K-means clustering with Krunning from 1 to 10, and com-\nputed the total within-sum of squares for each clustering, shown in Fig-\nure 14.8. Typically one looks for a kink in the sum of squares curve (or its\nlogarithm) to locate the optimal number of clusters (see Section 14.3.11).\nHere there is no clear indication: for illustration we chose K= 3 giving the\nthree clusters shown in Table 14.2.", "532": "514 14. Unsupervised Learning\nFIGURE 14.9. Sir Ronald A. Fisher ( 1890\u22121962) was one of the founders\nof modern day statistics, to whom we owe maximum-likelihood, su\ufb03ciency, and\nmany other fundamental concepts. The image on the left is a 1024\u00d71024grayscale\nimage at 8bits per pixel. The center image is the result of 2\u00d72block VQ, using\n200code vectors, with a compression rate of 1.9bits/pixel. The right image uses\nonly four code vectors, with a compression rate of 0.50bits/pixel\nWe see that the procedure is successful at grouping together samples of\nthe same cancer. In fact, the two breast cancers in the second cluster were\nlater found to be misdiagnosed and were melanomas that had metastasized.\nHowever, K-means clustering has shortcomings in this application. For one,\nit does not give a linear ordering of objects within a cluster: we have simply\nlisted them in alphabetic order above. Secondly, as the number of clusters\nKis changed, the cluster memberships can change in arbitrary ways. That\nis, with say four clusters, the clusters need not be nested within the three\nclusters above. For these reasons, hierarchical clustering (described later),\nis probably preferable for this application.\n14.3.9 Vector Quantization\nTheK-means clustering algorithm represents a key tool in the apparently\nunrelated area of image and signal compression, particularly in vector quan-\ntization or VQ (Gersho and Gray, 1992). The left image in Figure 14.92is a\ndigitized photograph of a famous statistician, Sir Ronald Fisher. It consist s\nof 1024 \u00d71024 pixels, where each pixel is a grayscale value ranging from 0\nto 255, and hence requires 8 bits of storage per pixel. The entire image oc-\ncupies 1 megabyte of storage. The center image is a VQ-compressed version\nof the left panel, and requires 0 .239 of the storage (at some loss in quality).\nThe right image is compressed even more, and requires only 0 .0625 of the\nstorage (at a considerable loss in quality).\nThe version of VQ implemented here \ufb01rst breaks the image into small\nblocks, in this case 2 \u00d72 blocks of pixels. Each of the 512 \u00d7512 blocks of four\n2This example was prepared by Maya Gupta.", "533": "14.3 Cluster Analysis 515\nnumbers is regarded as a vector in IR4. AK-means clustering algorithm\n(also known as Lloyd\u2019s algorithm in this context) is run in this space.\nThe center image uses K= 200, while the right image K= 4. Each of\nthe 512 \u00d7512 pixel blocks (or points) is approximated by its closest cluster\ncentroid, known as a codeword. The clustering process is called the encoding\nstep, and the collection of centroids is called the codebook .\nTo represent the approximated image, we need to supply for each block\nthe identity of the codebook entry that approximates it. This will require\nlog2(K) bits per block. We also need to supply the codebook itself, which\nisK\u00d74 real numbers (typically negligible). Overall, the storage for the\ncompressed image amounts to log2(K)/(4\u22648) of the original (0 .239 for\nK= 200, 0 .063 for K= 4). This is typically expressed as a ratein bits\nper pixel: log2(K)/4, which are 1 .91 and 0 .50, respectively. The process\nof constructing the approximate image from the centroids is called the\ndecoding step.\nWhy do we expect VQ to work at all? The reason is that for typical\neveryday images like photographs, many of the blocks look the same. In\nthis case there are many almost pure white blocks, and similarly pure gray\nblocks of various shades. These require only one block each to represent\nthem, and then multiple pointers to that block.\nWhat we have described is known as lossycompression, since our im-\nages are degraded versions of the original. The degradation or distortion is\nusually measured in terms of mean squared error. In this case D= 0.89\nforK= 200 and D= 16.95 for K= 4. More generally a rate/distortion\ncurve would be used to assess the tradeo\ufb00. One can also perform lossless\ncompression using block clustering, and still capitalize on the repeated pat-\nterns. If you took the original image and losslessly compressed it, the bes t\nyou would do is 4.48 bits per pixel.\nWe claimed above that log2(K) bits were needed to identify each of the K\ncodewords in the codebook. This uses a \ufb01xed-length code, and is ine\ufb03cient\nif some codewords occur many more times than others in the image. Using\nShannon coding theory, we know that in general a variable length code\nwill do better, and the rate then becomes \u2212/summationtextK\n\u2113=1p\u2113log2(p\u2113)/4. The term\nin the numerator is the entropy of the distribution p\u2113of the codewords\nin the image. Using variable length coding our rates come down to 1 .42\nand 0.39, respectively. Finally, there are many generalizations of VQ that\nhave been developed: for example, tree-structured VQ \ufb01nds the centroids\nwith a top-down, 2-means style algorithm, as alluded to in Section 14.3.12.\nThis allows successive re\ufb01nement of the compression. Further details may\nbe found in Gersho and Gray (1992).\n14.3.10 K-medoids\nAs discussed above, the K-means algorithm is appropriate when the dis-\nsimilarity measure is taken to be squared Euclidean distance D(xi,xi\u2032)", "534": "516 14. Unsupervised Learning\nAlgorithm 14.2 K-medoids Clustering.\n1. For a given cluster assignment C\ufb01nd the observation in the cluster\nminimizing total distance to other points in that cluster:\ni\u2217\nk= argmin\n{i:C(i)=k}/summationdisplay\nC(i\u2032)=kD(xi,xi\u2032). (14.35)\nThen mk=xi\u2217\nk, k= 1,2,... ,K are the current estimates of the\ncluster centers.\n2. Given a current set of cluster centers {m1,... ,m K}, minimize the to-\ntal error by assigning each observation to the closest (current) cluster\ncenter:\nC(i) = argmin\n1\u2264k\u2264KD(xi,mk). (14.36)\n3. Iterate steps 1 and 2 until the assignments do not change.\n(14.112). This requires all of the variables to be of the quantitative t ype. In\naddition, using squared Euclidean distance places the highest in\ufb02uence on\nthe largest distances. This causes the procedure to lack robustness against\noutliers that produce very large distances. These restrictions can be re-\nmoved at the expense of computation.\nThe only part of the K-means algorithm that assumes squared Eu-\nclidean distance is the minimization step (14.32); the cluster representatives\n{m1,... ,m K}in (14.33) are taken to be the means of the currently assigned\nclusters. The algorithm can be generalized for use with arbitrarily de\ufb01ned\ndissimilarities D(xi,xi\u2032) by replacing this step by an explicit optimization\nwith respect to {m1,... ,m K}in (14.33). In the most common form, cen-\nters for each cluster are restricted to be one of the observations assigned\nto the cluster, as summarized in Algorithm 14.2. This algorithm assumes\nattribute data, but the approach can also be applied to data described\nonlyby proximity matrices (Section 14.3.1). There is no need to explicitly\ncompute cluster centers; rather we just keep track of the indices i\u2217\nk.\nSolving (14.32) for each provisional cluster krequires an amount of com-\nputation proportional to the number of observations assigned to it, whereas\nfor solving (14.35) the computation increases to O(N2\nk). Given a set of clus-\nter \u201ccenters,\u201d {i1,... ,i K}, obtaining the new assignments\nC(i) = argmin\n1\u2264k\u2264Kdii\u2217\nk(14.37)\nrequires computation proportional to K\u2264Nas before. Thus, K-medoids is\nfar more computationally intensive than K-means.\nAlternating between (14.35) and (14.37) represents a particular heuristic\nsearch strategy for trying to solve", "535": "14.3 Cluster Analysis 517\nTABLE 14.3. Data from a political science survey: values are average pair wise\ndissimilarities of countries from a questionnaire given to pol itical science students.\nBEL BRA CHI CUB EGY FRA IND ISR USA USS YUG\nBRA 5.58\nCHI 7.00 6.50\nCUB 7.08 7.00 3.83\nEGY 4.83 5.08 8.17 5.83\nFRA 2.17 5.75 6.67 6.92 4.92\nIND 6.42 5.00 5.58 6.00 4.67 6.42\nISR 3.42 5.50 6.42 6.42 5.00 3.92 6.17\nUSA 2.50 4.92 6.25 7.33 4.50 2.25 6.33 2.75\nUSS 6.08 6.67 4.25 2.67 6.00 6.17 6.17 6.92 6.17\nYUG 5.25 6.83 4.50 3.75 5.75 5.42 6.08 5.83 6.67 3.67\nZAI 4.75 3.00 6.08 6.67 5.00 5.58 4.83 6.17 5.67 6.50 6.92\nmin\nC,{ik}K\n1K/summationdisplay\nk=1/summationdisplay\nC(i)=kdiik. (14.38)\nKaufman and Rousseeuw (1990) propose an alternative strategy for directly\nsolving (14.38) that provisionally exchanges each center ikwith an obser-\nvation that is not currently a center, selecting the exchange that produces\nthe greatest reduction in the value of the criterion (14.38). This is repeated\nuntil no advantageous exchanges can be found. Massart et al. (1983) derive\na branch-and-bound combinatorial method that \ufb01nds the global minimum\nof (14.38) that is practical only for very small data sets.\nExample: Country Dissimilarities\nThis example, taken from Kaufman and Rousseeuw (1990), comes from a\nstudy in which political science students were asked to provide pairwise dis-\nsimilarity measures for 12 countries: Belgium, Brazil, Chile, Cuba, Egypt,\nFrance, India, Israel, United States, Union of Soviet Socialist Republics,\nYugoslavia and Zaire. The average dissimilarity scores are given in Ta -\nble 14.3. We applied 3-medoid clustering to these dissimilarities. Note that\nK-means clustering could not be applied because we have only distances\nrather than raw observations. The left panel of Figure 14.10 shows the\ndissimilarities reordered and blocked according to the 3-medoid clustering.\nThe right panel is a two-dimensional multidimensional scaling plot, with\nthe 3-medoid clusters assignments indicated by colors (multidimensional\nscaling is discussed in Section 14.8.) Both plots show three well-separated\nclusters, but the MDS display indicates that \u201cEgypt\u201d falls about halfway\nbetween two clusters.", "536": "518 14. Unsupervised Learning\nCHICUBUSSYUGBRAINDZAIBELEGYFRAISR\nCUBUSSYUGBRAINDZAIBELEGYFRAISRUSA\nReordered Dissimilarity Matrix First MDS CoordinateSecond MDS Coordinate\n-2 0 2 4-2 -1 0 1 2 3CHI\nCUB\nUSS\nYUGBRAIND ZAI\nBELEGY\nFRAISRUSA\nFIGURE 14.10. Survey of country dissimilarities. (Left panel:) dissimilari ties\nreordered and blocked according to 3-medoid clustering. Heat map is coded from\nmost similar (dark red) to least similar (bright red). (Righ t panel:) two-dimen-\nsional multidimensional scaling plot, with 3-medoid clusters indicated by di\ufb00erent\ncolors.\n14.3.11 Practical Issues\nIn order to apply K-means or K-medoids one must select the number of\nclusters K\u2217and an initialization. The latter can be de\ufb01ned by specifying\nan initial set of centers {m1,... ,m K}or{i1,... ,i K}or an initial encoder\nC(i). Usually specifying the centers is more convenient. Suggestions range\nfrom simple random selection to a deliberate strategy based on forward\nstepwise assignment. At each step a new center ikis chosen to minimize\nthe criterion (14.33) or (14.38), given the centers i1,... ,i k\u22121chosen at the\nprevious steps. This continues for Ksteps, thereby producing Kinitial\ncenters with which to begin the optimization algorithm.\nA choice for the number of clusters Kdepends on the goal. For data\nsegmentation Kis usually de\ufb01ned as part of the problem. For example,\na company may employ Ksales people, and the goal is to partition a\ncustomer database into Ksegments, one for each sales person, such that the\ncustomers assigned to each one are as similar as possible. Often, however,\ncluster analysis is used to provide a descriptive statistic for ascertaining t he\nextent to which the observations comprising the data base fall into natural\ndistinct groupings. Here the number of such groups K\u2217is unknown and\none requires that it, as well as the groupings themselves, be estimated from\nthe data.\nData-based methods for estimating K\u2217typically examine the within-\ncluster dissimilarity WKas a function of the number of clusters K. Separate\nsolutions are obtained for K\u2208 {1,2,... ,K max}. The corresponding values", "537": "14.3 Cluster Analysis 519\n{W1,W2,... ,W Kmax}generally decrease with increasing K. This will be\nthe case even when the criterion is evaluated on an independent test set,\nsince a large number of cluster centers will tend to \ufb01ll the feature space\ndensely and thus will be close to all data points. Thus cross-validation\ntechniques, so useful for model selection in supervised learning, cannot be\nutilized in this context.\nThe intuition underlying the approach is that if there are actually K\u2217\ndistinct groupings of the observations (as de\ufb01ned by the dissimilarity mea-\nsure), then for K < K\u2217the clusters returned by the algorithm will each\ncontain a subset of the true underlying groups. That is, the solution will\nnot assign observations in the same naturally occurring group to di\ufb00erent\nestimated clusters. To the extent that this is the case, the solution criterion\nvalue will tend to decrease substantially with each successive increase in the\nnumber of speci\ufb01ed clusters, WK+1\u226aWK, as the natural groups are suc-\ncessively assigned to separate clusters. For K > K\u2217, one of the estimated\nclusters must partition at least one of the natural groups into two sub-\ngroups. This will tend to provide a smaller decrease in the criterion as Kis\nfurther increased. Splitting a natural group, within which the observations\nare all quite close to each other, reduces the criterion less than partitioning\nthe union of two well-separated groups into their proper constituents.\nTo the extent this scenario is realized, there will be a sharp decrease in\nsuccessive di\ufb00erences in criterion value, WK\u2212WK+1, atK=K\u2217. That\nis,{WK\u2212WK+1|K < K\u2217} \u226b { WK\u2212WK+1|K\u2265K\u2217}. An estimate\n\u02c6K\u2217forK\u2217is then obtained by identifying a \u201ckink\u201d in the plot of WKas a\nfunction of K. As with other aspects of clustering procedures, this approach\nis somewhat heuristic.\nThe recently proposed Gap statistic (Tibshirani et al., 2001b) compares\nthe curve log WKto the curve obtained from data uniformly distributed\nover a rectangle containing the data. It estimates the optimal number of\nclusters to be the place where the gap between the two curves is largest.\nEssentially this is an automatic way of locating the aforementioned \u201cki nk.\u201d\nIt also works reasonably well when the data fall into a single cluster, and\nin that case will tend to estimate the optimal number of clusters to be one.\nThis is the scenario where most other competing methods fail.\nFigure 14.11 shows the result of the Gap statistic applied to simulated\ndata of Figure 14.4. The left panel shows log WKfork= 1,2,... ,8 clusters\n(green curve) and the expected value of log WKover 20 simulations from\nuniform data (blue curve). The right panel shows the gap curve, which is the\nexpected curve minus the observed curve. Shown also are error bars of half-\nwidth s\u2032\nK=sK/radicalbig\n1 + 1/20, where sKis the standard deviation of log WK\nover the 20 simulations. The Gap curve is maximized at K= 2 clusters. If\nG(K) is the Gap curve at Kclusters, the formal rule for estimating K\u2217is\nK\u2217= argmin\nK{K|G(K)\u2265G(K+ 1)\u2212s\u2032\nK+1}. (14.39)", "538": "520 14. Unsupervised Learning\nNumber of Clusters2 4 6 8-3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\nNumber of ClustersGap\n2 4 6 8-0.5 0.0 0.5 1.0\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022logWK\nFIGURE 14.11. (Left panel): observed (green) and expected (blue) values of\nlogWKfor the simulated data of Figure 14.4. Both curves have been t ranslated\nto equal zero at one cluster. (Right panel): Gap curve, equal to the di\ufb00erence\nbetween the observed and expected values of logWK. The Gap estimate K\u2217is the\nsmallest Kproducing a gap within one standard deviation of the gap at K+ 1;\nhereK\u2217= 2.\nThis gives K\u2217= 2, which looks reasonable from Figure 14.4.\n14.3.12 Hierarchical Clustering\nThe results of applying K-means or K-medoids clustering algorithms de-\npend on the choice for the number of clusters to be searched and a starting\ncon\ufb01guration assignment. In contrast, hierarchical clustering methods do\nnot require such speci\ufb01cations. Instead, they require the user to specify a\nmeasure of dissimilarity between (disjoint) groups of observations, based\non the pairwise dissimilarities among the observations in the two groups.\nAs the name suggests, they produce hierarchical representations in which\nthe clusters at each level of the hierarchy are created by merging clusters\nat the next lower level. At the lowest level, each cluster contains a single\nobservation. At the highest level there is only one cluster containing all of\nthe data.\nStrategies for hierarchical clustering divide into two basic paradigms: ag-\nglomerative (bottom-up) and divisive (top-down). Agglomerative strategies\nstart at the bottom and at each level recursively merge a selected pair of\nclusters into a single cluster. This produces a grouping at the next higher\nlevel with one less cluster. The pair chosen for merging consist of the two\ngroups with the smallest intergroup dissimilarity. Divisive methods s tart\nat the top and at each level recursively split one of the existing clusters at", "539": "14.3 Cluster Analysis 521\nthat level into two new clusters. The split is chosen to produce two new\ngroups with the largest between-group dissimilarity. With both paradigms\nthere are N\u22121 levels in the hierarchy.\nEach level of the hierarchy represents a particular grouping of the data\ninto disjoint clusters of observations. The entire hierarchy represents an\nordered sequence of such groupings. It is up to the user to decide which\nlevel (if any) actually represents a \u201cnatural\u201d clustering in the sense that\nobservations within each of its groups are su\ufb03ciently more similar to eac h\nother than to observations assigned to di\ufb00erent groups at that level. The\nGap statistic described earlier can be used for this purpose.\nRecursive binary splitting/agglomeration can be represented by a rooted\nbinary tree. The nodes of the trees represent groups. The root node repre-\nsents the entire data set. The Nterminal nodes each represent one of the\nindividual observations (singleton clusters). Each nonterminal node (\u201cpar-\nent\u201d) has two daughter nodes. For divisive clustering the two daughters\nrepresent the two groups resulting from the split of the parent; for agglom-\nerative clustering the daughters represent the two groups that were merged\nto form the parent.\nAll agglomerative and some divisive methods (when viewed bottom-up)\npossess a monotonicity property. That is, the dissimilarity between merged\nclusters is monotone increasing with the level of the merger. Thus the\nbinary tree can be plotted so that the height of each node is proportional\nto the value of the intergroup dissimilarity between its two daughters. The\nterminal nodes representing individual observations are all plotted at zero\nheight. This type of graphical display is called a dendrogram .\nA dendrogram provides a highly interpretable complete description of\nthe hierarchical clustering in a graphical format. This is one of the main\nreasons for the popularity of hierarchical clustering methods.\nFor the microarray data, Figure 14.12 shows the dendrogram resulting\nfrom agglomerative clustering with average linkage; agglomerative cl uster-\ning and this example are discussed in more detail later in this chapter.\nCutting the dendrogram horizontally at a particular height partitions the\ndata into disjoint clusters represented by the vertical lines that intersect\nit. These are the clusters that would be produced by terminating the pro-\ncedure when the optimal intergroup dissimilarity exceeds that threshold\ncut value. Groups that merge at high values, relative to the merger values\nof the subgroups contained within them lower in the tree, are candidates\nfor natural clusters. Note that this may occur at several di\ufb00erent levels,\nindicating a clustering hierarchy: that is, clusters nested within clusters.\nSuch a dendrogram is often viewed as a graphical summary of the data\nitself, rather than a description of the results of the algorithm. However,\nsuch interpretations should be treated with caution. First, di\ufb00erent hierar-\nchical methods (see below), as well as small changes in the data, can lead\nto quite di\ufb00erent dendrograms. Also, such a summary will be valid only to\nthe extent that the pairwise observation dissimilarities possess the hierar-", "540": "522 14. Unsupervised Learning\nCNSCNSCNSRENAL\nBREASTCNSCNS\nBREASTNSCLC\nNSCLCRENAL\nRENALRENALRENAL\nRENALRENALRENALBREAST\nNSCLCRENAL\nUNKNOWN\nOVARIAN\nMELANOMA\nPROSTATEOVARIANOVARIAN\nOVARIANOVARIAN\nOVARIAN\nPROSTATENSCLCNSCLCNSCLCLEUKEMIAK562B-reproK562A-reproLEUKEMIA\nLEUKEMIALEUKEMIALEUKEMIALEUKEMIA\nCOLONCOLON\nCOLON\nCOLONCOLONCOLON\nCOLONMCF7A-repro\nBREAST\nMCF7D-reproBREASTNSCLC\nNSCLCNSCLCMELANOMA\nBREASTBREAST\nMELANOMA\nMELANOMA\nMELANOMAMELANOMAMELANOMA\nMELANOMA\nFIGURE 14.12. Dendrogram from agglomerative hierarchical clustering with\naverage linkage to the human tumor microarray data.\nchical structure produced by the algorithm. Hierarchical methods impose\nhierarchical structure whether or not such structure actually exists in the\ndata.\nThe extent to which the hierarchical structure produced by a dendro-\ngram actually represents the data itself can be judged by the cophenetic\ncorrelation coe\ufb03cient . This is the correlation between the N(N\u22121)/2 pair-\nwise observation dissimilarities dii\u2032input to the algorithm and their corre-\nsponding cophenetic dissimilarities Cii\u2032derived from the dendrogram. The\ncophenetic dissimilarity Cii\u2032between two observations ( i,i\u2032) is the inter-\ngroup dissimilarity at which observations iandi\u2032are \ufb01rst joined together\nin the same cluster.\nThe cophenetic dissimilarity is a very restrictive dissimilarity measure.\nFirst, the Cii\u2032over the observations must contain many ties, since only N\u22121\nof the total N(N\u22121)/2 values can be distinct. Also these dissimilarities\nobey the ultrametric inequality\nCii\u2032\u2264max{Cik,Ci\u2032k} (14.40)", "541": "14.3 Cluster Analysis 523\nfor any three observations ( i,i\u2032,k). As a geometric example, suppose the\ndata were represented as points in a Euclidean coordinate system. In order\nfor the set of interpoint distances over the data to conform to (14.40), the\ntriangles formed by all triples of points must be isosceles triangles wit h the\nunequal length no longer than the length of the two equal sides (Jain and\nDubes, 1988). Therefore it is unrealistic to expect general dissimilarities\nover arbitrary data sets to closely resemble their corresponding cophenetic\ndissimilarities as calculated from a dendrogram, especially if there are not\nmany tied values. Thus the dendrogram should be viewed mainly as a de-\nscription of the clustering structure of the data as imposed by the particular\nalgorithm employed.\nAgglomerative Clustering\nAgglomerative clustering algorithms begin with every observation repre-\nsenting a singleton cluster. At each of the N\u22121 steps the closest two (least\ndissimilar) clusters are merged into a single cluster, producing one less clus-\nter at the next higher level. Therefore, a measure of dissimilarity between\ntwo clusters (groups of observations) must be de\ufb01ned.\nLetGandHrepresent two such groups. The dissimilarity d(G,H) be-\ntween GandHis computed from the set of pairwise observation dissim-\nilarities dii\u2032where one member of the pair iis inGand the other i\u2032is\ninH.Single linkage (SL) agglomerative clustering takes the intergroup\ndissimilarity to be that of the closest (least dissimilar) pair\ndSL(G,H) = min\ni\u2208G\ni\u2032\u2208Hdii\u2032. (14.41)\nThis is also often called the nearest-neighbor technique. Complete linkage\n(CL) agglomerative clustering ( furthest-neighbor technique) takes the in-\ntergroup dissimilarity to be that of the furthest (most dissimilar) pai r\ndCL(G,H) = max\ni\u2208G\ni\u2032\u2208Hdii\u2032. (14.42)\nGroup average (GA) clustering uses the average dissimilarity between the\ngroups\ndGA(G,H) =1\nNGNH/summationdisplay\ni\u2208G/summationdisplay\ni\u2032\u2208Hdii\u2032 (14.43)\nwhere NGandNHare the respective number of observations in each group.\nAlthough there have been many other proposals for de\ufb01ning intergroup\ndissimilarity in the context of agglomerative clustering, the above thr ee are\nthe ones most commonly used. Figure 14.13 shows examples of all three.\nIf the data dissimilarities {dii\u2032}exhibit a strong clustering tendency, with\neach of the clusters being compact and well separated from others, then all\nthree methods produce similar results. Clusters are compact if all of the", "542": "524 14. Unsupervised Learning\nAverage Linkage Complete Linkage Single Linkage\nFIGURE 14.13. Dendrograms from agglomerative hierarchical clustering of h u-\nman tumor microarray data.\nobservations within them are relatively close together (small dissimilar ities)\nas compared with observations in di\ufb00erent clusters. To the extent this is\nnot the case, results will di\ufb00er.\nSingle linkage (14.41) only requires that a single dissimilarity dii\u2032,i\u2208G\nandi\u2032\u2208H, be small for two groups GandHto be considered close\ntogether, irrespective of the other observation dissimilarities between the\ngroups. It will therefore have a tendency to combine, at relatively low\nthresholds, observations linked by a series of close intermediate observa-\ntions. This phenomenon, referred to as chaining , is often considered a de-\nfect of the method. The clusters produced by single linkage can violate the\n\u201ccompactness\u201d property that all observations within each cluster tend to\nbe similar to one another, based on the supplied observation dissimilari-\nties{dii\u2032}. If we de\ufb01ne the diameter DGof a group of observations as the\nlargest dissimilarity among its members\nDG= max\ni\u2208G\ni\u2032\u2208Gdii\u2032, (14.44)\nthen single linkage can produce clusters with very large diameters.\nComplete linkage (14.42) represents the opposite extreme. Two groups\nGandHare considered close only if all of the observations in their union\nare relatively similar. It will tend to produce compact clusters with small\ndiameters (14.44). However, it can produce clusters that violate the \u201cclose-\nness\u201d property. That is, observations assigned to a cluster can be much", "543": "14.3 Cluster Analysis 525\ncloser to members of other clusters than they are to some members of their\nown cluster.\nGroup average clustering (14.43) represents a compromise between the\ntwo extremes of single and complete linkage. It attempts to produce rel-\natively compact clusters that are relatively far apart. However, its results\ndepend on the numerical scale on which the observation dissimilarities dii\u2032\nare measured. Applying a monotone strictly increasing transformation h(\u2264)\nto the dii\u2032,hii\u2032=h(dii\u2032), can change the result produced by (14.43). In\ncontrast, (14.41) and (14.42) depend only on the ordering of the dii\u2032and\nare thus invariant to such monotone transformations. This invariance is\noften used as an argument in favor of single or complete linkage over group\naverage methods.\nOne can argue that group average clustering has a statistical consis-\ntency property violated by single and complete linkage. Assume we have\nattribute-value data XT= (X1,... ,X p) and that each cluster kis a ran-\ndom sample from some population joint density pk(x). The complete data\nset is a random sample from a mixture of Ksuch densities. The group\naverage dissimilarity dGA(G,H) (14.43) is an estimate of\n/integraldisplay /integraldisplay\nd(x,x\u2032)pG(x)pH(x\u2032)dx dx\u2032, (14.45)\nwhere d(x,x\u2032) is the dissimilarity between points xandx\u2032in the space\nof attribute values. As the sample size Napproaches in\ufb01nity dGA(G,H)\n(14.43) approaches (14.45), which is a characteristic of the relationshi p\nbetween the two densities pG(x) and pH(x). For single linkage, dSL(G,H)\n(14.41) approaches zero as N\u2192 \u221e independent of pG(x) and pH(x). For\ncomplete linkage, dCL(G,H) (14.42) becomes in\ufb01nite as N\u2192 \u221e, again\nindependent of the two densities. Thus, it is not clear what aspects of the\npopulation distribution are being estimated by dSL(G,H) and dCL(G,H).\nExample: Human Cancer Microarray Data (Continued)\nThe left panel of Figure 14.13 shows the dendrogram resulting from average\nlinkage agglomerative clustering of the samples (columns) of the microarra y\ndata. The middle and right panels show the result using complete and single\nlinkage. Average and complete linkage gave similar results, while single\nlinkage produced unbalanced groups with long thin clusters. We focus on\nthe average linkage clustering.\nLikeK-means clustering, hierarchical clustering is successful at clustering\nsimple cancers together. However it has other nice features. By cutting o\ufb00\nthe dendrogram at various heights, di\ufb00erent numbers of clusters emerge,\nand the sets of clusters are nested within one another. Secondly, it gives\nsome partial ordering information about the samples. In Figure 14.14, w e\nhave arranged the genes (rows) and samples (columns) of the expression\nmatrix in orderings derived from hierarchical clustering.", "544": "526 14. Unsupervised Learning\nNote that if we \ufb02ip the orientation of the branches of a dendrogram at any\nmerge, the resulting dendrogram is still consistent with the series of hierar-\nchical clustering operations. Hence to determine an ordering of the leaves,\nwe must add a constraint. To produce the row ordering of Figure 14.14,\nwe have used the default rule in S-PLUS: at each merge, the subtree with\nthe tighter cluster is placed to the left (toward the bottom in the rotated\ndendrogram in the \ufb01gure.) Individual genes are the tightest clusters possi-\nble, and merges involving two individual genes place them in order by their\nobservation number. The same rule was used for the columns. Many other\nrules are possible\u2014for example, ordering by a multidimensional scaling of\nthe genes; see Section 14.8.\nThe two-way rearrangement of Figure14.14 produces an informative pic-\nture of the genes and samples. This picture is more informative than the\nrandomly ordered rows and columns of Figure 1.3 of Chapter 1. Further-\nmore, the dendrograms themselves are useful, as biologists can, for example,\ninterpret the gene clusters in terms of biological processes.\nDivisive Clustering\nDivisive clustering algorithms begin with the entire data set as a single\ncluster, and recursively divide one of the existing clusters into two daugh-\nter clusters at each iteration in a top-down fashion. This approach has not\nbeen studied nearly as extensively as agglomerative methods in the cluster-\ning literature. It has been explored somewhat in the engineering literature\n(Gersho and Gray, 1992) in the context of compression. In the clustering\nsetting, a potential advantage of divisive over agglomerative methods can\noccur when interest is focused on partitioning the data into a relatively\nsmall number of clusters.\nThe divisive paradigm can be employed by recursively applying any of\nthe combinatorial methods such as K-means (Section 14.3.6) or K-medoids\n(Section 14.3.10), with K= 2, to perform the splits at each iteration. How-\never, such an approach would depend on the starting con\ufb01guration speci\ufb01ed\nat each step. In addition, it would not necessarily produce a splitting se-\nquence that possesses the monotonicity property required for dendrogram\nrepresentation.\nA divisive algorithm that avoids these problems was proposed by Mac-\nnaughton Smith et al. (1965). It begins by placing all observations in a\nsingle cluster G. It then chooses that observation whose average dissimi-\nlarity from all the other observations is largest. This observation for ms the\n\ufb01rst member of a second cluster H. At each successive step that observation\ninGwhose average distance from those in H, minus that for the remaining\nobservations in Gis largest, is transferred to H. This continues until the\ncorresponding di\ufb00erence in averages becomes negative. That is, there are\nno longer any observations in Gthat are, on average, closer to those in\nH. The result is a split of the original cluster into two daughter clusters,", "545": "14.3 Cluster Analysis 527\nFIGURE 14.14. DNA microarray data: average linkage hierarchical clusteri ng\nhas been applied independently to the rows (genes) and columns (sam ples), de-\ntermining the ordering of the rows and columns (see text). The color s range from\nbright green (negative, under-expressed) to bright red (posit ive, over-expressed).", "546": "528 14. Unsupervised Learning\nthe observations transferred to H, and those remaining in G. These two\nclusters represent the second level of the hierarchy. Each successive level\nis produced by applying this splitting procedure to one of the clusters at\nthe previous level. Kaufman and Rousseeuw (1990) suggest choosing the\ncluster at each level with the largest diameter (14.44) for splitting. An al -\nternative would be to choose the one with the largest average dissimilar ity\namong its members\n\u00afdG=1\nNG/summationdisplay\ni\u2208G/summationdisplay\ni\u2032\u2208Gdii\u2032.\nThe recursive splitting continues until all clusters either become singletons\nor all members of each one have zero dissimilarity from one another.\n14.4 Self-Organizing Maps\nThis method can be viewed as a constrained version of K-means clustering,\nin which the prototypes are encouraged to lie in a one- or two-dimensional\nmanifold in the feature space. The resulting manifold is also referred to\nas aconstrained topological map , since the original high-dimensional obser-\nvations can be mapped down onto the two-dimensional coordinate system.\nThe original SOM algorithm was online\u2014observations are processed one at\na time\u2014and later a batch version was proposed. The technique also bears\na close relationship to principal curves and surfaces , which are discussed in\nthe next section.\nWe consider a SOM with a two-dimensional rectangular grid of Kproto-\ntypes mj\u2208IRp(other choices, such as hexagonal grids, can also be used).\nEach of the Kprototypes are parametrized with respect to an integer\ncoordinate pair \u2113j\u2208 Q1\u00d7 Q2. Here Q1={1,2,... ,q 1}, similarly Q2, and\nK=q1\u2264q2. Themjare initialized, for example, to lie in the two-dimensional\nprincipal component plane of the data (next section). We can think of the\nprototypes as \u201cbuttons,\u201d \u201csewn\u201d on the principal component plane in a\nregular pattern. The SOM procedure tries to bend the plane so that the\nbuttons approximate the data points as well as possible. Once the model is\n\ufb01t, the observations can be mapped down onto the two-dimensional grid.\nThe observations xiare processed one at a time. We \ufb01nd the closest\nprototype mjtoxiin Euclidean distance in IRp, and then for all neighbors\nmkofmj, move mktoward xivia the update\nmk\u2190mk+\u03b1(xi\u2212mk). (14.46)\nThe \u201cneighbors\u201d of mjare de\ufb01ned to be all mksuch that the distance\nbetween \u2113jand\u2113kis small. The simplest approach uses Euclidean distance,\nand \u201csmall\u201d is determined by a threshold r. This neighborhood always\nincludes the closest prototype mjitself.", "547": "14.4 Self-Organizing Maps 529\nNotice that distance is de\ufb01ned in the space Q1\u00d7Q2of integer topological\ncoordinates of the prototypes, rather than in the feature space IRp. The\ne\ufb00ect of the update (14.46) is to move the prototypes closer to the data,\nbut also to maintain a smooth two-dimensional spatial relationship between\nthe prototypes.\nThe performance of the SOM algorithm depends on the learning rate\n\u03b1and the distance threshold r. Typically \u03b1is decreased from say 1 .0 to\n0.0 over a few thousand iterations (one per observation). Similarly ris\ndecreased linearly from starting value Rto 1 over a few thousand iterations.\nWe illustrate a method for choosing Rin the example below.\nWe have described the simplest version of the SOM. More sophisticated\nversions modify the update step according to distance:\nmk\u2190mk+\u03b1h(\u221d\u230aa\u2207\u2308\u230al\u2113j\u2212\u2113k\u221d\u230aa\u2207\u2308\u230al)(xi\u2212mk), (14.47)\nwhere the neighborhood function hgives more weight to prototypes mkwith\nindices \u2113kcloser to \u2113jthan to those further away.\nIf we take the distance rsmall enough so that each neighborhood contains\nonly one point, then the spatial connection between prototypes is lost. In\nthat case one can show that the SOM algorithm is an online version of\nK-means clustering, and eventually stabilizes at one of the local minima\nfound by K-means. Since the SOM is a constrained version of K-means\nclustering, it is important to check whether the constraint is reasonable\nin any given problem. One can do this by computing the reconstruction\nerror\u221d\u230aa\u2207\u2308\u230alx\u2212mj\u221d\u230aa\u2207\u2308\u230al2, summed over observations, for both methods. This will\nnecessarily be smaller for K-means, but should not be much smaller if the\nSOM is a reasonable approximation.\nAs an illustrative example, we generated 90 data points in three dimen-\nsions, near the surface of a half sphere of radius 1. The points were in each\nof three clusters\u2014red, green, and blue\u2014located near (0 ,1,0), (0,0,1) and\n(1,0,0). The data are shown in Figure 14.15\nBy design, the red cluster was much tighter than the green or blue ones.\n(Full details of the data generation are given in Exercise 14.5.) A 5 \u00d75 grid\nof prototypes was used, with initial grid size R= 2; this meant that about\na third of the prototypes were initially in each neighborhood. We did a\ntotal of 40 passes through the dataset of 90 observations, and let rand\u03b1\ndecrease linearly over the 3600 iterations.\nIn Figure 14.16 the prototypes are indicated by circles, and the points\nthat project to each prototype are plotted randomly within the correspond-\ning circle. The left panel shows the initial con\ufb01guration, while the right\npanel shows the \ufb01nal one. The algorithm has succeeded in separating the\nclusters; however, the separation of the red cluster indicates that the man-\nifold has folded back on itself (see Figure 14.17). Since the distances in the\ntwo-dimensional display are not used, there is little indication in the SOM\nprojection that the red cluster is tighter than the others.", "548": "530 14. Unsupervised Learning\n\u22121\u22120.500.511.5\n\u22121\u22120.500.511.5\u22121\u22120.500.511.5\nFIGURE 14.15. Simulated data in three classes, near the surface of a half\u2013\nsphere.\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022 \u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n1 2 3 4 512345\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022 \u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022 \u2022\u2022 \u2022\u2022\n\u2022 \u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\u2022\u2022\n\u2022 \u2022 \u2022\u2022\n\u2022\u2022\u2022\n1 2 3 4 512345\nFIGURE 14.16. Self-organizing map applied to half-sphere data example. Left\npanel is the initial con\ufb01guration, right panel the \ufb01nal one. The 5\u00d75grid of\nprototypes are indicated by circles, and the points that projec t to each prototype\nare plotted randomly within the corresponding circle.", "549": "14.4 Self-Organizing Maps 531\nFIGURE 14.17. Wiremesh representation of the \ufb01tted SOM model in I R3. The\nlines represent the horizontal and vertical edges of the topolog ical lattice. The\ndouble lines indicate that the surface was folded diagonally ba ck on itself in order\nto model the red points. The cluster members have been jittere d to indicate their\ncolor, and the purple points are the node centers.\nFigure 14.18 shows the reconstruction error, equal to the total sum of\nsquares of each data point around its prototype. For comparison we carried\nout aK-means clustering with 25 centroids, and indicate its reconstruction\nerror by the horizontal line on the graph. We see that the SOM signi\ufb01cantly\ndecreases the error, nearly to the level of the K-means solution. This pro-\nvides evidence that the two-dimensional constraint used by the SOM is\nreasonable for this particular dataset.\nIn the batch version of the SOM, we update each mjvia\nmj=/summationtextwkxk/summationtextwk. (14.48)\nThe sum is over points xkthat mapped (i.e., were closest to) neighbors mk\nofmj. The weight function may be rectangular, that is, equal to 1 for the\nneighbors of mk, or may decrease smoothly with distance \u221d\u230aa\u2207\u2308\u230al\u2113k\u2212\u2113j\u221d\u230aa\u2207\u2308\u230alas before.\nIf the neighborhood size is chosen small enough so that it consists only\nofmk, with rectangular weights, this reduces to the K-means clustering\nprocedure described earlier. It can also be thought of as a discrete version\nof principal curves and surfaces, described in Section 14.5.", "550": "532 14. Unsupervised Learning\nIterationReconstruction Error\n0 500 1000 1500 2000 25000 10 20 30 40 50\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\nFIGURE 14.18. Half-sphere data: reconstruction error for the SOM as a func-\ntion of iteration. Error for k-means clustering is indicated by the horizontal line.\nExample: Document Organization and Retrieval\nDocument retrieval has gained importance with the rapid development of\nthe Internet and the Web, and SOMs have proved to be useful for organiz-\ning and indexing large corpora. This example is taken from the WEBSOM\nhomepage http://websom.hut.fi/ (Kohonen et al., 2000). Figure 14.19 rep-\nresents a SOM \ufb01t to 12,088 newsgroup comp.ai.neural-nets articles. The\nlabels are generated automatically by the WEBSOM software and provide\na guide as to the typical content of a node.\nIn applications such as this, the documents have to be reprocessed in\norder to create a feature vector. A term-document matrix is created, where\neach row represents a single document. The entries in each row are the\nrelative frequency of each of a prede\ufb01ned set of terms. These terms could\nbe a large set of dictionary entries (50,000 words), or an even larger set\nof bigrams (word pairs), or subsets of these. These matrices are typically\nvery sparse, and so often some preprocessing is done to reduce the number\nof features (columns). Sometimes the SVD (next section) is used to reduce\nthe matrix; Kohonen et al. (2000) use a randomized variant thereof. These\nreduced vectors are then the input to the SOM.", "551": "14.4 Self-Organizing Maps 533\nFIGURE 14.19. Heatmap representation of the SOM model \ufb01t to a corpus\nof12,088 newsgroup comp.ai.neural-nets contributions (courtesy WEBSOM\nhomepage). The lighter areas indicate higher-density areas. Populated nodes are\nautomatically labeled according to typical content.", "552": "534 14. Unsupervised Learning\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\n\u2022v1v1v1v1v1v1v1v1\nui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1\nxixixixixixixixi\nFIGURE 14.20. The \ufb01rst linear principal component of a set of data. The line\nminimizes the total squared distance from each point to its orth ogonal projection\nonto the line.\nIn this application the authors have developed a \u201czoom\u201d feature, which\nallows one to interact with the map in order to get more detail. The \ufb01nal\nlevel of zooming retrieves the actual news articles, which can then be read.\n14.5 Principal Components, Curves and Surfaces\nPrincipal components are discussed in Sections 3.4.1, where they shed light\non the shrinkage mechanism of ridge regression. Principal components are\na sequence of projections of the data, mutually uncorrelated and ordered\nin variance. In the next section we present principal components as linear\nmanifolds approximating a set of Npoints xi\u2208IRp. We then present\nsome nonlinear generalizations in Section 14.5.2. Other recent proposals\nfor nonlinear approximating manifolds are discussed in Section 14.9.\n14.5.1 Principal Components\nThe principal components of a set of data in IRpprovide a sequence of best\nlinear approximations to that data, of all ranks q\u2264p.\nDenote the observations by x1,x2,... ,x N, and consider the rank- qlinear\nmodel for representing them", "553": "14.5 Principal Components, Curves and Surfaces 535\nf(\u03bb) =\u03b8+Vq\u03bb, (14.49)\nwhere \u03b8is a location vector in IRp,Vqis ap\u00d7qmatrix with qorthogonal\nunit vectors as columns, and \u03bbis aqvector of parameters. This is the\nparametric representation of an a\ufb03ne hyperplane of rank q. Figures 14.20\nand 14.21 illustrate for q= 1 and q= 2, respectively. Fitting such a model\nto the data by least squares amounts to minimizing the reconstruction error\nmin\n\u03b8,{\u03bbi},VqN/summationdisplay\ni=1\u221d\u230aa\u2207\u2308\u230alxi\u2212\u03b8\u2212Vq\u03bbi\u221d\u230aa\u2207\u2308\u230al2. (14.50)\nWe can partially optimize for \u03b8and the \u03bbi(Exercise 14.7) to obtain\n\u02c6\u03b8= \u00afx, (14.51)\n\u02c6\u03bbi=VT\nq(xi\u2212\u00afx). (14.52)\nThis leaves us to \ufb01nd the orthogonal matrix Vq:\nmin\nVqN/summationdisplay\ni=1||(xi\u2212\u00afx)\u2212VqVT\nq(xi\u2212\u00afx)||2. (14.53)\nFor convenience we assume that \u00af x= 0 (otherwise we simply replace the\nobservations by their centered versions \u02dc xi=xi\u2212\u00afx). The p\u00d7pmatrix\nHq=VqVT\nqis aprojection matrix , and maps each point xionto its rank-\nqreconstruction Hqxi, the orthogonal projection of xionto the subspace\nspanned by the columns of Vq. The solution can be expressed as follows.\nStack the (centered) observations into the rows of an N\u00d7pmatrix X. We\nconstruct the singular value decomposition ofX:\nX=UDVT. (14.54)\nThis is a standard decomposition in numerical analysis, and many algo-\nrithms exist for its computation (Golub and Van Loan, 1983, for example).\nHereUis anN\u00d7porthogonal matrix ( UTU=Ip) whose columns ujare\ncalled the left singular vectors ;Vis ap\u00d7porthogonal matrix ( VTV=Ip)\nwith columns vjcalled the right singular vectors , andDis ap\u00d7pdiagonal\nmatrix, with diagonal elements d1\u2265d2\u2265 \u2264\u2264\u2264 \u2265 dp\u22650 known as the sin-\ngular values . For each rank q, the solution Vqto (14.53) consists of the \ufb01rst\nqcolumns of V. The columns of UDare called the principal components\nofX(see Section 3.5.1). The Noptimal \u02c6\u03bbiin (14.52) are given by the \ufb01rst\nqprincipal components (the Nrows of the N\u00d7qmatrix UqDq).\nThe one-dimensional principal component line in IR2is illustrated in Fig-\nure 14.20. For each data point xi, there is a closest point on the line, given\nbyui1d1v1. Here v1is the direction of the line and \u02c6\u03bbi=ui1d1measures\ndistance along the line from the origin. Similarly Figure 14.21 shows the", "554": "536 14. Unsupervised Learning\nFirst principal componentSecond principal component\n\u22121.0 \u22120.5 0.0 0.5 1.0\u22121.0 \u22120.5 0.0 0.5 1.0\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\nFIGURE 14.21. The best rank-two linear approximation to the half-sphere data .\nThe right panel shows the projected points with coordinates giv en byU2D2, the\n\ufb01rst two principal components of the data.\ntwo-dimensional principal component surface \ufb01t to the half-sphere data\n(left panel). The right panel shows the projection of the data onto the\n\ufb01rst two principal components. This projection was the basis for the initial\ncon\ufb01guration for the SOM method shown earlier. The procedure is quite\nsuccessful at separating the clusters. Since the half-sphere is nonlinear, a\nnonlinear projection will do a better job, and this is the topic of the next\nsection.\nPrincipal components have many other nice properties, for example, the\nlinear combination Xv1has the highest variance among all linear com-\nbinations of the features; Xv2has the highest variance among all linear\ncombinations satisfying v2orthogonal to v1, and so on.\nExample: Handwritten Digits\nPrincipal components are a useful tool for dimension reduction and com-\npression. We illustrate this feature on the handwritten digits data described\nin Chapter 1. Figure 14.22 shows a sample of 130 handwritten 3\u2019s, each a\ndigitized 16 \u00d716 grayscale image, from a total of 658 such 3\u2019s. We see\nconsiderable variation in writing styles, character thickness and orienta-\ntion. We consider these images as points xiin IR256, and compute their\nprincipal components via the SVD (14.54).\nFigure 14.23 shows the \ufb01rst two principal components of these data. For\neach of these \ufb01rst two principal components ui1andui2, we computed the\n5%, 25%, 50%, 75% and 95% quantile points, and used them to de\ufb01ne\nthe rectangular grid superimposed on the plot. The circled points indicate", "555": "14.5 Principal Components, Curves and Surfaces 537\nFIGURE 14.22. A sample of 130handwritten 3\u2019s shows a variety of writing\nstyles.\nthose images close to the vertices of the grid, where the distance measure\nfocuses mainly on these projected coordinates, but gives some weight to the\ncomponents in the orthogonal subspace. The right plot shows the images\ncorresponding to these circled points. This allows us to visualize the nature\nof the \ufb01rst two principal components. We see that the v1(horizontal move-\nment) mainly accounts for the lengthening of the lower tail of the three,\nwhile v2(vertical movement) accounts for character thickness. In terms of\nthe parametrized model (14.49), this two-component model has the form\n\u02c6f(\u03bb) = \u00af x+\u03bb1v1+\u03bb2v2\n= +\u03bb1\u2264 +\u03bb2\u2264 . (14.55)\nHere we have displayed the \ufb01rst two principal component directions, v1\nandv2, as images. Although there are a possible 256 principal components,\napproximately 50 account for 90% of the variation in the threes, 12 ac-\ncount for 63%. Figure 14.24 compares the singular values to those obtained\nfor equivalent uncorrelated data, obtained by randomly scrambling each\ncolumn of X. The pixels in a digitized image are inherently correlated,\nand since these are all the same digit the correlations are even stronger.", "556": "538 14. Unsupervised Learning\nFirst Principal ComponentSecond Principal Component\n-6 -4 -2 0 2 4 6 8-5 0 5\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022 \u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022 \u2022\n\u2022\n\u2022\u2022\u2022\n\u2022 \u2022\u2022 \u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022 \u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022 \u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\nO O O OOO O OO\nOOO\nOO OO O O OOO O OOO\nFIGURE 14.23. (Left panel:) the \ufb01rst two principal components of the hand-\nwritten threes. The circled points are the closest projected images to the vertices\nof a grid, de\ufb01ned by the marginal quantiles of the principal compone nts. (Right\npanel:) The images corresponding to the circled points. These sh ow the nature of\nthe \ufb01rst two principal components.\nDimensionSingular Values\n0 50 100 150 200 2500 20 40 60 80\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Real Trace\n\u2022Randomized Trace\nFIGURE 14.24. The256singular values for the digitized threes, compared to\nthose for a randomized version of the data (each column of Xwas scrambled).", "557": "14.5 Principal Components, Curves and Surfaces 539\nA relatively small subset of the principal components serve as excellent\nlower-dimensional features for representing the high-dimensional data.\nExample: Procrustes Transformations and Shape Averaging\nFIGURE 14.25. (Left panel:) Two di\ufb00erent digitized handwritten Ss, each rep-\nresented by 96 corresponding points in I R2. The green S has been deliberately\nrotated and translated for visual e\ufb00ect. (Right panel:) A Procr ustes transforma-\ntion applies a translation and rotation to best match up the two set of points.\nFigure 14.25 represents two sets of points, the orange and green, in the\nsame plot. In this instance these points represent two digitized versions\nof a handwritten S, extracted from the signature of a subject \u201cSuresh.\u201d\nFigure 14.26 shows the entire signatures from which these were extracted\n(third and fourth panels). The signatures are recorded dynamically using\ntouch-screen devices, familiar sights in modern supermarkets. There are\nN= 96 points representing each S, which we denote by the N\u00d72 matrices\nX1andX2. There is a correspondence between the points\u2014the ith rows\nofX1andX2are meant to represent the same positions along the two S\u2019s.\nIn the language of morphometrics, these points represent landmarks on\nthe two objects. How one \ufb01nds such corresponding landmarks is in general\ndi\ufb03cult and subject speci\ufb01c. In this particular case we used dynamic time\nwarping of the speed signal along each signature (Hastie et al., 1992), but\nwill not go into details here.\nIn the right panel we have applied a translation and rotation to the green\npoints so as best to match the orange\u2014a so-called Procrustes3transforma-\ntion (Mardia et al., 1979, for example).\nConsider the problem\nmin\n\u03b8,R||X2\u2212(X1R+1\u03b8T)||F, (14.56)\n3Procrustes was an African bandit in Greek mythology, who str etched or squashed\nhis visitors to \ufb01t his iron bed (eventually killing them).", "558": "540 14. Unsupervised Learning\nwithX1andX2bothN\u00d7pmatrices of corresponding points, Ran or-\nthonormal p\u00d7pmatrix4, and \u03b8ap-vector of location coordinates. Here\n||X||2\nF= trace( XTX) is the squared Frobenius matrix norm.\nLet \u00afx1and \u00afx2be the column mean vectors of the matrices, and \u02dcX1and\n\u02dcX2be the versions of these matrices with the means removed. Consider\nthe SVD \u02dcXT\n1\u02dcX2=UDVT. Then the solution to (14.56) is given by (Exer-\ncise 14.8)\n\u02c6R=UVT\n\u02c6\u03b8= \u00afx2\u2212\u02c6R\u00afx1,(14.57)\nand the minimal distances is referred to as the Procrustes distance . From\nthe form of the solution, we can center each matrix at its column centroid,\nand then ignore location completely. Hereafter we assume this is the case.\nTheProcrustes distance with scaling solves a slightly more general\nproblem,\nmin\n\u03b2,R||X2\u2212\u03b2X1R||F, (14.58)\nwhere \u03b2 >0 is a positive scalar. The solution for Ris as before, with\n\u02c6\u03b2= trace( D)/||X1||2\nF.\nRelated to Procrustes distance is the Procrustes average of a collection\nofLshapes, which solves the problem\nmin\n{R\u2113}L\n1,ML/summationdisplay\n\u2113=1||X\u2113R\u2113\u2212M||2\nF; (14.59)\nthat is, \ufb01nd the shape Mclosest in average squared Procrustes distance to\nall the shapes. This is solved by a simple alternating algorithm:\n0. Initialize M=X1(for example).\n1. Solve the LProcrustes rotation problems with M\ufb01xed, yielding\nX\u2032\n\u2113\u2190X\u02c6R\u2113.\n2. Let M\u21901\nL/summationtextL\n\u2113=1X\u2032\n\u2113.\nSteps 1. and 2. are repeated until the criterion (14.59) converges.\nFigure 14.26 shows a simple example with three shapes. Note that we can\nonly expect a solution up to a rotation; alternatively, we can impose a\nconstraint, such as that Mbe upper-triangular, to force uniqueness. One\ncan easily incorporate scaling in the de\ufb01nition (14.59); see Exercise 14.9.\nMost generally we can de\ufb01ne the a\ufb03ne-invariant average of a set of\nshapes via\n4To simplify matters, we consider only orthogonal matrices w hich include re\ufb02ections\nas well as rotations [the O(p) group]; although re\ufb02ections are unlikely here, these meth ods\ncan be restricted further to allow only rotations [ SO(p) group].", "559": "14.5 Principal Components, Curves and Surfaces 541\nFIGURE 14.26. The Procrustes average of three versions of the leading S in\nSuresh\u2019s signatures. The left panel shows the preshape average, with each of the\nshapes X\u2032\n\u2113in preshape space superimposed. The right three panels map th e pre-\nshapeMseparately to match each of the original S\u2019s.\nmin\n{A\u2113}L\n1,ML/summationdisplay\n\u2113=1||X\u2113A\u2113\u2212M||2\nF, (14.60)\nwhere the A\u2113are any p\u00d7pnonsingular matrices. Here we require a stan-\ndardization, such as MTM=I, to avoid a trivial solution. The solution is\nattractive, and can be computed without iteration (Exercise 14.10):\n1. Let H\u2113=X\u2113(XT\n\u2113X\u2113)\u22121X\u2113be the rank- pprojection matrix de\ufb01ned\nbyX\u2113.\n2.Mis the N\u00d7p matrix formed from the plargest eigenvectors of \u00afH=\n1\nL/summationtextL\n\u2113=1H\u2113.\n14.5.2 Principal Curves and Surfaces\nPrincipal curves generalize the principal component line, providing a smooth\none-dimensional curved approximation to a set of data points in IRp. A prin-\ncipal surface is more general, providing a curved manifold approximation\nof dimension 2 or more.\nWe will \ufb01rst de\ufb01ne principal curves for random variables X\u2208IRp, and\nthen move to the \ufb01nite data case. Let f(\u03bb) be a parameterized smooth\ncurve in IRp. Hence f(\u03bb) is a vector function with pcoordinates, each a\nsmooth function of the single parameter \u03bb. The parameter \u03bbcan be chosen,\nfor example, to be arc-length along the curve from some \ufb01xed origin. For\neach data value x, let\u03bbf(x) de\ufb01ne the closest point on the curve to x. Then\nf(\u03bb) is called a principal curve for the distribution of the random vector\nXif\nf(\u03bb) = E( X|\u03bbf(X) =\u03bb). (14.61)\nThis says f(\u03bb) is the average of all data points that project to it, that is, the\npoints for which it is \u201cresponsible.\u201d This is also known as a self-consistency\nproperty. Although in practice, continuous multivariate distributes have\nin\ufb01nitely many principal curves (Duchamp and Stuetzle, 1996), we are", "560": "542 14. Unsupervised Learning\n....\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022 \u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022.....\nf(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)] f(\u03bb) = [f1(\u03bb), f2(\u03bb)]\nFIGURE 14.27. The principal curve of a set of data. Each point on the curve\nis the average of all data points that project there.\ninterested mainly in the smooth ones. A principal curve is illustrated in\nFigure 14.27.\nPrincipal points are an interesting related concept. Consider a set of k\nprototypes and for each point xin the support of a distribution, identify\nthe closest prototype, that is, the prototype that is responsible for it. T his\ninduces a partition of the feature space into so-called Voronoi regions. The\nset of kpoints that minimize the expected distance from Xto its prototype\nare called the principal points of the distribution. Each principal point is\nself-consistent, in that it equals the mean of Xin its Voronoi region. For\nexample, with k= 1, the principal point of a circular normal distribution is\nthe mean vector; with k= 2 they are a pair of points symmetrically placed\non a ray through the mean vector. Principal points are the distributional\nanalogs of centroids found by K-means clustering. Principal curves can be\nviewed as k=\u221eprincipal points, but constrained to lie on a smooth curve,\nin a similar way that a SOM constrains K-means cluster centers to fall on\na smooth manifold.\nTo \ufb01nd a principal curve f(\u03bb) of a distribution, we consider its coordinate\nfunctions f(\u03bb) = [f1(\u03bb),f2(\u03bb),... ,f p(\u03bb)] and let XT= (X1,X2,... ,X p).\nConsider the following alternating steps:\n(a) \u02c6fj(\u03bb)\u2190E(Xj|\u03bb(X) =\u03bb);j= 1,2,... ,p,\n(b) \u02c6\u03bbf(x)\u2190argmin\u03bb\u2032||x\u2212\u02c6f(\u03bb\u2032)||2.(14.62)\nThe \ufb01rst equation \ufb01xes \u03bband enforces the self-consistency requirement\n(14.61). The second equation \ufb01xes the curve and \ufb01nds the closest point on", "561": "14.5 Principal Components, Curves and Surfaces 543\n-0.1 0.0 0.1 0.2-0.2 -0.1 0.0 0.1 0.2\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022 \u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022 \u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u03bb1\u03bb2\nFIGURE 14.28. Principal surface \ufb01t to half-sphere data. (Left panel:) \ufb01tted\ntwo-dimensional surface. (Right panel:) projections of data po ints onto the sur-\nface, resulting in coordinates \u02c6\u03bb1,\u02c6\u03bb2.\nthe curve to each data point. With \ufb01nite data, the principal curve algorithm\nstarts with the linear principal component, and iterates the two steps in\n(14.62) until convergence. A scatterplot smoother is used to estimate the\nconditional expectations in step (a) by smoothing each Xjas a function of\nthe arc-length \u02c6\u03bb(X), and the projection in (b) is done for each of the ob-\nserved data points. Proving convergence in general is di\ufb03cult, but one can\nshow that if a linear least squares \ufb01t is used for the scatterplot smoothing,\nthen the procedure converges to the \ufb01rst linear principal component, and\nis equivalent to the power method for \ufb01nding the largest eigenvector of a\nmatrix.\nPrincipal surfaces have exactly the same form as principal curves, but\nare of higher dimension. The mostly commonly used is the two-dimensional\nprincipal surface, with coordinate functions\nf(\u03bb1,\u03bb2) = [f1(\u03bb1,\u03bb2),... ,f p(\u03bb1,\u03bb2)].\nThe estimates in step (a) above are obtained from two-dimensional surface\nsmoothers. Principal surfaces of dimension greater than two are rarely used,\nsince the visualization aspect is less attractive, as is smoothing in high\ndimensions.\nFigure 14.28 shows the result of a principal surface \ufb01t to the half-sphere\ndata. Plotted are the data points as a function of the estimated nonlinear\ncoordinates \u02c6\u03bb1(xi),\u02c6\u03bb2(xi). The class separation is evident.\nPrincipal surfaces are very similar to self-organizing maps. If we use a\nkernel surface smoother to estimate each coordinate function fj(\u03bb1,\u03bb2),\nthis has the same form as the batch version of SOMs (14.48). The SOM\nweights wkare just the weights in the kernel. There is a di\ufb00erence, however:", "562": "544 14. Unsupervised Learning\nthe principal surface estimates a separate prototype f(\u03bb1(xi),\u03bb2(xi)) for\neach data point xi, while the SOM shares a smaller number of prototypes\nfor all data points. As a result, the SOM and principal surface will agree\nonly as the number of SOM prototypes grows very large.\nThere also is a conceptual di\ufb00erence between the two. Principal sur-\nfaces provide a smooth parameterization of the entire manifold in terms\nof its coordinate functions, while SOMs are discrete and produce only the\nestimated prototypes for approximating the data. The smooth parameter-\nization in principal surfaces preserves distance locally: in Figure 14.28 it\nreveals that the red cluster is tighter than the green or blue clusters. In\nsimple examples the estimates coordinate functions themselves can be in-\nformative: see Exercise14.13.\n14.5.3 Spectral Clustering\nTraditional clustering methods like K-means use a spherical or elliptical\nmetric to group data points. Hence they will not work well when the clus-\nters are non-convex, such as the concentric circles in the top left panel of\nFigure 14.29. Spectral clustering is a generalization of standard clustering\nmethods, and is designed for these situations. It has close connections with\nthe local multidimensional-scaling techniques (Section 14.9) that generalize\nMDS.\nThe starting point is a N\u00d7Nmatrix of pairwise similarities sii\u2032\u22650 be-\ntween all observation pairs. We represent the observations in an undirected\nsimilarity graph G=\u221dan}\u230a\u2207a\u230bketle{tV, E\u221dan}\u230a\u2207a\u230bket\u2207i}ht. The Nvertices virepresent the observations,\nand pairs of vertices are connected by an edge if their similarity is positive\n(or exceeds some threshold). The edges are weighted by the sii\u2032. Clustering\nis now rephrased as a graph-partition problem, where we identify connected\ncomponents with clusters. We wish to partition the graph, such that edges\nbetween di\ufb00erent groups have low weight, and within a group have high\nweight. The idea in spectral clustering is to construct similarity graphs that\nrepresent the local neighborhood relationships between observations.\nTo make things more concrete, consider a set of Npoints xi\u2208IRp, and let\ndii\u2032be the Euclidean distance between xiandxi\u2032. We will use as similarity\nmatrix the radial-kernel gram matrix; that is, sii\u2032= exp( \u2212d2\nii\u2032/c), where\nc >0 is a scale parameter.\nThere are many ways to de\ufb01ne a similarity matrix and its associated\nsimilarity graph that re\ufb02ect local behavior. The most popular is the mutual\nK-nearest-neighbor graph . De\ufb01ne NKto be the symmetric set of nearby\npairs of points; speci\ufb01cally a pair ( i,i\u2032) is in NKif point iis among the\nK-nearest neighbors of i\u2032, or vice-versa. Then we connect all symmetric\nnearest neighbors, and give them edge weight wii\u2032=sii\u2032; otherwise the\nedge weight is zero. Equivalently we set to zero all the pairwise similarit ies\nnot in NK, and draw the graph for this modi\ufb01ed similarity matrix.", "563": "14.5 Principal Components, Curves and Surfaces 545\nAlternatively, a fully connected graph includes all pairwise edges with\nweights wii\u2032=sii\u2032, and the local behavior is controlled by the scale param-\neterc.\nThe matrix of edge weights W={wii\u2032}from a similarity graph is called\ntheadjacency matrix . The degree of vertex iisgi=/summationtext\ni\u2032wii\u2032, the sum of\nthe weights of the edges connected to it. Let Gbe a diagonal matrix with\ndiagonal elements gi.\nFinally, the graph Laplacian is de\ufb01ned by\nL=G\u2212W (14.63)\nThis is called the unnormalized graph Laplacian ; a number of normalized\nversions have been proposed\u2014these standardize the Laplacian with respect\nto the node degrees gi, for example, \u02dcL=I\u2212G\u22121W.\nSpectral clustering \ufb01nds the meigenvectors ZN\u00d7mcorresponding to the\nmsmallest eigenvalues of L(ignoring the trivial constant eigenvector).\nUsing a standard method like K-means, we then cluster the rows of Zto\nyield a clustering of the original data points.\nAn example is presented in Figure 14.29. The top left panel shows 450\nsimulated data points in three circular clusters indicated by the colors. K-\nmeans clustering would clearly have di\ufb03culty identifying the outer clusters.\nWe applied spectral clustering using a 10-nearest neighbor similarity graph,\nand display the eigenvector corresponding to the second and third smallest\neigenvalue of the graph Laplacian in the lower left. The 15 smallest eigen-\nvalues are shown in the top right panel. The two eigenvectors shown have\nidenti\ufb01ed the three clusters, and a scatterplot of the rows of the eigenvector\nmatrix Yin the bottom right clearly separates the clusters. A procedure\nsuch as K-means clustering applied to these transformed points would eas-\nily identify the three groups.\nWhy does spectral clustering work? For any vector fwe have\nfTLf=N/summationdisplay\ni=1gif2\ni\u2212N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1fifi\u2032wii\u2032\n=1\n2N/summationdisplay\ni=1N/summationdisplay\ni\u2032=1wii\u2032(fi\u2212fi\u2032)2. (14.64)\nFormula 14.64 suggests that a small value of fTLfwill be achieved if pairs\nof points with large adjacencies have coordinates fiandfi\u2032close together.\nSince1TL1= 0 for any graph, the constant vector is a trivial eigenvector\nwith eigenvalue zero. Not so obvious is the fact that if the graph is con-\nnected5, it is the onlyzero eigenvector (Exercise 14.21). Generalizing this\nargument, it is easy to show that for a graph with mconnected components,\n5A graph is connected if any two nodes can be reached via a path o f connected nodes.", "564": "546 14. Unsupervised Learning\n\u22124 \u22122 0 2 4\u22124 \u22122 0 2 4\nx1x2\n0.0 0.1 0.2 0.3 0.4 0.5\nNumberEigenvalue\n1 3 5 10 15\n0 100 200 300 400Eigenvectors\nIndex2nd Smallest 3rd Smallest\n\u22120.05  0.05 \u22120.05  0.05\n\u22120.04 \u22120.02 0.00 0.02\u22120.06 \u22120.02 0.02 0.06\nSecond Smallest EigenvectorThird Smallest EigenvectorSpectral Clustering\nFIGURE 14.29. Toy example illustrating spectral clustering. Data in top left are\n450points falling in three concentric clusters of 150points each. The points are\nuniformly distributed in angle, with radius 1,2.8and5in the three groups, and\nGaussian noise with standard deviation 0.25added to each point. Using a k= 10\nnearest-neighbor similarity graph, the eigenvector correspo nding to the second and\nthird smallest eigenvalues of Lare shown in the bottom left; the smallest eigen-\nvector is constant. The data points are colored in the same way as in the top left.\nThe 15 smallest eigenvalues are shown in the top right panel. Th e coordinates of\nthe 2nd and 3rd eigenvectors (the 450rows of Z) are plotted in the bottom right\npanel. Spectral clustering does standard (e.g., K-means) clustering of these points\nand will easily recover the three original clusters.", "565": "14.5 Principal Components, Curves and Surfaces 547\nthe nodes can be reordered so that Lis block diagonal with a block for each\nconnected component. Then Lhasmeigenvectors of eigenvalue zero, and\nthe eigenspace of eigenvalue zero is spanned by the indicator vectors of the\nconnected components. In practice one has strong and weak connections,\nso zero eigenvalues are approximated by small eigenvalues.\nSpectral clustering is an interesting approach for \ufb01nding non-convex clus-\nters. When a normalized graph Laplacian is used, there is another way to\nview this method. De\ufb01ning P=G\u22121W, we consider a random walk on\nthe graph with transition probability matrix P. Then spectral clustering\nyields groups of nodes such that the random walk seldom transitions from\none group to another.\nThere are a number of issues that one must deal with in applying spec-\ntral clustering in practice. We must choose the type of similarity graph\u2014eg.\nfully connected or nearest neighbors, and associated parameters such as the\nnumber of nearest of neighbors kor the scale parameter of the kernel c. We\nmust also choose the number of eigenvectors to extract from Land \ufb01nally,\nas with all clustering methods, the number of clusters. In the toy example\nof Figure 14.29 we obtained good results for k\u2208[5,200], the value 200 cor-\nresponding to a fully connected graph. With k <5 the results deteriorated.\nLooking at the top-right panel of Figure 14.29, we see no strong separation\nbetween the smallest three eigenvalues and the rest. Hence it is not clear\nhow many eigenvectors to select.\n14.5.4 Kernel Principal Components\nSpectral clustering is related to kernel principal components , a non-linear\nversion of linear principal components. Standard linear principal compo-\nnents (PCA) are obtained from the eigenvectors of the covariance matrix,\nand give directions in which the data have maximal variance. Kernel PCA\n(Sch\u00a8 olkopf et al., 1999) expand the scope of PCA, mimicking what we would\nobtain if we were to expand the features by non-linear transformations, and\nthen apply PCA in this transformed feature space.\nWe show in Section 18.5.2 that the principal components variables Zof\na data matrix Xcan be computed from the inner-product (gram) matrix\nK=XXT. In detail, we compute the eigen-decomposition of the double-\ncentered version of the gram matrix\n/tildewideK= (I\u2212M)K(I\u2212M) =UD2UT, (14.65)\nwithM=11T/N, and then Z=UD. Exercise 18.15 shows how to com-\npute the projections of new observations in this space.\nKernel PCA simply mimics this procedure, interpreting the kernel ma-\ntrixK={K(xi,xi\u2032)}as an inner-product matrix of the implicit fea-\ntures\u221dan}\u230a\u2207a\u230bketle{t\u03c6(xi),\u03c6(xi\u2032)\u221dan}\u230a\u2207a\u230bket\u2207i}htand \ufb01nding its eigenvectors. The elements of the mth\ncomponent zm(mth column of Z) can be written (up to centering) as\nzim=/summationtextN\nj=1\u03b1jmK(xi,xj), where \u03b1jm=ujm/dm(Exercise 14.16).", "566": "548 14. Unsupervised Learning\nWe can gain more insight into kernel PCA by viewing the zmas sam-\nple evaluations of principal component functions gm\u2208 H K, with HKthe\nreproducing kernel Hilbert space generated by K(see Section 5.8.1). The\n\ufb01rst principal component function g1solves\nmax\ng1\u2208HKVarTg1(X) subject to ||g1||HK= 1 (14.66)\nHere Var Trefers to the sample variance over training data T. The norm\nconstraint ||g1||HK= 1 controls the size and roughness of the function g1,\nas dictated by the kernel K. As in the regression case it can be shown that\nthe solution to (14.66) is \ufb01nite dimensional with representation g1(x) =/summationtextN\nj=1cjK(x,xj). Exercise 14.17 shows that the solution is de\ufb01ned by \u02c6 cj=\n\u03b1j1, j= 1,... ,N above. The second principal component function is de-\n\ufb01ned in a similar way, with the additional constraint that \u221dan}\u230a\u2207a\u230bketle{tg1,g2\u221dan}\u230a\u2207a\u230bket\u2207i}htHK= 0,\nand so on.6\nSch\u00a8 olkopf et al. (1999) demonstrate the use of kernel principal compo-\nnents as features for handwritten-digit classi\ufb01cation, and show that they\ncan improve the performance of a classi\ufb01er when these are used instead of\nlinear principal components.\nNote that if we use the radial kernel\nK(x,x\u2032) = exp( \u2212\u221d\u230aa\u2207\u2308\u230alx\u2212x\u2032\u221d\u230aa\u2207\u2308\u230al2/c), (14.67)\nthen the kernel matrix Khas the same form as the similarity matrix Sin\nspectral clustering. The matrix of edge weights Wis a localized version of\nK, setting to zero all similarities for pairs of points that are not nearest\nneighbors.\nKernel PCA \ufb01nds the eigenvectors corresponding to the largest eigenval-\nues of /tildewideK; this is equivalent to \ufb01nding the eigenvectors corresponding to the\nsmallest eigenvalues of\nI\u2212/tildewideK. (14.68)\nThis is almost the same as the Laplacian (14.63), the di\ufb00erences being the\ncentering of /tildewideKand the fact that Ghas the degrees of the nodes along the\ndiagonal.\nFigure 14.30 examines the performance of kernel principal components\nin the toy example of Figure 14.29. In the upper left panel we used the ra-\ndial kernel with c= 2, the same value that was used in spectral clustering.\nThis does not separate the groups, but with c= 10 (upper right panel), the\n\ufb01rst component separates the groups well. In the lower-left panel we ap-\nplied kernel PCA using the nearest-neighbor radial kernel Wfrom spectral\nclustering. In the lower right panel we use the kernel matrix itself as the\n6This section bene\ufb01ted from helpful discussions with Jonath an Taylor.", "567": "14.5 Principal Components, Curves and Surfaces 549\n\u22120.10 \u22120.06 \u22120.02 0.02\u22120.10 \u22120.05 0.00 0.05 0.10\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=2)\n\u22120.06 \u22120.02 0.02 0.06\u22120.05 0.00 0.05\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=10)\n0.00 0.05 0.10 0.15\u22120.2 \u22120.1 0.0 0.1 0.2\nFirst Largest EigenvectorSecond Largest EigenvectorNN Radial Kernel (c=2)\n\u22120.05 0.00 0.05 0.10 0.15\u22120.10 0.00 0.05 0.10 0.15\nSecond Smallest EigenvectorThird Smallest EigenvectorRadial Kernel Laplacian (c=2)\nFIGURE 14.30. Kernel principal components applied to the toy example of Fig-\nure 14.29, using di\ufb00erent kernels. (Top left:) Radial kernel (14 .67) with c= 2.\n(Top right:) Radial kernel with c= 10. (Bottom left): Nearest neighbor radial ker-\nnelWfrom spectral clustering. (Bottom right:) Spectral clusteri ng with Laplacian\nconstructed from the radial kernel.", "568": "550 14. Unsupervised Learning\nsimilarity matrix for constructing the Laplacian (14.63) in spectral cl uster-\ning. In neither case do the projections separate the two groups. Adjusting\ncdid not help either.\nIn this toy example, we see that kernel PCA is quite sensitive to the scale\nand nature of the kernel. We also see that the nearest-neighbor truncation\nof the kernel is important for the success of spectral clustering.\n14.5.5 Sparse Principal Components\nWe often interpret principal components by examining the direction vectors\nvj, also known as loadings , to see which variables play a role. We did this\nwith the image loadings in (14.55). Often this interpretation is made eas ier\nif the loadings are sparse. In this section we brie\ufb02y discuss some methods\nfor deriving principal components with sparse loadings. They are all based\non lasso ( L1) penalties.\nWe start with an N\u00d7pdata matrix X, with centered columns. The\nproposed methods focus on either the maximum-variance property of prin-\ncipal components, or the minimum reconstruction error. The SCoTLASS\nprocedure of Joli\ufb00e et al. (2003) takes the \ufb01rst approach, by solving\nmaxvT(XTX)v,subject to/summationtextp\nj=1|vj| \u2264t,vTv= 1. (14.69)\nThe absolute-value constraint encourages some of the loadings to be zero\nand hence vto be sparse. Further sparse principal components are found\nin the same way, by forcing the kth component to be orthogonal to the\n\ufb01rstk\u22121 components. Unfortunately this problem is not convex and the\ncomputations are di\ufb03cult.\nZou et al. (2006) start instead with the regression/reconstruction prop-\nerty of PCA, similar to the approach in Section 14.5.1. Let xibe the ith row\nofX. For a single component, their sparse principal component technique\nsolves\nmin\n\u03b8,vN/summationdisplay\ni=1||xi\u2212\u03b8vTxi||2\n2+\u03bb||v||2\n2+\u03bb1||v||1 (14.70)\nsubject to ||\u03b8||2= 1.\nLets examine this formulation in more detail.\n\u2022If both \u03bband\u03bb1are zero and N > p , it is easy to show that v=\u03b8\nand is the largest principal component direction.\n\u2022When p\u226bNthe solution is not necessarily unique unless \u03bb >0. For\nany\u03bb >0 and \u03bb1= 0 the solution for vis proportional to the largest\nprincipal component direction.\n\u2022The second penalty on vencourages sparseness of the loadings.", "569": "14.5 Principal Components, Curves and Surfaces 551\nWalking Speed\nVerbal Fluency\nPrincipal Components Sparse Principal Components\nFIGURE 14.31. Standard and sparse principal components from a study of\nthe corpus callosum variation. The shape variations correspo nding to signi\ufb01cant\nprincipal components (red curves) are overlaid on the mean CC sh ape (black\ncurves).\nFor multiple components, the sparse principal components procedures\nminimizes\nN/summationdisplay\ni=1||xi\u2212\u0398VTxi||2+\u03bbK/summationdisplay\nk=1||vk||2\n2+K/summationdisplay\nk=1\u03bb1k||vk||1, (14.71)\nsubject to \u0398T\u0398=IK. Here Vis ap\u00d7Kmatrix with columns vkand\u0398\nis also p\u00d7K.\nCriterion (14.71) is not jointly convex in Vand\u0398, but it is convex in\neach parameter with the other parameter \ufb01xed7. Minimization over Vwith\n\u0398\ufb01xed is equivalent to Kelastic net problems (Section 18.4) and can be\ndone e\ufb03ciently. On the other hand, minimization over \u0398withV\ufb01xed is a\nversion of the Procrustes problem (14.56), and is solved by a simple SVD\ncalculation (Exercise 14.12). These steps are alternated until convergence.\nFigure 14.31 shows an example of sparse principal components analysis\nusing (14.71), taken from Sj\u00a8 ostrand et al. (2007). Here the shape of the\nmid-sagittal cross-section of the corpus callosum (CC) is related to various\nclinical parameters in a study involving 569 elderly persons8. In this exam-\n7Note that the usual principal component criterion, for exam ple (14.50), is not jointly\nconvex in the parameters either. Nevertheless, the solutio n is well de\ufb01ned and an e\ufb03cient\nalgorithm is available.\n8We thank Rasmus Larsen and Karl Sj\u00a8 ostrand for suggesting th is application, and\nsupplying us with the postscript \ufb01gures reproduced here.", "570": "552 14. Unsupervised Learning\nFIGURE 14.32. An example of a mid-saggital brain slice, with the corpus col-\nlosum annotated with landmarks.\nple PCA is applied to shape data, and is a popular tool in morphometrics.\nFor such applications, a number of landmarks are identi\ufb01ed along the cir-\ncumference of the shape; an example is given in Figure 14.32. These are\naligned by Procrustes analysis to allow for rotations, and in this case s cal-\ning as well (see Section 14.5.1). The features used for PCA are the sequence\nof coordinate pairs for each landmark, unpacked into a single vector.\nIn this analysis, both standard and sparse principal components were\ncomputed, and components that were signi\ufb01cantly associated with various\nclinical parameters were identi\ufb01ed. In the \ufb01gure, the shape variations cor-\nresponding to signi\ufb01cant principal components (red curves) are overlaid on\nthe mean CC shape (black curves). Low walking speed relates to CCs that\nare thinner (displaying atrophy) in regions connecting the motor control\nand cognitive centers of the brain. Low verbal \ufb02uency relates to CCs that\nare thinner in regions connecting auditory/visual/cognitive centers. The\nsparse principal components procedure gives a more parsimonious, and po-\ntentially more informative picture of the important di\ufb00erences.", "571": "14.6 Non-negative Matrix Factorization 553\n14.6 Non-negative Matrix Factorization\nNon-negative matrix factorization (Lee and Seung, 1999) is a recent al-\nternative approach to principal components analysis, in which the data\nand components are assumed to be non-negative. It is useful for modeling\nnon-negative data such as images.\nTheN\u00d7pdata matrix Xis approximated by\nX\u2248WH (14.72)\nwhere WisN\u00d7randHisr\u00d7p,r\u2264max(N,p). We assume that\nxij,wik,hkj\u22650.\nThe matrices WandHare found by maximizing\nL(W,H) =N/summationdisplay\ni=1p/summationdisplay\nj=1[xijlog(WH)ij\u2212(WH)ij]. (14.73)\nThis is the log-likelihood from a model in which xijhas a Poisson dis-\ntribution with mean ( WH)ij\u2014quite reasonable for positive data.\nThe following alternating algorithm (Lee and Seung, 2001) converges to\na local maximum of L(W,H):\nwik\u2190wik/summationtextp\nj=1hkjxij/(WH)ij/summationtextp\nj=1hkj\nhkj\u2190hkj/summationtextN\ni=1wikxij/(WH)ij/summationtextN\ni=1wik(14.74)\nThis algorithm can be derived as a minorization procedure for maximizing\nL(W,H) (Exercise 14.23) and is also related to the iterative-proportional-\nscaling algorithm for log-linear models (Exercise 14.24).\nFigure 14.33 shows an example taken from Lee and Seung (1999)9, com-\nparing non-negative matrix factorization (NMF), vector quantization (VQ,\nequivalent to k-means clustering) and principal components analysis (PCA).\nThe three learning methods were applied to a database of N= 2,429 fa-\ncial images, each consisting of 19 \u00d719 pixels, resulting in a 2 ,429\u00d7381\nmatrix X. As shown in the 7 \u00d77 array of montages (each a 19 \u00d719 image),\neach method has learned a set of r= 49 basis images. Positive values are\nillustrated with black pixels and negative values with red pixels. A par-\nticular instance of a face, shown at top right, is approximated by a linear\nsuperposition of basis images. The coe\ufb03cients of the linear superposition\nare shown next to each montage, in a 7 \u00d77 array10, and the resulting su-\nperpositions are shown to the right of the equality sign. The authors poin t\n9We thank Sebastian Seung for providing this image.\n10These 7 \u00d77 arrangements allow for a compact display, and have no struc tural\nsigni\ufb01cance.", "572": "554 14. Unsupervised Learning\nout that unlike VQ and PCA, NMF learns to represent faces with a set of\nbasis images resembling parts of faces.\nDonoho and Stodden (2004) point out a potentially serious problem with\nnon-negative matrix factorization. Even in situations where X=WHholds\nexactly, the decomposition may not be unique. Figure 14.34 illustrates the\nproblem. The data points lie in p= 2 dimensions, and there is \u201copen space\u201d\nbetween the data and the coordinate axes. We can choose the basis vectors\nh1andh2anywhere in this open space, and represent each data point\nexactly with a nonnegative linear combination of these vectors. This non-\nuniqueness means that the solution found by the above algorithm depends\non the starting values, and it would seem to hamper the interpretability of\nthe factorization. Despite this interpretational drawback, the non-negative\nmatrix factorization and its applications has attracted a lot of interest .\n14.6.1 Archetypal Analysis\nThis method, due to Cutler and Breiman (1994), approximates data points\nby prototypes that are themselves linear combinations of data points. In\nthis sense it has a similar \ufb02avor to K-means clustering. However, rather\nthan approximating each data point by a single nearby prototype, archety-\npal analysis approximates each data point by a convex combination of a\ncollection of prototypes. The use of a convex combination forces the proto-\ntypes to lie on the convex hull of the data cloud. In this sense, the prototypes\nare \u201cpure,\u201d, or \u201carchetypal.\u201d\nAs in (14.72), the N\u00d7pdata matrix Xis modeled as\nX\u2248WH (14.75)\nwhereWisN\u00d7randHisr\u00d7p. We assume that wik\u22650 and/summationtextr\nk=1wik=\n1\u2200i. Hence the Ndata points (rows of X) inp-dimensional space are\nrepresented by convex combinations of the rarchetypes (rows of H). We\nalso assume that\nH=BX (14.76)\nwhere Bisr\u00d7Nwithbki\u22650 and/summationtextN\ni=1bki= 1\u2200k. Thus the archetypes\nthemselves are convex combinations of the data points. Using both (14.75)\nand (14.76) we minimize\nJ(W,B) = ||X\u2212WH||2\n=||X\u2212WBX ||2(14.77)\nover the weights WandB. This function is minimized in an alternating\nfashion, with each separate minimization involving a convex optimizatio n.\nThe overall problem is not convex however, and so the algorithm converges\nto a local minimum of the criterion.", "573": "14.6 Non-negative Matrix Factorization 555\nVQ\n\u00d7\r =NMF\n= \u00d7\r\nPCA\n= \u00d7\r\nOriginal\nFIGURE 14.33. Non-negative matrix factorization (NMF), vector quantizatio n\n(VQ, equivalent to k-means clustering) and principal components analysis (PCA)\napplied to a database of facial images. Details are given in t he text. Unlike VQ\nand PCA, NMF learns to represent faces with a set of basis images r esembling\nparts of faces.", "574": "556 14. Unsupervised Learning\nh1\nh2\nFIGURE 14.34. Non-uniqueness of the non-negative matrix factorization.\nThere are 11 data points in two dimensions. Any choice of the basis vectors h1\nandh2in the open space between the coordinate axes and data, gives an e xact\nreconstruction of the data.\nFigure 14.35 shows an example with simulated data in two dimensions.\nThe top panel displays the results of archetypal analysis, while the bottom\npanel shows the results from K-means clustering. In order to best recon-\nstruct the data from convex combinations of the prototypes, it pays to\nlocate the prototypes on the convex hull of the data. This is seen in the top\npanels of Figure 14.35 and is the case in general, as proven by Cutler and\nBreiman (1994). K-means clustering, shown in the bottom panels, chooses\nprototypes in the middle of the data cloud.\nWe can think of K-means clustering as a special case of the archetypal\nmodel, in which each row of Whas a single one and the rest of the entries\nare zero.\nNotice also that the archetypal model (14.75) has the same general form\nas the non-negative matrix factorization model (14.72). However, the two\nmodels are applied in di\ufb00erent settings, and have somewhat di\ufb00erent goals.\nNon-negative matrix factorization aims to approximate the columns of the\ndata matrix X, and the main output of interest are the columns of W\nrepresenting the primary non-negative components in the data. Archetypal\nanalysis focuses instead on the approximation of the rows of Xusing the\nrows of H, which represent the archetypal data points. Non-negative matrix\nfactorization also assumes that r\u2264p. With r=p, we can get an exact\nreconstruction simply choosing Wto be the data Xwith columns scaled\nso that they sum to 1. In contrast, archetypal analysis requires r\u2264N,\nbut allows r > p . In Figure 14.35, for example, p= 2,N= 50 while\nr= 2,4 or 8. The additional constraint (14.76) implies that the archetypal\napproximation will not be perfect, even if r > p.\nFigure 14.36 shows the results of archetypal analysis applied to the\ndatabase of 3\u2019s displayed in Figure 14.22. The three rows in Figure 14.36\nare the resulting archetypes from three runs, specifying two, three and four", "575": "14.7 Independent Component Analysisand Exploratory Projection Pursu it 557\n2 Prototypes 4 Prototypes 8 Prototypes\nFIGURE 14.35. Archetypal analysis (top panels) and K-means clustering (bot-\ntom panels) applied to 50data points drawn from a bivariate Gaussian distribu-\ntion. The colored points show the positions of the prototypes in each case.\narchetypes, respectively. As expected, the algorithm has produced extreme\n3\u2019s both in size and shape.\n14.7 Independent Component Analysis and\nExploratory Projection Pursuit\nMultivariate data are often viewed as multiple indirect measurements aris-\ning from an underlying source, which typically cannot be directly measured.\nExamples include the following:\n\u2022Educational and psychological tests use the answers to questionnaires\nto measure the underlying intelligence and other mental abilities of\nsubjects.\n\u2022EEG brain scans measure the neuronal activity in various parts of\nthe brain indirectly via electromagnetic signals recorded at sensors\nplaced at various positions on the head.\n\u2022The trading prices of stocks change constantly over time, and re\ufb02ect\nvarious unmeasured factors such as market con\ufb01dence, external in-", "576": "558 14. Unsupervised Learning\nFIGURE 14.36. Archetypal analysis applied to the database of digitized 3\u2019s . The\nrows in the \ufb01gure show the resulting archetypes from three runs, specifying two,\nthree and four archetypes, respectively.\n\ufb02uences, and other driving forces that may be hard to identify or\nmeasure.\nFactor analysis is a classical technique developed in the statistical liter-\nature that aims to identify these latent sources. Factor analysis models\nare typically wed to Gaussian distributions, which has to some extent hin-\ndered their usefulness. More recently, independent component analysis has\nemerged as a strong competitor to factor analysis, and as we will see, relies\non the non-Gaussian nature of the underlying sources for its success.\n14.7.1 Latent Variables and Factor Analysis\nThe singular-value decomposition X=UDVT(14.54) has a latent variable\nrepresentation. Writing S=\u221a\nNUandAT=DVT/\u221a\nN, we have X=\nSAT, and hence each of the columns of Xis a linear combination of the\ncolumns of S. Now since Uis orthogonal, and assuming as before that the\ncolumns of X(and hence U) each have mean zero, this implies that the\ncolumns of Shave zero mean, are uncorrelated and have unit variance. In\nterms of random variables, we can interpret the SVD, or the corresponding\nprincipal component analysis (PCA) as an estimate of a latent variable\nmodel", "577": "14.7 Independent Component Analysis and Exploratory Projection Pursu it 559\nX1=a11S1+a12S2+\u2264\u2264\u2264+a1pSp\nX2=a21S1+s22S2+\u2264\u2264\u2264+a2pSp\n......\nXp=ap1S1+sp2S2+\u2264\u2264\u2264+appSp,(14.78)\nor simply X=AS. The correlated Xjare each represented as a linear\nexpansion in the uncorrelated, unit variance variables S\u2113. This is not too\nsatisfactory, though, because given any orthogonal p\u00d7pmatrix R, we can\nwrite\nX=AS\n=ARTRS\n=A\u2217S\u2217, (14.79)\nand Cov( S\u2217) =RCov(S)RT=I. Hence there are many such decom-\npositions, and it is therefore impossible to identify any particular lat ent\nvariables as unique underlying sources. The SVD decomposition does have\nthe property that any rank q < p truncated decomposition approximates\nXin an optimal way.\nThe classical factor analysis model, developed primarily by researchers in\npsychometrics, alleviates these problems to some extent; see, for example,\nMardia et al. (1979). With q < p, a factor analysis model has the form\nX1=a11S1+\u2264\u2264\u2264+a1qSq+\u03b51\nX2=a21S1+\u2264\u2264\u2264+a2qSq+\u03b52\n......\nXp=ap1S1+\u2264\u2264\u2264+apqSq+\u03b5p,(14.80)\norX=AS+\u03b5. Here Sis a vector of q < p underlying latent variables or\nfactors, Ais ap\u00d7qmatrix of factor loadings , and the \u03b5jare uncorrelated\nzero-mean disturbances. The idea is that the latent variables S\u2113are com-\nmon sources of variation amongst the Xj, and account for their correlation\nstructure, while the uncorrelated \u03b5jare unique to each Xjand pick up the\nremaining unaccounted variation. Typically the Sjand the \u03b5jare modeled\nas Gaussian random variables, and the model is \ufb01t by maximum likelihood.\nThe parameters all reside in the covariance matrix\n\u03a3=AAT+D\u03b5, (14.81)\nwhere D\u03b5= diag[Var( \u03b51),... ,Var(\u03b5p)]. The Sjbeing Gaussian and un-\ncorrelated makes them statistically independent random variables. Thus a\nbattery of educational test scores would be thought to be driven by the\nindependent underlying factors such as intelligence ,drive and so on. The\ncolumns of Aare referred to as the factor loadings , and are used to name\nand interpret the factors.", "578": "560 14. Unsupervised Learning\nUnfortunately the identi\ufb01ability issue (14.79) remains, since AandART\nare equivalent in (14.81) for any q\u00d7qorthogonal R. This leaves a certain\nsubjectivity in the use of factor analysis, since the user can search for ro-\ntated versions of the factors that are more easily interpretable. This aspect\nhas left many analysts skeptical of factor analysis, and may account for it s\nlack of popularity in contemporary statistics. Although we will not g o into\ndetails here, the SVD plays a key role in the estimation of (14.81). For ex-\nample, if the Var( \u03b5j) are all assumed to be equal, the leading qcomponents\nof the SVD identify the subspace determined by A.\nBecause of the separate disturbances \u03b5jfor each Xj, factor analysis can\nbe seen to be modeling the correlation structure of the Xjrather than the\ncovariance structure. This can be easily seen by standardizing the covari-\nance structure in (14.81) (Exercise 14.14). This is an important disti nction\nbetween factor analysis and PCA, although not central to the discussion\nhere. Exercise 14.15 discusses a simple example where the solutions from\nfactor analysis and PCA di\ufb00er dramatically because of this distinction.\n14.7.2 Independent Component Analysis\nThe independent component analysis (ICA) model has exactly the same\nform as (14.78), except the Siare assumed to be statistically indepen-\ndentrather than uncorrelated. Intuitively, lack of correlation determines\nthe second-degree cross-moments (covariances) of a multivariate distribu-\ntion, while in general statistical independence determines all of the cross-\nmoments. These extra moment conditions allow us to identify the elements\nofAuniquely. Since the multivariate Gaussian distribution is determined\nby its second moments alone, it is the exception, and any Gaussian inde-\npendent components can be determined only up to a rotation, as before.\nHence identi\ufb01ability problems in (14.78) and (14.80) can be avoided if we\nassume that the Siare independent and non-Gaussian .\nHere we will discuss the full p-component model as in (14.78), where the\nS\u2113are independent with unit variance; ICA versions of the factor analysis\nmodel (14.80) exist as well. Our treatment is based on the survey article\nby Hyv\u00a8 arinen and Oja (2000).\nWe wish to recover the mixing matrix AinX=AS. Without loss\nof generality, we can assume that Xhas already been whitened to have\nCov(X) =I; this is typically achieved via the SVD described above. This\nin turn implies that Ais orthogonal, since Salso has covariance I. So\nsolving the ICA problem amounts to \ufb01nding an orthogonal Asuch that\nthe components of the vector random variable S=ATXare independent\n(and non-Gaussian).\nFigure 14.37 shows the power of ICA in separating two mixed signals.\nThis is an example of the classical cocktail party problem , where di\ufb00er-\nent microphones Xjpick up mixtures of di\ufb00erent independent sources S\u2113\n(music, speech from di\ufb00erent speakers, etc.). ICA is able to perform blind", "579": "14.7 Independent Component Analysis and Exploratory Projection Pursu it 561\nSource Signals Measured Signals\nPCA Solution ICA Solution\nFIGURE 14.37. Illustration of ICA vs. PCA on arti\ufb01cial time-series data. Th e\nupper left panel shows the two source signals, measured at 1000uniformly spaced\ntime points. The upper right panel shows the observed mixed signa ls. The lower\ntwo panels show the principal components and independent component sol utions.\nsource separation , by exploiting the independence and non-Gaussianity of\nthe original sources.\nMany of the popular approaches to ICA are based on entropy. The dif-\nferential entropy Hof a random variable Ywith density g(y) is given by\nH(Y) =\u2212/integraldisplay\ng(y)logg(y)dy. (14.82)\nA well-known result in information theory says that among all random\nvariables with equal variance, Gaussian variables have the maximum en-\ntropy. Finally, the mutual information I(Y) between the components of the\nrandom vector Yis a natural measure of dependence:\nI(Y) =p/summationdisplay\nj=1H(Yj)\u2212H(Y). (14.83)\nThe quantity I(Y) is called the Kullback\u2013Leibler distance between the\ndensity g(y) ofYand its independence version/producttextp\nj=1gj(yj), where gj(yj)\nis the marginal density of Yj. Now if Xhas covariance I, and Y=ATX\nwithAorthogonal, then it is easy to show that\nI(Y) =p/summationdisplay\nj=1H(Yj)\u2212H(X)\u2212log|detA| (14.84)\n=p/summationdisplay\nj=1H(Yj)\u2212H(X). (14.85)\nFinding an Ato minimize I(Y) =I(ATX) looks for the orthogonal trans-\nformation that leads to the most independence between its components. In", "580": "562 14. Unsupervised Learning\n*\n**\n**\n**\n**\n**\n**\n*\n*\n**\n**\n**\n*\n**\n**\n****\n***\n*\n**\n*\n**\n*\n*\n**\n** ***\n**\n**\n**\n**\n**\n**\n***\n*\n***\n*****\n**\n**\n*\n**\n**\n*\n**\n*\n***\n*****\n**\n* **\n*\n***\n* **\n***\n** **\n**\n****\n*\n******\n***\n*\n**\n*\n* ***\n***\n**\n**\n**\n**\n**\n***\n*\n***\n** *\n**\n***\n****\n** *\n**\n*\n**\n**\n*\n**\n**\n*\n* **\n***\n***\n******\n***\n**\n**\n******\n*\n*****\n***\n**\n*\n*****\n**\n***\n*\n***\n*\n***\n**\n***\n*\n***\n*\n**\n*\n*\n**\n*\n***\n**\n*\n**\n****\n***\n*\n*\n**\n***\n**\n**\n***\n***\n*\n****\n**\n***\n*\n**\n**\n*\n*\n*\n****\n**\n***\n**\n**\n***\n*\n***\n****\n**\n**\n** *\n***\n**\n* ***\n***\n***\n*\n**\n**\n****\n**\n*\n*\n***\n*\n***\n**\n**\n**\n***\n* **\n**\n**\n*\n***\n*\n*\n***\n*\n***\n**\n****\n*\n***\n*\n**\n*\n**\n****\n**\n*\n***\n*****\n**\n**\n** **\n**\n***\n*** **\n***\n***\n***\n***\n**\n**\n**\n*\n** **\n*\n**\n**\n**\n**\n*\n**\n*\n***\n***\n***\n**Source S\n**\n*\n**\n**\n****\n**\n*\n*****\n****\n*\n**\n***\n***\n**\n***\n*\n**\n*\n**\n****\n*\n**\n*\n***\n*\n***\n**\n***\n*\n***\n***\n****\n****\n*\n**\n*\n**\n*****\n*\n*\n****\n***\n*\n*\n***\n**\n****\n**\n***\n***\n*\n*\n***\n*\n**\n*\n**\n**\n***\n***\n***\n****\n****\n**\n***\n**\n**\n**\n***\n***\n****\n***\n**\n**\n* **\n*\n****\n***\n***\n*\n***\n*\n**\n***\n***\n**\n**\n**\n****\n**\n*\n****\n*\n*\n**\n**\n**\n***\n*\n***\n**\n**\n****\n*\n*\n***\n****\n*\n***\n****\n*\n***\n**\n**\n*\n**\n*\n***\n**\n**\n*****\n*\n***\n*\n****\n******\n*\n**\n**\n**\n*\n*\n**\n****\n***\n**\n****\n***\n**\n**\n****\n*\n***\n***\n***\n**\n*\n***\n***\n***\n*\n***\n****\n*\n***\n**\n******\n*\n*****\n**\n*\n*\n**\n*\n*\n**\n*\n***\n**\n***\n***\n*\n**\n**\n**\n***\n*\n******\n****\n*\n**\n***\n****\n*\n**\n**\n***\n* *\n**\n***\n****\n******\n**\n***\n*\n**\n**\n***\n****\n*\n**\n**\n**\n**\n***\n****\n*\n**\n*\n**\n**Data X\n*\n***\n***\n*\n*\n***\n***\n**\n**\n***\n**\n*** ***\n*\n**\n* **\n***\n*\n***\n**\n***\n****\n**\n**\n***\n***\n**\n***\n*\n****\n****\n*\n** *\n***\n**\n**\n*****\n*\n***\n***\n***\n***\n***\n**\n**\n*\n*\n*****\n*\n*\n****\n**\n*\n*\n**\n*\n**\n****\n***\n***\n*\n***\n*\n**\n**\n****\n**\n**\n**\n**\n*** **\n*\n***\n**\n***\n**\n***\n*\n**\n*\n**\n* *\n*\n**\n**\n**\n**\n**\n***\n***\n****\n****\n*\n***\n***\n**\n**\n*\n**\n*\n***\n**\n**\n*\n*\n***\n*\n** *\n***\n***\n**\n**\n*\n***\n**\n**\n*\n**\n**\n* *\n**\n** *\n***\n**\n**\n*\n***\n**\n*\n****\n**\n**\n*\n*\n**\n**\n**\n*******\n** *\n**\n***\n***\n***\n**\n**\n**\n*\n* **\n**\n***\n*\n***\n*\n*******\n**\n**\n**\n**\n**\n*\n*\n** *\n*\n*****\n**\n**\n*\n**\n***\n**\n***\n**\n**\n** ***\n****\n*\n****\n*\n*\n***\n**\n*\n****\n*\n***\n**\n***\n****\n*\n*\n*\n***\n**\n*****\n***\n*****\n*\n***\n**\n*\n**\n*\n*****\n**\n*\n*****\n**\n*\n*\n***\n***\n***\n***\n***\n**PCA Solution\n*\n**\n**\n**\n**\n**\n* *\n*\n*\n**\n**\n**\n*\n**\n**\n****\n***\n*\n**\n*\n**\n*\n*\n**\n*****\n**\n**\n**\n**\n**\n**\n***\n*\n***\n*** **\n**\n**\n*\n**\n**\n*\n**\n*\n***\n**** *\n**\n*\n**\n*\n***\n* **\n***\n** **\n**\n****\n*\n** ****\n***\n*\n**\n*\n** **\n***\n**\n**\n*\n*\n**\n**\n**\n*\n*\n***\n** *\n**\n***\n****\n***\n**\n*\n**\n**\n*\n**\n**\n*\n* **\n***\n***\n*** ***\n***\n**\n**\n******\n*\n** ***\n***\n***\n*****\n**\n***\n*\n***\n*\n***\n**\n***\n*\n***\n*\n**\n*\n*\n**\n*\n***\n**\n*\n**\n* ***\n***\n*\n*\n**\n***\n**\n**\n***\n***\n*\n****\n**\n***\n*\n**\n**\n*\n*\n*\n****\n**\n***\n**\n**\n***\n*\n***\n****\n**\n**\n** *\n***\n* *\n****\n***\n***\n*\n**\n**\n*** *\n**\n*\n*\n***\n*\n***\n**\n**\n**\n***\n* **\n**\n**\n*\n***\n*\n* ***\n*\n*****\n****\n*\n***\n*\n**\n*\n**\n****\n**\n*\n***\n*\n****\n**\n**\n****\n**\n***\n* ** **\n***\n***\n** *\n*\n**\n**\n**\n**\n*\n****\n*\n**\n**\n**\n**\n*\n**\n*\n***\n***\n***\n**ICA Solution\nFIGURE 14.38. Mixtures of independent uniform random variables. The upper\nleft panel shows 500realizations from the two independent uniform sources, the\nupper right panel their mixed versions. The lower two panels show the PCA and\nICA solutions, respectively.\nlight of (14.84) this is equivalent to minimizing the sum of the entropies of\nthe separate components of Y, which in turn amounts to maximizing their\ndepartures from Gaussianity.\nFor convenience, rather than using the entropy H(Yj), Hyv\u00a8 arinen and\nOja (2000) use the negentropy measure J(Yj) de\ufb01ned by\nJ(Yj) =H(Zj)\u2212H(Yj), (14.86)\nwhere Zjis a Gaussian random variable with the same variance as Yj. Ne-\ngentropy is non-negative, and measures the departure of Yjfrom Gaussian-\nity. They propose simple approximations to negentropy which can be com-\nputed and optimized on data. The ICA solutions shown in Figures 14.37\u2013\n14.39 use the approximation\nJ(Yj)\u2248[EG(Yj)\u2212EG(Zj)]2, (14.87)\nwhere G(u) =1\nalog cosh( au) for 1 \u2264a\u22642. When applied to a sample\nofxi, the expectations are replaced by data averages. This is one of the\noptions in the FastICA software provided by these authors. More classical\n(and less robust) measures are based on fourth moments, and hence look for\ndepartures from the Gaussian via kurtosis. See Hyv\u00a8 arinen and Oja (2000)\nfor more details. In Section 14.7.4 we describe their approximate Newton\nalgorithm for \ufb01nding the optimal directions.\nIn summary then, ICA applied to multivariate data looks for a sequence\nof orthogonal projections such that the projected data look as far from", "581": "14.7 Independent Component Analysis and Exploratory Projection Pursu it 563\nComponent\n 1oooooo o\nooo\noo\noo\noo\no\noooooo\nooo\noooo ooo\nooooooo\noo\no\noo\noooooo\noo\nooo ooooo\nooooooooo\nooooooooooo\noooo\noo o\noo\noooooooooo\noo\no\noooooooo\nooo\noooooooooo\noooooo\noooooo\noooooo\noo\nooo\nooo ooo\nooooo\no\noo\nooooooooooo\noooo\nooooo\noooooooo\nooo\noo\nooooo\noo\nooo\nooooo\nooooooo\nooo\noooo\noo\noooo\noooooo\noo\nooo\noooo\noooooooooooooo\noooooooooooo\noooooo\nooo\nooo\noo\noo\nooooo\nooooooo\nooo\noooo\nooooo\noo\noooo\noooooo\nooooo\noooooooo\nooooooooooo\nooo\nooo\noooo\noo\noo\nooooo\nooooo\nooo\noo\noo\noo\nooooooooo\nooooooo\nooooo\noo\noooo ooooo\noo\no\noooo\nooo\noo\noooooooo\nooo\noo\nooo\nooo\noooo\nooo\noooo\nooo\noooooooooo\nooooo\nooo\nooooo\no\noo\nooo\noo\no\noooooooo\nooo\noooooooooooooo\nooooooo\nooooo oo\noo\noooo\noooo\nooooo o\noooooooo\nooooo\noo\nooooooo\nooo\noooo\noooo\no\nooo\noooooo\nooo\nooooo\nooo\nooooo\noooooooo\noo\no\nooo\nooooooooooo\nooooooo\noo\noo\noo\nooo oo\no\nooooo\noo\noooo\noooooooo\nooo\noo\noo\noo\no\noooooo\nooo\nooooooo\nooooooo\noo\no\noo\noooooo\noo\nooo ooooo\nooooooooo\nooooooooooo\noooo\noo o\noo\noooooooooo\noo\no\noooooooo\nooo\noooooooooo\noooooo\noooooo\noooooo\noo\nooo\nooo ooo\nooooo\no\noo\nooooooooooo\noooo\noooo o\noooooooo\nooo\noo\nooooo\noo\nooo\nooooo\nooooooo\nooo\noooo\noo\noooo\noooooo\noo\nooo\noooo\noooooo oooooooo\noooooooooooo\noooooo\nooo\nooo\noo\noo\nooooo\nooooooo\nooo\noooo\nooooo\noo\noooo\noooooo\nooooo\noooooooo\nooooooooooo\nooo\nooo\noooo\noo\noo\nooooo\nooooo\nooo\noo\noo\noo\nooooooooo\nooooooo\nooooo\noo\nooooooooo\noo\no\noooo\nooo\noo\noooooooo\nooo\noo\nooo\nooo\noooo\nooo\noooo\noo o\noooooooooo\nooooo\nooo\nooooo\no\noo\nooo\noo\no\noooooooo\nooo\noooo o ooooooooo\nooooooo\nooooo oo\noo\noooo\noooo\nooooo o\noooooooo\nooooo\noo\nooooooo\nooo\noooo\noooo\no\nooo\noo oooo\no oo\nooooo\nooo\nooooo\noooooooo\noo\no\nooo\noo ooooooooo\nooooooo\noo\noo\noo\nooooo\no\nooooo\noo\noooo\noooooooo\nooo\noo\noo\noo\no\noooooo\noo o\nooooooo\nooooooo\noo\no\noo\noooooo\noo\noooooooo\nooooooooo\nooooooooooo\noooo\noo o\noo\nooooo o oooo\noo\no\noooooooo\nooo\noooo oooooo\noooooo\noooooo\noooooo\noo\nooo\nooo ooo\nooooo\no\noo\noooooooo ooo\noooo\nooooo\noooooooo\nooo\noo\nooooo\noo\nooo\nooooo\nooooooo\nooo\noooo\noo\noooo\noooooo\noo\nooo\noooo\noooooo oooooooo\noooooooooooo\noooooo\nooo\nooo\noo\noo\nooooo\nooooooo\nooo\noooo\nooooo\noo\noooo\nooo ooo\nooooo\noooooooo\nooo oooooooo\nooo\nooo\noooo\noo\noo\nooooo\nooooo\nooo\noo\noo\noo\nooooooooo\nooooooo\nooooo\noo\noooo ooooo\noo\no\noooo\nooo\noo\noooooooo\nooo\noo\nooo\nooo\noooo\nooo\noooo\nooo\noooooooooo\nooooo\nooo\nooooo\no\noo\nooo\noo\no\noooooooo\nooo\nooooo o oooooooo\nooooooo\nooooo oo\noo\noooo\noooo\noooooo\noooooooo\nooooo\noo\nooooooo\nooo\noooo\no ooo\no\nooo\noo oooo\nooo\nooooo\nooo\nooooo\noooooooo\noo\no\nooo\nooooooooooo\nooooooo\noo\noo\noo\nooooo\no\nooooo\noo\noooo\noooooooo\nooo\noo\noo\noo\no\noooooo\noo o\nooooooo\nooooooo\noo\no\noo\noooooo\noo\nooo ooooo\nooooooooo\nooooooooooo\noooo\nooo\noo\noooooooooo\noo\no\noooooooo\nooo\noooo oooooo\noooo oo\noooooo\nooo ooo\noo\nooo\nooo ooo\nooooo\no\noo\nooooooooooo\noooo\nooooo\noooooooo\nooo\noo\nooooo\noo\nooo\nooooo\nooooooo\nooo\noooo\noo\noooo\noooooo\noo\nooo\noooo\nooo ooooooooooo\noooooooooooo\noooooo\nooo\nooo\noo\noo\nooooo\nooooooo\nooo\noooo\nooooo\noo\noooo\noooooo\nooooo\noooooooo\nooooooooooo\nooo\nooo\noooo\noo\noo\nooooo\nooooo\nooo\noo\noo\noo\nooooooooo\nooooooo\nooooo\noo\nooooooooo\noo\no\noooo\nooo\noo\noooooooo\nooo\noo\nooo\nooo\no ooo\noo o\noooo\nooo\noooooooooo\nooooo\nooo\nooooo\no\noo\nooo\noo\no\noooooooo\nooo\noooooooooooooo\nooooooo\nooooo oo\noo\noooo\no ooo\noooooo\noooooooo\nooooo\noo\nooooooo\nooo\noooo\noooo\no\nooo\noo oooo\nooo\nooooo\nooo\nooooo\noooooooo\noo\no\noo o\nooooooooooo\nooooooo\noo\noo\noo\nooooo\no\nooooo\noo\noooo\no\noo\noo\noo\noo\nooooo\noo\nooooo\noooo\nooo\noooo\noo\noooo\nooo\nooo\noo\noo\nooooo\nooo\noo\noooo\nooo\nooooo\noo\no\noo\noooooo\no\noo\noo oo\no\noo\noooooo\no\nooo\noooo\nooooo\no\nooo\no\noo\noo\noo\noo\nooo\noooo\nooo\noo\nooooo\no\noooo\noo\nooo\nooo\no\nooo\nooo\noo\noo\nooo\noo\nooooo oooo\noo\nooo\nooo\nooo\nooo\nooo\nooooo\no\noo\nooo\noo\noo\no\noo oo\noooo\noooo\nooo\noo\no\nooo\nooo\no\no\no\nooo\no\nooo\nooooo\noooo\noo\noooo o\noo\noo\noooooo\noo\noo\noo\nooooo\noooo\noooo\nooo\noo\noooo\noo\no\nooo\nooo\nooo\noo\noooo\noooo\nooooo\noo\noooooo\no\nooooo\nooooo\nooooo\noooo\noo\noooo\noooo\no\no\noooo oooooo\nooo\no\noooo\nooo\noo\nooo\noooo\nooo\noo\noo\no\noo\no\noo\nooo\nooo\noooo\noooo\nooooo\noooo\nooo\no\nooo\no\noo\noo\nooo\no\noo\noooo\noo\noo\noooo\noo\no\nooo\noo\no\noooo\noo\noo\noo\noo\noo\noo\noo\noooo\no\noooo\noooo\nooo\nooo\nooo\noooooo\noooooo\noo\nooo\noooo\no\nooooooo\noo\noo\no\nooo\noo\noooooo\no\no\nooooo\nooo\noo\noooo\nooooo\noo\nooooooo\noooo\nooo\noo ooo\nooo\nooo\noooo\nooooo\nooo\nooo\nooo\nooo\noo\nooo\nooooo\noo\no\noo\noo\nooo\noooooo\noo\no\noo\nooo\nComponent\n 2\noooo\nooooo\nooo\noooo\nooooooo\noo\noooo\noooo\noooo\noooo\nooooooo ooooooo\nooooooooooo\nooo\noo\noooo\noo\noooo\noo oooo o\noo\noooooooooo\nooo\no\noo\nooooooooooooooo\no o\nooo\nooooo\noooo\nooo\noooooo\no\noo\noo\nooo\no\no oooooooooooo\noooo\no\nooo\noo\nooooo oo\noo\no\nooo\noooo\nooo\noo\noo\no\noooo\noooooo\nooooo\noooo\noooooo\nooooo\nooo\noooo\nooooo\nooooo\noooo\no oooo\nooo\nooooooo\noo\no\noooo\nooooo\noo\noo\nooo\noo\noooo\noo\noo\nooo ooooo\noooooo\noo\nooooo\nooooo\nooo\noo\nooo\nooooo\noooooo\nooooo\nooooo\noooooo\noooo\noooooooo\no\noo oooooo\no\nooooooooo\noooooooo\noooo\nooo\noo\noo\noooo\nooo\nooo\no\nooo\noo\nooo\nooo\noo\noooooo\noo\nooooo\nooooo\noooo oooo\noo\noo\nooooo\noo\noooooooo\noooo\noo\noooo\no\noo\no oooooo\noooooooo\nooooooooooo\noo\noooo\no\nooooo\nooo\no\noooooo\noooooo\nooo\nooo\noo ooooooooo\nooo\no\nooo\nooo\noooooo\noo\noooo\nooo\noo oooo\nooo\noooooo\noooo\nooooo\noooooo\nooo\nooooo\noooo\no\noooo\noo\noo\nooooo\noo\nooooo oooo\no\no\noo\noooo oo\nooooo\nooo\noooo\nooooooo\noo\noooo\noooo\noooo\noooo\noooo ooo ooooooo\nooooooooooo\nooo\noo\noooo\noo\noooo\noo ooooo\noo\nooooo ooooo\nooo\no\noo\noo ooooooooooooo\no o\noo o\nooooo\noooo\nooo\noooooo\no\noo\noo\nooo\no\nooooooooooooo\noooo\no\nooo\noo\nooooooo\noo\no\nooo\noooo\nooo\noo\noo\no\noooo\noooooo\nooooo\noooo\noooooo\nooooo\nooo\noooo\nooooo\nooo oo\noooo\nooooo\nooo\nooooooo\noo\no\noooo\nooooo\noo\noo\nooo\noo\noooo\noo\noo\nooo ooooo\noooooo\noo\nooooo\nooooo\nooo\noo\nooo\nooooo\noo o ooo\nooooo\nooo oo\noooooo\noooo\noooooooo\no\noooooooo\no\nooooooooo\noooooooo\noooo\nooo\noo\noo\noooo\nooo\nooo\no\nooo\noo\nooo\nooo\noo\noooooo\noo\nooooo\no oooo\noooooooo\noo\noo\nooooo\noo\noooooooo\noooo\noo\noooo\no\noo\no oooooo\noooooooo\nooooooooooo\noo\noooo\no\nooooo\nooo\no\noooooo\noooooo\nooo\nooo\nooooooooooo\nooo\no\nooo\nooo\noooooo\noo\noooo\nooo\noooooo\nooo\noooooo\noooo\nooooo\noooooo\nooo\nooooo\noooo\no\noooo\noo\noo\nooooo\noo\nooooooooo\no\no\noo\noooo oo\nooooo\nooo\noooo\nooooooo\no o\noooo\noooo\noooo\noooo\noooo ooo ooooooo\nooooooooooo\nooo\noo\noooo\noo\noooo\noooo ooo\noo\noooooooooo\nooo\no\noo\noo ooooooooooooo\no o\noo o\nooooo\noooo\nooo\noo oooo\no\noo\noo\nooo\no\nooooo oooooooo\noooo\no\nooo\noo\nooooooo\noo\no\nooo\noooo\nooo\noo\noo\no\noooo\noooooo\nooooo\noooo\noooooo\nooooo\nooo\noooo\nooooo\nooooo\noooo\nooooo\nooo\nooooooo\noo\no\noooo\nooooo\noo\noo\nooo\noo\noooo\noo\noo\noooooooo\noooooo\noo\nooooo\nooooo\nooo\noo\nooo\nooooo\noooooo\nooooo\nooo oo\noooooo\noooo\noooooooo\no\noooooooo\no\nooooooooo\noooooooo\noooo\nooo\noo\noo\noooo\nooo\nooo\no\nooo\noo\nooo\nooo\noo\noooooo\noo\nooooo\noo oo o\noooooooo\noo\noo\nooooo\noo\noooooooo\noooo\noo\noooo\no\noo\no oooooo\noooooooo\nooooooooooo\noo\noooo\no\nooooo\nooo\no\nooo ooo\noooooo\nooo\nooo\nooooooooooo\nooo\no\nooo\nooo\noooooo\noo\noooo\nooo\noooooo\nooo\noooooo\noooo\nooooo\noooooo\no oo\nooooo\noooo\no\noooo\noo\noo\nooooo\noo\nooooooooo\no\no\noo\noo\no\noooooo\nooo\nooo\no\nooo\nooo\no\nooo oooo\noo\nooo\nooooo\noo\noo\no\noooooo\no\nooo\nooo\nooo\noooo\nooo\nooo\nooooo\noo\nooo\noo\nooooooo\no o\noo\no\nooo\noo\noo\noo\no\nooo\nooooo\noo\noo\nooo\no\nooo\nooo o\nooooo\noo\no\noo\noo\no\nooo\no\nooo\noo\noooo\nooo ooo\nooo\no\noo\noo\nooo\nooo o\noo\noo\no\nooo\no\noo\noooooo\nooo\noo\nooo\no\noo\nooo\nooo\noo\noo\noo\noo\noo\nooooo\no\no\noooo\nooo\noooo\noo\nooo\noo\no\nooooo\nooo\noo\no\no\noo\noo\no\noo\noo\noooooooo\nooooo\nooooo\no\noooo\noo\nooooo\noo\nooooo\noooo\nooo\noo\nooo\no\noo\no\noooo\nooo\noo\noo\noooo\noo\nooo\no\nooo\noo\noooooo\noo\noooo\noo\nooo\noooo\nooooo\noooo\no\noooo\nooooo\noo\no\noooo\noo\noo\no\no\noooo\noo\nooo\no\noo\noo\no\noooooo\no\no\nooooo\nooo\noooo\no\no\nooo\nooo\noooooooooo\noo\nooo\noo\noooooo\nooo\noooo\noo\nooo\nooo\noooooo\noo\no\noooooo\no\noo\no\nooo\no\nooo\no\noo oo\noo\nooo\nooo\no\noo\nooo\nooooo\noo\nooo\no\noooo\no\noooo\noo\no\noo\nooo\nooo\no\nooooo\noo\noo\nooo\noo\nooo\nooo\nooooo\no\nooooooo\noooo\nooo\nooooo\noo\noo\nooooooo\noo o\noo\no\nooo\noo o\noo\no\noo\noooooo\noo\no\nooo\noo\no\nooo\no\noo\noo\no\nooo\no\noooo\noooooo\nooo\nooo\no\nooo\nooo\no\nooooooo\noo\noo o\nooooo\noo\noo\no\nooooooo\nooo\nooo\nooo\noooo\nooo\nooo\nooooo\noo\nooo\noo\nooooooo\noo\noo\no\nooo\noo\noo\noo\no\nooo\noo ooo\noo\noo\nooo\no\nooo\nooo o\no oooo\noo\no\noo\noo\no\nooo\no\nooo\noo\noooo\noooooo\nooo\no\noo\noo\nooo\noooo\noo\noo\no\nooo\no\noo\noooooo\nooo\noo\nooo\no\noo\nooo\nooo\noo\noo\noo\noo\noo\nooooo\no\no\noooo\nooo\noooo\noo\nooo\noo\no\nooooo\nooo\noo\no\no\noo\noo\no\noo\noo\noooooo oo\nooooo\nooooo\no\noooo\noo\nooooo\noo\nooooo\noooo\nooo\noo\nooo\no\noo\no\noooo\nooo\noo\noo\noooo\noo\nooo\no\nooo\noo\noooooo\noo\noooo\noo\nooo\noooo\nooooo\noooo\no\no ooo\nooooo\noo\no\noooo\noo\noo\noo\noooo\noo\nooo\no\noo\noo\no\noooooo\no\no\nooo oo\nooo\noooo\no\no\nooo\nooo\noooooooooo\noo\nooo\noo\noooooo\nooo\noooo\noo\nooo\nooo\noooooo\noo\no\noooooo\no\noo\no\nooo\no\nooo\no\noooo\noo\nooo\nooo\no\noo\nooo\nooooo\noo\nooo\no\noooo\no\noooo\noo\no\noo\nooo\nooo\no\nooooo\noo\noo\nooo\no o\nooo\nooo\nooooo\no\nooooooo\noooo\nooo\nooooo\noo\noo\nooooooo\nooo\noo\no\nooo\nooo\noo\no\noo\noooooo\noo\no\nooo\noo\no\nooo\no\noo\noo\no\nooo\no\noooComponent\n 3oo\nooooooooo\noooo\noo\noo\noo\nooo\nooo\noo\nooooooo\nooooo\noooo\nooooooo\nooo\nooooo\nooo\nooooo\no ooooooo\nooo\no\noo\no o\nooo\noo\nooooo\no ooo\noooooo\nooo\noooooooo\noo\nooo\nooooo\noooooooooo\noo\noooooooo\noo\nooooo o ooooooooo\nooooo\nooooo\nooo\nooo\noo\nooooooo\nooooooo\no\no\noo\noooo\noo\noooo\noo\noo\nooooooo\nooo\noooooo\nooo\nooo\nooo\noooooo\nooo oo\noooo\noo\nooooo\no\noo\noooooooo\noo\noo\noo\noo\noo\noo\nooo\nooooooooo\noo\noooo\noooo\nooo\noooo\nooo\noo\noooooo\no\noooo\noooo\noooooo o\nooo\noooooo\nooo\noooo\no\noo\noo\noo\noooooooooo\no\noooo\nooo\nooooo\noooo\noo\nooo\nooo\noo\nooooooooo\nooo\nooo\noooooo\nooo\noooooooo\noooooo\no\noooo\noooooooooooo\noo\noo\noooooooo o\noo\nooo\noooooooo\noooooooo\noooo\noo\noooooooo\noo\noo\noooooo\noooooooo\noooo\nooo\noooooooo\noooooo\no\noo\no ooo\noooo\no\nooooo\noooo\noooo\nooooo\noooooooo\noo\nooo ooo\nooooooo\nooooooo\nooooo\noooo\nooooooooo\noooo oo\noooo\noooo\noooooooo\nooo o\nooo\no\nooo\noo\nooooooo\nooooooooo\noooo\noo\noo\noo\nooo\nooo\noo\nooooooo\nooooo\noooo\nooooooo\nooo\nooooo\nooo\nooooo\noooooooo\nooo\no\noo\noo\nooo\noo\nooooo\noooo\noooooo\nooo\noooooooo\noo\nooo\nooooo\noooooooooo\noo\noooooooo\noo\nooooooooooo o ooo\nooooo\nooo oo\nooo\nooo\noo\nooooo oo\nooooooo\no\no\noo\noooo\noo\noooo\noo\noo\nooooooo\nooo\noooooo\nooo\nooo\nooo\noooooo\nooooo\noooo\noo\nooooo\no\noo\noooooooo\noo\noo\noo\noo\noo\noo\nooo\nooooooooo\noo\noooo\noooo\nooo\noooo\nooo\noo\noooooo\no\noooo\noooo\nooooooo\nooo\noooooo\nooo\noooo\no\noo\noo\noo\noooooooooo\no\noooo\nooo\nooooo\noooo\noo\nooo\nooo\noo\nooooooooo\nooo\nooo\noooooo\nooo\noooooooo\noooooo\no\noooo\nooooo o oooooo\noo\noo\nooooo oooo\noo\nooo\noooooooo\noooooooo\noooo\noo\noooooooo\noo\noo\noooooo\noooooooo\noooo\nooo\noooooooo\noooooo\no\noo\no ooo\noooo\no\nooooo\noooo\noooo\nooooo\noooooooo\noo\noooooo\nooooooo\nooooooo\nooooo\noooo\noooo oo ooo\noooo oo\noooo\noooo\noooooooo\noo oo\nooo\no\nooo\noo\nooooo\noooo\noooooooo\noo\noo\noo\no\noooo\noo\noo\no\noooo\noo\nooo\no\noooo\noo\noo\nooo\noo\noo\noooo\no\nooo\noo\no\noo\nooo\noooo\noooo\no\no\nooo\no\noo\nooo\noo\no\nooooo\noo\nooo\nooo\noo\nooo\noooo\noooooo\noooo\noo\nooo\nooo\noo\noo\noo\noo\noo\nooo\noo\nooo\noo\noo\noo\nooo\nooooo\no\noo\noooo\nooo\noo\no\nooo\nooooooooooo\nooo\no\nooo\noooo\no\nooo\nooo\no\nooo\no\noo\nooo\nooo\noooo\nooo\noo o\nooooo\no\noo\nooo\nooo\nooo\nooo\noooooo\noo\no\nooo\no\nooooooooooo\nooo\no\noo\no\noooo\no\nooooooo\noo\noo\noo\nooooo\noo\nooooo\no\noooo\no\noo\noo\nooo\no\nooo oo\noo\nooo\noo\no\noooo\noo\noo\nooo\no\noo\nooooo\noooo\no\noo\no\noooooooo\noo\noooooo\nooo\nooooooo\noooo\no\noo\no\nooo\no\noooo\noooo\noo\noo\noo\no\no\noooo\noo\no\noooo\no oooooo\no\noooo\noo\noo\noooo\nooo\nooo\noo\noooo\nooo\no\nooo\noooo\noo\nooo\nooo\no\noooo\noo\no\noo\no\noooo\nooo\noooo\nooo\nooooooooo\no\nooooo\no\nooo\no\noo\nooo\noooo\nooo\noooo\no\nooooo\no\nooo\nooo\nooo\noo\noooooo\nooo\noo\noo o\nooooooooo\noo\noooo\nooo\noo\nooo\noo\nooo\noo\no\nooo\no\noo\nooooo\noo\noooooo\noooooo\noo\no\noooo\nooo\nooo\nooo\noo\no\nooo\noo\noo\no oooo\noooooooo\noo\noo\noo\no\noooo\noo\noo\no\noooo\noo\nooo\no\noooo\noo\noo\nooo\noo\noo\noooo\no\nooo\noo\no\noo\nooo\noooo\noooo\no\no\nooo\no\noo\nooo\noo\no\nooooo\noo\nooo\nooo\noo\noo\nooooo\noooo oo\noooo\noo\noo o\nooo\noo\noo\noo\noo\noo\nooo\noo\nooo\noo\noo\noo\nooo\no\noooo\no\noo\noooo\nooo\noo\no\no oo\noooo ooooo oo\nooo\no\nooo\noooo\no\nooo\nooo\no\nooo\no\noo\nooo\nooo\noooo\nooo\nooo\nooooo\no\noo\nooo\nooo\nooo\nooo\noooooo\noo\no\nooo\no\nooooooooooo\nooo\no\noo\no\noooo\no\noooo\nooo\noo\noo\noo\nooooo\noo\nooooo\no\noooo\no\noo\noo\nooo\no\nooooo\noo\nooo\noo\no\noooo\noo\noo\nooo\no\noo\nooooo\noooo\no\noo\no\noooooooo\noo\noooooo\nooo\nooooooo\noooo\no\noo\no\nooo\no\noooo\noooo\noo\noo\noo\no\no\noooo\noo\no\noooo\nooooooo\no\noooo\noo\noo\noooo\nooo\nooo\noo\noooo\nooo\no\nooo\noooo\noo\nooo\nooo\no\noooo\noo\no\noo\no\noooo\nooo\noooo\nooo\nooooooooo\no\nooooo\no\nooo\no\noo\nooo\noooo\nooo\noooo\no\nooooo\no\nooo\nooo\nooo\noo\noooooo\nooo\noo\nooo\nooooooooo\noo\noooo\nooo\noo\nooo\noo\nooo\noo\no\nooo\no\noo\nooooo\noo\nooo ooo\noooooo\noo\no\noooo\nooo\nooo\nooo\noo\no\nooo\noo\noo\nooooo\noooooooo\noo\noo\noo\no\noooo\noo\noo\no\noooo\noo\nooo\no\noooo\noo\noo\nooo\noo\noo\noooo\no\nooo\noo\no\noo\nooo\noooo\noooo\no\no\noo o\no\noo\nooo\noo\no\nooooo\noo\nooo\nooo\noo\nooo\noooo\noooooo\no ooo\noo\noo o\nooo\noo\noo\noo\noo\noo\nooo\noo\nooo\noo\noo\noo\nooo\nooooo\no\noo\noooo\nooo\noo\no\nooo\nooooooooooo\nooo\no\nooo\noooo\no\nooo\nooo\no\nooo\no\noo\nooo\nooo\noooo\nooo\nooo\nooooo\no\noo\nooo\nooo\nooo\nooo\noooooo\noo\no\nooo\no\noooo ooooooo\nooo\no\noo\no\noooo\no\nooooooo\noo\noo\noo\nooooo\noo\nooooo\no\noooo\no\noo\noo\nooo\no\nooooo\noo\nooo\noo\no\noooo\noo\noo\nooo\no\noo\nooooo\noooo\no\noo\no\noooooooo\noo\noooooo\nooo\nooooooo\noooo\no\noo\no\nooo\no\noooo\noooo\noo\noo\noo\no\no\noooo\noo\no\noooo\nooooooo\no\noooo\noo\noo\noooo\nooo\nooo\noo\noooo\nooo\no\nooo\noooo\noo\nooo\nooo\no\noooo\noo\no\noo\no\noooo\nooo\noooo\nooo\nooooooooo\no\nooooo\no\nooo\no\noo\nooo\noooo\nooo\noooo\no\nooooo\no\nooo\nooo\nooo\noo\noo oooo\nooo\noo\noo o\nooooooooo\noo\noooo\nooo\noo\nooo\noo\nooo\noo\no\nooo\no\noo\nooooo\noo\noooooo\noooooo\noo\no\noooo\nooo\nooo\nooo\noo\no\nooo\noo\noo\noComponent\n 4ooo\nooo\nooo\no\noooo oooooo\noooooo\noooooo\noo\noo\nooo\noo\noo ooo\no\nooo\noo\nooooo\no\nooo\no\noooo\no\no\noooooo ooo\nooooooo\no oooo\nooooo\no\noooooo\noo\noooooooo\no\nooo\noo\noooo\nooo\noo\nooo\noooooo\no\noooo\nooo\noo\noo\no\noo\noo\noooooo o\nooo\nooooo\noo\no ooo\no\noo\nooooooo\nooo\nooo\no\nooo\nooo\noo\noooo\nooooooooo\nooooooo\no\nooooo\noo\noooo\noooo\noo\no\nooooooo\noo\nooooooooooo\no\noooooo\noo\noooo\nooo o\nooooooo\nooo\nooo\no\noooo\noooo\nooo\noooooooooooo\nooooooooooo\noooo\nooooo\nooo\no\no\no\nooooo\noooooooooo\noooooo\noo o\no\noooo\noooooo\no\noooooooooo\noo\noooooooooo\nooooooo\nooo oo\noo\no\noooo\nooo\nooo oo\nooooooo\no\nooo\noo\no\no\nooooo\noo\noo\nooo\no\noo\noooo\nooo\nooo\noo\nooo\noo\noo\noo\noo\nooo\nooooooo\noo\noo\noooooooo\nooo\noo\noo\noo\noooooooo\nooo\nooooooo\nooo\nooooooooooo\noooo\noooo\no\noooooooo\noo\noooooo\no\noo\nooooooo\nooo\nooooooooo\nooo\noo\no\noooooo\noooo\noooooo\noo\noo\noo oo o\no\no\noo oooooo\nooo\no\noo\noo\no\noooo\no\noooo\nooooo\noo\no\noo\noo\noo\no\no\noo\noooo\no\no\noo\noo\nooo\nooo\nooo\no ooo\nooooo\no\no\noo\noo\no\noo\noo\no\nooooo\nooooo\noo\noo\nooo\noo\no\nooo\noooo\noooo\noooo\noooo\no\noo\no\no\noooooo\nooooooooo\nooo\noo\noo\noo\no\noo\noo\noo\no\noo\no\noo\no\noooo\nooo\noo\noo\no\noo\nooooo\noo\nooo\noo\nooo\no\noooooooooo\noo\no\noooo\noooo\noo\no\nooo\noo\noo\noooooo\nooo\noo\nooo\noooo\nooo\noo\nooooooo\noo\nooo\no\noo\nooooo\no\nooooo\no\noooo\noo\nooo\noooo\noooo\noo\no\no\noo\nooo\nooooo\noo\noo\no\no\nooo\nooo\noo\nooooooooo\no\noooo\no\noo\nooo\noo\noooo\noo oo\noo\noo\noo\noo\noo\noooo\nooo\nooooo\noo\noo\nooo\noo\noooooo\noo\no\nooooo\noo\noooooooooo\noooooo\noo\noooo\no\nooo\noo\no\noooooo\nooo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noooooo\nooooo\nooo\nooo\noo\nooo\no\nooo\nooo\nooooooo\no\nooooo\noo\no\noo\noo\nooo\noo\noo\nooo\no\noooo\no\nooo\noooooo\no\no\nooooooo\no\noooo\noo\noooo\nooo\nooo\no\nooooo\noooo\noooooo oooooo\noo\noooo\noo\nooooo\nooo\noo\nooo\noo\noooo\nooo\noo\no\noooo\nooooo\no\noo\no\noooo\noo\no\no\nooo\noo\no\nooo\noo\noooo\noo\no\noo\noo\noo\noooooo\noo\noo\noo\nooooo\nooo\noooo\noo\no\noo\no\nooooo\nooooo\no\noo\noooo\no\no\noo\noo\nooo\nooo\nooo\noooo\nooooo\no\no\noo\noo\no\noo\noo\no\nooooo\nooooo\noo\noo\nooo\noo\no\nooo\noooo\noooo\noooo\noooo\no\noo\no\no\noooooo\nooooooooo\nooooo\noo\noo\no\noo\noo\noo\no\noo\no\noo\no\noooo\nooo\noo\noo\no\noo\nooooo\noo\nooo\noo\nooo\no\no\nooooooooo\noo\no\noooo\noooo\noo\no\nooo\noo\noo\noooooo\nooo\noo\nooo\noooo\nooo\noo\nooooooo\noo\nooo\no\noo\nooooo\no\nooooo\no\noooo\noo\nooo\noooo\noooo\noo\no\no\noo\nooo\nooooo\noo\noo\no\no\nooo\nooo\noo\nooooooooo\no\noooo\no\noo\nooo\noo\noooo\noooo\noo\noo\noo\noo\noo\no ooo\nooo\nooooo\noo\noo\nooo\noo\noooooo\noo\no\nooooo\noo\noooooooooo\noooooo\noo\noooo\no\nooo\noo\no\noooooo\nooo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noooooo\nooooo\nooo\nooo\noo\nooo\no\nooo\nooo\nooooooo\no\nooooo\noo\no\noo\noo\nooo\noo\noo\nooo\no\noooo\no\nooo\nooo ooo\no\no\nooooooo\no\noooo\noo\noooo\nooo\nooo\no\nooooo\noooo\noooooooooooo\noo\noooo\noo\nooooo\nooo\noo\nooo\noo\nooo o\nooo\noo\no\noooo\nooooo\no\noo\no\noooo\noo\no\no\nooo\noo\no\nooo\noo\noooo\noo\no\noo\noo\noo\noooooo\noo\noo\noo\nooooo\no oo\noooo\noo\no\noo\no\nooooo\nooooo\no\noo\noooo\no\no\noo\noo\nooo\nooo\nooo\noooo\nooooo\no\no\noo\noo\no\noo\noo\no\nooooo\nooooo\noo\noo\nooo\noo\no\nooo\noooo\noooo\no ooo\noooo\no\noo\no\no\noooooo\nooooooo oo\nooo\noo\noo\noo\no\noo\noo\noo\no\noo\no\noo\no\noooo\nooo\noo\noo\no\noo\nooooo\noo\nooo\noo\nooo\no\noooooooooo\noo\no\noooo\noooo\noo\no\nooo\noo\noo\noooooo\nooo\noo\nooo\noooo\nooo\noo\noooo ooo\noo\nooo\no\noo\nooooo\no\nooooo\no\noooo\noo\nooo\noooo\noooo\noo\no\no\noo\nooo\nooooo\noo\noo\no\no\nooo\nooo\noo\nooooooooo\no\noooo\no\noo\nooo\noo\noooo\noooo\noo\noo\noo\noo\noo\noooo\nooo\nooooo\noo\noo\nooo\noo\noooooo\noo\no\nooooo\noo\nooooo ooooo\noooooo\noo\noooo\no\nooo\noo\no\noooooo\nooo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noooooo\nooooo\nooo\nooo\noo\nooo\no\nooo\nooo\nooooooo\no\nooooo\noo\no\noo\noo\nooo\noo\noo\nooo\no\noooo\no\nooo\noooooo\no\no\nooooooo\no\noooo\noo\noooo\nooo\nooo\no\nooooo\noooo\noooooo oooo oo\noo\noooo\noo\nooooo\nooo\noo\nooo\noo\noooo\nooo\noo\no\noooo\nooooo\no\noo\no\noooo\noo\no\no\nooo\noo\no\nooo\noo\noooo\noo\no\noo\noo\noo\noooooo\noo\noo\noo\nooooo\nooo\noooo\noo\no\noo\no\nooooo\nooooo\no\noo\noooo\no\no\noo\noo\nooo\nooo\nooo\noooo\nooooo\no\no\noo\noo\no\noo\noo\no\nooooo\nooooo\noo\noo\nooo\noo\no\nooo\noooo\noooo\noooo\noooo\no\noo\no\no\noooooo\nooooooooo\nooo\noo\noo\noo\no\noo\noo\noo\no\noo\no\noo\no\noooo\nooo\noo\noo\no\noo\nooooo\noo\nooo\noo\nooo\no\no\nooooooooo\noo\no\noooo\noooo\noo\no\nooo\noo\noo\noooooo\nooo\noo\nooo\noooo\nooo\noo\noooo ooo\noo\nooo\no\noo\nooooo\no\nooooo\no\noooo\noo\nooo\noooo\noooo\noo\no\no\noo\nooo\nooooo\noo\noo\no\no\nooo\nooo\noo\nooooooooo\no\noooo\no\noo\nooo\noo\noooo\noooo\noo\noo\noo\noo\noo\noooo\nooo\nooooo\noo\noo\nooo\noo\noooooo\noo\no\nooooo\noo\noooooooooo\noooooo\noo\noooo\no\nooo\noo\no\noooooo\nooo\nooo\nooo\nooo\noo\nooo\nooo\nooooo\noooooo\nooooo\nooo\nooo\noo\nooo\no\nooo\nooo\nooooo oo\no\nooooo\noo\no\noo\noo\nooo\noo\noo\nooo\no\noooo\no\nooo\noooooo\no\no\nooooooo\no\noooo\noooooo\nooo\nooo\no\nooooo\noooo\noooo\noo oooooo\noo\noooo\noo\nooooo\nooo\noo\nooo\noo\noooo\nooo\noo\no\noooo\nooooo\no\noo\no\noo\noo\noo\no\no\nooo\noo\no\nooo\noo\noooo\noo\no\noo\noo\noo\noooooo\noooo\noo\nooooo\nooo\noooo\noo\no\noo\no\nooooo\nooooComponent\n 5PCA ComponentsICA Components\nFIGURE 14.39. A comparison of the \ufb01rst \ufb01ve ICA components computed using\nFastICA (above diagonal) with the \ufb01rst \ufb01ve PCA components(below diagonal) .\nEach component is standardized to have unit variance.\nGaussian as possible. With pre-whitened data, this amounts to looking for\ncomponents that are as independent as possible.\nICA starts from essentially a factor analysis solution, and looks fo r rota-\ntions that lead to independent components. From this point of view, ICA is\njust another factor rotation method, along with the traditional \u201cvarimax \u201d\nand \u201cquartimax\u201d methods used in psychometrics.\nExample: Handwritten Digits\nWe revisit the handwritten threes analyzed by PCA in Section 14.5.1. Fig-\nure 14.39 compares the \ufb01rst \ufb01ve (standardized) principal components with\nthe \ufb01rst \ufb01ve ICA components, all shown in the same standardized units.\nNote that each plot is a two-dimensional projection from a 256-dimensional", "582": "564 14. Unsupervised Learning\nMean ICA 1 ICA 2 ICA 3 ICA 4 ICA 5\nFIGURE 14.40. The highlighted digits from Figure 14.39. By comparing with\nthe mean digits, we see the nature of the ICA component.\nspace. While the PCA components all appear to have joint Gaussian distri-\nbutions, the ICA components have long-tailed distributions. This is not too\nsurprising, since PCA focuses on variance, while ICA speci\ufb01cally looks for\nnon-Gaussian distributions. All the components have been standardized,\nso we do not see the decreasing variances of the principal components.\nFor each ICA component we have highlighted two of the extreme digits,\nas well as a pair of central digits and displayed them in Figure 14.40.\nThis illustrates the nature of each of the components. For example, ICA\ncomponent \ufb01ve picks up the long sweeping tailed threes.\nExample: EEG Time Courses\nICA has become an important tool in the study of brain dynamics\u2014the\nexample we present here uses ICA to untangle the components of signals\nin multi-channel electroencephalographic (EEG) data (Onton and Makeig,\n2006).\nSubjects wear a cap embedded with a lattice of 100 EEG electrodes,\nwhich record brain activity at di\ufb00erent locations on the scalp. Figure 14.4111\n(top panel) shows 15 seconds of output from a subset of nine of these elec-\ntrodes from a subject performing a standard \u201ctwo-back\u201d learning task over\na 30 minute period. The subject is presented with a letter (B, H, J, C, F, or\nK) at roughly 1500-ms intervals, and responds by pressing one of two but-\ntons to indicate whether the letter presented is the same or di\ufb00erent from\nthat presented two steps back. Depending on the answer, the subject earns\nor loses points, and occasionally earns bonus or loses penalty points. The\ntime-course data show spatial correlation in the EEG signals\u2014the signals\nof nearby sensors look very similar.\nThe key assumption here is that signals recorded at each scalp electrode\nare a mixture of independent potentials arising from di\ufb00erent cortical ac-\n11Reprinted from Progress in Brain Research , Vol. 159, Julie Onton and Scott Makeig,\n\u201cInformation based modeling of event-related brain dynami cs,\u201d Page 106 , Copyright\n(2006), with permission from Elsevier. We thank Julie Onton and Scott Makeig for\nsupplying an electronic version of the image.", "583": "14.7 Independent Component Analysis and Exploratory Projection Pursu it 565\ntivities, as well as non-cortical artifact domains; see the reference for a\ndetailed overview of ICA in this domain.\nThe lower part of Figure 14.41 shows a selection of ICA components.\nThe colored images represent the estimated unmixing coe\ufb03cient vectors \u02c6 aj\nas heatmap images superimposed on the scalp, indicating the location of\nactivity. The corresponding time-courses show the activity of the learned\nICA components.\nFor example, the subject blinked after each performance feedback signal\n(colored vertical lines), which accounts for the location and artifact signa l\nin IC1 and IC3. IC12 is an artifact associated with the cardiac pulse. IC4\nand IC7 account for frontal theta-band activities, and appear after a stretch\nof correct performance. See Onton and Makeig (2006) for a more detailed\ndiscussion of this example, and the use of ICA in EEG modeling.\n14.7.3 Exploratory Projection Pursuit\nFriedman and Tukey (1974) proposed exploratory projection pursuit, a\ngraphical exploration technique for visualizing high-dimensional data. Their\nview was that most low (one- or two-dimensional) projections of high-\ndimensional data look Gaussian. Interesting structure, such as clusters or\nlong tails, would be revealed by non-Gaussian projections. They proposed\na number of projection indices for optimization, each focusing on a di\ufb00er-\nent departure from Gaussianity. Since their initial proposal, a variety of\nimprovements have been suggested (Huber, 1985; Friedman, 1987), and a\nvariety of indices, including entropy, are implemented in the interactive\ngraphics package Xgobi (Swayne et al., 1991, now called GGobi). These\nprojection indices are exactly of the same form as J(Yj) above, where\nYj=aT\njX, a normalized linear combination of the components of X. In\nfact, some of the approximations and substitutions for cross-entropy coin-\ncide with indices proposed for projection pursuit. Typically with projection\npursuit, the directions ajare not constrained to be orthogonal. Friedman\n(1987) transforms the data to look Gaussian in the chosen projection, and\nthen searches for subsequent directions. Despite their di\ufb00erent origins, ICA\nand exploratory projection pursuit are quite similar, at least in the repre-\nsentation described here.\n14.7.4 A Direct Approach to ICA\nIndependent components have by de\ufb01nition a joint product density\nfS(s) =p/productdisplay\nj=1fj(sj), (14.88)\nso here we present an approach that estimates this density directly us-\ning generalized additive models (Section 9.1). Full details can be found in", "584": "566 14. Unsupervised Learning\nFIGURE 14.41. Fifteen seconds of EEG data (of 1917seconds) at nine (of\n100) scalp channels (top panel), as well as nine ICA components (lower pa nel).\nWhile nearby electrodes record nearly identical mixtures of bra in and non-brain\nactivity, ICA components are temporally distinct. The colored scalps represent the\nICA unmixing coe\ufb03cients \u02c6ajas a heatmap, showing brain or scalp location of the\nsource.", "585": "14.7 Independent Component Analysis and Exploratory Projection Pursu it 567\nHastie and Tibshirani (2003), and the method is implemented in the R\npackageProDenICA , available from CRAN.\nIn the spirit of representing departures from Gaussianity, we represent\neachfjas\nfj(sj) =\u03c6(sj)egj(sj), (14.89)\natilted Gaussian density. Here \u03c6is the standard Gaussian density, and\ngjsatis\ufb01es the normalization conditions required of a density. Assuming\nas before that Xis pre-whitened, the log-likelihood for the observed data\nX=ASis\n\u2113(A,{gj}p\n1;X) =N/summationdisplay\ni=1p/summationdisplay\nj=1/bracketleftbig\nlog\u03c6j(aT\njxi) +gj(aT\njxi/parenrightbig\n], (14.90)\nwhich we wish to maximize subject to the constraints that Ais orthogonal\nand that the gjresult in densities in (14.89). Without imposing any further\nrestrictions on gj, the model (14.90) is over-parametrized, so we instead\nmaximize a regularized version\np/summationdisplay\nj=1/bracketleft\uf8ecigg\n1\nNN/summationdisplay\ni=1/bracketleftbig\nlog\u03c6(aT\njxi) +gj(aT\njxi)/bracketrightbig\n\u2212/integraldisplay\n\u03c6(t)egj(t)dt\u2212\u03bbj/integraldisplay\n{g\u2032\u2032\u2032\nj(t)}2(t)dt/bracketright\uf8ecigg\n.\n(14.91)\nWe have subtracted two penalty terms (for each j) in (14.91), inspired by\nSilverman (1986, Section 5.4.4):\n\u2022The \ufb01rst enforces the density constraint/integraltext\n\u03c6(t)e\u02c6gj(t)dt= 1 on any\nsolution \u02c6 gj.\n\u2022The second is a roughness penalty, which guarantees that the solution\n\u02c6gjis a quartic-spline with knots at the observed values of sij=aT\njxi.\nIt can further be shown that the solution densities \u02c6fj=\u03c6e\u02c6gjeach have\nmean zero and variance one (Exercise 14.18). As we increase \u03bbj, these\nsolutions approach the standard Gaussian \u03c6.\nAlgorithm 14.3 Product Density ICA Algorithm: ProDenICA\n1. Initialize A(random Gaussian matrix followed by orthogonalization).\n2. Alternate until convergence of A:\n(a) Given A, optimize (14.91) w.r.t. gj(separately for each j).\n(b) Given gj, j= 1,... ,p , perform one step of a \ufb01xed point algo-\nrithm towards \ufb01nding the optimal A.\nWe \ufb01t the functions gjand directions ajby optimizing (14.91) in an\nalternating fashion, as described in Algorithm 14.3.", "586": "568 14. Unsupervised Learning\nStep 2(a) amounts to a semi-parametric density estimation, which can\nbe solved using a novel application of generalized additive models. For\nconvenience we extract one of the pseparate problems,\n1\nNN/summationdisplay\ni=1[log\u03c6(si) +g(si)]\u2212/integraldisplay\n\u03c6(t)eg(t)dt\u2212\u03bb/integraldisplay\n{g\u2032\u2032\u2032(t)}2(t)dt. (14.92)\nAlthough the second integral in (14.92) leads to a smoothing spline, the\n\ufb01rst integral is problematic, and requires an approximation. We construct\na \ufb01ne grid of Lvalues s\u2217\n\u2113in increments \u2206 covering the observed values si,\nand count the number of siin the resulting bins:\ny\u2217\n\u2113=#si\u2208(s\u2217\n\u2113\u2212\u2206/2,s\u2217\n\u2113+ \u2206/2)\nN. (14.93)\nTypically we pick Lto be 1000, which is more than adequate. We can then\napproximate (14.92) by\nL/summationdisplay\n\u2113=1/braceleft\uf8ecig\ny\u2217\ni[log(\u03c6(s\u2217\n\u2113)) +g(s\u2217\n\u2113)]\u2212\u2206\u03c6(s\u2217\n\u2113)eg(s\u2217\n\u2113)/braceright\uf8ecig\n\u2212\u03bb/integraldisplay\ng\u2032\u2032\u20322(s)ds. (14.94)\nThis last expression can be seen to be proportional to a penalized Poisson\nlog-likelihood with response y\u2217\n\u2113/\u2206 and penalty parameter \u03bb/\u2206, and mean\n\u03b8(s) =\u03c6(s)eg(s). This is a generalized additive spline model (Hastie and\nTibshirani, 1990; Efron and Tibshirani, 1996), with an o\ufb00set term log \u03c6(s),\nand can be \ufb01t using a Newton algorithm in O(L) operations. Although\na quartic spline is called for, we \ufb01nd in practice that a cubic spline is\nadequate. We have ptuning parameters \u03bbjto set; in practice we make\nthem all the same, and specify the amount of smoothing via the e\ufb00ective\ndegrees-of-freedom df( \u03bb). Our software uses 5df as a default value.\nStep 2(b) in Algorithm 14.3 requires optimizing (14.92) with respect to\nA, holding the \u02c6 gj\ufb01xed. Only the \ufb01rst terms in the sum involve A, and\nsinceAis orthogonal, the collection of terms involving \u03c6do not depend on\nA(Exercise 14.19). Hence we need to maximize\nC(A) =1\nNp/summationdisplay\nj=1N/summationdisplay\ni=1\u02c6gj(aT\njxi) (14.95)\n=p/summationdisplay\nj=1Cj(aj)\nC(A) is a log-likelihood ratio between the \ufb01tted density and a Gaussian,\nand can be seen as an estimate of negentropy (14.86), with each \u02c6 gja con-\ntrast function as in (14.87). The \ufb01xed point update in step 2(b) is a modi\ufb01ed\nNewton step (Exercise 14.20)", "587": "14.7 Independent Component Analysis and Exploratory Projection Pursu it 569\n1. For each jupdate\naj\u2190E/braceleftbig\nX\u02c6g\u2032\nj(aT\njX)\u2212E[\u02c6g\u2032\u2032\nj(aT\njX)]aj/bracerightbig\n, (14.96)\nwhere E represents expectation w.r.t the sample xi. Since \u02c6 gjis a \ufb01tted\nquartic (or cubic) spline, the \ufb01rst and second derivatives are readily\navailable.\n2. Orthogonalize Ausing the symmetric square-root transformation\n(AAT)\u22121\n2A. IfA=UDVTis the SVD of A, it is easy to show that\nthis leads to the update A\u2190UVT.\nOurProDenICA algorithm works as well as FastICA on the arti\ufb01cial time\nseries data of Figure 14.37, the mixture of uniforms data of Figure 14.38 ,\nand the digit data in Figure 14.39.\nExample: Simulations\na b c\nd e f\ng h i\nj k l\nm n o\np q r\nDistributionAmari Distance from True A\na b c d e f g h i j k l m n o p q r0.01 0.02 0.05 0.10 0.20 0.50FastICA\nKernelICA\nProdDenICA\nFIGURE 14.42. The left panel shows 18distributions used for comparisons.\nThese include the \u201ct\u201d, uniform, exponential, mixtures of exponential s, symmetric\nand asymmetric Gaussian mixtures. The right panel shows (on the log scale)\nthe average Amari metric for each method and each distributio n, based on 30\nsimulations in I R2for each distribution.\nFigure 14.42 shows the results of a simulation comparing ProDenICA to\nFastICA , and another semi-parametric competitor KernelICA (Bach and\nJordan, 2002). The left panel shows the 18 distributions used as a basis\nof comparison. For each distribution, we generated a pair of independent\ncomponents ( N= 1024), and a random mixing matrix in IR2with condition\nnumber between 1 and 2. We used our R implementations of FastICA , using\nthe negentropy criterion (14.87), and ProDenICA . ForKernelICA we used", "588": "570 14. Unsupervised Learning\nthe authors MATLAB code.12Since the search criteria are nonconvex, we\nused \ufb01ve random starts for each method. Each of the algorithms delivers\nan orthogonal mixing matrix A(the data were pre-whitened ), which is\navailable for comparison with the generating orthogonalized mixing matri x\nA0. We used the Amari metric (Bach and Jordan, 2002) as a measure of\nthe closeness of the two frames:\nd(A0,A) =1\n2pp/summationdisplay\ni=1/parenleft\uf8ecigg/summationtextp\nj=1|rij|\nmax j|rij|\u22121/parenright\uf8ecigg\n+1\n2pp/summationdisplay\nj=1/parenleftbigg/summationtextp\ni=1|rij|\nmax i|rij|\u22121/parenrightbigg\n,(14.97)\nwhere rij= (AoA\u22121)ij. The right panel in Figure 14.42 compares the\naverages (on the log scale) of the Amari metric between the truth and the\nestimated mixing matrices. ProDenICA is competitive with FastICA and\nKernelICA in all situations, and dominates most of the mixture simulations.\n14.8 Multidimensional Scaling\nBoth self-organizing maps and principal curves and surfaces map data\npoints in IRpto a lower-dimensional manifold. Multidimensional scaling\n(MDS) has a similar goal, but approaches the problem in a somewhat dif-\nferent way.\nWe start with observations x1,x2,... ,x N\u2208IRp, and let dijbe the dis-\ntance between observations iandj. Often we choose Euclidean distance\ndij=||xi\u2212xj||, but other distances may be used. Further, in some ap-\nplications we may not even have available the data points xi, but only\nhave some dissimilarity measure dij(see Section 14.3.10). For example, in\na wine tasting experiment, dijmight be a measure of how di\ufb00erent a sub-\nject judged wines iandj, and the subject provides such a measure for all\npairs of wines i,j. MDS requires only the dissimilarities dij, in contrast to\nthe SOM and principal curves and surfaces which need the data points xi.\nMultidimensional scaling seeks values z1,z2,... ,z N\u2208IRkto minimize\nthe so-called stress function13\nSM(z1,z2,... ,z N) =/summationdisplay\ni/ne}ationslash=i\u2032(dii\u2032\u2212 ||zi\u2212zi\u2032||)2. (14.98)\nThis is known as least squares orKruskal\u2013Shephard scaling. The idea is\nto \ufb01nd a lower-dimensional representation of the data that preserves the\npairwise distances as well as possible. Notice that the approximation is\n12Francis Bach kindly supplied this code, and helped us set up th e simulations.\n13Some authors de\ufb01ne stress as the square-root of SM; since it does not a\ufb00ect the\noptimization, we leave it squared to make comparisons with o ther criteria simpler.", "589": "14.8 Multidimensional Scaling 571\nin terms of the distances rather than squared distances (which results in\nslightly messier algebra). A gradient descent algorithm is used to minimize\nSM.\nA variation on least squares scaling is the so-called Sammon mapping\nwhich minimizes\nSSm(z1,z2,... ,z N) =/summationdisplay\ni/ne}ationslash=i\u2032(dii\u2032\u2212 ||zi\u2212zi\u2032||)2\ndii\u2032. (14.99)\nHere more emphasis is put on preserving smaller pairwise distances.\nInclassical scaling , we instead start with similarities sii\u2032: often we use\nthe centered inner product sii\u2032=\u221dan}\u230a\u2207a\u230bketle{txi\u2212\u00afx,xi\u2032\u2212\u00afx\u221dan}\u230a\u2207a\u230bket\u2207i}ht. The problem then is to\nminimize\nSC(z1,z2,... ,z N) =/summationdisplay\ni,i\u2032(sii\u2032\u2212 \u221dan}\u230a\u2207a\u230bketle{tzi\u2212\u00afz,zi\u2032\u2212\u00afz\u221dan}\u230a\u2207a\u230bket\u2207i}ht)2(14.100)\noverz1,z2,... ,z N\u2208IRk. This is attractive because there is an explicit\nsolution in terms of eigenvectors: see Exercise 14.11. If we have distances\nrather than inner-products, we can convert them to centered inner-products\nif the distances are Euclidean ;14see (18.31) on page 671 in Chapter 18.\nIf the similarities are in fact centered inner-products, classical scaling is\nexactly equivalent to principal components, an inherently linear dimension-\nreduction technique. Classical scaling is not equivalent to least squares\nscaling; the loss functions are di\ufb00erent, and the mapping can be nonlinear.\nLeast squares and classical scaling are referred to as metric scaling meth-\nods, in the sense that the actual dissimilarities or similarities are appro x-\nimated. Shephard\u2013Kruskal nonmetric scaling e\ufb00ectively uses only ranks.\nNonmetric scaling seeks to minimize the stress function\nSNM(z1,z2,... ,z N) =/summationtext\ni/ne}ationslash=i\u2032/bracketleftbig\n||zi\u2212zi\u2032|| \u2212\u03b8(dii\u2032)/bracketrightbig2\n/summationtext\ni/ne}ationslash=i\u2032||zi\u2212zi\u2032||2(14.101)\nover the ziand an arbitrary increasing function \u03b8. With \u03b8\ufb01xed, we min-\nimize over ziby gradient descent. With the zi\ufb01xed, the method of iso-\ntonic regression is used to \ufb01nd the best monotonic approximation \u03b8(dii\u2032)\nto||zi\u2212zi\u2032||. These steps are iterated until the solutions stabilize.\nLike the self-organizing map and principal surfaces, multidimensional\nscaling represents high-dimensional data in a low-dimensional coordinate\nsystem. Principal surfaces and SOMs go a step further, and approximate\nthe original data by a low-dimensional manifold, parametrized in the low\ndimensional coordinate system. In a principal surface and SOM, points\n14AnN\u00d7Ndistance matrix is Euclidean if the entries represent pairw ise Euclidean\ndistances between Npoints in some dimensional space.", "590": "572 14. Unsupervised Learning\nFirst MDS CoordinateSecond MDS Coordinate\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\nFIGURE 14.43. First two coordinates for half-sphere data, from classical m ulti-\ndimensional scaling.\nclose together in the original feature space should map close together on\nthe manifold, but points far apart in feature space might also map close\ntogether. This is less likely in multidimensional scaling since it explicitly\ntries to preserve all pairwise distances.\nFigure 14.43 shows the \ufb01rst two MDS coordinates from classical scaling\nfor the half-sphere example. There is clear separation of the clusters, and\nthe tighter nature of the red cluster is apparent.\n14.9 Nonlinear Dimension Reduction and Local\nMultidimensional Scaling\nSeveral methods have been recently proposed for nonlinear dimension re-\nduction, similar in spirit to principal surfaces. The idea is that the data lie\nclose to an intrinsically low-dimensional nonlinear manifold embedded in a\nhigh-dimensional space. These methods can be thought of as \u201c\ufb02attening\u201d\nthe manifold, and hence reducing the data to a set of low-dimensional co-\nordinates that represent their relative positions in the manifold. They are\nuseful for problems where signal-to-noise ratio is very high (e.g., physical\nsystems), and are probably not as useful for observational data with lower\nsignal-to-noise ratios.\nThe basic goal is illustrated in the left panel of Figure 14.44. The data\nlie near a parabola with substantial curvature. Classical MDS does not pre-", "591": "14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling 573\n\u22125 0 5\u221215 \u221210 \u22125 0Classical MDS\n\u22125 0 5\u221215 \u221210 \u22125 0Local MDS\nx1 x1\nx2x2\nFIGURE 14.44. The orange points show data lying on a parabola, while the blue\npoints shows multidimensional scaling representations in one dime nsion. Classical\nmultidimensional scaling (left panel) does not preserve the orde ring of the points\nalong the curve, because it judges points on opposite ends of the curve to be close\ntogether. In contrast, local multidimensional scaling (right p anel) does a good job\nof preserving the ordering of the points along the curve.\nserve the ordering of the points along the curve, because it judges points\non opposite ends of the curve to be close together. The right panel shows\nthe results of local multi-dimensional scaling , one of the three methods for\nnon-linear multi-dimensional scaling that we discuss below. These meth-\nods use only the coordinates of the points in pdimensions, and have no\nother information about the manifold. Local MDS has done a good job of\npreserving the ordering of the points along the curve.\nWe now brie\ufb02y describe three new approaches to nonlinear dimension\nreduction and manifold mapping.\nIsometric feature mapping (ISOMAP) (Tenenbaum et al., 2000) con-\nstructs a graph to approximate the geodesic distance between points along\nthe manifold. Speci\ufb01cally, for each data point we \ufb01nd its neighbors\u2014points\nwithin some small Euclidean distance of that point. We construct a graph\nwith an edge between any two neighboring points. The geodesic distance\nbetween any two points is then approximated by the shortest path be-\ntween points on the graph. Finally, classical scaling is applied to the graph\ndistances, to produce a low-dimensional mapping.\nLocal linear embedding (Roweis and Saul, 2000) takes a very di\ufb00erent ap-\nproach, trying to preserve the local a\ufb03ne structure of the high-dimensional\ndata. Each data point is approximated by a linear combination of neigh-\nboring points. Then a lower dimensional representation is constructed that", "592": "574 14. Unsupervised Learning\nbest preserves these local approximations. The details are interesting, so\nwe give them here.\n1. For each data point xiinpdimensions, we \ufb01nd its K-nearest neigh-\nborsN(i) in Euclidean distance.\n2. We approximate each point by an a\ufb03ne mixture of the points in its\nneighborhood:\nmin\nWik||xi\u2212/summationdisplay\nk\u2208N(i)wikxk||2(14.102)\nover weights wiksatisfying wik= 0, k /\u2208 N(i),/summationtextN\nk=1wik= 1.wik\nis the contribution of point kto the reconstruction of point i. Note\nthat for a hope of a unique solution, we must have K < p .\n3. Finally, we \ufb01nd points yiin a space of dimension d < p to minimize\nN/summationdisplay\ni=1||yi\u2212N/summationdisplay\nk=1wikyk||2(14.103)\nwithwik\ufb01xed.\nIn step 3, we minimize\ntr[(Y\u2212WY)T(Y\u2212WY)] = tr[ YT(I\u2212W)T(I\u2212W)Y] (14.104)\nwhere WisN\u00d7N;YisN\u00d7d, for some small d < p . The solutions \u02c6Y\nare the trailing eigenvectors of M= (I\u2212W)T(I\u2212W). Since 1is a trivial\neigenvector with eigenvalue 0, we discard it and keep the next d. This has\nthe side e\ufb00ect that 1TY= 0, and hence the embedding coordinates are\nmean centered.\nLocal MDS (Chen and Buja, 2008) takes the simplest and arguably the\nmost direct approach. We de\ufb01ne Nto be the symmetric set of nearby pairs\nof points; speci\ufb01cally a pair ( i,i\u2032) is in Nif point iis among the K-nearest\nneighbors of i\u2032, or vice-versa. Then we construct the stress function\nSL(z1,z2,... ,z N) =/summationdisplay\n(i,i\u2032)\u2208N(dii\u2032\u2212 ||zi\u2212zi\u2032||)2\n+/summationdisplay\n(i,i\u2032)/\u2208Nw\u2264(D\u2212 ||zi\u2212zi\u2032||)2.(14.105)\nHereDis some large constant and wis a weight. The idea is that points\nthat are not neighbors are considered to be very far apart; such pairs are\ngiven a small weight wso that they don\u2019t dominate the overall stress func-\ntion. To simplify the expression, we take w\u223c1/D, and let D\u2192 \u221e .\nExpanding (14.105), this gives", "593": "14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling 575\nFIGURE 14.45. Images of faces mapped into the embedding space described by\nthe \ufb01rst two coordinates of LLE. Next to the circled points, repre sentative faces\nare shown in di\ufb00erent parts of the space. The images at the bott om of the plot\ncorrespond to points along the top right path (linked by solid line ), and illustrate\none particular mode of variability in pose and expression.", "594": "576 14. Unsupervised Learning\nSL(z1,z2,... ,z N) =/summationdisplay\n(i,i\u2032)\u2208N(dii\u2032\u2212 ||zi\u2212zi\u2032||)2\u2212\u03c4/summationdisplay\n(i,i\u2032)/\u2208N||zi\u2212zi\u2032||,\n(14.106)\nwhere \u03c4= 2wD. The \ufb01rst term in (14.106) tries to preserve local structure\nin the data, while the second term encourages the representations zi,zi\u2032\nfor pairs ( i,i\u2032) that are non-neighbors to be farther apart. Local MDS\nminimizes the stress function (14.106) over zi, for \ufb01xed values of the number\nof neighbors Kand the tuning parameter \u03c4.\nThe right panel of Figure 14.44 shows the result of local MDS, using k= 2\nneighbors and \u03c4= 0.01. We used coordinate descent with multiple starting\nvalues to \ufb01nd a good minimum of the (nonconvex) stress function (14.106).\nThe ordering of the points along the curve has been largely preserved,\nFigure 14.45 shows a more interesting application of one of these meth-\nods (LLE)15. The data consist of 1965 photographs, digitized as 20 \u00d728\ngrayscale images. The result of the \ufb01rst two-coordinates of LLE are shown\nand reveal some variability in pose and expression. Similar pictures were\nproduced by local MDS.\nIn experiments reported in Chen and Buja (2008), local MDS shows su-\nperior performance, as compared to ISOMAP and LLE. They also demon-\nstrate the usefulness of local MDS for graph layout. There are also close\nconnections between the methods discussed here, spectral clustering (Sec-\ntion 14.5.3) and kernel PCA (Section 14.5.4).\n14.10 The Google PageRank Algorithm\nIn this section we give a brief description of the original PageRank algo-\nrithm used by the Google search engine, an interesting recent application\nof unsupervised learning methods.\nWe suppose that we have Nweb pages and wish to rank them in terms\nof importance. For example, the Npages might all contain a string match\nto \u201cstatistical learning\u201d and we might wish to rank the pages in terms of\ntheir likely relevance to a websurfer.\nThePageRank algorithm considers a webpage to be important if many\nother webpages point to it. However the linking webpages that point to a\ngiven page are not treated equally: the algorithm also takes into account\nboth the importance ( PageRank ) of the linking pages and the number of\noutgoing links that they have. Linking pages with higher PageRank are\ngiven more weight, while pages with more outgoing links are given less\nweight. These ideas lead to a recursive de\ufb01nition for PageRank , detailed\nnext.\n15Sam Roweis and Lawrence Saul kindly provided this \ufb01gure.", "595": "14.10 The Google PageRank Algorithm 577\nLetLij= 1 if page jpoints to page i, and zero otherwise. Let cj=/summationtextN\ni=1Lijequal the number of pages pointed to by page j(number of out-\nlinks). Then the Google PageRanks piare de\ufb01ned by the recursive rela-\ntionship\npi= (1\u2212d) +dN/summationdisplay\nj=1/parenleftbigLij\ncj/parenrightbig\npj (14.107)\nwhere dis a positive constant (apparently set to 0.85).\nThe idea is that the importance of page iis the sum of the importances of\npages that point to that page. The sums are weighted by 1 /cj, that is, each\npage distributes a total vote of 1 to other pages. The constant densures\nthat each page gets a PageRank of at least 1 \u2212d. In matrix notation\np= (1\u2212d)e+d\u2264LD\u22121\ncp (14.108)\nwhere eis a vector of Nones and Dc= diag( c) is a diagonal matrix with\ndiagonal elements cj. Introducing the normalization eTp=N(i.e., the\naverage PageRank is 1), we can write (14.108) as\np=/bracketleftbig\n(1\u2212d)eeT/N+dLD\u22121\nc/bracketrightbig\np\n=Ap (14.109)\nwhere the matrix Ais the expression in square braces.\nExploiting a connection with Markov chains (see below), it can be shown\nthat the matrix Ahas a real eigenvalue equal to one, and one is its largest\neigenvalue. This means that we can \ufb01nd \u02c6pby the power method: starting\nwith some p=p0we iterate\npk\u2190Apk\u22121;pk\u2190Npk\neTpk. (14.110)\nThe \ufb01xed points \u02c6pare the desired PageRanks .\nIn the original paper of Page et al. (1998), the authors considered PageR-\nankas a model of user behavior, where a random web surfer clicks on links\nat random, without regard to content. The surfer does a random walk on\nthe web, choosing among available outgoing links at random. The factor\n1\u2212dis the probability that he does not click on a link, but jumps instead\nto a random webpage.\nSome descriptions of PageRank have (1 \u2212d)/Nas the \ufb01rst term in def-\ninition (14.107), which would better coincide with the random surfer in-\nterpretation. Then the page rank solution (divided by N) is the stationary\ndistribution of an irreducible, aperiodic Markov chain over the Nwebpages.\nDe\ufb01nition (14.107) also corresponds to an irreducible, aperiodic Markov\nchain, with di\ufb00erent transition probabilities than those from he (1 \u2212d)/N\nversion. Viewing PageRank as a Markov chain makes clear why the matrix\nAhas a maximal real eigenvalue of 1. Since Ahas positive entries with", "596": "578 14. Unsupervised Learning\nPage 2\nPage 3Page 4Page 1\nFIGURE 14.46. PageRank algorithm: example of a small network\neach column summing to one, Markov chain theory tells us that it has a\nunique eigenvector with eigenvalue one, corresponding to the stationary\ndistribution of the chain (Bremaud, 1999).\nA small network is shown for illustration in Figure 14.46. The link ma trix\nis\nL=\uf8eb\n\uf8ec\uf8ec\uf8ed0 0 1 0\n1 0 0 0\n1 1 0 1\n0 0 0 0\uf8f6\n\uf8f7\uf8f7\uf8f8(14.111)\nand the number of outlinks is c= (2,1,1,1).\nThePageRank solution is \u02c6p= (1.49,0.78,1.58,0.15). Notice that page 4\nhas no incoming links, and hence gets the minimum PageRank of 0.15.\nBibliographic Notes\nThere are many books on clustering, including Hartigan (1975), Gordon\n(1999) and Kaufman and Rousseeuw (1990). K-means clustering goes back\nat least to Lloyd (1957), Forgy (1965), Jancey (1966) and MacQueen (1967 ).\nApplications in engineering, especially in image compression via vector\nquantization, can be found in Gersho and Gray (1992). The k-medoid pro-\ncedure is described in Kaufman and Rousseeuw (1990). Association rules\nare outlined in Agrawal et al. (1995). The self-organizing map was propos ed\nby Kohonen (1989) and Kohonen (1990); Kohonen et al. (2000) give a more\nrecent account. Principal components analysis and multidimensional scal-\ning are described in standard books on multivariate analysis, for exampl e,\nMardia et al. (1979). Buja et al. (2008) have implemented a powerful en-\nvironment called Ggvis for multidimensional scaling, and the user manual", "597": "Exercises 579\ncontains a lucid overview of the subject. Figures 14.17, 14.21 (left panel)\nand 14.28 (left panel) were produced in Xgobi, a multidimensional data\nvisualization package by the same authors. GGobi is a more recent im-\nplementation (Cook and Swayne, 2007). Goodall (1991) gives a technical\noverview of Procrustes methods in statistics, and Ramsay and Silverman\n(1997) discuss the shape registration problem. Principal curves and surfaces\nwere proposed in Hastie (1984) and Hastie and Stuetzle (1989). The idea of\nprincipal points was formulated in Flury (1990), Tarpey and Flury (1996 )\ngive an exposition of the general concept of self-consistency. An excellent\ntutorial on spectral clustering can be found in von Luxburg (2007); this was\nthe main source for Section 14.5.3. Luxborg credits Donath and Ho\ufb00man\n(1973) and Fiedler (1973) with the earliest work on the subject. A history\nof spectral clustering my be found in Spielman and Teng (1996). Indepen-\ndent component analysis was proposed by Comon (1994), with subsequent\ndevelopments by Bell and Sejnowski (1995); our treatment in Section 14.7\nis based on Hyv\u00a8 arinen and Oja (2000). Projection pursuit was proposed by\nFriedman and Tukey (1974), and is discussed in detail in Huber (1985). A\ndynamic projection pursuit algorithm is implemented in GGobi.\nExercises\nEx. 14.1 Weights for clustering . Show that weighted Euclidean distance\nd(w)\ne(xi,xi\u2032) =/summationtextp\nl=1wl(xil\u2212xi\u2032l)2\n/summationtextp\nl=1wl\nsatis\ufb01es\nd(w)\ne(xi,xi\u2032) =de(zi,zi\u2032) =p/summationdisplay\nl=1(zil\u2212zi\u2032l)2, (14.112)\nwhere\nzil=xil\u2264/parenleftbiggwl/summationtextp\nl=1wl/parenrightbigg1/2\n. (14.113)\nThus weighted Euclidean distance based on xis equivalent to unweighted\nEuclidean distance based on z.\nEx. 14.2 Consider a mixture model density in p-dimensional feature space,\ng(x) =K/summationdisplay\nk=1\u03c0kgk(x), (14.114)\nwhere gk=N(\u03b8k,L\u2264\u03c32) and \u03c0k\u22650\u2200kwith/summationtext\nk\u03c0k= 1. Here {\u03b8k,\u03c0k},k=\n1,... ,K and\u03c32are unknown parameters.", "598": "580 14. Unsupervised Learning\nSuppose we have data x1,x2,... ,x N\u223cg(x) and we wish to \ufb01t the mix-\nture model.\n1. Write down the log-likelihood of the data\n2. Derive an EM algorithm for computing the maximum likelihood es-\ntimates (see Section 8.1).\n3. Show that if \u03c3has a known value in the mixture model and we take\n\u03c3\u21920, then in a sense this EM algorithm coincides with K-means\nclustering.\nEx. 14.3 In Section 14.2.6 we discuss the use of CART or PRIM for con-\nstructing generalized association rules. Show that a problem occurs with ei-\nther of these methods when we generate the random data from the product-\nmarginal distribution; i.e., by randomly permuting the values for each of\nthe variables. Propose ways to overcome this problem.\nEx. 14.4 Cluster the demographic data of Table 14.1 using a classi\ufb01cation\ntree. Speci\ufb01cally, generate a reference sample of the same size of the train-\ning set, by randomly permuting the values within each feature. Build a\nclassi\ufb01cation tree to the training sample (class 1) and the reference sample\n(class 0) and describe the terminal nodes having highest estimated class 1\nprobability. Compare the results to the PRIM results near Table 14.1 and\nalso to the results of K-means clustering applied to the same data.\nEx. 14.5 Generate data with three features, with 30 data points in each of\nthree classes as follows:\n\u03b81=U(\u2212\u03c0/8, \u03c0/8)\n\u03c61=U(0,2\u03c0)\nx1= sin( \u03b81)cos(\u03c61) +W11\ny1= sin( \u03b81)sin(\u03c61) +W12\nz1= cos( \u03b81) +W13\n\u03b82=U(\u03c0/2\u2212\u03c0/4, \u03c0/2 +\u03c0/4)\n\u03c62=U(\u2212\u03c0/4, \u03c0/4)\nx2= sin( \u03b82)cos(\u03c62) +W21\ny2= sin( \u03b82)sin(\u03c62) +W22\nz2= cos( \u03b82) +W23\n\u03b83=U(\u03c0/2\u2212\u03c0/4, \u03c0/2 +\u03c0/4)\n\u03c63=U(\u03c0/2\u2212\u03c0/4, \u03c0/2 +\u03c0/4)\nx3= sin( \u03b83)cos(\u03c63) +W31\ny3= sin( \u03b83)sin(\u03c63) +W32\nz3= cos( \u03b83) +W33\nHereU(a,b) indicates a uniform variate on the range [ a,b] and Wjkare\nindependent normal variates with standard deviation 0 .6. Hence the data", "599": "Exercises 581\nlie near the surface of a sphere in three clusters centered at (1 ,0,0), (0,1,0)\nand (0 ,0,1).\nWrite a program to \ufb01t a SOM to these data, using the learning rates\ngiven in the text. Carry out a K-means clustering of the same data, and\ncompare the results to those in the text.\nEx. 14.6 Write programs to implement K-means clustering and a self-\norganizing map (SOM), with the prototype lying on a two-dimensional\ngrid. Apply them to the columns of the human tumor microarray data, us-\ningK= 2,5,10,20 centroids for both. Demonstrate that as the size of the\nSOM neighborhood is taken to be smaller and smaller, the SOM solution\nbecomes more similar to the K-means solution.\nEx. 14.7 Derive (14.51) and (14.52) in Section 14.5.1. Show that \u02c6 \u03b8is not\nunique, and characterize the family of equivalent solutions.\nEx. 14.8 Derive the solution (14.57) to the Procrustes problem (14.56).\nDerive also the solution to the Procrustes problem with scaling (14.58).\nEx. 14.9 Write an algorithm to solve\nmin\n{\u03b2\u2113,R\u2113}L\n1,ML/summationdisplay\n\u2113=1||X\u2113R\u2113\u2212M||2\nF. (14.115)\nApply it to the three S\u2019s, and compare the results to those shown in Fig-\nure 14.26.\nEx. 14.10 Derive the solution to the a\ufb03ne-invariant average problem (14.60).\nApply it to the three S\u2019s, and compare the results to those computed in\nExercise 14.9.\nEx. 14.11 Classical multidimensional scaling. LetSbe the centered in-\nner product matrix with elements \u221dan}\u230a\u2207a\u230bketle{txi\u2212\u00afx,xj\u2212\u00afx\u221dan}\u230a\u2207a\u230bket\u2207i}ht. Let \u03bb1> \u03bb2>\u2264\u2264\u2264>\n\u03bbkbe the klargest eigenvalues of S, with associated eigenvectors Ek=\n(e1,e2,... ,ek). Let Dkbe a diagonal matrix with diagonal entries\u221a\u03bb1,\u221a\u03bb2,... ,\u221a\u03bbk. Show that the solutions zito the classical scaling problem\n(14.100) are the rowsofEkDk.\nEx. 14.12 Consider the sparse PCA criterion (14.71).\n1. Show that with \u0398\ufb01xed, solving for Vamounts to Kseparate elastic-\nnet regression problems, with responses the Kelements of \u0398Txi.\n2. Show that with V\ufb01xed, solving for \u0398amounts to a reduced-rank\nversion of the Procrustes problem, which reduces to\nmax\n\u0398trace(\u0398TM) subject to \u0398T\u0398=IK, (14.116)\nwhere Mand\u0398are both p\u00d7KwithK\u2264p. IfM=UDQTis the\nSVD of M, show that the optimal \u0398=UQT.", "600": "582 14. Unsupervised Learning\nEx. 14.13 Generate 200 data points with three features, lying close to a\nhelix. In detail, de\ufb01ne X1= cos( s) + 0.1\u2264Z1,X2= sin( s) + 0.1\u2264Z2,X3=\ns+ 0.1\u2264Z3where stakes on 200 equally spaced values between 0 and 2 \u03c0,\nandZ1,Z2,Z3are independent and have standard Gaussian distributions.\n(a) Fit a principal curve to the data and plot the estimated coordinate\nfunctions. Compare them to the underlying functions cos( s),sin(s)\nands.\n(b) Fit a self-organizing map to the same data, and see if you can discover\nthe helical shape of the original point cloud.\nEx. 14.14 Pre- and post-multiply equation (14.81) by a diagonal matrix\ncontaining the inverse variances of the Xj. Hence obtain an equivalent\ndecomposition for the correlation matrix, in the sense that a simple scali ng\nis applied to the matrix A.\nEx. 14.15 Generate 200 observations of three variates X1,X2,X3according\nto\nX1\u223cZ1\nX2=X1+ 0.001\u2264Z2\nX3= 10 \u2264Z3 (14.117)\nwhere Z1,Z2,Z3are independent standard normal variates. Compute the\nleading principal component and factor analysis directions. Hence show\nthat the leading principal component aligns itself in the maximal variance\ndirection X3, while the leading factor essentially ignores the uncorrelated\ncomponent X3, and picks up the correlated component X2+X1(Geo\ufb00rey\nHinton, personal communication).\nEx. 14.16 Consider the kernel principal component procedure outlined in\nSection 14.5.4. Argue that the number Mof principal components is equal\nto the rank of K, which is the number of non-zero elements in D. Show\nthat the mth component zm(mth column of Z) can be written (up to\ncentering) as zim=/summationtextN\nj=1\u03b1jmK(xi,xj), where \u03b1jm=ujm/dm. Show that\nthe mapping of a new observation x0to the mth component is given by\nz0m=/summationtextN\nj=1\u03b1jmK(x0,xj).\nEx. 14.17 Show that with g1(x) =/summationtextN\nj=1cjK(x,xj), the solution to (14.66)\nis given by \u02c6 cj=uj1/d1, where u1is the \ufb01rst column of Uin (14.65), and\nd1the \ufb01rst diagonal element of D. Show that the second and subsequent\nprincipal component functions are de\ufb01ned in a similar manner ( hint: see\nSection 5.8.1.)\nEx. 14.18 Consider the regularized log-likelihood for the density estimation\nproblem arising in ICA,", "601": "Exercises 583\n1\nNN/summationdisplay\ni=1[log\u03c6(si) +g(si)]\u2212/integraldisplay\n\u03c6(t)eg(t)dt\u2212\u03bb/integraldisplay\n{g\u2032\u2032\u2032(t)}2(t)dt.(14.118)\nThe solution \u02c6 gis a quartic smoothing spline, and can be written as \u02c6 g(s) =\n\u02c6q(s) + \u02c6q\u22a5(s), where qis a quadratic function (in the null space of the\npenalty). Let q(s) =\u03b80+\u03b81s+\u03b82s2. By examining the stationarity condi-\ntions for \u02c6\u03b8k, k= 1,2,3, show that the solution \u02c6f=\u03c6e\u02c6gis a density, and\nhas mean zero and variance one. If we used a second-derivative penalty/integraltext\n{g\u2032\u2032(t)}2(t)dtinstead, what simple modi\ufb01cation could we make to the\nproblem to maintain the three moment conditions?\nEx. 14.19 IfAisp\u00d7porthogonal, show that the \ufb01rst term in (14.92) on\npage 568\np/summationdisplay\nj=1N/summationdisplay\ni=1log\u03c6(aT\njxi),\nwithajthejth column of A, does not depend on A.\nEx. 14.20 Fixed point algorithm for ICA (Hyv\u00a8 arinen et al., 2001). Consider\nmaximizing C(a) =E{g(aTX)}with respect to a, with ||a||= 1 and\nCov(X) =I. Use a Lagrange multiplier to enforce the norm constraint,\nand write down the \ufb01rst two derivatives of the modi\ufb01ed criterion. Use the\napproximation\nE{XXTg\u2032\u2032(aTX)} \u2248E{XXT}E{g\u2032\u2032(aTX)}\nto show that the Newton update can be written as the \ufb01xed-point update\n(14.96).\nEx. 14.21 Consider an undirected graph with non-negative edge weights\nwii\u2032and graph Laplacian L. Suppose there are mconnected components\nA1,A2,... ,A min the graph. Show that there are meigenvectors of Lcorre-\nsponding to eigenvalue zero, and the indicator vectors of these components\nIA1,IA2,... ,I Amspan the zero eigenspace.\nEx. 14.22\n(a) Show that de\ufb01nition (14.108) implies that the sum of the PageRanks\npiisN, the number of web pages.\n(b) Write a program to compute the PageRank solutions by the power\nmethod using formulation (14.107). Apply it to the network of Fig-\nure 14.47.\nEx. 14.23 Algorithm for non-negative matrix factorization (Wu and Lange,\n2007). A function g(x,y) to said to minorize a function f(x) if", "602": "584 14. Unsupervised Learning\nPage 2Page 1\nPage 3\nPage 4\nPage 5Page 6\nFIGURE 14.47. Example of a small network.\ng(x,y)\u2264f(x), g(x,x) =f(x) (14.119)\nfor all x,yin the domain. This is useful for maximizing f(x) since it is easy\nto show that f(x) is nondecreasing under the update\nxs+1= argmaxxg(x,xs) (14.120)\nThere are analogous de\ufb01nitions for majorization , for minimizing a function\nf(x). The resulting algorithms are known as MMalgorithms, for \u201cminorize-\nmaximize\u201d or \u201cmajorize-minimize\u201d (Lange, 2004). It also can be shown\nthat the EM algorithm (8.5) is an example of an MM algorithm: see Sec-\ntion 8.5.3 and Exercise 8.2 for details.\n(a) Consider maximization of the function L(W,H) in (14.73), written\nhere without the matrix notation\nL(W,H) =N/summationdisplay\ni=1p/summationdisplay\nj=1/bracketleft\uf8ecigg\nxijlog/parenleft\uf8eciggr/summationdisplay\nk=1wikhkj/parenright\uf8ecigg\n\u2212r/summationdisplay\nk=1wikhkj/bracketright\uf8ecigg\n.\nUsing the concavity of log( x), show that for any set of rvalues yk\u22650\nand 0 \u2264ck\u22641 with/summationtextr\nk=1ck= 1,\nlog/parenleft\uf8eciggr/summationdisplay\nk=1yk/parenright\uf8ecigg\n\u2265r/summationdisplay\nk=1cklog(yk/ck)\nHence\nlog/parenleft\uf8eciggr/summationdisplay\nk=1wikhkj/parenright\uf8ecigg\n\u2265r/summationdisplay\nk=1as\nikj\nbs\nijlog/parenleft\uf8ecigg\nbs\nij\nas\nikjwikhkj/parenright\uf8ecigg\n,\nwhere\nas\nikj=ws\nikhs\nkjandbs\nij=r/summationdisplay\nk=1ws\nikhs\nkj,\nandsindicates the current iteration.", "603": "Exercises 585\n(b) Hence show that, ignoring constants, the function\ng(W,H|Ws,Hs) =N/summationdisplay\ni=1p/summationdisplay\nj=1r/summationdisplay\nk=1uijas\nikj\nbs\nij/parenleft\uf8ecig\nlogwik+ loghkj/parenright\uf8ecig\n\u2212N/summationdisplay\ni=1p/summationdisplay\nj=1r/summationdisplay\nk=1wikhkj\nminorizes L(W,H).\n(c) Set the partial derivatives of g(W,H|Ws,Hs) to zero and hence\nderive the updating steps (14.74).\nEx. 14.24 Consider the non-negative matrix factorization (14.72) in the\nrank one case ( r= 1).\n(a) Show that the updates (14.74) reduce to\nwi\u2190wi/summationtextp\nj=1xij/summationtextp\nj=1wihj\nhj\u2190hj/summationtextN\ni=1xij/summationtextN\ni=1wihj(14.121)\nwhere wi=wi1,hj=h1j. This is an example of the iterative pro-\nportional scaling procedure, applied to the independence model for a\ntwo-way contingency table (Fienberg, 1977, for example).\n(b) Show that the \ufb01nal iterates have the explicit form\nwi=c\u2264/summationtextp\nj=1xij/summationtextN\ni=1/summationtextp\nj=1xij, h k=1\nc\u2264/summationtextN\ni=1xik/summationtextN\ni=1/summationtextp\nj=1xij(14.122)\nfor any constant c >0. These are equivalent to the usual row and\ncolumn estimates for a two-way independence model.\nEx. 14.25 Fit a non-negative matrix factorization model to the collection\nof two\u2019s in the digits database. Use 25 basis elements, and compare with a\n24- component (plus mean) PCA model. In both cases display the Wand\nHmatrices as in Figure 14.33.", "604": "586 14. Unsupervised Learning", "605": "This is page 587\nPrinter: Opaque this\n15\nRandom Forests\n15.1 Introduction\nBagging or bootstrap aggregation (section 8.7) is a technique for reducing\nthe variance of an estimated prediction function. Bagging seems to work\nespecially well for high-variance, low-bias procedures, such as trees. For\nregression, we simply \ufb01t the same regression tree many times to bootstra p-\nsampled versions of the training data, and average the result. For classi\ufb01-\ncation, a committee of trees each cast a vote for the predicted class.\nBoosting in Chapter 10 was initially proposed as a committee method as\nwell, although unlike bagging, the committee of weak learners evolves over\ntime, and the members cast a weighted vote. Boosting appears to dominate\nbagging on most problems, and became the preferred choice.\nRandom forests (Breiman, 2001) is a substantial modi\ufb01cation of bagging\nthat builds a large collection of de-correlated trees, and then averages them.\nOn many problems the performance of random forests is very similar to\nboosting, and they are simpler to train and tune. As a consequence, random\nforests are popular, and are implemented in a variety of packages.\n15.2 De\ufb01nition of Random Forests\nThe essential idea in bagging (Section 8.7) is to average many noisy but\napproximately unbiased models, and hence reduce the variance. Trees are\nideal candidates for bagging, since they can capture complex interaction", "606": "588 15. Random Forests\nAlgorithm 15.1 Random Forest for Regression or Classi\ufb01cation.\n1. For b= 1 to B:\n(a) Draw a bootstrap sample Z\u2217of size Nfrom the training data.\n(b) Grow a random-forest tree Tbto the bootstrapped data, by re-\ncursively repeating the following steps for each terminal node of\nthe tree, until the minimum node size nminis reached.\ni. Select mvariables at random from the pvariables.\nii. Pick the best variable/split-point among the m.\niii. Split the node into two daughter nodes.\n2. Output the ensemble of trees {Tb}B\n1.\nTo make a prediction at a new point x:\nRegression: \u02c6fB\nrf(x) =1\nB/summationtextB\nb=1Tb(x).\nClassi\ufb01cation: Let\u02c6Cb(x) be the class prediction of the bth random-forest\ntree. Then \u02c6CB\nrf(x) =majority vote {\u02c6Cb(x)}B\n1.\nstructures in the data, and if grown su\ufb03ciently deep, have relatively low\nbias. Since trees are notoriously noisy, they bene\ufb01t greatly from the averag-\ning. Moreover, since each tree generated in bagging is identically distributed\n(i.d.), the expectation of an average of Bsuch trees is the same as the ex-\npectation of any one of them. This means the bias of bagged trees is the\nsame as that of the individual trees, and the only hope of improvement is\nthrough variance reduction. This is in contrast to boosting, where the trees\nare grown in an adaptive way to remove bias, and hence are not i.d.\nAn average of Bi.i.d. random variables, each with variance \u03c32, has vari-\nance1\nB\u03c32. If the variables are simply i.d. (identically distributed, but not\nnecessarily independent) with positive pairwise correlation \u03c1, the variance\nof the average is (Exercise 15.1)\n\u03c1\u03c32+1\u2212\u03c1\nB\u03c32. (15.1)\nAsBincreases, the second term disappears, but the \ufb01rst remains, and\nhence the size of the correlation of pairs of bagged trees limits the bene\ufb01ts\nof averaging. The idea in random forests (Algorithm 15.1) is to improve\nthe variance reduction of bagging by reducing the correlation between the\ntrees, without increasing the variance too much. This is achieved in the\ntree-growing process through random selection of the input variables.\nSpeci\ufb01cally, when growing a tree on a bootstrapped dataset:\nBefore each split, select m\u2264pof the input variables at random\nas candidates for splitting.", "607": "15.2 De\ufb01nition of Random Forests 589\nTypically values for mare\u221apor even as low as 1.\nAfter Bsuch trees {T(x;\u0398b)}B\n1are grown, the random forest (regression)\npredictor is\n\u02c6fB\nrf(x) =1\nBB/summationdisplay\nb=1T(x;\u0398b). (15.2)\nAs in Section 10.9 (page 356), \u0398 bcharacterizes the bth random forest tree in\nterms of split variables, cutpoints at each node, and terminal-node values.\nIntuitively, reducing mwill reduce the correlation between any pair of trees\nin the ensemble, and hence by (15.1) reduce the variance of the average.\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060 0.065 0.070Spam Data\nNumber of TreesTest ErrorBagging\nRandom Forest\nGradient Boosting (5 Node)\nFIGURE 15.1. Bagging, random forest, and gradient boosting, applied to the\nspam data. For boosting, 5-node trees were used, and the number of trees were\nchosen by 10-fold cross-validation ( 2500trees). Each \u201cstep\u201d in the \ufb01gure corre-\nsponds to a change in a single misclassi\ufb01cation (in a test set of 1536).\nNot all estimators can be improved by shaking up the data like this.\nIt seems that highly nonlinear estimators, such as trees, bene\ufb01t the most.\nFor bootstrapped trees, \u03c1is typically small (0 .05 or lower is typical; see\nFigure 15.9), while \u03c32is not much larger than the variance for the original\ntree. On the other hand, bagging does not change linear estimates, such\nas the sample mean (hence its variance either); the pairwise correlation\nbetween bootstrapped means is about 50% (Exercise 15.4).", "608": "590 15. Random Forests\nRandom forests are popular. Leo Breiman\u2019s1collaborator Adele Cutler\nmaintains a random forest website2where the software is freely available,\nwith more than 3000 downloads reported by 2002. There is a randomForest\npackage in R, maintained by Andy Liaw, available from the CRANwebsite.\nThe authors make grand claims about the success of random forests:\n\u201cmost accurate,\u201d \u201cmost interpretable,\u201d and the like. In our experience ran-\ndom forests do remarkably well, with very little tuning required. A ran-\ndom forest classi\ufb01er achieves 4 .88% misclassi\ufb01cation error on the spamtest\ndata, which compares well with all other methods, and is not signi\ufb01cantly\nworse than gradient boosting at 4 .5%. Bagging achieves 5 .4% which is\nsigni\ufb01cantly worse than either (using the McNemar test outlined in Ex-\nercise 10.6), so it appears on this example the additional randomization\nhelps.\nRF\u22121 RF\u22123 Bagging GBM\u22121 GBM\u221260.00 0.05 0.10 0.15Nested SpheresTest Misclassification Error\nBayes Error\nFIGURE 15.2. The results of 50simulations from the \u201cnested spheres\u201d model in\nI R10. The Bayes decision boundary is the surface of a sphere (addit ive). \u201cRF-3\u201d\nrefers to a random forest with m= 3, and \u201cGBM-6\u201d a gradient boosted model\nwith interaction order six; similarly for \u201cRF-1\u201d and \u201cGBM-1.\u201d Th e training sets\nwere of size 2000, and the test sets 10,000.\nFigure 15.1 shows the test-error progression on 2500 trees for the three\nmethods. In this case there is some evidence that gradient boosting has\nstarted to over\ufb01t, although 10-fold cross-validation chose all 2500 tr ees.\n1Sadly, Leo Breiman died in July, 2005.\n2http://www.math.usu.edu/ \u223cadele/forests/", "609": "15.2 De\ufb01nition of Random Forests 591\n0 200 400 600 800 10000.32 0.34 0.36 0.38 0.40 0.42 0.44California Housing Data\nNumber of TreesTest Average Absolute ErrorRF m=2\nRF m=6\nGBM depth=4\nGBM depth=6\nFIGURE 15.3. Random forests compared to gradient boosting on the California\nhousing data. The curves represent mean absolute error on the t est data as a\nfunction of the number of trees in the models. Two random forests are shown, with\nm= 2andm= 6. The two gradient boosted models use a shrinkage parameter\n\u03bd= 0.05in (10.41), and have interaction depths of 4and6. The boosted models\noutperform random forests.\nFigure 15.2 shows the results of a simulation3comparing random forests\nto gradient boosting on the nested spheres problem [Equation (10.2) in\nChapter 10]. Boosting easily outperforms random forests here. Notice that\nsmaller mis better here, although part of the reason could be that the true\ndecision boundary is additive.\nFigure 15.3 compares random forests to boosting (with shrinkage) in a\nregression problem, using the California housing data (Section 10.14.1).\nTwo strong features that emerge are\n\u2022Random forests stabilize at about 200 trees, while at 1000 trees boost-\ning continues to improve. Boosting is slowed down by the shrinkage,\nas well as the fact that the trees are much smaller.\n\u2022Boosting outperforms random forests here. At 1000 terms, the weaker\nboosting model (GBM depth 4) has a smaller error than the stronger\n3Details: The random forests were \ufb01t using the R package randomForest 4.5-11 ,\nwith 500 trees. The gradient boosting models were \ufb01t using R p ackagegbm 1.5 , with\nshrinkage parameter set to 0.05, and 2000 trees.", "610": "592 15. Random Forests\n0 500 1000 1500 2000 25000.045 0.055 0.065 0.075\nNumber of TreesMisclassification ErrorOOB Error\nTest Error\nFIGURE 15.4. ooberror computed on the spamtraining data, compared to the\ntest error computed on the test set.\nrandom forest (RF m= 6); a Wilcoxon test on the mean di\ufb00erences\nin absolute errors has a p-value of 0 .007. For larger mthe random\nforests performed no better.\n15.3 Details of Random Forests\nWe have glossed over the distinction between random forests for classi\ufb01ca-\ntion versus regression. When used for classi\ufb01cation, a random forest obtains\na class vote from each tree, and then classi\ufb01es using majority vote (see Sec-\ntion 8.7 on bagging for a similar discussion). When used for regression, the\npredictions from each tree at a target point xare simply averaged, as in\n(15.2). In addition, the inventors make the following recommendations:\n\u2022For classi\ufb01cation, the default value for mis\u230a\u221ap\u230band the minimum\nnode size is one.\n\u2022For regression, the default value for mis\u230ap/3\u230band the minimum\nnode size is \ufb01ve.\nIn practice the best values for these parameters will depend on the problem,\nand they should be treated as tuning parameters. In Figure 15.3 the m= 6\nperforms much better than the default value \u230a8/3\u230b= 2.\n15.3.1 Out of Bag Samples\nAn important feature of random forests is its use of out-of-bag (oob) sam-\nples:", "611": "15.3 Details of Random Forests 593\nFor each observation zi= (xi,yi), construct its random forest\npredictor by averaging onlythose trees corresponding to boot-\nstrap samples in which zidid not appear.\nAnooberror estimate is almost identical to that obtained by N-fold cross-\nvalidation; see Exercise 15.2. Hence unlike many other nonlinear estimators,\nrandom forests can be \ufb01t in one sequence, with cross-validation being per-\nformed along the way. Once the ooberror stabilizes, the training can be\nterminated.\nFigure 15.4 shows the oobmisclassi\ufb01cation error for the spamdata, com-\npared to the test error. Although 2500 trees are averaged here, it appears\nfrom the plot that about 200 would be su\ufb03cient.\n15.3.2 Variable Importance\nVariable importance plots can be constructed for random forests in exactly\nthe same way as they were for gradient-boosted models (Section 10.13).\nAt each split in each tree, the improvement in the split-criterion is the\nimportance measure attributed to the splitting variable, and is accumulated\nover all the trees in the forest separately for each variable. The left plot\nof Figure 15.5 shows the variable importances computed in this way for\nthespamdata; compare with the corresponding Figure 10.6 on page 354 for\ngradient boosting. Boosting ignores some variables completely, while the\nrandom forest does not. The candidate split-variable selection increases\nthe chance that any single variable gets included in a random forest, while\nno such selection occurs with boosting.\nRandom forests also use the oobsamples to construct a di\ufb00erent variable-\nimportance measure, apparently to measure the prediction strength of each\nvariable. When the bth tree is grown, the oobsamples are passed down\nthe tree, and the prediction accuracy is recorded. Then the values for the\njth variable are randomly permuted in the oobsamples, and the accuracy\nis again computed. The decrease in accuracy as a result of this permuting\nis averaged over all trees, and is used as a measure of the importance of\nvariable jin the random forest. These are expressed as a percent of the\nmaximum in the right plot in Figure 15.5. Although the rankings of the\ntwo methods are similar, the importances in the right plot are more uni-\nform over the variables. The randomization e\ufb00ectively voids the e\ufb00ect of\na variable, much like setting a coe\ufb03cient to zero in a linear model (Exer-\ncise 15.7). This does not measure the e\ufb00ect on prediction were this variable\nnot available, because if the model was re\ufb01tted without the variable, other\nvariables could be used as surrogates.", "612": "594 15. Random Forests\n!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyouryougeorge000eduhplbusiness1999internet(willallemailrereceiveovermail;650meetinglabsorderaddresspmpeoplemake#creditfontdatatechnology85[labtelnetreportoriginalprojectconferencedirect415857addresses3dcspartstableGini\n0 20 40 60 80 100\nVariable Importance!remove$CAPAVEhpfreeCAPMAXedugeorgeCAPTOTyourour1999reyouhplbusiness000meetingmoney(willinternet650pmreceiveoveremail;fontmailtechnologyorderalllabs[85addressoriginallabtelnetpeopleprojectdatacreditconference857#415makecsreportdirectaddresses3dpartstableRandomization\n0 20 40 60 80 100\nVariable Importance\nFIGURE 15.5. Variable importance plots for a classi\ufb01cation random forest\ngrown on the spamdata. The left plot bases the importance on the Gini split-\nting index, as in gradient boosting. The rankings compare well with t he rankings\nproduced by gradient boosting (Figure 10.6 on page 354). The ri ght plot uses oob\nrandomization to compute variable importances, and tends to spr ead the impor-\ntances more uniformly.", "613": "15.3 Details of Random Forests 595\nProximity Plot\n12\n34\n5\n6Random Forest Classifier\n123456\nDimension 1Dimension 2\nX1X2\nFIGURE 15.6. (Left): Proximity plot for a random forest classi\ufb01er grown to\nthe mixture data. (Right): Decision boundary and training data fo r random forest\non mixture data. Six points have been identi\ufb01ed in each plot.\n15.3.3 Proximity Plots\nOne of the advertised outputs of a random forest is a proximity plot . Fig-\nure 15.6 shows a proximity plot for the mixture data de\ufb01ned in Section 2.3.3\nin Chapter 2. In growing a random forest, an N\u00d7Nproximity matrix is\naccumulated for the training data. For every tree, any pair of oobobser-\nvations sharing a terminal node has their proximity increased by one. This\nproximity matrix is then represented in two dimensions using multidimen-\nsional scaling (Section 14.8). The idea is that even though the data may be\nhigh-dimensional, involving mixed variables, etc., the proximity plot gives\nan indication of which observations are e\ufb00ectively close together in the eyes\nof the random forest classi\ufb01er.\nProximity plots for random forests often look very similar, irrespect ive of\nthe data, which casts doubt on their utility. They tend to have a star shape,\none arm per class, which is more pronounced the better the classi\ufb01cation\nperformance.\nSince the mixture data are two-dimensional, we can map points from the\nproximity plot to the original coordinates, and get a better understanding of\nwhat they represent. It seems that points in pure regions class-wise map to\nthe extremities of the star, while points nearer the decision boundaries map\nnearer the center. This is not surprising when we consider the construction\nof the proximity matrices. Neighboring points in pure regions will often\nend up sharing a bucket, since when a terminal node is pure, it is no longer", "614": "596 15. Random Forests\nsplit by a random forest tree-growing algorithm. On the other hand, pairs\nof points that are close but belong to di\ufb00erent classes will sometimes share\na terminal node, but not always.\n15.3.4 Random Forests and Over\ufb01tting\nWhen the number of variables is large, but the fraction of relevant variables\nsmall, random forests are likely to perform poorly with small m. At each\nsplit the chance can be small that the relevant variables will be selected.\nFigure 15.7 shows the results of a simulation that supports this claim. De-\ntails are given in the \ufb01gure caption and Exercise 15.3. At the top of each\npair we see the hyper-geometric probability that a relevant variable will be\nselected at any split by a random forest tree (in this simulation, the relevant\nvariables are all equal in stature). As this probability gets small, the ga p\nbetween boosting and random forests increases. When the number of rele-\nvant variables increases, the performance of random forests is surprisingly\nrobust to an increase in the number of noise variables. For example, with 6\nrelevant and 100 noise variables, the probability of a relevant variable bei ng\nselected at any split is 0.46, assuming m=/radicalbig\n(6 + 100) \u224810. According to\nFigure 15.7, this does not hurt the performance of random forests compared\nwith boosting. This robustness is largely due to the relative insensitivity of\nmisclassi\ufb01cation cost to the bias and variance of the probability estimates\nin each tree. We consider random forests for regression in the next section.\nAnother claim is that random forests \u201ccannot over\ufb01t\u201d the data. It is\ncertainly true that increasing Bdoes not cause the random forest sequence\nto over\ufb01t; like bagging, the random forest estimate (15.2) approximates t he\nexpectation\n\u02c6frf(x) = E \u0398T(x;\u0398) = lim\nB\u2192\u221e\u02c6f(x)B\nrf (15.3)\nwith an average over Brealizations of \u0398. The distribution of \u0398 here is con-\nditional on the training data. However, this limit can over\ufb01t the data ; the\naverage of fully grown trees can result in too rich a model, and incur unnec-\nessary variance. Segal (2004) demonstrates small gains in performance by\ncontrolling the depths of the individual trees grown in random forests. Our\nexperience is that using full-grown trees seldom costs much, and results in\none less tuning parameter.\nFigure 15.8 shows the modest e\ufb00ect of depth control in a simple regression\nexample. Classi\ufb01ers are less sensitive to variance, and this e\ufb00ect of over-\n\ufb01tting is seldom seen with random-forest classi\ufb01cation.", "615": "15.4 Analysis of Random Forests 597Test Misclassification Error\n0.10 0.15 0.20 0.25 0.30Bayes Error\n(2, 5) (2, 25) (2, 50) (2, 100) (2, 150)\nNumber of (Relevant, Noise) Variables0.52 0.34 0.25 0.19 0.15\nRandom Forest\nGradient Boosting\nFIGURE 15.7. A comparison of random forests and gradient boosting on prob-\nlems with increasing numbers of noise variables. In each case the true decision\nboundary depends on two variables, and an increasing number of noise variables\nare included. Random forests uses its default value m=\u221ap. At the top of each\npair is the probability that one of the relevant variables is ch osen at any split.\nThe results are based on 50simulations for each pair, with a training sample of\n300, and a test sample of 500.\n15.4 Analysis of Random Forests\nIn this section we analyze the mechanisms at play with the additional\nrandomization employed by random forests. For this discussion we focus\non regression and squared error loss, since this gets at the main points,\nand bias and variance are more complex with 0\u20131 loss (see Section 7.3.1).\nFurthermore, even in the case of a classi\ufb01cation problem, we can consider\nthe random-forest average as an estimate of the class posterior probabilit ies,\nfor which bias and variance are appropriate descriptors.\n15.4.1 Variance and the De-Correlation E\ufb00ect\nThe limiting form ( B\u2192 \u221e) of the random forest regression estimator is\n\u02c6frf(x) = E \u0398|ZT(x;\u0398(Z)), (15.4)\nwhere we have made explicit the dependence on the training data Z. Here\nwe consider estimation at a single target point x. From (15.1) we see that", "616": "598 15. Random Forests\n50 30 20 10 51.00 1.05 1.10\nMinimum Node SizeMean Squared Test ErrorShallow Deep\nFIGURE 15.8. The e\ufb00ect of tree size on the error in random forest regres-\nsion. In this example, the true surface was additive in two of th e12variables,\nplus additive unit-variance Gaussian noise. Tree depth is contr olled here by the\nminimum node size; the smaller the minimum node size, the deeper t he trees.\nVar\u02c6frf(x) =\u03c1(x)\u03c32(x). (15.5)\nHere\n\u2022\u03c1(x) is the sampling correlation between any pair of trees used in the\naveraging:\n\u03c1(x) = corr[ T(x;\u03981(Z)),T(x;\u03982(Z))], (15.6)\nwhere \u0398 1(Z) and \u0398 2(Z) are a randomly drawn pair of random forest\ntrees grown to the randomly sampled Z;\n\u2022\u03c32(x) is the sampling variance of any single randomly drawn tree,\n\u03c32(x) = Var T(x;\u0398(Z)). (15.7)\nIt is easy to confuse \u03c1(x) with the average correlation between \ufb01tted trees\nin agiven random-forest ensemble; that is, think of the \ufb01tted trees as N-\nvectors, and compute the average pairwise correlation between these vec-\ntors, conditioned on the data. This is notthe case; this conditional corre-\nlation is not directly relevant in the averaging process, and the dependence\nonxin\u03c1(x) warns us of the distinction. Rather, \u03c1(x) is the theoretical\ncorrelation between a pair of random-forest trees evaluated at x, induced\nby repeatedly making training sample draws Zfrom the population, and\nthen drawing a pair of random forest trees. In statistical jargon, this is the\ncorrelation induced by the sampling distribution ofZand \u0398.\nMore precisely, the variability averaged over in the calculations in (15.6)\nand (15.7) is both", "617": "15.4 Analysis of Random Forests 599\n\u2022conditional on Z: due to the bootstrap sampling and feature sampling\nat each split, and\n\u2022a result of the sampling variability of Zitself.\nIn fact, the conditional covariance of a pair of tree \ufb01ts at xis zero, because\nthe bootstrap and feature sampling is i.i.d; see Exercise 15.5.\n1 4 7 13 19 25 31 37 43 490.00 0.02 0.04 0.06 0.08\nNumber of Randomly Selected Splitting Variables mCorrelation between Trees\nFIGURE 15.9. Correlations between pairs of trees drawn by a random-forest\nregression algorithm, as a function of m. The boxplots represent the correlations\nat600randomly chosen prediction points x.\nThe following demonstrations are based on a simulation model\nY=1\u221a\n5050/summationdisplay\nj=1Xj+\u03b5, (15.8)\nwith all the Xjand\u03b5iid Gaussian. We use 500 training sets of size 100, and\na single set of test locations of size 600. Since regression trees are nonlinear\ninZ, the patterns we see below will di\ufb00er somewhat depending on the\nstructure of the model.\nFigure 15.9 shows how the correlation (15.6) between pairs of trees de-\ncreases as mdecreases: pairs of tree predictions at xfor di\ufb00erent training\nsetsZare likely to be less similar if they do not use the same splitting\nvariables.\nIn the left panel of Figure 15.10 we consider the variances of single tree\npredictors, Var T(x;\u0398(Z)) (averaged over 600 prediction points xdrawn\nrandomly from our simulation model). This is the total variance, and can be", "618": "600 15. Random Forests\ndecomposed into two parts using standard conditional variance arguments\n(see Exercise 15.5):\nVar\u0398,ZT(x;\u0398(Z)) = Var ZE\u0398|ZT(x;\u0398(Z)) + E ZVar\u0398|ZT(x;\u0398(Z))\nTotal Variance = Var Z\u02c6frf(x) + within- ZVariance\n(15.9)\nThe second term is the within- Zvariance\u2014a result of the randomization,\nwhich increases as mdecreases. The \ufb01rst term is in fact the sampling vari-\nance of the random forest ensemble (shown in the right panel), which de-\ncreases as mdecreases. The variance of the individual trees does not change\nappreciably over much of the range of m, hence in light of (15.5), the vari-\nance of the ensemble is dramatically lower than this tree variance.\n0 10 20 30 40 501.80 1.85 1.90 1.95Single Tree\nmVariance\nWithin Z\nTotal\n0 10 20 30 40 500.65 0.70 0.75 0.80 0.85Random Forest Ensemble\nmMean Squared Error and Squared Bias\nVariance\nMean Squared Error\nSquared Bias\nVariance\n0.0 0.05 0.10 0.15 0.20\nFIGURE 15.10. Simulation results. The left panel shows the average variance o f\na single random forest tree, as a function of m. \u201cWithin Z\u201d refers to the average\nwithin-sample contribution to the variance, resulting from the bootstrap sampling\nand split-variable sampling (15.9). \u201cTotal\u201d includes the samp ling variability of\nZ. The horizontal line is the average variance of a single fully gro wn tree (with-\nout bootstrap sampling). The right panel shows the average mea n-squared error,\nsquared bias and variance of the ensemble, as a function of m. Note that the\nvariance axis is on the right (same scale, di\ufb00erent level). The h orizontal line is\nthe average squared-bias of a fully grown tree.\n15.4.2 Bias\nAs in bagging, the bias of a random forest is the same as the bias of any\nof the individual sampled trees T(x;\u0398(Z)):", "619": "15.4 Analysis of Random Forests 601\nBias(x) = \u03b8(x)\u2212EZ\u02c6frf(x)\n=\u03b8(x)\u2212EZE\u0398|ZT(x;\u0398(Z)). (15.10)\nThis is also typically greater (in absolute terms) than the bias of an un-\npruned tree grown to Z, since the randomization and reduced sample space\nimpose restrictions. Hence the improvements in prediction obtained by bag-\nging or random forests are solely a result of variance reduction .\nAny discussion of bias depends on the unknown true function. Fig-\nure 15.10 (right panel) shows the squared bias for our additive model simu-\nlation (estimated from the 500 realizations). Although for di\ufb00erent model s\nthe shape and rate of the bias curves may di\ufb00er, the general trend is that\nasmdecreases, the bias increases. Shown in the \ufb01gure is the mean-squared\nerror, and we see a classical bias-variance trade-o\ufb00 in the choice of m. For\nallmthe squared bias of the random forest is greater than that for a single\ntree (horizontal line).\nThese patterns suggest a similarity with ridge regression (Section 3.4.1) .\nRidge regression is useful (in linear models) when one has a large number\nof variables with similarly sized coe\ufb03cients; ridge shrinks their coe\ufb03cients\ntoward zero, and those of strongly correlated variables toward each other.\nAlthough the size of the training sample might not permit all the variables\nto be in the model, this regularization via ridge stabilizes the model and al-\nlows all the variables to have their say (albeit diminished). Random forests\nwith small mperform a similar averaging. Each of the relevant variables\nget their turn to be the primary split, and the ensemble averaging reduces\nthe contribution of any individual variable. Since this simulation exam-\nple (15.8) is based on a linear model in all the variables, ridge regression\nachieves a lower mean-squared error (about 0 .45 with df( \u03bbopt)\u224829).\n15.4.3 Adaptive Nearest Neighbors\nThe random forest classi\ufb01er has much in common with the k-nearest neigh-\nbor classi\ufb01er (Section 13.3); in fact a weighted version thereof. Since each\ntree is grown to maximal size, for a particular \u0398\u2217,T(x;\u0398\u2217(Z)) is the re-\nsponse value for one of the training samples4. The tree-growing algorithm\n\ufb01nds an \u201coptimal\u201d path to that observation, choosing the most informative\npredictors from those at its disposal. The averaging process assigns weight s\nto these training responses, which ultimately vote for the prediction. Hence\nvia the random-forest voting mechanism, those observations close to the\ntarget point get assigned weights\u2014an equivalent kernel\u2014which combine to\nform the classi\ufb01cation decision.\nFigure 15.11 demonstrates the similarity between the decision boundary\nof 3-nearest neighbors and random forests on the mixture data.\n4We gloss over the fact that pure nodes are not split further, a nd hence there can be\nmore than one observation in a terminal node", "620": "602 15. Random Forests\nRandom Forest Classifier\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\nooooo\noo\noo\noo\nooo\nooooooo\noo oooo\noo\noo\noooo\no\noo\noo\nooooo\nooo\no\no\nooo\noooooo\noo\no\noo\nooo\no\nTraining Error: 0.000\nTest Error:       0.238\nBayes Error:    0.2103\u2212Nearest Neighbors\no\noooo\nooo\noo\no\noo\noo\nooo\noo\nooo\noo\noo\no\noo\no\noooo\noo\no\noo\noo\nooo\noo\no\nooo\no\noo\noo\no\nooo\noo\no\no\noo\no\noo\nooooo\no\noo\no oo\noo\nooo\noo\noo\noo\noo\noo\nooo\no\nooooo\noooo\nooo\noo\nooo\noo\no\noo\no\nooo\noo ooo\noo\nooooo\noo\noo\noo\nooo\nooooooo\noo oooo\noo\noo\noooo\no\noo\noo\nooooo\nooo\no\no\nooo\noooooo\noo\no\noo\nooo\no\nTraining Error: 0.130\nTest Error:       0.242\nBayes Error:    0.210\nFIGURE 15.11. Random forests versus 3-NN on the mixture data. The axis-ori-\nented nature of the individual trees in a random forest lead to dec ision regions\nwith an axis-oriented \ufb02avor.\nBibliographic Notes\nRandom forests as described here were introduced by Breiman (2001), al-\nthough many of the ideas had cropped up earlier in the literature in dif-\nferent forms. Notably Ho (1995) introduced the term \u201crandom forest,\u201d and\nused a consensus of trees grown in random subspaces of the features. The\nidea of using stochastic perturbation and averaging to avoid over\ufb01tting was\nintroduced by Kleinberg (1990), and later in Kleinberg (1996). Amit and\nGeman (1997) used randomized trees grown on image features for image\nclassi\ufb01cation problems. Breiman (1996a) introduced bagging, a precursor\nto his version of random forests. Dietterich (2000b) also proposed an im -\nprovement on bagging using additional randomization. His approach was\nto rank the top 20 candidate splits at each node, and then select from the\nlist at random. He showed through simulations and real examples that this\nadditional randomization improved over the performance of bagging. Fried-\nman and Hall (2007) showed that sub-sampling (without replacement) is\nan e\ufb00ective alternative to bagging. They showed that growing and aver-\naging trees on samples of size N/2 is approximately equivalent (in terms\nbias/variance considerations) to bagging, while using smaller fractions of\nNreduces the variance even further (through decorrelation).\nThere are several free software implementations of random forests. In\nthis chapter we used the randomForest package in R, maintained by Andy\nLiaw, available from the CRANwebsite. This allows both split-variable se-\nlection, as well as sub-sampling. Adele Cutler maintains a random forest\nwebsitehttp://www.math.usu.edu/ \u223cadele/forests/ where (as of Au-\ngust 2008) the software written by Leo Breiman and Adele Cutler is freely", "621": "Exercises 603\navailable. Their code, and the name \u201crandom forests\u201d, is exclusively li-\ncensed to Salford Systems for commercial release. The Wekamachine learn-\ning archive http://www.cs.waikato.ac.nz/ml/weka/ at Waikato Univer-\nsity, New Zealand, o\ufb00ers a free javaimplementation of random forests.\nExercises\nEx. 15.1 Derive the variance formula (15.1). This appears to fail if \u03c1is\nnegative; diagnose the problem in this case.\nEx. 15.2 Show that as the number of bootstrap samples Bgets large, the\nooberror estimate for a random forest approaches its N-fold CV error\nestimate, and that in the limit, the identity is exact.\nEx. 15.3 Consider the simulation model used in Figure 15.7 (Mease and\nWyner, 2008). Binary observations are generated with probabilities\nPr(Y= 1|X) =q+ (1\u22122q)\u22641\uf8ee\n\uf8f0J/summationdisplay\nj=1Xj> J/2\uf8f9\n\uf8fb, (15.11)\nwhere X\u223cU[0,1]p, 0\u2264q\u22641\n2, and J\u2264pis some prede\ufb01ned (even)\nnumber. Describe this probability surface, and give the Bayes error rate.\nEx. 15.4 Suppose xi, i= 1,... ,N are iid ( \u03b8,\u03c32). Let \u00af x\u2217\n1and \u00afx\u2217\n2be two\nbootstrap realizations of the sample mean. Show that the sampling cor-\nrelation corr(\u00af x\u2217\n1,\u00afx\u2217\n2) =n\n2n\u22121\u224850%. Along the way, derive var(\u00af x\u2217\n1) and\nthe variance of the bagged mean \u00af xbag. Here \u00af xis alinear statistic; bagging\nproduces no reduction in variance for linear statistics.\nEx. 15.5 Show that the sampling correlation between a pair of random-\nforest trees at a point xis given by\n\u03c1(x) =VarZ[E\u0398|ZT(x;\u0398(Z))]\nVarZ[E\u0398|ZT(x;\u0398(Z))] + E ZVar\u0398|Z[T(x,\u0398(Z)]. (15.12)\nThe term in the numerator is Var Z[\u02c6frf(x)], and the second term in the\ndenominator is the expected conditional variance due to the randomization\nin random forests.\nEx. 15.6 Fit a series of random-forest classi\ufb01ers to the spamdata, to explore\nthe sensitivity to the parameter m. Plot both the ooberror as well as the\ntest error against a suitably chosen range of values for m.", "622": "604 15. Random Forests\nEx. 15.7 Suppose we \ufb01t a linear regression model to Nobservations with\nresponse yiand predictors xi1,... ,x ip. Assume that all variables are stan-\ndardized to have mean zero and standard deviation one. Let RSSbe the\nmean-squared residual on the training data, and \u02c6\u03b2the estimated coe\ufb03cient.\nDenote by RSS\u2217\njthe mean-squared residual on the training data using the\nsame \u02c6\u03b2, but with the Nvalues for the jth variable randomly permuted\nbefore the predictions are calculated. Show that\nEP[RSS\u2217\nj\u2212RSS] = 2\u02c6\u03b22\nj, (15.13)\nwhere E Pdenotes expectation with respect to the permutation distribution.\nArgue that this is approximately true when the evaluations are done using\nan independent test set.", "623": "This is page 605\nPrinter: Opaque this\n16\nEnsemble Learning\n16.1 Introduction\nThe idea of ensemble learning is to build a prediction model by combining\nthe strengths of a collection of simpler base models. We have already seen\na number of examples that fall into this category.\nBagging in Section 8.7 and random forests in Chapter 15 are ensemble\nmethods for classi\ufb01cation, where a committee of trees each cast a vote for\nthe predicted class. Boosting in Chapter 10 was initially proposed as a\ncommittee method as well, although unlike random forests, the committee\nofweak learners evolves over time, and the members cast a weighted vote.\nStacking (Section 8.8) is a novel approach to combining the strengths of\na number of \ufb01tted models. In fact one could characterize any dictionary\nmethod, such as regression splines, as an ensemble method, with the basis\nfunctions serving the role of weak learners.\nBayesian methods for nonparametric regression can also be viewed as\nensemble methods: a large number of candidate models are averaged with\nrespect to the posterior distribution of their parameter settings (e.g. (Neal\nand Zhang, 2006)).\nEnsemble learning can be broken down into two tasks: developing a pop-\nulation of base learners from the training data, and then combining them\nto form the composite predictor. In this chapter we discuss boosting tech-\nnology that goes a step further; it builds an ensemble model by conducting\na regularized and supervised search in a high-dimensional space of weak\nlearners.", "624": "606 16. Ensemble Learning\nAn early example of a learning ensemble is a method designed for multi-\nclass classi\ufb01cation using error-correcting output codes (Dietterich and Bakiri,\n1995, ECOC). Consider the 10-class digit classi\ufb01cation problem, and the\ncoding matrix Cgiven in Table 16.1.\nTABLE 16.1. Part of a 15-bit error-correcting coding matrix Cfor the 10-class\ndigit classi\ufb01cation problem. Each column de\ufb01nes a two-class cl assi\ufb01cation prob-\nlem.\nDigit C1C2C3C4C5C6\u00b7 \u00b7 \u00b7 C15\n0 1 1 0 0 0 0 \u00b7 \u00b7 \u00b7 1\n1 0 0 1 1 1 1 \u00b7 \u00b7 \u00b7 0\n2 1 0 0 1 0 0 \u00b7 \u00b7 \u00b7 1\n.....................\u00b7 \u00b7 \u00b7...\n8 1 1 0 1 0 1 \u00b7 \u00b7 \u00b7 1\n9 0 1 1 1 0 0 \u00b7 \u00b7 \u00b7 0\nNote that the \u2113th column of the coding matrix C\u2113de\ufb01nes a two-class\nvariable that merges all the original classes into two groups. The method\nworks as follows:\n1. Learn a separate classi\ufb01er for each of the L= 15 two class problems\nde\ufb01ned by the columns of the coding matrix.\n2. At a test point x, let \u02c6p\u2113(x) be the predicted probability of a one for\nthe\u2113th response.\n3. De\ufb01ne \u03b4k(x) =/summationtextL\n\u2113=1|Ck\u2113\u2212\u02c6p\u2113(x)|, the discriminant function for the\nkth class, where Ck\u2113is the entry for row kand column \u2113in Table 16.1.\nEach row of Cis a binary code for representing that class. The rows have\nmore bits than is necessary, and the idea is that the redundant \u201cerror-\ncorrecting\u201d bits allow for some inaccuracies, and can improve performance.\nIn fact, the full code matrix Cabove has a minimum Hamming distance1\nof 7 between any pair of rows. Note that even the indicator response coding\n(Section 4.2) is redundant, since 10 classes require only \u2308log210 = 4 bits for\ntheir unique representation. Dietterich and Bakiri (1995) showed impressive\nimprovements in performance for a variety of multiclass problems when\nclassi\ufb01cation trees were used as the base classi\ufb01er.\nJames and Hastie (1998) analyzed the ECOC approach, and showed\nthat random code assignment worked as well as the optimally constructed\nerror-correcting codes. They also argued that the main bene\ufb01t of the coding\nwas in variance reduction (as in bagging and random forests), because the\ndi\ufb00erent coded problems resulted in di\ufb00erent trees, and the decoding step\n(3) above has a similar e\ufb00ect as averaging.\n1The Hamming distance between two vectors is the number of mis matches between\ncorresponding entries.", "625": "16.2 Boosting and Regularization Paths 607\n16.2 Boosting and Regularization Paths\nIn Section 10.12.2 of the \ufb01rst edition of this book, we suggested an analogy\nbetween the sequence of models produced by a gradient boosting algorithm\nand regularized model \ufb01tting in high-dimensional feature spaces. This was\nprimarily motivated by observing the close connection between a boosted\nversion of linear regression and the lasso (Section 3.4.2). These connec-\ntions have been pursued by us and others, and here we present our current\nthinking in this area. We start with the original motivation, which \ufb01ts m ore\nnaturally in this chapter on ensemble learning.\n16.2.1 Penalized Regression\nIntuition for the success of the shrinkage strategy (10.41) of gradient bo ost-\ning (page 364 in Chapter 10) can be obtained by drawing analogies with\npenalized linear regression with a large basis expansion. Consider the dic-\ntionary of all possible J-terminal node regression trees T={Tk}that could\nbe realized on the training data as basis functions in IRp. The linear model\nis\nf(x) =K/summationdisplay\nk=1\u03b1kTk(x), (16.1)\nwhere K= card( T). Suppose the coe\ufb03cients are to be estimated by least\nsquares. Since the number of such trees is likely to be much larger than\neven the largest training data sets, some form of regularization is required.\nLet \u02c6\u03b1(\u03bb) solve\nmin\n\u03b1\uf8f1\n\uf8f2\n\uf8f3N/summationdisplay\ni=1/parenleft\uf8ecigg\nyi\u2212K/summationdisplay\nk=1\u03b1kTk(xi)/parenright\uf8ecigg2\n+\u03bb\u2264J(\u03b1)\uf8fc\n\uf8fd\n\uf8fe, (16.2)\nJ(\u03b1) is a function of the coe\ufb03cients that generally penalizes larger values.\nExamples are\nJ(\u03b1) =K/summationdisplay\nk=1|\u03b1k|2ridge regression , (16.3)\nJ(\u03b1) =K/summationdisplay\nk=1|\u03b1k|lasso, (16.4)\n(16.5)\nboth covered in Section 3.4. As discussed there, the solution to the lasso\nproblem with moderate to large \u03bbtends to be sparse; many of the \u02c6 \u03b1k(\u03bb) =\n0. That is, only a small fraction of all possible trees enter the model (16. 1).", "626": "608 16. Ensemble Learning\nAlgorithm 16.1 Forward Stagewise Linear Regression.\n1. Initialize \u02c7 \u03b1k= 0, k= 1,... ,K . Set\u03b5 >0 to some small constant,\nandMlarge.\n2. For m= 1 to M:\n(a) (\u03b2\u2217,k\u2217) = arg min \u03b2,k/summationtextN\ni=1/parenleft\uf8ecig\nyi\u2212/summationtextK\nl=1\u02c7\u03b1lTl(xi)\u2212\u03b2Tk(xi)/parenright\uf8ecig2\n.\n(b) \u02c7\u03b1k\u2217\u2190\u02c7\u03b1k\u2217+\u03b5\u2264sign(\u03b2\u2217).\n3. Output fM(x) =/summationtextK\nk=1\u02c7\u03b1kTk(x).\nThis seems reasonable since it is likely that only a small fraction of all po s-\nsible trees will be relevant in approximating any particular target function.\nHowever, the relevant subset will be di\ufb00erent for di\ufb00erent targets. Those\ncoe\ufb03cients that are not set to zero are shrunk by the lasso in that their\nabsolute values are smaller than their corresponding least squares values2:\n|\u02c6\u03b1k(\u03bb)|<|\u02c6\u03b1k(0)|. As\u03bbincreases, the coe\ufb03cients all shrink, each one\nultimately becoming zero.\nOwing to the very large number of basis functions Tk, directly solving\n(16.2) with the lasso penalty (16.4) is not possible. However, a feasibl e\nforward stagewise strategy exists that closely approximates the e\ufb00ect of\nthe lasso, and is very similar to boosting and the forward stagewise A lgo-\nrithm 10.2. Algorithm 16.1 gives the details. Although phrased in terms\nof tree basis functions Tk, the algorithm can be used with any set of ba-\nsis functions. Initially all coe\ufb03cients are zero in line 1; this corresponds\nto\u03bb=\u221ein (16.2). At each successive step, the tree Tk\u2217is selected that\nbest \ufb01ts the current residuals in line 2(a). Its corresponding coe\ufb03cient \u02c7 \u03b1k\u2217\nis then incremented or decremented by an in\ufb01nitesimal amount in 2(b),\nwhile all other coe\ufb03cients \u02c7 \u03b1k, k\u221dne}ationslash=k\u2217are left unchanged. In principle, this\nprocess could be iterated until either all the residuals are zero, or \u03b2\u2217= 0.\nThe latter case can occur if K < N , and at that point the coe\ufb03cient values\nrepresent a least squares solution. This corresponds to \u03bb= 0 in (16.2).\nAfter applying Algorithm 16.1 with M <\u221eiterations, many of the coef-\n\ufb01cients will be zero, namely, those that have yet to be incremented. The oth-\ners will tend to have absolute values smaller than their corresponding least\nsquares solution values, |\u02c7\u03b1k(M)|<|\u02c6\u03b1k(0)|. Therefore this M-iteration\nsolution qualitatively resembles the lasso, with Minversely related to \u03bb.\nFigure 16.1 shows an example, using the prostate data studied in Chap-\nter 3. Here, instead of using trees Tk(X) as basis functions, we use the origi-\n2IfK > N , there is in general no unique \u201cleast squares value,\u201d since i n\ufb01nitely many\nsolutions will exist that \ufb01t the data perfectly. We can pick t he minimum L1-norm solution\namongst these, which is the unique lasso solution.", "627": "16.2 Boosting and Regularization Paths 609\u22120.2 0.0 0.2 0.4 0.6lcavol\nlweight\nagelbphsvi\nlcpgleasonpgg45\n0.0 0.5 1.0 1.5 2.0\n\u22120.2 0.0 0.2 0.4 0.6lcavol\nlweight\nagelbphsvi\nlcpgleasonpgg45\n0 50 100 150 200\nt=P\nk|\u03b1k|\nCoe\ufb03cientsCoe\ufb03cientsLasso Forward Stagewise\nIteration\nFIGURE 16.1. Pro\ufb01les of estimated coe\ufb03cients from linear regression, for the\nprostate data studied in Chapter 3. The left panel shows the re sults from the lasso,\nfor di\ufb00erent values of the bound parameter t=P\nk|\u03b1k|. The right panel shows\nthe results of the stagewise linear regression Algorithm 16. 1, using M= 220\nconsecutive steps of size \u03b5=.01.\nnal variables Xkthemselves; that is, a multiple linear regression model. The\nleft panel displays the pro\ufb01les of estimated coe\ufb03cients from the lasso, for\ndi\ufb00erent values of the bound parameter t=/summationtext\nk|\u03b1k|. The right panel shows\nthe results of the stagewise Algorithm 16.1, with M= 250 and \u03b5= 0.01.\n[The left and right panels of Figure 16.1 are the same as Figure 3.10 and\nthe left panel of Figure 3.19, respectively.] The similarity between the two\ngraphs is striking.\nIn some situations the resemblance is more than qualitative. For example,\nif all of the basis functions Tkare mutually uncorrelated, then as \u03b5\u21930,M\u2191\nsuch that M\u01eb\u2192t, Algorithm 16.1 yields exactly the same solution as the\nlasso for bound parameter t=/summationtext\nk|\u03b1k|(and likewise for all solutions along\nthe path). Of course, tree-based regressors are not uncorrelated. However,\nthe solution sets are also identical if the coe\ufb03cients \u02c6 \u03b1k(\u03bb) are all monotone\nfunctions of \u03bb. This is often the case when the correlation between the\nvariables is low. When the \u02c6 \u03b1k(\u03bb) are not monotone in \u03bb, then the solution\nsets are not identical. The solution sets for Algorithm 16.1 tend to change\nless rapidly with changing values of the regularization parameter than those\nof the lasso.", "628": "610 16. Ensemble Learning\nEfron et al. (2004) make the connections more precise, by characterizing\nthe exact solution paths in the \u03b5-limiting case. They show that the coe\ufb03-\ncient paths are piece-wise linear functions, both for the lasso and forward\nstagewise. This facilitates e\ufb03cient algorithms which allow the entire pat hs\nto be computed with the same cost as a single least-squares \ufb01t. This least\nangle regression algorithm is described in more detail in Section 3.8.1.\nHastie et al. (2007) show that this in\ufb01nitesimal forward stagewise alg o-\nrithm (FS 0) \ufb01ts a monotone version of the lasso, which optimally reduces\nat each step the loss function for a given increase in the arc length of the\ncoe\ufb03cient path (see Sections 16.2.3 and 3.8.1). The arc-length for the \u01eb >0\ncase is M\u01eb, and hence proportional to the number of steps.\nTree boosting (Algorithm 10.3) with shrinkage (10.41) closely resembl es\nAlgorithm 16.1, with the learning rate parameter \u03bdcorresponding to \u03b5. For\nsquared error loss, the only di\ufb00erence is that the optimal tree to be selected\nat each iteration Tk\u2217is approximated by the standard top-down greedy\ntree-induction algorithm. For other loss functions, such as the exponential\nloss of AdaBoost and the binomial deviance, Rosset et al. (2004a) show\nsimilar results to what we see here. Thus, one can view tree boosting with\nshrinkage as a form of monotone ill-posed regression on all possible ( J-\nterminal node) trees, with the lasso penalty (16.4) as a regularizer. We\nreturn to this topic in Section 16.2.3.\nThe choice of no shrinkage [ \u03bd= 1 in equation (10.41)] is analogous to\nforward-stepwise regression, and its more aggressive cousin best-subset se-\nlection, which penalizes the number of non zero coe\ufb03cients J(\u03b1) =/summationtext\nk|\u03b1k|0.\nWith a small fraction of dominant variables, best subset approaches often\nwork well. But with a moderate fraction of strong variables, it is well k nown\nthat subset selection can be excessively greedy (Copas, 1983), often yielding\npoor results when compared to less aggressive strategies such as the lasso\nor ridge regression. The dramatic improvements often seen when shrinkage\nis used with boosting are yet another con\ufb01rmation of this approach.\n16.2.2 The \u201cBet on Sparsity\u201d Principle\nAs shown in the previous section, boosting\u2019s forward stagewise strategy\nwith shrinkage approximately minimizes the same loss function with a\nlasso-style L1penalty. The model is built up slowly, searching through\n\u201cmodel space\u201d and adding shrunken basis functions derived from impor-\ntant predictors. In contrast, the L2penalty is computationally much easier\nto deal with, as shown in Section 12.3.7. With the basis functions and L2\npenalty chosen to match a particular positive-de\ufb01nite kernel, one can solve\nthe corresponding optimization problem without explicitly searching over\nindividual basis functions.\nHowever, the sometimes superior performance of boosting over proce-\ndures such as the support vector machine may be largely due to the im-\nplicit use of the L1versus L2penalty. The shrinkage resulting from the", "629": "16.2 Boosting and Regularization Paths 611\nL1penalty is better suited to sparse situations, where there are few basis\nfunctions with nonzero coe\ufb03cients (among all possible choices).\nWe can strengthen this argument through a simple example, taken from\nFriedman et al. (2004). Suppose we have 10 ,000 data points and our model\nis a linear combination of a million trees. If the true population coe\ufb03cients\nof these trees arose from a Gaussian distribution, then we know that in a\nBayesian sense the best predictor is ridge regression (Exercise 3.6). That is ,\nwe should use an L2rather than an L1penalty when \ufb01tting the coe\ufb03cients.\nOn the other hand, if there are only a small number (e.g., 1000) coe\ufb03cients\nthat are nonzero, the lasso ( L1penalty) will work better. We think of this\nas asparse scenario, while the \ufb01rst case (Gaussian coe\ufb03cients) is dense.\nNote however that in the dense scenario, although the L2penalty is best,\nneither method does very well since there is too little data from which to\nestimate such a large number of nonzero coe\ufb03cients. This is the curse of\ndimensionality taking its toll. In a sparse setting, we can potentially do\nwell with the L1penalty, since the number of nonzero coe\ufb03cients is small.\nTheL2penalty fails again.\nIn other words, use of the L1penalty follows what we call the \u201cbet on\nsparsity\u201d principle for high-dimensional problems:\nUse a procedure that does well in sparse problems, since no pr o-\ncedure does well in dense problems.\nThese comments need some quali\ufb01cation:\n\u2022For any given application, the degree of sparseness/denseness depends\non the unknown true target function, and the chosen dictionary T.\n\u2022The notion of sparse versus dense is relative to the size of the train-\ning data set and/or the noise-to-signal ratio (NSR). Larger training\nsets allow us to estimate coe\ufb03cients with smaller standard errors.\nLikewise in situations with small NSR, we can identify more nonzero\ncoe\ufb03cients with a given sample size than in situations where the NSR\nis larger.\n\u2022The size of the dictionary plays a role as well. Increasing the size of the\ndictionary may lead to a sparser representation for our function, but\nthe search problem becomes more di\ufb03cult leading to higher variance.\nFigure 16.2 illustrates these points in the context of linear models us-\ning simulation. We compare ridge regression and lasso, both for classi\ufb01-\ncation and regression problems. Each run has 50 observations with 300\nindependent Gaussian predictors. In the top row all 300 coe\ufb03cients are\nnonzero, generated from a Gaussian distribution. In the middle row, only\n10 are nonzero and generated from a Gaussian, and the last row has 30\nnon zero Gaussian coe\ufb03cients. For regression, standard Gaussian noise is", "630": "612 16. Ensemble Learning\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Squared Prediction Error Explained\nNoise\u2212to\u2212Signal RatioRegression\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Misclassification Error Explained\nNoise\u2212to\u2212Signal RatioClassification\nFIGURE 16.2. Simulations that show the superiority of the L1(lasso) penalty\noverL2(ridge) in regression and classi\ufb01cation. Each run has 50observations\nwith300independent Gaussian predictors. In the top row all 300coe\ufb03cients are\nnonzero, generated from a Gaussian distribution. In the middle r ow, only 10are\nnonzero, and the last row has 30nonzero. Gaussian errors are added to the linear\npredictor \u03b7(X)for the regression problems, and binary responses generated via the\ninverse-logit transform for the classi\ufb01cation problems. Scali ng of \u03b7(X)resulted in\nthe noise-to-signal ratios shown. Lasso is used in the left sub- columns, ridge in the\nright. We report the optimal percentage of error explained on te st data (relative\nto the error of a constant model), displayed as boxplots over 20realizations for\neach combination. In the only situation where ridge beats lasso (top row), neither\ndo well.", "631": "16.2 Boosting and Regularization Paths 613\nadded to the linear predictor \u03b7(X) =XT\u03b2to produce a continuous re-\nsponse. For classi\ufb01cation the linear predictor is transformed via the inverse-\nlogit to a probability, and a binary response is generated. Five di\ufb00er-\nent noise-to-signal ratios are presented, obtained by scaling \u03b7(X) prior\nto generating the response. In both cases this is de\ufb01ned to be NSR =\nVar(Y|\u03b7(X))/Var(\u03b7(X)). Both the ridge regression and lasso coe\ufb03cient\npaths were \ufb01t using a series of 50 values of \u03bbcorresponding to a range of\ndffrom 1 to 50 (see Chapter 3 for details). The models were evaluated on\na large test set (in\ufb01nite for Gaussian, 5000 for binary), and in each case the\nvalue for \u03bbwas chosen to minimize the test-set error. We report percentage\nvariance explained for the regression problems, and percentage misclassi\ufb01-\ncation error explained for the classi\ufb01cation problems (relative to a baseline\nerror of 0 .5). There are 20 simulation runs for each scenario.\nNote that for the classi\ufb01cation problems, we are using squared-error loss\nto \ufb01t the binary response. Note also that we do not using the training\ndata to select \u03bb, but rather are reporting the best possible behavior for\neach method in the di\ufb00erent scenarios. The L2penalty performs poorly\neverywhere. The Lasso performs reasonably well in the only two situations\nwhere it can (sparse coe\ufb03cients). As expected the performance gets worse\nas the NSR increases (less so for classi\ufb01cation), and as the model becomes\ndenser. The di\ufb00erences are less marked for classi\ufb01cation than for regression.\nThese empirical results are supported by a large body of theoretical\nresults (Donoho and Johnstone, 1994; Donoho and Elad, 2003; Donoho,\n2006b; Candes and Tao, 2007) that support the superiority of L1estimation\nin sparse settings.\n16.2.3 Regularization Paths, Over-\ufb01tting and Margins\nIt has often been observed that boosting \u201cdoes not over\ufb01t,\u201d or more as-\ntutely is \u201cslow to over\ufb01t.\u201d Part of the explanation for this phenomenon was\nmade earlier for random forests \u2014 misclassi\ufb01cation error is less sensitive to\nvariance than is mean-squared error, and classi\ufb01cation is the major focus\nin the boosting community. In this section we show that the regulariza-\ntion paths of boosted models are \u201cwell behaved,\u201d and that for certain loss\nfunctions they have an appealing limiting form.\nFigure 16.3 shows the coe\ufb03cient paths for lasso and in\ufb01nitesimal forward\nstagewise (FS 0) in a simulated regression setting. The data consists of a\ndictionary of 1000 Gaussian variables, strongly correlated ( \u03c1= 0.95) within\nblocks of 20, but uncorrelated between blocks. The generating model has\nnonzero coe\ufb03cients for 50 variables, one drawn from each block, and the\ncoe\ufb03cient values are drawn from a standard Gaussian. Finally, Gaussian\nnoise is added, with a noise-to-signal ratio of 0 .72 (Exercise 16.1.) The\nFS0algorithm is a limiting form of algorithm 16.1, where the step size \u03b5\nis shrunk to zero (Section 3.8.1). The grouping of the variables is intended\nto mimic the correlations of nearby trees, and with the forward-stagewise", "632": "614 16. Ensemble Learning\n0.0 0.2 0.4 0.6 0.8 1.0\u221220 \u221210 0 10 20 30Standardized CoefficientsLASSO\n0.0 0.2 0.4 0.6 0.8 1.0\u221220 \u221210 0 10 20 30Standardized CoefficientsForward Stagewise\n|\u03b1(m)|/|\u03b1(\u221e)| |\u03b1(m)|/|\u03b1(\u221e)|\nFIGURE 16.3. Comparison of lasso and in\ufb01nitesimal forward stagewise paths\non simulated regression data. The number of samples is 60and the number of\nvariables is 1000. The forward-stagewise paths \ufb02uctuate less than those of la sso\nin the \ufb01nal stages of the algorithms.\nalgorithm, this setup is intended as an idealized version of gradient boosting\nwith shrinkage. For both these algorithms, the coe\ufb03cient paths can be\ncomputed exactly, since they are piecewise linear (see the LARS algorithm\nin Section 3.8.1).\nHere the coe\ufb03cient pro\ufb01les are similar only in the early stages of the\npaths. For the later stages, the forward stagewise paths tend to be mono-\ntone and smoother, while those for the lasso \ufb02uctuate widely. This is due\nto the strong correlations among subsets of the variables \u2014lasso su\ufb00ers\nsomewhat from the multi-collinearity problem (Exercise 3.28).\nThe performance of the two models is rather similar (Figure 16.4), and\nthey achieve about the same minimum. In the later stages forward stagewise\ntakes longer to over\ufb01t, a likely consequence of the smoother paths.\nHastie et al. (2007) show that FS 0solves a monotone version of the lasso\nproblem for squared error loss. Let Ta=T \u222a {\u2212T } be the augmented\ndictionary obtained by including a negative copy of every basis element\ninT. We consider models f(x) =/summationtext\nTk\u2208Ta\u03b1kTk(x) with non-negative co-\ne\ufb03cients \u03b1k\u22650. In this expanded space, the lasso coe\ufb03cient paths are\npositive, while those of FS 0are monotone nondecreasing.\nThe monotone lasso path is characterized by a di\ufb00erential equation\n\u2202\u03b1\n\u2202\u2113=\u03c1ml(\u03b1(\u2113)), (16.6)", "633": "16.2 Boosting and Regularization Paths 615\no\noo\no\no\noo\noo\noo\noooo\nooo\nooooooooooooooooooooooooooooooo ooooooo ooooooooooooooooooooooooooooooooooooooooo ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo oo ooo\noo\no\no o\noo\nooo\nooo oo\no ooooo\noooooooo ooooo ooo ooooooooooo oooooo o ooo oooo oo ooooo oo oo ooooooo oo oo ooooo oo oo oo oo ooo oo oo oo ooooooo oo ooo oooooo oo ooo oo oooooooo oooo ooo ooooooo ooo ooo ooooo oooo ooo oooooo ooooooo oooooooooooooo ooooo ooooooooooo ooooooooo oo oooo oooooo oo ooooooooooo ooo oo ooo oo ooo oooooooooo ooooo ooooooo ooooooooo oooo oooooo oooo ooo oooooo oooo oo oo oooo ooooo ooooo ooooooo ooooo o o oooooooooooooooooo oo ooo oo oo oo oooo o oo o o o o o oo o o oooo oo o oo o o oo oo o ooo o o o ooo o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o o\n0 10 20 30 40 50 60 7025 30 35 40 45 50 55Lasso\nForward Stagewise\n|\u03b1(m)|Mean Squared Error\nFIGURE 16.4. Mean squared error for lasso and in\ufb01nitesimal forward stagewise\non the simulated data. Despite the di\ufb00erence in the coe\ufb03cient p aths, the two\nmodels perform similarly over the critical part of the regul arization path. In the\nright tail, lasso appears to over\ufb01t more rapidly.\nwith initial condition \u03b1(0) = 0, where \u2113is the L1arc-length of the path\n\u03b1(\u2113) (Exercise 16.2). The monotone lasso move direction (velocity vector)\n\u03c1ml(\u03b1(\u2113)) decreases the loss at the optimal quadratic rate per unit increase\nin the L1arc-length of the path. Since \u03c1ml\nk(\u03b1(\u2113))\u22650\u2200k,\u2113, the solution\npaths are monotone.\nThe lasso can similarly be characterized as the solution to a di\ufb00erential\nequation as in (16.6), except that the move directions decrease the loss\noptimally per unit increase in the L1norm of the path. As a consequence,\nthey are not necessarily positive, and hence the lasso paths need not be\nmonotone.\nIn this augmented dictionary, restricting the coe\ufb03cients to be positive is\nnatural, since it avoids an obvious ambiguity. It also ties in more natura lly\nwith tree boosting\u2014we always \ufb01nd trees positively correlated with the\ncurrent residual.\nThere have been suggestions that boosting performs well (for two-class\nclassi\ufb01cation) because it exhibits maximal-margin properties, much like the\nsupport-vector machines of Chapters 4.5.2 and 12. Schapire et al. (1998)\nde\ufb01ne the normalized L1margin of a \ufb01tted model f(x) =/summationtext\nk\u03b1kTk(x) as\nm(f) = min\niyif(xi)/summationtextK\nk=1|\u03b1k|. (16.7)\nHere the minimum is taken over the training sample, and yi\u2208 {\u22121,+1}.\nUnlike the L2margin (4.40) of support vector machines, the L1margin\nm(f) measures the distance to the closest training point in L\u221eunits (max-\nimum coordinate distance).", "634": "616 16. Ensemble Learning\n\u22120.3 \u22120.2 \u22120.1 0.0 0.1\nNumber of TreesMargin\n0 2K 4K 6K 8K 10K\n0.25 0.26 0.27 0.28\nNumber of TreesTest Error\n0 2K 4K 6K 8K 10K\nFIGURE 16.5. The left panel shows the L1margin m(f)for the Adaboost clas-\nsi\ufb01er on the mixture data, as a function of the number of 4-node trees. The model\nwas \ufb01t using the R package gbm, with a shrinkage factor of 0.02. After 10,000\ntrees, m(f)has settled down. Note that when the margin crosses zero, the t raining\nerror becomes zero. The right panel shows the test error, whic h is minimized at\n240trees. In this case, Adaboost over\ufb01ts dramatically if run to c onvergence.\nSchapire et al. (1998) prove that with separable data, Adaboost in-\ncreases m(f) with each iteration, converging to a margin-symmetric so-\nlution. R\u00a8 atsch and Warmuth (2002) prove the asymptotic convergence of\nAdaboost with shrinkage to a L1-margin-maximizing solution. Rosset et\nal. (2004a) consider regularized models of the form (16.2) for general loss\nfunctions. They show that as \u03bb\u21930, for particular loss functions the solution\nconverges to a margin-maximizing con\ufb01guration. In particular they show\nthis to be the case for the exponential loss of Adaboost, as well as binomia l\ndeviance.\nCollecting together the results of this section, we reach the following\nsummary for boosted classi\ufb01ers:\nThe sequence of boosted classi\ufb01ers form an L1-regularized mono-\ntone path to a margin-maximizing solution.\nOf course the margin-maximizing end of the path can be a very poor, over\ufb01t\nsolution, as it is in the example in Figure 16.5. Early stopping amounts\nto picking a point along the path, and should be done with the aid of a\nvalidation dataset.\n16.3 Learning Ensembles\nThe insights learned from the previous sections can be harnessed to produce\na more e\ufb00ective and e\ufb03cient ensemble model. Again we consider functions", "635": "16.3 Learning Ensembles 617\nof the form\nf(x) =\u03b10+/summationdisplay\nTk\u2208T\u03b1kTk(x), (16.8)\nwhere Tis a dictionary of basis functions, typically trees. For gradient\nboosting and random forests, |T |is very large, and it is quite typical for the\n\ufb01nal model to involve many thousands of trees. In the previous section we\nargue that gradient boosting with shrinkage \ufb01ts an L1regularized monotone\npath in this space of trees.\nFriedman and Popescu (2003) propose a hybrid approach which breaks\nthis process down into two stages:\n\u2022A \ufb01nite dictionary TL={T1(x),T2(x),... ,T M(x)}of basis functions\nis induced from the training data;\n\u2022A family of functions f\u03bb(x) is built by \ufb01tting a lasso path in this\ndictionary:\n\u03b1(\u03bb) = arg min\n\u03b1N/summationdisplay\ni=1L[yi,\u03b10+M/summationdisplay\nm=1\u03b1mTm(xi)] +\u03bbM/summationdisplay\nm=1|\u03b1m|.(16.9)\nIn its simplest form this model could be seen as a way of post-processing\nboosting or random forests, taking for TLthe collection of trees produced\nby the gradient boosting or random forest algorithms. By \ufb01tting the lasso\npath to these trees, we would typically use a much reduced set, which would\nsave in computations and storage for future predictions. In the next section\nwe describe modi\ufb01cations of this prescription that reduce the correlations in\nthe ensemble TL, and improve the performance of the lasso post processor.\nAs an initial illustration, we apply this procedure to a random forest\nensemble grown on the spam data.\nFigure 16.6 shows that a lasso post-processing o\ufb00ers modest improve-\nment over the random forest (blue curve), and reduces the forest to about\n40 trees, rather than the original 1000. The post-processed performance\nmatches that of gradient boosting. The orange curves represent a modi\ufb01ed\nversion of random forests, designed to reduce the correlations between trees\neven more. Here a random sub-sample (without replacement) of 5% of the\ntraining sample is used to grow each tree, and the trees are restricted to be\nshallow (about six terminal nodes). The post-processing o\ufb00ers more dra-\nmatic improvements here, and the training costs are reduced by a factor\nof about 100. However, the performance of the post-processed model falls\nsomewhat short of the blue curves.\n16.3.1 Learning a Good Ensemble\nNot all ensembles TLwill perform well with post-processing. In terms of\nbasis functions, we want a collection that covers the space well in places", "636": "618 16. Ensemble Learning\n0 100 200 300 400 5000.04 0.05 0.06 0.07 0.08 0.09Spam Data\nNumber of TreesTest ErrorRandom Forest\nRandom Forest (5%, 6)\nGradient Boost (5 node)\nFIGURE 16.6. Application of the lasso post-processing (16.9) to the spam d ata.\nThe horizontal blue line is the test error of a random forest \ufb01t to t he spam data,\nusing 1000trees grown to maximum depth (with m= 7; see Algorithm 15.1).\nThe jagged blue curve is the test error after post-processing the \ufb01rst 500trees\nusing the lasso, as a function of the number of trees with nonzero co e\ufb03cients.\nThe orange curve/line use a modi\ufb01ed form of random forest, where a random\ndraw of 5% of the data are used to grow each tree, and the trees are forced to\nbe shallow (typically six terminal nodes). Here the post-proc essing o\ufb00ers much\ngreater improvement over the random forest that generated the e nsemble.\nwhere they are needed, and are su\ufb03ciently di\ufb00erent from each other for\nthe post-processor to be e\ufb00ective.\nFriedman and Popescu (2003) gain insights from numerical quadrature\nand importance sampling. They view the unknown function as an integral\nf(x) =/integraldisplay\n\u03b2(\u03b3)b(x;\u03b3)d\u03b3, (16.10)\nwhere \u03b3\u2208\u0393 indexes the basis functions b(x;\u03b3). For example, if the basis\nfunctions are trees, then \u03b3indexes the splitting variables, the split-points\nand the values in the terminal nodes. Numerical quadrature amounts to\n\ufb01nding a set of Mevaluation points \u03b3m\u2208\u0393 and corresponding weights\n\u03b1mso that fM(x) =\u03b10+/summationtextM\nm=1\u03b1mb(x;\u03b3m) approximates f(x) well over\nthe domain of x. Importance sampling amounts to sampling \u03b3at random,\nbut giving more weight to relevant regions of the space \u0393. Friedman and\nPopescu (2003) suggest a measure of (lack of) relevance that uses the loss\nfunction (16.9):", "637": "16.3 Learning Ensembles 619\nQ(\u03b3) = min\nc0,c1N/summationdisplay\ni=1L(yi,c0+c1b(xi;\u03b3)), (16.11)\nevaluated on the training data.\nIf a single basis function were to be selected (e.g., a tree), it would be\nthe global minimizer \u03b3\u2217= arg min \u03b3\u2208\u0393Q(\u03b3). Introducing randomness in the\nselection of \u03b3would necessarily produce less optimal values with Q(\u03b3)\u2265\nQ(\u03b3\u2217). They propose a natural measure of the characteristic width \u03c3of the\nsampling scheme S,\n\u03c3= ES[Q(\u03b3)\u2212Q(\u03b3\u2217)]. (16.12)\n\u2022\u03c3too narrow suggests too many of the b(x;\u03b3m) look alike, and similar\ntob(x;\u03b3\u2217);\n\u2022\u03c3too wide implies a large spread in the b(x;\u03b3m), but possibly con-\nsisting of many irrelevant cases.\nFriedman and Popescu (2003) use sub-sampling as a mechanism for intro-\nducing randomness, leading to their ensemble-generation algorithm 16.2.\nAlgorithm 16.2 ISLE Ensemble Generation.\n1.f0(x) = arg min c/summationtextN\ni=1L(yi,c)\n2. For m= 1 to Mdo\n(a)\u03b3m= arg min \u03b3/summationtext\ni\u2208Sm(\u03b7)L(yi,fm\u22121(xi) +b(xi;\u03b3))\n(b)fm(x) =fm\u22121(x) +\u03bdb(x;\u03b3m)\n3.TISLE={b(x;\u03b31),b(x;\u03b32),... ,b (x;\u03b3M)}.\nSm(\u03b7) refers to a subsample of N\u2264\u03b7(\u03b7\u2208(0,1]) of the training obser-\nvations, typically without replacement. Their simulations suggest picking\n\u03b7\u22641\n2, and for large Npicking \u03b7\u223c1/\u221a\nN. Reducing \u03b7increases the\nrandomness, and hence the width \u03c3. The parameter \u03bd\u2208[0,1] introduces\nmemory into the randomization process; the larger \u03bd, the more the pro-\ncedure avoids b(x;\u03b3) similar to those found before. A number of familiar\nrandomization schemes are special cases of Algorithm 16.2:\nBagging has\u03b7= 1, but samples with replacement, and has \u03bd= 0. Fried-\nman and Hall (2007) argue that sampling without replacement with\n\u03b7= 1/2 is equivalent to sampling with replacement with \u03b7= 1, and\nthe former is much more e\ufb03cient.", "638": "620 16. Ensemble Learning\nRandom forest sampling is similar, with more randomness introduced by\nthe selection of the splitting variable. Reducing \u03b7 <1/2 in algo-\nrithm 16.2 has a similar e\ufb00ect to reducing min random forests, but\ndoes not su\ufb00er from the potential biases discussed in Section 15.4.2.\nGradient boosting with shrinkage (10.41) uses \u03b7= 1, but typically does\nnot produce su\ufb03cient width \u03c3.\nStochastic gradient boosting (Friedman, 1999) follows the recipe exactly.\nThe authors recommend values \u03bd= 0.1 and \u03b7\u22641\n2, and call their combined\nprocedure (ensemble generation and post processing) Importance sampled\nlearning ensemble (ISLE).\nFigure 16.7 shows the performance of an ISLE on the spam data. It does\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060Spam Data\nNumber of TreesTest ErrorGradient Boosting (5 Node)\nLasso Post\u2212processed\nFIGURE 16.7. Importance sampling learning ensemble (ISLE) \ufb01t to the spam\ndata. Here we used \u03b7= 1/2,\u03bd= 0.05, and trees with \ufb01ve terminal nodes. The\nlasso post-processed ensemble does not improve the predictio n error in this case,\nbut it reduces the number of trees by a factor of \ufb01ve.\nnot improve the predictive performance, but is able to produce a more\nparsimonious model. Note that in practice the post-processing includes\nthe selection of the regularization parameter \u03bbin (16.9), which would be", "639": "16.3 Learning Ensembles 621\nchosen by cross-validation. Here we simply demonstrate the e\ufb00ects of post-\nprocessing by showing the entire path on the test data.\nFigure 16.8 shows various ISLEs on a regression example. The generating\n0 500 1000 1500 2000 25001.0 1.5 2.0 2.5 3.0 3.5\nNumber of TreesMean Squared ErrorGBM (1, 0.01)\nGBM (0.1, 0.01)\nISLE  GB\nISLE RF\nRandom Forest\nFIGURE 16.8. Demonstration of ensemble methods on a regression simulation\nexample. The notation GBM (0.1, 0.01) refers to a gradient boost ed model, with\nparameters (\u03b7, \u03bd). We report mean-squared error from the true (known) function.\nNote that the sub-sampled GBM model (green) outperforms the f ull GBM model\n(orange). The lasso post-processed version achieves simila r error. The random\nforest is outperformed by its post-processed version, but bo th fall short of the\nother models.\nfunction is\nf(X) = 10 \u22645/productdisplay\nj=1e\u22122X2\nj+35/summationdisplay\nj=6Xj, (16.13)\nwhere X\u223cU[0,1]100(the last 65 elements are noise variables). The re-\nsponse Y=f(X) +\u03b5where \u03b5\u223cN(0,\u03c32); we chose \u03c3= 1.3 resulting in a\nsignal-to-noise ratio of approximately 2. We used a training sample of size\n1000, and estimated the mean squared error E( \u02c6f(X)\u2212f(X))2by averaging\nover a test set of 500 samples. The sub-sampled GBM curve (light blue)\nis an instance of stochastic gradient boosting (Friedman, 1999) discussed in\nSection 10.12, and it outperforms gradient boosting on this example.", "640": "622 16. Ensemble Learning\n16.3.2 Rule Ensembles\nHere we describe a modi\ufb01cation of the tree-ensemble method that focuses\non individual rules (Friedman and Popescu, 2003). We encountered rules\nin Section 9.3 in the discussion of the PRIM method. The idea is to enlarge\nan ensemble of trees by constructing a set of rules from each of the trees\nin the collection.\n1 2\n30\n4\n5 6X1<2.1 X1\u22652.1\nX3\u2208 {M,L} X3\u2208 {S}\nX7<4.5 X7\u22654.5\nFIGURE 16.9. A typical tree in an ensemble, from which rules can be derived.\nFigure 16.9 depicts a small tree, with numbered nodes. The following\nrules can be derived from this tree:\nR1(X) = I(X1<2.1)\nR2(X) = I(X1\u22652.1)\nR3(X) = I(X1\u22652.1)\u2264I(X3\u2208 {S})\nR4(X) = I(X1\u22652.1)\u2264I(X3\u2208 {M, L})\nR5(X) = I(X1\u22652.1)\u2264I(X3\u2208 {S})\u2264I(X7<4.5)\nR6(X) = I(X1\u22652.1)\u2264I(X3\u2208 {S})\u2264I(X7\u22654.5)(16.14)\nA linear expansion in rules 1, 4, 5 and 6 is equivalent to the tree itself\n(Exercise 16.3); hence (16.14) is an over-complete basis for the tree.\nFor each tree Tmin an ensemble T, we can construct its mini-ensemble\nof rules Tm\nRULE, and then combine them all to form a larger ensemble\nTRULE=M/uniondisplay\nm=1Tm\nRULE. (16.15)\nThis is then treated like any other ensemble, and post-processed via the\nlasso or similar regularized procedure.\nThere are several advantages to this approach of deriving rules from the\nmore complex trees:\n\u2022The space of models is enlarged, and can lead to improved perfor-\nmance.", "641": "16.3 Learning Ensembles 623\nRules Rules + Linear0.9 1.0 1.1 1.2 1.3Mean Squared Error\nFIGURE 16.10. Mean squared error for rule ensembles, using 20realizations\nof the simulation example (16.13).\n\u2022Rules are easier to interpret than trees, so there is the potential for\na simpli\ufb01ed model.\n\u2022It is often natural to augment TRULEby including each variable Xj\nseparately as well, thus allowing the ensemble to model linear func-\ntions well.\nFriedman and Popescu (2008) demonstrate the power of this procedure on a\nnumber of illustrative examples, including the simulation example (16.13).\nFigure 16.10 shows boxplots of the mean-squared error from the true model\nfor twenty realizations from this model. The models were all \ufb01t using the\nRulefit software, available on the ESL homepage3, which runs in an auto-\nmatic mode.\nOn the same training set as used in Figure 16.8, the rule based model\nachieved a mean-squared error of 1.06. Although slightly worse than the\nbest achieved in that \ufb01gure, the results are not comparable because cross-\nvalidation was used here to select the \ufb01nal model.\nBibliographic Notes\nAs noted in the introduction, many of the new methods in machine learning\nhave been dubbed \u201censemble\u201d methods. These include neural networks\nboosting, bagging and random forests; Dietterich (2000a) gives a survey o f\ntree-based ensemble methods. Neural networks (Chapter 11) are perhaps\nmore deserving of the name, since they simultaneously learn the parameters\n3ESL homepage: www-stat.stanford.edu/ElemStatLearn", "642": "624 16. Ensemble Learning\nof the hidden units (basis functions), along with how to combine them.\nBishop (2006) discusses neural networks in some detail, along with the\nBayesian perspective (MacKay, 1992; Neal, 1996). Support vector machines\n(Chapter 12) can also be regarded as an ensemble method; they perform\nL2regularized model \ufb01tting in high-dimensional feature spaces. Boosting\nand lasso exploit sparsity through L1regularization to overcome the high-\ndimensionality, while SVMs rely on the \u201ckernel trick\u201d characteristic of L2\nregularization.\nC5.0 (Quinlan, 2004) is a commercial tree and rule generation package,\nwith some goals in common with Rulefit .\nThere is a vast and varied literature often referred to as \u201ccombining clas-\nsi\ufb01ers\u201d which abounds in ad-hoc schemes for mixing methods of di\ufb00erent\ntypes to achieve better performance. For a principled approach, see Kittler\net al. (1998).\nExercises\nEx. 16.1 Describe exactly how to generate the block correlated data used\nin the simulation in Section 16.2.3.\nEx. 16.2 Let\u03b1(t)\u2208IRpbe a piecewise-di\ufb00erentiable and continuous coef-\n\ufb01cient pro\ufb01le, with \u03b1(0) = 0. The L1arc-length of \u03b1from time 0 to tis\nde\ufb01ned by\n\u039b(t) =/integraldisplayt\n0|\u02d9\u03b1(t)|1dt. (16.16)\nShow that \u039b( t)\u2265 |\u03b1(t)|1, with equality i\ufb00 \u03b1(t) is monotone.\nEx. 16.3 Show that \ufb01tting a linear regression model using rules 1, 4, 5 and\n6 in equation (16.14) gives the same \ufb01t as the regression tree corresponding\nto this tree. Show the same is true for classi\ufb01cation, if a logistic regressi on\nmodel is \ufb01t.\nEx. 16.4 Program and run the simulation study described in Figure 16.2.", "643": "This is page 625\nPrinter: Opaque this\n17\nUndirected Graphical Models\n17.1 Introduction\nA graph consists of a set of vertices (nodes), along with a set of edges join-\ning some pairs of the vertices. In graphical models, each vertex represents\na random variable, and the graph gives a visual way of understanding the\njoint distribution of the entire set of random variables. They can be use-\nful for either unsupervised or supervised learning. In an undirected graph ,\nthe edges have no directional arrows. We restrict our discussion to undi-\nrected graphical models, also known as Markov random \ufb01elds orMarkov\nnetworks . In these graphs, the absence of an edge between two vertices has\na special meaning: the corresponding random variables are conditionally\nindependent, given the other variables.\nFigure 17.1 shows an example of a graphical model for a \ufb02ow-cytometry\ndataset with p= 11 proteins measured on N= 7466 cells, from Sachs\net al. (2003). Each vertex in the graph corresponds to the real-valued ex-\npression level of a protein. The network structure was estimated assuming\na multivariate Gaussian distribution, using the graphical lasso procedure\ndiscussed later in this chapter.\nSparse graphs have a relatively small number of edges, and are convenient\nfor interpretation. They are useful in a variety of domains, including ge-\nnomics and proteomics, where they provide rough models of cell pathways.\nMuch work has been done in de\ufb01ning and understanding the structure of\ngraphical models; see the Bibliographic Notes for references.", "644": "626 17. Undirected Graphical Models\nRaf\nMek\nPlcg\nPIP2\nPIP3\nErk AktPKAPKCP38Jnk\nFIGURE 17.1. Example of a sparse undirected graph, estimated from a \ufb02ow\ncytometry dataset, with p= 11proteins measured on N= 7466 cells. The net-\nwork structure was estimated using the graphical lasso proce dure discussed in this\nchapter.\nAs we will see, the edges in a graph are parametrized by values or po-\ntentials that encode the strength of the conditional dependence between\nthe random variables at the corresponding vertices. The main challenges in\nworking with graphical models are model selection (choosing the structure\nof the graph), estimation of the edge parameters from data, and compu-\ntation of marginal vertex probabilities and expectations, from their joint\ndistribution. The last two tasks are sometimes called learning andinference\nin the computer science literature.\nWe do not attempt a comprehensive treatment of this interesting area.\nInstead, we introduce some basic concepts, and then discuss a few sim-\nple methods for estimation of the parameters and structure of undirected\ngraphical models; methods that relate to the techniques already discussed\nin this book. The estimation approaches that we present for continuous\nand discrete-valued vertices are di\ufb00erent, so we treat them separately. Sec-\ntions 17.3.1 and 17.3.2 may be of particular interest, as they describe new,\nregression-based procedures for estimating graphical models.\nThere is a large and active literature on directed graphical models or\nBayesian networks ; these are graphical models in which the edges have\ndirectional arrows (but no directed cycles). Directed graphical models rep-\nresent probability distributions that can be factored into products of condi-\ntional distributions, and have the potential for causal interpretations. We\nrefer the reader to Wasserman (2004) for a brief overview of both undi-\nrected and directed graphs; the next section follows closely his Chapter 18.", "645": "17.2 Markov Graphs and Their Properties 627\nX\nXXX\nYYYY\nZ\nZZ\nZ\nWWW\n(a) (b)\n(c) (d)\nFIGURE 17.2. Examples of undirected graphical models or Markov networks.\nEach node or vertex represents a random variable, and the lack of a n edge between\ntwo nodes indicates conditional independence. For example, in graph (a),Xand\nZare conditionally independent, given Y. In graph (b), Zis independent of each\nofX,Y, and W.\nA longer list of useful references is given in the Bibliographic Notes on\npage 645.\n17.2 Markov Graphs and Their Properties\nIn this section we discuss the basic properties of graphs as models for the\njoint distribution of a set of random variables. We defer discussion of (a)\nparametrization and estimation of the edge parameters from data, and (b)\nestimation of the topology of a graph, to later sections.\nFigure 17.2 shows four examples of undirected graphs. A graph Gconsists\nof a pair ( V,E), where Vis a set of vertices and Ethe set of edges (de\ufb01ned\nby pairs of vertices). Two vertices XandYare called adjacent if there\nis a edge joining them; this is denoted by X\u223cY. ApathX1,X2,... ,X n\nis a set of vertices that are joined, that is Xi\u22121\u223cXifori= 2,... ,n . A\ncomplete graph is a graph with every pair of vertices joined by an edge.\nAsubgraph U\u2208Vis a subset of vertices together with their edges. For\nexample, ( X,Y,Z ) in Figure 17.2(a) form a path but not a complete graph.\nSuppose that we have a graph Gwhose vertex set Vrepresents a set of\nrandom variables having joint distribution P. In a Markov graph G, the\nabsence of an edge implies that the corresponding random variables are\nconditionally independent given the variables at the other vertices. This is\nexpressed with the following notation:", "646": "628 17. Undirected Graphical Models\nNo edge joining XandY\u21d0\u21d2X\u22a5Y|rest (17.1)\nwhere \u201crest\u201d refers to all of the other vertices in the graph. For example\nin Figure 17.2(a) X\u22a5Z|Y. These are known as the pairwise Markov\nindependencies ofG.\nIfA,BandCare subgraphs, then Cis said to separate AandBif every\npath between AandBintersects a node in C. For example, Yseparates\nXandZin Figures 17.2(a) and (d), and Zseparates YandWin (d). In\nFigure 17.2(b) Zis not connected to X,Y,W so we say that the two sets\nare separated by the empty set. In Figure 17.2(c), C={X,Z}separates\nYandW.\nSeparators have the nice property that they break the graph into con-\nditionally independent pieces. Speci\ufb01cally, in a Markov graph Gwith sub-\ngraphs A,BandC,\nifCseparates AandBthenA\u22a5B|C. (17.2)\nThese are known as the global Markov properties ofG. It turns out that the\npairwise and global Markov properties of a graph are equivalent (for gra phs\nwith positive distributions). That is, the set of graphs with associated prob-\nability distributions that satisfy the pairwise Markov independencies and\nglobal Markov assumptions are the same. This result is useful for inferring\nglobal independence relations from simple pairwise properties. For example\nin Figure 17.2(d) X\u22a5Z|{Y,W}since it is a Markov graph and there is no\nlink joining XandZ. But Yalso separates XfromZandWand hence by\nthe global Markov assumption we conclude that X\u22a5Z|YandX\u22a5W|Y.\nSimilarly we have Y\u22a5W|Z.\nThe global Markov property allows us to decompose graphs into smaller\nmore manageable pieces and thus leads to essential simpli\ufb01cations in com-\nputation and interpretation. For this purpose we separate the graph into\ncliques. A clique is a complete subgraph\u2014 a set of vertices that are all\nadjacent to one another; it is called maximal if it is a clique and no other\nvertices can be added to it and still yield a clique. The maximal cliques for\nthe graphs of Figure 17.2 are\n(a){X,Y},{Y,Z},\n(b){X,Y,W },{Z},\n(c){X,Y},{Y,Z},{Z,W},{X,W}, and\n(d){X,Y},{Y,Z},{Z,W}.\nAlthough the following applies to both continuous and discrete distri-\nbutions, much of the development has been for the latter. A probability\ndensity function fover a Markov graph Gcan be can represented as", "647": "17.2 Markov Graphs and Their Properties 629\nf(x) =1\nZ/productdisplay\nC\u2208C\u03c8C(xC) (17.3)\nwhere Cis the set of maximal cliques, and the positive functions \u03c8C(\u2264) are\ncalled clique potentials . These are not in general density functions1, but\nrather are a\ufb03nities that capture the dependence in XCby scoring certain\ninstances xChigher than others. The quantity\nZ=/summationdisplay\nx\u2208X/productdisplay\nC\u2208C\u03c8C(xC) (17.4)\nis the normalizing constant, also known as the partition function. Alterna-\ntively, the representation (17.3) implies a graph with independence prop-\nerties de\ufb01ned by the cliques in the product. This result holds for Markov\nnetworks Gwith positive distributions, and is known as the Hammersley-\nCli\ufb00ord theorem (Hammersley and Cli\ufb00ord, 1971; Cli\ufb00ord, 1990).\nMany of the methods for estimation and computation on graphs \ufb01rst de-\ncompose the graph into its maximal cliques. Relevant quantities are com-\nputed in the individual cliques and then accumulated across the entire\ngraph. A prominent example is the join tree orjunction tree algorithm for\ncomputing marginal and low order probabilities from the joint distribution\non a graph. Details can be found in Pearl (1986), Lauritzen and Spiegel-\nhalter (1988), Pearl (1988), Shenoy and Shafer (1988), Jensen et al. (1990),\nor Koller and Friedman (2007).\nXY\nZ\nFIGURE 17.3. A complete graph does not uniquely specify the higher-order\ndependence structure in the joint distribution of the variable s.\nA graphical model does not always uniquely specify the higher-order\ndependence structure of a joint probability distribution. Consider the com-\nplete three-node graph in Figure 17.3. It could represent the dependence\nstructure of either of the following distributions:\nf(2)(x,y,z) =1\nZ\u03c8(x,y)\u03c8(x,z)\u03c8(y,z);\nf(3)(x,y,z) =1\nZ\u03c8(x,y,z).(17.5)\nThe \ufb01rst speci\ufb01es only second order dependence (and can be represented\nwith fewer parameters). Graphical models for discrete data are a special\n1If the cliques are separated, then the potentials can be dens ities, but this is in general\nnot the case.", "648": "630 17. Undirected Graphical Models\ncase of loglinear models for multiway contingency tables (Bishop et al.,\n1975, e.g.); in that language f(2)is referred to as the \u201cno second-order\ninteraction\u201d model.\nFor the remainder of this chapter we focus on pairwise Markov graphs\n(Koller and Friedman, 2007). Here there is a potential function for each\nedge (pair of variables as in f(2)above), and at most second\u2013order interac-\ntions are represented. These are more parsimonious in terms of parameters,\neasier to work with, and give the minimal complexity implied by the graph\nstructure. The models for both continuous and discrete data are functions\nof only the pairwise marginal distributions of the variables represented in\nthe edge set.\n17.3 Undirected Graphical Models for Continuous\nVariables\nHere we consider Markov networks where all the variables are continuous.\nThe Gaussian distribution is almost always used for such graphical models,\nbecause of its convenient analytical properties. We assume that the observa-\ntions have a multivariate Gaussian distribution with mean \u03b8and covariance\nmatrix \u03a3. Since the Gaussian distribution represents at most second-order\nrelationships, it automatically encodes a pairwise Markov graph. The graph\nin Figure 17.1 is an example of a Gaussian graphical model.\nThe Gaussian distribution has the property that all conditional distri-\nbutions are also Gaussian. The inverse covariance matrix \u03a3\u22121contains\ninformation about the partial covariances between the variables; that is,\nthe covariances between pairs iandj, conditioned on all other variables.\nIn particular, if the ijth component of \u0398=\u03a3\u22121is zero, then variables iand\njare conditionally independent, given the other variables (Exercise 17.3).\nIt is instructive to examine the conditional distribution of one variable\nversus the rest, where the role of \u0398is explicit. Suppose we partition X=\n(Z,Y) where Z= (X1,... ,X p\u22121) consists of the \ufb01rst p\u22121 variables and\nY=Xpis the last. Then we have the conditional distribution of YgiveZ\n(Mardia et al., 1979, e.g.)\nY|Z=z\u223cN/parenleftbig\n\u03b8Y+ (z\u2212\u03b8Z)T\u03a3\u22121\nZZ\u03c3ZY, \u03c3Y Y\u2212\u03c3T\nZY\u03a3\u22121\nZZ\u03c3ZY/parenrightbig\n,(17.6)\nwhere we have partitioned \u03a3as\n\u03a3=/parenleftbigg\u03a3ZZ\u03c3ZY\n\u03c3T\nZY\u03c3Y Y/parenrightbigg\n. (17.7)\nThe conditional mean in (17.6) has exactly the same form as the pop-\nulation multiple linear regression of YonZ, with regression coe\ufb03cient\n\u03b2=\u03a3\u22121\nZZ\u03c3ZY[see (2.16) on page 19]. If we partition \u0398in the same way,\nsince\u03a3\u0398=Istandard formulas for partitioned inverses give", "649": "17.3 Undirected Graphical Models for Continuous Variables 631\n\u03b8ZY=\u2212\u03b8Y Y\u2264\u03a3\u22121\nZZ\u03c3ZY, (17.8)\nwhere 1 /\u03b8Y Y=\u03c3Y Y\u2212\u03c3T\nZY\u03a3\u22121\nZZ\u03c3ZY>0. Hence\n\u03b2=\u03a3\u22121\nZZ\u03c3ZY\n=\u2212\u03b8ZY/\u03b8Y Y.(17.9)\nWe have learned two things here:\n\u2022The dependence of YonZin (17.6) is in the mean term alone. Here\nwe see explicitly that zero elements in \u03b2and hence \u03b8ZYmean that\nthe corresponding elements of Zare conditionally independent of Y,\ngiven the rest.\n\u2022We can learn about this dependence structure through multiple linear\nregression.\nThus\u0398captures all the second-order information (both structural and\nquantitative) needed to describe the conditional distribution of each node\ngiven the rest, and is the so-called \u201cnatural\u201d parameter for the Gaussian\ngraphical model2.\nAnother (di\ufb00erent) kind of graphical model is the covariance graph orrel-\nevance network , in which vertices are connected by bidirectional edges if the\ncovariance (rather than the partial covariance) between the corresponding\nvariables is nonzero. These are popular in genomics, see especially Butte\net al. (2000). The negative log-likelihood from these models is not convex,\nmaking the computations more challenging (Chaudhuri et al., 2007).\n17.3.1 Estimation of the Parameters when the Graph\nStructure is Known\nGiven some realizations of X, we would like to estimate the parameters\nof an undirected graph that approximates their joint distribution. Suppose\n\ufb01rst that the graph is complete (fully connected). We assume that we have\nNmultivariate normal realizations xi, i= 1,... ,N with population mean\n\u03b8and covariance \u03a3. Let\nS=1\nNN/summationdisplay\ni=1(xi\u2212\u00afx)(xi\u2212\u00afx)T(17.10)\nbe the empirical covariance matrix, with \u00af xthe sample mean vector. Ignoring\nconstants, the log-likelihood of the data can be written as\n2The distribution arising from a Gaussian graphical model is a Wishart distribution.\nThis is a member of the exponential family, with canonical or \u201cnatural\u201d parameter\n\u0398=\u03a3\u22121. Indeed, the partially maximized log-likelihood (17.11) i s (up to constants)\nthe Wishart log-likelihood.", "650": "632 17. Undirected Graphical Models\n\u2113(\u0398) = log det \u0398\u2212trace(S\u0398). (17.11)\nIn (17.11) we have partially maximized with respect to the mean parameter\n\u03b8. The quantity \u2212\u2113(\u0398) is a convex function of \u0398. It is easy to show that\nthe maximum likelihood estimate of \u03a3is simply S.\nNow to make the graph more useful (especially in high-dimensional set-\ntings) let\u2019s assume that some of the edges are missing; for example, the\nedge between PIP3andErkis one of several missing in Figure 17.1. As we\nhave seen, for the Gaussian distribution this implies that the correspond-\ning entries of \u0398=\u03a3\u22121are zero. Hence we now would like to maximize\n(17.11) under the constraints that some pre-de\ufb01ned subset of the parame-\nters are zero. This is an equality-constrained convex optimization problem,\nand a number of methods have been proposed for solving it, in particular\nthe iterative proportional \ufb01tting procedure (Speed and Kiiveri, 1986). This\nand other methods are summarized for example in Whittaker (1990) and\nLauritzen (1996). These methods exploit the simpli\ufb01cations that arise from\ndecomposing the graph into its maximal cliques, as described in the previ-\nous section. Here we outline a simple alternate approach, that exploits the\nsparsity in a di\ufb00erent way. The fruits of this approach will become apparent\nlater when we discuss the problem of estimation of the graph structure.\nThe idea is based on linear regression, as inspired by (17.6) and (17.9).\nIn particular, suppose that we want to estimate the edge parameters \u03b8ijfor\nthe vertices that are joined to a given vertex i, restricting those that are not\njoined to be zero. Then it would seem that the linear regression of the node\nivalues on the other relevant vertices might provide a reasonable estimate.\nBut this ignores the dependence structure among the predictors in this\nregression. It turns out that if instead we use our current (model-based)\nestimate of the cross-product matrix of the predictors when we perform\nour regressions, this gives the correct solutions and solves the constrained\nmaximum-likelihood problem exactly. We now give details.\nTo constrain the log-likelihood (17.11), we add Lagrange constants for\nall missing edges\n\u2113C(\u0398) = log det \u0398\u2212trace(S\u0398)\u2212/summationdisplay\n(j,k)/ne}ationslash\u2208E\u03b3jk\u03b8jk. (17.12)\nThe gradient equation for maximizing (17.12) can be written as\n\u0398\u22121\u2212S\u2212\u0393=0, (17.13)\nusing the fact that the derivative of log det \u0398equals \u0398\u22121(Boyd and Van-\ndenberghe, 2004, for example, page 641). \u0393is a matrix of Lagrange param-\neters with nonzero values for all pairs with edges absent.\nWe will show how we can use regression to solve for \u0398and its inverse\nW=\u0398\u22121one row and column at a time. For simplicity let\u2019s focus on the\nlast row and column. Then the upper right block of equation (17.13) can\nbe written as", "651": "17.3 Undirected Graphical Models for Continuous Variables 633\nw12\u2212s12\u2212\u03b312= 0. (17.14)\nHere we have partitioned the matrices into two parts as in (17.7): part 1\nbeing the \ufb01rst p\u22121 rows and columns, and part 2 the pth row and column.\nWithWand its inverse \u0398partitioned in a similar fashion, we have\n/parenleftbiggW11w12\nwT\n12w22/parenrightbigg/parenleftbigg\u039811\u03b812\n\u03b8T\n12\u03b822/parenrightbigg\n=/parenleftbiggI0\n0T1/parenrightbigg\n. (17.15)\nThis implies\nw12=\u2212W11\u03b812/\u03b822 (17.16)\n=W11\u03b2 (17.17)\nwhere \u03b2=\u2212\u03b812/\u03b822as in (17.9). Now substituting (17.17) into (17.14)\ngives\nW11\u03b2\u2212s12\u2212\u03b312= 0. (17.18)\nThese can be interpreted as the p\u22121 estimating equations for the con-\nstrained regression of Xpon the other predictors, except that the observed\nmean cross-products matrix S11is replaced by W11, the current estimated\ncovariance matrix from the model.\nNow we can solve (17.18) by simple subset regression. Suppose there are\np\u2212qnonzero elements in \u03b312\u2014i.e., p\u2212qedges constrained to be zero. These\np\u2212qrows carry no information and can be removed. Furthermore we can\nreduce \u03b2to\u03b2\u2217by removing its p\u2212qzero elements, yielding the reduced\nq\u00d7qsystem of equations\nW\u2217\n11\u03b2\u2217\u2212s\u2217\n12= 0, (17.19)\nwith solution \u02c6\u03b2\u2217=W\u2217\n11\u22121s\u2217\n12. This is padded with p\u2212qzeros to give \u02c6\u03b2.\nAlthough it appears from (17.16) that we only recover the elements \u03b812\nup to a scale factor 1 /\u03b822, it is easy to show that\n1\n\u03b822=w22\u2212wT\n12\u03b2 (17.20)\n(using partitioned inverse formulas). Also w22=s22, since the diagonal of\n\u0393in (17.13) is zero.\nThis leads to the simple iterative procedure given in Algorithm 17.1 for\nestimating both \u02c6Wand its inverse \u02c6\u0398, subject to the constraints of the\nmissing edges.\nNote that this algorithm makes conceptual sense. The graph estimation\nproblem is not pseparate regression problems, but rather pcoupled prob-\nlems. The use of the common Win step (b), in place of the observed\ncross-products matrix, couples the problems together in the appropriate\nfashion. Surprisingly, we were not able to \ufb01nd this procedure in the lit-\nerature. However it is related to the covariance selection procedures of", "652": "634 17. Undirected Graphical Models\nAlgorithm 17.1 A Modi\ufb01ed Regression Algorithm for Estimation of an\nUndirected Gaussian Graphical Model with Known Structure.\n1. Initialize W=S.\n2. Repeat for j= 1,2,... ,p, 1,2,... ,p,... until convergence:\n(a) Partition the matrix Winto part 1: all but the jth row and\ncolumn, and part 2: the jth row and column.\n(b) Solve W\u2217\n11\u03b2\u2217\u2212s\u2217\n12= 0 for the unconstrained edge parameters\n\u03b2\u2217, using the reduced system of equations as in (17.19). Obtain\n\u02c6\u03b2by padding \u02c6\u03b2\u2217with zeros in the appropriate positions.\n(c) Update w12=W11\u02c6\u03b2\n3. In the \ufb01nal cycle (for each j) solve for \u02c6\u03b812=\u2212\u02c6\u03b2\u2264\u02c6\u03b822, with 1 /\u02c6\u03b822=\ns22\u2212wT\n12\u02c6\u03b2.\nX1X2 X3\nX4S=\uf8eb\n\uf8ec\uf8ec\uf8ed10 1 5 4\n1 10 2 6\n5 2 10 3\n4 6 3 10\uf8f6\n\uf8f7\uf8f7\uf8f8\nFIGURE 17.4. A simple graph for illustration, along with the empirical cova ri-\nance matrix.\nDempster (1972), and is similar in \ufb02avor to the iterative conditional \ufb01tti ng\nprocedure for covariance graphs, proposed by Chaudhuri et al. (2007).\nHere is a little example, borrowed from Whittaker (1990). Suppose that\nour model is as depicted in Figure 17.4, along with its empirical covariance\nmatrix S. We apply algorithm (17.1) to this problem; for example, in the\nmodi\ufb01ed regression for variable 1 in step (b), variable 3 is left out. The\nprocedure quickly converged to the solutions:\n\u02c6\u03a3=\uf8eb\n\uf8ec\uf8ed10.00 1 .00 1.31 4.00\n1.00 10 .00 2 .00 0.87\n1.31 2.00 10 .00 3 .00\n4.00 0.87 3.00 10 .00\uf8f6\n\uf8f7\uf8f8,\u02c6\u03a3\u22121=\uf8eb\n\uf8ec\uf8ed0.12\u22120.010.00\u22120.05\n\u22120.01 0 .11\u22120.020.00\n0.00\u22120.02 0 .11\u22120.03\n\u22120.050.00\u22120.03 0 .13\uf8f6\n\uf8f7\uf8f8.\nNote the zeroes in \u02c6\u03a3\u22121, corresponding to the missing edges (1,3) and (2,4).\nNote also that the corresponding elements in \u02c6\u03a3are the only elements dif-\nferent from S. The estimation of \u02c6\u03a3is an example of what is sometimes\ncalled the positive de\ufb01nite \u201ccompletion\u201d of S.", "653": "17.3 Undirected Graphical Models for Continuous Variables 635\n17.3.2 Estimation of the Graph Structure\nIn most cases we do not know which edges to omit from our graph, and\nso would like to try to discover this from the data itself. In recent years a\nnumber of authors have proposed the use of L1(lasso) regularization for\nthis purpose.\nMeinshausen and B\u00a8 uhlmann (2006) take a simple approach to the prob-\nlem: rather than trying to fully estimate \u03a3or\u0398=\u03a3\u22121, they only estimate\nwhich components of \u03b8ijare nonzero. To do this, they \ufb01t a lasso regression\nusing each variable as the response and the others as predictors. The com-\nponent \u03b8ijis then estimated to be nonzero if either the estimated coe\ufb03cient\nof variable ionjis nonzero, orthe estimated coe\ufb03cient of variable jon\niis nonzero (alternatively they use an andrule). They show that asymp-\ntotically this procedure consistently estimates the set of nonzero elements\nof\u0398.\nWe can take a more systematic approach with the lasso penalty, follow ing\nthe development of the previous section. Consider maximizing the penalized\nlog-likelihood\nlog det \u0398\u2212trace(S\u0398)\u2212\u03bb||\u0398||1, (17.21)\nwhere ||\u0398||1is the L1norm\u2014the sum of the absolute values of the elements\nof\u03a3\u22121, and we have ignored constants. The negative of this penalized\nlikelihood is a convex function of \u0398.\nIt turns out that one can adapt the lasso to give the exact maximizer of\nthe penalized log-likelihood. In particular, we simply replace the modi\ufb01ed\nregression step (b) in Algorithm 17.1 by a modi\ufb01ed lasso step. Here are the\ndetails.\nThe analog of the gradient equation (17.13) is now\n\u0398\u22121\u2212S\u2212\u03bb\u2264Sign(\u0398) =0. (17.22)\nHere we use sub-gradient notation, with Sign( \u03b8jk) = sign( \u03b8jk) if\u03b8jk\u221dne}ationslash= 0,\nelse Sign( \u03b8jk)\u2208[\u22121,1] if\u03b8jk= 0. Continuing the development in the\nprevious section, we reach the analog of (17.18)\nW11\u03b2\u2212s12+\u03bb\u2264Sign(\u03b2) = 0 (17.23)\n(recall that \u03b2and\u03b812have opposite signs). We will now see that this system\nis exactly equivalent to the estimating equations for a lasso regression.\nConsider the usual regression setup with outcome variables yand pre-\ndictor matrix Z. There the lasso minimizes\n1\n2(y\u2212Z\u03b2)T(y\u2212Z\u03b2) +\u03bb\u2264 ||\u03b2||1 (17.24)\n[see (3.52) on page 68; here we have added a factor1\n2for convenience]. The\ngradient of this expression is", "654": "636 17. Undirected Graphical Models\nAlgorithm 17.2 Graphical Lasso.\n1. Initialize W=S+\u03bbI. The diagonal of Wremains unchanged in\nwhat follows.\n2. Repeat for j= 1,2,... p, 1,2,... p,... until convergence:\n(a) Partition the matrix Winto part 1: all but the jth row and\ncolumn, and part 2: the jth row and column.\n(b) Solve the estimating equations W11\u03b2\u2212s12+\u03bb\u2264Sign(\u03b2) = 0\nusing the cyclical coordinate-descent algorithm (17.26) for the\nmodi\ufb01ed lasso.\n(c) Update w12=W11\u02c6\u03b2\n3. In the \ufb01nal cycle (for each j) solve for \u02c6\u03b812=\u2212\u02c6\u03b2\u2264\u02c6\u03b822, with 1 /\u02c6\u03b822=\nw22\u2212wT\n12\u02c6\u03b2.\nZTZ\u03b2\u2212ZTy+\u03bb\u2264Sign(\u03b2) = 0 (17.25)\nSo up to a factor 1 /N,ZTyis the analog of s12, and we replace ZTZby\nW11, the estimated cross-product matrix from our current model.\nThe resulting procedure is called the graphical lasso , proposed by Fried-\nman et al. (2008b) building on the work of Banerjee et al. (2008). It is\nsummarized in Algorithm 17.2.\nFriedman et al. (2008b) use the pathwise coordinate descent method\n(Section 3.8.6) to solve the modi\ufb01ed lasso problem at each stage. Here are\nthe details of pathwise coordinate descent for the graphical lasso algorithm.\nLetting V=W11, the update has the form\n\u02c6\u03b2j\u2190S/parenleft\uf8ecig\ns12j\u2212/summationdisplay\nk/ne}ationslash=jVkj\u02c6\u03b2k,\u03bb/parenright\uf8ecig\n/Vjj (17.26)\nforj= 1,2,... ,p \u22121,1,2,... ,p \u22121,..., where Sis the soft-threshold\noperator:\nS(x,t) = sign( x)(|x| \u2212t)+. (17.27)\nThe procedure cycles through the predictors until convergence.\nIt is easy to show that the diagonal elements wjjof the solution matrix\nWare simply sjj+\u03bb, and these are \ufb01xed in step 1 of Algorithm 17.23.\nThe graphical lasso algorithm is extremely fast, and can solve a moder-\nately sparse problem with 1000 nodes in less than a minute. It is easy to\nmodify the algorithm to have edge-speci\ufb01c penalty parameters \u03bbjk; since\n3An alternative formulation of the problem (17.21) can be pos ed, where we don\u2019t\npenalize the diagonal of \u0398. Then the diagonal elements wjjof the solution matrix are\nsjj, and the rest of the algorithm is unchanged.", "655": "17.3 Undirected Graphical Models for Continuous Variables 637\n\u03bbjk=\u221ewill force \u02c6\u03b8jkto be zero, this algorithm subsumes Algorithm 17.1.\nBy casting the sparse inverse-covariance problem as a series of regressions,\none can also quickly compute and examine the solution paths as a function\nof the penalty parameter \u03bb. More details can be found in Friedman et al.\n(2008b).\nRaf\nMek\nPlcg\nPIP2\nPIP3\nErk AktPKAPKCP38JnkRaf\nMek\nPlcg\nPIP2\nPIP3\nErk AktPKAPKCP38Jnk\nRaf\nMek\nPlcg\nPIP2\nPIP3\nErk AktPKAPKCP38JnkRaf\nMek\nPlcg\nPIP2\nPIP3\nErk AktPKAPKCP38Jnk\u03bb= 0 \u03bb= 7\u03bb= 27 \u03bb= 36\nFIGURE 17.5. Four di\ufb00erent graphical-lasso solutions for the \ufb02ow-cytomet ry\ndata.\nFigure 17.1 shows the result of applying the graphical lasso to the \ufb02ow-\ncytometry dataset. Here the lasso penalty parameter \u03bbwas set at 14. In\npractice it is informative to examine the di\ufb00erent sets of graphs that are\nobtained as \u03bbis varied. Figure 17.5 shows four di\ufb00erent solutions. The\ngraph becomes more sparse as the penalty parameter is increased.\nFinally note that the values at some of the nodes in a graphical model can\nbe unobserved; that is, missing or hidden. If only some values are missing\nat a node, the EM algorithm can be used to impute the missing values", "656": "638 17. Undirected Graphical Models\n(Exercise 17.9). However, sometimes the entire node is hidden or latent.\nIn the Gaussian model, if a node has all missing values, due to linearity\none can simply average over the missing nodes to yield another Gaussian\nmodel over the observed nodes. Hence the inclusion of hidden nodes does\nnot enrich the resulting model for the observed nodes; in fact, it imposes\nadditional structure on its covariance matrix. However in the discrete model\n(described next) the inherent nonlinearities make hidden units a powerful\nway of expanding the model.\n17.4 Undirected Graphical Models for Discrete\nVariables\nUndirected Markov networks with all discrete variables are popular, and\nin particular pairwise Markov networks with binary variables being the\nmost common. They are sometimes called Ising models in the statistical\nmechanics literature, and Boltzmann machines in the machine learning lit-\nerature, where the vertices are referred to as \u201cnodes\u201d or \u201cunits\u201d and are\nbinary-valued.\nIn addition, the values at each node can be observed (\u201cvisible\u201d) or un-\nobserved (\u201chidden\u201d). The nodes are often organized in layers, similar to a\nneural network. Boltzmann machines are useful both for unsupervised and\nsupervised learning, especially for structured input data such as images,\nbut have been hampered by computational di\ufb03culties. Figure 17.6 shows\na restricted Boltzmann machine (discussed later), in which some variables\nare hidden, and only some pairs of nodes are connected. We \ufb01rst consider\nthe simpler case in which all pnodes are visible with edge pairs ( j,k) enu-\nmerated in E.\nDenoting the binary valued variable at node jbyXj, the Ising model\nfor their joint probabilities is given by\np(X,\u0398) = exp/bracketleft\uf8ecig/summationdisplay\n(j,k)\u2208E\u03b8jkXjXk\u2212\u03a6(\u0398)/bracketright\uf8ecig\nforX\u2208 X, (17.28)\nwithX={0,1}p. As with the Gaussian model of the previous section,\nonly pairwise interactions are modeled. The Ising model was developed in\nstatistical mechanics, and is now used more generally to model the joint\ne\ufb00ects of pairwise interactions. \u03a6( \u0398) is the log of the partition function,\nand is de\ufb01ned by\n\u03a6(\u0398) = log/summationdisplay\nx\u2208X/bracketleft\uf8ecig\nexp/parenleft\uf8ecig/summationdisplay\n(j,k)\u2208E\u03b8jkxjxk/parenright\uf8ecig/bracketright\uf8ecig\n. (17.29)\nThe partition function ensures that the probabilities add to one over the\nsample space. The terms \u03b8jkXjXkrepresent a particular parametrization", "657": "17.4 Undirected Graphical Models for Discrete Variables 639\nof the (log) potential functions (17.5), and for technical reasons requires\naconstant nodeX0\u22611 to be included (Exercise 17.10), with \u201cedges\u201d to\nall the other nodes. In the statistics literature, this model is equivalent\nto a \ufb01rst-order-interaction Poisson log-linear model for multiway tables of\ncounts (Bishop et al., 1975; McCullagh and Nelder, 1989; Agresti, 2002).\nThe Ising model implies a logistic form for each node conditional on the\nothers (exercise 17.11):\nPr(Xj= 1|X\u2212j=x\u2212j) =1\n1 + exp( \u2212\u03b8j0\u2212/summationtext\n(j,k)\u2208E\u03b8jkxk),(17.30)\nwhere X\u2212jdenotes all of the nodes except j. Hence the parameter \u03b8jk\nmeasures the dependence of XjonXk, conditional on the other nodes.\n17.4.1 Estimation of the Parameters when the Graph\nStructure is Known\nGiven some data from this model, how can we estimate the parameters?\nSuppose we have observations xi= (xi1,xi2,... ,x ip)\u2208 {0,1}p, i= 1,... ,N .\nThe log-likelihood is\n\u2113(\u0398) =N/summationdisplay\ni=1log Pr \u0398(Xi=xi)\n=N/summationdisplay\ni=1\uf8ee\n\uf8f0/summationdisplay\n(j,k)\u2208E\u03b8jkxijxik\u2212\u03a6(\u0398)\uf8f9\n\uf8fb (17.31)\nThe gradient of the log-likelihood is\n\u2202\u2113(\u0398)\n\u2202\u03b8jk=N/summationdisplay\ni=1xijxik\u2212N\u2202\u03a6(\u0398)\n\u2202\u03b8jk(17.32)\nand\n\u2202\u03a6(\u0398)\n\u2202\u03b8jk=/summationdisplay\nx\u2208Xxjxk\u2264p(x,\u0398)\n= E \u0398(XjXk) (17.33)\nSetting the gradient to zero gives\n\u02c6E(XjXk)\u2212E\u0398(XjXk) = 0 (17.34)\nwhere we have de\ufb01ned", "658": "640 17. Undirected Graphical Models\n\u02c6E(XjXk) =1\nNN/summationdisplay\ni=1xijxik, (17.35)\nthe expectation taken with respect to the empirical distribution of the data.\nLooking at (17.34), we see that the maximum likelihood estimates simply\nmatch the estimated inner products between the nodes to their observed\ninner products. This is a standard form for the score (gradient) equation\nfor exponential family models, in which su\ufb03cient statistics are set equal t o\ntheir expectations under the model.\nTo \ufb01nd the maximum likelihood estimates, we can use gradient search\nor Newton methods. However the computation of E \u0398(XjXk) involves enu-\nmeration of p(X,\u0398) over 2p\u22122of the |X|= 2ppossible values of X, and is\nnot generally feasible for large p(e.g., larger than about 30). For smaller\np, a number of standard statistical approaches are available:\nPoisson log-linear modeling , where we treat the problem as a large regres-\nsion problem (Exercise 17.12). The response vector yis the vector of\n2pcounts in each of the cells of the multiway tabulation of the data4.\nThe predictor matrix Zhas 2prows and up to 1+ p+p2columns that\ncharacterize each of the cells, although this number depends on the\nsparsity of the graph. The computational cost is essentially that of a\nregression problem of this size, which is O(p42p) and is manageable\nforp <20. The Newton updates are typically computed by iteratively\nreweighted least squares, and the number of steps is usually in the\nsingle digits. See Agresti (2002) and McCullagh and Nelder (1989) for\ndetails. Standard software (such as the Rpackageglm) can be used\nto \ufb01t this model.\nGradient descent requires at most O(p22p\u22122) computations to compute\nthe gradient, but may require many more gradient steps than the\nsecond\u2013order Newton methods. Nevertheless, it can handle slightly\nlarger problems with p\u226430. These computations can be reduced\nby exploiting the special clique structure in sparse graphs, using the\njunction-tree algorithm. Details are not given here.\nIterative proportional \ufb01tting (IPF) performs cyclical coordinate descent on\nthe gradient equations (17.34). At each step a parameter is updated\nso that its gradient equation is exactly zero. This is done in a cyclical\nfashion until all the gradients are zero. One complete cycle costs the\nsame as a gradient evaluation, but may be more e\ufb03cient. Jirou\u00b4 sek and\nP\u02c7 reu\u02c7 cil (1995) implement an e\ufb03cient version of IPF, using junction\ntrees.\n4Each of the cell counts is treated as an independent Poisson v ariable. We get the\nmultinomial model corresponding to (17.28) by conditionin g on the total count N(which\nis also Poisson under this framework).", "659": "17.4 Undirected Graphical Models for Discrete Variables 641\nWhen pis large ( >30) other approaches have been used to approximate\nthe gradient.\n\u2022The mean \ufb01eld approximation (Peterson and Anderson, 1987) esti-\nmates E \u0398(XjXk) by E \u0398(Xj)E\u0398(Xj), and replaces the input vari-\nables by their means, leading to a set of nonlinear equations for the\nparameters \u03b8jk.\n\u2022To obtain near-exact solutions, Gibbs sampling (Section 8.6) is used\nto approximate E \u0398(XjXk) by successively sampling from the esti-\nmated model probabilities Pr \u0398(Xj|X\u2212j) (see e.g. Ripley (1996)).\nWe have not discussed decomposable models , for which the maximum\nlikelihood estimates can be found in closed form without any iteration\nwhatsoever. These models arise, for example, in trees: special graphs with\ntree-structured topology. When computational tractability is a concern,\ntrees represent a useful class of models and they sidestep the computational\nconcerns raised in this section. For details, see for example Chapter 12 of\nWhittaker (1990).\n17.4.2 Hidden Nodes\nWe can increase the complexity of a discrete Markov network by including\nlatent or hidden nodes. Suppose that a subset of the variables XHare\nunobserved or \u201chidden\u201d, and the remainder XVare observed or \u201cvisible.\u201d\nThen the log-likelihood of the observed data is\n\u2113(\u0398) =N/summationdisplay\ni=1log[Pr \u0398(XV=xiV)]\n=N/summationdisplay\ni=1/bracketleft\uf8ecig\nlog/summationdisplay\nxH\u2208XHexp/summationdisplay\n(j,k)\u2208E(\u03b8jkxijxik\u2212\u03a6(\u0398))/bracketright\uf8ecig\n.(17.36)\nThe sum over xHmeans that we are summing over all possible {0,1}values\nfor the hidden units. The gradient works out to be\nd\u2113(\u0398)\nd\u03b8jk=\u02c6EVE\u0398(XjXk|XV)\u2212E\u0398(XjXk) (17.37)\nThe \ufb01rst term is an empirical average of XjXkif both are visible; if one\nor both are hidden, they are \ufb01rst imputed given the visible data, and then\naveraged over the hidden variables. The second term is the unconditional\nexpectation of XjXk.\nThe inner expectation in the \ufb01rst term can be evaluated using basic rules\nof conditional expectation and properties of Bernoulli random variables. In\ndetail, for observation i", "660": "642 17. Undirected Graphical Models\nE\u0398(XjXk|XV=xiV) =/braceleftbigxijxik ifj,k\u2208 V\nxijPr\u0398(Xk= 1|XV=xiV) if j\u2208 V,k\u2208 H\nPr\u0398(Xj= 1,Xk= 1|XV=xiV) ifj,k\u2208 H.\n(17.38)\nNow two separate runs of Gibbs sampling are required; the \ufb01rst to estimate\nE\u0398(XjXk) by sampling from the model as above, and the second to esti-\nmate E \u0398(XjXk|XV=xiV). In this latter run, the visible units are \ufb01xed\n(\u201cclamped\u201d) at their observed values and only the hidden variables are\nsampled. Gibbs sampling must be done for each observation in the training\nset, at each stage of the gradient search. As a result this procedure can be\nvery slow, even for moderate-sized models. In Section 17.4.4 we consider\nfurther model restrictions to make these computations manageable.\n17.4.3 Estimation of the Graph Structure\nThe use of a lasso penalty with binary pairwise Markov networks has been\nsuggested by Lee et al. (2007) and Wainwright et al. (2007). The \ufb01rst au-\nthors investigate a conjugate gradient procedure for exact maximization of\na penalized log-likelihood. The bottleneck is the computation of E \u0398(XjXk)\nin the gradient; exact computation via the junction tree algorithm is man-\nageable for sparse graphs but becomes unwieldy for dense graphs.\nThe second authors propose an approximate solution, analogous to the\nMeinshausen and B\u00a8 uhlmann (2006) approach for the Gaussian graphical\nmodel. They \ufb01t an L1-penalized logistic regression model to each node as\na function of the other nodes, and then symmetrize the edge parameter\nestimates in some fashion. For example if \u02dc\u03b8jkis the estimate of the j-k\nedge parameter from the logistic model for outcome node j, the \u201cmin\u201d\nsymmetrization sets \u02c6\u03b8jkto either \u02dc\u03b8jkor\u02dc\u03b8kj, whichever is smallest in abso-\nlute value. The \u201cmax\u201d criterion is de\ufb01ned similarly. They show that under\ncertain conditions either approximation estimates the nonzero edges cor-\nrectly as the sample size goes to in\ufb01nity. Hoe\ufb02ing and Tibshirani (2008)\nextend the graphical lasso to discrete Markov networks, obtaining a pro-\ncedure which is somewhat faster than conjugate gradients, but still must\ndeal with computation of E \u0398(XjXk). They also compare the exact and\napproximate solutions in an extensive simulation study and \ufb01nd the \u201cmin\u201d\nor \u201cmax\u201d approximations are only slightly less accurate than the exact pro-\ncedure, both for estimating the nonzero edges and for estimating the actual\nvalues of the edge parameters, and are much faster. Furthermore, they can\nhandle denser graphs because they never need to compute the quantities\nE\u0398(XjXk).\nFinally, we point out a key di\ufb00erence between the Gaussian and binary\nmodels. In the Gaussian case, both \u03a3and its inverse will often be of interest,\nand the graphical lasso procedure delivers estimates for both of these quan-\ntities. However, the approximation of Meinshausen and B\u00a8 uhlmann (2006)\nfor Gaussian graphical models, analogous to the Wainwright et al. (2007 )", "661": "17.4 Undirected Graphical Models for Discrete Variables 643\nXjXk\nX\u2113\nVisible V1 Visible V2Hidden H\n\u03b8jk\nFIGURE 17.6. A restricted Boltzmann machine (RBM) in which there are no\nconnections between nodes in the same layer. The visible units are subdivided to\nallow the RBM to model the joint density of feature V1and their labels V2.\napproximation for the binary case, only yields an estimate of \u03a3\u22121. In con-\ntrast, in the Markov model for binary data, \u0398is the object of interest, and\nits inverse is not of interest. The approximate method of Wainwright et a l.\n(2007) estimates \u0398e\ufb03ciently and hence is an attractive solution for the\nbinary problem.\n17.4.4 Restricted Boltzmann Machines\nIn this section we consider a particular architecture for graphical models\ninspired by neural networks, where the units are organized in layers. A\nrestricted Boltzmann machine (RBM) consists of one layer of visible units\nand one layer of hidden units with no connections within each layer. It\nis much simpler to compute the conditional expectations (as in 17.37 and\n17.38) if the connections between hidden units are removed5. Figure 17.6\nshows an example; the visible layer is divided into input variables V1and\noutput variables V2, and there is a hidden layer H. We denote such a\nnetwork by\nV1\u2194 H \u2194 V 2. (17.39)\nFor example, V1could be the binary pixels of an image of a handwritten\ndigit, and V2could have 10 units, one for each of the observed class labels\n0-9.\nThe restricted form of this model simpli\ufb01es the Gibbs sampling for es-\ntimating the expectations in (17.37), since the variables in each layer are\nindependent of one another, given the variables in the other layers. Hence\nthey can be sampled together, using the conditional probabilities given by\nexpression (17.30).\nThe resulting model is less general than a Boltzmann machine, but is still\nuseful; for example it can learn to extract interesting features from images.\n5We thank Geo\ufb00rey Hinton for assistance in the preparation of the material on RBMs.", "662": "644 17. Undirected Graphical Models\nBy alternately sampling the variables in each layer of the RBM shown\nin Figure 17.6, it is possible to generate samples from the joint density\nmodel. If the V1part of the visible layer is clamped at a particular feature\nvector during the alternating sampling, it is possible to sample from the\ndistribution over labels given V1. Alternatively classi\ufb01cation of test items\ncan also be achieved by comparing the unnormalized joint densities of each\nlabel category with the observed features. We do not need to compute the\npartition function as it is the same for all of these combinations.\nAs noted the restricted Boltzmann machine has the same generic form\nas a single hidden layer neural network (Section 11.3). The edges in the\nlatter model are directed, the hidden units are usually real-valued, and the\n\ufb01tting criterion is di\ufb00erent. The neural network minimizes the error (cross-\nentropy) between the targets and their model predictions, conditional on\nthe input features. In contrast, the restricted Boltzmann machine maxi-\nmizes the log-likelihood for the joint distribution of all visible units\u2014tha t\nis, the features and targets. It can extract information from the input fea-\ntures that is useful for predicting the labels, but, unlike supervised learning\nmethods, it may also use some of its hidden units to model structure in the\nfeature vectors that is not immediately relevant for predicting the labels.\nThese features may turn out to be useful, however, when combined with\nfeatures derived from other hidden layers.\nUnfortunately, Gibbs sampling in a restricted Boltzmann machine can\nbe very slow, as it can take a long time to reach stationarity. As the net -\nwork weights get larger, the chain mixes more slowly and we need to run\nmore steps to get the unconditional estimates. Hinton (2002) noticed em-\npirically that learning still works well if we estimate the second expectatio n\nin (17.37) by starting the Markov chain at the data and only running for a\nfew steps (instead of to convergence). He calls this contrastive divergence :\nwe sample HgivenV1,V2, then V1,V2givenHand \ufb01nally HgivenV1,V2\nagain. The idea is that when the parameters are far from the solution, it\nmay be wasteful to iterate the Gibbs sampler to stationarity, as just a si ngle\niteration will reveal a good direction for moving the estimates.\nWe now give an example to illustrate the use of an RBM. Using con-\ntrastive divergence, it is possible to train an RBM to recognize hand-written\ndigits from the MNIST dataset (LeCun et al., 1998). With 2000 hidden\nunits, 784 visible units for representing binary pixel intensities and one\n10-way multinomial visible unit for representing labels, the RBM achieves\nan error rate of 1.9% on the test set. This is a little higher than the 1.4%\nachieved by a support vector machine and comparable to the error rate\nachieved by a neural network trained with backpropagation. The error rate\nof the RBM, however, can be reduced to 1.25% by replacing the 784 pixel\nintensities by 500 features that are produced from the images without using\nany label information. First, an RBM with 784 visible units and 500 hidden\nunits is trained, using contrastive divergence, to model the set of images.\nThen the hidden states of the \ufb01rst RBM are used as data for training a", "663": "Exercises 645\nFIGURE 17.7. Example of a restricted Boltzmann machine for handwritten\ndigit classi\ufb01cation. The network is depicted in the schematic o n the left. Displayed\non the right are some di\ufb03cult test images that the model class i\ufb01es correctly.\nsecond RBM that has 500 visible units and 500 hidden units. Finally, the\nhidden states of the second RBM are used as the features for training an\nRBM with 2000 hidden units as a joint density model. The details and\njusti\ufb01cation for learning features in this greedy, layer-by-layer way are de-\nscribed in Hinton et al. (2006). Figure 17.7 gives a representation of t he\ncomposite model that is learned in this way and also shows some examples\nof the types of distortion that it can cope with.\nBibliographic Notes\nMuch work has been done in de\ufb01ning and understanding the structure of\ngraphical models. Comprehensive treatments of graphical models can be\nfound in Whittaker (1990), Lauritzen (1996), Cox and Wermuth (1996),\nEdwards (2000), Pearl (2000), Anderson (2003), Jordan (2004), and Kol ler\nand Friedman (2007). Wasserman (2004) gives a brief introduction, and\nChapter 8 of Bishop (2006) gives a more detailed overview. Boltzmann\nmachines were proposed in Ackley et al. (1985). Ripley (1996) has a detailed\nchapter on topics in graphical models that relate to machine learning. We\nfound this particularly useful for its discussion of Boltzmann machines.\nExercises\nEx. 17.1 For the Markov graph of Figure 17.8, list all of the implied condi-\ntional independence relations and \ufb01nd the maximal cliques.", "664": "646 17. Undirected Graphical Models\nX1\nX2X3X4\nX5\nX6\nFIGURE 17.8.\nEx. 17.2 Consider random variables X1,X2,X3,X4. In each of the following\ncases draw a graph that has the given independence relations:\n(a)X1\u22a5X3|X2andX2\u22a5X4|X3.\n(b)X1\u22a5X4|X2,X3andX2\u22a5X4|X1,X3.\n(c)X1\u22a5X4|X2,X3,X1\u22a5X3|X2,X4andX3\u22a5X4|X1,X2.\nEx. 17.3 Let\u03a3be the covariance matrix of a set of pvariables X. Consider\nthe partial covariance matrix \u03a3a.b=\u03a3aa\u2212\u03a3ab\u03a3\u22121\nbb\u03a3babetween the two\nsubsets of variables Xa= (X1,X2) consisting of the \ufb01rst two, and Xb\nthe rest. This is the covariance matrix between these two variables, after\nlinear adjustment for all the rest. In the Gaussian distribution, this is the\ncovariance matrix of the conditional distribution of Xa|Xb. The partial\ncorrelation coe\ufb03cient \u03c1jk|restbetween the pair Xaconditional on the rest\nXb, is simply computed from this partial covariance. De\ufb01ne \u0398=\u03a3\u22121.\n1. Show that \u03a3a.b=\u0398\u22121\naa.\n2. Show that if any o\ufb00-diagonal element of \u0398 is zero, then the partial\ncorrelation coe\ufb03cient between the corresponding variables is zero.\n3. Show that if we treat \u0398as if it were a covariance matrix, and compute\nthe corresponding \u201ccorrelation\u201d matrix\nR= diag(\u0398)\u22121/2\u2264\u0398\u2264diag(\u0398)\u22121/2, (17.40)\nthenrjk=\u2212\u03c1jk|rest\nEx. 17.4 Denote by\nf(X1|X2,X3,... ,X p)\nthe conditional density of X1given X2,... ,X p. If\nf(X1|X2,X3,... ,X p) =f(X1|X3,... ,X p),\nshow that X1\u22a5X2|X3,... ,X p.", "665": "Exercises 647\nEx. 17.5 Consider the setup in Section 17.4.1 with no missing edges. Show\nthat\nS11\u03b2\u2212s12= 0\nare the estimating equations for the multiple regression coe\ufb03cients of the\nlast variable on the rest.\nEx. 17.6 Recovery of \u02c6\u0398=\u02c6\u03a3\u22121from Algorithm 17.1 . Use expression (17.16)\nto derive the standard partitioned inverse expressions\n\u03b812=\u2212W\u22121\n11w12\u03b822 (17.41)\n\u03b822= 1/(w22\u2212wT\n12W\u22121\n11w12). (17.42)\nSince \u02c6\u03b2=W\u22121\n11w12, show that \u02c6\u03b822= 1/(w22\u2212wT\n12\u02c6\u03b2) and \u02c6\u03b812=\u2212\u02c6\u03b2\u02c6\u03b822.\nThus \u02c6\u03b812is a simply rescaling of \u02c6\u03b2by\u2212\u02c6\u03b822.\nEx. 17.7 Write a program to implement the modi\ufb01ed regression procedure\n(17.1) for \ufb01tting the Gaussian graphical model with pre-speci\ufb01ed edges\nmissing. Test it on the \ufb02ow cytometry data from the book website, using\nthe graph of Figure 17.1.\nEx. 17.8\n(a) Write a program to \ufb01t the lasso using the coordinate descent procedure\n(17.26). Compare its results to those from the larsprogram or some\nother convex optimizer, to check that it is working correctly.\n(b) Using the program from (a), write code to implement the graphical\nlasso algorithm (17.2). Apply it to the \ufb02ow cytometry data from the\nbook website. Vary the regularization parameter and examine the\nresulting networks.\nEx. 17.9 Suppose that we have a Gaussian graphical model in which some\nor all of the data at some vertices are missing.\n(a) Consider the EM algorithm for a dataset of Ni.i.d. multivariate ob-\nservations xi\u2208IRpwith mean \u03b8and covariance matrix \u03a3. For each\nsample i, letoiandmiindex the predictors that are observed and\nmissing, respectively. Show that in the E step, the observations are\nimputed from the current estimates of \u03b8and\u03a3:\n\u02c6xi,mi= E(xi,mi|xi,oi,\u03b8) = \u02c6\u03b8mi+\u02c6\u03a3mi,oi\u02c6\u03a3\u22121\noi,oi(xi,oi\u2212\u02c6\u03b8oi)\n(17.43)\nwhile in the M step, \u03b8and\u03a3are re-estimated from the empirical\nmean and (modi\ufb01ed) covariance of the imputed data:\n\u02c6\u03b8j=N/summationdisplay\ni=1\u02c6xij/N", "666": "648 17. Undirected Graphical Models\n\u02c6\u03a3jj\u2032=N/summationdisplay\ni=1[(\u02c6xij\u2212\u02c6\u03b8j)(\u02c6xij;\u2212\u02c6\u03b8j\u2032) +ci,jj\u2032]/N (17.44)\nwhere ci,jj\u2032=\u02c6\u03a3jj\u2032ifj,j\u2032\u2208miand zero otherwise. Explain the reason\nfor the correction term ci,jj\u2032(Little and Rubin, 2002).\n(b) Implement the EM algorithm for the Gaussian graphical model using\nthe modi\ufb01ed regression procedure from Exercise 17.7 for the M-step.\n(c) For the \ufb02ow cytometry data on the book website, set the data for the\nlast protein Jnkin the \ufb01rst 1000 observations to missing, \ufb01t the model\nof Figure 17.1, and compare the predicted values to the actual values\nforJnk. Compare the results to those obtained from a regression of\nJnkon the other vertices with edges to Jnkin Figure 17.1, using only\nthe non-missing data.\nEx. 17.10 Using a simple binary graphical model with just two variables,\nshow why it is essential to include a constant node X0\u22611 in the model.\nEx. 17.11 Show that the Ising model (17.28) (17.28) for the joint probabili-\nties in a discrete graphical model implies that the conditional distributions\nhave the logistic form (17.30).\nEx. 17.12 Consider a Poisson regression problem with pbinary variables\nxij, j= 1,... ,p and response variable yiwhich measures the number of\nobservations with predictor xi\u2208 {0,1}p. The design is balanced, in that all\nn= 2ppossible combinations are measured. We assume a log-linear model\nfor the Poisson mean in each cell\nlog\u03b8(X) =\u03b800+/summationdisplay\n(j,k)\u2208Exijxik\u03b8jk, (17.45)\nusing the same notation as in Section 17.4.1 (including the constant variable\nxi0= 1\u2200i). We assume the response is distributed as\nPr(Y=y|X=x) =e\u2212\u03b8(x)\u03b8(x)y\ny!. (17.46)\nWrite down the conditional log-likelihood for the observed responses yi,\nand compute the gradient.\n(a) Show that the gradient equation for \u03b800computes the partition func-\ntion (17.29).\n(b) Show that the gradient equations for the remainder of the parameters\nare equivalent to the gradient (17.34).", "667": "This is page 649\nPrinter: Opaque this\n18\nHigh-Dimensional Problems: p\u226bN\n18.1 When pis Much Bigger than N\nIn this chapter we discuss prediction problems in which the number of\nfeatures pis much larger than the number of observations N, often written\np\u226bN. Such problems have become of increasing importance, especially in\ngenomics and other areas of computational biology. We will see that high\nvariance and over\ufb01tting are a major concern in this setting. As a result,\nsimple, highly regularized approaches often become the methods of choice.\nThe \ufb01rst part of the chapter focuses on prediction in both the classi\ufb01cation\nand regression settings, while the second part discusses the more basic\nproblem of feature selection and assessment.\nTo get us started, Figure 18.1 summarizes a small simulation study that\ndemonstrates the \u201cless \ufb01tting is better\u201d principle that applies when p\u226bN.\nFor each of N= 100 samples, we generated pstandard Gaussian features\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\nto a linear model\nY=p/summationdisplay\nj=1Xj\u03b2j+\u03c3\u03b5 (18.1)\nwhere \u03b5was generated from a standard Gaussian distribution. For each\ndataset, the set of coe\ufb03cients \u03b2jwere also generated from a standard Gaus-\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\nstandard deviation \u03c3was chosen in each case so that the signal-to-noise\nratio Var[E( Y|X)]/\u03c32equaled 2. As a result, the number of signi\ufb01cant uni-", "668": "650 18. High-Dimensional Problems: p\u226bN1.0 1.5 2.0 2.5 3.0Relative error\n20 9 220 featuresTest Error\n1.0 1.5 2.0 2.5 3.0\n99 35 7100 features\n1.0 1.5 2.0 2.5 3.0\n99 87 431000 features\nEffective Degrees of Freedom\nFIGURE 18.1. Test-error results for simulation experiments. Shown are box-\nplots of the relative test errors over 100simulations, for three di\ufb00erent values\nofp, the number of features. The relative error is the test error d ivided by the\nBayes error, \u03c32. From left to right, results are shown for ridge regression w ith\nthree di\ufb00erent values of the regularization parameter \u03bb:0.001,100and1000. The\n(average) e\ufb00ective degrees of freedom in the \ufb01t is indicated be low each plot.\nvariate regression coe\ufb03cients1was 9, 33 and 331, respectively, averaged\nover the 100 simulation runs. The p= 1000 case is designed to mimic the\nkind of data that we might see in a high-dimensional genomic or proteomic\ndataset, for example.\nWe \ufb01t a ridge regression to the data, with three di\ufb00erent values for the\nregularization parameter \u03bb: 0.001, 100, and 1000. When \u03bb= 0.001, this\nis nearly the same as least squares regression, with a little regularizati on\njust to ensure that the problem is non-singular when p > N . Figure 18.1\nshows boxplots of the relative test error achieved by the di\ufb00erent estimator s\nin each scenario. The corresponding average degrees of freedom used in\neach ridge-regression \ufb01t is indicated (computed using formula (3.50) on\npage 682). The degrees of freedom is a more interpretable parameter than\n\u03bb. We see that ridge regression with \u03bb= 0.001 (20 df) wins when p= 20;\n\u03bb= 100 (35 df) wins when p= 100, and \u03bb= 1000 (43 df) wins when\np= 1000,\nHere is an explanation for these results. When p= 20, we \ufb01t all the way\nand we can identify as many of the signi\ufb01cant coe\ufb03cients as possible with\n1We call a regression coe\ufb03cient signi\ufb01cant if |b\u03b2j/bsej| \u22652, where \u02c6\u03b2jis the estimated\n(univariate) coe\ufb03cient and bsejis its estimated standard error.\n2For a \ufb01xed value of the regularization parameter \u03bb, the degrees of freedom depends\non the observed predictor values in each simulation. Hence w e compute the average\ndegrees of freedom over simulations.", "669": "18.2 Nearest Shrunken Centroids 651\nlow bias. When p= 100, we can identify some non-zero coe\ufb03cients using\nmoderate shrinkage. Finally, when p= 1000, even though there are many\nnonzero coe\ufb03cients, we don\u2019t have a hope for \ufb01nding them and we need\nto shrink all the way down. As evidence of this, let tj=/hatwide\u03b2j//hatwidesej, where \u02c6\u03b2j\nis the ridge regression estimate and /hatwidesejits estimated standard error. Then\nusing the optimal ridge parameter in each of the three cases, the median\nvalue of |tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\nRidge regression with \u03bb= 0.001 successfully exploits the correlation in\nthe features when p < N , but cannot do so when p\u226bN. In the latter case\nthere is not enough information in the relatively small number of samples\nto e\ufb03ciently estimate the high-dimensional covariance matrix. In that case,\nmore regularization leads to superior prediction performance.\nThus it is not surprising that the analysis of high-dimensional data re-\nquires either modi\ufb01cation of procedures designed for the N > p scenario, or\nentirely new procedures. In this chapter we discuss examples of both kinds\nof approaches for high dimensional classi\ufb01cation and regression; these meth-\nods tend to regularize quite heavily, using scienti\ufb01c contextual knowledge\nto suggest the appropriate form for this regularization. The chapter ends\nwith a discussion of feature selection and multiple testing.\n18.2 Diagonal Linear Discriminant Analysis and\nNearest Shrunken Centroids\nGene expression arrays are an important new technology in biology, and\nare discussed in Chapters 1 and 14. The data in our next example form\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\nmicroarray experiments. Each expression value is a log-ratio log( R/G).R\nis the amount of gene-speci\ufb01c RNA in the target sample that hybridizes\nto a particular (gene-speci\ufb01c) spot on the microarray, and Gis the corre-\nsponding amount of RNA from a reference sample. The samples arose from\nsmall, round blue-cell tumors (SRBCT) found in children, and are classi\ufb01ed\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing\u2019s sarcoma) ,\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is an addi-\ntional test data set of 20 observations. We will not go into the scienti\ufb01c\nbackground here.\nSince p\u226bN, we cannot \ufb01t a full linear discriminant analysis (LDA) to\nthe data; some sort of regularization is needed. The method we describe\nhere is similar to the methods of Section 4.3.1, but with important modi\ufb01-\ncations that achieve feature selection. The simplest form of regularization\nassumes that the features are independent within each class, that is, the\nwithin-class covariance matrix is diagonal. Despite the fact that features\nwill rarely be independent within a class, when p\u226bNwe don\u2019t have", "670": "652 18. High-Dimensional Problems: p\u226bN\nenough data to estimate their dependencies. The assumption of indepen-\ndence greatly reduces the number of parameters in the model and often\nresults in an e\ufb00ective and interpretable classi\ufb01er.\nThus we consider the diagonal-covariance LDA rule for classifying the\nclasses. The discriminant score [see (4.12 on page 110] for class kis\n\u03b4k(x\u2217) =\u2212p/summationdisplay\nj=1(x\u2217\nj\u2212\u00afxkj)2\ns2\nj+ 2log \u03c0k. (18.2)\nHerex\u2217= (x\u2217\n1,x\u2217\n2,... ,x\u2217\np)Tis a vector of expression values for a test ob-\nservation, sjis the pooled within-class standard deviation of the jth gene,\nand \u00afxkj=/summationtext\ni\u2208Ckxij/Nkis the mean of the Nkvalues for gene jin class\nk, with Ckbeing the index set for class k. We call \u02dc xk= (\u00afxk1,\u00afxk2,...\u00afxkp)T\nthecentroid of class k. The \ufb01rst part of (18.2) is simply the (negative)\nstandardized squared distance of x\u2217to the kth centroid. The second part\nis a correction based on the class prior probability \u03c0k, where/summationtextK\nk=1\u03c0k= 1.\nThe classi\ufb01cation rule is then\nC(x\u2217) =\u2113if\u03b4\u2113(x\u2217) = max k\u03b4k(x\u2217). (18.3)\nWe see that the diagonal LDA classi\ufb01er is equivalent to a nearest centroid\nclassi\ufb01er after appropriate standardization. It is also a special case of the\nnaive-Bayes classi\ufb01er, as described in Section 6.6.3. It assumes that the\nfeatures in each class have independent Gaussian distributions with the\nsame variance.\nThe diagonal LDA classi\ufb01er is often e\ufb00ective in high dimensional set-\ntings. It is also called the \u201cindependence rule\u201d in Bickel and Levina (2004),\nwho demonstrate theoretically that it will often outperform standard lin-\near discriminant analysis in high-dimensional problems. Here the diagonal\nLDA classi\ufb01er yielded \ufb01ve misclassi\ufb01cation errors for the 20 test samples.\nOne drawback of the diagonal LDA classi\ufb01er is that it uses all of the fea-\ntures (genes), and hence is not convenient for interpretation. With further\nregularization we can do better\u2014both in terms of test error and inter-\npretability.\nWe would like to regularize in a way that automatically drops out fea-\ntures that are not contributing to the class predictions. We can do this\nby shrinking the classwise mean toward the overall mean, for each feature\nseparately. The result is a regularized version of the nearest centroid clas-\nsi\ufb01er, or equivalently a regularized version of the diagonal-covariance form\nof LDA. We call the procedure nearest shrunken centroids (NSC).\nThe shrinkage procedure is de\ufb01ned as follows. Let\ndkj=\u00afxkj\u2212\u00afxj\nmk(sj+s0), (18.4)\nwhere \u00af xjis the overall mean for gene j,m2\nk= 1/Nk\u22121/Nands0is a\nsmall positive constant, typically chosen to be the median of the sjvalues.", "671": "18.2 Nearest Shrunken Centroids 653\n(0,0)\u2206\nFIGURE 18.2. Soft thresholding function sign(x)(|x|\u2212\u2206)+is shown in orange,\nalong with the 45\u25e6line in red.\nThis constant guards against large dkjvalues that arise from expression\nvalues near zero. With constant within-class variance \u03c32, the variance of\nthe contrast \u00af xkj\u2212\u00afxjin the numerator is m2\nk\u03c32, and hence the form of the\nstandardization in the denominator. We shrink the dkjtoward zero using\nsoft thresholding\nd\u2032\nkj= sign( dkj)(|dkj| \u2212\u2206)+; (18.5)\nsee Figure 18.2. Here \u2206 is a parameter to be determined; we used 10-fold\ncross-validation in the example (see the top panel of Figure 18.4). Each\ndkjis reduced by an amount \u2206 in absolute value, and is set to zero if its\nabsolute value is less than zero. The soft-thresholding function is shown\nin Figure 18.2; the same thresholding is applied to wavelet coe\ufb03cients in\nSection 5.9. An alternative is to use hard thresholding\nd\u2032\nkj=dkj\u2264I(|dkj| \u2265\u2206); (18.6)\nwe prefer soft-thresholding, as it is a smoother operation and typically\nworks better. The shrunken versions of \u00af xkjare then obtained by reversing\nthe transformation in (18.4):\n\u00afx\u2032\nkj= \u00afxj+mk(sj+s0)d\u2032\nkj. (18.7)\nWe then use the shrunken centroids \u00af x\u2032\nkjin place of the original \u00af xkjin the\ndiscriminant score (18.2). The estimator (18.5) can also be viewed as a\nlasso-style estimator for the class means (Exercise 18.2).\nNotice that only the genes that have a nonzero d\u2032\nkjfor at least one of the\nclasses play a role in the classi\ufb01cation rule, and hence the vast majority\nof genes can often be discarded. In this example, all but 43 genes were\ndiscarded, leaving a small interpretable set of genes that characterize each\nclass. Figure 18.3 represents the genes in a heatmap.\nFigure 18.4 (top panel) demonstrates the e\ufb00ectiveness of the shrinkage.\nWith no shrinkage we make 5/20 errors on the test data, and several errors", "672": "654 18. High-Dimensional Problems: p\u226bN\non the training and CV data. The shrunken centroids achieve zero test er-\nrors for a fairly broad band of values for \u2206. The bottom panel of Figure 18. 4\nshows the four centroids for the SRBCT data (gray), relative to the overall\ncentroid. The blue bars are shrunken versions of these centroids, obtained\nby soft-thresholding the gray bars, using \u2206 = 4 .3. The discriminant scores\n(18.2) can be used to construct class probability estimates:\n\u02c6pk(x\u2217) =e1\n2\u03b4k(x\u2217)\n/summationtextK\n\u2113=1e1\n2\u03b4\u2113(x\u2217). (18.8)\nThese can be used to rate the classi\ufb01cations, or to decide not to classify a\nparticular sample at all.\nNote that other forms of feature selection can be used in this setting,\nincluding hard thresholding. Fan and Fan (2008) show theoretically the\nimportance of carrying out some kind of feature selection with diagonal\nlinear discriminant analysis in high-dimensional problems.\n18.3 Linear Classi\ufb01ers with Quadratic\nRegularization\nRamaswamy et al. (2001) present a more di\ufb03cult microarray classi\ufb01cation\nproblem, involving a training set of 144 patients with 14 di\ufb00erent types of\ncancer, and a test set of 54 patients. Gene expression measurements were\navailable for 16 ,063 genes.\nTable 18.1 shows the prediction results from eight di\ufb00erent classi\ufb01cation\nmethods. The data from each patient was \ufb01rst standardized to have mean\n0 and variance 1; this seems to improve prediction accuracy overall this\nexample, suggesting that the \u201cshape\u201d of each gene-expression pro\ufb01le is\nimportant, rather than the absolute expression levels. In each case, the\nBL EWS NB RMS\nFIGURE 18.3. Heat-map of the chosen 43 genes. Within each of the horizontal\npartitions, we have ordered the genes by hierarchical cluster ing, and similarly\nfor the samples within each vertical partition. Yellow repre sents over- and blue\nunder-expression.", "673": "18.3 Linear Classi\ufb01ers with Quadratic Regularization 655\n0 2 4 6Misclassification Error2308 2059 1223 598 284 159 81 43 23 15 10 5 1Number of Genes0.0 0.2 0.4 0.6 0.8Training\n10\u2212fold CV\nTest\nAmount of Shrinkage \u2206\n\u22121.0 \u22120.5 0.0 0.5 1.0BL0 500 1000 1500 2000\n\u22121.0 \u22120.5 0.0 0.5 1.0EWS\n\u22121.0 \u22120.5 0.0 0.5 1.0NB\n\u22121.0 \u22120.5 0.0 0.5 1.0RMSGene\nCentroids: Average Expression Centered at Overall Centroid\nFIGURE 18.4. (Top): Error curves for the SRBCT data. Shown are the train-\ning, 10-fold cross-validation, and test misclassi\ufb01cation erro rs as the threshold\nparameter \u2206is varied. The value \u2206 = 4 .34is chosen by CV, resulting in a sub-\nset of 43selected genes. (Bottom): Four centroids pro\ufb01les dkjfor the SRBCT\ndata (gray), relative to the overall centroid. Each centroid h as2308components,\nand we see considerable noise. The blue bars are shrunken versions d\u2032\nkjof these\ncentroids, obtained by soft-thresholding the gray bars, using \u2206 = 4 .3.", "674": "656 18. High-Dimensional Problems: p\u226bN\nTABLE 18.1. Prediction results for microarray data with 14 cancer classe s.\nMethod 1 is described in Section 18.2. Methods 2, 3 and 6 are disc ussed in Sec-\ntion 18.3, while 4, 7 and 8 are discussed in Section 18.4. Method 5 is described in\nSection 13.3. The elastic-net penalized multinomial does the be st on the test data,\nbut the standard error of each test-error estimate is about 3, so such comparisons\nare inconclusive.\nMethods CV errors (SE) Test errors Number of\nOut of 144 Out of 54 Genes Used\n1. Nearest shrunken centroids 35 (5.0) 17 6,520\n2.L2-penalized discriminant 25 (4.1) 12 16,063\nanalysis\n3. Support vector classi\ufb01er 26 (4.2) 14 16,063\n4. Lasso regression (one vs all) 30.7 (1.8) 12.5 1,429\n5.k-nearest neighbors 41 (4.6) 26 16,063\n6.L2-penalized multinomial 26 (4.2) 15 16,063\n7.L1-penalized multinomial 17 (2.8) 13 269\n8. Elastic-net penalized 22 (3.7) 11.8 384\nmultinomial\nregularization parameter has been chosen to minimize the cross-validation\nerror, and the test error at that value of the parameter is shown. When\nmore than one value of the regularization parameter yields the minimal\ncross-validation error, the average test error at these values is reported.\nRDA (regularized discriminant analysis), regularized multinomial logist ic\nregression, and the support vector machine are more complex methods that\ntry to exploit multivariate information in the data. We describe each in\nturn, as well as a variety of regularization methods, including both L1and\nL2and some in between.\n18.3.1 Regularized Discriminant Analysis\nRegularized discriminant analysis (RDA) is described in Section 4.3.1. Lin-\near discriminant analysis involves the inversion of a p\u00d7pwithin-covariance\nmatrix. When p\u226bN, this matrix can be huge, has rank at most N < p ,\nand hence is singular. RDA overcomes the singularity issues by regulariz-\ning the within-covariance estimate \u02c6\u03a3. Here we use a version of RDA that\nshrinks \u02c6\u03a3towards its diagonal:\n\u02c6\u03a3(\u03b3) =\u03b3\u02c6\u03a3+ (1\u2212\u03b3)diag( \u02c6\u03a3),with\u03b3\u2208[0,1]. (18.9)\nNote that \u03b3= 0 corresponds to diagonal LDA, which is the \u201cno shrinkage\u201d\nversion of nearest shrunken centroids. The form of shrinkage in (18.9) is", "675": "18.3 Linear Classi\ufb01ers with Quadratic Regularization 657\nmuch like ridge regression (Section 3.4.1), which shrinks the total covar iance\nmatrix of the features towards a diagonal (scalar) matrix. In fact, view ing\nlinear discriminant analysis as linear regression with optimal scoring of the\ncategorical response [see (12.58) in Section 12.6], the equivalence becomes\nmore precise.\nThe computational burden of inverting this large p\u00d7pmatrix is overcome\nusing the methods discussed in Section 18.3.5. The value of \u03b3was chosen\nby cross-validation in line 2 of Table 18.1; all values of \u03b3\u2208(0.002,0.550)\ngave the same CV and test error. Further development of RDA, including\nshrinkage of the centroids in addition to the covariance matrix, can be\nfound in Guo et al. (2006).\n18.3.2 Logistic Regression with Quadratic Regularization\nLogistic regression (Section 4.4) can be modi\ufb01ed in a similar way, to deal\nwith the p\u226bNcase. With Kclasses, we use a symmetric version of the\nmulticlass logistic model (4.17) on page 119:\nPr(G=k|X=x) =exp(\u03b2k0+xT\u03b2k)/summationtextK\n\u2113=1exp(\u03b2\u21130+xT\u03b2\u2113). (18.10)\nThis has Kcoe\ufb03cient vectors of log-odds parameters \u03b21,\u03b22,... ,\u03b2 K. We\nregularize the \ufb01tting by maximizing the penalized log-likelihood\nmax\n{\u03b20k,\u03b2k}K\n1/bracketleft\uf8eciggN/summationdisplay\ni=1log Pr( gi|xi)\u2212\u03bb\n2K/summationdisplay\nk=1||\u03b2k||2\n2/bracketright\uf8ecigg\n. (18.11)\nThis regularization automatically resolves the redundancy in the paramet-\nrization, and forces/summationtextK\nk=1\u02c6\u03b2kj= 0, j= 1,... ,p (Exercise 18.3). Note that\nthe constant terms \u03b2k0are not regularized (and so one should be set to\nzero). The resulting optimization problem is convex, and can be solved by\na Newton algorithm or other numerical techniques. Details are given in Zhu\nand Hastie (2004). Friedman et al. (2010) provide software for computi ng\nthe regularization path for the two- and multiclass logistic regression mod-\nels. Table 18.1, line 6 reports the results for the multiclass logistic r egres-\nsion model, referred to there as \u201cmultinomial\u201d. It can be shown (Rosset\net al., 2004a) that for separable data, as \u03bb\u21920, the regularized (two-\nclass) logistic regression estimate (renormalized) converges to the maxi mal\nmargin classi\ufb01er (Section 12.2). This gives an attractive alternative t o the\nsupport-vector machine, discussed next, especially in the multiclass case.\n18.3.3 The Support Vector Classi\ufb01er\nThe support vector classi\ufb01er is described for the two-class case in Sec-\ntion 12.2. When p > N , it is especially attractive because in general the", "676": "658 18. High-Dimensional Problems: p\u226bN\nclasses are perfectly separable by a hyperplane unless there are identical\nfeature vectors in di\ufb00erent classes. Without any regularization the support\nvector classi\ufb01er \ufb01nds the separating hyperplane with the largest margin;\nthat is, the hyperplane yielding the biggest gap between the classes in\nthe training data. Somewhat surprisingly, when p\u226bNthe unregularized\nsupport vector classi\ufb01er often works about as well as the best regularized\nversion. Over\ufb01tting often does not seem to be a problem, partly because of\nthe insensitivity of misclassi\ufb01cation loss.\nThere are many di\ufb00erent methods for generalizing the two-class support-\nvector classi\ufb01er to K >2 classes. In the \u201cone versus one\u201d ( ovo) approach,\nwe compute all/parenleftbigK\n2/parenrightbig\npairwise classi\ufb01ers. For each test point, the predicted\nclass is the one that wins the most pairwise contests. In the \u201cone versus all\u201d\n(ova) approach, each class is compared to all of the others in Ktwo-class\ncomparisons. To classify a test point, we compute the con\ufb01dences (signed\ndistance from the hyperplane) for each of the Kclassi\ufb01ers. The winner is the\nclass with the highest con\ufb01dence. Finally, Vapnik (1998) and Weston and\nWatkins (1999) suggested (somewhat complex) multiclass criteria which\ngeneralize the two-class criterion (12.6).\nTibshirani and Hastie (2007) propose the margin tree classi\ufb01er, in which\nsupport-vector classi\ufb01ers are used in a binary tree, much as in CART\n(Chapter 9). The classes are organized in a hierarchical manner, which can\nbe useful for classifying patients into di\ufb00erent cancer types, for example.\nLine 3 of Table 18.1 shows the results for the support vector classi\ufb01er\nusing the ovamethod; Ramaswamy et al. (2001) reported (and we con-\n\ufb01rmed) that this approach worked best for this problem. The errors are\nvery similar to those in line 6, as we might expect from the comments\nat the end of the previous section. The error rates are insensitive to the\nchoice of C[the regularization parameter in (12.8) on page 420], for values\nofC >0.001. Since p > N , the support vector hyperplane can perfectly\nseparate the training data by setting C=\u221e.\n18.3.4 Feature Selection\nFeature selection is an important scienti\ufb01c requirement for a classi\ufb01er when\npis large. Neither discriminant analysis, logistic regression, nor the suppo rt-\nvector classi\ufb01er perform feature selection automatically, because all use\nquadratic regularization. All features have nonzero weights in both models.\nAd-hoc methods for feature selection have been proposed, for example,\nremoving genes with small coe\ufb03cients, and re\ufb01tting the classi\ufb01er. This is\ndone in a backward stepwise manner, starting with the smallest weights and\nmoving on to larger weights. This is known as recursive feature elimination\n(Guyon et al., 2002). It was not successful in this example; Ramaswamy\net al. (2001) report, for example, that the accuracy of the support-vector\nclassi\ufb01er starts to degrade as the number of genes is reduced from the full", "677": "18.3 Linear Classi\ufb01ers with Quadratic Regularization 659\nset of 16 ,063. This is rather remarkable, as the number of training samples\nis only 144. We do not have an explanation for this behavior.\nAll three methods discussed in this section (RDA, LR and SVM) can\nbe modi\ufb01ed to \ufb01t nonlinear decision boundaries using kernels. Usually the\nmotivation for such an approach is to increase the model complexity. With\np\u226bNthe models are already su\ufb03ciently complex and over\ufb01tting is always\na danger. Yet despite the high dimensionality, radial kernels (Section 12.3.3)\nsometimes deliver superior results in these high dimensional problems. The\nradial kernel tends to dampen inner products between points far away from\neach other, which in turn leads to robustness to outliers. This occurs often\nin high dimensions, and may explain the positive results. We tried a radial\nkernel with the SVM in Table 18.1, but in this case the performance was\ninferior.\n18.3.5 Computational Shortcuts When p\u226bN\nThe computational techniques discussed in this section apply to any method\nthat \ufb01ts a linear model with quadratic regularization on the coe\ufb03cients.\nThat includes all the methods discussed in this section, and many more.\nWhen p > N , the computations can be carried out in an N-dimensional\nspace, rather than p, via the singular value decomposition introduced in\nSection 14.5. Here is the geometric intuition: just like two points in thr ee-\ndimensional space always lie on a line, Npoints in p-dimensional space lie\nin an ( N\u22121)-dimensional a\ufb03ne subspace.\nGiven the N\u00d7pdata matrix X, let\nX=UDVT(18.12)\n=RVT(18.13)\nbe the singular-value decomposition (SVD) of X; that is, Visp\u00d7Nwith\northonormal columns, UisN\u00d7Northogonal, and Da diagonal matrix\nwith elements d1\u2265d2\u2265dN\u22650. The matrix RisN\u00d7N, with rows rT\ni.\nAs a simple example, let\u2019s \ufb01rst consider the estimates from a ridge re-\ngression:\n\u02c6\u03b2= (XTX+\u03bbI)\u22121XTy. (18.14)\nReplacing XbyRVTand after some further manipulations, this can be\nshown to equal\n\u02c6\u03b2=V(RTR+\u03bbI)\u22121RTy (18.15)\n(Exercise 18.4). Thus \u02c6\u03b2=V\u02c6\u03b8, where \u02c6\u03b8is the ridge-regression estimate\nusing the Nobservations ( ri,yi),i= 1,2,... ,N . In other words, we can\nsimply reduce the data matrix from XtoR, and work with the rows of\nR. This trick reduces the computational cost from O(p3) toO(pN2) when\np > N .", "678": "660 18. High-Dimensional Problems: p\u226bN\nThese results can be generalized to allmodels that are linear in the\nparameters and have quadratic penalties. Consider any supervised learning\nproblem where we use a linear function f(X) =\u03b20+XT\u03b2to model a\nparameter in the conditional distribution of Y|X. We \ufb01t the parameters \u03b2\nby minimizing some loss function/summationtextN\ni=1L(yi,f(xi)) over the data with a\nquadratic penalty on \u03b2. Logistic regression is a useful example to have in\nmind. Then we have the following simple theorem:\nLetf\u2217(ri) =\u03b80+rT\ni\u03b8withride\ufb01ned in (18.13), and consider the pair of\noptimization problems:\n(\u02c6\u03b20,\u02c6\u03b2) = arg min\n\u03b20,\u03b2\u2208I RpN/summationdisplay\ni=1L(yi,\u03b20+xT\ni\u03b2) +\u03bb\u03b2T\u03b2; (18.16)\n(\u02c6\u03b80,\u02c6\u03b8) = arg min\n\u03b80,\u03b8\u2208I RNN/summationdisplay\ni=1L(yi,\u03b80+rT\ni\u03b8) +\u03bb\u03b8T\u03b8. (18.17)\nThen the \u02c6\u03b20=\u02c6\u03b80, and \u02c6\u03b2=V\u02c6\u03b8.\nThe theorem says that we can simply replace the pvectors xiby the\nN-vectors ri, and perform our penalized \ufb01t as before, but with far fewer\npredictors. The N-vector solution \u02c6\u03b8is then transformed back to the p-\nvector solution via a simple matrix multiplication. This result is part of\nthe statistics folklore, and deserves to be known more widely\u2014see Hastie\nand Tibshirani (2004) for further details.\nGeometrically, we are rotating the features to a coordinate system in\nwhich all but the \ufb01rst Ncoordinates are zero. Such rotations are allowed\nsince the quadratic penalty is invariant under rotations, and linear models\nare equivariant.\nThis result can be applied to many of the learning methods discussed\nin this chapter, such as regularized (multiclass) logistic regression, linea r\ndiscriminant analysis (Exercise 18.6), and support vector machines. It als o\napplies to neural networks with quadratic regularization (Section 11.5.2).\nNote, however, that it does not apply to methods such as the lasso, which\nuses nonquadratic ( L1) penalties on the coe\ufb03cients.\nTypically we use cross-validation to select the parameter \u03bb. It can be\nseen (Exercise 18.12) that we only need to construct Ronce, on the original\ndata, and use it as the data for each of the CV folds.\nThe support vector \u201ckernel trick\u201d of Section 12.3.7 exploits the same re-\nduction used in this section, in a slightly di\ufb00erent context. Suppose we have\nat our disposal the N\u00d7Ngram (inner-product) matrix K=XXT. From\n(18.12) we have K=UD2UT, and so Kcaptures the same information as\nR. Exercise 18.13 shows how we can exploit the ideas in this section to \ufb01t\na ridged logistic regression with Kusing its SVD.", "679": "18.4 Linear Classi\ufb01ers with L1Regularization 661\n18.4 Linear Classi\ufb01ers with L1Regularization\nThe methods of the previous chapter use an L2penalty to regularize their\nparameters, just as in ridge regression. All of the estimated coe\ufb03cients\nare nonzero, and hence no feature selection is performed. In this section we\ndiscuss methods that use L1penalties instead, and hence provide automatic\nfeature selection.\nRecall the lasso of Section 3.4.2,\nmin\n\u03b21\n2N/summationdisplay\ni=1/parenleft\uf8ecig\nyi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j/parenright\uf8ecig2\n+\u03bbp/summationdisplay\nj=1|\u03b2j|, (18.18)\nwhich we have written in the Lagrange form (3.52). As discussed there, the\nuse of the L1penalty causes a subset of the solution coe\ufb03cients \u02c6\u03b2jto be\nexactly zero, for a su\ufb03ciently large value of the tuning parameter \u03bb.\nIn Section 3.8.1 we discussed the LARS algorithm, an e\ufb03cient procedure\nfor computing the lasso solution for all \u03bb. When p > N (as in this chapter),\nas\u03bbapproaches zero, the lasso \ufb01ts the training data exactly. In fact, by\nconvex duality one can show that when p > N the number of non-zero\ncoe\ufb03cients is at most Nfor all values of \u03bb(Rosset and Zhu, 2007, for\nexample). Thus the lasso provides a (severe) form of feature selection.\nLasso regression can be applied to a two-class classi\ufb01cation problem by\ncoding the outcome \u00b11, and applying a cuto\ufb00 (usually 0) to the predictions.\nFor more than two classes, there are many possible approaches, including\ntheovaandovomethods discussed in Section 18.3.3. We tried the ova-\napproach on the cancer data in Section 18.3. The results are shown in\nline (4) of Table 18.1. Its performance is among the best.\nA more natural approach for classi\ufb01cation problems is to use the lasso\npenalty to regularize logistic regression. Several implementations have been\nproposed in the literature, including path algorithms similar to LARS (Par k\nand Hastie, 2007). Because the paths are piecewise smooth but nonlinear,\nexact methods are slower than the LARS algorithm, and are less feasible\nwhen pis large.\nFriedman et al. (2010) provide very fast algorithms for \ufb01tting L1-pen-\nalized logistic and multinomial regression models. They use the symmetric\nmultinomial logistic regression model as in (18.10) in Section 18.3.2 , and\nmaximize the penalized log-likelihood\nmax\n{\u03b20k,\u03b2k\u2208I Rp}K\n1\uf8ee\n\uf8f0N/summationdisplay\ni=1log Pr( gi|xi)\u2212\u03bbK/summationdisplay\nk=1p/summationdisplay\nj=1|\u03b2kj|\uf8f9\n\uf8fb; (18.19)\ncompare with (18.11). Their algorithm computes the exact solution at a\npre-chosen sequence of values for \u03bbby cyclical coordinate descent (Sec-\ntion 3.8.6), and exploits the fact that solutions are sparse when p\u226bN,", "680": "662 18. High-Dimensional Problems: p\u226bN\nas well as the fact that solutions for neighboring values of \u03bbtend to be\nvery similar. This method was used in line (7) of Table 18.1, with the over -\nall tuning parameter \u03bbchosen by cross-validation. The performance was\nsimilar to that of the best methods, except here the automatic feature se-\nlection chose 269 genes altogether. A similar approach is used in Genkin\net al. (2007); although they present their model from a Bayesian point of\nview, they in fact compute the posterior mode, which solves the penalized\nmaximum-likelihood problem.\n\u22128 \u22127 \u22126 \u22125 \u22124 \u22123 \u22122 \u221210.0 0.5 1.0 1.5 2.0\n\u22128 \u22127 \u22126 \u22125 \u22124 \u22123 \u22122 \u221210.0 0.5 1.0 1.5 2.0Coe\ufb03cientsCoe\ufb03cients\nlog(\u03bb) log(\u03bb)Lasso Elastic Net\nFIGURE 18.5. Regularized logistic regression paths for the leukemia dat a. The\nleft panel is the lasso path, the right panel the elastic-net pat h with \u03b1= 0.8. At\nthe ends of the path (extreme left), there are 19nonzero coe\ufb03cients for the lasso,\nand39for the elastic net. The averaging e\ufb00ect of the elastic net resul ts in more\nnon-zero coe\ufb03cients than the lasso, but with smaller magnitudes .\nIn genomic applications, there are often strong correlations among the\nvariables; genes tend to operate in molecular pathways. The lasso penalty\nis somewhat indi\ufb00erent to the choice among a set of strong but corre-\nlated variables (Exercise 3.28). The ridge penalty, on the other hand, tends\nto shrink the coe\ufb03cients of correlated variables toward each other (Exer-\ncise 3.29 on page 99). The elastic net penalty (Zou and Hastie, 2005) is a\ncompromise, and has the form\np/summationdisplay\nj=1/parenleftbig\n\u03b1|\u03b2j|+ (1\u2212\u03b1)\u03b22\nj/parenrightbig\n. (18.20)\nThe second term encourages highly correlated features to be averaged, while\nthe \ufb01rst term encourages a sparse solution in the coe\ufb03cients of these aver-", "681": "18.4 Linear Classi\ufb01ers with L1Regularization 663\naged features. The elastic net penalty can be used with any linear model,\nin particular for regression or classi\ufb01cation.\nHence the multinomial problem above with elastic-net penalty becomes\nmax\n{\u03b20k,\u03b2k\u2208I Rp}K\n1\uf8ee\n\uf8f0N/summationdisplay\ni=1log Pr( gi|xi)\u2212\u03bbK/summationdisplay\nk=1p/summationdisplay\nj=1/parenleftbig\n\u03b1|\u03b2kj|+ (1\u2212\u03b1)\u03b22\nkj/parenrightbig\uf8f9\n\uf8fb.\n(18.21)\nThe parameter \u03b1determines the mix of the penalties, and is often pre-\nchosen on qualitative grounds. The elastic net can yield more that Nnon-\nzero coe\ufb03cients when p > N , a potential advantage over the lasso. Line\n(8) in Table 18.1 uses this model, with \u03b1and\u03bbchosen by cross-validation.\nWe used a sequence of 20 values of \u03b1between 0 .05 and 1 .0, and a 100\nvalues of \u03bbuniform on the log scale covering the entire range. Values of\n\u03b1\u2208[0.75,0.80] gave the minimum CV error, with values of \u03bb <0.001 for all\ntied solutions. Although it has the lowest test error among all methods, the\nmargin is small and not signi\ufb01cant. Interestingly, when CV is performed\nseparately for each value of \u03b1, a minimum test error of 8 .8 is achieved at\n\u03b1= 0.10, but this is not the value chosen in the two-dimensional CV.\n\u22128 \u22127 \u22126 \u22125 \u22124 \u22123 \u22122 \u221210.0 0.1 0.2 0.3 0.4Misclassification ErrorTraining\nTest\n10\u2212fold CV\n\u22128 \u22127 \u22126 \u22125 \u22124 \u22123 \u22122 \u221210 5 10 15 20 25 30Deviance\nlog(\u03bb) log(\u03bb)\nFIGURE 18.6. Training, test, and 10-fold cross validation curves for lasso l ogis-\ntic regression on the leukemia data. The left panel shows misc lassi\ufb01cation errors,\nthe right panel shows deviance.\nFigure 18.5 shows the lasso and elastic-net coe\ufb03cient paths on the two-\nclass leukemia data (Golub et al., 1999). There are 7129 gene-expression\nmeasurements on 38 samples, 27 of them in class ALL (acute lymphocytic\nleukemia), and 11 in class AML (acute myelogenous leukemia). There is\nalso a test set with 34 samples (20, 14). Since the data are linearly separa-\nble, the solution is unde\ufb01ned at \u03bb= 0 (Exercise 18.11), and degrades for\nvery small values of \u03bb. Hence the paths have been truncated as the \ufb01tted\nprobabilities approach 0 and 1. There are 19 non-zero coe\ufb03cients in the\nleft plot, and 39 in the right. Figure 18.6 (left panel) shows the misclas-", "682": "664 18. High-Dimensional Problems: p\u226bN\nsi\ufb01cation errors for the lasso logistic regression on the training and t est\ndata, as well as for 10-fold cross-validation on the training data. The ri ght\npanel uses binomial deviance to measure errors, and is much smoother. The\nsmall sample sizes lead to considerable sampling variance in these curves,\neven though individual curves are relatively smooth (see, for example, Fig-\nure 7.1 on page 220). Both of these plots suggest that the limiting solutio n\n\u03bb\u21930 is adequate, leading to 3/34 misclassi\ufb01cations in the test set. The\ncorresponding \ufb01gures for the elastic net are qualitatively similar and are\nnot shown.\nForp\u226bN, the limiting coe\ufb03cients diverge for all regularized logistic\nregression models, so in practical software implementations a minimum\nvalue for \u03bb >0 is either explicitly or implicitly set. However, renormalized\nversions of the coe\ufb03cients converge, and these limiting solutions can be\nthought of as interesting alternatives to the linear optimal separating hy-\nperplane (SVM). With \u03b1= 0 the limiting solution coincides with the SVM\n(see end of Section 18.3.2), but all the 7129 genes are selected. With \u03b1= 1,\nthe limiting solution coincides with an L1separating hyperplane (Rosset\net al., 2004a), and includes at most 38 genes. As \u03b1decreases from 1, the\nelastic-net solutions include more genes in the separating hyperplane.\n18.4.1 Application of Lasso to Protein Mass Spectroscopy\nProtein mass spectrometry has become a popular technology for analyzing\nthe proteins in blood, and can be used to diagnose a disease or understand\nthe processes underlying it.\nFor each blood serum sample i, we observe the intensity xijfor many\ntime of \ufb02ight values tj. This intensity is related to the number of particles\nobserved to take approximately tjtime to pass from the emitter to the\ndetector during a cycle of operation of the machine. The time of \ufb02ight has\na known relationship to the mass over charge ratio ( m/z) of the constituent\nproteins in the blood. Hence the identi\ufb01cation of a peak in the spectrum\nat a certain tjtells us that there is a protein with a corresponding mass\nand charge. The identity of this protein can then be determined by other\nmeans.\nFigure 18.7 shows an example taken from Adam et al. (2003). It shows\nthe average spectra for healthy patients and those with prostate cancer.\nThere are 16,898 m/zsites in total, ranging in value from 2000 to 40,000.\nThe full dataset consists of 157 healthy patients and 167 with cancer, and\nthe goal is to \ufb01nd m/zsites that discriminate between the two groups.\nThis is an example of functional data; the predictors can be viewed as a\nfunction of m/z. There has been much interest in this problem in the past\nfew years; see e.g. Petricoin et al. (2002).\nThe data were \ufb01rst standardized (baseline subtraction and normaliza-\ntion), and we restricted attention to m/zvalues between 2000 and 40,000\n(spectra outside of this range were not of interest). We then applied near-", "683": "18.4 Linear Classi\ufb01ers with L1Regularization 665\n2e+03 5e+03 1e+04 2e+04 5e+04 1e+05 2e+0510 20 30 40\nm/zIntensityNormal\nCancer\nFIGURE 18.7. Protein mass spectrometry data: average pro\ufb01les from normal\nand prostate cancer patients.\nest shrunken centroids and lasso regression to the data, with the results for\nboth methods shown in Table 18.2.\nBy \ufb01tting harder to the data, the lasso achieves a considerably lower\ntest error rate. However, it may not provide a scienti\ufb01cally useful solu-\ntion. Ideally, protein mass spectrometry resolves a biological sample int o\nits constituent proteins, and these should appear as peaks in the spectra.\nThe lasso doesn\u2019t treat peaks in any special way, so not surprisingly only\nsome of the non-zero lasso weights were situated near peaks in the spectra.\nFurthermore, the same protein may yield a peak at slightly di\ufb00erent m/z\nvalues in di\ufb00erent spectra. In order to identify common peaks, some kind\nofm/zwarping is needed from sample to sample.\nTo address this, we applied a standard peak-extraction algorithm to each\nspectrum, yielding a total of 5178 peaks in the 217 training spectra. Our\nidea was to pool the collection of peaks from all patients, and hence con-\nstruct a set of common peaks. For this purpose, we applied hierarchical\nclustering to the positions of these peaks along the log m/zaxis. We cut\nthe resulting dendrogram horizontally at height log(0 .005)3, and computed\naverages of the peak positions in each resulting cluster. This process yielded\n728 common clusters and their corresponding peak centers.\nGiven these 728 common peaks, we determined which of these were\npresent in each individual spectrum, and if present, the height of the peak.\nA peak height of zero was assigned if that peak was not found. This pro-\nduced a 217 \u00d7728 matrix of peak heights as features, which was used in a\nlasso regression. We scored the test spectra for the same 728 peaks.\n3Use of the value 0 .005 means that peaks with positions less than 0.5% apart are\nconsidered the same peak, a fairly common assumption.", "684": "666 18. High-Dimensional Problems: p\u226bN\nTABLE 18.2. Results for the prostate data example. The standard deviation for\nthe test errors is about 4.5.\nMethod Test Errors/108 Number of Sites\n1. Nearest shrunken centroids 34 459\n2. Lasso 22 113\n3. Lasso on peaks 28 35\nThe prediction results for this application of the lasso to the peaks are\nshown in the last line of Table 18.2: it does fairly well, but not as well\nas the lasso on the raw spectra. However, the \ufb01tted model may be more\nuseful to the biologist as it yields 35 peak positions for further study. On\nthe other hand, the results suggest that there may be useful discriminatory\ninformation between the peaks of the spectra, and the positions of the lasso\nsites from line (2) of the table also deserve further examination.\n18.4.2 The Fused Lasso for Functional Data\nIn the previous example, the features had a natural order, determined by\nthe mass-to-charge ratio m/z. More generally, we may have functional fea-\ntures xi(t) that are ordered according to some index variable t. We have\nalready discussed several approaches for exploiting such structure.\nWe can represent xi(t) by their coe\ufb03cients in a basis of functions in t,\nsuch as splines, wavelets or Fourier bases, and then apply a regression using\nthese coe\ufb03cients as predictors. Equivalently, one can instead represent the\ncoe\ufb03cients of the original features in these bases. These approaches are\ndescribed in Section 5.3.\nIn the classi\ufb01cation setting, we discuss the analogous approach of penal-\nized discriminant analysis in Section 12.6. This uses a penalty that explicitly\ncontrols the resulting smoothness of the coe\ufb03cient vector.\nThe above methods tend to smooth the coe\ufb03cients uniformly. Here we\npresent a more adaptive strategy that modi\ufb01es the lasso penalty to take\ninto account the ordering of the features. The fused lasso (Tibshirani et\nal., 2005) solves\nmin\n\u03b2\u2208I Rp/braceleft\uf8eciggN/summationdisplay\ni=1(yi\u2212\u03b20\u2212p/summationdisplay\nj=1xij\u03b2j)2+\u03bb1p/summationdisplay\nj=1|\u03b2j|+\u03bb2p\u22121/summationdisplay\nj=1|\u03b2j+1\u2212\u03b2j|/braceright\uf8ecigg\n.(18.22)\nThis criterion is strictly convex in \u03b2, so a unique solution exists. The \ufb01rst\npenalty encourages the solution to be sparse, while the second encourages\nit to be smooth in the index j.\nThe di\ufb00erence penalty in (18.22) assumes an uniformly spaced index j. If\ninstead the underlying index variable thas nonuniform values tj, a natural\ngeneralization of (18.22) would be based on divided di\ufb00erences", "685": "18.4 Linear Classi\ufb01ers with L1Regularization 667\n0 200 400 600 800 1000\u22122 0 2 4\nGenome orderlog2 ratio\nFIGURE 18.8. Fused lasso applied to CGH data. Each point represents the\ncopy-number of a gene in a tumor sample, relative to that of a cont rol (on the log\nbase-2 scale).\n\u03bb2p\u22121/summationdisplay\nj=1|\u03b2j+1\u2212\u03b2j|\n|tj+1\u2212tj|. (18.23)\nThis amounts to having a penalty modi\ufb01er for each of the terms in the\nseries.\nA particularly useful special case arises when the predictor matrix X=\nIN, theN\u00d7Nidentity matrix. This is a special case of the fused lasso,\nused to approximate a sequence {yi}N\n1. Thefused lasso signal approximator\nsolves\nmin\n\u03b2\u2208I RN/braceleft\uf8eciggN/summationdisplay\ni=1(yi\u2212\u03b20\u2212\u03b2i)2+\u03bb1N/summationdisplay\ni=1|\u03b2i|+\u03bb2N\u22121/summationdisplay\ni=1|\u03b2i+1\u2212\u03b2i|/braceright\uf8ecigg\n.(18.24)\nFigure 18.8 shows an example taken from Tibshirani and Wang (2007). The\ndata in the panel come from a Comparative Genomic Hybridization (CGH)\narray, measuring the approximate log (base-two) ratio of the number of\ncopies of each gene in a tumor sample, as compared to a normal sample.\nThe horizontal axis represents the chromosomal location of each gene. The\nidea is that in cancer cells, genes are often ampli\ufb01ed (duplicated) or deleted,\nand it is of interest to detect these events. Furthermore, these events tend\nto occur in contiguous regions. The smoothed signal estimate from the\nfused lasso signal approximator is shown in dark red (with appropriately\nchosen values for \u03bb1and\u03bb2). The signi\ufb01cantly nonzero regions can be used\nto detect locations of gains and losses of genes in the tumor.\nThere is also a two-dimensional version of the fused lasso, in which the\nparameters are laid out in a grid of pixels, and a penalty is applied to the", "686": "668 18. High-Dimensional Problems: p\u226bN\n\ufb01rst di\ufb00erences to the left, right, above and below the target pixel. This\ncan be useful for denoising or classifying images. Friedman et al. (2007)\ndevelop fast generalized coordinate descent algorithms for the one- and\ntwo-dimensional fused lasso.\n18.5 Classi\ufb01cation When Features are Unavailable\nIn some applications the objects under study are more abstract in nature,\nand it is not obvious how to de\ufb01ne a feature vector. As long as we can \ufb01ll\nin anN\u00d7Nproximity matrix of similarities between pairs of objects in our\ndatabase, it turns out we can put to use many of the classi\ufb01ers in our arsenal\nby interpreting the proximities as inner-products. Protein structures fall\ninto this category, and we explore an example in Section 18.5.1 below.\nIn other applications, such as document classi\ufb01cation, feature vectors are\navailable but can be extremely high-dimensional. Here we may not wish\nto compute with such high-dimensional data, but rather store the inner-\nproducts between pairs of documents. Often these inner-products can be\napproximated by sampling techniques.\nPairwise distances serve a similar purpose, because they can be turned\ninto centered inner-products. Proximity matrices are discussed in more de-\ntail in Chapter 14.\n18.5.1 Example: String Kernels and Protein Classi\ufb01cation\nAn important problem in computational biology is to classify proteins int o\nfunctional and structural classes based on their sequence similarities. Pro-\ntein molecules are strings of amino acids, di\ufb00ering in both length and com-\nposition. In the example we consider, the lengths vary between 75\u2013160\namino-acid molecules, each of which can be one of 20 di\ufb00erent types, labeled\nusing letters. Here are two examples, of length 110 and 153, respectively:\nIPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGG TV\nERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDY LQEFLGVMNTEWI\nPHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAER LQENLQAYRTFHVLLA\nRLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKK\nLWGLKV LQELSQWTVRSIHDLRFISSHQTGIP\nThere have been many proposals for measuring the similarity between a\npair of protein molecules. Here we focus on a measure based on the count\nof matching substrings (Leslie et al., 2004), such as the LQEabove.\nTo construct our features, we count the number of times that a given\nsequence of length moccurs in our string, and we compute this number", "687": "18.5 Classi\ufb01cation When Features are Unavailable 669\nfor all possible sequences of length m. Formally, for a string x, we de\ufb01ne a\nfeature map\n\u03a6m(x) ={\u03c6a(x)}a\u2208Am (18.25)\nwhere Amis the set of subsequences of length m, and \u03c6a(x) is the number\nof times that \u201c a\u201d occurs in our string x. Using this, we de\ufb01ne the inner\nproduct\nKm(x1,x2) =\u221dan}\u230a\u2207a\u230bketle{t\u03a6m(x1),\u03a6m(x2)\u221dan}\u230a\u2207a\u230bket\u2207i}ht, (18.26)\nwhich measures the similarity between the two strings x1, x2.This can be\nused to drive, for example, a support vector classi\ufb01er for classifying strings\ninto di\ufb00erent protein classes.\nNow the number of possible sequences ais|Am|= 20m, which can be\nvery large for moderate m, and the vast majority of the subsequences do\nnot match the strings in our training set. It turns out that we can compute\ntheN\u00d7Ninner-product matrix or string kernel Km(18.26) e\ufb03ciently\nusing tree-structures, without actually computing the individual vectors.\nThis methodology, and the data to follow, come from Leslie et al. (2004).4\nThe data consist of 1708 proteins in two classes\u2014 negative (1663) and\npositive (45). The two examples above, which we will call \u201c x1\u201d and \u201c x2\u201d,\nare from this set. We have marked the occurrences of subsequence LQE,\nwhich appears in both proteins. There are 203possible subsequences, so\n\u03a63(x) will be a vector of length 8000. For this example \u03c6LQE(x1) = 1 and\n\u03c6LQE(x2) = 2.\nUsing software from Leslie et al. (2004), we computed the string kernel\nform= 4, which was then used in a support vector classi\ufb01er to \ufb01nd the\nmaximal margin solution in this 204= 160 ,000-dimensional feature space.\nWe used 10-fold cross-validation to compute the SVM predictions on all of\nthe training data. The orange curve in Figure 18.9 shows the cross-validated\nROC curve for the support vector classi\ufb01er, computed by varying the cut-\npoint on the real-valued predictions from the cross-validated support vector\nclassi\ufb01er. The area under the curve is 0.84. Leslie et al. (2004) show that\nthe string kernel method is competitive with, but perhaps not as accurate\nas, more specialized methods for protein string matching.\nMany other classi\ufb01ers can be computed using only the information in the\nkernel matrix; some details are given in the next section. The results for\nthe nearest centroid classi\ufb01er (green), and distance-weighted one-nearest\nneighbors (blue) are shown in Figure 18.9. Their performance is similar to\nthat of the support vector classi\ufb01er.\n4We thank Christina Leslie for her help and for providing the d ata, which is available\non our book website.", "688": "670 18. High-Dimensional Problems: p\u226bN\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0ROC Curves for String Kernel\nSpecificitySensitivity\nSVM  0.84\nNearest Centroid  0.84\nOne\u2212Nearest Neighbor 0.86\nFIGURE 18.9. Cross-validated ROC curves for protein example using the stri ng\nkernel. The numbers next to each method in the legend give the area u nder the\ncurve, an overall measure of accuracy. The SVM achieves bette r sensitivities than\nthe other two, which achieve better speci\ufb01cities.\n18.5.2 Classi\ufb01cation and Other Models Using Inner-Product\nKernels and Pairwise Distances\nThere are a number of other classi\ufb01ers, besides the support-vector ma-\nchine, that can be implemented using only inner-product matrices. This\nalso implies they can be \u201ckernelized\u201d like the SVM.\nAn obvious example is nearest-neighbor classi\ufb01cation, since we can trans-\nform pairwise inner-products to pairwise distances:\n||xi\u2212xi\u2032||2=\u221dan}\u230a\u2207a\u230bketle{txi,xi\u221dan}\u230a\u2207a\u230bket\u2207i}ht+\u221dan}\u230a\u2207a\u230bketle{txi\u2032,xi\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht \u22122\u221dan}\u230a\u2207a\u230bketle{txi,xi\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht. (18.27)\nA variation of 1-NN classi\ufb01cation is used in Figure 18.9, which produces\na continuous discriminant score needed to construct a ROC curve. This\ndistance-weighted 1-NN makes use of the distance of a test points to the\nclosest member of each class; see Exercise 18.14.\nNearest-centroid classi\ufb01cation follows easily as well. For training pair s\n(xi,gi), i= 1,... ,N , a test point x0, and class centroids \u00af xk,k= 1,... ,K\nwe can write\n||x0\u2212\u00afxk||2=\u221dan}\u230a\u2207a\u230bketle{tx0,x0\u221dan}\u230a\u2207a\u230bket\u2207i}ht \u22122\nNk/summationdisplay\ngi=k\u221dan}\u230a\u2207a\u230bketle{tx0,xi\u221dan}\u230a\u2207a\u230bket\u2207i}ht+1\nN2\nk/summationdisplay\ngi=k/summationdisplay\ngi\u2032=k\u221dan}\u230a\u2207a\u230bketle{txi,xi\u2032\u221dan}\u230a\u2207a\u230bket\u2207i}ht,(18.28)", "689": "18.5 Classi\ufb01cation When Features are Unavailable 671\nHence we can compute the distance of the test point to each of the cen-\ntroids, and perform nearest centroid classi\ufb01cation. This also implies that\nmethods like K-means clustering can also be implemented, using only the\ninner products of the data points.\nLogistic and multinomial regression with quadratic regularization can\nalso be implemented with inner-product kernels; see Section 12.3.3 and\nExercise 18.13. Exercise 12.10 derives linear discriminant analysis using a n\ninner-product kernel.\nPrincipal components can be computed using inner-product kernels as\nwell; since this is frequently useful, we give some details. Suppose \ufb01rst\nthat we have a centered data matrix X, and let X=UDVTbe its SVD\n(18.12). Then Z=UDis the matrix of principal component variables (see\nSection 14.5.1). But if K=XXT, then it follows that K=UD2UT, and\nhence we can compute Zfrom the eigen decomposition of K. IfXisnot\ncentered, then we can center it using \u02dcX= (I\u2212M)X, where M=1\nN11T\nis the mean operator. Thus we compute the eigenvectors of the double-\ncentered kernel ( I\u2212M)K(I\u2212M) for the principal components from an\nuncentered inner-product matrix. Exercise 18.15 explores this further, and\nSection 14.5.4 discusses in more detail kernel PCA for general kernels, such\nas the radial kernel used in SVMs.\nIf instead we had available only the pairwise (squared) Euclidean dis-\ntances between observations,\n\u22062\nii\u2032=||xi\u2212xi\u2032||2, (18.29)\nit turns out we can do all of the above as well. The trick is to convert the\npairwise distances to centered inner-products, and then proceed as before.\nWe write\n\u22062\nii\u2032=||xi\u2212\u00afx||2+||xi\u2032\u2212\u00afx||2\u22122\u221dan}\u230a\u2207a\u230bketle{txi\u2212\u00afx,xi\u2032\u2212\u00afx\u221dan}\u230a\u2207a\u230bket\u2207i}ht. (18.30)\nDe\ufb01ning B={\u2212\u22062\nii\u2032/2}, we double center B:\n\u02dcK= (I\u2212M)B(I\u2212M); (18.31)\nit is easy to check that \u02dcKii\u2032=\u221dan}\u230a\u2207a\u230bketle{txi\u2212\u00afx,xi\u2032\u2212\u00afx\u221dan}\u230a\u2207a\u230bket\u2207i}ht, the centered inner-product\nmatrix.\nDistances and inner-products also allow us to compute the medoid in each\nclass\u2014the observation with smallest average distance to other observations\nin that class. This can be used for classi\ufb01cation (closest medoids), as well as\nto drive k-medoids clustering (Section 14.3.10). With abstract data objects\nlike proteins, medoids have a practical advantage over means. The medoid is\none of the training examples, and can be displayed. We tried closest medoids\nin the example in the next section (see Table 18.3), and its performance is\ndisappointing.\nIt is useful to consider what we cannot do with inner-product kernels and\ndistances:", "690": "672 18. High-Dimensional Problems: p\u226bN\nTABLE 18.3. Cross-validated error rates for the abstracts example. The ne arest\nshrunken centroids ended up using no-shrinkage, but does use a word -by-word\nstandardization (section 18.2). This standardization gives it a distinct advantage\nover the other methods.\nMethod CV Error (SE)\n1. Nearest shrunken centroids 0.17 (0.05)\n2. SVM 0.23 (0.06)\n3. Nearest medoids 0.65 (0.07)\n4. 1-NN 0.44 (0.07)\n5. Nearest centroids 0.29 (0.07)\n\u2022We cannot standardize the variables; standardization signi\ufb01cantly im-\nproves performance in the example in the next section.\n\u2022We cannot assess directly the contributions of individual variables.\nIn particular, we cannot perform individual t-tests, \ufb01t the nearest\nshrunken centroids model, or \ufb01t any model that uses the lasso penalty.\n\u2022We cannot separate the good variables from the noise: all variables get\nan equal say. If, as is often the case, the ratio of relevant to irrelevant\nvariables is small, methods that use kernels are not likely to work as\nwell as methods that do feature selection.\n18.5.3 Example: Abstracts Classi\ufb01cation\nThis somewhat whimsical example serves to illustrate a limitation of ker -\nnel approaches. We collected the abstracts from 48 papers, 16 each from\nBradley Efron (BE), Trevor Hastie and Rob Tibshirani (HT) (frequent co-\nauthors), and Jerome Friedman (JF). We extracted all unique words from\nthese abstracts, and de\ufb01ned features xijto be the number of times word\njappears in abstract i. This is the so-called bag of words representation.\nQuotations, parentheses and special characters were \ufb01rst removed from the\nabstracts, and all characters were converted to lower case. We also removed\nthe word \u201cwe\u201d, which could unfairly discriminate HT abstracts from the\nothers.\nThere were 4492 total words, of which p= 1310 were unique. We sought\nto classify the documents into BE, HT or JF on the basis of the features\nxij. Although it is arti\ufb01cial, this example allows us to assess the possible\ndegradation in performance if information speci\ufb01c to the raw features is\nnot used.\nWe \ufb01rst applied the nearest shrunken centroid classi\ufb01er to the data, using\n10-fold cross-validation. It essentially chose no shrinkage, and so used all the\nfeatures; see the \ufb01rst line of Table 18.3. The error rate is 17%; the number\nof features can be reduced to about 500 without much loss in accuracy.", "691": "18.5 Classi\ufb01cation When Features are Unavailable 673\nNote that the nearest shrunken classi\ufb01er requires the raw feature matrix\nXin order to standardize the features individually. Figure 18.10 shows the\npredictivebayesusingthanalgorithmareproceduretechnologyvaluesaccuracyvariableswheninferencethosebayesianfrequentistproposepresentedmethodproblemsBE HT JF\nFIGURE 18.10. Abstracts example: top 20scores from nearest shrunken cen-\ntroids. Each score is the standardized di\ufb00erence in frequency f or the word in the\ngiven class (BE, HT or JF) versus all classes. Thus a positive score (to the right\nof the vertical grey zero lines) indicates a higher frequency in that class; a negative\nscore indicates a lower relative frequency.\ntop 20 discriminating words, with a positive score indicating that a word\nappears more in that class than in the other classes.\nSome of these terms make sense: for example \u201cfrequentist\u201d and \u201cBayesian\u201d\nre\ufb02ect Efron\u2019s greater emphasis on statistical inference. However, many oth-\ners are surprising, and re\ufb02ect personal writing styles: for example, Fried-\nman\u2019s use of \u201cpresented\u201d and HT\u2019s use of \u201cpropose\u201d.\nWe then applied the support vector classi\ufb01er with linear kernel and no\nregularization, using the \u201call pairs\u201d ( ovo) method to handle the three\nclasses (regularization of the SVM did not improve its performance). The\nresult is shown in Table 18.3. It does somewhat worse than the nearest\nshrunken centroid classi\ufb01er.\nAs mentioned, the \ufb01rst line of Table 18.3 represents nearest shrunken cen-\ntroids (with no shrinkage). Denote by sjthe pooled within-class standard\ndeviation for feature j, and s0the median of the sjvalues. Then line (1)\nalso corresponds to nearest centroid classi\ufb01cation, after \ufb01rst standardizing\neach feature by sj+s0[recall (18.4) on page 652].\nLine (3) shows that the performance of nearest medoids is very poor,\nsomething which surprised us. It is perhaps due to the small sample sizes", "692": "674 18. High-Dimensional Problems: p\u226bN\nand high dimensions, with medoids having much higher variance than\nmeans. The performance of the one-nearest neighbor classi\ufb01er is also poor.\nThe performance of the nearest centroid classi\ufb01er is also shown in Ta-\nble 18.3 in line (5): it is better than nearest medoids, but worse than that\nof nearest shrunken centroids, even with no shrinkage. The di\ufb00erence seems\nto be the standardization of each feature that is done in nearest shrunken\ncentroids. This standardization is important here, and requires access to\nthe individual feature values. Nearest centroids uses a spherical metric, and\nrelies on the fact that the features are in similar units. The support vector\nmachine estimates a linear combination of the features and can better deal\nwith unstandardized features.\n18.6 High-Dimensional Regression: Supervised\nPrincipal Components\nIn this section we describe a simple approach to regression and generalized\nregression that is especially useful when p\u226bN. We illustrate the method\non another microarray data example. The data is taken from Rosenwald\net al. (2002) and consists of 240 samples from patients with di\ufb00use large\nB-cell lymphoma (DLBCL), with gene expression measurements for 7399\ngenes. The outcome is survival time, either observed or right censored. We\nrandomly divided the lymphoma samples into a training set of size 160 and\na test set of size 80.\nAlthough supervised principal components is useful for linear regression,\nits most interesting applications may be in survival studies, which is the\nfocus of this example.\nWe have not yet discussed regression with censored survival data in this\nbook; it represents a generalized form of regression in which the outcome\nvariable (survival time) is only partly observed for some individuals. Sup-\npose for example we carry out a medical study that lasts for 365 days, and\nfor simplicity all subjects are recruited on day one. We might observe one\nindividual to die 200 days after the start of the study. Another individ-\nual might still be alive at 365 days when the study ends. This individual\nis said to be \u201cright censored\u201d at 365 days. We know only that he or she\nlivedat least 365 days. Although we do not know how long past 365 days\nthe individual actually lived, the censored observation is still informative.\nThis is illustrated in Figure 18.11. Figure 18.12 shows the survival cur ve\nestimated by the Kaplan\u2013Meier method for the 80 patients in the test set.\nSee for example Kalb\ufb02eisch and Prentice (1980) for a description of the\nKaplan\u2013Meier method.\nOur objective in this example is to \ufb01nd a set of features (genes) that\ncan predict the survival of an independent set of patients. This could be", "693": "18.6 High-Dimensional Regression: Supervised Principal Components 675\nTime(days)Patient\n0 100 200 300 365 1234\nFIGURE 18.11. Censored survival data. For illustration there are four pati ents.\nThe \ufb01rst and third patients die before the study ends. The second pa tient is alive\nat the end of the study ( 365days), while the fourth patient is lost to follow-up\nbefore the study ends. For example, this patient might have move d out of the\ncountry. The survival times for patients two and four are said to be \u201ccensored.\u201d\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0l\nlllllllll lll\nll ll ll l lllll\nl ll l l l lPr(T\u2265t)\nMonths tSurvival Function\nFIGURE 18.12. Lymphoma data. The Kaplan\u2013Meier estimate of the survival\nfunction for the 80patients in the test set, along with one-standard-error curves.\nThe curve estimates the probability of surviving past tmonths. The ticks indicate\ncensored observations.", "694": "676 18. High-Dimensional Problems: p\u226bNprobability densityPoor Cell Type Good Cell Type\nSurvival Time\nFIGURE 18.13. Underlying conceptual model for supervised principal compo-\nnents. There are two cell types, and patients with the good cell ty pe live longer on\nthe average. Supervised principal components estimate the cell type, by averaging\nthe expression of genes that re\ufb02ect it.\nuseful as a prognostic indicator to aid in choosing treatments, or to help\nunderstand the biological basis for the disease.\nThe underlying conceptual model for supervised principal components\nis shown in Figure 18.13. We imagine that there are two cell types, and\npatients with the good cell type live longer on the average. However there\nis considerable overlap in the two sets of survival times. We might think\nof survival time as a \u201cnoisy surrogate\u201d for cell type. A fully supervised\napproach would give the most weight to those genes having the strongest\nrelationship with survival. These genes are partially, but not perfectly, re-\nlated to cell type. If we could instead discover the underlying cell types of\nthe patients, often re\ufb02ected by a sizable signature of genes acting together\nin pathways, then we might do a better job of predicting patient survival.\nAlthough the cell type in Figure 18.13 is discrete, it is useful to imagine\na continuous cell type, de\ufb01ne by some linear combination of the features.\nWe will estimate the cell type as a continuous quantity, and then discretize\nit for display and interpretation.\nHow can we \ufb01nd the linear combination that de\ufb01nes the important under-\nlying cell types? Principal components analysis (Section 14.5) is an e\ufb00ective\nmethod for \ufb01nding linear combinations of features that exhibit large varia-\ntion in a dataset. But what we seek here are linear combinations with both\nhigh variance andsigni\ufb01cant correlation with the outcome. The lower right\npanel of Figure 18.14 shows the result of applying standard principal com-\nponents in this example; the leading component does not correlate strongly\nwith survival (details are given in the \ufb01gure caption).\nHence we want to encourage principal component analysis to \ufb01nd linear\ncombinations of features that have high correlation with the outcome. To\ndo this, we restrict attention to features which by themselves have a siz-\nable correlation with the outcome. This is summarized in the supervised\nprincipal components Algorithm 18.1, and illustrated in Figure 18.14.\nThe details in steps (1) and (2b) will depend on the type of outcome\nvariable. For a standard regression problem, we use the univariate linear\nleast squares coe\ufb03cients in step (1) and a linear least squares model in", "695": "18.6 High-Dimensional Regression: Supervised Principal Components 677\n7399 7350 50 27 1 Genes\nPatientsSupervised PC\n1 80 160\nAbsolute Cox Score0 2 40 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of Survivallow score\nhigh scoreBest Single Gene\nP=0.15\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of SurvivalSupervised Principal Component \u2212 27 Genes\nP=0.006\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0\nMonthsProbability of SurvivalPrincipal Component \u2212 7399 Genes\nP=0.14\nFIGURE 18.14. Supervised principal components on the lymphoma data. The\nleft panel shows a heatmap of a subset of the gene-expression tra ining data. The\nrows are ordered by the magnitude of the univariate Cox-score, s hown in the mid-\ndle vertical column. The top 50 and bottom 50genes are shown. The supervised\nprincipal component uses the top 27genes (chosen by 10-fold CV). It is repre-\nsented by the bar at the top of the heatmap, and is used to order th e columns\nof the expression matrix. In addition, each row is multiplied by the sign of the\nCox-score. The middle panel on the right shows the survival cur ves on the test\ndata when we create a low and high group by splitting this superv ised PC at zero\n(training data mean). The curves are well separated, as indicate d by the p-value\nfor the log-rank test. The top panel does the same, using the top- scoring gene on\nthe training data. The curves are somewhat separated, but not si gni\ufb01cantly. The\nbottom panel uses the \ufb01rst principal component on all the genes, and t he separa-\ntion is also poor. Each of the top genes can be interpreted as nois y surrogates for\na latent underlying cell-type characteristic, and supervised p rincipal components\nuses them all to estimate this latent factor.", "696": "678 18. High-Dimensional Problems: p\u226bN\nAlgorithm 18.1 Supervised Principal Components.\n1. Compute the standardized univariate regression coe\ufb03cients for the\noutcome as a function of each feature separately.\n2. For each value of the threshold \u03b8from the list 0 \u2264\u03b81< \u03b82<\u2264\u2264\u2264< \u03b8K:\n(a) Form a reduced data matrix consisting of only those features\nwhose univariate coe\ufb03cient exceeds \u03b8in absolute value, and\ncompute the \ufb01rst mprincipal components of this matrix.\n(b) Use these principal components in a regression model to predict\nthe outcome.\n3. Pick \u03b8(andm) by cross-validation.\nstep (2b). For survival problems, Cox\u2019s proportional hazards regression\nmodel is widely used; hence we use the score test from this model in step (1)\nand the multivariate Cox model in step (2b). The details are not essential\nfor understanding the basic method; they may be found in Bair et al. (2006).\nFigure 18.14 shows the results of supervised principal components in this\nexample. We used a Cox-score cuto\ufb00 of 3.53, yielding 27 genes, where the\nvalue 3.53 was found through 10-fold cross-validation. We then computed\nthe \ufb01rst principal component ( m= 1) using just this subset of the data,\nas well as its value for each of the test observations. We included this as\na quantitative predictor in a Cox regression model, and its likelihood-rati o\nsigni\ufb01cance was p= 0.005. When dichotomized (using the mean score on\nthe training data as a threshold), it clearly separates the patients in the\ntest set into low and high risk groups (middle-right panel of Figure 18.14,\np= 0.006).\nThe top-right panel of Figure 18.14 uses the top scoring gene (dichot-\nomized) alone as a predictor of survival. It is not signi\ufb01cant on the test set.\nLikewise, the lower-right panel shows the dichotomized principal compo-\nnent using all the training data, which is also not signi\ufb01cant.\nOur procedure allows m >1 principal components in step (2a). However,\nthe supervision in step (1) encourages the principal components to align\nwith the outcome, and thus in most cases only the \ufb01rst or \ufb01rst few com-\nponents tend to be useful for prediction. In the mathematical development\nbelow, we consider only the \ufb01rst component, but extensions to more than\none component can be derived in a similar way.\n18.6.1 Connection to Latent-Variable Modeling\nA formal connection between supervised principal components and the un-\nderlying cell type model (Figure 18.13) can be seen through a latent variable\nmodel for the data. Suppose we have a response variable Ywhich is related", "697": "18.6 High-Dimensional Regression: Supervised Principal Components 679\nto an underlying latent variable Uby a linear model\nY=\u03b20+\u03b21U+\u03b5. (18.32)\nIn addition, we have measurements on a set of features Xjindexed by j\u2208 P\n(for pathway), for which\nXj=\u03b10j+\u03b11jU+\u01ebj, j\u2208 P. (18.33)\nThe errors \u03b5and\u01ebjare assumed to have mean zero and are independent of\nall other random variables in their respective models.\nWe also have many additional features Xk,k\u221dne}ationslash\u2208 Pwhich are independent\nofU. We would like to identify P, estimate U, and hence \ufb01t the predic-\ntion model (18.32). This is a special case of a latent-structure model, or\nsingle-component factor-analysis model (Mardia et al., 1979, see also Sec-\ntion 14.7). The latent factor Uis a continuous version of the cell type\nconceptualized in Figure 18.13.\nThe supervised principal component algorithm can be seen as a method\nfor \ufb01tting this model:\n\u2022The screening step (1) estimates the set P.\n\u2022Given /hatwideP, the largest principal component in step (2a) estimates the\nlatent factor U.\n\u2022Finally, the regression \ufb01t in step (2b) estimates the coe\ufb03cient in\nmodel (18.32).\nStep (1) is natural, since on average the regression coe\ufb03cient is nonzero\nonly if \u03b11jis non-zero. Hence this step should select the features j\u2208 P.\nStep (2a) is natural if we assume that the errors \u01ebjhave a Gaussian dis-\ntribution, with the same variance. In this case the principal component is\nthe maximum likelihood estimate for the single factor model (Mardia et\nal., 1979). The regression in (2b) is an obvious \ufb01nal step.\nSuppose there are a total of pfeatures, with p1features in the relevant set\nP. Then if pandp1grow but p1is small relative to p, one can show (under\nreasonable conditions) that the leading supervised principal component\nis consistent for the underlying latent factor. The usual leading principal\ncomponent may not be consistent, since it can be contaminated by the\npresence of a large number of \u201cnoise\u201d features.\nFinally, suppose that the threshold used in step (1) of the supervised\nprincipal component procedure yields a large number of features for com-\nputation of the principal component. Then for interpretational purposes, as\nwell as for practical uses, we would like some way of \ufb01nding a reduced a set\nof features that approximates the model. Pre-conditioning (Section 18.6.3)\nis one way of doing this.", "698": "680 18. High-Dimensional Problems: p\u226bN\n18.6.2 Relationship with Partial Least Squares\nSupervised principal components is closely related to partial least squares\nregression (Section 3.5.2). Bair et al. (2006) found that the key to the goo d\nperformance of supervised principal components was the \ufb01ltering out of\nnoisy features in step (2a). Partial least squares (Section 3.5.2) downwei ghts\nnoisy features, but does not throw them away; as a result a large number\nof noisy features can contaminate the predictions. However, a modi\ufb01cation\nof the partial least squares procedure has been proposed that has a similar\n\ufb02avor to supervised principal components [Brown et al. (1991),Nadler and\nCoifman (2005), for example]. We select the features as in steps (1) and\n(2a) of supervised principal components, but then apply PLS (rather than\nprincipal components) to these features. For our current discussion, we call\nthis \u201cthresholded PLS.\u201d\nThresholded PLS can be viewed as a noisy version of supervised principal\ncomponents, and hence we might not expect it to work as well in practice.\nAssume the variables are all standardized. The \ufb01rst PLS variate has the\nform\nz=/summationdisplay\nj\u2208P\u221dan}\u230a\u2207a\u230bketle{ty,xj\u221dan}\u230a\u2207a\u230bket\u2207i}htxj, (18.34)\nand can be thought of as an estimate of the latent factor Uin model (18.33).\nIn contrast, the supervised principal components direction \u02c6usatis\ufb01es\n\u02c6u=1\nd2/summationdisplay\nj\u2208P\u221dan}\u230a\u2207a\u230bketle{t\u02c6u,xj\u221dan}\u230a\u2207a\u230bket\u2207i}htxj, (18.35)\nwhere dis the leading singular value of XP. This follows from the de\ufb01nition\nof the leading principal component. Hence thresholded PLS uses weights\nwhich are the inner product of ywith each of the features, while supervised\nprincipal components uses the features to derive a \u201cself-consistent\u201d estimate\n\u02c6u. Since many features contribute to the estimate \u02c6u, rather than just the\nsingle outcome y, we can expect \u02c6uto be less noisy than z. In fact, if there\narep1features in the set P, andN, pandp1go to in\ufb01nity with p1/N\u21920,\nthen it can be shown using the techniques in Bair et al. (2006) that\nz=u+Op(1)\n\u02c6u=u+Op(/radicalbig\np1/N), (18.36)\nwhere uis the true (unobservable) latent variable in the model (18.32),\n(18.33).\nWe now present a simulation example to compare the methods numeri-\ncally. There are N= 100 samples and p= 5000 genes. We generated the\ndata as follows:", "699": "18.6 High-Dimensional Regression: Supervised Principal Components 681\nFIGURE 18.15. Heatmap of the outcome (left column) and \ufb01rst 500genes from\na realization from model (18.37). The genes are in the columns, and the samples\nare in the rows.\nxij=/braceleft\uf8ecigg\n3 +\u01ebijifi\u226450,\n4 +\u01ebijifi >50j= 1,... ,50\nxij=/braceleft\uf8ecigg\n1.5 +\u01ebijif 1\u2264i\u226425 or 51 \u2264i\u226475\n5.5 +\u01ebijif 26\u2264i\u226450 or 76 \u2264i\u2264100j= 51,... ,250\nxij=\u01ebij j= 251 ,... ,5000\nyi= 2\u22641\n50/summationtext50\nj=1xij+\u03b5i\n(18.37)\nwhere \u01ebijand\u03b5iare independent normal random variables with mean 0 and\nstandard deviations 1 and 1.5, respectively. Thus in the \ufb01rst 50 genes, there\nis an average di\ufb00erence of 1 unit between samples 1\u201350 and 51\u2013100, and this\ndi\ufb00erence correlates with the outcome y. The next 200 genes have a large\naverage di\ufb00erence of 4 units between samples (1\u201325, 51\u201375) and (26\u201350,\n76\u2013100), but this di\ufb00erence is uncorrelated with the outcome. The rest of\nthe genes are noise. Figure 18.15 shows a heatmap of a typical realization,\nwith the outcome at the left, and the \ufb01rst 500 genes to the right.\nWe generated 100 simulations from this model, and summarize the test\nerror results in Figure 18.16. The test errors of principal components and\npartial least squares are shown at the right of the plot; both are badly\na\ufb00ected by the noisy features in the data. Supervised principal components\nand thresholded PLS work best over a wide range of the number of selected\nfeatures, with the former showing consistently lower test errors.\nWhile this example seems \u201ctailor-made\u201d for supervised principal com-\nponents, its good performance seems to hold in other simulated and real\ndatasets (Bair et al., 2006).\n18.6.3 Pre-Conditioning for Feature Selection\nSupervised principal components can yield lower test errors than competing\nmethods, as shown in Figure 18.16. However, it does not always produce a\nsparse model involving only a small number of features (genes). Even if the\nthresholding in Step (1) of the algorithm yields a relatively small number", "700": "682 18. High-Dimensional Problems: p\u226bN1.00 1.05 1.10 1.15 1.20 1.25\nNumber of FeaturesRelative Root Mean Square Test Error\n0 50 100 150 200 250 300 ... 5000Thresholded PLS\nSupervised Principal Components\nFIGURE 18.16. Root mean squared test error ( \u00b1one standard error), for\nsupervised principal components and thresholded PLS on 100realizations from\nmodel (18.37). All methods use one component, and the errors are r elative to\nthe noise standard deviation (the Bayes error is 1.0). For both methods, di\ufb00erent\nvalues for the \ufb01ltering threshold were tried and the number of fea tures retained\nis shown on the horizontal axis. The extreme right points corresp ond to regular\nprincipal components and partial least squares, using all the gene s.\nof features, it may be that some of the omitted features have sizable inner\nproducts with the supervised principal component (and could act as a good\nsurrogate). In addition, highly correlated features will tend to be chosen\ntogether, and there may be great deal of redundancy in the set of selected\nfeatures.\nThe lasso (Sections 18.4 and 3.4.2), on the other hand, produces a sparse\nmodel from the data. How do the test errors of the two methods compare on\nthe simulated example of the last section? Figure 18.17 shows the test errors\nfor one realization from model (18.37) for the lasso, supervised principal\ncomponents, and the pre-conditioned lasso (described below).\nWe see that supervised principal components (orange curve) reaches its\nlowest error when about 50 features are included in the model, which is\nthe correct number for the simulation. Although a linear model in the \ufb01rst\n50 features is optimal, the lasso (green) is adversely a\ufb00ected by the large\nnumber of noisy features, and starts over\ufb01tting when far fewer are in the\nmodel.\nCan we get the low test error of supervised principal components along\nwith the sparsity of the lasso? This is the goal of pre-conditioning (Paul\net al., 2008). In this approach, one \ufb01rst computes the supervised principal\ncomponent predictor \u02c6 yifor each observation in the training set (with the", "701": "18.7 Feature Assessment and the Multiple-Testing Problem 683\n0 50 100 150 200 2503.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4\nNumber of Features in ModelMean Test ErrorLasso\nSupervised Principal Components\nPreconditioned Lasso\nFIGURE 18.17. Test errors for the lasso, supervised principal components,\nand pre-conditioned lasso, for one realization from model (18.3 7). Each model is\nindexed by the number of non-zero features. The supervised princip al component\npath is truncated at 250features. The lasso self-truncates at 100, the sample size\n(see Section 18.4). In this case, the pre-conditioned lasso ach ieves the lowest error\nwith about 25features.\nthreshold selected by cross-validation). Then we apply the lasso with \u02c6 yias\nthe outcome variable, in place of the usual outcome yi. All features are used\nin the lasso \ufb01t, not just those that were retained in the thresholding step\nin supervised principal components. The idea is that by \ufb01rst denoising the\noutcome variable, the lasso should not be as adversely a\ufb00ected by the large\nnumber of noise features. Figure 18.17 shows that pre-conditioning (purple\ncurve) has been successful here, yielding much lower test error than the\nusual lasso, and as low (in this case) as for supervised principal components.\nIt also can achieve this using less features. The usual lasso, applied to\nthe raw outcome, starts to over\ufb01t more quickly than the pre-conditioned\nversion. Over\ufb01tting is not a problem, since the outcome variable has been\ndenoised. We usually select the tuning parameter for the pre-conditioned\nlasso on more subjective grounds, like parsimony.\nPre-conditioning can be applied in a variety of settings, using initial\nestimates other than supervised principal components and post-processors\nother than the lasso. More details may be found in Paul et al. (2008).\n18.7 Feature Assessment and the Multiple-Testing\nProblem\nIn the \ufb01rst part of this chapter we discuss prediction models in the p\u226bN\nsetting. Here we consider the more basic problem of assessing the signif-", "702": "684 18. High-Dimensional Problems: p\u226bN\nicance of each of the pfeatures. Consider the protein mass spectrometry\nexample of Section 18.4.1. In that problem, the scientist might not be inter-\nested in predicting whether a given patient has prostate cancer. Rather the\ngoal might be to identify proteins whose abundance di\ufb00ers between nor-\nmal and cancer samples, in order to enhance understanding of the disease\nand suggest targets for drug development. Thus our goal is to assess the\nsigni\ufb01cance of individual features. This assessment is usually done without\nthe use of a multivariate predictive model like those in the \ufb01rst part of this\nchapter. The feature assessment problem moves our focus from prediction\nto the traditional statistical topic of multiple hypothesis testing . For the\nremainder of this chapter we will use Minstead of pto denote the number\nof features, since we will frequently be referring to p-values .\nTABLE 18.4. Subset of the 12,625genes from microarray study of radiation\nsensitivity. There are a total of 44samples in the normal group and 14in the\nradiation sensitive group; we only show three samples from eac h group.\nNormal Radiation Sensitive\nGene 1 7.85 29.74 29.50 . . . 17.20 -50.75 -18.89 . . .\nGene 2 15.44 2.70 19.37 . . . 6.57 -7.41 79.18 . . .\nGene 3 -1.79 15.52 -3.13 . . . -8.32 12.64 4.75 . . .\nGene 4 -11.74 22.35 -36.11 . . . -52.17 7.24 -2.32 . . .\n...........................\nGene 12,625 -14.09 32.77 57.78 . . . -32.84 24.09 -101.44 . . .\nConsider, for example, the microarray data in Table 18.4, taken from a\nstudy on the sensitivity of cancer patients to ionizing radiation treatment\n(Rieger et al., 2004). Each row consists of the expression of genes in 58\npatient samples: 44 samples were from patients with a normal reaction, and\n14 from patients who had a severe reaction to radiation. The measurements\nwere made on oligo-nucleotide microarrays. The object of the experiment\nwas to \ufb01nd genes whose expression was di\ufb00erent in the radiation sensitive\ngroup of patients. There are M= 12,625 genes altogether; the table shows\nthe data for some of the genes and samples for illustration.\nTo identify informative genes, we construct a two-sample t-statistic for\neach gene.\ntj=\u00afx2j\u2212\u00afx1j\nsej, (18.38)\nwhere \u00af xkj=/summationtext\ni\u2208C\u2113xij/N\u2113. Here C\u2113are the indices of the N\u2113samples in\ngroup \u2113, where \u2113= 1 is the normal group and \u2113= 2 is the sensitive group.\nThe quantity se jis the pooled within-group standard error for gene j:", "703": "18.7 Feature Assessment and the Multiple-Testing Problem 685\n\u22124 \u22122 0 2 40 200 400 600 800\nt\u2212statistics\nFIGURE 18.18. Radiation sensitivity microarray example. A histogram of the\n12,625t-statistics comparing the radiation-sensitive versus insensit ive groups.\nOverlaid in blue is the histogram of the t-statistics from 1000permutations of the\nsample labels.\nsej= \u02c6\u03c3j/radical\uf8ecig\n1\nN1+1\nN2; \u02c6\u03c32\nj=1\nN1+N2\u22122/parenleft\uf8ecigg/summationdisplay\ni\u2208C1(xij\u2212\u00afx1j)2+/summationdisplay\ni\u2208C2(xij\u2212\u00afx2j)2/parenright\uf8ecigg\n.\n(18.39)\nA histogram of the 12,625 t-statistics is shown in orange in Figure 18 .18,\nranging in value from \u22124.7 to 5.0. If the tjvalues were normally distributed\nwe could consider any value greater than two in absolute value to be sig-\nni\ufb01cantly large. This would correspond to a signi\ufb01cance level of about 5%.\nHere there are 1189 genes with |tj| \u22652. However with 12,625 genes we\nwould expect many large values to occur by chance, even if the group-\ning is unrelated to any gene. For example, if the genes were independent\n(which they are surely not), the number of falsely signi\ufb01cant genes would\nhave a binomial distribution with mean 12 ,625\u22640.05 = 631 .3 and standard\ndeviation 24.5; the actual 1189 is way out of range.\nHow do we assess the results for all 12,625 genes? This is called the mul-\ntiple testing problem. We can start as above by computing a p-value for\neach gene. This can be done using the theoretical t-distribution probabil-\nities, which assumes the features are normally distributed. An attractive\nalternative approach is to use the permutation distribution, since it avoids\nassumptions about the distribution of the data. We compute (in principle)\nallK=/parenleftbig58\n14/parenrightbig\npermutations of the sample labels, and for each permutation\nkcompute the t-statistics tk\nj. Then the p-value for gene jis", "704": "686 18. High-Dimensional Problems: p\u226bN\npj=1\nKK/summationdisplay\nk=1I(|tk\nj|>|tj|). (18.40)\nOf course,/parenleftbig58\n14/parenrightbig\nis a large number (around 1013) and so we can\u2019t enumer-\nate all of the possible permutations. Instead we take a random sample of\nthe possible permutations; here we took a random sample of K= 1000\npermutations.\nTo exploit the fact that the genes are similar (e.g., measured on the\nsame scale), we can instead pool the results for all genes in computing the\np-values.\npj=1\nMKM/summationdisplay\nj\u2032=1K/summationdisplay\nk=1I(|tk\nj\u2032|>|tj|). (18.41)\nThis also gives more granular p-values than does (18.40), since there many\nmore values in the pooled null distribution than there are in each individual\nnull distribution.\nUsing this set of p-values, we would like to test the hypotheses:\nH0j= treatment has no e\ufb00ect on gene j\nversus (18.42)\nH1j= treatment has an e\ufb00ect on gene j\nfor all j= 1,2,... ,M . We reject H0jat level \u03b1ifpj< \u03b1. This test has\ntype-I error equal to \u03b1; that is, the probability of falsely rejecting H0jis\u03b1.\nNow with many tests to consider, it is not clear what we should use\nas an overall measure of error. Let Ajbe the event that H0jis falsely\nrejected; by de\ufb01nition Pr( Aj) =\u03b1. The family-wise error rate (FWER)\nis the probability of at least one false rejection, and is a commonly used\noverall measure of error. In detail, if A=\u222aM\nj=1Ajis the event of at least\none false rejection, then the FWER is Pr( A). Generally Pr( A)\u226b\u03b1for\nlargeM, and depends on the correlation between the tests. If the tests are\nindependent each with type-I error rate \u03b1, then the family-wise error rate\nof the collection of tests is (1 \u2212(1\u2212\u03b1)M). On the other hand, if the tests\nhave positive dependence, that is Pr( Aj|Ak)>Pr(Aj), then the FWER\nwill be less than (1 \u2212(1\u2212\u03b1)M). Positive dependence between tests often\noccurs in practice, in particular in genomic studies.\nOne of the simplest approaches to multiple testing is the Bonferroni\nmethod. It makes each individual test more stringent, in order to make the\nFWER equal to at most \u03b1: we reject H0jifpj< \u03b1/M . It is easy to show\nthat the resulting FWER is \u2264\u03b1(Exercise 18.16). The Bonferroni method\ncan be useful if Mis relatively small, but for large Mit is too conservative,\nthat is, it calls too few genes signi\ufb01cant.\nIn our example, if we test at level say \u03b1= 0.05, then we must use the\nthreshold 0 .05/12,625 = 3 .9\u00d710\u22126. None of the 12 ,625 genes had a p-value\nthis small.", "705": "18.7 Feature Assessment and the Multiple-Testing Problem 687\nThere are variations to this approach that adjust the individual p-values\nto achieve an FWER of at most \u03b1, with some approaches avoiding the\nassumption of independence; see, e.g., Dudoit et al. (2002b).\n18.7.1 The False Discovery Rate\nA di\ufb00erent approach to multiple testing does not try to control the FWER,\nbut focuses instead on the proportion of falsely signi\ufb01cant genes. As we will\nsee, this approach has a strong practical appeal.\nTable 18.5 summarizes the theoretical outcomes of Mhypothesis tests.\nNote that the family-wise error rate is Pr( V\u22651). Here we instead focus\nTABLE 18.5. Possible outcomes from Mhypothesis tests. Note that Vis the\nnumber of false-positive tests; the type-I error rate is E(V)/M0. The type-II error\nrate is E(T)/M1, and the power is 1\u2212E(T)/M1.\nCalled Called\nNot Signi\ufb01cant Signi\ufb01cant Total\nH0True U V M0\nH0False T S M1\nTotal M\u2212R R M\non the false discovery rate\nFDR = E( V/R). (18.43)\nIn the microarray setting, this is the expected proportion of genes that\nare incorrectly called signi\ufb01cant, among the Rgenes that are called signif-\nicant. The expectation is taken over the population from which the data\nare generated. Benjamini and Hochberg (1995) \ufb01rst proposed the notion of\nfalse discovery rate, and gave a testing procedure (Algorithm 18.2) whose\nFDR is bounded by a user-de\ufb01ned level \u03b1. The Benjamini\u2013Hochberg (BH)\nprocedure is based on p-values; these can be obtained from an asymptotic\napproximation to the test statistic (e.g., Gaussian), or a permutatio n dis-\ntribution, as is done here.\nIf the hypotheses are independent, Benjamini and Hochberg (1995) show\nthat regardless of how many null hypotheses are true and regardless of the\ndistribution of the p-values when the null hypothesis is false, this procedure\nhas the property\nFDR\u2264M0\nM\u03b1\u2264\u03b1. (18.45)\nFor illustration we chose \u03b1= 0.15. Figure 18.19 shows a plot of the or-\ndered p-values p(j), and the line with slope 0 .15/12625.", "706": "688 18. High-Dimensional Problems: p\u226bN\nAlgorithm 18.2 Benjamini\u2013Hochberg (BH) Method.\n1. Fix the false discovery rate \u03b1and let p(1)\u2264p(2)\u2264 \u2264\u2264\u2264 \u2264 p(M)denote\nthe ordered p-values\n2. De\ufb01ne\nL= max/braceleft\uf8ecig\nj:p(j)< \u03b1\u2264j\nM/braceright\uf8ecig\n. (18.44)\n3. Reject all hypotheses H0jfor which pj\u2264p(L), the BH rejection\nthreshold.\nGenes ordered by p\u2212valuep\u2212value\n1 5 10 50 1005*10^\u22126 5*10^\u22125 5*10^\u22124 5*10^\u22123\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\nFIGURE 18.19. Microarray example continued. Shown is a plot of the ordered\np-values p(j)and the line 0.15\u2264(j/12,625), for the Benjamini\u2013Hochberg method.\nThe largest jfor which the p-value p(j)falls below the line, gives the BH threshold.\nHere this occurs at j= 11, indicated by the vertical line. Thus the BH method\ncalls signi\ufb01cant the 11genes (in red) with smallest p-values.", "707": "18.7 Feature Assessment and the Multiple-Testing Problem 689\nAlgorithm 18.3 The Plug-in Estimate of the False Discovery Rate.\n1. Create Kpermutations of the data, producing t-statistics tk\njfor fea-\ntures j= 1,2,... ,M and permutations k= 1,2,... ,K .\n2. For a range of values of the cut-point C, let\nRobs=M/summationdisplay\nj=1I(|tj|> C),/hatwideE(V) =1\nKM/summationdisplay\nj=1K/summationdisplay\nk=1I(|tk\nj|> C).(18.46)\n3. Estimate the FDR by /hatwideFDR = /hatwideE(V)/Robs.\nStarting at the left and moving right, the BH method \ufb01nds the last time\nthat the p-values fall below the line. This occurs at j= 11, so we reject\nthe 11 genes with smallest p-values. Note that the cuto\ufb00 occurs at the 11th\nsmallest p-value, 0.00012, and the 11th largest of the values |tj|is 4.101\nThus we reject the 11 genes with |tj| \u22654.101.\nFrom our brief description, it is not clear how the BH procedure works;\nthat is, why the corresponding FDR is at most 0 .15, the value used for \u03b1.\nIndeed, the proof of this fact is quite complicated (Benjamini and Hochberg,\n1995).\nA more direct way to proceed is a plug-in approach. Rather than starting\nwith a value for \u03b1, we \ufb01x a cut-point for our t-statistics, say the value\n4.101 that appeared above. The number of observed values |tj|equal or\ngreater than 4 .101 is 11. The total number of permutation values |tk\nj|equal\nor greater than 4 .101 is 1518, for an average of 1518 /1000 = 1 .518 per\npermutation. Thus a direct estimate of the false discovery rate is /hatwideFDR =\n1.518/11\u224814%. Note that 14% is approximately equal to the value of\n\u03b1= 0.15 used above (the di\ufb00erence is due to discreteness). This procedure\nis summarized in Algorithm 18.3. To recap:\nThe plug-in estimate of FDR of Algorithm 18.3 is equivalent to the BH\nprocedure of Algorithm 18.2, using the permutation p-values (18.40).\nThis correspondence between the BH method and the plug-in estimate is\nnot a coincidence. Exercise 18.17 shows that they are equivalent in general.\nNote that this procedure makes no reference to p-values at all, but rather\nworks directly with the test statistics.\nThe plug-in estimate is based on the approximation\nE(V/R)\u2248E(V)\nE(R), (18.47)\nand in general /hatwideFDR is a consistent estimate of FDR (Storey, 2002; Storey et\nal., 2004). Note that the numerator /hatwideE(V) actually estimates ( M/M 0)E(V),", "708": "690 18. High-Dimensional Problems: p\u226bN\nsince the permutation distribution uses Mrather M0null hypotheses.\nHence if an estimate of M0is available, a better estimate of FDR can be\nobtained from ( \u02c6M0/M)\u2264/hatwideFDR. Exercise 18.19 shows a way to estimate M0.\nThe most conservative (upwardly biased) estimate of FDR uses M0=M.\nEquivalently, an estimate of M0can be used to improve the BH method,\nthrough relation (18.45).\nThe reader might be surprised that we chose a value as large as 0 .15 for\n\u03b1, the FDR bound. We must remember that the FDR is not the same as\ntype-I error, for which 0.05 is the customary choice. For the scientist, the\nfalse discovery rate is the expected proportion of false positive genes am ong\nthe list of genes that the statistician tells him are signi\ufb01cant. Microarra y\nexperiments with FDRs as high as 0.15 might still be useful, especially i f\nthey are exploratory in nature.\n18.7.2 Asymmetric Cutpoints and the SAM Procedure\nIn the testing methods described above, we used the absolute value of the\ntest statistic tj, and hence applied the same cut-points to both positive and\nnegative values of the statistic. In some experiments, it might happen that\nmost or all of the di\ufb00erentially expressed genes change in the positive direc-\ntion (or all in the negative direction). For this situation it is advanta geous\nto derive separate cut-points for the two cases.\nThesigni\ufb01cance analysis of microarrays (SAM) approach o\ufb00ers a way of\ndoing this. The basis of the SAM method is shown in Figure 18.20. On the\nvertical axis we have plotted the ordered test statistics t(1)\u2264t(2)\u2264 \u2264\u2264\u2264 \u2264\nt(M), while the horizontal axis shows the expected order statistics from the\npermutations of the data: \u02dct(j)= (1/K)/summationtextK\nk=1tk\n(j), where tk\n(1)\u2264tk\n(2)\u2264 \u2264\u2264\u2264 \u2264\ntk\n(M)are the ordered test statistics from permutation k.\nTwo lines are drawn, parallel to the 45\u25e6line, \u2206 units away. Starting at\nthe origin and moving to the right, we \ufb01nd the \ufb01rst place that the genes\nleave the band. This de\ufb01nes the upper cutpoint Chiand all genes beyond\nthat point are called signi\ufb01cant (marked red). Similarly we \ufb01nd the lower\ncutpoint Clowfor genes in the bottom left corner. Thus each value of the\ntuning parameter \u2206 de\ufb01nes upper and lower cutpoints, and the plug-in\nestimate /hatwideFDR for each of these cutpoints is estimated as before. Typically\na range of values of \u2206 and associated /hatwideFDR values are computed, from which\na particular pair are chosen on subjective grounds.\nThe advantage of the SAM approach lies in the possible asymmetry of\nthe cutpoints. In the example of Figure 18.20, with \u2206 = 0 .71 we obtain\n11 signi\ufb01cant genes; they are all in the upper right. The data points in the\nbottom left never leave the band, and hence Clow=\u2212\u221e. Hence for this\nvalue of \u2206, no genes are called signi\ufb01cant on the left (negative) side. We\ndo not impose symmetry on the cutpoints, as was done in Section 18.7.1,\nas there is no reason to assume similar behavior at the two ends.", "709": "18.7 Feature Assessment and the Multiple-Testing Problem 691\nExpected Order Statisticst\u2212statistic\n\u22124 \u22122 0 2 4\u22124 \u22122 0 2 4\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022 \u2022 \u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022 \u2022 \u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2206\nChi\nFIGURE 18.20. SAM plot for the radiation sensitivity microarray data. On the\nvertical axis we have plotted the ordered test statistics, wh ile the horizontal axis\nshows the expected order statistics of the test statistics fr om permutations of the\ndata. Two lines are drawn, parallel to the 45\u25e6line,\u2206units away from it. Starting\nat the origin and moving to the right, we \ufb01nd the \ufb01rst place that the g enes leave\nthe band. This de\ufb01nes the upper cut-point Chiand all genes beyond that point are\ncalled signi\ufb01cant (marked in red). Similarly we de\ufb01ne a lower cutpo intClow. For\nthe particular value of \u2206 = 0 .71in the plot, no genes are called signi\ufb01cant in the\nbottom left.", "710": "692 18. High-Dimensional Problems: p\u226bN\nThere is some similarity between this approach and the asymmetry possi-\nble with likelihood-ratio tests. Suppose we have a log-likelihood \u21130(tj) under\nthe null-hypothesis of no e\ufb00ect, and a log-likelihood \u2113(tj) under the alterna-\ntive. Then a likelihood ratio test amounts to rejecting the null-hypothesis\nif\n\u2113(tj)\u2212\u21130(tj)>\u2206, (18.48)\nfor some \u2206. Depending on the likelihoods, and particularly their relative\nvalues, this can result in a di\ufb00erent threshold for tjthan for \u2212tj. The SAM\nprocedure rejects the null-hypothesis if\n|t(j)\u2212\u02dct(j)|>\u2206 (18.49)\nAgain, the threshold for each t(j)depends on the corresponding value of\nthe null value \u02dct(j).\n18.7.3 A Bayesian Interpretation of the FDR\nThere is an interesting Bayesian view of the FDR, developed in Storey\n(2002) and Efron and Tibshirani (2002). First we need to de\ufb01ne the positive\nfalse discovery rate (pFDR) as\npFDR = E/bracketleftbiggV\nR/vextendsingle/vextendsingle/vextendsingle/vextendsingleR >0/bracketrightbigg\n. (18.50)\nThe additional term positive refers to the fact that we are only interested\nin estimating an error rate where positive \ufb01ndings have occurred. It is\nthis slightly modi\ufb01ed version of the FDR that has a clean Bayesian inter-\npretation. Note that the usual FDR [expression (18.43)] is not de\ufb01ned if\nPr(R= 0)>0.\nLet \u0393 be a rejection region for a single test; in the example above we used\n\u0393 = (\u2212\u221e,\u22124.10)\u222a(4.10,\u221e). Suppose that Midentical simple hypothe-\nsis tests are performed with the i.i.d. statistics t1,... ,t Mand rejection\nregion \u0393. We de\ufb01ne a random variable Zjwhich equals 0 if the jth null\nhypothesis is true, and 1 otherwise. We assume that each pair ( tj,Zj) are\ni.i.d random variables with\ntj|Zj\u223c(1\u2212Zj)\u2264F0+Zj\u2264F1 (18.51)\nfor some distributions F0andF1. This says that each test statistic tjcomes\nfrom one of two distributions: F0if the null hypothesis is true, and F1\notherwise. Letting Pr( Zj= 0) = \u03c00, marginally we have:\ntj\u223c\u03c00\u2264F0+ (1\u2212\u03c00)\u2264F1. (18.52)\nThen it can be shown (Efron et al., 2001; Storey, 2002) that", "711": "18.8 Bibliographic Notes 693\npFDR(\u0393) = Pr( Zj= 0|tj\u2208\u0393). (18.53)\nHence under the mixture model (18.51), the pFDR is the posterior proba-\nbility that the null hypothesis it true, given that test statistic falls i n the\nrejection region for the test; that is, given that we reject the null hypothesis\n(Exercise 18.20).\nThe false discovery rate provides a measure of accuracy for tests based\non an entire rejection region, such as |tj| \u22652. But if the FDR of such a test\nis say 10%, then a gene with say tj= 5 will be more signi\ufb01cant than a gene\nwithtj= 2. Thus it is of interest to derive a local (gene-speci\ufb01c) version\nof the FDR. The q-value (Storey, 2003) of a test statistic tjis de\ufb01ned to\nbe the smallest FDR over all rejection regions that reject tj. That is, for\nsymmetric rejection regions, the q-value for tj= 2 is de\ufb01ned to be the\nFDR for the rejection region \u0393 = {\u2212(\u221e,\u22122)\u222a(2,\u221e)}. Thus the q-value\nfortj= 5 will be smaller than that for tj= 2, re\ufb02ecting the fact that tj= 5\nis more signi\ufb01cant than tj= 2. The local false discovery rate (Efron and\nTibshirani, 2002) at t=t0is de\ufb01ned to be\nPr(Zj= 0|tj=t0). (18.54)\nThis is the (positive) FDR for an in\ufb01nitesimal rejection region surrounding\nthe value tj=t0.\n18.8 Bibliographic Notes\nMany references were given at speci\ufb01c points in this chapter; we give some\nadditional ones here. Dudoit et al. (2002a) give an overview and compar-\nison of discrimination methods for gene expression data. Levina (2002)\ndoes some mathematical analysis comparing diagonal LDA to full LDA, as\np,N\u2192 \u221e withp > N . She shows that with reasonable assumptions diago-\nnal LDA has a lower asymptotic error rate than full LDA. Tibshirani et al.\n(2001a) and Tibshirani et al. (2003) proposed the nearest shrunken-centroid\nclassi\ufb01er. Zhu and Hastie (2004) study regularized logistic regression. H igh-\ndimensional regression and the lasso are very active areas of research, and\nmany references are given in Section 3.8.5. The fused lasso was proposed\nby Tibshirani et al. (2005), while Zou and Hastie (2005) introduced the\nelastic net. Supervised principal components is discussed in Bair and Tib-\nshirani (2004) and Bair et al. (2006). For an introduction to the analys is\nof censored survival data, see Kalb\ufb02eisch and Prentice (1980).\nMicroarray technology has led to a \ufb02urry of statistical research: see for\nexample the books by Speed (2003), Parmigiani et al. (2003), Simon et al.\n(2004), and Lee (2004).\nThe false discovery rate was proposed by Benjamini and Hochberg (1995),\nand studied and generalized in subsequent papers by these authors and", "712": "694 18. High-Dimensional Problems: p\u226bN\nmany others. A partial list of papers on FDR may be found on Yoav Ben-\njamini\u2019s homepage. Some more recent papers include Efron and Tibshirani\n(2002), Storey (2002), Genovese and Wasserman (2004), Storey and Tib-\nshirani (2003) and Benjamini and Yekutieli (2005). Dudoit et al. (2002b)\nreview methods for identifying di\ufb00erentially expressed genes in microarray\nstudies.\nExercises\nEx. 18.1 For a coe\ufb03cient estimate \u02c6\u03b2j, let\u02c6\u03b2j/||\u02c6\u03b2j||2be the normalized ver-\nsion. Show that as \u03bb\u2192 \u221e, the normalized ridge-regression estimates con-\nverge to the renormalized partial-least-squares one-component estimates.\nEx. 18.2 Nearest shrunken centroids and the lasso. Consider a (naive Bayes)\nGaussian model for classi\ufb01cation in which the features j= 1,2,... ,p are\nassumed to be independent within each class k= 1,2,... ,K . With ob-\nservations i= 1,2,... ,N andCkequal to the set of indices of the Nk\nobservations in class k, we observe xij\u223cN(\u03b8j+\u03b8jk,\u03c32\nj) fori\u2208Ckwith/summationtextK\nk=1\u03b8jk= 0. Set \u02c6 \u03c32\nj=s2\nj, the pooled within-class variance for feature j,\nand consider the lasso-style minimization problem\nmin\n{\u03b8j,\u03b8jk}\uf8f1\n\uf8f2\n\uf8f31\n2p/summationdisplay\nj=1K/summationdisplay\nk=1/summationdisplay\ni\u2208Ck(xij\u2212\u03b8j\u2212\u03b8jk)2\ns2\nj+\u03bb/radicalbig\nNkp/summationdisplay\nj=1K/summationdisplay\nk=1|\u03b8jk|\nsj.\uf8fc\n\uf8fd\n\uf8fe(18.55)\nShow that the solution is equivalent to the nearest shrunken centroid es-\ntimator (18.5), with s0set to zero, and Mkequal to 1 /Nkinstead of\n1/Nk\u22121/Nas before.\nEx. 18.3 Show that the \ufb01tted coe\ufb03cients for the regularized multiclass\nlogistic regression problem (18.10) satisfy/summationtextK\nk=1\u02c6\u03b2kj= 0, j= 1,... ,p .\nWhat about the \u02c6\u03b2k0? Discuss issues with these constant parameters, and\nhow they can be resolved.\nEx. 18.4 Derive the computational formula (18.15) for ridge regression.\n[Hint: Use the \ufb01rst derivative of the penalized sum-of-squares criterion to\nshow that if \u03bb >0, then \u02c6\u03b2=XTsfor some s\u2208IRN.]\nEx. 18.5 Prove the theorem (18.16)\u2013(18.17) in Section 18.3.5, by decom-\nposing \u03b2and the rows of Xinto their projections into the column space of\nVand its complement in IRp.\nEx. 18.6 Show how the theorem in Section 18.3.5 can be applied to regu-\nlarized discriminant analysis [Section 4.14 and Equation (18.9)].", "713": "Exercises 695\nEx. 18.7 Consider a linear regression problem where p\u226bN, and assume\nthe rank of XisN. Let the SVD of X=UDVT=RVT, where Ris\nN\u00d7Nnonsingular, and Visp\u00d7Nwith orthonormal columns.\n(a) Show that there are in\ufb01nitely many least-squares solutions all with\nzero residuals.\n(b) Show that the ridge-regression estimate for \u03b2can be written\n\u02c6\u03b2\u03bb=V(RTR+\u03bbI)\u22121RTy (18.56)\n(c) Show that when \u03bb= 0, the solution \u02c6\u03b20=VD\u22121UTyhas residuals\nall equal to zero, and is unique in that it has the smallest Euclidean\nnorm amongst all zero-residual solutions.\nEx. 18.8 Data Piling . Exercise 4.2 shows that the two-class LDA solution\ncan be obtained by a linear regression of a binary response vector ycon-\nsisting of \u22121s and +1s. The prediction \u02c6\u03b2Txfor any xis (up to a scale and\nshift) the LDA score \u03b4(x). Suppose now that p\u226bN.\n(a) Consider the linear regression model f(x) =\u03b1+\u03b2Tx\ufb01t to a binary\nresponse Y\u2208 {\u22121,+1}. Using Exercise 18.7, show that there are\nin\ufb01nitely many directions de\ufb01ned by \u02c6\u03b2in IRponto which the data\nproject to exactly two points, one for each class. These are known as\ndata piling directions (Ahn and Marron, 2005).\n(b) Show that the distance between the projected points is 2 /||\u02c6\u03b2||, and\nhence these directions de\ufb01ne separating hyperplanes with that mar-\ngin.\n(c) Argue that there is a single maximal data piling direction for which\nthis distance is largest, and is de\ufb01ned by \u02c6\u03b20=VD\u22121UTy=X\u2212y,\nwhere X=UDVTis the SVD of X.\nEx. 18.9 Compare the data piling direction of Exercise 18.8 to the direction\nof the optimal separating hyperplane (Section 4.5.2) qualitatively. Which\nmakes the widest margin, and why? Use a small simulation to demonstrate\nthe di\ufb00erence.\nEx. 18.10 When p\u226bN, linear discriminant analysis (see Section 4.3) is\ndegenerate because the within-class covariance matrix Wis singular. One\nversion of regularized discriminant analysis (4.14) replaces Wby a ridged\nversion W+\u03bbI, leading to a regularized discriminant function \u03b4\u03bb(x) =\nxT(W+\u03bbI)\u22121(\u00afx1\u2212\u00afx\u22121). Show that \u03b40(x) = lim \u03bb\u21930\u03b4\u03bb(x) corresponds to\nthe maximal data piling direction de\ufb01ned in Exercise 18.8.\nEx. 18.11 Suppose you have a sample of Npairs ( xi,yi), with yibinary\nandxi\u2208IR1. Suppose also that the two classes are separable; e.g., for each", "714": "696 18. High-Dimensional Problems: p\u226bN\npairi,i\u2032withyi= 0 and yi\u2032= 1,xi\u2032\u2212xi\u2265Cfor some C >0. You wish\nto \ufb01t a linear logistic regression model logitPr( Y= 1|X) =\u03b1+\u03b2Xby\nmaximum-likelihood. Show that \u02c6\u03b2is unde\ufb01ned.\nEx. 18.12 Suppose we wish to select the ridge parameter \u03bbby 10-fold cross-\nvalidation in a p\u226bNsituation (for any linear model). We wish to use the\ncomputational shortcuts described in Section 18.3.5. Show that we need\nonly to reduce the N\u00d7pmatrix Xto the N\u00d7Nmatrix Ronce, and can\nuse it in all the cross-validation runs.\nEx. 18.13 Suppose our p > N predictors are presented as an N\u00d7Ninner-\nproduct matrix K=XXT, and we wish to \ufb01t the equivalent of a linear\nlogistic regression model in the original features with quadratic regular iza-\ntion. Our predictions are also to be made using inner products; a new x0\nis presented as k0=Xx0. LetK=UD2UTbe the eigen-decomposition of\nK. Show that the predictions are given by \u02c6f0=kT\n0\u02c6\u03b1, where\n(a) \u02c6\u03b1=UD\u22121\u02c6\u03b2, and\n(b)\u02c6\u03b2is the ridged logistic regression estimate with input matrix R=\nUD.\nArgue that the same approach can be used for any appropriate kernel\nmatrix K.\nEx. 18.14 Distance weighted 1-NN classi\ufb01cation . Consider the 1-nearest-\nneighbor method (Section 13.3) in a two-class classi\ufb01cation problem. Let\nd+(x0) be the shortest distance to a training observation in class +1, and\nlikewise d\u2212(x0) the shortest distance for class \u22121. Let N\u2212be the number\nof samples in class \u22121,N+the number in class +1, and N=N\u2212+N+.\n(a) Show that\n\u03b4(x0) = logd\u2212(x0)\nd+(x0)(18.57)\ncan be viewed as a nonparametric discriminant function correspond-\ning to 1-NN classi\ufb01cation. [ Hint: Show that \u02c6f+(x0) =1\nN+d+(x0)can\nbe viewed as a nonparametric estimate of the density in class +1 at\nx0].\n(b) How would you modify this function to introduce class prior probabil-\nities\u03c0+and\u03c0\u2212di\ufb00erent from the sample-priors N+/NandN\u2212/N?\n(c) How would you generalize this approach for K-NN classi\ufb01cation?\nEx. 18.15 Kernel PCA. In Section 18.5.2 we show how to compute the\nprincipal component variables Zfrom an uncentered inner-product matrix\nK. We compute the eigen-decomposition ( I\u2212M)K(I\u2212M) =UD2UT,\nwithM=11T/N, and then Z=UD. Suppose we have the inner-product", "715": "Exercises 697\nvector k0, containing the Ninner-products between a new point x0and\neach of the xiin our training set. Show that the (centered) projections of\nx0onto the principal-component directions are given by\nz0=D\u22121UT(I\u2212M)[k0\u2212K1/N]. (18.58)\nEx. 18.16 Bonferroni method for multiple comparisons. Suppose we are in\na multiple-testing scenario with null hypotheses H0j,j= 1,2,... ,M , and\ncorresponding p-values pj,i= 1,2,... ,M . LetAbe the event that at least\none null hypothesis is falsely rejected, and let Ajbe the event that the\njth null hypothesis is falsely rejected. Suppose that we use the Bonferroni\nmethod, rejecting the jth null hypothesis if pj< \u03b1/M .\n(a) Show that Pr( A)\u2264\u03b1. [Hint: Pr(Aj\u222aAj\u2032) = Pr( Aj) + Pr( Aj\u2032)\u2212\nPr(Aj\u2229Aj\u2032)]\n(b) If the hypotheses H0j,j= 1,2,... ,M , are independent, then Pr( A) =\n1\u2212Pr(AC) = 1\u2212/producttextM\nj=1Pr(AC\nj) = 1\u2212(1\u2212\u03b1/M)M. Use this to show\nthat Pr( A)\u2248\u03b1in this case.\nEx. 18.17 Equivalence between Benjamini\u2013Hochberg and plug-in method s.\n(a) In the notation of Algorithm 18.2, show that for rejection threshold\np0=p(L), a proportion of at most p0of the permuted values tk\nj\nexceed |T|(L)where |T|(L)is the Lth largest value among the |tj|.\nHence show that the plug-in FDR estimate /hatwideFDR is less than or equal\ntop0\u2264M/L=\u03b1.\n(b) Show that the cut-point |T|(L+1)produces a test with estimated FDR\ngreater than \u03b1.\nEx. 18.18 Use result (18.53) to show that\npFDR =\u03c00\u2264 {Type I error of \u0393 }\n\u03c00\u2264 {Type I error of \u0393 }+\u03c01{Power of \u0393 }(18.59)\n(Storey, 2003).\nEx. 18.19 Consider the data in Table 18.4 of Section (18.7), available from\nthe book website.\n(a) Using a symmetric two-sided rejection region based on the t-statistic,\ncompute the plug-in estimate of the FDR for various values of the\ncut-point.\n(b) Carry out the BH procedure for various FDR levels \u03b1and show the\nequivalence of your results, with those from part (a).", "716": "698 18. High-Dimensional Problems: p\u226bN\n(c) Let ( q.25,q.75) be the quartiles of the t-statistics from the permuted\ndatasets. Let \u02c6 \u03c00={#tj\u2208(q.25,q.75)}/(.5M), and set \u02c6 \u03c00= min(\u02c6 \u03c00,1).\nMultiply the FDR estimates from (a) by \u02c6 \u03c00and examine the results.\n(d) Give a motivation for the estimate in part (c).\n(Storey, 2003)\nEx. 18.20 Proof of result 18.20 . Write\npFDR = E/parenleft\uf8ecigg\nV\nR|R >0/parenright\uf8ecigg\n(18.60)\n=M/summationdisplay\nk=1E/bracketleft\uf8ecigg\nV\nR|R=k/bracketright\uf8ecigg\nPr(R=k|R >0) (18.61)\nUse the fact that given R=k,Vis a binomial random variable, with k\ntrials and probability of success Pr( H= 0|T\u2208\u0393), to complete the proof.", "717": "This is page 699\nPrinter: Opaque this\nReferences\nAbu-Mostafa, Y. (1995). Hints, Neural Computation 7: 639\u2013671.\nAckley, D. H., Hinton, G. and Sejnowski, T. (1985). A learning algorithm\nfor Boltzmann machines, Trends in Cognitive Sciences 9: 147\u2013169.\nAdam, B.-L., Qu, Y., Davis, J. W., Ward, M. D., Clements, M. A.,\nCazares, L. H., Semmes, O. J., Schellhammer, P. F., Yasui, Y.,\nFeng, Z. and Wright, G. (2003). Serum protein \ufb01ngerprinting cou-\npled with a pattern-matching algorithm distinguishes prostate cancer\nfrom benign prostate hyperplasia and healthy mean, Cancer Research\n63(10): 3609\u20133614.\nAgrawal, R., Mannila, H., Srikant, R., Toivonen, H. and Verkamo, A. I.\n(1995). Fast discovery of association rules, Advances in Knowledge\nDiscovery and Data Mining , AAAI/MIT Press, Cambridge, MA.\nAgresti, A. (1996). An Introduction to Categorical Data Analysis , Wiley,\nNew York.\nAgresti, A. (2002). Categorical Data Analysis (2nd Ed.) , Wiley, New York.\nAhn, J. and Marron, J. (2005). The direction of maximal data piling in high\ndimensional space, Technical report , Statistics Department, University\nof North Carolina, Chapel Hill.\nAkaike, H. (1973). Information theory and an extension of the maximum\nlikelihood principle, Second International Symposium on Information\nTheory , pp. 267\u2013281.", "718": "700 References\nAllen, D. (1974). The relationship between variable selection and data\naugmentation and a method of prediction, Technometrics 16: 125\u20137.\nAmbroise, C. and McLachlan, G. (2002). Selection bias in gene extraction\non the basis of microarray gene-expression data, Proceedings of the\nNational Academy of Sciences 99: 6562\u20136566.\nAmit, Y. and Geman, D. (1997). Shape quantization and recognition with\nrandomized trees, Neural Computation 9: 1545\u20131588.\nAnderson, J. and Rosenfeld, E. (eds) (1988). Neurocomputing: Foundations\nof Research , MIT Press, Cambridge, MA.\nAnderson, T. (2003). An Introduction to Multivariate Statistical Analysis,\n3rd ed. , Wiley, New York.\nBach, F. and Jordan, M. (2002). Kernel independent component analysis,\nJournal of Machine Learning Research 3: 1\u201348.\nBair, E. and Tibshirani, R. (2004). Semi-supervised methods to predict\npatient survival from gene expression data, PLOS Biology 2: 511\u2013522.\nBair, E., Hastie, T., Paul, D. and Tibshirani, R. (2006). Prediction by\nsupervised principal components, Journal of the American Statistical\nAssociation 101: 119\u2013137.\nBakin, S. (1999). Adaptive regression and model selection in data mining\nproblems, Technical report , PhD. thesis, Australian National Univer-\nsity, Canberra.\nBanerjee, O., Ghaoui, L. E. and d\u2019Aspremont, A. (2008). Model selection\nthrough sparse maximum likelihood estimation for multivariate gaus-\nsian or binary data, Journal of Machine Learning Research 9: 485\u2013516.\nBarron, A. (1993). Universal approximation bounds for superpositions o f a\nsigmoid function, IEEE Transactions on Information Theory 39: 930\u2013\n945.\nBartlett, P. and Traskin, M. (2007). Adaboost is consistent, in\nB. Sch\u00a8 olkopf, J. Platt and T. Ho\ufb00man (eds), Advances in Neural Infor-\nmation Processing Systems 19 , MIT Press, Cambridge, MA, pp. 105\u2013\n112.\nBecker, R., Cleveland, W. and Shyu, M. (1996). The visual design and con-\ntrol of trellis display, Journal of Computational and Graphical Statis-\ntics5: 123\u2013155.\nBell, A. and Sejnowski, T. (1995). An information-maximization approac h\nto blind separation and blind deconvolution, Neural Computation\n7: 1129\u20131159.", "719": "References 701\nBellman, R. E. (1961). Adaptive Control Processes , Princeton University\nPress.\nBenjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery\nrate: a practical and powerful approach to multiple testing, Journal of\nthe Royal Statistical Society Series B. 85: 289\u2013300.\nBenjamini, Y. and Yekutieli, Y. (2005). False discovery rate controlling\ncon\ufb01dence intervals for selected parameters, Journal of the American\nStatistical Association 100: 71\u201380.\nBickel, P. and Levina, E. (2004). Some theory for Fisher\u2019s linear discrim-\ninant function,\u201cNaive Bayes\u201d, and some alternatives when there are\nmany more variables than observations, Bernoulli 10: 989\u20131010.\nBickel, P. J., Ritov, Y. and Tsybakov, A. (2008). Simultaneous analysi s of\nlasso and Dantzig selector, Annals of Statistics . to appear.\nBishop, C. (1995). Neural Networks for Pattern Recognition , Clarendon\nPress, Oxford.\nBishop, C. (2006). Pattern Recognition and Machine Learning , Springer,\nNew York.\nBishop, Y., Fienberg, S. and Holland, P. (1975). Discrete Multivariate\nAnalysis , MIT Press, Cambridge, MA.\nBoyd, S. and Vandenberghe, L. (2004). Convex Optimization , Cambridge\nUniversity Press.\nBreiman, L. (1992). The little bootstrap and other methods for dimension-\nality selection in regression: X-\ufb01xed prediction error, Journal of the\nAmerican Statistical Association 87: 738\u2013754.\nBreiman, L. (1996a). Bagging predictors, Machine Learning 26: 123\u2013140.\nBreiman, L. (1996b). Stacked regressions, Machine Learning 24: 51\u201364.\nBreiman, L. (1998). Arcing classi\ufb01ers (with discussion), Annals of Statistics\n26: 801\u2013849.\nBreiman, L. (1999). Prediction games and arcing algorithms, Neural Com-\nputation 11(7): 1493\u20131517.\nBreiman, L. (2001). Random forests, Machine Learning 45: 5\u201332.\nBreiman, L. and Friedman, J. (1997). Predicting multivariate responses\nin multiple linear regression (with discussion), Journal of the Royal\nStatistical Society Series B. 59: 3\u201337.", "720": "702 References\nBreiman, L. and Ihaka, R. (1984). Nonlinear discriminant analysis via\nscaling and ACE, Technical report , University of California, Berkeley.\nBreiman, L. and Spector, P. (1992). Submodel selection and evaluation\nin regression: the X-random case, International Statistical Review\n60: 291\u2013319.\nBreiman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classi\ufb01cation\nand Regression Trees , Wadsworth, New York.\nBremaud, P. (1999). Markov Chains: Gibbs Fields, Monte Carlo Simula-\ntion, and Queues , Springer, New York.\nBrown, P., Spiegelman, C. and Denham, M. (1991). Chemometrics and\nspectral frequency selection, Transactions of the Royal Society of Lon-\ndon Series A. 337: 311\u2013322.\nBruce, A. and Gao, H. (1996). Applied Wavelet Analysis with S-PLUS ,\nSpringer, New York.\nB\u00a8 uhlmann, P. and Hothorn, T. (2007). Boosting algorithms: regulariza-\ntion, prediction and model \ufb01tting (with discussion), Statistical Science\n22(4): 477\u2013505.\nBuja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoothers and\nadditive models (with discussion), Annals of Statistics 17: 453\u2013555.\nBuja, A., Swayne, D., Littman, M., Hofmann, H. and Chen, L. (2008). Data\nvizualization with multidimensional scaling, Journal of Computational\nand Graphical Statistics . to appear.\nBunea, F., Tsybakov, A. and Wegkamp, M. (2007). Sparsity oracle inequal-\nities for the lasso, Electronic Journal of Statistics 1: 169\u2013194.\nBurges, C. (1998). A tutorial on support vector machines for pattern recog-\nnition, Knowledge Discovery and Data Mining 2(2): 121\u2013167.\nButte, A., Tamayo, P., Slonim, D., Golub, T. and Kohane, I. (2000).\nDiscovering functional relationships between RNA expression and\nchemotherapeutic susceptibility using relevance networks, Proceedings\nof the National Academy of Sciences pp. 12182\u201312186.\nCandes, E. (2006). Compressive sampling, Proceedings of the Interna-\ntional Congress of Mathematicians , European Mathematical Society,\nMadrid, Spain.\nCandes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimati on\nwhen p is much larger than n, Annals of Statistics 35(6): 2313\u20132351.", "721": "References 703\nChambers, J. and Hastie, T. (1991). Statistical Models in S ,\nWadsworth/Brooks Cole, Paci\ufb01c Grove, CA.\nChaudhuri, S., Drton, M. and Richardson, T. S. (2007). Estimation of a\ncovariance matrix with zeros, Biometrika 94(1): 1\u201318.\nChen, L. and Buja, A. (2008). Local multidimensional scaling for nonlinear\ndimension reduction, graph drawing and proximity analysis, Journal\nof the American Statistical Association .\nChen, S. S., Donoho, D. and Saunders, M. (1998). Atomic decomposition\nby basis pursuit, SIAM Journal on Scienti\ufb01c Computing 20(1): 33\u201361.\nCherkassky, V. and Ma, Y. (2003). Comparison of model selection for\nregression, Neural computation 15(7): 1691\u20131714.\nCherkassky, V. and Mulier, F. (2007). Learning from Data (2nd Edition) ,\nWiley, New York.\nChui, C. (1992). An Introduction to Wavelets , Academic Press, London.\nCli\ufb00ord, P. (1990). Markov random \ufb01elds in statistics, inG. R. Grimmett\nand D. J. A. Welsh (eds), Disorder in Physical Systems. A Volume in\nHonour of John M. Hammersley , Clarendon Press, Oxford, pp. 19\u201332.\nComon, P. (1994). Independent component analysis\u2014a new concept?, Sig-\nnal Processing 36: 287\u2013314.\nCook, D. and Swayne, D. (2007). Interactive and Dynamic Graphics for\nData Analysis; with R and GGobi , Springer, New York. With con-\ntributions from A. Buja, D. Temple Lang, H. Hofmann, H. Wickham\nand M. Lawrence.\nCook, N. (2007). Use and misuse of the receiver operating characteristic\ncurve in risk prediction, Circulation 116(6): 928\u201335.\nCopas, J. B. (1983). Regression, prediction and shrinkage (with discus-\nsion),Journal of the Royal Statistical Society, Series B, Methodo logical\n45: 311\u2013354.\nCover, T. and Hart, P. (1967). Nearest neighbor pattern classi\ufb01cation,\nIEEE Transactions on Information Theory IT-11 : 21\u201327.\nCover, T. and Thomas, J. (1991). Elements of Information Theory , Wiley,\nNew York.\nCox, D. and Hinkley, D. (1974). Theoretical Statistics , Chapman and Hall,\nLondon.", "722": "704 References\nCox, D. and Wermuth, N. (1996). Multivariate Dependencies: Models,\nAnalysis and Interpretation , Chapman and Hall, London.\nCressie, N. (1993). Statistics for Spatial Data (Revised Edition) , Wiley-\nInterscience, New York.\nCsiszar, I. and Tusn\u00b4 ady, G. (1984). Information geometry and alternat-\ning minimization procedures, Statistics & Decisions Supplement Issue\n1: 205\u2013237.\nCutler, A. and Breiman, L. (1994). Archetypal analysis, Technometrics\n36(4): 338\u2013347.\nDasarathy, B. (1991). Nearest Neighbor Pattern Classi\ufb01cation Techniques ,\nIEEE Computer Society Press, Los Alamitos, CA.\nDaubechies, I. (1992). Ten Lectures in Wavelets , Society for Industrial and\nApplied Mathematics, Philadelphia, PA.\nDaubechies, I., Defrise, M. and De Mol, C. (2004). An iterative threshold-\ning algorithm for linear inverse problems with a sparsity constraint,\nCommunications on Pure and Applied Mathematics 57: 1413\u20131457.\nde Boor, C. (1978). A Practical Guide to Splines , Springer, New York.\nDempster, A. (1972). Covariance selection, Biometrics 28: 157\u2013175.\nDempster, A., Laird, N. and Rubin, D. (1977). Maximum likelihood from\nincomplete data via the EM algorithm (with discussion), Journal of\nthe Royal Statistical Society Series B 39: 1\u201338.\nDevijver, P. and Kittler, J. (1982). Pattern Recognition: A Statistical Ap-\nproach , Prentice-Hall, Englewood Cli\ufb00s, N.J.\nDietterich, T. (2000a). Ensemble methods in machine learning, Lecture\nNotes in Computer Science 1857: 1\u201315.\nDietterich, T. (2000b). An experimental comparison of three methods for\nconstructing ensembles of decision trees: bagging, boosting, and ran-\ndomization, Machine Learning 40(2): 139\u2013157.\nDietterich, T. and Bakiri, G. (1995). Solving multiclass learning problems\nvia error-correcting output codes, Journal of Arti\ufb01cial Intelligence Re-\nsearch 2: 263\u2013286.\nDonath, W. E. and Ho\ufb00man, A. J. (1973). Lower bounds for the partition-\ning of graphs, IBM Journal of Research and Development pp. 420\u2013425.\nDonoho, D. (2006a). Compressed sensing, IEEE Transactions on Informa-\ntion Theory 52(4): 1289\u20131306.", "723": "References 705\nDonoho, D. (2006b). For most large underdetermined systems of equations,\nthe minimal \u21131-norm solution is the sparsest solution, Communications\non Pure and Applied Mathematics 59: 797\u2013829.\nDonoho, D. and Elad, M. (2003). Optimally sparse representation from\novercomplete dictionaries via \u21131-norm minimization, Proceedings of\nthe National Academy of Sciences 100: 2197\u20132202.\nDonoho, D. and Johnstone, I. (1994). Ideal spatial adaptation by wavelet\nshrinkage, Biometrika 81: 425\u2013455.\nDonoho, D. and Stodden, V. (2004). When does non-negative matrix\nfactorization give a correct decomposition into parts?, inS. Thrun,\nL. Saul and B. Sch\u00a8 olkopf (eds), Advances in Neural Information Pro-\ncessing Systems 16 , MIT Press, Cambridge, MA.\nDuan, N. and Li, K.-C. (1991). Slicing regression: a link-free regression\nmethod, Annals of Statistics 19: 505\u2013530.\nDuchamp, T. and Stuetzle, W. (1996). Extremal properties of principal\ncurves in the plane, Annals of Statistics 24: 1511\u20131520.\nDuda, R., Hart, P. and Stork, D. (2000). Pattern Classi\ufb01cation (2nd Edi-\ntion), Wiley, New York.\nDudoit, S., Fridlyand, J. and Speed, T. (2002a). Comparison of discrimi-\nnation methods for the classi\ufb01cation of tumors using gene expression\ndata,Journal of the American Statistical Association 97(457): 77\u201387.\nDudoit, S., Yang, Y., Callow, M. and Speed, T. (2002b). Statistical meth-\nods for identifying di\ufb00erentially expressed genes in replicated cDNA\nmicroarray experiments, Statistica Sinica pp. 111\u2013139.\nEdwards, D. (2000). Introduction to Graphical Modelling, 2nd Edition ,\nSpringer, New York.\nEfron, B. (1975). The e\ufb03ciency of logistic regression compared to normal\ndiscriminant analysis, Journal of the American Statistical Association\n70: 892\u2013898.\nEfron, B. (1979). Bootstrap methods: another look at the jackknife, Annals\nof Statistics 7: 1\u201326.\nEfron, B. (1983). Estimating the error rate of a prediction rule: some\nimprovements on cross-validation, Journal of the American Statistical\nAssociation 78: 316\u2013331.\nEfron, B. (1986). How biased is the apparent error rate of a prediction\nrule?, Journal of the American Statistical Association 81: 461\u201370.", "724": "706 References\nEfron, B. and Tibshirani, R. (1991). Statistical analysis in the computer\nage,Science 253: 390\u2013395.\nEfron, B. and Tibshirani, R. (1993). An Introduction to the Bootstrap ,\nChapman and Hall, London.\nEfron, B. and Tibshirani, R. (1996). Using specially designed exponential\nfamilies for density estimation, Annals of Statistics 24(6): 2431\u20132461.\nEfron, B. and Tibshirani, R. (1997). Improvements on cross-validation: t he\n632+ bootstrap: method, Journal of the American Statistical Associ-\nation92: 548\u2013560.\nEfron, B. and Tibshirani, R. (2002). Microarrays, empirical Bayes methods ,\nand false discovery rates, Genetic Epidemiology 1: 70\u201386.\nEfron, B., Hastie, T. and Tibshirani, R. (2007). Discussion of \u201cDantzi g\nselector\u201d by Candes and Tao, Annals of Statistics 35(6): 2358\u20132364.\nEfron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angl e\nregression (with discussion), Annals of Statistics 32(2): 407\u2013499.\nEfron, B., Tibshirani, R., Storey, J. and Tusher, V. (2001). Empirical\nBayes analysis of a microarray experiment, Journal of the American\nStatistical Association 96: 1151\u20131160.\nEvgeniou, T., Pontil, M. and Poggio, T. (2000). Regularization networ ks\nand support vector machines, Advances in Computational Mathemat-\nics13(1): 1\u201350.\nFan, J. and Fan, Y. (2008). High dimensional classi\ufb01cation using features\nannealed independence rules, Annals of Statistics . to appear.\nFan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Appli-\ncations , Chapman and Hall, London.\nFan, J. and Li, R. (2005). Variable selection via nonconcave penalized\nlikelihood and its oracle properties, Journal of the American Statistical\nAssociation 96: 1348\u20131360.\nFiedler, M. (1973). Algebraic connectivity of graphs, Czechoslovak Mathe-\nmatics Journal 23(98): 298\u2013305.\nFienberg, S. (1977). The Analysis of Cross-Classi\ufb01ed Categorical Data ,\nMIT Press, Cambridge.\nFisher, R. A. (1936). The use of multiple measurements in taxonomic\nproblems, Eugen. 7: 179\u2013188.", "725": "References 707\nFisher, W. (1958). On grouping for maximum homogeniety, Journal of the\nAmerican Statistical Association 53(284): 789\u2013798.\nFix, E. and Hodges, J. (1951). Discriminatory analysis\u2014nonparametric\ndiscrimination: Consistency properties, Technical Report 21-49-004,4 ,\nU.S. Air Force, School of Aviation Medicine, Randolph Field, TX.\nFlury, B. (1990). Principal points, Biometrika 77: 33\u201341.\nForgy, E. (1965). Cluster analysis of multivariate data: e\ufb03ciency vs. i nter-\npretability of classi\ufb01cations, Biometrics 21: 768\u2013769.\nFrank, I. and Friedman, J. (1993). A statistical view of some chemometri cs\nregression tools (with discussion), Technometrics 35(2): 109\u2013148.\nFreund, Y. (1995). Boosting a weak learning algorithm by majority, Infor-\nmation and Computation 121(2): 256\u2013285.\nFreund, Y. and Schapire, R. (1996a). Experiments with a new boosting\nalgorithm, Machine Learning: Proceedings of the Thirteenth Interna-\ntional Conference , Morgan Kau\ufb00man, San Francisco, pp. 148\u2013156.\nFreund, Y. and Schapire, R. (1996b). Game theory, on-line prediction and\nboosting, Proceedings of the Ninth Annual Conference on Computa-\ntional Learning Theory , Desenzano del Garda, Italy, pp. 325\u2013332.\nFreund, Y. and Schapire, R. (1997). A decision-theoretic generalization of\nonline learning and an application to boosting, Journal of Computer\nand System Sciences 55: 119\u2013139.\nFriedman, J. (1987). Exploratory projection pursuit, Journal of the Amer-\nican Statistical Association 82: 249\u2013266.\nFriedman, J. (1989). Regularized discriminant analysis, Journal of the\nAmerican Statistical Association 84: 165\u2013175.\nFriedman, J. (1991). Multivariate adaptive regression splines (with dis cus-\nsion), Annals of Statistics 19(1): 1\u2013141.\nFriedman, J. (1994a). Flexible metric nearest-neighbor classi\ufb01cation, Tech-\nnical report , Stanford University.\nFriedman, J. (1994b). An overview of predictive learning and function\napproximation, inV. Cherkassky, J. Friedman and H. Wechsler (eds),\nFrom Statistics to Neural Networks , Vol. 136 of NATO ISI Series F ,\nSpringer, New York.\nFriedman, J. (1996). Another approach to polychotomous classi\ufb01cation,\nTechnical report , Stanford University.", "726": "708 References\nFriedman, J. (1997). On bias, variance, 0-1 loss and the curse of dimen-\nsionality, Journal of Data Mining and Knowledge Discovery 1: 55\u201377.\nFriedman, J. (1999). Stochastic gradient boosting, Technical report , Stan-\nford University.\nFriedman, J. (2001). Greedy function approximation: A gradient boosting\nmachine, Annals of Statistics 29(5): 1189\u20131232.\nFriedman, J. and Fisher, N. (1999). Bump hunting in high dimensional\ndata,Statistics and Computing 9: 123\u2013143.\nFriedman, J. and Hall, P. (2007). On bagging and nonlinear estimation,\nJournal of Statistical Planning and Inference 137: 669\u2013683.\nFriedman, J. and Popescu, B. (2003). Importance sampled learning ensem-\nbles,Technical report , Stanford University, Department of Statistics.\nFriedman, J. and Popescu, B. (2008). Predictive learning via rule ensem-\nbles,Annals of Applied Statistics, to appear .\nFriedman, J. and Silverman, B. (1989). Flexible parsimonious smoothing\nand additive modelling (with discussion), Technometrics 31: 3\u201339.\nFriedman, J. and Stuetzle, W. (1981). Projection pursuit regression, Jour-\nnal of the American Statistical Association 76: 817\u2013823.\nFriedman, J. and Tukey, J. (1974). A projection pursuit algorithm for\nexploratory data analysis, IEEE Transactions on Computers, Series\nC23: 881\u2013889.\nFriedman, J., Baskett, F. and Shustek, L. (1975). An algorithm for \ufb01nding\nnearest neighbors, IEEE Transactions on Computers 24: 1000\u20131006.\nFriedman, J., Bentley, J. and Finkel, R. (1977). An algorthm for \ufb01nd-\ning best matches in logarithmic expected time, ACM Transactions on\nMathematical Software 3: 209\u2013226.\nFriedman, J., Hastie, T. and Tibshirani, R. (2000). Additive logistic r e-\ngression: a statistical view of boosting (with discussion), Annals of\nStatistics 28: 337\u2013307.\nFriedman, J., Hastie, T. and Tibshirani, R. (2008a). Response to \u201cMease\nand Wyner: Evidence contrary to the statistical view of boosting\u201d,\nJournal of Machine Learning Research 9: 175\u2013180.\nFriedman, J., Hastie, T. and Tibshirani, R. (2008b). Sparse inverse covari -\nance estimation with the graphical lasso, Biostatistics 9: 432\u2013441.", "727": "References 709\nFriedman, J., Hastie, T. and Tibshirani, R. (2010). Regularization paths f or\ngeneralized linear models via coordinate descent, Journal of Statistical\nSoftware 33(1): 1\u201322.\nFriedman, J., Hastie, T., Hoe\ufb02ing, H. and Tibshirani, R. (2007). Pathwis e\ncoordinate optimization, Annals of Applied Statistics 2(1): 302\u2013332.\nFriedman, J., Hastie, T., Rosset, S., Tibshirani, R. and Zhu, J. (2004).\nDiscussion of three boosting papers by Jiang, Lugosi and Vayatis, and\nZhang, Annals of Statistics 32: 102\u2013107.\nFriedman, J., Stuetzle, W. and Schroeder, A. (1984). Projection pursuit\ndensity estimation, Journal of the American Statistical Association\n79: 599\u2013608.\nFu, W. (1998). Penalized regressions: the bridge vs. the lasso, Journal of\nComputational and Graphical Statistics 7(3): 397\u2013416.\nFurnival, G. and Wilson, R. (1974). Regression by leaps and bounds, Tech-\nnometrics 16: 499\u2013511.\nGelfand, A. and Smith, A. (1990). Sampling based approaches to calculat-\ning marginal densities, Journal of the American Statistical Association\n85: 398\u2013409.\nGelman, A., Carlin, J., Stern, H. and Rubin, D. (1995). Bayesian Data\nAnalysis , CRC Press, Boca Raton, FL.\nGeman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distribu-\ntions and the Bayesian restoration of images, IEEE Transactions on\nPattern Analysis and Machine Intelligence 6: 721\u2013741.\nGenkin, A., Lewis, D. and Madigan, D. (2007). Large-scale Bayesian logis -\ntic regression for text categorization, Technometrics 49(3): 291\u2013304.\nGenovese, C. and Wasserman, L. (2004). A stochastic process approach to\nfalse discovery rates, Annals of Statistics 32(3): 1035\u20131061.\nGersho, A. and Gray, R. (1992). Vector Quantization and Signal Compres-\nsion, Kluwer Academic Publishers, Boston, MA.\nGirosi, F., Jones, M. and Poggio, T. (1995). Regularization theory and\nneural network architectures, Neural Computation 7: 219\u2013269.\nGolub, G. and Van Loan, C. (1983). Matrix Computations , Johns Hopkins\nUniversity Press, Baltimore.\nGolub, G., Heath, M. and Wahba, G. (1979). Generalized cross-validation\nas a method for choosing a good ridge parameter, Technometrics\n21: 215\u2013224.", "728": "710 References\nGolub, T., Slonim, D., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov,\nJ., Coller, H., Loh, M., Downing, J., Caligiuri, M., Bloom\ufb01eld, C. and\nLander, E. (1999). Molecular classi\ufb01cation of cancer: Class discovery\nand class prediction by gene expression monitoring, Science 286: 531\u2013\n536.\nGoodall, C. (1991). Procrustes methods in the statistical analysis of s hape,\nJournal of the Royal Statistical Society, Series B 53: 285\u2013321.\nGordon, A. (1999). Classi\ufb01cation (2nd edition) , Chapman and Hall/CRC\nPress, London.\nGreen, P. and Silverman, B. (1994). Nonparametric Regression and Gener-\nalized Linear Models: A Roughness Penalty Approach , Chapman and\nHall, London.\nGreenacre, M. (1984). Theory and Applications of Correspondence Analy-\nsis, Academic Press, New York.\nGreenshtein, E. and Ritov, Y. (2004). Persistence in high-dimensional lin-\near predictor selection and the virtue of overparametrization, Bernoulli\n10: 971\u2013988.\nGuo, Y., Hastie, T. and Tibshirani, R. (2006). Regularized linear discrim-\ninant analysis and its application in microarrays, Biostatistics 8: 86\u2013\n100.\nGuyon, I., Gunn, S., Nikravesh, M. and Zadeh, L. (eds) (2006). Feature\nExtraction, Foundations and Applications , Springer, New York.\nGuyon, I., Weston, J., Barnhill, S. and Vapnik, V. (2002). Gene selection for\ncancer classi\ufb01cation using support vector machines, Machine Learning\n46: 389\u2013422.\nHall, P. (1992). The Bootstrap and Edgeworth Expansion , Springer, New\nYork.\nHammersley, J. M. and Cli\ufb00ord, P. (1971). Markov \ufb01eld on \ufb01nite graphs\nand lattices, unpublished.\nHand, D. (1981). Discrimination and Classi\ufb01cation , Wiley, Chichester.\nHanley, J. and McNeil, B. (1982). The meaning and use of the area under\na receiver operating characteristic (roc) curve, Radiology 143: 29\u201336.\nHart, P. (1968). The condensed nearest-neighbor rule, IEEE Transactions\non Information Theory 14: 515\u2013516.\nHartigan, J. A. (1975). Clustering Algorithms , Wiley, New York.", "729": "References 711\nHartigan, J. A. and Wong, M. A. (1979). [(Algorithm AS 136] A k-means\nclustering algorithm (AS R39: 81v30 p355-356), Applied Statistics\n28: 100\u2013108.\nHastie, T. (1984). Principal Curves and Surfaces , PhD thesis, Stanford\nUniversity.\nHastie, T. and Herman, A. (1990). An analysis of gestational age, neona-\ntal size and neonatal death using nonparametric logistic regression,\nJournal of Clinical Epidemiology 43: 1179\u201390.\nHastie, T. and Simard, P. (1998). Models and metrics for handwritten digit\nrecognition, Statistical Science 13: 54\u201365.\nHastie, T. and Stuetzle, W. (1989). Principal curves, Journal of the Amer-\nican Statistical Association 84(406): 502\u2013516.\nHastie, T. and Tibshirani, R. (1987). Nonparametric logistic and propo r-\ntional odds regression, Applied Statistics 36: 260\u2013276.\nHastie, T. and Tibshirani, R. (1990). Generalized Additive Models , Chap-\nman and Hall, London.\nHastie, T. and Tibshirani, R. (1996a). Discriminant adaptive nearest-\nneighbor classi\ufb01cation, IEEE Pattern Recognition and Machine In-\ntelligence 18: 607\u2013616.\nHastie, T. and Tibshirani, R. (1996b). Discriminant analysis by Gaussi an\nmixtures, Journal of the Royal Statistical Society Series B. 58: 155\u2013\n176.\nHastie, T. and Tibshirani, R. (1998). Classi\ufb01cation by pairwise coupling ,\nAnnals of Statistics 26(2): 451\u2013471.\nHastie, T. and Tibshirani, R. (2003). Independent components analysis\nthrough product density estimation, inS. T. S. Becker and K. Ober-\nmayer (eds), Advances in Neural Information Processing Systems 15 ,\nMIT Press, Cambridge, MA, pp. 649\u2013656.\nHastie, T. and Tibshirani, R. (2004). E\ufb03cient quadratic regularization f or\nexpression arrays, Biostatistics 5(3): 329\u2013340.\nHastie, T. and Zhu, J. (2006). Discussion of \u201cSupport vector machines\nwith applications\u201d by Javier Moguerza and Alberto Munoz, Statistical\nScience 21(3): 352\u2013357.\nHastie, T., Botha, J. and Schnitzler, C. (1989). Regression with an ordered\ncategorical response, Statistics in Medicine 43: 884\u2013889.", "730": "712 References\nHastie, T., Buja, A. and Tibshirani, R. (1995). Penalized discriminant\nanalysis, Annals of Statistics 23: 73\u2013102.\nHastie, T., Kishon, E., Clark, M. and Fan, J. (1992). A model for\nsignature veri\ufb01cation, Technical report , AT&T Bell Laboratories.\nhttp://www-stat.stanford.edu/ \u223chastie/Papers/signature.pdf .\nHastie, T., Rosset, S., Tibshirani, R. and Zhu, J. (2004). The entire reg-\nularization path for the support vector machine, Journal of Machine\nLearning Research 5: 1391\u20131415.\nHastie, T., Taylor, J., Tibshirani, R. and Walther, G. (2007). Forwa rd\nstagewise regression and the monotone lasso, Electronic Journal of\nStatistics 1: 1\u201329.\nHastie, T., Tibshirani, R. and Buja, A. (1994). Flexible discriminant ana ly-\nsis by optimal scoring, Journal of the American Statistical Association\n89: 1255\u20131270.\nHastie, T., Tibshirani, R. and Buja, A. (1998). Flexible discriminant and\nmixture models, inJ. Kay and M. Titterington (eds), Statistics and\nArti\ufb01cial Neural Networks , Oxford University Press.\nHastie, T., Tibshirani, R. and Friedman, J. (2003). A note on \u201cCompari-\nson of model selection for regression\u201d by Cherkassky and Ma, Neural\ncomputation 15(7): 1477\u20131480.\nHathaway, R. J. (1986). Another interpretation of the EM algorithm for\nmixture distributions, Statistics & Probability Letters 4: 53\u201356.\nHebb, D. (1949). The Organization of Behavior , Wiley, New York.\nHertz, J., Krogh, A. and Palmer, R. (1991). Introduction to the Theory of\nNeural Computation , Addison Wesley, Redwood City, CA.\nHinton, G. (1989). Connectionist learning procedures, Arti\ufb01cial Intelli-\ngence40: 185\u2013234.\nHinton, G. (2002). Training products of experts by minimizing contrastive\ndivergence, Neural Computation 14: 1771\u20131800.\nHinton, G., Osindero, S. and Teh, Y.-W. (2006). A fast learning algorithm\nfor deep belief nets, Neural Computation 18: 1527\u20131554.\nHo, T. K. (1995). Random decision forests, inM. Kavavaugh and P. Storms\n(eds), Proc. Third International Conference on Document Analysis\nand Recognition , Vol. 1, IEEE Computer Society Press, New York,\npp. 278\u2013282.", "731": "References 713\nHoe\ufb02ing, H. and Tibshirani, R. (2008). Estimation of sparse Markov net-\nworks using modi\ufb01ed logistic regression and the lasso, submitted.\nHoerl, A. E. and Kennard, R. (1970). Ridge regression: biased estimation\nfor nonorthogonal problems, Technometrics 12: 55\u201367.\nHothorn, T. and B\u00a8 uhlmann, P. (2006). Model-based boosting in high di-\nmensions, Bioinformatics 22(22): 2828\u20132829.\nHuber, P. (1964). Robust estimation of a location parameter, Annals of\nMathematical Statistics 53: 73\u2013101.\nHuber, P. (1985). Projection pursuit, Annals of Statistics 13: 435\u2013475.\nHunter, D. and Lange, K. (2004). A tutorial on MM algorithms, The\nAmerican Statistician 58(1): 30\u201337.\nHyv\u00a8 arinen, A. and Oja, E. (2000). Independent component analysis: algo-\nrithms and applications, Neural Networks 13: 411\u2013430.\nHyv\u00a8 arinen, A., Karhunen, J. and Oja, E. (2001). Independent Component\nAnalysis , Wiley, New York.\nIzenman, A. (1975). Reduced-rank regression for the multivariate linear\nmodel, Journal of Multivariate Analysis 5: 248\u2013264.\nJacobs, R., Jordan, M., Nowlan, S. and Hinton, G. (1991). Adaptive mix-\ntures of local experts, Neural computation 3: 79\u201387.\nJain, A. and Dubes, R. (1988). Algorithms for Clustering Data , Prentice-\nHall, Englewood Cli\ufb00s, N.J.\nJames, G. and Hastie, T. (1998). The error coding method and PICTs,\nJournal of Computational and Graphical Statistics 7(3): 377\u2013387.\nJancey, R. (1966). Multidimensional group analysis, Australian Journal of\nBotany 14: 127\u2013130.\nJensen, F. V., Lauritzen, S. and Olesen, K. G. (1990). Bayesian updating\nin recursive graphical models by local computation, Computational\nStatistics Quarterly 4: 269\u2013282.\nJiang, W. (2004). Process consistency for Adaboost, Annals of Statistics\n32(1): 13\u201329.\nJirou\u00b4 sek, R. and P\u02c7 reu\u02c7 cil, S. (1995). On the e\ufb00ective implementation of t he\niterative proportional \ufb01tting procedure, Computational Statistics and\nData Analysis 19: 177\u2013189.\nJohnson, N. (2008). A study of the NIPS feature selection challenge, Sub-\nmitted.", "732": "714 References\nJoli\ufb00e, I. T., Trenda\ufb01lov, N. T., and Uddin, M. (2003). A modi\ufb01ed principal\ncomponent technique based on the lasso, Journal of Computational\nand Graphical Statistics 12: 531\u2013547.\nJones, L. (1992). A simple lemma on greedy approximation in Hilbert space\nand convergence rates for projection pursuit regression and neural\nnetwork training, Annals of Statistics 20: 608\u2013613.\nJordan, M. (2004). Graphical models, Statistical Science (Special Issue on\nBayesian Statistics) 19: 140\u2013155.\nJordan, M. and Jacobs, R. (1994). Hierachical mixtures of experts and the\nEM algorithm, Neural Computation 6: 181\u2013214.\nKalb\ufb02eisch, J. and Prentice, R. (1980). The Statistical Analysis of Failure\nTime Data , Wiley, New York.\nKaufman, L. and Rousseeuw, P. (1990). Finding Groups in Data: An In-\ntroduction to Cluster Analysis , Wiley, New York.\nKearns, M. and Vazirani, U. (1994). An Introduction to Computational\nLearning Theory , MIT Press, Cambridge, MA.\nKittler, J., Hatef, M., Duin, R. and Matas, J. (1998). On combining classi-\n\ufb01ers,IEEE Transaction on Pattern Analysis and Machine Intellige nce\n20(3): 226\u2013239.\nKleinberg, E. M. (1990). Stochastic discrimination, Annals of Mathematical\nArti\ufb01cial Intelligence 1: 207\u2013239.\nKleinberg, E. M. (1996). An overtraining-resistant stochastic modeling\nmethod for pattern recognition, Annals of Statistics 24: 2319\u20132349.\nKnight, K. and Fu, W. (2000). Asymptotics for lasso-type estimators,\nAnnals of Statistics 28(5): 1356\u20131378.\nKoh, K., Kim, S.-J. and Boyd, S. (2007). An interior-point method\nfor large-scale L1-regularized logistic regression, Journal of Machine\nLearning Research 8: 1519\u20131555.\nKohavi, R. (1995). A study of cross-validation and bootstrap for accu-\nracy estimation and model selection, International Joint Conference\non Arti\ufb01cial Intelligence (IJCAI) , Morgan Kaufmann, pp. 1137\u20131143.\nKohonen, T. (1989). Self-Organization and Associative Memory (3rd edi-\ntion), Springer, Berlin.\nKohonen, T. (1990). The self-organizing map, Proceedings of the IEEE\n78: 1464\u20131479.", "733": "References 715\nKohonen, T., Kaski, S., Lagus, K., Saloj\u00a8 arvi, J., Paatero, A. and Saarela,\nA. (2000). Self-organization of a massive document collection, IEEE\nTransactions on Neural Networks 11(3): 574\u2013585. Special Issue on\nNeural Networks for Data Mining and Knowledge Discovery.\nKoller, D. and Friedman, N. (2007). Structured Probabilistic Models , Stan-\nford Bookstore Custom Publishing. (Unpublished Draft).\nKressel, U. (1999). Pairwise classi\ufb01cation and support vector machines,\ninB. Sch\u00a8 olkopf, C. Burges and A. Smola (eds), Advances in Ker-\nnel Methods - Support Vector Learning , MIT Press, Cambridge, MA.,\npp. 255\u2013268.\nLambert, D. (1992). Zero-in\ufb02ated Poisson regression, with an applicatio n\nto defects in manufacturing, Technometrics 34(1): 1\u201314.\nLange, K. (2004). Optimization , Springer, New York.\nLauritzen, S. (1996). Graphical Models , Oxford University Press.\nLauritzen, S. and Spiegelhalter, D. (1988). Local computations with proba-\nbilities on graphical structures and their application to expert systems,\nJ. Royal Statistical Society B. 50: 157\u2013224.\nLawson, C. and Hansen, R. (1974). Solving Least Squares Problems ,\nPrentice-Hall, Englewood Cli\ufb00s, NJ.\nLe Cun, Y. (1989). Generalization and network design strategies, Techni-\ncal Report CRG-TR-89-4 , Department of Computer Science, Univ. of\nToronto.\nLe Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard,\nW. and Jackel, L. (1990). Handwritten digit recognition with a back-\npropogation network, inD. Touretzky (ed.), Advances in Neural In-\nformation Processing Systems , Vol. 2, Morgan Kaufman, Denver, CO,\npp. 386\u2013404.\nLe Cun, Y., Bottou, L., Bengio, Y. and Ha\ufb00ner, P. (1998). Gradient-based\nlearning applied to document recognition, Proceedings of the IEEE\n86(11): 2278\u20132324.\nLeathwick, J., Elith, J., Francis, M., Hastie, T. and Taylor, P. (20 06). Vari-\nation in demersal \ufb01sh species richness in the oceans surrounding new\nzealand: an analysis using boosted regression trees, Marine Ecology\nProgress Series 77: 802\u2013813.\nLeathwick, J., Rowe, D., Richardson, J., Elith, J. and Hastie, T. (200 5).\nUsing multivariate adaptive regression splines to predict the distribu-\ntions of New Zealand\u2019s freshwater diadromous \ufb01sh, Freshwater Biology\n50: 2034\u20132051.", "734": "716 References\nLeblanc, M. and Tibshirani, R. (1996). Combining estimates in regres-\nsion and classi\ufb01cation, Journal of the American Statistical Association\n91: 1641\u20131650.\nLeCun, Y., Bottou, L., Bengio, Y. and Ha\ufb00ner, P. (1998). Gradient-based\nlearning applied to document recognition, Proceedings of the IEEE\n86(11): 2278\u20132324.\nLee, D. and Seung, H. (1999). Learning the parts of objects by non-negative\nmatrix factorization, Nature 401: 788.\nLee, D. and Seung, H. (2001). Algorithms for non-negative matrix factor-\nization, Advances in Neural Information Processing Systems, (NIPS\n2001), Vol. 13, Morgan Kaufman, Denver., pp. 556\u2013562.\nLee, M.-L. (2004). Analysis of Microarray Gene Expression Data , Kluwer\nAcademic Publishers.\nLee, S.-I., Ganapathi, V. and Koller, D. (2007). E\ufb03cient structure learning\nof markov networks using l1-regularization, inB. Sch\u00a8 olkopf, J. Platt\nand T. Ho\ufb00man (eds), Advances in Neural Information Processing\nSystems 19 , MIT Press, Cambridge, MA, pp. 817\u2013824.\nLeslie, C., Eskin, E., Cohen, A., Weston, J. and Noble, W. S. (2004). Mis-\nmatch string kernels for discriminative protein classi\ufb01cation, Bioinfor-\nmatics 20(4): 467\u2013476.\nLevina, E. (2002). Statistical issues in texture analysis , PhD thesis, De-\npartment. of Statistics, University of California, Berkeley.\nLin, H., McCulloch, C., Turnbull, B., Slate, E. and Clark, L. (2000). A\nlatent class mixed model for analyzing biomarker trajectories in lon-\ngitudinal data with irregularly scheduled observations, Statistics in\nMedicine 19: 1303\u20131318.\nLin, Y. and Zhang, H. (2006). Component selection and smoothing in\nsmoothing spline analysis of variance models, Annals of Statistics\n34: 2272\u20132297.\nLittle, R. and Rubin, D. (2002). Statistical Analysis with Missing Data\n(2nd Edition) , Wiley, New York.\nLloyd, S. (1957). Least squares quantization in PCM., Technical report , Bell\nLaboratories. Published in 1982 in IEEE Transactions on Information\nTheory 28128-137.\nLoader, C. (1999). Local Regression and Likelihood , Springer, New York.", "735": "References 717\nLoh, W. and Vanichsetakul, N. (1988). Tree structured classi\ufb01cation via\ngeneralized discriminant analysis, Journal of the American Statistical\nAssociation 83: 715\u2013728.\nLugosi, G. and Vayatis, N. (2004). On the bayes-risk consistency of regu-\nlarized boosting methods, Annals of Statistics 32(1): 30\u201355.\nMacnaughton Smith, P., Williams, W., Dale, M. and Mockett, L. (1965).\nDissimilarity analysis: a new technique of hierarchical subdivision, Na-\nture202: 1034\u20131035.\nMacKay, D. (1992). A practical Bayesian framework for backpropagation\nneural networks, Neural Computation 4: 448\u2013472.\nMacQueen, J. (1967). Some methods for classi\ufb01cation and analysis of mul-\ntivariate observations, Proceedings of the Fifth Berkeley Symposium\non Mathematical Statistics and Probability, eds. L.M. LeCa m and J.\nNeyman , University of California Press, pp. 281\u2013297.\nMadigan, D. and Raftery, A. (1994). Model selection and accounting for\nmodel uncertainty using Occam\u2019s window, Journal of the American\nStatistical Association 89: 1535\u201346.\nMardia, K., Kent, J. and Bibby, J. (1979). Multivariate Analysis , Academic\nPress.\nMason, L., Baxter, J., Bartlett, P. and Frean, M. (2000). Boosting al go-\nrithms as gradient descent, 12: 512\u2013518.\nMassart, D., Plastria, F. and Kaufman, L. (1983). Non-hierarchical clus-\ntering with MASLOC, The Journal of the Pattern Recognition Society\n16: 507\u2013516.\nMcCullagh, P. and Nelder, J. (1989). Generalized Linear Models , Chapman\nand Hall, London.\nMcCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immi-\nnent in nervous activity, Bulletin of Mathematical Biophysics 5: 115\u2013\n133. Reprinted in Anderson and Rosenfeld (1988), pp 96-104.\nMcLachlan, G. (1992). Discriminant Analysis and Statistical Pattern\nRecognition , Wiley, New York.\nMease, D. and Wyner, A. (2008). Evidence contrary to the statistical view\nof boosting (with discussion), Journal of Machine Learning Research\n9: 131\u2013156.\nMeinshausen, N. (2007). Lasso with relaxation, Computational Statistics\nand Data Analysis 52(1): 374\u2013293.", "736": "718 References\nMeinshausen, N. and B\u00a8 uhlmann, P. (2006). High-dimensional graphs and\nvariable selection with the lasso, Annals of Statistics 34: 1436\u20131462.\nMeir, R. and R\u00a8 atsch, G. (2003). An introduction to boosting and leverag -\ning,inS. Mendelson and A. Smola (eds), Lecture notes in Computer\nScience , Advanced Lectures in Machine Learning, Springer, New York.\nMichie, D., Spiegelhalter, D. and Taylor, C. (eds) (1994). Machine Learn-\ning, Neural and Statistical Classi\ufb01cation , Ellis Horwood Series in Ar-\nti\ufb01cial Intelligence, Ellis Horwood.\nMorgan, J. N. and Sonquist, J. A. (1963). Problems in the analysis of surv ey\ndata, and a proposal, Journal of the American Statistical Association\n58: 415\u2013434.\nMurray, W., Gill, P., and Wright, M. (1981). Practical Optimization , Aca-\ndemic Press.\nMyles, J. and Hand, D. (1990). The multiclass metric problem in nearest\nneighbor classi\ufb01cation, Pattern Recognition 23: 1291\u20131297.\nNadler, B. and Coifman, R. R. (2005). An exact asymptotic formula for t he\nerror in CLS and in PLS: The importance of dimensional reduction in\nmultivariate calibration, Journal of Chemometrics 102: 107\u2013118.\nNeal, R. (1996). Bayesian Learning for Neural Networks , Springer, New\nYork.\nNeal, R. and Hinton, G. (1998). A view of the EM algorithm that justi\ufb01es\nincremental, sparse, and other variants; in Learning in Gra phical Mod-\nels, M. Jordan (ed.) , Dordrecht: Kluwer Academic Publishers, Boston,\nMA., pp. 355\u2013368.\nNeal, R. and Zhang, J. (2006). High dimensional classi\ufb01cation with\nbayesian neural networks and dirichlet di\ufb00usion trees, inI. Guyon,\nS. Gunn, M. Nikravesh and L. Zadeh (eds), Feature Extraction, Foun-\ndations and Applications , Springer, New York, pp. 265\u2013296.\nOnton, J. and Makeig, S. (2006). Information-based modeling of event-\nrelated brain dynamics, inNeuper and Klimesch (eds), Progress in\nBrain Research , Vol. 159, Elsevier, pp. 99\u2013120.\nOsborne, M., Presnell, B. and Turlach, B. (2000a). A new approach to\nvariable selection in least squares problems, IMA Journal of Numerical\nAnalysis 20: 389\u2013404.\nOsborne, M., Presnell, B. and Turlach, B. (2000b). On the lasso and its\ndual,Journal of Computational and Graphical Statistics 9: 319\u2013337.", "737": "References 719\nPace, R. K. and Barry, R. (1997). Sparse spatial autoregressions, Statistics\nand Probability Letters 33: 291\u2013297.\nPage, L., Brin, S., Motwani, R. and Winograd, T. (1998). The\npagerank citation ranking: bringing order to the web, Tech-\nnical report , Stanford Digital Library Technologies Project.\nhttp://citeseer.ist.psu.edu/page98pagerank.html .\nPark, M. Y. and Hastie, T. (2007). l1-regularization path algorithm for gen-\neralized linear models, Journal of the Royal Statistical Society Series\nB69: 659\u2013677.\nParker, D. (1985). Learning logic, Technical Report TR-87 , Cambridge MA:\nMIT Center for Research in Computational Economics and Manage-\nment Science.\nParmigiani, G., Garett, E. S., Irizarry, R. A. and Zeger, S. L. (eds) (200 3).\nThe Analysis of Gene Expression Data , Springer, New York.\nPaul, D., Bair, E., Hastie, T. and Tibshirani, R. (2008). \u201cPre-conditioni ng\u201d\nfor feature selection and regression in high-dimensional problems, An-\nnals of Statistics 36(4): 1595\u20131618.\nPearl, J. (1986). On evidential reasoning in a hierarchy of hypotheses,\nArti\ufb01cial Intelligence 28: 9\u201315.\nPearl, J. (1988). Probabilistic reasoning in intelligent systems: networks of\nplausible inference , Morgan Kaufmann, San Francisco, CA.\nPearl, J. (2000). Causality: Models, Reasoning and Inference , Cambridge\nUniversity Press.\nPeterson and Anderson, J. R. (1987). A mean \ufb01eld theory learning algo-\nrithm for neural networks, Complex Systems 1: 995\u20131019.\nPetricoin, E. F., Ardekani, A. M., Hitt, B. A., Levine, P. J., Fusaro, V.,\nSteinberg, S. M., Mills, G. B., Simone, C., Fishman, D. A., Kohn,\nE. and Liotta, L. A. (2002). Use of proteomic patterns in serum to\nidentify ovarian cancer, Lancet 359: 572\u2013577.\nPlatt, J. (1999). Fast Training of Support Vector Machines using Sequen-\ntial Minimal Optimization; in Advances in Kernel Methods\u2014S upport\nVector Learning, B. Sch\u00a8 olkopf and C. J. C. Burges and A. J. Sm ola\n(eds), MIT Press, Cambridge, MA., pp. 185\u2013208.\nQuinlan, R. (1993). C4.5: Programs for Machine Learning , Morgan Kauf-\nmann, San Mateo.\nQuinlan, R. (2004). C5.0, www.rulequest.com .", "738": "720 References\nRamaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., Yeang, C., Angelo,\nM., Ladd, C., Reich, M., Latulippe, E., Mesirov, J., Poggio, T., Gerald,\nW., Loda, M., Lander, E. and Golub, T. (2001). Multiclass cancer\ndiagnosis using tumor gene expression signature, PNAS 98: 15149\u2013\n15154.\nRamsay, J. and Silverman, B. (1997). Functional Data Analysis , Springer,\nNew York.\nRao, C. R. (1973). Linear Statistical Inference and Its Applications , Wiley,\nNew York.\nR\u00a8 atsch, G. and Warmuth, M. (2002). Maximizing the margin with boost -\ning,Proceedings of the 15th Annual Conference on Computational\nLearning Theory , pp. 334\u2013350.\nRavikumar, P., Liu, H., La\ufb00erty, J. and Wasserman, L. (2008). Spam:\nSparse additive models, inJ. Platt, D. Koller, Y. Singer and S. Roweis\n(eds), Advances in Neural Information Processing Systems 20 , MIT\nPress, Cambridge, MA, pp. 1201\u20131208.\nRidgeway, G. (1999). The state of boosting, Computing Science and Statis-\ntics31: 172\u2013181.\nRieger, K., Hong, W., Tusher, V., Tang, J., Tibshirani, R. and Chu, G.\n(2004). Toxicity from radiation therapy associated with abnormal\ntranscriptional responses to DNA damage, Proceedings of the National\nAcademy of Sciences 101: 6634\u20136640.\nRipley, B. D. (1996). Pattern Recognition and Neural Networks , Cambridge\nUniversity Press.\nRissanen, J. (1983). A universal prior for integers and estimation by mini -\nmum description length, Annals of Statistics 11: 416\u2013431.\nRobbins, H. and Munro, S. (1951). A stochastic approximation method,\nAnnals of Mathematical Statistics 22: 400\u2013407.\nRoosen, C. and Hastie, T. (1994). Automatic smoothing spline projection\npursuit, Journal of Computational and Graphical Statistics 3: 235\u2013248.\nRosenblatt, F. (1958). The perceptron: a probabilistic model for infor-\nmation storage and organization in the brain, Psychological Review\n65: 386\u2013408.\nRosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the\nTheory of Brain Mechanisms , Spartan, Washington, D.C.", "739": "References 721\nRosenwald, A., Wright, G., Chan, W. C., Connors, J. M., Campo, E.,\nFisher, R. I., Gascoyne, R. D., Muller-Hermelink, H. K., Smeland,\nE. B. and Staudt, L. M. (2002). The use of molecular pro\ufb01ling to\npredict survival after chemotherapy for di\ufb00use large b-cell lymphoma,\nThe New England Journal of Medicine 346: 1937\u20131947.\nRosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths,\nAnnals of Statistics 35(3): 1012\u20131030.\nRosset, S., Zhu, J. and Hastie, T. (2004a). Boosting as a regularized path to\na maximum margin classi\ufb01er, Journal of Machine Learning Research\n5: 941\u2013973.\nRosset, S., Zhu, J. and Hastie, T. (2004b). Margin maximizing loss func-\ntions,inS. Thrun, L. Saul and B. Sch\u00a8 olkopf (eds), Advances in Neural\nInformation Processing Systems 16 , MIT Press, Cambridge, MA.\nRousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotze, J., Jooste, P.\nand Ferreira, J. (1983). Coronary risk factor screening in three rural\ncommunities, South African Medical Journal 64: 430\u2013436.\nRoweis, S. T. and Saul, L. K. (2000). Locally linear embedding, Science\n290: 2323\u20132326.\nRumelhart, D., Hinton, G. and Williams, R. (1986). Learning internal rep-\nresentations by error propagation, inD. Rumelhart and J. McClelland\n(eds), Parallel Distributed Processing: Explorations in the Micr ostruc-\nture of Cognition , The MIT Press, Cambridge, MA., pp. 318\u2013362.\nSachs, K., Perez, O., Pe\u2019er, D., Lau\ufb00enburger, D. and Nolan, G. (2003).\nCausal protein-signaling networks derived from multiparameter single-\ncell data, Science 308(5721): 523\u2013529.\nSchapire, R. (1990). The strength of weak learnability, Machine Learning\n5(2): 197\u2013227.\nSchapire, R. (2002). The boosting approach to machine learning: an\noverview, inD. Denison, M. Hansen, C. Holmes, B. Mallick and B. Yu\n(eds), MSRI workshop on Nonlinear Estimation and Classi\ufb01cation ,\nSpringer, New York.\nSchapire, R. and Singer, Y. (1999). Improved boosting algorithms using\ncon\ufb01dence-rated predictions, Machine Learning 37(3): 297\u2013336.\nSchapire, R., Freund, Y., Bartlett, P. and Lee, W. (1998). Boosting the\nmargin: a new explanation for the e\ufb00ectiveness of voting methods,\nAnnals of Statistics 26(5): 1651\u20131686.", "740": "722 References\nSch\u00a8 olkopf, B., Smola, A. and M\u00a8 uller, K.-R. (1999). Kernel principal compo-\nnent analysis, inB. Sch\u00a8 olkopf, C. Burges and A. Smola (eds), Advances\nin Kernel Methods\u2014Support Vector Learning , MIT Press, Cambridge,\nMA, USA, pp. 327\u2013352.\nSchwarz, G. (1978). Estimating the dimension of a model, Annals of Statis-\ntics6(2): 461\u2013464.\nScott, D. (1992). Multivariate Density Estimation: Theory, Practice, and\nVisualization , Wiley, New York.\nSeber, G. (1984). Multivariate Observations , Wiley, New York.\nSegal, M. (2004). Machine learning benchmarks and random forest regres-\nsion,Technical report , eScholarship Repository, University of Califor-\nnia.http://repositories.edlib.org/cbmb/bench rfregn.\nShao, J. (1996). Bootstrap model selection, Journal of the American Sta-\ntistical Association 91: 655\u2013665.\nShenoy, P. and Shafer, G. (1988). An axiomatic framework for Bayesian\nand belief-function propagation, AAAI Workshop on Uncertainty in\nAI, North-Holland, pp. 307\u2013314.\nShort, R. and Fukunaga, K. (1981). The optimal distance measure for near-\nest neighbor classi\ufb01cation, IEEE Transactions on Information Theory\n27: 622\u2013627.\nSilverman, B. (1986). Density Estimation for Statistics and Data Analysis ,\nChapman and Hall, London.\nSilvey, S. (1975). Statistical Inference , Chapman and Hall, London.\nSimard, P., Cun, Y. L. and Denker, J. (1993). E\ufb03cient pattern recognition\nusing a new transformation distance, Advances in Neural Information\nProcessing Systems , Morgan Kaufman, San Mateo, CA, pp. 50\u201358.\nSimon, R. M., Korn, E. L., McShane, L. M., Radmacher, M. D., Wright,\nG. and Zhao, Y. (2004). Design and Analysis of DNA Microarray\nInvestigations , Springer, New York.\nSj\u00a8 ostrand, K., Rostrup, E., Ryberg, C., Larsen, R., Studholme, C., Baezner,\nH., Ferro, J., Fazekas, F., Pantoni, L., Inzitari, D. and Waldemar,\nG. (2007). Sparse decomposition and modeling of anatomical shape\nvariation, IEEE Transactions on Medical Imaging 26(12): 1625\u20131635.\nSpeed, T. and Kiiveri, H. T. (1986). Gaussian Markov distributions over\n\ufb01nite graphs, Annals of Statistics 14: 138\u2013150.", "741": "References 723\nSpeed, T. (ed.) (2003). Statistical Analysis of Gene Expression Microarray\nData, Chapman and Hall, London.\nSpiegelhalter, D., Best, N., Gilks, W. and Inskip, H. (1996). Hepatitis\nB: a case study in MCMC methods, inW. Gilks, S. Richardson and\nD. Spegelhalter (eds), Markov Chain Monte Carlo in Practice , Inter-\ndisciplinary Statistics, Chapman and Hall, London, pp. 21\u201343.\nSpielman, D. A. and Teng, S.-H. (1996). Spectral partitioning works: Pla-\nnar graphs and \ufb01nite element meshes, IEEE Symposium on Founda-\ntions of Computer Science , pp. 96\u2013105.\nStamey, T., Kabalin, J., McNeal, J., Johnstone, I., Freiha, F., Redwine, E.\nand Yang, N. (1989). Prostate speci\ufb01c antigen in the diagnosis and\ntreatment of adenocarcinoma of the prostate II radical prostatectomy\ntreated patients, Journal of Urology 16: 1076\u20131083.\nStone, C., Hansen, M., Kooperberg, C. and Truong, Y. (1997). Polynomial\nsplines and their tensor products (with discussion), Annals of Statistics\n25(4): 1371\u20131470.\nStone, M. (1974). Cross-validatory choice and assessment of statistica l\npredictions, Journal of the Royal Statistical Society Series B 36: 111\u2013\n147.\nStone, M. (1977). An asymptotic equivalence of choice of model by cross-\nvalidation and Akaike\u2019s criterion, Journal of the Royal Statistical So-\nciety Series B. 39: 44\u20137.\nStone, M. and Brooks, R. J. (1990). Continuum regression: cross-validated\nsequentially constructed prediction embracing ordinary least squares,\npartial least squares and principal components regression (Corr: V54\np906-907), Journal of the Royal Statistical Society, Series B 52: 237\u2013\n269.\nStorey, J. (2002). A direct approach to false discovery rates, Journal of the\nRoyal Statistical Society B. 64(3): 479\u2013498.\nStorey, J. (2003). The positive false discovery rate: A Bayesian inter preta-\ntion and the q-value, Annals of Statistics 31: 2013\u20132025.\nStorey, J. and Tibshirani, R. (2003). Statistical signi\ufb01cance for genomewide\nstudies, Proceedings of the National Academy of Sciences 100-: 9440\u2013\n9445.\nStorey, J., Taylor, J. and Siegmund, D. (2004). Strong control, conservativ e\npoint estimation, and simultaneous conservative consistency of false\ndiscovery rates: A uni\ufb01ed approach., Journal of the Royal Statistical\nSociety, Series B 66: 187\u2013205.", "742": "724 References\nSurowiecki, J. (2004). The Wisdom of Crowds: Why the Many are Smarter\nthan the Few and How Collective Wisdom Shapes Business, Eco-\nnomics, Societies and Nations. , Little, Brown.\nSwayne, D., Cook, D. and Buja, A. (1991). Xgobi: Interactive dynamic\ngraphics in the X window system with a link to S, ASA Proceedings\nof Section on Statistical Graphics , pp. 1\u20138.\nTanner, M. and Wong, W. (1987). The calculation of posterior distribu-\ntions by data augmentation (with discussion), Journal of the American\nStatistical Association 82: 528\u2013550.\nTarpey, T. and Flury, B. (1996). Self-consistency: A fundamental concept\nin statistics, Statistical Science 11: 229\u2013243.\nTenenbaum, J. B., de Silva, V. and Langford, J. C. (2000). A global\ngeometric framework for nonlinear dimensionality reduction, Science\n290: 2319\u20132323.\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso,\nJournal of the Royal Statistical Society, Series B 58: 267\u2013288.\nTibshirani, R. and Hastie, T. (2007). Margin trees for high-dimensional\nclassi\ufb01cation, Journal of Machine Learning Research 8: 637\u2013652.\nTibshirani, R. and Knight, K. (1999). Model search and inference by boot-\nstrap \u201cbumping, Journal of Computational and Graphical Statistics\n8: 671\u2013686.\nTibshirani, R. and Wang, P. (2007). Spatial smoothing and hot spot de-\ntection for CGH data using the fused lasso, Biostatistics 9: 18\u201329.\nTibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2001a). Diagno sis\nof multiple cancer types by shrunken centroids of gene expression,\nProceedings of the National Academy of Sciences 99: 6567\u20136572.\nTibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2003). Class\nprediction by nearest shrunken centroids, with applications to DNA\nmicroarrays, Statistical Science 18(1): 104\u2013117.\nTibshirani, R., Saunders, M., Rosset, S., Zhu, J. and Knight, K. (2005).\nSparsity and smoothness via the fused lasso, Journal of the Royal\nStatistical Society, Series B 67: 91\u2013108.\nTibshirani, R., Walther, G. and Hastie, T. (2001b). Estimating the number\nof clusters in a dataset via the gap statistic, Journal of the Royal\nStatistical Society, Series B. 32(2): 411\u2013423.\nTropp, J. (2004). Greed is good: algorithmic results for sparse approxim a-\ntion,IEEE Transactions on Information Theory 50: 2231\u2013 2242.", "743": "References 725\nTropp, J. (2006). Just relax: convex programming methods for identify-\ning sparse signals in noise, IEEE Transactions on Information Theory\n52: 1030\u20131051.\nValiant, L. G. (1984). A theory of the learnable, Communications of the\nACM27: 1134\u20131142.\nvan der Merwe, A. and Zidek, J. (1980). Multivariate regression analysis\nand canonical variates, The Canadian Journal of Statistics 8: 27\u201339.\nVapnik, V. (1996). The Nature of Statistical Learning Theory , Springer,\nNew York.\nVapnik, V. (1998). Statistical Learning Theory , Wiley, New York.\nVidakovic, B. (1999). Statistical Modeling by Wavelets , Wiley, New York.\nvon Luxburg, U. (2007). A tutorial on spectral clustering, Statistics and\nComputing 17(4): 395\u2013416.\nWahba, G. (1980). Spline bases, regularization, and generalized cross-\nvalidation for solving approximation problems with large quantities\nof noisy data, Proceedings of the International Conference on Approx-\nimation theory in Honour of George Lorenz , Academic Press, Austin,\nTexas, pp. 905\u2013912.\nWahba, G. (1990). Spline Models for Observational Data , SIAM, Philadel-\nphia.\nWahba, G., Lin, Y. and Zhang, H. (2000). GACV for support vector ma-\nchines, inA. Smola, P. Bartlett, B. Sch\u00a8 olkopf and D. Schuurmans\n(eds), Advances in Large Margin Classi\ufb01ers , MIT Press, Cambridge,\nMA., pp. 297\u2013311.\nWainwright, M. (2006). Sharp thresholds for noisy and high-dimensional\nrecovery of sparsity using \u21131-constrained quadratic programming,\nTechnical report , Department of Statistics, University of California,\nBerkeley.\nWainwright, M. J., Ravikumar, P. and La\ufb00erty, J. D. (2007). High-\ndimensional graphical model selection using \u21131-regularized logistic re-\ngression, inB. Sch\u00a8 olkopf, J. Platt and T. Ho\ufb00man (eds), Advances\nin Neural Information Processing Systems 19 , MIT Press, Cambridge,\nMA, pp. 1465\u20131472.\nWasserman, L. (2004). All of Statistics: a Concise Course in Statistical\nInference , Springer, New York.\nWeisberg, S. (1980). Applied Linear Regression , Wiley, New York.", "744": "726 References\nWerbos, P. (1974). Beyond Regression , PhD thesis, Harvard University.\nWeston, J. and Watkins, C. (1999). Multiclass support vector machines, in\nM. Verleysen (ed.), Proceedings of ESANN99 , D. Facto Press, Brussels.\nWhittaker, J. (1990). Graphical Models in Applied Multivariate Statistics ,\nWiley, Chichester.\nWickerhauser, M. (1994). Adapted Wavelet Analysis from Theory to Soft-\nware, A.K. Peters Ltd, Natick, MA.\nWidrow, B. and Ho\ufb00, M. (1960). Adaptive switching circuits, IRE\nWESCON Convention record , Vol. 4. pp 96-104; Reprinted in An-\ndersen and Rosenfeld (1988).\nWold, H. (1975). Soft modelling by latent variables: the nonlinear iterativ e\npartial least squares (NIPALS) approach, Perspectives in Probability\nand Statistics, In Honor of M. S. Bartlett , pp. 117\u2013144.\nWolpert, D. (1992). Stacked generalization, Neural Networks 5: 241\u2013259.\nWu, T. and Lange, K. (2007). The MM alternative to EM, unpublished.\nWu, T. and Lange, K. (2008). Coordinate descent procedures for lasso\npenalized regression, Annals of Applied Statistics 2(1): 224\u2013244.\nYee, T. and Wild, C. (1996). Vector generalized additive models, Journal\nof the Royal Statistical Society, Series B. 58: 481\u2013493.\nYuan, M. and Lin, Y. (2007). Model selection and estimation in regression\nwith grouped variables, Journal of the Royal Statistical Society, Series\nB68(1): 49\u201367.\nZhang, P. (1993). Model selection via multifold cross-validation, Annals of\nStatistics 21: 299\u2013311.\nZhang, T. and Yu, B. (2005). Boosting with early stopping: convergence\nand consistency, Annals of Statistics 33: 1538\u20131579.\nZhao, P. and Yu, B. (2006). On model selection consistency of lasso, Jour-\nnal of Machine Learning Research 7: 2541\u20132563.\nZhao, P., Rocha, G. and Yu, B. (2008). The composite absolute penalties\nfor grouped and hierarchichal variable selection, Annals of Statistics .\n(to appear).\nZhu, J. and Hastie, T. (2004). Classi\ufb01cation of gene microarrays by pena l-\nized logistic regression, Biostatistics 5(2): 427\u2013443.\nZhu, J., Zou, H., Rosset, S. and Hastie, T. (2005). Multiclass adaboost ,\nUnpublished.", "745": "References 727\nZou, H. (2006). The adaptive lasso and its oracle properties, Journal of\nthe American Statistical Association 101: 1418\u20131429.\nZou, H. and Hastie, T. (2005). Regularization and variable selection via\nthe elastic net, Journal of the Royal Statistical Society Series B.\n67(2): 301\u2013320.\nZou, H., Hastie, T. and Tibshirani, R. (2006). Sparse principal com-\nponent analysis, Journal of Computational and Graphical Statistics\n15(2): 265\u201328.\nZou, H., Hastie, T. and Tibshirani, R. (2007). On the degrees of freedom\nof the lasso, Annals of Statistics 35(5): 2173\u20132192.", "746": "728 References", "747": "This is page 729\nPrinter: Opaque this\nAuthor Index\nAbu-Mostafa, Y. 95, 474\nAckley, D. H. 645\nAdam, B.-L. 664\nAgrawal, R. 489\u2013491, 578\nAgresti, A. 385, 638, 640\nAhn, J. 695\nAkaike, H. 257\nAllen, D. 257\nAmbroise, C. 247\nAmit, Y. 602\nAnderson, J. R. 641\nAnderson, T. 645\nAngelo, M. 654, 658\nArdekani, A. M. 664\nBach, F. 569\nBaezner, H. 551\nBair, E. 676, 679\u2013683, 693\nBakin, S. 90\nBakiri, G. 605, 606\nBanerjee, O. 636\nBarnhill, S. 658\nBarron, A. 415\nBarry, R. 371\nBartlett, P. 384, 615Baskett, F. 480\nBaxter, J. 384\nBecker, R. 369\nBell, A. 578\nBellman, R. E. 22\nBenade, A. 122\nBengio, Y. 404, 407, 408, 414, 644\nBenjamini, Y. 687, 689, 693\nBentley, J. 480\nBest, N. 292\nBibby, J. 94, 135, 441, 539, 559,\n578, 630, 679\nBickel, P. 652\nBickel, P. J. 89\nBishop, C. 38, 233, 414, 623, 645\nBishop, Y. 629, 638\nBloom\ufb01eld, C. 663\nBoser, B. 404, 414\nBotha, J. 334\nBottou, L. 404, 407, 408, 414, 644\nBoyd, S. 125, 632\nBreiman, L. 85, 243, 251, 257, 292,\n308, 310, 334, 339, 367,\n384, 451, 453, 455, 554,\n587, 602", "748": "730 Author Index\nBremaud, P. 577\nBrin, S. 577\nBrooks, R. J. 81\nBrown, P. 679\nBruce, A. 181\nB\u00a8 uhlmann, P. 87, 361, 384\nBuja, A. 110, 297, 441, 446, 451,\n455, 565, 574, 576, 578\nBunea, F. 91\nBurges, C. 455\nButte, A. 631\nCaligiuri, M. 663\nCallow, M. 686, 693\nCampo, E. 674\nCandes, E. 86, 89, 613\nCarlin, J. 292\nCazares, L. H. 664\nChambers, J. 334\nChan, W. C. 674\nChaudhuri, S. 631, 633\nChen, L. 574, 576, 578\nChen, S. S. 68, 94\nCherkassky, V. 38, 239, 257\nChu, G. 684, 693\nChui, C. 181\nClark, L. 331\nClark, M. 539\nClements, M. A. 664\nCleveland, W. 369\nCli\ufb00ord, P. 629\nCohen, A. 668, 669\nCoifman, R. R. 679\nColler, H. 663\nComon, P. 578\nConnors, J. M. 674\nCook, D. 565, 578\nCook, N. 317\nCopas, J. B. 94, 610\nCover, T. 257, 465, 481\nCox, D. 292, 645\nCressie, N. 171\nCsiszar, I. 292\nCun, Y. L. 407, 471, 481\nCutler, A. 554Dale, M. 526\nDasarathy, B. 480, 481\nd\u2019Aspremont, A. 636\nDaubechies, I. 92, 181\nDavis, J. W. 664\nde Boor, C. 181\nDe Mol, C. 92\nde Silva, V. 573\nDefrise, M. 92\nDempster, A. 292, 449, 633\nDenham, M. 679\nDenker, J. 404, 407, 414, 471, 481\nDevijver, P. 480\nDietterich, T. 286, 602, 605, 606,\n623\nDonath, W. E. 578\nDonoho, D. 68, 86, 91, 94, 179,\n181, 554, 613\nDowning, J. 663\nDrton, M. 631, 633\ndu Plessis, J. 122\nDuan, N. 480\nDubes, R. 508, 522\nDuchamp, T. 541\nDuda, R. 38, 135\nDudoit, S. 686, 693\nDuin, R. 624\nEdwards, D. 645\nEfron, B. 73, 86, 90, 94, 97, 98,\n128, 231, 254, 257, 292,\n334, 568, 609, 692, 693\nElad, M. 613\nElith, J. 375, 376, 378\nEskin, E. 668, 669\nEvgeniou, T. 168, 181, 455\nFan, J. 92, 216, 539, 654\nFan, Y. 654\nFazekas, F. 551\nFeng, Z. 664\nFerreira, J. 122\nFerro, J. 551\nFiedler, M. 578\nFienberg, S. 585, 629, 638", "749": "Author Index 731\nFinkel, R. 480\nFisher, N. 334\nFisher, R. A. 136, 455\nFisher, R. I. 674\nFisher, W. 310\nFishman, D. A. 664\nFix, E. 481\nFlury, B. 578\nForgy, E. 578\nFrancis, M. 375, 376, 378\nFrank, I. 81, 82, 94\nFrean, M. 384\nFreiha, F. 3, 49\nFreund, Y. 337, 383, 384, 615\nFridlyand, J. 693\nFriedman, J. 38, 81, 82, 85, 92\u201394,\n111, 121, 126, 251, 257,\n258, 308, 310, 334, 339,\n345, 365, 367, 384, 391,\n414, 437, 451, 453, 475,\n480, 565, 578, 602, 611,\n617\u2013621, 623, 636, 657,\n661, 667\nFriedman, N. 629, 630, 645\nFu, W. 91, 92\nFukunaga, K. 475\nFurnival, G. 57\nFusaro, V. 664\nGaasenbeek, M. 663\nGanapathi, V. 642\nGao, H. 181\nGascoyne, R. D. 674\nGelfand, A. 292\nGelman, A. 292\nGeman, D. 292, 602\nGeman, S. 292\nGenkin, A. 661\nGenovese, C. 693\nGerald, W. 654, 658\nGersho, A. 514, 515, 526, 578\nGhaoui, L. E. 636\nGijbels, I. 216\nGilks, W. 292\nGill, P. 96, 421Girosi, F. 168, 174, 181, 415\nGolub, G. 257, 335, 535\nGolub, T. 631, 654, 658, 663\nGoodall, C. 578\nGordon, A. 578\nGray, R. 514, 515, 526, 578\nGreen, P. 181, 183, 334\nGreenacre, M. 455\nGreenshtein, E. 91\nGuo, Y. 657\nGuyon, I. 658\nHa\ufb00ner, P. 404, 407, 408, 414, 644\nHall, P. 292, 602, 619\nHammersley, J. M. 629\nHand, D. 135, 475\nHanley, J. 317\nHansen, M. 328\nHansen, R. 93\nHart, P. 38, 135, 465, 480, 481\nHartigan, J. A. 510, 578\nHastie, T. 72, 73, 78, 86, 88, 90,\n92\u201394, 97, 98, 110, 121,\n122, 126, 137, 174, 216,\n257, 297, 299, 304, 334,\n339, 345, 348, 349, 375,\n376, 378, 384, 385, 414,\n428, 431, 434, 437, 441,\n446, 451, 455, 475, 478,\n480, 481, 519, 539, 550,\n565, 568, 578, 606, 609\u2013\n611, 614, 615, 636, 657,\n658, 660\u2013662, 664, 667,\n676, 679\u2013683, 693\nHatef, M. 624\nHathaway, R. J. 292\nHeath, M. 257\nHebb, D. 414\nHenderson, D. 404, 414\nHerman, A. 334\nHertz, J. 414\nHinkley, D. 292\nHinton, G. 292, 334, 408, 414, 644,\n645\nHitt, B. A. 664", "750": "732 Author Index\nHo, T. K. 602\nHochberg, Y. 687, 689, 693\nHodges, J. 481\nHoe\ufb02ing, H. 92, 93, 642, 667\nHoerl, A. E. 64, 94\nHo\ufb00, M. 396, 414\nHo\ufb00man, A. J. 578\nHofmann, H. 578\nHolland, P. 629, 638\nHong, W. 684\nHothorn, T. 87, 361, 384\nHoward, R. 404, 414\nHuard, C. 663\nHubbard, W. 404, 414\nHuber, P. 349, 414, 435, 565, 578\nHunter, D. 294\nHyv\u00a8 arinen, A. 560, 562, 578, 583\nIhaka, R. 455\nInskip, H. 292\nInzitari, D. 551\nIzenman, A. 84\nJackel, L. 404, 414\nJacobs, R. 334\nJain, A. 508, 522\nJames, G. 606\nJancey, R. 578\nJensen, F. V. 629\nJiang, W. 384\nJirou\u00b4 sek, R. 640\nJohnson, N. 412\nJohnstone, I. 3, 49, 73, 86, 94, 97,\n98, 179, 181, 609, 613\nJoli\ufb00e, I. T. 550\nJones, L. 415\nJones, M. 168, 174, 181, 415\nJooste, P. 122\nJordaan, P. 122\nJordan, M. 334, 569, 645\nKabalin, J. 3, 49\nKalb\ufb02eisch, J. 674, 693\nKarhunen, J. 583\nKaski, S. 531, 532, 578Kaufman, L. 517, 526, 578\nKearns, M. 380\nKennard, R. 64, 94\nKent, J. 94, 135, 441, 539, 559,\n578, 630, 679\nKiiveri, H. T. 632\nKim, S.-J. 125\nKishon, E. 539\nKittler, J. 480, 624\nKleinberg, E. M. 602\nKnight, K. 91, 292, 666, 693\nKoh, K. 125\nKohane, I. 631\nKohavi, R. 243, 257\nKohn, E. 664\nKohonen, T. 462, 481, 531, 532,\n578\nKoller, D. 629, 630, 642, 645\nKooperberg, C. 328\nKorn, E. L. 693\nKotze, J. 122\nKressel, U. 437\nKrogh, A. 414\nLadd, C. 654, 658\nLa\ufb00erty, J. 90, 304\nLa\ufb00erty, J. D. 642\nLagus, K. 531, 532, 578\nLaird, N. 292, 449\nLambert, D. 376\nLander, E. 654, 658, 663\nLange, K. 92, 294, 583, 584\nLangford, J. C. 573\nLarsen, R. 551\nLatulippe, E. 654, 658\nLau\ufb00enburger, D. 625\nLauritzen, S. 629, 632, 645\nLawson, C. 93\nLe Cun, Y. 404, 406\u2013408, 414\nLeathwick, J. 375, 376, 378\nLeblanc, M. 292\nLeCun, Y. 644\nLee, D. 552, 553\nLee, M.-L. 693\nLee, S.-I. 642", "751": "Author Index 733\nLee, W. 384, 615\nLeslie, C. 668, 669\nLevina, E. 652, 693\nLevine, P. J. 664\nLewis, D. 661\nLi, K.-C. 480\nLi, R. 92\nLin, H. 331\nLin, Y. 90, 304, 428, 455\nLiotta, L. A. 664\nLittle, R. 332, 647\nLittman, M. 578\nLiu, H. 90, 304\nLloyd, S. 481, 578\nLoader, C. 209, 216\nLoda, M. 654, 658\nLugosi, G. 384\nLoh, M. 663\nLoh, W. 310\nMa, Y. 257\nMacnaughton Smith, P. 526\nMacKay, D. 623\nMacQueen, J. 481, 578\nMadigan, D. 257, 292, 661\nMakeig, S. 564, 565\nMannila, H. 489\u2013491, 578\nMardia, K. 94, 135, 441, 539, 559,\n578, 630, 679\nMarron, J. 695\nMason, L. 384\nMassart, D. 517\nMatas, J. 624\nMcCullagh, P. 638, 640\nMcCulloch, C. 331\nMcCulloch, W. 414\nMcLachlan, G. 135, 247\nMcNeal, J. 3, 49\nMcNeil, B. 317\nMcShane, L. M. 693\nMease, D. 384, 603\nMeinshausen, N. 91, 635, 642\nMeir, R. 384\nMesirov, J. 654, 658, 663\nMills, G. B. 664Mockett, L. 526\nMorgan, J. N. 334\nMotwani, R. 577\nMukherjee, S. 654, 658\nMulier, F. 38, 239\nMuller-Hermelink, H. K. 674\nM\u00a8 uller, K.-R. 547, 548\nMunro, S. 397\nMurray, W. 96, 421\nMyles, J. 475\nNadler, B. 679\nNarasimhan, B. 693\nNeal, R. 268, 292, 409\u2013412, 414,\n605, 623\nNelder, J. 638, 640\nNoble, W. S. 668, 669\nNolan, G. 625\nNowlan, S. 334\nOja, E. 560, 562, 578, 583\nOlesen, K. G. 629\nOlshen, R. 251, 308, 310, 334, 367,\n451, 453\nOnton, J. 564, 565\nOsborne, M. 76, 94\nOsindero, S. 644\nPaatero, A. 531, 532, 578\nPace, R. K. 371\nPage, L. 577\nPalmer, R. 414\nPantoni, L. 551\nPark, M. Y. 94, 126, 661\nParker, D. 414\nPaul, D. 676, 679\u2013683, 693\nPearl, J. 629, 645\nPe\u2019er, D. 625\nPerez, O. 625\nPeterson 641\nPetricoin, E. F. 664\nPitts, W. 414\nPlastria, F. 517\nPlatt, J. 453\nPoggio, T. 168, 174, 181, 415, 455,\n654, 658", "752": "734 Author Index\nPontil, M. 168, 181, 455\nPopescu, B. 617\u2013619, 621, 623\nPrentice, R. 674, 693\nPresnell, B. 76, 94\nP\u02c7 reu\u02c7 cil, S. 640\nQu, Y. 664\nQuinlan, R. 312, 334, 624\nRadmacher, M. D. 693\nRaftery, A. 257, 292\nRamaswamy, S. 654, 658\nRamsay, J. 181, 578\nRao, C. R. 455\nR\u00a8 atsch, G. 384, 615\nRavikumar, P. 90, 304, 642\nRedwine, E. 3, 49\nReich, M. 654, 658\nRichardson, J. 375\nRichardson, T. S. 631, 633\nRidgeway, G. 361\nRieger, K. 684\nRifkin, R. 654, 658\nRipley, B. D. 38, 131, 135, 136,\n234, 308, 310, 400, 414,\n415, 455, 468, 480, 481,\n641, 645\nRissanen, J. 257\nRitov, Y. 89, 91\nRobbins, H. 397\nRocha, G. 90\nRoosen, C. 414\nRosenblatt, F. 102, 129, 414\nRosenwald, A. 674\nRosset, S. 89, 98, 348, 349, 385,\n426, 428, 434, 610, 611,\n615, 657, 661, 664, 666,\n693\nRostrup, E. 551\nRousseauw, J. 122\nRousseeuw, P. 517, 526, 578\nRowe, D. 375\nRoweis, S. T. 573\nRubin, D. 292, 332, 449, 647\nRumelhart, D. 414Ryberg, C. 551\nSaarela, A. 531, 532, 578\nSachs, K. 625\nSaloj\u00a8 arvi, J. 531, 532, 578\nSaul, L. K. 573\nSaunders, M. 68, 94, 666, 693\nSchapire, R. 337, 380, 383, 384,\n615\nSchellhammer, P. F. 664\nSchnitzler, C. 334\nSch\u00a8 olkopf, B. 547, 548\nSchroeder, A. 391\nSchwarz, G. 233, 257\nScott, D. 216\nSeber, G. 94\nSegal, M. 596\nSejnowski, T. 578, 645\nSemmes, O. J. 664\nSeung, H. 552, 553\nShafer, G. 629\nShao, J. 257\nShenoy, P. 629\nShort, R. 475\nShustek, L. 480\nShyu, M. 369\nSiegmund, D. 689\nSilverman, B. 181, 183, 216, 334,\n486, 567, 578\nSilvey, S. 292\nSimard, P. 407, 471, 480, 481\nSimon, R. M. 693\nSimone, C. 664\nSinger, Y. 384\nSj\u00a8 ostrand, K. 551\nSlate, E. 331\nSlonim, D. 631, 663\nSmeland, E. B. 674\nSmith, A. 292\nSmola, A. 547, 548\nSonquist, J. A. 334\nSpector, P. 243, 257\nSpeed, T. 632, 686, 693\nSpiegelhalter, D. 292, 629\nSpiegelman, C. 679", "753": "Author Index 735\nSpielman, D. A. 578\nSrikant, R. 489\u2013491, 578\nStamey, T. 3, 49\nStaudt, L. M. 674\nSteinberg, S. M. 664\nStern, H. 292\nStodden, V. 554\nStone, C. 251, 308, 310, 328, 334,\n367, 451, 453\nStone, M. 81, 257\nStorey, J. 689, 692, 693, 697, 698\nStork, D. 38, 135\nStudholme, C. 551\nStuetzle, W. 391, 414, 541, 578\nSurowiecki, J. 286\nSwayne, D. 565, 578\nTamayo, P. 631, 654, 658, 663\nTang, J. 684\nTanner, M. 292\nTao, T. 89, 613\nTarpey, T. 578\nTaylor, J. 88, 94, 610, 614, 689\nTaylor, P. 375, 376, 378\nTeh, Y.-W. 644\nTenenbaum, J. B. 573\nTeng, S.-H. 578\nThomas, J. 257\nTibshirani, R. 73, 78, 86, 88, 90,\n92\u201394, 97, 98, 110, 121,\n122, 126, 137, 216, 257,\n292, 297, 299, 304, 334,\n339, 345, 384, 428, 431,\n434, 437, 441, 446, 451,\n455, 475, 478, 480, 481,\n519, 550, 565, 568, 609\u2013\n611, 614, 636, 642, 657,\n658, 660, 661, 666, 667,\n676, 679\u2013684, 692, 693\nToivonen, H. 489\u2013491, 578\nTraskin, M. 384\nTrenda\ufb01lov, N. T. 550\nTropp, J. 91\nTruong, Y. 328\nTsybakov, A. 89, 91Tukey, J. 414, 565, 578\nTurlach, B. 76, 94\nTurnbull, B. 331\nTusher, V. 684, 692\nTusn\u00b4 ady, G. 292\nUddin, M. 550\nValiant, L. G. 380\nvan der Merwe, A. 84\nVan Loan, C. 335, 535\nVandenberghe, L. 632\nVanichsetakul, N. 310\nVapnik, V. 38, 102, 132, 135, 171,\n257, 438, 455, 658\nVayatis, N. 384\nVazirani, U. 380\nVerkamo, A. I. 489\u2013491, 578\nVidakovic, B. 181\nvon Luxburg, U. 578\nWahba, G. 168, 169, 181, 257, 268,\n428, 429, 455\nWainwright, M. 91\nWainwright, M. J. 642\nWaldemar, G. 551\nWalther, G. 88, 94, 519, 610, 614\nWang, P. 667\nWard, M. D. 664\nWarmuth, M. 615\nWasserman, L. 90, 304, 626, 645,\n693\nWatkins, C. 658\nWegkamp, M. 91\nWeisberg, S. 94\nWerbos, P. 414\nWermuth, N. 645\nWeston, J. 658, 668, 669\nWhittaker, J. 632, 633, 641, 645\nWickerhauser, M. 181\nWidrow, B. 396, 414\nWild, C. 300\nWilliams, R. 414\nWilliams, W. 526\nWilson, R. 57", "754": "736 Author Index\nWinograd, T. 577\nWold, H. 94\nWolpert, D. 292\nWong, M. A. 510\nWong, W. 292\nWright, G. 664, 674, 693\nWright, M. 96, 421\nWu, T. 92, 294, 583\nWyner, A. 384, 603\nYang, N. 3, 49\nYang, Y. 686, 693\nYasui, Y. 664\nYeang, C. 654, 658\nYee, T. 300\nYekutieli, Y. 693\nYu, B. 90, 91, 384\nYuan, M. 90\nZhang, H. 90, 304, 428, 455\nZhang, J. 409\u2013412, 605\nZhang, P. 257\nZhang, T. 384\nZhao, P. 90, 91\nZhao, Y. 693\nZhu, J. 89, 98, 174, 348, 349, 385,\n426, 428, 434, 610, 611,\n615, 657, 661, 664, 666,\n693\nZidek, J. 84\nZou, H. 72, 78, 92, 349, 385, 550,\n662, 693", "755": "This is page 737\nPrinter: Opaque this\nIndex\nL1regularization, seeLasso\nActivation function, 392\u2013395\nAdaBoost, 337\u2013346\nAdaptive lasso, 92\nAdaptive methods, 429\nAdaptive nearest neighbor meth-\nods, 475\u2013478\nAdaptive wavelet \ufb01ltering, 181\nAdditive model, 295\u2013304\nAdjusted response, 297\nA\ufb03ne set, 130\nA\ufb03ne-invariant average, 482, 540\nAIC,seeAkaike information cri-\nterion\nAkaike information criterion (AIC),\n230\nAnalysis of deviance, 124\nApplications\nabstracts, 672\naorta, 204\nbone, 152\nCalifornia housing, 371\u2013372,\n591\ncountries, 517demographics, 379\u2013380\ndocument, 532\n\ufb02ow cytometry, 637\ngalaxy, 201\nheart attack, 122, 146, 207\nlymphoma, 674\nmarketing, 488\nmicroarray, 5, 505, 532\nnested spheres, 590\nNew Zealand \ufb01sh, 375\u2013379\nnuclear magnetic resonance,\n176\nozone, 201\nprostate cancer, 3, 49, 61, 608\nprotein mass spectrometry, 664\nsatellite image, 470\nskin of the orange, 429\u2013432\nspam, 2, 300\u2013304, 313, 320,\n328, 352, 593\nvowel, 440, 464\nwaveform, 451\nZIP code, 4, 404, 536\u2013539\nArchetypal analysis, 554\u2013557\nAssociation rules, 492\u2013495, 499\u2013\n501", "756": "738 Index\nAutomatic relevance determination,\n411\nAutomatic selection of smoothing\nparameters , 156\nB-Spline, 186\nBack-propagation, 392\u2013397, 408\u2013\n409\nBack\ufb01tting, 297, 391\nBackward\nselection, 58\nstepwise selection, 59\nBackward pass, 396\nBagging, 282\u2013288, 409, 587\nBasis expansions and regulariza-\ntion, 139\u2013189\nBasis functions, 141, 186, 189, 321,\n328\nBatch learning, 397\nBaum\u2013Welch algorithm, 272\nBayes\nclassi\ufb01er, 21\nfactor, 234\nmethods, 233\u2013235, 267\u2013272\nrate, 21\nBayesian, 409\nBayesian information criterion (BIC),\n233\nBenjamini\u2013Hochberg method, 688\nBest-subset selection, 57, 610\nBetween class covariance matrix,\n114\nBias, 16, 24, 37, 160, 219\nBias-variance decomposition, 24,\n37, 219\nBias-variance tradeo\ufb00, 37, 219\nBIC,seeBayesian Information Cri-\nterion\nBoltzmann machines, 638\u2013648\nBonferroni method, 686\nBoosting, 337\u2013386, 409\nas lasso regression, 607\u2013609\nexponential loss and AdaBoost,\n343\ngradient boosting, 358implementations, 360\nmargin maximization, 613\nnumerical optimization, 358\npartial-dependence plots, 369\nregularization path, 607\nshrinkage, 364\nstochastic gradient boosting,\n365\ntree size, 361\nvariable importance, 367\nBootstrap, 249, 261\u2013264, 267, 271\u2013\n282, 587\nrelationship to Bayesian method,\n271\nrelationship to maximum like-\nlihood method, 267\nBottom-up clustering, 520\u2013528\nBump hunting, seePatient rule\ninduction method\nBumping, 290\u2013292\nC5.0, 624\nCanonical variates, 441\nCART, seeClassi\ufb01cation and re-\ngression trees\nCategorical predictors, 10, 310\nCensored data, 674\nClassical multidimensional scaling,\n570\nClassi\ufb01cation, 22, 101\u2013137, 305\u2013\n317, 417\u2013429\nClassi\ufb01cation and regression trees\n(CART), 305\u2013317\nClique, 628\nClustering, 501\u2013528\nk-means, 509\u2013510\nagglomerative, 523\u2013528\nhierarchical, 520\u2013528\nCodebook, 515\nCombinatorial algorithms, 507\nCombining models, 288\u2013290\nCommittee, 289, 587, 605\nComparison of learning methods,\n350\u2013352\nComplete data, 276", "757": "Index 739\nComplexity parameter, 37\nComputational shortcuts\nquadratic penalty, 659\nCondensing procedure, 480\nConditional likelihood, 31\nConfusion matrix, 301\nConjugate gradients, 396\nConsensus, 285\u2013286\nConvolutional networks, 407\nCoordinate descent, 92, 636, 668\nCOSSO, 304\nCost complexity pruning, 308\nCovariance graph, 631\nCpstatistic, 230\nCross-entropy, 308\u2013310\nCross-validation, 241\u2013245\nCubic smoothing spline, 151\u2013153\nCubic spline, 151\u2013153\nCurse of dimensionality, 22\u201326\nDantzig selector, 89\nData augmentation, 276\nDaubechies symmlet-8 wavelets,\n176\nDe-correlation, 597\nDecision boundary, 13\u201315, 21\nDecision trees, 305\u2013317\nDecoder, 515, seeencoder\nDecomposable models, 641\nDegrees of freedom\nin an additive model, 302\nin ridge regression, 68\nof a tree, 336\nof smoother matrices, 153\u2013154,\n158\nDelta rule, 397\nDemmler-Reinsch basis for splines,\n156\nDensity estimation, 208\u2013215\nDeviance, 124, 309\nDiagonal linear discriminant anal-\nysis, 651\u2013654\nDimension reduction, 658\nfor nearest neighbors, 479\nDiscrete variables, 10, 310\u2013311Discriminant\nadaptive nearest neighbor clas-\nsi\ufb01er, 475\u2013480\nanalysis, 106\u2013119\ncoordinates, 108\nfunctions, 109\u2013110\nDissimilarity measure, 503\u2013504\nDummy variables, 10\nEarly stopping, 398\nE\ufb00ective degrees of freedom, 17,\n68, 153\u2013154, 158, 232, 302,\n336\nE\ufb00ective number of parameters,\n15, 68, 153\u2013154, 158, 232,\n302, 336\nEigenvalues of a smoother matrix,\n154\nElastic net, 662\nEM algorithm, 272\u2013279\nas a maximization-maximization\nprocedure, 277\nfor two component Gaussian\nmixture, 272\nEncoder, 514\u2013515\nEnsemble, 616\u2013623\nEnsemble learning, 605\u2013624\nEntropy, 309\nEquivalent kernel, 156\nError rate, 219\u2013230\nError-correcting codes, 606\nEstimates of in-sample prediction\nerror, 230\nExpectation-maximization algorithm,\nseeEM algorithm\nExtra-sample error, 228\nFalse discovery rate, 687\u2013690, 692,\n693\nFeature, 1\nextraction, 150\nselection, 409, 658, 681\u2013683\nFeed-forward neural networks, 392\u2013\n408", "758": "740 Index\nFisher\u2019s linear discriminant, 106\u2013\n119, 438\nFlexible discriminant analysis, 440\u2013\n445\nForward\nselection, 58\nstagewise, 86, 608\nstagewise additive modeling,\n342\nstepwise, 73\nForward pass algorithm, 395\nFourier transform, 168\nFrequentist methods, 267\nFunction approximation, 28\u201336\nFused lasso, 666\nGap statistic, 519\nGating networks, 329\nGauss-Markov theorem, 51\u201352\nGauss-Newton method, 391\nGaussian (normal) distribution, 16\nGaussian graphical model, 630\nGaussian mixtures, 273, 463, 492,\n509\nGaussian radial basis functions,\n212\nGBM, seeGradient boosting\nGBM package, seeGradient boost-\ning\nGCV, seeGeneralized cross-validation\nGEM (generalized EM), 277\nGeneralization\nerror, 220\nperformance, 220\nGeneralized additive model, 295\u2013\n304\nGeneralized association rules, 497\u2013\n499\nGeneralized cross-validation, 244\nGeneralized linear discriminant anal-\nysis, 438\nGeneralized linear models, 125\nGibbs sampler, 279\u2013280, 641\nfor mixtures, 280\nGini index, 309Global Markov property, 628\nGradient Boosting, 359\u2013361\nGradient descent, 358, 395\u2013397\nGraph Laplacian, 545\nGraphical lasso, 636\nGrouped lasso, 90\nHaar basis function, 176\nHammersley-Cli\ufb00ord theorem, 629\nHard-thresholding, 653\nHat matrix, 46\nHelix, 582\nHessian matrix, 121\nHidden nodes, 641\u2013642\nHidden units, 393\u2013394\nHierarchical clustering, 520\u2013528\nHierarchical mixtures of experts,\n329\u2013332\nHigh-dimensional problems, 649\nHints, 96\nHyperplane, seeSeparating Hy-\nperplane\nICA,seeIndependent components\nanalysis\nImportance sampling, 617\nIn-sample prediction error, 230\nIncomplete data, 332\nIndependent components analysis,\n557\u2013570\nIndependent variables, 9\nIndicator response matrix, 103\nInference, 261\u2013294\nInformation\nFisher, 266\nobserved, 274\nInformation theory, 236, 561\nInner product, 53, 668, 670\nInputs, 10\nInstability of trees, 312\nIntercept, 11\nInvariance manifold, 471\nInvariant metric, 471\nInverse wavelet transform, 179", "759": "Index 741\nIRLS, seeIteratively reweighted\nleast squares\nIrreducible error, 224\nIsing model, 638\nISOMAP, 572\nIsometric feature mapping, 572\nIterative proportional scaling, 585\nIteratively reweighted least squares\n(IRLS), 121\nJensen\u2019s inequality, 293\nJoin tree, 629\nJunction tree, 629\nK-means clustering, 460, 509\u2013514\nK-medoid clustering, 515\u2013520\nK-nearest neighbor classi\ufb01ers, 463\nKarhunen-Loeve transformation (prin-\ncipal components), 66\u2013\n67, 79, 534\u2013539\nKarush-Kuhn-Tucker conditions,\n133, 420\nKernel\nclassi\ufb01cation, 670\ndensity classi\ufb01cation, 210\ndensity estimation, 208\u2013215\nfunction, 209\nlogistic regression, 654\nprincipal component, 547\u2013550\nstring, 668\u2013669\ntrick, 660\nKernel methods, 167\u2013176, 208\u2013215,\n423\u2013438, 659\nKnot, 141, 322\nKriging, 171\nKruskal-Shephard scaling, 570\nKullback-Leibler distance, 561\nLagrange multipliers, 293\nLandmark, 539\nLaplacian, 545\nLaplacian distribution, 72\nLAR, seeLeast angle regression\nLasso, 68\u201369, 86\u201390, 609, 635, 636,\n661fused, 666\nLatent\nfactor, 674\nvariable, 678\nLearning, 1\nLearning rate, 396\nLearning vector quantization, 462\nLeast angle regression, 73\u201379, 86,\n610\nLeast squares, 11, 32\nLeave-one-out cross-validation, 243\nLeNet, 406\nLikelihood function, 265, 273\nLinear basis expansion, 139\u2013148\nLinear combination splits, 312\nLinear discriminant function, 106\u2013\n119\nLinear methods\nfor classi\ufb01cation, 101\u2013137\nfor regression, 43\u201399\nLinear models and least squares,\n11\nLinear regression of an indicator\nmatrix, 103\nLinear separability, 129\nLinear smoother, 153\nLink function, 296\nLLE,seeLocal linear embedding\nLocal false discovery rate, 693\nLocal likelihood, 205\nLocal linear embedding, 572\nLocal methods in high dimensions,\n22\u201327\nLocal minima, 400\nLocal polynomial regression, 197\nLocal regression, 194, 200\nLocalization in time/frequency, 175\nLoess (local regression), 194, 200\nLog-linear model, 639\nLog-odds ratio (logit), 119\nLogistic (sigmoid) function, 393\nLogistic regression, 119\u2013128, 299\nLogit (log-odds ratio), 119\nLoss function, 18, 21, 219\u2013223, 346\nLoss matrix, 310", "760": "742 Index\nLossless compression, 515\nLossy compression, 515\nLVQ,seeLearning Vector Quan-\ntization\nMahalanobis distance, 441\nMajority vote, 337\nMajorization, 294, 553\nMajorize-Minimize algorithm, 294,\n584\nMAP (maximum aposteriori) es-\ntimate, 270\nMargin, 134, 418\nMarket basket analysis, 488, 499\nMarkov chain Monte Carlo (MCMC)\nmethods, 279\nMarkov graph, 627\nMarkov networks, 638\u2013648\nMARS, seeMultivariate adaptive\nregression splines\nMART, seeMultiple additive re-\ngression trees\nMaximum likelihood estimation,\n31, 261, 265\nMCMC, seeMarkov Chain Monte\nCarlo Methods\nMDL, seeMinimum description\nlength\nMean \ufb01eld approximation, 641\nMean squared error, 24, 285\nMemory-based method, 463\nMetropolis-Hastings algorithm, 282\nMinimum description length (MDL),\n235\nMinorization, 294, 553\nMinorize-Maximize algorithm, 294,\n584\nMisclassi\ufb01cation error, 17, 309\nMissing data, 276, 332\u2013333\nMissing predictor values, 332\u2013333\nMixing proportions, 214\nMixture discriminant analysis, 449\u2013\n455\nMixture modeling, 214\u2013215, 272\u2013\n275, 449\u2013455, 692Mixture of experts, 329\u2013332\nMixtures and the EM algorithm,\n272\u2013275\nMM algorithm, 294, 584\nMode seekers, 507\nModel averaging and stacking, 288\nModel combination, 289\nModel complexity, 221\u2013222\nModel selection, 57, 222\u2013223, 230\u2013\n231\nModi\ufb01ed regression, 634\nMonte Carlo method, 250, 495\nMother wavelet, 178\nMultidimensional scaling, 570\u2013572\nMultidimensional splines, 162\nMultiedit algorithm, 480\nMultilayer perceptron, 400, 401\nMultinomial distribution, 120\nMultiple additive regression trees\n(MART), 361\nMultiple hypothesis testing, 683\u2013\n693\nMultiple minima, 291, 400\nMultiple outcome shrinkage and\nselection, 84\nMultiple outputs, 56, 84, 103\u2013106\nMultiple regression from simple uni-\nvariate regression, 52\nMultiresolution analysis, 178\nMultivariate adaptive regression\nsplines (MARS), 321\u2013327\nMultivariate nonparametric regres-\nsion, 445\nNadaraya\u2013Watson estimate, 193\nNaive Bayes classi\ufb01er, 108, 210\u2013\n211, 694\nNatural cubic splines, 144\u2013146\nNearest centroids, 670\nNearest neighbor methods, 463\u2013\n483\nNearest shrunken centroids, 651\u2013\n654, 694\nNetwork diagram, 392\nNeural networks, 389\u2013416", "761": "Index 743\nNewton\u2019s method (Newton-Raphson\nprocedure), 120\u2013122\nNon-negative matrix factorization,\n553\u2013554\nNonparametric logistic regression,\n299\u2013304\nNormal (Gaussian) distribution,\n16, 31\nNormal equations, 12\nNumerical optimization, 395\u2013396\nObject dissimilarity, 505\u2013507\nOnline algorithm, 397\nOptimal scoring, 445, 450\u2013451\nOptimal separating hyperplane, 132\u2013\n135\nOptimism of the training error rate,\n228\u2013230\nOrdered categorical (ordinal) pre-\ndictor, 10, 504\nOrdered features, 666\nOrthogonal predictors, 53\nOver\ufb01tting, 220, 228\u2013230, 364\nPageRank, 576\nPairwise distance, 668\nPairwise Markov property, 628\nParametric bootstrap, 264\nPartial dependence plots, 369\u2013370\nPartial least squares, 80\u201382, 680\nPartition function, 638\nParzen window, 208\nPasting, 318\nPath algorithm, 73\u201379, 86\u201389, 432\nPatient rule induction method(PRIM),\n317\u2013321, 499\u2013501\nPeeling, 318\nPenalization, 607, seeregulariza-\ntion\nPenalized discriminant analysis, 446\u2013\n449\nPenalized polynomial regression,\n171\nPenalized regression, 34, 61\u201369, 171\nPenalty matrix, 152, 189Perceptron, 392\u2013416\nPiecewise polynomials and splines,\n36, 143\nPosterior\ndistribution, 268\nprobability, 233\u2013235, 268\nPower method, 577\nPre-conditioning, 681\u2013683\nPrediction accuracy, 329\nPrediction error, 18\nPredictive distribution, 268\nPRIM, seePatient rule induction\nmethod\nPrincipal components, 66\u201367, 79\u2013\n80, 534\u2013539, 547\nregression, 79\u201380\nsparse, 550\nsupervised, 674\nPrincipal curves and surfaces, 541\u2013\n544\nPrincipal points, 541\nPrior distribution, 268\u2013272\nProcrustes\naverage, 540\ndistance, 539\nProjection pursuit, 389\u2013392, 565\nregression, 389\u2013392\nPrototype classi\ufb01er, 459\u2013463\nPrototype methods, 459\u2013463\nProximity matrices, 503\nPruning, 308\nQR decomposition, 55\nQuadratic approximations and in-\nference, 124\nQuadratic discriminant function,\n108, 110\nRadial basis function (RBF) net-\nwork, 392\nRadial basis functions, 212\u2013214,\n275, 393\nRadial kernel, 548\nRandom forest, 409, 587\u2013604\nalgorithm, 588", "762": "744 Index\nbias, 596\u2013601\ncomparison to boosting, 589\nexample, 589\nout-of-bag ( oob), 592\nover\ufb01t, 596\nproximity plot, 595\nvariable importance, 593\nvariance, 597\u2013601\nRao score test, 125\nRayleigh quotient, 116\nReceiver operating characteristic\n(ROC) curve, 317\nReduced-rank linear discriminant\nanalysis, 113\nRegression, 11\u201314, 43\u201399, 200\u2013204\nRegression spline, 144\nRegularization, 34, 167\u2013176\nRegularized discriminant analysis,\n112\u2013113, 654\nRelevance network, 631\nRepresenter of evaluation, 169\nReproducing kernel Hilbert space,\n167\u2013176, 428\u2013429\nReproducing property, 169\nResponsibilities, 274\u2013275\nRidge regression, 61\u201368, 650, 659\nRisk factor, 122\nRobust \ufb01tting, 346\u2013350\nRosenblatt\u2019s perceptron learning\nalgorithm, 130\nRug plot, 303\nRule\ufb01t, 623\nSAM, 690\u2013693, seeSigni\ufb01cance Anal-\nysis of Microarrays\nSammon mapping, 571\nSCAD, 92\nScaling of the inputs, 398\nSchwarz\u2019s criterion, 230\u2013235\nScore equations, 120, 265\nSelf-consistency property, 541\u2013543\nSelf-organizing map (SOM), 528\u2013\n534\nSensitivity of a test, 314\u2013317\nSeparating hyperplane, 132\u2013135Separating hyperplanes, 136, 417\u2013\n419\nSeparator, 628\nShape average, 482, 540\nShrinkage methods, 61\u201369, 652\nSigmoid, 393\nSigni\ufb01cance Analysis of Microar-\nrays, 690\u2013693\nSimilarity measure, seeDissimi-\nlarity measure\nSingle index model, 390\nSingular value decomposition, 64,\n535\u2013536, 659\nsingular values, 535\nsingular vectors, 535\nSliced inverse regression, 480\nSmoother, 139\u2013156, 192\u2013199\nmatrix, 153\nSmoothing parameter, 37, 156\u2013161,\n198\u2013199\nSmoothing spline, 151\u2013156\nSoft clustering, 512\nSoft-thresholding, 653\nSoftmax function, 393\nSOM, seeSelf-organizing map\nSparse, 175, 304, 610\u2013613, 636\nadditive model, 91\ngraph, 625, 635\nSpeci\ufb01city of a test, 314\u2013317\nSpectral clustering, 544\u2013547\nSpline, 186\nadditive, 297\u2013299\ncubic, 151\u2013153\ncubic smoothing, 151\u2013153\ninteraction, 428\nregression, 144\nsmoothing, 151\u2013156\nthin plate, 165\nSquared error loss, 18, 24, 37, 219\nSRM, seeStructural risk minimiza-\ntion\nStacking (stacked generalization),\n290\nStarting values, 397\nStatistical decision theory, 18\u201322", "763": "Index 745\nStatistical model, 28\u201329\nSteepest descent, 358, 395\u2013397\nStepwise selection, 60\nStochastic approximation, 397\nStochastic search (bumping), 290\u2013\n292\nStress function, 570\u2013572\nStructural risk minimization (SRM),\n239\u2013241\nSubset selection, 57\u201360\nSupervised learning, 2\nSupervised principal components,\n674\u2013681\nSupport vector classi\ufb01er, 417\u2013421,\n654\nmulticlass, 657\nSupport vector machine, 423\u2013437\nSURE shrinkage method, 179\nSurvival analysis, 674\nSurvival curve, 674\nSVD, seeSingular value decom-\nposition\nSymmlet basis, 176\nTangent distance, 471\u2013475\nTanh activation function, 424\nTarget variables, 10\nTensor product basis, 162\nTest error, 220\u2013223\nTest set, 220\nThin plate spline, 165\nThinning strategy, 189\nTrace of a matrix, 153\nTraining epoch, 397\nTraining error, 220\u2013223\nTraining set, 219\u2013223\nTree for regression, 307\u2013308\nTree-based methods, 305\u2013317\nTrees for classi\ufb01cation, 308\u2013310\nTrellis display, 202\nUndirected graph, 625\u2013648\nUniversal approximator, 390\nUnsupervised learning, 2, 485\u2013585Unsupervised learning as super-\nvised learning, 495\u2013497\nValidation set, 222\nVapnik-Chervonenkis (VC) dimen-\nsion, 237\u2013239\nVariable importance plot, 594\nVariable types and terminology, 9\nVariance, 16, 25, 37, 158\u2013161, 219\nbetween, 114\nwithin, 114, 446\nVariance reduction, 588\nVarying coe\ufb03cient models, 203\u2013\n204\nVC dimension, seeVapnik\u2013Chervon-\nenkis dimension\nVector quantization, 514\u2013515\nVoronoi regions, 510\nWald test, 125\nWavelet\nbasis functions, 176\u2013179\nsmoothing, 174\ntransform, 176\u2013179\nWeak learner, 383, 605\nWeakest link pruning, 308\nWebpages, 576\nWebsite for book, 8\nWeight decay, 398\nWeight elimination, 398\nWeights in a neural network, 395\nWithin class covariance matrix, 114,\n446"}}