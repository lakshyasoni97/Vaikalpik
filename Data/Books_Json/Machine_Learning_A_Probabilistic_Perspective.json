{"Name": "Machine_Learning_A_Probabilistic_Perspective.pdf", "Pages": {"0": "Contents\nPreface xxvii\n1 Introduction 1\n1.1 Machine learning: what and why? 1\n1.1.1 Types of machine learning 2\n1.2 Supervised learning 3\n1.2.1 Classi\ufb01cation 3\n1.2.2 Regression 8\n1.3 Unsupervised learning 9\n1.3.1 Discovering clusters 101.3.2 Discovering latent factors 11\n1.3.3 Discovering graph structure 13\n1.3.4 Matrix completion 14\n1.4 Some basic concepts in machine learning 16\n1.4.1 Parametric vs non-parametric models 161.4.2 A simple non-parametric classi\ufb01er: K-nearest neighbors 16\n1.4.3 The curse of dimensionality 181.4.4 Parametric models for classi\ufb01cation and regression 19\n1.4.5 Linear regression 191.4.6 Logistic regression 211.4.7 Over\ufb01tting 221.4.8 Model selection 221.4.9 No free lunch theorem 24\n2 Probability 27\n2.1 Introduction 27\n2.2 A brief review of probability theory 28\n2.2.1 Discrete random variables 282.2.2 Fundamental rules 282.2.3 Bayes rule 292.2.4 Independence and conditional independence 30\n2.2.5 Continuous random variables 32\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "1": "viii CONTENTS\n2.2.6 Quantiles 33\n2.2.7 Mean and variance 33\n2.3 Some common discrete distributions 34\n2.3.1 The binomial and Bernoulli distributions 342.3.2 The multinomial and multinoulli distributions 352.3.3 The Poisson distribution 372.3.4 The empirical distribution 37\n2.4 Some common continuous distributions 38\n2.4.1 Gaussian (normal) distribution 382.4.2 Degenerate pdf 39\n2.4.3 The Laplace distribution 412.4.4 The gamma distribution 41\n2.4.5 The beta distribution 422.4.6 Pareto distribution 43\n2.5 Joint probability distributions 44\n2.5.1 Covariance and correlation 442.5.2 The multivariate Gaussian 462.5.3 Multivariate Student tdistribution 46\n2.5.4 Dirichlet distribution 47\n2.6 Transformations of random variables 49\n2.6.1 Linear transformations 492.6.2 General transformations 502.6.3 Central limit theorem 51\n2.7 Monte Carlo approximation 52\n2.7.1 Example: change of variables, the MC way 532.7.2 Example: estimating \u03c0by Monte Carlo integration 54\n2.7.3 Accuracy of Monte Carlo approximation 54\n2.8 Information theory 56\n2.8.1 Entropy 562.8.2 KL divergence 572.8.3 Mutual information 59\n3 Generative models for discrete data 65\n3.1 Introduction 65\n3.2 Bayesian concept learning 65\n3.2.1 Likelihood 673.2.2 Prior 673.2.3 Posterior 683.2.4 Posterior predictive distribution 713.2.5 A more complex prior 72\n3.3 The beta-binomial model 72\n3.3.1 Likelihood 733.3.2 Prior 74\n3.3.3 Posterior 75\n3.3.4 Posterior predictive distribution 77\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "2": "CONTENTS ix\n3.4 The Dirichlet-multinomial model 78\n3.4.1 Likelihood 793.4.2 Prior 793.4.3 Posterior 793.4.4 Posterior predictive 81\n3.5 Naive Bayes classi\ufb01ers 82\n3.5.1 Model \ufb01tting 833.5.2 Using the model for prediction 85\n3.5.3 The log-sum-exp trick 863.5.4 Feature selection using mutual information 863.5.5 Classifying documents using bag of words 87\n4 Gaussian models 97\n4.1 Introduction 97\n4.1.1 Notation 974.1.2 Basics 974.1.3 MLE for an MVN 994.1.4 Maximum entropy derivation of the Gaussian * 101\n4.2 Gaussian discriminant analysis 101\n4.2.1 Quadratic discriminant analysis (QDA) 1024.2.2 Linear discriminant analysis (LDA) 1034.2.3 Two-class LDA 1044.2.4 MLE for discriminant analysis 1064.2.5 Strategies for preventing over\ufb01tting 1064.2.6 Regularized LDA * 1074.2.7 Diagonal LDA 108\n4.2.8 Nearest shrunken centroids classi\ufb01er * 109\n4.3 Inference in jointly Gaussian distributions 110\n4.3.1 Statement of the result 1114.3.2 Examples 1114.3.3 Information form 1154.3.4 Proof of the result * 116\n4.4 Linear Gaussian systems 119\n4.4.1 Statement of the result 1194.4.2 Examples 120\n4.4.3 Proof of the result * 124\n4.5 Digression: The Wishart distribution * 125\n4.5.1 Inverse Wishart distribution 1264.5.2 Visualizing the Wishart distribution * 127\n4.6 Inferring the parameters of an MVN 127\n4.6.1 Posterior distribution of \u03bc128\n4.6.2 Posterior distribution of \u03a3* 128\n4.6.3 Posterior distribution of \u03bcand\u03a3* 132\n4.6.4 Sensor fusion with unknown precisions * 138\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "3": "x CONTENTS\n5 Bayesian statistics 149\n5.1 Introduction 149\n5.2 Summarizing posterior distributions 149\n5.2.1 MAP estimation 1495.2.2 Credible intervals 152\n5.2.3 Inference for a difference in proportions 154\n5.3 Bayesian model selection 155\n5.3.1 Bayesian Occam\u2019s razor 1565.3.2 Computing the marginal likelihood (evidence) 1585.3.3 Bayes factors 1635.3.4 Jeffreys-Lindley paradox * 164\n5.4 Priors 165\n5.4.1 Uninformative priors 1655.4.2 Jeffreys priors * 1665.4.3 Robust priors 1685.4.4 Mixtures of conjugate priors 168\n5.5 Hierarchical Bayes 171\n5.5.1 Example: modeling related cancer rates 171\n5.6 Empirical Bayes 172\n5.6.1 Example: beta-binomial model 1735.6.2 Example: Gaussian-Gaussian model 173\n5.7 Bayesian decision theory 176\n5.7.1 Bayes estimators for common loss functions 1775.7.2 The false positive vs false negative tradeoff 180\n5.7.3 Other topics * 184\n6 Frequentist statistics 191\n6.1 Introduction 191\n6.2 Sampling distribution of an estimator 191\n6.2.1 Bootstrap 192\n6.2.2 Large sample theory for the MLE * 193\n6.3 Frequentist decision theory 194\n6.3.1 Bayes risk 1956.3.2 Minimax risk 1966.3.3 Admissible estimators 197\n6.4 Desirable properties of estimators 200\n6.4.1 Consistent estimators 2006.4.2 Unbiased estimators 2006.4.3 Minimum variance estimators 201\n6.4.4 The bias-variance tradeoff 202\n6.5 Empirical risk minimization 204\n6.5.1 Regularized risk minimization 2056.5.2 Structural risk minimization 206\n6.5.3 Estimating the risk using cross validation 206\n6.5.4 Upper bounding the risk using statistical learning theory * 209\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "4": "CONTENTS xi\n6.5.5 Surrogate loss functions 210\n6.6 Pathologies of frequentist statistics * 211\n6.6.1 Counter-intuitive behavior of con\ufb01dence intervals 2126.6.2 p-values considered harmful 2136.6.3 The likelihood principle 2146.6.4 Why isn\u2019t everyone a Bayesian? 215\n7 Linear regression 217\n7.1 Introduction 217\n7.2 Model speci\ufb01cation 2177.3 Maximum likelihood estimation (least squares) 217\n7.3.1 Derivation of the MLE 2197.3.2 Geometric interpretation 2207.3.3 Convexity 221\n7.4 Robust linear regression * 2237.5 Ridge regression 225\n7.5.1 Basic idea 2257.5.2 Numerically stable computation * 2277.5.3 Connection with PCA * 2287.5.4 Regularization effects of big data 230\n7.6 Bayesian linear regression 231\n7.6.1 Computing the posterior 2327.6.2 Computing the posterior predictive 233\n7.6.3 Bayesian inference when \u03c3\n2is unknown * 234\n7.6.4 EB for linear regression (evidence procedure) 238\n8 Logistic regression 245\n8.1 Introduction 245\n8.2 Model speci\ufb01cation 2458.3 Model \ufb01tting 245\n8.3.1 MLE 2468.3.2 Steepest descent 2478.3.3 Newton\u2019s method 2498.3.4 Iteratively reweighted least squares (IRLS) 250\n8.3.5 Quasi-Newton (variable metric) methods 251\n8.3.6 /lscript\n2regularization 252\n8.3.7 Multi-class logistic regression 252\n8.4 Bayesian logistic regression 254\n8.4.1 Laplace approximation 2558.4.2 Derivation of the BIC 255\n8.4.3 Gaussian approximation for logistic regression 2568.4.4 Approximating the posterior predictive 256\n8.4.5 Residual analysis (outlier detection) * 260\n8.5 Online learning and stochastic optimization 261\n8.5.1 Online learning and regret minimization 262\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "5": "xii CONTENTS\n8.5.2 Stochastic optimization and risk minimization 262\n8.5.3 The LMS algorithm 2648.5.4 The perceptron algorithm 2658.5.5 A Bayesian view 266\n8.6 Generative vs discriminative classi\ufb01ers 267\n8.6.1 Pros and cons of each approach 268\n8.6.2 Dealing with missing data 2698.6.3 Fisher\u2019s linear discriminant analysis (FLDA) * 271\n9 Generalized linear models and the exponential family 281\n9.1 Introduction 281\n9.2 The exponential family 281\n9.2.1 De\ufb01nition 2829.2.2 Examples 2829.2.3 Log partition function 2849.2.4 MLE for the exponential family 2869.2.5 Bayes for the exponential family * 287\n9.2.6 Maximum entropy derivation of the exponential family * 289\n9.3 Generalized linear models (GLMs) 290\n9.3.1 Basics 2909.3.2 ML and MAP estimation 2929.3.3 Bayesian inference 293\n9.4 Probit regression 293\n9.4.1 ML/MAP estimation using gradient-based optimization 2949.4.2 Latent variable interpretation 2949.4.3 Ordinal probit regression * 2959.4.4 Multinomial probit models * 295\n9.5 Multi-task learning 296\n9.5.1 Hierarchical Bayes for multi-task learning 2969.5.2 Application to personalized email spam \ufb01ltering 2969.5.3 Application to domain adaptation 2979.5.4 Other kinds of prior 297\n9.6 Generalized linear mixed models * 298\n9.6.1 Example: semi-parametric GLMMs for medical data 298\n9.6.2 Computational issues 300\n9.7 Learning to rank * 300\n9.7.1 The pointwise approach 3019.7.2 The pairwise approach 3019.7.3 The listwise approach 3029.7.4 Loss functions for ranking 303\n10 Directed graphical models (Bayes nets) 307\n10.1 Introduction 307\n10.1.1 Chain rule 30710.1.2 Conditional independence 308\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "6": "CONTENTS xiii\n10.1.3 Graphical models 308\n10.1.4 Graph terminology 30910.1.5 Directed graphical models 310\n10.2 Examples 311\n10.2.1 Naive Bayes classi\ufb01ers 31110.2.2 Markov and hidden Markov models 312\n10.2.3 Medical diagnosis 31310.2.4 Genetic linkage analysis * 315\n10.2.5 Directed Gaussian graphical models * 318\n10.3 Inference 31910.4 Learning 320\n10.4.1 Plate notation 32010.4.2 Learning from complete data 32210.4.3 Learning with missing and/or latent variables 323\n10.5 Conditional independence properties of DGMs 324\n10.5.1 d-separation and the Bayes Ball algorithm (global Markov\nproperties) 324\n10.5.2 Other Markov properties of DGMs 32710.5.3 Markov blanket and full conditionals 327\n10.6 In\ufb02uence (decision) diagrams * 328\n11 Mixture models and the EM algorithm 337\n11.1 Latent variable models 337\n11.2 Mixture models 337\n11.2.1 Mixtures of Gaussians 33911.2.2 Mixture of multinoullis 34011.2.3 Using mixture models for clustering 34011.2.4 Mixtures of experts 342\n11.3 Parameter estimation for mixture models 345\n11.3.1 Unidenti\ufb01ability 34611.3.2 Computing a MAP estimate is non-convex 347\n11.4 The EM algorithm 348\n11.4.1 Basic idea 34911.4.2 EM for GMMs 35011.4.3 EM for mixture of experts 35711.4.4 EM for DGMs with hidden variables 35811.4.5 EM for the Student distribution * 35911.4.6 EM for probit regression * 36211.4.7 Theoretical basis for EM * 36311.4.8 Online EM 36511.4.9 Other EM variants * 367\n11.5 Model selection for latent variable models 370\n11.5.1 Model selection for probabilistic models 370\n11.5.2 Model selection for non-probabilistic methods 370\n11.6 Fitting models with missing data 372\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "7": "xiv CONTENTS\n11.6.1 EM for the MLE of an MVN with missing data 373\n12 Latent linear models 381\n12.1 Factor analysis 381\n12.1.1 FA is a low rank parameterization of an MVN 38112.1.2 Inference of the latent factors 382\n12.1.3 Unidenti\ufb01ability 383\n12.1.4 Mixtures of factor analysers 385\n12.1.5 EM for factor analysis models 38612.1.6 Fitting FA models with missing data 387\n12.2 Principal components analysis (PCA) 387\n12.2.1 Classical PCA: statement of the theorem 38712.2.2 Proof * 38912.2.3 Singular value decomposition (SVD) 39212.2.4 Probabilistic PCA 39512.2.5 EM algorithm for PCA 396\n12.3 Choosing the number of latent dimensions 398\n12.3.1 Model selection for FA/PPCA 398\n12.3.2 Model selection for PCA 399\n12.4 PCA for categorical data 40212.5 PCA for paired and multi-view data 404\n12.5.1 Supervised PCA (latent factor regression) 40512.5.2 Partial least squares 40612.5.3 Canonical correlation analysis 407\n12.6 Independent Component Analysis (ICA) 407\n12.6.1 Maximum likelihood estimation 41012.6.2 The FastICA algorithm 41112.6.3 Using EM 41412.6.4 Other estimation principles * 415\n13 Sparse linear models 421\n13.1 Introduction 421\n13.2 Bayesian variable selection 422\n13.2.1 The spike and slab model 424\n13.2.2 From the Bernoulli-Gaussian model to /lscript\n0regularization 425\n13.2.3 Algorithms 426\n13.3/lscript1regularization: basics 429\n13.3.1 Why does /lscript1regularization yield sparse solutions? 430\n13.3.2 Optimality conditions for lasso 43113.3.3 Comparison of least squares, lasso, ridge and subset selection 43513.3.4 Regularization path 43613.3.5 Model selection 439\n13.3.6 Bayesian inference for linear models with Laplace priors 440\n13.4/lscript\n1regularization: algorithms 441\n13.4.1 Coordinate descent 441\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "8": "CONTENTS xv\n13.4.2 LARS and other homotopy methods 441\n13.4.3 Proximal and gradient projection methods 44213.4.4 EM for lasso 447\n13.5/lscript\n1regularization: extensions 449\n13.5.1 Group Lasso 44913.5.2 Fused lasso 454\n13.5.3 Elastic net (ridge and lasso combined) 455\n13.6 Non-convex regularizers 457\n13.6.1 Bridge regression 45813.6.2 Hierarchical adaptive lasso 45813.6.3 Other hierarchical priors 462\n13.7 Automatic relevance determination (ARD)/sparse Bayesian learning (SBL) 463\n13.7.1 ARD for linear regression 46313.7.2 Whence sparsity? 46513.7.3 Connection to MAP estimation 465\n13.7.4 Algorithms for ARD * 46613.7.5 ARD for logistic regression 468\n13.8 Sparse coding * 468\n13.8.1 Learning a sparse coding dictionary 46913.8.2 Results of dictionary learning from image patches 470\n13.8.3 Compressed sensing 47213.8.4 Image inpainting and denoising 472\n14 Kernels 479\n14.1 Introduction 479\n14.2 Kernel functions 479\n14.2.1 RBF kernels 48014.2.2 Kernels for comparing documents 48014.2.3 Mercer (positive de\ufb01nite) kernels 48114.2.4 Linear kernels 48214.2.5 Matern kernels 48214.2.6 String kernels 483\n14.2.7 Pyramid match kernels 484\n14.2.8 Kernels derived from probabilistic generative models 485\n14.3 Using kernels inside GLMs 486\n14.3.1 Kernel machines 48614.3.2 L1VMs, RVMs, and other sparse vector machines 487\n14.4 The kernel trick 488\n14.4.1 Kernelized nearest neighbor classi\ufb01cation 48914.4.2 Kernelized K-medoids clustering 489\n14.4.3 Kernelized ridge regression 492\n14.4.4 Kernel PCA 493\n14.5 Support vector machines (SVMs) 496\n14.5.1 SVMs for regression 49714.5.2 SVMs for classi\ufb01cation 498\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "9": "xvi CONTENTS\n14.5.3 Choosing C504\n14.5.4 Summary of key points 50414.5.5 A probabilistic interpretation of SVMs 505\n14.6 Comparison of discriminative kernel methods 50514.7 Kernels for building generative models 507\n14.7.1 Smoothing kernels 50714.7.2 Kernel density estimation (KDE) 50814.7.3 From KDE to KNN 50914.7.4 Kernel regression 51014.7.5 Locally weighted regression 512\n15 Gaussian processes 515\n15.1 Introduction 515\n15.2 GPs for regression 516\n15.2.1 Predictions using noise-free observations 51715.2.2 Predictions using noisy observations 51815.2.3 Effect of the kernel parameters 51915.2.4 Estimating the kernel parameters 52115.2.5 Computational and numerical issues * 52415.2.6 Semi-parametric GPs * 524\n15.3 GPs meet GLMs 525\n15.3.1 Binary classi\ufb01cation 52515.3.2 Multi-class classi\ufb01cation 52815.3.3 GPs for Poisson regression 531\n15.4 Connection with other methods 532\n15.4.1 Linear models compared to GPs 532\n15.4.2 Linear smoothers compared to GPs 53315.4.3 SVMs compared to GPs 534\n15.4.4 L1VM and RVMs compared to GPs 53415.4.5 Neural networks compared to GPs 53515.4.6 Smoothing splines compared to GPs * 53615.4.7 RKHS methods compared to GPs * 538\n15.5 GP latent variable model 54015.6 Approximation methods for large datasets 542\n16 Adaptive basis function models 543\n16.1 Introduction 543\n16.2 Classi\ufb01cation and regression trees (CART) 544\n16.2.1 Basics 54416.2.2 Growing a tree 54516.2.3 Pruning a tree 54916.2.4 Pros and cons of trees 55016.2.5 Random forests 55016.2.6 CART compared to hierarchical mixture of experts * 551\n16.3 Generalized additive models 552\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "10": "CONTENTS xvii\n16.3.1 Back\ufb01tting 552\n16.3.2 Computational efficiency 55316.3.3 Multivariate adaptive regression splines (MARS) 553\n16.4 Boosting 554\n16.4.1 Forward stagewise additive modeling 55516.4.2 L2boosting 557\n16.4.3 AdaBoost 55816.4.4 LogitBoost 559\n16.4.5 Boosting as functional gradient descent 56016.4.6 Sparse boosting 56116.4.7 Multivariate adaptive regression trees (MART) 56216.4.8 Why does boosting work so well? 562\n16.4.9 A Bayesian view 563\n16.5 Feedforward neural networks (multilayer perceptrons) 563\n16.5.1 Convolutional neural networks 56416.5.2 Other kinds of neural networks 56816.5.3 A brief history of the \ufb01eld 56816.5.4 The backpropagation algorithm 569\n16.5.5 Identi\ufb01ability 57216.5.6 Regularization 572\n16.5.7 Bayesian inference * 576\n16.6 Ensemble learning 580\n16.6.1 Stacking 58016.6.2 Error-correcting output codes 581\n16.6.3 Ensemble learning is not equivalent to Bayes model averaging 581\n16.7 Experimental comparison 582\n16.7.1 Low-dimensional features 58216.7.2 High-dimensional features 583\n16.8 Interpreting black-box models 585\n17 Markov and hidden Markov models 589\n17.1 Introduction 589\n17.2 Markov models 589\n17.2.1 Transition matrix 589\n17.2.2 Application: Language modeling 591\n17.2.3 Stationary distribution of a Markov chain * 59617.2.4 Application: Google\u2019s PageRank algorithm for web page ranking * 600\n17.3 Hidden Markov models 603\n17.3.1 Applications of HMMs 604\n17.4 Inference in HMMs 606\n17.4.1 Types of inference problems for temporal models 60617.4.2 The forwards algorithm 60917.4.3 The forwards-backwards algorithm 61017.4.4 The Viterbi algorithm 612\n17.4.5 Forwards \ufb01ltering, backwards sampling 616\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "11": "xviii CONTENTS\n17.5 Learning for HMMs 617\n17.5.1 Training with fully observed data 61717.5.2 EM for HMMs (the Baum-Welch algorithm) 61817.5.3 Bayesian methods for \u201c\ufb01tting\u201d HMMs * 62017.5.4 Discriminative training 620\n17.5.5 Model selection 621\n17.6 Generalizations of HMMs 621\n17.6.1 Variable duration (semi-Markov) HMMs 62217.6.2 Hierarchical HMMs 62417.6.3 Input-output HMMs 62517.6.4 Auto-regressive and buried HMMs 62617.6.5 Factorial HMM 62717.6.6 Coupled HMM and the in\ufb02uence model 62817.6.7 Dynamic Bayesian networks (DBNs) 628\n18 State space models 631\n18.1 Introduction 631\n18.2 Applications of SSMs 632\n18.2.1 SSMs for object tracking 63218.2.2 Robotic SLAM 63318.2.3 Online parameter learning using recursive least squares 63618.2.4 SSM for time series forecasting * 637\n18.3 Inference in LG-SSM 640\n18.3.1 The Kalman \ufb01ltering algorithm 64018.3.2 The Kalman smoothing algorithm 643\n18.4 Learning for LG-SSM 646\n18.4.1 Identi\ufb01ability and numerical stability 64618.4.2 Training with fully observed data 647\n18.4.3 EM for LG-SSM 64718.4.4 Subspace methods 64718.4.5 Bayesian methods for \u201c\ufb01tting\u201d LG-SSMs 647\n18.5 Approximate online inference for non-linear, non-Gaussian SSMs 647\n18.5.1 Extended Kalman \ufb01lter (EKF) 64818.5.2 Unscented Kalman \ufb01lter (UKF) 65018.5.3 Assumed density \ufb01ltering (ADF) 652\n18.6 Hybrid discrete/continuous SSMs 655\n18.6.1 Inference 65618.6.2 Application: data association and multi-target tracking 65818.6.3 Application: fault diagnosis 65918.6.4 Application: econometric forecasting 660\n19 Undirected graphical models (Markov random \ufb01elds) 661\n19.1 Introduction 661\n19.2 Conditional independence properties of UGMs 661\n19.2.1 Key properties 661\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "12": "CONTENTS xix\n19.2.2 An undirected alternative to d-separation 663\n19.2.3 Comparing directed and undirected graphical models 664\n19.3 Parameterization of MRFs 665\n19.3.1 The Hammersley-Clifford theorem 66519.3.2 Representing potential functions 667\n19.4 Examples of MRFs 668\n19.4.1 Ising model 668\n19.4.2 Hop\ufb01eld networks 66919.4.3 Potts model 67119.4.4 Gaussian MRFs 67219.4.5 Markov logic networks * 674\n19.5 Learning 676\n19.5.1 Training maxent models using gradient methods 67619.5.2 Training partially observed maxent models 67719.5.3 Approximate methods for computing the MLEs of MRFs 67819.5.4 Pseudo likelihood 678\n19.5.5 Stochastic maximum likelihood 67919.5.6 Feature induction for maxent models * 680\n19.5.7 Iterative proportional \ufb01tting (IPF) * 681\n19.6 Conditional random \ufb01elds (CRFs) 684\n19.6.1 Chain-structured CRFs, MEMMs and the label-bias problem 68419.6.2 Applications of CRFs 68619.6.3 CRF training 692\n19.7 Structural SVMs 693\n19.7.1 SSVMs: a probabilistic view 693\n19.7.2 SSVMs: a non-probabilistic view 69519.7.3 Cutting plane methods for \ufb01tting SSVMs 69819.7.4 Online algorithms for \ufb01tting SSVMs 700\n19.7.5 Latent structural SVMs 701\n20 Exact inference for graphical models 707\n20.1 Introduction 707\n20.2 Belief propagation for trees 707\n20.2.1 Serial protocol 707\n20.2.2 Parallel protocol 709\n20.2.3 Gaussian BP * 71020.2.4 Other BP variants * 712\n20.3 The variable elimination algorithm 714\n20.3.1 The generalized distributive law * 71720.3.2 Computational complexity of VE 71720.3.3 A weakness of VE 720\n20.4 The junction tree algorithm * 720\n20.4.1 Creating a junction tree 720\n20.4.2 Message passing on a junction tree 72220.4.3 Computational complexity of JTA 725\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "13": "xx CONTENTS\n20.4.4 JTA generalizations * 726\n20.5 Computational intractability of exact inference in the worst case 726\n20.5.1 Approximate inference 727\n21 Variational inference 731\n21.1 Introduction 731\n21.2 Variational inference 732\n21.2.1 Alternative interpretations of the variational objective 733\n21.2.2 Forward or reverse KL? * 733\n21.3 The mean \ufb01eld method 735\n21.3.1 Derivation of the mean \ufb01eld update equations 73621.3.2 Example: mean \ufb01eld for the Ising model 737\n21.4 Structured mean \ufb01eld * 739\n21.4.1 Example: factorial HMM 740\n21.5 Variational Bayes 742\n21.5.1 Example: VB for a univariate Gaussian 74221.5.2 Example: VB for linear regression 746\n21.6 Variational Bayes EM 749\n21.6.1 Example: VBEM for mixtures of Gaussians * 750\n21.7 Variational message passing and VIBES 75621.8 Local variational bounds * 756\n21.8.1 Motivating applications 75621.8.2 Bohning\u2019s quadratic bound to the log-sum-exp function 75821.8.3 Bounds for the sigmoid function 76021.8.4 Other bounds and approximations to the log-sum-exp function * 76221.8.5 Variational inference based on upper bounds 763\n22 More variational inference 767\n22.1 Introduction 767\n22.2 Loopy belief propagation: algorithmic issues 767\n22.2.1 A brief history 76722.2.2 LBP on pairwise models 76822.2.3 LBP on a factor graph 769\n22.2.4 Convergence 77122.2.5 Accuracy of LBP 774\n22.2.6 Other speedup tricks for LBP * 775\n22.3 Loopy belief propagation: theoretical issues * 776\n22.3.1 UGMs represented in exponential family form 77622.3.2 The marginal polytope 77722.3.3 Exact inference as a variational optimization problem 77822.3.4 Mean \ufb01eld as a variational optimization problem 77922.3.5 LBP as a variational optimization problem 77922.3.6 Loopy BP vs mean \ufb01eld 783\n22.4 Extensions of belief propagation * 783\n22.4.1 Generalized belief propagation 783\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "14": "CONTENTS xxi\n22.4.2 Convex belief propagation 785\n22.5 Expectation propagation 787\n22.5.1 EP as a variational inference problem 78822.5.2 Optimizing the EP objective using moment matching 78922.5.3 EP for the clutter problem 79122.5.4 LBP is a special case of EP 79222.5.5 Ranking players using TrueSkill 79322.5.6 Other applications of EP 799\n22.6 MAP state estimation 799\n22.6.1 Linear programming relaxation 799\n22.6.2 Max-product belief propagation 800\n22.6.3 Graphcuts 801\n22.6.4 Experimental comparison of graphcuts and BP 80422.6.5 Dual decomposition 806\n23 Monte Carlo inference 815\n23.1 Introduction 815\n23.2 Sampling from standard distributions 815\n23.2.1 Using the cdf 81523.2.2 Sampling from a Gaussian (Box-Muller method) 817\n23.3 Rejection sampling 817\n23.3.1 Basic idea 81723.3.2 Example 81823.3.3 Application to Bayesian statistics 819\n23.3.4 Adaptive rejection sampling 81923.3.5 Rejection sampling in high dimensions 820\n23.4 Importance sampling 820\n23.4.1 Basic idea 82023.4.2 Handling unnormalized distributions 82123.4.3 Importance sampling for a DGM: likelihood weighting 82223.4.4 Sampling importance resampling (SIR) 822\n23.5 Particle \ufb01ltering 823\n23.5.1 Sequential importance sampling 82423.5.2 The degeneracy problem 82523.5.3 The resampling step 82523.5.4 The proposal distribution 82723.5.5 Application: robot localization 82823.5.6 Application: visual object tracking 82823.5.7 Application: time series forecasting 831\n23.6 Rao-Blackwellised particle \ufb01ltering (RBPF) 831\n23.6.1 RBPF for switching LG-SSMs 83123.6.2 Application: tracking a maneuvering target 83223.6.3 Application: Fast SLAM 834\n24 Markov chain Monte Carlo (MCMC) inference 837\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "15": "xxii CONTENTS\n24.1 Introduction 837\n24.2 Gibbs sampling 838\n24.2.1 Basic idea 83824.2.2 Example: Gibbs sampling for the Ising model 83824.2.3 Example: Gibbs sampling for inferring the parameters of a GMM 84024.2.4 Collapsed Gibbs sampling * 84124.2.5 Gibbs sampling for hierarchical GLMs 84424.2.6 BUGS and JAGS 84624.2.7 The Imputation Posterior (IP) algorithm 84724.2.8 Blocking Gibbs sampling 847\n24.3 Metropolis Hastings algorithm 848\n24.3.1 Basic idea 84824.3.2 Gibbs sampling is a special case of MH 84924.3.3 Proposal distributions 85024.3.4 Adaptive MCMC 85324.3.5 Initialization and mode hopping 85424.3.6 Why MH works * 85424.3.7 Reversible jump (trans-dimensional) MCMC * 855\n24.4 Speed and accuracy of MCMC 856\n24.4.1 The burn-in phase 856\n24.4.2 Mixing rates of Markov chains * 85724.4.3 Practical convergence diagnostics 85824.4.4 Accuracy of MCMC 86024.4.5 How many chains? 862\n24.5 Auxiliary variable MCMC * 863\n24.5.1 Auxiliary variable sampling for logistic regression 86324.5.2 Slice sampling 86424.5.3 Swendsen Wang 86624.5.4 Hybrid/Hamiltonian MCMC * 868\n24.6 Annealing methods 868\n24.6.1 Simulated annealing 86924.6.2 Annealed importance sampling 87124.6.3 Parallel tempering 871\n24.7 Approximating the marginal likelihood 872\n24.7.1 The candidate method 87224.7.2 Harmonic mean estimate 87224.7.3 Annealed importance sampling 873\n25 Clustering 875\n25.1 Introduction 875\n25.1.1 Measuring (dis)similarity 87525.1.2 Evaluating the output of clustering methods * 876\n25.2 Dirichlet process mixture models 879\n25.2.1 From \ufb01nite to in\ufb01nite mixture models 879\n25.2.2 The Dirichlet process 882\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "16": "CONTENTS xxiii\n25.2.3 Applying Dirichlet processes to mixture modeling 885\n25.2.4 Fitting a DP mixture model 886\n25.3 Affinity propagation 88725.4 Spectral clustering 890\n25.4.1 Graph Laplacian 89125.4.2 Normalized graph Laplacian 892\n25.4.3 Example 893\n25.5 Hierarchical clustering 893\n25.5.1 Agglomerative clustering 89525.5.2 Divisive clustering 898\n25.5.3 Choosing the number of clusters 89925.5.4 Bayesian hierarchical clustering 899\n25.6 Clustering datapoints and features 901\n25.6.1 Biclustering 90325.6.2 Multi-view clustering 903\n26 Graphical model structure learning 907\n26.1 Introduction 907\n26.2 Structure learning for knowledge discovery 908\n26.2.1 Relevance networks 90826.2.2 Dependency networks 909\n26.3 Learning tree structures 910\n26.3.1 Directed or undirected tree? 91126.3.2 Chow-Liu algorithm for \ufb01nding the ML tree structure 91226.3.3 Finding the MAP forest 912\n26.3.4 Mixtures of trees 914\n26.4 Learning DAG structures 914\n26.4.1 Markov equivalence 91426.4.2 Exact structural inference 91626.4.3 Scaling up to larger graphs 920\n26.5 Learning DAG structure with latent variables 922\n26.5.1 Approximating the marginal likelihood when we have missing data 92226.5.2 Structural EM 92526.5.3 Discovering hidden variables 926\n26.5.4 Case study: Google\u2019s Rephil 928\n26.5.5 Structural equation models * 929\n26.6 Learning causal DAGs 931\n26.6.1 Causal interpretation of DAGs 93126.6.2 Using causal DAGs to resolve Simpson\u2019s paradox 93326.6.3 Learning causal DAG structures 935\n26.7 Learning undirected Gaussian graphical models 938\n26.7.1 MLE for a GGM 93826.7.2 Graphical lasso 93926.7.3 Bayesian inference for GGM structure * 94126.7.4 Handling non-Gaussian data using copulas * 942\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "17": "xxiv CONTENTS\n26.8 Learning undirected discrete graphical models 942\n26.8.1 Graphical lasso for MRFs/CRFs 94226.8.2 Thin junction trees 944\n27 Latent variable models for discrete data 945\n27.1 Introduction 945\n27.2 Distributed state LVMs for discrete data 946\n27.2.1 Mixture models 946\n27.2.2 Exponential family PCA 947\n27.2.3 LDA and mPCA 94827.2.4 GaP model and non-negative matrix factorization 949\n27.3 Latent Dirichlet allocation (LDA) 950\n27.3.1 Basics 95027.3.2 Unsupervised discovery of topics 95327.3.3 Quantitatively evaluating LDA as a language model 95327.3.4 Fitting using (collapsed) Gibbs sampling 95527.3.5 Example 95627.3.6 Fitting using batch variational inference 95727.3.7 Fitting using online variational inference 95927.3.8 Determining the number of topics 960\n27.4 Extensions of LDA 961\n27.4.1 Correlated topic model 96127.4.2 Dynamic topic model 96227.4.3 LDA-HMM 96327.4.4 Supervised LDA 967\n27.5 LVMs for graph-structured data 970\n27.5.1 Stochastic block model 97127.5.2 Mixed membership stochastic block model 973\n27.5.3 Relational topic model 974\n27.6 LVMs for relational data 975\n27.6.1 In\ufb01nite relational model 97627.6.2 Probabilistic matrix factorization for collaborative \ufb01ltering 979\n27.7 Restricted Boltzmann machines (RBMs) 983\n27.7.1 Varieties of RBMs 985\n27.7.2 Learning RBMs 98727.7.3 Applications of RBMs 991\n28 Deep learning 995\n28.1 Introduction 995\n28.2 Deep generative models 995\n28.2.1 Deep directed networks 99628.2.2 Deep Boltzmann machines 99628.2.3 Deep belief networks 99728.2.4 Greedy layer-wise learning of DBNs 998\n28.3 Deep neural networks 999\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "18": "CONTENTS xxv\n28.3.1 Deep multi-layer perceptrons 999\n28.3.2 Deep auto-encoders 100028.3.3 Stacked denoising auto-encoders 1001\n28.4 Applications of deep networks 1001\n28.4.1 Handwritten digit classi\ufb01cation using DBNs 100128.4.2 Data visualization and feature discovery using deep auto-encoders 1002\n28.4.3 Information retrieval using deep auto-encoders (semantic hashing) 100328.4.4 Learning audio features using 1d convolutional DBNs 1004\n28.4.5 Learning image features using 2d convolutional DBNs 1005\n28.5 Discussion 1005\nNotation 1009\nBibliography 1015\nIndexes 1047\nIndex to code 1047\nIndex to keywords 1050\nMurphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n", "19": "Murphy, Kevin P.. Machine Learning : A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/jiouniv/detail.action?docID=3339490.\nCreated from jiouniv on 2023-03-30 07:21:06.\nCopyright \u00a9 2012. MIT Press. All rights reserved. \n"}}