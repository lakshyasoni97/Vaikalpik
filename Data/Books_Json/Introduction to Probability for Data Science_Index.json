{"1": {"Title": "Mathematical Background", "Sub Topics": {"1.1": {"Title": "Infinite Series", "Sub Topics": {"1.1.1": {"Title": "Geometric Series"}, "1.1.2": {"Title": "Binomial Series"}}}, "1.2": {"Title": "Approximation", "Sub Topics": {"1.2.1": {"Title": "Taylor approximation"}, "1.2.2": {"Title": "Exponential series"}, "1.2.3": {"Title": "Logarithmic approximation"}}}, "1.3": {"Title": "Integration", "Sub Topics": {"1.3.1": {"Title": "Odd and even functions"}, "1.3.2": {"Title": "Fundamental Theorem of Calculus"}}}, "1.4": {"Title": "Linear Algebra", "Sub Topics": {"1.4.1": {"Title": "Why do we need linear algebra in data science?"}, "1.4.2": {"Title": "Everything you need to know about linear algebra"}, "1.4.3": {"Title": "Inner products and norms"}, "1.4.4": {"Title": "Matrix calculus"}}}, "1.5": {"Title": "Basic Combinatorics", "Sub Topics": {"1.5.1": {"Title": "Birthday paradox"}, "1.5.2": {"Title": "Permutation"}, "1.5.3": {"Title": "Combination"}}}, "1.6": {"Title": "Summary", "Sub Topics": {}}, "1.7": {"Title": "Reference", "Sub Topics": {}}, "1.8": {"Title": "Problems", "Sub Topics": {}}}}, "2": {"Title": "Probability", "Sub Topics": {"2.1": {"Title": "Set Theory", "Sub Topics": {"2.1.1": {"Title": "Why study set theory?"}, "2.1.2": {"Title": "Basic concepts of a set"}, "2.1.3": {"Title": "Subsets"}, "2.1.4": {"Title": "Empty set and universal set"}, "2.1.5": {"Title": "Union"}, "2.1.6": {"Title": "Intersection"}, "2.1.7": {"Title": "Complement and difference"}, "2.1.8": {"Title": "Disjoint and partition"}, "2.1.9": {"Title": "Set operations"}, "2.1.10": {"Title": "Closing remarks about set theory"}}}, "2.2": {"Title": "Probability Space", "Sub Topics": {"2.2.1": {"Title": "Sample space \u2126"}, "2.2.2": {"Title": "Event space F"}, "2.2.3": {"Title": "Probability law P"}, "2.2.4": {"Title": "Measure zero sets"}, "2.2.5": {"Title": "Summary of the probability space"}}}, "2.3": {"Title": "Axioms of Probability", "Sub Topics": {"2.3.1": {"Title": "Why these three probability axioms?"}, "2.3.2": {"Title": "Axioms through the lens of measure"}, "2.3.3": {"Title": "Corollaries derived from the axioms"}}}, "2.4": {"Title": "Conditional Probability", "Sub Topics": {"2.4.1": {"Title": "Definition of conditional probability"}, "2.4.2": {"Title": "Independence"}, "2.4.3": {"Title": "Bayes\u2019 theorem and the law of total probability"}, "2.4.4": {"Title": "The Three Prisoners problem"}}}, "2.5": {"Title": "Summary", "Sub Topics": {}}, "2.6": {"Title": "References", "Sub Topics": {}}, "2.7": {"Title": "Problems", "Sub Topics": {}}}}, "3": {"Title": "Discrete Random Variables", "Sub Topics": {"3.1": {"Title": "Random Variables", "Sub Topics": {"3.1.1": {"Title": "A motivating example"}, "3.1.2": {"Title": "Definition of a random variable"}, "3.1.3": {"Title": "Probability measure on random variables"}}}, "3.2": {"Title": "Probability Mass Function", "Sub Topics": {"3.2.1": {"Title": "Definition of probability mass function"}, "3.2.2": {"Title": "PMF and probability measure"}, "3.2.3": {"Title": "Normalization property"}, "3.2.4": {"Title": "PMF versus histogram"}, "3.2.5": {"Title": "Estimating histograms from real data"}}}, "3.3": {"Title": "Cumulative Distribution Functions (Discrete)", "Sub Topics": {"3.3.1": {"Title": "Definition of the cumulative distribution function"}, "3.3.2": {"Title": "Properties of the CDF"}, "3.3.3": {"Title": "Converting between PMF and CDF"}}}, "3.4": {"Title": "Expectation", "Sub Topics": {"3.4.1": {"Title": "Definition of expectation"}, "3.4.2": {"Title": "Existence of expectation"}, "3.4.3": {"Title": "Properties of expectation"}, "3.4.4": {"Title": "Moments and variance"}}}, "3.5": {"Title": "Common Discrete Random Variables", "Sub Topics": {"3.5.1": {"Title": "Bernoulli random variable"}, "3.5.2": {"Title": "Binomial random variable"}, "3.5.3": {"Title": "Geometric random variable"}, "3.5.4": {"Title": "Poisson random variable"}}}, "3.6": {"Title": "Summary", "Sub Topics": {}}, "3.7": {"Title": "References", "Sub Topics": {}}, "3.8": {"Title": "Problems", "Sub Topics": {}}}}, "4": {"Title": "Continuous Random Variables", "Sub Topics": {"4.1": {"Title": "Probability Density Function", "Sub Topics": {"4.1.1": {"Title": "Some intuitions about probability density functions"}, "4.1.2": {"Title": "More in-depth discussion about PDFs"}, "4.1.3": {"Title": "Connecting with the PMF"}}}, "4.2": {"Title": "Expectation, Moment, and Variance", "Sub Topics": {"4.2.1": {"Title": "Definition and properties"}, "4.2.2": {"Title": "Existence of expectation"}, "4.2.3": {"Title": "Moment and variance"}}}, "4.3": {"Title": "Cumulative Distribution Function", "Sub Topics": {"4.3.1": {"Title": "CDF for continuous random variables"}, "4.3.2": {"Title": "Properties of CDF"}, "4.3.3": {"Title": "Retrieving PDF from CDF"}, "4.3.4": {"Title": "CDF: Unifying discrete and continuous random variables"}}}, "4.4": {"Title": "Median, Mode, and Mean", "Sub Topics": {"4.4.1": {"Title": "Median"}, "4.4.2": {"Title": "Mode"}, "4.4.3": {"Title": "Mean"}}}, "4.5": {"Title": "Uniform and Exponential Random Variables", "Sub Topics": {"4.5.1": {"Title": "Uniform random variables"}, "4.5.2": {"Title": "Exponential random variables"}, "4.5.3": {"Title": "Origin of exponential random variables"}, "4.5.4": {"Title": "Applications of exponential random variables"}}}, "4.6": {"Title": "Gaussian Random Variables", "Sub Topics": {"4.6.1": {"Title": "Definition of a Gaussian random variable"}, "4.6.2": {"Title": "Standard Gaussian"}, "4.6.3": {"Title": "Skewness and kurtosis"}, "4.6.4": {"Title": "Origin of Gaussian random variables"}}}, "4.7": {"Title": "Functions of Random Variables", "Sub Topics": {"4.7.1": {"Title": "General principle"}, "4.7.2": {"Title": "Examples"}}}, "4.8": {"Title": "Generating Random Numbers", "Sub Topics": {"4.8.1": {"Title": "General principle"}, "4.8.2": {"Title": "Examples"}}}, "4.9": {"Title": "Summary", "Sub Topics": {}}, "4.10": {"Title": "Reference", "Sub Topics": {}}, "4.11": {"Title": "Problems", "Sub Topics": {}}}}, "5": {"Title": "Joint Distributions", "Sub Topics": {"5.1": {"Title": "Joint PMF and Joint PDF", "Sub Topics": {"5.1.1": {"Title": "Probability measure in 2D"}, "5.1.2": {"Title": "Discrete random variables"}, "5.1.3": {"Title": "Continuous random variables"}, "5.1.4": {"Title": "Normalization"}, "5.1.5": {"Title": "Marginal PMF and marginal PDF"}, "5.1.6": {"Title": "Independent random variables"}, "5.1.7": {"Title": "Joint CDF"}}}, "5.2": {"Title": "Joint Expectation", "Sub Topics": {"5.2.1": {"Title": "Definition and interpretation"}, "5.2.2": {"Title": "Covariance and correlation coefficient"}, "5.2.3": {"Title": "Independence and correlation"}, "5.2.4": {"Title": "Computing correlation from data"}}}, "5.3": {"Title": "Conditional PMF and PDF", "Sub Topics": {"5.3.1": {"Title": "Conditional PMF"}, "5.3.2": {"Title": "Conditional PDF"}}}, "5.4": {"Title": "Conditional Expectation", "Sub Topics": {"5.4.1": {"Title": "Definition"}, "5.4.2": {"Title": "The law of total expectation"}}}, "5.5": {"Title": "Sum of Two Random Variables", "Sub Topics": {"5.5.1": {"Title": "Intuition through convolution"}, "5.5.2": {"Title": "Main result"}, "5.5.3": {"Title": "Sum of common distributions"}}}, "5.6": {"Title": "Random Vectors and Covariance Matrices", "Sub Topics": {"5.6.1": {"Title": "PDF of random vectors"}, "5.6.2": {"Title": "Expectation of random vectors"}, "5.6.3": {"Title": "Covariance matrix"}, "5.6.4": {"Title": "Multidimensional Gaussian"}}}, "5.7": {"Title": "Transformation of Multidimensional Gaussians", "Sub Topics": {"5.7.1": {"Title": "Linear transformation of mean and covariance"}, "5.7.2": {"Title": "Eigenvalues and eigenvectors"}, "5.7.3": {"Title": "Covariance matrices are always positive semi-definite"}, "5.7.4": {"Title": "Gaussian whitening"}}}, "5.8": {"Title": "Principal-Component Analysis", "Sub Topics": {"5.8.1": {"Title": "The main idea: Eigendecomposition"}, "5.8.2": {"Title": "The eigenface problem"}, "5.8.3": {"Title": "What cannot be analyzed by PCA?"}}}, "5.9": {"Title": "Summary", "Sub Topics": {}}, "5.10": {"Title": "References", "Sub Topics": {}}, "5.11": {"Title": "Problems", "Sub Topics": {}}}}, "6": {"Title": "Sample Statistics", "Sub Topics": {"6.1": {"Title": "Moment-Generating and Characteristic Functions", "Sub Topics": {"6.1.1": {"Title": "Moment-generating function"}, "6.1.2": {"Title": "Sum of independent variables via MGF"}, "6.1.3": {"Title": "Characteristic functions"}}}, "6.2": {"Title": "Probability Inequalities", "Sub Topics": {"6.2.1": {"Title": "Union bound"}, "6.2.2": {"Title": "The Cauchy-Schwarz inequality"}, "6.2.3": {"Title": "Jensen\u2019s inequality"}, "6.2.4": {"Title": "Markov\u2019s inequality"}, "6.2.5": {"Title": "Chebyshev\u2019s inequality"}, "6.2.6": {"Title": "Chernoff\u2019s bound"}, "6.2.7": {"Title": "Comparing Chernoff and Chebyshev"}, "6.2.8": {"Title": "Hoeffding\u2019s inequality"}}}, "6.3": {"Title": "Law of Large Numbers", "Sub Topics": {"6.3.1": {"Title": "Sample average"}, "6.3.2": {"Title": "Weak law of large numbers (WLLN)"}, "6.3.3": {"Title": "Convergence in probability"}, "6.3.4": {"Title": "Can we prove WLLN using Chernoff\u2019s bound?"}, "6.3.5": {"Title": "Does the weak law of large numbers always hold?"}, "6.3.6": {"Title": "Strong law of large numbers"}, "6.3.7": {"Title": "Almost sure convergence"}, "6.3.8": {"Title": "Proof of the strong law of large numbers"}}}, "6.4": {"Title": "Central Limit Theorem", "Sub Topics": {"6.4.1": {"Title": "Convergence in distribution"}, "6.4.2": {"Title": "Central Limit Theorem"}, "6.4.3": {"Title": "Examples"}, "6.4.4": {"Title": "Limitation of the Central Limit Theorem"}}}, "6.5": {"Title": "Summary", "Sub Topics": {}}, "6.6": {"Title": "References", "Sub Topics": {}}, "6.7": {"Title": "Problems", "Sub Topics": {}}}}, "7": {"Title": "Regression", "Sub Topics": {"7.1": {"Title": "Principles of Regression", "Sub Topics": {"7.1.1": {"Title": "Intuition: How to fit a straight line?"}, "7.1.2": {"Title": "Solving the linear regression problem"}, "7.1.3": {"Title": "Extension: Beyond a straight line"}, "7.1.4": {"Title": "Overdetermined and underdetermined systems"}, "7.1.5": {"Title": "Robust linear regression"}}}, "7.2": {"Title": "Overfitting", "Sub Topics": {"7.2.1": {"Title": "Overview of overfitting"}, "7.2.2": {"Title": "Analysis of the linear case"}, "7.2.3": {"Title": "Interpreting the linear analysis results"}}}, "7.3": {"Title": "Bias and Variance Trade-Off", "Sub Topics": {"7.3.1": {"Title": "Decomposing the testing error"}, "7.3.2": {"Title": "Analysis of the bias"}, "7.3.3": {"Title": "Variance"}, "7.3.4": {"Title": "Bias and variance on the learning curve"}}}, "7.4": {"Title": "Regularization", "Sub Topics": {"7.4.1": {"Title": "Ridge regularization"}, "7.4.2": {"Title": "LASSO regularization"}}}, "7.5": {"Title": "Summary", "Sub Topics": {}}, "7.6": {"Title": "References", "Sub Topics": {}}, "7.7": {"Title": "Problems", "Sub Topics": {}}}}, "8": {"Title": "Estimation", "Sub Topics": {"8.1": {"Title": "Maximum-Likelihood Estimation", "Sub Topics": {"8.1.1": {"Title": "Likelihood function"}, "8.1.2": {"Title": "Maximum-likelihood estimate"}, "8.1.3": {"Title": "Application 1: Social network analysis"}, "8.1.4": {"Title": "Application 2: Reconstructing images"}, "8.1.5": {"Title": "More examples of ML estimation"}, "8.1.6": {"Title": "Regression versus ML estimation"}}}, "8.2": {"Title": "Properties of ML Estimates", "Sub Topics": {"8.2.1": {"Title": "Estimators"}, "8.2.2": {"Title": "Unbiased estimators"}, "8.2.3": {"Title": "Consistent estimators"}, "8.2.4": {"Title": "Invariance principle"}}}, "8.3": {"Title": "Maximum A Posteriori Estimation", "Sub Topics": {"8.3.1": {"Title": "The trio of likelihood, prior, and posterior"}, "8.3.2": {"Title": "Understanding the priors"}, "8.3.3": {"Title": "MAP formulation and solution"}, "8.3.4": {"Title": "Analyzing the MAP solution"}, "8.3.5": {"Title": "Analysis of the posterior distribution"}, "8.3.6": {"Title": "Conjugate prior"}, "8.3.7": {"Title": "Linking MAP with regression"}}}, "8.4": {"Title": "Minimum Mean-Square Estimation", "Sub Topics": {"8.4.1": {"Title": "Positioning the minimum mean-square estimation"}, "8.4.2": {"Title": "Mean squared error"}, "8.4.3": {"Title": "MMSE estimate = conditional expectation"}, "8.4.4": {"Title": "MMSE estimator for multidimensional Gaussian"}, "8.4.5": {"Title": "Linking MMSE and neural networks"}}}, "8.5": {"Title": "Summary", "Sub Topics": {}}, "8.6": {"Title": "References", "Sub Topics": {}}, "8.7": {"Title": "Problems", "Sub Topics": {}}}}, "9": {"Title": "Confidence and Hypothesis", "Sub Topics": {"9.1": {"Title": "Confidence Interval", "Sub Topics": {"9.1.1": {"Title": "The randomness of an estimator"}, "9.1.2": {"Title": "Understanding confidence intervals"}, "9.1.3": {"Title": "Constructing a confidence interval"}, "9.1.4": {"Title": "Properties of the confidence interval"}, "9.1.5": {"Title": "Student\u2019s t-distribution"}, "9.1.6": {"Title": "Comparing Student\u2019s t-distribution and Gaussian"}}}, "9.2": {"Title": "Bootstrapping", "Sub Topics": {"9.2.1": {"Title": "A brute force approach"}, "9.2.2": {"Title": "Bootstrapping"}}}, "9.3": {"Title": "Hypothesis Testing", "Sub Topics": {"9.3.1": {"Title": "What is a hypothesis?"}, "9.3.2": {"Title": "Critical-value test"}, "9.3.3": {"Title": "p-value test"}, "9.3.4": {"Title": "Z-test and T-test"}}}, "9.4": {"Title": "Neyman-Pearson Test", "Sub Topics": {"9.4.1": {"Title": "Null and alternative distributions"}, "9.4.2": {"Title": "Type 1 and type 2 errors"}, "9.4.3": {"Title": "Neyman-Pearson decision"}}}, "9.5": {"Title": "ROC and Precision-Recall Curve", "Sub Topics": {"9.5.1": {"Title": "Receiver Operating Characteristic (ROC)"}, "9.5.2": {"Title": "Comparing ROC curves"}, "9.5.3": {"Title": "The ROC curve in practice"}, "9.5.4": {"Title": "The Precision-Recall (PR) curve"}}}, "9.6": {"Title": "Summary", "Sub Topics": {}}, "9.7": {"Title": "Reference", "Sub Topics": {}}, "9.8": {"Title": "Problems", "Sub Topics": {}}}}, "10": {"Title": "Random Processes", "Sub Topics": {"10.1": {"Title": "Basic Concepts", "Sub Topics": {"10.1.1": {"Title": "Everything you need to know about a random process"}, "10.1.2": {"Title": "Statistical and temporal perspectives"}}}, "10.2": {"Title": "Mean and Correlation Functions", "Sub Topics": {"10.2.1": {"Title": "Mean function"}, "10.2.2": {"Title": "Autocorrelation function"}, "10.2.3": {"Title": "Independent processes"}}}, "10.3": {"Title": "Wide-Sense Stationary Processes", "Sub Topics": {"10.3.1": {"Title": "Definition of a WSS process"}, "10.3.2": {"Title": "Properties of RX(\u03c4)"}, "10.3.3": {"Title": "Physical interpretation of RX(\u03c4)"}}}, "10.4": {"Title": "Power Spectral Density", "Sub Topics": {"10.4.1": {"Title": "Basic concepts"}, "10.4.2": {"Title": "Origin of the power spectral density"}}}, "10.5": {"Title": "WSS Process through LTI Systems", "Sub Topics": {"10.5.1": {"Title": "Review of linear time-invariant systems"}, "10.5.2": {"Title": "Mean and autocorrelation through LTI Systems"}, "10.5.3": {"Title": "Power spectral density through LTI systems"}, "10.5.4": {"Title": "Cross-correlation through LTI Systems"}}}, "10.6": {"Title": "Optimal Linear Filter", "Sub Topics": {"10.6.1": {"Title": "Discrete-time random processes"}, "10.6.2": {"Title": "Problem formulation"}, "10.6.3": {"Title": "Yule-Walker equation"}, "10.6.4": {"Title": "Linear prediction"}, "10.6.5": {"Title": "Wiener filter"}}}, "10.7": {"Title": "Summary", "Sub Topics": {}}, "10.8": {"Title": "Appendix", "Sub Topics": {"10.8.1": {"Title": "The Mean-Square Ergodic Theorem"}}}, "10.9": {"Title": "References", "Sub Topics": {}}}}}