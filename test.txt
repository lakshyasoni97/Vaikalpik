{'toc.xhtml': '', 'cover.xhtml': '', '9086c6fe-88dc-4027-938e-fc82e8d27425.xhtml': '\xa0 \xa0  \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0   ', '9d3d4223-f359-4a3d-9f0f-f1d37839432d.xhtml': '', 'eb58a7dd-2d23-469f-9de5-10e72000ee0f.xhtml': 'Copyright © 2018 Packt Publishing All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews. Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book. Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information. Commissioning Editor: Pravin Dhandre\nAcquisition Editor: Divya Poojari\nContent Development Editor: Eisha Dsouza\nTechnical Editors: Jovita Alva, Ishita Vora\nCopy Editor: Safis Editing\nProject Coordinator: Shweta H Birwatkar\nProofreader:\xa0Safis Editing\nIndexer: Priyanka Dhadke\nGraphics: Jisha Chirayil\nProduction Coordinator: Aparna Bhagat First published: May 2018 Production reference: 1240518 Published by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham\nB3 2PB, UK. ISBN 978-1-78862-111-3 www.packtpub.com ', '4aef5f3a-3f1d-4c22-bca6-16ee766e135e.xhtml': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n', 'c2fb13f3-f18f-4d3c-9855-35b93cd76292.xhtml': 'Mapt is an online digital library that gives you full access to over 5,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.', 'd318a3ad-86d1-47f9-b704-3a86ce22d558.xhtml': 'Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals Improve your learning with Skill Plans built especially for you Get a free eBook or video every month Mapt is fully searchable Copy and paste, print, and bookmark content', '089d02c7-324f-4a7d-b3b8-bd4e714be822.xhtml': 'Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at www.PacktPub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at service@packtpub.com for more details. At www.PacktPub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks. ', '197457ac-41f8-4eb1-83f3-ae08eccc01a0.xhtml': '', 'c41aafe7-68f6-42df-bfde-1e9d31d5205f.xhtml': 'Giuseppe Bonaccorso is an experienced team leader/manager in AI, machine/deep learning solution design, management, and delivery. He got his M.Sc.Eng. in Electronics in 2005 from University of Catania, Italy, and continued his studies at University of Rome Tor Vergata and University of Essex, UK. His main interests include machine/deep learning, reinforcement learning, big data, bio-inspired adaptive systems, cryptocurrencies, and NLP.Â\xa0', '0c66324b-81de-4529-bfcc-0cc1bcdccb08.xhtml': 'Francesco Azzola is an electronics engineer with over 15 years of experience in computer programming. He is the author of Android Things Projects\xa0by Packt. He loves creating IoT projects using Arduino, Raspberry Pi, Android, and other IoT platforms. He is interested in convergence of IoT and mobile applications. He is certified in\xa0SCEA, SCWCD, and SCJP. \xa0 \xa0', '56bce4a9-745f-4e17-93dc-f2cd3ad95ee2.xhtml': "If you're interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea. ", '273696f5-eee2-4751-9437-3e55ff13dec9.xhtml': "In the last few years, machine learning has become a more and more important field in the majority of industries. Many tasks once considered impossible to automate are now completely managed by computers, allowing human beings to focus on more creative tasks. This revolution has been made possible by the dramatic improvement of standard algorithms, together with a continuous reduction in hardware prices. The complexity that was a huge obstacle only a decade ago is now a problem than even a personal computer can solve. The general availability of high-level open source frameworks has allowed everybody to design and train extremely powerful models. The main goal of this book is to introduce the reader to complex techniques (such as semi-supervised and manifold learning, probabilistic models, and neural networks), balancing mathematical theory with practical examples written in Python. I wanted to keep a pragmatic approach, focusing on the applications but not neglecting the necessary theoretical foundation. In my opinion, a good knowledge of this field can be acquired only by understanding the underlying logic, which is always expressed using mathematical concepts. This extra effort is rewarded with a more solid awareness of every specific choice and helps the reader understand how to apply, modify, and improve all the algorithms in specific business contexts. Machine learning is an extremely wide field and it's impossible to cover all the topics in a book. In this case, I've done my best to cover a selection of algorithms belonging to supervised, semi-supervised, unsupervised, and Reinforcement Learning, providing all the references necessary to further explore each of them. The examples have been designed to be easy to understand without any deep insight into the code; in fact, I believe it's more important to show the general cases and let the reader improve and adapt them to cope with particular scenarios. I apologize for mistakes: even if many revisions have been made, it's possible that some details (both in the formulas and in the code) got away. I hope this book will be the starting point for many professionals struggling to enter this fascinating world with a pragmatic and business-oriented viewpoint!", 'f04e4ff7-707e-49e4-8ef1-87266c11eda3.xhtml': 'The ideal audience for this book is computer science students and professionals who want to acquire detailed knowledge of complex machine learning algorithms and applications. The approach is always pragmatic; however, the theoretical part requires some advanced mathematical skills that all graduates (in computer science, engineering, mathematics, or science) should have acquired. The book can be also utilized by more business-oriented professionals (such as CPOs and product managers) to understand how machine learning can be employed to improve existing products and businesses.', 'e18d8538-6855-4a7b-8fa4-ef2b31b311fa.xhtml': "Chapter 1, Machine Learning Model Fundamentals, explains the most important theoretical concepts regarding machine learning models, including bias, variance, overfitting, underfitting, data normalization, and cost functions. It can be skipped by those readers with a strong knowledge of these concepts. Chapter 2, Introduction to Semi-Supervised Learning, introduces the reader to the main elements of semi-supervised learning, focusing on inductive and transductive learning algorithms. Chapter 3, Graph-Based Semi-Supervised Learning, continues the exploration of semi-supervised learning algorithms belonging to the families of graph-based and manifold learning models. Label propagation and non-linear dimensionality reduction are analyzed in different contexts, providing some effective solutions that can be immediately exploited using Scikit-Learn functionalities. Chapter 4, Bayesian Networks and Hidden Markov Models, introduces the concepts of probabilistic modeling using direct acyclic graphs, Markov chains, and sequential processes. Chapter 5, EM Algorithm and Applications, explains the generic structure of the Expectation-Maximization (EM) algorithm. We discuss some common applications, such as Gaussian mixture, Principal Component Analysis, Factor Analysis, and Independent Component Analysis. This chapter requires deep mathematical knowledge; however, the reader can skip the proofs and concentrate on the final results. Chapter 6, Hebbian Learning and Self-Organizing Maps, introduces Hebb's rule, which is one of the oldest neuro-scientific concepts and whose applications are incredibly powerful. The chapter explains how a single neuron works and presents two complex models (Sanger network and Rubner-Tavan network) that can perform a Principal Component Analysis without the input covariance matrix. Chapter 7, Clustering Algorithms, introduces some common and important unsupervised algorithms, such as k-Nearest Neighbors (based on KD Trees and Ball Trees), K-means (with K-means++ initialization), fuzzy C-means, and spectral clustering. Some important metrics (such as Silhouette score/plots) are also analyzed. Chapter 8, Ensemble Learning, explains the main concepts of ensemble learning (bagging, boosting, and stacking), focusing on Random Forests, AdaBoost (with its variants), Gradient Boosting, and Voting Classifiers. Chapter 9, Neural Networks for Machine Learning, introduces the concepts of neural computation, starting with the behavior of a perceptron and continuing the analysis of multi-layer perceptron, activation functions, back-propagation, stochastic gradient descent (and the most important optimization algorithm), regularization, dropout, and batch normalization. Chapter 10,\xa0Advanced Neural Models, continues the explanation of the most important deep learning methods focusing on convolutional networks, recurrent networks, LSTM, and GRU. Chapter 11,\xa0Autoencoders, explains the main concepts of an autoencoder, discussing its application in dimensionality reduction, denoising, and data generation (variational autoencoders). Chapter 12,\xa0Generative Adversarial Networks, explains the concept of adversarial training. We focus on Deep Convolutional GANs and Wasserstein GANs. Both techniques are extremely powerful generative models that can learn the structure of an input data distribution and generate brand new samples without any additional information. Chapter 13,\xa0Deep Belief Networks, introduces the concepts of Markov random fields, Restricted Boltzmann Machines, and Deep Belief Networks. These models can be employed both in supervised and unsupervised scenarios with excellent performance. Chapter 14,\xa0Introduction to Reinforcement Learning, explains the main concepts of Reinforcement Learning (agent, policy, environment, reward, and value) and applies them to introduce policy and value iteration algorithms and Temporal-Difference Learning (TD(0)). The examples are based on a custom checkerboard environment. Chapter 15,\xa0Advanced Policy Estimation Algorithms, extends the concepts defined in the previous chapter, discussing the TD(λ) algorithm, TD(0) Actor-Critic, SARSA, and Q-Learning. A basic example of Deep Q-Learning is also presented to allow the reader to immediately apply these concepts to more complex environments.", '2e0d95c2-8a21-4cb6-a225-78c18d1f4fe0.xhtml': "There are no strict prerequisites for this book; however, it's important to have basic-intermediate Python knowledge with a specific focus on NumPy. Whenever necessary, I will provide instructions/references to install specific packages and exploit more advanced functionalities.Â\xa0As Python is based on a semantic indentation, the published version can contain incorrect newlines that raise exceptions when executing the code. For this reason, I invite all readers without deep knowledge of this language to refer to the original source code provided with the book. All the examples are based on Python 3.5+. I suggest using the Anaconda distribution (https://www.anaconda.com/download/), which is probably the most complete and powerful one for scientific projects. The majority of the required packages are already built in and it's very easy to install the new ones (sometimes with optimized versions). However, any other Python distribution can be used. Moreover, I invite readers to test the examples using Jupyter (formerly known as IPython) notebooks so as to avoid rerunning the whole example when a change is made. If instead an IDE is preferred, I suggest PyCharm, which offers many built-in functionalities that are very helpful in data-oriented and scientific projects (such as the internal Matplotlib viewer). A good mathematics background is necessary to fully understand the theoretical part. In particular, basic skills in probability theory, calculus, and linear algebra are required. However, I advise you not to give up when a concept seems too difficult. The reference sections contain many useful books, and the majority of concepts are explained quite well on Wikipedia too. When something unknown is encountered, I suggest reading the specific documentation before continuing. In many cases, it's not necessary to have complete knowledge and even an introductory paragraph can be enough to understand their rationale.", '687db04a-1ead-458e-9e78-43fefdffbaae.xhtml': "You can download the example code files for this book from your account at www.packtpub.com. If you purchased this book elsewhere, you can visit www.packtpub.com/support and register to have the files emailed directly to you. You can download the code files by following these steps: Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of: The code bundle for the book is also hosted on GitHub at\xa0https://github.com/PacktPublishing/Mastering-Machine-Learning-Algorithms.\xa0In case there's an update to the code, it will be updated on the existing GitHub repository. We also have other code bundles from our rich catalog of books and videos available at\xa0https://github.com/PacktPublishing/. Check them out!", '3da0a54e-331f-474e-9f0f-533dbb2347a8.xhtml': 'We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: http://www.packtpub.com/sites/default/files/downloads/MasteringMachineLearningAlgorithms_ColorImages.pdf.', 'c5787085-7111-4728-a46d-6c546c408602.xhtml': 'There are a number of text conventions used throughout this book. CodeInText: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: "In Scikit-Learn, it\'s possible to split the original dataset using the\xa0train_test_split()\xa0function." A block of code is set as follows: Bold: Indicates a new term, an important word, or words that you see onscreen.\xa0', 'ec18dcf2-2bf8-4d74-abaf-fd53d1527543.xhtml': 'Feedback from our readers is always welcome. General feedback: Email feedback@packtpub.com and mention the book title in the subject of your message. If you have questions about any aspect of this book, please email us at questions@packtpub.com. Errata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit www.packtpub.com/submit-errata, selecting your book, clicking on the Errata Submission Form link, and entering the details. Piracy: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at copyright@packtpub.com with a link to the material. If you are interested in becoming an author: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit authors.packtpub.com.', 'bb08a4ca-8434-4c95-9bbe-865fbfc96ffb.xhtml': 'Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you! For more information about Packt, please visitÂ\xa0packtpub.com.  ', '27d8d1e2-52f5-4ea5-a308-d05243d35d34.xhtml': "Machine learning models are mathematical systems that share many common features. Even if, sometimes, they have been defined only from a theoretical viewpoint, research advancement allows us to apply several concepts to better understand the behavior of complex systems such as deep neural networks. In this chapter, we're going to introduce and discuss some fundamental elements that some skilled readers may already know, but that, at the same time, offer several possible interpretations and applications. In particular, in this chapter we're discussing the main elements of:", '376bf55d-dc23-4ed9-b378-0bf62b730c36.xhtml': "Machine learning algorithms work with data. They create associations, find out relationships, discover patterns, generate new samples, and more, working with well-defined datasets. Unfortunately, sometimes the assumptions or the conditions imposed on them are not clear, and a lengthy training process can result in a complete validation failure. Even if this condition is stronger in deep learning contexts, we can think of a model as a gray box (some transparency is guaranteed by the simplicity of many common algorithms), where a vectorial input \xa0is transformed into a vectorial output : In the previous diagram, the model has been represented by a pseudo-function that depends on a set of parameters defined by the vector θ. In this section, we are only considering\xa0parametric\xa0models, although there's a family of algorithms that are called\xa0non-parametric, because they are based only on the structure of the data. We're going to discuss some of them in upcoming chapters. The task of a parametric learning process is therefore to find the best parameter set that maximizes a target function whose value is proportional to the accuracy (or the error, if we are trying to minimize them) of the model given a specific input X and output Y. This definition is not very rigorous, and it will be improved in the following sections; however, it's useful as a way to understand the context we're working in. Then, the first question to ask is: What is the nature of X? A machine learning problem is focused on learning abstract relationships that allow a consistent generalization when new samples are provided. More specifically, we can define a stochastic data generating process with an associated joint probability distribution:  Sometimes, it's useful to express the joint probability p(x, y) as a product of the conditional p(y|x), which expresses the probability of a label given a sample, and the marginal probability of the samples p(x). This expression is particularly useful when the prior probability p(x) is known in semi-supervised contexts, or when we are interested in solving problems using the\xa0Expectation Maximization (EM)\xa0algorithm. We're going to discuss this approach in upcoming chapters. In many cases, we are not able to derive a precise distribution; however, when considering a dataset, we always assume that it's drawn from the original data-generating distribution. This condition isn't a purely theoretical assumption, because, as we're going to see, whenever our data points are drawn from different distributions, the accuracy of the model can dramatically decrease. If we sample N\xa0independent and identically distributed (i.i.d.) values from pdata, we can create a finite dataset X made up of k-dimensional real vectors:  In a supervised scenario, we also need the corresponding labels (with t output values):  When the output has more than two classes, there are different possible strategies to manage the problem. In classical machine learning, one of the most common approaches is One-vs-All, which is based on training N different binary classifiers where each label is evaluated against all the remaining ones. In this way, N-1\xa0is performed to determine the right class. With shallow and deep neural networks, instead, it's preferable to use a softmax function to represent the output probability distribution for all classes:  This kind of output (zi represents the intermediate values, and the sum of the terms is normalized to 1) can be easily managed using the cross-entropy cost function (see the corresponding paragraph in the\xa0Loss and cost functions section).\xa0", 'e13814c1-bbe3-4d9c-82e6-f8d758f8bfca.xhtml': "Many algorithms show better performances (above all, in terms of training speed) when the dataset is symmetric (with a zero-mean). Therefore, one of the most important preprocessing steps is so-called zero-centering, which consists in subtracting the feature-wise mean Ex[X]\xa0from all samples:  This operation, if necessary, is normally reversible, and doesn't alter relationships both among samples and among components of the same sample. In deep learning scenarios, a zero-centered dataset allows exploiting the symmetry of some activation function, driving to a faster convergence (we're going to discuss these details in the next chapters). Another very important preprocessing step is called whitening, which is the operation of imposing an identity covariance matrix to a zero-centered dataset:  As the covariance matrix Ex[XTX] is real and symmetric, it's possible to eigendecompose it without the need to invert the eigenvector matrix:  The matrix V contains the eigenvectors (as columns), and the diagonal matrix\xa0Ω contains the eigenvalues. To solve the problem, we need to find a matrix A, such that:  Using the eigendecomposition previously computed, we get:  Hence, the matrix A is:  One of the main advantages of whitening is the decorrelation of the dataset, which allows an easier separation of the components. Furthermore, if X is whitened, any orthogonal transformation induced by the matrix P is also whitened:  Moreover, many algorithms that need to estimate parameters that are strictly related to the input covariance matrix can benefit\xa0from this condition, because it reduces the actual number of independent variables (in general, these algorithms work with matrices that become symmetric after applying the whitening). Another important advantage in the field of deep learning is that the gradients are often higher around the origin, and decrease in those areas where the activation functions (for example, the hyperbolic tangent or the sigmoid) saturate (|x|\xa0→\xa0∞). That's why the convergence is generally faster for whitened (and zero-centered) datasets.\xa0 In the following graph, it's possible to compare an original dataset, zero-centering,\xa0and whitening: When a whitening is needed, it's important to consider some important details. The first one is that there's a scale difference between the real sample covariance and the estimation XTX,\xa0often adopted with the singular value decomposition (SVD). The second one concerns some common classes implemented by many frameworks, like Scikit-Learn's\xa0StandardScaler. In fact, while zero-centering is a feature-wise operation, a whitening filter needs to be computed considering the whole covariance matrix (StandardScaler implements only unit variance, feature-wise scaling). Luckily, all Scikit-Learn algorithms that benefit from or need a whitening preprocessing step provide a built-in feature, so no further actions are normally required; however, for all\xa0readers who want to implement some algorithms directly, I've written two Python functions that can be used both for zero-centering and whitening. They assume a matrix X with a shape (NSamples\xa0× n). Moreover, the whiten() function accepts the parameter correct,\xa0which allows us to apply the scaling correction (the default value is True):", '527542c5-4231-4e17-8789-b15c43831c95.xhtml': "In real problems, the number of samples is limited, and it's usually necessary to split the initial set X (together with Y) into two subsets as follows: According to the nature of the problem, it's possible to choose a split percentage ratio of 70%\xa0– 30% (a good practice in machine learning, where the datasets are relatively small), or a higher training percentage (80%, 90%, up to 99%) for deep learning tasks where the number of samples is very high. In both cases, we are assuming that the training set contains all the information required for a consistent generalization. In many simple cases, this is true and can be easily verified; but with more complex datasets, the problem becomes harder. Even if we think to draw all the samples from the same distribution, it can happen that a randomly selected test set contains features that are not present in other training samples. Such a condition can have a very negative impact on global accuracy and, without other methods, it can also be very difficult to identify. This is one of the reasons why, in deep learning, training sets are huge: considering the complexity of the features and structure of the data generating distributions, choosing large test sets can limit the possibility of learning particular associations. In Scikit-Learn, it's possible to split the original dataset using the train_test_split() function, which allows specifying the train/test size, and if we expect to have randomly shuffled sets (default). For example, if we want to split X and Y, with 70% training and 30% test, we can use: Shuffling the sets is always a good practice, in order to reduce the correlation between samples. In fact, we have assumed that X is made up of i.i.d samples, but several times two subsequent samples have a strong correlation, reducing the training performance. In some cases, it's also useful to re-shuffle the training set after each training epoch; however, in the majority of our examples, we are going to work with the same shuffled dataset throughout the whole process. Shuffling has to be avoided when working with sequences and models with memory: in all those cases, we need to exploit the existing correlation to determine how the future samples are distributed.\xa0", '02dbd5eb-fa55-4237-ba62-65bbd30441ef.xhtml': "A valid method to detect the problem of wrongly selected test sets is provided by the cross-validation technique. In particular, we're going to use the K-Fold cross-validation approach. The idea is to split the whole dataset X into a moving test set and a training set (the remaining part). The size of the test set is determined by the number of folds so that, during k iterations, the test set covers the whole original dataset. In the following diagram, we see a schematic representation of the process: In this way, it's possible to assess the accuracy of the model using different sampling splits, and the\xa0training process can be performed on larger datasets; in particular, on (k-1)*N\xa0samples. In an ideal scenario, the accuracy should be very similar in all iterations; but in most real cases, the accuracy is quite below average. This means that the training set has been built excluding samples that contain features necessary to let the model fit the separating hypersurface considering the real pdata. We're going to discuss these problems later in this chapter; however, if the standard deviation of the accuracies is too high (a threshold must be set according to the nature of the problem/model), that probably means that X hasn't been drawn uniformly from pdata, and it's useful to evaluate the impact of the outliers in a preprocessing stage. In the following graph, we see the plot of a 15-fold cross-validation performed on a logistic regression: The values oscillate from 0.84 to 0.95, with an average (solid horizontal line) of 0.91. In this particular case, considering the initial purpose was to use a linear classifier, we can say that all folds yield high accuracies, confirming that the dataset is linearly separable; however, there are some samples (excluded in the ninth fold) that are necessary to achieve a minimum accuracy of about 0.88.\xa0 K-Fold cross-validation has different variants that can be employed to solve specific problems:  Scikit-Learn implements all those methods (with some other variations), but I suggest always using the\xa0cross_val_score()\xa0function, which is a helper that allows applying the different methods to a specific problem. In the following snippet based on a polynomial Support Vector Machine (SVM) and the MNIST digits dataset, the function is applied specifying the number of folds (parameter cv). In this way, Scikit-Learn will automatically use Stratified K-Fold for categorical classifications, and Standard K-Fold\xa0for all other cases: The accuracy is very high (> 0.9) in every fold, therefore we expect to have even higher accuracy using the LOO method. As we have 1,797 samples, we expect the same number of accuracies: As expected, the average score is very high, but there are still samples that are misclassified. As we're going to discuss, this situation could be a potential candidate for overfitting, meaning that the model is learning perfectly how to map the training set, but it's losing its ability to generalize; however, LOO is not a good method to measure this model ability, due to the size of the validation set. We can now evaluate our algorithm with the LPO technique. Considering what\xa0was explained before, we have selected the smaller Iris dataset and a classification based on a logistic regression. As there are N=150 samples, choosing p = 3, we get 551,300 folds: As in the previous example, we have printed only the first 100 accuracies; however, the global trend can be immediately understood with only a few values. The cross-validation technique is a powerful tool that is particularly useful when the performance cost is not too high. Unfortunately, it's not the best choice for deep learning models, where the datasets are very large and the training processes can take even days to complete. However, as we're going to discuss, in those cases the right choice (the split percentage), together with an accurate analysis of the datasets and the employment of techniques such as normalization and regularization, allows fitting models that show an excellent generalization ability.\xa0", '3804671d-c27b-4c21-a179-caee69964917.xhtml': "In this section, we're going to consider supervised models, and try to determine how it's possible to measure their theoretical potential accuracy and their ability to generalize correctly over all possible samples drawn from pdata. The majority of these concepts were developed before the deep learning age,Â\xa0but continue to have an enormous influence on research projects. The idea of capacity, for example, is an open-ended question that neuroscientists keep on asking themselves about the human brain. Modern deep learning models with dozens of layers and millions of parameters reopened the theoretical question from a mathematical viewpoint. Together with this, other elements, like the limits for the variance of an estimator, again attracted the limelight because the algorithms are becoming more and more powerful, and performances that once were considered far from any feasible solution are now a reality. Being able to train a model, so as to exploit its full capacity, maximize its generalization ability, and increase the accuracy, overcoming even human performances, is what a deep learning engineer nowadays has to expect from his work.", 'f3ab7903-3bde-44b0-b9cb-ad090724f272.xhtml': "If we consider a supervised model as a set of parameterized functions, we can defineÂ\xa0representational capacity as the intrinsic ability of a certain generic function to map a relatively large number of data distributions. To understand this concept, let's consider a function f(x) that admits infinite derivatives, and rewrite it as a Taylor expansion:  We can decide to take only the first n terms, so to have an n-degree polynomial function. Consider a simple bi-dimensional scenario with six functions (starting from a linear one); we can observe the different behavior with a small set of data points: The ability to rapidly change the curvature is proportional to the degree. If we choose a linear classifier, we can only modify its slope (the example is always in a bi-dimensional space) and the intercept. Instead, if we pick a higher-degree function, we have more possibilities to bend the curvature when it's necessary. If we consider n=1 and n=2 in the plot (on the top-right, they are the first and the second functions), with n=1, we can include the dot corresponding to x=11, but this choice has a negative impact on the dot at x=5. Only a parameterized non-linear function can solve this problem efficiently, because this simple problem requires a representational capacity higher than the one provided by linear classifiers. Another classical example is the XOR function. For a long time, several researchers opposed perceptrons (linear neural networks), because they weren't able to classify a dataset generated by the XOR function. Fortunately, the introduction of multilayer perceptrons, with non-linear functions, allowed us to overcome this problem, and many whose complexity is beyond the possibilities of any classic machine learning model.", '691bd18e-2020-4d8b-b9d4-be5540f37ef4.xhtml': "\xa0A mathematical formalization of the capacity of a classifier is provided by the Vapnik-Chervonenkis theory. To introduce the definition, it's first necessary to define the concept of shattering. If we have a class of sets C and a set M, we say that C shatters M if:  In other words, given any subset of M, it can be obtained as the intersection of a particular instance of C (cj) and M itself. If we now consider a model as a parameterized function:  We want to determine its capacity in relation to a finite dataset X:  According to the\xa0Vapnik-Chervonenkis theory, we can say that the model f shatters X if there are no classification errors for every possible label assignment. Therefore, we can define the Vapnik-Chervonenkis-capacity\xa0or VC-capacity (sometimes called VC-dimension) as the maximum cardinality of a subset of X so that f can shatter it. For example, if we consider a linear classifier in a bi-dimensional space, the VC-capacity is equal to 3, because it's always possible to label three samples so that f shatters them; however, it's impossible to do it in all situations where N > 3. The XOR problem is an example that needs a VC-capacity higher than 3. Let's explore the following plot: This particular label choice makes the set non-linearly separable. The only way to overcome this problem is to use higher-order functions (or non-linear ones). The curve lines (belonging to a classifier whose VC-capacity is greater than 3) can separate both the upper-left and the lower-right regions from the remaining space, but no straight line can do the same (while it can always separate one point from the other three).", 'cb56ffa3-fd16-4246-b59c-5fe13df033b9.xhtml': "Let's now consider a parameterized model with a single vectorial parameter (this isn't a limitation, but only a didactic choice):  The goal of a learning process is to estimate the parameter\xa0θ so as, for example, to maximize the accuracy of a classification. We define the\xa0bias of an estimator (in relation to a parameter θ):  In other words, the bias is the difference between the expected value of the estimation and the real parameter value. Remember that the estimation is a function of X, and cannot be considered a constant in the sum. An estimator is said to be unbiased\xa0if:  Moreover, the estimator is defined as\xa0consistent if the sequence of estimations converges (at least with probability 1) to the real value when k → ∞:  Given a dataset X whose samples are drawn from pdata, the accuracy of an estimator is inversely proportional to its bias. Low-bias (or unbiased) estimators are able to map the dataset X with high-precision levels, while high-bias estimators are very likely to have a capacity that isn't high enough for the problem to solve, and therefore their ability to detect the whole dynamic is poor.\xa0 Let's now compute the derivative of the bias with respect to the vector θ (it will be useful later):  Consider that the last equation, thanks to the linearity of E[•], holds also if we add a term that doesn't depend on x to the estimation of\xa0θ. In fact, in line with the laws of probability, it's easy to verify that: ", '526902b3-36c2-44db-b633-2e5f9f2d709e.xhtml': "A model with a high bias is likely to underfit the training set. Let's consider the scenario shown in the following graph: Even if the problem is very hard, we could try to adopt a linear model and, at the end of the training process, the slope and the intercept of the separating line are about -1 and 0 (as shown in the plot); however, if we measure the accuracy, we discover that it's close to 0! Independently from the number of iterations, this model will never be able to learn the association between X and Y. This condition is called underfitting, and its major indicator is a very low training accuracy. Even if some data preprocessing steps can improve the accuracy, when a model is underfitted, the only valid solution is to adopt a higher-capacity model. In a machine learning task, our goal is to achieve the maximum accuracy, starting from the training set and then moving on to the validation set. More formally, we can say that we want to improve our models so to get as close as possible toÂ\xa0Bayes accuracy. This is not a well-defined value, but a theoretical upper limit that is possible to achieve using an estimator. In the following diagram, we see a representation of this process: Bayes accuracy is often a purely theoretical limit and, for many tasks, it's almost impossible to achieve using even biological systems; however, advancements in the field of deep learning allow to create models that have a target accuracy slightly below the Bayes one. In general, there's no closed form for determining the Bayes accuracy, therefore human abilities are considered as a benchmark. In the previous classification example, a human being is immediately able to distinguish among different dot classes, but the problem can be very hard for a limited-capacity classifier. Some of the models we're going to discuss can solve this problem with a very high target accuracy, but at this point, we run another risk that can be understood after defining the concept of variance of an estimator.", '16fa4c61-b86a-4a83-8717-00f230560c51.xhtml': "At the beginning of this chapter, we have defined the data generating process pdata, and we have assumed that our dataset X has been drawn from this distribution; however, we don't want to learn existing relationships limited to X, but we expect our model to be able to generalize correctly to any other subset drawn from pdata. A good measure of this ability is provided by the variance of the estimator:  The variance can be also defined as the square of the standard error (analogously to the standard deviation). A high variance implies dramatic changes in the accuracy when new subsets are selected, because the model has probably reached a very high training accuracy through an over-learning of a limited set of relationships, and it has almost completely lost its ability to generalize.Â\xa0", '021a729b-da72-43be-9623-a04dd5f161ce.xhtml': "If underfitting was the consequence of a low capacity and a high bias, overfitting is a phenomenon that a high variance can detect. In general, we can observe a very high training accuracy (even close to the Bayes level), but not a poor validation accuracy. This means that the capacity of the model is high enough or even excessive for the task (the higher the capacity, the higher the probability of large variances), and that the training set isn't a good representation of pdata. To understand the problem, consider the following classification scenarios: The left plot has been obtained using logistic regression, while, for the right one, the algorithm is SVM with a\xa0sixth-degree polynomial kernel. If we consider the second model, the decision boundaries seem much more precise, with some samples just over them. Considering the shapes of the two subsets, it would be possible to say that a non-linear SVM can better capture the dynamics; however, if we sample another dataset from pdata and the diagonal tail becomes wider, logistic regression continues to classify the points correctly, while the SVM accuracy decreases dramatically. The second model is very likely to be overfitted, and some corrections are necessary. When the validation accuracy is much lower than the training one, a good strategy is to increase the number of training samples to consider the real pdata. In fact, it can happen that a training set is built starting from a hypothetical distribution that doesn't reflect the real one; or the number of samples used for the validation is too high, reducing the amount of information carried by the remaining samples. Cross-validation is a good way to assess the quality of datasets, but it can always happen that we find completely new subsets (for example, generated when the application is deployed in a production environment) that are misclassified, even if they were supposed to belong to pdata. If it's not possible to enlarge the training set, data augmentation could be a valid solution, because it allows creating artificial samples (for images, it's possible to mirror, rotate, or blur them) starting from the information stored in the known ones. Other strategies to prevent\xa0overfitting are based on a technique called regularization, which we're going to discuss in the last part of this chapter. For now, we can say that the effect of regularization is similar to a partial linearization, which implies a capacity reduction with a consequent variance decrease.", '5ffd4459-e7fb-4f10-95a3-8b3ccb8bb9ad.xhtml': "If it's theoretically possible to create an unbiased model (even asymptotically), this is not true for variance. To understand this concept, it's necessary to introduce an important definition: the Fisher information. If we have a parameterized model and a data-generating process pdata, we can define the likelihood function\xa0by considering the following parameters:  This function allows measuring how well the model describes the original data generating process. The shape of the likelihood can vary substantially, from well-defined, peaked curves, to almost flat surfaces. Let's consider the following graph, showing two examples based on a single parameter: We can immediately understand that, in the first case, the maximum likelihood can be easily reached by gradient ascent, because the surface is very peaked. In the second case, instead, the gradient magnitude is smaller, and it's rather easy to stop before reaching the actual maximum because of numerical imprecisions or tolerances. In worst cases, the surface can be almost flat in very large regions, with a corresponding gradient close to zero. Of course, we'd like to always work with very sharp and peaked likelihood functions, because they carry more information about their maximum. More formally, the Fisher information quantifies this value. For a single parameter, it is defined as follows:  The Fisher information is an unbounded non-negative number that is proportional to the amount of information carried by the log-likelihood; the use of logarithm has no impact on the gradient ascent, but it simplifies complex expressions by turning products into sums. This value can be interpreted as the speed of the gradient when the function is reaching the maximum; therefore, higher values imply better approximations, while a hypothetical value of zero means that the probability to determine the right parameter estimation is also null. When working with a set of K parameters, the Fisher information becomes a positive semidefinite matrix:  This matrix is symmetric, and also has another important property: when a value is zero, it means that the corresponding couple of parameters are orthogonal for the purpose of the maximum likelihood estimation, and they can be considered separately. In many real cases, if a value is close to zero, it determines a very low correlation between parameters and, even if it's not mathematically rigorous, it's possible to decouple them anyway. At this point, it's possible to introduce the Cramér-Rao bound, which states that for every unbiased estimator that adopts x (with probability distribution p(x; θ)) as a measure set, the variance\xa0of any estimator of θ is always lower-bounded according to the following inequality:  In fact, considering initially a generic estimator and exploiting Cauchy-Schwarz inequality with the variance and the Fisher information (which are both expressed as expected values), we obtain:  Now, if we use the expression for derivatives of the bias with respect to\xa0θ, considering that the expected value of the estimation of\xa0θ doesn't depend on x, we can rewrite the right side of the inequality as:  If the estimator is unbiased, the derivative on the right side is equal to zero, therefore, we get:  In other words, we can try to reduce the variance, but it will be always lower-bounded by the inverse Fisher information. Therefore, given a dataset and a model, there's always a limit to the ability to generalize. In some cases, this measure is easy to determine; however, its real value is theoretical, because it provides the likelihood function with another fundamental property: it carries all the information needed to estimate the worst case for variance. This is not surprising: when we discussed the capacity of a model, we saw how different functions could drive to higher or lower accuracies. If the training accuracy is high enough, this means that the capacity is appropriate or even excessive for the problem; however, we haven't considered the role of the likelihood p(X| θ). High-capacity models, in particular, with small or low-informative datasets, can drive to flat likelihood surfaces with a higher probability than lower-capacity models. Therefore, the Fisher information tends to become smaller, because there are\xa0more and more parameter sets that yield similar probabilities, and this, at the end of the day, drives to higher variances and an increased risk of overfitting. To conclude this section, it's useful to consider a general empirical rule derived from the Occam's razor principle: whenever a simpler model can explain a phenomenon with enough accuracy, it doesn't make sense to increase its capacity. A simpler model is always preferable (when the performance is good and it represents accurately the specific problem), because it's normally faster both in the training and in the inference phases, and more efficient. When talking about deep neural networks, this principle can be applied in a more precise way, because it's easier to increase or decrease the number of layers and neurons until the desired accuracy has been achieved.", '6259e762-01d5-4711-972a-04f723c1ba6d.xhtml': "At the beginning of this chapter, we discussed the concept of generic target function so as to optimize in order to solve a machine learning problem. More formally, in a supervised scenario, where we have finite datasets X and Y:   We can define the generic loss function for a single sample as:  J is a function of the whole parameter set, and must be proportional to the error between the true label and the predicted. Another important property is convexity. In many real cases, this is an almost impossible condition; however, it's always useful to look for convex loss functions, because they can be easily optimized through the gradient descent method. We're going to discuss this topic in Chapter 9, Neural Networks for Machine Learning. However, for now, it's useful to consider a loss function as an intermediate between our training process and a pure mathematical optimization. The missing link is the complete data. As already discussed, X is drawn from pdata, so it should represent the true distribution. Therefore, when minimizing the loss function, we're considering a potential subset of points, and never the whole real dataset. In many cases, this isn't a limitation, because, if the bias is null and the variance is small enough, the resulting model will show a good generalization ability (high training and validation accuracy); however, considering the data generating process, it's useful to introduce another measure called expected risk:  This value can be interpreted as an average of the loss function over all possible samples drawn from pdata. Minimizing the expected risk implies the maximization of the global accuracy. When working with a finite number of training samples, instead, it's common to define a cost function\xa0(often called\xa0a loss function as well, and not to be confused with the log-likelihood):  This is the actual function that we're going to minimize and, divided by the number of samples (a factor that doesn't have any impact), it's also called empirical risk, because it's an approximation\xa0(based on\xa0real data) of the expected risk. In other words, we want to find a set of parameters so that:  When the cost function has more than two parameters, it's very difficult and perhaps even impossible to understand its internal structure; however, we can analyze some potential conditions using a bidimensional diagram: The different situations we can observe are: Even if local minima are likely when the number of parameters is small, they become very unlikely when the model has a large number of parameters. In fact, an n-dimensional point\xa0θ*\xa0is a local minimum for a convex function (and here, we're assuming L to be convex) only if:  The second condition imposes a positive semi-definite Hessian matrix (equivalently, all principal minors Hn\xa0made with the first n rows and n columns must be non-negative), therefore all its\xa0eigenvalues\xa0λ0,\xa0λ1, ...,\xa0λN must be non-negative. This probability decreases with the number of parameters (H is a n×n square matrix and has n eigenvalues), and becomes close to zero in deep learning models where the number of weights can be in the order of 10,000,000 (or even more). The reader interested in a complete mathematical proof can read High Dimensional Spaces, Deep Learning and Adversarial Examples,\xa0Dube S., arXiv:1801.00634 [cs.CV]. As a consequence, a more common condition to consider is instead the presence of saddle points, where the eigenvalues have different signs and the orthogonal directional derivatives\xa0are null, even if the points are neither local maxima nor minima. Consider, for example, the following plot: The function is y=x3 whose first and second derivatives are y'=3x2 and y''=6x. Therefore,\xa0y'(0)=y''(0)=0. In this case (single-valued\xa0function), this point is also called a point of inflection, because at x=0, the function shows a change in the concavity. In three dimensions, it's easier to understand why a saddle point has been called in this way. Consider, for example, the following plot: The surface is very similar to a horse saddle, and if we project the point on an orthogonal plane,\xa0XZ\xa0is a minimum, while on another plane (YZ) it is a maximum. Saddle points are quite dangerous, because many simpler optimization algorithms can slow down and even stop, losing the ability to find the right direction. In Chapter 9, Neural Networks for Machine Learning, we're going to discuss some methods that are able to mitigate this kind of problem, allowing deep models to converge.", '32991105-fbc6-4fd7-bab2-533573f6d945.xhtml': 'In this section, we expose some common cost functionsÂ\xa0that are employed in both classification and regression tasks. Some of them will be extensively adopted in our examples in the next chapters, particularly when discussing training processes in shallow and deep neural networks.', '8dda91ea-a227-4ff5-a798-bae2876bf01d.xhtml': "Mean squared error is one of the most common regression cost functions. Its generic expression is:  This function is differentiable at every point of its domain and it's convex, so it can be optimized using the stochastic gradient descent (SGD) algorithm; however, there's a drawback when employed in regressions where there are outliers. As its value is always quadratic when the distance between the prediction and the actual value (corresponding to an outlier) is large, the relative error is high, and this can lead to an unacceptable correction.", '11d1de37-872b-407e-903f-802981d786d3.xhtml': "As explained, mean squared error isn't robust to outliers, because it's always quadratic independently of the distance between actual value and prediction. To overcome this problem, it's possible to employ the Huber cost function, which is based on threshold tH, so that for distances less than\xa0tH, its behavior is quadratic, while for a distance greater than\xa0tH, it becomes linear, reducing the entity of the error and, therefore, the relative importance of the outliers. The analytical expression is: ", 'f823f9f4-d8ff-4905-81d8-7347b1ccece5.xhtml': "This cost function is adopted by SVM, where the goal is to maximize the distance between the separation boundaries (where the support vector lies). It's analytic expression is:  Contrary to the other examples, this cost function is not optimized using classic stochastic gradient descent methods, because it's not differentiable at all points where:  For this reason, SVM algorithmsÂ\xa0are optimized using quadratic programming techniques.", 'a559f341-4277-4904-9df3-0d4f48f75707.xhtml': "Categorical cross-entropy is the most diffused classification cost function, adopted by logistic regression and the majority of neural architectures. The generic analytical expression is:  This cost function is convex and can be easily optimized using stochastic gradient descent techniques; moreover, it has another important interpretation. If we are training a classifier, our goal is to create a model whose distribution is as similar as possible to pdata. This condition can be achieved by minimizing the Kullback-Leibler divergence between the two distributions:  In the previous expression, pM is the distribution generated by the model. Now, if we rewrite the divergence, we get:  The first term is the entropy of the data-generating distribution, and it doesn't depend on the model parameters, while the second one is the cross-entropy. Therefore, if we minimize the cross-entropy, we also minimize the Kullback-Leibler divergence, forcing the model to reproduce a distribution that is very similar to pdata. This is a very elegant explanation as to why the cross-entropy cost function is an excellent choice for classification problems.", '8314a347-d6da-4dcd-8a93-8728d91fcfaa.xhtml': "When a model is ill-conditioned or prone to overfitting, regularization offers some valid tools to mitigate the problems. From a mathematical viewpoint, a regularizer is a penalty added to the cost function, so to impose an extra-condition on the evolution of the parameters:  The parameter\xa0λ controls the strength of the regularization, which is expressed through the function g(θ). A fundamental condition on g(θ) is that it must be differentiable so that the new composite cost function can still be optimized using SGD algorithms. In general, any regular function can be employed; however, we normally need a function that can contrast the indefinite growth of the parameters. To understand the principle, let's consider the following diagram: In the first diagram, the model is linear and has two parameters, while in the second one, it is quadratic and has three parameters. We already know that the second option is more prone to overfitting, but if we apply a regularization term, it's possible to avoid the growth of a (first quadratic parameter), transforming the model into a linearized version. Of course, there's a difference between choosing a lower-capacity model and applying a regularization constraint. In fact, in the first case, we are renouncing the possibility offered by the extra capacity, running the risk of increasing the bias, while with regularization we keep the same model but optimize it so to reduce the variance. Let's now explore the most common regularization techniques.", 'd136418b-b44c-49ec-8e3c-e797b4ea54e9.xhtml': "Ridge regularization (also known as Tikhonov regularization) is based on the squared L2-norm of the parameter vector:  This penalty avoids an infinite growth of the parameters (for this reason, it's also known as weight shrinkage), and it's particularly useful when the model is ill-conditioned, or there is multicollinearity, due to the fact that the samples are completely independent (a relatively common condition). In the following diagram, we see a schematic representation of the Ridge regularization in a bidimensional scenario: The zero-centered circle represents the Ridge boundary, while the shaded surface is the original cost function. Without regularization, the minimum (w1, w2) has a magnitude (for example, the distance from the origin) which is\xa0about double the one obtained by applying a Ridge constraint, confirming the expected shrinkage. When applied to regressions solved with the Ordinary Least Squares (OLS) algorithm, it's possible to prove that there always exists a Ridge coefficient, so that the weights are shrunk with respect the OLS ones. The same result, with some restrictions, can be extended to other cost functions.\xa0", '736ec8f0-b285-446b-8236-3982382c3b57.xhtml': "Lasso regularization is based on the L1-norm of the parameter vector:  Contrary to Ridge, which shrinks all the weights, Lasso can shift the smallest one to zero, creating a sparse parameter vector. The mathematical proof is beyond the scope of this book; however, it's possible to understand it intuitively by considering the following diagram (bidimensional): The zero-centered square represents the Lasso boundaries. If we consider a generic line, the probability of being tangential to the square is higher at the corners, where at least one (exactly one in a bidimensional scenario) parameter is null. In general, if we have a vectorial convex function f(x) (we provide a definition of convexity in Chapter 5, EM Algorithm and Applications), we can define:  As any Lp-norm is convex, as well as the sum of convex functions,\xa0g(x) is also convex. The regularization term is always non-negative, therefore the minimum corresponds to the norm of the null vector. When minimizing g(x), we need to also consider the contribution of the gradient of the norm in the ball centered in the origin where, however, the partial derivatives don't exist. Increasing the value of p, the norm becomes smoothed around the origin, and the partial derivatives approach zero for |xi|\xa0→ 0. On the other side, with p=1 (excluding the L0-norm and all the norms with p\xa0∈ ]0, 1[ that allow an even stronger sparsity, but are non-convex), the partial derivatives are always +1 or -1, according to the sign of xi (xi ≠ 0). Therefore, it's easier for the L1-norm to push the smallest components to zero, because the contribution to the minimization (for example, with a gradient descent) is independent of xi,\xa0while an L2-norm\xa0decreases its speed when approaching the origin. This is a non-rigorous explanation of the sparsity achieved using the L1-norm. In fact, we also need to consider the term f(x), which bounds the value of the global minimum; however, it may help the reader to develop an intuitive understanding of the concept. It's possible to find further and mathematically rigorous details in Optimization for Machine Learning, (edited by) Sra S., Nowozin S., Wright S. J., The MIT Press. Lasso regularization is particularly useful whenever a sparse representation of a dataset is needed. For example, we could be interested in finding the feature vectors corresponding to a group of images. As we expect to have many features but only a subset present in each image, applying the Lasso regularization allows forcing all the smallest coefficients to become null, suppressing the presence of the secondary features. Another potential application is latent semantic analysis, where our goal is to describe the documents belonging to a corpus in terms of a limited number of topics. All these methods can be summarized in a technique called sparse coding, where the objective is to reduce the dimensionality of a dataset (also in non-linear scenarios) by extracting the most representative atoms, using different approaches to achieve sparsity.\xa0", 'bf8501db-3d1e-4107-8849-1b3d16b34a35.xhtml': "In many real cases, it's useful to apply both Ridge and Lasso regularization in order to force weight shrinkage and a global sparsity. It is possible by employing the ElasticNet regularization, defined as:  The strength of each regularization is controlled by the parameters λ1 and\xa0λ2. ElasticNet can yield excellent results whenever it's necessary to mitigate overfitting effects while encouraging sparsity. We are going to apply all the regularization techniques when discussing some deep learning architectures.", '35e7963e-69a7-4776-bb0d-0e11bd174762.xhtml': "Even though it's a pure regularization technique, early stopping is often considered as a last resort when all other approaches to prevent overfitting and maximize validation accuracy fail. In many cases (above all, in deep learning scenarios), it's possible to observe a typical behavior of the training process considering both training and the validation cost functions: During the first epochs, both costs decrease, but it can happen that after a threshold epoch es, the validation cost starts increasing. If we continue with the training process, this results in overfitting the training set and increasing the variance. For this reason, when there are no other options, it's possible to prematurely stop the training process. In order to do so, it's necessary to store the last parameter vector before the beginning of a new iteration and, in the case of no improvements or the accuracy worsening, to stop the process and recover the last parameters. As explained, this procedure must never be considered as the best choice, because a better model or an improved dataset could yield higher performances. With early stopping, there's no way to verify alternatives, therefore it must be adopted only at the last stage of the process and never at the beginning. Many deep learning frameworks such as Keras include helpers to implement an early stopping callback; however, it's important to check whether the last parameter vector is the one stored before the last epoch or the one corresponding to es. In this case, it could be useful to repeat the training process, stopping it at the epoch previous to es (where the minimum validation cost has been achieved).", '9205fdf4-c59f-4654-8330-cc194c488086.xhtml': "In this chapter, we discussed fundamental concepts shared by almost any machine learning model. In the first part, we have introduced the data generating process, as a generalization of a finite dataset. We explained which are the most common strategies to split a finite dataset into a training block and a validation set, and we introduced cross-validation, with some of the most important variants, as one of the best approaches to avoid the limitations of a static split. In the second part, we discussed the main properties of an estimator: capacity, bias, and variance. We also introduced the\xa0Vapnik-Chervonenkis theory, which is a mathematical formalization of the concept of representational capacity, and we analyzed the effects of high biases and high variances. In particular, we discussed effects called underfitting and overfitting, defining the relationship with high bias and high variance. In the third part, we introduced the loss and cost functions, first as proxies of the expected risk, and then we detailed some common situations that can be experienced during an optimization problem. We also exposed some common cost functions, together with their main features. In the last part, we discussed regularization, explaining how it can mitigate the effects of overfitting. In the next chapter, Chapter 2,\xa0Introduction to Semi-Supervised Learning, we're going to introduce semi-supervised learning, focusing our attention on the concepts of transductive and inductive learning.", 'e910c028-718a-483b-aec7-8d701904ba18.xhtml': "Semi-supervised learning is a machine learning branch that tries to solve problems with both labeled and unlabeled data with an approach that employs concepts belonging to clustering and classification methods. The high availability of unlabeled samples, in contrast with the difficulty of labeling huge datasets correctly, drove many researchers to investigate the best approaches that allow extending the knowledge provided by the labeled samples to a larger unlabeled population without loss of accuracy. In this chapter, we're going to introduce this branch and, in particular, we will discuss:", 'fb2e45fd-681f-4f4a-98d3-72f213ae3eee.xhtml': "A typical semi-supervised scenario is not very different from a supervised one. Let's suppose we have a data generating process, pdata:  However, contrary to a supervised approach, we have only a limited number N of samples drawn from pdata and provided with a label, as follows:  Instead, we have a larger amount (M) of unlabeled samples drawn from the marginal distribution p(x):  In general, there are no restrictions on the values of N and M; however, a semi-supervised problem arises when the number of unlabeled samples is much higher than the number of complete samples. If we can draw N >> M labeled samples from pdata, it's probably useless to keep on working with semi-supervised approaches and preferring classical supervised methods is likely to be the best choice. The extra complexity we need is justified by M >> N, which is a common condition in all those situations where the amount of available unlabeled data is large, while the number of correctly labeled samples is quite a lot lower. For example, we can easily access millions of free images but detailed labeled datasets are expensive and include only a limited subset of possibilities. However, is it always possible to apply semi-supervised learning to improve our models? The answer to this question is almost obvious: unfortunately no. As a rule of thumb, we can say that if the knowledge of Xu increases our knowledge about the prior distribution p(x), a semi-supervised algorithm is likely to perform better than a purely supervised (and thus limited to Xl) counterpart. On the other hand, if the unlabeled samples are drawn from different distributions, the final result can be quite a lot worse. In real cases, it's not so immediately necessary to decide whether a semi-supervised algorithmÂ\xa0is the best choice; therefore, cross-validation and comparisons are the best practices to employ when evaluating a scenario.", 'fc7a4068-a4b4-44da-a309-31c3d6de7bd2.xhtml': "When a semi-supervised model is aimed at finding the labels for the unlabeled samples, the approach is called transductive learning. In this case, we are not interested in modeling the whole distribution p(x|y), which implies determining the density of both datasets, but rather in finding p(y|x) only for the unlabeled points. In many cases, this strategy can be time-saving and it's always preferable when our goal is more oriented at improving our knowledge about the unlabeled dataset.", '7190223f-5286-4780-8adc-792f072c9ee8.xhtml': "Contrary to transductive learning, inductive learningÂ\xa0considers all the X samples and tries to determine a complete p(x|y) or a function y=f(x) that can map both labeled and unlabeled points to their corresponding labels. In general, this method is more complex and requires more computational time; therefore, according to Vapnik's principle, if not required or necessary, it's always better to pick the most pragmatic solution and, possibly, expand it if the problem requires further details.", '0e24b122-e880-48e6-93ec-f6a2a343fa2a.xhtml': "As explained in the previous section, semi-supervised learning is not guaranteed to improve a supervised model. A wrong choice could lead to a dramatic worsening in performance; however, it's possible to state some fundamental assumptions which are required for semi-supervised learning to work properly. They are not always mathematically proven theorems, but rather empirical observations that justify the choice of an approach otherwise completely arbitrary.", 'caf11176-f54f-4db6-9737-468e4f46dcec.xhtml': "Let's consider a real-valued function f(x) and the corresponding metric spaces X and Y. Such a function is said to be Lipschitz-continuous if:  In other words, if two points x1 and x2 are near, the corresponding output values y1 and\xa0y2 cannot be arbitrarily far from each other. This condition is fundamental in regression problems where a generalization is often required for points that are\xa0between training samples. For example, if we need to predict the output for a point xt :\xa0x1\xa0< xt < x2 and the regressor is Lipschitz-continuous, we can be sure that yt will be correctly bounded by\xa0y1 and y2. This condition is often called general smoothness, but in semi-supervised it's useful to add a restriction (correlated with the cluster assumption): if two points are in a\xa0high density region (cluster) and they are close, then the corresponding outputs must be close too. This extra condition is very important because, if two samples are in a low density region they can belong to different clusters and their labels can be very different. This is not always true, but it's useful to include this constraint to allow some further assumptions in many definitions of semi-supervised models.", '9d54a3c0-a064-4734-8726-ff47ee00b27e.xhtml': "This assumption is strictly linked to the previous one and it's probably easier to accept. It can be expressed with a chain of interdependent conditions. Clusters are high density regions; therefore, if two points are close, they are likely to belong to the\xa0same cluster and their labels must be the same. Low density regions are separation spaces; therefore, samples belonging to a low density region are likely to be boundary points and their classes can be different. To better understand this concept, it's useful to think about supervised SVM: only the support vectors should be in low density regions. Let's consider the following bidimensional example: In a semi-supervised scenario, we couldn't know the label of a point belonging to a high density region; however, if it is close enough to a labeled point that it's possible to build a ball where all the points have the same average density, we are allowed to predict the label of our test sample. Instead, if we move to a\xa0low-density region, the process becomes harder, because two points can be very close but with different labels. We are going to discuss the semi-supervised, low-density separation problem at the end of this chapter.", '1f332c14-fc1d-416a-97c8-f22647c45888.xhtml': "This is the less intuitive assumption, but it can be extremely useful to reduce the complexity of many problems. First of all, we can provide a non-rigorous definition of a manifold. An n-manifold is a topological space that is globally curved, but locally homeomorphic to an n-dimensional Euclidean space. In the following diagram, there's an example of a\xa0manifold: the surface of a sphere in\xa0ℜ3: The small patch around P (for\xa0ε\xa0→ 0) can be mapped to a flat circular surface. Therefore, the properties of a manifold are locally based on the Euclidean geometry, while, globally, they need a proper mathematical extension which is beyond the scope of this book (further information can be found in Semi-supervised learning on Riemannian manifolds,\xa0Belkin M., Niyogi P., Machine Learning 56, 2004). The manifold assumption states that p-dimensional samples (where p >> 1) approximately lie on a q-dimensional manifold with p << q. Without excessive mathematical rigor, we can say that, for example, if we have N 1000-dimensional bounded vectors, they are enclosed into a 1000-dimensional hypercube with edge-length equal to r. The corresponding n-volume is rp\xa0= r1000, therefore, the probability of filling the entire space is very small (and decreases with p). What we observe, instead, is a high density on a lower dimensional manifold. For example, if we look at the Earth from space, we might think that its inhabitants are uniformly distributed over the whole volume. We know that this is false and, in fact, we can create maps and atlases which are represented on two-dimensional manifolds. It doesn't make sense to use three-dimensional vectors to map the position of a human being. It's easier to use a projection and work with latitude and longitude. This assumption\xa0authorizes us to apply dimensionality reduction methods in order to avoid the Curse of Dimensionality, theorized by Bellman (in Dynamic Programming and Markov Process, Ronald A. Howard, The MIT Press).\xa0In the scope of machine learning, the main consequence of such an effect is that when the dimensionality of the samples increases, in order to achieve a high accuracy, it's necessary to use more and more samples. Moreover, Hughes observed (the phenomenon has been named after him and it's presented in the paper Hughes G. F., On the mean accuracy of statistical pattern recognizers, IEEE Transactions on Information Theory, 1968, 14/1) that the accuracy of statistical classifiers is inversely proportional to the dimensionality of the samples. This means that whenever it's possible to work on lower dimensional manifolds (in particular in semi-supervised scenarios), two advantages are achieved:", '6ef4cb08-82de-4123-989f-7b97385f54fc.xhtml': "Generative Gaussian mixtures is an inductive algorithm for semi-supervised clustering. Let's suppose we have a labeled dataset (Xl, Yl) containing N samples (drawn from pdata)\xa0and an unlabeled dataset Xu containing M >> N samples (drawn from the marginal distribution p(x)). It's not necessary that M >> N, but we want to create a real semi-supervised scenario, with only a few labeled samples. Moreover, we are assuming that all unlabeled samples are consistent with pdata. This can seem like a vicious cycle, but without this assumption, the procedure does not have a strong mathematical foundation. Our goal is to determine a complete p(x|y) distribution using a generative model. In general, it's possible to use different priors, but we are now employing multivariate Gaussians to model our data:  Thus, our model parameters are\xa0means and covariance matrices for all Gaussians. In other contexts, it's possible to use binomial or multinomial distributions. However, the procedure doesn't change; therefore, let's assume that it's possible to approximate p(x|y) with a parametrized distribution p(x|y, θ). We can achieve this goal by minimizing the Kullback-Leibler divergence between the two distributions:  In Chapter 5,\xa0EM Algorithm and Applications we are going to show that this is equivalent to maximizing the likelihood of the dataset. To obtain the likelihood, it's necessary to define the number of expected Gaussians (which is known from the labeled samples) and a weight-vector that represents the marginal probability of a specific Gaussian:  Using the Bayes' theorem, we get:  As we are working with both labeled and unlabeled samples, the previous expression has a double interpretation: With this distinction, we can consider a single log-likelihood function where the term fw(yi|xj) has been substituted by a per sample weight:  It's possible to maximize the log-likelihood using the EM algorithm (see Chapter 5, EM Algorithm and Applications). In this context, we provide the steps directly:    N is the total number of samples. The procedure must be iterated until the parameters stop modifying or the modifications are lower than a fixed threshold.", '40581017-86be-4135-8b5c-9182fe5a68e8.xhtml': "We can now implement this model in Python using a simple bidimensional dataset, created using the make_blobs() function provided by Scikit-Learn: We have created 1,000 samples belonging to 2 classes. 750 points have then been randomly selected to become our unlabeled dataset (the corresponding class has been set to -1). We can now initialize two Gaussian distributions by defining their mean, covariance, and weight. One possibility is to use random values: However, as the covariance matrices must be positive semi definite, it's useful to alter the random values (by multiplying each matrix by the corresponding transpose) or to set hard-coded initial parameters. In this case, we could pick the following example: The resulting plot is shown in the following graph, where the small diamonds represent the unlabeled points and the bigger dots, the samples belonging to the known classes: The two Gaussians are represented by the concentric ellipses. We can now execute the training procedure. For simplicity, we repeat the update for a fixed number of iterations. The reader can easily modify the code in order to introduce a threshold: The first thing at the beginning of each cycle is to initialize the Pij matrix that will be used to store the\xa0p(yi|xj,θ,w) values. Then, for each sample, we can compute\xa0p(yi|xj,θ,w) considering whether it's labeled or not. The Gaussian probability is computed using the SciPy function multivariate_normal.pdf(). When the whole Pij matrix has been populated, we can update the parameters (means and covariance matrix) of both Gaussians and the relative weights. The algorithm is very fast; after five iterations, we get the stable state represented in the following graph: The two Gaussians have perfectly mapped the space by setting their parameters so as to cover the high-density regions. We can check for some unlabeled points, as follows: It's easy to locate them in the previous plot. The corresponding classes can be obtained through the last Pij matrix: This immediately verifies that they have been correctly labeled and assigned to the right cluster. This algorithm is very fast and produces excellent results in terms of density estimation. In Chapter 5,\xa0EM Algorithm and Applications, we are going to discuss a general version of this algorithm, explaining the complete training procedure based on the EM algorithm.", '8a3b6be5-ce22-4157-b721-d7c97d7381cb.xhtml': 'In the previous example, we have considered a single log-likelihood for both labeled and unlabeled samples: This is equivalent to saying that we trust the unlabeled points just like the labeled ones. However, in some contexts, this assumption can lead to completely wrong estimations, as shown in the following graph: In this case, the means and covariance matrices of both Gaussian distributions have been biased by the unlabeled points and the resulting density estimation is clearly wrong. When this phenomenon happens, the best thing to do is to consider a double weighted log-likelihood. If the first N samples are labeled and the following M are unlabeled, the log-likelihood can be expressed as follows:  In the previous formula, the term\xa0λ, if less than 1, can underweight the unlabeled terms, giving more importance to the labeled dataset. The modifications to the algorithm are trivial because each unlabeled weight has to be scaled according to\xa0λ, reducing its estimated probability. In\xa0Semi-Supervised Learning,\xa0Chapelle O.,\xa0Schölkopf B., Zien A., (edited by), The MIT Press,\xa0the reader can find a very detailed discussion about the choice of\xa0λ. There are no golden rules; however, a possible strategy could be based on the cross-validation performed on the labeled dataset. Another (more complex) approach is to consider different increasing values of\xa0λ and pick the first one where the log-likelihood is maximum. I recommend the aforementioned book for further details and strategies.', '6246b03c-f2fe-4c72-a98f-8f991e5a4cbe.xhtml': "As explained at the beginning of this chapter, in many real life problems, it's cheaper to retrieve unlabeled samples, rather than correctly labeled ones. For this reason, many researchers worked to find out the best strategies to carry out a semi-supervised classification that could outperform the supervised counterpart. The idea is to train a classifier with a few labeled samples and then improve its accuracy after adding weighted unlabeled samples. One of the best results is the Contrastive Pessimistic Likelihood Estimation (CPLE) algorithm, proposed by M. Loog (in Loog M.,\xa0Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification, arXiv:1503.00269). Before explaining this algorithm, an introduction is necessary. If we have a labeled dataset (X, Y) containing N samples, it's possible to define the log-likelihood cost function of a generic estimator, as follows:  After training the model, it should be possible to determine p(yi|xi, θ), which is the probability of a label given a sample xi. However, some classifiers are not based on this approach (like SVM) and evaluate the right class, for example, by checking the sign of a parametrized function f(xi,\xa0θ). As CPLE is a generic framework that can be used with any classification algorithm when the probabilities are not available, it's useful to implement a technique called Platt scaling, which allows transforming the decision function into a probability through a parametrized sigmoid. For a binary classifier, it can be expressed as follows:  α and\xa0β are parameters that must be learned in order to maximize the likelihood. Luckily Scikit-Learn provides the method predict_proba(), which returns the probabilities for all classes. Platt scaling is performed automatically or on demand; for example, the SCV classifier needs to have the parameter probability=True in order to compute the probability mapping. I always recommend checking the documentation before implementing a custom solution. We can consider a full dataset, made up of labeled and unlabeled samples. For simplicity, we can reorganize the original dataset, so that the first N samples are labeled, while the next M are unlabeled:  As we don't know the labels for all xu samples, we can decide to use M k-dimensional (k is the number of classes)\xa0soft-labels qi that can be optimized during the training process:  The second condition in the previous formula is necessary to guarantee that each qi represents a discrete probability (all the elements must sum up to 1.0). The complete log-likelihood cost function can, therefore, be expressed as follows:  The first term represents the log-likelihood for the supervised part, while the second one is responsible for the unlabeled points. If we train a classifier with only the labeled samples, excluding the second addend, we get a parameter set\xa0θsup. CPLE defines a contrastive condition (as a log-likelihood too), by defining the improvement in the total cost function given by the semi-supervised approach, compared to the supervised solution:  This condition allows imposing that the semi-supervised solution must outperform the supervised one, in fact, maximizing it; we both increase the first term and reduce the second one, obtaining a proportional increase of CL (the term contrastive is very common in machine learning and it normally indicates a condition which is achieved as the difference between two opposite constraints). If CL doesn't increase, it probably means that the unlabeled samples have not been drawn from the marginal distribution p(x) extracted from pdata. Moreover, in the previous expression, we have implicitly used soft-labels, but as they are initially randomly chosen and there's no ground truth to support their values, it's a good idea not to trust them by imposing a pessimistic condition (as another log-likelihood):  By imposing this constraint, we try to find the soft-labels that minimize the contrastive log-likelihood; that's why this is defined as a pessimistic approach. It can seem a contradiction; however, trusting soft-labels can be\xa0dangerous, because the semi-supervised log-likelihood could be increased even with a large percentage of misclassification. Our goal is to find the best parameter set that is able to guarantee the highest accuracy starting from the supervised baseline (which has been obtained using the labeled samples) and improving it, without forgetting the structural features provided by the labeled samples. Therefore, our final goal can be expressed as follows: ", '8a45ed4d-3abe-4c93-b35b-ee784e5eb75a.xhtml': "We are going to implement the CPLE algorithm in Python using a subset extracted from the MNIST dataset. For simplicity, we are going to use only the samples representing the digits 0 and 1: After creating the restricted dataset (X, Y) which contain 360 samples, we randomly select 150 samples (about 42%) to become unlabeled (the corresponding y is -1). At this point, we can measure the performance of logistic regression trained only on the labeled dataset: So, the logistic regression shows 57% accuracy for the classification of the unlabeled samples. We can also evaluate the cross-validation score on the whole dataset (before removing some random labels): Thus, the classifier achieves an average 48% accuracy when using 10 folds (each test set contains 36 samples) if all the labels are known. We can now implement a CPLE algorithm. The first thing is to initialize a LogisticRegression instance and the soft-labels: q0 is a random array of values bounded in the half-open interval [0, 1]; therefore, we also need a converter to transform qi into an actual binary label:  We can achieve this using the NumPy function np.vectorize(), which allows us to apply a transformation to all the elements of a vector: In order to compute the log-likelihood, we need also a weighted log-loss (similar to the Scikit-Learn function log_loss(), which, however, computes the negative log-likelihood but doesn't support weights): This function computes the following expression:  We need also a function to build the dataset with variable soft-labels qi: At this point, we can define our contrastive log-likelihood: This method will be called by the optimizer, passing a different q vector each time. The first step is building the new dataset and computing Y_soft, which are the labels corresponding to q. Then the logistic regression classifier is trained with with the dataset (as Y_n is a (k, 1) array, it's necessary to squeeze it to avoid a warning. The same thing is done when using Y as a boolean indicator). At this point, it's possible to compute both psup and psemi using the method predict_proba() and, finally, we can compute the semi-supervised and supervised log-loss, which is the term, a function of qi, that we want to minimize, while the maximization of\xa0θ is done implicitly when training the logistic regression. The optimization is carried out using the BFGS algorithm implemented in SciPy: This is a very fast algorithm, but the user is encouraged to experiment with methods or libraries. The two parameters we need in this case are f, which is the function to minimize, and x0, which is the initial condition for the independent variables. maxiter is useful for avoiding an excessive number of iterations when no improvements are achieved. Once the optimization is complete, q_end contains the optimal soft-labels. We can, therefore, rebuild our dataset: With this final configuration, we can retrain the logistic regression and check the cross-validation accuracy: The semi-supervised solution based on the CPLE algorithms achieves an average 84% accuracy, outperforming, as expected, the supervised approach. The reader can try other examples using different classifiers, such SVM or Decision Trees, and verify when CPLE allows obtaining higher accuracy than other supervised algorithms.", '72cfae7a-e50a-452a-81f2-86b84e4f8bc5.xhtml': "When we discussed the cluster assumption, we also defined the low-density regions as boundaries and the corresponding problem as low-density separation. A common supervised classifier which is based on this concept is a Support Vector Machine (SVM), the objective of which is to maximize the distance between the dense regions where the samples must be. For a complete description of linear and kernel-based SVMs, please refer to Bonaccorso G., Machine Learning Algorithms, Packt Publishing; however, it's useful to remind yourself of the basic model for a linear SVM with slack variables\xa0ξi:  This model is based on the assumptions that yi can be either -1 or 1. The slack variables\xa0ξi\xa0or soft-margins are variables, one for each sample, introduced to reduce the strength\xa0imposed by the original condition (min ||w||), which is based on a hard margin that misclassifies all the samples that are on the wrong side. They are defined by the Hinge loss, as follows:  With those variables, we allow some points to overcome the limit without being misclassified if they remain within a distance controlled by the corresponding slack variable (which is also minimized during the training phase, so as to avoid uncontrollable growth). In the following diagram, there's a schematic representation of this process: The last\xa0elements of each high-density regions are the support vectors. Between them, there's a low-density region (it can also be zero-density in some cases) where our separating hyperplane lies. In Chapter 1,\xa0Machine Learning Model Fundamentals, we defined the concept of empirical risk as a proxy for expected risk; therefore, we can turn the SVM problem into the minimization of empirical risk under the Hinge cost function (with or without Ridge Regularization on w):  Theoretically, every function which is always bounded by two hyperplanes containing the support vectors is a good classifier, but we need to minimize the empirical risk (and, so, the expected risk); therefore we look for the maximum margin between high-density regions. This model is able to separate two dense regions with irregular boundaries and, by adopting a kernel function, also in non-linear scenarios. The natural question, at this point, is about the best strategy to integrate labeled and unlabeled samples when we need to solve this kind of problem in a semi-supervised scenario. The first element to consider is the ratio: if we have a low percentage of unlabeled points, the problem is mainly supervised and the generalization ability learned using the training set should be enough to correctly classify all the unlabeled points. On the other hand, if the number of unlabeled samples is much larger, we return to an almost pure clustering scenario (like the one discussed in the paragraph about the Generative Gaussian mixtures). In order to exploit the strength of semi-supervised methods in low-density separation problems, therefore, we should consider situations where the ratio labeled/unlabeled is about 1.0. However, even if we have the predominance of a class (for example, if we have a huge unlabeled dataset and only a few labeled samples), it's always possible to use the algorithms we're going to discuss, even if, sometimes, their performance could be equal to or lower than a pure supervised/clustering solution. Transductive SMVs, for example, showed better accuracies when the labeled/unlabeled ratio is very small, while other methods can behave in a completely different way. However, when working with semi-supervised learning (and its assumptions), it is always important to bear in mind that each problem is supervised and unsupervised at the same time and the best solution must be evaluated in every different context. A solution for this problem is offered by the Semi-Supervised SVM (also known as S3VM) algorithm. If we have N labeled samples and M unlabeled samples, the objective function becomes as follows:  The first term imposes the standard SVM condition about the maximum separation distance, while the second block is divided\xa0into two parts: The constraints necessary to solve the problems become as follows:  The first constraint is limited to the labeled points and it's the same as a supervised SVM. The following two, instead, take into account the possibility that an unlabeled sample could be classified as +1 or -1. Let's suppose, for example, that the label yj\xa0for the sample xj\xa0should be +1 and the first member of the second inequality is a positive\xa0number K (so the corresponding term of the third equation is -K). It's easy to verify that the first slack variable is\xa0ξi\xa0≥ 1 - K, while the second one is zj\xa0≥ 1 + K. Therefore, in the objective,\xa0ξi\xa0is chosen to be minimized. This method is inductive and yields good (if not excellent) performances; however, it has a very high computational cost and should be solved using optimized (native) libraries. Unfortunately, it is a non-convex problem and there are no standard methods to solve it so it always reaches the optimal configuration.", 'f26b4943-8e5f-45ac-ac5c-99e0a8a2ea28.xhtml': "We now implement an S3VM in Python using the SciPy optimization methods, which are mainly\xa0based on C and FORTRAN implementations. The reader can try it with other libraries such as NLOpt and LIBSVM and compare the results. A possibility suggested by Bennet and Demiriz is to use the L1-norm for w, so as to linearize the objective\xa0function; however, this choice seems to produce good results only for small datasets. We are going to keep the original formulation based on the L2-norm, using an Sequential Least Squares Programming (SLSQP) algorithm to optimize the objective.\xa0 Let's start by creating a bidimensional dataset with both labeled and unlabeled samples: For simplicity (and without any impact, because the samples are shuffled), we set last 200 samples as unlabeled (y = 0). The corresponding plot is shown in the following graph: The crosses represent unlabeled points, which are spread throughout the entire dataset. At this point we need to initialize all variables required for the optimization problem: As the optimization algorithm requires a single array, we have stacked all vectors into a horizontal array theta0 using the np.hstack() function. We also need to vectorize the min() function in order to apply it to arrays: Now, we can define the objective function: The arguments are the current theta vector and the complete datasets Xd and Yd. The dot product of w has been multiplied by 0.5 to keep the conventional notation used for supervised SVMs. The constant can be omitted without any impact. At this point, we need to define all the constraints, as they are based on the slack variables; each function (which shares the same parameters of the objectives) is parametrized with an index, idx. The labeled constraint is as follows: The unlabeled constraints, instead, are as follows: They are parametrized with the current theta vector, the Xd dataset, and an idx index. We need also to include the constraints for each slack variable (≥ 0): We can now set up the problem using the SciPy convention: Each constraint is represented with a dictionary, where type is set to ineq to indicate that it is an inequality, fun points to the callable object and args contains all extra arguments (theta is the main x variable and it's automatically added). Using SciPy, it's possible to minimize the objective using the Sequential Least Squares Programming (SLSQP) or Constraint Optimization by Linear Approximation\xa0(COBYLA) algorithms. We preferred the former, because it works more rapidly and is more stable: After the training process is complete, we can compute the labels for the unlabeled points: In the next graph, it's possible to compare the initial plot (left) with the final one where all points have been assigned a label (right): As you can see, S3VM succeeded in finding the right label for all unlabeled points, confirming the existence of two very dense regions for\xa0x between [0, 2] (square dots)\xa0and y between [0, 2] (circular dots).", '3752d50b-0562-444e-af94-758825b6bb57.xhtml': "Another approach to the same problem is offered by the TSVM, proposed by T. Joachims (in Transductive Inference for Text Classification using Support Vector Machines,\xa0Joachims T., ICML Vol. 99/1999). The idea is to keep the original objective with two sets of slack variables: the first for the labeled samples and the other for the unlabeled ones:  As this is a transductive approach, we need to consider the unlabeled samples as variable-labeled ones (subject to the learning process), imposing a constraint similar to the supervised points. As for the previous algorithm, we assume we have N labeled samples and M unlabeled ones; therefore, the conditions become as follows:  The first constraint is the classical SVM one and it works only on labeled samples. The second one uses the variable y(u)j with the corresponding slack variables\xa0ξj to impose a similar condition on the unlabeled samples, while the third one is necessary to constrain the labels to being equal to -1 and 1. Just like the semi-supervised SVMs, this algorithm is non-convex and it's useful to try different methods to optimize it. Moreover, the author, in the aforementioned paper, showed how TSVM works better when the test set (unlabeled) is large and the training set (labeled) is relatively small (when a standard supervised SVM is outperformed). On the other hand, with large training sets and small test sets, a supervised SVM (or other algorithms) are always preferable because they are faster and yield better accuracy.", 'a071ccf2-bd7d-4b13-8160-571b19123eac.xhtml': "In our Python implementation, we are going to use a bidimensional dataset similar to one employed in the previous method; however, in this case, we impose 400 unlabeled samples out of a total of 500 points: The corresponding plot is shown in the following graph: The procedure is similar to the one we used before. First of all, we need to initialize our variables: In this case, we also need to define the y_unlabeled vector for variable-labels. The author also suggests using two C constants (C_labeled and C_unlabeled) in order to be able to weight the misclassification of labeled and unlabeled samples differently. We used a value of 1.0 for C_labeled and 10.0 for C_unlabled, because we want to penalize more the misclassification of unlabeled samples. The objective function to optimize is as follows: While the labeled and unlabeled constraints are as follows: We need also to impose the constraints on the slack variables and on the y(u): As in the previous example, we can create the constraint dictionary needed by SciPy: In this case, the last constraint is an equality, because we want to force\xa0y(u) to be equal either to -1 or 1. At this point, we minimize the objective function: When the process is complete, we can compute the labels for the unlabeled samples and compare the plots: The plot comparison is shown in the following graph: The misclassification (based on the density distribution) is slightly higher than S3VM, but it's possible to change the C values and the optimization method until the expected result has been reached. A good benchmark is provided by a supervised SVM, which can have better performances when the training set is huge enough (and when it represents the whole pdata correctly). It's interesting to evaluate different combinations of the C parameters, starting from a standard supervised SVM. The dataset is smaller, with a high number of unlabeled samples: We use the standard SVM implementation provided by Scikit-Learn (the\xa0SVC() class) with a linear kernel and C=1.0: The SVM is trained with the labeled samples and the vector yu_svc contains the prediction for the unlabeled samples. The resulting plot (in comparison with the original dataset) is shown in the following graph: All the labeled samples are represented with bigger squares and circles. The result meets our expectations, but there's an area (X [-1, 0] - Y [-2, -1]), where the SVM decided to impose the circle class even if the unlabeled points are close to a square. This hypothesis can't be acceptable considering the clustering assumption; in fact, in a high-density region there are samples belonging to two classes. A similar (or even worse) result is obtained using an S3VM with CL=10 and CU=5: In this case, the classification accuracy is lower because the penalty for the unlabeled samples is lower than the one imposed on the labeled points. A supervised SVM has obviously better performances. Let's try\xa0with CL=10 and CU=50: Now, the penalty is quite a lot higher for the unlabeled samples and the result appears much more reasonable considering the clustering assumption. All the high-density regions are coherent and separated by low-density ones. These examples show how the value chosen for the parameters and the optimization method can dramatically change the result. My suggestion is to test several configurations (on sub-sampled datasets), before picking the final one. In Semi-Supervised Learning,\xa0Chapelle O.,\xa0Schölkopf B., Zien A., (edited by), The MIT Press, there are further details about possible optimization strategies, with strengths and weaknesses.", '63a9bfcb-2921-442a-9416-e084af85a8d7.xhtml': "In this chapter, we introduced semi-supervised learning, starting from the scenario and the assumptions needed to justify the approaches. We discussed the importance of the smoothness assumption when working with both supervised and semi-supervised classifiers in order to guarantee a reasonable generalization ability. Then we introduced the clustering assumption, which is strictly related to the geometry of the datasets and allows coping with density estimation problems with a strong structural condition. Finally, we discussed the manifold assumption and its importance in order to avoid the curse of dimensionality. The chapter continued by introducing a generative and inductive model: Generative Gaussian mixtures, which allow clustering labeled and unlabeled samples starting from the assumption that the prior probabilities are modeled by multivariate Gaussian distributions. The next topic was about a very important algorithm: contrastive pessimistic likelihood estimation, which is an inductive, semi-supervised classification framework that can be adopted together with any supervised classifier. The main concept is to define a contrastive log-likelihood based on soft-labels (representing the probabilities for the unlabeled samples) and impose a pessimistic condition in order to minimize the trust in the soft-labels. The algorithm can find the best configuration that maximizes the log-likelihood, taking into account both labeled and unlabeled samples. Another inductive classification approach is provided by the S3VM algorithm, which is an extension of the classical SVM approach, based on two extra optimization constraints to address the unlabeled samples. This method is relatively powerful, but it's non-convex and, therefore, very sensitive to the algorithms employed to minimize the objective function. An alternative to\xa0S3VM is provided by the TSVM, which tries to minimize the objective with a condition based on variable labels. The problem is, hence, divided into two parts: the supervised one, which is exactly the same as standard SVM, and the semi-supervised one, which has a similar structure but without fixed y labels. This problem is non-convex too and it's necessary to evaluate different optimization strategies to find the best trade-off between accuracy and computational complexity. In the reference section, there are some useful resources so you can examine all these problems in depth and find a suitable solution for each particular scenario. In the next chapter, Chapter 3,\xa0Graph-Based Semi-Supervised Learning we're continuing this exploration by discussing some important algorithms based on the structure underlying the dataset. In particular, we're going to employ graph theory to perform the propagation of labels to unlabeled samples and to reduce the dimensionality of datasets in non-linear contexts.", '080898fe-d12f-4919-b88e-4269133000be.xhtml': "In this chapter, we continue our discussion about semi-supervised learning, considering a family of algorithms that is based on the graph obtained from the dataset and the existing relationships among samples. The problems that we are going to discuss belong to two main categories: the propagation of class labels to unlabeled samples and the use of non-linear techniques based on the manifold assumption to reduce the dimensionality of the original dataset. In particular, this chapter covers the following propagation algorithms: For the manifold learning section, we're discussing:", 'd9a62a92-66f1-499b-b80e-6954a0cba091.xhtml': "Label propagation is a family of semi-supervised algorithms based on a graph representation of the dataset. In particular, if we have N labeled points (with bipolar labels +1 and -1) and M unlabeled points (denoted by y=0), it's possible to build an undirected graph based on a measure of geometric affinity among samples. If G = {V, E} is the formal definition of the graph, the set of vertices is made up of sample labels V = { -1, +1, 0 }, while the edge set is based on an affinity matrix W (often called adjacency matrix when the graph is unweighted), which depends only on the X values, not on the labels. In the following graph, there's an example of such a structure: In the preceding example graph, there are four labeled points (two with y=+1\xa0and two with y=-1), and two unlabeled points (y=0). The affinity matrix is normally symmetric and square with dimensions equal to (N+M) x (N+M). It can be obtained with different approaches. The most common ones, also adopted by Scikit-Learn, are:   Sometimes, in the radial basis function kernel, the parameter γ is represented as the reciprocal of 2σ²; however, small γ values corresponding to a large variance increase the radius, including farther points and smoothing the class over a number of samples, while large γ values restrict the boundaries to a subset that tends to a single sample. Instead, in the k-nearest neighbors kernel, the parameter k controls the number of samples to consider as neighbors. To describe the basic algorithm, we also need to introduce the degree matrix (D):  It is a diagonal matrix where each non-null element represents the degree of the corresponding vertex. This can be the number of incoming edges, or a measure proportional to it (as in the case of W based on the radial basis function). The general idea of label propagation is to let each node propagate its label to its neighbors and iterate the procedure until convergence. Formally, if we have a dataset containing both labeled and unlabeled samples:  The complete steps of the label propagation algorithm (as proposed by Zhu and Ghahramani in Learning from Labeled and Unlabeled Data with Label Propagation, Zhu X., Ghahramani Z., CMU-CALD-02-107) are:  The first update performs a propagation step with both labeled and unlabeled points. Each label is spread from a node through its outgoing edges, and the corresponding weight, normalized with the degree, increases or decreases the effect of each contribution. The second command instead resets all y values for the labeled samples. The final labels can be obtained as:  The proof of convergence is very easy. If we partition the matrix D-1W according to the relationship among labeled and unlabeled samples, we get:  If we consider that only the first N components of Y are non-null and they are clamped at the end of each iteration, the matrix can be rewritten as:  We are interested in proving the convergence for the part regarding the unlabeled samples (the labeled ones are fixed), so we can write the update rule as:  Transforming the recursion into an iterative process, the previous formula becomes:  In the previous expression, the second term is null, so we need to prove that the first term converges; however, it's easy to recognize a truncated matrix geometrical series (Neumann series), and AUU is constructed to have all eigenvalues |λi| < 1, therefore the series converges to: ", '444d85ce-b88f-4181-a63a-378aa1da8b17.xhtml': "We can implement the algorithm in Python, using a test bidimensional dataset: As in the other examples, we set y = 0 for all unlabeled samples (75 out of 100). The corresponding plot is shown in the following graph: The dots marked with a cross are unlabeled. At this point, we can define the affinity matrix. In this case, we compute it using both methods: The KNN matrix is obtained using the Scikit-Learn function\xa0kneighbors_graph() with the parameters\xa0n_neighbors=2 and mode='connectivity'; the alternative is 'distance', which returns the distances instead of 0 and 1 to indicate the absence/presence of an edge. The\xa0include_self=True\xa0parameter is useful, as we want to have Wii = 1. For the RBF matrix, we need to define it manually: The default value for\xa0γ is 10, corresponding to a standard deviation σ equal to 0.22. When using this method, it's important to set a correct value for\xa0γ;\xa0otherwise, the propagation can degenerate in the predominance of a class (γ too small). Now, we can compute the degree matrices and its inverse. As the procedure is identical, from this point on we continue using the RBF affinity matrix: The algorithm is implemented using a variable threshold. The value adopted here is 0.01: The final result is shown in the following double plot: As it's possible to see, in the original dataset there's a round dot surrounded by square ones (-0.9, -1). As this algorithm keeps the original labels, we find the same situation after the propagation of labels. This condition could be acceptable, even if both the smoothness and clustering assumptions are contradicted. Assuming that it's reasonable, it's possible to force a correction\xa0by relaxing the algorithm: In this way, we don't reset the original labels, letting the propagation change all those values that disagree with the neighborhood. The result is shown in the following plot:", 'be2131ff-0daf-47c3-8380-4cae20ef2429.xhtml': "Scikit-Learn implements a slightly different algorithm proposed by\xa0Zhu and Ghahramani (in the aforementioned paper) where the affinity matrix W can be computed using both methods (KNN and RBF), but it is normalized to become a probability transition matrix:  The algorithm operates like a Markov random walk, with the following sequence (assuming that there are Q different labels):  The first update performs a label propagation step. As we're working with probabilities, it's necessary (second step) to renormalize the rows so that their element sums up to 1. The last update resets the original labels for all labeled samples. In this case, it means imposing a P(label=yi) = 1 to the corresponding label, and setting all the others to zero. The proof of convergence is very similar to the one for label propagation algorithms, and can be found in\xa0Learning from Labeled and Unlabeled Data with Label Propagation,\xa0Zhu X., Ghahramani Z., CMU-CALD-02-107.\xa0The most important result is that the solution can be obtained in closed form (without any iteration) through this formula: The first term is the sum of a generalized geometric series, where Puu is the unlabeled-unlabeled part of the transition matrix P. Pul, instead, is the unlabeled-labeled part of the same matrix. For our Python example, we need to build the dataset differently, because Scikit-Learn considers a sample unlabeled if y=-1: We can now train a LabelPropagation instance with an RBF kernel and gamma=10.0: The result is shown in the following double plot: As expected, the propagation converged to a solution that respects both the smoothness and the clustering assumption.", '29a0a20e-ebfc-4c3d-be24-1d5ceec66cd4.xhtml': "The last algorithm (proposed by Zhou et al.) that we need to analyze is called label spreading, and it's based on the normalized graph Laplacian:  This matrix has each a diagonal element lii equal to 1, if the degree deg(lii)\xa0> 0 (0 otherwise) and all the other elements equal to:  The behavior of this matrix is analogous to a discrete Laplacian operator, whose real-value version is the fundamental element of all diffusion equations. To better understand this concept, let's consider the generic heat equation:  This equation describes the behavior of the temperature of a room when a point is suddenly heated. From basic physics concepts, we know that heat will spread until the temperature reaches an equilibrium point and the speed of variation is proportional to the Laplacian of the distribution. If we consider a bidimensional grid at the equilibrium (the derivative with respect to when time becomes null) and we discretize the Laplacian operator (∇2 =\xa0∇\xa0·\xa0∇) considering the incremental ratios, we obtain:  Therefore, at the equilibrium, each point has a value that is the mean of the direct neighbors. It's possible to prove the finite-difference equation has a single fixed point that can be found iteratively, starting from every initial condition. In addition to this idea, label spreading adopts a clamping factor\xa0α for the labeled samples. If\xa0α=0, the algorithm will always reset the labels to the original values (like for label propagation), while with a value in the interval (0, 1], the percentage of clamped labels decreases progressively until\xa0α=1, when all the labels are overwritten. The complete steps of the label spreading algorithm are:  It's possible to show (as demonstrated in\xa0Semi-Supervised Learning,\xa0Chapelle O.,\xa0Schölkopf B., Zien A., (edited by), The MIT Press) that this algorithm is equivalent to the minimization of a quadratic cost function with the following structure:  The first term imposes consistency between original labels and estimated ones (for the labeled samples). The second term acts as a normalization factor, forcing the unlabeled terms to become zero, while the third term, which is probably the least intuitive, is needed to guarantee geometrical coherence in terms of smoothness. As we have seen in the previous paragraph, when a hard-clamping is adopted, the smoothness assumption could be violated. By minimizing this term (μ is proportional to\xa0α), it's possible to penalize the rapid changes inside\xa0the high-density regions. Also in this case, the proof of convergence is very similar to the one for label propagation algorithms, and will be omitted. The interested reader can find it in\xa0Semi-Supervised Learning,\xa0Chapelle O.,\xa0Schölkopf B., Zien A., (edited by),\xa0The MIT Press.", 'a5d9e94d-2279-447e-add0-c87fa9d965ca.xhtml': "We can test this algorithm using the Scikit-Learn implementation. Let's start by creating a very dense dataset: We can train a LabelSpreading instance with a clamping factor alpha=0.2. We want to preserve 80% of the original labels but, at the same time, we need a smooth solution: The result is shown, as usual, together with the original dataset: As it's possible to see in the first figure (left), in the central part of the cluster (x [-1, 0]), there's an area of circle dots. Using a hard-clamping, this aisle\xa0would remain unchanged, violating both the smoothness and clustering assumptions. Setting\xa0α > 0, it's possible to avoid this problem. Of course, the choice of\xa0α is strictly correlated with each single problem. If we know that the original labels are absolutely correct, allowing the algorithm to change them can be counterproductive. In this case, for example, it would be better to preprocess the dataset, filtering out all those samples that violate the semi-supervised assumptions. If, instead, we are not sure that all samples are drawn from the same pdata, and it's possible to be in the presence of spurious elements, using a higher\xa0α value can smooth the dataset without any other operation.", 'e459fce6-a265-4726-be88-68d0bc3b0c1f.xhtml': 'The goal of this algorithm proposed by\xa0Zhu and Ghahramani is to find the probability distribution of target labels for unlabeled samples given a mixed dataset. This objective is achieved through the simulation of a stochastic process, where each unlabeled sample walks through the graph until it reaches a stationary absorbing state, a labeled sample where it stops acquiring the corresponding label. The main difference with other similar approaches is that in this case, we consider the probability of reaching a labeled sample. In this way, the problem acquires a closed form and can be easily solved. The first step is to always build a k-nearest neighbors graph with all N samples, and define a weight matrix W based on an RBF kernel:  Wij = 0 is xi,\xa0and xj are not neighbors and Wii = 1. The transition probability matrix, similarly to the Scikit-Learn label propagation algorithm, is built as:  In a more compact way, it can be rewritten as P = D-1W. If we now consider a test sample, starting from the state xi\xa0and randomly walking until an absorbing labeled state is found (we call this label y∞), the probability (referred to as binary classification) can be expressed as:  When xi is labeled, the state is final, and it is represented by the indicator function based on the condition yi=1. When the sample is unlabeled, we need to consider the sum of all possible transitions starting from xi and ending in the closest absorbing state, with label y=1 weighted by the relative transition probabilities.\xa0 We can rewrite this expression in matrix form. If we create a vector P∞ = [ PL(y∞=1|XL), PU(y∞=1|XU) ], where the first component is based on labeled samples and the second on the unlabeled ones, we can write:  If we now expand the matrices, we get:  As we are interested only in the unlabeled samples, we can consider only the second equation:  Simplifying the expression, we get the following linear system:  The term (Duu - Wuu) is the unlabeled-unlabeled part of the unnormalized graph Laplacian L = D - W. By solving this system, we can get the probabilities for the class y=1 for all unlabeled samples.', 'e3b4ffa1-c3e0-4a15-ae58-d871fd868fd2.xhtml': "For this Python example of label propagation based on Markov random walks, we are going to use a bidimensional dataset containing 50 labeled samples belonging to two different classes, and 1,950 unlabeled samples: The plot of the dataset is shown in the following diagram (the crosses represent the unlabeled samples): We can now create the graph (using n_neighbors=15) and the weight matrix: Now, we need to compute the unlabeled part of the unnormalized graph Laplacian and the unlabeled-labeled part of the matrix W: At this point, it's possible to solve the linear system using the NumPy function np.linalg.solve(), which accepts as parameters the matrix A and the vector b of a generic system in the form Ax=b. Once we have the solution, we can merge the new labels with the original ones (where the unlabeled samples have been marked with -1). In this case, we don't need to convert the probabilities, because we are using 0 and 1 as labels. In general, it's necessary to use a threshold (0.5) to select the right label: Replotting the dataset, we get: As expected, without any iteration, the labels have been successfully propagated to all samples in perfect compliance with the clustering assumption. Both this algorithm and label propagation can work using a closed-form solution, so they are very fast even when the number of samples is high; however, there's a fundamental problem regarding the choice of\xa0σ/γ for the RBF kernel. As the same authors\xa0Zhu and Ghahramani remark, there is no standard solution, but it's possible to consider when\xa0σ\xa0→ 0 and when\xa0σ\xa0→\xa0∞. In the first case, only the nearest point has an influence, while in the second case, the influence is extended to the whole sample space, and the unlabeled points tend to acquire the same label. The authors suggest considering the entropy of all samples, trying to find the best\xa0σ value that minimizes it. This solution can be very effective, but sometimes the minimum entropy corresponds to a label configuration that isn't impossible to achieve using these algorithms. The best approach is to try different values (at different scales) and select the one corresponding to a valid configuration with the lowest entropy. In our case, it's possible to compute the entropy of the unlabeled samples as:  The Python code to perform this computation is: The term 1e-6 has been added to avoid numerical problems when the probability is null. Repeating this process for different values allows us to find a set of candidates that can be restricted to a single value with a direct evaluation of the labeling accuracy (for example, when there is no precise information about the real distribution, it's possible to consider the coherence of each cluster and the separation between them). Another approach is called class rebalancing, and it's based on the idea of reweighting the probabilities of unlabeled samples to rebalance the number of\xa0points belonging to each class when the new unlabeled samples are added to the set. If we have N labeled points and M unlabeled ones, with K classes, the weight factor wj for the class j can be obtained as:  The numerator is the average computed\xa0over the labeled samples belonging to class k, while the denominator is the average over the unlabeled ones whose estimated class is k. The final decision about a class is no longer based only on the highest probability, but on:\xa0 ", 'e77026d7-8731-4a12-bf78-679f00cc08c6.xhtml': "In Chapter 02, Introduction to Semi-Supervised Learning, we discussed the manifold assumption, saying that high-dimensional data normally lies on low-dimensional manifolds. Of course, this is not a theorem, but in many real cases, the assumption is proven to be correct, and it allows us to work with non-linear dimensionality reduction algorithms that would be otherwise unacceptable. In this section, we're going to analyze some of these algorithms. They are all implemented in Scikit-Learn, therefore it's easy to try them with complex datasets.", 'cecd6f7a-741c-4635-af55-de909c0495c2.xhtml': "Isomap is one of the simplest algorithms, and it's based on the idea of reducing the dimensionality while trying to preserve the geodesic distances measured on the original manifold where the input data lies. The algorithm works in three steps. The first operation is a k-nearest neighbors clustering and the construction of the following graph. The vertices will be the samples, while the edges represent the connections among nearest neighbors, and their weight is proportional to the distance to the corresponding neighbor.\xa0 The second step adopts the Dijkstra algorithm to compute the shortest pairwise distances on the graph of all couples of samples. In the following graph, there's a portion of a graph, where some shortest distances are marked: For example, as x3 is a neighbor of\xa0x5 and\xa0x7, applying the\xa0Dijkstra algorithm, we could get the shortest paths d(x3,\xa0x5) = w53 and\xa0d(x3,\xa0x7) = w73. The computational complexity of this step is about O(n²log n + n²k), which is lower than O(n³) when k << n (a condition normally met); however, for large graphs (with n >> 1), this is often the most expensive part of the whole algorithm. The third step is called metric multidimensional scaling, which is a technique for finding a low-dimensional representation while trying to preserve the inner product among samples. If we have a P-dimensional dataset X, the algorithm must find a Q-dimensional set Φ with Q < P minimizing the function:  As proven in\xa0Semi-Supervised Learning\xa0 Chapelle O.,\xa0Schölkopf B., Zien A., (edited by), The MIT Press,\xa0the optimization is achieved by taking the top Q eigenvectors of the Gram matrix Gij =\xa0xi\xa0· xj\xa0(or in matrix form, G=XXT if X\xa0∈ ℜn\xa0× M);\xa0however, as the Isomap algorithm works with pairwise distances, we need to compute the matrix D of squared distances:  If the X dataset is zero-centered, it's possible to derive a simplified Gram matrix from D, as described by M. A. A. Cox and T. F. Cox:  Isomap computes the top Q eigenvalues\xa0λ1,\xa0λ2, ...,\xa0λQ\xa0of GD\xa0and the corresponding eigenvectors\xa0ν1,\xa0ν2, ...,\xa0νQ and determines the Q-dimensional vectors as:  As we're going to discuss in Chapter 5, EM Algorithm and Applications\xa0(and also as pointed out by Saul, Weinberger, Sha, Ham, and Lee in\xa0Spectral Methods for Dimensionality Reduction,\xa0Saul L. K., Weinberger K. Q., Sha F., Ham J., and Lee D. D.), this kind of projection is also exploited by\xa0Principal Component Analysis (PCA), which finds out the direction with the highest variance, corresponding to the top k eigenvectors of the covariance matrix. In fact, when applying the SVD to the dataset X, we get:  The diagonal matrix\xa0Λ contains the eigenvalues of both XXT and XTX;\xa0therefore, the eigenvalues λGi of G are equal to MλΣi where\xa0λΣi\xa0are the eigenvalues of the covariance\xa0matrix Σ = M-1XTX. Hence, Isomap achieves the dimensionality reduction, trying to preserve the pairwise distances, while projecting the dataset in the subspace determined by a group of eigenvectors, where the maximum explained variance is achieved. In terms of information theory, this condition guarantees the minimum loss with an effective reduction of dimensionality.", '7ca20feb-f7b5-44e8-8103-a1a183fe1a53.xhtml': "We can now test the Scikit-Learn Isomap implementation using the Olivetti faces dataset (provided by AT&T Laboratories, Cambridge), which is made up of 400 64 × 64 grayscale portraits belonging to 40 different people. Examples of these images are shown here: The original dimensionality is 4096, but we want to visualize the dataset in two dimensions. It's important to understand that using the Euclidean distance for measuring the similarity of images might not the best choice, and it's surprising to see how well the samples are clustered by such a simple algorithm. The first step is loading the dataset: The\xa0faces\xa0dictionary contains three main elements: At this point, we can instantiate the\xa0Isomap\xa0class provided by Scikit-Learn, setting n_components=2 and n_neighbors=5 (the reader can try different configurations), and then fitting the model: As the resulting plot with 400 elements is very dense, I preferred to show in the following plot only the first 100 samples: As it's possible to see, samples belonging to the same class are grouped in rather dense agglomerates. The classes that seem better separated are 7 and 1. Checking the corresponding faces, for class 7, we get: The set contains portraits of a young woman with a fair complexion, quite different from the majority of other people. Instead, for class 1, we get: In this case, it's a man with big glasses and a particular mouth expression. In the dataset, there are only a few people with glasses, and one of them has a dark beard. We can conclude that Isomap created a low-dimensional representation that is really coherent with the original geodesic distances. In some cases, there's a partial clustering overlap that can be mitigated by increasing the dimensionality or adopting a more complex strategy.", '22aa0177-a8f6-4960-b511-451a6ee3c7ff.xhtml': "Contrary to Isomap, which works with the pairwise distances, this algorithm is based on the assumption that a high-dimensional dataset lying on a smooth manifold can have local linear structures that it tries to preserve during the dimensionality reduction process.\xa0Locally Linear Embedding (LLE), like Isomap, is based on three steps. The first one is applying the k-nearest neighbor algorithm to create a directed graph (in Isomap, it was undirected), where the vertices are the input samples and the edges represent a neighborhood relationship. As the graph is direct, a point\xa0xi can be a neighbor of xj, but the opposite could be false. It means that the weight matrix can be asymmetric. The second step is based on the main assumption of local linearity. For example, consider the following graph: The rectangle delimits a small neighboorhood. If we consider the point x5, the local linearity assumption allows us to think that x5 = w56x6 + w53x3,\xa0without considering the cyclic relationship. This concept can be formalized for all N P-dimensional points through the minimization of the following function:  In order to address the problem of low-rank neighborhood matrices (think about the previous example, with a number of neighbors equal to 20), Scikit-Learn also implements a regularizer that\xa0is based on a\xa0small arbitrary additive constant that is added to the local weights\xa0(according to a variant called Modified LLE or MLLE). At the end of this step, the matrix W that better matches the linear relationships among neighbors will be selected for the next phase. In the third step, locally linear embedding tries to determine the low-dimensional (Q < P) representation that best reproduces the original relationship among nearest neighbors. This is achieved by minimizing the following function:  The solution for this problem is obtained through the adoption of the Rayleigh-Ritz method, an algorithm to extract a subset of eigenvectors and eigenvalues from a very large sparse matrix. For further details, read A spectrum slicing method for the Kohn–Sham problem,\xa0Schofield G. Chelikowsky J. R.; Saad Y., Computer Physics Communications. 183. The initial part of the final procedure consists of determining the matrix D:  It's possible to prove the last eigenvector (if the eigenvalues are sorted in descending order, it's the bottom one) has all components v1(N),\xa0v2(N), ...,\xa0vN(N)\xa0= v, and the corresponding eigenvalue is null. As Saul and Roweis (An introduction to locally linear embedding,\xa0Saul L. K., Roweis S. T.) pointed out, all the other Q eigenvectors (from the bottom) are orthogonal, and this allows them to have zero-centered embedding. Hence, the last eigenvector is discarded, while the remaining Q eigenvectors determine the embedding vectors φi.", 'be6e4402-71f1-42cb-a2d3-7baf408f619d.xhtml': "Â\xa0We can now apply this algorithm to the Olivetti faces dataset, instantiating the Scikit-Learn class LocallyLinearEmbedding with n_components=2 and n_neighbors=15: The result (limited to the first 100 samples) is shown in the following plot: Even if the strategy is different from Isomap, we can determine some coherent clusters. In this case, the similarity is obtained through the conjunction of small linear blocks; for the faces, they can represent particular micro-features, like the shape of the nose or the presence of glasses, that remain invariant in the different portraits of the same person. LLE is, in general, preferable when the original dataset is intrinsically locally linear, possibly lying on a smooth manifold. In other words, LLE is a reasonable choice when small parts of a sample are structured in a way that allows the reconstruction of a point given the neighbors and the weights. This is often true for images, but it can be difficult to determine for a generic dataset. When the result doesn't reproduce the original clustering, it's possible to employ the next algorithm or t-SNE, which is one the most advanced.", '2926834c-59e1-4806-b701-b29795f75035.xhtml': "This algorithm, based on the spectral decomposition of a graph Laplacian, has been proposed in order to perform a non-linear dimensionality reduction to try to preserve the nearness of points in the P-dimensional manifold when remapping on a Q-dimensional (with Q < P) subspace. The procedure is very similar to the other algorithms. The first step is a k-nearest neighbor clustering to generate a graph where the vertices (we can assume to have N elements) are the samples, and the edges\xa0are weighted using an RBF kernel:  The resulting graph is undirected and symmetric. We can now define a pseudo-degree matrix D:  The low-dimensional representation\xa0Φ is obtained by minimizing the function:  If the two points xi and\xa0xj are near, the corresponding Wij is close to 1, while it tends to 0 when the distance tends to\xa0∞. Dii is the sum of all weights originating from xi (and the same for Djj). Now, let's suppose that xi\xa0is very close only to\xa0xj\xa0so, to approximate Dii\xa0= Djj ≈ Wij. The resulting formula is a square loss based on the difference between the vectors\xa0φi and\xa0φj. When instead there are multiple closeness relationships to consider, the factor Wij divided by the square root of DiiDjj\xa0allows reweighting the new distances to find the best trade-off for the whole dataset. In practice, LΦ is not minimized directly. In fact, it's possible to prove that the minimum can be obtained through the spectral decomposition of the symmetric normalized graph Laplacian (the name derives from this procedure):  Just like for the\xa0LLE\xa0algorithm, Laplacian Spectral Embedding also works with the bottom Q + 1 eigenvectors. The mathematical theory behind the last step is always based on the application of the Rayleigh-Ritz method. The last one is discarded, and the remaining Q determines the low-dimensional representation\xa0φi.", '11eb87e8-513d-4876-80f7-70b37f27e54e.xhtml': "Let's apply this algorithm to the same dataset using the Scikit-Learn class SpectralEmbedding, with n_components=2 and n_neighbors=15: The resulting plot (zoomed in due to the presence of a high-density region) is shown in the following graph: Even in this case, we can see that some classes are grouped into small clusters, but at the same time, we\xa0observe many agglomerates where there are mixed samples. Both this and the previous method work with local pieces of information, trying to find low-dimensional representations that could preserve the geometrical structure of micro-features. This condition drives to a mapping where close points share local features (this is almost always true for images, but it's very difficult to prove for generic samples). Therefore, we can observe small clusters containing elements belonging to the same class, but also some apparent outliers, which, on the original manifold, can be globally different even if they share local patches. Instead, methods like Isomap or t-SNE\xa0work with the whole distribution, and try to determine a representation that is almost isometric with the original dataset considering its global properties.", 'f918a39c-b3b1-4d7c-aad9-e50617245f47.xhtml': 'This algorithm, proposed by Van der Mateen and Hinton and formally known as t-Distributed Stochastic Neighbor Embedding (t-SNE), is one of the most powerful manifold dimensionality reduction techniques. Contrary to the other methods, this algorithm starts with a fundamental assumption: the similarity between two N-dimensional points xi and\xa0xj\xa0can be represented as the conditional probability p(xj|xi) where each point is represented by a Gaussian distribution centered in\xa0xi and with variance\xa0σi. The variances are selected starting from the desired perplexity, defined as:  Low-perplexity values indicate a low uncertainty, and are normally preferable. In common t-SNE tasks, values in the range 10÷50 are normally acceptable. The assumption on the conditional probabilities can be interpreted thinking that if two samples are very similar, the probability\xa0associated with the first sample conditioned to the second one is high, while dissimilar points yield low conditional probabilities. For example, thinking about images, a point centered in the pupil can have as neighbors some points belonging to an eyelash. In terms of probabilities, we can think that p(eyelash|pupil) is quite high, while p(nose|pupil) is obviously lower. t-SNE models these conditional probabilities as:  The probabilities\xa0p(xi|xi) are set to zero, so the previous formula can be extended to the whole graph. In order to solve the problem in an easier way, the conditional probabilities are also symmetrized:  The probability distribution so obtained represents the high-dimensional input relationship. As our goal is to reduce the dimensionality to a value M < N, we can think about a similar probabilistic representation for the target points\xa0φi, using a student-t distribution with one degree of freedom:  We want the low-dimensional distribution Q to be as close as possible to the high-dimensional distribution P; therefore, the aim of the t-SNE algorithm is to minimize the Kullback-Leibler divergence between P and Q:  The first term is the entropy of the original distribution P, while the second one is the cross-entropy H(P, Q), which has to be minimized to solve the problem. The best approach is based on a gradient-descent algorithm, but there are also some useful variations that can improve the performance discussed in\xa0Visualizing High-Dimensional Data Using t-SNE,\xa0Van der Maaten\xa0L.J.P., Hinton G.E., Journal of Machine Learning Research 9 (Nov), 2008.', '621833e9-07ed-44a4-8c7f-fb2afd50a027.xhtml': "We can apply this powerful algorithm to the same Olivetti faces dataset, using the Scikit-Learn class TSNE with n_components=2 and perplexity=20: The result for all 400 samples is shown in the following graph: A visual inspection of the label distribution can confirm that\xa0t-SNE\xa0recreated the optimal clustering starting from the original high-dimensional distribution. This algorithm can be employed in several non-linear dimensionality reduction tasks, such as images, word embeddings, or complex feature vectors. Its main strength is hidden in the assumption to consider the similarities as probabilities, without the need to impose any constraint on the pairwise distances, either global or local. Under a certain viewpoint, it's possible to consider\xa0t-SNE as a reverse multiclass classification problem based on a cross-entropy cost function. Our goal is to find the labels (low-dimensional representation) given the original distribution and an assumption about the output distribution. At this point, we could try to answer a natural question: which algorithm must be employed? The obvious answer is it depends on the single problem. When it's useful to reduce the dimensionality, preserving the global similarity among vectors (this is the case when the samples are long feature vectors without local properties, such as word embeddings or data encodings),\xa0t-SNE\xa0or Isomap are good choices. When instead it's necessary to keep the local distances (for example, the structure of a visual patch that can be shared by different samples also belonging to different classes) as close as possible to the original representation, locally linear embedding or\xa0spectral embedding\xa0algorithms are preferable.", '18614dd5-bdef-468a-a767-45f3697aa902.xhtml': "In this chapter, we have introduced the most important label propagation techniques. In particular, we have seen how to build a dataset graph based on a weighting kernel, and how to use the geometric information provided by unlabeled samples to determine the most likely class. The basic approach works by iterating the multiplication of the label vector times the weight matrix until a stable point is reached and we have proven that, under simple assumptions, it is always possible. Another approach, implemented by Scikit-Learn, is based on the transition probability from a state (represented by a sample) to another one, until the convergence to a labeled point. The probability matrix is obtained using a normalized weight matrix to encourage transitions associated to close points and discourage all the long jumps. The main drawback of these two methods is the hard-clamping of labeled samples; this constraint can be useful if we trust our dataset, but it can be a limitation in the presence of outliers whose label has been wrongly assigned. Label spreading solves this problem by introducing a clamping factor that determines the percentage of clamped labels. The algorithm is very similar to label propagation, but it's based on graph Laplacian and can be employed in all those problems where the data-generating distribution is not well-determined and the probability of noise is high. The propagation based on Markov random walks is a very simple algorithm that can estimate the class distribution of unlabeled samples through a stochastic process. It's possible to imagine it as a\xa0test sample\xa0that walks through the graph until it reaches a final labeled state (acquiring the corresponding label). The algorithm is very fast and it has a closed-form solution that can be found by solving a linear system. The next topic was the introduction of manifold learning with the Isomap algorithm, which is a simple but powerful solution based on a graph built using a k-nearest neighbors algorithm (this is a common step in most of these algorithms). The original pairwise distances are processed using the multidimensional scaling technique, which allows obtaining a low-dimensional representation where the distances between samples are preserved. Two different approaches, based on local pieces of information, are locally linear embedding and Laplacian Spectral Embedding. The former tries to preserve the local linearity present in the original manifold, while the latter, which is based on the spectral decomposition of the normalized graph Laplacian, tries to preserve the nearness of original samples. Both methods are suitable for all those tasks where it's important not to consider the whole original distribution, but the similarity induced by small data patches. We closed this chapter by discussing t-SNE, which is a very powerful algorithm that tries to model a low-dimensional distribution that is as similar as possible to the original high-dimensional one. This task is achieved by minimizing the Kullback-Leibler divergence between the two distributions. t-SNE is a state-of-the-art algorithm, useful whenever it's important to consider the whole original distribution and the similarity between entire samples. In the next chapter, Chapter 4, Bayesian Networks and Hidden Markov Models\xa0we're going to introduce Bayesian networks in both a static and dynamic context, and hidden Markov models, with practical prediction examples. These algorithms allow modeling complex probabilistic scenarios made up of observed and latent variables, and infer future states using optimized sampling methods based only on the observations.", 'c90de697-dc7b-489f-b605-6fabfc98ef9e.xhtml': "In this chapter, we're going to introduce the basic concepts of Bayesian models, which allow working with several scenarios where it's necessary to consider uncertainty as a structural part of the system. The discussion will focus on static (time-invariant) and dynamic methods that can be employed where necessary toÂ\xa0model time sequences. In particular, the chapter covers the following topics:", '6e04b5bb-6134-4924-b77c-25d544000e27.xhtml': "If we have a probability space S and two events A and B, the probability of A given B is called conditional probability, and it's defined as: As P(A, B) = P(B, A), it's possible to derive\xa0Bayes' theorem:  This theorem allows expressing\xa0a conditional probability as a function of the opposite one and the two marginal probabilities P(A) and P(B). This result is fundamental to many machine learning problems, because, as we're going to see in this and in the next chapters, normally it's easier to work with a conditional probability in order to get the opposite, but it's hard to work directly from the latter. A common form of this theorem can be expressed as:  Let's suppose that we need to estimate the probability of an event A given some observations B, or using the standard notation, the posterior probability of A; the previous formula expresses this value as proportional to the term P(A), which is the marginal probability of A, called prior probability, and the conditional probability of the observations B given the event A. P(B|A) is called likelihood, and defines how event A is likely to determine B. Therefore, we can summarize the relation as posterior probability\xa0∝ likelihood\xa0· prior probability. The proportion is not a limitation, because the term P(B) is always a normalizing constant that can be omitted. Of course, the reader must remember to normalize P(A|B) so that its terms always sum up to one. This is a key concept of Bayesian statistics, where we don't directly trust the prior probability, but we reweight it using the likelihood of some observations. As an example, we can think to toss a coin 10 times (event A). We know that P(A) = 0.5 if the coin is fair. If we'd like to know what the probability is to get 10 heads, we could employ the Binomial distribution\xa0obtaining P(10 heads) = 0.5k; however, let's suppose that we don't know whether the coin is fair or not, but we suspect it's loaded with a prior probability P(Loaded) = 0.7 in favor of tails. We can define a complete prior probability P(Coin status) using the indicator functions: Where P(Fair) = 0.5 and P(Loaded) = 0.7, the indicator ICoin=Fair\xa0is equal to 1 only if the coin is fair, and 0 otherwise. The same happens with\xa0ICoin=Loaded\xa0when the coin is loaded. Our goal now is to determine the posterior probability P(Coin status|B1, B2, ...,\xa0Bn)\xa0to be able to confirm or to reject our hypothesis. Let's imagine to observe n = 10 events with\xa0B1\xa0= Head and B2, ...,\xa0Bn\xa0= Tail. We can express the probability using the binomial distribution:  After simplifying the expression, we get: We still need to normalize by dividing both terms by 0.083 (the sum of the two terms), so we get the final posterior probability\xa0P(Coin status|B1, B2, ...,\xa0Bn) = 0.04IFair + 0.96ILoaded. This result confirms and strengthens our hypothesis. The probability of a loaded coin is now about 96%, thanks to the sequence of nine tail observations after one head. This example was presented to show how the data (observations) is plugged into the Bayesian framework. If the reader is interested in studying these concepts in more detail, in Introduction to Statistical Decision Theory, Pratt J., Raiffa H., Schlaifer R., The MIT Press,\xa0it's possible to find many interesting examples and explanations; however, before introducing Bayesian networks, it's useful to define two other essential concepts. The first concept is called conditional independence, and it can be formalized considering two variables A and B, which are conditioned to a third one, C. We say that A and B are conditionally independent given C if: Now, let's suppose we have an event A that is conditioned to a series of causes C1, C2, ..., Cn;\xa0the conditional probability is, therefore, P(A|C1, C2, ..., Cn). Applying Bayes' theorem, we get: \xa0If there is conditional independence, the previous expression can be simplified and rewritten as: This property is fundamental in Naive Bayes classifiers, where we assume that the effect produced by a cause does not influence the other causes. For example, in a spam detector, we could say that the length of the mail and the presence of some particular keywords are independent events, and we only need to compute P(Length|Spam) and P(Keywords|Spam) without considering the joint probability P(Length, Keywords|Spam). Another important element is the chain rule of probabilities. Let's suppose we have the joint probability P(X1, X2, ..., Xn). It can be expressed as: Repeating the procedure with the joint probability on the right side, we get: In this way, it's possible to express a full joint probability as the product of hierarchical conditional probabilities, until the last term, which is a marginal distribution. We are going to use this concept extensively in the next paragraph when exploring Bayesian networks.", '439fceb4-48be-4ccd-aa74-e1e98eef805c.xhtml': "A Bayesian network is a probabilistic model represented by a direct acyclic graph G = {V, E}, where the vertices are random variables Xi, and the edges determine a conditional dependence among them. In the following diagram, there's an example of simple Bayesian networks with four variables: The variable x4 is dependent on x3, which is dependent on x1 and\xa0x2.\xa0To describe the network, we need the marginal probabilities P(x1) and P(x2) and the conditional probabilities P(x3|x1,x2) and P(x4|x3). In fact, using the chain rule, we can derive the full joint probability as: The previous expression shows an important concept: as the graph is direct and acyclic, each variable is conditionally independent of all other variables that are not successors given its predecessors. To formalize this concept, we can define the function Predecessors(xi), which returns the set of nodes that influence xi directly, for example, Predecessors(x3) = {x1,x2} (we are using lowercase letters, but we are considering the random variable, not a sample). Using this function, it's possible to write a general expression for the full joint probability of a Bayesian network with N nodes: The general procedure to build a Bayesian network should always start with the first causes, adding their effects one by one, until the last nodes are inserted into the graph. If this rule is not respected, the resulting graph can contain useless relations that can increase the complexity of the model. For example, if\xa0x4 is caused indirectly by both x1 and\xa0x2, therefore adding the edges x1\xa0→ x4 and\xa0x2\xa0→ x4 could seem a good modeling choice; however, we know that the final influence on x4 is determined only by the values of x3,\xa0whose probability must be conditioned on x1 and\xa0x2, hence we can remove the spurious edges. I suggest reading Introduction to Statistical Decision Theory, Pratt J., Raiffa H., Schlaifer R., The MIT Press\xa0to learn many best practices that should be employed in this procedure.", '59577e9b-7147-4c92-9ca5-5ee9a3bdc1be.xhtml': "Performing a direct inference on a Bayesian network can be a very complex operation when the number of variables and edges is high. For this reason, several sampling methods have been proposed. In this paragraph, we are going to show how to determine the full joint probability sampling from a network using a direct approach, and two MCMC algorithms. Let's start considering the previous network and, for simplicity, let's assume to have only Bernoulli distributions.\xa0X1 and X2\xa0are modeled as: The conditional distribution X3 is defined as: While the conditional distribution X4 is defined as: We can now use a direct sampling to estimate the full joint probability P(x1, x2,\xa0x3, x4) using the chain rule previously introduced.", '795b1062-142c-44d0-b8bf-de36da42909f.xhtml': 'With direct sampling, our goal is to approximate the full joint probability through a sequence of samples drawn from each conditional distribution. If we assume that the graph is well-structured (without unnecessary edges) and we have N variables, the algorithm is made up of the following steps: From a mathematical viewpoint, we are first creating a frequency vector\xa0FSamples(x1, x2,\xa0x3, ..., xN;\xa0NSamples) and then we approximate the full joint probability considering\xa0NSamples\xa0→\xa0∞:', '9ec22291-a23d-4379-ac37-4cbf39a42902.xhtml': "We can now implement this algorithm in Python. Let's start by defining the sample methods using the NumPy function np.random.binomial(1, p), which draws a sample from a Bernoulli distribution with probability\xa0p: At this point, we can implement the main cycle. As the variables are Boolean, the total number of probabilities is 16, so we set\xa0Nsamples\xa0to 5000 (smaller values are also acceptable): When the sampling is complete, it's possible to extract the full joint probability: We can also query the model. For example, we could be interested in P(X4=True). We can do this by looking for all the elements where X4=True, and summing up the relative probabilities: This value is coherent with the definition of X4, which is always p >= 0.5. The reader can try to change the values and repeat the simulation.", '7d1f6730-5ff7-4113-b493-9e1ff2a7b1ba.xhtml': "In order to discuss the MCMC algorithms, it's necessary to introduce the concept of Markov chains. In fact, while the direct sample method draws samples without any particular order, the MCMC strategies draw a sequence of samples according to a precise transition probability from a sample to the following one. Let's consider a time-dependent random variable X(t), and let's assume a discrete time sequence X1, X2, ..., Xt, Xt+1, ... where Xt represents the value assumed at time t. In the following diagram, there's a schematic representation of this sequence: We can suppose to have N different states si for i=1..N, therefore it's possible to consider the probability P(Xt=si|Xt-1=sj, ..., X1=sp). X(t) is defined as a\xa0first-order Markov process if: In other words, in a\xa0Markov process (from now on, we omit first-order, even if there are cases when it's useful to consider more previous states), the probability that X(t) is in a certain state depends only on the state assumed in the previous time instant. Therefore, we can define a transition probability\xa0for every couple i, j: Considering all the couples (i, j), it's also possible to build a transition probability matrix T(i, j) = P(i\xa0→ j). The marginal probability that Xt=si using a standard notation is defined as: At this point, it's easy to prove (Chapman-Kolmogorov equation) that: In the previous expression, in order to compute\xa0πi(t+1), we need to sum over all possible previous states, considering the relative transition probability. This operation can be rewritten in matrix form, using a vector\xa0π(t) containing all states and the transition probability matrix T (the uppercase superscript T means that the matrix is transposed). The evolution of the chain can be computed recursively: For our purposes, it's important to consider Markov chains that are able to reach a stationary distribution\xa0πs: In other words, the state does not depend on the initial condition\xa0π(1), and it's no longer able to change. The stationary distribution is unique if the underlying Markov process is ergodic. This concept means that the process has the same properties if averaged over time (which is often impossible), or averaged vertically\xa0(freezing the time) over the states (which is simpler in the majority of cases). The process of ergodicity for Markov chains is assured by two conditions. The first is\xa0aperiodicity\xa0for all states, which means that it is impossible to find a positive number p so that the chain returns in the same state sequence after a number of instants equal to a\xa0multiple of p. The second condition is that all states must be positive recurrent: this means that, given a random variable\xa0Ninstants(i), describing the number of time instants needed to return to the state si,\xa0E[Ninstants(i)] < ∞;\xa0therefore, potentially, all the states can be revisited in a finite time. The reason why we need the ergodicity condition, and hence the existence of a unique stationary distribution, is that we are considering the sampling processes modeled as Markov chains, where the next value is sampled according to the current state. The transition from one state to another is done in order to find better samples, as we're going to see in the Metropolis-Hastings sampler, where we can also decide to reject a sample and keep the chain in the same state. For this reason, we need to be sure that the algorithms converge to the unique stable distribution (that approximates the real full joint distribution of our Bayesian network). It's possible to prove that a chain always reaches a stationary distribution if: The previous equation is called detailed balance, and implies the reversibility of the chain. Intuitively, it means that the probability of finding the chain in the state A times the probability of a transition to the state B is equal to the probability of finding the chain in the state B times the probability of a transition to A. For both methods that we are going to discuss, it's possible to prove that they satisfy the previous condition, and therefore their convergence is assured.", 'da0b7cb1-bec8-43e8-a078-e44b7c2b279d.xhtml': "Let's suppose that we want to obtain the full joint probability for a Bayesian network P(x1, x2,\xa0x3, ..., xN);\xa0however, the number of variables is large and there's no way to solve this problem easily in a closed form. Moreover, imagine that we would like to get some marginal distribution, such as\xa0P(x2), but to do so we should integrate the full joint probability, and this task is even harder. Gibbs sampling allows approximating of all marginal distributions with an iterative process. If we have N variables, the algorithm proceeds with the following steps: At the end of the iterations, vector S will contain\xa0NIterations\xa0samples for each distribution. As we need to determine the probabilities, it's necessary to proceed like in the direct sampling algorithm, counting the number of single occurrences and normalizing dividing by\xa0NIterations. If the variables are continuous, it's possible to consider intervals, counting how many samples are contained in each of them.\xa0 For small networks, this procedure is very similar to direct sampling, except that when working with very large networks, the sampling process could become slow; however, the algorithm can be simplified after introducing the concept of the\xa0Markov blanket of Xi, which is the set of random variables that are predecessors, successors, and successors' predecessors of\xa0Xi (in some books, they use the terms parents and children). In a Bayesian network, a variable Xi is a conditional independent of all other variables given its Markov blanket. Therefore, if we define the function MB(Xi), which returns the set of variables in the blanket, the generic sampling step can be rewritten as p(xi|MB(Xi)), and there's no more need to consider all the other variables. To understand this concept, let's consider the network shown in the following diagram: The Markov blankets are: In general, if N is very large, the cardinality of |MB(Xi)| << N, thus simplifying the process (the vanilla Gibbs sampling needs N-1 conditions for each variable). We can prove that the Gibbs sampling generates samples from a Markov chain that is in\xa0detailed balance: Therefore, the procedure converges to the unique stationary distribution. This algorithm is quite simple; however, its performance is not excellent, because the random walks are not tuned up in order to explore the right regions of the state-space, where the probability to find good samples is high. Moreover, the trajectory can also return to bad states, slowing down the whole process. An alternative (also implemented by PyMC3 for continuous random variables) is the No-U-Turn algorithm, which we don't discuss in this book. The reader interested in this topic can find a full description in\xa0The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo, Hoffmann M. D., Gelman A., arXiv:1111.4246.", 'ac570310-4104-4b1b-9e40-6bd64c9f1d54.xhtml': "We have seen that the full joint probability distribution of a Bayesian network P(x1, x2,\xa0x3, ..., xN)\xa0can become intractable when the number of variables is large. The problem can become even harder when it's needed to marginalize it in order to obtain, for example, P(xi), because it's necessary to integrate a very complex function. The same problem happens when applying the Bayes' theorem in simple cases. Let's suppose we have the expression p(A|B) = K\xa0· P(B|A)P(A). I've expressly inserted the normalizing constant K, because if we know it, we can immediately obtain the posterior probability; however, finding it normally requires integrating P(B|A)P(A), and this operation can be impossible in closed form. The Metropolis-Hastings algorithm can help us in solving this problem. Let's imagine that we need to sample from\xa0P(x1, x2,\xa0x3, ..., xN), but we know this distribution up to a normalizing constant, so\xa0P(x1, x2,\xa0x3, ..., xN)\xa0∝ g(x1, x2,\xa0x3, ..., xN). For simplicity, from now on we collapse all variables into a single vector, so P(x)\xa0∝ g(x). Let's take another distribution q(x'|x(i-1)), which is called candidate-generating distribution. There are no particular restrictions on this choice, only that q is easy to sample. In some situations, q can be chosen as a function very similar\xa0to the distribution p(x), which is our target, while in other cases, it's possible to use a normal distribution with mean equal to\xa0x(i-1). As we're going to see, this function acts as a proposal-generator, but we're not obliged to accept all the samples drawn from it therefore, potentially any distribution with the same domain of P(X) can be employed. When a sample is accepted, the Markov chain transitions to the next state, otherwise it remains in the current one. This decisional process is based on the idea that the sampler must explore the most important state-space regions and discard the ones where the probability to find good samples is low.\xa0 The algorithm\xa0proceeds with the following steps: It's possible to prove (the proof will be omitted, but it's available in\xa0Markov Chain Monte Carlo and Gibbs Sampling,\xa0Walsh B., Lecture Notes for EEB 596z) that the transition probability of the\xa0Metropolis-Hastings algorithm satisfies the detailed balance equation, and therefore the algorithm converges to the true posterior distribution.", '7bd97e49-a60f-49f1-83ad-2b99c98331ec.xhtml': "We can implement this algorithm to find the posterior distribution P(A|B) given the product of P(B|A) and P(A), without considering the normalizing constant that requires a complex integration. Let's suppose that: Therefore, the resulting g(x) is: To solve this problem, we adopt the random walk Metropolis-Hastings, which consists of choosing q\xa0∼ Normal(μ=x(t-1)). This choice allows simplifying the value α, because the two terms q(x(t-1)|x') and q(x'|x(t-1)) are equal (thanks to the symmetry around the vertical axis passing through xmean) and can be canceled out, so\xa0α becomes the ratio between g(x') and g(x(t-1)). The first thing is defining the functions: Now, we can start our sampling process with 100,000 iterations and x(0) = 1.0: To get a representation of the posterior distribution, we need to create a histogram through the NumPy function np.histogram(), which accepts an array of values and the number of desired intervals (bins); in our case, we set 100 intervals: The resulting plot of p(x) is shown in the following graph:", '67ba5003-2edf-42d1-aa27-2904006b8252.xhtml': "PyMC3 is a powerful Python Bayesian framework that relies on Theano to perform high-speed computations (see the information box at the end of this paragraph for the installation instructions). It implements all the most important continuous and discrete distributions, and performs the sampling process mainly using the No-U-Turn and Metropolis-Hastings algorithms. For all the details about the API (distributions, functions, and plotting utilities), I suggest visiting the documentation home page\xa0http://docs.pymc.io/index.html, where it's also possible to find some very intuitive tutorials. The example we want to model and simulate is based on this scenario: a daily flight from London to Rome has a scheduled departure time at 12:00 am, and a standard flight time of two hours. We need to organize the\xa0operations at the destination airport, but we don't want to allocate resources when the plane hasn't landed yet. Therefore, we want to model the process using a Bayesian network and considering some common factors that can influence the arrival time. In particular, we know that the onboarding process can be longer than expected, as well as the refueling one, even if they are carried out in parallel. London air traffic control can also impose a delay, and the same can happen when the plane is approaching Rome. We also know that the presence of rough weather can cause another delay due to a change of route. We can summarize this analysis with the following plot: Considering our experience, we decide to model the random variables using the following distributions: The probability density functions are: Departure Time and Arrival Time are functions of random variables, and the parameter λ of Flight Time is also a function of Rough Weather.\xa0 Even if the model is not very complex, the direct inference is rather inefficient, and therefore we want to simulate the process using PyMC3. The first step is to create a model instance: From now on, all operations must be performed using the context manager provided by the\xa0model\xa0variable. We can now set up all the random variables of our Bayesian network: We have imported two namespaces,\xa0pymc3.distributions.continuous and\xa0pymc3.distributions.discrete, because we are using both kinds of variable. Wald and exponential are continuous distributions, while Bernoulli is discrete. In the first three rows, we declare the variables passenger_onboarding, refueling, and departure_traffic_delay. The structure is always the same: we need to specify the class corresponding to the desired distribution, passing the name of the variable and all the required parameters. The\xa0departure_time\xa0variable is declared as pm.Deterministic. In PyMC3, this means that, once all the random elements have been set, its value becomes completely determined. Indeed, if we sample from departure_traffic_delay, passenger_onboarding, and refueling, we get a determined value for departure_time. In this declaration, we've also used the utility function pmm.switch, which operates a binary choice based on its first parameter (for example, if A > B, return\xa0A, else return B). The other variables are very similar, except for flight_time, which is an exponential variable with a parameter λ, which is a function of another variable (rough_weather). As a Bernoulli\xa0variable outputs 1 with probability p and 0 with probability 1 - p,\xa0λ = 0.4 if there's rough weather, and 0.5 otherwise. Once the model has been set up, it's possible to simulate it through a sampling process. PyMC3 picks the best sampler automatically, according to the type of variables. As the model is not very complex, we can limit the process to 500 samples: The output can be analyzed using the built-in pm.traceplot() function, which generates the plots for each of the sample's variables. The following graph shows the detail of one of them: The right column shown the samples generated for the random variable (in this case, the arrival time), while the left column shows the relative frequencies. This plot can be useful to have a visual confirmation of our initial ideas; in fact, the arrival time has the majority of its mass concentrated in the interval 14:00 to 16:00 (the numbers are always decimal, so it's necessary to convert the times); however, we should integrate to get the probabilities. Instead, through the pm.summary() function,\xa0PyMC3 provides a statistical summary that can help us in making the right decisions. In the following snippet, the output containing the summary of a single variable is shown: For each variable, it contains mean, standard deviation, Monte Carlo error, 95% highest posterior density interval, and the posterior quantiles. In our case, we know that the plane will land at about 15:10 (15.174). This is only a very simple example to show the power of Bayesian networks. For deep insight, I suggest the book Introduction to Statistical Decision Theory, Pratt J., Raiffa H., Schlaifer R., The MIT Press, where it's possible to study different Bayesian applications that are out of the scope of this book.", '51872ade-92b3-4149-afb7-b59665cbc0da.xhtml': "Let's consider a stochastic process X(t) that can assume N different states: s1, s2, ..., sN with first-order Markov chain dynamics. Let's also suppose that we cannot observe the state of X(t), but we have access to another process O(t), connected to X(t), which produces observable outputs (often known as emissions). The resulting process is called a Hidden Markov Model (HMM), and a generic schema is shown in the following diagram: For each hidden state si, we need to define a transition probability P(i\xa0→ j), normally represented as a matrix if the variable is discrete. For the Markov assumption, we have: Moreover, given a sequence of observations o1, o2, ..., oM, we also assume the following assumption about the independence of the\xa0emission probability: In other words, the probability of the observation oi\xa0(in this case, we mean the value at time i) is conditioned only by the state of the hidden variable at time i (xi). Conventionally, the first state x0 and the last one\xa0xEnding are never emittied, and therefore all the sequences start with the index 1 and end with an extra timestep corresponding to the final state. HMMs can be employed in all those contexts where it's impossible to measure the state of a system (we can only model it as a stochastic variable with a known transition probability), but it's possible to access some data connected to it. An example can be a complex engine that is made up of a large number of parts. We can define some internal states and learn a transition probability matrix (we're going to learn how to do that), but we can only receive measures provided by specific sensors. Sometimes, even if not extremely realistic, but it's useful to include the Markov assumption and the emission probability independence into our model. The latter can be justified considering that we can sample all the peak emissions corresponding to precise states and, as the random process O(t) is implicitly dependent on X(t), it's not unreasonable to think of it like a pursuer of X(t). The Markov assumption holds for many real-life processes if either they are naturally first-order Markov ones, or if the states contain all the history needed to justify a transition. In other words, in many cases, if the state is A, then there's a transit to B and finally to C. We assume that when in C, the system moved from a state (B) that carries a part of the information provided by A. For example, if we are filling a tank, we can measure the level (the state of our system) at time t, t+1, ... If the water flow is modeled by a random variable because we don't have a stabilizer, we can find the probability that the water has reached a certain level at time t, p(Lt=x|Lt-1). Of course, it doesn't make sense to condition over all the previous states, because if the level is, for example, 80 m at time t-1, all the information needed to determine the probability of a new level (state) at time t is already contained in this state (80 m). At this point, we can start analyzing how to train a hidden Markov model, and how to determine the most likely hidden states given a sequence of observations. For simplicity, we call A the transition probability matrix, and B the matrix containing all P(oi|xt). The resulting model can be determined by the knowledge of those elements: HMM = { A, B }.", 'b3867d60-aba3-4938-9b6f-0572577827cf.xhtml': "The forward-backward algorithm is a simple but effective method to find the transition probability matrix T given a sequence of observationsÂ\xa0o1, o2, ..., ot. The first step is called the forward phase, and consists of determining the probability of a sequence of observations P(o1, o2, ..., oSequence Length|A, B). This piece of information can be directly useful if we need to know the likelihood of a sequence and it's necessary, together with the backward phase, to estimate the structure (A and B) of the underlying HMM. Both algorithms are based on the concept of dynamic programming, which consists of splitting a complex problem into sub-problems that can be easily solved, and reusing the solutions to solve more complex steps in a recursive/iterative fashion. For further information on this, please refer to Dynamic Programming and Markov Process, Ronald A. Howard, The MIT Press.", '35993817-d2f1-4d06-bfdd-2a874ec4869c.xhtml': 'If we call pij the transition probability\xa0P(i\xa0→ j), we define a recursive procedure considering the following probability: The variable fti represents the probability that the HMM is in the state i\xa0(at time t) after t observations (from 1 to t). Considering the HMM assumptions, we can state that\xa0fti\xa0depends on all possible\xa0ft-1j. More precisely, we have: With this process, we are considering that the HMM can reach any of the states at time t-1 (with the first t-1 observations), and transition to the state\xa0i at time t with probability pji. We need also to consider the emission probability for the final state ot conditioned to each of the possible previous states. For definition, the initial and ending states are not emitting. It means that we can write any sequence of observations as 0,\xa0o1, o2, ..., oSequence Length, 0, where the first and the final values are null. The procedure starts with computing the forward message at time 1: The non-emitting ending state must be also considered: The expression for the last state\xa0xEnding\xa0is interpreted here as the index of the ending state in both A and B matrices. For example, we indicate pij as A[i, j], meaning the transition probability at a generic time instant from the state xt = i to the state xt+1 = j. In the same way, piEnding is represented as A[i,\xa0xEnding], meaning the transition probability from the penultimate state xSequence Length-1 = i to the ending one\xa0xSequence Length = Ending State. The Forward algorithm can, therefore, be summarized in the following steps (we assume to have N states, hence we need to allocate N+2 positions, considering the initial and the ending states): Now it should be clear that the name forward derives from the procedure to propagate the information from the previous step to the next one, until the ending state, which is not emittied.', '7d7ff83e-319c-4cc5-adf2-e469748401b6.xhtml': 'During the backward phase, we need to compute the probability of a sequence starting at time t+1: ot+1, ot+2, ...,\xa0oSequence Length, given that the state at time t is i. Just like we have done before, we define the following probability: The backward algorithm is very similar to the forward one, but in this case, we need to move in the opposite direction, assuming we know that the state at time t is\xa0i. The first state to consider is the last one xEnding, which is not emitting, like the initial state; therefore we have: We terminate the recursion with the initial state: The steps are the following ones:\xa0', '7887d284-90be-401e-8513-c6babe61638e.xhtml': "Now that we have defined both the forward and the backward algorithms, we can use them to estimate the structure of the underlying HMM. The procedure is an application of the Expectation-Maximization algorithm, which will be discussed in the next chapter, Chapter 5,\xa0EM Algorithm and Applications, and its goal can be summarized as defining how we want to estimate the values of A and B. If we define N(i, j)\xa0as the number of transitions from the state i to the state j, and N(i) the total number of transitions from the state i, we can approximate the transition probability P(i\xa0→ j)\xa0with: In the same way, if we define M(i, p) the number of times we have observed the emission op in the state i, we can approximate the emission probability\xa0P(op|xi) with: Let's start with the estimation of the transition probability matrix A. If we consider the probability that the HMM is in the state i at time t, and in the state j at time t+1 given the observations, we have: We can compute this probability using the forward and backward algorithms, given a sequence of observations o1, o2, ..., oSequence Length. In fact, we can use both the forward message\xa0fti, which is the probability that the HMM is in the state i after t observations, and the backward message bt+1j, which is the probability of a sequence\xa0ot+1, ot+1, ..., oSequence Length starting at time t+1, given that the HMM is in state j at time t+1. Of course, we need also to include the emission probability and the transition probability pij, which is what we are estimating. The algorithm, in fact, starts with a random hypothesis and iterates until the values of A become stable. The estimation\xa0αij at time t is equal to: In this context, we are omitting the full proof due to its complexity; however, the reader can find it in\xa0A tutorial on hidden Markov models and selected applications in speech recognition, Rabiner L. R., Proceedings of the IEEE 77.2. To compute the emission probabilities, it's easier to start with the probability of being in the state i at time t given the sequence of observations: In this case, the computation is immediate, because we can multiply the forward and backward messages computed at the same time t and state i (remember that considering the observations, the backward message is conditioned to xt = i, while the forward message computes the probability of the observations joined with\xa0xt\xa0=\xa0i. Hence, the multiplication is the unnormalized probability of being in the state\xa0i\xa0at time t). Therefore, we have: The proof of how the normalizing constant is obtained can be found in the aforementioned paper. We can now plug these expressions to the estimation of aij and bip: In the numerator of the second formula, we adopted the indicator function (it's 1 only if the condition is true, 0 otherwise) to limit the sum only where those elements are ot = p. During an iteration k, pij is the estimated value aij found in the previous iteration k-1. The algorithm is based on the following steps: Alternatively, it's possible to fix the number of iterations, even if the best solution is using both a tolerance and a maximum number of iterations, to terminate the process when the first condition is met.", '0be4e2e1-4afb-40af-8da3-637fb8a9286a.xhtml': "For this example, we are going to use hmmlearn, which is a package for HMM computations (see the information box at the end of this section for further details). For simplicity, let's consider the airport example discussed in the paragraph about the Bayesian networks, and let's suppose we have a single hidden variable that represents the weather (of course, this is not a real hidden variable!), modeled as a multinomial distribution with two components (good and rough). We observe the arrival time of our flight London-Rome (which partially depends on the weather conditions), and we want to train an HMM to infer future states and compute the posterior probability of hidden states corresponding to a given sequence. The schema for our example is shown in the following diagram: Let's start by defining our observation vector. As we have two states, its values will be 0 and 1. Let's assume that 0 means On-time\xa0and 1 means\xa0Delay: We have 35 consecutive observations whose values are either 0 or 1. To build the HMM, we are going to use the MultinomialHMM class, with n_components=2,\xa0n_iter=100, and random_state=1000 (it's important to always use the same seed to avoid differences in the results). The number of iterations is sometimes hard to determine; for this reason, hmmlearn provides a utility ConvergenceMonitor\xa0class which can be checked to be sure that the algorithm has successfully converged. Now we can train our model using the\xa0fit()\xa0method, passing as argument the list of observations (the array must be always bidimensional with shape Sequence Length × NComponents): The process is very fast, and the monitor (available as instance variable monitor) has confirmed the convergence. If the model is very big and needs to be retrained, it's also possible to check smaller values of n_iter). Once the model is trained, we can immediately visualize the transition probability matrix, which is available as an instance variable transmat_: We can interpret these values as saying that the probability to transition from 0 (good weather) to 1 (rough weather) is higher (p01 is close to 1) than the opposite, and it's more likely to remain in state 1 than in state 0 (p00 is almost null). We could deduce that the observations have been collected during the winter period! After explaining the Viterbi algorithm\xa0in the next paragraph, we can also check, given some observations, what the most likely hidden state sequence is.", '758658c9-fe65-47ee-8ddd-95e52f481a86.xhtml': "The Viterbi algorithm is one of most common decoding algorithms for HMM. Its goal is to find the most likely hidden state sequence corresponding to a series of observations. The structure is very similar to the forward algorithm, but instead of computing the probability of a sequence of observations joined with the state at the last time instant, this algorithm looks for: The variable vti represents that maximum probability of the given observation sequence joint with xt = i, considering all possible hidden state paths (from time instant 1 to t-1). We can compute\xa0vti recursively by evaluating all the\xa0vt-1j\xa0multiplied by the corresponding transition probabilities pji and emission probability P(ot|xi), and always picking the maximum overall\xa0possible values of j: The algorithm is based on a backtracking approach, using a backpointer bpti whose recursive expression is the same as vti, but with the argmax function instead of max: Therefore,\xa0bpti\xa0represents\xa0the partial sequence of hidden states x1, x2, ...,\xa0xt-1\xa0that maximizes\xa0vti. During the recursion, we add the timesteps one by one, so the previous path could be invalidated by the last observation. That's why we need to backtrack the partial result and replace the sequence built at time t that doesn't maximize\xa0vt+1i anymore. The algorithm is based on the following steps (like in the other cases, the initial and ending states are not emitting): The output of the Viterbi algorithm is a tuple with the most likely sequence BP, and the corresponding probabilities V.", 'd0d50c53-ee64-43e1-b95c-d95df5e5f68a.xhtml': "At this point, we can continue with the previous example, using our model to find the most likely hidden state sequence given a set of possible observations. We can use either the decode() method or the predict()\xa0method. The first one returns the log probability of the whole sequence and the sequence itself; however, they all use the Viterbi algorithm as a default decoder: The sequence is coherent with the transition probability matrix; in fact, it's more likely the persistence of rough weather (1) than the opposite. As a consequence, the transition from 1 to X is less likely than the one from 0 to 1. The choice of state is made by selecting the highest probability; however, in some cases, the differences are minimal (in our example, it can happen to have p = [0.49, 0.51], meaning that there's a high error chance), so it's useful to check the posterior probabilities for all the states in the sequence: In our case, there are a couple of states that have p\xa0∼ [0.495, 0.505], so even if the output state is 1 (rough weather), it's also useful to consider a moderate probability to observe good weather. In general, if a sequence is coherent with the transition probability previously learned (or manually input), those cases are not very common. I suggest trying different configurations and observations sequences, and to also assess the probabilities for the strangest situations (like a sequence of zero second). At that point, it's possible to retrain the model and recheck the new evidence has been correctly processed.", 'b80f1999-6545-4f51-9d07-20fc530c0a06.xhtml': "In this chapter, we have introduced Bayesian networks, describing their structure and relations. We have seen how it's possible to build a network to model a probabilistic scenario where some elements can influence the probability of others. We have also described how to obtain the full joint probability using the most common sampling methods, which allow reducing the computational complexity through an approximation. The most common sampling methods belong to the family of MCMC algorithms, which model the transition probability from a sample to another one as a first-order Markov chain. In particular, the Gibbs sampler is based on the assumption that it's easier to sample from conditional distribution than work directly with the full joint probability. The method is very easy to implement, but it has some performance drawbacks that can be avoided by adopting more complex strategies. The Metropolis-Hastings sampler, instead, works with a candidate-generating distribution and a criterion to accept or reject the samples. Both methods satisfy the detailed balance equation, which guarantees the convergence (the underlying Markov chain will reach the unique stationary distribution). In the last part of the chapter, we introduced HMMs, which allow modeling time sequences based on observations corresponding to a series of hidden states. The main concept of such models, in fact, is the presence of unobservable states that condition the emission of a particular observation (which is observable). We have discussed the main assumptions and how to build, train, and infer from a model. In particular, the Forward-Backward algorithm can be employed when it's necessary to learn the transition probability matrix and the emission probabilities, while the Viterbi algorithm is adopted to find the most likely hidden state sequence given a set of consecutive observations. In the next chapter,\xa0Chapter 5,\xa0EM Algorithm and Applications, we're going to briefly discuss the Expectation-Maximization algorithm, focusing on some important applications based on the Maximum Likelihood Estimation (MLE) approach.", 'ddc93b54-a1d1-460f-9a64-abd8b633111a.xhtml': 'In this chapter, we are going to introduce a very important algorithmic framework for many statistical learning tasks: the EM algorithm. Contrary to its name, this is not a method to solve a single problem, but a methodology that can be applied in several contexts. Our goal is to explain the rationale and show the mathematical derivation, together with some practical examples. In particular, we are going to discuss the following topics:', '7dd71c22-73b1-4d61-9f7b-f32add19fb44.xhtml': "Let's suppose we have a data generating process pdata,\xa0used to draw a dataset X: In many statistical learning tasks, our goal is to find the optimal parameter set\xa0θ according to a maximization criterion. The most common approach is based on the likelihood and is called MLE. In this case, the optimal set θ is found as follows: This approach has the advantage of being unbiased by wrong preconditions, but, at the same time, it excludes any possibility of incorporating prior knowledge into the model. It simply looks for the best\xa0θ\xa0in a wider subspace, so that p(X|θ) is maximized. Even if this approach is almost unbiased, there's a higher probability of finding a sub-optimal solution that can also be quite different from a reasonable (even if not sure) prior. After all, several models are too complex to allow us to define a suitable prior probability (think, for example, of reinforcement learning strategies where there's a huge number of complex states). Therefore, MLE offers the most reliable solution. Moreover, it's possible to prove that the MLE of a parameter\xa0θ converges in probability to the real value: On the other hand, if we consider Bayes' theorem, we can derive the following relation: The posterior probability, p(θ|X), is obtained using both the likelihood and a prior probability, p(θ), and hence takes into account existing knowledge encoded in p(θ). The choice to maximize p(θ|X) is called the MAP approach and it's often a good alternative to MLE when it's possible to formulate trustworthy priors or, as in the case of Latent Dirichlet Allocation (LDA), where the model is on purpose based on some specific prior assumptions. Unfortunately, a wrong or incomplete prior distribution can bias the model leading to unacceptable results. For this reason, MLE is often the default choice even when it's possible to formulate reasonable assumptions on the structure of p(θ). To understand the impact of a prior on an estimation, let's consider to have observed n=1000 binomial distributed (θ corresponds to the parameter p) experiments and k=800 had a successful outcome. The likelihood is as follows: For simplicity, let's compute the log-likelihood:  If we compute the derivative with respect to\xa0θ and set it equal to zero, we get the following: So the MLE for\xa0θ is 0.8, which is coherent with the observations (we can say that after observing 1000 experiments with 800 successful outcomes, p(X|Success)=0.8). If we have only the data X, we could say that a success is more likely than a failure because 800 out of 1000 experiments are positive. However, after this simple exercise, an expert can tell us that, considering the largest possible population, the marginal probability p(Success)=0.001 (Bernoulli distributed with p(Failure) = 1 - P(success)) and our sample is not representative. If we trust the expert, we need to compute the posterior probability using Bayes' theorem: Surprisingly, the posterior probability is very close to zero and we should reject our initial hypothesis! At this point, there are two options: if we want to build a model based only on our data, the MLE is the only reasonable choice, because, considering the posterior, we need to accept we have a very poor dataset (this is probably a bias when drawing the samples from the data generating process pdata). On the other hand, if we really trust the expert, we have a few options for managing the problem: I suggest that the reader tries both approaches with simple models, to be able to compare the relative accuracies.\xa0In this book, we're always going to adopt the MLE when it's necessary to estimate the parameters of a model with a statistical approach. This choice is based on the assumption that our datasets are correctly sampled from pdata. If this is not possible (think about an image classifier that must distinguish between horses, dogs, and cats, built with a dataset where there are pictures of 500 horses, 500 dogs, and 5 cats), we should expand our dataset or use data augmentation techniques to create artificial\xa0samples.", '60b445ad-5252-4aaf-bb84-853b31f82c41.xhtml': "The EM algorithm is a generic framework that can be employed in the optimization of many generative models. It was originally proposed in\xa0Maximum likelihood from incomplete data via the em algorithm,\xa0Dempster A. P., Laird N. M., Rubin D. B., Journal of the Royal Statistical Society, B, 39(1):1–38, 11/1977, where the authors also proved its convergence at different levels of genericity. For our purposes, we are going to consider a dataset, X, and a set of latent variables, Z, that we cannot observe. They can be part of the original model or introduced artificially as a trick to simplify the problem. A generative model parameterized with the vector\xa0θ has a log-likelihood equal to the following: Of course, a large log-likelihood implies that the model is able to generate the original distribution with a small error. Therefore, our goal is to find the optimal set of parameters\xa0θ that maximizes the marginalized log-likelihood (we need to sum—or integrate out for continuous variables—the latent variables out because we cannot observe them): Theoretically, this operation is correct, but, unfortunately, it's almost always impracticable because of its complexity (in particular, the logarithm of a sum is often very problematic to manage). However, the presence of the latent variables can help us in finding a good proxy that is easy to compute and whose maximization corresponds to the maximization of the original log-likelihood. Let's start by rewriting the expression of the likelihood using the chain rule: If we consider an iterative process, our goal is to find a procedure that satisfies the following condition: We can start by considering a generic step: The first problem to solve is the logarithm of the sum. Fortunately, we can employ the Jensen's inequality, which allows us to move the logarithm inside the summation. Let's first define the concept of a\xa0convex function: a function, f(x), defined on a convex set, D, is said to be convex if the following applies:\xa0 If the inequality is strict, the function is said to be\xa0strictly convex. Intuitively, and considering a function of a single variable f(x), the previous definition states that the function is never above the segment that connects two points (x1, f(x1)) and\xa0(x2, f(x2)). In the case of strict convexity, f(x) is always below the segment. Inverting these definitions, we obtain the conditions for a function to be concave or strictly concave. If a function f(x) is concave in D, the function -f(x) is convex in D; therefore, as log(x) is concave in [0,\xa0∞) (or with an equivalent notation in [0,\xa0∞[), -log(x) is convex in [0,\xa0∞), as shown in the following diagram: The Jensen's inequality\xa0(the proof is omitted but further details can be found in Jensen's Operator Inequality, Hansen F., Pedersen G. K., arXiv:math/0204049 [math.OA]\xa0states that if f(x) is a convex function defined on a convex set D, if we select n points x1, x2, ..., xn\xa0∈ D and n constants λ1,\xa0λ2, ...,\xa0λn\xa0≥ 0 satisfying the condition\xa0λ1 + λ2 + ... + λn = 1, then the following applies: Therefore, considering that -log(x) is convex, the\xa0Jensen's inequality for log(x) becomes as follows: Hence, the generic iterative step can be rewritten, as follows: Applying the Jensen's inequality, we obtain the following: All the conditions are met, because the terms P(zi|X,\xa0θt) are, by definition, bounded between [0, 1] and the sum over all z must always be equal to 1 (laws of probability). The previous\xa0expression implies that the following is true: Therefore, if we maximize the right side of the inequality, we also maximize the log-likelihood. However, the problem can be further simplified, considering that we are optimizing only the parameter vector\xa0θ and we can remove all the terms that don't depend on it. Hence, we can define a Q function (there are no relationships with the Q-Learning that we're going to discuss in Chapter 14, Introduction to Reinforcement Learning) whose expression is as follows: Q is the expected value of the log-likelihood considering the complete data Y = (X, Z) and the current iteration parameter set\xa0θt. At each iteration, Q is computed considering the current estimation\xa0θt\xa0and it's maximized considering the variable\xa0θ.\xa0It's now clearer why the latent variables can be often\xa0artificially introduced: they allow us to apply the Jensen's inequality and transform the original expression into an expected value that is easy to evaluate and optimize. At this point, we can formalize the EM algorithm: The procedure ends when the log-likelihood stops increasing or after a fixed number of iterations.", '29d72916-f0ee-48c7-b713-6563a23095da.xhtml': "In this example, we see how it's possible to apply the EM algorithm for the estimation of unknown parameters (inspired by an example discussed in the original paper Maximum likelihood from incomplete data via the em algorithm,\xa0Dempster A. P., Laird N. M., Rubin D. B., Journal of the Royal Statistical Society, B, 39(1):1–38, 11/1977). Let's consider a sequence of n independent experiments modeled with a multinomial distribution with three possible outcomes x1, x2, x3 and corresponding probabilities p1, p2 and p3. The probability mass function is as follows: Let's suppose that we can observe z1 = x1\xa0+ x2 and x3, but we don't have any direct access to the single values x1 and x2. Therefore,\xa0x1 and x2\xa0are latent variables, while z1 and x3\xa0are observed ones. The probability vector p is parameterized in the following way: Our goal is to find the MLE for\xa0θ given n, z1, and\xa0x3. Let's start computing the log-likelihood: We can derive the expression for the corresponding Q function, exploiting the linearity of the expected value operator E[•]: The variables x1 and\xa0x2, given z1, are binomially distributed and can be expressed as a function of\xa0θt (we need to recompute them at each iteration). Hence, the expected value of x1(t+1) becomes as follows: While the expected value of x2(t+1) is as follows: If we apply these expressions in \xa0and compute the derivative with respect to\xa0θ, we get the following: Therefore, solving for\xa0θ, we get the following: At this point, we can derive the iterative expression for\xa0θ: Let's compute the value of\xa0θ for z1 = 50 and x3 = 10: In this example, we have parameterized all probabilities and, considering that\xa0z1\xa0= x1\xa0+ x2, we have one degree of freedom for the choice of\xa0θ. The reader can repeat the example by setting the value of one of\xa0p1 or\xa0p2 and leaving the other probabilities as functions of\xa0θ. The computation is almost identical but in this case, there are no degrees of freedom.", '8d5286bf-615b-4c16-9efb-0b01c71565ae.xhtml': "In Chapter 2, Introduction to Semi-Supervised Learning, we discussed the generative Gaussian mixture model in the context of semi-supervised learning. In this paragraph, we're going to apply the EM algorithm to derive the formulas for the parameter updates. Let's start considering a dataset, X, drawn from a data generating process, pdata: We assume that the whole distribution is generated by the sum of k Gaussian distributions so that the probability of each sample can be expressed as follows: In the previous expression, the term wj = P(N=j)\xa0is the relative weight of the jth Gaussian, while\xa0μj\xa0and\xa0Σj\xa0are the mean and the covariance matrix. For consistency with the laws of probability, we also need to impose the following: Unfortunately, if we try to solve the problem directly, we need to manage the logarithm of a sum and the procedure becomes very complex. However, we have learned that it's possible to use latent variables as helpers, whenever this trick can simplify the solution. Let's consider a single parameter set\xa0θ=(wj,\xa0μj,\xa0Σj) and a latent indicator matrix Z where each element zij is equal to 1 if the point xi\xa0has been generated by the jth Gaussian, and 0 otherwise. Therefore, each zij\xa0is Bernoulli distributed with parameters equal to p(j|xi,\xa0θt). The joint log-likelihood can hence be expressed using the exponential-indicator notation, as follows: The index, i, is referred to the samples, while j refers to the Gaussian distributions. If we apply the chain rule and the properties of a logarithm, the expression becomes as follows: The first term represents the probability of xi\xa0under the jth Gaussian, while the second one is the relative weight of the jth Gaussian. We can now compute the Q(θ;θt) function using the joint log-likelihood: Exploiting the linearity of E[•], the previous expression becomes as follows: The term\xa0p(j|xi,\xa0θt)\xa0corresponds to the expected value of zij\xa0considering the complete data, and expresses the probability of the jth Gaussian given the sample xi. It can be simplified considering Bayes' theorem: The first term is the probability of\xa0xi\xa0under the jth Gaussian with parameters\xa0θt, while the second one is the weight of the jth Gaussian considering the same parameter set\xa0θt. In order to derive the iterative expressions for the parameters, it's useful to write the complete formula for the logarithm of a multivariate Gaussian distribution: To simplify this expression, we use the trace trick. In fact, as (xi -\xa0μj)T\xa0Σ-1\xa0(xi\xa0-\xa0μj) is a scalar, we can exploit the properties\xa0tr(AB) = tr(BA) and tr(c) = c where A and B are matrices and c ∈ ℜ: Let's start considering the estimation of the mean (only the first term of Q(θ;θt) depends on mean and covariance): Setting the derivative equal to zero, we get the following: In the same way, we obtain the expression of the covariance matrix: To obtain the iterative expressions for the weights, the procedure is a little bit more complex, because we need to use the Lagrange multipliers\xa0(further information can be found in http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html). Considering that the sum of the weights must always be equal to 1, it's possible to write the following equation: Setting both derivatives equal to zero, from the first one, considering that wj = p(j|θ), we get the following: While from the second derivative, we obtain the following: The last step derives from the fundamental condition:\xa0 Therefore, the final expression of the weights is as follows: At this point, we can formalize the\xa0Gaussian mixture algorithm: The process must be iterated until the parameters become stable. In general, the best practice is using both a threshold and a maximum number of iterations.", '795a9155-d05f-477d-aabb-9e168e115661.xhtml': "We can now implement the Gaussian mixture algorithm using the Scikit-Learn implementation. The direct approach has already been shown in Chapter 2, Introduction to Semi-Supervised Learning. The dataset is generated to have three cluster centers and a moderate overlap due to a standard deviation equal to 1.5: The corresponding plot is shown in the following diagram: The Scikit-Learn implementation is based on the GaussianMixture\xa0class\xa0, which accepts as parameters the number of Gaussians (n_components), the type of covariance (covariance_type), which can be full\xa0(the default value), if all components have their own matrix, tied if the matrix is shared, diag if all components have their own diagonal matrix (this condition imposes an uncorrelation among the features), and spherical\xa0when each Gaussian is symmetric in every direction. The other parameters allow setting regularization and initialization factors (for further information, the reader can directly check the documentation). Our implementation is based on full covariance: After fitting the model, it's possible to access to the learned parameters through the instance variables weights_, means_, and covariances_: Considering the covariance matrices, we can already understand that the features are very uncorrelated and the Gaussians are almost spherical. The final plot can be obtained by assigning each point to the corresponding cluster (Gaussian distribution) through the\xa0Yp = gm.transform(X) command: The reader should have noticed a strong analogy between Gaussian mixture and k-means\xa0(which we're going to discuss in Chapter 7, Clustering Algorithms). In particular, we can state that K-means is a particular case of spherical Gaussian mixture with a covariance Σ → 0. This condition transforms the approach from a soft clustering, where each sample belongs to all clusters with a precise probability distribution, into a hard clustering, where the assignment is done by considering the shortest distance between sample and centroid (or mean). For this reason, in some books, the Gaussian mixture algorithm is also called soft K-means.\xa0A conceptually similar approach that we are going to present is Fuzzy K-means, which is based on assignments characterized by membership functions, which are analogous to probability distributions.", '572d12e6-7b8f-41ba-a20d-82979fc99798.xhtml': "Let's suppose we have a Gaussian data generating process, pdata\xa0∼ N(0,\xa0Σ), and M n-dimensional zero-centered samples drawn from it: If pdata\xa0has a mean\xa0μ\xa0≠ 0, it's also possible to use this model, but it's necessary to account for this non-null value with slight changes in some formulas. As the zero-centering normally has no drawbacks, it's easier to remove the mean to simplify the model. One of the most common problems in unsupervised learning is finding a lower dimensional distribution plower such that the Kullback-Leibler divergence with pdata is minimized. When performing a factor analysis\xa0(FA), following the original proposal published in EM algorithms for ML factor analysis, Rubin D., Thayer D., Psychometrika, 47/1982, Issue 1, and\xa0The EM algorithm for Mixtures of Factor Analyzers, Ghahramani Z., Hinton G. E., CRC-TG-96-1, 05/1996, we start from the assumption to model the generic sample x as a linear combination of Gaussian latent variables, z, (whose dimension p is normally p < n) plus an additive and decorrelated Gaussian noise term, ν: The matrix, A, is called a\xa0factor loading matrix because it determines the contribution of each latent variable (factor) to the reconstruction of x. Factors and input data are assumed to be statistically independent. Instead, considering the last term, if ω02\xa0≠ ω12\xa0≠ ...\xa0≠ ωn2 the noise is called heteroscedastic, while it's defined homoscedastic if the variances are equal\xa0ω02\xa0= ω12\xa0= ... = ωn2\xa0= ω2. To understand the difference between these two kinds of noise, think about a signal x which is the sum of two identical voices, recorded in different places (for example, an airport and a wood). In this case, we can suppose to also have different noise variances (the first one should be higher than the second considering the number of different noise sources). If instead both voices are recorded in a soundproofed room or even in the same airport, homoscedastic noise is surely more likely (we're not considering the power, but the difference between the variances). One of the most important strengths of FA in respect to other methods (such as PCA) is its intrinsic robustness to heteroscedastic noise. In fact, including the noise term in the model (with only the constraint to be decorrelated) allows partial denoising filtering based on the single components, while one of the\xa0preconditions for the PCA is to impose only homoscedastic noise (which, in many cases, is very similar to the total absence of noise). Considering the previous example, we could make the assumption to have the first variance be\xa0ω02 = k ω12 with k > 1. In this way, the model will be able to understand that a high variance in the first component should be considered (with a higher probability) as the product of the noise and not an intrinsic\xa0property of the component. Let's now analyze the linear relation: Considering the properties of Gaussian distributions, we know that x\xa0∼ N(μ,\xa0Σ) and it's easy to determine either the mean or the covariance matrix: Therefore, in order to solve the problem, we need to find the best\xa0θ=(A,\xa0Ω) so that AAT +\xa0Ω\xa0≈\xa0Σ (with a zero-centered dataset, the estimation is limited to the input covariance matrix\xa0Σ).The ability to cope with noisy variables should be clearer now. If\xa0AAT\xa0+\xa0Ω is exactly equal to Σ and the estimation of\xa0Ω is correct, the algorithm will optimize the factor loading matrix A, excluding the interference produced by the noise term; therefore, the components will be approximately denoised. In order to adopt the EM algorithm, we need to determine the joint probability p(X, z;\xa0θ) = p(X|z; θ)p(z|θ). The first term on the right side can be easily determined, considering that x - Az\xa0∼ N(0,\xa0Ω); therefore, we get the following: We can now determine the Q(θ;θt) function, discarding the constant term (2π)k and term zTz, which don't depend on\xa0θ (in this particular case, as we're going to see, we don't need to compute the probability\xa0p(z|X;θ) because it's enough to obtain sufficient statistics for expected value and second moment).\xa0Moreover, it's useful to expand the multiplication in the exponential: Using the trace trick with the last term (which is a scalar), we can rewrite it as follows: Exploiting the linearity of E[•], we obtain the following: This expression is similar to what we have seen in the Gaussian mixture model, but in this case, we need to compute the conditional expectation and the conditional second moment of z. Unfortunately, we cannot do this directly, but it's possible to compute them exploiting the joint normality of x and z. In particular, using a classic theorem, we can partition the full joint probability p(z, x), considering the following relations: The conditional distribution p(z|x=xi) has a mean equal to the following: The conditional variance is as follows: Therefore, the conditional second moment is equal to the following: If we define the auxiliary matrix K = (AAT +\xa0Ω)-1, the previous expressions become as follows: The reader in search of further details about this technique can read Preview\nIntroduction to Statistical Decision Theory, Pratt J., Raiffa H., Schlaifer R., The MIT Press. Using the previous expression, it's possible to build the inverse model (sometimes called a\xa0recognition model\xa0because it starts with the effects and rebuilds the causes), which is still Gaussian distributed: We are now able to maximize\xa0Q(θ;θt) with respect to A and\xa0Ω, considering θt=(At,\xa0Ωt) and both the conditional expectation and the second moment computed according to the previous estimation θt-1=(At-1,\xa0Ωt-1). For this reason, they are not involved in the derivation process. We are adopting the convention that the term subject to maximization is computed at time\xa0t, while all the others are obtained through the previous estimations (t - 1): The expression for At\xa0is therefore as follows (Q is the biased input covariance matrix E[XTX] for a zero-centered dataset): In the same way, we can obtain an expression for\xa0Ωt by computing the derivative with respect to\xa0Ω-1 (this choice simplifies the calculation and doesn't affect the result, because we must set the derivative equal to zero): The derivative of the first term, which is the determinant of a real diagonal matrix, is obtained using the adjugate matrix Adj(Ω) and exploiting the properties of the inverse matrix T-1\xa0= det(T)-1Adj(T) and the properties det(T)-1\xa0= det(T-1) and\xa0det(TT) = det(T): The expression for\xa0Ωt (imposing the diagonality constraint) is as follows: Summarizing the steps, we can define the complete\xa0FA\xa0algorithm: The process must be repeated until\xa0A(t),\xa0Ω(t),\xa0and K(t) stop modifying their values (using a threshold) together with a constraint on the maximum number of iterations. The factors can be easily obtained using the inverse model z = Bx + λ.", '19677e08-9e9d-4ec6-95e2-a64a75dcfcb8.xhtml': 'We can now make an example of FA\xa0with Scikit-Learn using the MNIST handwritten digits dataset (70,000 28 × 28 grayscale images) in the original version and with added heteroscedastic noise (ωi\xa0randomly selected from [0, 0.75]). The first step is to load and zero-center the original dataset (I\'m using the functions defined in the first chapter, Chapter 1, Machine Learning Model Fundamentals): After this step, the\xa0\xa0X\xa0variable\xa0\xa0will contain the zero-center original dataset, while Xh is the noisy version. The following screenshot shows a random selection of samples from both versions: We can perform FA on both datasets using the Scikit-Learn FactorAnalysis class with the n_components=64 parameter and check the score (the average log-likelihood over all samples). If the noise variance is known (or there\'s a good estimation), it\'s possible to include the starting point through the noise_variance_init parameter; otherwise, it will be initialized with the identity matrix: As expected, the presence of noise has reduced the final accuracy (MLE). Following an example provided by A. Gramfort and D. A. Engemann in the original Scikit-Learn documentation, we can create a benchmark\xa0for the MLE using the Lodoit-Wolf algorithm (a shrinking method for improving the condition of the covariance that is beyond the scope of this book. For further information, read\xa0A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices, Ledoit O., Wolf M., Journal of Multivariate Analysis, 88, 2/2004": With the original dataset, FA performs much better than the benchmark, while it\'s slightly worse in the presence of heteroscedastic noise. The reader can try other combinations using the grid search with different numbers of components and noise variances, and experiment with the effect of removing the zero-centering step. It\'s possible to plot the extracted components using the components_ instance variable: A careful analysis shows that the components are a superimposition of many low-level visual features. This is a consequence of the assumption to have a Gaussian prior distribution over the components (z\xa0∼ N(0, I)). In fact, one of the disadvantages of this distribution is its intrinsic denseness (the probability of sampling values far from the mean is often too high, while in some case, it would be desirable to\xa0have a peaked distribution that discourages values not close to its mean, to be able to observe more selective components). Moreover, considering the distribution p[Z|X; θ], the covariance matrix ψ could not be diagonal (trying to impose this constraint can lead to an unsolvable problem), leading to a resulting multivariate Gaussian distribution, which isn\'t normally made up of independent components. In general, the single variables\xa0zi, (conditioned to an input sample, xi) are statistically dependent and the reconstruction\xa0xi, is obtained with the participation of almost all extracted features. In all these cases, we say that the coding is dense and the dictionary of features in under-complete (the dimensionality of the components is lower than dim(xi)). The lack of independence can be also an issue considering that any orthogonal transformation Q applied to A (the factor loading matrix) don\'t affect the distribution p[X|Z,\xa0θ]. In fact, as QQT=I,\xa0the following applies: In other words, any feature rotation (x = AQz + ν) is always a solution to the original problem and it\'s impossible to decide which is the real loading matrix. All these conditions lead to the further conclusion that the mutual information among components is not equal to zero and neither close to a minimum (in this case, each of them carries a specific portion of information). On the other side, our main goal was to reduce the dimensionality. Therefore, it\'s not surprising to have dependent components because we aim to preserve the maximum amount of original information contained in p(X) (remember that the amount of information is related to the entropy and the latter is proportional to the variance). The same phenomenon can be observed in the PCA (which is still based on the Gaussian assumption), but in the last paragraph, we\'re going to discuss a technique, called\xa0ICA, whose goal is to create a representation\xa0of each sample\xa0(without the constraint of the dimensionality reduction) after starting from a set of statistically independent features. This approach, even if it has its peculiarities, belongs to a large family of algorithms called sparse coding. In this scenario, if the corresponding dictionary has dim(zi) > dim(xi),it is called over-complete (of course, the main goal is no longer the dimensionality reduction). However, we\'re going to consider only the case when the dictionary is at most complete\xa0dim(zi) = dim(xi), because ICA with over-complete dictionaries requires a more complex approach. The level of sparsity, of course, is proportional to\xa0dim(zi) and with ICA, it\'s always achieved as a secondary goal (the primary one is always the independence between components).', 'ff47c95d-8b43-4633-9d6d-7024c31f4562.xhtml': "Another common approach to the problem of reducing the dimensionality of a high-dimensional dataset is based on the assumption that, normally, the total variance is not explained equally by all components. If pdata is a multivariate Gaussian distribution with covariance\xa0matrix\xa0Σ, then the entropy (which is a measure of the amount of information contained in the distribution) is as follows: Therefore, if some components have a very low variance, they also have a limited contribution to the entropy, providing little additional information. Hence, they can be removed without a high loss of accuracy. Just as we've done with FA, let's consider a dataset drawn from pdata\xa0∼ N(0, Σ)\xa0(for simplicity, we assume that it's zero-centered, even if it's not necessary): Our goal is to define a linear transformation, z = ATx\xa0(a vector is normally considered a column, therefore x has a shape (n × 1)), such as the following: As we want to find out the directions where the variance is higher, we can build our transformation matrix, A, starting from the eigen decomposition of the input covariance matrix, Σ (which is real, symmetric, and positive definite): V is an (n\xa0× n) matrix containing the eigenvectors (as columns), while\xa0Ω is a diagonal matrix containing the eigenvalues. Moreover, V is also orthogonal, hence the eigenvectors constitute a basis. An alternative approach is based on the singular value decomposition (SVD), which has an incremental variant and there are algorithms that can perform a decomposition truncated at an arbitrary number of components, speeding up the convergence process (such as the Scikit-Learn implementation\xa0TruncatedSVD). In this case, it's immediately noticeable that the sample covariance is as follows: If we apply the SVD to the matrix X (each row represents a single sample with a shape (1, n)), we obtain the following: U is a unitary matrix containing (as rows) the left singular vectors (the eigenvectors of XXT), V (also unitary) contains (as rows) the right singular vectors (corresponding to the eigenvectors of XTX), while\xa0Λ is a diagonal matrix containing the singular values of\xa0Σs (which are the square roots of the eigenvalues of both XXT\xa0and XTX). Conventionally, the eigenvalues are sorted by descending order and the eigenvectors are rearranged to match the corresponding position. Hence, we can directly use the matrix\xa0Λ to select the most relevant eigenvalues (the square root is an increasing function and doesn't change the order) and the matrix V to retrieve the corresponding eigenvectors (the factor 1/M is a proportionality constant). In this way, we don't need to compute and eigen decompose the covariance matrix\xa0Σ (contains n\xa0× n elements) and we can exploit some very fast approximate algorithms that work only with the dataset (without computing XTX). Using the SVD, the transformation of X can be done directly, considering that U and V are unitary matrices (this means that UUT\xa0= UTU = I; therefore, the conjugate transpose is also the inverse): Right now, X has only been projected in the eigenvector space (it has been simply rotated) and its dimensionality hasn't changed. However, from the definition of the eigenvector, we know that the following is true: If\xa0λ is large, the projection of v will be amplified proportionally to the variance explained by the direction of the corresponding eigenvector. Therefore, if it has not been already done, we can sort (and rename) the eigenvalues and the corresponding eigenvectors to have the following: If we select the first top k eigenvalues, we can build a transformation matrix based on the corresponding eigenvectors (principal components) that projects X onto a subspace of the original eigenvector space: Using the SVD, instead of Ak, we can directly truncate U and\xa0Λ, creating the matrices Uk (which contains only the top k eigenvectors) and\xa0Λk, a diagonal matrix with the top k eigenvalues. When choosing the value for k, we are assuming that the following is true: To achieve this goal, it is normally necessary to compare the performances with a different number of components. In the following graph, there's a plot where the variance ratio (variance explained by component n/total variance) and the cumulative variance are plotted as functions of the components: In this case, the first 10 components are able to explain 80% of the total variance. The remaining 25 components have a slighter and slighter impact and could be removed. However, the choice must be always based on the specific context, considering the loss of value induced by the loss of information. Once we've defined the transformation matrix Ak, it's possible to perform the actual projection of the original vectors in the new subspace, through the relation: The complete transformation of the whole dataset is simply obtained as follows: Now, let's analyze the new covariance matrix E[ZTZ]. If the original distribution pdata\xa0x\xa0∼ N(0,\xa0Σ),\xa0p(z) will also be Gaussian with mean and covariance: We know that\xa0Σ is orthogonal; therefore, vi\xa0• vj = 0 if i\xa0≠ j. If we analyze the term ATV, we get the following: Considering that\xa0Ω is diagonal, the resulting matrix\xa0Σz will be diagonal as well. This means that the PCA decorrelates the transformed covariance matrix. At the same time, we can state that every algorithm that decorrelates the input covariance matrix performs a PCA (with or without dimensionality reduction). For example, the whitening process is a particular PCA without dimensionality reduction, while Isomap (see\xa0Chapter 3,\xa0Graph-Based Semi-Supervised Learning) performs the same operation working with the Gram matrix with a more geometric approach. This result will be used in Chapter 6, Hebbian Learning, to show how some particular neural networks can perform a PCA without eigen decomposing\xa0Σ. Let's now consider a FA with homoscedastic noise. We have seen that the covariance matrix of the conditional distribution, p(X|Z;\xa0θ), is equal to AAT +\xa0Ω. In the case of homoscedastic noise, it becomes AAT +\xa0ωI. For a generic covariance matrix, Σ, it's possible to prove that adding a constant diagonal matrix (Σ + aI)\xa0doesn't modify the original eigenvectors and shifts the eigenvalues by the same quantity: Therefore, we can consider the generic case of absence of noise without loss of generality. We know that the goal of FA (with\xa0Ω = (0)) is finding the matrix, A, so that AAT\xa0≈ Q\xa0(the input covariance). Hence, thanks to the symmetry and imposing the asymptotic equality, we can write the following: This result implies that the FA is a more generic (and robust) way to manage the dimensionality reduction in the presence of heteroscedastic noise, and the PCA is a restriction to homoscedastic noise. When a PCA is performed on datasets affected by heteroscedastic noise, the MLE worsens because the different noise components, altering the magnitude of the eigenvalues at different levels, can drive to the selection of eigenvectors that, in the original dataset, explain only a low percentage of the variance (and in a noiseless scenario, it would be normally discarded in favor of more important directions). If you think of the example discussed at the beginning of the previous paragraph, we know that the noise is strongly heteroscedastic, but we don't have any tools to inform the PCA to cope with it and the variance of the first component will be much higher than expected, considering that the two sources are identical. Unfortunately, in a real- life scenario, the noise is correlated and neither a factor nor a PCA can efficiently solve the problem when the noise power is very high. In all those cases, more sophisticated denoising techniques must be employed. Whenever, instead, it's possible to define an approximate diagonal noise covariance matrix, FA is surely more robust and efficient than PCA. The latter should be considered only in noiseless or quasi-noiseless scenarios. In both cases, the results can never lead to well-separated features. For this reason, the ICA has been studied and many different strategies have been engineered. The complete algorithm for the PCA is as follows:", '42f7ce38-337f-4850-b8be-dfa67a678f31.xhtml': "We can repeat the same experiment made with the FA and heteroscedastic noise to assess the MLE score of the PCA. We are going to use the PCA class with the same number of components (n_components=64). To achieve the maximum accuracy, we also set the\xa0\xa0svd_solver='full'\xa0parameter, to force Scikit-Learn to apply a full SVD instead of the truncated version. In this way, the top eigenvalues are selected only after the decomposition, avoiding the risk of imprecise estimations: The result is not surprising: the MLE is much lower than FA, because of the wrong estimations made due to the heteroscedastic noise. I invite the reader to compare the results with different datasets and noise levels, considering that the training performance of PCA is normally higher than FA. Therefore, when working with large datasets, a good trade-off is surely desirable. As with FA, it's possible to retrieve the components through the\xa0\xa0components_\xa0instance variable. It's interesting to check the total explained variance (as a fraction of the total input variance) through the component-wise instance array explained_variance_ratio_: With 64 components, we are explaining 86% of the total input variance. Of course, it's also useful to compare the explained variance using a plot: As usual, the first components explain the largest part of the variance; however, after about the twentieth component, each contribution becomes lower than 1% (decreasing till about 0%). This analysis suggests two observations: it's possible to further reduce the number of components with an acceptable loss (using the previous snippet, it's easy to extend the sum only the first n components and compare the results) and, at the same time, the PCA will be able to overcome a higher threshold (such as 95%) only by adding a large number of new components. In this particular case, we know that the dataset is made up of handwritten digits; therefore, we can suppose that the tail is due to secondary differences (a line slightly longer than average, a marked stroke, and so on); hence, we can drop all the components with n > 64 (or less) without problems (it's also easy to verify visually a rebuilt image using the inverse_transform() method). However, it is always best practice to perform a complete analysis before moving on to further processing steps, particularly when the dimensionality of X is high.", 'c509b744-2106-4b88-92e1-40b33e142a11.xhtml': "We have seen that the factors extracted by a PCA are decorrelated, but not independent. A classic example is the cocktail party: we have a recording of many overlapped voices and we would like to separate them. Every single voice can be modeled as a random process and it's possible to assume that they are statistically independent (this means that the joint probability can be factorized using the marginal probabilities of each source). Using FA or PCA, we are able to find uncorrelated factors, but there's no way to assess whether they are also independent (normally, they aren't). In this section, we are going to study a model that is able to produce sparse representations (when the dictionary isn't under-complete) with a set of statistically independent components. Let's assume we have a zero-centered and whitened dataset X sampled from N(0, I) and noiseless linear transformation: In this case, the prior over, z, is modeled as a product of independent variables (α is the normalization factor), each of them represented as a generic exponential where the function fk(z) must be non-quadratic, that is,\xa0p(z;\xa0θ) cannot be Gaussian. Furthermore, we assume that the variance of\xa0zi is equal to 1, therefore, p(x|z;\xa0θ)\xa0∼ N(Az, AAT). The joint probability\xa0p(X, z;\xa0θ) = p(X|z; θ)p(z|θ) is equal to the following: If X has been whitened, A is orthogonal (the proof is straightforward); hence, the previous expression can be simplified. However, applying the EM algorithm requires determining p(z|X;\xa0θ) and this is quite difficult. The process could be easier after choosing a suitable prior distribution for z, that is, fk(z), but as we discussed at the beginning of the chapter, this assumption can have dramatic consequences if the real factors are distributed differently. For these reasons, other strategies have been studied. The main concept that we need to enforce is having a non-Gaussian distribution of the factors. In particular, we'd like to have a peaked distribution (inducing sparseness) with heavy tails. From the theory, we know that the standardized fourth moment (also called Kurtosis) is a perfect measure: For a Gaussian distribution, Kurt[X] is equal to three (which is often considered as the reference point, determining the so called Excess Kurtosis = Kurtosis - 3), while it's larger for a family of distributions, called Leptokurtotic or super-Gaussian, which are peaked and heavy-tailed (also, the distributions with Kurt[X] < 3, called Platykurtotic or sub-Gaussian, can be good candidates, but they are less peaked and normally only the super-Gaussian distributions are taken into account).\xa0However, even if accurate, this measure is very sensitive to outliers because of the fourth power. For example, if x\xa0∼ N(0, 1) and z = x +\xa0ν, where\xa0ν is a noise term that alters a few samples, increasing their value to two, the result can be a super-Gaussian (Kurt[x] > 3) even if, after filtering the outliers out, the distribution has Kurt[x] = 3 (Gaussian). To overcome this problem,\xa0Hyvärinen and Oja\xa0(Independent Component Analysis: Algorithms and Applications, Hyvarinen A., Oja E., Neural Networks 13/2000) proposed a solution based on another measure, the negentropy. We know that the entropy is proportional to the variance and, given the variance, the Gaussian distribution has the maximum entropy (for further information, read\xa0Mathematical Foundations of Information Theory,\xa0Khinchin A. I., Dover Publications); therefore, we can define the measure: Formally, the negentropy of X is the difference between the entropy of a Gaussian distribution with the same covariance and the entropy of X (we are assuming both zero-centered). It's immediately possible to understand that HN(X)\xa0≥ 0, hence the only way to maximize it is by reducing H(X). In this way, X becomes less random, concentrating the probability around the mean (in other words, it becomes super-Gaussian). However, the previous expression cannot be easily adapted to closed-form solutions, because H(X) needs to be computed over all the distribution of X, which must be estimated. For this reason, the same authors proposed an approximation based on non-quadratic functions (remember that in the context of ICA, a quadratic function can be never be employed because it would lead to a Gaussian distribution) that is useful to\xa0derive a fixed-point iterative algorithm called FastICA\xa0(indeed, it's really faster than EM). Using k functions fk(x), the approximation becomes as follows: In many real-life scenarios, a single function is enough to achieve a reasonable accuracy and one of the most common choices for f(x) is as follows: In the aforementioned paper, the reader can find some alternatives that can be employed when this function fails in forcing statistical independence between components. If we invert the model, we get z = Wx with W = A-1; therefore, considering a single sample, the approximation becomes as follows: Clearly, the second term doesn't depend on w (in fact, it's only a reference) and can be excluded from the optimization. Moreover, considering the initial assumptions, E[ZTZ]=W E[XTX] WT = I, therefore WWT = I, i.e. ||w||2 = 1. Hence, our goal is to find the following: In this way, we are forcing the matrix W to transform the input vector x, so that z has the lowest possible entropy; therefore, it's super-Gaussian. The maximization process is based on convex optimization techniques that are beyond the scope of this book (the reader can find all the details of Lagrange theorems in Luenberger D. G., Optimization by Vector Space Methods, Wiley); therefore, we directly provide the iterative step that must be performed: Of course, to ensure\xa0||w||2\xa0= 1,\xa0after each step, the weight vector w must be normalized (wt+1 = wt+1 / ||wt+1||). In a more general context, the matrix W contains more than one weight vector and, if we apply the previous rule to find out the independent factors, it can happen that some elements, wiTx, are correlated. A strategy to avoid this problem is based on the\xa0gram-schmidt orthonormalization process, which decorrelates the components one by one, subtracting the projections of the current component (wn) onto all the previous ones (w1, w2, ..., wn-1) to wn. In this way,\xa0wn is forced to be orthogonal to all the other components. Even if this method is simple and doesn't require much effort, it's preferable a global\xa0approach that can work directly with the matrix W at the end of an iteration (so that the order of the weights is not fixed). As explained in Fast and robust fixedpoint\nalgorithms for independent component analysis, Hyvarinen A., IEEE Transactions on Neural Networks this result can be achieved with a simple sub-algorithm that we are including in the final FastICA algorithm: This process can be also iterated for a fixed number of times, but the best approach is based on using both a threshold and a maximum number of iterations.", '753ef7f9-b52e-40aa-8ca3-41c298ea1642.xhtml': "Using the same dataset, we can now test the performance of the ICA. However, in this case, as explained, we need to zero-center and whiten the dataset, but fortunately these preprocessing steps are done by the\xa0Scikit-Learn implementation (if the parameter whiten=True\xa0is omitted). To perform the ICA on the MNIST dataset, we're going to instantiate the\xa0\xa0FastICA\xa0class, passing the arguments n_components=64 and the maximum number of iterations max_iter=5000. It's also possible to specify which function will be used to approximate the negentropy; however, the default is log cosh(x), which is normally a good choice: At this point, we can visualize the components (which are always available through the\xa0\xa0components_\xa0instance variance): There are still some redundancies (the reader can try to increase the number of components) and background noise; however, it's now possible to distinguish some low-level features (such as oriented stripes) that are common to many digits. This representation isn't very sparse yet. In fact, we're always using 64 components (like for FA and PCA); therefore, the dictionary is under-complete (the input dimensionality is 28\xa0× 28 = 784). To see the difference, we can repeat the experiment with a dictionary ten times larger, setting\xa0n_components=640: A subset of the new components (100) is shown in the following screenshot: The structure of these components is almost elementary. They represent oriented stripes and positional dots. To check how an input is rebuilt, we can consider the mixing matrix A (which is available as the\xa0mixing_\xa0instance variable). Considering the first input sample, we can check how many factors have a weight less than half of the average: The sample is rebuilt using approximately 410 components. The level of sparsity is higher, but considering the granularity of the factors, it's easy to understand that many of them are needed to rebuild even a single structure (like the image of a 1) where long lines are present. However, this is not a drawback because, as already mentioned, the main goal of the ICA is to extract independent components. Considering an analogy with the cocktail party example, we could deduce that each component represents a phoneme, not the complete sound of a word or a sentence. The reader can test a different number of components and compare the results with the ones achieved by other sparse coding algorithms (such as Dictionary Learning or Sparse PCA).", 'da99c008-a7e0-48e9-ba4c-48b35eae4e0b.xhtml': "In the previous chapter, we\xa0discussed how it's possible to train a HMM using the forward-backward algorithm\xa0and we have seen that it is a particular application of the EM algorithm. The reader can now understand the internal dynamic in terms of E and M steps. In fact, the procedure starts with randomly initialized A and B matrices and proceeds in an alternating manner: The procedure is repeated until the convergence is reached. Even if there's no explicit definition of a Q function, the E-step determines a split expression for the expected complete data likelihood of the model given the observations (using both the Forward and Backward algorithms), while the M-Step corrects parameters A and B to maximize this likelihood.", 'ef45c532-1914-4d8f-a0c0-9a0ce86d7381.xhtml': "In this chapter, we presented the EM algorithm, explaining the reasons that justify its application in many statistical learning contexts. We also discussed the fundamental role of hidden (latent) variables, in order to derive an expression that is easier to maximize (the Q function). We applied the EM algorithm to solve a simple parameter estimation problem and afterward to prove the Gaussian Mixture estimation formulas. We showed how it's possible to employ the Scikit-Learn implementation instead of writing the whole procedure from scratch (like in Chapter 2,Â\xa0Introduction to Semi-Supervised Learning). Afterward, we analyzed three different approaches to component extraction. FA assumes that we have a small number of Gaussian latent variables and a Gaussian decorrelated noise term. The only restriction on the noise is to have a diagonal covariance matrix, so two different scenarios are possible. When we are in the presence of heteroscedastic noise, the process is an actual FA. When, instead, the noise is homoscedastic, the algorithm becomes the equivalent of a PCA. In this case, the process is equivalent to check the sample space in order to find the directions where the variance is higher. Selecting only the most important directions, we can project the original dataset onto a low-dimensional subspace, where the covariance matrix becomes decorrelated. One of the problems of both FA and PCA is their assumption to model the latent variables with Gaussian distributions. This choice simplifies the model, but at the same time, yields dense representations where the single components are statistically dependent. For this reason, we have investigated how it's possible to force the factor distribution to become sparse. The resulting algorithm, which is generally faster and more accurate than the MLE, is called FastICA and its goal is to extract a set of statistically independent components with the maximization of an approximation of the negentropy. In the end, we provided a brief explanation of the HMM forward-backward algorithm (discussed in the previous chapter) considering the subdivision into E and M steps. Other EM-specific applications will be discussed in the next chapters. In the next chapter, we are going to introduce the fundamental concepts of Hebbian learning and self-organizing maps, which are still very useful to solve many specific problems, such as principal component extraction, and have a strong neurophysiological foundation.", '74b29e86-0a29-4711-8494-82b9f3492352.xhtml': "In this chapter, we're going to introduce the concept of Hebbian learning, based on the methods defined by the psychologist Donald Hebb. These theories immediately showed how a very simple biological law is able to describe the behavior of multiple neurons in achieving complex goals and was a pioneering strategy that linked the research activities in the fields of artificial intelligence and computational neurosciences. In particular, we are going to discuss the following topics:", 'ec8e9853-28d3-430d-ab2b-3e8a1712d969.xhtml': "Hebb's rule has been proposed as a conjecture in 1949 by the Canadian psychologist Donald Hebb to describe the synaptic plasticity of natural neurons. A few years after its publication, this rule was confirmed by neurophysiological studies, and many research studies have shown its validity in many application, of Artificial Intelligence. Before introducing the rule, it's useful to describe the generic Hebbian neuron, as shown in the following diagram: The neuron is a simple computational unit that receives an input vector\xa0x, from the pre-synaptic units (other neurons or perceptive systems) and outputs a single scalar value, y. The internal structure of the neuron is represented by a weight vector, w, that models the strength of each synapse. For a single multi-dimensional input, the output is obtained as follows:\xa0 In this model, we are assuming that each input signal is encoded in the corresponding component of the vector, x; therefore, xi is processed by the synaptic weight\xa0wi, and so on. In the original version of Hebb's theory, the input vectors represent neural firing rates, which are always non-negative. This means that the synaptic weights can only be strengthened (the neuroscientific term for this phenomenon is long-term potentiation\xa0(LTP)). However, for our purposes, we assume that x is a real-valued vector, as is w. This condition allows modeling more artificial scenarios without a loss of generality.\xa0 The same operation performed on a single vector holds when it's necessary to process many input\xa0samples organized in a matrix. If we have N m-dimensional input vectors, the formula becomes as follows: The basic form of Hebb's rule in a discrete form can be expressed (for a single input) as follows: The weight correction is hence a vector that has the same orientation of x and magnitude equal to |x| multiplied by a positive parameter, η,\xa0which is called the learning rate and the corresponding output, y (which can have either a positive or a negative sign). The sense\xa0of Δw is determined by the sign of y; therefore, under the assumption that x and y are real values,\xa0two different scenarios arise from this rule: It's easy to understand this behavior considering two-dimensional vectors: Therefore, if the initial angle\xa0α between w and x is less than 90°, w will have the same orientation of x and viceversa if\xa0α is greater than 90°. In the following diagram, there's a schematic representation of this process: It's possible to simulate this behavior using a very simple Python snippet. Let's start with a scenario where\xa0α is less than 90° and 50 iterations: As expected, the final angle, α, is close to zero and w has the same orientation and sense of x. We can now repeat the experiment with\xa0α greater than 90° (we change only the value of w because the procedure is the same): In this case, the final\xa0angle, α, is about 180° and, of course, w has the opposite sense with respect to x. The scientist S. Löwel expressed this concept with the famous sentence: We can re-express this concept (adapting it to a machine learning scenario) by saying that the main assumption of this approach is based on the idea that when pre- and post-synaptic units are coherent (their signals have the same sign), the connection between neurons becomes stronger and stronger. On the other side, if they are discordant, the corresponding synaptic weight is decreased. For the sake of precision, if x is a spiking rate, it should be represented as a real function x(t) as well as y(t). According to the original Hebbian theory, the discrete equation must be replaced by a differential equation: If x(t) and y(t) have the same fire rate, the synaptic weight is strengthened proportionally to the product of both rates. If instead there's a relatively long delay between the pre-synaptic activity x(t) and the post-synaptic one y(t), the corresponding weight is weakened. This is a more biologically plausible explanation of the relation fire together\xa0→ wire together.\xa0 However, even if the theory has a strong neurophysiological basis, some modifications are necessary. In fact, it's easy to understand that the resulting system is always unstable. If two inputs are repeatedly applied (both real values and firing rates), the norm of the vector, w, grows indefinitely and this isn't a plausible assumption for a biological system. In fact, if we consider a discrete iteration step, we have the following equation: The previous output, yk, is always multiplied by a factor greater than 1 (except in the case of null input), therefore it grows without a bound.\xa0As y = w · x, this condition implies that the magnitude of w increases (or remains constant if the magnitude of x is null) at each iteration (a more rigorous proof can be easily obtained considering the original differential equation). Such a situation is not only biologically unacceptable, but it's also necessary to properly manage it in machine learning problems in order to avoid a numerical overflow after a few iterations. In the next paragraph, we're going to discuss some common methods to overcome this issue. For now, we can continue our analysis without introducing a correction factor. Let's now consider a dataset, X: We can apply the rule iteratively to all elements, but it's easier (and more useful) to average the weight modifications over the input samples (the index now refers to the whole specific vector, not to the single components): In the previous formula, C is the input correlation matrix: For our purposes, however, it's useful to consider a slightly different Hebbian rule based on a threshold\xa0θ for the input vector (there's also a biological reason that justifies this choice, but it's beyond the scope of this book; the reader who is interested can find it in Theoretical Neuroscience,\xa0Dayan P., Abbott F. L., The MIT Press). It's easy to understand that in the original theory where x(t) and y(t) are firing rates, this modification allows a phenomenon opposite to LTP and called long-term depression (LTD). In fact, when x(t) <\xa0θ and y(t) is positive, the product (x(t) -\xa0θ)y(t) is negative and the synaptic weight is weakened. If we set\xa0θ = 〈x〉 ≈ E[X], we can derive an expression very similar to the previous one, but based on the input covariance matrix (unbiased through the Bessel's correction): For obvious reasons, this variant of the original Hebb's rule is called the\xa0covariance rule.", '2c8fba95-e36d-4eb6-b507-974239ff270e.xhtml': "The covariance matrix\xa0Σ is real and symmetric. If we apply the eigendecomposition, we get (for our purposes it's more useful to keep V-1 instead of the simplified version VT): V is an orthogonal matrix (thanks to the fact that\xa0Σ is symmetric) containing the eigenvectors of\xa0Σ (as columns), while\xa0Ω is a diagonal matrix containing the eigenvalues. Let's suppose we sort both eigenvalues (λ1,\xa0λ2, ..., λm) and the corresponding eigenvectors (v1, v2, ..., vm)\xa0so that: Moreover, let's suppose that\xa0λ1\xa0is dominant over all the other eigenvalues (it's enough that λ1 >\xa0λi with i\xa0≠ 1). As the eigenvectors are orthogonal, they constitute a basis and it's possible to express the vector w, with a linear combination of the eigenvectors: The vector\xa0u\xa0contains the coordinates in the new basis. Let's now consider the modification to the covariance rule: If we apply the rule iteratively, we get a matrix polynomial: Exploiting the Binomial theorem and considering\xa0that\xa0Σ0=I, we can get a general expression for w(k) as a function of\xa0w(0): Let's now rewrite the previous formula using the change of basis: The vector u(0) contains the coordinates of w(0) in the new basis; hence, w(k) is expressed as a polynomial where the generic term is proportional to VΩiu(0). Let's now consider the diagonal matrix Ωk: The last step derives from the hypothesis that\xa0λ1\xa0is greater than any other eigenvalue and when k\xa0→\xa0∞, all λi≠1k\xa0<<\xa0λ1k. Of course, if\xa0λi≠1 > 1,\xa0λi≠1k\xa0will grow as well as\xa0λ1k\xa0however, the contribution of the secondary eigenvalues to w(k) becomes significantly weaker when\xa0k\xa0→\xa0∞. Just to understand the validity of this approximation, let's consider the following situation where λ1 is slightly larger that\xa0λ2: The result shows a very important property: not only is the approximation correct, but as we're going to show, if an eigenvalue\xa0λi is larger than all the other ones, the covariance rule will always converge to the corresponding eigenvector vi. No other stable fixed points exist!\xa0 This hypothesis is no more valid if\xa0λ1\xa0= λ2\xa0= ... = λn. In this case, the total variance is explained equally by the direction of each eigenvector (a condition that implies a symmetry which isn't very common in real-life scenarios). This situation can also happen when working with finite-precision arithmetic, but in general, if the difference between the largest eigenvalue and the second one is less than the maximum achievable precision (for example, 32-bit floating point), it's plausible to accept the equality. Of course, we assume that the dataset is not whitened, because our goal (also in the next paragraphs) is to reduce the original dimensionality considering only a subset of components with the highest total variability (the decorrelation, like in\xa0Principal Component Analysis (PCA), must be an outcome of the algorithm, not a precondition). On the other side, zero-centering the dataset could be useful, even if not really necessary for this kind of algorithm. If we rewrite the expression for wk considering this approximation, we obtain the following: As a1v + a2v + ... + akv\xa0∝ v, this result shows that,\xa0when k\xa0→\xa0∞,\xa0wk will become proportional to the first eigenvector of the covariance matrix\xa0Σ (if u1(0) is not null) and its magnitude, without normalization, will grow indefinitely.\xa0The spurious effect due to the other eigenvalues becomes negligible (above all, if w is divided by its norm, so that the length is always ||w|| = 1) after a limited number of iterations. However, before drawing our conclusions, an important condition must be added: In fact, if w(0)\xa0were orthogonal to v1, we would get (the eigenvectors are orthogonal to each other): This important result shows how a Hebbian neuron working with the covariance rule is able to perform a\xa0PCA limited to the first component without the need for eigendecomposing Σ. In fact, the vector w (we're not considering the problem of the increasing magnitude, which can be easily managed) will rapidly converge to the orientation where the input dataset X has the highest variance. In Chapter 5, EM Algorithm and Applications, we discussed the details of PCA; in the next paragraph, we're going to discuss a couple of methods to find the first N principal components using a variant of the Hebb's rule.", '60a5ba77-4c12-4df1-8d92-7707144b8bbe.xhtml': "Before moving on, let's simulate this behavior with a simple Python example. We first generate 1000 values sampled from a bivariate Gaussian distribution (the variance is voluntarily asymmetric) and then we apply the covariance rule to find the first principal component\xa0(w(0)\xa0has been chosen so not to be orthogonal to v1): The algorithm is straightforward, but there are a couple of elements that we need to comment on. The first one is the normalization of vector w at the end of each iteration. This is one of the techniques needed to avoid the uncontrolled growth of w. The second tricky element is the final multiplication, w • 50. As we are multiplying by a positive scalar, the direction of w is not impacted, but it's easier to show the vector in the complete plot. The result is shown in the following diagram: After a limited number of iterations, w∞ has the same orientation of the principal eigenvector which is, in this case, parallel to the x axes. The sense depends on the initial value w0; however, in a PCA, this isn't an important element.", '68285a05-72c1-49ed-b384-ea5e4b1d0248.xhtml': "The easiest way to stabilize the weight vector is normalizing it after each update. In this way, its length will be always kept equal to one. In fact, in this kind of neural networks we are not interested in the magnitude, but only in the direction (that remains unchanged after the normalization). However, there are two main reasons that discourage this approach: In many machine learning contexts, these conditions are not limiting and they can be freely adopted, but when it's necessary to work with neuroscientific models, it's better to look for other solutions. In a discrete form, we need to determine a correction term for the standard Hebb's rule: The\xa0f function\xa0can work both as a local and non-local normalizer. An example of the first type is Oja's rule: The\xa0α\xa0parameter is a positive number that controls the strength of the normalization. A non-rigorous proof of the stability of this rule can be obtained considering the condition: The second expression implies that: Therefore, when t\xa0→\xa0∞, the magnitude of the weight correction becomes close to zero and the length of the weight vector w will approach a finite limit value:", '8da82ab8-9b89-4f02-b647-2f3cc4f43cbf.xhtml': "A Sanger's network is a neural network model for online Principal Component extraction proposed by T. D. Sanger in\xa0Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network,\xa0Sanger T. D., Neural Networks, 1989/2. The author started with the standard version of Hebb's rule and modified it to be able to extract a variable number of principal components (v1, v2, ..., vm) in descending order (λ1 >\xa0λ2 > ... >\xa0λm). The resulting approach, which is a natural extension of Oja's rule, has been called the\xa0Generalized Hebbian Rule (GHA) (or Learning). The structure of the network is represented in the following diagram: The network is fed with samples extracted from an n-dimensional dataset: The m output neurons are connected to the input through a weight matrix, W = {wij}, where the first index refers to the input components (pre-synaptic units) and the second one to the neuron. The output of the network can be easily computed with a scalar product; however, in this case, we are not interested in it, because just like for the covariance (and Oja's) rules, the principal components are extracted through the weight updates. The problem that arose after the formulation of Oja's rule was about the extraction of multiple components. In fact, if we applied the original rule to the previous network, all weight vectors (the rows of w) would converge to the first principal component. The main idea (based on the\xa0Gram-Schmidt orthonormalization method) to overcome this limitation is based on the observation that once we have extracted the first component w1, the second one w2 can be forced to be orthogonal to w1, the third one w3 can be forced to be orthogonal to w1 and w2, and so on. Consider the following representation: In this case, we are assuming that w1 is stable and w20 is another weight vector that is converging to w1. The projection of w20 onto w1 is as follows: In the previous formula, we can omit the norm if we don't need to normalize (in the network, this process is done after a complete weight update). The orthogonal\xa0component of w20 is simply obtained with a difference: Applying this method to the original Oja's rule, we obtain a new expression for the weight update (called Sanger's\xa0rule): The rule is referred to a single input vector x, hence xj is the jth component of x. The first term is the classic Hebb's rule, which forces weight w to become parallel to the first principal component, while the second one acts in a way similar to the Gram-Schmidt orthogonalization, by subtracting a term proportional to the projection of w onto all the weights connected to the previous post-synaptic units and considering, at the same time, the normalization constraint provided by Oja's rule (which is proportional to the square of the output). In fact, expanding the last term, we get the following:  The term subtracted to each component wij is proportional to all the components where the index j is fixed and the first index is equal to 1, 2, ..., i. This procedure doesn't produce an immediate orthogonalization but requires several iterations to converge. The proof is non-trivial, involving convex optimization and dynamic systems methods, but, it can be found in the aforementioned paper. Sanger showed that the algorithm converges always to the sorted first n principal components (from the largest eigenvalue to the smallest one) if the learning_rate\xa0η(t) decreases monotonically and converges to zero when t\xa0→\xa0∞. Even if necessary for the formal proof, this condition can be relaxed (a stable\xa0η < 1 is normally sufficient). In our implementation, matrix\xa0W\xa0is normalized after each iteration, so that, at the end of the process, WT (the weights are in the rows) is orthonormal and constitutes a basis for the eigenvector subspace.\xa0 In matrix form, the rule becomes as follows: Tril(•) is a matrix function that transforms its argument into a lower-triangular matrix and the term yyT\xa0is equal to WxxTW. The algorithm for a Sanger's network is as follows: The algorithm can also be iterated a fixed number of times (like in our example), or the two stopping approaches can be used together.", '2ec9439f-fa94-4c04-bc59-e5da5e9af065.xhtml': "For this Python example, we consider a bidimensional zero-centered dataset X with 500 samples (we are using the function defined in the first chapter). After the initialization of X, we also compute the eigendecomposition, to be able to double-check the result: The eigenvalues are in reverse order; therefore, we expect to have a final W with the rows swapped. The initial condition (with the weights multiplied by 15) is shown in the following diagram: Dataset with W initial condition, we can implement the algorithm. For simplicity, we preferred a fixed number of iterations (5000) with a learning_rate of η=0.01. The reader can modify the snippet to stop when the weight matrix becomes stable: The first thing to check is the final state of W (we transposed the matrix to be able to compare the columns): As expected, W has converged to the eigenvectors of the input correlation matrix (the sign – which is associated with the sense of w—is not important because we care only about the orientation). The second eigenvalue is the highest, so the columns are swapped. Replotting the diagram, we get the following: The two components are perfectly orthogonal (the final orientations can change according to the initial conditions or the random state) and w0 points in the direction of the first principal component, while w1 points in the direction of the second component. Considering this nice property, it's not necessary to check the magnitude of the eigenvalues; therefore, this algorithm can operate without eigendecomposing the input covariance matrix. Even if a formal proof is needed to explain this behavior, it's possible to understand it intuitively. Every single neuron converges to the first principal component given a full eigenvector subspace. This property is always maintained, but after the orthogonalization, the subspace is implicitly reduced by a dimension. The second neuron will always converge to the first component, which now corresponds to the global second component, and so on. One of the advantages of this algorithm (and also of the next one) is that a standard PCA is normally a bulk process (even if there are batch algorithms), while a Sanger's network is an online algorithm that is trained incrementally. In general, the time performance of a Sanger's network is worse than the direct approach because of the iterations (some optimizations can be achieved using more vectorization or GPU support). On the other side, a Sanger's network is memory-saving when the number of components is less than the input dimensionality (for example, the covariance matrix for n=1000 has 106 elements, if m = 100, the weight matrix has 104 elements).", 'c9690f9f-9374-4db5-b458-72519899efdb.xhtml': "In Chapter 5,\xa0EM Algorithm and Applications, we said that any algorithm\xa0that decorrelates the input covariance matrix is performing a PCA without dimensionality reduction. Starting from this approach, Rubner, and Tavan (in the paper A Self-Organizing Network for Principal-Components Analysis,\xa0Rubner J., Tavan P., Europhysics. Letters, 10(7), 1989) proposed a neural model whose goal is decorrelating the output components to force the consequent decorrelation of the output covariance matrix (in lower-dimensional subspace). Assuming a zero-centered dataset and E[y] = 0, the output covariance matrix for m\xa0principal components is as follows: Hence, it's possible to achieve an approximate decorrelation, forcing the terms yiyj with i\xa0≠ j to become close to zero. The main difference with a standard approach (such as whitening or vanilla PCA) is that this procedure is local, while all the standard methods operate globally, directly with the covariance matrix. The neural model proposed by the authors is shown in the following diagram (the original model was proposed for binary units, but it works quite well also for linear ones): The network has m output units and the last m-1 neurons have a summing node that receives\xa0the weighted output of the previous units (hierarchical lateral connections). The dynamic is simple: the first output isn't modified. The second one is forced to become decorrelated with the first one. The third one is forced to become decorrelated with both the first and the second one and so on. This procedure must be iterated a number of times because the inputs are presented one by one and the cumulative term that appears in the correlation/covariance matrix (it's always easier to zero-center the dataset and work with the correlation matrix) must be implicitly split into its addends. It's not difficult to understand that the convergence to the only stable fixed point (which has been proven to exist by the authors) needs some iterations to correct the wrong output estimations. The output of the network is made up of two contributions: The notation y/x(i) indicates the ith element of y/x. The first term produces a partial output based only on the input, while the second one uses hierarchical lateral connections to correct the values and enforce the decorrelation. The internal weights wij are updated using the standard version of Oja's rule (this is mainly responsible for the convergence of each weight vector to the first principal component): Instead, the external weights vjk are updated using an anti-Hebbian rule: The previous formula can be split into two parts: the first term -ηyjyk acts in the opposite direction of a standard version of Hebb's rule (that's why it's called anti-Hebbian) and forces the decorrelation. The second one -ηyjykvjk acts as a regularizer and it's analogous to Oja's rule. The term -ηyjyk works as a feedback signal for the Oja's rule that readapts the updates according to the new magnitude of the actual output. In fact, after modifying the lateral connections, the outputs are also forced to change and this modification impacts on the update of wij. When all the outputs are decorrelated, the vectors wi are implicitly obliged to be orthogonal. It's possible to imagine an analogy with the Gram-Schmidt orthogonalization, even if in this case the relation between the extraction of different components and the decorrelation is more complex. Like for Sanger's network, this model\xa0extracts the first m principal components in descending order (the reason is the same that has been intuitively explained), but for a complete (non-trivial) mathematical proof, please refer to the aforementioned paper. If input dimensionality is n and the number of components is equal to m,\xa0it's possible to use a lower-triangular matrix V (m × m) with all diagonal elements set to 0 and a standard matrix for W (n × m). The structure of W is as follows: Therefore,\xa0wi is a column-vector that must converge\xa0to the corresponding eigenvector. The structure of V is\xa0instead: Using this notation, the output becomes as follows: As the output is based on recurrent lateral connections, its value must be stabilized by iterating the previous formula for a fixed number times or until the norm between two consecutive values becomes smaller than a predefined threshold. In our example, we use a fixed number of iterations equal to five. The update rules cannot be written directly in matrix notation, but it's possible to use the vectors wi\xa0(columns)\xa0and vj\xa0(rows): In this case,\xa0y(i) means the ith component of y. The two matrices must be populated with a loop. The complete Rubner-Tavan's network algorithm is (the dimensionality of x is n, the number of components is denoted with m): In this case, we have adopted both a threshold and a maximum number of iterations because this algorithms normally converges very quickly. Moreover, I suggest the reader always checks the shapes of vectors and matrices when performing dot products.", '7ac2ba84-5f8c-4bb8-bd3e-ffdf6247d67e.xhtml': "For our Python example, we are going to use the same dataset already created for the Sanger's network (which is expected to be available in the variable Xs). Therefore, we can start setting up all the constants and\xa0variables: At this point, it's possible to implement the training loop: The final W and the output covariance matrix are as follows: As expected, the algorithm has successfully converged to the eigenvectors (in descending order) and the output covariance matrix is almost completely decorrelated (the sign of the non-diagonal elements can be either positive or negative). Rubner-Tavan's networks are generally faster than Sanger's network, thanks to the feedback signal created by the anti-Hebbian rule; however, it's important to choose the right value for the learning rate. A possible strategy is to implement a temporal decay (as done in Sanger's network) starting with a value not greater than 0.0001. However, it's important to reduce\xa0η when n increases (for example,\xa0η = 0.0001 / n), because the normalization strength of Oja's rule on the lateral connections vjk is often not enough to avoid over and underflows when n >> 1. I don't suggest any extra normalization on V (which must be carefully analyzed considering that V is singular) because it can slow down the process and reduce the final accuracy.", 'ba49720c-adb0-496a-9239-b742072ecf97.xhtml': "Self-organizing maps (SOMs) have been proposed by Willshaw and Von Der Malsburg (Willshaw D. J., Von Der Malsburg C., How patterned neural connections can be set up by self-organization, Proceedings of the Royal Society of London, B/194, N. 1117) to model different neurobiological phenomena observed in animals. In particular, they discovered that some areas of the brain develop structures with different areas, each of them with a high sensitivity for a specific input pattern. The process behind such a behavior is quite different from what\xa0we have discussed up until now, because it's based on competition among neural units based on a principle called winner-takes-all. During the training period, all the units are excited with the same signal, but only one will produce the highest response. This unit is automatically candidate to become the receptive basin for that specific pattern. The particular model we are going to present has been introduced by Kohonen (in the paper Kohonen T.,\xa0Self-organized formation of topologically correct feature maps,\xa0Biological Cybernetics, 43/1) and it's named after him. The main idea is to implement a gradual winner-takes-all paradigm, to avoid the premature convergence of a neuron (as a definitive winner) and increment the level of plasticity of the network. This concept is expressed graphically in the following graph (where we are considering a linear sequence of neurons): In this case, the same pattern is presented to all the neurons. At the beginning of the training process (t=0), a positive response is observed in xi-2 to\xa0xi+2 with a peak in xi. The potential winner is obviously xi, but all these units are potentiated according to their distance from xi. In other words, the network (which is trained sequentially) is still receptive to change if other patterns produce a stronger activation. If instead\xa0xi keeps on being the winner, the radius is slightly reduced, until the only potentiated unit will be xi. Considering the shape of this function, this dynamic is often called Mexican Hat. With this approach, the network remains plastic until all the patterns have been repeatedly presented. If, for example, another pattern elicits a stronger response in xi, it's important that its activation is still not too high, to allow a fast reconfiguration of the network. At the same time, the new winner will probably be a neighbor of xi, which has received a partial potentiation and can easily take the place of xi. A Kohonen SOM\xa0(also known as Kohonen network or simply Kohonen map) is normally represented as a bidimensional map (for example, a square matrix m\xa0× m, or any other rectangular shape), but 3D surfaces, such as spheres or toruses are also possible (the only necessary condition is the existence of a suitable metric). In our case, we always refer to a square matrix where each cell is a receptive neuron characterized by a synaptic weight w with the dimensionality of the input patterns: During both training and working phases, the winning unit is determined according to a similarity measure between a sample and each weight vector. The most common metric is the Euclidean; hence, if we consider a bidimensional map W with a shape (k × p)\xa0so that W\xa0∈\xa0ℜk × p\xa0× n, the winning unit (in terms of its coordinates) is computed as follows: As explained before, it's important to avoid the premature convergence because the complete final configuration could be quite different from the initial one. Therefore, the training process is normally subdivided into two different stages. During the first one, whose duration is normally about 10-20% of the total number of iterations (let's call this value tmax), the correction is applied to the winning unit and its neighbors (computed by adopting a decaying radius). Instead, during the second one, the radius is set to 1.0 and the correction is applied only to the winning unit. In this way, it's possible to analyze a larger number of possible configurations, automatically selecting the one associated with the least error. The neighborhood can have different shapes; it can be square (in closed 3D maps, the boundaries don't exist anymore), or, more easily, it's possible to employ a radial basis function based on an exponentially decaying distance-weight: The relative weight of each neuron is determined by the\xa0σ(t).\xa0σ0\xa0function\xa0is the initial radius and\xa0τ is a time-constant that must be considered as a hyperparameter which determines the slope of the decaying weight. Suitable values are 5-10% of the total number of iterations. Adopting a radial basis function, it's not necessary to compute an actual neighborhood because the multiplication factor n(i, j) becomes close to zero outside of the boundaries. A drawback is related to\xa0the computational cost, which is higher than a square neighborhood (as the function must be computed for the whole map); however, it's possible to speed up the process by precomputing all the squared distances (the numerator) and exploiting the vectorization features offered by packages such as NumPy (a single exponential is computed every time). The update rule is very simple and it's based on the idea to move the winning unit synaptic weights closer to the pattern, xi, (repeated for the whole dataset, X): The\xa0η(t)\xa0function\xa0is the learning rate, which can be fixed, but it's preferable to start with a higher value, η0\xa0and let it decay to a target final value, η∞: In this way, the initial changes force the weights to align with the input patterns, while all the subsequent updates allow slight modifications to improve the overall accuracy. Therefore, each update is proportional to the learning rate, the neighborhood weighted distance, and the difference between each pattern and the synaptic vector. Theoretically, if\xa0Δwij\xa0is equal to 0.0 for the winning unit, it means that a neuron has become the attractor of a specific input pattern, and its neighbors will be receptive to noisy/altered versions. The most interesting aspect is that the complete final map will contain the attractors for all patterns which are organized to maximize the similarity between adjacent units. In this way, when a new pattern is presented, the area of neurons that maps the most similar shapes will show a higher response. For example, if the patterns are made up of handwritten digits, attractors for the digit 1 and for digit 7 will be closer than the attractor, for example, for digit 8. A malformed 1 (which could be interpreted as 7) will elicit a response that is between the first two attractors, allowing us to assign a relative probability based on the distance. As we're going to see in the example, this feature yields to a smooth transition between different variants of the same pattern class avoiding rigid boundaries that oblige a binary decision (like in a K-means clustering or in a hard classifier).\xa0 The complete\xa0Kohonen SOM\xa0algorithm is as follows:", '29d1c365-e34d-45cd-b470-d6cdf45b4c9b.xhtml': "We can now implement an SOM using the Olivetti faces dataset. As the process can be very long, in this example we limit the number of input patterns to 100 (with a 5\xa0× 5 matrix). The reader can try with the whole dataset and a larger map. The first step is loading the data, normalizing it so that all values are bounded between 0.0 and 1.0, and setting the constants: At this point, we can initialize the weight matrix using a normal distribution with a small standard deviation: Now, we need to define the functions to determine the winning unit based on the least distance: It's also useful to define the functions\xa0η(t) and\xa0σ(t): As explained before, instead of computing the radial basis function for each unit, it's preferable to use a precomputed distance matrix (in this case,\xa05\xa0× 5\xa0× 5\xa0× 5) containing all the possible distances between couples of units. In this way, NumPy allows a faster calculation thanks to its vectorization features: The distance_matrix function returns the value of the radial basis function for the whole map given the center point (the winning unit) xt, yt and the current value of\xa0σ sigmat. Now, it's possible to start the training process (in order to avoid correlations, it's preferable to shuffle the input sequence at the beginning of each iteration): In this case, we have set\xa0η∞\xa0= 0.2 but I invite the reader to try different values and evaluate the final result. After training for 5000 epochs, we got the following weight matrix (each weight is plotted as a bidimensional array): As it's possible to see, the weights have converged to faces with slightly different features. In particular, looking at the shapes of the faces and the expressions, it's easy to notice the transition between different attractors (some faces are smiling, while others are more serious; some have glasses, mustaches, and beards, and so on). It's also important to consider that the matrix is larger than the minimum capacity (there are ten different individuals in the dataset). This allows mapping\xa0more patterns that cannot be easily attracted by the right neuron. For example, an individual can have pictures with and without a beard and this can lead to confusion. If the matrix is too small, it's possible to observe an instability in the convergence process, while if it's too large, it's easy to see redundancies. The right choice depends on each different dataset and on the internal variance and there's no way to define a standard criterion. A good starting point is picking a matrix whose capacity is between 2.0 and 3.0 times larger than the number of desired attractors and then increasing or reducing its size until the accuracy reaches a maximum. The last element to consider is the labeling phase. At the end of the training process, we have no knowledge about the weight distribution in terms of winning neurons, so it's necessary to process the dataset and annotate the winning unit for each pattern. In this way, it's possible to submit new patterns to get the most likely label. This process has not been shown, but it's straightforward and the reader can easily implement it for every different scenario.", 'acdca5e3-80f4-4532-b1e8-381c0d372dc7.xhtml': "In this chapter, we have discussed Hebb's rule, showing how it can drive the computation of the first principal component of the input dataset. We have also seen that this rule is unstable because it leads to the infinite growth of the synaptic weights and how it's possible to solve this problem using normalization or Oja's rule.\xa0 We have introduced two different neural networks based on Hebbian learning (Sanger's and Rubner-Tavan's networks), whose internal dynamics are slightly different, which are able to extract the first n principal components in the right order (starting from the largest eigenvalue) without eigendecomposing the input covariance matrix. Finally, we have introduced the concept of SOM and presented a model called a Kohonen network, which is able to map the input patterns onto a surface where some attractors (one per class) are placed through a competitive learning process. Such a model is able to recognize\xa0new patterns (belonging to the same distribution) by eliciting a strong response in the attractor, that is most similar to the pattern. In this way, after a labeling process, the model can be employed as a soft classifier that can easily manage noisy or altered patterns. In the next chapter, we're going to discuss some important clustering algorithms, focusing on the difference (already discussed in the previous chapters) between hard and soft clustering and discussing the main techniques employed to evaluate the performance of an algorithm.", '934045c3-fd09-48cb-bdbd-3860370c95cd.xhtml': "In this chapter, we are going to introduce some fundamental clustering algorithms, discussing both their strengths and weaknesses. The field of unsupervised learning, as well as any other machine learning approach, must be always based on the concept of Occam's razor. Simplicity must always be preferred when performance meets the requirements. However, in this case, the ground truth can be unknown. When a clustering algorithm is adopted as an exploratory tool, we can only assume that the dataset represents a precise data generating process. If this assumption is correct, the best strategy is to determine the number of clusters to maximize the internal cohesion (denseness) and the external separation. This means that we expect to find blobs (or isles) whose samples share some common and partially unique features. In particular, the algorithms we are going to present are:", '1893e3d3-4984-4dee-980e-b75df1a5f511.xhtml': "This algorithm belongs to a particular family called instance-based (the methodology is called instance-based learning). It differs from other approaches because it doesn't work with an actual mathematical model. On the contrary, the inference is performed by direct comparison of new samples with existing ones (which are defined as instances). KNN\xa0is an approach that can be easily employed to solve clustering, classification, and regression problems (even if, in this case, we are going to consider only the first technique). The main idea behind the clustering\xa0algorithm is very simple. Let's consider a data generating process pdata and a finite a dataset drawn from this distribution: Each sample has a dimensionality equal to N. We can now introduce a distance function d(x1, x2), which in the majority of cases can be generalized with the Minkowski\xa0distance: When p = 2,\xa0dp represents the classical Euclidean distance, that is normally the default choice. In particular cases, it can be useful to employ other variants, such as p = 1 (which is the Manhattan distance) or p\xa0> 2. Even if all the properties of a metric function remain unchanged, different values of p yield results that can be semantically diverse. As an example, we can consider the distance between points x1 = (0, 0) and x2 = (15, 10) as a function of p: The distance decreases monotonically with p and converges to the largest component absolute difference, |x1(j) -\xa0x2(j)|, when p\xa0→\xa0∞. Therefore, whenever it's important to weight all the components in the same way in order to have a consistent metric, small values of p are preferable (for example, p=1 or 2). This result has also been studied and formalized by\xa0Aggarwal, Hinneburg, and Keim (in On the Surprising Behavior of Distance Metrics in High Dimensional Space,\xa0Aggarwal C. C., Hinneburg A., Keim D. A., ICDT 2001), who proved a fundamental inequality. If we consider a generic distribution G of M points xi\xa0∈ (0, 1)d, a distance function based on the Lp norm, and the maximum Dmaxp and minimum Dminp distances (computed using the Lp\xa0norm) between two points, xj and xk\xa0drawn from G and (0, 0), the following inequality holds: It's clear that when the input dimensionality is very high and p >> 2, the expected value, E[Dmaxp\xa0- Dminp], becomes bounded between two constants, k1 (Cpd1/p-1/2)\xa0and k2\xa0\xa0((M-1)Cpd1/p-1/2)\xa0→ 0, reducing the actual effect of almost any distance. In fact, given two generic couples of points (x1,\xa0x2)\xa0and\xa0(x3,\xa0x4)\xa0drawn from G, the natural consequence of the following inequality is that dp(x1, x2) ≈\xa0dp(x3, x4) when p → ∞, independently of their relative positions. This important result confirms the importance of choosing the right metric according to the dimensionality of the dataset and that p = 1 is the best choice when d >> 1, while p >> 1 can produce inconsistent results due the ineffectiveness of the metric. To see direct confirmation of this phenomenon, it's possible to run the following snippet, which computes the average difference between maximum and minimum distances considering 100 sets containing 100 samples drawn from a uniform distribution, G ∼ U(0, 1). In the snippet, the case of d=2, 100, 1000\xa0is analyzed with Minkowski metrics with P= 1, 2, 10, 100 (the final values depend on the random seed and how many times the experiment is repeated): A particular case, that is a direct consequence of the previous inequality is when the largest absolute difference between components determines the most important factor of a distance, large values of p\xa0can be employed. For example, if we consider three points,\xa0x1\xa0= (0, 0), x2\xa0= (15, 10), and x3\xa0= (15, 0), d2(x1,\xa0x2)\xa0≈ 18 and\xa0d2(x1,\xa0x3) = 15. So, if we set a threshold at d = 16 centered at\xa0x1, x2\xa0is outside the boundaries. If instead\xa0p = 15, both distances become close to 15 and the two points (x2 and\xa0x3) are inside the boundaries. A particular use of large values of p is when it's important to take into account the inhomogeneity among components. For example, some feature vectors can represent the age and height of a set of people. Considering a test person x = (30, 175), with large p values, the distances between x and two samples (35, 150) and (25, 151) are almost identical (about 25.0), and the only dominant factor becomes the height difference (independent from the age).\xa0 The KNN algorithm determines the k closest samples of each training point. When a new sample is presented, the procedure is repeated with two possible variants: The philosophy of KNN is that similar samples can share their features. For example, a recommendation system can cluster users using this algorithm and, given a new user, find the most similar ones (based, for example, on the products they bought) to recommend the same category of items. In general, a similarity function is defined as the reciprocal of a distance (there are some exceptions, such as the cosine similarity): Two different users, A and B, who are classified as neighbors, will differ under some viewpoints, but, at the same time, they will share some peculiar features. This statement authorizes us to increase the homogeneity by suggesting the differences. For example, if A liked book b1 and B liked b2, we can recommend b1\xa0to B and b2 to A. If our hypothesis was correct, the similarity between A and B will be increased; otherwise, the two users will move towards other clusters that better represent their behavior. Unfortunately, the vanilla algorithm (in Scikit-Learn it is called\xa0the\xa0brute-force algorithm) can become extremely slow with a large number of samples because it's necessary to compute all the pairwise distances in order to answer any query. With M points, this number is equal to M2, which is often unacceptable (if M = 1,000, each query needs to compute a million distances). More precisely, as the computation of a distance in an N-dimensional space requires N operations, the total complexity becomes O(M2N), which can be reasonable only for small values of both M and N.\xa0That's why some important strategies have been implemented to reduce the computational complexity.\xa0", 'c24360b7-edc7-4ae9-9e5d-7649277dba81.xhtml': "As all KNN queries can be considered\xa0search problems, one of the most efficient way to reduce the overall complexity is to reorganize the dataset into a tree structure. In a binary tree (one-dimensional data), the average computational complexity of a query is O(log M), because we assume we have almost the same number of elements in each branch (if the tree is completely unbalanced, all the elements are inserted sequentially and the resulting structure has a single branch,\xa0 so the complexity becomes O(M)). In general, the real complexity is slightly higher than O(log M), but the operation is always much more efficient than a vanilla search, which is O(M2). However, we normally work\xa0with N-dimensional data and the previous structure cannot be immediately employed. KD Trees\xa0extend the concept of a binary for N > 1. In this case, a split cannot be immediately performed and a different strategy must be chosen. The easiest way to solve this problem is to select a feature at each level (1, 2, ..., N) and repeat the process until the desired depth is reached. In the following diagram, there's an example of KD Trees with three-dimensional points: The root is point (5, 3, 7). The first split is performed considering the first feature, so two children are (2, 1, 1) and (8, 4, 3). The second one operates on the second feature and so on.\xa0The average computational complexity is O(N log M), but if the distribution is very asymmetric, the probability that the tree becomes unbalanced is very high. To mitigate this issue, it's possible to select the feature corresponding to the median of the (sub-)dataset and to continue splitting with this criterion. In this way, the tree is guaranteed to be balanced. However, the average complexity is always proportional to the dimensionality and this can dramatically affect the performance. For example, if M = 10,000 and N = 10, using the log10,\xa0O(N log M) = O(40), while, with N = 1,000, the complexity becomes O(40,000). Generally, KD Trees suffers the curse of dimensionality and when N becomes large, the average complexity is about O(MN), which is always better than the vanilla algorithm, but often too expensive for real-life applications.\xa0 Therefore, KD Trees is really effective only when the dimensionality is not too high. In all other cases, the probability of having an unbalanced tree and the resulting computational complexity suggest employing a different method.", '41dc2655-118b-46f6-9e91-89adbdd1b991.xhtml': "An alternative to KD Trees is provided by Ball Trees. The idea is to rearrange the dataset in a way that is almost insensitive to high-dimensional samples. A ball is defined as a set of points whose distance from a center sample is less than or equal to a fixed radius: Starting from the first main ball, it's possible to build smaller ones nested into the parent ball and stop the process when the desired depth has been reached. A fundamental condition is that a point can always belong to a single ball. In this way, considering the cost of the N-dimensional distance, the computational complexity is O(N log M) and doesn't suffer the curse of dimensionality like KD Trees. The structure is based on hyperspheres, whose boundaries are defined by the equations (given a center point x and a radius Ri): Therefore, the only operation needed to find the right ball is measuring the distance between a sample and the centers starting from the smallest balls. If a point is outside the ball, it's necessary to move upwards and check the parents, until the ball containing the sample is found. In the following diagram, there's an example of Ball Trees with two levels: In this example, the seven bidimensional points are split first into two balls containing respectively three and four points. At the second level, the second ball is split again into two smaller balls containing two points each. This procedure can be repeated until a fixed depth is reached or by imposing the maximum number of elements that a leaf must contain (in this case, it can be equal to 3). Both KD Trees and Ball Trees can be efficient structures to reduce the complexity of KNN queries. However, when fitting a model, it's important to consider both the\xa0k\xa0parameter (which normally represents the average or the standard number of neighbors computed in a query) and the maximum tree depth. These particular structures are not employed for common tasks (such as sorting) and their efficiency is maximized when all the requested neighbors can be found in the same sub-structure (with a size K<< M, to avoid an implicit fallback to the vanilla algorithm). In other words, the tree has the role of reducing the dimensionality of the search space by partitioning it into reasonably small regions. At the same time, if the number of samples contained in a leaf is small, the number of tree nodes grows and the complexity is subsequently increased. The negative impact is doubled because on average it's necessary to explore more nodes and if k is much greater than the number of elements contained in a node, it's necessary to merge the samples belonging to different nodes. On the other side, a very large number of samples per node leads to a condition that is close to the vanilla algorithm. For example, if M = 1,000 and each node contains 250 elements, once the right node is computed, the number of distances to compute is comparable with the initial dataset size and no real advantage is achieved by employing a tree structure. An acceptable practice is to set the size of a life equal to 5 ÷ 10 times the average value of k, to maximize the probability to find all the neighbors inside the same leaf. However, every specific problem must be analyzed (while also benchmarking the performances) in order to find the most appropriate value. If different values for k are necessary, it's important to consider the relative frequencies of the queries. For example, if a program needs 10\xa05-NN queries and 1\xa050-NN query, it's probably better to set a leaf size equal to 25, even if the 50-NN query will be more expensive. In fact, setting a good value for a second query (for example, 200) will dramatically increase the complexity of the first 10 queries, driving to a performance loss.", '586c7dfb-6a01-4557-ae03-a9336e5823c2.xhtml': "In order to test the KNN algorithm, we are going to use the MNIST handwritten digit dataset provided directly by Scikit-Learn. It is made up of 1,797 8 × 8 grayscale images representing the digits from 0 to 9. The first step is loading it and normalizing all the values to be bounded between 0 and 1: The dictionary digits contains both the images,\xa0digits['images'], and the flattened 64-dimensional arrays,\xa0digits['data']. Scikit-Learn implements different classes (for example, it's possible to work directly with KD Trees and Ball Trees using the KDTree and BallTree\xa0classes) that can be used in the context of KNN (as clustering, classification, and regression algorithms). However, we're going to employ the main class, NearestNeighbors, which allows performing clustering and queries based either on the number of neighbors or on the radius of a ball centered on a sample: We have chosen to have a default number of neighbors equal to 50 and an algorithm based on a ball_tree. The leaf size (leaf_size)\xa0parameter has been kept to its default value equal to 30. We have also employed the default metric (Euclidean), but it's possible to change it using the metric and p\xa0parameters (which is the order of the Minkowski metric). Scikit-Learn supports all the metrics implemented by SciPy in the scipy.spatial.distance\xa0package. However, in the majority of cases, it's sufficient to use a Minkowski metric and adjust the value of p if the results are not acceptable with any number of neighbors. Other metrics, such as the cosine distance, can be employed when the similarity must not be affected by the Euclidean distance, but only by the angle between two vectors pointing at the samples. Applications that use this metric include, for example, deep learning models for natural language processing, where the words are embedded into feature vectors whose semantic similarity is proportional to their Cosine distance. We can now query the model in order to find 50 neighbors of a sample. For our purposes, we have selected the sample with index 100, which represents a 4 (the images have a very low resolution, but it's always possible to distinguish the digit): The query can be performed using the instance method\xa0kneighbors, which allows specifying the number of neighbors (n_neighbors\xa0parameter the default is the value selected during the instantiation of the class) and whether we want to also get the distances of each neighbor (the\xa0return_distance\xa0parameter). In this example, we are also interested in evaluating how far the neighbors are from the center, so we set return_distance=True: The first neighbor is always the center, so its distance is 0. The other ones range from 0.9 to 1.9. Considering that, in this case, the maximum possible distance is 8 (between a 64-dimensional vector a = (1, 1, ..., 1) and the null vector), the result could be acceptable. In order to get confirmation, we can plot the neighbors as bidimensional 8 × 8 arrays (the returned array,\xa0neighbors, contains the indexes of the samples). The result is shown in the following screenshot: As it's possible to see, there are no errors, but all the shapes are slightly different. In particular, the last one, which is also the farthest, has a lot of white pixels (corresponding to the value 1.0), explaining the reason of a distance equal to about 2.0. I invite the reader to test the radius_neighbors\xa0method until spurious values appear among the results. It's also interesting to try this algorithm with the Olivetti faces dataset, whose complexity is higher and many more geometrical parameters can influence the similarity.", '7bc06e98-37b5-4ccd-9041-9ebd7bf2aede.xhtml': "When we\xa0discussed the Gaussian mixture algorithm, we defined it as Soft K-means. The reason is that each cluster was represented by three elements: mean, variance, and weight. Each sample always belongs to all clusters with a probability provided by the Gaussian distributions. This approach can be very useful when it's possible to manage the probabilities as weights, but in many other situations, it's preferable to determine a single cluster per sample. Such an approach is called hard clustering and K-means can be considered the hard version of a Gaussian mixture. In fact, when all variances\xa0Σi\xa0→ 0, the distributions degenerate to Dirac's Deltas, which represent perfect spikes centered at a specific point. In this scenario, the only possibility to determine the most appropriate cluster is to find the shortest distance between a sample point and all the centers (from now on, we are going to call them centroids). This approach is also based on an important double principle that should be taken into account in every clustering algorithm. The clusters must be set up to maximize: This means that we expect to label high-density regions that are well separated from each other. When this is not possible, the criterion must try to minimize the intra-cluster average distance between samples and centroid. This quantity is also called inertia and it's defined as: High levels of inertia imply low cohesion because there are probably too many points belongings to clusters whose centroids are too far away. The problem can be solved by minimizing the previous quantity. However, the computational complexity needed to find the global minimum is exponential (K-means belongs to the class of NP-Hard problems). The alternative approach employed by the K-means algorithm, also known as Lloyd's algorithm, is iterative and starts from selecting k random\xa0centroids (in the next section, we're going to analyze a more efficient method) and adjusting them until their configuration becomes stable. The dataset to cluster (with M samples) is represented as: An initial guess for the centroids is:  There are no particular restrictions on the initial values. However, the choice can influence both the convergence speed and the minimum that is found. The iterative procedure will loop over the dataset, computing the Euclidean distance between xi\xa0and each\xa0μj and assigning a cluster based on the criterion: Once all the samples have been clustered, the new centroids are computed: The quantity NCj\xa0represents the number of points belonging to cluster j. At this point, the inertia is recomputed and the new value is compared with the previous one. The procedure will stop either after a fixed number of iterations or when the variations in the inertia become smaller than a predefined threshold. Lloyd's algorithm is very similar to a particular case of the EM algorithm. In fact, the first step of each iteration is the computation of an expectation (the centroid configuration), while the second step maximizes the intra-cluster cohesion by minimizing the inertia. The complete vanilla\xa0K-means algorithm is: The algorithm is quite simple and intuitive, and there are many real-life applications based on it. However, there are two important elements to consider. The first one is the convergence speed. It's easy to show that every initial guess drives to a convergence point, but the number of iterations is dramatically influenced by this choice and there's no guarantee to find the global minimum. If the initial centroids\xa0are close to the final ones, the algorithm needs only a few steps to correct the values, but when the choice is totally random, it's not uncommon to need a very high number of iterations. If there are N samples and k centroids, Nk distances must be computed at each iteration, leading to an inefficient result. In the next paragraph, we'll show how it's possible to initialize the centroids to minimize the convergence time. Another important aspect is that, contrary to KNN, K-means needs to predefine the number of expected clusters. In some cases, this is a secondary problem because we already know the most appropriate value for k. However, when the dataset is high-dimensional and our knowledge is limited, this choice could be hazardous. A good approach to solve the issue is to analyze the final inertia for a different number of clusters. As we expect to maximize the intra-cluster cohesion, a small number of clusters will lead to an increased inertia. We try to pick the highest point below a maximum tolerable value. Theoretically, we can also pick k = N. In this case, the inertia becomes zero because each point represents the centroid of its cluster, but a large value for k transforms the clustering scenario into a fine-grained partitioning that might not be the best strategy to capture the feature of a consistent group. It's impossible to define a rule for the upper bound kmax, but we assume that this value is always much less than N. The best choice is achieved by selecting k\xa0to minimize the inertia, selecting the values from a set bounded, for example, between 2 and kmax.", '8170ca3c-63b4-4875-8bae-eb32806252ca.xhtml': "We have said that a good choice for the initial centroids can improve the convergence speed and leads to a minimum that is closer to the global optimum of the inertia S. Arthur and Vassilvitskii (in The Advantages of Careful Seeding,\xa0Arthur, D., Vassilvitskii S., k-means++: Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms)\xa0proposed a method called K-means++, which allows increasing the accuracy of the initial centroid guess considering the most likely final configuration. In order to expose the algorithm, it's useful to introduce a function,\xa0D(x, i), which is defined as: D(x, i) defines the shortest distance between each sample and one of the centroids already selected. As the process is incremental, this function must be recomputed after all steps. For our purposes, let's also define an auxiliary probability distribution (we omit the index variable for simplicity): The first centroid\xa0μ0 is sampled from X using a uniform distribution. The next steps are: In the aforementioned paper, the authors showed a very important property. If we define S* as the global optimum of S, a K-means++ initialization determines an upperbound for the expected value of the actual inertia: This condition is often expressed by saying that K-means++ is O(log k)-competitive. When k is sufficiently small, the probability of finding a local minimum close to the global one increases. However, K-means++ is still a probabilistic approach and different initializations on the same dataset lead to different initial configurations. A good practice is to run a limited number of initializations (for example, ten) and pick the one associated with the smallest inertia. When training complexity is not a primary issue, this number can be increased, but different experiments showed that the improvement achievable with a very large number of trials is negligible when compared to the actual computational cost. The default value in Scikit-Learn is ten and the author suggests to keep this value in the majority of cases. If the result continues to be poor, it's preferable to pick another method. Moreover, there are problems that cannot be solved using K-means (even with the best possible initialization), because one of the assumptions of the algorithm is that each cluster is a hypersphere and the distances are measured using a Euclidean function. In the following sections, we're going to analyze other algorithms that are not constrained to work with such limitations and can easily solve clustering problems using asymmetric cluster geometries.", 'e522b290-7123-4428-832b-6e14cd3c7096.xhtml': "In this example, we continue using the MNIST dataset (the\xa0X_train\xa0array is the same defined in the paragraph dedicated to KNN), but we want also to analyze different clustering evaluation methods. The first step is visualizing the inertia corresponding\xa0to different numbers of clusters. We are going to use the KMeans\xa0class, which accepts the n_clusters\xa0parameter and employs the K-means++ initialization as the default method (as explained in the previous section, in order to find the best initial configuration, Scikit-Learn performs several attempts and selects the\xa0configuration with the lowest inertia; it's possible to change the number of attempts through the n_iter\xa0parameter): We are supposing to analyze the range [2, 20]. After each training session, the final inertia can be retrieved using the\xa0inertia_\xa0instance variable. The following graph shows the plot of the values as a function of the number of clusters: As expected, the function is decreasing, starting from a value of about 7,500 and reaching about 3,700 with 20 clusters. In this case, we know that the real number is 10, but it's possible to discover it by observing the trend. The slope is quite high before 10, but it starts decreasing more and more slowly after this threshold. This is a signal that informs us that some clusters are not well separated, even if their internal cohesion is high. In order to confirm this hypothesis, we can set n_clusters=10 and, first of all, check the centroids at the end of the training process: The centroids are available through the\xa0cluster_centers_\xa0instance variable. In the following screenshot, there's a plot of the corresponding bidimensional arrays: All the digits are present and there are no duplicates. This confirms that the algorithm has successfully separated the sets, but the final inertia (which is about 4,500) informs us that there are probably wrong assignments. To obtain confirmation, we can plot the dataset using a dimensionality-reduction method, such as t-SNE (see Chapter 3, Graph-Based Semi-Supervised Learning for further details): At this point, we can plot the bidimensional dataset with the corresponding cluster labels: The plot confirms that the dataset is made up of well-separated blobs, but a few samples are assigned to the wrong cluster (this is not surprising considering the similarity between some pairs of digits). An important observation can further explain the trend of the inertia. In fact, the point where the slope changes almost abruptly corresponds to 9 clusters. Observing the t-SNE plot, we can immediately discover the reason: the cluster corresponding to the digit 7 is indeed split into 3 blocks. The main one contains the majority of samples, but there are another 2 smaller blobs that are wrongly\xa0attached to clusters 1 and 9. This is not surprising, considering that the digit 7 can be very similar to a distorted 1 or 9. However, these two spurious blobs are always at the boundaries of the wrong clusters (remember that the geometric structures are hyperspheres), confirming that the metric has successfully detected a low similarity. If a group of wrongly assigned samples were in the middle of a cluster, it would have meant that the separation failed dramatically and another method should be employed.", 'e49ab775-74d3-4aa8-90f6-18f88c8445f9.xhtml': "In many cases, it's impossible to evaluate the performance of a clustering algorithm using only a visual inspection. Moreover, it's important to use standard objective metrics that allow for comparing different approaches. We are now going to introduce some methods based on the knowledge of the ground truth (the correct assignment for each sample) and one common strategy employed when the true labels are unknown. Before discussing the scoring functions, we need to introduce a standard notation. If there are k clusters, we define the true labels as: In the same way, we can define the predicted labels: Both sets can be considered as sampled from two discrete random variables (for simplicity, we denote them with the same names), whose probability mass functions are Ptrue(y) and Ppred(y) with a generic y ∈ {y1, y2, ..., yk} (yi represents the index of the ith cluster). These two probabilities can be approximated with a frequency count; so, for example, the probability Ptrue(1) is computed as the number of samples whose true label is 1 ntrue(1) over the total number of samples M. In this way, we can define the entropies: These quantities describe the intrinsic uncertainty of the random variables. They are maximized when all the classes have the same probability, while, for example, they are null if all the samples belong to a single class (minimum uncertainty). We also need to know the uncertainty of a random variable Y given another one X. This can be achieved using the conditional entropy H(Y|X). In this case, we need to compute the joint probability p(x, y) because the definition of H(Y|X) is: In order to approximate the previous expression, we can define the function n(itrue, jpred), which counts the number of samples with\xa0the true label i assigned to cluster j. In this way, if there are M samples, the approximated conditional entropies become:", '2830f738-c6a5-460a-b518-23ecd3745c2d.xhtml': "This score is useful to check whether the clustering algorithm meets an important requirement: a cluster should contain only samples belonging to a single class. It's defined as: It's bounded between 0 and 1, with low values indicating a low homogeneity. In fact, when the knowledge of Ypred reduces the uncertainty of\xa0Ytrue, H(Ytrue|Ypred) becomes smaller (h\xa0→ 1) and viceversa. For our example, the homogeneity score can be computed as: The\xa0digits['target']\xa0array contains the true labels while\xa0Y\xa0contains the predictions (all the functions we are going to use accept the true labels as the first parameter and the predictions as the second one).\xa0The homogeneity score confirms that the clusters are rather homogeneous, but there's still a moderate level of uncertainty because some clusters contain wrong assignments. This method, together with the other ones, can be used to search for the right number of clusters and tune up all supplementary hyperparameters (such as the number of iterations or the metric function).", 'f4a7b48b-0021-4917-ad0d-e89cef3b4df6.xhtml': "This score is complementary to the previous one. Its purpose is to provide a piece of information about the assignment of samples belonging to the same class. More precisely, a good clustering algorithm should assign all samples with the same true label to the same cluster. From our previous analysis, we know that, for example, the digit 7 has been wrongly assigned to both clusters 9 and 1; therefore, we expect a non-perfect completeness score. The definition is symmetric to the homogeneity score: The rationale is very intuitive. When H(Ypred|Ytrue) is low (c â†’ 1), it means that the knowledge of the ground truth reduces the uncertainty about the predictions. Therefore, if we know that all the sample of subset A have the same label yi, we are quite sure that all the corresponding predictions have been assigned to the same cluster. The completeness score for our example is: Again, the value confirms our hypothesis. The residual uncertainty is due to a lack of completeness because a few samples with the same label have been split into blocks that are assigned to wrong clusters. It's obvious that a perfect scenario is characterized by having both homogeneity and completeness scores equal to 1.", '13262867-c270-4d85-acb0-323983d4a970.xhtml': "This score is useful to compare the\xa0original label distribution with the clustering prediction. Ideally, we'd like to reproduce the\xa0exact ground truth distribution, but in general, this is very difficult in real-life scenarios. A way to measure the discrepancy is provided by the Adjusted Rand Index. In order to compute this score, we need to define the auxiliary variables: The Rand Index is defined as: The Adjusted Rand Index is the Rand Index corrected for chance and it's defined as: The\xa0RA measure\xa0is bounded between -1 and 1. A value close to -1 indicates a prevalence of wrong assignments, while a value close to 1 indicates that the clustering algorithm is correctly reproducing the ground truth distribution. The Adjusted Rand Score for our example is: This value confirms that the algorithm is working well (because it's positive), but it can be further optimized by trying to reduce the number of wrong assignments. The Adjusted Rand Score is a very powerful tool when the ground truth is known and can be employed as a single method to optimize all the hyperparameters.", 'f11524df-733c-4062-a5bd-125e338c6760.xhtml': "This measure doesn't need to know the ground truth and can be used to check, at the same time, the intra-cluster cohesion and the inter-cluster separation. In order to define the Silhouette score, we need to introduce two auxiliary functions. The first one is the average intra-cluster distance of a sample xi belonging to a cluster Cj: In the previous expression, n(k) is the number of samples assigned to the cluster Cj and d(a, b) is a standard distance function (in the majority of cases, the Euclidean distance is chosen). We need also to define the lowest inter-cluster distance which can be interpreted as the average nearest-cluster distance. In the sample xi\xa0∈\xa0Cj, let's call Ct\xa0the nearest cluster; therefore, the function is defined as: The Silhouette score for sample xi is: The value of s(xi), like for the Adjusted Rand Index, is bounded between -1 and 1. A value close to -1 indicates that b(xi) << a(xi), so the average intra-cluster distance is greater than the average nearest-cluster index and sample xi is wrongly assigned. Viceversa, a value close to 1 indicates that the algorithm achieved a very good level of internal cohesion and inter-cluster separation (because\xa0a(xi) << b(xi)). Contrary to the other measure, the\xa0Silhouette score isn't a cumulative function and must be\xa0computed for each sample. A feasible strategy is to analyze the average value, but in this way, it's not possible to determine which clusters have the highest impact on the result. Another approach (the most common), is based on Silhouette plots, which display the score for each cluster in descending order. In the following snippet, we create plots for four different values of n_clusters (3, 5, 10, 12): The result is shown in the following graph: The analysis of a Silhouette plot should follow some common guidelines: In our particular case,\xa0 we cannot accept having a number of clusters different from ten. However, the corresponding Silhouette plot is not perfect. We know the reasons for such imperfections (the structure of the samples and the high similarity of different digits) and it's quite difficult to avoid them using an algorithm like K-means. The reader can try to improve the performances by increasing the number of iterations, but in these cases, if the result doesn't meet the requirements, it's preferable to adopt another method (like the spectral clustering method, which can manage asymmetric clusters and more complex geometries).", 'cff71e89-12ec-43df-84ef-aae070c9691c.xhtml': "We have already talked about the difference between hard and soft clustering, comparing K-means with Gaussian mixtures. Another way to address this problem is based on the concept of fuzzy logic, which was proposed for the first time by Lotfi Zadeh in 1965 (for further details, a very good reference is An Introduction to Fuzzy Sets,\xa0Pedrycz W., Gomide F., The MIT Press). Classic logic sets are based on the law of excluded middle that, in a clustering scenario, can be expressed by saying that a sample xi can belong only to a single cluster cj. Speaking more generally, if we split our universe into labeled partitions, a hard clustering approach will assign a label to each sample, while a fuzzy (or soft) approach allows managing a membership degree (in Gaussian mixtures, this is an actual probability),\xa0wij\xa0which expresses how strong the relationship is between sample xi and cluster cj. Contrary to other methods, by employing fuzzy logic it's possible to define asymmetric sets that are not representable with continuous functions (such as trapezoids). This allows for achieving further flexibility and an increased ability to adapt to more complex geometries. In the following graph, there's an example of fuzzy sets: The graph represents the seniority level of an employee given his/her years of experience. As we want to cluster the entire population into three groups (Junior, Middle level, and Senior), three fuzzy sets have been designed. We have assumed that a young employee is keen and can quickly reach a Junior level after an initial apprenticeship period. The possibility to work with complex problems allows him/her to develop skills that are fundamental to allowing the transition between the Junior and Middle levels. After about 10 years, the employee can begin to consider himself/herself as a senior apprentice and, after about 25 years, the experience is enough to qualify him/her as a full Senior until the end of his/her career. As this is an imaginary example, we haven't tuned all the values up, but it's easy to compare, for example, employee A with 9 years of experience with another employee B with 18 years of experience. The former is about 50% Junior (decreasing), 90% Middle level (reaching its climax), and 10% Senior (increasing). The latter, instead, is 0% Junior (ending plateau), 30% Middle level (decreasing), and 60% Senior (increasing). In both cases, the values are not normalized so always sum up to 1 because we are more interested in showing the process and the proportions. The fuzziness level is lower in extreme cases, while it becomes higher when two sets intersect. For example, at about 15%, the Middle level and Senior are about 50%. As we're going to discuss, it's useful to avoid a very high fuzziness when clustering a dataset because it can lead to a lack of precision as the boundaries fade out, becoming completely fuzzy. Fuzzy C-means is a generalization of a standard K-means, with a soft assignment and more flexible clusters. The dataset to cluster (containing M samples) is represented by: If we assume we have k clusters, it's necessary to define a matrix W\xa0∈\xa0ℜM\xa0× k\xa0containing the membership degrees for each sample: Each degree\xa0wij\xa0∈ [0, 1] and all rows must be normalized so that they always sum up to 1. In this way, the membership degrees can be considered as probabilities (with the same semantics) and it's easier to make decisions with a prediction result. If a hard assignment is needed, it's possible to employ the same approach normally used with Gaussian mixtures: the winning cluster is selected by applying the argmax function. However, it's a good practice to employ soft clustering only when it's possible to manage the vectorial output. For example, the probabilities/membership degrees can be fed into a classifier in order to yield more complex predictions. As with K-means, the problem can be expressed as the minimization of a generalized inertia: The constant m (m > 1) is an exponent employed to re-weight the membership degrees. A value very close to 1 doesn't affect the actual values. Greater m values reduce their magnitude. The same parameter is also used when recomputing the centroids and the new membership degrees and can drive to a different clustering result. It's rather difficult to define a global acceptable value; therefore, a good practice is to start with an average m (for example, 1.5) and perform a grid search (it's possible to sample from a Gaussian or uniform distribution) until the desired accuracy has been achieved. Minimizing the previous expression is even more difficult than with a standard inertia; therefore, a pseudo-Lloyd's algorithm is employed. After a random initialization, the algorithm proceeds, alternating two steps (like an EM procedure) in order to determine the centroids, and recomputing the membership degrees to maximize the internal cohesion. The centroids are determined by a weighted average: Contrary to K-means, the sum is not limited to the points belonging to a specific cluster because the weight factor will force the farthest points (wij\xa0≈ 0.0) to produce a contribution close to 0. At the same time, as this is a soft-clustering algorithm, no exclusions are imposed, to allow a sample to belong to any number of clusters with different membership degrees. Once the centroids have been recomputed, the membership degrees must be updated using this formula: This function behaves like a similarity. In fact, when sample xi is very close to centroid\xa0μj (and relatively far from\xa0μp with p\xa0≠ j), the denominator becomes small and wij increases. The exponent m\xa0directly influences the fuzzy partitioning, because when m\xa0≈ 1 (m\xa0> 1), the denominator is a sum of quasi-squared terms and the closest centroid can dominate the sum, yielding to a higher preference for a specific cluster. When m >> 1, all the terms in the sum tend to 1, producing a more flat weight distribution with no well-defined preference. It's important to understand that, even when working with soft clustering, a fuzziness excess leads to inaccurate decisions because there are no factors that push a sample to clearly belong to a specific cluster. This means that problem is either ill-posed or, for example, the number of expected clusters is too high and doesn't represent the real underlying data structure. A good way to measure how much this algorithm is similar to a hard-clustering approach (such as K-means) is provided by the normalized Dunn's partitioning coefficient: When Pc is bounded between 0 and 1, when it's close to 0, it means that the membership degrees have a flat distribution and the level of fuzziness is the highest possible. On the other side, if it's close to 1, each row of W has a single dominant value, while all the others are negligible. This scenario resembles a hard-clustering approach. Higher Pc values are normally preferable because, even without renouncing to a degree of fuzziness, it allows making more precise decisions. Considering the previous example, Pc tends to 1 when the sets don't intersect, while it becomes 0 (complete fuzziness) if, for example, the three seniority levels are chosen to be identical and overlapping. Of course, we are interested in avoiding such extreme scenarios by limiting the number of borderline cases. A grid search can be performed by analyzing different numbers of clusters and m values (in the example, we're going to do it with the MNIST handwritten digit dataset). A reasonable rule of thumb is to accept Pc\xa0values higher than 0.8, but in some cases, that can be impossible. If we are sure that the problem is well-posed, the best approach is to choose the configuration that maximizes Pc, considering, however, that a final value less than 0.3-0.5 will lead to a\xa0very high level of uncertainty because the clusters are extremely overlapping.\xa0 \xa0 The complete\xa0Fuzzy C-means\xa0algorithm is:", '6967d36f-e04e-46d3-8c99-e30e2193d464.xhtml': "Scikit-Fuzzy (http://pythonhosted.org/scikit-fuzzy/) is a Python package based on SciPy that allows implementing all the most important fuzzy logic algorithms (including fuzzy C-means). In this example, we continue using the MNIST dataset, but with a major focus on fuzzy partitioning. To perform the clustering, Scikit-Fuzzy implements the cmeans method (in the skfuzzy.cluster package) which requires a few mandatory parameters: data, which must be an array D\xa0∈\xa0ℜN\xa0× M (N is the number of features; therefore, the array used with Scikit-Learn must be transposed);\xa0c, the number of clusters; the coefficient m, error, which is the maximum tolerance; and maxiter, which is the maximum number of iterations. Another useful parameter (not mandatory) is the seed\xa0parameter which allows specifying the random seed to be able to easily reproduce the experiments. I invite the reader to check the official documentation for further information. The first step of this example is performing the clustering: The cmeans\xa0function returns many values, but for our purposes, the most important are: the first one, which is the array containing the cluster centroids; the second one, which is the final membership degree matrix; and the last one, the partition coefficient. In order to analyze the result, we can start with the partition coefficient: This value informs us that the clustering is not very far from a hard assignment, but there's still a residual fuzziness. In this particular case, such a situation may be reasonable because we know that many digits are partially distorted and may appear very similar to other ones (such as 1, 7, and 9). However, I invite the reader to try different values for m and check how the partition coefficient changes. We can now display the centroids: All the different digit classes have been successfully found, but now, contrary to K-means, we can check the fuzziness of a problematic digit (representing a 7, with index 7), as shown in the following diagram: The membership degrees associated with the previous sample are: The corresponding plot is:  In this case, the choice of m has forced the algorithm to reduce the fuzziness. However, it's still possible to see three smaller peaks corresponding to the clusters centered respectively on 1, 8, and 5 (remember that the cluster indexes correspond to digits shown previously in the centroid plot). I invite the reader to analyze the fuzzy partitioning of different digits and replot it with different values of the m\xa0parameter. It will be possible to observe an increased fuzziness (corresponding also to smaller partitioning coefficients) with larger m values. This effect is due to a stronger overlap among clusters (observable also by plotting the centroids) and could be useful when it's necessary to detect the distortion of a sample. In fact, even if the main peak indicates the right cluster, the secondary ones, in descending order, inform us how much the sample is similar to other centroids and, therefore, if it contains features that are characteristics of other subsets. Contrary to Scikit-Learn, in order to perform predictions, Scikit-Fuzzy implements the cmeans_predict\xa0method (in the same package), which requires the same parameters of cmeans, but instead of the number of clusters,\xa0c\xa0needs the final centroid array (the name of the parameter is cntr_trained). The function returns as a first value the corresponding membership degree matrix (the other ones are the same as\xa0cmeans). In the following snippet, we repeat the prediction for the same sample digit (representing a 7):", '3d25409d-2ca3-4767-9d0d-15e9ac905255.xhtml': "One of the most common problems of K-means and other similar algorithms is the assumption we have only hyperspherical clusters. This condition can be acceptable when the dataset is split into blobs that can be easily embedded into a regular geometric structure. However, it fails whenever the sets are not separable using regular shapes. Let's consider, for example, the following bidimensional dataset: As we are going to see in the example, any attempt to separate the upper sinusoid from the lower one using K-means will fail. The reason is quite obvious: a circle that contains the upper set will also contain part of the (or the whole) lower set. Considering the criterion adopted by K-means and imposing two clusters, the inertia will be minimized by a vertical separation corresponding to about x0 = 0. Therefore, the resulting clusters are completely mixed and only a dimension is contributing to the final configuration. However, the two sinusoidal sets are well-separated and it's not difficult to check that, selecting a point xi from the lower set, it's always possible to find a ball containing only samples belonging to the same set. We have already discussed this kind of problem when Label Propagation algorithms were discussed and the logic behind spectral clustering is essentially the same (for further details, I invite the reader to check Chapter 2, Graph-Based Semi-Supervised Learning). Let's suppose we have a dataset X sampled from a data generating process pdata: We can build a graph G = {V, E}, where the vertices are the points and the edges are determined using an affinity matrix W. Each element wij must express the affinity between sample xi and sample xj. W is normally built using two different approaches: Alternatively, it's possible to build a distance matrix: The\xa0γ\xa0parameter allows controlling the amplitude of the Gaussian function, reducing or increasing the number of samples with a high weight (so\xa0actual neighbors). However, a weight is assigned to all points and the resulting graph will always be connected (even if many elements are close to zero). In both cases, the elements of W will represent a measure of affinity (or closeness) between points and no restrictions are imposed on the global geometry (contrary to K-means). In particular, using a KNN connectivity matrix, we are implicitly segmenting the original dataset into smaller regions with a high level of internal cohesion. The problem that we need to solve now is to find out a way to merge all the regions belonging to the same cluster. The approach we are going to present here has been proposed by Normalized Cuts and Image Segmentation,\xa0J. Shi and J. Malik,\xa0IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 22, 08/2000, and it's based on the normalized graph Laplacian: The matrix D, called the degree matrix, is the same as discussed in Chapter 3, Graph-Based Semi-Supervised Learning\xa0and it's defined as: It's possible to prove the following properties (the formal proofs are omitted but they can be found in texts such as Functions and Graphs Vol. 2,\xa0Gelfand I. M., Glagoleva E. G., Shnol E. E., The MIT Press: In other words, the normalized graph Laplacian encodes the information about the number of connected components and provides us with a new reference system where the clusters can be separated using regular geometric shapes (normally hyperspheres). To better understand how this approach works without a non-trivial mathematical approach, it's important to expose another property of Ln. From linear algebra, we know that each eigenvalue\xa0λ of a matrix M\xa0∈\xa0ℜn\xa0× n spans a corresponding eigenspace, which is a subset of\xa0ℜn containing all eigenvectors associated with\xa0λ plus the null vector. Moreover, given a set S ⊆ ℜn\xa0and a countable subset C (it's possible to extend the definition to generic subsets but in our context the datasets\xa0are always countable), we can define a vector v\xa0∈\xa0ℜn as an\xa0indicator vector, if v(i) = 1 if the vector ci\xa0∈ S and v(i)\xa0= 0 otherwise. If we consider the null eigenvalues of\xa0Ln and we assume that their number is k (corresponding to the multiplicity of the eigenvalue 0), it's possible to prove that the corresponding eigenvectors are indicator vectors for eigenspaces spanned by each of them. From the previous statements, we know that these eigenspaces correspond to the connected components of the graph G; therefore, performing a standard clustering (like K-means or K-means++) with the points projected into these subspaces allows for an easy separation with symmetric shapes. As Ln\xa0∈\xa0ℜM × M, its eigenvectors vi\xa0∈\xa0ℜM. Selecting the first k eigenvectors, it's possible to build a matrix A ∈\xa0ℜM × k: Each row of A, aj\xa0∈\xa0ℜk\xa0can be considered as the projection of an original sample xj in the low-dimensional subspace spanned by each of the null eigenvalues of Ln. At this point, the separability of the new dataset A = {aj} depends only on the structure of the graph G and, in particular, on the number of neighbors or the γ parameter for RBFs. As in many other similar cases, it's impossible to define a standard value suitable for all problems, above all when the dimensionality doesn't allow a visual inspection. A reasonable approach should start with a small number of neighbors (for example, five) or\xa0γ = 1.0 and increase the values until a performance metric (such as the Adjusted Rand Index) reaches its maximum. Considering the nature of the problems, it can also be useful to measure the homogeneity and the completeness because these two measures are more sensitive to irregular geometric structures and can easily show when the clustering is not separating the sets correctly. If the ground truth is unknown, the Silhouette score can be employed to assess the intra-cluster cohesion and the inter-cluster separation as functions of all hyperparameters (number of clusters, number of neighbors, or γ). The complete\xa0Shi-Malik spectral clustering\xa0algorithm is:", '6479364d-bd3b-4ffb-93bb-73d7569ebdf9.xhtml': "In this example, we are going to use the sinusoidal dataset previously shown. The first step is creating it (with 1,000 samples):\xa0 At this point, we can try to cluster it using K-means (with n_clusters=2): The result is shown in the following graph: As expected, K-means isn't able to separate the two sinusoids. The reader is free to try with different parameters, but the result will always be unacceptable because K-means bidimensional clusters are circles and no valid configurations exist. We can now employ spectral clustering using an affinity matrix based on the KNN algorithm (in this case, Scikit-Learn can produce a warning because the graph is not fully connected, but this normally doesn't affect the results). Scikit-Learn implements the SpectralClustering\xa0class, whose most important parameters are\xa0n_clusters, the number of expected clusters;\xa0affinity, which can be either 'rbf' or 'nearest_neighbors';\xa0gamma (only for RBF); and n_neighbors (only for KNN). For our test, we have chosen to have 20 neighbors: The result of the spectral clustering is shown in the following graph: As expected, the algorithm was able to separate the two sinusoids perfectly. As an exercise, I invite the reader to apply this method to the MNIST dataset, using both an RBF (with different gamma values) and KNN (with different numbers of neighbors). I also suggest to replot the t-SNE diagram and compare all the assignment errors. As the clusters are strictly non-convex, we don't expect a high Silhouette score. Other useful exercises can be: drawing the Silhouette plot and checking the result, assigning ground truth labels, and measuring the homogeneity and the completeness.", '83eb3b8f-c8af-407e-8854-fcff9bb76b47.xhtml': "In this chapter, we presented some fundamental clustering algorithms. We started with KNN, which is an instance-based method that restructures the dataset to find the most similar samples given a query point. We discussed three approaches: a naive one, which is also the most expensive in terms of computational complexity, and two strategies based respectively on the construction of a KD Tree and a Ball Tree. These two data structures can dramatically improve performance even when the number of samples is very large. The next topic was a classic algorithm: K-means, which is a symmetric partitioning strategy, comparable to a Gaussian mixture with variances close to zero, that can solve many real-life problems. We discussed both a vanilla algorithm, which wasn't able to find a valid sub-optimal solution, and an optimized initialization method, called K-means++, which was able to speed up the convergence towards solutions quite close to the global minimum. In the same section, we also presented some evaluation methods that can be employed to assess the performance of a generic clustering algorithm. We also presented a soft-clustering method called fuzzy C-means, which resembles the structure of a standard K-means, but allows managing membership degrees (analogous to probabilities) that encode the similarity of a sample with all cluster centroids. This kind of approach allows processing the membership vectors in a more complex pipeline, where the output of a clustering process, for example, is fed into a classifier. One of the most important limitations of K-means and similar algorithms is the symmetric structure of the clusters. This problem can be solved with methods such as spectral clustering, which is a very powerful approach based on the dataset graph and is quite similar to non-linear dimensionality reduction methods. We analyzed an algorithm proposed by Shi and Malik, showing how it can easily separate a non-convex dataset. In the next chapter, Chapter 8, Ensemble Learning, we're going to discuss some common ensemble learning methods, which are based on the use of a large set of weak classifiers. We focused on their peculiarities, comparing the performances of different ensembles with single strong classifiers.", 'c40f6c93-f0a4-4efd-aad5-2bb1f9ffab3c.xhtml': 'In this chapter, we are going to discuss some important algorithms that exploit different estimators to improve the overall performance of an ensemble or committee. These techniques work either by introducing a medium level of randomness in every estimator belonging to a predefined set or by creating a sequence of estimators where, each new model is forced to improve the performance of the previous ones. These techniques allow us to reduce both the bias and the variance (thereby increasing validation accuracy) when employing models with a limited capacity or more prone to overfit the training set. In particular, the topics covered in the chapter are as follows:', 'ca05a680-ceee-4124-a677-42d86209b36e.xhtml': "The main concept behind ensemble learning is the distinction between strong and weak learners. In particular, a strong learner is a classifier or a regressor which has enough capacity to reach the highest potential accuracy, minimizing both bias and variance (thus achieving also a satisfactory level of generalization). More formally, if we consider a parametrized binary classifier f(x;\xa0θ), we define it as a strong learner if the following is true: This expression can appear cryptic; however, it's very easy to understand. It simply expresses the concept that a strong learner is theoretically able to achieve any non-null probability of misclassification with a probability greater than or equal to 0.5 (that is, the threshold for a binary random guess). All the models normally employed in Machine Learning tasks are normally strong learners, even if their domain can be limited (for example, a logistic regression cannot solve non-linear problems). On the other hand, a weak learner is a model that is generically able to achieve an accuracy slightly higher than a random guess, but whose complexity is very low (they can be trained very quickly, but can never be used alone to solve complex problems). There is a formal definition also in this case, but it's simpler to consider that the real main property of a weak learner is a limited ability to achieve a reasonable accuracy. In some very particular and small regions of the training space, a weak learner could reach a low probability of misclassification, but in the whole space its performance is only a little bit superior to a random guess. The previous one is more a theoretical definition than a practice one, because all the models currently available are normally quite better than a random oracle. However, an ensemble is defined as a set of weak learners that are trained together (or in a sequence) to make up a committee. Both in classification and regression problems, the final result is obtained by averaging the predictions or employing a majority vote. At this point, a reasonable question is—Why do we need to train many weak learners instead of a single strong one? The answer is double—in ensemble learning, we normally work with medium-strong learners (such as decision trees or support vector machines (SVMs)) and we use them as a committee to increase the overall accuracy and reduce the variance thanks to a wider exploration of the sample space. In fact, while a single strong learner is often able to overfit the training set, it's more difficult to keep a high accuracy over the whole sample subspace without saturating the capacity. In order to avoid overfitting, a trade-off must be found and the result is a less accurate classifier/regressor with a simpler separation hyperplane. The adoption of many weak learners (that are actually quite strong, because even the simplest models are more accurate than a random guess), allows us to force them to focus only on a limited subspace, so as to be able to reach a very high local accuracy with a low variance. The committee, employing an averaging technique, can easily find out which prediction is the most suitable. Alternatively, it can ask each learner to vote, assuming that a successful training process must always lead the majority to propose the most accurate classification or prediction. The most common approaches to ensemble learning are as follows:", 'f240dfcf-f0cf-4e13-9707-2f19695e94e8.xhtml': "A random forest is the bagging ensemble model based on decision trees. If the reader is not familiar with this kind of model, I suggest reading the\xa0Introduction to Machine Learning, Alpaydin E., The MIT Press, where a complete explanation can be found. However, for our purposes, it's useful to provide a brief explanation of the most important concepts. A decision tree is a model that resembles a standard hierarchical decision process. In the majority of cases, a special family is employed, called binary decision trees, as each decision yields only two outcomes. This kind of tree is often the simplest and most reasonable choice and the training process (which consists in building the tree itself) is very intuitive. The root contains the whole dataset: Each level is obtained by applying a selection tuple, defined as follows:  The first index of the tuple corresponds to an input feature, while the threshold ti is a value chosen in the specific range of each feature. The application of a selection tuple leads to a split and two nodes that contain each a non-overlapping subset of the input dataset. In the following diagram, there's an example of a slip performed at the level of the root (initial split): The set X is split into two subsets defined as X11 and X12 whose samples have respectively the feature with i=2 less or greater than the threshold ti=0.8. The intuition behind classification decision trees is to continue splitting until the leaves contain samples belonging to a single category yi (these nodes are defined as pure). In this way, a new sample xj can traverse the tree with a computation complexity O(log(M)) and reach a final node that determines its category. In a very similar way, it's possible to build regression trees whose output is continuous\xa0(even if, for our purposes, we are going to consider only classification scenarios). At this point, the main problem is how to perform each split. We cannot pick any feature and any threshold, because the final tree will be completely unbalanced and very deep. Our goal is to find the optimal selection tuple at each node considering the final goal, which is classification into discrete categories (the process is almost identical for regressions). The technique is very similar to a problem based on a cost function that must be minimized, but, in this case, we operate locally, applying an impurity measure proportional to the heterogeneity of a node. A high impurity indicates that samples belonging to many different categories are present, while an impurity equal to 0 indicates that a single category is present. As we need to continue splitting until a pure leaf appears, the optimal choice is based on a function that scores each selection tuple, allowing us to select the one that yields the lowest impurity (theoretically, the process should continue until all the leaves are pure, but normally a maximum depth is provided, so\xa0as to avoid excessive complexity). If there are p classes, the category set can be defined as follows:  A very common impurity measure is called Gini impurity and it's based on the probability of a misclassification if a sample is categorized using a label randomly chosen from the node subset distribution. Intuitively, if all the samples belong\xa0to the same category, any random choice leads to a correct classification (and the impurity becomes 0). On the other side, if the node contains samples from many categories, the probability of a misclassification increases. Formally, the measure is defined as follows:  The subset is indicated by Xk and p(j|k) is obtained as the ratio of the samples belonging to the class j over the total number of samples. The selection tuple must be chosen so as to minimize the Gini impurity of the children. Another common approach is the cross-entropy impurity, defined as follows: The main difference between this measure and the previous one is provided by some fundamental information theory concepts. In particular, the goal we want to reach is the minimization of the uncertainty, which is measured using the (Cross-)Entropy. If we have a discrete distribution and all the samples belong to the same category, a random choice is can fully describe the distribution; therefore, the uncertainty is null. On the contrary, if, for example, we have a fair die, the probability of each outcome is 1/6 and the corresponding entropy is about 2.58 bits (if the base of the logarithm is 2). When the nodes become purer and purer, the cross-entropy impurity decreases and reaches 0 in an optimal scenario. Moreover, adopting the concept of mutual information, we can define the information gain obtained after a split has been performed:  Given a node, we want to create two children to maximize the information gain. In other words, by choosing the cross-entropy impurity we implicitly grow the tree until the information gain becomes null. Considering again the example of a fair die, we need 2.58 bits of information to decide which is the right outcome. If, instead, the die is loaded and the probability of an outcome is 1.0, we need no information to make a decision. In a decision tree, we'd like to resemble this situation, so that, when a new sample has completely traversed the tree, we don't need any further information to classify it. If a maximum depth is imposed, the final information gain cannot be null. This means that we need to pay an extra cost to finalize the classification. This cost is proportional to the residual uncertainty and should be minimized to increase the accuracy. Other methods can also be employed (even if Gini and cross-entropy are the most common) and I invite the reader to check the references for further information. However, at this point, a consideration naturally arises. Decision trees are simple models (they are not weak learners!), but the procedure for building them is more complex than, for example, training a logistic regression or a SVM. Why are they so popular? One reason is already clear—they represent a structural process that can be shown using a diagram; however, this is not enough to justify their usage. Two important properties allow the employment of decision trees without any data preprocessing. In fact, it's easy to understand that, contrary to other methods, there's no need for any scaling or whitening and it's possible to use continuous and categorical features at the same time. For example, if in a bidimensional dataset a feature has a variance equal to 1 and the other equal to 100, the majority of classifiers will achieve a low accuracy; therefore, a preprocessing step becomes necessary. In a decision tree, a selection tuple has the same effect also when the ranges are very different. It goes without saying that a split can be easily performed considering also categorical features and there's no need, for example, to use techniques such as one-hot encoding (which is necessary in most cases to avoid generalization errors). However, unfortunately, the separation hypersurface obtained with a decision tree is normally much more complex than the one obtained using other algorithms and this drives to a higher variance with a consequential loss of generalization ability. To understand the reason, it's possible to imagine a very simple bidimensional dataset made up of two blobs located in the second and fourth quarters. The first set is characterized by (x < 0, y > 0), but the second one by (x < 0, y < 0). Let's also suppose that we have a few outliers, but our knowledge about the data generating process is not enough to qualify them as noisy samples (the original distribution can have tails that are extended over the axes; for example, it may be a mixture of two Gaussians). In this scenario, the simplest separation line is a diagonal splitting the plane into two subplanes containing regions belonging also to the first and third quarters. However, this decision can be made only considering both coordinates at the same time. Using a decision tree, we need to split initially, for example, using the first feature and again with the second one. The result is a piece-wise separation line (for example, splitting the plane into the region corresponding to the second quarter and its complement), leading to a very high classification variance. Paradoxically, a better solution can be obtained with an incomplete tree (limiting the process, for example, to a single split) and with the selection of the y-axis as the separation line (this is why it's important to impose a maximum depth), but the price you pay is an increased bias (and a consequently worse accuracy). Another important element to consider when working with decision trees (and related models) is the maximum depth. It's possible to grow the tree until the all leaves are pure, but sometimes it's preferable to impose a maximum depth (and, consequently, a maximum number of terminal nodes). A maximum depth equal to 1 drives to binary models called decision stumps, which don't allow any interaction among the features (they can simply be represented as If... Then conditions). Higher values yield more terminal nodes and allow an increasing interaction among features (it's possible to think about a combination of many If... Then statements together with AND logical operators). The right value must be tuned considering every single problem and it's important to remember that very deep trees are more prone to overfitting than pruned ones. In some contexts, it's preferable to achieve a slightly worse accuracy with a higher generalization ability and, in those case, a maximum depth should be imposed. The common tool to determine the best value is always a grid search together with a cross-validation technique. Random forests provide us with a powerful tool to solve the bias-variance trade-off problem. They were proposed by L. Breiman (in Breiman L., Random Forests, Machine Learning, 45, 2001) and their logic is very simple. As already explained in the previous section, the bagging method starts with the choice of the number of weak learners, Nc. The second step is the generation of Nc datasets (called bootstrap samples) D1, D2, ..., DNc:  Each decision tree is trained using the corresponding dataset using a common impurity criterion; however, in a random forest, in order to reduce the variance, the selection splits are not computed considering all the features, but only via a random subset containing a quite smaller number of features (common choices are the rounded square root, log2 or natural logarithm). This approach indeed weakens each learner, as the optimality is partially lost, but allows us to obtain a drastic variance reduction by limiting the over-specialization. At the same time, a bias reduction and an increased accuracy are a result of the ensemble (in particular for a large number of estimators). In fact, as the learners are trained with slightly different data distributions, the average of a prediction converges to the right value when Nc → ∞ (in practice, it's not always necessary to employ a very large number of decision trees, however, the correct value must be tuned using a grid search with cross-validation). Once all the models, represented with a function di(x), have been trained, the final prediction can be obtained as an average:  Alternatively, it's possible to employ a majority vote (but only for classifications):  These two methods are very similar and, in most cases, they yield the same result. However, averaging is more robust and allows an improved flexibility when the samples are almost on the boundaries. Moreover, it can be used for both classification and regression tasks. Random forests limit their randomness by picking the best selection tuple from a smaller sample subset. In some cases, for example, when the number of features is not very large, this strategy drives to a minimum variance reduction and the computational cost is no longer justified by the result. It's possible to achieve better performances with a variant called extra-randomized trees (or simply extra-trees). The procedure is almost the same; however, in this case, before performing a split, n random thresholds are computed (for each feature) and the one which leads to the least impurity is chosen. This approach further weakens the learners but, at the same time, reduces residual variance and prevents overfitting. The dynamic is not very different from many techniques such as regularization or dropout (we're going to discuss this approach in the next chapter); in fact, the extra-randomness reduces the capacity of the model, forcing it to a more linearized solution (which is clearly sub-optimal). The price to pay for this limitation is a consequent bias worsening, which, however, is compensated by the presence of many different learners. Even with random splits, when Nc is large enough, the probability of a wrong classification (or regression prediction) becomes smaller and smaller because both the average and the majority vote tend to compensate the outcome of trees whose structure is strongly sub-optimal in particular regions. This result is easier to obtain, in particular, when the number of training samples is large. In this case, in fact, sampling with replacement leads to slightly different distributions that could be considered (even if this is not formally correct) as partially and randomly boosted. Therefore, every weak learner will implicitly focus on the whole dataset with extra-attention to a smaller subset that, however, is randomly selected (differently from actual boosting). The complete random forest algorithm is as follows:", '3cd7f4d7-351c-4cc0-995b-bf0e45bca30d.xhtml': "In this example, we are going to use the famous Wine dataset (178 13-dimensional samples split into three classes) that is directly available in Scikit-Learn. Unfortunately, it's not so easy to find good and simple datasets for ensemble learning algorithms, as they are normally employed with large and complex sets that require too long a computational time. As the Wine dataset is not particularly complex, the first step is to assess the performances of different classifiers (logistic regression, decision tree, and polynomial SVM) using a k-fold cross-validation: As expected, the performances are quite good, with a top value of average cross-validation accuracy equal to about 96% achieved by the polynomial (the default degree is 3) SVM. A very interesting element is the performance of the decision tree, the worst of the set (with Gini impurity it's lower). Even if it's not correct, we can define this model as the weakest of the group and it's a perfect candidate for our bagging test. We can now fit a Random Forest by instantiating the class RandomForestClassifier and selecting n_estimators=50 (I invite the reader to try different values): As expected, the average cross-validation accuracy is the highest, about 98.3%. Therefore, the random forest has successfully found a global configuration of decision trees, so as to specialize them in almost any region of the sample space. The parameter n_jobs=cpu_count() tells Scikit-Learn to parallelize the training process using all of the CPU cores available in the machine. To better understand the dynamics of this model, it's useful to plot the cross-validation accuracy as a function of the number of trees: It's not surprising to observe some oscillations and a plateau when the number of trees becomes greater at about 320. The effect of the randomness can cause a performance loss, even increasing the number of learners. In fact, even if the training accuracy grows, the validation accuracy on different folds can be affected by an over-specialization. Moreover, in this case, it's very interesting to notice that the top accuracy is achievable with 50 trees instead of 400 or more. For this reason, I always suggest performing at least a grid search, in order not only to achieve the best accuracy but also to minimize the complexity of the model. Another important element to consider when working with decision trees and random forests is\xa0feature importance (also called Gini importance when this criterion is chosen), which is a measure proportional to the impurity reduction that a particular feature allows us achieve. For a decision tree, it is defined as follows:  In the previous formula, n(j) denotes the number of samples reaching the node j (the sum must be extended to all nodes where the feature is chosen) and ΔIi is the impurity reduction achieved at node j after splitting using the feature i. In a random forest, the importance must be computed by averaging over all trees:  After fitting a model (decision tree or random forest), Scikit-Learn outputs the feature importance vector in the feature_importances_ instance variable. In the following graph, there's a plot showing the importance of each feature (the labels can be obtained with the command load_wine()['feature_names']) in descending order: We don't want to analyze the chemical meaning of each element, but it's clear that, for example, the presence of proline and the color intensity are much more important than the presence of non-flavonoid phenols. As the model is working with features that are semantically independent (it's not the same for the pixels of an image), it's possible to reduce the dimensionality of a dataset by removing all those features whose importance doesn't have a high impact on the final accuracy. This process, called feature selection, should be performed using more complex statistical techniques, such as Chi-squared, but when a classifier is able to produce an importance index, it's also possible to use a Scikit-Learn class called SelectFromModel. Passing an estimator (that can be fitted or not) and a threshold, it's possible to transform the dataset by filtering out all the features whose value is below the threshold. Applying it to our model and setting a minimum importance equal to 0.02, we get the following: The new dataset now contains 10 features instead of the 13 of the original Wine dataset (for example., it's easy to verify that ash and non-flavonoid phenols have been removed). Of course, as for any other dimensionality reduction method, it's always suggested you verify the final accuracy with a cross-validation and make decisions only if the trade-off between loss of accuracy and complexity reduction is reasonable.", '54c1e493-bf75-4618-b06e-94f2e29d556c.xhtml': "In the previous section, we have seen that sampling with a replacement leads to datasets where the samples are randomly reweighted. However, if M is very large, most of the samples will appear only once and, moreover, all the choices are totally random. AdaBoost is an algorithm proposed by Schapire and Freund that tries to maximize the efficiency of each weak learner by employing adaptive boosting (the name derives from this). In particular, the ensemble is grown sequentially and the data distribution is recomputed at each step so as to increase the weight of those samples that were misclassified and reduce the weight of the ones that were correctly classified. In this way, every new learner is forced to focus on those regions that were more problematic for the previous estimators. The reader can immediately understand that, contrary to random forests and other bagging methods, boosting doesn't rely on randomness to reduce the variance and improve the accuracy. Rather, it works in a deterministic way and each new data distribution is chosen with a precise goal. In this paragraph, we are going to consider a variant called Discrete AdaBoost (formally AdaBoost.M1), which needs a classifier whose output is thresholded (for example, -1 and 1). However, real-valued versions (whose output behaves like a probability) have been developed (a classical example is shown in Additive Logistic Regression: a Statistical View of Boosting,\xa0Friedman J., Hastie T., Tibshirani R.,\xa0Annals of Statistics, 28/1998). As the main concepts are always the same, the reader interested in the theoretical details of other variants can immediately find them in the referenced papers. For simplicity, the training dataset of AdaBoost.M1 is defined as follow:  This choice is not a limitation because, in multi-class problems, a one-versus-the-rest strategy can be easily employed, even if algorithms like AdaBoost.SAMME guarantee a much better performance. In order to manipulate the data distribution, we need to define a weight set:  The weight set allows defining an implicit data distribution D(t)(x), which initially is equivalent to the original one but that can be easily reshaped by changing the values wi. Once the family and the number of estimators, Nc, have been chosen, it's possible to start the global training process. The algorithm can be applied to any kind of learner that is able to produce thresholded estimations (while the real-valued variants can work with probabilities, for example, obtained through the Platt scaling method). The first instance d1(x) is trained with the original dataset, which means with the data distribution D(1)(x). The next instances, instead, are trained with the reweighted distributions\xa0D(2)(x),\xa0D(3)(x), ...,\xa0D(Nc)(x). In order to compute them, after each training process, the normalized weighted error sum is computed:  This value is bounded between 0 (no misclassifications) and 1 (all samples have been misclassified) and it's employed to compute the estimator weight\xa0α(t):  To understand how this function works, it's useful to consider its plot (shown in the following diagram): This diagram unveils an implicit assumption: the worst classifier is not the one that misclassifies all samples (ε(t) = 1), but a totally random binary guess (corresponding to\xa0ε(t) = 0.5). In this case,\xa0α(t) is null and, therefore, the outcome if the estimator is completely discarded. When\xa0ε(t)\xa0< 0.5, a boosting is applied (between about 0.05 and 0.5, the trend is almost linear), but it becomes greater than 1 only when ε(t)\xa0< about 0.25 (larger values drive to a penalty because the weight is smaller than 1). This value is a threshold to qualify an estimator as trusted or very strong and\xa0α(t)\xa0→ +∞ in the particular case of a perfect estimator (no errors). In practice, an upper bound should be imposed in order to avoid overflows or divisions by zero. Instead, when\xa0ε(t)\xa0> 0.5, the estimator is unacceptably weak, because it's worse than a random guess and the resulting boosting would be negative. To avoid this problem, real implementations must invert the output of such estimators, transforming them de facto into learners with ε(t)\xa0< 0.5 (this is not an issue, as the transformation is applied to all output values in the same way). It's important to consider that this algorithm shouldn't be directly applied to multi-class scenarios because, as pointed out in Multi-class AdaBoost,\xa0Zhu J., Rosset S., Zou H., Hastie T.,\xa001/2006, the threshold 0.5 corresponds to a random guess accuracy only for binary choices. When the number of classes is larger than two, a random estimator outputs a class with a probability 1/Ny (where Ny is the number of classes) and, therefore, AdaBoost.M1 will boost the classifiers in a wrong way, yielding poor final accuracies (the real threshold should be 1 - 1/Ny, which is larger than 0.5 when\xa0Ny > 2). The AdaBoost.SAMME algorithm (implemented by Scikit-Learn) has been proposed to solve this problem and exploit the power of boosting also in multi-class scenarios. The global decision function is defined as follows:  In this way, as the estimators are added sequentially, the importance of each of them will decrease while the accuracy of di(x) increases. However, it's also possible to observe a plateau if the complexity of X is very high. In this case, many learners will have a high weight, because the final prediction must take into account a sub-combination of learners in order to achieve an acceptable accuracy. As this algorithm specializes the learners at each step, a good practice is to start with a small number of estimators (for example, 10 or 20) and increase the number until no improvement is achieved. Sometimes, a minimum number of good learners (like SVM or decision trees) is sufficient to reach the highest possible accuracy (limited to this kind of algorithm), but in some other\xa0cases, the number of estimators can be some thousands. Grid search and cross-validation are again the only good strategies to make the right choice. After each training step it is necessary to update the weights in order to produce a boosted distribution. This is achieved using an exponential function (based on bipolar outputs {-1, 1}):  Given a sample xi, if it has been misclassified, its weight will be increased considering the overall estimator weight. This approach allows a further adaptive behavior because a classifier with a high\xa0α(t) is already very accurate and it's necessary a higher attention level to focus only on the (few) misclassified samples. On the contrary, if\xa0α(t) is small, the estimator must improve its overall performance and the over-weighting process must be applied to a large subset (therefore, the distribution won't peak around a few samples, but will penalize only the small subset that has been correctly classified, leaving the estimator free to explore the remaining space with the same probability). Even if not present in the original proposal, it's also possible to include a learning rate\xa0η that multiplies the exponent:  A value\xa0η = 1 has no effect, while smaller values have been proven to increase the accuracy by avoiding a premature specialization. Of course, when\xa0η << 1, the number of estimators must be increased in order to compensate the\xa0minor reweighting and this can drive to a training performance loss. As for the other hyperparameters, the right value for\xa0η must be discovered using a cross-validation technique (alternatively, if it's the only value that must be fine-tuned, it's possible to start with one and proceed by decreasing its value until the maximum accuracy has been reached). The complete\xa0AdaBoost.M1\xa0algorithm is as follows:", '7874d709-9c66-49f7-94de-08d4c5e4de91.xhtml': "This variant, called Stagewise Additive Modeling using a Multi-class Exponential loss (SAMME), was proposed by Zhu, Rosset, Zou, and Hastie in Multi-class AdaBoost,\xa0Zhu J., Rosset S., Zou H., Hastie T.,\xa001/2006. The goal is to adapt AdaBoost.M1 in order to work properly in multi-class scenarios. As this is a discrete version, its structure is almost the same, with a difference in the estimator weight computation. Let's consider a label dataset, Y:  Now, there are p different classes and it's necessary to consider that a random guess estimator cannot reach an accuracy equal to 0.5; therefore, the new estimator weights are computed as follows:  In this way, the threshold is pushed forward and\xa0α(t) will be zero when the following is true: \xa0 The following graph shows the plot of\xa0α(t) with p = 10: Employing this correction, the boosting process can successfully cope with multi-class problems without the bias normally introduced by AdaBoost.M1 when p > 2 (α(t)\xa0> 0 when the error is less than an actual random guess, which is a function of the number of classes). As the performance of this algorithm is clearly superior, the majority of AdaBoost implementations aren't based on the original algorithm anymore (as already mentioned, for example, Scikit-Learn implements AdaBoost.SAMME and the real-valued version AdaBoost.SAMME.R). Of course, when p = 2, AdaBoost.SAMME is exactly equivalent to AdaBoost.M1.", 'a8321198-f131-462b-871e-1c809ec727a3.xhtml': "AdaBoost.SAMME.R is a variant that works with classifiers that can output prediction probabilities. This is normally possible employing techniques such as Platt scaling, but it's important to check whether a specific classifier implementation is able to output the probabilities without any further action. For example, SVM implementations provided by Scikit-Learn don't compute the probabilities unless the parameter probability=True (because they require an extra step\xa0that could be useless in some cases). In this case, we assume that the output of each classifier is a probability vector:  Each component is the conditional probability that the jth\xa0class is output given the input xi. When working with a single estimator, the winning class is obtained through the argmax(•) function; however, in this case, we want to re-weight each learner so as to obtain a sequentially grown ensemble. The basic idea is the same as AdaBoost.M1, but, as now we manage probability vectors, we also need an estimator weighting function that depends on the single sample xi (this function indeed wraps every single estimator that is now expressed as a probability vectorial function pi(t)(y=i|x)):\xa0  Considering the properties of logarithms, the previous expression is equivalent to a discrete\xa0α(t); however, in this case, we don't rely on a weighted error sum (the theoretical explanation is rather complex and is beyond the scope of this book. The reader can find it in the aforementioned paper, even if the method presented in the next chapter discloses a fundamental part of the logic). To better understand the behavior of this function, let's consider a simple scenario with p = 2. The first case is a sample that the learner isn't able to classify (p=(0.5, 0.5)):  In this case, the uncertainty is maximal and the classifier cannot be trusted for this sample, so the weight becomes null for all output probabilities. Now, let's apply the boosting, obtaining the probability vector p=(0.7, 0.3):  The first class will become positive and its magnitude will increase when p\xa0→ 1, while the other one is the opposite value. Therefore, the functions are symmetric and allow working with a sum: This approach is very similar to a weighted majority vote because the winning class yi is computed taking into account not only the number of estimators whose output is yi but also their relative weight and the negative weight of the remaining classifiers. A class can be selected only if the strongest classifiers predicted it and the impact of the other learners is not sufficient to overturn this result. In order to update the weights, we need to consider the impact of all probabilities. In particular, we want to reduce the uncertainty (which can degenerate to a purely random guess) and force a superior attention focused on all those samples that have been misclassified. To achieve this goal, we need to define the yi and p(t)(xi) vectors, which contain, respectively, the one-hot encoding of the true class (for example, (0, 0, 1, ..., 0)) and the output probabilities yielded by the estimator (as a column vector). Hence, the update rule becomes as follows:  If, for example, the true vector is (1, 0) and the output probabilities are (0.1, 0.9), with\xa0η=1, the weight of the sample will be multiplied by about 3.16. If instead, the output probabilities are (0.9, 0.1), meaning the sample has been successfully classified, the multiplication factor will become closer to 1. In this way, the new data distribution D(t+1), analogously to AdaBoost.M1, will be more peaked on the samples that need more attention. All implementations include the learning rate as a hyperparameter because, as already explained, the default value equal to 1.0 cannot be the best choice for specific problems. In general, a lower learning rate allows reducing the instability when there are many outliers and improves the generalization ability thanks to a slower convergence towards the optimum. When η < 1, every new distribution is slightly more focused on the misclassified samples, allowing the estimators to search for a better parameter set without big jumps (that can lead the estimator to skip an optimal point). However, contrary to Neural Networks that normally work with small batches, AdaBoost can often perform quite well also with\xa0η=1 because the correction is applied only after a full training step. As usual, I recommend performing a grid search to select the right values for each specific problem. The complete\xa0AdaBoost.SAMME.R\xa0algorithm is as follows:", '20e5ca65-17df-4b5b-b85d-c033ba4b9df3.xhtml': "A slightly more complex variant has been proposed by Drucker (in Improving Regressors using Boosting Techniques,\xa0Drucker H.,\xa0ICML 1997) to manage regression problems. The weak learners are commonly decision trees and the main concepts are very similar to the other variants (in particular, the re-weighting process applied to the training dataset). The real difference is the strategy adopted in order to choose the final prediction yi given the input sample xi. Assuming that there are Nc estimators and each of them is represented as function dt(x), we can compute the absolute residual ri(t)\xa0for every input sample:  Once the set Ri containing all the absolute residuals has been populated, we can compute the quantity Sr = sup Ri and compute the values of a cost function that must be proportional to the error. The common choice that is normally implemented (and suggested by the author himself) is a linear loss:  This loss is very flat and it's directly proportional to the error. In most cases, it's a good choice because it avoids premature over-specialization and allows the estimators to readapt their structure in a\xa0gentler way. The most obvious alternative is the square loss, which starts giving more importance to those samples whose prediction error is larger. It is defined as follows: \xa0 The last cost function is strictly related to AdaBoost.M1 and it's exponential:  This is normally a less robust choice because, as we are also going to discuss in the next section, it penalizes small errors in favor of larger ones. Considering that these functions are also employed in the re-weighting process, an exponential loss can force the distribution to assign very high probabilities to samples whose misclassification error is high, driving the estimators to become over-specialized with effect from the first iterations. In many cases (such as in neural networks), the loss functions are normally chosen according to their specific properties but, above all, to the ease to minimize them. In this particular scenario, loss functions are a fundamental part of the boosting process and they must be chosen considering the impact on the data distribution. Testing and cross-validation provide the best tool to make a reasonable decision. Once the loss function has been evaluated for all training samples, it's possible to build the global cost function as the weighted average of all losses. Contrary to many algorithms that simply sum or average the losses, in this case, it's necessary to consider the structure of the distribution. As the boosting process reweights the samples, also the corresponding loss values must be filtered to avoid a bias. At the iteration t, the cost function is computed as follows:  This function is proportional to the weighted errors, which can be either linearly filtered or emphasized using a quadratic or exponential function. However, in all cases, a sample whose weight is lower will yield a smaller contribution, letting the algorithm focus on the samples more difficult to be predicted. Consider that, in this case, we are working with classifications; therefore, the only measure we can exploit is the loss. Good samples yield low losses, hard samples yield proportionally higher losses. Even if it's possible to use C(t) directly, it's preferable to define a confidence measure:  This index is inversely proportional to the average confidence at the iteration t. In fact, when C(t)\xa0→ 0,\xa0γ(t)\xa0→ 0 and when\xa0C(t)\xa0→\xa0∞,\xa0γ(t)\xa0→ 1. The weight update is performed considering the overall confidence and the specific loss value:  A weight will be decreased proportionally to the loss associated with the corresponding absolute residual. However, instead of using a fixed base, the global confidence index is chosen. This strategy allows a further degree of adaptability, because an estimator with a low confidence doesn't need to focus only on a small subset and, considering that\xa0γ(t)\xa0is bounded between 0 and 1 (worst condition), the exponential becomes ineffective when the cost function is very high (1x = 1), so that the weights remain unchanged. This procedure is not very dissimilar to the one adopted in other variants, but it tries to find a trade-off between global accuracy and local misclassification problems, providing an extra degree of robustness. The most complex part of this algorithm is the approach employed to output a global prediction. Contrary to classification algorithms, we cannot easily compute an average, because it's necessary to consider the global confidence at each iteration. Drucker proposed a method based on the weighted median of all outputs. In particular, given a sample xi, we define the set of predictions:  As weights, we consider the log(1 /\xa0γ(t)), so we can define a weight set:  The final output is the median of Y weighted according to Γ (normalized so that the sum is 1.0). As\xa0γ(t)\xa0→ 1 when the confidence is low, the corresponding weight will tend to 0. In the same way, when the confidence is high (close to 1.0), the weight will increase proportionally and the chance to pick the output associated with it will be higher. For example, if the outputs are Y = {1, 1.2, 1.3, 2.0, 2.2, 2.5, 2.6} and the weights are\xa0Γ = { 0.35, 0.15, 0.12, 0.11, 0.1, 0.09, 0.08 }, the weighted median corresponds to the second index, therefore the global estimator will output 1.2 (which is, also intuitively, the most reasonable choice). The procedure to find the median is quite simple: The complete\xa0AdaBoost.R2\xa0algorithm is as follows:", 'b42bb20c-cb15-4ccf-a192-1cec5be05aea.xhtml': "Let's continue using the Wine dataset in order to analyze the performance of AdaBoost with different parameters. Scikit-Learn,\xa0like almost all algorithms, implements both a classifier AdaBoostClassfier (based on the algorithm SAMME and SAMME.R) and a regressor AdaBoostRegressor (based on the algorithm R2). In this case, we are going to use the classifier, but I invite the reader to test the regressor using a custom dataset or one of the built-in toy datasets. In both classes, the most important parameters are n_estimators and learning_rate (default value set to 1.0). The default underlying weak learner is always a decision tree, but it's possible to employ other models creating a base instance and passing it through the parameter base_estimator. As explained in the chapter, real-valued AdaBoost algorithms require an output based on a probability vector. In Scikit-Learn, some classifiers/regressors (such as SVM) don't compute the probabilities unless it is explicitly required (setting the parameter probability=True); therefore, if an exception is raised, I invite you to check the documentation in order to learn how to force the algorithm to compute them. The examples we are going to discuss have only a didactic purpose because they focus on a single parameter. In a real-world scenario, it's always better to perform a grid search (which is more expensive), so as to analyze a set of combinations. Let's start analyzing the cross-validation score as a function of the number of estimators (the vectors X and Y are the ones defined in the previous example): We have considered a range starting from 10 trees and ending with 200 trees with steps of 10 trees. The learning rate is kept constant and equal to 0.8. The resulting plot is shown in the following graph: The maximum is reached with 50 estimators. Larger values cause performance worsening due to the over-specialization and a consequent variance increase. As explained also in other chapters, the capacity of a model must be tuned according to the Occam's Razor principle, not only because the resulting model can be faster to train, but also because a capacity excess is normally saturated, overfitting the training set and reducing the scope for generalization. Cross-validation can immediately show this effect, which, instead, can remain hidden when a standard training/test set split is done (above all when the samples are not shuffled). Let's now check the performance with different learning rates (keeping the number of trees fixed): The final plot is shown in the following graph: Again, different learning rates yield different accuracies. The choice of η = 0.8 seems to be the most effective, as higher and lower values lead to performance worsening. As explained, the learning rate has a direct impact on the re-weighting process. Very small values require a larger number of estimators because subsequent distributions are very similar. On the other side, large values can lead to a premature over-specialization. Even if the default value is 1.0, I always suggest checking the accuracy also with smaller values. There's no golden rule for picking the right learning rate in every case, but it's important to remember that lower values allow the algorithm to smoothly adapt to fit the training set in a gentler way, while higher values reduce the robustness to outliers, because the samples that have been misclassified are immediately boosted and the probability of sampling them increases very rapidly. The result of this behavior is a constant focus on those samples that may be affected by noise, almost forgetting the structure of the remaining sample space. The last experiment we want to make is analyzing the performance\xa0after a dimensionality reduction performed with Principal Component Analysis (PCA) and Factor Analysis (FA) (with 50 estimators and η = 0.8): The resulting plot is shown in the following graph: This exercise confirms some important features analyzed in Chapter 5, EM Algorithm and Applications. First of all, performances are not dramatically affected even by a 50% dimensionality reduction. This consideration is further confirmed by the feature importance analysis performed in the previous example. Decision trees can perform quite a good classification considering only 6/7 features because the remaining ones offer a marginal contribution to the characterization of a sample. Moreover, FA is almost always superior to PCA. With 7 components, the accuracy achieved using the FA algorithm is higher than 0.95 (very close to the value achieved with no reduction), while a PCA reaches this value with 12 components. The reader should remember that PCA is a particular case of FA, with the assumption of homoscedastic noise. The diagram confirms that this condition is not acceptable with the Wine dataset. Assuming different noise variances allows remodeling the reduced dataset in a more accurate way, minimizing the cross-effect of the missing features. Even if PCA is normally the first choice, with large datasets, I suggest you always compare the performance with a Factor Analysis and choose the technique that guarantees the best result (given also that FA is more expensive in terms of computational complexity).\xa0", '4397d9ee-ca00-4159-98a0-cdca94a43e89.xhtml': "At this point, we can introduce a more general method of creating boosted ensembles. Let's choose a generic algorithm family, represented as follows:  Each model is parametrized using the vector\xa0θi and there are no restrictions on the kind of method that is employed. In this case, we are going to consider decision trees (which is one of the most diffused algorithms when this boosting strategy is employed—in this case, the algorithm is known as gradient tree boosting), but the theory is generic and can be easily applied to more complex models, such as neural networks. In a decision tree, the parameter vector\xa0θi is made up of selection tuples, so the reader can think of this method as a pseudo-random forest where, instead of randomness, we look for extra optimality exploiting the previous experience. In fact, as with AdaBoost, a gradient boosting ensemble is built sequentially, using a technique that is formally defined as Forward Stage-wise Additive Modeling. The resulting estimator is represented as a weighted sum:  Therefore the variables to manage are the single estimator weights\xa0αi and the parameter vectors\xa0θi. However, we don't have to work with the whole set, but with a single tuple (αi,\xa0θi), without modifying the values already chosen during the previous iterations. The general procedure can be summarized with a loop: How is it possible to find out the best tuple? We have already presented a strategy for improving the performance of every learner through boosting the dataset. In this case, instead, the algorithm is based on a cost function that we need to minimize:  In particular, the generic optimal tuple is obtained as follows:  As the process is sequential, each estimator is optimized to improve the previous one's accuracy. However, contrary to AdaBoost, we are not constrained to impose a specific loss function (it's possible to prove that AdaBoost.M1 is equivalent to this algorithm with an exponential loss but the proof is beyond the scope of this book). As we are going to discuss, other cost functions can yield better performances in several different scenarios, because they avoid the premature convergence towards sub-optimal minima. The problem could be considered as solved by employing the previous formula to optimize each new learner; however, the argmin(•) function needs a complete exploration of the cost function space and, as C(•) depends on each specific model instance and, therefore, on\xa0θi, it's necessary to perform several retraining processes in order to find the optimal solution. Moreover, the problem is generally non-convex and the number of variables can be very high. Numerical algorithms such as L-BFGS or other quasi-Newton methods need too many iterations and a prohibitive computational time. It's clear that such an approach is not affordable in the vast majority of cases and the Gradient Boosting algorithm has been proposed as an intermediate solution. The idea is to find a sub-optimal solution with a gradient descent strategy limited to a single step for each iteration. In order to present the algorithm, it's useful to rewrite the additive model with an explicit reference to the optimal goal:  Note that the cost function is computed carrying on all the previously trained models; therefore, the correction is always incremental. If the cost function L is differentiable (a fundamental condition that is not difficult to meet), it's possible to compute the gradient with respect to the current additive model (at the ith iteration, we need to consider the additive model obtained summing all the previous i-1 models):  At this point, a new classifier can be added by moving the current additive model into the negative direction of the gradient:  We haven't considered the parameter\xa0αi yet (nor the learning rate η, which is a constant), however the reader familiar with some basic calculus can immediately understand the effect of an update is to reduce the value of the global loss function by forcing the next model to improve its accuracy with respect to its predecessors. However, a single gradient step isn't enough to guarantee an appropriate boosting strategy. In fact, as discussed previously, we also need to weight each classifier according to its ability to reduce the loss. Once the gradient has been computed, it's possible to determine the best value for the weight\xa0αi\xa0with a direct minimization of the loss function (using a line search algorithm) computed considering the current additive model with\xa0α as an extra variable:  When using the gradient tree boosting variant, an improvement can be achieved by splitting the weight\xa0αi into m sub-weights\xa0αi(j) associated with each terminal node of the tree. The computational complexity is slightly increased, but the final accuracy can be higher than the one obtained with a single weight. The reason derives from the functional structure of a tree. As the boosting forces a specialization in specific regions, a single weight could drive to an over-estimation of a learner also when a specific sample cannot be correctly classified. Instead, using different weights, it's possible to operate a fine-grained filtering of the result, accepting or discarding an outcome according to its value and to the properties of the specific tree.\xa0 This solution cannot provide the same accuracy of a complete optimization, but it's rather fast and it's possible to compensate for this loss using more estimators and a lower learning rate. Like many other algorithms, gradient boosting must be tuned up in order to yield the maximum accuracy with a low variance. The learning rate is normally quite smaller than 1.0 and its value should be found by validating the results and considering the total number of estimators (it's better to reduce it when more learners are employed). Moreover, a regularization technique could be added in order to prevent overfitting. When working with specific classifier families (such as logistic regression or neural networks), it's very easy to include an L1 or L2 penalty, but it's not so easy with other estimators. For this reason, a common regularization technique (implemented also by Scikit-Learn) is the downsampling of the training dataset. Selecting P < N random samples allows the estimators to reduce the variance and prevent overfitting. Alternatively, it's possible to employ a random feature selection (for gradient tree boosting only) as in a random forest; picking a fraction of the total number of features increases the uncertainty and avoids over-specialization.\xa0Of course, the main drawback to these techniques is a loss of accuracy (proportional to the downsampling/feature selection ratio) that must be analyzed in order to find the most appropriate trade-off.\xa0 Before moving to the next section, it's useful to briefly discuss the main cost functions that are normally employed with this kind of algorithms. In the first chapter, we have presented some common cost functions, like mean squared error, Huber Loss (very robust in regression contexts), and cross-entropy. They are all valid examples, but there are other functions that are peculiar to classification problems. The first one is Exponential Loss, defined as follows:  As pointed out by\xa0Hastie, Tibshirani and, Friedman, this function transforms the gradient boosting into an AdaBoost.M1 algorithm.\xa0The corresponding cost function has a very precise behavior that sometimes is not the most adequate to solve particular problems. In fact, the result of an exponential loss has a very high impact when the error is large, yielding distributions that are strongly peaked around a few samples. The subsequent classifiers can be consequently driven to over-specialize their structure to cope only with a small data region, with a concrete risk of losing the ability to\xa0correctly classify other samples. In many situations, this behavior is not dangerous and the final bias-variance trade-off is absolutely reasonable; however, there are problems where a softer loss function can allow a better final accuracy and generalization ability. The most common choice for real-valued binary classification problems is Binomial Negative Log-Likelihood Loss (deviance), defined as follows (in this case we are assuming that the classifier f(•) is not thresholded, but outputs a positive-class probability):  This loss function is the same employed in Logistic Regressions and, contrary to Exponential Loss, doesn't yield peaked distributions. Two misclassified samples with different probabilities are boosted proportionally to the error (not the exponential value), so as to force the classifiers to focus on all the misclassified population with almost the same probability (of course, a higher probability assigned to samples whose error is very large is desirable, assuming that all the other misclassified samples have always a good chance to be selected). The natural extension of the Binomial Negative Log-Likelihood Loss to multi-class problems is the Multinomial Negative Log-Likelihood Loss, defined as follows (the classifier f(•) is represented as probability vector with p components):  In the previous formula, the notation Iy=j must be interpreted as an indicator function, which is equal to 1 when y=j and 0 otherwise. The behavior of this loss function is perfectly analogous to the binomial variant and, in general, it is the default choice for classification problems. The reader is invited to test the examples with both exponential loss and deviance and compare the results. The complete gradient boosting\xa0algorithm is as follows:", '5b842c3f-60ed-492c-9b43-25ffc60bfabd.xhtml': "In this example, we want to employ a gradient tree boosting classifier (class GradientBoostingClassifier) and check the impact of the maximum tree depth (parameter max_depth) on the performance. Considering the previous example, we start by setting n_estimators=50 and learning_rate=0.8: The result is shown in the following diagram: As explained in the first section, the maximum depth of a decision tree is strictly related to the possibility of interaction among features. This can be a positive or negative aspect when the trees are employed in an ensemble. A very high interaction level can create over-complex separation hyperplanes and reduce the overall variance. In other cases, a limited interaction results in a higher bias. With this particular (and simple) dataset, the gradient boosting algorithm can achieve better performances when the max depth is 2 (consider that the root has a depth equal to zero) and this is partially confirmed by both the feature importance analysis and dimensionality reductions. In many real-world situations, the result of such a research could be completely different, with increased performance, therefore I suggest\xa0you cross-validate the results (it's better to\xa0employ a grid search) starting from a minimum depth and increasing the value until the maximum accuracy has been achieved. With max_depth=2, we want now to tune up the learning rate, which is a fundamental parameter in this algorithm: The corresponding plot is shown in the following diagram: Unsurprisingly, gradient tree boosting outperforms AdaBoost with\xa0η\xa0≈ 0.9, achieving a cross-validation accuracy slightly lower than 0.99. The example is very simple, but it clearly shows the power of this kind of techniques. The main drawback is the complexity. Contrary to single models, ensembles are more sensitive to changes to the hyperparameters and more detailed research must be conducted in order to optimize the models. When the datasets are not excessively large, cross-validation remains the best choice. If, instead, we are pretty sure that the dataset represents almost perfectly the underlying data generating process, it's possible to shuffle it and split it into two (training/test) or three blocks (training/test/validation) and proceed by optimizing the hyperparameters and trying to overfit the test set (this expression can seem strange, but overfitting the test set means maximizing the generalization ability while learning perfectly the training set structure).", '20aac16d-b474-44db-ab16-093519e7535e.xhtml': "A simpler but no less effective way to create an ensemble is based on the idea of exploiting a limited number of strong learners whose peculiarities allow them to yield better performances in particular regions of the sample space. Let's start considering a set of Nc discrete-valued classifiers f1(x), f2(x), ..., fNc(x). The algorithms are different, but they are all trained with the same dataset and output the same label set. The simplest strategy is based on a hard-voting approach:  In this case, the function n(•) counts the number of estimators that output the label yi. This method is rather powerful in many cases, but has some limitations. If we rely only on a majority vote, we are implicitly assuming that a correct classification is obtained by a large number of estimators. Even if, Nc/2 + 1 votes are necessary to output a result, in many cases their number is much higher. Moreover, when k is not very large, also Nc/2 + 1 votes imply a symmetry that involves a large part of the population. This condition often drives to the training of useless models that could be simply replaced by a single well-fitted strong learner. In fact, let's suppose that the ensemble is made up of three classifiers and one of them is more specialized in regions where the other two can easily be driven to misclassifications. A hard-voting strategy applied to this ensemble could continuously penalize the more complex estimator in favor of the other classifiers. A more accurate solution can be obtained by considering real-valued outcomes. If each estimator outputs a probability vector, the confidence of a decision is implicitly encoded in the values. For example, a binary classifier whose output is (0.52, 0.48) is much more uncertain than another classifier outputting (0.95, 0.05). Applying a threshold is equivalent to flattening the probability vectors and discarding the uncertainty. Let's consider an ensemble with three classifiers and a sample that is hard to classify because it's very close to the separation hyperplane. A hard-voting strategy decides for the first class because the thresholded output is (1, 1, 2). Then we check the output probabilities, obtaining (0.51, 0.49), (0.52, 0.48), (0.1, 0.9). After averaging the probabilities, the ensemble output becomes about (0.38, 062) and by applying\xa0argmax(•), we get the second class as the final decision. In general, it's also a good practice to consider a weighted average, so that the final class is obtained as follows (assuming the output of the classifier is a probability vector):\xa0  The weights can be simply equal to 1.0 if no weighting is required or they can reflect the level of trust we have for each classifier. An important rule is to avoid the dominance of a classifier in the majority of cases because it would be an implicit fallback to a single estimator scenario. A good voting example should always allow a minority to overturn a result when their confidence is quite higher than the majority. In this strategies, the weights can be considered as hyperparameters and tuned up using a grid search with cross-validation. However, contrary to other ensemble methods, they are not fine-grained, therefore the optimal value is often a compromise among some different possibilities. A slightly more complex technique is called stacking and consists of using an extra classifier as a post-filtering step. The classical approach consists of training the classifiers separately, then the whole dataset is transformed into a prediction set (based on class labels or probabilities) and the combining classifier is trained to associate the predictions to the final classes. Using even very simple models like Logistic Regressions or Perceptrons, it's possible to mix up the predictions so as to implement a dynamic reweighting that is a function of the input values. A more complex approach is feasible only when a single training strategy can be used to train the whole ensemble (including the combiner). For example, it could be employed with neural networks that, however, have already an implicit flexibility and can often perform quite better than complex ensembles.", '521ec9e8-2d72-4c78-879e-b008ee7fa9c7.xhtml': 'In this example, we are going to employ the MNIST handwritten digits dataset. As the concept is very simple, our goal is to show how to combine two completely different estimators to improve the overall cross-validation accuracy. For this reason, we have selected a Logistic Regression and a decision tree, which are structurally different. In particular, while the former is a linear model that works with the whole vectors, the latter is a feature-wise estimator that can support the decision only in particular cases (images are not made up of semantically consistent features, but the over-complexity of a Decision Tree can help with particular samples which are very close to the separation hyperplane and, therefore, more difficult to classify with a linear method). The first step is loading and normalizing the dataset (this operation is not important with a Decision\xa0Tree, but has a strong impact on the performances of a Logistic Regression): At this point, we need to evaluate the performances of both estimators individually: As expected, the Logistic Regression (∼94% accuracy) outperforms the decision tree (83% accuracy); therefore, a hard-voting strategy is not the best choice. As we trust the Logistic Regression more, we can employ soft voting with a weight vector set to (0.9, 0.1). The class VotingClassifier accepts a list of tuples (name of the estimator, instance) that must be\xa0supplied\xa0through the estimators parameter. The strategy can be specified using parameter voting (it can be either "soft" or "hard") and the optional weights, using the parameter with the same name: Using a soft-voting strategy, the estimator is able to outperform Logistic Regression by reducing the global uncertainty. I invite the reader to test this algorithm with other datasets, using more estimators, and try to find out the optimal combination using both the hard and soft voting strategies.\xa0', '0cac2df1-4f31-4e30-befc-f7e7d2094c91.xhtml': "This is not a proper ensemble learning technique, but it is sometimes known as bucketing. In the previous section, we have discussed how a few strong learners with different peculiarities can be employed to make up a committee. However, in many cases, a single learner is enough to achieve a good bias-variance trade-off but it's not so easy to choose among the whole Machine Learning algorithm population. For this reason, when a family of similar problems must be solved (they can differ but it's better to consider scenarios that can be easily compared), it's possible to create an ensemble containing several models and use cross-validation to find the one whose performances are the best. At the end of the process, a single learner will be used, but its choice can be considered like a grid search with a voting system. Sometimes this technique can unveil important differences even using similar datasets. For example, during the development of a system, a first dataset (X1, Y1) is provided. Everybody expects that it is correctly sampled from an underlying data generating process pdata and, so, a generic model is fitted and evaluated. Let's imagine that a SVM achieves a very high validation accuracy (evaluated using a k-fold cross-validation) and, therefore, it is chosen as the final model. Unfortunately, a second, larger dataset\xa0(X2, Y2) is provided and the final mean accuracy worsens. We might simply think that the residual variance of the model cannot let it generalize correctly or, as sometimes happens, we can say the second dataset contains many outliers which are not correctly classified. The real situation is a little more complex: given a dataset, we can only suppose that it represents a complete data distribution. Even when the number of samples is very high or we use data augmentation techniques, the population might not represent some particular samples that will be analyzed by the system we are developing. Bucketing is a good way to create a security buffer that can be exploited whenever the scenario changes. The ensemble can be made up of completely different models, models belonging to the same family but differently parametrized (for example, different kernel SVMs) or a mixture\xa0of composite algorithms (like PCA + SVM, PCA + decision trees/random forests, and so on). The most important element is the cross-validation. As explained in the first chapter, splitting the dataset into training and test sets can be an acceptable solution only when the number of samples and their variability is high enough to justify the belief that it correctly represents the final data distribution. This often happens in deep learning, where the dimensions of the datasets are quite large and the computational complexity doesn't allow retraining the model too many times. Instead, in classical Machine Learning contexts, cross-validation is the only way to check the behavior of a model when trained with a large random subset and tested on the remaining samples. Ideally, we'd like to observe the same performances, but it can also happen that the accuracy is higher in some folds and quite lower in other. When this phenomenon is observed and the dataset is the final one, it probably means that the model is not able to manage one or more regions of the sample space and a boosting approach could dramatically improve the final accuracy.", '0e8ce2fb-95d4-42a5-a285-148d7081eb8e.xhtml': "In this chapter, we introduced the main concepts of ensemble learning, focusing on both bagging and boosting techniques. In the first section, we explained the difference between strong and weak learners and we presented the big picture of how it's possible to combine the estimators to achieve specific goals. The next topic focused on the properties of decision trees and their main strengths and weaknesses. In particular, we explained that the structure of a tree causes a natural increase in the variance. The bagging technique called random forests allow mitigating this problem, improving at the same time the overall accuracy. A further variance reduction can be achieved by increasing the randomness and employing a variant called extra randomized trees. In the example, we have also seen how it's possible to evaluate the importance of each input feature and perform dimensionality reduction without involving complex statistical techniques. In the third section, we presented the most famous boosting techniques, AdaBoost, which is based on the concept of creating a sequential additive model, when each new estimator is trained using a reweighted (boosted) data distribution. In this way, every learner is added to focus on the misclassified samples without interfering with the previously added models. We analyzed the original M1 discrete variant and the most effective alternatives called SAMME and SAMME.R (real-valued), and R2 (for regressions), which are implemented in many Machine Learning packages. After AdaBoost, we extended the concept to a generic Forward Stage-wise Additive Model, where the task of each new estimator is to minimize a generic cost function. Considering the complexity of a full optimization, a gradient descent technique was presented that, combined with an estimator weight line search, can yield excellent performances both in classification and in regression problems. The final topics concerned how to build ensembles using a few strong learners, averaging their prediction or considering a majority vote. We discussed the main drawback of thresholded classifiers and we showed how it's possible to build a soft-voting model that is able to trust the estimator that show less uncertainty. Other useful topics are the Stacking method, which consists of using an extra classifier to process the prediction of each member of the ensemble and how it's possible to create candidate ensembles that are evaluated using a cross-validation technique to find out the best estimator for each specific problem. In the next chapter, we are going to begin discussing the most important deep learning techniques, introducing the fundamental concepts regarding neural networks and the algorithms involved in their training processes.", '6c534ffd-77de-4a84-ac83-5685c14d29c8.xhtml': 'This chapter is the introduction to the world of deep learning, whose methods make it possible to achieve the state-of-the-art performance in many classification and regression fields often considered extremely difficult to manage (such as image segmentation, automatic translation, voice synthesis, and so on). The goal is to provide the reader with the basic instruments to understand the structure of a fully connected neural network and model it using the Python tool Keras (employing all the modern techniques to speed the training process and prevent overfitting). In particular, the topics covered in the chapter are as follows:', 'cae8934f-2350-4bba-b816-bba88b2d54a7.xhtml': "The building block of a neural network is the abstraction of a biological neuron, a quite simplistic but powerful computational unit that was proposed for the first time by F. Rosenblatt in 1957, to make up the simplest neural architecture, called a perceptron, that we are going to analyze in the next section. Contrary to Hebbian Learning, which is more biologically plausible but has some strong limitations, the artificial neuron has been designed with a pragmatic viewpoint and, of course, only its structure is based on a few elements characterizing a biological cell. However, recent deep learning research activities have unveiled the enormous power of this kind of architecture. Even if there are more complex and specialized computational cells, the basic artificial neuron can be summarized as the conjunction of two blocks, which are clearly shown in the following diagram: The input of a neuron is a real-valued vector x ∈ ℜn, while the output is a scalar y ∈ ℜ. The first operation is linear: The vector w ∈ ℜn is called weight-vector (or synaptic weight vector, because, analogously to a biological neuron, it reweights the input values), while the scalar term b ∈ ℜ is a constant called bias. In many cases, it's easier to consider only the weight vector. It's possible to get rid of the bias by adding an extra input feature equal to 1 and a corresponding weight: In this way, the only element that must be learned is the weight vector. The following block is called an\xa0activation function, and it's responsible for remapping the input into a different subset. If the function is\xa0fa(z) = z, the neuron is called linear and the transformation can be omitted. The first experiments were based on linear neurons that are much less powerful than non-linear ones, and this was a reason that led many researchers to consider the perceptron as a failure, but, at the same time, this limitation opened the door for a new architecture that, instead, showed its excellent abilities. Let's now start this analysis with the first neural network ever proposed.", 'd204c52c-8f46-4308-abcc-34daefe856cd.xhtml': "Perceptron was the name that Frank Rosenblatt gave to the first neural model in 1957. A perceptron is a neural network with a single layer of input linear neurons, followed by an output unit based on the sign(•) function (alternatively, it's possible to consider a bipolar unit whose output is -1 and 1). The architecture of a perceptron is shown in the following diagram: Even if the diagram can appear as quite complex, a perceptron can be summarized by the following equation: All the vectors are conventionally column-vectors; therefore, the dot product wTxi transforms the input into a scalar, then the bias is added, and the binary output is obtained using the step function, which outputs 1 when z > 0 and 0 otherwise. At this point, a reader could object that the step function is non-linear; however, a non-linearity applied to the output layer is only a filtering operation that has no effect on the actual computation. Indeed, the output is already decided by the linear block, while the step function is employed only to impose a binary threshold. Moreover, in this analysis, we are considering only single-value outputs (even if there are multi-class variants) because our goal is to show the dynamics and also the limitations, before moving to more generic architectures that can be used to solve extremely complex problems. A perceptron can be trained with an online algorithm (even if the dataset is finite) but it's also possible to employ an offline approach that repeats for a fixed number of iterations or until the total error becomes smaller than a predefined threshold. The procedure is based on the squared error loss function (remember that, conventionally, the term loss is applied to single samples, while the term cost refers to the sum/average of every single loss): When a sample is presented, the output is computed, and if it is wrong, a weight correction is applied (otherwise the step is skipped). For simplicity, we don't consider the bias, as it doesn't affect the procedure. Our goal is to correct the weights so as to minimize the loss. This can be achieved by computing the partial derivatives with respect to wi: Let's suppose that w(0) = (0, 0) (ignoring the bias) and the sample, x = (1, 1), has y = 1. The perceptron misclassifies the sample, because sign(wTx) = 0. The partial derivatives are both equal to -1; therefore, if we subtract them from the current weights, we obtain w(1) = (1, 1) and now the sample is correctly classified because sign(wTx) = 1. Therefore, including a learning rate η, the weight update rule becomes as follows: When a sample is misclassified, the weights are corrected proportionally to the difference between actual linear output and true label. This is a variant of a learning rule called the\xa0delta rule, which represented the first step toward the most famous training algorithm, employed in almost any supervised deep learning scenario (we're going to discuss it in the next sections). The algorithm has been proven to converge to a stable solution in a finite number of states as the dataset is linearly separable. The formal proof is quite tedious and very technical, but the reader who is interested can find it in Perceptrons, Minsky M. L., Papert S. A., The MIT Press. In this chapter, the role of the learning rate becomes more and more important, in particular when the update is performed after the evaluation of a single sample (like in a perceptron) or a small batch. In this case, a high learning rate (that is, one greater than 1.0) can cause an instability in the convergence process because of the magnitude of the single corrections. When working with neural networks, it's preferable to use a small learning rate and repeat the training session for a fixed number of epochs. In this way, the single corrections are limited, and only if they are confirmed by the majority of samples/batches, they can become stable, driving the network to converge to an optimal solution. If, instead, the correction is the consequence of an outlier, a small learning rate can limit its action, avoiding destabilizing the whole network only for a few noisy samples. We are going to discuss this problem in the next sections. Now, we can describe the full perceptron algorithm and close the paragraph with some important considerations: The algorithm is very simple, and the reader should have noticed an analogy with a logistic regression. Indeed, this method is based on a structure that can be considered as a perceptron with a sigmoid output activation function (that outputs a real value that can be considered as a probability). The main difference is the training strategy—in a logistic regression, the correction is performed after the evaluation of a cost function based on the negative log likelihood: This cost function is the well-known cross-entropy and, in the first chapter, we showed that minimizing it is equivalent to reducing the Kullback-Leibler divergence between the true and predicted distribution. In almost all deep learning classification tasks, we are going to employ it, thanks to its robustness and convexity (this is a convergence guarantee in a logistic regression, but unfortunately the property is normally lost in more complex architectures).", '27da22c6-315b-47ae-9c3e-4f27a6fadc4b.xhtml': "Even if the algorithm is very simple to implement from scratch, I prefer to employ the Scikit-Learn implementation Perceptron, so as to focus the attention on the limitations that led to non-linear neural networks. The historical problem that showed the main weakness of a perceptron is based on the XOR dataset. Instead of explaining, it's better to build it and visualize the structure: The plot showing the true labels is shown in the following diagram: As it's possible to see, the dataset is split into four blocks that are organized as the output of a logical XOR operator. Considering that the separation hypersurface of a two-dimensional perceptron (as well as the one of a logistic regression) is a line; it's easy to understand that any possible final configuration can achieve an accuracy that is about 50% (a random guess). To have a confirmation, let's try to solve this problem: The Scikit-Learn implementation offers the possibility to add a regularization term (see Chapter 1, Machine Learning Models Fundamentals) through the parameter penalty (it can be 'l1', 'l2' or 'elasticnet') to avoid overfitting and improve the convergence speed (the strength can be specified using the parameter alpha). This is not always necessary, but as the algorithm is offered in a production-ready package, the designers decided to add this feature. Nevertheless, the average cross-validation accuracy is slightly higher than 0.5 (the reader is invited to test any other possible hyperparameter configuration). The corresponding plot (that can change with different random states or subsequent experiments) is shown in the following diagram: It's obvious that a perceptron is another linear model without specific peculiarities, and its employment is discouraged in favor of other algorithms like logistic regression or SVM. After 1957, for a few years, many researchers didn't hide their delusion and considered the neural network like a promise never fulfilled. It was necessary to wait until a simple modification to the architecture, together with a powerful learning algorithm, opened officially the door of a new fascinating machine learning branch (later called deep learning).", 'e569299f-cb79-4361-80a2-31f8b125fa4c.xhtml': "The main limitation of a perceptron is its linearity. How is it possible to exploit this kind of architecture by removing such a constraint? The solution is easier than any speculation. Adding at least a non-linear layer between input and output leads to a highly non-linear combination, parametrized with a larger number of variables. The resulting architecture, called Multilayer Perceptron (MLP) and containing a single (only for simplicity) Hidden Layer, is shown in the following diagram: This is a so-called feed-forward network, meaning that the flow of information begins in the first layer, proceeds always in the same direction and ends at the output layer. Architectures that allow a partial feedback (for example, in order to implement a local memory) are called recurrent networks and will be analyzed in the next chapter. In this case, there are two weight matrices, W and H, and two corresponding bias vectors, b and c. If there are m hidden neurons, xi ∈ ℜn × 1 (column vector), and yi ∈ ℜk × 1, the dynamics are defined by the following transformations: A fundamental condition for any MLP is that at least one hidden-layer activation function fh(•) is non-linear. It's straightforward to prove that m linear hidden layers are equivalent to a single linear network and, hence, an MLP falls back into the case of a standard perceptron. Conventionally, the activation function is fixed for a given layer, but there are no limitations in their combinations. In particular, the output activation is normally chosen to meet a precise requirement (such as multi-label classification, regression, image reconstruction, and so on). That's why the first step of this analysis concerns the most common activation functions and their features.", '1c8eff9d-a941-4549-a18e-56a1d05dd73e.xhtml': "In general, any continuous and differentiable function could be employed as activation; however, some of them have particular properties that allow achieving a good accuracy while improving the learning process speed. They are commonly used in the state-of-the-art models, and it's important to understand their properties in order to make the most reasonable choice.", '913c98cf-b765-4840-92e1-d873df659c6a.xhtml': "These two activations are very similar but with an important difference. Let's start defining them: The corresponding plots are shown in the following diagram: A sigmoid σ(x) is bounded between 0 and 1, with two asymptotes (σ(x) → 0 when x → -∞ and σ(x) → 1 when x → ∞). Similarly, the hyperbolic tangent (tanh) is bounded between -1 and 1 with two asymptotes corresponding to the extreme values. Analyzing the two plots, we can discover that both functions are almost linear in a short range (about [-2, 2]), and they become almost flat immediately after. This means that the gradient is high and about constant when x has small values around 0 and it falls down to about 0 for larger absolute values. A sigmoid perfectly represents a probability or a set of weights that must be bounded between 0 and 1, and therefore, it can be a good choice for some output layers. However, the hyperbolic tangent is completely symmetric, and, for optimization purposes, it's preferable because the performances are normally superior. This activation function is often employed in intermediate layers, whenever the input is normally small. The reason will be clear when the back-propagation algorithm is analyzed; however, it's obvious that large absolute inputs lead to almost constant outputs, and as the gradient is about 0, the weight correction can become extremely slow (this problem is formally known as vanishing gradients). For this reason, in many real-world applications, the next family of activation functions is often employed.", 'f1517d9b-d506-4f65-994e-63baec85a582.xhtml': "These functions are all linear (or quasi-linear for Swish) when x > 0, while they differ when x < 0. Even if some of them are differentiable when x = 0, the derivative is set to 0 in this case. The most common functions are as follows: The corresponding plots are shown in the following diagram: The basic function (and also the most commonly employed) is the ReLU, which has a constant gradient when x > 0, while it is null for x < 0. This function is very often employed in visual processing when the input is normally greater than 0 and has the extraordinary advantage to mitigate the vanishing gradient problem, as a correction based on the gradient is always possible. On the other side, ReLU is null (together with its first derivative) when x < 0, therefore every negative input doesn't allow any modification. In general, this is not an issue, but there are some deep networks that perform much better when a small negative gradient was allowed. This consideration drove to the other variants, which are characterized by the presence of the hyperparameter α, that controls the strength of the negative tail. Common values between 0.01 and 0.1 allow a behavior that is almost identical to ReLU, but with the possibility of a small weight update when x < 0. The last function, called Swish and proposed in Searching for Activation Functions, Ramachandran P., Zoph P., Le V. L., arXiv:1710.05941 [cs.NE], is based on the sigmoid and offers the extra advantage to converge to 0 when x → 0, so the non-null effect is limited to a short region bounded between [-b, 0] with b > 0. This function can improve the performance of some particular visual processing deep networks, as discussed in the aforementioned paper. However, I always suggest starting the analysis with ReLU (that is very robust and computationally inexpensive) and switch to an alternative only if no other techniques can improve the performance of a model.", 'e49d9f3a-03c0-49e8-a84f-13004df35c3f.xhtml': 'This function characterized the output layer of almost all classification networks, as it can immediately represent a discrete probability distribution. If there are k outputs yi, the softmax is computed as follows: In this way, the output of a layer containing k neurons is normalized so that the sum is always 1. It goes without saying that, in this case, the best cost function is the cross-entropy. In fact, if all true labels are represented with a one-hot encoding, they implicitly become probability vectors with 1 corresponding to the true class. The goal of the classifier is hence to reduce the discrepancy between the training distribution of its output by minimizing the function (see Chapter 1, Machine Learning Models Fundamentals, for further information):', '9128d7a4-a3d4-4129-aaed-df94a664e707.xhtml': "We can now discuss the training approach employed in an MLP (and almost all other neural networks). This algorithm is more a methodology than an actual one; therefore I prefer to define the main concepts without focusing on a particular case. The reader who is interested in implementing it will be able to apply the same techniques to different kinds of networks with minimal effort (assuming that all requirements are met). The goal of a training process using a deep learning model is normally achieved by minimizing a cost function. Let's suppose to have a network parameterized with a global vector θ, the cost function (using the same notation for loss and cost but with different parameters to disambiguate) is defined as follows: We have already explained that the minimization of the previous expression (which is the empirical risk) is a way to minimize the real expected risk and, therefore, to maximize the accuracy. Our goal is, hence, to find an optimal parameter set so that the following applies: If we consider a single loss function (associated with a sample xi and a true label yi), we know that such a function can be expressed with an explicit dependence on the predicted value: Now, the parameters have been embedded into the prediction. From calculus (without an excessive mathematical rigor that can be found in many books about optimization techniques), we know that the gradient of L, a scalar function, computed at any point (we are assuming the L is differentiable) is a vector with components: As\xa0∇L points always in the direction of the closest maximum, so the negative gradient points in the direction of the closest minimum. Hence, if we compute the gradient of L, we have a ready-to-use piece of information that can be used to minimize the cost function. Before proceeding, it's useful to expose an important mathematical property called the chain rule of derivatives: Now, let's consider a single step in an MLP (starting from the bottom) and let's exploit the chain rule: Each component of the vector y is independent of the others, so we can simplify the example by considering only an output value: In the previous expression (discarding the bias), there are two important elements—the weights, hj (that are the columns of H), and the expression, zj, which is a function of the previous weights. As L is, in turn, a function of all predictions, yi, applying the chain rule (using the variable t as the generic argument of the activation functions), we get the following: As we normally cope with vectorial functions, it's easier to express this concept using the gradient operator. Simplifying the transformations performed by a generic layer, we can express the relations (with respect to a row of H, so to a weight vector hi corresponding to a hidden unit, zi) as follows: Employing the gradient and considering the vectorial output y can be written as y = (y1, y2, ..., ym), we can derive the following expression: In this way we get all the components of the gradient of L computed with respect to the weightvectors, hi. If we move back, we can derive the expression of zj: Reapplying the chain rule, we can compute the partial derivative of L with respect to wpj (to avoid confusion, the argument of the prediction yi is called t1, while the argument of zj is called t2): Observing this expression (that can be easily rewritten using the gradient) and comparing it with the previous one, it's possible to understand the philosophy of the back-propagation algorithm, presented for the first time in Learning representations by back-propagating errors, Rumelhart D. E., Hinton G. E., Williams R. J., Nature 323/1986. The samples are fed into the network and the cost function is computed. At this point, the process starts from the bottom, computing the gradients with respect to the closest weights and reusing a part of the calculation δi (proportional to the error) to move back until the first layer is reached. The correction is indeed propagated from the source (the cost function) to the origin (the input layer), and the effect is proportional to the responsibility of each different weight (and bias). Considering all the possible different architectures, I think that writing all the equations for a single example is useless. The methodology is conceptually simple, and it's purely based on the chain rule of derivatives. Moreover, all existing frameworks, such as Tensorflow, Caffe, CNTK, PyTorch, Theano, and so on, can compute the gradients for all weights of a complete network with a single operation, so as to allow the user to focus attention on more pragmatic problems (like finding the best way to avoid overfitting and improving the training process). A very important phenomenon that is worth considering was already outlined in the previous section and now it should be clearer: the chain rule is based on multiplications; therefore, when the gradients start to become smaller than 1, the multiplication effect forces the last values to be close to 0. This problem is known as vanishing gradients and can really stop the training process of very deep models that use saturating activation functions (like sigmoid or tanh). Rectifier units provide a good solution to many specific issues, but sometimes when functions like hyperbolic tangent are necessary, other methods, like normalization, must be employed to mitigate the phenomenon. We are going to discuss some specific techniques in this chapter and in the next one, but a generic best practice is to work always with normalized datasets and, if necessary, also testing the effect of whitening.", '1c571fec-faa1-4796-9708-ed9e57d2768b.xhtml': "Once the gradients have been computed, the cost function can be moved in the direction of its minimum. However, in practice, it is better to perform an update after the evaluation of a fixed number of training samples (batch). Indeed, the algorithms that are normally employed don't compute the global cost for the whole dataset, because this operation could be very computationally expensive. An approximation is obtained with partial steps, limited to the experience accumulated with the evaluation of a small subset. According to some literature, the expression stochastic gradient descent (SGD) should be used only when the update is performed after every single sample. When this operation is carried out on every k sample, the algorithm is also known as mini-batch gradient descent; however, conventionally SGD is referred to all batches containing k ≥ 1 samples, and we are going to use this expression from now on. The process can be expressed considering a partial cost function computed using a batch containing k samples: The algorithm performs a gradient descent by updating the weights according to the following rule: If we start from an initial configuration θstart, the stochastic gradient descent process can be imagined like the path shown in the following diagram: The weights are moved towards the minimum θopt, with many subsequent corrections that could also be wrong considering the whole dataset. For this reason, the process must be repeated several times (epochs), until the validation accuracy reaches its maximum. In a perfect scenario, with a convex cost function L, this simple procedure converges to the optimal configuration. Unfortunately, a deep network is a very complex and non-convex function where plateaus and saddle points are quite common (see Chapter 1, Machine Learning Models Fundamentals). In such a scenario, a vanilla SGD wouldn't be able to find the global optimum and, in many cases, could not even find a close point. For example, in flat regions, the gradients can become so small (also considering the numerical imprecisions) as to slow down the training process until no change is possible (so θ(t+1) ≈ θ(t)). In the next section, we are going to present some common and powerful algorithms that have been developed to mitigate this problem and dramatically accelerate the convergence of deep models. Before moving on, it's important to mark two important elements. The first one concerns the learning rate, η. This hyperparameter plays a fundamental role in the learning process. As also shown in the figure, the algorithm proceeds jumping from a point to another one (which is not necessarily closer to the optimum). Together with the optimization algorithms, it's absolutely important to correctly tune up the learning rate. A high value (such as 1.0) could move the weights too rapidly increasing the instability. In particular, if a batch contains a few outliers (or simply non-dominant samples), a large η will consider them as representative elements, correcting the weights so to minimize the error. However, subsequent batches could better represent the data generating process, and, therefore, the algorithm must partially revert its modifications in order to compensate the wrong update. For this reason, the learning rate is usually quite small with common values bounded between 0.0001 and 0.01 (in some particular cases, η = 0.1 can be also a valid choice). On the other side, a very small learning rate leads to minimum corrections, slowing down the training process. A good trade-off, which is often the best practice, is to let the learning rate decay as a function of the epoch. In the beginning, η can be higher, because the probability to be close to the optimum is almost null; so, larger jumps can be easily adjusted. While the training process goes on, the weights are progressively moved towards their final configuration and, hence, the corrections become smaller and smaller. In this case, large jumps should be avoided, preferring a fine-tuning. That's why the learning rate is decayed. Common techniques include the exponential decay or a linear one. In both cases, the initial and final values must be chosen according to the specific problem (testing different configurations) and the optimization algorithm. In many cases, the ratio between the\xa0start and end value is about 10 or even larger. Another important hyperparameter is the batch size. There are no silver bullets that allow us to automatically make the right choice, but some considerations can be made. As SGD is an approximate algorithm, larger batches drive to corrections that are probably more similar to the ones obtained considering the whole dataset. However, when the number of samples is extremely high, we don't expect the deep model to map them with a one-to-one association, but instead our efforts are directed to improving the generalization ability. This feature can be re-expressed saying that the network must learn a smaller number of abstractions and reuse them in order to classify new samples. A batch, if sampled correctly, contains a part of these abstract elements and part of the corrections automatically improve the evaluation of a subsequent batch. You can imagine a waterfall process, where a new training step never starts from scratch. However, the algorithm is also called mini-batch gradient descent, because the usual batch size normally ranges from 16 to 512 (larger sizes are uncommon, but always possible), which are values smaller than the number of total samples (in particular in deep learning contexts). A reasonable default value could be 32 samples, but I always invite the reader to test larger values, comparing the performances in terms of training speed and final accuracy.", 'a00b4f4b-62c2-4837-9407-6abf682aa621.xhtml': "A very important element is the initial configuration of a neural network. How should the weights be initialized? Let's imagine we that have set them all to zero. As all neurons in a layer receive the same input, if the weights are 0 (or any other common, constant number), the output will be equal. When applying the gradient correction, all neurons will be treated in the same way; so, the network is equivalent to a sequence of single neuron layers. It's clear that the initial weights must be different to achieve a goal called symmetry breaking, but which is the best choice? If we knew (also approximately) the final configuration, we could set them to easily reach the optimal point in a few iterations, but, unfortunately, we have no idea where the minimum is located. Therefore, some empirical strategies have been developed and tested, with the goal of minimizing the training time (obtaining state-of-the-art accuracies). A general rule of thumb is that the weights should be small (compared to the input sample variance). Large values lead to large outputs that negatively impact on saturating functions (such as tanh and sigmoid), while small values can be more easily optimized because the corresponding gradients are larger and the corrections have a stronger effect. The same is true also for rectifier units because the maximum efficiency is achieved by working in a segment crossing the origin (where the non-linearity is actually located). For example, when coping with images, if the values are positive and large, a ReLU neuron becomes almost a linear unit, losing a lot of its advantages (that's why images are normalized, so as to bound each pixel value between 0 and 1 or -1 and 1). At the same time, ideally, the activation variances should remain almost constant throughout the network, as well as the weight variances after every back-propagation step. These two conditions are fundamental in order to improve the convergence process and to avoid the vanishing and exploding gradient problems (the latter, which is the opposite of vanishing gradients, will be discussed in the section dedicated to recurrent network architectures). A very common strategy considers the number of neurons in a layer and initializes the weights as follows: This method is called variance scaling and can be applied using the number of input units (Fan-In), the number of output units (Fan-Out), or their average. The idea is very intuitive: if the number of incoming or outgoing connections is large, the weights must be smaller, so as to avoid large outputs. In the degenerate case of a single neuron, the variance is set to 1.0, which is the maximum value allowed(in general, all methods keep the initial values for the biases equal to 0.0 because it's not necessary to initialize them with a random value). Other variations have been proposed, even if they all share the same basic ideas. LeCun proposed initializing the weights as follows: Another method called Xavier initialization (presented in Understanding the difficulty of training deep feedforward neural networks, Glorot X., Bengio Y., Proceedings of the 13th International Conference on Artificial Intelligence and Statistics), is similar to LeCun initialization, but it's based on the average between the number of units of two consecutive layers (to mark the sequentiality, we have substituted the terms Fan-In and Fan-Out with explicit indices): This is a more robust variant, as it considers both the incoming connections and also the outgoing ones (which are in turn incoming connections). The goal (widely discussed by the authors in the aforementioned papers) is trying to meet the two previously presented requirements. The first one is to avoid oscillations in the variance of the activations of each layer (ideally, this condition can avoid saturation). The second one is strictly related to the back-propagation algorithm, and it's based on the observation that, when employing a variance scaling (or an equivalent uniform distribution), the variance of a weight matrix is proportional to the reciprocal of 3nk. Therefore, the averages of Fan-In and Fan-Out are multiplied by three, trying to avoid large variations in the weights after the updates. Xavier initialization has been proven to be very effective in many deep architectures, and it's often the default choice. Other methods are based on a different way to measure the variance during both the feed-forward and back-propagation phases and trying to correct the values to minimize residual oscillations in specific contexts. For example, He, Zhang, Ren, and Sun (in Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, He K., Zhang X., Ren S., Sun J., arXiv:1502.01852 [cs.CV]) analyzed the initialization problem in the context of convolutional networks (we are going to discuss them in the next chapter) based on ReLU or variable Leaky-ReLU activations (also known as PReLU, parametric ReLU), deriving an optimal criterion (often called the He initializer), which is slightly different from the Xavier initializer: All these methods share some common principles and, in many cases, they are interchangeable. As already mentioned, Xavier is one of the most robust and, in the majority of real-life problems, there's no need to look for other methods; however, the reader should be always aware that the complexity of deep models must be often faced using empirical methods based on sometimes simplistic mathematical assumptions. Only the validation with real dataset can confirm if a hypothesis is correct or it's better to continue the investigation in another direction.", 'aa6fbde0-290f-48c8-a15f-1371bd9fb929.xhtml': "Keras (https://keras.io) is a powerful Python toolkit that allows modeling and training complex deep learning architectures with minimum effort. It relies on low-level frameworks, such as Tensorflow, Theano, or CNTK, and provides high-level blocks to build the single layers of a model. In this book, we need to be very pragmatic because there's no room for a complete explanation; however, all the examples will be structured to allow the reader to try different configurations and options without a full knowledge (for further details, I suggest the book Deep Learning with Keras, Gulli A, Pal S., Packt Publishing). In this example, we want to build a small MLP with a single hidden layer to solve the XOR problem (the dataset is the same created in the previous example). The simplest and most common way is to instantiate the class Sequential, which defines an empty container for an indefinite model. In this initial part, the fundamental method is add(), which allows adding a layer to the model. For our example, we want to employ four hidden layers with hyperbolic tangent activation and two softmax output layers. The following snippet defines the MLP: The Dense class defines a fully connected layer (a classical MLP layer), and the first parameter is used to declare the number of desired units. The first layer must declare the input_shape or input_dim, which specify the dimensions (or the shape) of a single sample (the batch size is omitted as it's dynamically set by the framework). All the subsequent layers compute the dimensions automatically. One of the strengths of Keras is the possibility to avoid setting many parameters (like weight initializers), as they will be automatically configured using the most appropriate default values (for example, the default weight initializer is Xavier). In the next examples, we are going to explicitly set some of them, but I suggest that the reader checks the official documentation to get acquainted with all the possibilities and features. The other layer involved in this experiment is Activation, which specifies the desired activation function (it's also possible to declare it using the parameter activation implemented by almost all layers, but I prefer to decouple the operations to emphasize the single roles, and also because some techniques—such as batch normalization—are normally applied to the linear output, before the activation). At this point, we must ask Keras to compile the model (using the preferred backend): The parameter optimizer defines the stochastic gradient descent algorithm that we want to employ. Using optimizer='sgd', it's possible to implement a standard version (as described in the previous paragraph). In this case, we are employing Adam (with the default parameters), which is a much more performant variant that will be discussed in the next section. The parameter loss is used to define the cost function (in this case, cross-entropy) and metrics is a list of all the evaluation score we want to be computed ('accuracy' is enough for many classification tasks). Once the model is compiled, it's possible to train it: The operations are quite simple. We have split the dataset into training and test/validation sets (in deep learning, cross-validation is seldom employed) and, then, we have trained the model setting\xa0batch_size=32 and epochs=100. The dataset is automatically shuffled at the beginning of each epoch, unless setting shuffle=False. In order to convert the discrete labels into one-hot encoding, we have used the utility function to_categorical. In this case, the label 0 becomes (1, 0) and the label 1 (0, 1). The model converges before reaching 100 epochs; therefore, I invite the reader to optimize the parameters as an exercise. However, at the end of the process, the training accuracy is about 0.999 and the validation accuracy is 1.0. The final classification plot is shown in the following diagram: Only three points have been misclassified, but it's clear that the MLP successfully separated the XOR dataset. To have a confirmation of the generalization ability, we've plotted the decision surfaces for a hyperbolic tangent hidden layer and ReLU one: In both cases, the MLPs delimited the areas in a reasonable way. However, while a tanh hidden layer seems to be overfitted (this is not true in our case, as the dataset represents exactly the data generating process), the ReLU layer generates less smooth boundaries with an apparent lower variance (in particular for considering the outliers of a class). We know that the final validation accuracies confirm an almost perfect fit, and the decision plots (which is easy to create with two dimensions) show in both cases acceptable boundaries, but this simple exercise is useful to understand the complexity and the sensitivity of a deep model. For this reason, it's absolutely necessary to select a valid training set (representing the ground-truth) and employ all possible techniques to avoid the overfitting (as we're going to discuss later). The easiest way to detect such a situation is checking the validation loss. A good model should reduce both training and validation loss after each epoch, reaching a plateau for the latter. If, after n epochs, the validation loss (and, consequently, the accuracy) begins to increase, while the training loss keeps decreasing, it means that the model is overfitting the training set. Another empirical indicator that the training process is evolving correctly is that, at least at the beginning, the validation accuracy should be higher than the training one. This can seem strange, but we need to consider that the validation set is slightly smaller and less complex than the training set; therefore, if the capacity of the model is not saturated with training samples, the probability of misclassification is higher for the training set than for the validation set. When this trend is inverted, the model is very likely to overfit after a few epochs. To verify these concepts, I invite the reader to repeat the exercise using a large number of hidden neurons (so as to increase dramatically the capacity), but they will be clearer when working with much more complex and unstructured datasets.", '34133325-0d1d-4ecc-9a50-76522e90efe3.xhtml': 'When discussing the back-propagation algorithm, we have shown how the SGD strategy can be easily employed to train deep networks with large datasets. This method is quite robust and effective; however, the function to optimize is generally non-convex and the number of parameters is extremely large. These conditions increase dramatically the probability to find saddle points (instead of local minima) and can slow down the training process when the surface is almost flat. A common result of applying a vanilla SGD algorithm to these systems is shown in the following diagram: Instead of reaching the optimal configuration, θopt, the algorithm reaches a sub-optimal parameter configuration, θsubopt, and loses the ability to perform further corrections. To mitigate all these problems and their consequences, many SGD optimization algorithms have been proposed, with the purpose of speeding up the convergence (also when the gradients become extremely small) and avoiding the instabilities of ill-conditioned systems.', '7092e3fa-df3c-4ed5-b6ea-d137b9711b5c.xhtml': "A common problem arises when the hypersurface is flat (plateaus) the gradients become close to zero. A very simple way to mitigate this problem is based on adding a small homoscedastic noise component to the gradients: The covariance matrix is normally diagonal with all elements set to Ïƒ2(t), and this value is decayed during the training process to avoid perturbations when the corrections are very small. This method is conceptually reasonable, but its implicit randomness can yield undesired effects when the noise component is dominant. As it's very difficult to tune up the variances in deep models, other (more deterministic) strategies have been proposed.", '9445d7b0-b308-4864-b329-dccdb97baa89.xhtml': "A more robust way to improve the performance of SGD when plateaus are encountered is based on the idea of momentum (analogously to physical momentum). More formally, a momentum is obtained employing the weighted moving average of subsequent gradient estimations instead of the punctual value: The new vector\xa0v(t), contains a component which is based on the past history (and weighted using the parameter\xa0μ\xa0which is a forgetting factor) and a term referred to the current gradient estimation (multiplied by the learning rate). With this approach, abrupt changes become more difficult, and when the exploration leaves a sloped region to enter a plateau, the momentum doesn't become immediately null (but for a time proportional to μ)\xa0a portion of the previous gradients will be kept, making it possible to traverse flat regions. The value assigned to the hyperparameter μ is normally bounded between 0 and 1. Intuitively, small values imply a short memory as the first term decays very quickly, while values close to 1.0 (for example, 0.9) allow a longer memory, less influenced by local oscillations. Like for many other hyperparameters, μ needs to be tuned according to the specific problem, considering that a high momentum is not always the best choice. High values could slow down the convergence when very small adjustments are needed, but, at the same time, values close to 0.0 are normally ineffective because the memory contribution decays too early. Using momentum, the update rule becomes as follows: A variant is provided by Nesterov momentum, which is based on the results obtained in the field of mathematical optimization by Nesterov that have been proven to speed up the convergence of many algorithms. The idea is to determine a temporary parameter update based on the current momentum and then apply the gradient to this vector to determine the next momentum (it can be interpreted as a look-ahead gradient evaluation aimed to mitigate the risk of a wrong correction considering the moving history of each parameter): This algorithm showed a performance improvement in several deep models; however, its usage is still limited because the next algorithms very soon outperformed the standard SGD with momentum, and they became the first choice in almost any real-life task.", 'dc30eb8e-23aa-4585-9d54-b50c785432a2.xhtml': "When using Keras, it's possible to customize the SGD optimizer by directly instantiating the SGD class and using it while compiling the model: The class SGD accepts the parameter lr (the learning rate η with a default set to 0.01), momentum (the parameter μ), nesterov (a boolean indicating whether employing the Nesterov momentum), and an optional decay parameter to indicate whether the learning rate must be decayed over the updates with the following formula:", 'b6008aeb-4a07-4d7a-a33e-90f1b1cfe9f6.xhtml': 'RMSProp was proposed by Hinton as an adaptive algorithm, partially based on the concept of momentum. Instead of considering the whole gradient vector, it tries to optimize each parameter separately to increase the corrections of slowly changing weights (that probably need more drastic modifications) and decreasing the update magnitudes of quickly changing ones (which are normally the more unstable). The algorithm computes the exponentially weighted moving average of the changing speed of every parameter considering the square of the gradient (which is insensitive to the sign): The weight update is then performed, as follows: The parameter Î´ is a small constant (such as 10-6) that is added to avoid numerical instabilities when the changing speed becomes null. The previous expression could be rewritten in a more compact way: Using this notation, it is clear that the role of RMSProp is adapting the learning rate for every parameter so it can increase it when necessary (almost frozen weights) and decrease it when the risk of oscillations is higher. In a practical implementation, the learning rate is always decayed over the epochs using an exponential or linear function.', '439549a6-b73f-43e9-838a-0699ed26ad73.xhtml': "The following snippet shows the usage of RMSProp with Keras: The learning rate and decay are the same as SGD. The parameter rho corresponds to the exponential moving average weight, Î¼, and epsilon is the constant added to the changing speed to improve the stability. As with any other algorithm, if the user wants to use the default values, it's possible to declare the optimizer without instantiating the class (for example, optimizer='rmsprop').", '9f32eee5-641d-4d56-897d-dd6a4086cb6d.xhtml': "Adam (the contraction of Adaptive Moment Estimation) is an algorithm proposed by Kingma and Ba (in Adam: A Method for Stochastic Optimization, Kingma D. P., Ba J., arXiv:1412.6980 [cs.LG]) to further improve the performance of RMSProp. The algorithm determines an adaptive learning rate by computing the exponentially weighted averages of both the gradient and its square for every parameter: In the aforementioned paper, the authors suggest to unbias the two estimations (which concern the first and second moment) by dividing them by 1 - μi, so the new moving averages become as follows: The weight update rule for Adam is as follows: Analyzing the previous expression, it is possible to understand why this algorithm is often called RMSProp with momentum. In fact, the term g(•) acts just like the standard momentum, computing the moving average of the gradient for each parameter (with all the advantages of this procedure), while the denominator acts as an adaptive term with the same exact semantics of RMSProp. For this reason, Adam is very often one\xa0of the most widely employed algorithms, even if, in many complex tasks, its performances are comparable to a standard RMSProp. The choice must be made considering the extra complexity due to the presence of two forgetting factors. In general, the default values (0.9) are acceptable, but sometimes it's better to perform an analysis of several scenarios before deciding on a specific configuration. Another important element to remember is that all momentum based methods can lead to instabilities (oscillations) when training some deep architectures. That's why RMSProp is very diffused in almost any research paper; however, don't consider this statement as a limitation, because Adam has shown outstanding performances in many tasks. It's helpful to remember that, whenever the training process seems unstable also with low learning rates, it's preferable to employ methods that are not based on momentum (the inertial term, in fact, can slow down the fast modifications necessary to avoid oscillations).", '275dc22a-8706-47d2-a23c-c459a57d868e.xhtml': 'The following snippet shows the usage of Adam with Keras: The forgetting factors, μ1 and μ2, are represented by the parameters beta_1 and beta_2. All the other elements are the same as the other algorithms.', '7f74a19e-684f-4cfa-8be1-10e1a77d6eb3.xhtml': "This algorithm has been proposed by Duchi, Hazan, and Singer (in Adaptive Subgradient Methods for Online Learning and Stochastic Optimizatioln, Duchi J., Hazan E., Singer Y., Journal of Machine Learning Research 12/2011). The idea is very similar to RMSProp, but, in this case, the whole history of the squared gradients is taken into account: The weights are updated exactly like in RMSProp: However, as the squared gradients are non-negative, the implicit sum v(t)(•) → ∞ when t → ∞. As the growth continues until the gradients are non-null, there's no way to keep the contribution stable while the training process proceeds. The effect is normally quite strong at the beginning, but vanishes after a limited number of epochs, yielding a null learning rate. AdaGrad keeps on being a powerful algorithm when the number of epochs is very limited, but it cannot be a first-choice solution for the majority of deep models (the next algorithm has been proposed to solve this problem).", '966350a8-68c8-4a64-a235-b2ea367a1151.xhtml': 'The following snippet shows the use of AdaGrad with Keras: The AdaGrad implementation has no other parameters but the common ones.', '2cb7a1ba-757b-4514-b8c7-f79b91a349ba.xhtml': "AdaDelta is an algorithm (proposed in ADADELTA: An Adaptive Learning Rate Method, Zeiler M. D., arXiv:1212.5701 [cs.LG]) in order to address the main issue of AdaGrad, which arises to managing the whole squared gradient history. First of all, instead of the accumulator, AdaDelta employs an exponentially weighted moving average, like RMSProp: However, the main difference with RMSProp is based on the analysis of the update rule. When we consider the operation x + Δx, we assume that both terms have the same unit; however, the author noticed that the adaptive learning rate η(θi) obtained with RMSProp (as well as AdaGrad) is unitless (instead of having the unit of θi). In fact, as the gradient is split into partial derivatives that can be approximated as ΔL/Δθi and the cost function L is assumed to be unitless, we obtain the following relations: Therefore, Zeiler proposed to apply a correction term proportional to the unit of each weight θi. This factor is obtained by considering the exponentially weighted moving average of every squared difference: The resulting updated rule hence becomes as follows: This approach is indeed more similar to RMSProp than AdaGrad, but the boundaries between the two algorithms are very thin, in particular when the history is limited to a finite sliding window. AdaDelta is a powerful algorithm, but it can outperform Adam or RMSProp only in very particular tasks. My suggestion is to employ a method and, before moving to another one, try to optimize the hyperparameters until the accuracy reaches its maximum. If the performances keep on being bad and the model cannot be improved in any other way, it's a good idea to test other optimization algorithms.", '23060c48-d7d5-4725-8576-c82183cfc65f.xhtml': 'The following snippet shows the usage of AdaDelta with Keras: The forgetting factor, Î¼, is represented by the parameter rho.', 'e428f609-4d3e-465a-96d6-55c80a0dd084.xhtml': "Overfitting is a common issue in deep models. Their extremely high capacity can often become problematic even with very large datasets because the ability to learn the structure of the training set is not always related to the ability to generalize. A deep neural network can easily become an associative memory, but the final internal configuration couldn't be the most suitable to manage samples belonging to the same distribution but was never presented during the training process. It goes without saying that this behavior is proportional to the complexity of the separation hypersurface. A linear classifier has a minimum chance to overfit, and a polynomial classifier is incredibly more prone to do it. A combination of hundreds, thousands, or more non-linear functions yields a separation hypersurface, which is beyond any possible analysis. In 1991, Hornik (in Approximation Capabilities of Multilayer Feedforward Networks,Hornik K., Neural Networks, 4/2) generalized a very important result obtained two years before by the mathematician Cybenko (and published in Approximations by Superpositions of Sigmoidal Functions, Cybenko G., Mathematics of Control, Signals, and Systems, 2 /4). Without any mathematical detail (which is, however, not very complex), the theorem states that an MLP (not the most complex architecture!) can approximate any function that is continuous in a compact subset of ℜn. It's clear that such a result formalized what almost any researcher already intuitively knew, but its power goes beyond the first impact, because the MLP is a finite system (not a mathematical series) and the theorem assumes a finite number of layers and neurons. Obviously, the precision is proportional to the complexity; however, there are no unacceptable limitations for almost any problem. However, our goal is not learning an existing continuous function, but managing samples drawn from an unknown data generating process with the purpose to maximize the accuracy when a new sample is presented. There are no guarantees that the function is continuous or that the domain is a compact subset. In Chapter 1, Machine Learning Models Fundamentals, we have presented the main regularization techniques based on a slightly modified cost function: The additional term g(θ) is a non-negative function of the weights (such as L2 norm) that forces the optimization process to keep the parameters as small as possible. When working with saturating functions (such as tanh), regularization methods based on the L2 norm try to limit the operating range of the function to the linear part, reducing de facto its capacity. Of course, the final configuration won't be the optimal one (that could be the result of an overfitted model) but the suboptimal trade-off between training and validation accuracy (alternatively, we can say between bias and variance). A system with a bias close to 0 (and a training accuracy close to 1.0) could be extremely rigid in the classification, succeeding only when the samples are very similar to ones evaluated during the training process. That's why this price is often paid considering the advantages obtained when working with new samples. L2 regularization can be employed with any kind of activation function, but the effect could be different. For example, ReLU units have an increased probability to become linear (or constantly null) when the weights are very large. Trying to keep them close to 0.0 means forcing the function to exploit its non-linearity without the risk of extremely large outputs (that can negatively affect very deep architectures). This result can sometimes be more useful, because it allows training bigger models in a smoother way, obtaining better final performances. In general, it's almost impossible to decide whether a regularization can improve the result without several tests, but there are some scenarios where it's very common to introduce a dropout (we discuss this approach in the next paragraph) and tune up its hyperparameter. This is more an empirical choice than a precise architectural decision because many real-life examples (including state-of-the-art models) obtained outstanding results employing this regularization technique. I suggest the reader prefer a rational skepticism to blind trust and double-checking its models before picking a specific solution. Sometimes, an extremely high-performing network turns to being ineffective when a different (but analogous) dataset is chosen. That's why testing different alternatives can provide the best experience in order to solve specific problem classes. Before moving on, I want to show how it's possible to implement an L1 (useful to enforce sparsity), L2, or ElasticNet (the combination of L1 and L2) regularization using Keras. The framework provides a fine-grained approach that allows imposing a different constraint to each layer. For example, the following snippet shows how to add a\xa0l2 constraint with the strength parameter set to 0.05 to a generic fully connected layer: The keras.regularizers package contains the functions l1(), l2(), and l1_l2(), which can be applied to Dense and convolutional layers (we're going to discuss them in the next chapter). These layers allow us\xa0to impose a regularization on the weights (kernel_regularizer), on the bias (bias_regularizer), and on the activation output (activation_regularizer), even if the first one is normally the most widely employed. Alternatively, it's possible to impose specific constraints on the weights and biases that in a more selective way. The following snippet shows how to set a maximum norm (equal to 1.5) on the weights of a layer: Keras, in the keras.constraints package, provides some functions that can be used to impose a maximum norm on the weights or biases maxnorm(), a unit norm along an axis unit_norm(), non-negativity non_neg(), and upper and lower bounds for the norm min_max_norm(). The difference between this approach and regularization is that it is applied only if necessary. Considering the previous example, imposing an L2 regularization always has an effect, while a constraint on the maximum norm is inactive until the value is lower than the predefined threshold.", 'cc75db40-e556-48e5-8e10-11625d050ed9.xhtml': "This method has been proposed by Hinton and co. (in Improving neural networks by preventing co-adaptation of feature detectors, Hinton G. E., Srivastava N., Krizhevsky A., Sutskever I., Salakhutdinov R. R., arXiv:1207.0580 [cs.NE]) as an alternative to prevent overfitting and allow bigger networks to explore more regions of the sample space. The idea is rather simpleâ€”during every training step, given a predefined percentage nd, a dropout layer randomly selects ndN incoming units and sets them to 0.0 (the operation is only active during the training phase, while it's completely removed when the model is employed for new predictions). This operation can be interpreted in many ways. When more dropout layers are employed, the result of their selection is a sub-network with a reduced capacity that can, with more difficultly, overfit the training set. The overlap of many trained sub-networks makes up an implicit ensemble whose prediction is an average over all models. If the dropout is applied on input layers, it works like a weak data augmentation, by adding a random noise to the samples (setting a few units to zero can lead to potential corrupted patterns). At the same time, employing several dropout layers allows exploring several potential configurations that are continuously combined and refined. This strategy is clearly probabilistic, and the result can be affected by many factors that are impossible to anticipate; however, several tests confirmed that the employment of a dropout is a good choice when the networks are very deep because the resulting sub-networks have a residual capacity that allows them to model a wide portion of the samples, without driving the whole network to freeze its configuration overfitting the training set. On the other hand, this method is not very effective when the networks are shallow or contain a small number of neurons (in these cases, L2 regularization is probably a better choice). According to the authors, dropout layers should be used in conjunction with high learning rates and maximum norm constraints on the weights. In this way, in fact, the model can easily learn more potential configurations that would be avoided when the learning rate is kept very small. However, this is not an absolute rule because many state-of-the-art models use a dropout together with optimization algorithms, such as RMSProp or Adam, and not excessively high learning rates. The main drawback of a dropout is that it slows down the training process and can lead to an unacceptable sub-optimality. The latter problem can be mitigated by adjusting the percentages of dropped units, but, in general, it's very difficult to solve it completely. For this reason, some new image-recognition models (like residual networks) avoid the dropout and employ more sophisticated techniques to train very deep convolutional networks that overfit both training and validation sets.", 'c50fa25d-1a08-4d9e-adfe-1b5df536712d.xhtml': "We cannot test the effectiveness of the dropout with a more challenging classification problem. The dataset is the classical MNIST handwritten digits, but Keras allows downloading and working with the original version that is made up of 70\xa0thousand (60\xa0thousand training and 10\xa0thousand test) 28 × 28 grayscale images. Even if this is not the best strategy, because a convolutional network should be the first choice to manage images, we want to try to classify the digits considering them as flattened 784-dimensional arrays. The first step is loading and normalizing the dataset so that each value becomes a float bounded between 0 and 1: At this point, we can start testing a model without dropout. The structure, which is common to all experiments, is based on three fully connected ReLU layers (2048-1024-1024) followed by a softmax layer with 10 units. Considering the problem, we can try to train the model using an Adam optimizer with η = 0.0001 and a decay set to 10-6: The model is trained for 200 epochs with a batch size of 256 samples: Even without a further analysis, we can immediately notice that the model is overfitted. After 200 epochs, the training accuracy is 1.0 with a loss close to 0.0, while the validation accuracy is reasonably high, but with a validation loss slightly lower than the one obtained at the end of the second epoch. To better understand what happened, it's useful to plot both accuracy and loss during the training process: As it's possible to see, the validation loss reached a minimum during the first 10 epochs and immediately restarted to grow (this is sometimes called a U-curve because of its shape). At the same moment, the training accuracy reached 1.0. From that epoch on, the model started overfitting, learning a perfect structure of the training set, but losing the generalization ability. In fact, even if the final validation accuracy is rather high, the loss function indicates a lack of robustness when new samples are presented. As the loss is a categorical cross-entropy, the result can be interpreted as saying that the model has learned a distribution that partially mismatches the validation set one. As our goal is to use the model to predict new samples, this configuration could not be acceptable. Therefore, we try again, using some dropout layers. As suggested by the authors, we also increment the learning rate to 0.1 (switching to a Momentum SGD optimizer in order to avoid explosions due to adaptivity of RMSProp or Adam), initialize the weight with a uniform distribution (-0.05, 0.05), and impose a maximum norm constraint set to 2.0. This choice allows the exploration of more sub-configurations without the risk of excessively high weights. The dropout is applied to the 25% of input units and to all ReLU fully connected layers with a percentage set to 50%: The training process is performed with the same parameters: The final condition is dramatically changed. The model is no longer overfitted (even if it's possible to improve it in order to increase the validation accuracy) and the validation loss is lower than the initial one. To have a confirmation, let's analyze the accuracy/loss plots: The result shows some imperfections because the validation loss is almost flat for many epochs; however, the same model, with a higher learning rate and a weaker algorithm achieved a better final performance (0.988 validation accuracy) and a superior generalization ability. State-of-the-art models can\xa0also reach a validation accuracy equal to 0.995, but our goal was to show the effect of dropout layers in preventing the overfitting and, moreover, yielding a final configuration that is much more robust to new samples or noisy ones. I invite the reader to repeat the experiment with different parameters, bigger or smaller networks, and other optimization algorithms, trying to further reduce the final validation loss. Keras also implements two additional dropout layers: GaussianDropout, which multiplies the input samples by a Gaussian noise: The value for the constant ρ can be set through the parameter rate (bounded between 0 and 1). When ρ → 1, σ2 → ∞, while small values yield a null effect as n ≈ 1. This layer can be very useful as input one, in order to simulate a random data augmentation process. The other class is AlphaDropout, which works like the previous one, but renormalizing the output to keep the original mean and variance (this effect is very similar to the one obtained employing the technique described in the next paragraph together with noisy layers).", '75383fa9-62f2-459b-9793-a5ce00934839.xhtml': "Let's consider a mini-batch of k samples: Before traversing the network, we can measure a mean and a variance: After the first layer (for simplicity, let's suppose that the activation function, f(•), is the always the same), the batch is transformed into the following: In general, there's no guarantee that the new mean and variance are the same. On the contrary, it's easy to observe a modification that increases throughout the network. This phenomenon is called covariate shift, and it's responsible for a progressive training speed decay due to the different adaptations needed in each layer. Ioffe and Szegedy (in Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Ioffe S., Szegedy C., arXiv:1502.03167 [cs.LG]) proposed a method to mitigate this problem, which has been called batch normalization (BN). The idea is to renormalize the linear output of a layer (before or after applying the activation function), so that the batch has null mean and unit variance. Therefore, the first task of a BN layer is to compute: Then each sample is transformed into a normalized version (the parameter δ is included to improve the numerical stability): However, as the batch normalization has no computational purposes other than speeding up the training process, the transformation must always be an identity (in order to avoid to distort and bias the data); therefore, the actual output will be obtained by applying the linear operation: The two parameters α(j) and β(j) are variables optimized by the SGD algorithm; therefore, each transformation is guaranteed not to alter the scale and the position of data. These layers are active only during the training phase (like dropout), but, contrary to other algorithms, they cannot be simply discarded when the model is used to make predictions on new samples because the output would be constantly biased. To avoid this problem, the authors suggest approximating both mean and variance of X by averaging over the batches (assuming that there are Nb batches with k samples): Using these values, the batch normalization layers can be transformed into the following linear operations: It's not difficult to prove that this approximation becomes more and more accurate when the number of batches increases and that the error is normally negligible. However, when the batch size is very small, the statistics can be quite inaccurate; therefore, this method should be used considering the representativeness of a batch. If the data generating process is simple, even a small batch can be enough to describe the actual distribution. When, instead, pdata is more complex, batch normalization requires larger batches to avoid wrong adjustments (a feasible strategy is to compare global mean and variance with the ones computed sampling some batches and trying to set the batch size that minimizes the discrepancy). However, this simple process can dramatically reduce the covariate shift and improve the convergence speed of very deep networks (including the famous residual networks). Moreover, it allows employing higher learning rates as the layers are implicitly saturated and can never explode. Additionally, it has been proven that batch normalization has also a secondary regularization effect even if it doesn't work on the weights. The reason is not very different from the one proposed for L2, but, in this case, there's a residual effect due to the transformation itself (partially caused by the variability of the parameters α(j) and β(j)) that can encourage the exploration of different regions of the sample space. However, this is not the primary effect, and it's not a good practice employing this method as a regularizer.", '57f05162-0ec0-48f8-bc9f-7f9bddd98193.xhtml': "In order to show the feature of this technique, let's repeat the previous example using an MLP without dropout but applying a batch normalization after each fully connected layer before the ReLU activation. The example is very similar to the first one, but, in this case, we increase the Adam learning rate to 0.001 keeping the same decay: We can now train using the same parameters again: The model is again overfitted, but now the final validation accuracy is only slightly higher than the one achieved using the dropout layers. Let's plot accuracy and loss to better analyze the training process: The effect of the batch normalization improved the performances and slowed down the overfitting. At the same time, the elimination of the covariate shift avoided the U-curve keeping a quite low validation loss. Moreover, the model reached a validation accuracy of about 0.99 during the epochs 135-140 with a residual positive trend. Analogously to the previous example, this solution is imperfect, but it's a good starting point for further optimization. It would be a good idea to continue the training process for a larger number of epochs, monitoring both the validation loss and accuracy. Moreover, it's possible to mix dropout and batch normalization or experiment with the Keras AlphaDropout layer. However, if, in the first example (without dropout), the climax of training accuracy was associated with a starting positive trend for the validation loss, in this case, the learned distribution doesn't seem to be very different from the validation set one. In other words, batch normalization is not preventing overfitting the training set, but it's avoiding a decay in the generalization ability (observed when there was no batch normalization). I suggest repeating the test with other hyperparameter and architectural configurations in order to decide whether this model can be used for prediction purposes or it's better to look for other solutions.", 'a4c60568-eb33-4fa7-bb9b-d66b9600b9e5.xhtml': "In this chapter, we started the exploration of the deep learning world by introducing the basic concepts that led the first researchers to improve the algorithms until they achieved the top results we have nowadays. The first part explained the structure of a basic artificial neuron, which combines a linear operation followed by an optional non-linear scalar function. A single layer of linear neurons was initially proposed as the first neural network, with the name of the perceptron. Even though it was quite powerful for many problems, this model soon showed its limitations when working with non-linear separable datasets. A perceptron is not very different from a logistic regression, and there's no concrete reason to employ it. Nevertheless, this model opened the doors to a family of extremely powerful models obtained combining multiple non-linear layers. The multilayer perceptron, which has been proven to be a universal approximator, is able to manage almost any kind of dataset, achieving high-level performances when other methods fail. In the next section, we analyzed the building bricks of an MLP. We started with the activation functions, describing their structure and features, and focusing on the reasons they lead the choice for specific problems. Then, we discussed the training process, considering the basic idea behind the back-propagation algorithm and how it can be implemented using the stochastic gradient descent method. Even if this approach is quite effective, it can be slow when the complexity of the network is very high. For this reason, many optimization algorithms were proposed. In this chapter, we analyzed the role of momentum and how it's possible to manage adaptive corrections using RMSProp. Then, we combined both, momentum and RMSProp to derive a very powerful algorithm called Adam. In order to provide a complete vision, we also presented two slightly different adaptive algorithms, called AdaGrad and AdaDelta. In the next sections, we discussed the regularization methods and how they can be plugged into a Keras model. An important section was dedicated to a very diffused technique called dropout, which consists in setting to zero (dropping) a fixed percentage of samples through a random selection. This method, although very simple, prevents the overfitting of very deep networks and encourages the exploration of different regions of the sample space,Â\xa0 obtaining a result not very dissimilar to the ones analyzed in Chapter 8, Ensemble Learning. The last topic was the batch normalization technique, which is a method to reduce the mean and variance shift (called covariate shift) caused by subsequent neural transformations. This phenomenon can slow down the training process as each layer requires different adaptations and it's more difficult to move all the weights in the best direction. Applying batch normalization means very deep networks can be trained in a shorter time, thanks also to the possibility of employing higher learning rates. In the next chapter, we are going to continue this exploration, analyzing very important advanced layers like convolutions (that achieve extraordinary performances in image-oriented tasks) and recurrent units (for the processing of time series) and discussing some practical applications that can be experimented on and readapted using Keras and Tensorflow.", 'c6f60ddd-a19c-4b9b-b434-01cb5f37d0d7.xhtml': 'In this chapter, we continue our pragmatic exploration of the world of deep learning, analyzing two very important elements: deep convolutional networks and recurrent neural networks (RNN). The former represents the most accurate and best performing visual processing technique for almost any purpose. Results like the ones obtained in fields such as real-time image recognition, self-driving cars, and Deep Reinforcement Learning have been possible thanks to the expressivity of this kind of network. On the other hand, in orderÂ\xa0to fully manage the temporal dimension, it is necessary to introduce advanced recurrent layers, whose performance must be greater than any other regression method. Employing these two techniques together with all the elements already discussed in the previous chapter makes it possible to achieve extraordinary results in the field of video processing, decoding, segmentation, and generation. In particular, in this chapter, we are going to discuss the following topics:', 'ac736c5b-74f2-49bd-b0f4-a4e019144b2d.xhtml': "In the previous chapter, Chapter 9,\xa0Neural Networks for Machine Learning we have seen how a multi-layer perceptron can achieve a very high accuracy when working with an complex image dataset that is not very complex, such as the MNIST handwritten digits one. However, as the fully-connected layers are horizontal, the\xa0images, which in general are three-dimensional structures (width\xa0× height\xa0× channels), must be flattened and transformed into one-dimensional arrays where the geometric properties\xa0are definitively lost. With more complex datasets, where the distinction between classes depends on more details and on their relationships, this approach can yield moderate accuracies, but it can never reach the precision required by production-ready applications. The conjunction of neuroscientific studies and image processing techniques suggested experimenting with neural networks where the first layers work with bidimensional structures (without the channels), trying to extract a hierarchy of features that are strictly dependent on the geometric properties of the image. In fact, as confirmed by neuroscientific research about the visual cortex, a human being doesn't decode an image directly. The process is sequential and starts\xa0by detecting low-level elements such as lines are orientations; progressively, it proceeds by focusing on sub-properties that define more and more complex shapes, different colors, structural features, and so on, until the amount of information is enough to resolve any possible ambiguity (for further scientific details, I recommend the book Vision and Brain: How We Perceive the World, Stone J. V., MIT Press). For example, we can image the decoding process of an eye as a sequence made up of these filters (of course, this is only a didactic\xa0example): directions (dominant horizontal dimension), a central circle inside an ellipsoidal\xa0shape, a darker center (pupil) and a clear background (bulb), a smaller darker circle in the middle of the pupil, the presence of eyebrows, and so on. Even if the process is not biologically correct, it can be considered as a reasonable hierarchical process where a higher level sub-feature is obtained after a lower-level filtering. This approach has been synthesized using the bidimensional convolutional operator, which was already known as a powerful image processing tool. However, in this case, there's a very important difference: the structure of the filters is not pre-imposed but learned by the network using the same back-propagation algorithm employed for MLPs. In this way, the model can adapt the weights considering a final goal (which is the classification output), without taking into account any pre-processing steps. In fact, a deep convolutional network, more than an MLP, is based on the concept of end-to-end learning, which is a different way to express what we have described before. The input is the source; in the middle, there's a flexible structure; and, at the end, we define a global cost function, measuring the accuracy of the classification. The learning process has to back-propagate the errors and correct the weights to reach a specific goal, but we don't know exactly how this process works. What we can easily do is analyze the structure of the filters at the end of the learning phase, discovering that the network has specialized the first layers on low-level details (such as orientations) and the last ones on high-level, sometimes recognizable, ones (such as the components of a face). It's not surprising that such models achieved state-of-the-art performance in tasks such as image recognition, segmentation (detecting the boundaries of different parts composing an image), and tracking (detecting the position of moving objects). Nevertheless, deep convolutional networks have become the first block of many different architectures (such as deep reinforcement learning or neural style transfer) and, even with a few known limitations, continue to be the first choice for solving several complex real-life problems. The main drawback of such models (which is also a common objection) is that they require very large datasets to reach high accuracies. All the most important models are trained with millions of images and their generalization ability (that is, the main goal) is proportional to the number of different samples. There were researchers who noticed that a human being learns to generalize without this huge amount of experience and, in the coming decades, we are likely to observe improvements under this viewpoint. However, deep convolutional networks have revolutionized many Artificial Intelligence fields, allowing results that were considered almost impossible just a few years ago. In this section, we are going to discuss different kinds of convolutions and how they can be implemented using Keras; therefore, for specific technical details I continue suggesting to check the official documentation and the book\xa0Deep Learning with Keras, Gulli A, Pal S.,\xa0Packt.", '2154074f-f88c-4770-b79f-56c97a024c4f.xhtml': "Even if we work only with finite and discrete convolutions, it's useful to start providing the standard definition based on integrable functions. For simplicity, let's suppose that f(τ) and k(τ) are two real functions of a single variable defined in\xa0ℜ. The convolution of f(τ) and k(τ) (conventionally denoted as f\xa0∗ k), which we are going to call kernel, is defined as follows: The expression may not be very easy to understand without a mathematical background, but it can become exceptionally simple with a few considerations. First of all, the integral sums over all values of\xa0τ; therefore, the convolution is a function of the remaining variable, t. The second fundamental element is a sort of dynamic property: the kernel is reversed (-τ) and transformed into a function of a new variable z = t -\xa0τ. Without deep mathematical knowledge, it's possible to understand that this operation shifts the function along the τ (independent variable) axis. In the following graphs, there's an example based on a parabola: The first diagram is the original kernel (which is also symmetric). The other two plots show, respectively, a forward and a backward shift. It should be clearer now that a convolution multiplies the function\xa0f(τ) times the shifted kernel and computes the area under the resulting curve. As the variable t is not integrated, the area is a function of t and defines a new function, which is the convolution itself. In other words, the value of convolution of f(τ) and k(τ) computed for t = 5 is the area under the curve obtained by the multiplication f(τ)k(5 - τ). By definition, a convolution is commutative (f\xa0∗ k = k ∗ f) and distributive (f\xa0∗ (k + g) = (f\xa0∗ k) + (f\xa0∗ g)). Moreover, it's also possible to prove that it's associative (f\xa0∗ (k ∗ g) = (f\xa0∗ k) ∗ g). However, in deep learning, we never work with continuous convolutions; therefore, I omit all the properties and mathematical details, focusing the attention on the discrete case. The reader who is interested in the theory can find further details in Circuits, Signals, and Systems, Siebert W. M., MIT Press.\xa0A common practice is, instead, to stack multiple convolutions with different kernels (often called filters), to transform an input containing n channels into an output with m channels, where m corresponds to the number of kernels. This approach allows the unleashing of the full power of convolutions, thanks to the synergic actions of different outputs. Conventionally, the output of a convolution layer with n filters is called a\xa0feature map\xa0(w(t) × h(t)\xa0×\xa0n), because its structure is no longer related to a specific image but resembles the overlap of different feature detectors. In this chapter, we often talk about images (considering a hypothetical first layer), but all the considerations are implicitly extended to any feature map.\xa0", '3b8729aa-b09d-4373-bf2a-c990c576c936.xhtml': "The most common type of convolution employed in deep learning is based on bidimensional arrays with any number of channels (such as grayscale or RGB images). For simplicity, let's analyze a single layer (channel) convolution because the extension to n layers is straightforward. If X\xa0∈\xa0ℜw\xa0× h and k\xa0∈\xa0ℜn × m, the convolution X\xa0∗ k is defined as (the indexes start from 0):  It's clear that the previous expression is a natural derivation of the continuous definition. In the following graph, there's an example with a 3\xa0× 3 kernel: The kernel is shifted horizontally and vertically, yielding the sum of the element-wise multiplication of corresponding elements. Therefore, every operation leads to the output of a single pixel. The kernel employed in the example is called the\xa0discrete Laplacian operator (because it's obtained by discretizing the real Laplacian); let's observe the effect of this kernel on a complete greyscale diagram: As it's possible to notice, the effect of the convolution is to emphasize the borders of the various shapes. The reader can now understand how variable kernels can be tuned up in order to fulfill precise requirements. However, instead of trying to do it manually, a deep convolutional network leaves this tasks to the learning process, which is subject to a precise goal expressed as the minimization of a cost function. A parallel application of different filters yields complex overlaps that can simplify the extraction of those features that are really important for a classification. The main difference between a fully-connected layer and a convolutional one is the ability of the latter to work with an existing geometry, which encodes all the elements needed to distinguish an object from another one. These elements cannot be immediately generalizable (think about the branches of a decision tree, where a split defines a precise path towards a final class), but require subsequent processing steps to perform a necessary disambiguation. Considering the previous photo, for example, eyes and nose are rather similar. How is it possible to segment the picture correctly? The answer is provided by a double analysis: there are subtle differences that can be discovered by fine-grained filters and, above all, the global geometry of real objects is based on internal relationships that are almost invariant. For example (only for didactic purposes), eyes and nose should make up an isosceles triangle, because the symmetry of a face implies the same distance between each eye and the nose. This consideration can be made apriori, like in many visual processing techniques, or, thanks to the power of deep learning, it can be left to the training process. As the cost function and the output classes implicitly control the differences, a deep convolutional network can learn what is important to reach a specific goal, discarding at the same time all those details that are useless. In the previous section, we have said that the feature extraction process is mainly hierarchical. Now, it should be clear that different kernel sizes and subsequent convolutions achieve exactly this objective. Let's suppose that we have a 100\xa0× 100 image and a (3\xa0× 3) kernel. The resulting image will be 98\xa0× 98 pixels (we will explain this concept later). However, each pixel encodes the information of\xa0a\xa03\xa0× 3 block and, as these blocks are overlapping, two consecutive pixels will share some knowledge but, at the same time, they emphasize the difference between the corresponding blocks. In the following diagram, the same Laplacian Kernel is applied to a simple white square on a black background: Even if the image is very simple, it's possible to notice that the result of a convolution enriched the output image with some very important pieces of information: the borders of the square are now clearly visible (they are black and white) and they can be immediately detected by thresholding the image. The reason is straightforward: the effect of the kernel on the compact surfaces is compact too but, when the kernel is shifted upon the border, the effect of the difference becomes visible. Three adjacent pixels in the original image can be represented as (0, 1, 1), indicating the horizontal transition between black and white. After the convolution, the result is approximately (0.75, 0.0, 0.25). All the original black pixels have been transformed into a light gray, the white square became darker, and the border (which is not marked in the original picture) is now black (or white, depending on the shift direction). Reapplying the same filter to the output of the previous convolution, we obtain the following: A sharp eye can immediately notice three results: the compact surfaces (black and white) are becoming more and more similar, the borders are still visible, and, above all, the top and lower left corners are now more clearly marked with white pixels. Therefore, the result of the second convolution added a finer-grained piece of information, which was much more difficult to detect in the original image. Indeed, the effect of the Laplacian operator is very straightforward and it's useful only for didactic purposes. In real deep convolutional networks, the filters are trained to perform more complex processing operations that can reveal details (together with their internal and external relationships) that are not immediately exploited to classify the image. Their isolation (obtained thanks to the effect of many parallel filters) allows the network to mark similar elements (like the corners of the square) in a different way and make more accurate decisions. The purpose of this example is to show how a sequence of convolutions allows the generation of a hierarchical process that will extract coarse-grained features at the beginning and very high-level ones at the end, without losing the information already collected. Metaphorically, we could say that a deep convolutional network starts placing labels indicating lines, orientations, and borders and proceeds by enriching the existing ontology with further details (such as corners, particular shapes, and so on). Thanks to this ability, such models can easily outperform any MLP and reach almost to the Bayes level if the number of training samples is large enough. The main drawback of this models is their inability to easily recognize objects after the application of affine transformations (such as rotations or translations). In other words, if a network is trained with a dataset containing only faces in their natural position, it will achieve poor performance when a rotated (or upside-down) sample is presented. In the next sections, we are going to discuss a couple of methods that are helpful for mitigating this problem (in the case of translations); however, a new experimental architecture called a\xa0capsule network (which is beyond the scope of this book) has been proposed in order to solve this problem with a slightly different and much more robust approach (the reader can find further details in Dynamic Routing Between Capsules,\xa0Sabour S., Frosst N., Hinton G. E., arXiv:1710.09829 [cs.CV]).", '42dacec0-b8c2-4e68-a888-453bf6373ad9.xhtml': "Two important parameters common to all convolutions are padding and strides. Let's consider the bidimensional case, but keep in mind that the concepts are always the same. When a kernel (n × m with n, m > 1) is shifted upon an image and it arrives at the end of a dimension, there are two possibilities. The first one, called valid padding, consists of not continuing even if the resulting image is smaller than the original. In particular, if X is a w\xa0× h matrix, the resulting convolution output will have dimensions equal to (w - n + 1)\xa0× (h - m + 1). However, there are many cases when it's useful to keep the original dimensions, for example, to be able to sum different outputs. This approach is called same padding\xa0and it's based on the simple idea to add n - 1 blank columns and m - 1 blank rows to allow the kernel to shift over the original image, yielding a number of pixels equal to the initial dimensions. In many implementations, the default value is set to valid padding. The other parameter, called strides, defines the number of pixels to skip during each shift. For example, a value set to (1, 1) corresponds to a standard convolution, while strides set to (2, 1) are shown in the following diagram: In this case, every horizontal shift skips a pixel. Larger strides force a dimensionality reduction when a high granularity is not necessary (for example, in the first layers), while strides set to (1, 1) are normally employed in the last layers to capture smaller details. There are no standard rules to find out the optimal value and testing different configurations is always the best approach. Like any other hyperparameter, too many elements should be taken into account when determining whether a choice is acceptable or not; however, some general pieces of information about the dataset (and therefore about the underlying data generating process) can help in making a reasonable initial decision. For example, if we are working with pictures of buildings whose dimension is vertical, it's possible to start picking a value of (1, 2), because we can assume that there's more informative redundancy in the y-axis than in the x-axis. This choice can dramatically speed up the training process, as the output has one dimension, which is half (with the same padding) of the original one. In this way, larger strides produce a partial denoising and can improve the training speed. At the same time, the information loss could have a negative impact on the accuracy. If that happens, it probably means that the scale isn't high enough to allow skipping some elements without compromising the semantics. For example, an image with very small faces could be irreversibly damaged with large strides, yielding an inability to detect the right feature and a consequent worsening of the classification accuracy.", '17aeafe3-aa6e-4e99-9bdb-4c989cdfcacf.xhtml': "In some cases, a stride larger than one could be a good solution because it reduces the dimensionality and speeds up the training process, but it can lead to distorted images where the main features are not detectable anymore. An alternative approach is provided by the atrous convolution (also known as dilated convolution). In this case, the kernel is applied to a larger image patch, but skips some pixels inside the area itself (that's why someone called it convolution with holes). In the following graph, there's an example with (3\xa0× 3) and dilation rate set to 2: Every patch is now 9\xa0× 9, but the kernel remains a\xa03\xa0× 3 Laplacian operator. The effect of this approach is more robust than increasing the strides because the kernel perimeter will always contain a group of pixels with the same geometrical relationships. Of course, fine-grained features could be distorted, but as the strides are normally set to (1, 1), the final result is normally more coherent. The main difference with a standard convolution is that in this case, we are assuming that farther elements can be taken into account to determine the nature of an output pixel. For example, if the main features don't contain very small details, an atrous convolution can consider larger areas, focusing directly on elements that a standard convolution can detect only after several operations. The choice of this technique must be made considering the final accuracy, but just like for the strides, it can be considered from the beginning whenever the geometric properties can be detected more efficiently, considering larger patches with a few representative elements. Even if this method can be very effective in particular contexts, it isn't normally the first choice for very deep models. In the most important image classification models, standard convolutions (with or without larger strides) are employed because they have been proven to yield the best performance with very generic datasets (such as ImageNet or Microsoft Coco). However, I suggest the reader experiment with this method and compare the results. In particular, it would be a good idea to analyze which classes are better classified and try to find a rational explanation for the observed behavior.", 'a5eb8168-de2f-4a08-b07e-c60a20d00c48.xhtml': "If we consider an image\xa0X\xa0∈\xa0ℜw\xa0× h (single channel)\xa0and a kernel k\xa0∈\xa0ℜn × m, the number of operations is nmwh. When the kernel is not very small and the image is large, the cost of this computation can be quite high, even with GPU support. An improvement can be achieved by taking into account the associated property of convolutions. In particular, if the original kernel can be split into the dot product of two vectorial kernels,\xa0k(1) with dimensions\xa0(n\xa0× 1) and k(2) with dimensions\xa0(1\xa0× m), the convolution is said to be separable. This means that we can perform a (n\xa0× m) convolution with two subsequent operations:  The advantage is clear, because now the number of operations is (n + m)wh. In particular, when nm >> n + m, it's possible to avoid a large number of multiplications and speed up both the training and the prediction process. A slightly different approach has been proposed in Xception: Deep Learning with Depthwise Separable Convolutions,\xa0Chollet F.,\xa0arXiv:1610.02357 [cs.CV]. In this case, which is properly called depthwise separable convolution, the process is split into two steps. The first one operates along the channel axis, transforming it into a single dimensional map with a variable number of channels (for example, if the original diagram is 768\xa0× 1024\xa0× 3, the output of the first stage will be n\xa0×\xa0768\xa0× 1024\xa0× 1). Then, a standard convolution is applied to the single layer (which can have indeed more than one channel). In the majority of implementations, the default number of output channels for the depthwise convolution is 1 (this is conventionally expressed by saying that the depth multiplier is 1). This approach allows a dramatic parameter reduction with respect to a standard convolution. In fact, if the input generic feature map is\xa0X\xa0∈\xa0ℜw\xa0× h × p\xa0and we want to perform a standard convolution with q kernels k(i)\xa0∈ ℜn × m, we need to learn nmqp parameters (each\xa0kernel k(i) is applied to all input channels). Employing the Depthwise Separable Convolution, the first step (working with only the channels) requires nmp parameters. As the output has still p feature maps and we need to output q channels, the process employs a trick:\xa0processing each feature map with q 1\xa0× 1 kernels (in this way, the output will have q layers and the same dimensions). The number of parameters required for the second step is pq, so the total number of parameters becomes nmp + pq. Comparing this value with the one required for a standard convolution, we obtain an interesting result:  As this condition is easily true, this approach is extremely effective in optimizing the training and prediction processes, as well as the memory consumption in any scenario. It's not surprising that the Xception model has been immediately implemented in mobile devices, allowing real-time image classification with very limited resources. Of course, depthwise separable convolutions don't always have the same accuracy as standard ones, because they are based on the assumption that the geometrical features observable inside a channel of a composite feature map are independent of each other. This is not always true, because we know that the effect of multiple layers is based also on their combinations (which increases the expressivity of a network). However, in many cases the final result has an accuracy comparable to some state-of-the-art models; therefore, this technique can very often be considered as a valid alternative to a standard convolution.\xa0", '62224106-d11b-40f3-9fdf-2e33f1d018d4.xhtml': "A transpose convolution (sometimes wrongly called deconvolution, even if the mathematical definition is different) is not very different from a standard convolution, but its goal is to rebuild a structure with the same features as the input sample. Let's suppose that the output of a convolutional network is the feature map\xa0X\xa0∈\xa0ℜw' × h' × p and we need to build an output element\xa0Y\xa0∈\xa0ℜw\xa0× h × 3 (assuming the w and h are the original dimensions). We can achieve this result by applying a transpose convolution with appropriate strides and padding to X. For example, let's suppose that\xa0X\xa0∈\xa0ℜ128 × 128 × 256 and our output must be 512\xa0× 512\xa0× 3. The last transpose convolution must learn three filters with strides set to four and\xa0same padding. We are going to see some practical examples of this method in the next chapter Chapter 11, Autoencoders when discussing autoencoders; however, there are no very important differences between transpose and standard convolution in terms of internal dynamics. The main difference is the cost function, because when a transpose convolution is used as the last layer, the comparison must be done between a target image and a reconstructed one. In the next chapter,\xa0\xa0Chapter 11,\xa0Autoencoders\xa0 we are also going to analyze some techniques to improve the quality of the output even when the cost function doesn't focus on specific areas of the image.", '6ecae810-579b-487e-a67f-837251048942.xhtml': "In a deep convolutional network, pooling layers are extremely useful elements. There are mainly two kinds of these structures:\xa0max pooling and average pooling. They both work on patches p\xa0∈\xa0ℜn\xa0× m,\xa0shifting horizontally and vertically according to the predefined stride value and\xa0transforming the patches into single pixels according to the following rules:  There are two main reasons that justify the use of these layers. The first one is a dimensionality reduction with limited information loss (for example, setting the strides to (2, 2), it's possible to halve the dimensions of an image/feature map). Clearly, all pooling techniques can be more or less lossy (in particular max pooling) and the specific result depends on the single image. In general, pooling layers try to summarize the information contained in a small chunk into a single pixel. This idea is supported by a perceptual-oriented approach; in fact, when the pools are not too large, it's rather unlikely to find high variances in subsequent shifts (natural images have very few isolated pixels). Therefore, all the pooling operations allow us to set up strides greater than one with a mitigated risk of compromising the information content. However, considering several experiments and architectures, I suggest that you set up larger strides in the convolutional layers (in particular, in the first layer of a convolutional sequence) instead of in pooling ones. In this way, it's possible to apply the transformation with a minimum loss and to fully exploit the next fundamental property.\xa0 The second (and probably the most important) reason is that they slightly increase the robustness to translations and limited distortions with an effect that is proportional to the pool size. Let's consider the following diagram, representing an original image of a cross and the version after a 10-pixel diagonal translation: This is a very simple example and the translated image is not very different from the original one. However, in a more complex scenario, a classifier could also fail to correctly classify an object in similar conditions. Applying a max pooling (with a (2\xa0× 2) pool size and 2-pixel strides) on the translated image, we get the following: The result is a larger cross, whose arms are slightly more aligned to the axis. When compared with the original image, it's easier for a classifier with a good generalization ability to filter out the spurious elements and recognize the original shape (which can be considered a cross surrounded by a noisy frame). Repeating the same experiment with average pooling (same parameters), we obtain the following: In this case, the picture is partially smoothed, but it's still possible to see a better alignment (thanks mainly to the\xa0fading effect). Also, if these methods are simple and somewhat effective, the robustness to invariant transformations is never dramatically improved and higher levels of invariance are possible only by increasing the pool size. This choice leads to coarser-grained feature maps whose amount of information is drastically reduced; therefore, whenever it's necessary to extend the classification to samples that can be distorted or rotated, it can be a good idea (which allows working with a dataset that better represents the real data generating process) to use a data augmentation technique to produce artificial images and to also train the classifier on them. However, as pointed out in\xa0Deep Learning, Goodfellow I., Bengio Y.,\u200e Courville A.,\xa0MIT Press, pooling layers can also provide a robust invariance to rotations when they are used together with the output of a multiple convolution layer or a rotated image stack. In fact, in these cases, a single pattern response is elicited and the effect of the pooling layer becomes similar to a collector that standardizes the output. In other words, it will produce the same result without an explicit selection of the best matching pattern. For this reason, if the dataset contains enough samples, pooling layers in intermediate positions of the network can provide a moderate robustness to small rotations, increasing the generalization ability of the whole deep architecture. As it's easy to see in the previous example, the main difference between the two variants is the final result. Average pooling performs a sort of very simple interpolation, smoothing the borders and avoiding abrupt changes. On the other hand, max pooling is less noisy and can yield better results when the features need to be detected without any kind of smoothing (which could alter their geometry). I always suggest testing both techniques, because it's almost impossible to pick the best method with the right pool size according only to heuristic considerations (above all, when the datasets are not made up of very simple images). Clearly, it's always preferable to use these layers after a group of convolutions, avoiding very large pool sizes that can irreversibly destroy the information content. In many important deep architectures, the pooling layers are always based on (2, 2) or (3, 3) pools, independently of their position, and the strides are always set to 1 or 2.\xa0In both cases, the information loss is proportional to the pool size/strides; therefore, large pools are normally avoided when small features must be detected together with larger ones (for example, foreground and background faces).", 'bf03613e-8af1-4e20-835c-301a35471257.xhtml': 'Even if convolution and pooling layers are the backbone of almost all deep convolutional networks, other layers can be helpful to manage specific situations. They are as follows:', 'e723f412-4988-4cb3-8476-a9d1bc2c3d72.xhtml': "In the first example, we want to consider again the complete MNIST handwritten digit dataset, but instead of using an MLP, we are going to employ a small deep convolutional network. The first step consists of loading and normalizing the dataset: We can now define the model architecture. The samples are rather small (28\xa0× 28); therefore it can be helpful to use small kernels. This is not a general rule and it's useful to also evaluate larger kernels (in particular in the first layers); however, many state-of-the-art architectures confirmed large kernel sizes with small images can lead to a performance loss. In my personal experiments, I've always obtained the best results when the largest kernels were 8\xa0÷ 10 smaller than the image dimensions. Our model is made up of the following layers: The goal is to capture the low-level features (horizontal and vertical lines, intersections, and so on) in the first layers and use the pooling layers and all the subsequent convolutions to increase the accuracy when distorted samples are presented. At this point, we can create and compile the model (using the Adam optimizer with η = 0.001 and a decay\xa0rate equal to 10-5): We can now proceed to train the model with 200 epochs and a batch size of 256 samples: The final validation accuracy is now 0.9950, which means that only 50 samples (out of 10,000) have been misclassified. To better understand the behavior, we can plot the accuracy and loss diagrams: As it's possible to see, both validation accuracy and loss easily reach the optimal values. In particular, the initial validation accuracy is about 0.97 and the remaining epochs are necessary to improve the performance with all those samples, whose shapes can lead to confusion (for example, malformed 8s that resemble 0s, or 7s that are very similar to 1s). It's evident that the geometric approach employed by convolutions guarantees a much higher robustness than a standard fully-connected network, thanks also to the contribution of pooling layers, which reduce the variance due to noisy samples.", '252f1ae0-1a45-4039-b40c-eef358d75c4d.xhtml': "In this example, we are going to use the Fashion MNIST dataset, which was freely provided by Zalando as a more difficult replacement for the standard MNIST dataset. In this case, instead of handwritten digits, there are greyscale photos of different articles of clothing. An example of a few samples is shown in the following screenshot: However, in this case, we want to employ a utility class provided by Keras (ImageDataGenerator) in order to create a data-augmented sample set to improve the generalization ability of the deep convolutional network. This class allows us to add random transformations (such as standardization, rotations, shifting, flipping, zooming, shearing, and so on) and output the samples using a Python generator (with an infinite loop). Let's start loading the dataset (we don't need to standardize it, as this transformation is performed by the generator): At this point, we can create the generators, selecting the transformation that best suits our case. As the dataset is rather standard (all the samples are represented only in a few positions), we've decided to augment the dataset by applying a sample-wise standardization (which doesn't rely on the entire dataset), horizontal flip, zooming, small rotations, and small shears. This choice has been made according to an objective analysis, but I suggest the reader repeat the experiment with different parameters (for example, adding whitening, vertical flip, horizontal/vertical shifting, and extended rotations). Of course, increasing the augmentation variability needs larger processed sets. In our case, we are going to use 384,000 training samples (the original size is 60,000), but larger values can be employed to train deeper networks: Once an image data generator has been initialized, it must be fitted, specifying the input dataset and the desired batch size (the output of this operation is the actual Python generator). The test image generator is voluntarily kept without transformations except for normalization and standardization, in order to avoid a validation on a dataset drawn from a different distribution. At this point, we can create and compile our network, using 2D convolutions based on Leaky ReLU activations (using the\xa0LeakyReLU\xa0class, which replaces the standard layer Activation), batch normalizations, and max poolings: All the batch normalizations are always applied to the linear transformation before the activation function. Considering the additional complexity, we are also going to use a callback, which is a class that Keras uses in order to perform in-training operations. In our case, we want to reduce the learning rate when the validation loss stops improving. The specific callback is called\xa0ReduceLROnPlateau and it's tuned in order to reduce\xa0η multiplying it by 0.1 (after a number of epochs equal to the value of the patience\xa0parameter) with a cooldown period (the number of epochs to wait before restoring the original learning rate) of 1 epoch and a minimum η = 10-6. The training method is now fit_generator(), which accepts Python generators instead of finite datasets and the number of iterations per epoch (all the other parameters are the same as implemented by fit()): In this case, the complexity is higher and the result is not as accurate as the one obtained with the standard MNIST dataset. The validation and loss plots are shown in the following graph: The loss plot doesn't show a U-curve, but it seems that there are no real improvements starting from the 20th epoch. This is also confirmed by the validation plot, which continues oscillating between 0.935 and about 0.94. On the other side, the training loss hasn't reached its minimum (nor has the training accuracy), mainly because of the batch normalizations. However, considering several benchmarks, the result is not bad (even if state-of-the-art models can reach a validation accuracy of about 0.96). I suggest that the reader try different configurations (with and without dropout and other activations) based on deeper architectures with larger training sets. This example offers many chances to practice with this kind of models, as the complexity is not as high as to require dedicated hardware, but at the same time, there are many ambiguities (for example, between shirts and t-shirts) that can reduce the generalization ability.", '17e7c9ea-9390-406b-9d6a-362ebeed5e59.xhtml': "All the models that we have analyzed until now have a common feature. Once the training process is completed, the weights are frozen and the output depends only on the input sample. Clearly, this is the expected behavior of a classifier, but there are many scenarios where a prediction must take into account the history of the input values. A time series is a classic example. Let's suppose that we need to predict the temperature for the next week. If we try to use only the last known x(t) value and an MLP trained to predict x(t+1), it's impossible to take into account temporal conditions like the season, the history of the season over the years, the position in the season, and so on. The regressor will be able to associate the output that yields the minimum average error, but in real-life situations, this isn't enough. The only reasonable way to solve this problem is to define a new architecture for the artificial neuron, to provide it with a memory. This concept is shown in the following diagram: Now the neuron is no longer a pure feed-forward computational unit because the feedback connection forces it to remember its past and use it in order to predict new values. The new dynamic rule is now as follows:  The previous prediction is fed back and summed to new linear output. The resulting value is transformed by the activation function in order to produce the actual new output (conventionally the first output is null, but this is not a constraint). An immediate consideration concerns the activation function—this is a dynamic system that could easily become unstable. The only way to prevent this phenomenon is to employ saturating functions (such as the sigmoid or hyperbolic tangent). In fact, whatever the input is, the output can never explode\xa0by moving towards +∞ or -∞. Suppose that, instead, we were to use a ReLU activation—under some conditions, the output will grow indefinitely, leading to an overflow. Clearly, the situation is even worse with a linear activation and could be very similar even when using a Leaky ReLU or ELU. Hence, it's obvious that we need to select saturating functions, but is this enough to ensure stability? Even if a hyperbolic tangent (as well as a sigmoid) has two stable points (-1 and +1), this isn't enough to ensure stability. Let's imagine that the output is affected by noise and oscillates around 0.0. The unit cannot converge towards a value and remains trapped in a limit cycle. Luckily, the possibility to learn the weights allows us to increase the robustness to noise, avoiding that limited changes in the input could invert the dynamic of the neuron. This is a very important (and easy to prove) result that guarantees stability under very simple conditions, but again, what is the price that we need to pay? Is it anything simple and straightforward? Unfortunately, the answer is negative and the price for stability is extremely high. However, before discussing this problem, let's show how a simple recurrent network can be trained.", '4d66aa01-8f18-48ec-9392-8c71fccc0113.xhtml': "The simplest way to train an\xa0RNN\xa0is based on a representational trick. As the input sequences are limited and their length can be fixed, it's possible to restructure the simple neuron with a feedback connection as an unrolled feed-forward network. In the following diagram, there's an example with k timesteps: This network (which can be easily extended to more complex architecture with several layers) is exactly like an MLP, but in this case, the weights of each clone are the same. The algorithm called BPTT\xa0is the natural extension of the standard learning technique to unrolled recurrent networks. The procedure is straightforward. Once all the outputs have been computed, it's possible to determine the value of the cost function for every single network. At this point, starting from the last step, the corrections (the gradients) are computed and stored, and the process is repeated until the initial step. Then, all of the gradients are summed and applied to the network. As every single contribution is based on a precise temporal experience (made up of a local sample and a previous memory element), the standard backpropagation will learn how to manage a dynamic condition as if it were a point-wise prediction. However, we know that the actual network is not unrolled and the past dependencies are theoretically propagated and remembered. I voluntarily used the word theoretically, because all practical experiments show a completely different behavior that we are going to discuss. This technique is very easy to implement, but it can be very expensive for deep networks that must be unrolled for a large number of timesteps. For this reason, a variant called truncated backpropagation through time\xa0(TBPTT) has been proposed (in Subgrouping reduces complexity and speeds up learning in recurrent networks, Zipser D., Advances in Neural Information Processing Systems, II 1990). The idea is to use two sequence lengths t1 and\xa0t2\xa0(with t1\xa0>>\xa0t2)—the longer one (t1) is employed for the feed-forward phase, while the shorter length (t2) is used to train the network. At first sight, this version seems like a normal BPTT with a short sequence; however, the key idea is to force the network to update the hidden states with more pieces of information and then compute the corrections according to the result of the longer sequence (even if the updates are propagated to a limited number of previous timesteps). Clearly, this is an approximation that can speed up the training process, but the final result is normally comparable with the one obtained by processing long sequences, in particular when the dependencies can be split into shorter temporal chunks (and therefore the assumption is that there are no very long dependencies). Even if the BPTT algorithm is mathematically correct and it's not difficult to learn short-term dependencies (corresponding to short unrolled networks), several experiments confirmed that it's extremely difficult (or almost impossible) learning long-term dependencies. In other words, it's easy to exploit past experiences whose contribution is limited to a short window (and therefore whose importance is limited because they cannot manage the most complex trends) but the network cannot easily learn all behaviors that, for example, have a periodicity of hundreds of timesteps. In 1994, Bengio, Simard, and Frasconi provided a theoretical explanation of the problem (in\xa0Learning Long-Term Dependencies with Gradient Descent is Difficult, Bengio Y., Simard P., Frasconi P., IEEE Transactions on Neural Networks, 5/1994). The mathematical details are rather complex, because they involve dynamic system theory; however, the final result is that a network whose neurons are forced to become robust to noise (the normal expected behavior) is affected by the vanishing gradients problem when t\xa0→\xa0∞. More generally, we can represent a vectorial recurrent neuron dynamic as follows:  The multiplicative effect of BPTT forces the gradients to be proportional to Wt. If the largest absolute eigenvalue (also known as spectral radius) of W is smaller than 1, then the following applies:  More simply, we can re-express the result saying that the magnitude of the gradients is proportional to the length of the sequences and even if the condition is asymptotically valid, many experiments confirmed that the limited precision of numeric computations and the exponential decay due to subsequent multiplications can force the gradients to vanish even when the sequences are not extremely long. This seems to be the end of any RNN architecture, but luckily more recent approaches have been designed and proposed to resolve this problem, allowing RNNs to learn both short and long-term dependencies without particular complications. A new era of RNNs started and the results\xa0were immediately outstanding.", '678b4fc9-a975-4ee4-9e01-2e442c85d563.xhtml': "This model (which represents the state-of-the-art recurrent cell in many fields) was proposed in 1997 by\xa0Hochreiter and Schmidhuber (in\xa0Long Short-Term Memory, Hochreiter S., Schmidhuber J.,\xa0Neural Computation, Vol. 9, 11/1997) with the emblematic name long-short-term memory\xa0 (LSTM). As the name suggests, the idea is to create a more complex artificial recurrent neuron that can be plugged into larger networks and trained without the risk of vanishing and, of course, exploding gradients. One of the key elements of classic recurrent networks is that they are focused on learning, but not on selectively forgetting. This ability is indeed necessary for optimizing the memory in order to remember what is really important and removing all those pieces of information that are not necessary to predict new values. To achieve this goal, LSTM exploits two important features (it's helpful to expose them before discussing the model). The first one is an explicit state, which is a separate set of variables that store the elements necessary to build long and short-term dependencies, including the current state. These variables are the building blocks of a mechanism called constant error carousel (CEC), named in this way because it's responsible for the cyclical and internal management of the error provided by the backpropagation algorithm. This approach allows the correction of the weights without suffering the multiplicative effect anymore. The internal LSTM dynamics allow better understanding of how the error is safely fed back; however, the exact explanation of the training procedure (which is always based on the gradient descent) is beyond the scope of this book and can be found in the aforementioned paper. The second feature is the presence of gates. We can simply define a gate as an element that can modulate the amount of information flowing through it. For example, if y = ax and a is a variable bounded between 0 and 1, it can be considered as a gate, because when it's equal to 0, it blocks the input x;\xa0when it's equal to 1, it allows the input to flow in without restrictions; and when it has an intermediate value, it reduces the amount of information proportionally. In LSTMs, gates are managed by sigmoid functions, while the activations are based on hyperbolic tangents (whose symmetry guarantees better performances).\xa0At this point, we can show the structural diagram of an LSTM cell and discuss its internal dynamics: The first (and most important) element is the memory state, which is responsible for the dependencies and for the actual output. In the diagram, it is represented by the upper line and its dynamics are represented by the following general equation:  So, the state depends on the previous value, on the current input, and on the previous output. Let's start with the first term, introducing the forget gate. As the name says, it's responsible for the persistence of the existing memory elements or for their deletion. In the diagram, it's represented by the first vertical block and its value is obtained by considering the concatenation of previous output and current input:  The operation is a classical neuron activation with a vectorial output. An alternative version can use two weight matrices and keep the input elements separated:  However, I prefer the previous version, because it can better express the homogeneity of input and output, and also their consequentiality. Using the forget gate, it's possible to determine the value of g1(C(t)) using the Hadamard (or element-wise) product:  The effect of this computation is filtering the content of C(t) that must be preserved and the validity degree (which is proportional to the value of f(t+1)). If the forget gate outputs a value close to 1, the corresponding element is still considered valid, while lower values determine a sort of obsolescence that can even lead the cell to completely remove an element when the forget gate value is 0 or close to it. The next step is to consider the amount of the input sample that must be considered to update the state. This task is achieved by the input gate (second vertical block). The equation is perfectly analogous to the previous one:  However, in this case, we also need to compute the term that must be added to the current state. As already mentioned, LSTM cells employ hyperbolic tangents for the activations; therefore, the new contribution to the state is obtained as follows:  Using the input gate and the state contribution, it's possible to determine the function g2(x(t+1), y(t)):  Hence, the complete state equation becomes as follows:  Now, the inner logic of an LSTM cell\xa0is more evident. The state is based on the following: Realistic scenarios are many. It's possible that a new input forces the LSTM to reset the state and store the new incoming value. On the other hand, the input gate can also remain closed, giving a very low priority to the new input (together with the previous output). In this case, the LSTM, considering the long-term dependencies, can decide to discard a sample that is considered noisy and not necessarily able to contribute to an accurate prediction. In other situations, both the forget and input gates can be partially open, letting only some values influence the state. All these possibilities are managed by the learning process through the correction of the weight matrices and the biases. The difference with BPTT is that the long-term dependencies are no longer impeded by the vanishing gradients problem. The last step is determining the output. The third vertical block is called the output gate and controls the information that must transit from the state to the output unit. Its equation is as follows:  The actual output is hence determined as follows: An important consideration concerns the gates. They are all fed with the same vector, containing the previous output and the current input. As they are homogenous values, the concatenation yields a coherent entity that encodes a sort of inverse cause-effect relationship (this is an improper definition, as we work with previous effect and current cause). The gates work like logistic regressions without thresholding; therefore, they can be considered as pseudo-probability vectors (not distributions, as each element is independent). The forget gate expresses the probability that last sequence (effect, cause) is more important than the current state; however, only the input gate has the responsibility to grant it the right to influence the new state. Moreover, the output gate expresses the probability that the current sequence is able to let the current state flow out. The dynamic is indeed very complex and has some drawbacks. For example, when the output gate remains closed, the output is close to zero and this influences both forget and input gates. As they control the new state and the CEC, they could limit the amount of incoming information and consequent corrections, leading to poor performance. A simple solution that can mitigate this problem is provided by a variant called peephole LSTM. The idea is to feed the previous state to every gate so that they can take decisions more independently. The generic gate equation becomes as follows:  The new set of weights Ug (for all three gates) must be learned in the same way as the standard Wg and bg. The main difference with a classic LSTM is that the sequential dynamic: forget gate | input gate | new state | output gate | actual output is now partially shortcutted. The presence of the state in every gate activation allows them to exploit multiple recurrent connections, yielding a better accuracy in many complex situations. Another important consideration is about the learning process: in this case, the peepholes are closed and the only feedback channel is the output gate. Unfortunately, not every LSTM implementation support peepholes; however, several studies confirmed that in most cases all the models yield similar performances. Xingjian et al. (in Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting, Xingjian S., Zhourong C., Hao W., Dit-Yan Y., Wai-kin W., Wang-Chun W., arXiv:1506.04214 [cs.CV]) proposed a variant called convolutional LSTM, which clearly mixes Convolutions and LSTM cells. The main internal difference concerns the gate computations, which now become (without peepholes, which however, can always be added):  Wg is now a kernel that is convoluted with the input-output vector (which is usually the concatenation of two images). Of course, it's possible to train any number of kernels to increase the decoding power of the cell and the output will have a shape equal to (batch size × width × height × kernels). This kind of cell is particularly useful for joining spatial processing with a robust temporal approach. Given a sequence of images (for example, satellite images, game screenshots, and so on), a convolutional LSTM network can learn long-term relationships that are manifested through geometric feature evolutions (for example, cloud movements or specific sprite strategies that it's possible to anticipate considering a long history of events). This approach (even with a few modifications) is widely employed in Deep Reinforcement Learning in order to solve complex problems where the only input is provided by a sequence of images. Of course, the computational complexity is very high, in particular when many subsequent layers are used; however, the results outperformed any existing method and this approach became one of the first choices to manage this kind of problem. Another important variant, which is common to many Recurrent Neural Networks, is provided by a bidirectional interface. This isn't an actual layer, but a strategy that is employed in order to join the forward analysis of a sequence with the backward one. Two cellblocks are fed with a sequence and its inverse and the output, for example, is concatenated and used for further processing steps. In fields such as NLP, this method allows us to dramatically improve the accuracy of classifications and real-time translations. The reason is strictly related to the rules underlying the structure of a sequence. In natural language, a sentence w1 w2 ... wn has forward relationships (for example, a singular noun can be followed by is), but the knowledge of backward relationships (for example, the sentence this place is pretty awful) permits avoiding common mistakes that, in the past, had to be corrected using post-processing steps (the initial translation of pretty could be similar to the translation of nice, but a subsequent analysis can reveal that the adjective mismatches and a special rule can be applied). Deep learning, on the other side, is not based on special rules, but on the ability to learn an internal representation that should be autonomous in making final decisions (without further external aids) and bidirectional LSTM networks help in reaching this goal in many important contexts.", '57957edd-fdc6-4134-b4b3-d32bfa9630cf.xhtml': "This model, named Gated recurrent unit (GRU), proposed by Cho et al. (in Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, Cho K., Van Merrienboer B., Gulcehre C., Bahdanau D., Bougares F., Schwenk H., Bengio Y., arXiv:1406.1078\u202f[cs.CL]) can be considered as a simplified LSTM with a few variations. The structure of a generic full-gated unit is represented in the following diagram: The main differences from LSTM are the presence of only two gates and the absence of an explicit state. These simplifications can speed both the training and the prediction phases while avoiding the vanishing gradient problem. The first gate is called the\xa0reset gate (conventionally denoted with the letter r) and its function is analogous to the forget gate:  Similar to the forget gate, its role is to decide what content of the previous output must be preserved and the relative degree. In fact, the additive contribution to new output is obtained as follows:  In the previous expression, I've preferred to separate the weight matrices to better exposes the behavior. The argument of tanh(•) is the sum of a linear function of the new input and a weighted term that is a function of the previous state. Now, it's clear how the reset gate works: it modulates the amount of history (accumulated in the previous output value) that must be preserved and what instead can be discarded. However, the reset gate is not enough to determine the right output with enough accuracy, considering both short and long-term dependencies. In order to increase the expressivity of the unit, an update gate (with a role similar to the LSTM input gate) has been added:  The update gate controls the amount of information that must contribute to the new output (and hence to the state). As it's a value bounded between 0 and 1, GRUs are trained to mix old output and new additive contribution with an operation similar to a weighted average:  Therefore, the update gate becomes a modulator that can select which components of each flow must be output and stored for the next operation. This unit is structurally simpler than an LSTM, but several studies confirmed that its performance is on average, equivalent to LSTM, with some particular cases when GRU has even outperformed the more complex cell. My suggestion is that you test both models, starting with LSTM. The computational cost has been dramatically reduced by modern hardware and in many contexts the advantage of GRUs is negligible. In both cases, the philosophy is the same: the error is kept inside the cell and the weights of the gates are corrected in order to maximize the accuracy. This behavior prevents the multiplicative cascade of small gradients and increases the ability to learn very complex temporal behaviors. However, a single cell/layer would not be able to successfully achieve the desired accuracy. In all these cases, it's possible to stack multiple layers made up of a variable number of cells. Every layer can normally output the last value or the entire sequence. The former is used when connecting the LSTM/GRU layer to a fully-connected one, while the whole sequence is necessary to feed another recurrent layer. We are going to see how to implement these techniques with Keras in the following example.", 'af111a04-d9f8-4c2b-bf88-98aceb024f9e.xhtml': "In this example, we want to test the ability of an LSTM network to learn long-term dependencies. For this reason, we employ a dataset called Zuerich Monthly Sunspots (freely provided by Andrews and Herzberg in 1985) containing the numbers observed in all the months starting from 1749 to 1983 (please read the information box for how to download the dataset). As we are not interested in the dates, we need to parse the file in order to extract only the values needed for the time series (which contains 2,820 steps): Alternatively, it's possible to load the CSV dataset using pandas (https://pandas.pydata.org), which is a powerful data manipulation/analysis library (for further information, please refer to Learning pandas Second Edition, Heydt M., Packt): The values are unnormalized and as LSTMs work with hyperbolic tangents, it's helpful to normalize them in the interval -1 and 1. We can easily perform this step using the Scikit-Learn class MinMaxScaler: The complete dataset is shown in the following diagram: In order to train the model, we have decided to use 2,300 samples for training and the remaining 500 for validation (corresponding to about 42 years). The input of the model is a batch of sequences of 15 samples (shifted along the time axis) and the output is the subsequent month; therefore, before training, we need to prepare the dataset: Now, we can create and compile a simple model with a single stateful LSTM layer containing four cells, followed by a hyperbolic tangent output neuron (I always suggest that the reader experiment with more complex architectures and different parameters): Setting the stateful=True parameter\xa0in the LSTM class forces Keras not to reset the state after each batch. In fact, our goal is learning long-term dependencies and the internal LSTM state must reflect the overall trend. When an LSTM network is stateful, it's also necessary to specify the batch size in the input shape (through the batch_input_shape\xa0parameter\xa0). In our case, we have selected a batch size equal to 20 samples. The optimizer is Adam with a higher decay (to avoid instabilities) and a loss based on the mean squared error (which is the most common choice in this kind of scenario). At this point, we can train the model (for 100 epochs): This is an example whose purpose is only didactic; therefore, the final validation mean squared error is not extremely low. However, as it's possible to see in the following diagram (representing the predictions on the validation set), the model has successfully learned the global trend: The model is still unable to achieve a very high accuracy in correspondence of all the very rapid spikes, but it's able to correctly model the amplitude of the oscillations and the length of the tails. For the sake of intellectual honesty, we must consider that this validation is performed on true data; however, when working with time series, it's normal to predict a new value using the ground truth. In this case, it's like a moving prediction where each value is obtained using the training history and a set of real observations. It's clear that the model is able to predict the long-term oscillations and also some local ones (for example, the sequence starting from step 300), but it can be improved in order to have better performance on the whole validation set. To achieve this goal, it is necessary to increase the network complexity and tune up the learning rate (it's a very interesting exercise on a real dataset). Observing the previous diagram, it's possible to see that the model is relatively more accurate at some high frequencies (rapid changes), while it's more imprecise on others. This is not a strange behavior, because very oscillating functions need more non-linearity (think about the Taylor expansion and the relative error when it's truncated to a specific degree) to achieve high accuracies (this means employing more layers). My suggestion is that you repeat the experiment using more LSTM layers, considering that we need to pass the whole output sequence to the following recurrent layer (this can be achieved by setting the\xa0return_sequences=True\xa0parameter). The last layer, instead, must return only the final value (which is the default behavior). I also suggest testing the GRU layers, comparing the performance with the LSTM version and picking the simplest (benchmarking the training time) and most accurate solution.", '32d4c4f7-5574-4b2d-a64d-8c34fef1fcfa.xhtml': "We have discussed how deep learning is fundamentally based on gray-box models that learn how to associate input patterns to specific classification/regression outcomes. All the processing pipeline that is often employed to prepare the data for specific detections is absorbed by the complexity of the neural architecture. However, the price to pay for high accuracies is a proportionally large number of training samples. State-of-the-art visual networks are trained with millions of images and, obviously, each of them must be properly labeled. Even if there are many free datasets that can be employed to train several models, many specific scenarios need hard preparatory work that sometimes is very difficult to achieve. Luckily, deep neural architectures are hierarchical models that learn in a structured way. As we have seen in the examples of deep convolutional networks, the first layers become more and more sensitive to detect low-level features, while the higher ones concentrate their work on extracting more detailed high-level features. In several tasks, it's reasonable to think that a network trained, for example, with a large visual dataset (such as ImageNet or Microsoft Coco) could be reused to achieve a specialization in a slightly different task. This concept is known as transfer learning and it's one of the most useful techniques when it's necessary to create state-of-the-art models with brand new datasets and specific objectives. For example, a customer can ask for a system to monitor a few cameras with the goal to segment the images and highlight the boundaries of specific targets. The input is made up of video frames with the same geometric properties as thousands of images employed in training very powerful models (for example, Inception, ResNet, or VGG); therefore, we can take a pre-trained model, remove the highest layers (normally dense ones ending in a softmax classification layer) and connect the flattening layer to an MLP that outputs the coordinates of the bounding boxes. The first part of the network can be frozen (the weights are not modified anymore), while the SGD is applied to tune up the weights of the newly specialized sub-network. Clearly, such an approach can dramatically speed up the training process, because the most complex part of the model is already trained and can also guarantee an extremely high accuracy (with respect to a naive solution), thanks to the optimization already performed on the original model. Obviously, the most natural question is how does this method work? Is there any formal proof? Unfortunately, there are no mathematical proofs, but there's enough evidence to assure about us of this approach. Generally speaking, the goal of a neural training process is to specialize each layer in order to provide a more particular (detailed, filtered, and so on) representation to the following one. Convolutional networks are a clear example of this behavior, but the same is observable in MLPs as well. The analysis of very deep convolutional networks showed how the content is still visual until reaching the flattening layer, where it's sent to a series of dense layers that are responsible for feeding the final softmax layer. In other words, the output of the convolutional block is a higher-level, segmented representation of the input, which is seldom affected by the specific classification problem. For this reason, transfer learning is generally sound and doesn't normally require a retraining of the lower layers. However, it's difficult to understand which model can yield the best performances and it's very useful to know which dataset has been used to train the original network. General purpose datasets (for example, ImageNet) are very useful in many contexts, while specific ones (such as Cifar-10 or Fashion; MNIST can be too restrictive). Luckily, Keras offers (in the package keras.applications) many models (even quite complex ones) that are always trained with ImageNet datasets and that can be immediately employed in a production-ready application. Even if using them is extremely simple, it requires a deeper knowledge of this framework, which is beyond the scope of this book. I invite the reader interested in this topic to check the book Deep Learning with Keras, Gulli A., Pal S., Packt.", 'dc33919b-e1e2-43f6-8157-ea24d80472e6.xhtml': "In this chapter, we have presented the concept of a deep convolutional network, which is a generic architecture that can be employed in any visual processing task. The idea is based on hierarchical information management, aimed at extracting the features starting from low-level elements and moving forward until the high-level details that can be helpful to achieve specific goals. The first topic was the concept of convolution and how it's applied in discrete and finite samples. We discussed the properties of standard convolution, before analyzing some important variants such as atrous (or dilated convolution), separable (and depthwise separable) convolution and, eventually, transpose convolution. All these methods can work with 1D, 2D, and 3D samples, even if the most diffused applications are based on bidimensional (not considering the channels) matrices representing static images. In the same section, we also discussed how pooling layers can be employed to reduce the dimensionality and improve the robustness to small translations. In the next section, we introduced the concept of RNN, emphasizing the issues that normally arise when classic models are trained using the backpropagation through time algorithm. In particular, we explained why these networks cannot easily learn long-term dependencies. For this reason, new models have been proposed, whose performance was immediately outstanding. We discussed the most famous recurrent cell, called Long-short-term memory (LSTM), which can be used in layers that can easily learn all the most important dependencies of a sequence, allowing us to minimize the prediction error even in contexts with a very high variance (such as stock market quotations). The last topic was a simplified version of the idea implemented in LSTMs, which led to a model called a Gated recurrent unit (GRU). This cell is simpler and more computationally efficient, and many benchmarks confirmed that its performance is approximately the same as LSTM. In the next chapter, Chapter 11,\xa0Autoencoders\xa0we are going to discuss some particular models called autoencoders, whose main property is to create internal representations of an arbitrarily complex input distribution.", '7a83229b-d1e3-4fdc-9e15-a09555700ae6.xhtml': "In this chapter, we are going to look at an unsupervised model family whose performance has been boosted by modern deep learning techniques. Autoencoders offer a different approach to classic problems such as dimensionality reduction or dictionary learning, but unlike many other algorithms, they don't suffer the capacity limitations that affect many famous models. Moreover, they can exploit specific neural layers (such as convolutions) to extract pieces of information based on specialized criteria. In this way, the internal representations can be more robust to different kinds of distortions and much more efficient in terms of the amount of information they can process. In particular, we are going to discuss the following:", '8486ed78-5dfd-4c48-8964-904db0c53f53.xhtml': "In the previous chapters, we discussed how real datasets are very often high-dimensional representations of samples that lie on low-dimensional manifolds (this is one of the semi-supervised pattern's assumptions, but it's generally true). As the complexity of a model is proportional to the dimensionality of the input data, many techniques have been analyzed and optimized in order to reduce the actual number of valid components. For example, PCA selects the features according to the relative explained variance, while ICA and generic dictionary learning techniques look for basic atoms that can be combined to rebuild the original samples. In this chapter, we are going to analyze a family of models based on a slightly different approach, but whose capabilities are dramatically increased by the employment of deep learning methods. A generic autoencoder is a model that is split into two separate (but not completely autonomous) components called an Encoder and a Decoder. The task of the encoder is to transform an input sample into an encoded feature vector, while the task of the decoder is the opposite: rebuilding the original sample using the feature vector as input. The following diagram shows a schematic representation of a generic model: More formally, we can describe the encoder as a parametrized function: The output zi is a vectorial code whose dimensionality is normally quite lower than the inputs. Analogously, the decoder is described as the following:  The goal of a standard algorithm is to minimize a cost function that is proportional to the reconstruction error. A classic method is based on the mean squared error (working on a dataset with M samples):  This function depends only on the input samples (which are constant) and the parameter vectors; therefore, this is de facto an unsupervised method where we can control the internal structure and the constraints imposed on the\xa0zi\xa0code. From a probabilistic viewpoint, if the input\xa0xi\xa0samples\xa0are drawn from a\xa0p(X)\xa0data-generating process, our goal is to find a\xa0q(•)\xa0parametric distribution that minimizes the Kullback–Leibler divergence with p(X). Considering the previous definitions, we can define\xa0q(•) as follows:  Therefore, the Kullback–Leibler divergence becomes the following:  The first term represents the negative entropy of the original distribution, which is constant and isn't involved in the optimization process. The other term is the cross-entropy between the p and q. If we assume Gaussian distributions for p and q, the mean squared error is proportional to the cross-entropy (for optimization purposes, it's equivalent to it), and therefore this cost function is still valid under a probabilistic approach. Alternatively, it's possible to consider Bernoulli distributions for p and q, and the cross-entropy becomes the following:  The main difference between the two approaches is that while a mean squared error can be applied to xi ∈ ℜq (or multidimensional matrices), Bernoulli distributions need xi ∈ [0, 1]q\xa0(formally, this condition should be xi\xa0∈ {0, 1}q; however, the optimization can also be successfully performed when the values are not binary). The same constraint is necessary for the reconstructions; therefore, when using neural networks, the most common choice is to employ sigmoid layers.", 'c35cb882-0e87-4213-85f4-5ee7382126c5.xhtml': "This example (like all the others in this and the following chapters) is based on TensorFlow (for information about the installation of TensorFlow, please refer to the information box at the end of the section), because this framework allows a greater flexibility that is sometimes much more problematic with Keras. We will approach this example pragmatically, and so we are not going to explore all the features because they are beyond the scope of this book; however, interested readers can refer to Deep Learning with TensorFlow - Second Edition, Zaccone G., Karim R., Packt. In this example, we are going to create a deep convolutional autoencoder and train it using the Fashion MNIST dataset. The first step is loading the data (using the Keras helper function), normalizing, and in order to speed up the computation, limiting the training set to 1,000 samples: At this point, we can create the Graph, setting up the whole architecture, which is made up of the following: As the images are (28\xa0× 28), we prefer to resize each batch to\xa0the dimensions of (32\xa0× 32) to easily manage all the subsequent operations that are based on sizes which are a power of 2: The loss function is a standard L2 without any other constraint. I invite the reader to test different optimizers and learning rates to employ a solution that guarantees the minimum loss value. After defining the Graph, it's possible to set up an InteractiveSession (or a standard one), initialize all variables, and begin the training process: Once the training process is finished, we can check the average code length for the whole dataset (this information is useful to compare this result with the one achieved by imposing a sparsity constraint): This value is very small, indicating that the representations are already rather sparse; however, we are going to compare it with the mean obtained by a sparse autoencoder. We can now process a few images (10) by encoding and decoding them: The result is shown in the following figure: As you can see, the reconstructions are rather lossy, but the autoencoder successfully learned how to reduce the dimensionality of the input samples. As an exercise, I invite the reader to split the code into two separate sections (encoder and decoder) and to optimize the architecture in order to achieve better accuracy on the whole Fashion MNIST dataset.", 'b18031e6-84f1-4ac2-a408-ad6c7fe7aaec.xhtml': "Autoencoders can be used to determine under-complete representations of a dataset; however, Bengio et al. (in P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol's book\xa0Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion, from the Journal of Machine Learning Research 11/2010) proposed to use them not to learn the exact representation of a sample in order to rebuild it from a low-dimensional code, but rather to denoise input samples. This is not a brand new idea, because, for example, Hopfield networks (proposed a few decades ago) had the same purpose, but its limitations in terms of capacity led researchers to look for different methods. Nowadays, deep autoencoders can easily manage high-dimensional data (such as images) with\xa0a consequent space requirement, that's why many people are now reconsidering the idea of teaching a network how to rebuild a sample image starting from a corrupted one. Formally, there are not many differences between denoising autoencoders and standard autoencoders. However, in this case, the encoder must work with noisy samples: The decoder's cost function remains the same. If the noise is sampled for each batch, repeating the process for a sufficiently large number of iterations allows the autoencoder to learn how to rebuild the original image when some fragments are missing or corrupted. To achieve this goal, the authors suggested different possible kinds of noise. The most common choice is to sample Gaussian noise, which has some helpful features and is coherent with many real noisy processes:  Another possibility is to employ an input dropout layer, zeroing some random elements:  This choice is clearly more drastic, and the rate must be properly tuned. A very large number of dropped pixels can irreversibly delete many pieces of information and the reconstruction can become more difficult and rigid (our purpose is to extend the autoencoder's ability to other samples drawn from the same distribution). Alternatively, it's possible to mix up Gaussian noise and the dropout's, switching between them with a fixed probability. Clearly, the models must be more complex than standard autoencoders because now they have to cope with missing information; the same concept applies to the code length: very under-complete code wouldn't be able to provide all the elements needed to reconstruct the original image in the most accurate way. I suggest testing all the possibilities, in particular when the noise is constrained by external conditions (for example, old photos or messages transmitted through channels affected by precise noise processes). If the model must also be employed for never-before-seen samples, it's extremely important to select samples that represent the true distribution,\xa0using data augmentation techniques (limited to operations compatible with the specific problem) whenever the number of elements is not enough to reach the desired level of accuracy.", 'cd96e7c7-7d02-44e1-95ec-8c83ef0c3b2e.xhtml': "In this example (based on the previous one), we are going to employ a very similar architecture, but as the goal is denoising the images, we will impose a code length equal to (width × height), setting all the strides to (1 × 1), and therefore we won't need to resize the images anymore: In this case, we need to pass both the noisy images (through the placeholder input_noisy_images) and the original ones (which are used to compute the final L2 loss function). For our example, we have decided to employ Gaussian noise with a standard deviation of σ = 0.2 (clipping the final values so that they are always constrained between 0 and 1): The result after 200 epochs is shown in the following figure: The denoising autoencoder has successfully learned to rebuild the original images in the presence of Gaussian noise. I invite the reader to test other methods (such as using an initial dropout) and increase the noise level to understand what the maximum corruption is that this model can effectively remove.", '01bf7343-17bc-4f47-aa62-1232823b339c.xhtml': "In general, standard autoencoders produce dense internal representations. This means that most of the values are different from zero. In some cases, however, it's more useful to have sparse codes that can better represent the atoms belonging to a dictionary. In this case, if zi = (0, 0, zin, ..., 0, zim, ...), we can consider each sample as the overlap of specific atoms weighted accordingly. To achieve this objective, we can simply apply an L1 penalty to the code layer, as explained in Chapter 1, Machine Learning Models Fundamentals. The loss function for a single sample\xa0therefore becomes the following: In this case, we need to consider the extra hyperparameter α, which must be tuned to increase the sparsity without a negative impact on the accuracy. As a general rule of thumb, I suggest starting with a value equal to 0.01 and reducing it until the desired result has been achieved. In most cases, higher values yield very poor performance, and therefore they are generally avoided. A different approach has been proposed by Andrew Ng (in his book Sparse Autoencoder, CS294A, Stanford University). If we consider the code layer as a set of independent Bernoulli random variables, we can enforce sparsity by considering a generic reference Bernoulli variable with a very low mean (for example, pr = 0.01) and adding the Kullback–Leibler divergence between the generic element zi(j) and pr to the cost function. For a single sample, the extra term is as follows (p is the code length):  The resulting loss function becomes the following:  The effect of this penalty is similar to L1 (with the same considerations about the α hyperparameter), but many experiments have confirmed that the resulting cost function is easier to optimize, and it's possible to achieve the same level of sparsity that reaches higher reconstruction accuracies. When working with sparse autoencoders, the code length is often larger because of the assumption that a single element is made up of a small number of atoms (compared to the dictionary size). As a result, I suggest that you evaluate the level of sparsity with different code lengths and select the combination that maximizes the former and minimizes the latter.", 'b12a04a0-a4ba-4d92-bc76-6c56cebd7030.xhtml': "In this example, we are going to add an L1 regularization term to the cost function that was defined in the first exercise: The training process is exactly the same, and therefore we can directly show the final code mean after 200 epochs: As you can see, the mean is now lower, indicating that more code values are close to 0. I invite the reader to implement the other strategy, considering that it's easier to create a constant vector filled with small values (for example, 0.01) and exploit the vectorization properties offered by TensorFlow. I also suggest simplifying the Kullbackâ€“Leibler divergence by splitting it into an entropy term H(pr) (which is constant) and a cross-entropy H(z, pr) term.", 'def84c59-22f0-49d8-a133-8dc962cda686.xhtml': "A variational autoencoder (VAE) is a generative model proposed by Kingma and Wellin (in their work Auto-Encoding Variational Bayes, arXiv:1312.6114 [stat.ML]) that partially resembles a standard autoencoder, but it has some fundamental internal differences. The goal, in fact, is not finding an encoded representation of a dataset, but determining the parameters of a generative process that is able to yield all possible outputs given an input data-generating process. Let's take the example of a model based on a learnable parameter vector θ and a set of latent variables z that have a probability density function p(z;θ). Our goal can therefore be expressed as the research of the θ parameters that maximize the likelihood of the marginalized distribution p(x;θ) (obtained through the integration of the joint probability p(x,z;θ)):  If this problem could be easily solved in closed form, a large set of samples drawn from the p(x) data generating process would be enough to find a p(x;θ) good approximation. Unfortunately, the previous expression is intractable in the majority of cases because the true prior p(z) is unknown (this is a secondary issue, as we can easily make some helpful assumptions) and the posterior distribution p(x|z;θ) is almost always close to zero. The first problem can be solved by selecting a simple prior (the most common choice is z ∼ N(0, I)), but the second one is still very hard because only a few z values can lead to the generation of acceptable samples. This is particularly true when the dataset is very high dimensional and complex (for example, images). Even if there are millions of combinations, only a small number of them can yield realistic samples (if the images are photos of cars, we expect four wheels in the lower part, but it's still possible to generate samples where the wheels are on the top). For this reason, we need to exploit a method to reduce the sample space. Variational Bayesian methods (read C. Fox and S. Roberts's work A Tutorial on Variational Bayesian Inference from Orchid for further information) are based on the idea of employing proxy distributions, which are easy to sample and, in this case, whose density is very high (that is, the probability of generating a reasonable output is much higher than the true posterior). In this case, we define an approximate posterior, considering the architecture of a standard autoencoder. In particular, we can introduce a q(z|x;θq) distribution that acts as an encoder (that doesn't behave determinastically anymore), which can be easily modeled with a neural network. Our goal, of course, is to find the best θq parameter set\xa0to maximize the similarity between q and the true posterior distribution p(z|x;θ). This result can be achieved by minimizing the Kullback–Leibler divergence:  In the last formula, the term log p(x;θ) doesn't depend on z, and therefore it can be extracted from the expected value operator and the expression can be manipulated to simplify it:  The equation can be also rewritten as the following: On the right-hand side, we now have the term ELBO (short for\xa0evidence lower bound) and the Kullback–Leibler divergence between the probabilistic encoder q(z|x;θq) and the true posterior distribution p(z|x;θ). As we want to maximize the log-probability of a sample under the θ parametrization, and considering that the KL divergence is always non-negative, we can only work with the ELBO (which is a lot easier to manage than the other term). Indeed, the loss function that we are going to optimize is the negative ELBO. To achieve this goal, we need two more important steps. The first one is choosing an appropriate structure for q(z|x;θq). As p(z;θ) is assumed to be normal, we can supposedly model q(z|x;θq) as a multivariate Gaussian distribution, splitting the probabilistic encoder into two blocks fed with the same lower layers: In this way, q(z|x;θq) = N(μ(z|x;θq), Σ(z|x;θq)), and therefore the second term on the right-hand side is the Kullback-Leibler divergence between two Gaussian distributions that can be easily expressed as follows (p is the dimension of both the mean and covariance vector): This operation is simpler than expected because, as Σ is diagonal, the trace corresponds to the sum of the elements Σ1 + Σ2 + [...] + Σp and log(|Σ|) = log(Σ1Σ2...Σp) = log Σ1 + log Σ2 + ... + log Σp. At this point, maximizing the right-hand side of the previous expression is equivalent to maximizing the expected value of the log probability to generate acceptable samples and minimizing the discrepancy between the normal prior and the Gaussian distribution synthesized by the encoder. Everything seems much simpler now, but there is still a problem to solve. We want to use neural networks and the stochastic gradient descent algorithm, and therefore we need differentiable functions. As the Kullback-Leibler divergence can be computed only using minibatches with n elements (the approximation becomes close to the true value after a sufficient number of iterations), it's necessary to sample n values from the distribution N(μ(z|x;θq), Σ(z|x;θq)) and, unfortunately, this operation is not differentiable. To solve this problem, the authors suggested a reparameterization trick: instead of sampling from q(z|x;θq), we can sample from a normal distribution, ε ∼ N(0, I), and build the actual samples as μ(z|x;θq) + ε · Σ(z|x;θq)2. Considering that ε is a constant vector during a batch (both the forward and backward phases), it's easy to compute the gradient with respect to the previous expression and optimize both the decoder and the encoder. The last element to consider is the first term on the right-hand side of the expression that we want to maximize:  This term represents the negative cross-entropy between the actual distribution and the reconstructed one. As discussed in the first section, there are two feasible choices: Gaussian or Bernoulli distributions. In general, variational autoencoders employ a Bernoulli distribution with input samples and reconstruction values constrained between 0 and 1. However, many experiments have confirmed that the mean squared error can speed up the training process, and therefore I suggest that the reader test both methods and pick the one that guarantees the best performance (both in terms of accuracy and training speed).", '24480f62-4f11-4443-896c-20f83302e2b6.xhtml': "Let's continue working with the Fashion MNIST dataset to build a variational autoencoder. As explained, the output of the encoder is now split into two components: the mean and covariance vectors (both with dimensions equal to (width Â· height)) and the decoder input is obtained by sampling from a normal distribution and projecting the code components. The complete Graph is as follows: As you can see, the only differences are as follows: The training process is identical to the first example in this chapter, as the sampling operations are performed directly by TensorFlow. The result after 200 epochs is shown in the following figure: As an exercise, I invite the reader to use RGB datasets (such as Cifar-10, which is found at https://www.cs.toronto.edu/~kriz/cifar.html) to test the generation ability of the VAE by comparing the output samples with the one drawn from the original distribution.", 'be78a065-f8ef-4b6e-b68f-5d612fee082d.xhtml': "In this chapter, we presented autoencoders as unsupervised models that can learn to represent high-dimensional datasets with lower-dimensional codes. They are structured into two separate blocks (which, however, are trained together): an encoder, responsible for mapping the input sample to an internal representation, and a decoder, which must perform the inverse operation, rebuilding the original image starting from the code. We have also discussed how autoencoders can be used to denoise samples and how it's possible to impose a sparsity constraint on the code layer to resemble the concept of standard dictionary learning. The last topic was about a slightly different pattern called a variational autoencoder. The idea is to build a generative model that is able to reproduce all the possible samples belonging to a training distribution. In the next chapter, we are going to briefly introduce a very important model family called generative adversarial networks (GANs), which are not very different from the purposes of a variational autoencoder, but which have a much more flexible approach.", '262ed42f-007e-4a7b-9042-740a7c801471.xhtml': 'In this chapter, we are going to provide a brief introduction to a family of generative models based on some game theory concepts. Their main peculiarity is an adversarial training procedure that is aimed at learning to distinguish between true and fake samples, driving, at the same time, another component that generates samples more and more similar to the\xa0training examples. In particular, we will be discussing:', 'a73195b6-45f3-49b3-a4dd-f32f7a9620dd.xhtml': "The brilliant idea of adversarial training, proposed by Goodfellow and others (in Generative Adversarial Networks,\xa0Goodfellow I. J., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y., arXiv:1406.2661 [stat.ML]), ushered in a new generation of generative models that immediately outperformed the majority of existing algorithms. All of the derived models are based on the same fundamental concept of adversarial training, which is an approach partially inspired by game theory. Let's suppose that we have a data generating process,\xa0pdata(x), that represents an actual data distribution and a finite number of samples that we suppose are drawn from pdata: Our goal is to train a model called a generator, whose distribution must be as close as possible to pdata. This is the trickiest part of the algorithm, because instead of standard methods (for example, variational autoencoders), adversarial training is based on a minimax game between two players (we can simply say that, given an objective, the goal of both players is to minimize the maximum possible loss; but in this case, each of them works on different parameters). One player is the generator, we can define as a parameterized function of a noise sample: The generator is fed with a noise vector (in this case, we have employed a uniform distribution, but there are no particular restrictions; therefore, we are simply going to say that z is drawn from a noise distribution pnoise), and outputs a value that has the same dimensionality of the samples drawn from pdata. Without any further control, the generator distribution will be completely different from the data generating process, but this is the moment for the other player to enter the scene. The second model is called the\xa0discriminator (or Critic), and it has the responsibility of evaluating the samples drawn from pdata and the ones produced by the generator: The role of this model is to output a probability that must reflect the fact that the sample is drawn from pdata, instead of being generated by G(z;\xa0θg). What happens is very simple: the first player (the generator) outputs a sample,\xa0x. If x actually belongs to pdata, the discriminator will output a value close to 1, while if it's very different from the other true samples, D(x;\xa0θd) will output a very low probability. The real structure of the game is based on the idea of training the generator to deceive the discriminator, by producing samples that can potentially be drawn from pdata. This result can be achieved by trying to maximize the log-probability,\xa0log(D(x;\xa0θd)), when x is a true sample (drawn from pdata), while minimizing the log-probability,\xa0log(1 - D(G(z;\xa0θg); θd)), with z sampled from a noise distribution. The first operation forces the discriminator to become more and more aware of the true samples (this condition is necessary to avoid being deceived too easily). The second objective is a little bit more complex, because the discriminator has to evaluate a sample that can be acceptable or not. Let's suppose that the generator is not smart enough, and outputs a sample that cannot belong to pdata. As the discriminator is learning how pdata is structured, it will very soon distinguish the wrong sample, outputting a low probability. Hence, by minimizing log(1 - D(G(z;\xa0θg); θd)), we are forcing the discriminator to become more and more critical when the samples are quite different from the ones drawn from pdata, and the becomes generator more and more able to produce acceptable samples. On the other hand, if the generator outputs a sample that belongs to the data generating process, the discriminator will output a high probability, and the minimization falls back in the previous case. The authors expressed this minimax game using a shared value function,\xa0V(G, D), that must be minimized by the generator and maximized by the discriminator: This formula represents the dynamics of a non-cooperative game between two players (for further information, refer to\xa0Tadelis S., Game Theory, Princeton University Press) that theoretically admits a special configuration, called a\xa0Nash equilibrium, that can be described by saying that if the two players know each other's strategy, they have no reason to change their own strategy if the other player doesn't. In this case, both the\xa0discriminator and generator will pursue their strategies until no change is needed, reaching a final, stable configuration, which is potentially a Nash equilibrium (even if there are many factors that can prevent reaching this goal). A common problem is the premature convergence of the discriminator, which forces the gradients to vanish because the loss function becomes flat in a region close to 0. As this is a game, a fundamental condition is the possibility to provide information to allow the player to make corrections. If the discriminator learns how to separate true samples from fake ones too quickly, the generator convergence slows down, and the player can remain trapped in a sub-optimal configuration. In general, when the distributions are rather complex, the discriminator is slower than the generator; but, in some cases, it is necessary to update the generator more times after each single discriminator update. Unfortunately, there are no rules of thumb; but, for example, when working with images, it's possible to observe the samples generated after a sufficiently large number of iterations. If the discriminator loss has become very small and the samples appear corrupted or incoherent, it means that the generator did not have enough time to learn the distribution, and it's necessary to slow down the discriminator. The authors in the aforementioned paper showed that, given a generator\xa0characterized by a distribution pg(x), the optimal discriminator is: At this point, considering the previous\xa0value function V(G, D) and using the optimal discriminator, we can rewrite it in a single objective (as a function of G) that must be minimized by the generator: To better understand how a GAN\xa0works, we need to expand the previous expression: Applying some simple manipulations, we get the following: The last term represents the Jensen-Shannon divergence between pdata and pg. This measure is similar to the Kullback-Leibler divergence, but it's symmetric and bounded between 0 and log(2). When the two distributions are identical, DJS = 0, but if their supports (the value sets where p(x) > 0) are disjoint,\xa0DJS\xa0= log(2) (while DKL =\xa0∞). Therefore, the value function can be expressed as: Now, it should be clearer that a GAN tries to minimize the Jensen-Shannon divergence between the data generating process and the generator distribution. In general, this procedure is quite effective; however, when the supports are disjointed, a GAN has no pieces of information about the true distance. This consideration (analyzed with more mathematical rigor in\xa0Improved Techniques for Training GANs,\xa0Salimans T., Goodfellow I., Zaremba W., Cheung V., Radford A.,\xa0and Chen X., arXiv:1606.03498 [cs.LG]) explains why training a GAN can become quite difficult, and, consequently, why the Nash equilibrium cannot be found in many cases. For these reasons, we are going to analyze an alternative approach in the next section. The complete GAN algorithm (as proposed by the authors) is:", 'e72a7a9d-268b-4015-ad2c-6f9203ba6124.xhtml': "In this example, we want to build a DCGAN (proposed in Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,\xa0Radford A., Metz L., Chintala S., , arXiv:1511.06434 [cs.LG]) with the Fashion-MNIST dataset (obtained through the keras helper function). As the training speed is not very high, we limit the number of samples to 5,000, but I suggest repeating the experiment with larger values. The first step is loading and normalizing (between -1 and 1) the dataset: According to the original paper, the generator is based on four transpose convolutions with kernel sizes equal to (4, 4) and strides equal to (2, 2). The input is a single multi-channel pixel (1 × 1 × code_length) that is expanded by subsequent convolutions. The number of filters is 1024, 512, 256, 128, and 1 (we are working with grayscale images). The authors suggest employing a symmetric-valued dataset (that's why we have normalized between -1 and 1), batch normalization after each layer, and leaky ReLU activation (with a default negative slope set to 0.2): The strides are set to work with 64\xa0× 64 images (unfortunately, the Fashion-MNIST dataset has\xa028 × 28 samples, which cannot be generated with power-of-two modules); therefore, we are going to resize the samples while training. As we need to compute the gradients of the discriminator and generator separately, it's necessary to set the variable scope (using the context manager tf.variable_scope()) to immediately extract only the variables whose names have the scope as a prefix (for example,\xa0generator/Conv_1_1/...). The\xa0is_training\xa0parameter is necessary to disable the batch normalization during the generation phase.\xa0 The discriminator is almost the same as a generator (the only main differences are the inverse convolution sequence and the absence of batch normalization after the first layer): In this case, we have an extra parameter (reuse_variables) that is necessary when building the loss functions. In fact, we need to declare two discriminators (fed with real samples and with the generator output), but they are not made up of separate layers; hence, the second one must reuse the variables defined by the first one. We can now create a\xa0graph and define all of the placeholders and operations: The first step is defining the placeholders: Then, we define the generator instance after reshaping the noise sample as a (1\xa0× 1\xa0× code_length) matrix (this is necessary to work efficiently with transpose convolutions). As this is a fundamental hyperparameter, I suggest testing different values and comparing the final performances. As explained previously, the input images are resized before defining the two discriminators (the second one reuses the variables previously defined). The\xa0\xa0discr_1_l\xa0instance is fed with the true samples, while\xa0discr_2_l works with the generator output.\xa0 The next step is defining the loss functions. As we are working with logarithms, there can be stability problems when the values become close to 0. For this reason, it's preferable to employ the TensorFlow built-in function tf.nn.sigmoid_cross_entropy_with_logits(), which guarantees numerical stability in every case. This function takes a logit as input and applies the sigmoid transformation internally. In general, the output is: Therefore, setting the label equal to 1 forces the second term to be null, and vice versa. At this point, we need to create two lists containing the variables belonging to each scope (this can be easily achieved by using the\xa0tf.trainable_variables()\xa0function, which outputs a list of all variables). The last step consists of defining the optimizers. As suggested in the official TensorFlow documentation, when working with batch normalizations, it's necessary to wrap the training operations in a context manager that checks whether all dependencies (in this case, batch average and variance) have been computed. We have employed the Adam optimizer with\xa0η = 0.0002, and a gradient momentum forgetting factor (μ1) equal to 0.5 (this is a choice motivated by the potential instability that a high momentum can yield). As it's possible to see, in both cases, the minimization is limited to a specific subset of the variables (providing a list through the\xa0var_list\xa0parameter). At this point, we can create a Session (we are going to use an InteractiveSession), initialize all variables, and start the training procedure (with 200 epochs and a batch size equal to 128): The training step (with a single discriminator iteration) is split into two phases: Once the training process has finished, we can generate some images (50) by executing the generator with a matrix of noise samples: The result is shown in the following screenshot: As an exercise, I invite the reader to employ more complex convolutional architectures and an RGB dataset such as CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html).\xa0", 'bd78e1e8-d6f6-404b-b403-e572661a0df0.xhtml': "As explained in the previous section, one of the most difficult problems with standard GANs is caused by the loss function based on the Jensen-Shannon divergence, whose value becomes constant when two distributions have disjointed supports. This situation is quite common with high-dimensional, semantically structured datasets. For example, images are constrained to having particular features in order to represent a specific subject (this is a consequence of the manifold assumption discussed in Chapter 2, Introduction to Semi-Supervised Learning). The initial generator distribution is very unlikely to overlap a true dataset, and in many cases, they are also very far from each other. This condition increases the risk of learning a wrong representation (a problem known as mode collapse), even when the discriminator is able to distinguish between true and generated samples (such a condition arises when the discriminator learns too quickly, with respect to the generator). Moreover, the Nash equilibrium becomes harder to achieve, and the GAN can easily remain blocked in a sub-optimal configuration. In order to mitigate this problem, Arjovsky, Chintala, and Bottou\xa0(in Wasserstein GAN,\xa0Arjovsky M., Chintala S., Bottou L., arXiv:1701.07875 [stat.ML]) proposed employing a different divergence, called the\xa0Wasserstein distance (or Earth Mover's distance), which is formally defined as follows: The term ∏(pdata, pg) represents the set of all possible joint probability distributions between pdata, pg. Hence, the Wasserstein distance is the infimum (considering all joint distributions) of the set of expected values of ||x - y||, where x and y are sampled from the joint distribution μ. The main property of DW is that, even when two distributions have disjointed support, its value is proportional to the actual distributional distance. The formal proof is not very complex, but it's easier to understand the concept intuitively. In fact, given two distributions with disjointed support, the infimum operator forces taking the shortest distance between each possible couple of samples. Clearly, this measure is more robust than the Jensen-Shannon divergence, but there's a practical drawback: it's extremely difficult to compute. As we cannot work with all possible joint distributions (nor with an approximation), a further step is necessary to employ this loss function. In the aforementioned paper, the authors proved that it's possible to apply a transformation, thanks to the Kantorovich-Rubinstein theorem (the topic is quite complex, but the reader can find further information in On the Kantorovich–Rubinstein Theorem,\xa0Edwards D. A., Expositiones Mathematicae, 2011): The first element to consider is the nature of f(•). The theorem imposes considering only L-Lipschitz functions, which means that\xa0f(•) (assuming a real-valued function of a single variable) must obey: At this point, the Wasserstein distance is proportional to the supremum (with respect to all\xa0L-Lipschitz functions) of the difference between two expected values, which are extremely easy to compute. In a WGAN, the\xa0f(•) function is represented by a neural network; therefore, we have no warranties about the\xa0Lipschitz condition. To solve this problem, the author suggested a very simple procedure: clipping the discriminator (which is normally called Critic, and whose responsibility is to represent the parameterized function f(•)) variables after applying the corrections. If the input is bounded, all of the transformations will yield a bounded output; however, the clipping factor must be small enough (0.01, or even smaller) to avoid the additive effect of multiple operations leading to an inversion of the Lipschitz condition. This is not an efficient solution (because it slows down the training process when it's not necessary), but it allows for exploiting the\xa0Kantorovich-Rubinstein theorem, even when there are no formal constraints imposed on the function family. Using a parameterized function (such as a Deep Convolutional Network), the Wasserstein distance becomes as follows (omitting the term L, which is constant): In the previous expression, we explicitly extracted the generator output, and in the last step, separated the term that will be optimized separately. The reader has probably noticed that the computation is simpler than a standard GAN because, in this case, we have to average over only the\xa0f(•) values of a batch (there's no more need for a logarithm). However, as the Critic variables are clipped, the number of required iterations is normally larger, and in order to compensate the difference between the training speeds of the Critic and generator, it's often necessary to set Ncritic > 1 (the authors suggest a value equal to 5, but this is a hyperparameter that must be tuned in every specific context).\xa0 The complete\xa0WGAN\xa0algorithm is:", '146d823a-63db-4127-90ed-6c201fdde1c2.xhtml': "This example can be considered a variant of the previous one because it uses the same dataset, generator, and discriminator. The only main difference is that in this case, the discriminator (together with its variable scope) has been renamed\xa0critic(): At this point, we can step directly to the creation of the Graph\xa0containing all of the placeholders, operations, and loss functions: As it's possible to see, there are no differences in the placeholder section, in the definition of the generator, and in the image resizing to the target dimensions of 64\xa0× 64. In the next block, we define the two Critic instances (which are perfectly analogous to the ones declared in the previous example). The two loss functions are simpler than a standard GAN, as they work directly with the Critic outputs, computing the sample mean over a batch. In the original paper, the authors suggest using RMSProp as the standard optimizer, in order to avoid the instabilities that a momentum-based algorithm can produce. However, Adam, with lower forgetting factors (μ1 = 0.5 and\xa0μ2 = 0.9) and a learning rate\xa0η = 0.00005, is faster than RMSProp, and doesn't lead to instabilities. I suggest testing both options, trying to maximize the training speed while preventing the mode collapse. Contrary to the previous example, in this case we need to clip all of the Critic variables after each training step. To avoid that, the internal concurrency can alter the order of some operations; it's necessary to employ a nested dependency control context manager. In this way, the actual training_step_c\xa0(responsible for clipping and reassigning the values to each variable) will be executed only after the optimizer_c step has completed. Now, we can create the InteractiveSession, initialize the variables, and start the training process, which is very similar to the previous one: The main difference is that, in this case, the Critic is trained n_critic times before each generator training step. The result of the generation of 50 random samples is shown in the following screenshot: As it's possible to see, the quality is slightly higher, and the samples are smoother. I invite the reader to also test this model with an RGB dataset, because the final quality is normally excellent.", '6d7f79a7-32a3-40a1-9fd1-734bcc48ea99.xhtml': "In this chapter, we discussed the main principles of adversarial training, and explained the roles of two players: the generator and discriminator. We described how to model and train them using a minimax approach whose double goal is to force the generator to learn the true data distribution pdata, and get the discriminator to distinguish perfectly between true samples (belonging to pdata) and unacceptable ones. In the same section, we analyzed the inner dynamics of a Generative Adversarial Network and some common problems that can slow down the training process and lead to a sub-optimal final configuration. One of the most difficult problems experienced with standard GANs arises when the data generating process and the generator distribution have disjointed support. In this case, the Jensen-Shannon divergence becomes constant and doesn't provide precise information about the distance. An excellent alternative is provided by the Wasserstein measure, which is employed in a more efficient model, called WGAN. This method can efficiently manage disjointed distributions, but it's necessary to enforce the L-Lipschitz condition on the Critic. The standard approach is based on clipping the parameters after each gradient ascent update. This simple technique guarantees the L-Lipschitz condition, but it's necessary to use very small clipping factors, and this can lead to a slower conversion. For this reason, it's normally necessary to repeat the training of the Critic a fixed number of times (such as five) before each single generator training step. In the next chapter, we are going to introduce another probabilistic generative neural model, based on a particular kind of neural network, called the Restricted Boltzmann Machine.", 'c22c7524-18f1-4342-8049-8dddb04e06be.xhtml': 'In this chapter, we are going to present two probabilistic generative models that employ a set of latent variables to represent a specific data generation process. Restricted Boltzmann Machines (RBMs), proposed in 1986, are the building blocks of a more complex model, called aÂ\xa0Deep Belief Network (DBN), which is capable of capturing complex relationships among features at different levels (in a way not dissimilar to a deep convolutional network). Both models can be used in unsupervised and supervised scenarios as preprocessors or, as is usual with DBN, fine-tuning the parameters using a standard backpropagation algorithm. In particular, we will discuss:', '050343c7-6e3a-40aa-9fc0-d42d0285a7a5.xhtml': "Let's consider a set of random variables,\xa0xi, organized in an undirected graph,\xa0G=(V, E), as shown in the\xa0following diagram:\xa0 Two random variables,\xa0a and b, are conditionally independent given the random variable, c if: Now, consider the graph again; if all generic couples of subsets of variables Si and Sj are conditionally independent given a separating subset,\xa0Sk (so that all connections between variables belonging to Si\xa0to variables belonging to Sj pass through Sk), the graph is called a\xa0Markov random field (MRF).\xa0 Given G=(V, E), a subset containing vertices such that every couple is adjacent is called a\xa0clique (the set of all cliques is often denoted as cl(G)). For example, consider the graph shown previously;\xa0(x0, x1) is a clique and if x0 and x5 were connected,\xa0(x0, x1, x5) would be a clique.\xa0A maximal clique is a clique that cannot be expanded by adding new vertices. A particular family of MRF is made up of all those graphs whose joint probability distribution can be factorized\xa0as: In this case,\xa0α is the normalizing constant and the product is extended to the set of all maximal cliques. According to the Hammersley–Clifford theorem (for further information, please refer to\xa0Proof of Hammersley-Clifford Theorem,\xa0Cheung S., University of Kentucky, 2008), if the joint probability density function is strictly positive, the MRF can be\xa0factorized and all the\xa0ρi\xa0functions are strictly positive too. Hence p(x), after some straightforward manipulations based on the properties of logarithms, can be rewritten as a Gibbs (or Boltzmann) distribution: The term E(x) is called energy, as it derives from the first application of such a distribution in statistical physics. 1/Z is now the normalizing constant employing the standard notation. In our scenarios, we always consider graphs containing observed (xi) and latent variables (hj). Therefore, it's useful to express the joint probability as: Whenever it's necessary to marginalize to\xa0obtain p(x), we can simply sum over hj:", 'efc93b43-874b-44e2-9bfc-7dc331dea58b.xhtml': "A RBM\xa0(originally called Harmonium) is a neural model proposed by Smolensky (in Information processing in dynamical systems: Foundations of harmony theory,\xa0Smolensky P., Parallel Distributed Processing, Vol 1, The MIT Press) that is made up of a layer of input (observable) neurons and a layer of hidden (latent) neurons. A generic structure is shown in the following diagram: As the undirected graph is bipartite (there are no connections between neurons belonging to the same layer), the underlying probabilistic structure is MRF. In the original model (even if this is not a restriction), all the neurons are assumed to be Bernoulli-distributed (xi, hi = {0, 1}), with a bias,\xa0bi (for the observed units) and cj (for the latent neurons). The resulting energy function is: A RBM is a probabilistic generative model that can learn a data-generating process,\xa0pdata, which is represented by the observed units but exploits the presence of the latent variables in order to model all the internal relationships. If we summarized all the parameters in a single vector,\xa0θ = {wij, bi, cj}, the Gibbs distribution becomes: The training goal of a RBM is to maximize the log-likelihood with respect to an input distribution. Hence, the first step is determining L(θ; x) after the marginalization of the previous expression: As we need to maximize the log-likelihood, it's useful to compute the gradient with respect to\xa0θ: Applying the chain rule of derivatives, we get: Using the conditional and joint probability equalities, the previous expression becomes: Considering the full joint probability, after some tedious manipulations (which we omit), it's possible to derive the following expressions (σ(•) is the sigmoid function): At this point, we can compute the gradient of the log-likelihood with respect to each single parameter,\xa0wij, bi, and cj. Starting with\xa0wij, and considering that ∇wij E(x, h;\xa0θ) = -xihj,\xa0we get: The expression can be rewritten as: Now, considering that all the units are Bernoulli-distributed, and isolating only the jth hidden unit, it's possible to apply the simplification: Therefore, the gradient becomes: Analogously, we can derive the gradient of L with respect to bi\xa0and cj: Hence, the first term of every gradient is very easy to compute, while the second one requires summing over all observed values. As this operation is impracticable, the only feasible alternative is an approximation based on sampling, using a method such as Gibbs sampling (for further information, see Chapter 4, Bayesian Networks and Hidden Markov Models). However, as this algorithm samples from the conditionals\xa0p(x|h) and p(h|x), rather than from the full joint distribution\xa0p(x, h), it requires the associated Markov chain to reach its stationary distribution,\xa0π, in order to provide valid samples. As we don't know how many sampling steps are required to reach\xa0π, Gibbs sampling can also be an unfeasible solution because of its potentially high computational cost. In order to solve this problem, Hinton proposed (in A Practical Guide to Training Restricted Boltzmann Machines,\xa0Hinton G., Dept. Computer Science, University of Toronto) an alternative algorithm called\xa0CD-k.\xa0The idea is very simple but extremely effective: instead of waiting for the Markov chain to reach the stationary distribution, we sample a fixed number of times starting from a training sample at t=0 x(0) and computing h(1) by sampling from p(h(1)|x(0)). Then, the hidden vector is employed to sample the reconstruction,\xa0x(2), from p(x(2)|h(1)). This procedure can be repeated any number of times, but in practice, a single sampling step is normally enough to ensure quite good accuracy. At this point, the gradient of the log-likelihood is approximated as (considering t steps): The single gradients with respect to\xa0wij, bi, and cj can be easily obtained considering the preceding procedure. The term contrastive derives from the approximation of the gradient of L computed at x(0)\xa0with a weighted difference between a term called the\xa0positive gradient and another defined as\xa0the negative gradient. This approach is analogous to the approximation of a derivative with this incremental ratio: The complete RBM training algorithm, based on a single-step CD-k is (assuming that there are M training samples): The outer product between two vectors is defined as: If vector a has an\xa0(n, 1)\xa0shape and b has an\xa0(m, 1)\xa0shape, the result is a matrix with a (n, m)\xa0shape.", '7e42d789-2292-4af7-9250-52ce7a5f936e.xhtml': "A Belief or Bayesian network is a concept already explored in Chapter 4, Bayesian Networks and Hidden Markov Models. In this particular case, we are going to consider Belief Networks where there are visible and latent variables, organized into homogeneous layers. The first layer always contains the input (visible) units, while all the remaining ones are latent. Hence, a DBN\xa0can be structured as a stack of RBMs, where each hidden layer is also the visible one of the subsequent RBM, as shown in the following diagram (the number of units can be different for each layer): The learning procedure is usually greedy and step-wise (as proposed in\xa0A fast learning algorithm for deep belief nets,\xa0Hinton G. E., Osindero S., Teh Y. W., Neural Computation, 18/7). The first RBM is trained with the dataset and optimized to reconstruct the original distribution using the CD-k algorithm. At this point, the internal (hidden) representations are employed as input for the next RBM, and so on until all the blocks are fully trained. In this way, the DBN is forced to create subsequent internal representations of the dataset that can be used for different purposes. Of course, when the model is trained, it's possible to infer from the recognition (inverse) model sampling from the hidden layers and compute the activation probability as (x represents a generic cause): As a DBN is always a generative process, in an unsupervised scenario, it can perform a component analysis/dimensionality reduction with an approach that is based on the idea of creating a chain of sub-processes, which are able to rebuild an internal representation. While a single RBM focuses on a single hidden layer and hence cannot learn sub-features, a DBN greedily learns how to represent each sub-feature vector using a refined hidden distribution. The concept behind this process is not very different from a cascade of convolutional layers, with the main difference that in this case, the learning procedure is greedy. Another distinction with methods such as PCA is that we don't know exactly how the internal representation is built. As the latent variables are optimized by maximizing the log-likelihood, there are possibly many optimal points but we cannot easily impose constraints on them. However, DBNs show very powerful properties in different scenarios, even if their computational cost is normally considerably higher than other methods. One of the main problems (common to the majority of deep learning methods) concerns the right choice of hidden units in every layer. As they represent latent variables, their number is a crucial factor for the success of a training procedure. The right choice is not immediate, because it's necessary to know the complexity of the data-generating process, however, as a rule of thumb, I suggest starting with a couple of layers containing 32/64 units and proceeding to increase the number of hidden neurons and the layers until the desired accuracy is reached (in the same way, I suggest starting with a small learning rate, for example, 0.01 -, increasing it if necessary). As the first RBM is responsible for reconstructing the original dataset, it's very useful to monitor the log-likelihood (or the error) after each epoch in order to understand whether the process is learning correctly (decreasing error) or it's saturating the capacity. It's clear that an initial bad reconstruction leads to subsequently worse representations. As the learning process is greedy, in an unsupervised task there's no way to improve the performance of lower layers when the previous training steps are finished therefore, I always suggest tuning up the parameters so that the first reconstruction is very accurate. Of course, all the considerations about overfitting are still valid, so, it's also important to monitor the generalization ability with validation samples. However, in a component analysis, we assume we're working with a distribution that is representative of the underlying data-generating process, so the risk of finding before-seen features should be minimal. In a supervised scenario, there are generally two options whose first step is always a greedy training of the DBN. However, the first approach performs a subsequent refinement using a standard algorithm, such as backpropagation (considering the whole architecture\xa0as a single deep network), while the second one uses the last internal representation as the input of a separate classifier. It goes without saying that the first method has many more degrees of freedom because it works with a pre-trained network whose weights can be adjusted until the validation accuracy reaches its maximum value. In this case, the first greedy step works with the same assumption that has been empirically confirmed by observing the internal behavior of deep models (similar to convolutional networks). The first layers learn how to detect low-level features, while all the subsequent ones increase the details. Therefore, the backpropagation step presumably starts from a point that is already quite close to the optimum and can converge more quickly. Conversely, the second approach is analogous to applying the kernel trick to a standard Support Vector Machine (SVM). In fact, the external classifier is generally a very simple one (such as a logistic regression or an SVM) and the increased accuracy is normally due to an improved linear separability obtained by projecting the original samples onto a sub-space (often higher-dimensional) where they can be easily classified. In general, this method yields worse performance than the first one because there's no way to tune up the parameters once the DBN is trained. Therefore, when the final projections are not suitable for a linear classification, it's necessary to employ more complex models and the resulting computational cost can be very high without a proportional performance gain. As deep learning is generally based on the concept of end-to-end learning, training the whole network can be useful to implicitly include the pre-processing steps in the complete structure, which becomes a black box that associates input samples with specific outcomes. On the other hand, whenever an explicit pipeline is requested, greedy-training the DBN and employing a separate classifier could be a more suitable solution.", 'cdb48a3c-1020-4120-8652-f69f6693a2b0.xhtml': "In this example, we are going to use a Python library freely available on GitHub (https://github.com/albertbup/deep-belief-network) that allows working with supervised and unsupervised DBN using NumPy (CPU-only) or Tensorflow (CPU or GPU support) with the standard Scikit-Learn interface. Our goal is to create a lower-dimensional representation of a subset of the mnist dataset (as the training process can be quite slow, we'll limit it to 400 samples). The first step is loading (using the Keras helper function), shuffling, and normalizing the dataset: At this point, we can create an instance of the the\xa0UnsupervisedDBN\xa0class, setting three layers with respectively 512, 256, and 64 sigmoid units (as we want to bind the values between 0 and 1). The learning rate,\xa0η (learning_rate_rbm), is set equal to 0.05, the batch size (batch_size) to 64, and the number of epochs for each RBM (n_epochs_rbm) to 100. The default value for the number of CD-k steps is 1, but it's possible to change it using the\xa0contrastive_divergence_iter\xa0parameter: Once the training process is complete, the\xa0X_dbn\xa0array contains the values sampled from the last hidden layer. Unfortunately, this library doesn't implement an inverse transformation method, but we can use the t-SNE algorithm to project the distribution onto a bidimensional space: The corresponding plot is shown in the following graph:  As you can see, even if there are still a few anomalies, the hidden low-dimensional representation is globally coherent with the original dataset because the group containing the same digits is organized in compact clusters that preserve some geometrical properties. For example, the group containing the digits representing a 1 is very close to the one containing the images of 7s, as well as the groups of\xa03s and 8s. This result confirms that a DBN can be successfully employed as a preprocessing layer for classification purposes, but in this case, rather than reducing the dimensionality, it's often preferable to increase it, in order to exploit the redundancy to use a simpler linear classifier (to better understand this concept, think about augmenting a dataset with polynomial features). I invite you to test this ability by preprocessing the whole MNIST dataset and then classifying it using a logistic regression, comparing the results with a direct approach.", 'ace58fee-09e0-48c2-8eb7-9c77cb48ad41.xhtml': "In this example, we are going to employ the KDD Cup '99 dataset (provided by Scikit-Learn), which contains the logs generated by an intrusion detection system exposed to normal and dangerous network activities. We are focusing only on the smtp sub-dataset, which is the smallest one, because, as explained before, the training process can be very long. This dataset is not extremely complex and it can be successfully classified with simpler methods; however, the example has only a didactic purpose and can be useful for understanding how to work with this kind of data. The first step is to load the dataset, encode the labels (which are strings), and standardize the values: At this point, we can create train and test sets: The model is based on an instance of the\xa0SupervisedDBNClassification\xa0class, which implements the backpropagation method. The parameters are very similar to the unsupervised case, but now we can also specify the stochastic gradient descent (SGD) learning rate (learning_rate), the number of backpropagation epochs (n_iter_backprop), and an optional dropout (dropout_p). The algorithm performs an initial greedy training (whose computational cost is normally higher than the SGD phase), followed by a fine-tuning: Once the training process is finished, we can evaluate\xa0performance on the test set: The validation accuracy is 1.0 (there are no misclassifications), but this is really a simple dataset that needs only a few minutes of training. I invite you to test the performance of a DBN in the classification of the MNIST/Fashion MNIST dataset, comparing the results with the one obtained using a deep convolutional network. In this case, it's important to monitor the reconstruction error of each RBM, trying to minimize it before running the backpropagation phase. At the end of this exercise, you should be able to answer this question: which is preferable, an end-to-end or a preprocessing-based approach?", 'dba16222-e53e-4dfd-81e8-86233f53b8a6.xhtml': 'In this chapter, we presented the MRF as the underlying structure of an RBM. An MRF is represented as an undirected graph whose vertices are random variables. In particular, for our purposes, we considered MRFs whose joint probability can be expressed as a product of the positive functions of each random variable. The most common distribution, based on an exponential, is called the Gibbs (or Boltzmann) distribution and it is particularly suitable for our problems because the logarithm cancels the exponential, yielding simpler expressions. An RBM is a simple bipartite, undirected graph, made up of visible and latent variables, with connections only between different groups. The goal of this model is to learn a probability distribution, thanks to the presence of hidden units that can model the unknown relationships. Unfortunately, the log-likelihood, although very simple, cannot be easily optimized because the normalization term requires summing over all the input values. For this reason, Hinton proposed an alternative algorithm, called CD-k, which outputs an approximation of the gradient of the log-likelihood based on a fixed number (normally 1) of Gibbs sampling steps. Stacking multiple RBMs allows modeling DBNs, where the hidden layer of each block is also the visible layer of the following one. DBN can be trained using a greedy approach, maximizing the log-likelihood of each RBM in sequence. In an unsupervised scenario, a DBN is able to extract the features of a data-generating process in a hierarchical way, and therefore the application includes component analysis and dimensionality reduction. In a supervised scenario, a DBN can be greedily pre-trained and fine-tuned using the backpropagation algorithm (considering the whole network) or sometimes using a preprocessing step in a pipeline where the classifier is generally a very simple model (such as a logistic regression). \xa0In the next chapter, Chapter 14,\xa0Introduction to Reinforcement Learning, we are going to introduce the concept of reinforcement learning, discussing the most important elements of systems that can autonomously learn to play a game or allow a robot to walk, jump, and perform tasks that are extremely difficult to model and control using classic methods.', 'db25bdb4-e426-44fe-ac6d-f111fa70894f.xhtml': 'In this chapter, we are going to introduce the fundamental concepts of Reinforcement Learning (RL), which is a set of approaches that allows an agent to learn how to behave in an unknown environment, thanks to the rewards that are provided after each possible action. RL has been studied for decades, but it has reached a very high maturity level in the last few years when it became possible to employ deep learning models together with standard (and often simple) algorithms in order to solve extremely complex problems (such as learning how to play an Atari game perfectly). In particular, we will discuss:', '2d6c9526-f03a-431e-8f65-d0279685baec.xhtml': "Imagine that you want to learn to ride a bike and ask a friend for advice. They explain how the gears work, how to release the brake and a few other technical details. In the end, you ask the secret to keeping balanced. What kind of answer do you expect? In an imaginary supervised world, you should be able to perfectly quantify your actions and correct the errors by comparing the outcomes with precise reference values. In the real world, you have no idea about the quantities underlying your actions and, above all, you will never know what the right value is. Increasing the level of abstraction, the scenario we're considering can be described as: a generic agent performs actions inside an environment and receives\xa0feedback that is somehow proportional to the competence of its actions. According to this feedback, the agent can correct its actions in order to reach a specific goal. This basic schema is represented in the following diagram: Returning to our initial example, when you ride a bike for the first time and try to keep your balance, you will notice that the wrong movement causes an increase in the slope, which in turn increases the horizontal component of the gravity force, pushing the bike laterally. As the vertical component is compensated, the result is a rotation that ends when the bike falls down completely. However, as you can use your legs to\xa0control the balance, when the bike starts falling, thanks to Newton's third law, the force on the leg increases and your brain understands that it's necessary to make a movement in the opposite direction. Even if this problem can be easily expressed in terms of physical laws, nobody learns to ride a bike by computing forces and momentums. This is one of the main concepts of RL: an agent must always make its choices considering a piece of information, usually defined as a reward, that represents the response, provided by the environment. If the action is correct, the reward will be positive, otherwise, it will be negative. After receiving a reward, an agent can fine-tune the strategy, called policy, in order to maximize the expected future reward. For example, after a few rides, you will be able to slightly move your body so as to keep the balance while turning, but probably, in the beginning, you needed to extend your leg to avoid falling down. Hence, your initial policy suggested a wrong action, which received repeated negative rewards and so your brain corrected it by increasing the probability of choosing another action. The implicit hypothesis that underlies this approach is that an agent is always rational, meaning that its goal is to maximize the expected return of its actions (nobody would like to fall down just to feel a different emotion). Before discussing the single components of an RL system, it's necessary to add a couple of fundamental assumptions. The first one is that an agent can repeat the experiences an infinite number of times. In other words, we assume that it's possible to learn a valid policy (possibly the optimal one) only if we have enough time. Clearly, this is unacceptable in the animal world and we all know that many experiences are extremely dangerous; however, this assumption is necessary to prove the convergence of some algorithms. Indeed, sub-optimal policies sometimes can be learned very quickly, but it's necessary to iterate many times to reach the optimal one. In real artificial systems, we always stop the learning process after a finite number of iterations, but it's almost impossible to find valid solutions if some experiences prevent the agent from continuing to interact with the environment. As many tasks have final states (either positive or negative), we assume that the agent can play any number of episodes (somewhat analogous to the epochs of supervised learning), exploiting the experience previously learned. The second assumption is a little bit more technical and it's usually known as the\xa0Markov property. When the agent interacts with the environment, it observes a sequence of states. Even if it can seem like an oxymoron, we assume that each state is stateful. We can explain this concept with a simple example; suppose that you're filling a tank and every five seconds you measure the level. Imagine that at t = 0, the level L = 10 and the water is flowing in. What do you expect at t = 1? Obviously, L > 10. In other words, without external unknown causes, we assume that a state contains the previous history, so that the sequence, even if discretized, represents a continuous evolution where no jumps are allowed. When an RL task satisfies this property, it's called a Markov Decision Process\xa0and it's very easy to employ simple algorithms to evaluate the actions. Luckily, the majority of natural events can be modeled as MDPs (when you're walking toward a door, every step in the right direction must decrease the distance), but there are some games that are implicitly stateless. For example, if you want to employ an RL algorithm to learn how to guess the outcome of a probabilistic sequence of independent events (such as tossing a coin), the result could be dramatically wrong. The reason is clear: any state is independent of the previous ones and every attempt to build up a history is a failure. Therefore, if you observe a sequence of 0, 0, 0, 0, ... you are not justified in increasing the value of betting on 0 unless, after considering the likelihood of the events, you suppose that the coin is loaded. However, if there's no reason to do so, the process isn't an MDP\xa0and every episode (event) is completely independent. All the assumptions that we, either implicitly or explicitly, make are based on this fundamental concept, so pay attention when evaluating new, unusual scenarios because you may discover that the employment of a specific algorithm isn't theoretically justified.", '1253174e-8670-4278-9192-ba47789b9440.xhtml': "The environment is the entity where the agent has to reach its goals. For our purposes, a generic environment is a system that receives an input action,\xa0at (we use the index t because this is a natural time process), and outputs a tuple composed by a state,\xa0st+1, and a reward,\xa0rt+1. These two elements are the only pieces of information provided to the agent to make its next decision. If we are working with an MDP and the sets of possible actions, A, and states, S, are discrete and finite, the problem is a defined finite MDP (in many continuous cases, it's possible to treat the problem as a finite MDP by discretizing the spaces). If there are final states, the task is called episodic and, in general, the goal is to reach a positive final state in the shortest amount of time or maximize a score. The schema of the cyclic interaction between agent an environment is shown in the following diagram: A very important feature of an environment is its internal nature. It can be either deterministic or stochastic. A deterministic environment is characterized by a function that associates each possible action,\xa0at, in a specific state,\xa0st, to a well-defined successor,\xa0st+1, with a precise reward,\xa0rt+1: Conversely, a stochastic environment is characterized by a transition probability between the current state,\xa0st, and a set of possible successors,\xa0sit+1, given an action,\xa0at: If a state,\xa0si, has a transitional probability,\xa0T(si, si, at) = 1\xa0∀ at\xa0∈ A, the state is defined as absorbing. In general, all ending states in episodic tasks are modeled as\xa0absorbing ones, to avoid any further transition. When an episode is not limited to a fixed number of steps, the only criterion to determine its end is to check whether the agent has reached an absorbing state. As we don't know which state will be the successor, it's necessary to consider the expected value of all possible rewards considering the initial state,\xa0st, and the action,\xa0at: In general, it's easier to manage stochastic environments because they can be immediately converted into deterministic ones by setting all probabilities to zero except the one corresponding to the actual successor (for example, T(•) = (0, 0, ..., 1, ..., 0)). In the same way, the expected return can be set equal to rt+1. The knowledge of T(•), as well as E[rit+1], is necessary to employ some specific algorithms, but it can become problematic when finding a suitable model for the environment requires an extremely complex analysis. In all those cases, model-free methods can be employed and, therefore, the environment is considered as a black-box, whose output at time,\xa0t (subsequent to an action performed by the agent,\xa0at-1), is the only available piece of information for the evaluation of a policy.", 'e021767e-03a4-49d4-a790-2fff16509e0f.xhtml': "We have seen that rewards (sometimes negative rewards are called penalties, but it's preferable to use a standardized notation) are the only feedback provided by the environment after each action. However, there are two different approaches to the use of rewards. The first one is the strategy of a very short-sighted agent and consists in taking into account only the reward just received. The main problem with this approach is clearly the inability to consider longer sequences that can lead to a very high reward. For example, an agent has to traverse a few states with negative reward (for example, -0.1), but after them, they arrive at a state with a very positive reward (for example, +5.0). A short-sighted agent couldn't\xa0 find out the best policy because it will simply try to avoid the immediate negative rewards. On the other side, it's better to suppose that a single reward contains a part of the future rewards that will be obtained following the same policy. This concept can be expressed by introducing a discounted reward, which is defined as: In the previous expression, we are assuming an infinite horizon with a discount factor, γ, which is a real number bounded between 0 and 1 (not included). When\xa0γ = 0, the agent is extremely short-sighted, because of Rt = rt+1, but when\xa0γ\xa0→ 1, the current reward takes into account the future contributions discounted in a way that is inversely proportional to the time-step. In this way, very close rewards will have a higher weight than very distant ones. If the absolute value of all rewards is limited by a maximum immediate absolute reward, |ri|\xa0≤ |rmax|, the previous expression will be always bounded. In fact, considering the properties of a geometric series, we get: Clearly, the right choice of\xa0γ is a crucial factor in many problems and cannot be easily generalized. As in many other similar cases, I suggest testing different values, picking the one that minimizes the convergence speed while yielding a quasi-optimal policy. Of course, if the tasks are episodic with length,\xa0T(ei), the discounted reward becomes:", 'a7c26518-ec7a-4cac-9dfd-26be61617fe9.xhtml': "We are going to consider an example based on a checkerboard environment representing a tunnel. The goal of the agent is to reach the ending state (lower-right corner), avoiding 10 wells that are negative absorbing states. The rewards are: Selecting a small negative reward for all non-terminal states is helpful to force the agent to move forward until the maximum (final) reward has been achieved. Let's start modeling an environment that has a 5\xa0× 15 matrix: The graphical representation of the environment (in terms of rewards) is shown in the following chart: The agent is allowed to move in four directions: up, down, left, and right. Clearly, in this case, the environment is deterministic because\xa0every action moves the agent to a predefined cell. We assume that whenever an action is forbidden (such as trying to move on the left when the agent is in the first column), the successor state is the same one (with the corresponding reward).", '4bfcc7c4-35ee-4937-9705-df81c25c568e.xhtml': "A policy is formally a deterministic or stochastic law that the agent follows in order to maximize its return. Conventionally, all policies are denoted with the letter\xa0π. A deterministic policy is usually a function of the current state that outputs a precise action: A stochastic policy, analogously to environments, outputs the probability of each action (in this case, we are assuming we work with a finite MPD): However, contrary to the environment, an agent must always pick a specific action, transforming any stochastic policy into a deterministic sequence of choices. In general, a policy where π(s, a) > 0 ∀ a ∈ A, is called soft and it's often very useful during the training process because it allows a more flexible modeling without the premature selection of a suboptimal action. Instead, when\xa0π(s, ai) = 0\xa0∀ i\xa0≠ j and\xa0π(s, aj) = 1, the policy is also defined as\xa0hard.\xa0This transformation can be performed in many ways, but the most common one is to define a policy that is greedy with respect to a value (we're going to discuss this concept in the next section). This means that, at every step, the policy will select the action that maximizes the value of the successor state. Obviously, this is a very rational approach, which could be too pragmatic. In fact, when the values of some states don't change, a greedy policy will always force the agent to perform the same actions. Such a problem is known as the exploration-exploitation dilemma and arises when it would be better to allow the agent to evaluate alternative strategies that could appear initially to be suboptimal. In other words, we want the agent to explore the environment before starting to exploit the policy, to know whether the policy is really the best one or if there are hidden alternatives. To solve this problem, it's possible to employ an\xa0ε-greedy policy, where the value,\xa0ε, is called the\xa0exploration factor and represents a probability. In this case, the policy will pick a random action with probability\xa0ε and a greedy one with probability 1 -\xa0ε. In general, at the beginning of the training process,\xa0ε is kept very close to 1.0 to incentivize the exploration and it's progressively\xa0decreased when the policy becomes more stable. In many Deep RL applications, this approach is fundamental, in particular, when there are no models of the environment. The reason is that greedy policies can be initially wrong and it's necessary to allow the agent to explore many possible state and action sequences before forcing a deterministic decision.", 'c3bbe867-f650-4428-8bd1-741ff53d5ed1.xhtml': "In this section, we are going to analyze a strategy to find an optimal policy based on a complete knowledge of the environment (in terms of transition probability and expected returns). The first step is to define a method that can be employed to build a greedy policy. Let's suppose we're working with a finite MDP and a generic policy, π; we can define the intrinsic value of a state,\xa0st, as the expected discounted return obtained by the agent starting from st and following the stochastic policy,\xa0π: In this case, we are assuming that, as the agent will follow\xa0π, state sa is more useful than sb if the expected return starting from sa is greater than the one obtained starting from sb. Unfortunately, trying to directly find the value of each state using the previous definition is almost impossible when\xa0γ > 0. However, this a problem that can be solved using Dynamic Programming (for further information, please refer to Dynamic Programming and Markov Process, Ronald A. Howard, The MIT Press), which allows us to solve the problem iteratively. In particular, we need to turn the previous formula into a Bellman equation: The first term on the right-hand side can be expressed as: In other words, it is the weighted average of all expected returns considering that the agent is state,\xa0st, and evaluates all possible actions and the consequent state transitions. For the second term, we need a small trick. Let's suppose we start from st+1, so that the expected value corresponds to V(st+1;π); however, as the sum starts from st, we need to consider all possible transitions starting from st.\xa0In this case, we can rewrite the term as: Again, the first terms take into account all possible transitions starting from st (and ending in\xa0st+1), while the second one is the value of each ending state.\xa0Therefore the complete expression becomes: For a deterministic policy, instead, the formula is: The previous equations are particular cases of a generic discrete Bellman equation for a finite MDP that can be expressed as a vectorial operator,\xa0Lπ, applied to the value vector: It's easy to prove that there exists a unique fixed point that corresponds to V(s;\xa0π), so\xa0Lπ\xa0V(s;\xa0π) =\xa0V(s;\xa0π). However, in order to solve the system, we need to consider all equations at the same time because, both on the left-hand and on the right-hand side of the Bellman equation, there is the\xa0V(•;\xa0π) term. Is it possible to transform the problem into an iterative procedure, so that a previous computation can be exploited for the following one? The answer is yes and it's the consequence of an important property of\xa0Lπ. Let's consider the infinity norm of the difference between two value vectors computed at time t and t+1: As the discount factor\xa0γ\xa0∈ [0, 1[, the Bellman operator,\xa0Lπ, is a\xa0γ-contraction that reduces the distance between the arguments by a factor of γ (they get more and more similar). The Banach Fixed-Point Theorem states that a contraction,\xa0L: D\xa0→ D, on a metric space,\xa0D, admits a unique fixed point,\xa0d*\xa0∈ D, that can be found by repeatedly applying the contraction to any d(0)\xa0∈ D. Hence, we know about the existence of a unique fixed point,\xa0V(s;\xa0π), that is the goal of our research. If we now consider a generic starting point,\xa0V(t), and we compute the norm of the difference with\xa0V(s;\xa0π), we obtain: Repeating this procedure iteratively until t = 0, we get: The term\xa0γt+1\xa0→ 0, while continuing the iterations over the distance between V(t) and\xa0V(s;\xa0π), gets smaller and smaller, authorizing us to employ the iterative approach instead of the one-shot closed method. Hence, the Bellman equation becomes: This formula allows us to find the value for each state (the step is formally called policy evaluation), but, of course, it requires a policy. At the first step, we can randomly select the actions because we don't have any other piece of information, but after a complete evaluation cycle, we can start defining a greedy policy with respect to the values. In order to achieve this goal, we need to introduce a very important concept in RL, the Q function\xa0(which must not be confused with the\xa0Q function defined in the EM algorithm), which is defined as the expected discounted return obtained by an agent starting from the state,\xa0st, and selecting a specific action,\xa0at: The definition is very similar to\xa0V(s;\xa0π), but, in this case, we include the action, at, as a variable. Clearly, it's possible to define a Bellman equation for Q(s, a; π) by simply removing the policy/action summation: Sutton and Barto (in\xa0Reinforcement Learning,\xa0Sutton R. S.,\u200e Barto A. G.,\xa0The MIT Press) proved a simple but very important theorem (called the\xa0Policy improvement theorem), which states that given the deterministic policies,\xa0π1 and\xa0π2, if Q(s,\xa0π2(s);\xa0π2)\xa0≥ V(s;\xa0π1)\xa0∀ s\xa0∈ S, then\xa0π2 is better than or equal to\xa0π1. The proof is very compact and can be found in their book, however, the result can be understood intuitively. If we consider a sequence of states,\xa0s1\xa0→ s2\xa0→ ...\xa0→ sn and\xa0π2(si) =\xa0π1(si)\xa0∀ i < m < n, while\xa0π2(si)\xa0≥ π1(si)\xa0∀ i\xa0≥ m, the policy,\xa0π2, is at least equal to\xa0π1 and it's become better if at least an inequality is strict. Conversely, if\xa0Q(s,\xa0π2(s);\xa0π2)\xa0≥ V(s;\xa0π1), this means that\xa0π2(s)\xa0≥ π1(s) and, again,\xa0Q(s,\xa0π2(s);\xa0π2) > V(s;\xa0π1) if there's at least a state,\xa0si, where\xa0π2(si) > π1(si). Hence, after a complete policy evaluation cycle, we are authorized to define a new greedy policy as: This step is called policy improvement and its goal is to set the action associated with each state as the one that leads to the transition to the successor state with the maximum value. It's not difficult to understand that an optimal policy will remain stable when\xa0V(t)\xa0→\xa0V(s;\xa0π). In fact, when t\xa0→\xa0∞, the Q function will converge to a stable fixed point determined by\xa0V(s;\xa0π) and the argmax(•) will always select the same actions. However, if we start with a random policy, in general, a single policy evaluation cycle isn't enough to assure the convergence. Therefore, after a policy improvement step, it's often necessary to repeat the evaluation and continue alternating the two phases until the policy becomes stable (that's why the algorithm is called policy iteration). In general, the convergence is quite fast, but the actual speed depends on the nature of the problem, the number of states and actions, and the consistency of the rewards. The complete policy iteration algorithm (as proposed by Sutton and Barto) is:", '2e4ff611-2b59-4344-af3f-ed4886dfdbb5.xhtml': "We want to apply the policy iteration algorithm in order to find an optimal policy for the tunnel environment. Let's start by defining a random initial policy and a value matrix with all values (except the terminal states) equal to 0: The initial random policy (t=0) is shown in the following chart: The states denoted with\xa0⊗ represent the wells, while the final positive one is represented by the capital letter E. Hence, the initial value matrix (t=0) is: At this point, we need to define the functions to perform the policy evaluation and improvement steps. As the environment is deterministic, the processes are slightly simpler because\xa0the generic transition probability,\xa0T(si, sj; ak), is equal to 1 for the only possible successor and 0 otherwise. In the same way, the policy is deterministic and only a single action is taken into account. The policy evaluation step is performed, freezing the current values and updating the whole matrix,\xa0V(t+1), with V(t); however, it's also possible to use the new values immediately. I invite the reader to test both strategies in order to find the fastest way. In this example, we are employing a discount factor, γ = 0.9 (it goes without saying that an interesting exercise consists of testing different values and comparing the result of the evaluation process and the final behavior): Once the functions have been defined, we start the policy iteration cycle (with a maximum number of epochs,\xa0Niter = 100,000, and a tolerance threshold equal to 10-5): At the end of the process (in this case, the algorithm converged after 182 iterations, but this value can change with different initial policies), the value matrix is: Analyzing the values, it's possible to see how the algorithm discovered that they are an implicit function of the distance between a cell and the ending state. Moreover, the policy always avoids the wells because the maximum value is always found in an adjacent state. It's easy to verify this behavior by plotting the final policy: Picking a random initial state, the agent will always reach the ending one, avoiding the wells and confirming the optimality of the policy iteration algorithm.", 'a9bb2e98-ad17-42c2-997c-f435b69a5db5.xhtml': "An alternative approach to policy iteration is provided by the value iteration algorithm. The main assumption is based on the empirical observation that the policy evaluation step converges rather quickly and it's reasonable to stop the process after a fixed number of steps (normally 1). In fact, policy iteration can be imagined like a game where the first player tries to find the correct values considering a stable policy, while the other one creates a new policy that is greedy with respect to the new values. Clearly, the second step compromises the validity of the previous evaluation, forcing the first player to repeat the process. However, as the Bellman equation uses a single fixed point, the algorithm converges to a solution characterized by the fact that the policy doesn't change anymore and, consequently, the evaluation becomes stable. This process can be simplified by removing the policy improvement step and continuing the evaluation in a greedy fashion. Formally, each step is based on the following update rule: Now the iteration doesn't consider the policy anymore (assuming implicitly that it will be greedy with respect to the values), and selects V(t+1) as the maximum possible value among all V(t)(at). In other words, value iteration anticipates the choice that is made by the policy improvement step by selecting the value that corresponds to the action that is likely (p\xa0→ 1) to be selected. It's not difficult to extend the convergence proof presented in the previous section to this case, therefore, V(∞) → V(opt), as well as policy iteration does. However, the average number of iterations is normally smaller because we are starting with a random policy that can contrast the value iteration process. When the values become stable, the optimal greedy policy is simply obtained as: This step is formally equivalent to a policy improvement iteration, which, however, is done only once at the end of the process. The complete value iteration algorithm (as proposed by Sutton and Barto) is:", 'caafe229-e963-4845-89b1-1ac93bf303a2.xhtml': "To test this algorithm, we need to set an initial value matrix with all values equal to 0 (they can be also randomly chosen but, as we don't have any prior information on the final configuration, every initial choice is probabilistically equivalent): At this point, we can define the two functions to perform the value evaluation and the final policy selection (the function is_final() is the one defined in the previous example): The main differences are in the value_evaluation() function, which now has to consider all possible successor states and select the value corresponding to the action that leads to the state with the highest value. Instead, the policy_selection() function is equivalent to policy_improvement(), but, as it is invoked only once, it outputs directly to the final optimal policy. At this point, we can run a training cycle (assuming the same constants as before): The final value configuration (after 127 iterations) is shown in the following chart: As in the previous example, the final value configuration is a function of the distance between each state and the ending one, but, in this case, the choice of\xa0γ = 0.9 isn't optimal. In fact, the wells close to the final state aren't considered very dangerous anymore. Plotting the final policy can help us understand the behavior: As expected, the wells that are far from the target are avoided, but the two that are close to the final state are accepted as reasonable penalties. This happens because the value iteration algorithm is very greedy with respect to the value and the discount factor,\xa0γ < 1.0; the effect of negative states can be compensated for by the final reward. In many scenarios, these states are absorbing, therefore their implicit reward is +∞ or -∞, meaning that no other actions can change the final value. I invite the reader to repeat the example with different discount factors (remember that an agent with\xa0γ\xa0→ 1 is very short-sighted and will avoid any obstacle, even reducing the efficiency of the policy) and change the values of the final states. Moreover, the reader should be able to answer the question: What is the agent's behavior when the standard reward (whose default value is -0.1) is increased or decreased?", '6e1cefc6-0f06-40ed-981c-d740ed0e67ff.xhtml': "One of the problems with Dynamic Programming algorithms is the need for a full knowledge of the environment in terms of states and transition probabilities. Unfortunately, there are many cases where these pieces of information are unknown before the direct experience. In particular, the states can be discovered by letting the agent explore the environment, but the transition probabilities require us to count the number of transitions to a certain state and this is often impossible. Moreover, an environment with absorbing states can prevent visiting many states if the agent has learned a good initial policy. For example, in a game, which can be described as an episodic MDP, the agent discovers the environment while learning how to move forward without ending in a negative absorbing state. A general solution to these problems is provided by a different evaluation strategy, called Temporal Difference (TD)\xa0RL. In this case, we start with an empty value matrix and we let the agent follow a greedy policy with respect to the value (but the initial one, which is generally random). Once the agent observes a transition,\xa0si\xa0→ sj, due to an action,\xa0at, with a reward,\xa0rij, it updates the estimation of V(si). The process is structured in episodes (which is the most natural way) and ends when a maximum number of steps have been done or a terminal state is met. In particular, the TD(0) algorithm updates the value according to the rule: The constant,\xa0α, is bound between 0 and 1 and acts as a learning rate. Each update considers a variation with respect to the current value,\xa0V(t)(si), which is proportional to the difference between the actual return and the previous estimation. The term rij + γV(t)(sj) is analogous to the one employed in the previous methods and represents the expected value given the current return and the discounted value starting from the successor state. However, as\xa0V(t)(sj) is an estimation, the process is based on a bootstrap from the previous values. In other words, we start from an estimation to determine the next one, which should be closer to the stable fixed point. Indeed, TD(0) is the simplest example of a family of TD algorithms that are based on a sequence (usually called backup) that can be generalized as (considering k steps): As we're using a single reward to approximate the expected discounted return, TD(0) is usually called a one-step TD method (or one-step backup). A more complex algorithm can be built considering more subsequent rewards or alternative strategies. We're going to analyze a generic variant called TD(λ) in Chapter 15, Advanced Policy Estimation Algorithms\xa0and explain why this algorithm corresponds to a choice of λ = 0. TD(0) has been proven to converge, even if the proof\xa0(which can be found for a model-based approach in Convergence of Model-Based Temporal Difference Learning for Control, Van Hasselt H., Wiering M. A., Proceedings of the 2007 IEEE Symposium on Approximate Dynamic\xa0Programming and Reinforcement Learning (ADPRL 2007))\xa0is more complex because it's necessary to consider the evolution of the Markov Process. In fact, in this case, we are approximating the expected discounted return with both a truncated estimation and a bootstrap value,\xa0V(sj), which is initially (and for a large number of iterations) unstable.\xa0However, assuming the convergence for t\xa0→\xa0∞, we get: The last formula expresses the value of the state,\xa0si, assuming that the greedy optimal policy forces the agent to perform the action that causes the transition to sj. Of course, at this point, it's natural to ask under which conditions the algorithm converges. In fact, we are considering episodic tasks and the estimation,\xa0V(∞)(si), can be correct only if the agent performs a transition to si\xa0an infinite number of times, selecting all possible actions an infinite number of times.\xa0Such a condition is often expressed by saying that the policy must be\xa0Greedy in the Limit with Infinite Explorations (GLIE). In other words, the real greediness is achieved only as an asymptotic state when the agent is able to explore the environment without limitations for an unlimited number of episodes. This is probably the most important limitation of TD RL, because, in real-life scenarios, some states can be very unlikely and, hence, the estimation can never accumulate the experience needed to converge to the actual value. We are going to analyze some methods to solve this problem in Chapter 15, Advanced Policy Estimation Algorithms, but, in our example, we employ a random start. In other words, as the policy is greedy and could always avoid some states, we force the agent to start each episode in a random nonterminal cell. In this way, we allow a deep exploration even with a greedy policy. Whenever this approach is not feasible (because, for example, the environment dynamics are not controllable), the exploration-exploitation dilemma can be solved only by employing an ε-greedy policy, which selects a fraction of suboptimal (or even wrong) actions. In this way, it's possible to observe a higher number of transitions paying the price of a slower convergence. However, as pointed out by Sutton and Barto, TD(0) converges to the maximum-likelihood estimation of the value function determined by the MDP, finding the implicit transition probabilities of the model. Therefore, if the number of observations is high enough, TD(0) can quickly find an optimal policy, but, at the same time, it's also more sensitive to biased estimations if some couple's state-action are never experienced (or experienced very seldom). In our example, we don't know which the initial state is, hence selecting a fixed starting point yields a policy that is extremely rigid and almost completely unable to manage noisy situations. For example, if the starting point is changed to an adjacent (but never explored) cell, the algorithm could fail to find the optimal path to the positive terminal state. On the other hand, if we know that the dynamics are well-defined, TD(0) will force the agent to select the actions that are most likely to produce the optimal result given the current knowledge of the environment. If the dynamics\xa0are partially stochastic, the advantage of an\xa0ε-greedy policy can be understood considering a sequence of episodes where the agent experiences the same transitions and the corresponding values are increased proportionally. If, for example, the environment changes one transition after many experiences, the agent has to face a brand new experience when the policy is already almost stable. The correction requires many episodes and, as this random change has a very low probability, it's possible that the agent will never learn the correct behavior. Instead, by selecting a few random actions, the probability of encountering a similar state (or even the same one) increases (think about a game where the state is represented by a screenshot) and the algorithm can become more robust with respect to very unlikely transitions. The complete\xa0TD(0)\xa0algorithm is:", '67ec2903-98a3-48f4-aaf7-b15f3357449d.xhtml': "At this point, we can test the TD(0) algorithm on the checkerboard environment. The first step is to define an initial random policy and a value matrix with all elements equal to 0: As we want to select a random starting point at the beginning of each episode, we need to define a helper function that must exclude the terminal states (all the constants are the same as previously defined): Now we can implement the function to evaluate a single episode (setting the maximum number of steps equal to 500 and the constant to\xa0α = 0.25): The function to determine the greedy policy with respect to the values is the same as already implemented in the previous examples; however, we report it to guarantee the consistency of the example: At this point, we can start a training cycle with 5,000 episodes: The final value matrix is shown in the following chart: Like in the previous examples, the final values are inversely proportional to the distance from the final positive state. Let's analyze the resulting policy to understand whether the algorithm converged to a consistent solution: As can be seen, the random choice of the starting state is allowed to find the best path independently from the initial condition. To better understand the advantage of this strategy, let's plot the final value matrix when the initial state is fixed to the cell (0, 0), corresponding to the upper-left corner: Without any further analysis, it's possible to see that many states have never been visited or visited only a few times, and the resulting policy is therefore extremely greedy with respect to the specific initial state. The blocks containing values equal to -1.0 indicate states where the agent often has to pick a random action because there's no difference in the values, hence it can be extremely difficult to solve the environment with a different initial state. The resulting policy confirms this analysis: As it's possible to see, the agent is able to reach the final state only when the initial point allows us to cross the trajectory starting from (0, 0). In all these cases, it's possible to recover the optimal policy, even if the paths longer than the ones obtained in the previous example. Instead, states such as\xa0(0, 4) are clearly situations where there's a loss of policy. In other words, the agent acts without any knowledge or awareness and the probability of success converges to 0. As an exercise, I invite the reader to test this algorithm with different starting points (for example, a set of fixed ones) and higher\xa0α values. The goal is also to answer these questions: Is it possible to speed up the learning process? Is it necessary to start from all possible states in order to obtain a global optimal policy?", '93916b02-cf7a-4ac5-873e-11ca023ffcfd.xhtml': "In this chapter, we introduced the most important RL concepts, focusing on the mathematical structure of an environment as a Markov Decision Process, and on the different kinds of policy and how they can be derived from the expected reward obtained by an agent. In particular, we defined the value of a state as the expected future reward considering a sequence discounted by a factor,\xa0γ. In the same way, we introduced the concept of the Q function, which is the value of an action when the agent is in a specific state. These concepts directly employed the policy iteration algorithm, which is based on a Dynamic Programming approach assuming complete knowledge of the environment. The task is split into two stages; during the first one, the agent evaluates all the states given the current policy, while in the second one, the policy is updated in order to be greedy with respect to the new value function. In this way, the agent is forced to always pick the action that leads to a transition that maximizes the obtained value. We also analyzed a variant, called value iteration, that performs a single evaluation and selects the policy in a greedy manner. The main difference from the previous approach is that now the agent immediately selects the highest value assuming that the result of this process is equivalent to a policy iteration. Indeed, it's easy to prove that, after infinite transitions, both algorithms converge on the optimal value function. The last algorithm is called TD(0) and it's based on a model-free approach. In fact, in many cases, it's difficult to know all the transition probabilities and, sometimes, even all possible states are unknown. This method is based on the Temporal Difference evaluation, which is performed directly while interacting with the environment. If the agent can visit all the states an infinite number of times (clearly, this is only a theoretical condition), the algorithm has been proven to converge to the optimal value function more quickly than other methods. In the next chapter, Chapter 15,\xa0Advanced Policy Estimation Algorithms we'll continue the discussion of RL algorithms, introducing some more advanced methods that can be immediately implemented using Deep Convolutional Networks.", 'ded12e8b-5909-45af-8a3f-6faa52a25ed8.xhtml': 'In this chapter, we will continue our exploration of the world of Reinforcement Learning (RL), focusing our attention on complex algorithms that can be employed to solve difficult problems. As this is still the introductory part of RL (the whole topic is extremely large), the structure of the chapter is based on many practical examples that can be used as a basis to work on more complex scenarios. The topics that will be discussed in this chapter are:', 'ae71efc5-6cfd-4547-a13b-8b99b7f1b3e1.xhtml': "In the previous chapter, we introduced the temporal difference strategy, and we discussed a simple example called TD(0). In the case of TD(0), the discounted reward is approximated by using a one-step backup. Hence, if the agent performs an action at in the state\xa0st, and the transition to the state st+1 is observed, the approximation becomes the following: If the task is episodic (as in many real-life scenarios) and has T(ei) steps, the complete backup for the episode ei is as follows: The previous expression ends when the MDP process reaches an absorbing state; therefore, Rt is the actual value of the discounted reward. The difference between TD(0) and this choice is clear: in the first case, we can update the value function after each transition, whereas with a complete backup, we need to wait for the end of the episode. We can say that this method (which is called Monte Carlo, because it's based on the idea of\xa0averaging the overall reward of an entire sequence) is exactly the opposite of TD(0); therefore, it's reasonable to think about an intermediate solution, based on k-step backups. In particular, our goal is to find an online algorithm that can exploit the backups once they are available. Let's imagine a sequence of four steps. The agent is in the first state and observes a transition; at this point, only a one-step backup is possible, and it's a good idea to update the value function in order to improve the convergence speed. After the second transition, the agent can use a two-step backup; however, it can also consider the first one-step backup in addition to the newer, longer one. So, we have two approximations: Which of the preceding is the most reliable? Obviously, the second one depends on the first one (in particular, when the value function is almost stabilized), and so on until the end of the episode. Hence, the most common strategy is to employ a weighted average that assigns a different level of importance to each backup (assuming the longest backup has k steps): Watkins (in Learning from Delayed Rewards,\xa0Watkins C.I.C.H.,\xa0Ph.D. Thesis, University of Cambridge, 1989) proved that this approach (with or without averaging) has the fundamental property of reducing the absolute error of the expected Rtk, with respect to the optimal value function,\xa0V(s;\xa0π). In fact, he proved that the following inequality holds: As γ is bounded between 0 and 1, the right-hand side is always smaller than the maximum absolute error V(t) - V(s;π), where V(s) is the value of a state during an episode. Therefore, the expected discounted return of a k-step backup (or of a combination of different backups) yields a more accurate estimation of the optimal value function if the policy is chosen to be greedy with respect to it. This is not surprising, as a longer backup incorporates more actual returns, but the importance of this theorem resides in its validity when an average of different k-step backups are employed. In other words, it provides us with the mathematical proof that an intuitive approach actually converges, and it can also effectively improve both the convergence speed and the final accuracy. However, managing k coefficients is generally problematic, and in many cases, useless. The main idea behind TD(λ) is to employ a single factor,\xa0λ, that can be tuned in order to meet specific requirements. The theoretical analysis (or forward view, as referred to by Sutton and Barto) is based, in a general case, on an exponentially decaying average. If we consider a geometric series with λ bounded between 0 and 1 (exclusive), we get: Hence, we can consider the averaged discounted return Rt(λ) with infinite backups as: Before defining the finite case, it's helpful to understand how\xa0Rt(λ) was built. As\xa0λ is bounded between 0 and 1, the factors decay proportionally to\xa0λ, so the first backup has the highest impact, and all of the subsequent ones have smaller and smaller influences on the estimation. This means that, in general, we are assuming that the estimation of Rt has more importance to the immediate backups (which become more and more precise), and we exploit the longer ones only to improve the estimated value. Now, it should be clear that\xa0λ = 0 is equivalent to TD(0), because only the one-step backup remains in the sum (remember that 00 = 1), while higher values involve all of the remaining backups.\xa0Let's now consider an episode ei whose length is T(ei). Conventionally, if the agent reached an absorbing state at t = T(ei), all of the remaining t+i returns are equal to Rt\xa0(this is straightforward, as all of the possible rewards have already been collected); therefore, we can truncate\xa0Rt(λ): The first term of the previous expression involves all of the non-terminal states, while the second is equal to Rt discounted proportionally to the distance between the first time step and the final state. Again, if\xa0λ = 0, we obtain TD(0), but we are now also authorized to consider\xa0λ = 1 (because the sum is always extended to a finite number of elements). When\xa0λ = 1, we obtain\xa0Rt(λ) = Rt, which means that we need to wait until the end of the episode to get the actual discounted reward. As explained previously, this method is normally not a first-choice solution, because when the episodes are very long, the agent selects the actions with a value function that is not up to date in the majority of cases. Therefore, TD(λ) is normally employed with λ values less than 1, in order to obtain the advantage of an online update, together with a correction based on the new states. To achieve this goal without looking at the future (we want to update V(s) as soon as new pieces of information are available), we need to introduce the concept of eligibility trace\xa0e(s) (sometimes, in the context of computational neuroscience, e(s) is also called stimulus trace). An eligibility trace for a state s is a function of time that returns the weight (greater than 0) of the specific state. Let's imagine a sequence, s1, s2, ..., sn, and consider a state, si. After a backup\xa0V(si) is updated, the agent continues its exploration. When is a new update of si (given longer backups) important? If si is not visited anymore, the effect of longer backups must be smaller and smaller, and si is said to not be eligible for changes in V(s). This is a consequence of the previous assumption that shorter backups must generally have higher importance. So, if si is an initial state (or is immediately after the initial state) and the agent moves to other states, the effect of si must decay. Conversely, if si is revisited, it means that the previous estimation of\xa0V(si) is probably wrong, and hence\xa0si is eligible for a change. (To better understand this concept, imagine a sequence, s1, s2, s1, ....\xa0It's clear that when the agent is in s1, as well as in s2, it cannot select the right action; therefore, it's necessary to reevaluate V(s) until the agent is able to move forward.) The most common strategy (which is also discussed in Reinforcement Learning,\xa0Sutton R. S.,\u200e Barto A. G., The MIT Press) is to define the eligibility traces in a recursive fashion. After each time step, et(s) decays by a factor equal to γλ (to meet the requirement imposed by the forward view); but, when the state s is revisited, et(s) is also increased by 1 (et(s) =\xa0γλet-1(s) + 1). In this way, we impose a jump in the trend of e(s) whenever we desire to emphasize its impact. However, as e(s) decays independently of the jumps, the states that are visited and revisited later have a lower impact than the ones that are revisited very soon. The reason for this choice is very intuitive: the importance of a state revisited after a long sequence is clearly lower than the importance of a state that is revisited after a few steps. In fact, the estimation of Rt is obviously wrong if the agent moves back and forth between two states at the beginning of the episode, but the error becomes less significant when the agent revisits a state after having explored other areas. For example, a policy can allow an initial phase in order to reach a partial goal, and then it can force the agent to move back to reach a terminal state. Exploiting the eligibility traces, TD(λ) can achieve a very fast convergence in more complex environments, with a trade-off between a one-step TD method and a Monte Carlo one (which is normally avoided). At this point, the reader might wonder if we are sure about the convergence, and luckily, the answer is positive. Dayan proved (in\xa0The convergence of TD (λ) for General λ, Dayan P., Machine Learning 8, 3–4/1992) that TD(λ) converges for a generic\xa0λ with only a few specific assumptions and the fundamental condition that the policy is GLIE. The proof is very technical, and it's beyond the scope of this book; however, the most important assumptions (which are generally met) are: The first condition is obvious, the absence of absorbing states yields infinite explorations, which are not compatible with a TD method (sometimes it's possible to prematurely end an episode, but this can either be unacceptable (in some contexts) or a sub-optimal choice (in many others)). Moreover, Sutton and Barto (in the aforementioned book) proved that TD(λ) is equivalent to employing the weighted average of discounted return approximations, but without the constraint of looking ahead in the future (which is clearly impossible). The complete\xa0TD(λ)\xa0algorithm (with an optional forced termination of the episode) is: The reader can better understand the logic of this algorithm by considering the TD error and its back-propagation. Even if this is only a comparison, it's possible to imagine the behavior of TD(λ) as similar to the Stochastic Gradient Descent (SGD) algorithms employed to train a neural network. In fact, the error is propagated to the previous states (analogous to the lower layers of an MLP) and affects them proportionally to their importance, which is defined by their eligibility traces. Hence, a state with a higher eligibility trace can be considered more responsible for the error; therefore, the corresponding value must be corrected proportionally. This isn't a formal explanation, but it can simplify comprehension of the dynamics without an excessive loss of rigor.", '3f636017-30fc-4528-82a8-fa3d2fc5c228.xhtml': "At this point, we want to test the TD(λ) algorithm with a slightly more complex tunnel environment. In fact, together with the absorbing states, we will also consider some intermediate positive states, which can be imagined as checkpoints. An agent should learn the optimal path from any cell to the final state, trying to pass through the highest number of checkpoints possible. Let's start by defining the new structure: The reward structure is shown in the following diagram: At this point, we can proceed to initialize all of the constants (in particular, we have chosen λ = 0.6, which is an intermediate solution that guarantees an awareness close to a Monte Carlo method, without compromising the learning speed): As we want to start from a random cell, we need to repeat the same procedure presented in the previous chapter; but, in this case, we are also including the checkpoint states: We can now define the\xa0episode() function, which implements a complete TD(λ) cycle. As we don't want the agent to roam around trying to pass through the checkpoints an infinite number of times, we have decided to reduce the reward during the exploration, to incentivize the agent to pass through only the necessary checkpoints—trying, at the same time, to reach the final state as soon as possible: The\xa0is_final() and policy_selection() functions\xa0are the same\xa0ones defined in the previous chapter, and need no explanation. Even if it's not really necessary, we have decided to implement a forced termination after a number of steps, equal to max_steps. This is helpful at the beginning because as the policy is not\xa0ε-greedy, the agent can remain stuck in a looping exploration that never ends. We can now train the model for a fixed number of episodes (alternatively, it's possible to stop the process when the value array doesn't change anymore): The episode() function returns the total rewards; therefore, it's useful to check how the agent learning process evolved: At the beginning (for about 500 episodes), the agent employs an unacceptable policy that yields very negative total rewards. However, in about 1,000 iterations, the algorithm reaches an optimal policy that is only slightly improved by the following episodes. The oscillations are due to the different starting points; however, the total rewards are never negative, and as the checkpoint weights decay, this is a positive signal, indicating that the agent reaches the final positive state. To have a confirmation of this hypothesis, we can plot the learned value function: The values are coherent with our initial analysis; in fact, they tend to be higher when the cell is close to a checkpoint, but, at the same time, the global configuration (considering a policy greedy with respect to V(s)) forces the agent to reach the ending state whose surrounding values are the highest. The last step is checking the actual policy, with a particular focus on the checkpoints: As it's possible to observe, the agent tries to pass through the checkpoints, but when it's close to the final state, it (correctly) prefers to end the episode as soon as possible. I invite the reader to repeat the experiment using different values for the constant\xa0λ, and changing the environment dynamics for the checkpoints. What happens if their values remain the same? Is it possible to improve the policy with a higher\xa0λ?", '56001c21-fe2d-4ca2-b0ed-330cef375ca5.xhtml': "In this example, we want to employ an alternative algorithm called Actor-Critic, together with TD(0). In this method, the agent is split into two components, a Critic, which is responsible for evaluating the quality of the value estimation, and an actor, which selects and performs an action. As pointed out by Dayan (in Theoretical Neuroscience,\xa0Dayan P.,\xa0Abbott L.\xa0F.,\xa0The MIT Press), the dynamics of an Actor-Critic approach are similar to the interleaving policy evaluation and policy improvement steps. In fact, the knowledge of the Critic is obtained through an iterative process, and its initial evaluations are normally sub-optimal. The structural schema is shown in the following diagram: In this particular case, it's preferable to employ a\xa0ε-greedy soft policy, based on the softmax function. The model stores a matrix (or an approximating function) called policy importance, where each entry pi(s, a) is a value representing the preference for a specific action in a certain state. The actual stochastic policy is obtained by applying the softmax with a simple trick to increase the numerical stability when the exponentials become very large: After performing the action a\xa0in the state si and observing the transition to the state sj with a reward rij, the Critic evaluates the TD error: If V(si) < rij +\xa0γV(sj), the transition is considered positive, because the value is increasing. Conversely, when\xa0V(si) > rij\xa0+\xa0γV(sj), the Critic evaluates the action as negative, because the previous value was higher than the new estimation. A more general approach is based on the concept of advantage, which is defined as: Normally, one of the terms from the previous expression can be approximated. In our case, we cannot compute the Q function directly; hence, we approximate it with the term rij +\xa0γV(sj). It's clear that the role of the advantage is analogous to the one of the TD error (which is an approximation) and must represent the confirmation that an action in a certain state is a good or bad choice. An analysis of all advantage Actor-Critic (A3C) algorithms (in other words, improvements of the standard policy gradient algorithm) is beyond the scope of this book. However, the reader can find some helpful pieces of information in High-Dimensional Continuous Control Using Generalized Advantage Estimation,\xa0Schulman J.,\xa0Moritz P.,\xa0Levine S.,\xa0Jordan M.\xa0I.,\xa0Abbeel P.,\xa0ICLR 2016. Of course, an Actor-Critic correction is not sufficient. To improve the policy, it's necessary to employ a standard algorithm (such as TD(0), TD(λ), or least square regression, which can be implemented using a neural network) in order to learn the correct value function,\xa0V(s). As for many other algorithms, this process can converge only after a sufficiently high number of iterations, which must be exploited to visit the states many times, experimenting with all possible actions. Hence, with a TD(0) approach, the first step after evaluating the TD error is updating V(s) using the rule defined in the previous chapter: The second step is more pragmatic; in fact, the main role of the Critic is actually to criticize every action, deciding when it's better to increase or decrease the probability of selecting it again in a certain state. This goal can be achieved by simply updating the policy importance: The role of the learning rate ρ is extremely important; in fact, incorrect values (in other words, values that are too high) can yield initial wrong corrections that may compromise the convergence. It's essential to not forget that the value function is almost completely unknown at the beginning, and therefore the Critic has no chance to increase the right probability with awareness. For this reason, I always suggest to start with very small value (ρ =\xa00.001) and increase it only if the convergence speed of the algorithm is effectively improved. As the policy is based on the softmax function, after a Critic update, the values will always be renormalized, resulting in an actual probability distribution. After an adequately large number of iterations, with the right choice of both\xa0ρ and\xa0γ, the model is able to learn both a stochastic policy and a value function. Therefore, it's possible to employ the trained agent by always selecting the action with the highest probability (which corresponds to an implicitly greedy behavior): Let's now apply this algorithm to the tunnel environment. The first step is defining the constants (as we are looking for a long sighted agent, we are setting the discount factor\xa0γ = 0.99): At this point, we need to define the policy importance array, and a function to generate the softmax policy: The functions needed to implement a single training step are very straightforward, and the reader should already be familiar with their structure: At this point, we can train the model with 50,000 iterations, and 30,000 explorative ones (with a linear decay of the exploration factor): The resulting greedy policy is shown in the following figure: The final greedy policy is consistent with the objective, and the agent always reaches the final positive state by avoiding the wells. This kind of algorithm can appear more complex than necessary; however, in complex situations, it turns out to be extremely effective. In fact, the learning process can be dramatically improved, thanks to the fast corrections performed by the Critic. Moreover, the author has noticed that the Actor-Critic is more robust to wrong (or noisy) evaluations. As the policy is learned separately, the effect of small variations in V(s) cannot easily change the probabilities\xa0π(s, a) (in particular, when an action is generally much stronger than the others). On the other hand, as discussed previously, it's necessary to avoid a premature convergence in order to let the algorithm modify the importance/probabilities, without an excessive number of iterations. The right trade-off can be found only after a complete analysis of each specific scenario, and unfortunately, there are no general rules that work in every case. My suggestion is to test various configurations, starting with small values (and, for example, a discount factor of\xa0γ\xa0∈ [0.7, 0.9]), evaluating the total reward achieved after the same exploration period. Complex deep learning models (such as asynchronous A3C; see Asynchronous Methods for Deep Reinforcement Learning,\xa0Mnih V.,\xa0Puigdomènech Badia A.,\xa0Mirza M.,\xa0Graves A.,\xa0Lillicrap T.\xa0P.,\xa0Harley T.,\xa0Silver D.,\xa0Kavukcuoglu K.,\xa0arXiv:1602.01783 [cs.LG] for further information) are based on a single network that outputs both the softmax policy (whose actions are generally proportional to their probability) and the value. Instead of employing an explicitly\xa0ε-greedy soft policy, it's possible to add a maximum-entropy constraint to the global cost function: As the entropy is at the maximum when all of the actions have the same probability, this constraint (with an appropriate weight) forces the algorithm to increase the exploration probability until an action becomes dominant and there's no more need to avoid a greedy selection. This is a sound and easy way to employ an adaptive\xa0ε-greedy policy, because as the model works with each state separately, the states where the uncertainty is very low can become greedy; it's possible to automatically keep a high entropy whenever it's necessary to continue the exploration, in order to maximize the reward. The effect of double correction, together with a maximum-entropy constraint, improves the convergence speed of the model, encourages the exploration during the initial iterations, and yields very high final accuracy. I invite the reader to implement this variant with other scenarios and algorithms. In particular, at the end of this chapter, we are going to experiment with an algorithm based on a neural network. As the example is pretty simple, I suggest using Tensorflow to create a small network based on the Actor-Critic approach. The reader can employ a mean squared error loss for the value\xa0and softmax cross entropy for the policy. Once the models work successfully with our toy examples, it will be possible to start working with more complex scenarios (like the ones proposed in OpenAI Gym at\xa0https://gym.openai.com/).", '791d7c82-0384-499e-babd-8aaa33321c68.xhtml': "SARSA (whose name is derived from the sequence state-action-reward-state-action) is a natural extension of TD(0) to the estimation of the Q function. Its standard formulation (which is sometimes called one-step SARSA, or SARSA(0), for the same reasons explained in the previous chapter) is based on a single next reward, rt+1, which is obtained by executing the action at in the state st.\xa0The temporal difference computation is based on the following update rule: The equation is equivalent to TD(0), and if the policy is chosen to be GLIE, it has been proven (in Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms, Singh S.,\xa0Jaakkola T.,\xa0Littman M.\xa0L.,\xa0Szepesvári C.,\xa0Machine Learning, 39/2000)\xa0that SARSA converges to an optimal policy, πopt(s), with the probability 1, when all couples (state, action) are experienced an infinite number of times. This means that if the policy is updated to be greedy with respect to the current value function induced by Q, it holds that: The same result is valid for the Q function. In particular, the most important conditions required by the proof are: The first condition is particularly important when\xa0α is a function of the state and the time step; however, in many cases, it is a constant bounded between 0 and 1, and hence,\xa0Σα2\xa0=\xa0∞. A common way to solve this problem (above all when a large number of iterations are required) is to let the learning rate decay (in other words, exponentially) during the training process. Instead, to mitigate the effect of very large rewards, it's possible to clip them in a suitable range ([-1, 1]). In many cases, it's not necessary to employ these strategies, but in more complex scenarios, they can become crucial in order to ensure the convergence of the algorithm.\xa0Moreover, as pointed out in the previous chapter, these kinds of algorithms need a long exploration phase before starting to stabilize the policy. The most common strategy is to employ a\xa0ε-greedy policy, with a temporal decay of the exploration factor. During the first iterations, the agent must explore without caring about the returns of the actions. In this way, it's possible to assess the actual values before the beginning of a final refining phase characterized by a purely greedy exploration, based on a more precise approximation of V(s). The complete\xa0SARSA(0)\xa0algorithm (with an optional forced termination of the episode) is:", '9a50dff4-f886-4164-b7bb-5e7b204fe839.xhtml': "We can now test the SARSA algorithm in the original tunnel environment (all of the elements that are not redefined are the same as the previous chapter). The first step is defining the Q(s, a) array and the constants employed in the training process: As we want to employ a\xa0ε-greedy policy, we can set the starting point to (0, 0), forcing the agent to reach the positive final state. We can now define the functions needed to perform a training step: The\xa0select_action()\xa0function has been designed to select a random action with the probability\xa0ε, and a greedy one with respect to Q(s, a), with the probability 1 -\xa0ε. The\xa0sarsa_step()\xa0function is straightforward, and executes a complete episode updating the Q(s, a) (that's why this is an online algorithm). At this point, it's possible to train the model for 20,000 episodes and employ a linear decay for\xa0ε during the first 15,000 episodes (when t > 15,000, ε is set equal to 0 in order to employ a purely greedy policy): As usual, let's check the learned values (considering that the policy is greedy, we're going to plot V(s) = maxa Q(s, a)): As expected, the Q function has been learned in a consistent way, and we can get a confirmation plotting the resulting policy: The policy is coherent with the initial objective, and the agent avoids all negative absorbing states, always trying to move towards the final positive state. However, some paths seem longer than expected. As an exercise, I invite the reader to retrain the model for a larger number of iterations, adjusting the exploration period. Moreover, is it possible to improve the model by increasing (or decreasing) the discount factor\xa0γ? Remember that\xa0γ\xa0→ 0 leads to a short-sighted agent, which is able to select actions only considering the immediate reward, while\xa0γ\xa0→ 1 forces the agent to take into account a larger number of future rewards. This particular example is based on a long environment, because the agent always starts from (0, 0) and must reach the farthest point; therefore, all intermediate states have less importance, and it's helpful to look at the future to pick the optimal actions. Using random starts can surely improve the policy for all initial states, but it's interesting to investigate how different\xa0γ values can affect the decisions; hence, I suggest repeating the experiment in order to evaluate the various configurations and increase awareness about the different factors that are involved in a TD algorithm.", 'dd176f16-53be-41f0-82d0-747316c39a1e.xhtml': "This algorithm was\xa0proposed by Watkins (in\xa0Learning from delayed rewards, Watkins C.I.C.H.,\xa0Ph.D. Thesis, University of Cambridge, 1989; and further analyzed in\xa0Watkins C.I.C.H., Dayan P., Technical Note Q-Learning, Machine Learning 8, 1992) as a more efficient alternative to SARSA. The main feature of Q-learning is that the TD update rule is immediately greedy with respect to the Q(st+1, a) function: The key idea is to compare the current Q(st, at) value with the maximum Q value achievable when the agent is in the successor state. In fact, as the policy must be GLIE, the convergence speed can be increased by avoiding wrong estimations due to the selection of a Q value that won't be associated with the final action. By choosing the maximum Q value, the algorithm will move towards the optimal solution faster than SARSA, and also, the convergence proof is less restrictive. In fact, Watkins and Dayan (in the aforementioned papers) proved that, if |ri| < R, the learning rate\xa0α\xa0∈ [0, 1[ (in this case,\xa0α must be always smaller than 1) with the same constraints imposed for SARSA (Σα =\xa0∞ and\xa0Σα2\xa0< ∞), then the estimated Q function converges with probability 1 to the optimal one: As discussed for SARSA, the conditions on the rewards and the learning rate can be managed by employing a clipping function and a temporal decay, respectively. In almost all deep Q-learning applications, these are extremely important factors to guarantee the convergence; therefore, I invite the reader to consider them whenever the training process isn't able to converge to an acceptable solution.\xa0 The complete\xa0Q-learning\xa0algorithm (with an optional forced termination of the episode) is:", '19e62c77-93c7-4976-ae0e-6e0a945f7b54.xhtml': "Let's repeat the previous experiment with the Q-learning algorithm. As all of the constants are the same (as well as the choice of a ε-greedy policy and the starting point set to (0, 0)), we can directly define the function that implements the training for a single episode: We can now train the model for 5,000 iterations, with 3,500 explorative ones: The resulting value matrix (defined as in the SARSA experiment) is: Again, the learned Q function (and obviously, also the greedy V(s)) is coherent with the initial objective (in particular, considering the starting point set to (0, 0)), and the resulting policy can immediately confirm this result: The behavior of Q-learning is not very different from SARSA (even if the convergence is faster), and some initial states are not perfectly managed. This is a consequence of our choice; therefore, I invite the reader to repeat the exercise using random starts and comparing the training speed of Q-learning and SARSA.", 'b03f949f-4332-4de4-9a37-20209ffeb184.xhtml': "Now, we want to test the Q-learning algorithm using a smaller checkerboard environment and a neural network (with Keras). The main difference from the previous examples is that now, the state is represented by a screenshot of the current configuration; hence, the model has to learn how to associate a value with each input image and action. This isn't actual deep Q-learning (which is based on Deep Convolutional Networks, and requires more complex environments that we cannot discuss in this book), but it shows how such a model can learn an optimal policy with the same input provided to a human being. In order to reduce the training time, we are considering a square checkerboard environment, with four negative absorbing states and a positive final one: A graphical representation of the rewards is shown in the following figure: As we want to provide the network with a graphical input, we need to define a function to create a matrix representing the tunnel: The\xa0reset_tunnel()\xa0function sets all values equal to 0, except for (which is marked with -1) and the final state (defined by 0.5). The position of the agent (defined with the value 1) is directly managed by the training function. At this point, we can create and compile our neural network. As the problem is not very complex, we are employing an MLP: The input is a flattened array, while the output is the Q function (all of the values corresponding to each action). The network is trained using RMSprop and a mean squared error loss function (our goal is to reduce the MSE between the actual value and the prediction). In order to train and query the network, it's helpful to create two dedicated functions: The behavior of these functions is straightforward. The only element that may be new to the reader is the use of the\xa0train_on_batch() method. Contrary to fit(), this function allows us to perform a single training step, given a batch of input-output couples (in our case, we always have a single couple). As our goal is finding an optimal path to the final state, starting from every possible cell, we are going to employ random starts: Now, we can define the functions needed to perform a single training step: The\xa0q_step_neural_network()\xa0function is very similar to the one defined in the previous example. The only difference is the management of the visual state. Every time there's a transition, the value 1.0 (denoting the agent) is moved from the old position to the new one, and the value of the previous cell is reset to its default (saved in the\xa0prev_value variable). Another\xa0secondary difference is the absence of\xa0α\xa0because there's already a learning rate set in the SGD algorithm, and it doesn't make sense to add another parameter to the model. We can now train the model for 10,000 iterations, with 7,500 explorative ones: When the training process has finished, we can analyze the total rewards, in order to understand whether the network has successfully learned the Q functions: It's clear that the model is working well, because after the exploration period, the total reward becomes stationary around 4, with small oscillations due to the different path lengths (however, the final plot can be different because of the internal random state employed by Keras). To see a confirmation, let's generate the trajectories for all of the possible initial states, using the greedy policy (equivalent to ε = 0): Twelve random trajectories are shown in the following figure: The agent always follows the optimal policy, independent from the initial state, and never ends up in a well. Even if the example is quite simple, it's helpful to introduce the reader to the concept of deep Q-learning (for further details, the reader can check the introductory paper,\xa0Deep Reinforcement Learning: An Overview, Li Y., arXiv:1701.07274 [cs.LG]). In a general case, the environment can be a more complex game (like Atari or Sega), and the number of possible actions is very limited. Moreover, there's no possibility to employ random starts, but it's generally a good practice to skip a number of initial frames, in order to avoid a bias to the estimator. Clearly, the network must be more complex (involving convolutions to better learn the geometric dependencies), and the number of iterations must be extremely large. Many other tricks and specific algorithms can be employed in order to speed up the convergence, but due to a lack of space, they are beyond the scope of this book. However, the general process and its logic are almost the same, and it's not difficult to understand why some strategies are preferable, and how the accuracy can be improved. As an exercise, I invite the reader to create more complex environments, with or without checkpoints and stochastic rewards. It's not surprising to see how the model will be able to easily learn the dynamics with a sufficiently large number of episodes. Moreover, as suggested in the Actor-Critic section, it's a good idea to use Tensorflow to implement such a model, comparing the performances with Q-learning.", '4f63a797-f301-4494-b471-f02c0b87a232.xhtml': "In this chapter, we presented the natural evolution of TD(0), based on an average of backups with different lengths. The algorithm, called TD(λ), is extremely powerful, and it assures a faster convergence than TD(0), with only a few (non-restrictive) conditions. We also showed how to implement the Actor-Critic method with TD(0), in order to learn about both a stochastic policy and a value function. In further sections, we discussed two methods based on the estimation of the Q function: SARSA and Q-learning. They are very similar, but the latter has a greedy approach, and its performance (in particular, the training speed) results in it being superior to SARSA. The Q-learning algorithm is one of the most important models for the latest developments. In fact, it was the first RL approach employed with a Deep Convolutional Network to solve complex environments (like Atari games). For this reason, we also presented a simple example, based on an MLP that processes a visual input and outputs the Q values for each action. The world of RL is extremely fascinating, and hundreds of researchers work every day to improve algorithms and solve more and more complex problems. I invite the reader to check the references in order to find useful resources that can be exploited to obtain a deeper understanding of the models and their developments. Moreover, I suggest reading\xa0the blog posts written by the Google DeepMind team, which is one of the pioneers in the field of deep RL. I also suggest searching for the papers freely available on arXiv. I'm happy to end this book with this topic, because I believe that RL can provide new and more powerful tools that will dramatically change our lives!", '2637f64e-40c8-4242-9340-4bd88f8f46d1.xhtml': 'If you enjoyed this book, you may be interested in these other books by Packt:  Feature Engineering Made Easy\nSinan Ozdemir, Divya Susarla ISBN: 978-1-78728-760-0  Machine Learning Solutions\nJalaj Thanaki ISBN: 978-1-78839-004-0', '4432d384-04c3-4aaa-a039-397b2e8c1cbc.xhtml': "Please share your thoughts on this book with others by leaving a review on the site that you bought it from. If you purchased the book from Amazon, please leave us an honest review on this book's Amazon page. This is vital so that other potential readers can see and use your unbiased opinion to make purchasing decisions, we can understand what our customers think about our products, and our authors can see your feedback on the title that they have worked with Packt to create. It will only take a few minutes of your time, but is valuable to other potential customers, our authors, and Packt. Thank you!"}